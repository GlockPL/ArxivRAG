{"title": "MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline", "authors": ["Donghoon Han", "Eunhwan Park", "Gisang Lee", "Adam Lee", "Nojun Kwak"], "abstract": "The rapid expansion of multimedia content has made accurately retrieving relevant videos from large collections increasingly challenging. Recent advancements in text-video retrieval have focused on cross-modal interactions, large-scale foundation model training, and probabilistic modeling, yet often neglect the crucial user perspective, leading to discrepancies between user queries and the content retrieved. To address this, we introduce MERLIN (Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel, training-free pipeline that leverages Large Language Models (LLMs) for iterative feedback learning. MERLIN refines query embeddings from a user perspective, enhancing alignment between queries and video content through a dynamic question answering process. Experimental results on datasets like MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves Recall@1, outperforming existing systems and confirming the benefits of integrating LLMs into multimodal retrieval systems for more responsive and context-aware multimedia retrieval\u00b9.", "sections": [{"title": "Introduction", "content": "Multimedia content has recently grown rapidly in both quantity and quality, making the task of finding relevant videos from vast collections increasingly challenging. While recent studies on text-video retrieval have primarily focused on cross-modal interaction (Wang et al., 2023; Huang et al., 2023; Wu et al., 2023; Jin et al., 2023), large-scale foundation model training (Chen et al., 2024b, 2023; Zhao et al., 2024; Wang et al., 2024a) and probabilistic modeling (Hao et al., 2023; Fang et al., 2023; Hao and Zhang, 2023), there remains a notable lack of consideration for the discrepancy in text-video retrieval. For instance, as illustrated in Figure 1, the video caption \"a baby playing with a cat's tail' fails to fully capture the additional context of a playful interaction between the baby and the cat. In real-world scenarios, such discrepancies often arise because users tend to submit succinct queries that do not capture the full context of the videos related to their search intent. Consequently, this mismatch can lead to unsatisfactory retrieval performance. Moreover, neglecting the user perspective, making users to refine their natural language query for multiple times to fully reflect their search intent, degrades the quality of user experience and makes it difficult to understand the search intent, leading to a discrepancy between user queries and the information within the retrieved videos.\nTo address this issue, we introduce MERLIN (Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel training-free and iterative feedback learning pipeline that leverages the power of Large Language Models (LLMs) to augment queries based on the user perspective, thereby mitigating the aforementioned discrepancies and significantly improving the text-video retrieval performance. Inspired by human problem-solving and cognitive feedback mechanisms (Flower and Hayes, 1981; Doherty and Balzer, 1988), we employ an interactive and iterative feedback learning (B\u00f6hm et al., 2019; Stiennon et al., 2020; Ziegler et al., 2019; Wu et al., 2020; Ouyang et al., 2022; Glaese et al., 2022; Aky\u00fcrek et al., 2023; Madaan et al., 2023; Lee et al., 2024; Liang et al., 2024) consisting of a question-and-answer process that iteratively refines query embeddings for text-video retrieval. Moreover, to our best knowledge, MERLIN presents the first implementation of a retrieval\u2013rerank pipeline in the domain of text-video retrieval, establishing a novel framework that prioritizes user intention and interaction in refining search results.\nThe primary strength of MERLIN lies in its capability to iteratively adapt and refine query embeddings without necessitating the costly re-training of pre-trained models. By generating a series of questions based on the metadata and content of the top-retrieved video and answering the question through LLMs, MERLIN iteratively refines query embedding with answer embedding, thereby enhancing semantic relevance of queries and the information contained within the videos. Experimental results on benchmark datasets, including MSR-VTT, MSVD, and ActivityNet, demonstrate the superiority of the retrieval performance (e.g. Recall@K) by showing significant improvement. Specifically, MERLIN boosts text-video retrieval performance (Recall@1) of Google Multimodal Embedding from 44.00 to 78.00 on MSR-VTT, from 52.39 to 77.61 on MSVD and from 56.58 to 68.44 on ActivityNet.\nThe key contributions of our paper are as follows: (1) Introduction of MERLIN, a novel LLM-based framework for multimodal embedding refinement that addresses discrepancies between user queries and video content by integrating user perspectives. (2) Implementation of an iterative, cost-effective method for refining query embeddings using LLMs, significantly reducing computational demands while improving retrieval accuracy. (3) Presentation of the first retrieval-rerank pipeline in text-video retrieval, enhancing interactivity and context-awareness within multimodal systems. (4) Experimental results shows that MERLIN substantially improves Recall@1 on MSR-VTT, MSVD and ActivityNet, thereby demonstrating notable enhancements in zero-shot text-video retrieval."}, {"title": "Related Works", "content": "Dataset. Text-to-video retrieval aims to retrieve relevant videos based on natural language descriptions and several benchmark video datasets (Anne Hendricks et al., 2017; Caba Heilbron et al., 2015; Chen and Dolan, 2011; Xu et al., 2016) have been curated for this task. One notable dataset is ActivityNet (Caba Heilbron et al., 2015), which consists of video-text pairs capturing various human activities. Another widely used dataset is MSR-VTT (Xu et al., 2016), which comprises open-domain web videos paired with natural language descriptions. These datasets provide a diverse range of video content and textual queries, enabling comprehensive evaluation of retrieval systems.\nMethod. Prior studies have focused on cross-modal interaction, large-scale foundation model training, and probabilistic modeling. In cross-modal interaction (Wang et al., 2023; Huang et al., 2023; Jin et al., 2023) have enhanced reasoning abilities by capturing cross-modal similarities at multiple granularity levels, introduced efficient video prompt mechanisms (Lester et al., 2021) with minimal trainable parameters, and improved retrieval with strategies like Disentangled Conceptualization and Set-to-Set Alignment. In foundation model training (Chen et al., 2024b, 2023; Zhao et al., 2024; Wang et al., 2024a) significant advances have been made with the development of large-scale video and vision-language models leveraging extensive web data, and fine-tuning techniques for better performance on downstream tasks. In probabilistic modeling (Hao et al., 2023; Fang et al., 2023; Hao and Zhang, 2023), novel alignment methods and modeling of video and text representations as probabilistic distributions have been proposed to improve text-video retrieval accuracy and address domain adaptation challenges.\nConcurrent to prior studies and our work, (Levy et al., 2023) proposed a chat-based image retrieval system (ChatIR) that interacts with users through conversation to gather additional information beyond the initial query, aiming to better understand and clarify the user's search intent. Different from ChatIR, MERLIN incorporates frame-level answer generation tailored to the specific requirements of text-video retrieval, employing a training-free approach. Furthermore, inspired by Composed Image Retrieval (Liu et al., 2021; Jang et al., 2024), we iteratively refine the embedding by employing"}, {"title": "Multimodal Embedding Refinement via LLM based Iterative Navigation", "content": "In this work, we consider a scenario where a user aims to find a specific video in a search engine, having a firm intent and precise specifications in mind for the desired video. We refer to this desired video as the \"video in mind\" an imagined video that the user seeks to retrieve, encompassing various attributes such as visual content, context, and temporal dynamics. However, expressing the full specifications of this \u201cvideo in mind\" through a single query often leads to a discrepancy between the user's intent and the retrieved results, hindering the search experience.\nWe incorporate this \u201cvideo in mind\" scenario into the text-video retrieval task, where the goal is to retrieve the most relevant video from a collection based on the user's textual query. Inspired by the success of multimodal systems like Google Lens\u00b2, which effectively handle and integrate both image and text inputs, we propose MERLIN, an interactive pipeline that dynamically elicits and incorporates user feedback to bridge the gap between the user's search intent and the retrieved video candidates. MERLIN leverages the power of LLMs to simulate a human-like feedback process, where a \u201chuman-simulating LLM agent\" engages in a multi-turn conversation with the user, asking clarifying questions and providing answers based on the \"video in mind\" \u2013 the imagined video that the agent has in mind based on the user's initial query and subsequent feedback."}, {"title": "Background", "content": "Suppose that we have the query text \\(q \\in Q\\), a video \\(v \\in V\\), where Q and V indicate a set of queries and videos. Using a pre-trained multimodal encoder \\(f_{enc}\\), we obtain the query and video embeddings \\((e_q, e_v)\\) as follows:\n\\[e_q = f_{enc}(q) \\in \\mathbb{R}^{d_q}\\]\n\\[e_v = f_{enc}(v) \\in \\mathbb{R}^{d_v},\\]\nwhere \\(d_q\\) and \\(d_v\\) denote the dimension of query and video embeddings, respectively. The goal of text-video retrieval is to search the most relevant videos \u00fb's from a collection of videos V given a query text q as follows:\n\\[[\\hat{v}_0,..., \\hat{v}_{k-1}] = TOP-K_{v \\in V} (SIM(e_q, e_v)), \\tag{1}\\]\nwhere SIM(\u00b7) is a similarity function (e.g., cosine distance, etc). Additionally, our system utilizes two key components: M and T. Here, M represents the LLMs and template function T applies a pre-defined template to inputs\u00b3,4. Based on this background, we would like to introduce LLM-based iterative navigation, involving multiple rounds of feedback learning and reranking, leading to better performance and interpretability."}, {"title": "Question Generation", "content": "Suppose that we have retrieved candidates \\(\\hat{v}_k^r\\) where r and k indicate the round and the index of the retrieved top K candidates, respectively. We choose \\(\\hat{v}_f\\) as an anchor candidate and generate the question with \\(M_{question}\\) as follows:\n\\[\\hat{q}^r = M_{question} (T_{question} (\\hat{v}_f)) \\tag{2}\\]\nIntuitively, top-ranked candidate is more likely to align with the user's query. This implies that assessing retrieved candidates with question generated from \\(\\hat{v}_f\\) using LLMs would enhance retrieval performance and interpretability."}, {"title": "Human-Simulating Agent", "content": "Video Question Answering. Our underlying assumption is mitigating the discrepancy between user queries and the information within the videos would be helpful for the better retrieval performance. To this end, we answer the question \\(\\hat{q}^r\\) with video \\(\\hat{v}\\) which consists of N frames sampled per second as follows:\n\\[[\\hat{a}^{(r,0)},..., \\hat{a}^{(r,N)}] = M_{answer} (T_{answer} (\\hat{v}), \\hat{q}^r) ) \\tag{3}\\]\nIt is worth noting that in a real-world scenario, \\(M_{answer}\\) could be replaced by a human to consider the user perspective better. Additionally, using N frames allows us to efficiently handle the temporal information inherent to video, capturing the dynamic aspects of the content. This approach enhances our ability to provide a more comprehensive understanding and alignment with the user's query.\nAggregation. The individual generated answers for each frame \\([\\hat{a}^{(r,0)}, . . ., \\hat{a}^{(r,N)}]\\) are now subsequently fed into an Aggregation Module which is designed to summarize the multiple frame-level answers into a coherent and concise response to the original query as follows:\n\\[a^{\\prime \\prime r} = M_{aggr} (T_{aggr} ([\\hat{a}^{(r,0)},..., \\hat{a}^{(r,N)}])) \\tag{4}\\]\nIt is worth noting that Equation 3 provides answers for each frame, however, the summarized answer should capture the importance of the video content. For instance, if the question is \u201cDid a cookie appear in the video?\u201d and individual answers for each frame are [\u201cNo\u201d, \u201cNo\u201d, \u201cYes\u201d, \u201cNo\u201d], the Aggregation Module will summarize and provide the final answer for the video as \"Yes\", since a cookie has appeared in the third frame. This process ensures that the temporal and contextual information from all frames is considered, resulting in a more accurate and relevant response."}, {"title": "Iterative Embedding Refinement for Reranking", "content": "Initially, we obtain the answer embedding \\(e_{a^{\\prime \\prime r}}\\) using the multimodal encoder \\(f_{enc}\\) as follows: \\(e_{a^{\\prime \\prime r}} = f_{enc}(a^{\\prime \\prime r})\\). Our objective is to dynamically refine the embedding by combining the information from the current round's answer with the previous round's refined embedding. To this end, in the pursuit of refining embeddings iteratively to enhance retrieval performance, we employ a spherical linear interpolation (SLERP) (Shoemake, 1985), which is particularly appropriated for interpolating between embeddings on the unit sphere, preserving the norm and the geometric properties of the embeddings. Given the embeddings \\(e_{a^{\\prime \\prime r-1}}\\) from the previous round and \\(e_{a^{\\prime \\prime r}}\\), the angle \u03b8 between them is computed as:\n\\[\\theta = arccos(e_{a^{\\prime \\prime r}} . e_{a^{\\prime \\prime r-1}}) \\tag{5}\\]\nNote that the angle is essential for determining the interpolation path. Finally, the refined embedding for the current round \\(e^{r}\\) is then calculated as:\n\\[e^r = \\frac{sin((1-\\alpha)\\theta)}{sin(\\theta)}e_{a^{\\prime \\prime r}} + \\frac{sin(\\alpha\\theta)}{sin(\\theta)}e^{r-1}, \\tag{6}\\]\nwhere \\(\\alpha \\in [0,1]\\) is a hyperparameter that balances the influence of the current answer embedding and the previous refined embedding. This interpolation not only ensures a smooth transition across embedding spaces but also incorporates both the originality of the current response and the semantic context retained from prior interactions. We assume that the potential risk of the iterative embedding refinement is query drift (Mitra et al., 1998; Zighelnic and Kurland, 2008; Shtok et al., 2012), a common phenomenon in information retrieval where the focus inadvertently shifts away from the original query intent due to the inclusion of progressively accumulated details. To mitigate the potential risk, we set the \\(\\alpha = 0.8\\), prioritizing the query and earlier answer embeddings over the most recent answers. We expect that this simple yet effective strategy would preserve the thematic integrity of the initial query, akin to human conversational patterns where early-mentioned topics typically set the context for the entire conversation."}, {"title": "Experimental Results", "content": "To utilize mutlimodal encoders and LLMs without needing private GPUs, we use Google Multimodal Embedding API for encoding video and text, and the OpenAI GPT-40 API (Achiam et al., 2023)7 for generating questions and answers. These APIs offers comparable performance and reproducibility on benchmarks without private GPUs.\nWe evaluate MERLIN across three datasets: MSR-VTT, MSVD, and ActivityNet. For MSR-VTT, we sampled 500 videos from its 1,000-sample validation split. From MSVD and ActivityNet, we sampled all 670 and 919 videos from their respective test sets. For videos with multiple captions, we randomly selected one query per video."}, {"title": "Performance on Text-Video Retrieval", "content": "The performance of our system is presented in Table 1, demonstrating its efficacy through multiple rounds of feedback learning, reflecting the system's ability to iteratively to refine and incorporate feedback. Particularly, MERLIN shows significant improvements with each round of feedback: On the MSR-VTT dataset, MERLIN shows improvements from a R@1 of 44.40 to 78.00, on the MSVD from 52.39 to 77.61, and on ActivityNet from 56.58 to 68.44 by the final round.\nThis highlights MERLIN's capacity to adapt and enhance its response through iterative feedback learning. Despite the distinct challenges posed by each dataset, MERLIN significantly boosts its performance, thereby affirming the effectiveness of leveraging iterative feedback learning to enhance text-video retrieval task."}, {"title": "Average Ranking of QA Rounds", "content": "In addition to the retrieval performance presented in Table 1, the effectiveness of the iterative query enrichment is further highlighted by examining the average ranking of the target videos across question-answer rounds. This analysis is helpful for understanding how the process enhances the ranking of the target videos. As illustrated Figure 4, the average ranking of the target video consistently improves each consecutive round across all datasets. For instance, on the MSR-VTT dataset, the average ranking significantly improves from 18.57 in round 0 to 2.5 by final round. Similar improvements are observed on other datasets, with the average ranking on MSVD improving from 13.84 to 2.4, and on ActivityNet from 6 to 2.6. This demonstrates the consistent improvement, thereby confirming the effectiveness of MERLIN in reranking through iterative feedback learning."}, {"title": "The Effect of \u03b1 in Iterative Embedding Refinement", "content": "As mentioned in Section 3.5, our assumption is mitigating query drift would preserve the thematic integrity of the initial query by assigning high \u03b1 value, prioritizing the query and earlier answer embeddings over the most recent answers.\nTo validate our assumption in contrast to the experiment's higher \u03b1 = 0.8, we conduct additional experiments with assigning a reduced value \u03b1 = 0.2, which allows us to observe the impact of shifting emphasis towards the latest answers. The results on the MSR-VTT and MSVD datasets show that setting a lower \u03b1 initially improves retrieval performance in early rounds but leads to a decline after a few rounds, indicating potential query drift. Furthermore, the average ranking of the target video deteriorates in later rounds, suggesting the query representation has deviated from the user's original intent.\nSpecifically, for MSR-VTT, MERLIN got 44.4/67.60/76.20 for recall@1/5/10 at round 0 respectively but ended up with 61.6/81.20/87.00 respectively at round 5. For MSVD, MERLIN got 52.39/77.16/84.78 for recall@1/5/10 at round 0 respectively but ended up with 56.87/78.51/84.63 respectively at round 5."}, {"title": "Conclusion", "content": "In conclusion, the MERLIN framework addresses a critical gap in the field of text-video retrieval by integrating the often-overlooked user perspective into the retrieval process. This integration is achieved through a novel, training-free pipeline that utilizes LLMs for iterative feedback learning, allowing for the dynamic refinement of query embeddings based on user interactions. MERLIN not only aligns more closely with user intent but also enhances the overall search experience by reducing discrepancies between user queries and retrieved video content.\nThe implementation of MERLIN shows a significant advancement in multimedia retrieval, introducing the first retrieval-rerank pipeline in this domain. By incorporating iterative feedback mechanisms inspired by human cognitive processes, MERLIN facilitates a more aligned and context-aware approach to text-video retrieval. Our experimental results demonstrate the effectiveness of this approach, with substantial improvements in retrieval performance observed across MSR-VTT, MSVD, and ActivityNet datasets."}, {"title": "Limitations", "content": "While our results are promising, we acknowledge that we cannot provide a comprehensive guide for adapting MERLIN to different settings, as we have not extensively explored the impact of changing various components. However, the core principle of integrating user feedback to iteratively refine the query embedding appears to be a robust approach, regardless of pipeline components, the specific domain or data modality. Future work could investigate the generalization of MERLIN to other multimedia retrieval tasks and explore the optimal configurations for different scenarios.\nOther limitation of our approach lies in the use of a human-simulating LLM agent for answering questions based on static video frames. While this agent aims to mimic the human feedback process, it lacks the capability to grasp temporal information and attributes that require a high-level understanding of motion and dynamics. Since the LLM agent first generates answers based on static images and then aggregates them, it struggles to capture knowledge about direction, speed, and other temporal aspects present in the videos.\nMoreover, as most pre-trained video encoders also have shortcomings in effectively modeling temporal capabilities (Liu et al., 2024), our video encoder may be affected by this limitation as well. This creates a kind of chicken-and-egg problem, where video encoders can benefit from temporal-rich information only when they can understand temporal information effectively. Conversely, even if the video question-answering module (or similar counterparts) can handle temporal-rich information, if the video encoder does not possess the same capability, it may not benefit from this information. This temporal modeling challenge is a prevalent issue that the community needs to address collectively."}, {"title": "Algorithm", "content": "Require: encoder fenc(), user query q \u2208 Q, video v \u2208 \u03bd, total question-answer round R, retrieved top-k videos at round r \u00fb, i-th candidate among top-k videos at round r \u1fe6\n1: Encode eq = fenc(q) given user query q\n2: Encode er = fenc(v) given video v\n3: Retrieve v\u00ba = TOP-Kv\u2208V (SIM(eq, ev)) (Equation 1)\n4: Initialize message list m = []\n5: for r = 1 to R do\n6: Append metadata of v\u00af\u00b9 to m\n7: Generate question q = Mquestion (m) (Equation 2)\n8: Append \u011d to m\n9: Generate frame-level answers [\u00e2(r,\u00ba), . . ., \u00e2(r,N)] =\nManswer (q: video in mind) (Equation 3)\n10: Aggregate frame-level answers\nMaggr ([a(r,0),..., (r,N)]) (Equation 4)\n11: Encode ear = fenc(A)\n12: Refine embedding e = Refine(eq,..., ear)\n13: Retrieve v = TOP-\u039a\u03c5\u03b5\u03bd (SIM(e, ev)) (Equation 1)\n14: end for\n15: return Reranked retrieved videos v"}, {"title": "Prompt Template", "content": "In order to get useful information from user, it is critical to ask good question that could draw user intention. As depicted in Table 2, we set top 1 ranked video as anchor video and prompted GPT-4o to refer to anchor video's metadata. In our case, we used the video's caption as metadata. However, we believe that questions could be more diverse if we could use other data such as Automatic Speech Recognition (ASR) captions, the characteristics of the video, and so on. As MERLIN proceeds with the chat with the user (a user-simulating agent), we stacked previous questions and answers and To encourage GPT-4 to generate diverse questions without repeating previous ones..\nAs human-simulating agent has two steps for answering to the question regarding \u201cvideo in mind\", we have two different settings for each step. This method lacks in understanding direction, speed and other temporal knowledge as we dealt in the Limitation. However, we experimentally showed that our human-simulating agent is helpful in enriching information."}, {"title": "Prompt for Question-Answering Module", "content": "As depicted in Table 3, we sampled frame from video for every 1 second. Then we asynchronously inputed sampled frame and question from MERLIN. We prompted GPT-40 to answer in detail about fact and not just answer in \u201cYes\u201d or \u201cNo\u201d. However this question-answering module is the part that takes up large portion of API cost so video may be sample in wider stride to lower the API cost."}, {"title": "Prompt for Aggregation Module", "content": "As depicted in Table 4, we aggregate all the answers generated from the question-answering module. We prompted GPT-40 to aggregate multiple answers made with multiple frame at question-answering module and appended an aggregating example."}, {"title": "Case Studies", "content": "The main objective of MERLIN is to improve the ranking of failure cases where the target video is not among the top-ranked candidates. At the same time, it is important to keep the success case to stay in the top-ranked candidates while MERLIN proceeds chatting with user. Retrieving the target video among the top-ranked candidates indicates that MERLIN consistently reflects user intention during the conversation. To qualitatively verify that MERLIN performs its tasks according to the aforementioned objectives, we reviewed several case studies. We focused on how MERLIN bring the rank of failure cases."}, {"title": "Case study for ActivityNet", "content": "As shown in the Figure 5, initial ranking of target video was 224 using paired query from dataset. However as MERLIN augmented query using user's response, rank boosted to 36 \u2192 14 \u2192 4 \u2192 1 as round proceeds. During the conversation, MERLIN was able to understand that the user was looking for a video about Christmas themes, featuring two people, and involving gift wrapping. It managed to rank the target video on top with augmented information."}, {"title": "Case study for MSVD", "content": "As shown in Figure 6, initial ranking of target video was 154 using paired query from dataset. However as MERLIN augmented query using user's response, rank boosted to 14 \u2192 1 \u2192 1 \u2192 1 as round proceeds. During the conversation, MERLIN was able to understand that the user was looking for a video about NBA All-Star game, broadcasted on TNT and score board telling 74:75. It managed to rank the target video on top with augmented information at early round and manage to keep top rank during multiple rounds."}, {"title": "Case study for MSR-VTT", "content": "As shown in Figure 7, initial ranking of target video was 361 using paired query from dataset. However as MERLIN augmented query using user's response, rank boosted to 197 \u2192 14 \u2192 1 \u2192 1 as round proceeds. During the conversation, MERLIN was able to understand that the detailed feature and gesture of human featured on \u201cvideo in mind\". It managed to rank the target video on top with augmented information at early round and manage to keep top rank during multiple rounds."}]}