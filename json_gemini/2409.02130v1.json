{"title": "From Predictive Importance to Causality: Which Machine Learning Model Reflects Reality?", "authors": ["Muhammad Arbab Arshad", "Pallavi Kandanur", "Saurabh Sonawani"], "abstract": "This study analyzes the Ames Housing Dataset using CatBoost and LightGBM models to explore feature importance and causal relationships in housing price prediction. We examine the correlation between SHAP values and EconML predictions, achieving high accuracy in price forecast-ing. Our analysis reveals a moderate Spearman rank correlation of 0.48 between SHAP-based feature importance and causally significant features, highlighting the complexity of aligning predictive modeling with causal understanding in housing market analysis. Through extensive causal analysis, including heterogeneity exploration and policy tree interpretation, we provide insights into how specific features like porches impact hous-ing prices across various scenarios. This work underscores the need for integrated approaches that combine predictive power with causal insights in real estate valuation, offering valuable guidance for stakeholders in the industry.", "sections": [{"title": "Introduction", "content": "The field of predictive modeling has seen remarkable advancements, where ac-curate predictions are the cornerstone of informed decision-making. However, alongside prediction lies a parallel pursuit of understanding the causal structure that underlies the data. This dual objective has become increasingly vital, espe-cially in domains where uncovering the true drivers of outcomes is of paramount importance.\n\nThe Ames Housing Dataset, a meticulously curated compilation of nearly every conceivable facet of residential homes in Ames, Iowa, serves as a com-pelling platform for this endeavor. Covering transactions from 2006 to 2010, this dataset encapsulates a wealth of information, from zoning classifications to the very contour of the land. Its 79 diverse features promise insights that transcend mere correlations, offering the potential to discern the features that truly influence house prices."}, {"title": "1.1 Dataset Overview", "content": "The Ames Housing Dataset stands as a comprehensive repository of housing information, meticulously compiled to encompass various facets of residential properties in Ames, Iowa. Covering transactions spanning the years 2006 to 2010, this dataset encapsulates a rich array of features, offering a nuanced per-spective on the determinants of housing prices.\n\nWith approximately 2900 observations, the dataset strikes a balance be-tween comprehensiveness and manageability, rendering it an ideal subject for in-depth analysis and modeling. Encompassing a total of 79 features, including both numerical and categorical variables, it paints a detailed picture of each property's characteristics. These features range from structural attributes like square footage and number of bedrooms, to contextual elements such as zoning classifications and proximity to various amenities.\n\nAt the heart of this dataset lies the sale price of each property, serving as the primary target variable for predictive modeling. This critical piece of infor-mation anchors our analysis, allowing us to unravel the factors that significantly influence real estate valuations.\n\nWhile known for its relative cleanliness, it's essential to acknowledge that, like any real-world dataset, the presence of outliers may necessitate careful con-sideration during the analysis. Thus, a meticulous approach to data preprocess-ing and exploratory data analysis will be pivotal in ensuring the integrity and reliability of our findings.\n\nIn the subsequent sections, we delve into our methodology, outlining the techniques and models that will be employed to extract valuable insights from this robust dataset."}, {"title": "1.2 Motivation and Scope", "content": "In the ever-evolving landscape of predictive modeling, understanding the un-derlying causal structure of a dataset is equally as vital as producing accurate"}, {"title": "1.3 Objective", "content": null}, {"title": "1.4 Primary Objective", "content": "The primary objective of this project is to conduct a comprehensive analysis of the Ames Housing Dataset, focusing on three key areas:\n\n1.  Determining Feature Importance: Using CatBoost and LightGBM to iden-tify features significantly influencing housing prices.\n\n2.  Uncovering Causal Relationships: Applying causal inference to discern genuine causal factors, providing deeper insights into Ames' housing mar-ket dynamics.\n\n3.  Exploring Correlation between SHAP Values and Econ ML Predictions: Investigating the relationship between SHAP values and predictions de-rived from economic machine learning models."}, {"title": "1.5 Secondary Objectives", "content": "In addition to the primary objectives outlined above, this project aims to achieve the following secondary objectives:\n\n1.  Evaluate Model Performance: Assessing the effectiveness of the chosen models (CatBoost, LightGBM) in accurately predicting housing prices within the Ames dataset.\n\n2.  Compare Feature Importance Rankings: Comparing the feature impor-tance rankings obtained from different models to identify consistent influ-ential factors.\n\nThese objectives collectively form the foundation of this project, guiding the analysis and providing a framework for evaluating the results."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Predictive Modeling in Real Estate", "content": "Predictive modeling in real estate has gained substantial traction in recent years. Researchers and practitioners alike recognize its potential for informing criti-cal decisions related to property transactions, investment strategies, and urban planning [2]. By harnessing the power of machine learning, analysts can distill valuable insights from vast datasets, offering a data-driven approach to under-standing complex real estate dynamics."}, {"title": "2.2 Interpretable Machine Learning", "content": "The adoption of interpretable machine learning techniques has emerged as a crucial aspect of model deployment in real-world applications. Methods like SHapley Additive exPlanations (SHAP) values provide a clear and intuitive way to understand the impact of individual features on model predictions [5]. By unraveling the 'black box' of complex models, interpretability tools enhance transparency and build trust in the predictive process."}, {"title": "2.3 Causal Inference in Real Estate", "content": "Understanding causal relationships in real estate is paramount for making in-formed policy decisions and investment strategies. Recent advances in causal inference, as exemplified by libraries like EconML, empower analysts to disen-tangle correlation from causation, shedding light on the true drivers of property values [1]. This approach goes beyond prediction, providing a deeper under-standing of the underlying mechanisms shaping the real estate market."}, {"title": "2.4 Advanced Techniques: CatBoost and LightGBM", "content": "In the realm of predictive modeling, CatBoost and LightGBM stand out as state-of-the-art techniques. CatBoost's innate capability to handle categorical variables and LightGBM's leaf-wise tree growth strategy render them powerful tools for accurate predictions [4]. Leveraging the strengths of both these mod-els holds great promise for unraveling the intricate web of factors influencing housing prices.\n\nThe convergence of advanced predictive modeling techniques [3], interpretable machine learning, causal inference, and ensemble learning presents a compelling opportunity to dissect the complex interplay of factors driving housing prices in Ames, Iowa. By integrating these methodologies, this project aims not only to produce accurate predictions but also to uncover the true causal narratives embedded within the dataset. Recent advancements in large language models may also offer new perspectives on data analysis and interpretation in various domains [7].\n\nIn the next sections, we detail our data exploration, preprocessing tech-niques, and the specific methodologies employed, including the application of CatBoost and LightGBM, as well as the implementation of EconML for causal inference."}, {"title": "3 Methodology:", "content": null}, {"title": "3.1 Data Preparation", "content": "To achieve our goal,f the Ames Housing Dataset, a comprehensive collection of data from Kaggle.. This dataset has 2,930 distinct observations, each represent-ing a unique residential property in Ames, Iowa. This dataset has 79 different features that describe a residential property. The target variable here is the sale price. The details are given in 3 and 4\n\nThe dataset comprises a variety of categorical and numerical values. We will detail how we processed and handled these values in the subsequent section."}, {"title": "4 Data Preprocessing", "content": "The dataset underwent a rigorous preprocessing regime informed by a previous study [6]. The overarching motivation for this preprocessing was to delve into the correlation between SHAP values, a prominent interpretability mechanism,"}, {"title": "4.1 Model Definitions", "content": null}, {"title": "4.1.1 LightGBM", "content": "Light Gradient Boosting Machine is a powerful gradient boosting framework, based on decision trees.\n\nGradient Boosting is a technique that combines multiple weak models to gener-ate a powerful model. This final model gives accurate predictions by iteratively correcting the errors generated by the weaker models.After each model (tree) is"}, {"title": "4.1.2 Cat Boost", "content": "CatBoost is a high-performance, open-source gradient boosting library devel-oped by Yandex, a Russian multinational corporation specializing in Internet-related products and services. CatBoost stands for \"Categorical Boosting\". Similar to LightGBM, CatBoost also uses gradient boosting technique. How-ever it employs the level wise growth architecture 5 which means that all nodes at the current depth (level) are expanded before moving to the next level. Every leaf node is at the same depth which helps prevent overfitting.\n\nIt has the capability of handling the categorical data directly without specifying the datatype. It uses the technique of Ordered boosting when encoding a cate-gorical variable for a specific row by only using information from rows before it so as to avoid target leakage.\n\nTo get optimal model performance, we used the same approach as in Light-GBM - GridSearchCV. The hyperparameters which we attempted to tune were learning_rate, depth, 12_leaf_reg, border_count.Here the 12_leaf_reg is the Coefficient at the L2 regularization term of the cost function and border_count defines number of bins in the numerical features. The parameters input to the grid search were : learning_rate was [0.1, 0.05, 0.01], depth for [3, 5, 10], 12_leaf_reg values [1, 5, 10] and border_count to be [32, 128, 255].\n\nThe model performed well when the hyperparameters had the values :"}, {"title": "4.2 Model Interpretability", "content": null}, {"title": "4.2.1 SHAP Values", "content": "SHAP(SHapley Additive exPlanations) values are a way to express how much each feature is affecting the model prediction or how much each feature con-tributes to the model prediction. These values are used to interpret machine learning models and improve explainability.\n\nThese values are computed based on the concept of SHAPLEY values in game theory. First subsets are created excluding the feature for which we want to cal-culate the SHAP values. Then for each subset, model prediction is calculated\n\nNow, model predictions is calculated again for that subset but with the fea-ture in question. Now, the difference in the model's prediction before and after"}, {"title": "4.3 Causal Inference and Effect Estimation", "content": null}, {"title": "4.3.1 EconML", "content": "EconML is a Python library developed by Microsoft Research that leverages recent advances in machine learning to estimate causal effects. For our analysis, it is important to understand the causal relationships between the target variable and the individual features. This will help in understanding how the housing prices change if a certain feature is changed, For instance, does adding a porch have a different impact in a residential versus a commercial zone? In our project, we've applied the CausalAnalysis module from EconML. This approach allows us to:\n\nIsolate True Causal Drivers: By distinguishing between mere correlation and causation, we can pinpoint which housing features genuinely drive price changes.\nEnhance Predictive Models: By ensuring our model is not basing predictions on correlations we can be more confident in its predictions and recommendations."}, {"title": "4.3.2 Average Treatment Effect", "content": "We are using the 'global causal effect' method from the 'Causal Analysis' module in the EconML. This method provides an ATE for each feature in the dataset, offering a broad view of the potential causal relationships within the data. The ATE essentially provides an understanding of the expected change in the out-come for a one-unit change in the feature, assuming all other factors remain constant. For instance, if we have a feature \"HasFireplace\", the ATE would give us: The expected difference in house prices between homes with a fireplace and those without, considering all other features remain unchanged. This will be done for all the features one by one. Since there are multiple features and we want to understand the effect one by one, we are using 'global causal effect'.\n\nIn essence, while traditional analyses might tell you what is happening, the global causal effect helps explain why it's happening, which is invaluable in many practical scenarios."}, {"title": "4.4 Evaluation Techniques", "content": null}, {"title": "4.4.1 Spearman's Rank Correlation", "content": "We aim to compute the Spearman's rank correlation coefficient between a list of causally significant features, denoted as $C$, and a list of features determined via SHAP analysis for a specific model, denoted as $F_m$.\n\nDefinitions and Notations:\n\n*   $C$: The list of causally significant features obtained from a specific causal analysis. This list is sorted in a particular order and represents a subset of all considered features.\n\n*   $F_m$: The list of features sorted based on their importance for model $m$ as determined by a SHAP analysis.\n\n*   $C_m$: Represents the intersection of features between $C$ and $F_m$.\n\nThe common features between the lists $C$ and $F_m$ are:\n\n$C_m = \\{f | f \\in F_m \\text{ and } f \\in C\\}$\n\nFor each feature $f$ in $C_m$, its rank is determined based on its position in $C$ and $F_m$:\n\n$r_C(f) = \\text{position of } f \\text{ in } C$\n\n$r_{F_m}(f) = \\text{position of } f \\text{ in } F_m$\n\nFrom this, we derive two ranked lists:\n\n*   $R_C$: Ranks of common features based on their order in $C$.\n\n*   $R_{F_m}$: Ranks of common features based on their order in $F_m$.\n\nThe Spearman's rank correlation coefficient, $\\rho$, for the ranked lists $R_C$ and $R_{F_m}$ is computed using:\n\n$\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)}$\n\nWhere:\n\n*   $d_i$ represents the difference between the ranks of the $i^{th}$ feature in the two lists.\n\n*   $n$ denotes the number of features in $C_m$.\n\nInterpretation: The Spearman's rank correlation coefficient, $\\rho$, yields a value between -1 and 1. A value of $\\rho = 1$ indicates a perfect positive correlation, meaning that the order of the rankings in $R_C$ and $R_{F_m}$ are identical. Conversely, a value of $\\rho = -1$ indicates a perfect negative correlation, signifying that the order of rankings in $R_C$ is the exact reverse of $R_{F_m}$. A value of $\\rho = 0$ suggests no correlation between the rankings of the two lists."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Model Training", "content": "After the steps of preprocessing and feature engineering, the data was segregated into a training and testing set, using an 80-20 split. A comprehensive grid search was executed for each model over a multitude of hyperparameters, as elucidated in the previous section (refer Table 1). During training, a 5-fold cross-validation was performed to ensure the robustness of the models.\n\nFor LightGBM, the model achieved a score of 86.48 on the test set. In contrast, Cat Boost showcased a score of 89.91 on the test set."}, {"title": "5.2 Feature Importance Analysis", "content": "To comprehend the model's decisions and dissect the contributions of each fea-ture to the predictions, we employed the SHAP (SHapley Additive exPlanations) framework. Particularly, we used the TreeExplainer from the SHAP library, optimized for tree-based models, leveraging the Tree SHAP algorithms. As seen in Figure 2, and Figure 3\n\nFor our model, the TreeExplainer was initialized with several parameters:\n\n*   Model: The trained model for which we want to determine the SHAP values.\n\n*   Feature Perturbation: Set to 'tree_path_dependent', this parameter de-termines how to address correlated or dependent features when estimating SHAP values. The tree path dependent method follows the structure of the trees, utilizing the count of training samples traversing each tree path as our background distribution. This approach abstains from requiring an explicit background dataset, simplifying the process.\n\n*   Feature Names: The names of the features in the training dataset were also provided for better interpretability.\n\nPost initialization, SHAP values for the test set were computed. It is im-perative to note that the SHAP framework, through TreeExplainer, allows for"}, {"title": "5.3 Causal Inference Examination", "content": "Table 5 presents features deemed unsuitable for causal analysis, along with their respective justifications. For our analysis, we utilized the CausalAnalysis tool from the econml.solutions package to explore causal relationships within the dataset's features. This tool facilitates identifying the causal effects of particular features on the outcome, while also considering heterogeneity across different attributes. Our training incorporated an extensive feature set, which included notable attributes like the presence of fireplaces, porches, and decks. Benefiting from parallel processing, our analytical approach was not only rigorous but also computationally efficient. After setting up the necessary parameters, we trained the model on our dataset. Results for few of the most casually significant features are given in 2"}, {"title": "5.4 Relation between SHAP and Economic ML Predic-tions", "content": "For LightGBM, the spearman metric value between the high ranked shap fea-tures and the casually significant features was calculated to be 0.35. On the other hand, CatBoost showcased a slightly higher value of 0.48."}, {"title": "6 Extensive Causal Analysis", "content": null}, {"title": "6.1 Exploring Heterogeneity: The Impact of Porches on Housing Prices", "content": "This section zeroes in on the 'HasPorch' feature in the Ames Housing Dataset, dissecting how porches affect property prices differently across various scenar-ios. Our goal is to understand these effects in detail, considering factors like property age, zoning, and additional features.\n\nRather than just identifying trends, we aim to unravel specific patterns in the causal relationship between porches and housing prices. This thorough analysis provides crucial insights for those in real estate, offering a nuanced understand-ing of how porches contribute to the pricing dynamics of residential properties. To accomplish this, we use a tree-based method, visualizing a tree specifically for the 'HasPorch' feature. This allows us to pinpoint groups where porch im-pact on housing prices may vary. By interpreting the tree's nodes and branches, we uncover situations where having a porch significantly influences property val-ues. The root node, with an age at sale less than or equal to 46.5 years, shows a moderate Conditional Average Treatment Effect (CATE) mean of 2467.727. This means that, on average, having a porch in properties of this age bracket contributes significantly to a house's value. However, delving deeper into the tree, we notice intriguing nuances. The left child node, representing properties with an age at sale less than or equal to 24.5 years, has a slightly lower CATE mean of 1552.238. This implies that, within this younger subset, the impact of having a porch is slightly diminished compared to the broader category.\n\nOn the other hand, the right child node, encompassing properties with an age at sale less than or equal to 74.5 years, exhibits a substantially higher CATE mean of 4216.309. This suggests that, for older properties in this range, the presence of a porch has a more pronounced positive effect on housing prices."}, {"title": "6.2 Policy Tree Interpretation: Guiding Strategic Deci-sions", "content": "As we shift our focus to the policy tree for the 'HasPorch' feature, our aim is to provide actionable insights for stakeholders in the real estate industry. Unlike heterogeneity analysis, which identifies patterns in the causal relationship"}, {"title": "6.3 What-If Analysis: Assessing the Impact of Adding a Porch", "content": "To delve deeper into the economic implications of adding a porch to residential properties, we conducted a What-If analysis using the CausalAnalysis mod-ule. This analysis simulates the hypothetical scenario of introducing porches to houses that initially lack this feature, allowing us to estimate the potential impact on housing prices. The goal is to provide stakeholders in the real es-tate industry with actionable insights into the financial consequences of such modifications.\n\nFor this analysis, we selected houses from the test set that currently do not have a porch (HasPorch = 0). The whatif function was then applied to these houses, considering the addition of a porch by setting the HasPorch variable to 1 while keeping other features constant.\n\nThe results of the What-If analysis indicate that the current average housing price on the test set is 146,936.89.dollars Upon simulating the addition of a"}, {"title": "6.4 Conclusion and Future Work", "content": "This study analyzed the Ames Housing Dataset using CatBoost and LightGBM models, focusing on feature importance and causal relationships in housing price prediction. Key findings include:\n\n*   High accuracy in price forecasting using both models, with Cat Boost achieving a score of 89.91 and LightGBM 86.48 on the test set\n\n*   A moderate Spearman rank correlation (0.48 for CatBoost and 0.35 for LightGBM) between SHAP-based feature importance and causally signif-icant features\n\n*   Insights into the causal impact of specific features, such as porches, on housing prices, revealing nuanced effects based on property age and zoning\n\n*   What-If analysis demonstrating that adding a porch could potentially in-crease average housing prices from 146,936.89 to 149,649.98 dollars\n\nThese results highlight the complexity of aligning predictive modeling with causal understanding in real estate valuation. The study underscores the need for integrated approaches that combine predictive power with causal insights, providing actionable information for stakeholders in the real estate industry. Future research directions include:\n\n1.  Expanding the dataset to enhance model generalizability and capture broader market trends\n\n2.  Deepening causal analysis on additional features to provide a more com-prehensive understanding of housing price determinants\n\n3.  Applying findings to real-world scenarios and policy development, partic-ularly in urban planning and real estate investment strategies\n\n4.  Collaborating with experts across disciplines, including economics and ur-ban studies, for a more holistic understanding of housing market dynamics\n\n5.  Exploring advanced machine learning techniques to further improve the alignment between predictive importance and causal significance\n\nThis work lays the foundation for a more comprehensive approach to housing market analysis, with implications for both theoretical research and practical applications in real estate. By bridging the gap between predictive modeling and causal inference, we pave the way for more informed decision-making in property valuation, investment, and policy formulation."}]}