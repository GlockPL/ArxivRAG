{"title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical Impacts, Constraints, and Emerging Questions.", "authors": ["Zeyneb N. Kaya", "Souvick Ghosh"], "abstract": "There have been rapid advancements in the capabilities of large language models (LLMs) in recent years, greatly revolutionizing the field of natural language processing (NLP) and artificial intelligence (AI) to understand and interact with human language. Therefore, in this work, we conduct a systematic investigation of the literature to identify the prominent themes and directions of LLM developments, impacts, and limitations. Our findings illustrate the aims, methodologies, limitations, and future directions of LLM research. It includes responsible development considerations, algorithmic improvements, ethical challenges, and societal implications of LLM development. Overall, this paper provides a rigorous and comprehensive overview of current research in LLM and identifies potential directions for future development. The article highlights the application areas that could have a positive impact on society along with the ethical considerations.", "sections": [{"title": "1 Introduction", "content": "The capacity of artificial intelligence (AI) models to understand human language holds paramount importance for their interaction with humans and other systems. The emergence of large language models (LLMs) has had a profound impact on the developments of Natural Language Processing (NLP), with models of growing scale. In particular, recent advancements in neural architectures, such as transformer models, have enabled the rapidly expanding capabilities of LLMs, incorporating vast datasets, significant computational power, and an increasing number of parameters. With scale, the transition from Language Models (LMs) to LLMs has brought about much greater language capabilities and abilities across tasks. While LMs have largely been limited to basic text processing and specialized applications, LLMs are more generalizable. LLMs have made notable contributions to language understanding and text generation, handling complex tasks including classification, translation, question-answering, summarization, and information retrieval. Their abilities across a range of diverse tasks and domains, adapting to even low-resource settings [1], demonstrate their transformative generalization capabilities. For instance, models like GPT-3 have showcased remarkable versatility in generating creative content and simulating conversation. The rise of open-source initiatives of LLM architectures and pretrained models has further propelled development. The significant technological impacts of LLMs, enhancing the ability of artificial intelligence systems to communicate with humans, have also had a growing influence on society. Concurrently, these developments raise pressing ethical questions, particularly around data privacy, bias, and the potential for misuse [2]. There have been increasing efforts on the release of LLMs, introducing implications of the potential benefits and challenges they pose to our interactions with language and technology.\nLLMs have now become the frontier of research and development in NLP and artificial intelligence as a whole. As LLMs grow rapidly with new breakthroughs, applications, and scales, we aim to provide a comprehensive and systematic overview of the directions of LLM research and facilitate further advancements. We summarize the course of existing efforts with LLMs, delving into each component of the model development process and detailing the goals and current capabilities in the field. We further identify the key considerations and gaps to address in LLM development to guide future research. Through these explorations, we aim to answer three guiding research questions (RQs):\n\u2022 RQ1: What are the prominent aims and objectives addressed in LLM research?\n\u2022 RQ2: What are the popular methodologies for advancing the capabilities of LLMS?\n\u2022 RQ3: What are the limitations and ethical considerations of LLM development as identified in contemporary influential research?\nWe investigate these questions through a systematic review characterized by methodological rigor, objectivity, and comprehensive and quantitative statistical analyses of studies, enabling synthesized, evidence-based contributions. The rest of the paper is organized as follows: Section 2 provides an overview of the systematic review process, while Section 3 discusses the methodology employed for conducting this review. Section 4 presents several statistics of the reviewed papers. Sections 5, 6,"}, {"title": "2 Systematic Reviews", "content": "Systematic reviews aim to provide a structured and comprehensive evaluation of existing literature utilizing explicit, replicable methods. Systematic reviews follow a rigorous, meta-analytical approach, employing a standardized procedure for identifying, appraising, and synthesizing relevant studies, culminating in an evidence and statistically-based [3]. This approach ensures that the summary of available studies is not only comprehensive but also underpinned by robust methodological rigor.\nOwing to their statistical precision, systematic reviews are particularly useful for researchers who rely on evidence-based decision-making. Initially popularized in the health sciences to enhance the reliability and validity of medical research [4, 5], the practice of systematic reviews has since broadened to include information science [6, 7] and related fields [8, 9], effectively informing future developments. The systematic review presents a methodology for establishing valuable findings and distinct contributions in IR beyond a simple overview of past works."}, {"title": "3 Systematic Review Methodology", "content": "The steps to conducting a systematic review follow three main phases and their sub-components, outlined in Figure 1 and described in detail below.\n3.1 Selection of Sources (Inclusion/Exclusion Criteria)\nThe first step in our systematic review process was the identification of works for study. We focused on literature that directly contributes to the development or capabilities of LLMs. Articles were sourced from Google Scholar and queried using a series of search strings, including: \"large language models,\u201d \u201cattention language model,\u201d \u201cgenerative language models, \u201c\u201cnatural language understanding,\u201d \u201clanguage model scaling,\u201d \u201cprompted language model,\u201d and \u201cprompt engineering.\u201d This review considered English-language publications from 2016 to 2023.\nTo identify authoritative sources, we compiled a list of venues known for significant LLM studies, limiting to papers published in select top NLP and AI journals and conferences, as well as reports from leading AI industry developers. The inclusion of industry publications was a deliberate choice aimed at capturing various impactful works in the field, particularly those contributing to novel methodologies and social impact, which might not have been published in academic journals or conferences yet. In selecting the most influential papers, we applied a citation threshold of 150 citations. The culmination of this process was the selection of 61 articles for our systematic review."}, {"title": "3.2 Thematic Analysis Procedure / Coding Scheme", "content": "The collected papers were coded for statistical analysis on various categories. Basic metadata features such as authors, publication year, publication venue, and citations were included. We also extracted author affiliations at the time of publication as indicated by the authors' email domains.\nThematic analysis was performed for the papers across various categories. After reviewing the paper, the authors first produced a list of themes for each piece derived from the qualitative thematic categories. The list was consolidated to generate a coding hierarchy. Each author conducted these steps independently to minimize bias in the process. Then, they compared and discussed the annotations to reach a consensus and resolve any disagreement."}, {"title": "4 Characteristics of Publications", "content": "In the following section, we provide an overview of the selected articles reviewed, discussing relevant statistics and features that characterize the content of the review."}, {"title": "5 Aims & Objectives (RQ1)", "content": "Literature in the development of LLMs pursues different aims. Of the works we studied, a significant 66% focused on best practices and practical considerations in ethics and research methodology. Meanwhile, 44% introduced originality in the advancement of LLM performance, focusing on aspects such as efficiency, robustness, and scalability. A smaller proportion, 18%, were investigative studies with the objective of furthering the understanding of LLMs. We explore the main avenues of LLM improvement and progress that are prevalent in the field."}, {"title": "5.1 Responsible Development Considerations", "content": "As LLMs gain prevalence in social and research contexts, attention has been increasingly directed to the best practices of the field in developing and releasing LLMs [2, 12, 14, 18, 28-30].\nVarious studies have aimed to address the ethical and societal impact of LLMs. For instance, Bender et al. [2] consider the pervasive risks associated with the growing size of LLMs with a critical review of the limitations and potential dangers of LLMs. Solaiman et al. [18] examine the release of LLMs, discussing recommendations for responsible publication in AI that consider the social impacts of OpenAI's release of the GPT-2 models. Their framework for best practices for responsible model release includes prioritizing and collaborating with the communities influenced by the models rather than focusing on general tradeoffs. Kirchenbauer et al. [28] present a method for 'watermarking' LLMs and discuss its applications and security in deployment. Carlini et al. [14] work to demonstrate further the potential for misuse presented by the growth and release of LLMs. They extract memorized individual training points from only black-box query access, showing that LLMs pose a threat to leaking personally identifiable information. Gehman et al. [12] find biased and harmful propagation learned by LLMs. Manakul et al. [29] and Turpin et al. [30] both note the faultiness of LLMs' information, including their capacity to misinform, hallucinate, and be manipulated.\nWith these risks in mind, researchers have aimed to implement safe methods in their work and be conscious of the impact of their models. In developing a suite of Open Pre-trained Transformer (OPT) language models, Zhang et al. [13] aim to bring best practices and accessibility in their efficient replication of GPT-3, released with full detailed documentation and an analysis of considerations for the safe deployment of their model."}, {"title": "5.2 Improving LLM Performance", "content": "Technical advancements are the primary focus of LLM research, and several key works [1, 11, 31, 34, 36-38, 41, 46, 55, 60], have concentrated on enhancing the performance of large language models across a diverse range of applications.\nA primary objective in LLM research is the generalizability of these models to diverse tasks and domains. For example, Radford et al. [41] demonstrate the capacity of LLMs to perform well across various domains and implicitly on downstream tasks without any parameter or architecture modification. Building on the effectiveness of this implicit learning, Sanh et al. [31] create a model that can better generalize to held-out tasks and perform robustly with diverse prompt wording, using explicit supervised multitask training. Brown et al. [1] examine the generalizability of LLMs on new tasks with limited task-specific data in the few-shot setting. Gruver et al. [60] examine LLMs zero-shot abilities in the task of time series forecasting. Howard and Ruder [37] present Universal Language Model Fine-tuning (ULMFIT) a transfer learning method to pretrain a language model on a large general-domain corpus, applicable to different"}, {"title": "5.3 Investigative Studies", "content": "The objective of some papers is to delve into the less understood mechanisms that drive the success of LLMs [30, 61, 63, 64].\nGiven the complexity yet significant effectiveness of attention mechanisms, Clark et al. [64] decipher the patterns in LLM attention and their association with specific linguistic features. In examining the performance gains achieved through prompting, Le Scao and Rush [61] analyze the effectiveness of prompting and the trends in its success. Wei et al. [63] experiment with chain-of-thought prompting to gauge LLMs' reasoning. Inspired by the human tendency to break down complex tasks into multi-step problems, they augmented the input examples with a chain of thought. Meanwhile, Turpin et al. [30] studied the weaknesses of chain-of-thought prompting and where it could result in unfaithful responses and be prone to manipulation.\nSuch investigative studies offer valuable insights into the capacities of LLMs and inform further development. Roberts et al. [53] leverage LLMs' implicit knowledge storage in question-answering tasks. LLMs are able to achieve competitive results in question-answering without access to external resources, offering insights into the uses and underlying learning processes of LLMs. Similarly, Petroni et al. [32] explore the potential of LLMs as knowledge bases, building on this inherent characteristic. Nie et al. [62] introduce a benchmark, Adversarial NLI, to identify and evaluate tasks that pose the greatest challenge for LLMs, like numerical and quantitative reasoning and complex inference types. These works all aim to better empirically understand LLMs. Biderman et al. [65] present Pythia, which enables the analysis of LLM capabilities with scale and training. Schaeffer et al. [66] argue that the unique abilities of"}, {"title": "6 Methodologies & Capabilities (RQ2)", "content": "Various stages of model development contribute significantly to the efficacy of LLMs, presenting multiple facets to address in refining their capabilities. In our review, 13% of the works' methodologies involved the development of datasets specifically for LLMs. 36% focused on studying model inputs and outputs, including aspects such as prompting, formatting, and pre-/post- processing techniques. The majority, 59%, delved into model training, examining a spectrum of architectures. A significant proportion, 41%, centered their methods on the analysis of LLMs. Through a comprehensive analysis of the methods employed, we aim to provide valuable insights into the key strategies and their contributions, illuminating paths for continued improvement and innovation."}, {"title": "6.1 Dataset and Benchmark Development", "content": "Large, high-quality corpora are foundational to the capabilities of LLMs. The development of diverse and effective datasets has been a pivotal method in furthering LLMS, and our review identified several studies that explored new datasets [11, 27, 29, 41, 56, 62].\nThe General Language Understanding Evaluation (GLUE) benchmark [11] has emerged as one of the most significant datasets in NLP, widely used to evaluate models' NLU capacity. This work presents a dataset comprising diverse linguistic tasks and domains to advance models' sample-efficient knowledge transferability and a diagnostic evaluation dataset tagged with various linguistic phenomena to facilitate error analysis. With the rapid advancement of LLMs surpassing human-level performance,"}, {"title": "6.2 Model Input/Output", "content": "The formatting and processing of natural language data for use in models is a relevant aspect of LLM development that has seen new methodologies [15, 31, 57, 59, 61, 63, 67].\nThe interpretation of tasks as prompts has arisen as an effective paradigm for adapting pretrained models. Scao & Rush (2021) Le Scao and Rush [61] establish the data efficiency provided by prompting and propose a novel metric to quantify the advantage of a prompt over a generic model head across tasks and data sizes. Given the benefits of prompts, literature has shifted towards improving and optimizing prompt generation. Gao et al. (2021) Gao et al. [67] present LM-BFF, incorporating effective techniques to test the limits of the prompting methodology. Wang et al. [59] approach images as a foreign language and aligning vision tasks with language tasks through the input format of natural language instructions.\nDelving into the low-data applications of prompting, Sanh et al. [31] demonstrate the capacity of prompting for implicit multitask learning, enhancing zero-shot generalization. Wei et al. [63] leverage processes in human problem-solving to examine \"chains of thought\" in prompting to improve reasoning-based tasks in LLMs. Adapting prompting with fine-tuning, Lester et al. [15] introduce \u201csoft prompts,\u201d which are learned end-to-end through backpropagation from a pretrained model and incorporated into downstream tasks. Yao et al. [58] further presents a tree of thoughts' to improve LLMs' abilities in problem-solving tasks.\nMany studies have explored the modification of inputs with new formats or additional information to support model learning [12, 19, 22, 48]. Raffel et al. [19] introduced an influential work with a unified framework that converts text-based tasks into a text-to-text format. Schick et al. [57] introduce APIs into LLMs, allowing their model Toolformer to access and use external tools to support their responses. Schick and Sch\u00fctze [36], motivated by including natural language task descriptions in inputs,"}, {"title": "6.3 Model Training", "content": "New methods of model architecture development bring significant advancements to the capabilities of LLMs. In particular, developments in the pretraining processes and objectives are a prevalent area of study [26, 33, 34, 39, 45, 52].\nFor large-scale language modeling with RNNs, Jozefowicz et al. [40] introduce a Softmax loss based on character-level CNNs and combine word- and character-level models, integrating them into the word-level LSTM. Radford et al. [43] explore pretraining methods for CNNs in multimodal learning with vision-based tasks, introducing Contrastive Language-Image Pre-training (CLIP). This method involves pretraining by learning a joint multimodal embedding with an image and text encoder and then predicting image-text pairings.\nVaswani et al. [22] introduce the Transformer architecture, featuring the self-attention mechanism to learn the relationships between words within sentences, enabling greater scale and efficiency. The development of BERT [45] was one of the most important advances of LLMs. BERT is designed to pretrain generalizable bidirectional representations from unlabeled text. Its pretraining employs two unsupervised tasks: Masked Language Modeling (MLM), where masked tokens are predicted, and Next Sentence Prediction (NSP), where the relationship between two sentences is predicted. Aiming for sample efficiency and reduced computational cost, Clark et al. [33] propose a novel pretraining method through ELECTRA. In this method tokens are replaced with alternatives sampled from a small generator and the model learns to discriminate between generated and true tokens. Yang et al. [52] address the limitations of BERT's autoencoding-based pretraining in learning masked dependencies with their autoregressive pretraining method in XLNet. Radford et al. [39] use generative pretraining an innovative approach to pretraining for LLMs, followed by discriminative fine-tuning for specific tasks. ERNIE [23] uses knowledge graphs as input to provide structured information alongside natural language. Dong et al. [34] introduce a new unified pre-training language model (UNILM), jointly optimized for multiple objectives to enable fine-tuning for both NLU and NLG tasks. This approach enhances task generalizability. To broaden capabilities across languages, CONNEAU and Lample [26] introduce two learning methods for cross-lingual language modeling."}, {"title": "6.4 LLM Understanding", "content": "Various research works attempt to advance LLMs through analysis-based methods [12, 24, 35, 61].\nThe development of metrics to quantify aspects of LLMs enhances understanding of both the limitations and effectiveness of these models. Le Scao and Rush [61] introduce an approach to quantify the advantage of prompting, aiming to guide practices in developing LLMs with scale by examining the impact of prompting across various data sizes. Gehman et al. [12] REALTOXICITYPROMPTS dataset provides valuable insights for gauging the risks of LLMs and understanding the factors to be conscious of. Metrics such as Majority Label Bias, Recency Bias, and Common Token Bias are crucial [35] to characterize model stability, aiding in the development of their calibration approach to address instability. Jiang et al. [24] propose a set of methods to better measure the accuracy of model knowledge that accounts for the role of prompts in the quality of generations. Their work enables a more accurate estimation of the knowledge in language models with the automatic generation of better prompts."}, {"title": "7 Limitations & Considerations (RQ3)", "content": "Understanding the limitations of studies and LLMs provides essential context and guidance for future work. Of the articles we examined, 43% explicitly recognized the limitations and ethical considerations of their study. The proportion of articles including these limitations has increased with recency, as the importance of discussing limitations has become increasingly emphasized, with some venues even requiring specific sections for them. Across these papers, 62% noted limitations regarding performance, 58% identified limitations in the study procedure, and 58% examined the ethical impacts of their work. In our analysis, we examine the acknowledged limitations in the development of LLMs to promote best practices in LLM research."}, {"title": "7.1 Performance Limitations", "content": "A significant and widely recognized limitation in LLM research is their weaknesses in performance for robust applicability. The articles have identified the limited capacity of LLMs in tackling certain complex tasks [1, 11, 18, 41, 57, 67]. Wang et al. [11], in examining the performance of NLU systems through their construction of the GLUE benchmark and its diagnostic dataset, note the challenges LLMs face on specific tasks and various linguistic phenomena: models often perform lower on the RTE and WNLI inference tasks, and Logic-based tasks, and struggle in cases involving restrictivity and double negation.\nRadford et al. [41] recognize that in tasks such as summarization, LLM performance is lower, a finding echoed by Brown et al. [1], who identify tasks like text synthesis as a weakness. The performance disparities in harder tasks are noted by Gao et al. [67], who observe that their method favors tasks with shorter inputs, fewer output classes, and straightforward structures amenable to a fill-in-the-blank\" format. Solaiman et al. [18] further note long input text as a challenge in the accuracy of LLMs. The growing capabilities of LLMs have relied heavily on the availability of large amounts of data, leaving low-data and zero- to few-shot settings as a persisting challenge of LLMs [1, 31, 43, 44, 67]. Brown et al. [1], Schick et al. [57], and Alayrac et al. [44] both note poor sample efficiency in training as a broader limitation of LLMs. In their analyses, Radford et al. [41] characterize the zero-shot performance of GPT-2 as practically \u201cfar from usable,\" mirroring the substantial gap in zero-shot performance from traditionally fine-tuned models identified by Sanh et al. [31]. Beyond absolute performance, Gao et al. [67] also identify substantial instability in few-shot learning. LLMs experience limited broad generalizability. Biderman et al. [65] analyze the capabilities of LLMs with scale using their suite of models Pythia, showing that with growing scale, LLMs improve in performance, albeit with a decreasing rate. The low zero-shot performance"}, {"title": "7.2 Study Limitations", "content": "Beyond the technical limitations of LLMs, the articles examine the limitations of their study approaches and analyses. To maintain best practices and provide context for their findings, multiple articles acknowledge the limitations of their study scope and its imperfect assumptions [10, 12, 16, 25, 43, 53, 61, 66].\nThe toxicity dataset composed by Gehman et al. [12] relies on a single available metric of toxicity detection in text, which they acknowledge as an imperfect measure that could bias toxicity detection towards lexical cues and miss subtle biases. Hoffmann et al. [16], Turpin et al. [30], and Manakul et al. [29] similarly note their limited scope and scale of study as a constraint to the broader generalizability of their findings. Roberts et al. [53] propose analyses beyond their scope, including evaluating tasks that require reasoning capabilities, for future work. Schaeffer et al. [66] also recognize their limited scope. In addition to expanding on the limitations of their evaluation datasets, Radford et al. [43] recognize their utilization of validation sets that do not accurately reflect true zero-shot scenarios despite their zero-shot focus as a limitation of their work in broader applicability for low-data settings. Le Scao and Rush [61] acknowledge their use of human-written prompts and recommend the analysis of automatic prompts for future work. Lee et al. [10] clarify the focuses of their study, describing that they examine the over-representation of duplicate texts but not under-representation, and they do not distinguish or analyze the positive versus negative impact of memorized content. These limited scopes introduce room for further examinations and analyses, as well as additional implications about the context of the findings presented."}, {"title": "7.3 Societal Impact", "content": "The limitations of language models as they pertain to large-scale release, application, and social impact are especially important to consider as LLMs grow in size and reach. The potentially harmful effects of LLMs pose risks in model release [1, 13, 14, 31, 43, 44, 56, 58, 68]. The most prominent mention of these risks is in their bias and toxicity, which are problems common across the use of LLMs, noted in the multimodal work ofAlayrac et al. [44] and Radford et al. [43] and the scaling studies of Rae et al. [21] and Hoffmann et al. [16]. In addition to issues of bias, Brown et al. [1], Saharia et al. [68], Yao et al. [58], Huang et al. [56], and Carlini et al. [14] highlight the potential for deliberate misuse of LLMs and their privacy limitations.\nThese concerns about bias and harmful uses bring risks in the release of LLMs, necessitating a balance between security and openness. Zhang et al. [13] cite these reasons as justification for releasing their OPT models to the research community before broader deployment. Sanh et al. [31] emphasize transparency and reproducibility as guiding principles in their decision to release their dataset, models, and tools. However, they acknowledge that their deliberate decisions to reduce the use of corpora with harmful content do not fully eliminate biases in LLMs."}, {"title": "8 Discussion", "content": "In this section, we answer the research questions and discuss the implications and future directions in LLM research.\n8.1 Answering Research Questions\nTo answer RQ1, we examined the main objectives of LLM research to identify the most prevalent areas to address in the field. One primary aim was to improve the absolute performance of LLMs, focusing on their efficiency, robustness, and scalability. With the increasing applications of LLMs, many studies also aimed to develop and apply best practices from both a technical and an ethical standpoint. Others bridged these two areas, analyzing both the abilities and impacts of LLMs.\nIn RQ2, we explored the methodologies of LLM studies to understand the various approaches to advancing LLM capabilities. The results demonstrated a focus on every stage of LLM development. A significant portion of the studies targeted model architectures, developing new learning strategies, and training and pre-training objectives. Concurrently, other studies highlighted the role of factors external to the model, such as constructing new datasets and processing and interpreting inputs and outputs. A set of studies applied methods that analyzed LLMs' capabilities.\nTo answer RQ3, we thoroughly examined the LLM studies and their ethical considerations. Firstly, we found weaknesses in the performance of LLMs on complex tasks, such as those involving logic, longer text, and specific linguistic phenomena. Furthermore, the data-reliance of LLMs raises concerns about their robustness. Secondly, we identified areas for expansion in the limited scope and assumptions used in the studies. Lastly, we found important considerations regarding the impact of LLMS, including learned toxicity and biases, potential for misuse, and additional factors that hold significant implications for the release of LLMs.\n8.2 Implications and Future Work\nBeyond summarizing current trends in LLM research, this systematic review has pinpointed pivotal directions and key considerations for future studies.\n8.2.1 Research Topics\nWe identify gaps and areas of progress in current research. In particular, the most underaddressed yet valuable areas include the scaling of LLMs, enhancing LLMs' linguistic understanding and reasoning capabilities, and improving the data efficiency of LLMs. Furthermore, as LLMs evolve and their capabilities expand, understanding"}, {"title": "9 Conclusion", "content": "LLMs are among the most influential developments in the field of NLP and AI, with rapid advancements making significant technical and societal impacts. However, the capabilities of LLMs are still rapidly evolving. Our aim was to examine the advancements and impacts of LLMs through a systematic review of studies on LLMs. The results contribute to a better understanding of the capabilities of LLMs and LLM research methodologies, the goals and avenues of the field, and the research and ethical limitations in LLMs. We also emphasize the value of studies that aim to understand and explain the workings of LLMs. Moreover, we identify best practices for responsible LLM development, offering recommendations for transparency, collaboration, and awareness of research impacts. Beyond providing a view of the current progress of LLM research, these potential directions can further assist in the growth and positive impacts of LLMs."}]}