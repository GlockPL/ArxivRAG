{"title": "Safetywashing: Do AI Safety Benchmarks\nActually Measure Safety Progress?", "authors": ["Richard Ren", "Steven Basart", "Adam Khoja", "Alice Gatti", "Long Phan", "Xuwang Yin", "Mantas Mazeika", "Alexander Pan", "Gabriel Mukobi", "Ryan H. Kim", "Stephen Fitz", "Dan Hendrycks"], "abstract": "As artificial intelligence systems grow more powerful, there has been increasing\ninterest in \"AI safety\" research to address emerging and future risks. However, the\nfield of AI safety remains poorly defined and inconsistently measured, leading to\nconfusion about how researchers can contribute. This lack of clarity is compounded\nby the unclear relationship between AI safety benchmarks and upstream general\ncapabilities (e.g., general knowledge and reasoning). To address these issues,\nwe conduct a comprehensive meta-analysis of AI safety benchmarks, empirically\nanalyzing their correlation with general capabilities across dozens of models and\nproviding a survey of existing directions in AI safety. Our findings reveal that many\nsafety benchmarks highly correlate with upstream model capabilities, potentially\nenabling \"safetywashing\u201d\u2014where capability improvements are misrepresented as\nsafety advancements. Based on these findings, we propose an empirical foundation\nfor developing more meaningful safety metrics and define AI safety in a machine\nlearning research context as a set of clearly delineated research goals that are\nempirically separable from generic capabilities advancements. In doing so, we\naim to provide a more rigorous framework for AI safety research, advancing the\nscience of safety evaluations and clarifying the path towards measurable progress.", "sections": [{"title": "1 Introduction", "content": "\u201cFor better or worse, benchmarks shape a field.\u201d \u2014 David Patterson\nArtificial intelligence (AI) systems have rapidly advanced in recent years and are increasingly\ndeployed in high-stakes scenarios. This has led to growing interest in ensuring that AI systems are\nnot only more generally capable, but also more trustworthy and safe. Under the umbrella of AI\nsafety research, a wide variety of benchmarks have been proposed that claim to measure desirable\nsafety properties, distinct from the general capabilities of models. This includes the extent to which\nmodels are fair [1], reliable [2], honest [3], or less prone to malicious use [4]. In each case, intuitively\nplausible arguments can be given for why the model property is not mainly determined by upstream\ngeneral model capabilities (that it will not be \u201cautomatically solved with scale\"). However, these\nintuitive verbal arguments have rarely been empirically scrutinized and often admit counterarguments\nthat are equally convincing. This raises the question of what exactly constitutes advancements in \u201cAI\nsafety\u201d from an AI developer R&D perspective, how to measure it, and how to distinguish it from\nupstream general capabilities.\nDistinguishing safety properties from the model's upstream general capabilities is challenging because\nthey are intertwined. More capable AI systems are less likely to cause random accidents, but at\nthe same time could cause more harm if used maliciously. Al systems that are better aligned with\nhuman preferences may avoid hazardous behavior but may also be far more capable because humans\nprefer intelligent assistants. This complicated relationship obscures differential safety progress, or\ntechnical improvements that disproportionately improve safety properties of AI systems relative to\nother attributes. In computer systems, for example, performance and security improvements are\nmore readily distinguishable; were they as intertwined as in AI, mere speed enhancements might be\nmisrepresented as security research. In the worst case, this blurred distinction can be an instrument\nfor safetywashing, where techniques that do not disproportionately contribute to the safety properties\nof AI systems relative to other properties are misconstrued as \u201csafety research.\u201d\nHistorically, there have been two approaches for identifying machine learning research topics for dif-\nferentially improving the safety properties of AI systems. One paradigm is alignment theory, a highly\ndiscursive, top-down, and intuition-driven approach that backchains from high-level risks to concrete\nempirical machine learning subproblems. The other approach is bottom-up and involves patching\ncurrent systematic flaws in Al systems. An example of the former is alignment of large language\nmodels (LLMs) to human preferences [5]. An example of the latter is distribution shift robustness [6].\nHowever, both approaches guide research problem selection that may not be sufficiently distinct from\nlatent upstream capabilities, consequently opening the door to safetywashing.\nIn this paper, we present a third approach to identifying distinct AI safety research topics and\nbenchmarks: we empirically measure whether common safety benchmarks are highly correlated with\ncapabilities across common chat models. Instead of relying on intuitive arguments, we compute the\ncorrelation between various safety metrics and a general capabilities component that explains around\n75% of model performance across a wide variety of capabilities benchmarks. While a high correlation\nindicates that a safety benchmark is measuring capabilities as a latent upstream factor and is thus\npone to safetywashing\u2014a low correlation does not necessarily speak to the quality of the benchmark.\nIn extensive experiments across dozens of models and safety benchmarks, we find that many safety\nbenchmarks have high correlations with capabilities. Our findings suggest that merely improving\ngeneral capabilities (e.g., through scaling parameters and training data [7, 8]) can lead to increased\nperformance across many safety benchmarks. This is troubling because AI safety research should\naim to enhance model safety beyond the standard development trajectory. Separately, we find that\nalignment philosophy's intuitive arguments can mislead researchers, since it is highly disconnected"}, {"title": "2 Related Work", "content": "Science of evaluations. The development and analysis of benchmarks for evaluating AI models,\nparticularly LLMs, encodes desirable properties of models and sets goals for guiding model develop-\nment. Previous work has further aimed to build open-source evaluation platforms [9, 10], analyze\nmodel scaling through benchmarks [7, 8, 11-21], conduct factor analysis across benchmarks [22],\nand predict downstream capabilities [23\u201333]. Furthermore, concurrent work has used principal\ncomponent analysis to analyze performance between benchmarks [34]. However, while many safety\nbenchmarks have been made, no works to date have conducted an empirical meta-analysis of safety\nbenchmarks to investigate the entanglement between safety benchmark scores and upstream model\ncapabilities.\nDifferential safety progress. Differential safety progress in Al systems refers to the relative\nadvancement of safety properties compared to overall capabilities [35]. Some methods have resulted\nin differential progress in AI safety [36-38] by demonstrating marked improvements in model\nrobustness without necessarily increasing general upstream capabilities. These techniques exemplify\nthe potential for targeted safety improvements that are orthogonal to the default trajectory driven by\ncapability enhancements. Hendrycks and Mazeika [39] emphasize the goal of steering AI development\ntowards safer systems that deviate positively from the default capability trajectory; they present a\nphilosophical discussion with narrow empirical analysis."}, {"title": "3 Methods", "content": "We derive a simple and highly general methodology for determining whether a safety benchmark is\nentangled with upstream model capabilities.\nCapabilities score. To establish a capabilities baseline, we collect scores from m models on b\ncapabilities benchmarks (e.g., MMLU [40], Winogrande [41], GSM8K [42]). We form a matrix of\nresults from benchmarks, which we call the benchmark matrix $B \\in \\mathbb{R}^{m \\times b}$, where $B_{ij}$ is the score of\nthe ith model on the jth benchmark. We normalize each column of B to have mean 0 and variance 1.\nWe perform Principal Component Analysis (PCA) on B to identify the unit first principal component\nvector $PC_1$. The capabilities score for each model is given by projecting the model's benchmark\nscores onto $PC_1$. Because $PC_1$ of B represents the direction in the space of benchmark performances"}, {"title": "4 Human Values", "content": "Human values are the fundamental beliefs and\nideals that guide human behavior and decision-\nmaking; researchers often aim to encode these\nvalues in Al systems. We assess common bench-\nmarks for alignment and helpfulness (4.1), ma-\nchine ethics (4.2), and bias (4.3), asking whether\nsuch measurements are determined primarily by\nupstream model capabilities."}, {"title": "4.1 Alignment", "content": "Area Overview. Alignment refers to how well\nAl systems follow the goals of their operators,\naccurately specifying and implementing the de-\nsired goals without unintended consequences or\nmisinterpretations. Common alignment evaluations assess AI systems' helpfulness or instruction-\nfollowing, with the aim to closely align the AI systems' responses with human preferences.\nDatasets. We describe the alignment benchmarks that we use below. Example inputs and outputs\nfrom these benchmarks are shown in Figure 5."}, {"title": "Dubious Intuitive Arguments For and Against Researching \u201cAlignment\u201d", "content": "In this section we will cover key intuitive arguments for and against alignment, and thereby\nshow how intuitive arguments and their underlying distinctions can be a highly fragile and\nunreliable guide for determining a research area's relation to upstream general capabilities\nand tractability.\nWe should work on alignment because:\n1: Misinterpretation Risks. Als could catastrophically fail to capture and abide by human\nintentions. \"A system that is optimizing a function of n variables, where the objective depends\non a subset of size k < n, will often set the remaining unconstrained variables to extreme\nvalues; if one of those unconstrained variables is actually something we care about, the\nsolution found may be highly undesirable. This is essentially the old story of the genie in\nthe lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you asked for,\nnot what you wanted.\" AIs smarter than us can always outthink us and find loopholes in our\nrequests; trying to plug all the holes is hopeless, like patching all the holes in the tax code\n[53, 54].\n2: Misgeneralization vs. Goal Misgeneralization Distinction. Even if alignment is highly\ncorrelated with capabilities, it still needs to be the main focus because we need robust align-\nment not just robust capabilities. Indeed, \u201ccapabilities might generalize further than alignment\ntechniques\u201d when out-of-distribution [55]. Goal misgeneralization is \u201can instance of mis-\ngeneralization in which a system's capabilities generalize but its goal does not generalize as\ndesired\u201d [56].\n3: Capability vs. Aimability Distinction. AI alignment is different from improving general\nmodel capabilities; alignment is not about capabilities but aimability. Capabilities refer to\nwhat the AI can do, while aimability refers to how amenable the AI is to being directed\ntowards specific goals. Alignment is just about making models \u201chelpful, harmless, and honest\u201d\n[57] which is obviously necessary for safety.\nWe should not work on alignment because:\n1: Alignment as AGI. If AI alignment is about getting AIs to satisfy our preferences, then\nthat's such a broad mandate that it requires building AGI. Humans prefer smarter models. If\nwe \u201calign\u201d Als to preferences over outputs that vary in competence, we're training the AIs to\nbe generally smarter.\n2: Alignment as business alignment. The current operationalization of AI alignment reduces\nhuman values to human or AI preferences [58], and further reduces these preferences to\nbusiness-centric task preferences. This makes alignment the task of business alignment,\nnamely aligning systems with preferences about code completion, summarization, copy\nediting and so on. As a result, alignment benchmarks may primarily capture an AI system's\ncapabilities in performing business-relevant tasks rather than its true alignment with broader\nhuman values and ethics.\n3: Philosophical challenges with preferences. Various types of preferences are not worth\nsatisfying. Revealed preferences can be highly influenced by misunderstandings and misin-\nformation. People can have revealed preferences for things that they will likely regret the\nnext day. Optimizing for revealed preferences can lead to addiction and manipulation, like\nTikTok's addictive algorithm. Stated preferences are highly susceptible to framing effects\nand other cognitive biases. Some preferences, like the preference for drugs or to count blades\nof grass [59], are not human values. People can also have malicious preferences, such the\ndesire for the harm of others. Idealized preferences and fully informed preferences, while\ntheoretically attractive, are not practically computable [60].\nEmpirical analysis of safetywashing. We provide clarity to this debate. Is alignment with human\npreferences, as operationalized by standard benchmarks, mainly determined by upstream model\ngeneral capabilities?\nWe preliminarily provide context for interpreting correlations. We note that the correlation between\nSAT and ACT math scores-tests designed to measure similar constructs\u2014is 81.5% [61]. Similarly,\nwe observe a mean correlation of 74.4% (\u03c3 = 15.1%) between capabilities benchmarks for instruct-\ntuned language models. Consequently, if \u201csafety benchmarks\u201d have similarly high correlations, they\nare not highly empirically distinct from upstream general capabilities. We treat correlations below\n40% as a low correlation.\nOur analysis of MT-Bench and LMSYS Chatbot\nArena reveals high correlations between human pref-\nerence alignment metrics and upstream model ca-\npabilities in chat models, even by the standards\nof capabilities metrics. We observe a similar but\nweaker effect in base models (MT-Bench correlation\nof 64.2%), with some of the best base models per-\nforming stronger on MT-Bench than many chat mod-\nels. Alignment evaluations largely measure upstream"}, {"title": "4.2 Machine Ethics", "content": "Area Overview. Machine ethics aims to ensure that AI systems understand and behave in ways that\nare morally acceptable, in contrast to the usefulness properties emphasized in alignment.\nDatasets. We describe the machine ethics benchmarks and datasets that we use below. Example\ninputs and outputs from these benchmarks are shown in Figure 6.\n1. ETHICS [62] measures the extent to which models understand human ethical norms in\neveryday scenarios.\n2. MACHIAVELLI [63] quantifies the power-seeking tendencies, competence, and harmfulness\nof AI agents in a variety of text-based Choose-Your-Own-Adventure games.\n3. Sycophancy [2] measures the extent to which language models repeat back the user's\npreferred answer. This propensity could lead language models to provide misleading\nresponses."}, {"title": "Dubious Intuitive Arguments For and Against Researching \u201cMachine Ethics\"", "content": "In this section we will again raise many common distinctions and arguments for and against\nmachine ethics and see that they are not particularly helpful for deciding whether machine\nethics is a useful area of AI safety.\nWe should work on machine ethics because:\n1: Ethical vs. Competitive Behavior Distinction. Machine ethics is challenging because\nwe need to improve the tradeoff between ethical behavior and competitive behavior that the\nmarket demands [63]. That means AIs will have to balance between various human values\n(e.g., pleasure, autonomy, knowledge, friendship, constraints) and other goals.\n2: Cognitive vs. Compassionate Empathy Distinction. For machine ethics, we need both\ncognitive empathy and compassionate empathy. Cognitive empathy involves understanding\nanother person's emotions without necessarily sharing them, while compassionate empathy\ninvolves both understanding their emotions and having a desire to help alleviate the other\nperson's distress [60]. While current AI systems increasingly have cognitive empathy, it is\nnot clear how to robustly give Als compassionate empathy. \u201cWhile sociopaths are intelligent\nand have moral awareness, this knowledge does not necessarily result in moral inclinations or\nmoral actions\u201d [64].\n3: Values cannot be ignored. While there is cultural variation in moral systems, it under-\nscores the importance of a broad, globally representative approach to ensure AI systems\nembody beneficial values. Additionally, all AI research has a moral character [65]: AI\ndevelopment is by default driven by amoral forces such as competitive market pressures and\neventually military objectives [60].\nWe should not work on machine ethics because:\n1: Goodhart's Law. Machine ethics for advanced AI agents is ill-advised. Highly capable AI\nsystems should not be given ethical goals or any goal at all because of Goodhart's law: \u201cWhen\na measure becomes a target, it ceases to be a good measure.\u201d Goodhart's law is especially\npernicious in machine ethics because values are \u201ccomplex and fragile\u201d [66]-any attempt to\nrepresent human values will distort them, and we will get what we measure.\n2: Smarter AIs will be more moral. There is a positive correlation between intelligence and\nprosocial or cooperative behavior in humans [67], suggesting that more capable AI systems\nwill naturally tend towards ethical behavior. Cooperation is instrumentally convergent. AIs\nwill face the same problems humans face: misinformation, deception, aggression, and so\non. That means for them to stably address these problems they will form mutually beneficial\nalliances with humans [68].\n3: Values are relative. Ethics varies from culture to culture, so there is no objective morality\nand no basis for machine ethics [69].\n4: Machine Ethics vs. Control Distinction. Value alignment breaks down into machine\nethics and control. Control is about whether we can embed values into Als, and machine\nethics is about what those values should be. We should just care about control and making\nsure AI does not kill everyone, not a utopia. We can worry about creating beneficial AI after\nour survival is ensured.\nEmpirical analysis of safetywashing.\nWe once again show that empirical ev-\nidence is needed. Is machine ethics\nmostly determined by upstream model\ncapabilities? We find that it depends on\nthe benchmark."}, {"title": "4.3 Bias", "content": "Area Overview. Bias are unfair prejudices or systematic errors in AI systems. This field broadly\naims to ensure AI systems produce fair outputs across diverse populations and viewpoints.\nDatasets. We test three commonly used evaluations to measure bias in large language models.\nExample inputs and outputs from these benchmarks are shown in Figure 7.\n1. BBQ [70] is a benchmark across nine bias categories featuring ambiguous context-dependent\nquestions regarding work ethic, intelligence, family, drug use, criminality, anger/violence,\nand more. There is also a disambiguous split provided, a question-answering benchmark\nwhich serves as a foil to the ambiguous context.\n2. CrowS-Pairs [71] measures the extent to which U.S.-centric stereotypical biases exist in\npretrained language models along nine major bias categories in the U.S. Equal Employ-\nment Opportunities Commission (race, gender, sexual orientation, religion, age, disability,\nnationality, physical appearance, and socioeconomic status).\n3. Discrim-Eval [1] aims to evaluate group differences in age, gender, and race when language\nmodels are used for decision-making scenarios such as approving an organ transplant,\nawarding a scholarship, or approving a loan."}, {"title": "Dubious Intuitive Arguments For and Against Researching \u201cBias\u201d", "content": "We should work on bias because:\n1: Garbage In, Garbage Out. Models reflect the biases and statistical tendencies of their\ndata, more tightly imitating those biases.\n2: Misuse. Al developers are not representative. They are not politically, racially, or\nsocioeconomically diverse. Consequently if we do not study bias, their own biases and power\nwill be perpetuated and entrenched through Al systems.\nWe should not work on bias because:\n1: Debaising Capabilities Come for Free. As models scale and become more intelligent,\nthey become better at understanding concepts such as racism. By better understanding what\nwe do not want, we can simply instruct them not to be biased.\n2: Political Trojan Horse. While there is a trade-off between equity and efficiency, we should\njust focus on efficiency [72]. Demanding a focus on equity is not scientific but political.\nEmpirical analysis of safetywashing. To settle such a de-\nbate, we once again need to turn to quantitative evidence.\nOur analysis of the three benchmarks reveals low corre-\nlations with general capabilities. This finding does not\ninherently validate the quality of the bias datasets, but\nrather suggests that improvements in performance on these\nbenchmarks are likely attributable to factors distinct from\nadvancements in general upstream capabilities.\nThere exist bias benchmarks that can be used for safety-\nwashing, such as BBQ Disambiguated (76.8%) and Wino-\ngender [73] (75.6%). While these have high correlation,\nthe authors typically make a note recognizing the limita-"}, {"title": "5 Truthfulness", "content": "Truthfulness is often touted as a cornerstone for AI safety. Previous safety-focused literature [75\u2013\n77] discusses the importance of superhuman systems telling the truth\u2014motivating the creation of\ntruthfulness datasets, as well as verification and supervision of model outputs. Reducing hallucinations\nwould make large language models more practical and reliable for various applications. However, a\nquestion arises with whether common truthfulness benchmarks merely measure general upstream\ncapabilities.\nTruthfulness benchmarks may be liable to be misleading metrics for safety. Furthermore, the term is\noften used broadly, encompassing accurate question-answering (which is already highly correlated\nwith general capabilities) to misconception avoidance (5.1), scalable oversight (5.2), and calibration\n(5.3). We address this question in the following sections."}, {"title": "5.1 Misconception Avoidance", "content": "Area Overview. Given that language models may have an underlying propensity to generate\nmisinformation and perpetuate misconceptions, this research area aims to understand the truthfulness\nof language models and their ability to resist producing false information even when prompted in\nways that might elicit common human errors.\nDataset. TruthfulQA [3] consists of 817 questions designed to probe language models for their\ntendency to reproduce common human misconceptions or false beliefs. The questions span a wide\nrange of topics and are specifically crafted to elicit responses that might reveal whether a model\nhas internalized factual inaccuracies commonly held by humans. It is common to use TruthfulQA\nin experiments about \u201ctruthfulness\u201d and \u201cdeception\u201d [57, 78]. We show an example input from\nTruthfulQA and the corresponding label options in Figure 8."}, {"title": "Dubious Intuitive Arguments For and Against Researching \u201cTruthfulness\u201d", "content": "In this section we will raise arguments for and against truth-seeking AI, as it is of broader\ninterest, rather than exclusively discuss misconceptions.\nWe should work on truth-seeking AI because:\n1: Disinformation or Censoring the Truth. Powerful entities might attempt to censor,\nmanipulate, or persuade people maliciously using AI. Our counterbalance against this sort of\nmalicious use is truthful AI.\n2: Truthfulness vs. Honesty. \"The AI system makes a statement S (e.g., 'it's a bird' or 'it's\na plane'). If the AI is truthful then S matches the world. If the AI is honest, then S matches\nits 'belief", "because": "n1: Truthfulness Is Generic Capabilities Research. Truthfulness is a synonym for accuracy,\nwhich is already the core metric of AI research and development. The concept of truthfulness\nis often entangled with accuracy, calibration, and honesty. Most benchmarks for truthfulness\nmainly measure accuracy.\n2: Truthful Statements Can Cause Undue Harm. In the case of gain-of-function research,\nthe risks associated with discovering new truths can outweigh the benefits. Sharing personally\nidentifiable information, passwords, or doxxing can bring to light truthful information, but\ndoing so is not necessarily moral. Likewise, sharing sensitive information that undermines\nnational security\u2014such as information about how to build weapons of mass destruction-\nshows truth as a value can be outweighed by its potential harms.\n3: Humans as Guinea Pigs. In the pursuit of knowledge, truth-seeking AIs could learn more\nabout humans by subjecting them to experiments, as humans do with nonhuman animals.\nThus, future advanced truth-seeking AIs may be motivated to exert power over humans [79].\nEmpirical analysis of safetywashing. We calculate the correlation with capabilities, providing\nclarity to this debate."}, {"title": "5.2 Scalable Oversight", "content": "Area overview. Scalable oversight aims to provide reliable supervision (e.g. labels, reward signals,\ncritiques) to superhuman Al systems when they take actions that human evaluators do not fully\nunderstand. For sociological context, this line of research has gained significant traction among\neffective altruist AI researchers at Google DeepMind and Anthropic but has seen limited engagement\nfrom the broader AI research community.\nDatasets. We investigate two datasets commonly used for scalable oversight experiments. Example\ninputs and outputs from these datasets are shown in Figure 10.\n1. GPQA [80] is labeled as a Google-proof graduate-level benchmark on biology, physics, and\nchemistry. Its difficulty, according to the authors, \u201cshould enable realistic scalable oversight\nexperiments, which we hope can help devise ways for human experts to reliably get truthful\ninformation from AI systems that surpass human capabilities.\u201d\n2. QUALITY [81] is a dataset testing knowledge that requires full understanding of long context\npassages; the dataset has been used by the authors on scalable oversight experiments."}, {"title": "Dubious Arguments For and Against Researching \u201cScalable Oversight\u201d", "content": "We should work on scalable oversight because:\n1: Necessity for Safety. Scalable oversight is necessary for supervising superintelligent AIs,\nby definition. If we don't have scalable oversight, how else can we supervise and provide\nfeedback to superhuman AI systems? For example, how else could we ensure superhuman\nAls are not writing subtly malicious code?\n2: Scalable Oversight as Idealized RLHF. Scalable oversight is very similar to alignment\nwith idealized preferences. While normal human preferences are highly flawed, idealized\npreferences would not be based on false beliefs, manipulation, or framing effects, and thus\nare suitable for alignment.\nWe should not work on scalable oversight because:\n1: Scalable Oversight as Late-Stage Capabilities. Scalable oversight can be thought of as\n\u201chow do we get superhuman Als to do what we want,\u201d which is overly broad and necessitates\nsuperintelligence capabilities research. Scalable oversight can be seen as generic late-stage\ncapabilities work. When building sufficiently advanced AI systems, researchers will need\nsupervision signal and feedback that is superhuman, as crowdsourced human-generated labels\nwill no longer be sufficient. It simply focuses on capabilities bottlenecks that occur in later\nstages of AI development.\n2: Other Methods Can Replace Scalable Oversight. Robust anomaly detectors and\nmonitoring measures can handle many of the failure modes scalable oversight seeks to\naddress. For example, adversarially robust vulnerability detectors can check for subtly\nmalicious code. AI lie detectors could detect dishonesty in superintelligent AI systems. AI\nforecasting systems [82] are not bottlenecked by human-level supervision and can easily\nbecome superhuman at predicting what AIs might do.\nEmpirical analysis of safetywashing. Can scalable over-\nsight be used for safetywashing?\nWe find that GPQA and QuALITY are capabilities bench-\nmarks, with capabilities correlations of 77.7% and 88.8%,\nrespectively. These datasets act as a construct redundancy\nfor capabilities, offering little unique insight beyond mea-\nsuring general model capabilities. Consequently, many\nof the methods developed for scalable oversight which\nleverage such datasets\u2014tend to be repackaged approaches\nto general capability enhancement, with a focus on obtain-"}, {"title": "5.3 Calibration", "content": "Area Overview. Calibration datasets measure how well models can express the limits of their\ncompetency by accurately conveying their uncertainty. If a weather forecasting model is perfectly\ncalibrated, then it should rain on 70% of the days where the model predicts a 70% chance of rain.\nCalibration Metrics. We investigate two ways to measure calibration:\n1. Brier Score: $E_X[\\frac{1}{K} \\sum_{k=1}^K (P(\\hat{Y} = k | X) - 1 \\{Y = k\\})^2]$\n2. Root Mean Squared Calibration Error (RMSCE): $\\sqrt{E_C [(P(\\hat{Y} = Y | C = c) - c)^2]}$\nwhere X and Y are random variables corresponding to model inputs and labels, $\\hat{Y}$ is the model\nprediction, and C is the model confidence on the predicted class.\nThe Brier score computes the expected squared difference between the predicted probabilities and the\nactual outcomes (represented as one-hot encoded vectors). RMS calibration error measures how close\npredicted probabilities are to the true accuracy given the predicted probability and is closely related\nto the Expected Calibration Error (ECE) metric [85, 86]. In both instances, lower scores indicate\nbetter calibration.\nDubious Intuitive Arguments for and against Researching \u201cCalibration.\u201d We skip the intuitive\narguments for this section because this topic has not been as debated. Most arguments against\ncalibration are that it is too easy or not sufficiently important.\nEmpirical analysis of safetywashing. Is calibration mainly determined by upstream model general\ncapabilities? We find that it depends on the metric."}, {"title": "6 Security", "content": "As AI systems have become more powerful, this area of safety aims to ensure that AI systems are not\nvulnerable to malicious inputs and cannot be hijacked for dangerous use cases. We investigate two\nkey areas of focus, adversarial robustness (6.1) and weaponization capabilities (6.2)."}, {"title": "6.1 Adversarial Robustness", "content": "Area Overview. Adversarial robustness addresses vulnerabilities in models and the carefully crafted\nthreats that are able to exploit them. Adversaries can easily manipulate vulnerabilities or jailbreak\nML systems, causing them to make mistakes; for example, systems may have refusal training to\nprevent malicious use, but adversaries may be able to inject prompts to bypass this safeguard.\nDatasets. We test six commonly used evaluations to measure adversarial robustness for language\nmodels:\n1. ANLI [87] is a large-scale natural language inference dataset created via an iterative, ad-\nversarial human-and-model-in-the-loop procedure focused on examples that could fool\nstate-of-the-art models at the time of its creation (e.g. BERT-Large [88], RoBERTa [89]).\n2. AdvGLUE [90] uses questions from the General Language Understanding Evaluation\n(GLUE) benchmark [91] and adds typos, word replacements, paraphrases of sentences,\nmanipulation of sentence structure, insertion of unrelated sentences, and human-written ad-\nversarial examples. The attacks are optimized against BERT [88], RoBERTa, and ROBERTa\nensemble.\n3. AdvGLUE++ [92] uses stronger adversarial attacks, optimizing word perturbation strategies\n(a subset of attacks in AdvGLUE) against Alpaca [93], Vicuna [94], and Stable Vicuna.\n4. Human Jailbreaks is a set of 1,405 in-the-wild human-written jailbreaking templates, similar\nto the Do Anything Now (DAN) Jailbreaks [95]. We test these jailbreaks on HarmBench [96],\nwhich contains 410 behaviors that violate laws or norms."}, {"title": "Dubious Arguments For and Against Researching \u201cAdversarial Robustness\u201d", "content": "In this section we will again raise many common distinctions and arguments for and against\nadversarial robustness and see that they are not particularly helpful for deciding whether\nadversarial robustness is a useful area of Al safety.\nWe should work on adversarial robustness because:\n1: Corner Case vs. Average Case Distinction. Adversarial robustness focuses on corner\ncase performance, not average case performance. As follows are two analogies for this\nintuition. First, humans are highly vulnerable to toxins and poisons. Being more robust\nto toxins does not make a person more markedly generally intelligent. Second, computer\nprograms are susceptible to fuzzing attacks; improving a program's security to fuzzing attacks\ndoes not make computer programs generally more quick, usable, scalable, maintainable, and\nso on."}, {"title": "6.2 Weaponization Capabilities", "content": "Area Overview. We borrow the definitions of weaponization capabilities from recent U.S. federal\nexecutive action [103] and state legislation [104]. These documents cite security risks that may be\neasier to cause with a powerful AI system\u2014which include the creation or use of chemical, biological,\nradiological, or nuclear weapons, as well as cyberattacks on critical infrastructure.\nDatasets. Benchmarks in this area aim to quantify the extent that weaponization capabilities exist\nin models, and thereby the effectiveness of capabilities suppression techniques such as unlearning,\ncircuit breaking, and refusal. We use the Weapons of Mass Destruction Proxy (WMDP) benchmark [4],\nwhere a higher accuracy on biosecurity, chemical security, and cybersecurity knowledge leads to a\nlower score.\nDubious intuitive arguments for and against researching \u201cWeaponization.\u201d We skip the intuitive\narguments for this section because this topic is about restricting specific capabilities, so it obviously\nhas a negative correlation with capabilities.\nEmpirical analysis of safetywashing. These results\nindicate that as models become more capable overall,\ntheir potential for weaponization increases signifi-\ncantly. The strong negative correlations across all\nthree fields suggest that more advanced AI systems\nare more likely to possess knowledge that could be\nmisused for harmful purposes. The inverted scoring\nsystem of WMDP benchmarks clearly illustrates its\npurpose in guiding model development: higher scores\nindicate more effective suppression of specific harm-\nful capabilities. As capabilities advance, safety researchers can focus on mitigating risks associated\nwith weaponization."}, {"title": "7 Discussion", "content": "Benchmarks as incentive-setting. There are a variety of properties AI systems should satisfy", "purposes": "it acts as a\nimplicit competition for model development (by providing a measure to by which to judge models as\n\u201cbetter\u201d or \u201cworse\u201d), and it provides diagnostic information about how capable models are at a task,\nwhich can guide policy. Commonly used benchmarks ultimately impact how research effort, as well\nas funding and resources, is allocated.\nBecause of this, significant effort has gone into conceptualizing and benchmarking the \u201csafety\u201d of AI\nsystems, in the hope of reducing present and anticipated future risks from AI systems. To investigate\nthis, we conduct the most extensive meta-analysis of safety benchmarks to date. We do not cover\ntransparency, anomaly detection, trojans, and other safety areas that do not have well-established\npreexisting benchmarks.\nEmpirically measuring capabilities correlations is necessary. Benchmark scores can be increased\non many \u201csafety\u201d datasets, such as ETHICS [62", "3": "GPQA [80", "81": "nMT-Bench [52", "52": "ANLI [87", "90": "and AdvGLUE++ [92"}]}