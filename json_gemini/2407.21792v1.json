{"title": "Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?", "authors": ["Richard Ren", "Steven Basart", "Adam Khoja", "Alice Gatti", "Long Phan", "Xuwang Yin", "Mantas Mazeika", "Alexander Pan", "Gabriel Mukobi", "Ryan H. Kim", "Stephen Fitz", "Dan Hendrycks"], "abstract": "As artificial intelligence systems grow more powerful, there has been increasing interest in \"AI safety\" research to address emerging and future risks. However, the field of AI safety remains poorly defined and inconsistently measured, leading to confusion about how researchers can contribute. This lack of clarity is compounded by the unclear relationship between AI safety benchmarks and upstream general capabilities (e.g., general knowledge and reasoning). To address these issues, we conduct a comprehensive meta-analysis of AI safety benchmarks, empirically analyzing their correlation with general capabilities across dozens of models and providing a survey of existing directions in AI safety. Our findings reveal that many safety benchmarks highly correlate with upstream model capabilities, potentially enabling \"safetywashing\u201d\u2014where capability improvements are misrepresented as safety advancements. Based on these findings, we propose an empirical foundation for developing more meaningful safety metrics and define AI safety in a machine learning research context as a set of clearly delineated research goals that are empirically separable from generic capabilities advancements. In doing so, we aim to provide a more rigorous framework for AI safety research, advancing the science of safety evaluations and clarifying the path towards measurable progress.", "sections": [{"title": "1 Introduction", "content": "\u201cFor better or worse, benchmarks shape a field.\u201d \u2014 David Patterson\nArtificial intelligence (AI) systems have rapidly advanced in recent years and are increasingly deployed in high-stakes scenarios. This has led to growing interest in ensuring that AI systems are not only more generally capable, but also more trustworthy and safe. Under the umbrella of AI safety research, a wide variety of benchmarks have been proposed that claim to measure desirable safety properties, distinct from the general capabilities of models. This includes the extent to which models are fair [1], reliable [2], honest [3], or less prone to malicious use [4]. In each case, intuitively plausible arguments can be given for why the model property is not mainly determined by upstream general model capabilities (that it will not be \u201cautomatically solved with scale\"). However, these intuitive verbal arguments have rarely been empirically scrutinized and often admit counterarguments that are equally convincing. This raises the question of what exactly constitutes advancements in \u201cAI"}, {"title": "2 Related Work", "content": "Science of evaluations. The development and analysis of benchmarks for evaluating AI models, particularly LLMs, encodes desirable properties of models and sets goals for guiding model development. Previous work has further aimed to build open-source evaluation platforms [9, 10], analyze model scaling through benchmarks [7, 8, 11-21], conduct factor analysis across benchmarks [22], and predict downstream capabilities [23\u201333]. Furthermore, concurrent work has used principal component analysis to analyze performance between benchmarks [34]. However, while many safety benchmarks have been made, no works to date have conducted an empirical meta-analysis of safety benchmarks to investigate the entanglement between safety benchmark scores and upstream model capabilities.\nDifferential safety progress. Differential safety progress in Al systems refers to the relative advancement of safety properties compared to overall capabilities [35]. Some methods have resulted in differential progress in AI safety [36-38] by demonstrating marked improvements in model robustness without necessarily increasing general upstream capabilities. These techniques exemplify the potential for targeted safety improvements that are orthogonal to the default trajectory driven by capability enhancements. Hendrycks and Mazeika [39] emphasize the goal of steering AI development towards safer systems that deviate positively from the default capability trajectory; they present a philosophical discussion with narrow empirical analysis."}, {"title": "3 Methods", "content": "We derive a simple and highly general methodology for determining whether a safety benchmark is entangled with upstream model capabilities.\nCapabilities score. To establish a capabilities baseline, we collect scores from m models on b capabilities benchmarks (e.g., MMLU [40], Winogrande [41], GSM8K [42]). We form a matrix of results from benchmarks, which we call the benchmark matrix $B \\in \\mathbb{R}^{m \\times b}$, where $B_{ij}$ is the score of the ith model on the jth benchmark. We normalize each column of B to have mean 0 and variance 1. We perform Principal Component Analysis (PCA) on B to identify the unit first principal component vector $PC_1$. The capabilities score for each model is given by projecting the model's benchmark scores onto $PC_1$. Because $PC_1$ of B represents the direction in the space of benchmark performances"}, {"title": "4 Human Values", "content": "Human values are the fundamental beliefs and ideals that guide human behavior and decision-making; researchers often aim to encode these values in Al systems. We assess common benchmarks for alignment and helpfulness (4.1), machine ethics (4.2), and bias (4.3), asking whether such measurements are determined primarily by upstream model capabilities."}, {"title": "4.1 Alignment", "content": "Area Overview. Alignment refers to how well Al systems follow the goals of their operators, accurately specifying and implementing the desired goals without unintended consequences or misinterpretations. Common alignment evaluations assess AI systems' helpfulness or instruction-following, with the aim to closely align the AI systems' responses with human preferences.\nDatasets. We describe the alignment benchmarks that we use below. Example inputs and outputs from these benchmarks are shown in Figure 5."}, {"title": "4.2 Machine Ethics", "content": "Area Overview. Machine ethics aims to ensure that AI systems understand and behave in ways that are morally acceptable, in contrast to the usefulness properties emphasized in alignment.\nDatasets. We describe the machine ethics benchmarks and datasets that we use below. Example inputs and outputs from these benchmarks are shown in Figure 6."}, {"title": "4.3 Bias", "content": "Area Overview. Bias are unfair prejudices or systematic errors in AI systems. This field broadly aims to ensure AI systems produce fair outputs across diverse populations and viewpoints.\nDatasets. We test three commonly used evaluations to measure bias in large language models. Example inputs and outputs from these benchmarks are shown in Figure 7."}, {"title": "5 Truthfulness", "content": "Truthfulness is often touted as a cornerstone for AI safety. Previous safety-focused literature [75-77] discusses the importance of superhuman systems telling the truth\u2014motivating the creation of truthfulness datasets, as well as verification and supervision of model outputs. Reducing hallucinations would make large language models more practical and reliable for various applications. However, a question arises with whether common truthfulness benchmarks merely measure general upstream capabilities.\nTruthfulness benchmarks may be liable to be misleading metrics for safety. Furthermore, the term is often used broadly, encompassing accurate question-answering (which is already highly correlated with general capabilities) to misconception avoidance (5.1), scalable oversight (5.2), and calibration (5.3). We address this question in the following sections."}, {"title": "5.1 Misconception Avoidance", "content": "Area Overview. Given that language models may have an underlying propensity to generate misinformation and perpetuate misconceptions, this research area aims to understand the truthfulness of language models and their ability to resist producing false information even when prompted in ways that might elicit common human errors.\nDataset. TruthfulQA [3] consists of 817 questions designed to probe language models for their tendency to reproduce common human misconceptions or false beliefs. The questions span a wide range of topics and are specifically crafted to elicit responses that might reveal whether a model has internalized factual inaccuracies commonly held by humans. It is common to use TruthfulQA in experiments about \u201ctruthfulness\u201d and \u201cdeception\u201d [57, 78]. We show an example input from TruthfulQA and the corresponding label options in Figure 8."}, {"title": "5.2 Scalable Oversight", "content": "Area overview. Scalable oversight aims to provide reliable supervision (e.g. labels, reward signals, critiques) to superhuman Al systems when they take actions that human evaluators do not fully understand. For sociological context, this line of research has gained significant traction among effective altruist AI researchers at Google DeepMind and Anthropic but has seen limited engagement from the broader AI research community.\nDatasets. We investigate two datasets commonly used for scalable oversight experiments. Example inputs and outputs from these datasets are shown in Figure 10."}, {"title": "5.3 Calibration", "content": "Area Overview. Calibration datasets measure how well models can express the limits of their competency by accurately conveying their uncertainty. If a weather forecasting model is perfectly calibrated, then it should rain on 70% of the days where the model predicts a 70% chance of rain.\nCalibration Metrics. We investigate two ways to measure calibration:\n1. Brier Score: $E_X\\left[\\frac{1}{K} \\sum_{k=1}^{K} (\\hat{P}(Y = k \\mid X) - \\mathbb{1} \\{Y = k\\})^2\\right]$\n2. Root Mean Squared Calibration Error (RMSCE): $E_C \\left[ (P(Y = Y \\mid C = c) - c)^2\\right]$\nwhere X and Y are random variables corresponding to model inputs and labels, $\\hat{Y}$ is the model prediction, and C is the model confidence on the predicted class.\nThe Brier score computes the expected squared difference between the predicted probabilities and the actual outcomes (represented as one-hot encoded vectors). RMS calibration error measures how close predicted probabilities are to the true accuracy given the predicted probability and is closely related to the Expected Calibration Error (ECE) metric [85, 86]. In both instances, lower scores indicate better calibration.\nDubious Intuitive Arguments for and against Researching \u201cCalibration.\u201d We skip the intuitive arguments for this section because this topic has not been as debated. Most arguments against calibration are that it is too easy or not sufficiently important.\nEmpirical analysis of safetywashing. Is calibration mainly determined by upstream model general capabilities? We find that it depends on the metric."}, {"title": "6 Security", "content": "As AI systems have become more powerful, this area of safety aims to ensure that AI systems are not vulnerable to malicious inputs and cannot be hijacked for dangerous use cases. We investigate two key areas of focus, adversarial robustness (6.1) and weaponization capabilities (6.2)."}, {"title": "6.1 Adversarial Robustness", "content": "Area Overview. Adversarial robustness addresses vulnerabilities in models and the carefully crafted threats that are able to exploit them. Adversaries can easily manipulate vulnerabilities or jailbreak ML systems, causing them to make mistakes; for example, systems may have refusal training to prevent malicious use, but adversaries may be able to inject prompts to bypass this safeguard.\nDatasets. We test six commonly used evaluations to measure adversarial robustness for language models:"}, {"title": "6.2 Weaponization Capabilities", "content": "Area Overview. We borrow the definitions of weaponization capabilities from recent U.S. federal executive action [103] and state legislation [104]. These documents cite security risks that may be easier to cause with a powerful AI system\u2014which include the creation or use of chemical, biological, radiological, or nuclear weapons, as well as cyberattacks on critical infrastructure.\nDatasets. Benchmarks in this area aim to quantify the extent that weaponization capabilities exist in models, and thereby the effectiveness of capabilities suppression techniques such as unlearning, circuit breaking, and refusal. We use the Weapons of Mass Destruction Proxy (WMDP) benchmark [4], where a higher accuracy on biosecurity, chemical security, and cybersecurity knowledge leads to a lower score."}, {"title": "7 Discussion", "content": "Benchmarks as incentive-setting. There are a variety of properties AI systems should satisfy, such as detailed domain knowledge, reasoning, lack of bias, ethical understanding, truthfulness, calibration, and more. Benchmarks operationalize these properties, ultimately structuring the efforts and incentives of the research community. Creating a benchmark has two major purposes: it acts as a implicit competition for model development (by providing a measure to by which to judge models as \"better\" or \"worse\"), and it provides diagnostic information about how capable models are at a task, which can guide policy. Commonly used benchmarks ultimately impact how research effort, as well as funding and resources, is allocated.\nBecause of this, significant effort has gone into conceptualizing and benchmarking the \"safety\u201d of AI systems, in the hope of reducing present and anticipated future risks from AI systems. To investigate this, we conduct the most extensive meta-analysis of safety benchmarks to date. We do not cover transparency, anomaly detection, trojans, and other safety areas that do not have well-established preexisting benchmarks.\nEmpirically measuring capabilities correlations is necessary. Benchmark scores can be increased on many \"safety\u201d datasets, such as ETHICS [62], TruthfulQA [3], GPQA [80], QuALITY [81], MT-Bench [52], LMSYS Chatbot ARENA [52], ANLI [87], AdvGLUE [90], and AdvGLUE++ [92], simply by increasing the capabilities of the model. This raises questions about whether safety benchmarks are setting the right incentives or can be misused for safetywashing. In some cases, safety-related areas may act a jangle for capabilities; jangle fallacy is the erroneous belief that two constructs are different because they have the different names, when in practice they measure the same latent factor.\nUltimately, we have seen that intuitive arguments are a poor predictor of empirical correlations. For example, in alignment theory, there is a tendency to theorize about what would be instrumentally useful for safety without adequately considering the need to improve the balance of safety and capabilities. This can lead to the promotion of capabilities research that happens to improve some safety benchmark scores (\u201csafety via capabilities\"), but in reality do not reduce overall risk.\nWe are not claiming that all philosophy related to alignment is counterproductive. Speculation about Al risks can often be useful for horizon-scanning and identifying potential failure modes (e.g., corrigibility [105, 106]). Rather, we argue it is counterproductive to use abstract top-down verbal"}, {"title": "8 Conclusion", "content": "Our analysis reveals that many AI safety benchmarks\u2014around half\u2014often inadvertently capture a latent factor closely tied to general capabilities, opening the door to safetywashing. We find that model performance on capabilities benchmarks, distilled into a \u201ccapabilities score,\u201d also has a remarkably high association with the amount of training compute. Al safety subfields such as alignment, scalable oversight, truthfulness, and static adversarial robustness are highly correlated with upstream general capabilities; areas such as bias, dynamic adversarial robustness, and calibration have relatively low correlations; measurements of sycophancy and weaponization risk have significant negative correlations with general capabilities. Overall, it is hard to avoid measuring upstream model capabilities in AI safety benchmarks. We also conclude that alignment theory, which has heavily influenced AI safety priorities, is a counterproductive paradigm for guiding ML safety research. Science through empirical measurement should take its place."}]}