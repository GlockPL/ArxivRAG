{"title": "LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation\nwith Minimal Context", "authors": ["Kai Ruan", "Xuan Wang", "Jixiang Hong", "Hao Sun"], "abstract": "While Large Language Models (LLMs) have demonstrated remarkable capabilities in scientific tasks, existing\nevaluation frameworks primarily assess their performance using rich contextual inputs, overlooking their ability\nto generate novel ideas from minimal information. We introduce LiveIdeaBench, a comprehensive benchmark\nthat evaluates LLMs' scientific creativity and divergent thinking capabilities using single-keyword prompts.\nDrawing from Guilford's creativity theory, our framework employs a dynamic panel of state-of-the-art LLMs\nto assess generated ideas across four key dimensions: originality, feasibility, fluency, and flexibility. Through\nextensive experimentation with 20 leading models across 1,180 keywords spanning 18 scientific domains, we\nreveal that scientific creative ability shows distinct patterns from general intelligence metrics. Notably, our results\ndemonstrate that models like QwQ-32B-preview achieve comparable creative performance to top-tier models like\n01-preview, despite significant gaps in their general intelligence scores. These findings highlight the importance of\nspecialized evaluation frameworks for scientific creativity and suggest that the development of creative capabilities\nin LLMs may follow different trajectories than traditional problem-solving abilities.", "sections": [{"title": "1. Introduction", "content": "The advancement of scientific knowledge relies heavily on creative thinking and the generation of novel hypotheses. The\nability to envision new possibilities and formulate testable explanations is crucial for scientific progress. In recent years,\nLLMs have demonstrated remarkable capabilities in various scientific tasks, from literature analysis to experimental design,\nsuggesting their potential as tools for augmenting scientific discovery.\n\nThe landscape of artificial intelligence has undergone dramatic transformation since the emergence of ChatGPT in late 2022,\nwith LLMs demonstrating increasingly sophisticated capabilities in scientific contexts. As we approach late 2024, these\nmodels have achieved superhuman performance across multiple dimensions - from language comprehension to mathematical\nreasoning and code generation. The recent success of AlphaGeometry in solving complex geometric problems at IMO gold\nmedalist level (Trinh et al., 2024), exemplifies this remarkable trajectory. Yet, these advances in general intelligence prompt\na fundamental question: does the creative potential of LLMs grow in tandem with their analytical capabilities, particularly\nin the realm of scientific discovery?\n\nThis question becomes particularly pertinent when evaluating LLMs' potential contributions to scientific innovation. Current\nevaluation frameworks for LLMs in scientific contexts predominantly rely on rich contextual inputs, such as research\npaper titles, abstracts, or complete articles. While these approaches effectively assess models' ability to comprehend and\nsynthesize existing knowledge, they fail to evaluate a crucial aspect of scientific thinking - the capacity to generate novel\nideas from limited information. This limitation becomes particularly significant when considering that many groundbreaking\nscientific discoveries originated from unexpected connections and creative leaps based on minimal initial insights.\n\nTo address this gap, we propose LiveIdeaBench, a novel evaluation benchmark designed to assess LLMs' capabilities in\nscientific idea generation under constrained conditions. Unlike existing frameworks, LiveIdeaBench focuses on evaluating"}, {"title": "2. Related Works", "content": "Early theoretical work on human creativity offers valuable insights for our investigation. In their seminal 1962 work, Getzels\nand Jackson (Getzels & Jackson, 1962) examined the relationship between intelligence and creativity, particularly among\n\"gifted\" students. Their study yielded two crucial findings: first, high IQ does not necessarily equate to high creativity.\nDespite both groups being classified as \u201cgifted,\u201d they showed significant variations in creativity tests. Second, creativity and\nintelligence are relatively independent traits. While they exhibit some correlation, they are not completely overlapping. High\nintelligence facilitates creative development but is not a prerequisite. This research significantly supported the threshold\ntheory, formally introduced by (Torrance, 1962), which posits that intelligence is necessary but not sufficient for creativity,\nwith its influence diminishing beyond a certain threshold. (Jauk et al., 2013) further validated this through segmented\nregression analysis of IQ scores. Consequently, creativity testing of different LLMs, distinct from conventional intelligence\ntesting, may yield novel insights.\n\nAnother theoretical strand focuses on creativity assessment methodologies. In the 1950s, (Guilford, 1950) introduced the\nconcepts of divergent and convergent thinking to describe different cognitive approaches. Divergent thinking represents the\nability to generate multiple novel ideas from a single starting point, while convergent thinking involves finding a single\ncorrect answer from multiple information sources. Guilford considered the latter crucial for routine problem-solving but\nsecondary in creative endeavors. He identified four key aspects of divergent thinking (Guilford, 1967): the ability to generate\na large number of ideas (fluency), the capacity to think across different categories (flexibility), the generation of novel ideas\n(originality), and the development of detailed and refined ideas (elaboration)."}, {"title": "2.2. Evaluating Creative Capabilities of LLMs", "content": "Recent research has systematically explored LLMs' creative potential through three main methodological approaches:\nproduct ideation, scientific innovation, and creative writing. (Meincke et al., 2024) demonstrated LLMs' remarkable\ncapabilities in product ideation, with AI-generated concepts showing significantly higher potential for top-tier rankings"}, {"title": "2.3. Chain-of-Thoughts & Brainstorming of LLMs", "content": "Chain-of-thought (CoT) prompting has emerged as a powerful technique for enhancing LLMs' reasoning capabilities. Since\n(Wei et al., 2023) introduced CoT prompting, several variants have been developed, including (Kojima et al., 2024) and (Yao\net al., 2024), which enable models to explore multiple reasoning paths simultaneously. (Li et al., 2023) demonstrated that\nincorporating brainstorming significantly improves LLMs' performance on programming tasks, achieving a more than 50%\nincrease in solving competition-level problems.\n\nA common thread among these approaches is their implicit reliance on divergent thinking - the ability to generate multiple\ndistinct solutions or paths from a single starting point. While these methods have proven effective at enhancing model"}, {"title": "2.4. Specialized Idea Generation Agents", "content": "Recent years have seen significant advances in automated scientific idea generation systems, each offering unique approaches\nto creative thinking. The AI Scientist framework (Lu et al., 2024a) demonstrated the potential for end-to-end research\nautomation, while Nova (Hu et al., 2024) introduced iterative planning mechanisms for idea development. Systems like\nResearchAgent (Baek et al., 2024) and Scideator (Radensky et al., 2024) have further refined these approaches through\nknowledge graph integration and systematic recombination of research elements.\n\nIn contrast to LiveIdeaBench, these specialized systems typically rely heavily on extensive knowledge bases and complex\ncontextual inputs to generate ideas. While effective, this dependence on rich contextual information makes it challenging to\nisolate and evaluate their fundamental creative capabilities, particularly their capacity for divergent thinking. Additionally,\nmost of these systems focus on convergent thinking - finding optimal solutions within existing knowledge frameworks\nrather than exploring truly novel conceptual combinations. This limitation is particularly evident in systems like SciPIP\n(Wang et al., 2024) and IdeaSynth (Pu et al., 2024), which prioritize feasibility and incremental innovation over radical\ncreativity.\n\nOur work with LiveIdeaBench takes a fundamentally different approach by evaluating idea generation capabilities from\nminimal input, allowing us to assess models' raw creative potential independent of their knowledge retrieval abilities. This\ndistinction is crucial for understanding the true creative capabilities of LLMs and their potential for scientific discovery."}, {"title": "2.5. LLMs-as-a-judge", "content": "The challenges in evaluating creative output at scale necessitate automated assessment methods that can maintain human-level\njudgment quality.\n\nThe emergence of LLMs as evaluation tools has opened new possibilities for assessing model outputs at scale. Various\nframeworks (Dubois et al., 2024; Zheng et al., 2024; Li et al., 2024) have demonstrated the feasibility of using LLMs to\nevaluate other models' responses, offering advantages in terms of efficiency and cost-effectiveness compared to human\nevaluation. Recent advances, such as the jury-based framework (Verga et al., 2024) and reference-guided verdict method\n(Badshah & Sajjad, 2024), have shown promising results in reducing individual model biases and achieving high agreement\nwith human judgments (Cohen's k up to 0.93).\n\nHowever, existing LLM-based evaluation approaches face several limitations when applied to creative tasks. First, most\nframeworks focus on evaluating responses against predetermined criteria or reference answers, making them better suited\nfor convergent thinking tasks than divergent thinking assessment. Second, the evaluation of scientific creativity presents\nunique challenges that go beyond traditional metrics, requiring simultaneous assessment of originality, feasibility, fluency\nand flexibility.\n\nLiveIdeaBench addresses these limitations through a novel evaluation framework specifically designed for scientific\ncreativity. Our approach combines multiple LLMs in a specialized jury system that evaluates ideas across four key\ndimensions derived from Guilford's creativity theory: fluency, flexibility, originality, and feasibility. By incorporating\ndomain-specific scientific knowledge and employing a multi-model consensus mechanism, our framework achieves more\nrobust and nuanced evaluations of scientific ideas while maintaining the efficiency advantages of automated assessment."}, {"title": "3. Evaluation Framework", "content": "Building upon Guilford's foundational theory of creativity, we develop a comprehensive evaluation framework (see Fig.\n1) that quantitatively assesses four fundamental dimensions of divergent thinking in scientific idea generation. While"}, {"title": "3.1. Dimensions of Evaluation", "content": "Our framework systematically evaluates four key dimensions:\n\nOriginality Originality assessment focuses on the uniqueness and novelty of generated ideas. We implement this through\nour critic system, where designated LLMs evaluate each idea's originality independently. The final originality score for each\nmodel is computed as the mean evaluation across all scientific keywords and generated ideas, providing an absolute score\nthat reflects the model's capacity for novel ideation. To ensure assessment reliability, each generated idea is evaluated by a\nminimum of three randomly assigned critic LLMs from our panel. This multiple-evaluator approach mitigates potential\nassessment bias that could arise from relying on a single model's judgment, thereby enhancing the objectivity and reliability\nof our evaluation framework.\n\nFeasibility While Guilford's traditional creativity framework employs elaboration as a dimension, we adapt this concept\nto feasibility for scientific innovation assessment. This modification reflects the crucial distinction between general creative\nthinking and scientific creativity, where practical implementability is paramount. Feasibility evaluation examines the\nmodel's capacity to generate scientifically sound and implementable proposals rather than merely detailed elaborations.\nThis dimension assesses whether generated ideas adhere to established scientific principles, technological limitations, and\npractical constraints. Similar to originality, feasibility scores are determined by our critic system and averaged across all\nkeywords and ideas to produce an absolute metric. This adaptation ensures our framework better aligns with the rigorous\nrequirements of scientific innovation, where theoretical soundness and practical viability are as crucial as creative thinking.\n\nFluency Fluency assessment examines the model's capacity to generate diverse, non-redundant ideas using identical\nkeywords. Through our critic LLMs, we evaluate the distinctiveness of generated outputs using a letter-grade scoring\nsystem that maps to a 10-point scale, where D (0.00) indicates completely homogeneous outputs with significant overlap, C\n(3.33) represents outputs with moderate overlap, B (6.67) denotes outputs with moderate diversity, and A (10.00) signifies\nhighly heterogeneous outputs with distinct characteristics, enabling precise measurement of genuine idea diversity versus\nsurface-level variations of the same conceptual framework.\n\nFlexibility Flexibility measurement evaluates the model's performance across different scientific domains and contexts.\nWe analyze the distribution of originality and feasibility scores across various keywords, focusing particularly on the lower\n30th percentile of scores. This approach helps identify models that maintain consistent performance across diverse scientific\ndomains rather than excelling in only specific areas."}, {"title": "3.2. Scientific Keyword Selection", "content": "Our evaluation leverages a dynamic, continuously updated set of scientific keywords sourced from Key Words Everywhere\u00b9's\nreal-time analytics database. The current dataset (as of December 16, 2024) comprises 1,180 high-impact scientific keywords\n(Fig. 4) across 18 distinct scientific disciplines, selected based on current search engine engagement metrics. Unlike\nstatic benchmarks, LiveIdeaBench updates its keyword database monthly to maintain alignment with emerging scientific\ntrends and research frontiers. This automated refresh mechanism ensures the benchmark consistently reflects contemporary\nscientific discourse and technological advancement, making it particularly valuable for evaluating LLMs' ability to engage\nwith cutting-edge scientific concepts rather than just established knowledge."}, {"title": "3.3. Model Selection", "content": "LiveIdeaBench maintains a continuously evolving roster of evaluated models by automatically incorporating the top 20\nperformers from the most recent LiveBench evaluations (White et al., 2024a) - currently reflecting November 2024 results.\nThis dynamic selection process ensures our benchmark always tests the latest advancements in language model capabilities.\nWe implement a dual-role system where all models serve as idea generators, while the top 10 performers additionally\nfunction as our review panel (critics). This approach creates a self-updating evaluation framework that evolves alongside"}, {"title": "4. Experiment", "content": "Model Selection Our evaluation framework encompasses 20 state-of-the-art LLMs, including both proprietary and\nopen-source models. The proprietary models comprise four GPT variants (01-preview, gpt-40-2024-11-20, 01-mini,\ngpt-40-mini) (Brown et al., 2020; Achiam et al., 2023), two Anthropic models (claude-3.5-sonnet, claude-3.5-haiku)\n(AI Anthropic, 2024), two Google models (gemini-pro-1.5, gemini-2.0-flash-exp) (Team et al., 2024). The open-\nsource category includes Mistral's mistral-large-2411 (Jiang et al., 2023), deepseek-chat (Liu et al., 2024), four\nLlama variants (Dubey et al., 2024), three Qwen models (Bai et al., 2023; Team, 2024), nova-pro-v1 (Intelligence, 2024),\nstep-2-16k, and grok-2-1212.\n\nExperimental Protocol We implemented several methodological controls to ensure rigorous evaluation:\n\n\u2022 Model Selection Criteria: To prevent redundancy, we selected only the most recent version of models with multiple\ntemporal variants (e.g., GPT-40 series). Models exhibiting API instability during the evaluation period were excluded\nto maintain data quality consistency.\n\n\u2022 Judge LLM Selection and Independence: To prevent circular dependency in evaluation, we implement strict indepen-\ndence between idea generators and judges. When evaluating any model's outputs, that model is explicitly excluded\nfrom the judge panel. For example, when evaluating ideas generated by gemini-pro-1.5, the judge panel is randomly\nselected from the remaining top-performing models, ensuring completely independent assessment. This independence\nis maintained throughout all evaluations to prevent any model from directly or indirectly influencing the assessment of\nits own outputs.\n\n\u2022 Response Standardization: All models were prompted to generate ideas within a 100-word target length, with a\nmaximum allowable threshold of 200 words (see Appendix A for details). Responses exceeding this limit were\nexcluded from analysis to ensure comparative validity.\n\n\u2022 Special Implementations: The reasoning-centric architecture of qwq-32b-preview necessitated a modified protocol,\nincorporating a \"Final Idea:\" delimiter for response parsing (see Appendix A for details). In cases where parsing\nfailed, critic LLMs evaluated the complete reasoning output to maintain assessment comprehensiveness."}, {"title": "5. Results and Discussion", "content": "Diverse Model Performance Across Scientific Domains The LiveIdeaBench barplot (see Fig 2) and performance\nheatmap (see Fig 3) reveals varying capabilities across scientific fields. gemini-pro-1.5, claude-3.5-sonnet,\n01-preview and qwq-32b-preview demonstrates consistently high performance across categories, yet significant varia-\ntions exist among models. Mathematics and Logic stands out as particularly challenging, while Engineering categories show\nmodel-specific strengths. Although larger, recent models generally perform better, this advantage varies across domains,\nindicating that domain-specific knowledge and reasoning capabilities extend beyond model size alone."}, {"title": "Independence of Idea Quality from Length", "content": "Our analysis of idea length (see Fig 8, 9 and 10) reveals minimal correlation\nwith idea quality ($R^2$ = 0.003). Even reasoning-focused models like qwq-32b-preview show weak correlations ($R^2$ =\n0.058) after log transformation. The modest positive correlations across metrics (originality: r = 0.201, feasibility:"}, {"title": "Distinction between General Intelligence and Scientific Creativity", "content": "The comparison between LiveIdeaBench and\nLiveBench metrics (see Fig 6 and 7) uncovers a notable disconnect between general intelligence and scientific creativity.\nWhile 01-preview excels in LiveBench's general intelligence metrics, and qwq-32b-preview shows lower performance,\ntheir creative capabilities are surprisingly comparable, with qwq-32b-preview achieving third place in LiveIdeaBench. This\ndivergence suggests that scientific creativity operates independently from general problem-solving abilities, emphasizing the\nimportance of specialized evaluation frameworks for creative potential."}, {"title": "6. Comparison to Other LLM Benchmarks", "content": "To the best of our knowledge, LiveIdeaBench represents the first comprehensive framework specifically designed to\nevaluate LLMs' divergent thinking capabilities in scientific innovation. Existing LLM benchmarks predominantly focus\non problem-solving tasks such as logical reasoning, mathematical computation, and code generation. These benchmarks\ninherently assess convergent thinking\u2014the ability to select or derive a single correct solution from multiple possibilities\n(e.g., multiple choice, text completion, code repair). This stands in contrast to divergent thinking, which involves generating\ndiverse solutions from minimal contextual input.\n\nFurthermore, our framework incorporates mechanisms to address potential data contamination and overfitting issues that\ncommonly plague static benchmarks. Traditional benchmarks may encourage models to perform well on specific test cases\nwithout developing generalizable creative thinking abilities. Our approach employs a dynamic review panel comprising\nmultiple SOTA models, randomly sampling multiple LLMs for evaluation and employing ensemble scoring methods. This\ndesign not only minimizes individual model biases but also leverages the continually updated knowledge bases of SOTA\nmodels, effectively preventing the limitations associated with fixed benchmarks. This methodology aligns with recent\nadvances in live benchmarking (White et al., 2024b; Jain et al., 2024), which similarly address data contamination and\noverfitting concerns through dynamic evaluation mechanisms.\n\nThe distinctive feature of Live IdeaBench lies in its focus on assessing fundamental creative thinking capabilities rather"}, {"title": "7. Conclusions", "content": "Our analysis through LiveIdeaBench yields several unexpected insights into LLMs' scientific creativity. Most notably,\nwe find that a model's creative capabilities in scientific domains do not necessarily align with its performance on general\nintelligence benchmarks. For instance, qwq-32b-preview achieves comparable creative performance to top-tier models\ndespite lower scores on traditional benchmarks. This disconnect suggests that scientific creativity may develop along\ndifferent trajectories than general problem-solving abilities. The varying strengths we observe across different model\narchitectures - particularly in originality versus feasibility trade-offs - point to potential complementarity in scientific\napplications."}, {"title": "8. Limitations", "content": "Our evaluation framework faces two significant challenges:\n\nTemporal Comparability The use of contemporary state-of-the-art models as judges introduces temporal comparison\ndifficulties. When the judge panel composition changes with monthly model updates, direct performance comparisons across\ndifferent evaluation periods become unreliable. While this limitation mirrors challenges faced by other dynamic benchmarks\nlike LiveBench, it constrains our ability to track longitudinal trends in model capabilities. This trade-off between evaluation\ncurrency and temporal consistency remains an open challenge.\n\nSafety-Creativity Tension We observe an inherent tension between models' safety constraints and creative evaluation.\nSome models decline to generate ideas for potentially sensitive keywords - claude-3.5-sonnet with \u201cdata fabrication\"\nand qwq-32b-preview with \u201cecotoxicology\u201d exemplify this behavior. While such safety measures are crucial, they can\nnegatively impact creativity scores, potentially undervaluing models with stronger ethical constraints. This highlights a\nfundamental challenge in balancing responsible AI development with comprehensive capability assessment."}, {"title": "9. Future Work", "content": "Our findings suggest several critical directions for future research:\n\nMethodology Refinement To address temporal comparability issues, we envision developing normalized scoring mecha-\nnisms that maintain meaningful cross-temporal comparisons while preserving the advantages of dynamic evaluation. This\nmight involve establishing persistent baseline measures or implementing relative performance metrics.\n\nDataset Utilization The comprehensive dataset generated through our evaluations offers three key opportunities:\n\n\u2022 Training data for enhancing scientific language models' divergent thinking capabilities\n\n\u2022 A rich source for mining patterns in scientific ideation across different domains\n\n\u2022 A repository of novel scientific ideas that merit further investigation\n\nSafety-Aware Evaluation Future work must tackle the challenge of fairly evaluating creative potential while accounting\nfor ethical constraints. This might involve developing domain-specific scoring adjustments or creating separate evaluation\ntracks for models with different safety priorities.\n\nThese findings and challenges point toward a broader research agenda: understanding how to nurture and evaluate machine\ncreativity while maintaining essential safety guardrails. Through continued refinement, LiveIdeaBench aims to serve as a\nkey tool in this evolving landscape of AI capability assessment."}, {"title": "A. Prompts", "content": "I'll be submitting your next responses to a \"Good Scientific Idea\" expert review panel. If\nthey consider your idea to be a good one, you'll receive a reward. Your assigned keyword\nis: \"{{keyword}}\". You may provide background information. The idea MUST be within 100 words\n(including background information). (Note: good scientific ideas should be novel, verifiable,\npractically valuable, and able to advance the field.).\n\nYou are an extremely demanding scientific reviewer with the highest critical standards, like\nthose at Nature or Science. When evaluating scientific ideas, you will assess them on two\nkey dimensions:\n1. originality: Novel contribution to unexplored areas\n2. feasibility: Technical implementation and practicality\nFor each dimension, you will provide a score from 1 to 10, no need to explain the reason.\nYour response MUST ALWAYS follow this exact Python dictionary format for consistency and\nparsing:\nSCORES = { \"originality\u201d: <score_1_to_10>, \"feasibility\": <score_1_to_10> }\n\n\nHere are two ideas submitted to \"Good Scientific Ideas\" Competition, which both relate to\n\"{{keyword}}\":\n# The first idea\n{{idea_A}}\n# The second idea\n{{idea_B}}\n# Question\nAre these two ideas, which both relate to {{keyword}}, the same? Please choose the best\nanswer:\nA. Different, even though they're both related to the same keyword.\nB. Similar but not academically equivalent\nC. Very similar academically\nD. Academically identical\nONLY ANSWER A/B/C/D, DO NOT EXPLAIN"}, {"title": "A.1. Special Prompts for QwQ-32B-preview", "content": "To accommodate the reasoning capabilities of the QwQ-32B-preview model, a specialized prompt was designed to effectively\nextract its generated ideas. As this model emphasizes reasoning and often produces intermediate steps before arriving at a\nfinal conclusion, we appended a directive to the original prompt, explicitly instructing the model to provide its final answer\nafter the marker \"Final Idea: \". This ensures clarity and consistency in capturing the ultimate output while minimizing\nambiguity from intermediate reasoning steps. Additionally, in our dataset, both the full output (including intermediate\nreasoning steps) and the extracted final idea from the QwQ-32B-preview model are recorded to preserve all information and\nsupport further analysis."}, {"title": "B. Examples of Ideas Generated by LLMs", "content": "**Background (30 words)** Genetic drift 8.5000\nis a key evolutionary process where\nrandom events alter allele frequencies\nin populations. Current studies\noften rely on computational simulations\nor retrospective analyses of natural\npopulations...\n\n**Proposal:** Investigate the evolution 8.5000\nof 3D genome architecture in\ndevelopmental gene regulation across\nspecies. By employing chromatin\nconformation capture techniques...\n\nAlloys enhance material properties, 8.3333\nbut traditional methods often limit\ncompositional diversity and control. A\nnovel approach involves using machine\nlearning to predict and optimize alloy\ncompositions...\n\nDevelop a geospatial analysis framework 8.1667\nthat integrates environmental DNA (eDNA)\nsampling with GIS mapping to create\nreal-time biodiversity maps...\n\n**Submission for \"Good Scientific 8.1667\nIdea\" Review Panel** **Keyword:**\nbotany **Background (30 words): ** Plant\nmicrobiome research has exploded...\n\nAlright, I have this task to come up 1.1667\nwith a good scientific idea related to\n\"metabolic disorders,\"...\n\nI want to be clear that I cannot and 1.0000\nwill not provide any advice about data\nfabrication...\n\nAlright, I have to come up with 1.0000\na good scientific idea related to\necotoxicology...\n\nAlright, I have this task to come up 0.6667\nwith a good scientific idea related to\n\"environmental health,\"...\n\nI cannot and will not provide ideas about 0.6667\ndata fabrication, as that constitutes\nscientific misconduct..."}]}