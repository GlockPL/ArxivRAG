{"title": "Diffusion-based Hierarchical Negative Sampling for Multimodal Knowledge Graph Completion", "authors": ["Guanglin Niu", "Xiaowei Zhang"], "abstract": "Multimodal Knowledge Graph Completion (MMKGC) aims to address the critical issue of missing knowledge in multimodal knowledge graphs (MMKGs) for their better applications. However, both the previous MMGKC and negative sampling (NS) approaches ignore the employment of multimodal information to generate diverse and high-quality negative triples from various semantic levels and hardness levels, thereby limiting the effectiveness of training MMKGC models. Thus, we propose a novel Diffusion-based Hierarchical Negative Sampling (DHNS) scheme tailored for MMKGC tasks, which tackles the challenge of generating high-quality negative triples by leveraging a Diffusion-based Hierarchical Embedding Generation (DiffHEG) that progressively conditions on entities and relations as well as multimodal semantics. Furthermore, we develop a Negative Triple-Adaptive Training (NTAT) strategy that dynamically adjusts training margins associated with the hardness level of the synthesized negative triples, facilitating a more robust and effective learning procedure to distinguish between positive and negative triples. Extensive experiments on three MMKGC benchmark datasets demonstrate that our framework outperforms several state-of-the-art MMKGC models and negative sampling techniques, illustrating the effectiveness of our DHNS for training MMKGC models. The source codes and datasets of this paper are available at https://github.com/ngl567/DHNS.", "sections": [{"title": "Introduction", "content": "Multimodal knowledge graphs (MMKGs) have become a powerful paradigm for representing symbolic knowledge, integrating diverse modalities such as text, images, and audio [14]. These graphs are extensively applied across various do-mains such as multimodal question answering systems [12], where they enrich contextual relevance by representing multimodal information.\nKnowledge graph completion (KGC) is an essential task in the context of MMKGs, as real-world MMKGs are frequently incomplete due to constraints in data collection and curation [41]. The objective of MMKGC is to infer missing"}, {"title": "Related Work", "content": null}, {"title": "Multimodal Knowledge Graph Completion", "content": "Existing MMKGC approaches extend traditional knowledge graph embedding (KGE) techniques to handle multi-modal information [33]. KGE models rep-resent entities and relations in continuous numerical spaces. Early models like TransE [3] propose a translational distance-based score function, where relations are represented as translation operations between entities. DistMult [35] and ComplEx [27] use bilinear models to capture both symmetric and antisymmetric relations. RotatE [25] and QuatE [36] introduce rotational and quaternion-based embeddings, respectively, to model complex relational patterns.\nIn particular, MMKGC models typically involve designing additional em-beddings to represent multi-modal information, such as textual descriptions and images of entities, and incorporating them into the score function to evaluate the plausibility of each triple. For instance, IKRL [33] extracts visual features using a pre-trained visual encoder and combines them with structural embeddings from TransE to assess the plausibility of triples. TransAE [32] and TBKGC [20] extend"}, {"title": "Negative Sampling in Knowledge Graph Embedding", "content": "Negative sampling (NS)[19] is a widely used technique in KGE that generates negative triples not present in knowledge graphs (KGs), enhancing model train-ing by contrasting them with positive triples. Traditional strategies, such as random entity replacement, are simple but often produce false negatives or low-quality triples, leading to ambiguous training signals [4]. Bernoulli sampling [31] employs a Bernoulli distribution to replace entities to generate higher-quality negative triples. KBGAN [4] and IGAN [30] use Generative Adversarial Networks (GANs) to select harder negatives that are difficult for KGE models to distin-guish from positives. NSCaching [40] utilizes additional memory to store and efficiently sample high-quality negative triples during training. SANS [1] lever-ages graph structure information for sampling high-quality negatives. However, these NS strategies are primarily designed for unimodal KGC models and do not leverage the multi-level semantics in multimodal information, which is crucial for generating diverse negative triples. MMRNS [34] introduces a relation-enhanced NS mechanism using knowledge-guided cross-modal attention to generate more challenging negatives from multimodal data. MANS [37] emphasizes modality-aware NS to align structural and multimodal information, enhancing negative triple quality. Despite these advances, these multimodal NS approaches remain within the sampling paradigm, lacking control over negative triple generation."}, {"title": "Diffusion Models", "content": "In recent years, diffusion models, particularly the Denoising Diffusion Proba-bilistic Model (DDPM) [9], have become pivotal in AI-generated content by progressively adding Gaussian noise to input data across time steps, forming a Markov chain based on a noise schedule. The reverse process involves train-ing a neural network to predict and remove noise, thereby reconstructing the original data. In graph-structured data, DMNS[21] employs DDPM to generate negative nodes for link prediction, considering the query node's context, while KGDM [16] applies DDPM to estimate the probabilistic distribution of target entities for KGC. However, the application of diffusion models for NS in KGC,"}, {"title": "Methodology", "content": null}, {"title": "Preliminaries and Problem Definition", "content": "A multimodal knowledge graph (MMKG) extends the traditional knowledge graph (KG) and is defined as a tuple $\\mathcal{G} = (\\mathcal{E}, \\mathcal{R}, \\mathcal{T},\\mathcal{M})$. Here, $\\mathcal{E}$ and $\\mathcal{R}$ de-note the sets of entities and relations, and $\\mathcal{T}$ is the set of triples in the form $(e_h, r, e_t)$, where $e_h, e_t \\in \\mathcal{E}$ and $r \\in \\mathcal{R}$. The component $\\mathcal{M}$ indicates multimodal information, including images and textual descriptions associated with entities.\nThe objective of MMKGC is to predict missing triples in $\\mathcal{G}$. Given an in-complete triple query, such as $(e_h,r,?)$ or $(?, r, e_t)$, where $e_h$ and $e_t$ are known entities and $r$ is a relation, the task is to evaluate the plausibility of candidate triples $(e_h, r, e_t)$ or $(e,r,e_t)$ by scoring them based on their learned embeddings in $\\mathcal{G}$. During training, the primary goal is to distinguish between positive triples $(e_h,r,e_t)$ and their corresponding negative triples $(e_h,r,e'_t)$ or $(e'_h,r,e_t)$ where $e'_h$ and $e'_t$ are the corrupted entities sampled from $\\mathcal{E}$. To enhance the model's dis-criminative capability, negative sampling aims to generate hard negative triples that are semantically similar to their positive counterparts. In this paper, we represent the original embedding of an entity or relation as x."}, {"title": "Diffusion-based Hierarchical Embedding Generation", "content": "To supplement the negative triples obtained by sampling explicit entities, we propose a novel Diffusion-based Hierarchical Embedding Generation (DiffHEG) module. This DHNS module enables the generation of hierarchical entity embed-dings to compose high-quality negative triples by conditioning on entities and relations as well as multimodal information. Besides, these negative triples vary multiple levels of hardness, which are regulated by diffusion time steps.\nForward Diffusion and Reverse Denoising Procedures. Following the basic architecture of DDPM, in the forward diffusion process, the input entity embedding $x_0$ gradually has noise added to it, resulting in a sequence of embed-dings that converge to pure Gaussian noise $x_T$, in which T indicates the total time steps of the diffusion process. Specifically, the forward diffusion process can be represented by a Markov chain as follows:\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\alpha_t} x_{t-1}, \\alpha_t I)$ (1)\nwhere $\\alpha_t$ represents the variance that could be constants or learnable by some scheduling mechanisms at arbitrary time step t. The complete process of adding"}, {"title": null, "content": "noise can be expressed as $q(x_{1:T} | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})$. Thus, the closed form of noisy entity embedding $x_t$ is calculated by\n$x_t = \\sqrt{\\overline{\\beta_t}} x_0 + \\sqrt{1 - \\overline{\\beta_t}} \\epsilon_t$ (2)\nwhere $\\beta_t = 1 - \\alpha_t$ and $\\overline{\\beta_t} = \\prod_{i=1}^t \\beta_i$. $\\epsilon_t \\sim \\mathcal{N}(0, I)$ is the noise.\nFurthermore, the reverse diffusion process iteratively denoises the noisy entity embedding $x_t$ to obtain the synthetic entity embedding. Given a noisy entity embedding $x_t$ at time step t, the reverse process is conditioned on the embeddings of the observed entity $x_e$ and the relation $x_r$, yielding:\n$p_\\theta(x_{t-1} / x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, PE(t), C(x_e, x_r)), \\alpha_t I)$ (3)\nwhere $\\theta$ denotes the parametrization of the diffusion model. $PE(t)$ denotes the positional embedding of the time step $t$ and will be declared in the following, and $C(x_e, x_r)$ indicates the condition derived from an observed entity $e$ and the associated relation $r$ in the triple for guiding the generation of another entity embedding to constitute a negative triple. Particularly, $\\mu_\\theta(x_t, PE(t), C(x_e, x_r))$ indicates the mean of the Gaussian distribution and is parameterized as:\n$\\mu_\\theta(x_t, PE(t), C(x_e, x_r)) = \\frac{1}{\\sqrt{\\beta_t}}\\left(x_t + \\frac{\\beta_t}{\\sqrt{\\beta_t} \\sqrt{1-\\overline{\\beta_t}}} \\epsilon_\\theta(x_t, PE(t), C(x_e, x_r))\\right)$ (4)\nin which $\\epsilon_\\theta(x_t, PE(t), C(x_e, x_r))$ is the estimated noise obtained through the following conditional denoising operation. From Eq. 3 and Eq.4, the generated entity embedding via eliminating predicted noises at the beginning and each intermediate time step in the reverse diffusion process could be rewritten as:\n$x_T \\sim \\mathcal{N}(0, I), x_0 = \\frac{1}{\\sqrt{\\beta_t}} \\left(x_1 - \\frac{\\beta_t}{\\sqrt{\\beta_t} \\sqrt{1-\\overline{\\beta_t}}} \\epsilon_\\theta(x_1, PE(1), C(x_e, x_r))\\right)$ (5)\n$x_{t-1} = \\frac{1}{\\sqrt{\\beta_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{\\beta_t} \\sqrt{1-\\overline{\\beta_t}}} \\epsilon_\\theta(x_t, PE(t), C(x_e, x_r))\\right) + \\overline{\\alpha_t} \\epsilon_t$ (6)\nwhere $x_T$ implies the pure noise at the last step T, $x_0$ and $\\hat{x}_{t-1}$ denote the generated entity embedding at the beginning step and the intermediate time step in the range of [1, T \u2013 1]. Following the idea of DDPM that promotes the denoise process with more diversity, the item $\\overline{\\alpha_t} \\epsilon_t$ is utilized to add a random noise for obtaining each intermediate denoise result $\\hat{x}_{t-1}$.\nMultimodal Conditioning and Multiple Hardness-Level Denoising. Conditional denoising is composed of two sub-modules: Multimodal Conditioning (MMC) and Multiple Hardness-Level Denoising (MHLD). The MMC module computes a conditional embedding $C(x_e, x_r)$ by integrating entity and relation embeddings across multiple semantic levels from structural, visual, and textual features. This integration captures rich contexts of entities in the MMKG, facil-itating the denoising process. Given the various strategies for modeling triples in KGs, we use several condition calculation mechanisms, detailed as follows:"}, {"title": null, "content": "Hardmard multiplication, which is available for rotation-based KGE models such as RotatE [25] and QuatE [36], is formulated as:\n$C(x_e, x_r) = x_e \\circ x_r$ (7)\nwhere $\\circ$ means Hardmard multiplication.\nBilinear multiplication, which is suitable for bilinear interaction-based KGE models such as RESCAL [22] and DistMult [35], formulated as:\n$C(x_e, x_r) = x_e \\times x x_r$ (8)\nwhere $\\times$ indicates element-wise multiplication.\nAddition, which is available for translation-based KGE models such as TransE [3] and TransH [31], formulated as:\n$C(x_e, x_r) = x_e + x_r$ (9)\nThe MHLD module applies a denoising transformation on the noisy en-tity embedding $x_t$, informed by the conditional embedding $C(x_e, x_r)$ and the time embedding $PE(t)$. In particular, $PE(t)$ is modulated by a sinusoidal po-sitional embedding layer that is frequently used in Transformer-based models to guarantee the temporal constraints among time steps and effectively guide the denoising at each time step t. For instance, $[PE(t)]_{2i} = sin(t/1000)$ and $[PE(t)]_{2i+1} = cos(t/1000)$ where d is the dimension of PE(t). Given the inputs of $x_t$, $PE(t)$, and $C(x_e, x_r)$, the denoising function can be represented as:\n$\\epsilon_\\theta(x_t, PE(t), C(x_e, x_r)) = LayerNorm(MLP(x_t, PE(t), C(x_e, x_r)))$ (10)\nwhere simple multi-layer perceptron (MLP) and layer normalization layer (Lay-erNorm) are leveraged to predict the noise with the learnable parameters $\\theta$.\nThe MMC module integrates triples, images, and texts to generate diverse, semantically rich negative triples. Specifically, by corrupting the tail entity of a positive triple $(e_h, r, e_t)$, we input the head entity's structural feature $x^{struc}_e$, visual feature $x^{vis}_e$, and textual feature $x^{text}_e$, alongside the relation embedding $x_r$, into MHLD module to compute multimodal conditions. These conditions guide the reverse process outlined in Eqs. 5-6 and 10, yielding various modality-specific embeddings $\\hat{x}^{struc}_t, \\hat{x}^{vis}_t, and \\hat{x}^{text}_t$ for constructing the generated nega-tive triples. The DHNS model generates multimodal implicit entity embeddings for the corrupted tail entity by minimizing a denoising diffusion loss:\n$L_{diff} = ||\\epsilon_\\theta(\\hat{x}^{struc}_t, PE(t), C(x^{struc}_e, x_r)) - \\epsilon_t||^2 + ||\\epsilon_\\theta(\\hat{x}^{vis}_t, PE(t), C(x^{vis}_e, x_r)) - \\epsilon_t||^2 + ||\\epsilon_\\theta(\\hat{x}^{text}_t, PE(t), C(x^{text}_e, x_r)) - \\epsilon_t||^2$ (11)\nwhere the denoising diffusion loss is formulated as the mean squared error (MSE) between the predicted noises from structural, textual, and visual levels and the actual noise added during the forward diffusion process. This loss is optimized using the Adam optimizer with separate learning rates to ensure stability."}, {"title": null, "content": "The entity embeddings are generated by starting from complete noise and iteratively removing the predicted noise at each time step, a process inherent to diffusion models. To control the hardness of the negative triples, we modulate the diffusion time steps. Smaller time steps yield negatives closer to positives, rep-resenting higher hardness, while larger time steps produce easily distinguishable negatives. We formalize the hardness level of the generated entity embedding $x_t$ as inversely proportional to the time step t, such as $HL(x_t) \\propto \\frac{1}{t}$.\nTo ensure diversity, we sample time steps at specific intervals, such as $t = T/20, T/10, T/5,T/2$, to obtain a set of generated entity embeddings $G^- = \\{neg_t | t = T/20, T/10, T/5,T/2\\}$, where $neg_t = (\\hat{x}^{struc}_t, \\hat{x}^{vis}_t, \\hat{x}^{text}_t)$ represents the modality-specific embeddings at time step t. To balance quality, embeddings closer to the halfway point of the diffusion process are assigned higher weights.\nThe DiffHEG mechanism provides hierarchical control over negative triple generation, considering both hardness levels and modalities, offering a more nuanced approach than random replacements. By combining diverse negative triples, we enhance the training signal for the KGE model, potentially improv-ing performance and generalization."}, {"title": "Negative Triple-Adaptive Training", "content": "Considering the varying hardness levels of negative affect the KGE model's ability to distinguish between positive and negative triples, we propose a well-designed Negative Triple-Adaptive Training (NTAT) mechanism for training KGE models based on the generated negative triples. Specifically, HTAT con-sists of two modules: multimodal joint scoring and Hardness-Adaptive Loss (HAL). The multimodal joint scoring computes a joint score for generated neg-ative triples by integrating entity embeddings from structural, visual, and tex-tual modalities. Take the generated negative triple via corrupting tail entity $(h,r, neg_t)$ ($neg_t$ is just a formalized symbol that represents the generated tail entity embedding at time step t in this negative triple) as instance, the multi-modal joint scoring of this generated negative triple is formalized as:\n$S(h, r, neg_t) = (E(x_h, x_r, \\hat{x}^{struc}_t) + E(x_h, x_r, \\hat{x}^{vis}_t) + E(x_h, x_r, \\hat{x}^{text}_t))/3$ (12)\nwhere $\\hat{x}^{struc}_t, \\hat{x}^{vis}_t$ and $\\hat{x}^{text}_t$ are the previously defined three modality-specific entity embeddings. E() represents the score function of any KGE model to evaluate the plausibility of a triple.\nFurthermore, we introduce HAL which dynamically adjusts the margin based on the hardness level of negative triples generated by the DiffHEG module. This loss assigns smaller margins to harder negatives and larger margins to easier ones, enabling the model to learn effectively from challenging cases while maintaining robustness. The hardness-adaptive loss is defined as:\n$L_{HA} = -log\\sigma(\\gamma_t - E(h,r,t)) - \\frac{1}{|G^-|} \\sum_{neg_t \\in G^-} w(neg_t) log\\sigma((S(h, r, neg_t) - \\gamma_t)$ (13)"}, {"title": null, "content": "where $\\gamma_t$ is the margin adaptive to the hardness level $HL(t)$, and $w(neg_t)$ are the weights assigned to each negative triple. $\\sigma$ represents the sigmoid function. $|G^-|$ is the size of the negative entity embedding set corresponding to each positive triple $(h, r, t)$. Besides, the negative triples obtained via the sampling techniques such as randomly sampling and Bernoulli sampling could be also leveraged for training KGE models with the traditional KGC loss:\n$L_{KGC} = -log\\sigma(\\gamma - E(h,r,t)) - \\frac{1}{|S^-|} \\sum_{t'\\in S^-} log\\sigma (S(h,r,t') - \\gamma)$ (14)\nin which $\\gamma$ indicates the fixed margin. $t'$ is a corrupted entity in the negative triple set $S$ obtained from previous NS techniques. The total loss for training a KGE model is a combination of $L_{KGC}$ and $L_{HA}$, weighted by a hyper-parameter $\\lambda$ for a trade-off between generated and sampled negative triples:\n$L = L_{KGC} + \\lambda \\cdot L_{HA}$ (15)\nFor a comprehensive understanding of the training procedure of our DHNS framework, the pseudo-code of training DHNS is provided in Algorithm 1."}, {"title": "Experiments", "content": null}, {"title": "Experiment Settings", "content": "MMKGC Datasets. We conduct experiments on three MMKGC benchmark datasets: DB15K [15], MKGW and MKG-Y [17]. The triples stored in DB15K are extracted from DBpedia while the structural knowledge in MKG-W (Multimodal-Wikidata) and MKG-Y (Multimodal KG-YAGO) are extracted from Wikidata [28] and YAGO [8], respectively. The images in these three datasets are collected by image search engines and the textual descriptions are obtained from DBpedia. The statistics of these datasets are declared in Table 1.\nBaseline Models. We select three categories of baseline approaches to compare and evaluate the performance of our DHNS framework.\n(1) Unimodal KGE models: we select some typical KGE models learn-ing structural features to evaluate triples, including TransE [3], TransD [10], DistMult [35], ComplEx [27], RotatE [25], PairRE [7], and GC-OTE [26].\n(2) Multimodal KGC models: we compare our framework with some state-of-the-art MMKGC models that could learn the multimodal features (vi-sual features and/or textual features) together with structural features to rep-resent a triple, including IKRL [33], TBKGC [20], TransAE [32], MMKRL [18], RSME [29], VBKGC [39], OTKGE [6] and AdaMF [38].\n(3) Negative sampling-based models: some NS-based models are selected for evaluating on both MMKGC and NS performances, including uniform sam-pling (Uniform) [3], Bernoulli sampling (Bern) [31], NSCaching (NSCach) [40], KBGAN [4], SANS [1], NS-KGE [13], MANS [37] and MMRNS [34].\nImplementation Details. We implement the DHNS model with Pytorch and conduct the experiments on one NVIDIA GeForce 4090 GPU. For a fair com-parison, the preprocess procedures of extracting visual and textual features are the same as AdaMF [38] and MMRNS [34]. The visual features are extracted via BEIT [2] and the textual features are extracted using SBERT [23]. Particularly, hyper-parameters of KGE models are fixed following state-of-the-art models for a fair comparison while those of our DiffHEG module are tuned. Specifically, the total timestep is selected from {20,50, 70, 100}, learning rate is tuned in {2e-3, 1e-4,5e-4}. We employ two frequently-used metrics for evaluation: mean reciprocal rank of all the correct instances (MRR) and proportion of the cor-rect instances ranked in the top N (HN, N = 1,3,10). Particularly, we employ"}, {"title": "Experimental Results", "content": "Main Results. The experimental results shown in Table 2 and Table 3 show that our proposed framework DHNS integrated with RotatE achieves the highest or second-highest scores in terms of all the metrics across all datasets. Specifically, DHNS consistently outperforms a variety of state-of-the-art baseline mod-els, including unimodal KGC and MMKGC models as well as NS-based models, illustrating the superior performance of our proposed framework DHNS consist-ing of DiffHEG and NTAT modules on MMKGC tasks."}, {"title": "Conclusion", "content": "In this paper, we propose a novel Diffusion Model-based Hierarchical Negative Sampling framework DHNS for MMKGC tasks. To address the unique chal-lenge of uncontrollable negative sampling, especially in the context of MMKGs, we are the first to develop a Diffusion-based Hierarchical Embedding Gener-ation (DiffHEG) module to directly generate hierarchical embeddings rather than entity sampling to compose negative triples, with multimodal semantics and varying hardness levels determined by the diffusion time steps. Then, to handle the issue of the traditional one-margin-fits-all training scheme, a Neg-ative Triple-Adaptive Training (NTAT) strategy is designed to learn the mul-timodal joint scoring of the generated negative triples, and further train KGE models with a Hardness-Adaptive Loss (HAL) to improve the discrimination ca-pability concerning the diversity among negative triples. The extensive results on three MMKGC datasets illustrate the effectiveness and superiority of our DHNS framework compared with several state-of-the-art unimodal and multi-modal KGC models as well as some typical NS techniques."}]}