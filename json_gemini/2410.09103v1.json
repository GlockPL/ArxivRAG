{"title": "Parameter-Efficient Fine-Tuning via Selective Discrete Cosine Transform", "authors": ["Yixian Shen", "Qi Bi", "Jia-Hong Huang", "Hongyi Zhu", "Anuj Pathania"], "abstract": "In the era of large language models, parameter-efficient fine-tuning (PEFT) has been extensively studied. However, these approaches usually rely on the space domain, which encounters storage challenges especially when handling extensive adaptations or larger models. The frequency domain, in contrast, is more effective in compressing trainable parameters while maintaining the expressive capability. In this paper, we propose a novel Selective Discrete Cosine Transformation (sDCTFT) fine-tuning scheme to push this frontier. Its general idea is to exploit the superior energy compaction and decorrelation properties of DCT to improve both model efficiency and accuracy. Specifically, it projects the weight change from the low-rank adaptation into the discrete cosine space. Then, the weight change is partitioned over different levels of the discrete cosine spectrum, and the most critical frequency components in each partition are selected. Extensive experiments on four benchmark datasets demonstrate the superior accuracy, reduced computational cost, and lower storage requirements of the proposed method over the prior arts. For instance, when performing instruction tuning on the LLaMA3.1-8B model, sDCTFT outperforms LoRA with just 0.05M trainable parameters compared to LoRA's 38.2M, and surpasses FourierFT with 30% less trainable parameters. The source code will be publicly available.", "sections": [{"title": "Introduction", "content": "The rapid expansion of Large Language Models (LLMs) has significantly advanced natural language processing, enabling the development of increasingly sophisticated applications. However, as these models continue to grow in size and complexity, the challenge of efficiently adapting them for specific tasks becomes more pronounced. Fully fine-tuning such large models poses substantial memory and computational challenges. For instance, the state-of-the-art LLaMA 3.1 (Vavekanand and Sam 2024), where fully fine-tuning a model with 70 billion parameters requires approximately 500GB of GPU memory.\nTherefore, parameter-efficient fine-tuning (PEFT) of LLMs is essential for adapting pre-trained models to specific tasks while managing the considerable computational and storage demands inherent in these processes. Various PEFT methods have been developed. These approaches primarily focus on reducing resource requirements without sacrificing model performance. As a result, the GPU memory require-ment of LORA (Hu et al. 2022) and QLORA (Dettmers et al. 2024) has been reduced to 160GB and 48GB, respectively.\nIn general, existing PEFT methods can be categorized into indirect fine-tuning and direct weight-based methods.\nIndirect fine-tuning techniques, such as adapter tuning (He et al. 2021; Pfeiffer et al. 2020; Lin, Madotto, and Fung 2020) and prompt learning (Diao et al. 2022; Lester, Al-Rfou, and Constant 2021; Li and Liang 2021), achieve computational efficiency by optimizing auxiliary components rather than directly modifying the model weights. These methods are particularly useful for scenarios requiring quick adaptation to new tasks with minimal resource overhead. Direct weight-based methods, like LoRA (Hu et al. 2022), modify the model's parameters directly, typically by introducing low-rank approximations to reduce the number of trainable weights. While these approaches have shown significant improvements in efficiency, there remains potential for further reduction in storage and computational demands.\nRecent research starts to explore the frequency domain, which utilize frequency transformations to further compress and optimize model weights, while maintain expressive capability. For example, Gao et al. (Gao et al. 2024) demonstrated that applying the Discrete Fourier Transform (DFT) to LLM weight matrices can significantly reduce the number of trainable parameters by focusing on the most informative spectral components. This suggests that frequency-domain techniques hold substantial promise for improving the efficiency of model fine-tuning. However, DFT is more tailored for the periodic signals (Nagai, Akashi, and Sugino 2020), but the long-range dependencies of the language context can be diverse and far from periodic (Sun et al. 2021; Wen et al. 2023). Therefore, the energy compaction of LLM in the frequency domain remains less satisfactory than intended.\nIn this paper, we introduce a novel fine-tuning approach that leverages the Discrete Cosine Transform (DCT) to address this bottleneck. DCT is particularly well-suited for the non-periodic signals while at the same time demonstrating superior energy compaction and decorrelation properties. It enables a more concentrated representation of critical information using fewer coefficients. Besides, we further partition the frequency domain via the discrete cosine coefficient matrix, so as to select the most informative components that rest in the context. Concretely, the proposed Selective Discrete Cosine Transform Fine-Tuning (sDCTFT) further simplifies the computational process compared to DFT-based method, while maintaining the expressive ability of an LLM. Building on these insights, by systematically selecting the most informative frequency components, compressing the weight updates, and then applying the inverse DCT (iDCT) to revert to the time domain, the proposed sDCTFT achieves significant improvements in computational efficiency and model performance on a variety of benchmarks. It not only reduces redundancy but also enhances the expressive power of fine-tuning, making it a highly effective and efficient tool for optimizing LLMs. As shown in Fig. 1, on the instruction tuning task, our sDCTFT method surpasses LoRA by 0.1 points with just 50K trainable parameters. Moreover, it achieves a score within 0.01 points of Full Fine-tuning while using only 100K parameters. Compared to FourierFT, SD-CTFT delivers better accuracy while utilizing 30% fewer parameters, demonstrating superior parameter efficiency and competitive performance.\nOur contribution can be summarized as follows.\n\u2022 We propose a Selective Discrete Cosine Transform (sD-CTFT) scheme to parameter-effectively fine-tune the large language models.\n\u2022 We propose a novel frequency partitioning and selection strategy. It leverages DCT's energy compaction properties to enhance computational efficiency.\n\u2022 Experiments on a variety of benchmarks demonstrate the superior performance of the proposed sDCTFT compared to existing PEFT methods."}, {"title": "Related Work", "content": "Parameter-Efficient Fine-Tuning has been extensively studied in the demand of efficiently fine-tuning Large Language Models (LLMs). Indirect fine-tuning methods achieve model adaptation without directly modifying the pre-trained weights. Instead, they introduce additional modules or optimize auxiliary components (He et al. 2021; Pfeiffer et al. 2020; Lin, Madotto, and Fung 2020; Liao, Tan, and Monz 2023; Zhao et al. 2024; Houlsby et al. 2019; Re-buffi, Bilen, and Vedaldi 2017; R\u00fcckl\u00e9 et al. 2020; Li and Liang 2021; Lester, Al-Rfou, and Constant 2021; Hambardzumyan, Khachatrian, and May 2021; Liu et al. 2023), offering a computationally efficient way to fine-tune models for specific tasks. By freezing the original model weights and only fine-tuning the adapters, such methods significantly reduce the computational resources while maintain the flexibility to adapt to various tasks (Diao et al. 2022; Lester, Al-Rfou, and Constant 2021; Li and Liang 2021; Aghajanyan, Zettlemoyer, and Gupta 2020). Direct weight-based methods modify the pre-trained weights of LLMs to achieve fine-tuning, often through the introduction of low-rank adaptation (LoRA) that enable efficient learning with fewer parameters (Hu et al. 2022). These changes are then merged with the original weights to maintain inference efficiency, allowing effective fine-tuning without increasing inference latency. Some variations such as AdaLoRA (Zhang et al. 2023), Vera (Kopiczko, Blankevoort, and Asano 2023) and DORA (Liu et al. 2024) have also been recently proposed.\nFrequency-based PEFT is an emerging research line, which harnesses the transformation to the frequency domain to compress the trainable parameters while maintaining the expressive capability. For example, Gao proposed a method that applies the Fourier Transform to fine-tune LLMs by learning a subset of the spectral coefficients of the weight matrices (Gao et al. 2024). This approach reduces the number of parameters required for fine-tuning by focusing on the most informative frequency components.\nDiscrete Cosine Transform has also been effectively applied in various deep learning contexts, demonstrating its potential for model compression and efficiency improvements. For instance, Xu et al. analyzed the spectral bias from a frequency perspective and proposed a learning-based frequency selection method to identify and remove trivial frequency components without loss of accuracy (Xu et al. 2020). Ehrlich et al. leveraged DCT within the context of JPEG-compressed images, redefining convolution and batch normalization with a tunable approximation for ReLU operations (Ehrlich and Davis 2019). Furthermore, Zhu demonstrated that DCT could effectively recover data with very few parameters, highlighting its potential for model compression and efficiency (Zhu et al. 2024; Cheng et al. 2024)."}, {"title": "Methodology", "content": "Fig. 2 gives an overview of the proposed Selective Discrete Cosine Transform fine-tuning (sDCTFT) method. Different from the vanilla LoRA (low-rank adaptation) paradigm, sDCTFT operates in the frequency domain by learning a set of spectral coefficients on the cosine basis instead of directly modifying the spatial weights. The proposed sDCTFT identifies and retains the most critical frequency components by applying the DCT to the pre-trained weights. Then, it selects some spectral coefficients to learn trainable parameters, which form the spectral matrix. Finally, the iDCT is applied to the modified spectral matrix to transform it back to the spatial domain so as to update the weight changes."}, {"title": "Frequency-Domain Transformation Using DCT", "content": "The first step is to transform the weight matrices of the LLM into the frequency domain using the Discrete Cosine Transform (DCT). DCT is particularly advantageous due to its strong energy compaction properties, where the majority of the signal's energy is concentrated in a small number of low-frequency components.\nSpecifically, given a weight matrix W[i, j] of size M \u00d7 N, the 2D DCT is mathematically defined as:\n$W_F[u, v] = \\alpha(u)\\alpha(v) \\sum_{i=0}^{M-1} \\sum_{j=0}^{N-1} W[i, j] \\cos(\\frac{\\pi}{M}(i+\\frac{1}{2})u) \\cos(\\frac{\\pi}{N}(j+\\frac{1}{2})v)$         (1)\nwhere u = 0, 1, . . ., M \u2013 1 and v = 0, 1, . . ., N \u2013 1 represent the frequency indices. The scaling factors \u03b1(u) and \u03b1(v) are defined as:\n$\\alpha(x) = \\begin{cases}\\sqrt{\\frac{1}{M}}, & \\text{if } x = 0 \\\\\\sqrt{\\frac{2}{M}}, & \\text{if } x = 1, 2, ..., M - 1 \\end{cases}$         (2)\nThis transformation converts the spatial (or time-domain) information of the weight matrix into a frequency-domain representation, where each element $W_F[u, v]$ corresponds to a specific frequency component of the original matrix. Low-frequency components (where u and v are small) typically contain the most significant information, making them prime candidates for focused fine-tuning and significantly reducing the number of parameters that need to be fine-tuned."}, {"title": "Frequency Partitioning and Selection", "content": "The second step systematically partitions the frequency spectrum into three groups of distinct regions, namely, low, mid, and high frequencies. Each of these regions captures different aspects of the model's behavior, with low frequencies generally representing more global structures and high frequencies capturing finer details."}, {"title": "Distance-Based Frequency Partitioning", "content": "We define the frequency domain as a 2D grid where each point (u, v) corresponds to a specific frequency component in the Discrete Cosine Transform (DCT) of the weight matrix. The distance of each point from the origin (which represents the lowest frequency) is given by:\n$d(u, v) = \\sqrt{u^2 + v^2}$,         (3)\nwhere u and v are the frequency indices corresponding to the horizontal and vertical directions in the DCT matrix, respectively. The maximum distance from the origin is:\n$d_{max} = \\sqrt{(\\frac{M}{2})^2 + (\\frac{N}{2})^2}$        (4)\nwhere M and N are the dimensions of the DCT matrix.\nThe frequency domain is partitioned into three distinct regions: low, medium, and high frequencies. These regions are defined based on their distance from the origin:\n\u2022 Low-Frequency Components ($M_{low}$): These are frequencies close to the origin and are defined as:\n$M_{low} = \\{(u, v) : d(u, v) \\leq \\frac{d_{max}}{3}\\}$        (5)\n$M_{low}$ capture broad, global patterns and contain the majority of the energy due to the DCT's energy compaction property.\n\u2022 Medium-Frequency Components ($M_{mid}$): These frequencies are located at an intermediate distance from the origin and are defined as:\n$M_{mid} = \\{(u, v) : \\frac{d_{max}}{3} < d(u, v) < \\frac{2 \\cdot d_{max}}{3}\\}$      (6)\n$M_{mid}$ capture finer details and are crucial for representing medium-scale structures in the data.\n\u2022 High-Frequency Components ($M_{high}$): These are frequencies farthest from the origin, defined as:\n$M_{high} = \\{(u, v) : d(u, v) > \\frac{2 \\cdot d_{max}}{3}\\}$       (7)\n$M_{high}$ captures detailed features, often including noise, but is crucial for tasks requiring fine resolution."}, {"title": "Inverse DCT and Weight Updates", "content": "Once the most informative frequency components have been selected, the final step is to update the weights in the frequency domain and then transform them back to the space domain using the inverse Discrete Cosine Transform (iDCT). The updated weight matrix $AW$ is obtained via:\n$\\Delta W_T = IDCT(\\Delta W_F)$,         (9)\nwhere $AW_F$ represents the modified frequency-domain weights, and iDCT is mathematically defined as:\n$W[i, j] = \\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} \\alpha(u) \\alpha(v) W_F[u, v] \\cos(\\frac{\\pi}{M}(i+\\frac{1}{2})u) \\cos(\\frac{\\pi}{N}(j+\\frac{1}{2})v)$       (10)\nNote that only the selected frequency components in $AW_F$ are updated. The transformation back to the spatial domain integrates this fine-tuned information, preparing the model for inference or further training."}, {"title": "Memory Efficiency Analysis", "content": "We quantify the parameter efficiency and memory requirements of sDCTFT compared to LoRA across various base models. The number of trainable parameters in sDCTFT is determined by || = n \u00d7 L, where n is the number of selected frequencies and L is the number of fine-tuned layers. In contrast, LoRA's parameter count is || = r \u00d7 ($d_1$+$d_2$)\u00d7 L, where $d_1$ and $d_2$ represent the dimensions of each layer, and r is the rank used in LoRA.\nThe results are presented in Tab. 1. The proposed sDCTFT requires significantly fewer trainable parameters and much less memory to store weight updates compared to LoRA. For example, in RoBERTa, sDCTFT with n = 200 requires only 4.8K parameters and 18.8KB of memory, whereas LoRA with r = 4 needs 147K parameters and 574KB. The trend holds consistently across larger models, such as LLaMA-2 13B, where sDCTFT uses just 56K parameters compared to LORA's 13.1M parameters.\nThe memory advantage of sDCTFT becomes even more pronounced in larger models, such as ViT Large, where sDCTFT requires only 100.9K parameters (394.1KB) for n = 2400, while LoRA needs 786K parameters (2.93MB) for r = 8. This dramatic reduction in both parameter count and memory footprint allows sDCTFT to be a highly scalable and efficient solution, making it suitable for resource-constrained environments."}, {"title": "Experiments", "content": "Baselines\nThe proposed sDCTFT is compared with:\n\u2022 Full Fine-Tuning (FF): The entire model is fine-tuned, updating all parameters.\n\u2022 Adapter tuning (Houlsby et al. 2019; Lin, Madotto, and Fung 2020; R\u00fcckl\u00e9 et al. 2020; Pfeiffer et al."}, {"title": "Natural Language Understanding", "content": "Models and Datasets. We evaluate sDCTFT on the GLUE benchmark (Wang et al. 2019) using ROBERTa (Liu et al. 2019) models in both Base and Large configurations. The GLUE benchmark encompasses a diverse set of NLU tasks, providing a comprehensive assessment.\nImplementation Details. The proposed SDCTFT method is configured to use 700 out of the available 7682 spectral coefficients for ROBERTa Base and 10242 for RoBERTa Large, ensuring that each layer retains the most critical spectral components. This selection of spectral coefficients is consistent across all layers. We adhere to the same experimental settings as LoRA to maintain comparability across methods. Additional hyperparameter settings and details are provided in the supplementary material.\nResults and Analysis The results are presented in Tab. 2, where we report the median performance across 5 random seed runs, with the best epoch selected for each run. The proposed SDCTFT consistently delivers superior or comparable performance to baseline methods while significantly reducing the number of trainable parameters. For instance, SDCTFT achieved an 80.7% accuracy on RTE and a 92.0 Pearson correlation on STS-B, outperforming methods like LORA and FourierFT with significantly fewer parameters. On SST-2, SDCTFT maintained a high accuracy of 96.2%, matching or slightly outperforming other approaches. Additionally, it recorded a 94.5% accuracy on QNLI, further highlighting its effectiveness. Across all tasks, SDCTFT shows robust generalization while requiring minimal training parameters and memory requirements."}, {"title": "Natural Language Generation", "content": "Models and Datasets. We evaluate SDCTFT on the E2E natural language generation (NLG) task (Novikova, Du\u0161ek, and Rieser 2017) by fine-tuning GPT-2 (Medium and Large) (Radford et al. 2019) models, which are decoder-only architectures with 24 and 36 transformer blocks.\nImplementation Details. We fine-tune LoRA, DORA, FourierFT, VERA, and the proposed sDCTFT method on the GPT-2 Medium and Large models using a linear learning rate scheduler over 5 epochs. Results are averaged over 3 runs, and detailed hyperparameters are in the Appendix.\nResults and Analysis. Tab. 3 shows that sDCTFT consistently outperforms other methods across most metrics while requiring significantly fewer trainable parameters. Specifically, sDCTFT reduces trainable parameters by 9.59% and 6.6% compared to LoRA for GPT-2 Medium and Large models, respectively, and achieves a 30% parameter reduction compared to FourierFT."}, {"title": "Ablation Study", "content": "We explore the relationship between parameter number and model performance between the proposed sDCTFT and existing methods (LoRA, FourierFT). For LoRA, we evaluate ranks r = {1,2, 4, 6, 8, 16}. For FourierFT and sDCTFT, we evaluate n = {50, 100, 200, 1000, 6144, 12288} spectral coefficients. Experiments are conducted on 6 GLUE tasks.\nParameter Scalability. Fig. 3 demonstrates that simply increasing the number of trainable parameters does not always yield performance gains for LoRA. On tasks such as MRPC, COLA, and RTE, FourierFT achieves competitive or even better performance than LoRA with significantly fewer parameters. Notably, sDCTFT consistently outperforms FourierFT and LoRA as n increases, showcasing the efficiency of our frequency-domain selection strategy.\nA statistical analysis using the Student t-test confirms that SDCTFT significantly outperforms FourierFT across most tasks: RTE (p=0.0097, t=4.06), MRPC (p=0.0405, t=2.75), SST-2 (p=0.0272, t=3.09), QNLI (p=0.0027, t=5.51), and COLA (p=0.0093, t=4.10). While STS-B shows a smaller advantage (p=0.0348, t=2.87), sDCTFT consistently demonstrates superior performance across benchmarks, reinforcing its effectiveness as n increases and its robustness in parameter-efficient scenarios.\nInformed Frequency Selection v.s. Random Sampling. We further compare the proposed SDCTFT with the scenario where the frequency points are selected randomly (denoted as rDCTFT). The results illustrate that SDCTFT, which uses a systematic partitioning and hybrid selection strategy, consistently outperforms rDCTFT. This is statistically confirmed by the Student t-test, where significant improvements are observed in CoLA (p=0.0260, t=3.13) and SST-2 (p=0.0050, t=4.77), highlighting the advantage of informed frequency selection. Even in tasks with smaller gaps like MRPC (p=0.0380, t=2.80) and QNLI (p=0.0165, t=3.54), sDCTFT demonstrates clear benefits. Although the STS-B task shows a less pronounced difference (p=0.117, t=1.89), the overall performance trend strongly favors sDCTFT, especially in tasks where precise frequency selection is crucial for effective fine-tuning.\nEnergy Ratio Ablation Study Fig. 4 shows the energy ratio ablation study across 6 GLUE tasks with performance normalized to d = 0.7. The results indicate that d = 0.7 consistently yields stable performance across all tasks. Setting d too low (e.g., 0.5 or 0.6) reduces performance, particularly in CoLA and QNLI, where relative performance drops by up to 1%. Conversely, larger values (e.g., 0.8 or 0.9) also lead to performance declines in tasks like QNLI and RTE."}, {"title": "Conclusion", "content": "We proposed a novel Selective Discrete Cosine Transform based Fine-Tuning (sDCTFT) method for large language models. By leveraging the energy compaction properties of DCT, our method effectively reduces the number of trainable parameters while maintaining or improving model performance. Extensive experiments on multiple benchmarks demonstrate that sDCTFT outperforms existing state-of-the-art methods in terms of accuracy, computational efficiency, and storage requirements. These results highlight the practical value of sDCTFT in enabling scalable and resource-efficient fine-tuning across a wide range of applications."}, {"title": "Instruction Tuning", "content": "Models and Datasets. We employ sDCTFT and baseline methods to fine-tune LLaMA2-7B, LLaMA2-13B, and LLaMA3.1-8 using the Alpaca dataset (Taori et al. 2023). For evaluation, we generate responses to pre-defined questions from the MT-Bench (Zheng et al. 2024) and Vicuna Eval datasets. The generated responses are then scored by GPT-4, with evaluations based on a 10-point scale.\nImplementation Details. Following the prior setting (Dettmers et al. 2024, 2022), our fine-tuned methods, namely, LoRA, DORA, and VeRA, are applied to all linear layers except the top one. For FourierFT, we follow the settings in (Gao et al. 2024). For sDCTFT, we set n to 700. To train on a single GPU, we utilize the quantization technique from QLora (Dettmers et al. 2024). All methods are trained for one epoch, and we report the average scores of all answers. Hyperparameter details are in the Appendix.\nResults and Analysis. Tab. 4 show that the sDCTFT consistently outperforms or matches baseline methods across dif-"}, {"title": "Image Classification", "content": "Models and Datasets. We evaluate our method on the Vision Transformer (ViT) (Dosovitskiy et al. 2020) in both Base and Large variants. The image classification datasets include CIFAR-100 (Krause et al. 2013), DTD (Cimpoi et al. 2014), EuroSAT (Helber et al. 2019), and Oxford-Pets (Parkhi et al. 2012).\nImplementation Details. We evaluate LoRA, DORA and VeRA and other 6 baselines by applying them to the query and value layers of ViT. Only training the classification head is denoted as referred \"Head\". We set r = 16 for LORA, n = 3000 for FourierFT and n = 2400 for SDCTFT. Learning rates and weight decay are tuned for all methods, with training capped at 10 epochs. Hyperparameter details are provided in the Appendix.\nResults and Analysis. Tab. 5 presents the performance on ViT-B and ViT-L across four image classification datasets. For the ViT-B model, sDCTFT demonstrates competitive performance with only 50.4K trainable parameters, while LoRA and DORA require over 10 times more parameters. Specifically, sDCTFT matches the full fine-tuning performance on EuroSAT and OxfordPets, achieving 99.1% and 93.4% accuracy, respectively. For the ViT-L model, sDCTFT achieves near-optimal performance with only 100.9K parameters. SDCTFT records the best result on DTD at 81.9%, while matching the top accuracy on OxfordPets at 94.8%. These results underscore sDCTFT's capacity to deliver high accuracy with minimal trainable parameters, proving its efficiency in resource-limited settings."}, {"title": "Additional Experimental Results", "content": "Expressive Capability: A Comparison of sDCTFT, FourierFT, and LoRA\nTo intuitively evaluate the expressive power of our method, we design a simple classification task with a synthetic dataset to simulate a scenario where LoRA encounters performance bottlenecks. Specifically, we specify a 2D center point for each class of data in the 8 classes and randomly add Gaussian noise based on those points to obtain the 2D"}, {"title": "Additional Ablation Study", "content": "This ablation study further examines the performance of sDCTFT when trained for two epochs, with results shown in Table 11. Even with extended training, our method consistently surpasses other approaches in both efficiency and performance. For instance, in the LLaMA2-7B model, sDCTFT achieves an MT-Bench score of 5.27 and a Vicuna score of 7.51 using only 0.045M parameters. In comparison, FourierFT, which uses more parameters (0.064M), falls short in performance. Similarly, on the LLaMA2-13B model, sDCTFT outperforms all other methods, recording the highest scores of 5.97 on MT-Bench and 8.01 on Vicuna with only 0.056M trainable parameters, compared to FourierFT's 0.08M. These results, highlighted in Table 11, clearly indicate that SDCTFT remains robust and effective, solidifying its advantage in limited computational resources."}, {"title": "Examples of Instruction Tuning", "content": "This section highlights practical examples generated by the LLaMA3.1-8B model fine-tuned with FourierFT and SDCTFT, illustrating the comparative effectiveness of each method. FourierFT uses 146K trainable parameters, while SDCTFT requires only 100K. Each case includes a prompt, the answers generated by both models, and a review of the responses assessed by GPT-4.\nThis section presents three case studies comparing models fine-tuned with FourierFT and sDCTFT. In a restaurant review, sDCTFT provided more detailed and engaging insights, while FourierFT was concise but less descriptive (Tab. 12). In coding instructions for finding common elements in arrays, both models were correct, but sDCTFT's explanation was more comprehensive and user-friendly (Tab. 13). Lastly, in film analysis, sDCTFT offered a deeper, more nuanced critique of plot, character development, and directorial choices, outperforming FourierFT's straightforward evaluation (Tab. 14)."}, {"title": "Details of Datasets", "content": "GLUE Benchmark. The General Language Understanding Evaluation (GLUE) benchmark(Wang et al. 2019) is a comprehensive platform designed to evaluate and accelerate progress in natural language understanding (NLU). The benchmark comprises nine tasks that span a variety of NLU challenges such as sentiment analysis, paraphrase detection, linguistic acceptability, natural language inference, and textual similarity. Among these tasks are the Stanford Sentiment Treebank (SST-2), which focuses on binary sentiment classification for movie reviews, and the Microsoft Research Paraphrase Corpus (MRPC), which involves detecting whether two sentences are semantically equivalent. The Corpus of Linguistic Acceptability (CoLA) evaluates a model's ability to distinguish grammatically correct sentences from incorrect ones, reflecting linguistic competence in terms of syntactic judgments. The benchmark's diversity and inclusion of tasks with limited training data encourage the development of models that generalize well across multiple language tasks and genres.\nIn addition to single-sentence classification tasks, GLUE includes several sentence-pair tasks. The Question Natural Language Inference (QNLI) task is derived from the Stanford Question Answering Dataset (SQUAD). It requires models to determine if a given context sentence contains the answer to a corresponding question. The Recognizing Textual Entailment (RTE) task combines several textual entailment datasets from various domains like news and Wikipedia, testing whether a hypothesis can be logically inferred from a premise. The Semantic Textual Similarity Benchmark (STS-B) measures the similarity between sentence pairs using a regression-based approach, where models predict similarity scores on a continuous scale.\nE2E Benchmark. The E2E dataset (Novikova, Du\u0161ek, and Rieser 2017) is designed for training and evaluating end-to-end data-driven natural language generation (NLG) systems within the restaurant domain. It consists of over 50,000 instances and is known for its linguistic complexity, including greater lexical diversity, syntactic variation, and discourse phenomena compared to previous datasets. The evaluation is primarily conducted using five metrics: BLEU, NIST, METEOR, ROUGE-L, and CIDEr. BLEU measures the overlap of n-grams between the generated text and human references, emphasizing precision. METEOR considers synonymy and stemming, providing a more nuanced assessment of text similarity. ROUGE-L focuses on the longest common subsequence to evaluate fluency and structure. CIDEr captures consensus by weighting n-grams based on their relevance in human references, offering a comprehensive measure of output quality.\nInstruction Tuning Related Benchmarks The Alpaca dataset (Taori et al. 2023) is a collection of 51K instruction-following examples generated using OpenAI's text-davinci-003. It was created to fine-tune Meta's LLaMA 7B model into a lightweight, instruction-following model named Alpaca. The dataset covers a wide range of tasks, including question-answering, summarization, and classification, enabling the fine-tuned model to exhibit behavior similar to much larger models at a fraction of the cost. A specific example is as follows:\n{\n\"instructions\": Convert the\nfollowing temperature from Celsius\nto Fahrenheit.\n}\n\"input\": 25 \u00b0C\n\"output\": 25 \u00b0C is equal to 77\u00b0F.\nMT-Bench (Zheng et al. 2024) is a recently introduced benchmark designed to evaluate the instruction-following capabilities of language foundation models. It consists of a series of open-ended questions aimed at assessing model performance across diverse aspects such as writing, role-play, reasoning, mathematics, coding, information extraction, STEM, and the humanities. MT-Bench effectively distinguishes these abilities through tailored questions, providing a more comprehensive evaluation. A specific example from the benchmark is provided below.\n{\n\"Q1\": What is the square root of\n144?\n\"Q2(follow-up)\": If you multiply\nthe answer by 2, what is the result?\n\"Solution\": Q1: The square root is\n12. Q2: The result is 24.\nVicuna Eval (Chiang et al. 2023) is a benchmark designed to assess the alignment of large language models (LLMs) with human preferences and serves as the predecessor to MT-Bench. Vicuna Eval evaluates models across a diverse set of topics, including coding, writing, mathematics, counterfactual reasoning, Fermi estimation, common sense, role-play, knowledge, and generic tasks. It provides a comprehensive framework for understanding how well models align with human expectations in varied scenarios. A specific example from this evaluation is presented below.\n{\n\"question\": Explain the concept of\nrecursion with a simple example.\n\"category\": programming.\n}"}, {"title": "Hyperparamaters", "content": "Hyperparameters on GLUE benchmarks Tab. 7 describes the key hyperparameters used in the experiments across different GLUE tasks and model sizes (Base and Large). The table outlines the learning rate schedules, optimizer settings, warmup ratios, and seed values for reproducibility. For both Base and Large models, the AdamW optimizer is used with a linear learning rate schedule and a warmup ratio of 0.06. The frequency bias is set to false, and the frequency coefficient n is fixed at 700 for SDCTFT unless specified otherwise. Each experiment is run with 5 different seeds {0, 11111, 22222, 33333, 44444}.\nFor the Base models, the number of training epochs ranges from 30 to 100 depending on the task, with SST-2 requiring the longest training time. The FourierFT and sDCTFT methods use a higher learning rate for the base models compared to the learning rate used for fine-tuning the head layers.\nIn contrast, the Large models typically require fewer epochs but use slightly lower learning rates. The batch size remains consistent across both model sizes, set at 32 for all tasks. Additionally, max sequence lengths are adapted to fit the needs of each task, with longer sequences allocated for more complex tasks like CoLA and QNLI.\nHyperparameter settings on E2E benchmark Tab. 8 outlines the hyperparameter configurations used for the medium and large models on the E2E benchmark. Both models are optimized using AdamW with a linear learning rate schedule. The learning rates for SDCTFT and FourierFT are set to 2E - 2 for the medium model and 5E - 2 for the large model, while the head layers have lower learning rates of 2E-4 and 1E \u2013"}]}