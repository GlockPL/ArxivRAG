{"title": "TA-CLEANER: A FINE-GRAINED TEXT ALIGNMENT BACKDOOR\nDEFENSE STRATEGY FOR MULTIMODAL CONTRASTIVE\nLEARNING", "authors": ["Yuan Xun", "Siyuan Liang", "Xiaojun Jia", "Xinwei Liu", "Xiaochun Cao"], "abstract": "Pre-trained large models for multimodal contrastive learning, such as CLIP, have been widely\nrecognized in the industry as highly susceptible to data-poisoned backdoor attacks. This poses\nsignificant risks to downstream model training. In response to such potential threats, finetuning offers\na simpler and more efficient defense choice compared to retraining large models with augmented\ndata. In the supervised learning domain, fine-tuning defense strategies can achieve excellent defense\nperformance. However, in the unsupervised and semi-supervised domain, we find that when CLIP\nfaces some complex attack techniques, the existing fine-tuning defense strategy, CleanCLIP, has some\nlimitations on defense performance. The synonym substitution of its text-augmentation is insufficient\nto enhance the text feature space. To compensate for this weakness, we improve it by proposing\na fine-grained Text Alignment Cleaner (TA-Cleaner) to cut off feature connections of backdoor\ntriggers. We randomly select a few samples for positive and negative subtext generation at each\nepoch of CleanCLIP, and align the subtexts to the images to strengthen the text self-supervision. We\nevaluate the effectiveness of our TA-Cleaner against six attack algorithms and conduct comprehensive\nzero-shot classification tests on ImageNet1K. Our experimental results demonstrate that TA-Cleaner\nachieves state-of-the-art defensiveness among finetuning-based defense techniques. Even when faced\nwith the novel attack technique BadCLIP, our TA-Cleaner outperforms CleanCLIP by reducing the\nASR of Top-1 and Top-10 by 52.02% and 63.88%, respectively.", "sections": [{"title": "Introduction", "content": "In the field of artificial intelligence, contrastive learning serves as a powerful learning paradigm aimed at comparing\ndifferent representations of data, thereby bringing similar samples closer together in the embedding space while pushing\ndissimilar samples further apart Chen et al. [2020], Khosla et al. [2020], Gutmann and Hyv\u00e4rinen [2010]. This learning\nmethod has achieved significant success in many domains, particularly in natural language processing Gao et al. [2021],\nChen et al. [2022] and computer vision Bi et al. [2022], Park et al. [2020]. In addition to its application in single-modal\ndata, recent works have extended contrastive learning to multimodal data Zhang et al. [2023], Singh et al. [2023],\nYang et al. [2022], training on a vast scale of image-text pairs from the web to achieve joint feature representation\nand matching between images and text. Pre-trained large models, like CLIP Radford et al. [2021b], ALIGN Chen\net al. [2021b], and BASIC Chen et al. [2021a], have learned universal representations from large-scale unlabeled\ndata and performed exceptionally well even without task-specific data, as demonstrated by their impressive zero-shot\nclassification performance on ImageNet Deng et al. [2009]. By fine-tuning these models on specific tasks with a small\namount of labeled training samples, high-performance vertical domain applications can be realized quickly."}, {"title": "Related work and preliminaries", "content": "Contrastive Language-Image Pre-Training (CLIP) CLIP Radford et al. [2021b], released by OpenAI, stands as a\nprominent representative of MCL. Inspired by mapping images and texts into a shared feature embedding space Rd,\nCLIP enables the model to understand the semantic relationship between them. CLIP involves two encoders: an image\nencoder f1 : I \u2192 Rd and a text encoder fr : T \u2192 Rd, which transform the image and text data into representations of\ndimension d. The model is pre-trained through contrastive learning, leveraging vast amounts of internet image-text\npairs {Ii, Ti}1 to learn the associations between images and texts. During training, the CLIP model learns a mapping\nfunction that projects images and texts into the same feature space. This is achieved by maximizing the similarity\nbetween positive pairs (matching images I\u2081 and texts T\u2081) while minimizing the similarity between negative pairs\n(mismatched images and texts). This unsupervised joint learning approach enables the CLIP model to achieve superior\nperformance on various visual and language tasks, including image classification, text caption generation, and image\nretrieval. The mathematical expression for lossClip can be found in the Appendix B.\nBackdoor attacks Backdoor attacks generally refer to the implantation of specific trigger patterns during the model\ntraining process, which enables the model to perform normally under normal conditions but exhibit abnormal behavior\nunder specific conditions, such as when the input contains images with trigger patterns. In the domain of supervised\nlearning, backdoor attacks have garnered significant attention, with notable works including BadNet Gu et al. [2017],\nBlended Chen et al. [2017], SIG Liu et al. [2020], WaNet Nguyen and Tran [2021], and SSBA Li et al. [2021b]].\nBackdoor attacks targeting the CLIP model primarily leverage its capability in learning from multimodal data. Attackers\ncan add image-text pairs containing specific trigger patterns to the training data, allowing the model to learn the\nassociation between these trigger patterns and abnormal behaviors. Within the domain of MCL, Carlini and Terzis\n[2021] pioneered the revelation of its vulnerability to backdoor attacks, demonstrating a successful attack on CLIP,\nfor instance, by poisoning merely 0.01% of the data. Concurrently, Yang et al. [2023c] delved into the impact of\nattacks from different modalities on MCL. Additionally, research on attacks against self-supervised learning (SSL), a\nbroader category, is also ongoing, exemplified by BadEncoder Jia et al. [2022], GhostEncoder Wang et al. [2024], and\ndistribution-preserving attacks Tao et al. [2023]. The details about data-poisoning backdoor attacks on CLIP are shown\nin Appendix A.\nBackdoor Defenses on CLIP To address these threats mentioned above, some researchers have borrowed backdoor\ndefense techniques from supervised learning Zhu et al. [2023, 2024] to mitigate the backdoor effects in MCL models.\nCurrently, defense techniques for MCL can be categorized into two groups based on whether the defender can access the\npoisoned dataset: \u2460 defenders can access the entire poisoned dataset Yang et al. [2023b, 2024b], Bansal et al. [2023];\n\u2461 defenders can only access the poisoned model Bansal et al. [2023]. The former approach, which allows for complete\nretraining of large models with various data augmentation strategies, can achieve strong defense performance, such as\nROCLIP Yang et al. [2024b]. However, in reality, the feasibility of attackers manipulating the training set is low, as they\ncannot guarantee that their carefully crafted poisoned data will be incorporated into large-scale training sets. Therefore,\na more realistic attack strategy is to perform low-cost fine-tuning of existing pre-trained large models with dirty data. As\na result, defense techniques targeting the fine-tuning phase are necessary, which is the attack-defense scenario addressed\nin this paper. A representative example of such defenses is CleanCLIP Bansal et al. [2023]. Specifically, CleanCLIP\nintroduces a self-supervised loss based on multimodal data augmentation, which fine-tunes a clean dataset to reduce\nthe impact of backdoor models. Their self-supervised loss lossss and total fine-tuning loss lossCClip can be found in\nAppendix B."}, {"title": "Methodology", "content": "3.1 Threat model\nAdversary Objective: The primary objective of the adversary is to manipulate CLIP's textual output representation. By\npolluting the original dataset, the model can generate malicious adversarial text specified by the adversary for any input\nimage embedded with a trigger. During zero-shot testing, the attack objective manifests as poisoned images will be\nmisclassified as the adversarial category, while other benign images will be correctly classified.\nAdversary Capability: We assume the attacker possesses knowledge of the model's structure, training algorithm, and\nthe hyper-parameters used by the victim, but they can't directly modify the training process. While the attacker lacks\naccess to the entire dataset, they can inject a small number of poisoned samples into the training dataset. Furthermore,\nthe attacker can poison pre-trained MCL models by fine-tuning with carefully crafted dirty datasets and distributing\nthem through various channels on the internet, thereby creating uncontrollable risks for downstream tasks.\n3.2 Fine-grained text augmentation\nTo address the weakness of CleanCLIP in facing strongly poisoned triggers, particularly in the context of lower\nperformance in text-modal self-supervision, we propose a Fine-Grained Text Augmentation strategy based on positive\nand negative sub-captions, as illustrated in the left part of Figure 2. Our text augmentation strategy consists of two\nmain parts: positive sub-caption generation and negative sub-caption generation. Assuming the image-text dataset used\nby CLIP finetuning is Dft, we annotate each sample as (Ii, Ti) \u2208 Dft, where I\u00bf is the image and T\u2081 is its associated\ncaption. And we generate [ST, STh] for each T\u2081.\nPositive sub-caption generation For each textual data T\u2081, we decompose it by first extracting the relational verbs\ninvolved. We select one of these verbs as the central word for the positive sub-text ST. If the subject and object have\ncorresponding adjectives, we randomly remove or retain them. With the help of SceneGraphParser \u2171, we generate the\npositive sub-caption using the following template, ensuring the correct positioning of subjects and objects.\n< (Adjective of the subject ) + Subject + Relational Verb + (Adjective of the object ) + Object >\nWhen the original text is just a phrase or can't be further streamlined, such as \"a picture of an apple\", then we only keep\nthe core word \"apple\" as our positive sub-text.\nNegative sub-caption generation After extracting the sub-captions, we perform various forms of replacement oper-\nations on them. Since we do not know whether the malicious adversary's attack target is a specific entity, attribute,\nrelationship, or an entire line of text, our replacements need to consider all elements in the text comprehensively. There\nare three types of sub-text replacement operations, and the replacement method randomly selects one of them: \u2460\nReplace the adjectives of the subject and object. If the adjective is missing, skip this step and perform other types"}, {"title": "TA-Cleaner: Pos/Neg sub-texts finetuning", "content": "Previous defense efforts have focused on countering backdoor triggers by augmenting image and text self-supervised\nlearning. However, through our prior analysis, we found that the text self-supervision strength of CleanCLIP is\ninsufficient to withstand triggers carefully optimized in the feature space, unless sacrificing the expression capability of\nclean samples. Therefore, building upon this, we reinforced the text augmentation method by finely optimizing the\nfeature vectors of text through alternating optimization between self-supervised learning and positive-negative sample\nadversarial learning, enhancing CLIP's robustness against image backdoor triggers.\nIn the game-based finetuning between positive and negative samples, we do not perform fine-grained augmentation on all\ntexts, as this would disrupt the alignment of a large number of clean images and texts, thereby reducing the downstream\nzero-shot accuracy of clean samples. We randomly select K samples from all text data for fine-grained augmentation,\nobtaining K augmented data, denoted as {Ii, Ti, ST, ST}1. This random selection approach maximally retains the\noriginal feature expression capability's generalization on downstream tasks while achieving our defense objectives. The\nmapping of these K data points in the feature space is denoted as {z\u0142, z\u0142, 2, 1.\nOur positive-negative sample finetuning loss function consists of two parts: the lossi2t measures the similarity between\npositive sample images and text and the dissimilarity between negative sample texts and images, thereby minimizing\nthe information difference between positive sample images and text. The losst2i measures the similarity between\npositive sample text and images and the dissimilarity between negative sample images and text, thereby minimizing\nthe information difference between positive sample text and images. Both parts jointly optimize the consistency of\nmulti-modal embedding space. The specific mathematical expressions are as follows:\nlossi2t = \\frac{1}{K} \\sum_{i=1}^{K} \\log \\left( \\frac{\\exp \\left( \\left\\langle z_{i}^{I}, z_{i}^{T} \\right\\rangle / t_{p} \\right)}{\\sum_{j=1}^{K} \\exp \\left( \\left\\langle z_{i}^{I}, z_{j}^{T} \\right\\rangle / t_{p} \\right) + \\sum_{j=1}^{K} \\exp \\left( \\left\\langle z_{i}^{I}, z_{j}^{N} \\right\\rangle / t_{n} \\right)} \\right).\nlosst2i = \\frac{1}{K} \\sum_{i=1}^{K} \\log \\left( \\frac{\\exp \\left( \\left\\langle z_{i}^{T}, z_{i}^{I} \\right\\rangle / t_{p} \\right)}{\\sum_{j=1}^{K} \\exp \\left( \\left\\langle z_{i}^{T}, z_{j}^{I} \\right\\rangle / t_{p} \\right) + \\sum_{j=1}^{K} \\exp \\left( \\left\\langle z_{i}^{T}, z_{j}^{N} \\right\\rangle / t_{n} \\right)} \\right).\nlossp-n = (lossi2t + losst2i)/2.\nHere, tp and tn are the temperature parameters for positive and negative samples, which control the sensitivity of the\nloss function to positive and negative samples by adjusting the weight of the similarity score. Specifically, increasing"}, {"title": "Experiments", "content": "4.1 Setup\nDataset and models As a defense technique during the fine-tuning phase, we adopted the fine-tuning setting of Bansal\net al. [2023]. We utilized the open-source CLIP model from OpenAI Radford et al. [2021a] as the pre-trained clean\nmodel, which is trained on a dataset containing 400 million image-text pairs. We selected 500,000 image-text pairs\n(CC500K) as our fine-tuning dataset from the CC3M dataset Sharma et al. [2018]. And following Bansal et al. [2023],\nwe use the ResNet-50 model as the CLIP vision encoder and a transformer as the text encoder during fine-tuning. We\nconducted our experiments using an A100 GPU.\nThe victim models generation We also used the CC500K to simulate the adversary's attack process. Specifically, we\nrandomly selected 1500 samples from CC500K for different kinds of backdoor attacks, embedded the triggers into the\nimages, and modified the corresponding text to attack the target category with a specific template. The other samples\nare kept unchanged. This dirty dataset is then used to finetune the pre-trained large model. For finetuning, we use a\nbatch size of 128, an iteration number of 5, AdamW as our optimizer, a base learning rate of 1e - 6, and the number of\nsteps to warm up the learning rate is 10000, the weight decay of the optimizer is 0.1. The Adam momentum factor and\nAdam rmsprop factor are 0.9 and 0.999, and the Adam eps is le - 8.\nDefense finetuning We utilize the CC500K to conduct CleanCLIP and our TA-Cleaner finetuning. For both, we use\na batch size of 64, an iteration number of 10, AdamW as our optimizer. The learning rate warm up steps is 10000,\nthe weight decay of the optimizer is 0.1. The Adam momentum factor and Adam rmsprop factor are 0.9 and 0.999\nrespectively, and the Adam eps is le 8, the base learning rates are 45e 7 for CleanCLIP and 5e 5 for our\nTA-Cleaner.\nEvaluation metrics Following Yang et al. [2024a], Bansal et al. [2023] and most attacks like Liang et al. [2023], we\nadopt benign accuracy (BA, \u2191) and attack success rate (ASR, \u2193) as our evaluation metrics. For BA, a higher value\nindicates superior clean performance, while for ASR, a higher value indicates stronger attack performance. Using these\nmetrics, we assess the defense strategies across two commonly used tasks: zero-shot classification on the ImageNet-1K\nvalidation set and linear probe. In the linear probe task, the feature extraction layers remained fixed, and only the linear\nlayer was trained on 50,000 clean images from the ImageNet-1K training set, followed by testing on the ImageNet-1K\nvalidation set."}, {"title": "TA-Cleaner performance", "content": "Similar to Bansal et al. [2023] and Yang et al. [2024a], we conduct zero-shot testing on ImageNet1K to evaluate our\nperformance. We utilize six attack methods to generate victim models: BadNet Gu et al. [2017], Blended Chen et al.\n[2017], SIG Liu et al. [2020], WaNet Nguyen and Tran [2021], SSBA Li et al. [2021b], and BadCLIP Liang et al.\n[2023]. Among them, the first five are classic backdoor attack methods in supervised learning, while BadCLIP is a\nrecently developed attack technique specifically tailored for CLIP. For each attack method, we randomly select 1500\nimages from CC500K for poisoning and subsequently finetune to generate poisoned models. We apply both CleanCLIP\nand TA-Cleaner defenses separately to these six poisoned models and obtain the Top-k (k=1,3,5,10) BA (%) and ASR\n(%) after defense finetuning. Our final results are presented in Table 1. In our implementation of TA-Cleaner, for the\nfirst five attack methods, we randomly selected 1,000 images per iteration for positive and negative subtext generation\nand finetuning. However, for BadCLIP, we randomly sampled 3,000 images for defense, as this is an exceptionally\npotent attack method where a smaller sample size would be insufficient to generate a defense boundary to resist the\nproximity of poisoned image features.\nRegarding Top-1 performance Compared to the victim model (NoDefense), although CleanCLIP already exhibits\nexcellent defense capabilities, our TA-Cleaner can further reduce the ASR, achieving near-zero Top-1 ASR in defending"}, {"title": "Analysis", "content": "Ablation As shown in Table 1, Table 2 and Figure 3, when we introduced fine-grained augmentation of positive and\nnegative sub-samples, indicated by the addition of lossp\u2212n, our TA-Cleaner significantly improved defense performance\ncompared to the original CleanCLIP, without compromising the model's performance on clean samples.\nThe lossp-n weight We evaluated the impact of the proposed lossp-n on the overall loss for defense performance, as\nshown in Eq. 4. In this ablation study, we set a to 1 by default and adjusted \u1e9e to achieve different influence levels of\nlossp-n. As illustrated in Figure 4(a), we found that as the \u1e9e/a ratio increases, i.e., the higher the weight of lossp-n,\nthe better defense performance.\nThe pos-/neg- temperature factor Since we employ fine-grained alignment of positive and negative subtexts with\nimages, it is essential to consider the relationship between the model's focus on positive and negative samples and the\nfinal defense performance. This relationship can be modulated by adjusting the temperature factors tp and tn in Eq. 1\nand 2 to achieve different levels of attention to positive and negative samples. Specifically, the smaller the value of the\ntemperature factor, the higher the attention received. As shown in Figure 4(b), we conducted ablation experiments with\nfive different sets of temperature factors and found that when tp is higher than tn, the model de-emphasizes negative\nsamples, leading to an inability to fine-tune the distribution of text features in the feature space, thus failing to actively\ndistance itself from poisoned image features and resulting in poorer defense performance. Furthermore, if both factors"}, {"title": "Conclusions", "content": "In this paper, we focus on fine-tuning defense strategies against backdoor attacks targeting MCL. Building upon\nCleanCLIP, we propose an optimized approach utilizing fine-grained augmentation based on positive and negative\nsubtexts. This method significantly mitigates attack risks while preserving the model's normal feature expression\ncapabilities, achieving efficient defense.\nLimitations Since our proposed TA-Cleaner primarily addresses backdoor attacks in the image modality, the defense\nperformance against text modality attacks remains unknown. In the future, we will further explore comprehensive and\nefficient defense methods that are effective across various modalities."}]}