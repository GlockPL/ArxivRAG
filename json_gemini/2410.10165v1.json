{"title": "HSR-Enhanced Sparse Attention Acceleration", "authors": ["Bo Chen", "Yingyu Liang", "Zhizhou Sha", "Zhenmei Shi", "Zhao Song"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various\napplications, but their performance on long-context tasks is often limited by the computational\ncomplexity of attention mechanisms. This paper introduces a novel approach to accelerate at-\ntention computation in LLMs, particularly for long-context scenarios. We leverage the inherent\nsparsity within attention mechanisms, both in conventional Softmax attention and ReLU at-\ntention (with ReLU activation, a \u2208 N+), to significantly reduce the running time complexity.\nOur method employs a Half-Space Reporting (HSR) data structure to rapidly identify non-zero\nor \"massively activated\" entries in the attention matrix. We present theoretical analyses for\ntwo key scenarios: attention generation and full attention computation with long input context.\nOur approach achieves a running time of O(mn4/5) significantly faster than the naive approach\nO(mn) for attention generation, where n is the context length, m is the query length, and d is\nthe hidden dimension. We can also reduce the running time of full attention computation from\nO(mn) to O(mn1\u22121/[d/2] + mn4/5). Importantly, our method introduces no error for ReLU\nattention and only provably negligible error for Softmax attention, where the latter is supported\nby our empirical validation. This work represents a significant step towards enabling efficient\nlong-context processing in LLMs, potentially broadening their applicability across various do-\nmains.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have showcased remarkable capabilities across various applications,\nincluding context-aware question answering, content generation, summarization, and dialogue sys-\ntems, among others [TDFH+22, CDI+21, WTB+22, ZLD+24]. Long-context tasks of LLMs have\ngained more and more attention. Several LLMs extend their context length to 128K tokens, such as\nYarn [PQFS23], GPT-4 [Ope23], Claude 3.5 [Ant24], Llama 3.1 [Met24], Phi-3.5 [AJA+24], Mistral\nNemo [Mis24], etc. A bottleneck for long-context tasks is the computational cost of the attention\nmechanism in LLMs. The key to LLM success is the transformer architecture [VSP+17], wildly\nused in various practical scenarios [RWC+19, KT19, WSD+23, WCZ+23, WXZ+24], whose critical\ncomponent is the attention mechanism. Let n be the data length, m be the length of query tokens,\nand d be the feature dimension\u00b9. The conventional attention uses Softmax activation and is defined\nas follows:\nDefinition 1.1 (Softmax attention). Let $Q \\in \\mathbb{R}^{m\\times d}$ and $K, V \\in \\mathbb{R}^{n\\times d}$ denote the query, key, and\nvalue matrix. The Softmax attention is:\n$Attn_S(Q, K, V) := Softmax(Q K^T /\\sqrt{d})V = D^{-1}A_SV \\in \\mathbb{R}^{m\\times d}$,\nwhere (1) $A_S := exp(Q K^T /\\sqrt{d}) \\in \\mathbb{R}^{m\\times n}$ and exp is applied element-wise, (2) $D := diag(A_S \\cdot 1_n) \\in\n\\mathbb{R}^{m\\times m}$ denotes the normalization matrix, (3) $D^{-1}A_S \\in \\mathbb{R}^{m\\times n}$ denotes the attention matrix.\nIn practical LLM applications, there are two scenarios for attention computation depending\non the context length n and query length m. The first case, m = \u04e8(1), represents the iterative\ntext generation based on the pre-computed Key Value Cache (KV), which stores the intermediate\nattention key and value matrices. The second case, m = O(n), represents the full self-attention\ncomputation before text generation or the cross-attention computation. However, in both cases,\nwhen the context window n becomes larger, the running time will increase correspondingly, i.e.,\nit will be linear and quadratic in n for m = \u0398(1) and m = O(n), respectively. Thus, reducing\nthe running time of attention computations with long context input becomes essential to minimize\nresponse latency and increase throughput for LLM API calls.\nIn this work, we introduce novel methods to reduce the running time complexity for both cases,\ni.e., m = \u04e8(1) and m = O(n). Our approach is inspired by the inherent sparsity found within\nattention mechanisms. Numerous prior studies have highlighted the significant sparsity in the\nattention matrix [CGRS19, APB+23, LWD+23, TZZ+24, SCKL24]. This manifestation of sparsity\nin Softmax attention is that a large number of attention scores, i.e., $QK^T$, concentrate on a small\nnumber of entries, which is known as \u201cmassive activation\". Due to this nature, Softmax attention\ncan be accelerated by only calculating the entries that contain large attention scores, introducing\nnegligible approximation errors [ZSZ+23, LHY+24].\nMoreover, when considering ReLU attention (with ReLU activation, a \u2208 N+), we can accelerate\nthe attention computation without any approximation error. ReLU attention is another attention\nmechanism used in transformer architecture, substituting the conventional Softmax activation func-\ntion with ReLU, which has demonstrated performance comparable to Softmax attention in various\ndownstream tasks [WLGK23, HDLL22]; see Section 2 for more details. In the following, we present\nthe formal definition of ReLU attention."}, {"title": "Definition 1.2 (ReLU attention)", "content": "Let $Q \\in \\mathbb{R}^{m\\times d}$ and $K,V \\in \\mathbb{R}^{n\\times d}$ denote the query, key, and\nvalue matrix. Let $a \\in \\mathbb{N}+$. The ReLU attention is:\n$Attn_r(Q, K, V) := D^{-1}A_rV \\in \\mathbb{R}^{m\\times d}$,\nwhere (1) $A_r := ReLU^a(Q K^T /\\sqrt{d} - b) \\in \\mathbb{R}^{m\\times n}$ and $ReLU^a$ denotes the a-th power of ReLU\nactivation for any $a \\in \\mathbb{N}+$, (2) $D := diag(A_r \\cdot 1_n) \\in \\mathbb{R}^{m\\times m}$ denotes the normalization matrix, (3)\n$b \\in \\mathbb{R}$ denotes position bias, (4) $D^{-1}A_r \\in \\mathbb{R}^{m\\times n}$ denotes the attention matrix.\nTo expedite the computation, the critical\ntask is to identify the large/non-zero entries for\nSoftmax/ReLU attention, respectively. To do\nso, we utilize the half-space reporting (HSR)\ndata structure, which is introduced in [AEM92]\nto address the half-space range reporting prob-\nlem. This is a fundamental problem in compu-\ntational geometry and can be formally defined\nas follows:\nDefinition 1.3 (Half-space range reporting\n[AEM92, SYZ21]). Given a set S of n points\nin Rd with initialization, we have an operation\nQUERY(H): given a half-space $H \\subset \\mathbb{R}^d$, output\nall of the points in S that contain in H, i.e.,\n$S \\cap H$.\nIn our framework, we define the half-space\nas the region where the attention scores (the\ninner products of key and query vectors) exceed\nsome threshold. We leverage this data structure\nto expedite the identification of non-zero entries within the ReLU attention matrix and large entries\nin Softmax attention. Consequently, we can compute the ReLU attention only based on those non-\nzero entries without any approximation error, and compute the Softmax attention based on entries\nlarger than threshold with negligible approximation errors, resulting in a substantial reduction in\ncomputation time. When m = \u0398(1), our methods can significantly accelerate ReLU and Softmax\nattention computation time over the naive approach from O(mn) to O(mn4/5) with pre-processed\nKV cache. When m = \u0398(n), our online methods can also accelerate ReLU and Softmax attention\ncomputation time over the naive approach from O(mn) to O(mn1\u22121/[d/2] + mn4/5). In more\ndetails, when m = \u0398(1) and for any d \u2208 N+, our Algorithm 2 can achieve the fast generation\nwith pre-processed KV cache in O(mn4/5) (Theorem 4.1 and Theorem 4.2)). When m = \u04e8(n),\nour Algorithm 3 can achieve the full attention computation in O(mn1-1/[d/2] + mn4/5) including\nHSR initialization time and query time (Theorem 5.1 and Theorem 5.2). Thus, our methods\ncan improve both the generation speed and full attention computation for long input context,\ni.e., n being excessively large. Furthermore, our empirical results in Section 7 show that the\napproximation error associated with Softmax attention utilizing \"massive activated\" entries only\nis small in practice, which is consistent with our theoretical analysis."}, {"title": "Our contributions:", "content": "\u2022 To the best of our knowledge, this is the first work incorporating the HSR data structure with\nattention computation, to reduce the running time complexity with the help of the sparsity\nwithin the attention mechanisms.\n\u2022 Theoretically, we provide rigorous proofs for reducing the computational time (1) for ReLU\nattention generation from O(mm) to O(mn4/5) (Algorithm 2 and Theorem 4.1); (2) for full\nReLU attention computation from O(mm) to O(mn1-1/[d/2] +mn4/5) (Algorithm 3 and The-\norem 5.1), without incurring any approximation error in both cases.\n\u2022 We achieve the same running time speed up for the conventional Softmax attention, and\nwe give rigorous theoretical proofs to ensure that the resulting approximation error remains\nnegligible (Theorem 4.2, 5.2 and Theorem 4.3).\n\u2022 We conduct empirical experiments on prominent LLMs to verify the approximation error\nassociated with Softmax attention utilizing \u201cmassive activated\u201d entries only. The results show\nthat the error using a few top entries is already insignificant, consistent with our theoretical\nalysis.\nRoadmap. Section 2 presents related work. In Section 3, we introduce essential concepts and\nkey definitions used this paper. In Section 4, we present our main results, i.e., guarantees on run\ntime reduction and approximation error. In Section 5, we introduce the extension of our method\non full attention computation. In Section 6, we provide a brief summary of the techniques used\nin our proof. In Section 7, we provide our empirical results of evaluating three mainstream LLMS\nwith Softmax attention with top-r indices on different r. In Section 8, we discuss the potential of\nextending our method to other activation functions. In Section 9, we concludes our algorithm and\ncontributions."}, {"title": "2 Related Work", "content": "Attention acceleration for long context input. Long context window is essential for trans-\nformer based LLMs in many downstream tasks. However, due to the quadratic time complexity\nassociated with self-attention mechanisms, transformers are usually hard to inference efficiently.\nNumerous methods have been proposed to enhance the inference efficiency. One approach in-\nvolves using alternative architectures as proxies for attention to support faster inference, such\nas Mamba [GD23, DG24], PolySketchFormer [KMZ23], Hopfield Models [HYW+23, WHL+24,\nHLSL24, XHH+24, WHHL24, HCL+24, HCW+24] and Linearizing Transformers [ZBKR24, MVK+24].\nHowever, the broad applicability of these methods across different applications and modalities re-\nmains to be fully validated. Another line of research focuses on approximating attention matrix\ncomputation [AS23, AS24a, AS24b, HJK+24, ZHMK24, LSSZ24b, PMN+23, CTWC24, LSSY24,\nLLS+24b, GSWY23, DYZ+24, LSS+24b]. Nevertheless, these methods often rely on assump-\ntions that may not be practical. For instance, some approaches use polynomial methods to\napproximate the exponential function, which requires all entries to be bounded by a small con-\nstant. However, our HSR-enhanced attention framework is designed based on practical obser-\nvation and validated by empirical support. These advancements in attention mechanisms and\ntransformer efficiency not only improve general model performance but also play a crucial role\nin enhancing in-context learning capabilities, where models leverage information from the imme-\ndiate context to perform tasks without fine-tuning. We refer the readers to some other related\nworks [CLL+24, LSS+24a, LLS+24a, LLS+24c, LSSZ24a, LSSZ24b, LSSZ24c, SWXL24, XSL24,\nSMN+24, LLSS24, SYZ23, QSZZ23, SYYZ23, Zha22, SZZ24, SYZ23]."}, {"title": "ReLU attention.", "content": "ReLU attention is an innovative mechanism that employs the ReLU activation\nfunction in place of the traditional Softmax function for attention computation. Previous stud-\nies have highlighted the promise potential of ReLU attention in various domains. From empirical\nside, [WLGK23] has demonstrated that incorporating ReLU as the activation function in vision\ntransformers enhances performance on downstream tasks. [SGT+23] has shown that transformers\nequipped with ReLU attention outperform those with Softmax attention, particularly when dealing\nwith large key-value memory in machine translation tasks. From theoretical side, the scale-invariant\nproperty of ReLU attention [LBZ+22] facilitates the scalability of transformer networks. Further-\nmore, [BCW+23, FGBM23] have shown that the inherent properties of ReLU attention contribute\npositively to the learning process of transformer models. Another key advantage of ReLU attention\nis that the ReLU function effectively sets all negative values to zero, allowing us to bypass these\nnon-contributory elements during attention computation, thereby reducing the running time of at-\ntention computation. Importantly, omitting these zero and negative entries does not introduce any\nerror into the final output of the ReLU attention mechanism.\nHalf-space reporting (HSR) data structure. The Half-Space Reporting (HSR) data structure,\ninitially proposed by [AEM92], was developed to address the half-space range reporting problem.\nThe expedited range query capability inherent to HSR has been demonstrated to significantly\nenhance computational efficiency across a variety of tasks, as evidenced by numerous previous\nworks in the literature. Studies such as [JSWZ21] and [BKS23] have applied HSR to facilitate\nsolving general linear programming (LP) problems. Another line of research has highlighted HSR's\npotential in expediting the training process of contemporary neural networks [QSY23, GQSW22].\nThere is also a collection of research that concentrates on leveraging HSR for the advancement of\nsolutions to geometric and graphical challenges [CSX05, JFL+13, EGKM17]."}, {"title": "3 Preliminary", "content": "In Section 3.1, we introduce notations used in the paper. In Section 3.2, we introduce a modified\nversion of Softmax attention that operates on a specific subset of indices. It defines the top-r nearest\nneighbors Softmax attention, which focuses on the most relevant entries in the attention matrix. In\nSection 3.3, we describe the massive activation property for attention mechanisms. In Section 3.4,\nwe present a data structure for efficiently solving the half-space range reporting problem."}, {"title": "3.1 Notations", "content": "Here, we introduce basic notations used in this paper. For any positive integer n, we use [n] to\ndenote set {1,2,\u2026\u2026,n}. We use Var[] to denote the variance. For two vectors x \u2208 Rn and y \u2208R"}, {"content": ", we use (x, y) to denote the inner product between x,y. We use 1n to denote a length-n vector\nwhere all the entries are ones. We use Xi,j to denote the i-row, j-th column of X \u2208 Rm\u00d7n. We use\n||A||\u221e to denote the l\u221e norm of a matrix A \u2208 Rn\u00d7d, i.e. $||A||_{\\infty} := max_{i\\in[n],j\\in[d]} |A_{i,j}|$"}, {"title": "3.2 Softmax Attention with Index Set", "content": "Recall that we have already provided the definition of ReLU attention in Definition 1.2. Here, we\npresent the key concepts of Softmax attention. For Softmax attention, since we only calculate the\n\"massive activated\u201d entries to get our approximated results, we introduce the formal definition:\nDefinition 3.1 (Input with index set). Let $K \\in \\mathbb{R}^{n\\times d}$ and $V\\in \\mathbb{R}^{n\\times d}$ be defined in Definition 1.1.\nLet $R \\subseteq [n]$ be an index set of size $|R| = r \\in [n]$. Let $R := [n] \\setminus R$ be the complementary set, where"}, {"title": "$|\\bar{R}|$= n-r.", "content": "We define\n$\\\nK := K_R \\in \\mathbb{R}^{r\\times d} \\\n\\hat{V} := V_R \\in \\mathbb{R}^{r\\times d} \\\nK := K_{\\bar{R}} \\in \\mathbb{R}^{(n-r)\\times d} \\\n\\hat{V} := V_{\\bar{R}} \\in \\mathbb{R}^{(n-r)\\times d}$\nas the submatrix of K and V, i.e., whose row index is in R or R, respectively.\nIn this work, we consider calculating the Softmax attention on the \"massive activation\" index\nset, where we define the \"massive activation\" index set as the top-r indices. We introduce our\ndefinition for top-r indices of Softmax attention as follows:\nDefinition 3.2 ( Top-r indices Softmax attention ). Let $q \\in \\mathbb{R}^d$, $K,V \\in \\mathbb{R}^{n\\times d}$ be defined in\nDefinition 1.1. Let NN(r, q, K) \u2286 [n] denote the indices of top-r entries of $qK$, where |NN(r, q, K)| =\nr. Let $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{r\\times d}$ and $\\tilde{K}, \\tilde{V}\\in \\mathbb{R}^{(n-r)\\times d}$ be defined in Definition 3.1. We define the top-r nearest\nneighbors (NN) Softmax attention computation $Attn_{NN}(q, K, V) \\in \\mathbb{R}^d$ as follows:\n$Attn_S^{NN} (q, K, V) := Softmax(q \\hat{K}^T)\\hat{V} = \\hat{a}^{-1}\\hat{u} \\hat{V} \\in \\mathbb{R}^d$\nwhere\n$\\hat{u} := exp(q \\hat{K}^T) \\in \\mathbb{R}^r$ and $\\hat{a} := (\\hat{u}, 1_r) \\in \\mathbb{R}$.\nFurthermore, we define $\\bar{u} := exp(q \\tilde{K}^T) \\in \\mathbb{R}^{n-r}$, $\\bar{a} := (\\bar{u}, 1_{n-r}) \\in \\mathbb{R}$, and $u := exp(q K^T) \\in \\mathbb{R}^{n+1}$,\n$a := (u, 1_{n+1}) \\in \\mathbb{R}$.\nIn Definition 3.2, we view the \"massive activated\" entries as the top-r entries. Therefore, we\nonly calculate the Softmax attention based on $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{r\\times d}$, instead of $K, V \\in \\mathbb{R}^{n\\times d}$."}, {"title": "3.3 Massive Activation", "content": "Now, we introduce our observations on the properties of the attention scores (the inner products\nof query vectors and key vectors). This further facilitates the error analysis of the top-r indices\nSoftmax attention. To begin with, we provide the definition of the massive activation property as\nfollows:\nDefinition 3.3 (Massive activation property). Let $\\gamma\\in [0,1]$, $\\beta_1 \\geq \\beta_2 \\geq 0$. Let NN(r, q, K) \u2286 [n]\ndenote the indices of top-r entries of $qK$. We define $(\\gamma, \\beta_1, \\beta_2)$ massive activation for a query\n$q \\in \\mathbb{R}^d$ and key cache $K \\in \\mathbb{R}^{n\\times d}$, if the following conditions hold:\n\u2022 The top-n\u03b3 entries are massive, i.e., $\\frac{1}{\\|q\\|_2} \\sum_{i \\in NN(n^{\\gamma},q,K)} (q, K_i) \\geq \\beta_1 \\log(n)$.\n\u2022 The remaining terms are upper bounded, i.e, $ \\forall i \\in [n] \\setminus NN(n^{\\gamma}, q, K), \\frac{1}{\\|q\\|_2} (q, K_i) \\leq \\beta_2 \\log(n)$.\nAn intuitive understanding of Definition 3.3 is that, the summation of \"massive activated\"\nentries dominates the summation of all entries, and the entries we ignored only contributes little\nto the final summation. Therefore, it is reasonable for us to omit those non \"massive activated\"\nentries.\nRemark 3.4. There are many distributions satisfying the property in Definition 3.3, such as (1)\nK drawing from any subexponential distribution, e.g., multivariate Laplace distributions, (2) K\ndrawing from any mixture of Gaussian distribution with $n^{1-\\gamma}$ Gaussian clusters."}, {"title": "3.4 Half-Space Reporting (HSR) Data Structure", "content": "We restate the result from [AEM92] for solving the half-space range reporting problem. The\ninterface of their algortihm can be summarized as in Algorithm 1. Intuitively, the data-structure\nrecursively partitions the set S and organizes the points in a tree data-structure. Then for a given\nquery (a, b), all k points of S with sgn((a, x) \u2013 b) \u2265 0 are reported quickly. Note that the query\n(a, b) here defines the half-space H in Definition 1.3. We summarize the time complexity of HSR\ndata structure as follows:\nCorollary 3.5 (HSR data-structure time complexity [AEM92], informal version of Corollary A.7).\nLet Tinit denote the pre-processing time to build the data structure, Tquery denote the time per query\nand Tupdate time per update. Given a set of n points in Rd, the half-space range reporting problem\ncan be solved with the following performances:\n\u2022 Part 1. $T_{init}(n, d) = O(n\\log n)$, $T_{query} (n, d, k) = O(dn^{1-1/[d/2]} + dk)$.\n\u2022 Part 2. $T_{init}(n, d) = O(n^{\\lceil d/2 \\rceil})$, $T_{query} (n, d, k) = O(d\\log(n) + dk)$."}, {"title": "4 Main Results on Attention Generation", "content": "In this section, we present our key findings regarding attention generation, m = \u0398(1), for both ReLU\nand Softmax attention mechanisms. Across both scenarios, we have reduced the time complexity\nfrom a naive O(mn) to O(mn4/5). Specifically, for the ReLU attention model, we have managed\nto accelerate the processing time without introducing any approximation errors. In the case of\nSoftmax attention, our technique results in only an insignificant approximation error.\nWe begin with introducing our result on ReLU attention generation as follows:\nTheorem 4.1 (Running time of ReLU attention generation, informal version of Theorem C.2). Let\nReLU attention be defined as Definition 1.2. Assume each entry of K is from Gaussian $N(0, \\sigma^2)$,\nand each entry of Q is from Gaussian $N(0,\\sigma_q^2)$. Let \u03b4 \u2208 (0,1) denote the failure probability.\nLet $\\sigma_a = 4\\cdot (1 + d^{-1}\\log(m/d))^{1/2} \\cdot \\sigma_q\\sigma_K$. Let $b = \\sigma_a\\cdot \\sqrt{0.4\\log n}$. Suppose we have KV Cache\n$K, V\\in \\mathbb{R}^{n\\times d}$. We want to generate a m length answer, where $n \\gg m$. Then, our inference function\nin Algorithm 2, with probability at least 1 \u03b4, takes O(mn4/5) time to generate the answer.\nTheorem 4.1 shows that our Algorithm 2 accelerates the running time of ReLU attention gen-\neration from naive O(mn) to O(mn4/5), which is a significant speed up when the KV Cache is\nlarge. The at least 1 \u2013 \u03b4 success probability originates from the sparsity analysis of ReLU attention\n(Lemma 6.1), where with probability at least 1 \u2013 \u03b4, we have the number of non-zero entries of each\nrow of the attention matrix is at most n4/5.\nThen, we move on to presenting our result on Softmax attention generation. Our results consist\ntwo parts: the improved running time of Softmax attention generation, and the error analysis of\nSoftmax attention with index set. Firstly, we introduce our result about the imporved running\ntime of Softmax attention generation as follows:"}, {"title": "Theorem 4.2 (Running time of Softmax attention generation", "content": "informal version of Theorem E.1).\nLet $Q \\in \\mathbb{R}^{m\\times d}$, $K,V \\in \\mathbb{R}^{n\\times d}$ and the Softmax attention $Attn_S$ be defined in Definition 1.1. Let\nNN(r, q, K) \u2286 [n] and the Softmax attention with index set $Attn_{NN}$ be defined as Definition 3.2. We\nchoose the threshold $b \\in \\mathbb{R}$ in Algorithm 2 such that $R = NN(n^{4/5},q, K)$. Then, we can show that\nthe Softmax attention with index set $Attn_{NN}$, achieves outstanding running time under the Softmax\nattention generation scenario: Suppose we have KV Cache $K,V \\in \\mathbb{R}^{n\\times d}$. We want to generate a\nm length answer, where $n \\gg m$. Our inference function in Algorithm 2 (replacing ReLU attention\nwith Softmax attention) takes O(mn4/5) time to generate the answer.\nTheorem 4.2 demonstrates that if we choose the threshold b satisfying $R = NN(n^{4/5}, , q, K)$, we\ncan achieve a significant running time improve of the Softmax attention generation.\nIt is evident that this method introduces an approximation error due to the exclusion of certain\nentries. Nevertheless, under mild assumptions about the distribution of the attention scores, we\ndemonstrate that this approximation error is indeed negligible. The proof's intuitive explanation\nlies in the fact that the majority of attention scores are focused on the small subset of entries that\nwe retain. We organize our result as follows:\nTheorem 4.3 (Error analysis of Softmax attention with index set, informal version of Theo-\nrem F.2). Let $Q \\in \\mathbb{R}^{m\\times d}$, $K, V \\in \\mathbb{R}^{n\\times d}$ and the Softmax attention $Attn_S$ be defined in Definition 1.1.\nLet $q \\in \\mathbb{R}^d$ denote a single row of $Q \\in \\mathbb{R}^{m\\times d}$. Let $\\gamma \\in [0,1]$, $\\beta_1 \\geq \\beta_2 \\geq 0$. Let the index set $R\nand the Softmax attention with index set $Attn_{NN}$, be defined as Definition 3.2. Let NN(r, q, K) \u2286 [n]\ndenote the indices of top-r entries of qK. Let $R = NN(n^{\\gamma},q, K) \u2286 [n]$, where $|R| = n^{\\gamma}$. Assume"}, {"title": "the query q and key cache K have (\u03b3", "content": "\u03b21, \u03b22) massive activation property (Definition 3.3). Then,\nwe have\n$||Attn_{NN}(q, K, V) \u2013 Attn_S(q, K, V) ||_{\\infty} \\leq \\frac{2||V||_{\\infty}}{n^{\\gamma}+(\\beta_1-\\beta_2) \\cdot ||q||_2 - 1}$\nTheorem 4.3 presents the error of Softmax attention with index set is relatively small. Conse-\nquently, omitting the remaining less significant entries is a justifiable compromise.\nRemark 4.4. With mild assumptions on V, we can have more precious results from Theorem 4.3.\nFor example, if the entries in V conform to subgaussian distribution with constant variance, we\nhave $||V||_{\\infty} = O(\\log(n))$ with high probability."}, {"title": "5 Extension on Full Attention Computation", "content": "In this section, we extend our results to full attention computation scenario, where the number\nof queries and keys is proportional, i.e., m = \u0398(n). Essentially, the full attention computation is\nbeneficial in practical applications, particularly within the context of cross-attention computations.\nFor ReLU attention, we leverage Part 1 result of Corollary 3.5 to accelerate the identification of\nnon-zero entries (activated entries). We introduce our result on ReLU attention as follows:"}, {"title": "Algorithm 3 Full attention computation", "content": "data structure FULLATTENTION COMPUTATION\nmembers\nHALFSPACEREPORT HSR\nend members\nprocedure INFERENCE({Ki}i\u2208[n], {Qr}r\u2208[m], V, n, m, d)\nb \u2190 \u03c3\u03b1\u00b7 \u221a\u221a0.4log n.\nHSR.INIT({Ki}i\u2208[n], n, d)\nA\u2190 Omxn\nfor i = 1 \u2192 m do\nSi,fire \u2190 HSR.QUERY(Qi, b)\nfor j \u2208 Si fire do\nAi,j \u2190 ReLU\u00b0((Qi, Kj)/\u221ad \u2013 b) or Ai,j \u2190 Softmax((Qi, Kj)/\u221ad)\nend for\nend for\nreturn D-1AV\nend procedure\nend data structure\nTheorem 5.1 (Running time of full ReLU attention computation, informal version of Theo-\nrem B.2). Let ReLU attention be defined as Definition 1.2. Assume each entry of K is from\nGaussian $N(0, \\sigma^2_K)$, and each entry of Q is from Gaussian $N(0,\\sigma^2_q)$. Let \u03b4\u2208 (0,1) denote the\nfailure probability. Let oa = 4\u00b7 (1 + d\u22121 log(m/f))1/2 . \u03c3q\u03c3\u03ba. Let $b = \\sigma_a\\cdot \\sqrt{0.4\\log n}$. Suppose we\nhave Q, K, V \u2208 Rn\u00d7d. There exist an algorithm (Algorithm 3), with probability at least 1 \u2013 8, takes\nO(n2-1/[d/2] + n1+4/5) time to compute the full ReLU attention of Q, K,V."}, {"title": "In Theorem 5.1", "content": "we improve the running time of full ReLU attention computation from O(n\u00b2)\nto $O(n^{2-1/[d/2]} + n^{1+4/5})$, which is a notable uplift of the running time when n is extremely large.\nThen, we present our result on Softmax attention. Intuitively, we use the Part 1 result of\nCorollary 3.5 to identify those \"massive activated\" entries (top-r indices) within the attention\nmatrix of Softmax attention, and calculate the Softmax attention with top-r indices. We organize\nour result as follows:\nTheorem 5.2 (Running time of Softmax full attention computation, informal version of Theo-\nrem E.2). Let $Q \\in \\mathbb{R}^{m\\times d}$, $K, V \\in \\mathbb{R}^{n\\times d}$ and the Softmax attention $Attn_S$ be defined in Definition 1.1.\nLet NN(r, q, K) \u2286 [n] and the Softmax attention with index set $Attn_{NN}$ be defined as Definition 3.2.\nWe choose the threshold $b \\in \\mathbb{R}$ in Algorithm 3 such that $R = NN(n^{4/5},q, K)$. Then, we have the\nSoftmax attention with index set $Attn_{NN}$ achieves outstanding running time under full Soft-\nmax attention computation scenario: Suppose we have m = \u04e8(n). Algorithm 3 (replacing ReLU\nattention with Softmax attention) takes $O(n^{2\u22121/[d/2]} + n^{1+4/5})$ time to compute the full ReLU attention of\nQ, K, V.\nTheorem 5.2 demonstrates our $O(n^{2-1/[d/2]} + n^{1+4/5})$ running time on Softmax full attention\ncomputation, which improves from naive running time O(n\u00b2)."}, {"title": "6 Technical Overview", "content": "In Section 6.1, we introduce our analysis about the sparsity in the ReLU attention mechanism. In\nSection 6.2, we present our results of two general attention frameworks. In Section"}]}