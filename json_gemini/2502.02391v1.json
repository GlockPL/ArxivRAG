{"title": "FEWTOPNER: INTEGRATING FEW-SHOT LEARNING WITH\nTOPIC MODELING AND NAMED ENTITY RECOGNITION IN A\nMULTILINGUAL FRAMEWORK", "authors": ["Ibrahim Bouabdallaoui", "Fatima Guerouate", "Samya Bouhaddour", "Chaimae Saadi", "Mohammed Sbihi"], "abstract": "We introduce FewTopNER, a novel framework that integrates few-shot named entity recognition\n(NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-\nresource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-ROBERTa,\naugmented with language-specific calibration mechanisms, to generate robust contextual embed-\ndings. The architecture comprises a prototype-based entity recognition branch-employing BiL-\nSTM and Conditional Random Fields for sequence labeling-and a topic modeling branch that\nextracts document-level semantic features through hybrid probabilistic and neural methods. A cross-\ntask bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic\nrepresentations, thereby enhancing entity disambiguation by incorporating global semantic context.\nEmpirical evaluations on multilingual benchmarks across English, French, Spanish, German, and\nItalian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot\nNER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points\nin F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual\ninformation. Ablation studies further confirm the critical contributions of the shared encoder and\ncross-task integration mechanisms to the overall performance. These results underscore the efficacy\nof incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER\nfor robust cross-lingual applications in low-resource settings.", "sections": [{"title": "1 Introduction", "content": "Extracting structured insights from vast, unstructured textual data is a central challenge in natural language process-\ning (NLP). Two core tasks-Named Entity Recognition (NER) and Topic Modeling (TM)\u2014have traditionally been\naddressed in isolation, yet their integration promises a richer, more nuanced interpretation of text. NER targets the\nprecise identification and categorization of entities (e.g., persons, organizations, and locations), which is critical for\napplications such as customer service automation, business intelligence, and real-time information systems in domains\nlike healthcare and finance [1]. In contrast, TM uncovers latent themes that characterize large document collections,\nsupporting tasks such as content recommendation, summarization, and sentiment analysis.\nIntegrating NER and TM can yield significant benefits; recognizing entities within their broader thematic context en-\nhances disambiguation and enriches downstream analyses such as trend detection and targeted marketing [2]. However,\nthe joint modeling of fine-grained entity details and macro-level topic structures is inherently challenging-especially\nin few-shot and resource-poor language scenarios where annotated data is limited [3]. Traditional NLP models, even\nthose based on recent pre-trained architectures like BERT and XLM-ROBERTa, often falter when required to general-\nize across languages and domains without extensive retraining [4]."}, {"title": "2 Related Work", "content": "Named entity recognition (NER) has been a central task in natural language processing for decades, with early sys-\ntems relying on rule-based and statistical approaches. Traditional NER systems [6, 7, 8] exploited handcrafted rules,\nlexicons, and shallow syntactic features. While these systems achieved high precision on constrained domains, their\nrecall was often limited due to the difficulty in generalizing beyond predefined dictionaries and patterns. To overcome\nthese limitations, feature-based supervised methods, including Support Vector Machines (SVM) [9, 10] and Condi-\ntional Random Fields (CRF) [11, 12], emerged as the next generation of solutions, reducing the need for manual rule\nengineering but still relying heavily on carefully designed features."}, {"title": "Deep Neural Approaches", "content": "Recent advancements in deep learning have transformed Named Entity Recognition\n(NER), shifting from feature-engineered models to end-to-end neural architectures. Recurrent Neural Networks\n(RNNs) have been widely used for sequence labeling tasks, demonstrating superior performance in capturing con-\ntextual dependencies within text [13]. Meanwhile, Convolutional Neural Networks (CNNs) have been explored as\nan alternative approach, leveraging local context and character-level features to enhance entity recognition, as seen in\nbiomedical NER applications [14]. More recently, transformer-based models such as BERT and XLNet have set new\nbenchmarks in NER by leveraging large-scale pretraining to generate contextualized word embeddings, outperforming\ntraditional RNN- and CNN-based models [15]. However, these models require substantial amounts of annotated data\nto avoid overfitting and achieve optimal performance, making them less effective in low-resource settings [16]."}, {"title": "Transfer Learning and Domain Adaptation in NER", "content": "To mitigate the data dependency of deep neural models,\ntransfer learning has emerged as a key strategy for improving NER performance in low-resource domains. This ap-\nproach enables knowledge transfer from resource-rich domains to enhance entity recognition in data-scarce scenarios\n[17]. Techniques such as parameter-sharing and fine-tuning have been employed to optimize representations across\ndifferent domains, improving adaptability while minimizing the need for extensive labeled data [18]. However, conven-\ntional transfer learning methods often assume that source and target domains share the same label space (homogeneous\ntransfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous set-\ntings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning\ndomain-invariant representations, effectively enhancing generalization across diverse datasets [19]. Despite these im-\nprovements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the\nneed for more efficient domain adaptation techniques."}, {"title": "Few-Shot Learning for NER", "content": "Few-shot learning has become a crucial area of research for Named Entity Recog-\nnition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional\nsequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences\nwithin a sentence and the absence of a predefined entity class set [20]. To address these challenges, meta-learning\ntechniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that parti-"}, {"title": "Meta-Learning Approaches in NER", "content": "Meta-learning has gained significant traction in natural language processing\n(NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce. One of the most notable\napplications of meta-learning in NER is MetaNER, which combines meta-learning with adversarial training to develop\na robust and generalizable sequence encoder [25]. MetaNER is trained across multiple source domains, explicitly sim-\nulating domain shifts during training, and can adapt rapidly to new domains with minimal labeled data. By leveraging\nadversarial training, it enhances model generalization while mitigating overfitting issues. However, MetaNER requires\nupdating the entire network during adaptation, which can be computationally intensive. Beyond MetaNER, other meta-\nlearning approaches for NER have emerged, including FewNER, which decomposes the meta-learning process into\ntask-independent and task-specific components, reducing the risk of overfitting and improving adaptation efficiency\n[21]. Additionally, adversarial learning techniques have been explored to further enhance robustness against domain\nshifts and noisy data, demonstrating improvements in model generalization [26]. These advancements highlight the\ngrowing potential of meta-learning in NER, though challenges remain in making these methods more computationally\nefficient and scalable for real-world applications."}, {"title": "Prototypical Networks and Model Fusion", "content": "Prototypical networks have become a fundamental approach in few-\nshot Named Entity Recognition (NER), leveraging token-level representations to construct class prototypes and using\na distance metric-typically cosine similarity-for classification. ProtoNER exemplifies this paradigm, demonstrat-\ning strong adaptability in incremental learning scenarios where new entity classes can be incorporated with minimal\nadditional data [27]. To improve upon standard prototypical networks, researchers have introduced SpanProto, a two-\nstage span-based prototypical network that refines entity boundary detection and enhances classification accuracy [28].\nAdditionally, EP-Net addresses prototype dispersion by aligning entity spans in a projected embedding space, leading\nto improved few-shot NER performance [29]. Beyond standalone prototypical networks, researchers have explored\nmodel fusion techniques to mitigate overfitting in few-shot settings. Recent work has introduced logit fusion and\ndifferentiation fusion, which combine multiple model outputs to correct boundary detection and entity classification\nerrors [30]. These fusion strategies help stabilize model predictions by integrating diverse representations, thereby im-\nproving overall robustness. Furthermore, HEProto, a hierarchical enhancing prototypical network, employs multi-task\nlearning to jointly optimize span detection and type classification, ensuring better entity type differentiation [31]. Such\nadvancements underscore the growing importance of hybrid approaches in refining few-shot NER models."}, {"title": "Our Positioning", "content": "While existing approaches such as ProtoNER and MetaNER have made significant strides in ad-\ndressing the challenges of few-shot NER and domain adaptation, they typically treat entity recognition and topic mod-\neling as separate tasks. In contrast, our proposed FewTopNER integrates robust few-shot entity recognition with se-\nmantically rich topic modeling through cross-task attention and language-specific calibration. This integrated approach\nnot only improves entity recognition accuracy by leveraging topic context for disambiguation but also produces more\ncoherent topic representations, as evidenced by improved normalized pointwise mutual information (NPMI) scores.\nFurthermore, by retaining task-independent representations and updating only a small set of task-specific parameters,\nFewTopNER mitigates overfitting and enhances computational efficiency during adaptation. Prior work in NER has\nevolved from rule-based and statistical methods to deep neural approaches, with transfer learning and meta-learning\nemerging as effective strategies for low-resource settings. Methods like ProtoNER and MetaNER have paved the\nway for few-shot NER, yet challenges remain-especially in reconciling the sequence labeling nature of NER with\nfew-shot learning paradigms. FewTopNER builds on these advances by integrating topic modeling into the few-shot\nframework, offering mutual benefits for both entity recognition and topic coherence. Our work thus represents a\nsignificant step towards more robust and efficient few-shot, cross-lingual NER systems."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Model Architecture Overview", "content": "FewTopNER introduces a novel approach to few-shot Named Entity Recognition (NER) that leverages topic-aware\ncontextual representations to enhance entity detection across multiple languages. The model's innovation lies in its"}, {"title": "3.2 Shared Encoder", "content": "The shared encoder represents the fundamental architectural component of FewTopNER, serving as a neural trans-\nformation mechanism that converts multilingual input sequences into semantically enriched vector representations\n[37]. At its core, the encoder leverages the pre-trained XLM-RoBERTa model, which has demonstrated state-of-\nthe-art performance in multilingual natural language processing tasks [38]. This architectural choice is motivated\nby XLM-ROBERTa's proven capacity to generate contextually nuanced embeddings that capture both universal lin-\nguistic patterns and language-specific nuances across diverse languages [39]. Through its multi-layered transformer\narchitecture, the shared encoder processes input sequences through self-attention mechanisms and feed-forward net-\nworks, enabling the model to capture complex syntactic and semantic relationships within and across languages [40].\nThe preservation of language-specific features is achieved through specialized calibration mechanisms, while simul-\ntaneously maintaining cross-lingual alignment in the shared representation space. This dual capability is crucial for\nfew-shot learning scenarios, where the model must effectively transfer knowledge across languages while retaining\nthe distinctive characteristics that make each language unique."}, {"title": "3.2.1 Initial Contextual Embeddings", "content": "The initial contextual embeddings phase comprises three critical components that establish the foundation for effective\nmultilingual representation learning. First, the input tokenization process employs XLM-ROBERTa's SentencePiece\ntokenizer, which implements a language-agnostic subword segmentation strategy. This tokenizer operates by decom-\nposing input sequences into atomic subword units through a learned vocabulary of 250,000 tokens, derived from a\nlarge-scale multilingual corpus. The tokenization process preserves morphological patterns across languages while\nmanaging out-of-vocabulary words through subword decomposition, thereby ensuring robust handling of morpholog-\nically rich languages and rare tokens. Following tokenization, the transformer layers of XLM-ROBERTa process the\ntoken embeddings through a sequence of 12 transformer blocks. Each block implements multi-head self-attention\nmechanisms with 12 attention heads, allowing the model to capture diverse aspects of syntactic and semantic relation-\nships simultaneously. The self-attention operation computes attention weights through scaled dot-product attention,"}, {"title": "3.2.2 Language-Specific Calibration", "content": "Language-specific calibration constitutes a critical component in FewTopNER's architecture, enabling effective cross-\nlingual transfer while preserving language-specific characteristics. The calibration process operates through four inter-\nconnected mechanisms that work in concert to achieve optimal multilingual representations.\nThe cross-lingual attention mechanism serves as the primary means of aligning embeddings across different languages\n[41]. This mechanism extends the traditional transformer attention by introducing language-aware components. For\neach language l, we compute language-specific query ($Q_1$), key ($K_1$), and value ($V_1$) matrices through learned trans-\nformations:\n$Q_1 = W_Q H + b_Q, K_1 = W_K H + b_K, V_1 = W_V H + b_V$\nwhere H represents the input hidden states, and {$W_Q, W_K, W_V$} and {$b_Q, b_K, b_V$} are language-specific projec-\ntion matrices and bias terms, respectively.\nThe attention computation incorporates language-specific biases $\u03b1_l$ to account for linguistic variations:\n$Attention (Q_l, K_l, V_l) = softmax(\\frac{Q_l K_l^T}{\\sqrt{d_k}} + \u03b1_l)V_l$\nThe shared projection spaces are maintained through a constraint mechanism that enforces similarity between\nlanguage-specific projections while allowing for controlled deviation [42]:\n$L_{align} = ||W_Q W_Q^T - I||_F + ||W_K W_K^T - I||_F + ||W_V W_V^T - I||_F$\nwhere l and m represent different languages, and $|| ||_F$ denotes the Frobenius norm.\nTo further refine the representations, we employ language-specific adaptation networks [43]. These networks consist\nof feed-forward neural networks $F_l$ that apply language-specific transformations:\n$h_l = F_l(h) = W_1 ReLU(W_0 h + b_0) + b_1$\nwhere h represents the input embeddings, and {$W_0, W_1, b_0, b_1$} are learned parameters specific to language l. The\nadaptation networks are trained to optimize a combination of task-specific loss and cross-lingual alignment objectives:\n$L_{adapt} = L_{task} + L_{align}$\nThe positional encoding mechanism implements sinusoidal functions to maintain sequential information. For position\npos and dimension i, the encoding PE is computed as:\n$PE(pos, 2i) = sin(\\frac{pos}{10000^{2i/d_{model}}}), PE(pos, 2i+1) = cos(\\frac{pos}{10000^{2i/d_{model}}})$"}, {"title": "3.2.3 Contrastive Learning for Alignment", "content": "The contrastive learning module in FewTopNER implements a sophisticated approach to cross-lingual representation\nalignment, fundamentally enhancing the model's ability to transfer knowledge across languages in few-shot scenarios.\nThis module operates on the principle that semantically equivalent content should maintain similar representations in\nthe shared embedding space, regardless of the source language [44].\nThe alignment process employs a specialized contrastive loss function that operates on parallel sentences across dif-\nferent language pairs. For any given parallel sentence pair ($x_s, x_t$) in source and target languages, their respective\nembeddings ($e_s, e_t$) are computed through the shared encoder. The contrastive objective function is formulated as:\n$L_{contrast} = -log(\\frac{exp(sim(e_s, e_t)/\u03c4)}{\\sum_n exp(sim(e_s, e_n)/\u03c4)})$\nwhere $sim(\u00b7, \u00b7)$ denotes the cosine similarity between embeddings, \u03c4 represents a temperature parameter controlling\nthe sharpness of the distribution, and $e_n$ includes both the positive target embedding and a set of negative samples\ndrawn from other sentences in the batch. This formulation encourages the model to minimize the distance between\nparallel content while maintaining discriminative power for distinct semantic concepts.\nTo preserve language-specific characteristics while promoting cross-lingual alignment, we introduce a regularization\nterm that balances these competing objectives:\n$L_{reg} = L_{contrast} + L_{diversity}$\nThe diversity loss $L_{diversity}$ penalizes excessive homogenization of embeddings, ensuring that important linguistic nu-\nances are maintained:\n$L_{diversity} = max(0, \u03bc - \\frac{1}{N} \\sum_{i,j} ||e_i - e_j||^2)$\nwhere \u03bc represents a margin hyperparameter, and the summation operates over all pairs of embeddings within a mini-\nbatch. This term ensures that embeddings maintain sufficient distinctiveness to encode language-specific features\nwhile still achieving cross-lingual alignment.\nThe effectiveness of this contrastive learning approach is further enhanced through curriculum learning, where the\ncomplexity of alignment tasks gradually increases during training. Initially, the model focuses on aligning highly\nsimilar parallel sentences, progressively advancing to more challenging cases involving idiomatic expressions and\nculture-specific references. This curriculum is implemented through a dynamic sampling strategy [45]:\n$p(x_s, x_t) \\propto exp(-\u03b2 \u00b7 d(x_s, x_t))$\nwhere $d(\u00b7, \u00b7)$ measures the semantic distance between parallel sentences, and \u1e9e controls the sampling temperature\nthroughout training.\nThe integration of this contrastive learning module with the broader FewTopNER architecture creates a robust frame-\nwork for cross-lingual representation learning. The resulting embeddings demonstrate strong performance in few-\nshot scenarios, effectively leveraging knowledge across languages while maintaining the ability to capture language-\nspecific nuances essential for accurate named entity recognition."}, {"title": "3.3 Entity Recognition Branch", "content": "The entity recognition branch constitutes a crucial component of FewTopNER, implementing an innovative prototype-\nbased learning framework specifically designed to address the challenges inherent in low-resource named entity recog-\nnition scenarios. This architectural component advances beyond traditional sequence labeling approaches by introduc-\ning a meta-learning strategy that enables effective entity detection and classification with minimal annotated examples.\nThrough the integration of prototype networks with sophisticated feature extraction mechanisms, the branch learns\nto construct and maintain discriminative entity representations that generalize effectively across different entity types\nand linguistic contexts. At its theoretical foundation, the branch leverages the concept of prototypical networks, ex-\ntending their application to the sequential nature of named entity recognition tasks. Unlike conventional approaches\nthat require extensive labeled data to learn entity patterns, this framework operates on the principle of learning to\nlearn from few examples. It accomplishes this by maintaining a dynamic prototype space where entity representations\nare continuously refined through episodic training [46], allowing the model to capture essential entity characteristics\nwhile minimizing the dependency on large annotated datasets. The design of this branch is motivated by two key ob-\nservations in low-resource NER: first, that entity types often share common structural and contextual patterns across\nlanguages, and second, that effective few-shot learning requires the ability to rapidly adapt to new entity categories\nwhile maintaining stability on previously learned ones. These insights inform the implementation of a hierarchical\nfeature extraction pipeline that combines local contextual information with global semantic understanding, enabling\nrobust entity recognition even in scenarios where labeled data is scarce."}, {"title": "3.3.1 Entity Encoder", "content": "The Entity Encoder implements a sophisticated three-tier architecture designed to process and enhance token repre-\nsentations for effective named entity recognition. This component builds upon the initial embeddings provided by the\nshared encoder, introducing specialized layers that capture hierarchical linguistic patterns essential for entity detection\nand classification.\nThe first tier employs a Bidirectional Long Short-Term Memory (BiLSTM) network to model sequential dependencies.\nFor a given input sequence X = {$X_1,X_2,...,X_n$}, the BiLSTM processes the tokens in both forward and backward\ndirections, computing:\n$h_t^f = LSTM^f(x_t, h_{t-1}^f), h_t^b = LSTM^b (x_t, h_{t+1}^b)$\nThe concatenated output $h_t = [h_t^f; h_t^b]$ captures both preceding and following context for each token, enabling the\nmodel to understand entity boundaries and internal structure. This bidirectional processing is particularly crucial for\nidentifying entity spans that depend on both left and right context, such as person names or organizational titles.\nThe second tier introduces language-specific adapters that fine-tune the representations for each supported language.\nThese adapters implement a bottleneck architecture:\n$h_l = W_1 ReLU(W_0 h + b_0) + b_1$\nwhere {$W_0, W_1$} and {$b_0, b_1$} are language-specific parameters learned during training. The bottleneck design, with\n$W_0 \\in \\mathbb{R}^{d\u00d7r}$ and $W_1 \\in \\mathbb{R}^{r\u00d7d}$, where r < d, enforces efficient parameter usage while maintaining language-specific\ncustomization. This adaptation mechanism allows the model to account for language-specific entity formation patterns\nand contextual cues.\nThe third tier implements multi-head self-attention to aggregate token-level dependencies, computing attention scores:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$\nwith multiple attention heads operating in parallel:\n$MultiHead(H) = Concat(head_1, ..., head_l)W^O$\nwhere $head_i = Attention(HW_Q^i, HW_K^i, HW_V^i)$. This multi-head attention mechanism enables the model to cap-\nture different types of dependencies simultaneously, with each head potentially specializing in specific aspects of\nentity-related patterns. The resulting contextual representations incorporate both local sequential information from the\nBiLSTM and global dependencies from the attention mechanism.\nThe complete entity encoding process can be expressed as:\n$E(X) = MultiHead(Adapt_l (BiLSTM(X)))$\nwhere E(X) represents the final entity-aware representations that serve as input to the subsequent prototype learning\nstages. This hierarchical processing ensures that the encoder captures both the sequential nature of entity spans and the\ncomplex interdependencies between tokens, while maintaining language-specific adaptability crucial for cross-lingual\nscenarios."}, {"title": "3.3.2 Prototype Network", "content": "The Prototype Network constitutes a critical component of FewTopNER\u2019s entity recognition branch, implementing\na metric-based learning approach that enables effective few-shot adaptation to new entity types. This component\noperates on the principle that entity classes can be represented by prototypical vectors in a learned metric space, where\nclassification decisions are made based on distances to these prototypes [27] .\nFor each entity type e in the set of target entity classes E, the prototype computation process aggregates information\nfrom support set examples $S_e = {((x_1, y_1), . . . ,(x_k, y_k))}$ through a weighted averaging mechanism. The prototype $p_e$\nfor entity type e is computed as:\n$p_e = \\frac{\\sum_i \u03b1_i f_\u03b8(x_i)}{\\sum_i \u03b1_i}$\nwhere $f_\u03b8$ represents the entity encoder function parameterized by \u03b8, and $\u03b1_i$ are attention-based weights that deter mine the contribution of each support example. These weights are computed through a similarity-based attention\nmechanism:\n$\u03b1_i = softmax(f_\u03b8(x_i)^T W_a f_\u03b8(x_q))$\nwhere $x_q$ represents the query token being classified, and $W_a$ is a learnable attention matrix. This attention-weighted\nprototype computation ensures that the model can effectively handle noisy or ambiguous support examples while\ncapturing the most representative features of each entity type.\nThe distance-based classification mechanism operates in the metric space induced by the entity encoder. For a query\ntoken $x_q$ , the probability distribution over entity types is computed using a softmax over negative distances:\n$P(y = e|x_q) = \\frac{exp(\u2212d(f_\u03b8(x_q), p_e))}{\\sum_{e'} exp(\u2212d(f_\u03b8(x_q), p_{e'}))}$\nwhere d(\u00b7, \u00b7) represents the distance metric, typically implemented as Euclidean distance or cosine similarity. This for mulation ensures that the probability assignment is inversely proportional to the distance between the query embedding\nand each entity prototype.\nThe prototype memory bank introduces an efficient mechanism for maintaining and updating prototypes during both\ntraining and inference [29]. The memory bank M maintains a dynamic set of prototypes ${p_e}_{e\u2208E}$ along with their\nassociated statistics:\n$M = {(p_e, n_e, \u03c3_e)}_{e\u2208E}$\nwhere $n_e$ represents the number of examples seen for entity type e, and $\u03c3_e$ captures the variance of features around the\nprototype. The update mechanism for the memory bank follows an exponential moving average:\n$p_e^{new} = (1 \u2212 \u03b3)p_e + \u03b3p_e^{batch}, \u03c3_e^{new} = (1 \u2212 \u03b3)\u03c3_e + \u03b3\u03c3_e^{batch}$\nwhere \u03b3 is a momentum coefficient that controls the rate of prototype adaptation, and $p_e^{batch}$ and $\u03c3_e^{batch}$ are computed\nfrom the current batch of examples. This momentum-based updating ensures stable prototype evolution while allowing\nadaptation to new examples.\nThe integration of these three components\u2014prototype computation, distance-based classification, and the prototype\nmemory bank\u2014creates a robust framework for few-shot entity recognition. The system can rapidly adapt to new\nentity types while maintaining stable performance on previously learned categories, making it particularly effective in\nlow-resource scenarios where labeled data is scarce."}, {"title": "3.3.3 Conditional Random Fields (CRF)", "content": "The Conditional Random Fields layer represents the final component of the entity recognition branch, implementing\na structured prediction framework that enforces coherent entity labeling across entire sequences. Unlike token-level\nclassification approaches that make independent decisions for each token, the CRF layer explicitly models the interde pendencies between adjacent entity tags, capturing crucial sequential patterns in entity formation.\nThe CRF layer operates by defining a conditional probability distribution over the entire sequence of tags Y =\n{$y_1, y_2, . . . , y_n$} given the input sequence X = {$x_1, x_2, . . . , x_n$}:\n$P(Y |X) = \\frac{exp(score(X, Y ))}{Z(X)}$\nwhere score(X, Y ) combines both emission and transition scores:\n$score(X, Y ) = \\sum_i (E(y_i |x_i) + T (y_{i\u22121}, y_i))$"}, {"title": "3.4 Topic Modeling Branch", "content": "The topic modeling branch represents a novel fusion of classical statistical approaches and modern neural architectures,\ndesigned to extract and leverage document-level semantic information for enhanced named entity recognition. This\ncomponent advances beyond traditional topic modeling frameworks by integrating both probabilistic topic inference\nand neural representation learning, creating a synergistic system that captures semantic relationships at multiple levels\nof granularity. At its theoretical foundation, the branch builds upon the insight that entity recognition can be signif-\nicantly enhanced by understanding the broader thematic context in which entities appear. For instance, documents\ndiscussing technological innovations are more likely to contain company names and technical terms, while news arti-\ncles about sports events frequently mention athlete names and team organizations. This contextual awareness becomes\nparticularly crucial in few-shot scenarios, where limited training examples make it essential to leverage every avail-\nable signal for accurate entity classification [47]. The architecture implements a hybrid approach that combines the\ninterpretability and statistical rigor of traditional topic models with the representational power of neural networks. By\nprocessing document collections through both probabilistic topic inference and neural encoding pathways, the branch\nconstructs rich topic representations that capture both explicit thematic patterns and implicit semantic relationships.\nThese topic representations are then carefully integrated with token-level features, enabling the model to make entity\nrecognition decisions that are informed by both local syntactic patterns and global thematic context."}, {"title": "3.4.1 Topic Encoder", "content": "The Topic Encoder implements a sophisticated multi-stage architecture that combines classical probabilistic topic\nmodeling with modern neural approaches to create rich semantic representations. This hybrid design addresses the\nfundamental challenge of capturing both explicit thematic patterns and latent semantic relationships in multilingual\ndocuments.\nFor the Language-Specific LDA stage, we employ separate Latent Dirichlet Allocation models trained on monolingual\ncorpora for each supported language l. The probability of a document d generating a word w is formulated as:\n$P(w|d, l) = \\sum_k P(w/z_{k,l})P(z_k|d)$\nwhere $z_k$ represents the k-th topic, and the language-specific word distributions $P(w|z_{k, l})$ capture vocabulary patterns\nunique to each language. The document-topic distributions $\u03b8_d$ = P(z|d) are inferred using collapsed Gibbs sampling,\nproviding interpretable initial topic representations. These distributions are computed as:\n$\u03b8_d = \\frac{n_{d + \u03b1}}{\\sum_k n_{d,k + K\u03b1}}$"}, {"title": "3.4.2 Topic Prototype Network", "content": "The Topic Prototype Network extends the principles of metric-based few-shot learning to the domain of topic model-\ning, establishing a framework that allows rapid adaptation to new topics while maintaining semantic coherence [48].\nThis component operates in a specialized metric space where topic representations can be effectively compared and\nclassified.\nThe prototype representation mechanism implements a non-linear projection function that maps topic features into a\nshared embedding space where semantic relationships are preserved. For a given topic representation t, the projection\nis computed through a series of transformations:\n$\u03c6(t) = W_2 ReLU(W_1t + b_1) + b_2$\nwhere {$W_1, W_2,b_1, b_2$} are learnable parameters. This projected representation ensures that topics with similar se-\nmantic content cluster together in the embedding space. The prototype for each topic category c is then computed as\na weighted average of support set examples:\n$P_c = \\frac{\\sum_i \u03b1_i \u03c6(t_i)}{\\sum_i \u03b1_i}$\nThe attention weights $\u03b1_i$ are determined through a learned similarity function that considers both feature similarity\nand topic coherence:\n$\u03b1_i = softmax(s(\u03c6(t_i), \u03c6(t_q)) + \u039b_c(t_i))$\nwhere s(\u00b7, \u00b7) measures feature similarity, $t_q$ represents the query topic, $\u039b_c(\u00b7)$ evaluates topic coherence, and \u039b balances\nthese two factors.\nThe similarity metrics component employs a temperature-scaled cosine similarity function to compute classification\nprobabilities. For a query topic $t_q$, the probability of belonging to topic category c is:\n$P(c/t_q) = \\frac{exp(cos(\u03c6(t_q), P_c)/\u03c4)}{\\sum_{c'} exp(cos(\u03c6(t_q), P_{c'})/\u03c4)}$\nwhere is a learnable temperature parameter that controls the sharpness of the probability distribution. This for-mulation ensures that the model can make confident predictions when appropriate while maintaining uncertainty in\nambiguous cases."}, {"title": "3.5 Cross-Task Bridge", "content": "The Cross-Task Bridge represents a sophisticated architectural component that facilitates dynamic information ex-\nchange between named entity recognition and topic modeling processes. This bidirectional communication channel\nenables each branch to leverage complementary information from the other, enhancing both entity detection and topic\nunderstanding simultaneously."}, {"title": "3.5.1 Cross-Task Attention", "content": "The task-specific projection mechanism transforms features from both branches into compatible representation spaces\nwhile preserving their essential characteristics. For entity representations E \u2208 $\\mathbb{R}^{d_e}$ and topic representations T\u2208 $\\mathbb{R}^{d_t}$,\nthe projections are computed through learnable transformations:\n$E' = W_E E + b_E$\n$T' = W_T T + b_T$\nwhere $W_E, W_T \\in \\mathbb{R}^{d_s\u00d7d_e}$ and $b_E, b_T \\in \\mathbb{R}^{d_s}$ project both feature sets into a shared dimensionality $d_s$. These projec-\ntions are designed to maintain task-specific information while enabling meaningful cross-task comparisons.\nThe multi-head attention mechanism implements a sophisticated cross-task information exchange through multiple\nattention heads, each capturing different aspects of the relationship between entities and topics. For head h, the\nattention computation is formulated as:\n$A^h (E', T') = softmax(\\frac{( (E'W_Q^h)(T'W_K^h)^T )}{\\sqrt{d_k}}) T'W_V^h$\nwhere $W_Q^h, W_K^h, W_V^h$ are head-specific projection matrices. The multi-head output is computed as:\n$MultiHead(E', T') = Concat(A_1, ..., A_H)W^O$\nThis mechanism allows each head to focus on different semantic relationships between entities and topics, such as\ncontextual relevance, semantic similarity, or structural patterns.\nThe language awareness component introduces dynamic attention scaling based on language-specific features. For a\ngiven language l, the attention weights are adjusted through a language-dependent scaling factor:\n$\u03b1_l = \u03c3(W_l[h_l; g_l] + b)$\nwhere $h_l$ represents language-specific hidden states, $g_l$ captures global language characteristics, and o denotes a sig-\nmoid activation. The final attention computation incorporates these language-aware scaling factors:\n$Attention (E', T') = \u03b1_l \u2299 MultiHead(E', T')$"}, {"title": "3.6 Training Framework", "content": "FewTopNER employs a multi-faceted training strategy to balance task-specific and cross-task objectives."}, {"title": "3.6.1 Episode Construction", "content": "The Episode Construction mechanism implements a sophisticated sampling strategy that enables effective few-shot\nlearning across multiple languages and tasks. This component systematically organizes training data into episodic"}, {"title": "3.6.2 Loss Computation", "content": "The loss computation system implements a carefully balanced multi-objective optimization framework that guides\nthe model's learning across its interconnected components. This"}]}