{"title": "NOT ALL TOKENS ARE CREATED EQUAL: PERPLEXITY ATTENTION WEIGHTED NETWORKS FOR AI GENERATED TEXT DETECTION", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "abstract": "The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.", "sections": [{"title": "Introduction", "content": "The proliferation of large language models (LLMs) has ushered in a new era of text generation, where artificial intelligence can produce coherent, contextually relevant content with remarkable fluency. These advancements hold transformative potential across industries, yet they also raise serious concerns about the misuse of AI-generated text. The ability to massively produce misinformation or automatically solve school assignments underscore the urgent need for effective detection mechanisms. However, identifying AI-generated text is far from straightforward, particularly in scenarios involving unseen domains or unfamiliar generative models, or mixed texts produced by human-machine collaboration. The diversity in writing styles, prompt configurations, and the opaque nature of AI systems contribute to the complexity of this task.\nRecent research has explored various strategies for detecting AI-generated text, with approaches based on next-token distribution output emerging as promising. These outputs, which represent likelihood estimates calculated by LLMs during text generation, encapsulate knowledge derived from extensive pre-training on diverse corpora. They offer a theoretically grounded foundation for detection, leveraging the statistical properties of language encoded in LLMs. Despite their appeal, zero-shot methods that directly utilize these next-token distribution outputs have shown limited effectiveness and performance.\nOne of the limitations of current methods is their reliance on simple aggregation techniques, such as averaging next-token metrics across all tokens. This uniform treatment of tokens disregards the intrinsic differences in predictive complexity. For instance, predicting the completion of a word is often straightforward, while initiating a sentence, with its broader range of possible continuations, is inherently more challenging. The beginning of a text, unconditioned due to the unavailability of the generating prompt, is also naturally more random. Recognizing these nuances is critical to improving detection performance.\nTo address this gap, we propose the Perplexity Attention Weighted Network (PAWN), a novel approach that assigns dynamic weights to tokens based on their semantic, contextual and positional significance. PAWN leverages the semantic information encoded in the last hidden states of the LLM and positional indices to modulate the contribution of each token to a specific feature based on the next-token distribution metrics. Our method refines the aggregation process, resulting in more accurate and robust detection. Unlike zero-shot approaches, PAWN involves lightweight training, but it mitigates resource constraints by enabling the caching of hidden states and next-token distribution metrics on disk.\nEmpirical evaluations demonstrate that PAWN achieves competitive, and often superior, performance compared to fine-tuned LLM baselines. Notably, our model generalizes better to unseen domains and generative models, exhibiting consistent decision boundaries across distribution shifts and enhanced resilience to adversarial attacks. Furthermore, if the backbone LLM presents multilingual capabilities, PAWN showcases decent generalization to languages not seen during supervised training. These results highlight the potential of PAWN as a practical and resource-efficient solution for detecting AI-generated text.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose a novel detection framework, Perplexity Attention Weighted Network (PAWN), which dynamically weights next-token distribution metrics based on semantic information from the LLM's hidden states and positional information. This model:\nDespite not being zero-shot and requiring supervised training, has a very small number of training parameters (in the order of one million). The backbone is thus frozen and run in inference mode, and it also allows us to cache the hidden states and metrics on disk, significantly reducing resource requirements compared to fine-tuning large models.\nDemonstrated competitive or superior performance to the strongest baseline, that is, fine-tuned language models, in in-distribution detection.\nShowcased better generalization capabilities in unseen domains and with unfamiliar generative models, exhibiting more stable decision boundaries.\nAlthough still vulnerable to paraphrasing attacks, it achieved better overall robustness to adversarial attacks, mitigating vulnerabilities in AI-generated text detection.\nIf the backbone model has multilingual capabilities, PAWN achieves decent performance in languages not observed during supervised training.\n\u2022 By performing ablation studies with the different branches of the PAWN model, we study the effect of using semantic information from hidden states and next-token distribution metrics. As one might expect, we find that the former results in better performance in-distribution but worse generalization. By limiting the role of semantic information to weighting the metric-based features, PAWN achieves both great in-distribution performance and good generalization."}, {"title": "Related Work", "content": "This section reviews the state-of-the-art in the field of AI-generated text detection, highlighting key advances and challenges. We begin the discussion with the main challenges that researchers have found in this field. These challenges have shaped the most advanced datasets and benchmarks, recently developed, which we discuss next. We end the section with a classification of the main detection methods found in the literature."}, {"title": "Challenges", "content": "The detection of AI-generated text has been shown to face numerous challenges, preventing current detectors from being used reliably in real-world scenarios. Some of these challenges are as follows."}, {"title": "Datasets and Benchmarks", "content": "The foundation for effective AI-generated text detection lies in robust datasets and evaluation benchmarks. Although there is a wealth of work in this research line, it was not until very recent work that evaluation sets started to address the challenges detectors faced when applied in real-world scenarios. For example, the data from the Voight-Kampff Generative AI Authorship Verification Task at PAN and ELOQUENT 2024 and the TuringBench benchmark does not include information on the domain. The Ghostbuster dataset , HC3 and the GPT2 output dataset only include texts from a single source model. In this work, we focus on three modern datasets that address generalization to both new domains and new source models, and that contain a large number of examples.\nMAGE contains almost half a million examples, with a focus on quality and diversity. At the time of writing, MAGE contains the most diverse set of domains and source generative models. In addition, they standardize a set of eight testbeds to evaluate the detectors in different settings and run several existing baselines on them. They even include on testbed with samples attacked with paraphrasing.\nM4 is a slightly less diverse and smaller dataset of 122K examples. It also presents a decent range of domains and source generative models, but the best feature is the diversity of languages. M4 provides a multilingual set with examples from nine different languages.\nRAID is a massive dataset of 6 million examples from a wide range of domains and source generative models, although less varied than that of MAGE. Particularly interesting is their inclusion of multiple decoding strategies, as well as samples with and without repetition penalty. They also present examples with a large set of eleven adversarial attacks to evaluate the robustness of detectors."}, {"title": "Detection Methods", "content": "There is a large and growing body of work focused on detection methods for AI-generated text. We review some of the main methods that appear in the literature, breaking them into four main categories. Watermarking methods add hidden patterns to AI-generated text to make it identifiable. Zero-shot detectors rely on large language models (LLMs) pre-trained for next-token prediction to identify AI-generated content without additional training, using the next-token distributions of the text. Other approaches fine-tune language models, training them specifically for the task of detection. Finally, LLMs have also been prompted to detect whether a text is AI-generated, making it complete the given prompt with the answer."}, {"title": "Watermarking Technology", "content": "Watermarking technologies offer a proactive approach to text detection by embedding identifiable markers in AI-generated outputs. These methods are not strictly comparable to other types of detector, as they require the cooperation of the LLM provider.\nAn example of these methods is given by Kirchenbauer et al.. They proposed an algorithm that uses the hash of the last token and a random number generator to produce a list of red tokens that are banned or made unlikely to be selected. Thus, a text that has been generated with this algorithm can be detected because most of the tokens will be in the green list, that is, not in the red list. However, human text is expected to result in half of the tokens in each list. To avoid reducing the quality of the generated text, they applied a positive bias to the logits of tokens in the green list. In this way, sampling from very sharp distributions with low entropy is not greatly altered, and only high-entropy distributions with many available options as the next token are substantially modified.\nThe authors found that this method can be effectively attacked by using another LLM to paraphrase spans of the text. In fact, the robustness of the method to such attacks was studied by Kirchenbauer et al. in [14]. Other attacks have been pointed out in the literature. A simple example is to ask the LLM to produce an emoji after each generated token, removing them from the sequence afterwards. This effectively randomizes the red list on each token ."}, {"title": "Fine-Tuned LMS", "content": "As in any other text classification problems, fine-tuning pre-trained language models (LMs) has emerged as a powerful alternative. Encoder-only pre-trained models such as BERT [15], ROBERTa or XLM-ROBERTa appear in a large body of work , with OpenAI [20] even releasing their own ROBERTa large fine-tuned detector. Although simple, these methods very often provide the strongest detection baselines . Unfortunately, they also tend to suffer greatly in out-of-distribution domains and generative models ."}, {"title": "Zero-Shot Detectors", "content": "Many zero-shot detection methods have been proposed in prior work. These methods are attractive because they do not require fine-tuning of large language models, but only selecting a boundary between machine-generated and human-generated texts for some score or metric they produce. We review some of the main methods in this category, all of them employing the next-token distribution of some frozen LLM to calculate some metric for the text.\nGehrmann, Strobelt, and Rush proposed GLTR. They used the probability of the token that occurred next, the rank of that token, and the entropy of the next-token distribution to create a visualization tool for human detectors, showing different colors for each token depending on these metrics.\nMitchell et al. proposed DetectGPT. They applied local perturbations to the text using a different model such as T5 [23]. They mask some of the tokens and apply the mask-filling model to create perturbed versions of the text. Then, they use the average log-probability of both the original text and the sampled perturbed versions, subtracting the former with the mean of the latter.\nDetectGPT is very computationally expensive as it runs the paraphrasing model and detection LLM once per perturbed sample. To solve this, Bao et al. [24] presented FastDetectGPT. Their approach re-samples tokens independently of each other, using the same LLM as the one used for detection. In other words, after resampling one token, they do not regenerate the text that follows but keep all the following next-token distributions the same for sampling. This allows them to avoid multiple runs through the model. Despite the increased efficiency, they also seemed to increase the detection performance of the model.\nSu et al. proposed two methods, called DetectLLM-LRR and DetectLLM-NPR. The former leverages the ratio between the next-token log-probability and log-rank. The latter compares the average log-rank information with the mean across a range of perturbations of the text.\nHans et al. [26] proposed Binoculars. In their method, they use two very similar models (they must at least share the same tokenizer) called the observer and the performer, often the foundation and instruction-tuned versions of the same model. First, they used the observer model to calculate the average log-probability of the text. Next, they compute the average cross-entropy between the next-token distributions of the observer and performer models, which measures how surprising the output of one model is to the other. The final score is the ratio of these two metrics. This normalization is motivated by what they call the \u201ccapybara problem\". Without the prompt available to condition the text, the models will naturally assign higher perplexities depending on factors such as topic or style.\nDespite being very interesting ideas and not requiring supervised training, these detectors are often not the best performing ones (see e.g. or appendix A), falling behind fine-tuned LMs.\""}, {"title": "LLMs as Detectors", "content": "Large Language Models (LLMs) themselves can serve as detectors for AI-generated text, using their advanced understanding of linguistic patterns. In particular, we refer to methods that include the text in the prompt and ask the LLM whether it is machine-generated or human-generated. For example, Bhattacharjee and Liu used ChatGPT as detector, although it did not prove to be a reliable detector. Koike, Kaneko, and Okazaki further extend on this idea by using in-context learning (ICL), providing semantically similar labeled examples in the prompt. This methodology, together with their framework to produce adversarial examples, seems to improve the performance of the methods."}, {"title": "Methodology", "content": ""}, {"title": "Perplexity Attention Weighted Network (PAWN)", "content": "Like the zero-shot methods we reviewed in section 2.3.3, we use pretrained LLMs to obtain the next-token distributions from the given text. From these distributions, we generate five different metrics, measuring the likelihood of the next token and the randomness of the distribution.\nHowever, zero-shot methods often use a simple mean to aggregate different metrics. Our main addition is the use of semantic and positional information to weigh the relevance of each token. The reason for this is that some of the tokens are naturally easier or harder to predict. Consider two examples. First, completions of words are expected to be very easy for the LLM, as the possibilities are very reduced, and patterns are not too difficult. Second, the beginning of a text is always very unconditioned, making the first few tokens naturally very random. Thus, applying a simple aggregation where every token is equally weighted will add a lot of unnecessary noise. To address this, we used both semantic information from the last hidden states of the LLM and positional information to perform some filtering.\nFigure 1 shows the full diagram of our model. We explain it in four steps, both the computations and the rationale behind each design decision.\nLLM run. The texts initially go through a selected decoder-only LLM pre-trained for next-token prediction. This returns both the last hidden states and the logits of the next-token distribution.\nIn this work, we use openai-community/gpt2 and meta-llama/Llama-3.2-1B-Instruct, covering small and medium model sizes.\nComputing metric features. From the output logits we generate five different metrics that measure some aspect of the distribution, instead of using only the probability or log-probability of the next token. All of these extra metrics serve one of two purposes. The first is to get a metric that is closer to the Top-P or Top-K decoding algorithms. The second is measuring the randomness of the distribution. This helps in contextualizing the log-probability of the token, as it is not the same to have a low probability in a very random distribution where the probability mass is very distributed, than in a very sharp distribution with a single very likely token. But it is also useful as a metric itself: we expect human text to take the LLM to uncharted territory more often. The five metrics are listed below. They are also mathematically formulated for a sequence of tokens $t_1, ..., t_n$ of length N, and an LLM next-token distribution probability matrix $P \\in \\mathbb{R}^{N\\times V}$, with V being the size of the vocabulary.\n(i) Log-probability of the token that occurred next in the sequence. This metric is formulated as\n$M^{log-prob} = log P_{i,t_{i+1}}$.\n(ii) Entropy of the distribution. This metric measures the randomness in the distribution, that is, how spread the probability mass is across many tokens. In mathematical terms it is expressed as\n$M^{entropy} = \\sum_{j=1}^{V} P_{i,j} log P_{i,j}$.\n(iii) Maximum log-probability. Another measure of randomness, useful to compare with the log-probability of the token that occurred. If they are close, then the token was one of the most probable ones. This can be interesting in distributions where the mass is spread across a few tokens. For example, in the phrase \"My favorite color is\"the probability is distributed among the different colors. Mathematically:\n$M^{max-log-prob} = max_{j=1,...,V} log P_{i,j}$."}, {"title": "", "content": "(iv) Rank of the token that occurred next in the sequence. The quantile that the token that occurred next represented in the next-token distribution,. In other words, the minimum K parameter in Top-K decoding where the token could have been selected, divided by the size of the vocabulary. It can be expressed mathematically as\n$M^{rank} = rank (log P_{i,:, t_{i+1}}) /V$.\n(v) Sum of probabilities of tokens with higher or equal probability than the token that occurred. In other words, the minimum P parameter in Top-P decoding where the actual token is covered with its full probability. It is formulated as follows:\n$M^{top-P} = \\sum_{j=1,...,V;P_{i,j} \\geq P_{i,t_{i+1}}} P_{i,j}$\nThese metrics are processed by an MLP network to generate F different metric features.\nComputing weights. Next, the hidden states of the LLM and positional information are used to filter out and prioritize the different tokens. Instead of just using the corresponding hidden state, we also concatenate the hidden state of the next token. Both of these tokens are relevant when deciding which metrics to take into account and which to discard, and we found a slightly higher performance empirically.\nAs positional information, we plug in the index of each token, divided by a fixed maximum length to avoid very large numbers. Again, the rationale behind it is that the first tokens of the sequence are very unconditioned and will likely have a low log-probability, so we want the model to be able to take this into account, even if no hidden state feature accounts for it.\nAll of these features are processed by an MLP network, generating G feature logits, where G divides F. These new feature logits are converted to feature weights by applying a softmax operation across the length of the sequence.\nAggregating metrics. The G feature weights are broadcasted up to the dimension F. They are then used to compute a weighted sum of the metric features across the length of the sequence, summarizing the full text into a single feature vector.\nBy using the softmax function, we restrict the role of the hidden states to weighting, and prevent hidden state and positional information from appearing in the summary vector, at least directly. This is because any component of the metric features that is independent of the metrics, such as bias parameters, will be summed up to itself after being aggregated with weights summing up to one.\nProcessing summary vector. The final summary vector goes through a final MLP network to produce the logit or probability of the text being generated by AI."}, {"title": "Data", "content": "As discussed in section 2, our choice of datasets was mainly motivated by whether they addressed the challenges that detectors face when applied in the real world. Thus, our choices include texts from a variety of domains and source generative models. In some of them we also find different languages, decoding hyperparameters and adversarial attacks. In this section, we describe the three datasets we use in more detail, elaborating on their selection of models, domains and other features."}, {"title": "MAGE", "content": "The MAGE dataset is a large (half a million samples), comprehensive, and diverse dataset and benchmark. This is our main choice for testing and performing ablation studies, given the large number of domains and models covered, the openness of their data splits, the pre-selection of a number of testbeds to compare different models and the number of baselines they provide."}, {"title": "Domain selection.", "content": "A main corpus of data is provided, where human- and machine-generated texts are sourced from ten different domains: (i) opinion statements from the /r/ChangeMyView (CMV) Reddit subcommunity [31] (ii) and the Yelp dataset [32]; (iii) news articles from XSum [33] (iv) and TLDR_news (TLDR) [34]; (v) question answers from the ELI5 dataset [35]; (vi) stories from the Reddit WritingPrompts (WP) dataset [36] (vii) and the ROCStories Corpora (ROC) [37]; (viii) common sense reasoning from the HellaSwag dataset [38]; (ix) question answering from the SQUAD dataset [39]; (x) and scientific abstracts from SciXGen [40]. A separate corpus is also provided, containing samples from a complementary set of four domains: news CNN/DailyMail [41], dialogues from DialogSum [42], scientific answers from PubMedQA [43] and reviews from IMDb [44]."}, {"title": "M4/M4GT-Bench", "content": "The next choice is M4 , another modern dataset that covers several domains and models. They provide two separate sets, one with English texts only and one with texts from multiple languages. Although M4 is less varied in the English language and has fewer examples than MAGE (122K), we will be able to use it to test the ability to generalize to different datasets and languages."}, {"title": "Domain and language selection.", "content": "We find six different English domains, a smaller set than that of MAGE: (i) Wikipedia; (ii) Reddit ELI5 [35]; (iii) WikiHow [54]; (iv) PeerRead [55]; (v) arXiv abstract; (vi) and OUTFOX . However, they include more domains from eight extra languages: (i) Arabic (Wikipedia and news); (ii) Bulgarian (True & Fake News); (iii) Chinese (Baike/Web); (iv) Indonesian (id_newspapers_2018); (v) Russian (RuATD [56]); (vi) German (Wikipedia and news); (vii) Italian (CHANTE-it news); (viii) and Urdu (Urdu-news [57])."}, {"title": "Model selection.", "content": "The number of models included is also much smaller than in MAGE, with the monolingual set containing samples from (i) Davinci003; (ii) ChatGPT [58]; (iii) GPT4 ; (iv) Cohere [59]; (v) Dolly-v2 [60]; (vi) and BLOOMz [61]. The multilingual set includes samples from LLaMA-2 [62] and Jais in Italian and Arabic, respectively."}, {"title": "RAID", "content": "Our last choice is RAID [7], another set with a variety of domains and source models. It contains a total of six million examples, being the largest of the sets we used, although the number of source texts to sample from is still limited (13371). RAID is particularly interesting for their use of different decoding strategies and the inclusion of samples generated with repetition penalty. They also have a large selection of adversarial attacks. Unfortunately, the authors do not include results from baselines trained in the RAID training split. Instead, their baselines were trained on a different dataset each. The authors did not provide a testbed selection either."}, {"title": "Model selection.", "content": "The variety of selected models is somewhere between M4 and MAGE, as they include (i) Cohere [59] (Foundation and Chat); (ii) MPT-30B [64] (Foundation and Chat); (iii) Mistral-7B [62] (Foundation and Chat); (iv) ChatGPT [58]; (v) GPT-2 XL [29]; (vi) GPT-3 [65]; (vii) GPT-4 [53]; (viii) LLaMA 270B Chat [62]. The novelty of RAID is that they also use two different decoding strategies, greedy and sampling, as well as repetition penalty. These features are included in the dataset, and we use them to test the generalization of our model in new decoding settings."}, {"title": "Domain selection.", "content": "As for domains, they include factual knowledge from platforms like News [66] and Wikipedia [67], generalization and reasoning seen in Abstracts [68] and Recipes [69], creative and conversational abilities found in Reddit [70] and Poetry [71], and familiarity with specific media such as Books [72] and Reviews [44]."}, {"title": "Adversarial attacks.", "content": "Finally, the authors provide examples attacked with a variety of adversarial strategies:\n1. Synonym: Replace tokens with highly similar alternatives using BERT [15].\n2. Article Deletion: Remove articles such as 'the', 'a', 'an'.\n3. Alternative Spelling: Apply British spelling conventions.\n4. Add Paragraph: Insert \\n\\n between individual sentences to separate them.\n5. Upper-Lower Case Swap: Reverse the case of words (upper to lower and vice versa).\n6. Homoglyph: Replace characters with visually similar alternatives (e.g., replace \"e\" with similar character U+0435).\n7. Number: The digits are randomly shuffled in numbers.\n8. Paraphrase: Rephrase using a fine-tuned T5-11B model [5].\n9. Misspelling: Introduce common misspellings of words.\n10. Whitespace: Add extra spaces between characters.\n11. Zero-Width Space: Insert the zero-width space character (U+200B) between every other character."}, {"title": "Experiments", "content": ""}, {"title": "In-distribution performance and generalization to unseen domains and models on MAGE", "content": "Our main experiments are performed on the MAGE dataset . The authors provide a fixed set of settings to evaluate detectors. They generate six different testbeds from the main corpus. In the first four, detectors are tested in-distribution, that is, on the same domains and source models they were trained on:\nTB1: One domain & one source model. Training and testing are restricted to a single source model and domain. This is done for each of the ten domains, and the results are averaged. This testbed will not be used because of its ease.\nTB2: One source model & all domains. Training and testing are restricted to a single source model, but all domains are included. The results are obtained for each source model and averaged.\nTB3: All source model & one domain. Training and testing are restricted to a single domain, but all source models are included. The results are obtained for each domain and averaged.\nTB4: All source model & all domains. Training and testing are performed in all domains and source models.\nNext, we find two testbeds that evaluate the detectors out-of-distribution, that is, on unseen domains or source models. To do this, they proposed leave-one-out experiments across domains and source models."}, {"title": "Ablations", "content": "Next, we evaluate the relevance of our addition by comparing the performance of PAWN with that of the hidden state branch (HSFF) and the metrics branch (MPN). Table 4 shows these results. With both backbones, the PAWN model outperforms the alternatives in all settings with two exceptions. First, in the presence of paraphrasing attacks, where all models suffer greatly. Second, MPN has a slightly higher recall with the GPT2 backbone, but a smaller AUROC.\nAn interesting pattern emerges in the performance of the models in different settings. We observe that the hidden state branch is clearly the better alternative in the in-distribution fourth test. In unseen models, they perform relatively on-par, with HSFF still slightly ahead in terms of recall. However, in unseen domains (tests six and seven), HSFF falls far behind MPN.\nThis phenomenon is more apparent in the last two columns of table 4, where we show the variation in performance between each out-of-distribution testbed and the fourth testbed. MPN is the model that suffers the least, showing great generalization in terms of both AUROC and recall. HSFF is the one that suffers the most out-of-distribution on average. PAWN also suffers a bit more than MPN in new domains, with recall taking a larger hit, suggesting greater threshold variance across domains.\nWe theorize that using the next-token distribution of LLMs generalizes greatly, as the backbones are pre-trained in a massive dataset and not overfitted to specific domains. Using semantic information as in HSFF seems to overfit the training distribution and generalize worse, as we might expect given that they fit semantic information. PAWN is somewhere in the middle, since semantic information is only used to aggregate across tokens. This provides a greater ability to fit the model to in-distribution data while maintaining good generalization."}, {"title": "Evaluating ensembles as a way to improve generalization", "content": "In table 5 we show the results from running several ensembles of PAWN models, simply averaging the final logits. We obtain minor performance gains in the in-distribution setting, although more pronounced in terms of recall than in terms of AUROC. In the out-of-distribution setting with unseen domains and models we find no AUROC improvement, only macro-averaged recall ones. Ensembles do not appear to provide any meaningful improvement in the presence of paraphrasing attacks, which remain very challenging."}, {"title": "Generalization and multilingualism on M4", "content": "As explained earlier, we evaluated the PAWN model in two different settings in the M4 dataset . First, we compare models that are pre-trained on the main corpus of MAGE . Although M4 and MAGE may have some domains in common, all detectors are evaluated under the same conditions, and we still expect a difference in the data distributions. Second, we performed the full battery of experiments proposed in M4GTBench , modifying the unseen domain experiments to retain the binary classification task nature."}, {"title": "Models pre-trained on MAGE", "content": "Table 6 shows the M4 results for models pre-trained on the main corpus of MAGE. Let us discuss each section.\nNo fine-tuning. Consider first the English domains. In terms of AUROC, the best model is RoBERTa, with PAWN-LLaMA being a very close second. Overall, all models achieved a good AUROC and seem to generalize nicely to English texts. When looking at the macro-averaged recall, we observe that PAWN-LLaMA is the best with quite a margin, and PAWN-GPT2 is very close to RoBERTa. As in MAGE, we seem to find that threshold selection is more robust for PAWN models, even with worse discrimination capabilities, as measured by the AUROC.\nConsidering now non-English domains. we see that the only reliable detector is PAWN-LLaMA, with fairly good average results. Only Bulgarian and Russian texts result in low AUROC and recall scores. Some of the other models also achieve high scores for specific languages, but not on average. These results suggest that with a large backbone LLM that has been trained in multiple languages, PAWN is capable of generalizing to languages not seen during supervised training.\nOne epoch fine-tune on English domains. After fine-tuning in the English domains, all detectors seem to be able to fit the data very well, with the PAWN models having the best recall. Performance on other languages takes a hit in all cases, with PAWN-LLaMA remaining the only one better than random chance.\nOne epoch fine-tune on all domains. All detectors are capable of fitting the entire data distribution, including other languages, when trained in all domains. The best overall model is PAWN-LLaMA, but PAWN-GPT2 and ROBERTa are fairly close."}, {"title": "M4GTBench results", "content": "Table 7 shows the results for each of the experiments based on the M4GTBench benchmark. In the out-of-distribution domain tests we find that the strongest model overall is PAWN-GPT2. Although in terms of AUROC it is a close race, with RoBERTa surpassing PAWN-LLaMA, both PAWN models outperform in terms of accuracy and macro-averaged F1. Thus, we find again that decision boundaries seem to generalize better in PAWN. In the out-of-distribution language tests results are similar to those in the previous section, with PAWN-LLaMA generalizing nicely to other languages, although there is some performance degradation. PAWN-GPT2 does generalize to some of the languages, but still performs worse than XLM-ROBERTa. Finally, in out-of-distribution model tests the PAWN detectors outperform the baselines greatly."}, {"title": "Generalization to new decoding strategies and robustness to attacks on RAID", "content": "Table 8 shows the main results on the RAID dataset. This table reports the recall at 5% FPR by different categories of examples, depending on whether the model is open-source and instruction-tuned, the decoding strategy, and whether repetition penalty is used. As explained in the methodology, we compare models pre-trained on MAGE, and also fine-tune those models for one epoch on the training set of our own split of RAID. To address each factor individually, we include table 9, where we group the values by each factor (e.g., all greedy decoding results vs. all sampling decoding results), take the median values, and compute the difference. We remark that closed-source models are not included in the results without repetition penalty, as no corresponding counterparts are available.\nWithout fine-tuning, our models generalize slightly better than RoBERTa, and a lot better than Longformer, especially PAWN-LLaMA. Interestingly, in table 9 we find that our models are much more susceptible to the use of repetition penalty, and less susceptible to whether the model is instruction-tuned or not. One explanation for the first remark is that using repetition penalty tends to produce missing punctuation and stopwords at the end of the text, which results in very unexpected tokens in sharp distributions. This could easily mislead our models, which are based on these metrics.\nAfter a single epoch of fine-tuning, we see that all models score close to perfect, and are capable of adapting to the different factors without trouble.\nOne thing to note about these results is that the metric is not sensitive to threshold choice. This threshold is dynamically calculated to achieve the 5% FPR, and moreover, a different threshold is used for each domain. This hides the weakness we have been observing for fine-tuned LMs in other datasets.\nNext, we test the robustness of models"}]}