{"title": "I Could've Asked That: Reformulating Unanswerable Questions", "authors": ["Wenting Zhao", "Ge Gao", "Claire Cardie", "Alexander M. Rush"], "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate COULDASK, an evaluation benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on COULDASK. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the time, respectively. Error analysis shows that 62% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions. We publicly release the benchmark\u00b9 and the code to reproduce the experiments\u00b2.", "sections": [{"title": "Introduction", "content": "Applying large language models (LLMs) to perform question answering (QA) over documents, such as legal and medical texts, has become increasingly popular (Agrawal et al., 2022; Guha et al., 2023). However, users' limited knowledge of these documents often results in the formulation of unanswerable questions, whose assumptions either conflict with or cannot be verified with the information available in the documents. We will refer to these assumptions as presupposition errors.\u00b3 Gao et al. (2023) and Yu et al. (2023) found that around 30% of information-seeking questions written by users include presupposition errors. Research in the field has primarily focused on the detection of unanswerable questions (Rajpurkar et al., 2018; Tran et al., 2023; Hu et al., 2023) and providing explanations for why such questions cannot be answered (Yu et al., 2023; Kim et al., 2023). However, this goal is insufficient for fostering an effective interaction between users and LLMs. Identifying unanswerable questions only serves as a starting point in question reformulation; without additional guidance or feedback on how to rephrase the question, users, especially those unfamiliar with the content, might find themselves caught in a repetitive cycle of formulating questions. In a large-scale industrial experiment, Faustini et al. (2023) have shown that the practice of rewriting unanswerable questions users ask virtual assistants significantly enhances the experience for millions of users.\nThis work aims to improve the utility of QA systems by introducing a new task that requires both detecting unanswerable questions and generating questions closely related to the initial query and grounded in the document. Opting to generate a relevant question rather than a summary of related information emphasizes a user-centered approach. While producing a summary is more document-focused, formulating a relevant question targets understanding and predicting the user's intent, aligning the interaction more closely with the user's specific needs and queries. We provide an example of how to suggest such questions in Figure 1.\nAlthough generating relevant and grounded questions conditioned on initial queries offers greater utility to users, it remains a difficult task even for the best models in a few-shot setting.\nWe first characterize human-reformulated questions and describe several different strategies for updating questions to remove presupposition errors. Motivated by these strategies, we curate COULDASK, an evaluation benchmark for document-grounded QA that consists of a combination of existing and new datasets to study question reformulation in the presence of presupposition errors. We evaluate several prompting methods such as few-shot prompting and chain-of-thought prompting, employing state-of-the-art open-source and proprietary models. The results illustrate the limitations of existing models and prompting techniques in accurately detecting unanswerable questions and reformulating questions: the F1 scores for identifying unanswerable questions range from 41.16% to 67.82%, and success rates for reformulating questions range from 7.13% to 26.21%, depending on base models. Analysis shows that most of the unsuccessful reformulation come from rephrasing or repeating the original questions and that LLMs are worse at reformulating questions necessitating global edits compared to those solely requiring local edits."}, {"title": "Related Work", "content": "Several datasets have been proposed to study unanswerable questions. Rajpurkar et al. (2018) curate the first document-grounded QA dataset that features unanswerable questions. More recently, Yu et al. (2023) and Kim et al. (2023) have collected questions with presupposition errors from Google user queries and Reddit posts, respectively.\nHow to identify unanswerable questions, especially with off-the-shelf LLMs, has remained understudied. Kim et al. (2021) proposed to first extract presuppositions from a question and then perform natural language inference (NLI) to check for presupposition violations. However, this pipeline requires supervision. In practice, supervision is often not available; Kim et al. (2023) thus explore prompting large language models in the chain-of-thought style to identify unanswerable questions. However, the results remain unsatisfying; using GPT-3 only yields detection accuracy that is only slightly better than random guesses.\nFaustini et al. (2023) investigate unanswerable questions and their reformulations in the domain of spoken QA, focusing on issues stemming from disfluencies, grammatical errors, and awkward phrasing. We, however, study unanswerable questions arising from presupposition errors, which require a more profound semantic comprehension of both contexts and questions.\nUnanswerable questions are closely related to ambiguous questions (Min et al., 2020). While there has been extensive research into reformulating ambiguous questions (Rao and Daum\u00e9 III, 2018; White et al., 2021; Pyatkin et al., 2023; Majumder et al., 2021), the problem of reformulating unanswerable questions receives little attention. Strategies for rephrasing ambiguous questions often involve making questions more specific by mentioning precise entities or events (Min et al., 2020). In contrast, as we will show in Section 3, reformulating unanswerable questions necessitates a wide range of strategies.\nFinally, we discuss the connection between document-grounded QA and open-domain QA (Kim et al., 2023). Document-grounded QA is essentially open-domain QA with the correct document retrieved. Reformulating questions based on the identified document is a separate skill from retrieving the document. Therefore, question reformulation is an interesting task in itself."}, {"title": "Task: Question Reformulation", "content": "To assist users with question reformulation when reading unfamiliar documents, we define the following task. Given a document and a user question, the system must determine if the question is unanswerable. Upon identifying the unanswerable question, it must reformulate the question such that the new question is answerable by the document while remaining relevant to the original question."}, {"title": "The COULDASK Benchmark", "content": "Motivated by the need to reformulate questions both to rely on verified information and to avoid contradictions, we develop an evaluation benchmark. We consider two important challenges in constructing benchmarks for this task. (1) The benchmark should cover a wide range of domains to cover different types of presuppositions. (Existing QA datasets that study unanswerable questions mostly rely on Wikipedia articles (Rajpurkar et al., 2018; Gao et al., 2023; Kim et al., 2023).) (2) The evaluation method should be capable of fairly assessing equally good reformulated questions, considering the subjective nature of question reformulation."}, {"title": "Datasets", "content": "Following the desiderata, we select three existing datasets - SQuADv2, BanditQA (Gao et al., 2023), and QA2 (Kim et al., 2023) \u2013 and to cover a broader range of domains, we create three new datasets in the domains of news, review, and Reddit, where the questions are generated by models and verified by crowdworkers. We summarize statistics for all datasets in Table 2.\nBBC, Reddit, and Yelp. We synthetically generate questions with a question generation model."}, {"title": "Evaluating Reformulation", "content": "To improve the utility of QA systems in responding to unanswerable questions, the reformulated questions must be (1) answerable by the documents and (2) relevant to the original questions posed by the users. As illustrated by the examples in Table 3, a reformulation could be answerable but not relevant, or relevant but not answerable. A successful reformulation must satisfy both conditions. There are multiple equally good reformulations for each question. For instance, when given the original question, it is hard to determine which of the first two reformulations in Table 3 would be closer to the user's intent. As a result, we opt for a reference-free evaluation approach, where the evaluation does not rely on gold reformulations.\nMeasuring the relatedness of two questions involves determining how closely their topics, intents, and meanings are aligned. With these goals in mind, we propose three reference-free relevance metrics: edit distance, entity overlap ratio, and cosine similarity between the original question and"}, {"title": "Experimental Setup", "content": "Models. We test a range of instruction-finetuned LLMs. For proprietary models, we consider GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-0613). For open-source models, we consider a list of 7-billion-parameter models: Llama2 (Touvron et al., 2023), Mistral (Jiang et al., 2023), and Zephyr (Tunstall et al., 2023).\nComparisons. We consider several prompting approaches: zero-shot (ZS) and few-shot (FS) prompting and ZS and FS chain-of-thought (CoT) prompting. We first prompt LLMs to determine whether the input question cannot be answered by the provided document. For ZS and FS prompting, we use the prompt provided by SurgeAI, which explicitly instructs the model to not produce an answer if the answer cannot be determined from the document alone. For ZS and FS CoT prompting, we expand the aforementioned prompt by asking the model to think step by step to come up with a reason to explain and support its decision. Only questions determined to be unanswerable proceed to the question reformulation stage. In this stage, all methods are provided with their previous turns where the models determine the questions are unanswerable. ZS and FS prompting instruct the model to make minimum edits to the original question to make it answerable. ZS and FS CoT prompting"}, {"title": "Results", "content": "Detecting Unanswerable Questions Being able to detect unanswerable questions is a necessary precondition for successful question reformulation. Table 4 presents the F1 scores for unanswerable question detection using different prompting approaches with each base model. Performance is often better for existing datasets than new ones, which indicates our approach's effectiveness in generating more challenging questions. Among all models, GPT-4 performs best at identifying unanswerable questions. Surprisingly, both Mistral and Zephyr are more accurate at detecting unanswerable questions than GPT-3.5. Among all prompting techniques, FS CoT consistently improves upon ZS, with a larger degree of improvement observed in smaller models compared to larger ones.\nReformulating Questions Table 5 presents success rates for question reformulation vary dramatically from domain to domain. News (BBC) appears to be the least challenging domain, with success rates ranging from 10.17 to 52.54 depending on base models. Reddit is a challenging domain, with success rates ranging from 4.42 to 14.16. The results on Wikipedia are mixed. While SQuADv2, QA2, and BanditQA are in the Wikipedia domain, LLMs achieve the lowest success rates on BanditQA. We hypothesize that user queries written during interaction require deeper revision.\nAmong all base models, GPT-4 achieves the highest average success rate (26.21), while GPT-3.5 has the lowest average success rate (7.13). Among all open-source models, Zephyr has the best performance. When it comes to prompting methods, there is not a clear winner. Different LLMs can be improved with different prompting approaches. GPT-3.5, Mistral, and Zephyr benefit the most from FS CoT prompting, GPT-4 from ZS, Llama2 from FS. We include individual metrics for question reformulation in Appendix B."}, {"title": "Analysis", "content": "Qualitative analysis We randomly sample and analyze 20 reformulated questions from each base model (a total of 100 questions) that cannot be answered by the corresponding documents. We summarize the results in Table 6. We identify three major types of errors. The most frequent type is that the models simply rephrase or generate the same questions. Most of the errors in this type are contributed by open-source models such as Llama2.\nAnother type of error that occurs 14% of the time is that the models generate a question by copying a document span that looks similar to the original question. For example, given the original question"}, {"title": "Limited benefit from FS CoT on Question Reformulation.", "content": "We explore why FS CoT improves the detection of unanswerable questions but not question reformulation by conducting a qualitative analysis of GPT-4 outputs on a BanditQA example, as illustrated in Figure 2. It is relatively straightforward to determine why a question is unanswerable either the document does not provide the necessary information or there is a presupposition that conflicts with the document. However, question reformulation demands compositional reasoning. The model needs to first decide on a reformulation strategy and then plan the specific steps to achieve the reformulation. The reformulated question generated by GPT-4 via FS CoT prompting is closer to being answerable. However, the model misses a subtle detail the document only mentions the exercise habits of 30.8% of residents, not the general population. Therefore, FS CoT alone, without further methodological innovation, does not fully address the challenge of question reformulation."}, {"title": "Compositional Modifications vs. Answerability.", "content": "We hypothesize that it is more difficult to reformulate an unanswerable question to be answerable when it requires compositional modifications, which means making global edits to the question instead of making local edits.\nTo quantify edits required, we follow Lee et al. (2020) and annotate the minimum span of a question to explain why the question cannot be answered by the document. We use GPT-4 to annotate unanswerable questions in BanditQA, QA2, and Yelp. We divide the questions into two categories."}, {"title": "Conclusion", "content": "Users often ask unanswerable questions when they seek information from unfamiliar documents. Existing LLMs identify these questions but do not aid users in reformulating new questions, resulting in ineffective user-LLM interactions. We introduce COULDASK, a compiled set of grounded-document QA datasets designed to study both unanswerable question detection and question reformulation."}, {"title": "Limitations", "content": "While our benchmark offers advantages over existing sources, we acknowledge the following limitations. Questions in BBC, Reddit, and Yelp are generated by GPT-4, and they may not accurately represent questions posed by humans. Despite best efforts to ensure high-quality annotations, occasional human errors are possible. Additionally, our benchmark only collects English questions and thus lacks language diversity. Finally, regarding evaluation, the way we currently measure success rates only focuses on mistakes made on unanswerable questions. If an answerable question is detected to be unanswerable, we do not evaluate question reformulation in such cases."}]}