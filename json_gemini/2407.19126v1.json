{"title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining", "authors": ["Jianwei Li", "Yijun Dong", "Qi Lei"], "abstract": "To remove redundant components of large language models (LLMs) without incurring significant computational costs, this work focuses on single-shot pruning without a retraining phase. We simplify the pruning process for Transformer-based LLMs by identifying a depth-2 pruning structure that functions independently. Additionally, we propose two inference-aware pruning criteria derived from the optimization perspective of output approximation, which outperforms traditional training-aware metrics such as gradient and Hessian. We also introduce a two-step reconstruction technique to mitigate pruning errors without model retraining. Experimental results demonstrate that our approach significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models.", "sections": [{"title": "Introduction", "content": "With the development of LLMs displaying emergent capabilities like sophisticated reasoning, the focus of the community has shifted to models with billions of parameters, for example, GPT-4 and Llama2 [1, 56]. This transition introduces unprecedented computational costs both in the training and the inference phases [55, 34, 52, 5]. To address this challenge, pruning plays a constructive role by removing redundant components from models, thereby reducing computational costs [22, 46, 59, 36]. Notably, designing an optimal pruning strategy is an NP-hard problem (as it reduces to subset selection) and requires balancing accuracy, sparsity, generalizability, pruning costs, and hardware compatibility in practice [61, 37, 12]. Traditional pruning methods primarily focus on accuracy and sparsity, often neglecting other key factors. They typically involve model retraining and knowledge distillation to mitigate pruning errors. However, with current LLMs featuring billions of parameters, the training process is already a significant challenge, making the additional cost of model retraining even more unaffordable [18, 19]. Given these challenges, there's a pressing need for more efficient pruning approaches.\nRecently, some works have focused on structured pruning on pre-trained LLMs, directly addressing hardware compatibility and generalizability. This approach allows them to concentrate on the remaining trade-off factors: sparsity, accuracy, and pruning cost. For example, methods like LLM-Pruner, Shortened LLaMA, and Sheared LLaMA use a single-shot pruning strategy that requires only one round of retraining [32, 60, 40]. On the other hand, strategies such as FLAP, OPTIN, Sliced GPT, LLM Surgeon, Wanda, ZipLM, and KRP seek to eliminate the need for model retraining entirely [2, 4, 57, 51, 33, 31, 37]. However, these approaches have respective limitations, such as high computational costs from the calculation of higher-order information, a lack of fully structured pruning patterns [45], or compromised performance in some cases. The development of these methods marks a crucial phase in the evolution of LLMs, aiming to enhance model capabilities while ensuring computational efficiency."}, {"title": "Related Work", "content": "Pruning and Structured Pruning: Pruning is a technique used in machine learning to reduce the size of a model by eliminating unnecessary parameters, which can lead to reductions in storage require-ments and computational complexity without significantly affecting the model's performance [18, 17]. This process involves identifying and removing the parts of the model that have the least impact on its output, such as weights in a neural network with small magnitudes. By doing so, pruning discovers a more efficient model that is faster to execute and easier to deploy on devices with limited resources. Structured pruning, a method that imposes more constraints, focuses on eliminating entire units or structures within the model, such as neurons, channels, or layers, instead of individual weights [3, 15]. Being the focus of our paper, structured pruning is especially advantageous due to its compatibility with standard hardware, whereas unstructured pruning requires specially designed accelerators to deploy in practical scenarios.\nData-free/dependent and Training/Inference-aware Metrics: When choosing which redundant components to remove from a model, the selection is typically guided by specific metrics [28]."}, {"title": "Methodology", "content": "This section outlines our structured pruning scheme, which consists of three key components: pruning structure recognition, pruning criteria definition, and post-pruning recovery strategy."}, {"title": "Pruning Structure Recognition", "content": "Our approach involves single-shot pruning and targets structured components, such as entire rows or columns of weight matrices, rather than individual weights. We do not discuss layer or block pruning, as it disrupts inherent model correlations and requires retraining to restore layer dependencies."}, {"title": "Input or Output Channel Pruning for Sequential Layers", "content": "To clarify structured pruning, it is important to understand that pruning neurons can be approached in two directions: input channels and output channels. Consider a linear function \\(f(X) = XW\\), where \\(X \\in \\mathbb{R}^{d_{in}}\\) is the input and \\(W \\in \\mathbb{R}^{d_{in} \\times d_{out}}\\) is the weight matrix. When we prune neurons, we typically refer to pruning the output channels of \\(W\\), since the number of neurons generally corresponds to the number of output channels in each layer. After pruning, the weight matrix becomes \\(W \\in \\mathbb{R}^{d_{in} \\times d'_{out}}\\), where \\(d'_{out} < d_{out}\\). Alternatively, pruning the input channels of \\(W\\) equates to pruning the input \\(X\\), also known as feature selection. This paper focuses on a static approach to feature selection, where the same channels are removed for all samples, making feature selection equivalent to output channel pruning in the previous layer. An interesting phenomenon arises: in depth-2 sequential linear layers, pruning the input channels of the second layer simultaneously pruning the output channels of the first layer, using identical pruning indices. In contrast, pruning the output channels of the second layer does not affect the first layer. Both input and output channel pruning contribute to model compression, but they may have different impacts on performance."}, {"title": "Input or Output Channel Pruning for Transformer", "content": "Depth-2 Module Identification: In Transformer-based language models, the attention and feed-forward modules function as sequential layers with a depth of 2. For the attention module, the first level includes the weight matrices \\(W_Q\\), \\(W_K\\), and \\(W_V\\), which operate in parallel. Pruning the input channels of any one of these matrices does not affect the output channels of the others. The second level consists of the weight matrix \\(W_O\\). The symbols \\(W_Q\\), \\(W_K\\), \\(W_V\\), and \\(W_O\\) represent the weights for the query, key, value, and output in the attention block, respectively. The feed-forward module follows a similar structure: the upward projection and gated projection occur at the first level, while the downward projection occurs at the second level. These depth-2 modules have a unique characteristic: when pruning the input channels of layers at the second level, the output channel indices of the first-level layers must correspondingly match. This ensures that the structural integrity of the model is maintained while pruning."}, {"title": "Pruning Strategies for Depth-2 Modules", "content": "Given these depth-2 modules, we can employ two pruning strategies to achieve the same compression ratio. The first strategy involves pruning the output channels of the layers in the first level while concurrently pruning the input channels of the layers in the second level. This approach ensures that the dependencies outside the module remain invariant. The second strategy involves pruning the output channels and the initial input \\(X\\) to the entire depth-2 module. In the context of the Transformer architecture, which consists of multiple such modules in sequence, pruning the initial input \\(X\\) is effectively equivalent to pruning the output channels of a preceding module in the sequence. As this dependency propagates backward through the layers, it ultimately affects the model's embedding layer, meaning we are directly pruning the channels of the token embeddings."}, {"title": "Challenge of Residual Connection", "content": "Without considering the loss of tokens' semantic informa-tion, the two pruning strategies described above should not differ significantly. However, residual connections impose substantial constraints on the second strategy. In the Transformer architecture, every depth-2 module determines residual connections. This means that the pruned channels must be strictly aligned across all modules. If the pruned indices of one of them do not align with others, it could lead to an unpredictable loss of information. This constraint severely limits the choice of channels for pruning and could significantly decrease performance. In contrast, the first strategy maintains a fixed number of output channels across these modules, avoiding this limitation. Each module can independently select which internal channels to prune based on its needs, resulting in a larger search space for optimization. This paper will focus on the first pruning strategy."}, {"title": "Additional Structure for Attention Mechanism", "content": "The intricate topology of the attention block introduces an additional constraint: pruning must be conducted at the level of entire heads, encompassing continuous portions of the channels. Fortunately, given the design philosophy of multi-head attention that each head is designed to capture correlations between tokens independently-this setup easily leads to redundancy, making it highly amenable to similarity analysis."}, {"title": "Pruning Criteria Selection", "content": "This section begins by examining different pruning criteria from an optimization perspective. Then, we introduce two specific pruning metrics for the aforementioned depth-2 modules. Finally, an intuitive magnitude-based pruning method is employed to remove the least important channels."}, {"title": "Pruning Metric Analysis from an Optimization Perspective", "content": "Previous work has categorized pruning metrics based on their relationship with data, as discussed in Section 2. Diverging from these approaches, we analyze these metrics from an optimization perspective and describe in Fig. 1. Specifically, for a linear operation \\(f(X) = XW\\), our goal is to prune \\(W\\) while preserving the accuracy \\(f(X) \\approx Y\\). To minimize pruning error, we can approximate \\(W\\), \\(f(X)\\), and \\((f(X), Y)\\). We term these strategies as function approximation, output approximation, and objective approximation, respectively. Function approximation focuses on directly approximating \\(W\\), which is equivalent to approximating the function itself. Typical metrics in this category include the L1 and L2 norms of weights or neurons. Output approximation seeks to approximate the result of \\(XW\\). Known metrics in this category include contribution energy, the sensitivity of \\(f(X)\\) to deviations in \\(X\\), and the variance or similarity score of \\(f(X)\\). Objective approximation aims to directly approximate accuracy. This category encompasses metrics such as first-order or second-order information and regularization scores. However, this type of metric is computationally expensive as the optimization process involves backward propagation and calculation of the Hessian Matrix. By analyzing these strategies, this paper proposes two new metrics to guide the pruning of LLMs."}, {"title": "Similarity-based Metric for Attention Block", "content": "Previous research on pruning attention heads typically involves removing heads with the lowest importance scores. Surprisingly, our experiments indicate that random pruning also yields competitive results compared to magnitude-based pruning, especially when the pruning ratio is below 50%. Further experimentation with different random seeds, leading to various head indices for pruning, consistently produces comparable results. Notably, nearly all heads have been selected for removal at some point during this process, suggesting a potential oversight in our initial understanding. Recall that different attention heads are intended to independently capture correlations between tokens. Thus, it's common for similar information to be extracted across different heads. This observation prompted us to reconsider our strategy: we prioritize removing similar heads before eliminating those with the least importance score. By identifying and pruning heads that capture redundant information, we can optimize the model more effectively while preserving its performance.\nPrevious studies have conducted similarity analysis between neurons [13, 49, 14], examining the output differences across multiple samples to identify similar components. The redundant neurons are then removed, and the remaining neurons scale their weights or biases to minimize the impact of this removal. However, these methods are primarily effective in smaller neural networks, as the scaling technique struggles to handle the accumulated error across numerous layers. Fortunately, due to the parallelism and independence of attention heads, removing redundant heads does not lead to significant information loss that affects subsequent layers, thus eliminating the need for costly remedial operations. Based on this observation, we define a pairwise head divergence matrix \\(D \\in \\mathbb{R}^{h \\times h}\\) for each attention module, where \\(h\\) refers to the number of heads. Specifically, given an attention score matrix \\(Attn \\in \\mathbb{R}^{N \\times h \\times s \\times s}\\), where \\(N\\) represents the number of samples and \\(s\\) is the sequence length, let \\(P_i \\in \\mathbb{R}^{N \\times s \\times s}\\) and \\(Q_j \\in \\mathbb{R}^{N \\times s \\times s}\\) denote the attention scores of heads \\(h_i\\) and \\(h_j\\), respectively. Then \\(D(P_i || Q_j)\\) is calculated as:\n\\(D(P_i || Q_j) = \\frac{1}{N \\times s} \\sum_{n=1}^{N} D_{KL}(P^{(n)}_i || M^{(n)}_{ij}) + D_{KL}(Q^{(n)}_j || M^{(n)}_{ij})\\)\nwhere \\(M^{(n)}_{ij} = \\frac{1}{2} (P^{(n)}_i + Q^{(n)}_j)\\), \\(D_{KL}\\) denotes the Kullback-Leibler Divergence, and \\(D_{ij}\\) represents the average Jensen-Shannon divergence between heads \\(h_i\\) and \\(h_j\\) across the dataset. By visualizing the attention heads as graph nodes and connecting nodes with a divergence less than a predefined threshold via an edge, we can clearly illustrate the relationships between these heads. Fig. 3 demonstrates that some heads fall into the same group, signaling information redundancy, whereas others stand alone, highlighting the uniqueness of their information. We also observe that specific layers form large groups, indicating higher redundancy. The details of our pruning strategy for the attention module are outlined in Algo 1."}, {"title": "Second-moment-based Metric for Depth-2 Module", "content": null}, {"title": "Pre-Pruning Recovery Without Retraining", "content": "With the selection of the pruning structure and criteria, this paper proposes a module-wise pruning approach. Similar to layer-wise pruning, we prune these depth-2 modules sequentially. To prune one of them, we calculate importance scores for its inner channels based on the module's structure, weights, and inputs. Notably, due to errors introduced by pruning preceding modules, the input to the current module inevitably deviates from its dense version. Consequently, even without pruning the current module, a discrepancy between its output and the original output is unavoidable. Recall that our design philosophy is to approximate the output as closely as possible. Thus, it is crucial to reconstruct the weights of the current module before pruning. This reconstruction ensures that the output of this module can still align as closely as possible with the original, even with the new input. This way, the pruning criteria for each channel can be optimally up-to-date. Inspired by [37], this paper presents a pre-pruning recovery technique in Algo 2."}, {"title": "Experiment", "content": "This section initially presents the fundamental setup for our experiments. Subsequently, we demonstrate the results of experiments and provide an in-depth analysis from multiple perspectives."}, {"title": "Setup", "content": "Baselines: This paper presents a comprehensive comparison of state-of-the-art pruning methods across multiple dimensions, aiming for fair evaluations and in-depth analyses to uncover the reasons behind the observed results. First, we compare our approach with data-free pruning methods, including random pruning and magnitude-based pruning (L1 and L2 norms) [28]. Next, we evaluate our methods against data-dependent pruning techniques, encompassing training-aware, inference-aware, and retraining-required methods. In the training-aware category, we compare with various configurations of LLM-Pruner [40], such as Element1, Element2, and Vector-wise magnitude pruning. Within the inference-aware category, we compare with the structured version of Wanda [51] and FLAP [2]. Additionally, we extend our comparisons to include the LLM-Pruner method augmented with retraining. Such comprehensive evaluations will demonstrate the effectiveness of our pruning approach.\nModels: Our primary experiments are categorized into two series based on the model scale: LLaMA-7B with 7 billion parameters and GPT-2 with 110 million parameters [47, 55]. This aligns with our study's goal to assess pruning performance across different model sizes and ensure a thorough examination. Additionally, we extend our experiments to other models, including LLaMA-13B, Vicuna-7B [7]. This comprehensive selection allows us to explore a broader spectrum of capabilities and sizes, enhancing our understanding of how different architectures perform under various computational constraints. Additional experiment results can be found in the Appendix.\nEvaluation and Datasets: To evaluate performance, we adopt LLaMa's approach by conducting zero-shot task classification on a range of common sense reasoning datasets: BoolQ [8], PIQA [6], HellaSwag [63], WinoGrande [48], ARC-easy [9], ARC-challenge [9], and OpenbookQA [43]. Following the methodology in [21], the model either ranks the options in multiple-choice tasks or generates answers in open-ended formats. Additionally, we enhance our evaluation by conducting a zero-shot perplexity (PPL) analysis on WikiText2 [42] and the Penn Treebank (PTB) [41]."}, {"title": "Results and Analysis", "content": "We present the main results in Tab. 1. For the data-free comparison experiments, we leverage the inherent ability of LLMs to generate sentences. Our pruning method uses these generated sentences as calibration data because, given that the LLMs are well-trained, these sentences naturally conform to the semantic and syntactic token distributions of the training data. Compared to traditional data-free metrics (L1 or L2), our data-free version, which relies solely on the model itself, achieves significant improvements in perplexity and up to a 20% enhancement in zero-shot evaluation for downstream tasks. Moreover, our method surpasses random pruning by at least 6%, a significant improvement achieved without relying on existing datasets, while traditional metrics (L1 or L2) fail to outperform. These results demonstrate the superiority of our techniques in data-free pruning methods.\nOur approach outperforms data-dependent pruning methods and the inference-only method Wanda-SP. Impressively, it also surpasses the state-of-the-art training-aware pruning method LLM-Pruner, which includes different configurations such as Element1, Element2, and Vector. Our approach consistently demonstrates better pruning results without requiring computationally intensive first-order and second-order information. Moreover, our method even achieves better results compared to LLM-Pruner with LoRA, despite the latter involving model retraining.\nWe also compare our method with the state-of-the-art inference-only method FLAP and present the results in Tab. 3. Our approach exhibits significantly better results on the GPT-2 model and achieves comparable performance with LLaMA-7B. Overall, our method demonstrates superior performance in both data-free and data-dependent pruning categories."}, {"title": "Ablation Study", "content": "We also explore our pruning metrics by exclusively pruning attention heads. The experimental results in Tab. 2 demonstrate that for colossal LLMs like LLaMA-7B, our similarity analysis effectively identifies redundant attention heads with minimal negative impact on model performance. Compared to inference-aware metrics such as the L2 norm, training-aware metrics using first- and second-order information, and random pruning, our similarity-based metric consistently outperforms. When compared to the specifically designed metric of FLAP, we achieve better or comparable performance. These results strongly indicate that we should prioritize pruning redundant information rather than heads with small importance scores.\nAdditionally, we designed experiments to explore the influence of the number of calibration samples. Figure 4 shows that in LLaMA-7B pruning-only experiments, our method is insensitive to the number of calibration samples, achieving comparable results with as few as 8 samples and as many as 128 samples. Conversely, in GPT-2 pruning with remediation experiments, performance improves with an increasing number of calibration samples. These findings demonstrate that our pruning method is robust regardless of the number of calibration samples, while our pre-pruning recovery method benefits from a higher number of calibration samples. However, this improvement gradually diminishes once the number of samples reaches a critical threshold."}, {"title": "Discussion, Limitation, and Conclusion", "content": "Implicit Motivation and Call: In the pre-deep learning era, various pruning metrics and structures were designed. For example, variance-based pruning and bias-based remedy methods similar to FLAP were proposed by researchers 30 years ago [14, 49, 13, 54]. These early researchers already recognized that feature information is at least as crucial as model weights in constructing pruning criteria. In the early stages of deep learning (before 2022), many researchers found that multi-round model retraining could easily recover the lost performance induced by pruning, even when based solely on weight magnitudes. As a result, the importance of pruning metrics and structure design was often overlooked, with reliance placed on retraining to validate methods. However, this paradigm shifted after 2022, when colossal LLMs became mainstream in the community. Training such models is prohibitively expensive, making pruning that relies on multi-round retraining impractical. Although parameter-efficient training methods like LoRA can reduce costs, they still require rigorous data selection [29, 40]. Thus, we urge the community to return to designing metrics that better account for the influence of both weights and features, rather than focusing solely on dataset competition. Motivated by this, this paper focuses on inference-aware pruning metrics that do not require retraining.\nLimitation: This work evaluates the compressed LLMs primarily on perplexity and downstream tasks. However, we do not assess the emergent abilities of colossal LLMs, such as mathematical reasoning, safety alignment, common sense reasoning, contextual understanding, and creativity in text generation. Future research will focus on evaluating and enhancing these emergent abilities to provide a more comprehensive understanding of the compressed LLMs.\nConclusion: This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."}, {"title": "Appendix-A", "content": "Pruning is a promising method that can effectively reduce model inference costs. In this paper, we discuss general pruning methods and various classification philosophies. We summarize previous work and categorize pruning from multiple perspectives: structured and unstructured, data-free and data-dependent, training-aware and inference-aware, and retraining-free and retraining-dependent. We also propose an innovative optimization-oriented view of pruning, which includes: A: Function Approximation, B: Output Approximation, and C: Objective Approximation. Our pruning pattern is designed based on this perspective.\nAdditionally, we review more aspects of pruning, including the most popular techniques in the pre-deep learning era (before 2022), such as Iterative Magnitude Pruning and the comparison between random pruning and magnitude pruning. We also discussed the relationship between pruning and quantization. By considering these various dimensions and methodologies, we aim to provide a comprehensive understanding of pruning and its potential to enhance model efficiency."}, {"title": "Iterative Magnitude Pruning", "content": "Iterative Magnitude Pruning (IMP) is the most renowned strategy for achieving state-of-the-art results, surpassing other methods such as Single-shot Network Pruning (SNIP) [16, 17, 18, 35]. This approach divides the pruning process into multiple stages by gradually increasing the sparsity. At each stage, the goal is to identify and remove redundant parameters or neurons. The most intuitive approach is to assign an importance score to each element and keep only the top-k elements, where the score can be based on the absolute value of weights, output sensitivity, gradients, or other fine-designed metrics [23, 20, 53, 26, 64, 10, 39]. Weight magnitude is the most straightforward and data-free method, while other metrics can be computationally expensive as they require training with data [62, 27, 38, 44, 50, 11]. Moreover, IMP is accompanied by a retraining phase to restore performance, which can be computationally costly. Therefore, in the era of colossal LLMs, IMP and other methods that heavily depend on model retraining are no longer effective due to the immense costs involved."}, {"title": "Randomized Pruning v.s. Magnitude Pruning", "content": "Excluding the influence of model retraining, we discovered an interesting phenomenon for model pruning. For colossal LLMs such as LLaMA-7B, randomized pruning surprisingly produced competitive results. Specifically, compared to traditional data-free pruning metrics like L1 and L2 norm values, randomized pruning achieved several times better results, even rivaling data-dependent pruning methods. However, this advantage only existed when the pruning ratio was less than 2x. As the pruning ratio increased, magnitude pruning gradually yielded better results. Initially, we attributed this phenomenon to the high redundancy of parameters in LLMs. However, our experiments with GPT-2 showed that randomized pruning was still weaker than magnitude pruning. Therefore, we speculate that for colossal LLMs like Llama-7B, feature information plays a more crucial role in model activations compared to smaller LLMs like GPT-2.\nMagnitude-based pruning methods aim to remove weights or neurons from a neural network that appear least influential, primarily determined by the value of their weights. The rationale behind these methods is to reduce overall model size and computational requirements without a drastic loss in performance. However, several challenges arise with this approach, and one major challenge is the lack of variety if the magnitude is based on data-free metrics (L1 or L2). This kind of metric focuses solely on the magnitude of the weights for pruning decisions, potentially missing smaller weights that play pivotal roles, especially in edge cases or rarer instances.\nTo illustrate this more clearly, consider the following example. The output of a neural network can be represented as \\(y = \\sum(w_i \\cdot f_i)\\), where \\(y\\) is the network output, \\(f_i\\) represents a feature, and \\(w_i\\) is the corresponding weight. In magnitude-based pruning (L1 or L2), if \\(|w_i| < \\tau\\) (\\(\\tau\\) is pruning threshold), then \\(w_i \\cdot f_i\\) is pruned. However, the impact on \\(y\\) is not solely determined by \\(w_i\\), but by the combined effect of \\(w_i\\) and the sensitivity of \\(f_i\\). For instance, if \\(f_i\\) represents the sharpness of an image, even a small weight \\(|w_i| = 0.01\\) can significantly affect \\(y\\) if \\(f_i\\) is highly sensitive, such as affecting object recognition. Conversely, if \\(f_i\\) represents the hue of an image background, a large weight \\(w_i = 5\\) might have minimal impact on \\(y\\) if \\(f_i\\) is less sensitive, such as the background hue not altering recognition much. The influence on \\(y\\) is thus a joint effect of \\(w_i\\) and the sensitivity of \\(f_i\\). This example indicates that the influence of feature information plays a significant role in identifying redundant elements."}, {"title": "Pruning v.s. Quantization", "content": "Pruning, though considered less effective than quantization in the era of colossal LLMs, should not be underestimated. In practice, pruning and quantization can complement each other, yielding significant benefits when applied together [18]. Even pruning a small percentage of parameters, such as 5%, can be valuable if it meets practical performance requirements. Therefore, integrating pruning into the optimization process is always worthwhile."}, {"title": "Appendix-B", "content": "In Section 3.1.2, we introduced the 2nd moment-based pruning metric for a standard depth-2 module. However, there are different variants of depth-2 modules, including the attention module and the gated feed-forward module. We describe the calculation of the metric for these variants in the following section.\nNotations: To better demonstrate our method, let us first establish the notations. We focus on the pruning of Transformer-based large language models, thus we refer to the attention mechanism as \\(Catn_i[01(XW^{iK}W^{iQ}X^T)XW^{iV}]W^O\\), with i indicating the attention head index. The symbols \\(W^{iK}\\), \\(W^{iQ}\\), \\(W^{iV}\\) and \\(W^O\\) represent the weights for the key, query, value, and output in the attention block, respectively. For the general and gated feed-forward module, we denote the logic as \\(W^D \\sigma (W^U X)\\) and \\(W^D (\\sigma(W^U X) \\cdot  \\sigma(W^G X))\\). Here, \\(W^U\\), \\(W^D\\), and \\(W^G\\) stand for the weights for upward projection, downward projection, and gate projection. \\( \\sigma \\) refers to the activation function for all of them, which can be SoftMax, ReLU, GeLU, or SiLU function."}, {"title": "Calculation of 2nd-Moment-Based Metric for Attention Module", "content": "Based on the above notations, we can treat the entire output of \\(\\sigma_1(XW^{iK}W^{iQ}X^T)X\\) as the input to \\(W^i\\). Let X represent this new input. In this way, the attention module can be viewed as a module similar to a standard depth-2 module (XW), with each level having only one linear layer. However, we need to view an attention module as m standard depth-2 modules (m is the number of attention heads), as the attention heads operate independently."}, {"title": "Calculation of 2nd-Moment-Based Metric for Gate Feed-forward Module", "content": "For the gated feedforward module \\(W^D(\\sigma(W^U X) \\cdot  \\sigma(W^G X))\\), we treat it as a product of two standard depth-2 modules. Specifically, we can divide it into two modules: \\(W^D \\sigma(W^U X)\\) and \\(W^D \\sigma(W^G X)\\), and calculate the 2nd-moment metric separately for them. Finally, we use the product of their own metric as the 2nd-moment metric for the entire module."}, {"title": "Appendix-D: Broader Impact", "content": "Our proposed method efficiently prunes large language models with billions of parameters. Our proposal intends to mitigate AI risks from critical perspectives like economic inequality and concentration of power and further democratize the use of AI models."}]}