{"title": "From tools to thieves: Measuring and understanding public perceptions of Al through crowdsourced metaphors", "authors": ["MYRA CHENG", "ANGELA Y. LEE", "KRISTINA RAPUANO", "KATE NIEDERHOFFER", "ALEX LIEBSCHER", "JEFFREY HANCOCK"], "abstract": "How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view Al as warm and competent, and that over the past year, perceptions of Al's human-likeness and warmth have significantly increased (+34%, r = 0.80, p < 0.01; +41%, r = 0.62, p < 0.05). Furthermore, these implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI (r2 = 0.21, 0.18, p < 0.001). We further explore how differences in metaphors and implicit perceptions-such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI-shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.", "sections": [{"title": "1 Introduction", "content": "Advances in large language models (LLMs) have catalyzed public interest in artificial intelligence (AI) [11, 18, 29, 33, 54, 58, 66], with over 90% of Americans having heard of AI [39] and tools like ChatGPT now receiving over one billion queries per day [103]. Narratives of AI have been shaped by both hope and fear as well as trust and skepticism of its societal impacts [49, 107]. While some view AI as a transformative tool for productivity [37, 56, 111] or as a trusted assistant for personal and professional tasks [31], others see it as a threat to their livelihoods [74]. Perceptions of AI are important determinants of individual behavior, industry-level changes, and community support for regulatory measures [48]. For example, an individual's decision to use AI to answer their questions, process their emotions, or assist them with their work is highly dependent on their perception of the system's capabilities [71]. Decisions to regulate AI are also fundamentally shaped by perceptions about these systems [120]. Understanding public perceptions of AI is therefore integral to grasping Al's societal impacts.\nPerceptions about new technologies, however, can be difficult to capture using traditional survey measures [14, 45, 67, 68]. People often struggle to verbalize their nuanced perceptions of complex sociotechnical systems like AI, particularly when they have less experience with them [78]. To overcome these limitations, we study public perceptions of AI by analyzing the metaphors they use to describe AI [32]. People have long used metaphors to communicate complex ideas, like the turbulence of emotions (\u201crollercoaster of feelings", "data is the new oil": [76], "AI is a tool\", \\\"AI is a thief\") using a combination of automatic clustering and qualitative refinement. Beyond the explicit descriptions of the metaphors, we also demonstrate how these metaphors provide a window into people's implicit perceptions of Al. We measure implicit perceptions from the metaphors in two ways": 1, "AI": "Anthropomorphism, or the attribution of human-like qualities to AI, has come to the fore of discussions around societal impacts of AI [19, 105, 108, 115] as scholars have highlighted the consequences of users becoming overly dependent on human-like AI [1, 16, 23, 63] and failing to consider the implications of disclosing sensitive information [26, 64]. Warmth and competence are integral dimensions of how people form impressions of social actors [40, 93]. The extent to which people believe that others have warm, positive intentions and are capable of carrying out given tasks are strong predictors of people's attitudes, such as their trust in new technologies and willingness to adopt them in the future [72, 79, 91, 127].\nUsing this framework to analyze our dataset, we find that while participants' metaphors are generally warm and competent, they vary widely in anthropomorphism (e.g., \"AI is a friend", "AI is a library": ".", "perceptions": "anthropomorphism and warmth have significantly increased by 34%** and 41%* respectively, over this time period.\nWe next demonstrate how dominant metaphors and implicit perceptions predict attitudes toward AI: warmth, competence, and anthropomorphism, as well as the dominant metaphors of \"god\u201d, \u201cbrain,", "thief\u201d, are the most predictive of trust and adoption (\"thief": "eing negatively predictive).\nWe also discover significant demographic differences in the dominant metaphors, implicit perceptions, and attitudes Americans have about AI. For instance, gender differences in metaphors about Al help provide insight into why women may trust it less than men [39]. Surprisingly, we also find that Americans of color and older Americans tend to trust AI more - indicating the importance of investigating how social identities shape human-AI interactions. These differences inform efforts to preemptively recognize and address adverse effects of AI on marginalized communities.\nSummary of Contributions. Our study provides a fine-grained understanding of how public perceptions of AI have emerged over time and across individuals. We collect a dataset of over 12,000 metaphors of AI over 12 months and use mixed methods to identify 20 dominant metaphors. We provide a scalable framework for measuring implicit perceptions from the metaphors. We use this framework to demonstrate how temporal shifts in metaphors and implicit perceptions correspond to changes in people's attitudes, and explore how demographic differences among these reveal harmful consequences of AI deployment. Our framework is quantitative and scalable, and thus can be easily adapted to new data to capture evolutions in public perception [38, 126]."}, {"title": "2 Background", "content": "2.1 Using metaphors to reveal implicit perceptions\nThe metaphors people use to describe AI can reveal implicit perceptions that may be difficult for them to verbalize explicitly [14, 45]. People often struggle to accurately articulate their specific attitudes and perceptions when asked to provide their responses to new technologies. For instance, individuals' self-reported perceptions of anthropomorphic qualities often fail to align with their implicit beliefs or behaviors-such as treating computers as social actors but not realizing that they perceive computers as human-like [128]. Thus, to access these underlying perceptions, researchers use methodologies such as asking participants to answer indirect questions or provide metaphors [32]. Similarly, we use metaphors to conduct a large-scale study of people's perceptions of AI.\n2.2 Why metaphors matter\nMetaphors shape how people understand information by distilling complex concepts into more accessible ideas [12, 76]. People use metaphors to make abstract ideas feel tangible by describing them using the language and properties of more familiar concepts [125]. The implicit associations invoked by these comparisons can have powerful effects by orienting people towards specific conceptualizations and behaviors. Decades of psychological research demonstrates that the metaphors people use to understand abstract concepts, like crime, illness, and intelligence, can unconsciously change their behaviors [57, 113]. For example, those who viewed local crime as a \"virus", "beast": "reying on their city; framing crime as a virus caused people to endorse rehabilitative policies over punitive measures [112, 116].\nBecause they help people make sense of unfamiliar concepts, metaphors have long been used to help people understand new technologies. Early metaphors of the Internet as a \"superhighway", "surfing the web": "hat suggests the Internet as a vehicle for exploration [84]. Similarly, understanding the metaphors that people use to describe AI may reveal how the public is responding to the proliferation of AI-based technologies, and how they understand the potential values and uses that AI technologies may afford. Already, clear differences are emerging in how people think about AI. For instance, some people describe Al is a powerful tool that should be integrated into the workforce [37, 56, 111]. Others view AI as an assistant or companion that can be trusted to process personal and professional challenges [31]. However, many people view AI to be a threat to their livelihoods, invoking fears of replacement and may therefore instead advocate for restrictions on AI [74]. The metaphors people use to describe Al provide insight into how people feel about using it in their own lives, in the context of their work, and in society at large.\n2.3 Implicit perceptions of Al\nPeople's implicit perceptions play a powerful role in shaping human interactions with technology. Even when people interact with the same system, they can interpret the qualities of the system in vastly different ways that affect their trust and engagement. Understanding these perceptual differences is vital for building and deploying AI responsibly. Because the inner workings of these systems are often \"black boxes", "framework": ""}, {"title": "2.4 Attitudes toward Al: Trust and adoption", "content": "We focus on two core aspects of public attitudes toward AI: trust in AI and willingness to adopt AI. First, these variables are important because they are necessary for effective and productive integration of AI into society [2]. Lack of trust and unwillingness to adopt AI may lead to under-use and missed opportunities to improve people's lives[25]. Moreover, it may result in public fear or backlash [80] that diverts attention toward unfounded, hypothetical risks while neglecting the real and ongoing harms that AI technologies already impose on various populations [22, 62, 114]. At the same time, it is also critical for users to not over-trust AI and understand its biases and limitations [119], as failing to do so can lead to harms related to over-reliance [1, 23]."}, {"title": "3 Dataset Collection Methods", "content": "3.1 Participants and Recruitment\nWe recruited 12,933 participants from the crowdsourcing platform Prolific between May 2023 and August 2024, with approximately 1,000 individuals recruited each month, as part of a larger project understanding Americans' experiences with AI. We note that over the 16-month period, we were unable to collect data during July 2023 and May - July 2024 due to technical issues; thus, we have responses over 12 months total. All survey procedures were approved by the BetterUp Institutional Review Board. We assessed the degree to which the samples each month were nationally representative of the United States population by using the American National Election Studies' raking algorithms [6, 98]. The analysis revealed that our data were nationally representative of the US population with respect to gender, ethnicity, education, and age overall, as none of the variables differed by a margin of more than .5%. For full participant demographics and month-by-month splits, please see Appendix A.\n3.2 Procedure and Measures\nWe elicited metaphors from participants by asking them: \"Some people use metaphors to describe abstract concepts, like AI. What is the best metaphor for how Al works?", "This is not about accuracy as much as understanding how you are thinking\"). Participants entered their answer into an open text box in the form \\\"AI is like because .\" Next, participants completed a series of survey measures related to their experiences with and attitudes toward AI. We assessed the frequency of each individual's AI use by asking them to indicate if they had heard of or used 8 of the most commonly used, consumer-facing tools: ChatGPT, DALL-E, Claude, Bard, Anthropic, Midjourney, Gemini, and Perplexity. They could also write-in the names of additional AI tools that they used. Trust in Al was measured using a 3-item survey measure adapted from past work investigating trust in human vs. AI-generated content [65], building on the Organizational Model of Trust which is widely used to study trust in technologies and products [86]. Finally, we assessed participants willingness to adopt Al with a 5-item survey measure of their behavioral intentions to engage with Al in the future. Full details of these survey items are in Appendix B.\nEnsuring data quality. We took precautions to account for the possibility that people may use Al to generate inauthentic responses to our free response questions by disabling copy/paste and including attention-check questions [51]. However, it may still be possible that participants used AI to generate their responses to our free-response question. To filter such responses out, we evaluated the semantic similarity of participant responses with those from ChatGPT. Full details are in Appendix C. We exclude 276 such participant responses, resulting in 11,790 valid metaphors.\"\n    },\n    {\n      \"title\": \"4 Analytic Approach\",\n      \"content\": \"Our methods for large-scale analyses of the metaphors are summarized in Figure 1.\n4.1 Topic modeling to identify dominant metaphors\nTo identify thematic clusters in the metaphors that participants provide, we use a mixed-methods approach: 1) automatic clustering using embeddings, 2) manual refinement of clusters, and 3) outlier analysis.\n1) Automatic clustering: First, we removed frequently occurring words (e.g., \u201cAI": "nd \u201cartificial intelligence", "refinement": "Using iterative discussions and consensus-building methods that are standard in qualitative coding approaches to developing grounded theory [17, 28, 97], the research team iteratively grouped the clusters based on thematic and conceptual similarities (e.g., combining a cluster about \"stringing together ideas", "a chef combining ingredients\" into the dominant metaphor of \\\"synthesizer\"), focusing on distinctiveness between and coherence within clusters, until we reached consensus. This resulted in a final set of 20 clusters (Note that we did not pre-constrain the number of clusters in the final set.) We refer to these 20 most frequently occurring metaphor clusters as dominant metaphors. Names and descriptions for each dominant metaphor were also decided upon collaboratively.\n3) Outlier analysis: Finally, we re-assigned outlier metaphors that were not initially automatically assigned to a cluster as follows: For each dominant metaphor, we computed the centroid as the average of all the embeddings of the metaphors belonging to that cluster. Then, for each outlier metaphor mp, we measured the cosine similarity of its embedding \\(e(m_p)\\) to each centroid. mp was then categorized under the dominant metaphor with the highest cosine similarity if the similarity was greater than 0.6.3 This process enabled us to automatically categorize metaphors that aligned with the broader themes identified in the manual refinement phase, but may have been too semantically different to be initially identified. This process results in 10,629 of the 11,789 metaphors being assigned to a dominant metaphor.\n4.2 Measuring implicit perceptions from metaphors\nBecause metaphors are laden with implicit associations [47], they can reveal how people think about the fundamental attributes of AI, such as how human-like an Al is, or whether people view AI as part of their \\\"in-group.\"\nTo measure implicit perceptions, we developed a scalable, systematic framework to score metaphors on the dimensions of anthropomorphism, warmth, and competence. Our approach is based on methods to automatically score any open-ended text on these dimensions, and thus we apply them to every individual metaphor. For each dominant metaphor, we compute the mean anthropomorphism, warmth, and competence score across the individual metaphors in that cluster.\n4.2.1 Measuring anthropomorphism. To assess the extent to which people's metaphors of AI ascribed human-like characteristics to AI, we adapted AnthroScore [21], an automatic metric of implicit anthropomorphism in language. In their work, they use the masked language model ROBERTa to calculate the relative probability that a given entity (e.g., \u201cAI": "in a text would be replaced by human pronouns versus non-human pronouns. Specifically, the degree of anthropomorphism for entity x in sentence s is measured as\n\\(A(s_x) = log \\frac{\\sum P_{HUMAN}(s_x)}{\\sum P_{NON-HUMAN}(s_x)},\\)\nwhere \\(P_{HUMAN}(s_x) = \\sum_{w \\in \\{he, she\\}} P(w)\\), \\(P_{NON-HUMAN}(s_x) = \\sum_{w \\in \\{it\\}} P(w)\\),\n(1)\nwhere P(w) is the model's outputted probability of replacing the mask with the word w.\nWe apply AnthroScore to the entities \\(x \\in \\{\\textit{it}, AI, artificial intelligence\\}\\) to measure the anthropomorphism of AI in each metaphor mp. If the metaphor does not contain any of these entities, we use the spacy package [61] to identify whether the metaphor is only a noun phrase. If so, we prepend the phrase \"AI is", "AI": "o measure anthropomorphism. Since this score is a relative log probability, a score greater than 0 suggests that the entity is more likely to be human, and a score less than 0 suggests that the entity is more likely to be non-human. For example, in the metaphor \"AI is a teacher"}, {"AI": "we compute the probability of the sentence \"He is a teacher"}, {"title": "4.2.2 Measuring warmth and competence.", "content": "We were also interested in understanding the extent to which people's metaphors characterized AI as being warm and competent, two fundamental dimensions of social perception that are well-established psychological constructs for estimating perceptions of people [30] and anthropomorphized non-human entities [93]. To assess these qualities in the metaphors of AI, we follow Fraser et al. [42]'s method for constructing semantic axes for warmth and competence. This approach quantifies a text on these dimensions, by measuring the semantic similarity between a given text (in our case, a metaphor about AI) and a constructed semantic axis that represents that dimension. We employed validated lexicons [96] for warmth and competence as anchors to define two axes: warmth-coldness and competence-incompetence. These lexicons comprise 6180 words that have been previously validated to be strongly associated with high or low warmth and competence. Then, for each metaphor, we project its embedding onto the axis, obtaining a score between -1 and 1. This method enables us to position texts on a continuous spectrum between two extremes (e.g., warm vs. cold or competent vs. incompetent). Specifically, the semantic axes are defined as follows: for a given dimension D (warmth or competence), we construct the semantic axis by taking the mean of the embeddings for each word in the lexicon that are positively associated with that dimension, and subtract the mean of the embeddings for each word in the lexicon that are negatively associated with it:\n\\(A_D = \\frac{1}{k}\\sum_{i=1}^{k} e(w_i) - \\frac{1}{m}\\sum_{j=1}^{m} e(w'_j),\\)\n(3)\nwhere wi/w; is a word in the set of words positively/negatively associated with that dimension respectively. We again use all-mpnet-base-v2 to compute the embeddings. For each metaphor mp embedded as e(mp), we compute its warmth and competence as the cosine similarity of the embedding e(mp) to this axis, i.e.,\n\\(Warmth(m_p) = cos(e(m_p), A_{warmth}), Competence(m_p) = cos(e(m_p), A_{competence}),\\)\n(4)\nresulting in two scores (each between -1 and 1) that represent the metaphor's warmth and competence respectively."}, {"title": "4.3 Predicting attitudes about Al from dominant metaphors and implicit perceptions", "content": "Next, we measured how dominant metaphors and implicit perceptions help explain two attitudinal variables with critical downstream outcomes: trust in AI and willingness to adopt AI. Specifically, we conducted a pair of stepwise multiple regression analyses to examine the extent to which dominant metaphors and perceptions can explain variance in people's trust in AI and their willingness to adopt AI, relative to demographic differences and their use of AI. All predictors were standardized to [0,1] to ensure comparability of effect sizes. We first conducted a regression to explain the dependent variable (trust or adoption) using a foundational set of predictors: time, frequency of AI tool use, gender, race/ethnicity, and age. For gender, we binarize the variable into men versus non-men (women and non-binary people), and for race/ethnicity, we binarize the variable into white versus non-white. In the second step, we introduce the second block of variables, which consists of 20 variables each representing the presence of a dominant metaphor (tool, robot, assistant, etc.). Finally, we incorporated the third block of variables, which are the implicit perceptions measured by our framework: anthropomorphism, warmth, and competence. This sequential approach allowed us to assess the incremental explanatory power of metaphors and perceptions in predicting trust and adoption beyond demographic and usage factors. Model assumptions, including linearity, multicollinearity, and homoscedasticity, were also checked to ensure the validity of the results."}, {"title": "5 Results", "content": "5.1 Teacher, tool, pet, or friend: dominant metaphors of Al\nAs detailed in \u00a74.1, we used a combination of automatic clustering and manual iterative coding to identify the 20 dominant clusters of metaphors that people used to conceptualize AI in our dataset (Table 1). We find that people most commonly described AI as being like 1) a technological or analog tool, such as a calculator or Swiss Army knife (10%), 2) a brain capable of reasoning and logic (10%), and 3) a powerful search engine capable of navigating large databases (9%). While these were some of the most common, our participants used a wide range of metaphors to characterize their mental models of AI. For example, some viewed Al as an intelligent teacher (\"AI is like a professor because it always has the answer", "It needs to learn, but is happy to provide output and is proud of it": 4, "It's basically plagiarism": 0.5, "a fairy godmother - there to help you with whatever you need": 1, "a deity or divine creature that... can make decisions on a grand scale": 1, "a robot that tries to think": 8, "a software with enhanced data processing capabilities": 7, "distorted mirror": 4, "It's like training your dog to do something\", 2%), or an unexplored realm (\\\"AI is like the ocean, there is so much uncertainty and so much to discover\", 2%). The wide range of dominant metaphors reveals the complexity with which people view AI. Note that small percentages potentially reflect large populations, e.g., \\\"thief": "ppears in 0.5% of the dataset (n = 57) but represents millions when extrapolated to the U.S. population, where Al awareness exceeds 90% [39]."}, {"title": "5.2 Implicit perceptions of Al as anthropomorphic, warm and competent over time.", "content": "The metaphors people use to describe AI can reveal the implicit perceptions they hold about AI. Applying our framework to measure implicit perceptions from the metaphors, we compute mean anthropomorphism, warmth, and competence scores for each dominant metaphor (Figure 2). While there is a wide range of anthropomorphism in dominant metaphors, we observe that every dominant metaphor has positive mean values of both warmth and competence (with the exception of the \"thief\" dominant metaphor, which has positive competence but negative mean warmth). Based on qualitative inspection of the individual metaphors, we find that this is because although some of the dominant metaphors reflect concepts that may have negative connotations, participants focus on warm and competent aspects of these concepts in their responses. For example, when describing AI as a \"child,", "genie,": "articipants focus on its capabilities, which seem magical in their scope. For the dominant metaphor of \"unexplored realm", "wonder and beauty.": "ven for the dominant metaphor of \"thief,", "friend\" is higher in warmth but middling in competence, while the dominant metaphor of AI as \"search engine": "s associated with higher perceptions of competence but lower warmth. Note that perceiving Al as human-like does not necessarily engender positive perceptions; to the contrary, the anthropomorphic dominant metaphor of \"thief\u201d is lower in both warmth and competence, while the non-anthropomorphic dominant metaphor of AI as a \"library", "shifts": "People view AI as increasingly human-like and warm. Analyzing month-over-month shifts revealed how people's perceptions changed over time. As shown in Figure 3, anthropomorphism increased by 34% over the 12 months (r = .80**). Understanding how dominant metaphors vary in prevalence over time provides insight into how different narratives about AI affect these perceptions: people became more likely to describe Al as being like a teacher (r = .75***), a friend (r = .75**), or an assistant (r = .60*), and less likely to see it as a distinctly non-human entity like a computer (r = -.92***), a search engine (r = -.63*), or a mirror (r = -.70*) (Figure 3). In addition to seeing AI as more human-like, implicit perceptions of AI as warm increased by 41% over time (r = .62*). Notably, however, this did not correspond to an increased perception that Al is competent. Instead, implicit perceptions of AI as competent decreased by 8% over time (r = -.60*). This may corroborate previous work finding that people broadly prefer AI that is conceptualized with metaphors that signal relatively lower competence [72]. In particular, the highest-warmth dominant metaphors describing AI as a friend, assistant, god (r = .64*), and teacher became increasingly common over time, making up four of the six dominant metaphors with statistically significant temporal increase in prevalence. (The other increasing dominant metaphors, \"child", "robot": "r = .70*, 12th warmest), also rank relatively high in warmth.) In contrast, three of the top four highest-competence dominant metaphors-computer, synthesizer (r = -.91***), and search engine-are the three most significantly decreasing clusters. Taken together, our findings reflect a societal shift in seeing AI as being more human-like and warm."}, {"title": "5.3 Understanding attitudes: Dominant metaphors and implicit perceptions explain trust and adoption.", "content": "In our stepwise regression to predict attitudes from metaphors and implicit perceptions, the first block of variables included demographic and AI usage variables, which explain 12% and 10% of the variance in trust and willingness to adopt AI, respectively (adjusted r2 = 0.12, 0.10). The second block of variables, which included the dominant metaphors, explain 3%*** and 2%*** more variance for trust and adoption of Al respectively. Finally, the third block of variables, implicit perceptions, help explain an additional 6%*** and 5%*** of variance ( adjusted r\u00b2 = 0.21, 0.18). Together, the dominant metaphors and implicit perceptions explain 75% and 80% more of the variance than demographics and frequency of use alone.\nSpecific metaphors and perceptions associated with trust and adoption. Regression coefficients from the full model show that, beyond AI use, warmth and competence are the strongest predictors of trust and AI adoption, with effects similar in magnitude to AI use. This is likely due to warm and competent metaphors such as \"assistant,\u201d \"friend,\u201d \u201cteacher,\u201d and \"library,\u201d which stand out as predicting trust and adoption positively in a dominant-metaphor-only regression (though these effects appear mediated by the warmth and competence variables in the full model). Anthropomorphism also predicts positive trust and adoption, but to a lesser extent. Notably, even in combination with the perceptual variables, \u201cthief", "brain": "nd \"god", "adoption": "conceptualizations of AI as a helpful human-like entity (\"friend", "teacher\u201d, \u201cassistant": "or as a source of vast amounts of knowledge (\"library", "god": "brain", "thief": "re often related to the widespread public concerns about copyright issues and AI plagiarizing or stealing creative work [50, 106]."}, {"title": "5.4 Demographic differences reveal disparities in trust and adoption.", "content": "Corroborating prior work that women have more negative attitudes toward AI [87, 126], we find that women are less trusting of and less willing to adopt AI. However, in contrast to previous work finding that race is not a significant predictor of AI attitudes and that trust and age are negatively correlated [126], our findings reveal unexpected patterns in AI trust and adoption: surprisingly, non-white and older people trust AI more and are more willing to adopt AI, i.e., older participants and non-white participants (and in particular Black participants) reported significantly higher levels of trust in AI and greater willingness to adopt these technologies compared to younger and white participants respectively (Figure 5). Dominant metaphors and implicit perceptions provide insights into these phenomena, though we emphasize the need for additional research to establish causal relationships and deeper understanding. Full details of demographic differences in dominant metaphors are in Figure A2.\nGender: Men view AI as warmer and more competent, but less human-like. Our survey corroborates previous work that women overall view AI in more negative light and are significantly less trusting and willing to adopt AI compared to men [87, 126]. Women and non-binary people's metaphors are overall less warm and competent, though they are also significantly more anthropomorphic. Differences in rates of dominant metaphors give some insight into these patterns: based on a two-sample binomial test, \"robot", "genie": "re disproportionately more frequently provided by women and non-binary people. In contrast, men significantly more frequently provide highly competent or warm-but not necessarily anthropomorphic-metaphors like \"teacher", "search engine": "and \"computer", "77].\nRace/Ethnicity": "Participants of color view AI as more warm, competent, and anthropomorphic. Non-white participants, who surprisingly trust AI more and are more willing to adopt it, also provide metaphors that reflected higher levels of warmth, competence, and anthropomorphism. This pattern emerges despite well-documented concerns about AI systems performing poorly for non-white users [15, 59] and perpetuating harmful racial stereotypes [9, 20, 119]. Non-white participants more frequently described AI through metaphors suggesting power and agency: based on a two-sample binomial test, the \"genie", "god": "ominant metaphors-evoking magic and mysticism-were significantly more common among non-white participants. In contrast, white participants more frequently used the \"search engine"}, {"title": "Age: Older people trust AI more and view it as more anthropomorphic but less competent.", "content": "Using a two-sample binomial test, we find that the dominant metaphor of \"friend", "search engine": "genie", "mirror": "animal", "child": "re all significantly more common among the 18-37 age groups. This perhaps suggests that older people may be more curious about Al companions and see that as the primary potential use of AI, though further research is needed to understand this phenomenon."}, {"title": "6 Discussion", "content": "6.1 Navigating appropriate levels of trust and adoption\nOur findings provide evidence that certain fram"}]}