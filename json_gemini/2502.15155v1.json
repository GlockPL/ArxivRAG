{"title": "Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models", "authors": ["Sarthak Mahajan", "Nimmi Rangaswamy"], "abstract": "In recent years, widespread internet adoption and the growth in user- base of various social media platforms have led to an increase in the prolifera- tion of extreme speech online. While traditional language models have demon- strated proficiency in distinguishing between neutral text and non-neutral text (i.e. extreme speech), categorizing the diverse types of extreme speech presents significant challenges [1][2]. The task of extreme speech classification is par- ticularly nuanced, as it requires a deep understanding of socio-cultural contexts to accurately interpret the intent of the language used by the speaker. Even hu- man annotators often disagree on the appropriate classification of such content, emphasizing the complex and subjective nature of this task [3]. The use of hu- man moderators also presents a scaling issue, necessitating the need for auto- mated systems for extreme speech classification. The recent launch of ChatGPT has drawn global attention to the potential applications of Large Language Models (LLMs) across a diverse variety of tasks. Trained on vast and diverse corpora, and demonstrating the ability to effectively capture and encode contex- tual information, LLMs emerge as highly promising tools for tackling this spe- cific task of extreme speech classification. In this paper, we leverage the Indian subset of the extreme speech dataset from [3] to develop an effective classifica- tion framework using LLMs. We evaluate open-source Llama models against closed-source OpenAI models, finding that while pre-trained LLMs show mod- erate efficacy, fine-tuning with domain-specific data significantly enhances per- formance, highlighting their adaptability to linguistic and contextual nuances. Although GPT-based models outperform Llama models in zero-shot settings, the performance gap disappears after fine-tuning.", "sections": [{"title": "Introduction", "content": "With increasing polarization of society along the lines of religion, race, gender, eth- nicity, etc., there has been a corresponding increase in hate speech online. Its manifes- tations vary across countries, reflecting their unique sociopolitical and cultural con- texts- racial and sexual orientation-based in the United States, anti-immigrant in Ger- many, and religious or caste-based in India.\nSocial media companies have invested heavily in content moderation systems to address hate speech, yet the scale of user-generated content renders manual modera- tion insufficient, making automated systems essential for scalable and effective detec-"}, {"title": "Contributions", "content": "This study focuses on the Indian subset of the Xtreme Speech Dataset, introduced by Maronikolakis et al. [3], to evaluate the performance of large language models (LLMs) in classifying extreme speech into three categories: derogatory extreme speech, exclusionary extreme speech, and dangerous speech. Utilizing both open- source and proprietary closed source LLMs, we compare multiple approaches, includ- ing zero-shot classification, fine-tuned models, fine-tuning with Direct Preference Optimization (DPO), and an ensemble of fine-tuned models. We also explore whether open-source models (specifically the Llama family of models) are a viable alternative to their more famous closed source counterparts like the GPT models by OpenAI."}, {"title": "Defining Extreme Speech", "content": "Hate speech, broadly defined as \"public speech that expresses hate or encourages violence towards a person or group based on some characteristic such as race, color, religion, national origin, sexual orientation, disability, or other traits\", is a concerning issue on digital platforms [11]. Udupa et al. [12] take an anthropological perspective for defining extreme speech as speech that pushes the boundaries of civil language. While hate speech is often framed in legal and political terms, extreme speech en- compasses a wider range of harmful, provocative, or exclusionary discourse that may not meet legal definitions of hate [3].\nIn this paper, we adopt the definition of extreme speech as proposed by Pohjonen and Udupa, who define it as \"speech acts that push the boundaries of acceptable speech by expressing views that challenge or threaten core values, identities and so- cial arrangements.\" [12] This definition allows us to capture a broader range of prob- lematic speech beyond just hate speech, including speech that may not necessarily be hateful but is still harmful, exclusionary or divisive.\nTypes of Extreme Speech. Maronikolakis et al. [3] categorize extreme speech into:\n\u2022 Derogatory speech: Speech that can be considered uncivil and offensive to any community, person, group, or institution. This form of speech may also be part of a form of protest against prevailing social norms or political powers.\n\u2022 Dangerous speech: Speech that has the potential to lead to physical harm or violence.\n\u2022 Exclusionary speech: Speech that segregates or excludes a certain group or community. This exclusion is often promoted through the use of hu- mour in order to normalize it."}, {"title": "LLMs for Extreme Speech Moderation", "content": "In recent years, the advent of powerful LLMs has revolutionized the field of natural language processing. These models, trained on huge and diverse datasets, have achieved state-of-the-art performance in areas such as text generation, question an- swering, and sentiment analysis [4] [13]. Their ability to generalize across domains through fine-tuning or prompt-based conditioning has made them indispensable tools for both academic research and industry applications [14]. LLMs have been employed to generate coherent and contextually relevant text, answer complex queries with high accuracy, and analyze sentiment in social media data with remarkable precision [15].\nOpen source versus Closed source LLMs. We focus specifically on open-source models due to their transparency, which provides deeper insights into model behavior and limitations compared to proprietary, black-box alternatives. For extreme speech moderation, model choice is critical not only for performance but also for reproduci- bility and ethical considerations. Open-source models, such as Meta Al's Llama fami- ly, offer full access to their codebase, weights, and training methodologies, enabling reproducibility and facilitating scientific rigor [7]."}, {"title": "Related Work", "content": "Earlier works in hate speech focused on detecting hate speech on social media plat- forms [8][20] [21]. Some of these works have also contributed to data collection and annotation [20][21] to assist in hate speech detection.\nOthers have expanded on the data collection and curation efforts, by contributing multi-lingual datasets [22][23][24]. In conjunction, researchers have looked at hate speech detection in multilingual settings, and examined transfer learning techniques to improve performance on low-resource languages [25].\nComplementing these efforts on multilingual dataset curation, a recent work by Maronikolakis et al. introduced the Xtreme Speech Dataset, which expands the scope beyond just hate speech to include a broader range of problematic speech, called ex- treme speech, such as derogatory extreme speech, exclusionary extreme speech and dangerous speech [3].\nResearchers have also looked at unique challenges posed by hate speech modera- tion, focusing on aspects like the ambiguous and context-dependent nature of hate speech, cultural nuances of language, and the dynamic nature of hate speech [3][26].\nVarious machine learning techniques have been explored for detecting hate speech online, using approaches ranging from traditional ML models like SVM, Naive Bayes [27] to more recent deep learning methods [4] [8].\nIn recent years, with the growing popularity of LLMs in various NLP tasks, re- searchers have explored using LLMs for text classification [28]. There is a growing use of LLMs in content moderation [29], owing to their ability to generalize across tasks, having been pretrained on vast and diverse datasets."}, {"title": "Dataset", "content": "We use the Indian partition of the Xtreme Speech Dataset [3], which includes 4,933 samples (after removing duplicates from the original dataset) across three categories. The dataset is split into training (64%), DPO/ensemble (16%), and testing (20%) sets, with representative sampling to maintain consistent class distributions across splits."}, {"title": "Methodology", "content": "We evaluate the Llama family of models, which are pretrained on diverse multilin- gual corpora and have demonstrated strong performance across various NLP tasks. For benchmarking, we also include GPT-40 and GPT-40-mini to compare closed- source performance with open-source alternatives. To align the loss function with classification accuracy, we encode class labels as 0, 1, and 2 for derogatory, exclu- sionary, and dangerous speech, respectively\u00b9."}, {"title": "Inference through Zero shot", "content": "We evaluate the following models: Open Source - Llama 3.1 8B, Llama 3.2 1B, Lla- ma 3.2 3B, Llama 3.3 70B, and Closed Source - GPT 40, and GPT 40-mini. We used 4-bit quantized versions of the Llama models, reducing memory requirements and enabling faster inference. The models are prompted to categorize text from the test dataset into one of three predefined classes.\nWe tested two approaches: (1) predicting the class label (0/1/2) directly, and (2) first generating a justification followed by the predicted label. The latter approach"}, {"title": "Inference through Supervised Fine-tuned (SFT) Models", "content": "We fine-tuned the models using training samples of extreme speech text paired with their corresponding labels. Two approaches were evaluated: one incorporating both justifications and labels during SFT, and another using only labels. Interestingly, the variant with justifications underperformed compared to the label-only approach. While justifications enhance zero-shot reasoning, they may not improve supervised learning, as the loss function does not explicitly optimize for classification accuracy when justifications are included. In contrast, training with only labels directly mini- mizes classification error, as the output is constrained to the label itself."}, {"title": "Preference Optimization", "content": "We evaluated Direct Preference Optimization (DPO) [18] on the fine-tuned Llama 3.1 8B model to assess potential performance improvements. To construct the DPO dataset, we ran inference on the remaining 16% dataset using the SFT-ed Llama 3.1 8B model, extracting log probabilities for each output token (0/1/2, corresponding to class probabilities). For each example, the incorrect class with the highest probability was selected as the negative example, while the human annotated class label served as the positive example. DPO was then applied to the SFT-ed Llama 3.1 8B model using this dataset."}, {"title": "Ensembling SFT-ed models", "content": "To further enhance performance, we explored ensembling the fine-tuned Llama models. We computed F1-macro scores for all four models on a held-out 16% dataset and tested two ensembling approaches: (1) weighting each model's predicted label by its Fl-macro score and selecting the class with the highest weighted score, and (2) calculating a weighted average of class probabilities using F1-macro scores and se- lecting the class with the highest average probability. As shown in Table 3, both ap- proaches yielded nearly identical results."}, {"title": "Results and analysis", "content": ""}, {"title": "Zero Shot", "content": "LLMs demonstrate strong zero-shot performance, highlighting their ability to gen- eralize across tasks without task-specific training [4]. Among Llama models, Llama 3.3 70B outperforms smaller variants, consistent with the established correlation be- tween model size and performance. Larger models, with greater parameter capacity, capture complex linguistic patterns more effectively, leading to higher F1 scores."}, {"title": "Supervised Fine-tuned Models", "content": "Fine-tuning significantly boosts performance across all models, demonstrating that incorporating cultural context helps models better identify extreme speech in the Indi- an dataset. Notably, fine-tuned Llama models, regardless of size (1B to 70B parame- ters), perform comparably, with even smaller models like Llama 3.1 1B and Llama 3.2 3B outperforming GPT-40's zero-shot performance and nearing fine-tuned GPT- 40-mini. This underscores the importance of fine-tuning for domain-specific tasks."}, {"title": "Ensembling", "content": "As shown in Table 3, an ensemble of the fine-tuned Llama models does not outper- form the individual fine-tuned models. We observed that the various fine-tuned mod- els exhibit similar strengths and weaknesses; they perform well on the same classes and make comparable types of misclassifications. Ensembling typically relies on the principle of combining models with diverse error patterns to achieve superior perfor- mance [19]. However, in our case, the homogeneity in the fine-tuned models' perfor- mance limits the ensemble's ability to outperform individual models."}, {"title": "Preference Optimization", "content": "We observed no performance gains with DPO, indicating it offers no additional bene- fits over SFT for text classification, particularly for nuanced tasks like extreme speech. This aligns with DPO's design, which focuses on optimizing stylistic and preference-based text generation rather than discriminative tasks [17]."}, {"title": "Benchmarking Against State-of-the-Art Classification Models", "content": "Table 4 compares the best F1 scores from Maronikolakis et al. [3] with those achieved by our fine-tuned models. Our results, including those from smaller Llama models, significantly outperform the state-of-the-art benchmarks, demonstrating the advantages of using LLMs for extreme speech classification. Notably, our results are more balanced across all classes, addressing the poor performance of langBERT and mBERT on exclusionary extreme speech."}, {"title": "Conclusion", "content": "We investigated the use of LLMs for classifying extreme speech in the Indian da- taset from [3]. Fine-tuning even smaller Llama models yielded significant perfor- mance improvements over SVM, langBERT, and mBERT, the best-performing meth- ods in Maronikolakis et al. [3].\nOur findings highlight the potential of open-source models to match closed-source performance while being cost-effective, reflecting rapid advancements in the open- source LLM space. Due to computational constraints, we did not explore larger mod-"}]}