{"title": "Topology Optimization of Random Memristors for Input-Aware Dynamic SNN", "authors": ["Bo Wang", "Shaocong Wang", "Ning Lin", "Yi Li", "Yifei Yu", "Yue Zhang", "Jichang Yang", "Xiaoshan Wu", "Yangu He", "Songqi Wang", "Rui Chen", "Guoqi Li", "Xiaojuan Qi", "Zhongrui Wang", "Dashan Shang"], "abstract": "There is unprecedented development in machine learning, exemplified by recent large language models (GPT4) and world simulators (SORA), which are artificial neural networks (ANNs) running on digital computers. However, they still cannot parallel human brains in terms of energy efficiency and the streamlined adaptability to inputs of different difficulties, due to differences in signal representation, optimization, run-time reconfigurability, and hardware architecture. To address these fundamental challenges, we introduce pruning optimization for input-aware dynamic memristive spiking neural network (PRIME). Signal representation-wise, PRIME employs leaky integrate-and-fire neurons to emulate the brain's inherent spiking mechanism. Drawing inspiration from the brain's structural plasticity, PRIME optimizes the topology of a random memristive SNN, without expensive memristor conductance fine-tuning. For runtime reconfigurability, inspired by the brain's dynamic adjustment of computational depth, PRIME employs an input-aware dynamic early stop policy to minimize latency during inference, thereby boosting energy efficiency without compromising performance. Architecture-wise, PRIME leverages memristive in-memory computing, mirroring the brain and mitigating the von Neumann bottleneck. We validated our system using a 40 nm 256 Kb memristor-based in-memory computing macro on neuromorphic image classification and image inpainting. Our results demonstrate the classification accuracy and Inception Score (IS) are comparable to the software baseline, while achieving 37.83\u00d7 and 62.50\u00d7 improvements in energy efficiency. Furthermore, we attained 77.0% and 12.5% computational load savings with minimal reduction in performance. The system also exhibits robustness against stochastic synaptic noise of analogue memristors. Our software-hardware co-designed model paves the way to future brain-inspired neuromorphic computing of brain-like energy efficiency and adaptivity.", "sections": [{"title": "Introduction", "content": "Machine learning with artificial neural networks (ANNs) has undergone significant advancements in recent years1\u20133, as demonstrated by the development of sophisticated large language models4,5 such as GPT4 and advanced world simulators like SORA. These models, operating on digital computers, exhibit human-like capabilities and are steps toward the long-term goal of artificial general intelligence (AGI).\nDespite these achievements, a discernible discrepancy in performance persists compared to the human brain, especially in terms of energy efficiency7\u201311 and adapting to inputs of different difficulties by dynamically allocating computing resources 12-16.\nThis gap can be attributed to fundamental differences in signal representation, optimization, run-time reconfigurability, and architecture.\nIn terms of signal representation, the human brain utilizes spikes17,18 for information representation (Fig. 1a, left). These spikes are sparse, robust to signal noise, and enable the brain to perform advanced cognitive tasks at a minimal energy of only"}, {"title": "PRIME: topology pruning optimization for input-aware dynamic memristive SNN", "content": "PRIME is systematically illustrated in Fig. 2.\nFig. 2a left panel illustrates the pruning optimization inspired by human brain's structural synaptic plasticity21,22, which contributes to the maturation of human brain by preserving the functional synapses while eliminating redundant ones (Fig. 1b, left). In the training phase, random synaptic weights are generated by the inherent programming stochasticity of memristors (Fig. 2a, left). Each synapse has a pop-up score s, indicative of its significance. During the forward pass, the synapses with top k% pop-up scores are preserved while others are pruned. Subsequently, in the backward pass, these scores undergo gradient-based updates aimed at minimizing the training loss, thereby optimizing the topology of SNN (see Methods). This topology optimization gets rid of the expensive memristor conductance fine-tuning and transforms the memristor programming stochasticity into an advantage (see Supplementary Note 1 for a more detailed theoretical proof of topology optimization for memristor-based SNN).\nIn Fig. 2a right panel, drawing inspiration from the human brain's dynamic computational depth12\u201316, which enhances the brain's ability to rapidly and efficiently recognize and adapt to new stimuli, we implement the input-aware dynamic early stop policy in the pruned memristive SNN during the inference phase. The input spikes to SNN are temporally spanned in a"}, {"title": "Image classification for neuromorphic dataset using PRIME", "content": "We first validate PRIME on classifying the representative N-MNIST dataset using a 3-layer spiking convolutional neural network (Fig. 3a).\nN-MNIST35 is a neuromorphic image datasets of 10 digits captured by dynamic vision sensors. Each sample, comprising 'on' and 'off' spike streams within a 34\u00d734 pixel frame spanning 300ms, is processed into 10 time bins using SpikingJelly36 (Fig. 3a). Here we use a supernet consisting of two convolution layers and a linear classification layer, with all synaptic weights initially mapped to random memristor conductance differentials. The softmax-based confidence score (Fig. 3a, see Methods) dynamically adjusts the inference timestep for each input sample, optimizing processing efficiency.\nTo assess PRIME's performance in terms of classification accuracy and inference efficiency, we first conduct a comparative analysis between the software baseline (Software) and PRIME (Hardware) across various early stop thresholds (Fig. 3b). The average timesteps in the test dataset are used as evaluation metric. PRIME closely matches the accuracy of the software baseline at timestep 10 for N-MNIST classification. Moreover, PRIME achieves significant latency reductions while maintaining high accuracy, thanks to the dynamic early stop using thresholding method. For example, at a threshold of 0.5, PRIME attains 97.60% accuracy with about 59% reduction in calculation cost (the average timesteps), and at a threshold of 0.3, it achieves 94.9% accuracy with about a 77% reduction in calculation cost. We then visualize the embedded features for the classification head under different early stop thresholds of PRIME (Fig. 3c, Supplementary Figure 3a) and software counterpart (Supplementary Figure 3b) using tSNE37. The tSNE visualization results suggest that PRIME representations maintain a discernable subpopulation structure of ten clusters at the proper early stop threshold (Fig. 3c). The confusion matrices (Fig. 3d, Supplementary Figure 4) of PRIME is dominated by the diagonal elements at the proper threshold (e.g. 0.5) and hence corroborates the high classification accuracy with a substantial reduction in timesteps.\nWe compare PRIME with other optimization methods (Fig. 3e). PRIME parallels the performance of weight-optimized SNN, while eliminating the need for weight fine-tuning. Additionally, compared to memristor-initialized SNNs (fixed random weights without any further optimization), PRIME shows significant performance improvement, consistent with software-pruned SNNs. This reveals that the memristor programming stochasticity is well-suited for generating random weights for topology optimization. The performance gap between software-pruning and software-initialized networks and that between memristor-pruning and memristor-initialized networks are similar, proving again that our proposed pruning algorithm is particularly effective for memristor-based networks.\nAdditionally, we present a comparative analysis of energy consumption for a single image classification between a projected hybrid analogue-digital system and state-of-the-art digital system including RTX4090 graphic processing unit (GPU) (Fig. 3f) (see Supplementary Figure 5a, Supplementary Table 8 for comparison with other digital systems). The bottom panel (No early stop) illustrates that our PRIME can significantly decrease energy consumption, by approximately 37.83 times compared to the digital hardware, due to memristive in-memory computing. Moreover, the input-aware dynamic early stop further reduces energy consumption as the early stop threshold decreases (see Supplementary Figure 6a for detailed energy breakdown). This corroborates the superior energy efficiency of memristor in-memory computing.\nWe also compare PRIME with conventional memristor weights fine-tuning for network inference under different memristor programming noise.38\u201340. For conventional weight fine-tuning, the inevitable programming stochasticity degrades the precision in mapping weights to memristor conductance, thus degrading the network inference performance as the noise increases. In"}, {"title": "Image inpainting using PRIME", "content": "Despite neuromorphic image classification, we extended our validation to more complex tasks such as image inpainting, utilizing a spiking variational autoencoder (spiking-VAE)42. Image inpainting, the task of completing missing regions in images,"}, {"title": "Impact of memristor read noise", "content": "We further assess PRIME's capacity to mitigate memristor read noise using its input-aware dynamic early stop mechanism.\nThe memristive switching and transport mechanisms are illustrated in Fig. 5a. The charge transport through nanoscale conducting channels that are formed due to electrochemical reactions. This results in two types of noise: programming and read noise. The programming noise is due to random ionic motions in channel formation and rupture48, giving rise to cycle-to-cycle and device-to-device variation in programming conductance (Fig. 5b), which can be effectively addressed by our proposed pruning optimization. The read noise is the temporal fluctuation of memristor conductance due to charge trapping and detrapping (e.g. random telegraphic noise) and thermal noise49,50. (Fig. 5c).\nTo demonstrate PRIME's effectiveness in mitigating memristor read noise, we experiment on various early stop thresholds with different levels of simulated Gaussian read noise (Fig. 5d-e, see Supplementary Note 2 for the theoretical analysis of the impact of read noise on PRIME, and Supplementary Figure 9 for simulated conductance fluctuations). PRIME demonstrates remarkable stability in terms of model performance within the typical memristor read noise range of 0.01 to 0.03 (see definition of noise scale in Methods). In high-noise scenarios (e.g., 0.08, 0.1), PRIME shows more performance degradation in inpainting (Fig. 5e), while the impact on classification is relatively minor (Fig. 5d). Additionally, the study shows that variations in early stop thresholds affect timesteps and accuracy, especially under high-noise conditions, because confidence thresholding of PRIME dynamically balance timesteps and network performance."}, {"title": "Discussion", "content": "Although ANNs have shown unprecedented development, they still cannot parallel the brain in terms of adaptability and energy efficiency.\nPRIME offers a potential solution via the hardware-software co-design. In terms of hardware, the memristive neuromorphic computer mimics the in-memory computing of the brain. In addition, it also practises pruning optimization that is inspired by biological structural plasticity, making it robust to both programming and read noise of memristors. In terms of software,"}, {"title": "Methods", "content": "Fabrication of Resistive Memory Chip\nUtilizing the advanced 40nm technology node, the engineered resistive memory device showcases a sophisticated 512\u00d7512 crossbar configuration. This intricate arrangement features resistive memory elements strategically positioned between 4 and 5 metal layers, employing a backend-of-line fabrication technique. Each cell within this array is meticulously crafted, consisting of bottom and top electrodes (BE and TE), complemented by a dielectric layer made of transition-metal oxide. The fabrication process begins with the precise patterning of the BE via, which boasts a 60nm diameter, achieved through photolithography and subsequent etching. This is followed by the deposition of TaN using physical vapor deposition techniques, capped with a 10nm TaN buffer layer for enhanced stability. A thin, 5nm layer of Ta is then applied and subjected to oxidation, culminating in the formation of an 8nm-thick TaOx dielectric layer. The construction of the TE involves a carefully sequenced deposition of 3nm Ta and 40nm TiN, also via physical vapor deposition. The fabrication process is finalized with the deposition of the requisite interconnection metals, adhering to conventional logic processing protocols. Within this architecture, cells aligned"}, {"title": "Hybrid Analogue\u2013Digital Hardware System", "content": "The innovative hybrid system merges analogue-digital technologies, integrating a 40nm resistive memory chip with a Xilinx ZYNQ system-on-chip (SoC). This SoC amalgamates a field-programmable gate array (FPGA) with an advanced RISC machines (ARM) processor, all mounted on a printed circuit board (PCB) for cohesive operation. The resistive memory chip is designed to function in three distinct modes, each crucial for the edge pruning topology optimization: electroform mode, reset mode, and multiplication mode.\nIn the electroform mode, a controlled dielectric breakdown is initiated within the resistive memory arrays, effectively creating random conductance matrices. This is achieved by biasing all source lines (SLs) to a predetermined programming voltage, delivered by an eight-channel digital-to-analogue converter (DAC, DAC80508 from Texas Instruments) boasting a 16-bit resolution. Meanwhile, bit lines (BLs) are grounded, and word lines (WLs) receive biasing from the DAC to enforce a compliance current across the cells, thereby averting a hard breakdown. The nuances of the SL voltage amplitude and duration are key to shaping the post-breakdown conductance distribution and its sparsity.\nTransitioning to the reset mode enables the reversion of a resistive memory cell to its non-conductive state, wherein a selected BL is biased via the DAC, the corresponding SL is grounded, and the remaining SLs are left in a floating state. The multiplication mode involves the utilization of a 4-channel analogue multiplexer (CD4051B, Texas Instruments) coupled with an 8-bit shift register (SN74HC595, Texas Instruments), which collectively apply a DC voltage across the BLs of the resistive memory chip.\nThroughout each phase of training, the chip undergoes readings, and the resultant multiplication values, manifested as currents from the SLs, are transformed into voltages. This conversion is facilitated by trans-impedance amplifiers (OPA4322- Q1, Texas Instruments) and analogue-to-digital converters (ADS8324, Texas Instruments, with a 14-bit resolution), with the processed data subsequently relayed to the Xilinx SoC for advanced computation. The FPGA component of the SoC is intricately designed to manage the resistive memory operations and facilitate data exchange with the ARM processor via a direct memory access control unit, employing double-data rate memory access. Furthermore, the FPGA is tasked with the hardware implementation of certain neural network functions, including activation and pooling, enhancing the system's overall computational efficacy."}, {"title": "Reset and Set Operations Edge Pruning Topology Optimization", "content": "The process of synaptic pruning within this system is physically manifested by transitioning the relevant differential pairs of resistive memory into an off-state via the reset operation. Conversely, the reactivation of these synaptical weights is facilitated by restoring the resistive memory cells to their conductive states through a set operation. This set operation is executed by administering uniform pulses, characterized by a 3.3V amplitude and a 300ns duration, to the bit line of the resistive memory array. This procedure revives the previously pruned cells, reintegrating them into the active sub-network. On the other hand, the reset operation is effected through the application of uniform pulses, this time with a 2.6V amplitude and a 400ns duration, to the source line of the resistive memory array, effectively eliminating the conductive pathway.\nIt is crucial to highlight the substantial distinction between the off-state and the conducting state of these cells. This differentiation underscores the fact that programming the cells to precise conductance values is not requisite for the system's functionality, allowing for a more flexible and efficient approach to the modulation of conductive states within the network."}, {"title": "PRIME model", "content": "Spiking Neuron Model\nThe iterative leaky integrate-and-fire (iLIF) spiking model51 is utilized, which is a LIF model solved using the Euler method.\n$U_t = \\tau_{decay}U_{t-1}(1-O_{t-1})+X_t$  (1)\n$O_t = H(u_t -V_{th}),$\nwhere $\\tau_{decay}$ represents membrane decay, $u_t$, $U_{t-1}$, $O_t$, $O_{t-1}$ are the membrane potential and spike output (i.e. 0 or 1) at time step t and t 1. $x_t$ denotes the weighted sum of spikes from the connected neurons, where $x_{j,t} = \\sum_{j}w_jo_{j,t}$. H(x) represents the heaviside step function which will generate a spike when $x > 0$. $V_{th}$ is the threshold potential. Due to the discontinuity of the heaviside step function we utilize the approach of pseudo-derivative to solve the issue. In detail, we approximate it as follows:\n$\\frac{\\partial O_t}{\\partial u_t} = \\frac{1}{a}sign(u-V_{th}< \\frac{a}{2}),$   (2)"}, {"title": "Pruning Topology Optimization with random weights", "content": "In the pruning topology optimization method, there are two sets of parameters, i.e. randomly distributed weights W, and pop-up scores S. Initially, a spiking neural network with random weights is established using an analogue resistive memory chip. Unlike the traditional co-design model, where the weights W are expensively tuned, this configuration involves learning a pop-up score s for each synaptic weight w, while maintaining the weights in their initial random distribution.\nThe pruning process can be divided into two phases, the forward pass and the backward pass. On the forward pass, the hardware synapses are selected with top-k% highest pop-up scores in each layer according to the predefined sparsity, leading to a pop-up score based sub-network. Inputs are then fed into the sub-network for forward propagation and loss evaluation. The input of neuron i in layer I can be computed as:\n$I_i = \\sum_{j \\epsilon V^{l-1}} W_{ij}Z_jH(S_{ij}),$  (3)\nwhere the j neuron in layer l \u2013 1 is connected with the i neuron via the synaptic weight wij. Zj denotes the output of the neuron j. And the $H(s_{ij}) = 1$ if $s_{ij}$ is in the top-k% pop-up scores in layer l. On the backward pass, the general digital processor calculates the loss function's gradients to optimize the scores of all synapses with the random weights fixed. The pop-up score is updated using the straight-through estimator29,52:\n$S_{ij} \\leftarrow S_{ij} - \\alpha \\frac{\\partial L}{\\partial I_i}W_{ij}Z_j$   (4)\nwhere sij, \u03b1, $\\frac{\\partial L}{\\partial I_i}$ and $W_{ij}Z_j$ denote the pop-up score between the connected neuron i and j, the learning rate, the partial derivative of loss (L) with respect to the input of node i and the weighted output of node j, respectively. These processes are repeated until a well-performed sub-network is selected from the randomly initialized neural network."}, {"title": "Dynamic confidence thresholding method", "content": "The dynamic confidence thresholding method is input-dependent, which dynamically decides which time step to early stop for each input sample during inference, leading to faster, lower overhead, more energy-efficient edge computing. Consequently, the number of time steps varies for each input, potentially allowing for a reduction in the average time steps required per inference.\nIn the dynamic confidence thresholding method, the confidence metric is calculated based on the output, and then utilized to determine the optimal point to prematurely conclude the inference process. The confidence calculation varies across different tasks. Considering the classification tasks, given an input X with a label Y, the prediction probability distribution for each time step t is usually calculated by Softmax, P(\u1ef8 = Y) = softmax(f(Xt)) = [P1, P2,\u2026, PN], where \u1ef8 is the predicted output label, N is the number of categories. Then, the confidence is defined as Pt = max(P),\n$P_t = max(\\frac{e^{f(X_i)_i/a}}{\\sum_{j=1}^{N} e^{f(X_i)_i/a}}) i = 1,2,...,N,$  (5)\nwhere $P_t$ represents the calculated confidence of time step t, \u03b1 denotes denotes a scale parameter designed to prevent the saturation of confidence levels. Given a predefined threshold \u03b21, the inference process will terminate early, using the output at this time step for prediction, if the confidence reaches or exceeds \u03b21 (i.e. $P_t \u2265 \u03b2_1$).\nConsidering the impainting task, for a given impainting input image X, the output at each time step is the reconstructed image X\u0302. The confidence for this task is determined by the consistence calculation:\n$P_t = ||X-\\hat{X}_{t-1}||, $  (6)\nwhere || || represents the L1 norm, which is utilized to quantify the difference between two images at consecutive two time steps. When the predefined threshold \u03b22 is defined, the inference process will terminate early when the condition $P_t < P_{t-1} < \u03b2_2$ is met. This implies that when the change in consecutively generated images becomes minimal, indicating a negligible difference, the process is considered to have reached its termination point."}, {"title": "Details of the experiments", "content": "Classification on neuromorphic image dataset\nThe event-based dataset, N-MNIST35 is utlized to evaluate the performance of our model. The N-MNIST dataset is a spiking version of the MNIST dataset, created using a Dynamic Vision Sensor (DVS) mounted on a pan-tilt unit. It comprises 60,000 training samples and 10,000 testing samples, distributed across 10 classes representing digits from '0' to '9'. Each sample in"}, {"title": "Evaluation metric for classification", "content": "Accuracy. $Accuracy = \\frac{\\sum_{i=1}^{n}(y_i=\\hat{y}_i)}{n}$, where $y_i$ represents the predicted output for sample i, $\\hat{y}_i$ denotes the ground truth, and n is the total number of samples.\nAverage time steps. $Average\\,ts = \\frac{\\sum_{i}^{n}tsi}{n}$, where tsi denotes the inference time steps for the sample i, and the n represents the total number of samples.\nARI. $ARI = \\frac{\\sum_{ij}(^{n_{ij}}_2)-[\\sum_i(^{a_i}_2)\\sum_j(^{b_j}_2)]/(^{N}_2)}{[\\sum_i(^{a_i}_2)+\\sum_j(^{b_j}_2)]-[\\sum_i(^{a_i}_2)\\sum_j(^{b_j}_2)]/(^{N}_2)}$, where $n_{ij}$ represents the number of elements in both cluster i and cluster cluster j, $a_i$ and $b_j$ is the sum of elements in row i and column j, indicating the total elements in cluster i and j, N is the totam number of elements, $(^{n}_2)$ denotes the binomial coefficient, representing the number of unique pairs that can be formed from n items. The Adjusted Rand Index (ARI) is a measure used to evaluate the similarity between two data clusterings. A higher ARI value, closer to 1, indicates greater similarity between the two clusterings, suggesting a more accurate clustering process.\nEnergy Consumption. The detailed components of energy consumption and their calculation methods are illustrated in Supplementary Table 6."}, {"title": "Inpainting on image dataset", "content": "The MNIST45 dataset is comprised of single-channel grayscale images, representing digits from '0' to '9', thus encompassing 10 distinct classes. Each image in this dataset is characterized by a resolution of 28\u00d728 pixels. The dataset is divided into two subsets: 60,000 images for training and 10,000 for testing. In the inpainting task, as illustrated in Fig. 4a, the central 8x8 pixel region of each image is obscured. These modified images serve as inputs to a spiking Variational Autoencoder42 (spiking-VAE), which aims to reconstruct the original, unaltered image (the ground truth). The network's configuration follows the same structure as outlined in Kamata's work. However, a notable modification is made in the number of channels in both the encoder and decoder components of the network; they are adjusted to 32 channels, a reduction from the originally specified 64 and 128 channels. The detailed training settings and default parameters of PRIME on image inpainting is presented in Supplementary Table 4 and Supplementary Table 7, respectively.\nFashion-MNIST47 consists of 70,000 grayscale images, each with a resolution of 28 \u00d7 28 pixels. The images represent 10 different categories of fashion items, such as T-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots. Each category contains 7,000 images, with 60,000 images in the training set and 10,000 images in the test set. As illustrated in Supplementary Figure 8, the network's configuration follows deeper and larger structures. The detailed training settings and default parameters of the experiment is presented in Supplementary Table 4 and Supplementary Table 7, respectively."}, {"title": "Evaluation metric for inpainting", "content": "Reconstruction loss. $L_{reconstruction} = MSE(x,\\hat{x})$, where x and \u00ee distinctly represent the ground truth and reconstructed images, respectively.\nInception Score. $IS = exp(E_{x~p_g} [D_{KL}(p(y|x)||p(y))])$, where x is the generated data, pg is the model's generated data distribution, p(y) is the marginal class distribution over the generated data, p(y|x) represents the conditional class distribution given the generated sample x, typically obtained from an Inception model. The Inception Score (IS) is commonly used to evaluate the quality of images generated by models like VAEs. The Inception Score measures the diversity and quality of the generated images, where a higher score generally indicates better image quality and more variety in the generated samples.\nFr\u00e9chet Inception Distance. FID is a metric commonly used to assess the quality of generated images in generative models, similar to the Inception Score (IS). FID quantifies the similarity between the feature representations of real and generated images using a pre-trained Inception network. It is calculated as $FID = ||\\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g \u2013 2(\\Sigma_r\\Sigma_g)^{1/2})$, where \u03bcr and \u03bcg are the mean feature vectors of the real and generated images, respectively, and \u03a3r and \u03a3g are their covariance matrices. FID measures both the quality and diversity of the generated images, with lower values indicating better performance. It captures"}, {"title": "Memristor read conductance simulation", "content": "The memristor conductance with different levels of read noise is simulated using Gaussian noise, which is formulated as\n$mem\\_g = mem\\_g + \\Delta \\times noise\\_scale \\times mem\\_g,$  (7)\nwhere the mem_g denotes the memristor conductance, \u0394 is a random variable following a normal distribution with mean 0 and variance 1, and noise_scale represents the level of read noise in the memristor."}]}