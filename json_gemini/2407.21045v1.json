[{"title": "Keywords", "authors": ["Boyan Xu", "Liang Wen", "Zihao Li", "Yuxing Yang", "Guanlan Wu", "Xiongpeng Tang", "Yu Li", "Zihao Wu", "Qingxian Su", "Xueqing Shi", "Yue Yang", "Rui Tong", "How Yong Ng"], "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their potential applications across various fields. This paper embarked on a pivotal inquiry: Can existing LLMs effectively serve as \"water expert models\" for water engineering and research tasks? This study was the first to evaluate LLMs' contributions across various water engineering and research tasks by establishing a domain-specific benchmark suite, namely, WaterER. Herein, we prepared 983 tasks related to water engineering and research, categorized into \u201cwastewater treatment\u201d, \u201cenvironmental restoration\u201d, \u201cdrinking water treatment\u201d, \u201csanitation\u201d, \u201canaerobic digestion\u201d and \u201ccontaminants assessment\u201d. We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5, Gemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the strengths of GPT-4 in handling diverse and complex tasks of water engineering and water research, the specialized capabilities of Gemini in academic contexts, Llama3's strongest capacity to answer Chinese water engineering questions and the competitive performance of Chinese-oriented models like GLM-4, ERNIE and QWEN in some water engineering tasks. More specifically, current LLMs excelled particularly in generating precise research gaps for papers on \"contaminants and related water quality monitoring and assessment\". Additionally, they were more adept at creating appropriate titles for research papers on \"treatment processes for wastewaters\", \u201cenvironmental restoration", "drinking water treatment\". Overall, this study pioneered evaluating LLMs in water engineering and research by introducing the WaterER benchmark to assess the trustworthiness of their predictions. This standardized evaluation framework would also drive future advancements in LLM technology by using targeting datasets, propelling these models towards becoming true \"water expert\".": "sections", "content": "artificial intelligence (AI); wastewater treatment; drinking water treatment; contaminants; desalination;\nevaluation frameworks"}, {"title": "Introduction", "content": "One of the remarkable advancements in machine learning is the emergence of large language models (LLMs) (Zhao et al. 2023). These conversational systems are developed using the transformer architecture (Vaswani et al. 2017) and are trained on extensive internet data (Brown et al. 2020). Their appeal lies in their ability to perform general-purpose language generation and comprehension tasks with simplicity: when given a phrase, they generate text that seamlessly continues in natural language, often making it indistinguishable from human writing to a significant extent.\nRecent advancements of LLMs, such as GPT-4 (Achiam et al. 2023) and Gemini (Team et al. 2023) have showcased their strengths in open-domain question answering (WEN et al. 2022a), retrieval, reasoning and other natural language processing tasks (Wen et al. 2022b). These advancements reflect efforts to align these models with diverse scientific and societal needs (McIntosh et al. 2024). Moreover, LLMs have garnered significant attention for their conversational prowess and have shown promise in various fields (Cai et al. 2024, Kevian et al. 2024). They could be used to educate engineers by generating relevant technical knowledge, improving engineering design or assisting engineers in making informed decisions to overcome real-world engineering challenges (Doris et al. 2024, Kevian et al. 2024). For instance, LLMs can be leveraged to advance geoscience knowledge and its applications (Deng et al. 2024), as well as to enhance healthcare outcomes (Tan et al. 2024). Furthermore, their capability to handle a wide array of tasks beyond their training suggests significant potential in addressing complex scientific challenges, such as predicting atomic-level protein structures (Lin et al. 2023).\nWater engineering and research are vital for maintaining the health of our planet and its inhabitants. Water engineering includes various critical activities such as treating wastewater in treatment plants, providing potable water through drinking water and desalination plants and developing efficient sanitation systems (Pooi and Ng 2018). Water research focuses on understanding the behavior of water systems, innovating treatment technologies and assessing the impacts of human activities and climate change on water quality and availability. Despite their importance, modern water treatment plants encounter significant challenges such as inefficient manpower management, high consumption of energy and chemicals and stringent discharge regulations (Xu et al. 2023). The dependency on manual labor and experienced engineers in operating these processes also presents difficulties, especially with an aging workforce and the challenge of attracting younger talent. Moreover, water research faces obstacles like high manpower and resource costs. Issues such as interdisciplinary collaboration complexities and data limitations further complicate the accuracy and reliability of research findings (Dadson et al. 2020).\nIn this study, we proposed a critical question: How proficient are current LLMs in fulfilling the requirements of water engineering and research? In other words, how can we trust LLMs' effectiveness in water engineering/research tasks? To be more specifically, if LLMs demonstrate sufficient capability, which specific areas of water engineering and research could benefit from their direct application? Conversely, if LLMs fall short, which aspects of their development require further fine-tuning using targeting datasets to become a genuine water expert model? To our knowledge, these fundamental questions have not been studied and there was no rigorous answer.\nTo address the challenges of evaluating LLMs in the context of water engineering and research (WaterER), we introduced a groundbreaking benchmark called WaterER. A benchmark for LLMs typically refers to a standardized set of tasks or problems designed to evaluate and compare the performance and capabilities of various language models across specific domains. WaterER is designed to assess the capabilities of LLMs to interpret and respond to the intricate demands of technical water-related tasks in both water engineering and research settings (Figure 1). WaterER draws inspiration from existing tests for water practice and research, covering a wide range of activities including wastewater treatment processes, environmental restoration, drinking water treatment, sanitation, anaerobic digestion and water quality monitoring. Seven LLMs including GPT-4, GPT-3.5 (Achiam et al. 2023), Gemini (Team et al. 2023), GLM-4 (Team et al. 2024), Ernie Bot (Sun et al. 2021), Qwen (Bai et al. 2023) and Llama3 (Huang et al. 2024a) were tested to determine their performance. WaterER aimed to identify both the strengths and gaps in LLMs' ability to understand and analyze water engineering and research documentation, thereby setting the stage for future enhancements (e.g., building domain specific LLMs) in LLM technology (Li et al. 2024), towards becoming water expert agents and researchers (Figure 1)."}, {"title": "Methods", "content": "The Dataset\nTo establish a benchmark for evaluating the capabilities of LLMs in the field of water engineering and research, we have curated the questions under two main categories: Water Engineering and Water Research. The sources for these questions included reputable publications and practice-oriented materials available through both online platforms and academic journals. For Water Engineering, questions were derived from various sources, including technical books such as \u201cWastewater Treatment Fundamentals\" (Federation 2021, Federation et al. 2018) containing 500 questions (denoted by WTP), examination questions for Chinese Certificate of Registration for Environmental Engineers including 263 questions (denoted by ECREPE) and Water Operator Practice exams containing 100 questions (denoted by WOPT) (Table 1). These questions covered key aspects of water treatment processes, regulations and engineering practices. For Water Research, the dataset focused on six current research topics such as 1) Treatment processes for wastewaters (WasteW), 2) Environmental restoration (ER), 3) Drinking water treatment (DrinkingW), 4) Sanitation (SA), 5) Anaerobic digestion and waste management (AD) and 6) Contaminants and related water quality monitoring and assessment (Contaminant). Each topic included 30 questions, which were sourced from leading publications in Journal of \u201cWater Research\u201d and \u201cEnvironmental Science and technology\" (Table 1).\nData Processing\nAll questions were parsed either automatically where feasible or manually into a structured format. Each question related to Water Engineering in the WaterER dataset was formatted to include exactly four choices. Originally, most questions already featured four options; we discard questions with fewer than four choices and randomly eliminate excess incorrect options from questions that originally presented more than four. The questions (Examples in Figure 2b) then passed through multiple rounds of human validation by water experts. For both the categories of Water Engineering and Water Research, the datasets for each were randomly divided into validation and test sets using a 1:9 ratio. This setup ensured that the validation set, intended for hyperparameter tuning, comprised approximately 10% of the dataset, while the remaining 90% formed the test set."}, {"title": "Experimental setup", "content": "Models\nIn our study, we comprehensively assessed the scientific capabilities of LLMs by evaluating seven high performing LLMs that are widely accessible. These models included GPT-4 (gpt-4-0613), GPT-3.5-turbo (gpt-3.5-turbo-0125), Gemini (Gemini-1.0 pro), GLM (GLM-4), ERNIE Bot (ernie-4.0-8k-0329), Qwen (qwen-max-0428) and Llama3 (Meta-Llama-3-70B-Instruct), representing a diverse range of organizations and varying in size (Figure 2a). The specifics of these models were summarized in Figure 1. GPT-3.5 and GPT-4 are the most advanced GPT model variants from OpenAI, having undergone pretraining, instruction tuning and reinforcement learning from human feedback. ERNIE Bot, developed by Baidu, declares advanced semantic understanding and generation capabilities across various modalities and languages. LLaMa, created by Meta, is recognized as one of the most robust open-weight foundation models to date. ChatGLM, developed by Tsinghua University, leverages the GLM architecture, and has been further adapted for conversational data. Qwen, developed by Alibaba, is designed to excel in understanding and generating responses based on e-commerce data, integrating domain-specific knowledge to enhance user interaction and service automation. Gemini, from Google, focuses on multitasking and transfer learning, making it a versatile tool in multiple domains."}, {"title": "Prompts and Evaluation", "content": "We evaluated all the above LLMs in Answer-Only results on both zero- and five-shot settings (Brown et al. 2020), as illustrated in Figure 2b. Zero-shot settings involved asking the LLMs to perform a task without any examples, while five-shot settings provided five examples to guide the LLMs' response. For engineering questions, all items were objective, consisting of four multiple-choice options, with only one correct answer per question. We used LLMs to solve these questions through specific prompting techniques (Figure 2b). We used accuracy as the metric. In the context of research questions, we used two types of inquiries for each published paper: 1) The first one is to tests the model's ability to generate a proper title only using the abstract of the published paper with specific prompts (Figure 2b). The LLM should understand the abstract section to generate it concisely; and 2) The second one seeks to identify the research gap only using the abstract of the published paper with specific prompts (Figure 2b). We employed both automatic and human evaluation methods to assess the generated titles and the identification of research gaps. The human evaluation methods can be found in Text S1 in Supporting Information. The titles of published papers and the research gaps provided by human water experts based on the published papers were employed as the right answers for LLM evaluation. For the automatic evaluation methods, we used the ROUGE-L metric for automatic evaluation (Lin 2004), a metric that assessed the similarity between generated text and a reference. This method was advantageous as it allowed for in-sequence matches rather than requiring consecutive words, effectively reflecting the natural order of sentences, and including the longest in-sequence common n-grams without the need for a predefined n-gram length."}, {"title": "Experiment Results", "content": "General Comparison\nValidation Set and Test Set\nIn benchmarking LLMs, the validation set was used for tuning hyperparameters and selecting the best model configurations, while the test set provided a final measure of the model's generalization to unseen data, ensuring the results do reflect its true performance in real-world scenarios. We divided the raw dataset into a validation set (10%) and a test set (90%). Results from the validation set with zero-shot settings and five-shot settings were detailed in Tables S1 and S2, while those from the test set with zero-shot settings and five-shot settings were listed in Tables 2 and 3. The average accuracies on both the validation and test splits were closely aligned, indicating that the average validation accuracy can serve as a reliable indicator for accelerating development processes. For this study, we primarily relied on the test set for the main results and discussions.\nZero- and Five-shot Results\nTables 2 and 3 illustrated the performance of various LLMs in zero-shot and five-shot settings, comparing their efficiency across different metrics in water engineering and water research. In the zero-shot setting, models like GPT-4 and Llama3 showed notable performance, with GPT-4 achieving 69.2% in WTF and 71.6% in WOPT, and Llama3 excelling in ECREPE with 46.1%. In the five-shot setting, almost all models demonstrate enhanced performance, with GPT-4 improving to 73.5% in WTF and 79.0% in WOPT, indicating the value of additional contextual information. QWEN showed one of the largest increases in the ECREPE metric for water engineering, jumping from 28.9% to 41.8%. Of note, some LLMs experienced an accuracy drop beyond five-shot settings, and we hypothesized that this decline may result from inadequate incorporation of few-shot demonstrations during the instruction tuning stage, thereby favoring zero-shot instruction-following abilities over few-shot performance (Huang et al. 2024b). Overall, few-shot learning enhanced large language models by offering specific water tasks examples that clarify task requirements, improve contextual understanding, reduce biases, and increase accuracy compared to zero-shot settings that rely only on general pre-trained knowledge.\nDomain-Specific Performances\nWater Engineering\nIn the context of water engineering questions (WTF, WOPT, ECREPE) assessed under both zero-shot and five-shot settings, Llama3 and GPT-4 generally led in performance (Tables 2 and 3). Of note, while all the LLMs showed a spectrum of capabilities and improvements in handling complex engineering tasks in English, their performance on Chinese tasks remained a challenge (Tables 2 and 3), indicating a need for further optimization and adaptation to enhance proficiency in non-English languages.\nIn examining the performance across Chinese and English tasks, we observed some intriguing trends. For Chinese tasks (ECREPE), English-oriented models like GPT-3.5, GPT-4 and Gemini achieved accuracy rates between 31.0% and 38.4%, while Chinese-oriented models such as GLM-4, ERNIE 4.0 and QWEN demonstrated comparable performance, with accuracy rates ranging from 28.9% to 41.8% across both zero-shot and five-shot settings (Tables 2 and 3). Llama3, primarily an English model (over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data covering over 30 languages, according to Meta's official website), excelled unexpectedly in the Chinese task (ECREPE), leading with 46.1% in the zero-shot setting and 48.7% in the five-shot setting, showcasing its strong cross-lingual capabilities.\nFor the English tasks WTF and WOPT, English-oriented models such as GPT-4, Gemini and Llama3 achieved accuracy rates between 63.0% and 79.0% across both zero-shot and five-shot settings. In comparison, Chinese models like GLM-4, ERNIE 4.0, and QWEN showed slightly lower performance, with accuracy rates ranging from 63.0% to 70.4% (Tables 2 and 3). This indicated a robust adaptability of Chinese models beyond their primary language training.\nOverall, our evaluation suggested that current LLMs were not yet suitable for addressing Chinese water engineering questions. For English tasks, while some progress has been made, the accuracy remains below 80%, necessitating careful use. To develop a truly effective water engineering expert model, further fine-tuning with targeted datasets for LLMs was required.\nWater Research\nIn evaluating LLMs on water research tasks, focusing on title generation and research gap identification, Gemini and GPT-4 generally outperformed others across both zero-shot and five-shot settings (Tables 2 and 3). Especially, Gemini particularly shined in the five-shot scenario with highest scores of 29.0% for generating title and 32.0% for research gap identification, respectively. As shown in Tables S3 and S4, which included examples of generated titles and research gaps by seven LLMs, Gemini can provide paper titles and research gaps as accurately and precisely as human water experts, while other models often generated more irrelevant content, leading to relatively lower scores (Tables S3 and S4). Following Gemini, ERNIE 4.0 demonstrated strong capabilities for generating contextually relevant titles (25.3%) and identifying research gaps (23.6%) in the five-shot setting. Both ERNIE 4.0 and QWEN had a higher capability to generate proper paper titles compared to GPT-4 across both zero-shot and five-shot settings. Llama3 also showed competitive performance in generating research gaps, particularly improving with five-shot setting (from 19.7% to 28.8%). Overall, these findings suggested that these models could be further optimized with specific training techniques and more high-quality domain data to enhance their utility in specialized water research tasks.\nBased on human evaluation, the results in Tables S5 and S6 confirmed that Gemini generally exhibited the highest performance compared to other LLMs (Tables 2 and 3). While there were some discrepancies, which were attributable to differences between human and automatic evaluation methods, the automatic evaluation method was generally reliable and time-efficient (Lin 2004), and it was employed to assess the capability of LLMs in performing various academic tasks as discussed below.\nIn examining water research tasks with different types of sub-domain topics, we observed that current LLMs were generally effective at providing proper titles for papers on \u201cContaminant\u201d (Tables 4 and 5), compared to other research topics. Specifically, GPT-4 demonstrated consistent performance across all topics (14.7%-23.9% in both zero-shot and five-shot settings), particularly excelling in the \u201cContaminant\u201d category, indicating robustness in handling complex topics. In contrast, GPT-3.5 showed lower performance, highlighting the improvements in newer models of GPT-4. Gemini stood out compared to the other six LLMs, especially in five-shot settings. It demonstrated superior contextual academic understanding, particularly in the \"WasteW,\" \"ER,\" and \"DrinkW\" topics, with the highest scores reaching up to 31.8%, 30.7%, and 28.8%, respectively. Notably, ERNIE 4.0 outperformed GPT-4 across all topics in five-shot settings, with scores ranging from 22.3% to 27.7%. GLM-4 consistently scored slightly lower compared to GPT-4, typically around 13.9% to 21.7%, indicating potential gaps in handling specific research niches. QWEN demonstrated variability, excelling in \"AD\" among the seven LLMs, scoring 27.5% after five-shot settings. Llama3, while not leading, provided reasonable performance across categories compared to GPT-4, though with some inconsistencies, achieving scores between 14.1% and 23.8% after five-shot settings.\nTo devise precise research gaps for research papers, seven models demonstrated strong proficiency in the domains of \u201cWasteW\u201d, \u201cER\u201d, and \u201cDrinkingW\u201d (Tables 6 and 7) compared to other research topics. Specifically, GPT-4 performed impressively, peaking at 30.9% in \u201cWasteW\u201d and 36.3% in \u201cDrinkingW\u201d in the five-shot setting. Gemini stood out significantly among other 6 LLMs in both zero-shot and five-shot settings, especially in \u201cWasteW\u201d and \u201cDrinkingW,\u201d where it scored a remarkable 40.4% and 43.5% in the five-shot setting, respectively, indicating its exceptional capability in pinpointing detailed research gaps. Llama3 exhibited moderate performance compared to GPT-4, with a strong 34.5% in \u201cDrinkingW\u201d in the five-shot setting. In contrast, GLM-4, QWEN, and ERNIE showed relatively lower capabilities compared to GPT-3.5.\nAlthough the evaluation scores obtained using automatic evaluation methods were relatively low (Table 2 and Table 3), this can be attributed to the irrelevant content generated by LLMs during title generation and research gap identification (Tables S3 and S4). Our evaluation suggested that LLMs, particularly Gemini, can generally be trusted for executing academic tasks, while users needed to carefully remove or revise irrelevant key information generated from some of LLMs (Tables S3 and S4). Additionally, we found that LLMs exhibited varying capabilities across different types of academic tasks, such as generating titles or identifying research gaps, as well as across different water topics, including \u201ctreatment processes for wastewaters\u201d, \u201cenvironmental restoration\u201d, \u201cdrinking water treatment and distribution\u201d, \u201csanitation\u201d, \u201canaerobic digestion\u201d and \u201ccontaminants and related water quality monitoring and assessment\u201d. Therefore, further fine-tuning or retrieval-augmented generation (RAG) with targeted datasets was necessary to specifically develop a genuinely effective water research expert model."}, {"title": "Implications", "content": "While innovative LLMs have been developed, their application in real-world water-related contexts remains unexplored. In this study, we introduced WaterER, a pioneering evaluation suite tailored to measure the advanced knowledge and reasoning abilities of foundation LLMs in water engineering and research. We investigated a crucial question: How well do current LLMs meet the needs of water engineering and research? We highlighted the robust performance of GPT-4 across both water engineering and water research tasks, underscoring its versatility and broad applicability for tasks requiring both technical precision and nuanced comprehension. Gemini was particularly effective at understanding and generating academic language, showing enhanced capability in producing and analyzing research content. Surprisingly, Llama3 showed strongest capacity to answer Chinese water engineering questions. The Chinese-orientated models, such as GLM-4, ERNIE4.0 and QWEN, exhibited comparable performance to GPT-4 in water engineering tasks. For various water research questions, we observed that current LLMs excel in generating precise research gaps for papers related to \"Contaminants and related water quality monitoring and assessment\". Additionally, these models were adept at creating appropriate titles for research papers in the fields of \u201cTreatment processes for wastewaters\u201d, \u201cEnvironmental restoration\u201d and \u201cDrinking water treatment\u201d. Overall, we suggested that while LLMs identify correlations for effective predictions in water engineering and research tasks, they may not fully address complex questions in these fields. Therefore, the next step of our research involves enhancing LLMs by integrating specialized water engineering and scientific knowledge, informed by the benchmarking the strengths and limitations of the current models (Table 2 and Table 3).\nLooking ahead, as LLM technologies evolve and the water-related communities face new challenges and changes, evaluation benchmarks will be crucial for guiding the development of LLMs tailored to water-related knowledge. These benchmarks are fundamental in ensuring that LLM advancements remain aligned with the dynamic needs and obstacles within the water sectors. Therefore, it is essential to establish comprehensive and evolving benchmarks for LLMs in water engineering and research (Figure 1). However, achieving these benefits requires robust collaboration to form a benchmark committee. This committee should involve reproducible and standardized methodologies, open computational resources, and a committed community from academia, industry, and practitioners within the water sector. We invite water researchers and organizations to join us in forming a benchmark committee to strengthen the WaterER as an open benchmarking platform."}, {"title": "Data and Code availability", "content": "All the data (e.g., published paper and questions from technical books) used in this study for querying LLMs was obtained from public sources. The raw data generated by the LLMs, and the code are available upon request."}]