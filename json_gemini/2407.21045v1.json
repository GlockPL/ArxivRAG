{"title": "Unlocking the Potential: Benchmarking Large Language Models in\nWater Engineering and Research", "authors": ["Boyan Xu", "Liang Wen", "Zihao Li", "Yuxing Yang", "Guanlan Wu", "Xiongpeng\nTang", "Yu Li", "Zihao Wu", "Qingxian Su", "Xueqing Shi", "Yue Yang", "Rui Tong", "How Yong Ng"], "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their potential\napplications across various fields. This paper embarked on a pivotal inquiry: Can existing LLMs\neffectively serve as \"water expert models\" for water engineering and research tasks? This study was\nthe first to evaluate LLMs' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we prepared 983 tasks\nrelated to water engineering and research, categorized into \u201cwastewater treatment\u201d, \u201cenvironmental\nrestoration\u201d, \u201cdrinking water treatment\u201d, \u201csanitation\u201d, \u201canaerobic digestion\u201d and \u201ccontaminants\nassessment\u201d. We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5, Gemini, GLM-4,\nERNIE, QWEN and Llama3) on these tasks. We highlighted the strengths of GPT-4 in handling\ndiverse and complex tasks of water engineering and water research, the specialized capabilities of\nGemini in academic contexts, Llama3's strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like GLM-4, ERNIE and\nQWEN in some water engineering tasks. More specifically, current LLMs excelled particularly in\ngenerating precise research gaps for papers on \"contaminants and related water quality monitoring and\nassessment\". Additionally, they were more adept at creating appropriate titles for research papers on\n\"treatment processes for wastewaters\", \u201cenvironmental restoration\", and \"drinking water treatment\".\nOverall, this study pioneered evaluating LLMs in water engineering and research by introducing the\nWaterER benchmark to assess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using targeting datasets,\npropelling these models towards becoming true \"water expert\".", "sections": [{"title": "Keywords", "content": "artificial intelligence (AI); wastewater treatment; drinking water treatment; contaminants; desalination;\nevaluation frameworks"}, {"title": "Introduction", "content": "One of the remarkable advancements in machine learning is the emergence of large language\nmodels (LLMs) (Zhao et al. 2023). These conversational systems are developed using the transformer\narchitecture (Vaswani et al. 2017) and are trained on extensive internet data (Brown et al. 2020). Their\nappeal lies in their ability to perform general-purpose language generation and comprehension tasks\nwith simplicity: when given a phrase, they generate text that seamlessly continues in natural language,\noften making it indistinguishable from human writing to a significant extent.\nRecent advancements of LLMs, such as GPT-4 (Achiam et al. 2023) and Gemini (Team et al.\n2023) have showcased their strengths in open-domain question answering (WEN et al. 2022a),\nretrieval, reasoning and other natural language processing tasks (Wen et al. 2022b). These\nadvancements reflect efforts to align these models with diverse scientific and societal needs (McIntosh\net al. 2024). Moreover, LLMs have garnered significant attention for their conversational prowess and\nhave shown promise in various fields (Cai et al. 2024, Kevian et al. 2024). They could be used to\neducate engineers by generating relevant technical knowledge, improving engineering design or\nassisting engineers in making informed decisions to overcome real-world engineering challenges\n(Doris et al. 2024, Kevian et al. 2024). For instance, LLMs can be leveraged to advance geoscience\nknowledge and its applications (Deng et al. 2024), as well as to enhance healthcare outcomes (Tan et\nal. 2024). Furthermore, their capability to handle a wide array of tasks beyond their training suggests\nsignificant potential in addressing complex scientific challenges, such as predicting atomic-level\nprotein structures (Lin et al. 2023).\nWater engineering and research are vital for maintaining the health of our planet and its\ninhabitants. Water engineering includes various critical activities such as treating wastewater in"}, {"title": "", "content": "treatment plants, providing potable water through drinking water and desalination plants and\ndeveloping efficient sanitation systems (Pooi and Ng 2018). Water research focuses on understanding\nthe behavior of water systems, innovating treatment technologies and assessing the impacts of human\nactivities and climate change on water quality and availability. Despite their importance, modern water\ntreatment plants encounter significant challenges such as inefficient manpower management, high\nconsumption of energy and chemicals and stringent discharge regulations (Xu et al. 2023). The\ndependency on manual labor and experienced engineers in operating these processes also presents\ndifficulties, especially with an aging workforce and the challenge of attracting younger talent.\nMoreover, water research faces obstacles like high manpower and resource costs. Issues such as\ninterdisciplinary collaboration complexities and data limitations further complicate the accuracy and\nreliability of research findings (Dadson et al. 2020).\nIn this study, we proposed a critical question: How proficient are current LLMs in fulfilling the\nrequirements of water engineering and research? In other words, how can we trust LLMs' effectiveness\nin water engineering/research tasks? To be more specifically, if LLMs demonstrate sufficient\ncapability, which specific areas of water engineering and research could benefit from their direct\napplication? Conversely, if LLMs fall short, which aspects of their development require further fine-\ntuning using targeting datasets to become a genuine water expert model? To our knowledge, these\nfundamental questions have not been studied and there was no rigorous answer.\nTo address the challenges of evaluating LLMs in the context of water engineering and research\n(WaterER), we introduced a groundbreaking benchmark called WaterER. A benchmark for LLMs\ntypically refers to a standardized set of tasks or problems designed to evaluate and compare the\nperformance and capabilities of various language models across specific domains. WaterER is"}, {"title": "", "content": "designed to assess the capabilities of LLMs to interpret and respond to the intricate demands of\ntechnical water-related tasks in both water engineering and research settings (Figure 1). WaterER\ndraws inspiration from existing tests for water practice and research, covering a wide range of activities\nincluding wastewater treatment processes, environmental restoration, drinking water treatment,\nsanitation, anaerobic digestion and water quality monitoring. Seven LLMs including GPT-4, GPT-3.5\n(Achiam et al. 2023), Gemini (Team et al. 2023), GLM-4 (Team et al. 2024), Ernie Bot (Sun et al.\n2021), Qwen (Bai et al. 2023) and Llama3 (Huang et al. 2024a) were tested to determine their\nperformance. WaterER aimed to identify both the strengths and gaps in LLMs' ability to understand\nand analyze water engineering and research documentation, thereby setting the stage for future\nenhancements (e.g., building domain specific LLMs) in LLM technology (Li et al. 2024), towards\nbecoming water expert agents and researchers (Figure 1)."}, {"title": "Methods", "content": "The Dataset\nTo establish a benchmark for evaluating the capabilities of LLMs in the field of water engineering\nand research, we have curated the questions under two main categories: Water Engineering and Water\nResearch. The sources for these questions included reputable publications and practice-oriented\nmaterials available through both online platforms and academic journals. For Water Engineering,\nquestions were derived from various sources, including technical books such as \u201cWastewater\nTreatment Fundamentals\" (Federation 2021, Federation et al. 2018) containing 500 questions (denoted\nby WTP), examination questions for Chinese Certificate of Registration for Environmental Engineers\nincluding 263 questions (denoted by ECREPE) and Water Operator Practice exams containing 100\nquestions (denoted by WOPT) (Table 1). These questions covered key aspects of water treatment\nprocesses, regulations and engineering practices. For Water Research, the dataset focused on six\ncurrent research topics such as 1) Treatment processes for wastewaters (WasteW), 2) Environmental\nrestoration (ER), 3) Drinking water treatment (DrinkingW), 4) Sanitation (SA), 5) Anaerobic digestion\nand waste management (AD) and 6) Contaminants and related water quality monitoring and\nassessment (Contaminant). Each topic included 30 questions, which were sourced from leading\npublications in Journal of \u201cWater Research\u201d and \u201cEnvironmental Science and technology\" (Table 1).\nData Processing\nAll questions were parsed either automatically where feasible or manually into a structured format.\nEach question related to Water Engineering in the WaterER dataset was formatted to include exactly\nfour choices. Originally, most questions already featured four options; we discard questions with fewer\nthan four choices and randomly eliminate excess incorrect options from questions that originally"}, {"title": "", "content": "presented more than four. The questions (Examples in Figure 2b) then passed through multiple rounds\nof human validation by water experts. For both the categories of Water Engineering and Water\nResearch, the datasets for each were randomly divided into validation and test sets using a 1:9 ratio.\nThis setup ensured that the validation set, intended for hyperparameter tuning, comprised\napproximately 10% of the dataset, while the remaining 90% formed the test set.\nExperimental setup\nModels\nIn our study, we comprehensively assessed the scientific capabilities of LLMs by evaluating\nseven high performing LLMs that are widely accessible. These models included GPT-4 (gpt-4-0613),\nGPT-3.5-turbo (gpt-3.5-turbo-0125), Gemini (Gemini-1.0 pro), GLM (GLM-4), ERNIE Bot (ernie-\n4.0-8k-0329), Qwen (qwen-max-0428) and Llama3 (Meta-Llama-3-70B-Instruct), representing a\ndiverse range of organizations and varying in size (Figure 2a). The specifics of these models were\nsummarized in Figure 1. GPT-3.5 and GPT-4 are the most advanced GPT model variants from OpenAI,\nhaving undergone pretraining, instruction tuning and reinforcement learning from human feedback.\nERNIE Bot, developed by Baidu, declares advanced semantic understanding and generation\ncapabilities across various modalities and languages. LLaMa, created by Meta, is recognized as one of\nthe most robust open-weight foundation models to date. ChatGLM, developed by Tsinghua University,\nleverages the GLM architecture, and has been further adapted for conversational data. Qwen,\ndeveloped by Alibaba, is designed to excel in understanding and generating responses based on e-\ncommerce data, integrating domain-specific knowledge to enhance user interaction and service\nautomation. Gemini, from Google, focuses on multitasking and transfer learning, making it a versatile\ntool in multiple domains."}, {"title": "Experiment Results", "content": "General Comparison\nValidation Set and Test Set\nIn benchmarking LLMs, the validation set was used for tuning hyperparameters and selecting the\nbest model configurations, while the test set provided a final measure of the model's generalization to\nunseen data, ensuring the results do reflect its true performance in real-world scenarios. We divided\nthe raw dataset into a validation set (10%) and a test set (90%). Results from the validation set with\nzero-shot settings and five-shot settings were detailed in Tables S1 and S2, while those from the test\nset with zero-shot settings and five-shot settings were listed in Tables 2 and 3. The average accuracies\non both the validation and test splits were closely aligned, indicating that the average validation\naccuracy can serve as a reliable indicator for accelerating development processes. For this study, we\nprimarily relied on the test set for the main results and discussions.\nZero- and Five-shot Results\nTables 2 and 3 illustrated the performance of various LLMs in zero-shot and five-shot settings,\ncomparing their efficiency across different metrics in water engineering and water research. In the\nzero-shot setting, models like GPT-4 and Llama3 showed notable performance, with GPT-4 achieving\n69.2% in WTF and 71.6% in WOPT, and Llama3 excelling in ECREPE with 46.1%. In the five-shot\nsetting, almost all models demonstrate enhanced performance, with GPT-4 improving to 73.5% in\nWTF and 79.0% in WOPT, indicating the value of additional contextual information. QWEN showed\none of the largest increases in the ECREPE metric for water engineering, jumping from 28.9% to"}, {"title": "", "content": "41.8%. Of note, some LLMs experienced an accuracy drop beyond five-shot settings, and we\nhypothesized that this decline may result from inadequate incorporation of few-shot demonstrations\nduring the instruction tuning stage, thereby favoring zero-shot instruction-following abilities over few-\nshot performance (Huang et al. 2024b). Overall, few-shot learning enhanced large language models by\noffering specific water tasks examples that clarify task requirements, improve contextual\nunderstanding, reduce biases, and increase accuracy compared to zero-shot settings that rely only on\ngeneral pre-trained knowledge.\nDomain-Specific Performances\nWater Engineering\nIn the context of water engineering questions (WTF, WOPT, ECREPE) assessed under both zero-\nshot and five-shot settings, Llama3 and GPT-4 generally led in performance (Tables 2 and 3). Of note,\nwhile all the LLMs showed a spectrum of capabilities and improvements in handling complex\nengineering tasks in English, their performance on Chinese tasks remained a challenge (Tables 2 and\n3), indicating a need for further optimization and adaptation to enhance proficiency in non-English\nlanguages.\nIn examining the performance across Chinese and English tasks, we observed some intriguing\ntrends. For Chinese tasks (ECREPE), English-oriented models like GPT-3.5, GPT-4 and Gemini\nachieved accuracy rates between 31.0% and 38.4%, while Chinese-oriented models such as GLM-4,\nERNIE 4.0 and QWEN demonstrated comparable performance, with accuracy rates ranging from 28.9%\nto 41.8% across both zero-shot and five-shot settings (Tables 2 and 3). Llama3, primarily an English\nmodel (over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data covering\nover 30 languages, according to Meta's official website), excelled unexpectedly in the Chinese task"}, {"title": "", "content": "(ECREPE), leading with 46.1% in the zero-shot setting and 48.7% in the five-shot setting, showcasing\nits strong cross-lingual capabilities.\nFor the English tasks WTF and WOPT, English-oriented models such as GPT-4, Gemini and\nLlama3 achieved accuracy rates between 63.0% and 79.0% across both zero-shot and five-shot settings.\nIn comparison, Chinese models like GLM-4, ERNIE 4.0, and QWEN showed slightly lower\nperformance, with accuracy rates ranging from 63.0% to 70.4% (Tables 2 and 3). This indicated a\nrobust adaptability of Chinese models beyond their primary language training.\nOverall, our evaluation suggested that current LLMs were not yet suitable for addressing Chinese\nwater engineering questions. For English tasks, while some progress has been made, the accuracy\nremains below 80%, necessitating careful use. To develop a truly effective water engineering expert\nmodel, further fine-tuning with targeted datasets for LLMs was required."}, {"title": "Water Research", "content": "In evaluating LLMs on water research tasks, focusing on title generation and research gap\nidentification, Gemini and GPT-4 generally outperformed others across both zero-shot and five-shot\nsettings (Tables 2 and 3). Especially, Gemini particularly shined in the five-shot scenario with highest\nscores of 29.0% for generating title and 32.0% for research gap identification, respectively. As shown\nin Tables S3 and S4, which included examples of generated titles and research gaps by seven LLMs,\nGemini can provide paper titles and research gaps as accurately and precisely as human water experts,\nwhile other models often generated more irrelevant content, leading to relatively lower scores (Tables\nS3 and S4). Following Gemini, ERNIE 4.0 demonstrated strong capabilities for generating\ncontextually relevant titles (25.3%) and identifying research gaps (23.6%) in the five-shot setting. Both\nERNIE 4.0 and QWEN had a higher capability to generate proper paper titles compared to GPT-4\nacross both zero-shot and five-shot settings. Llama3 also showed competitive performance in"}, {"title": "", "content": "generating research gaps, particularly improving with five-shot setting (from 19.7% to 28.8%). Overall,\nthese findings suggested that these models could be further optimized with specific training techniques\nand more high-quality domain data to enhance their utility in specialized water research tasks.\nBased on human evaluation, the results in Tables S5 and S6 confirmed that Gemini generally\nexhibited the highest performance compared to other LLMs (Tables 2 and 3). While there were some\ndiscrepancies, which were attributable to differences between human and automatic evaluation\nmethods, the automatic evaluation method was generally reliable and time-efficient (Lin 2004), and it\nwas employed to assess the capability of LLMs in performing various academic tasks as discussed\nbelow.\nIn examining water research tasks with different types of sub-domain topics, we observed that\ncurrent LLMs were generally effective at providing proper titles for papers on \u201cContaminant\u201d (Tables\n4 and 5), compared to other research topics. Specifically, GPT-4 demonstrated consistent performance\nacross all topics (14.7%-23.9% in both zero-shot and five-shot settings), particularly excelling in the\n\u201cContaminant\u201d category, indicating robustness in handling complex topics. In contrast, GPT-3.5\nshowed lower performance, highlighting the improvements in newer models of GPT-4. Gemini stood\nout compared to the other six LLMs, especially in five-shot settings. It demonstrated superior\ncontextual academic understanding, particularly in the \"WasteW,\" \"ER,\" and \"DrinkW\" topics, with\nthe highest scores reaching up to 31.8%, 30.7%, and 28.8%, respectively. Notably, ERNIE 4.0\noutperformed GPT-4 across all topics in five-shot settings, with scores ranging from 22.3% to 27.7%.\nGLM-4 consistently scored slightly lower compared to GPT-4, typically around 13.9% to 21.7%,\nindicating potential gaps in handling specific research niches. QWEN demonstrated variability,\nexcelling in \"AD\" among the seven LLMs, scoring 27.5% after five-shot settings. Llama3, while not"}, {"title": "", "content": "leading, provided reasonable performance across categories compared to GPT-4, though with some\ninconsistencies, achieving scores between 14.1% and 23.8% after five-shot settings.\nTo devise precise research gaps for research papers, seven models demonstrated strong\nproficiency in the domains of \u201cWasteW\u201d, \u201cER\u201d, and \u201cDrinkingW\u201d (Tables 6 and 7) compared to other\nresearch topics. Specifically, GPT-4 performed impressively, peaking at 30.9% in \u201cWasteW\u201d and 36.3%\nin \u201cDrinkingW\u201d in the five-shot setting. Gemini stood out significantly among other 6 LLMs in both\nzero-shot and five-shot settings, especially in \u201cWasteW\u201d and \u201cDrinkingW,\u201d where it scored a\nremarkable 40.4% and 43.5% in the five-shot setting, respectively, indicating its exceptional capability\nin pinpointing detailed research gaps. Llama3 exhibited moderate performance compared to GPT-4,\nwith a strong 34.5% in \u201cDrinkingW\u201d in the five-shot setting. In contrast, GLM-4, QWEN, and ERNIE\nshowed relatively lower capabilities compared to GPT-3.5.\nAlthough the evaluation scores obtained using automatic evaluation methods were relatively low\n(Table 2 and Table 3), this can be attributed to the irrelevant content generated by LLMs during title\ngeneration and research gap identification (Tables S3 and S4). Our evaluation suggested that LLMs,\nparticularly Gemini, can generally be trusted for executing academic tasks, while users needed to\ncarefully remove or revise irrelevant key information generated from some of LLMs (Tables S3 and\nS4). Additionally, we found that LLMs exhibited varying capabilities across different types of\nacademic tasks, such as generating titles or identifying research gaps, as well as across different water\ntopics, including \u201ctreatment processes for wastewaters\u201d, \u201cenvironmental restoration\u201d, \u201cdrinking water\ntreatment and distribution\u201d, \u201csanitation\u201d, \u201canaerobic digestion\u201d and \u201ccontaminants and related water\nquality monitoring and assessment\u201d. Therefore, further fine-tuning or retrieval-augmented generation"}, {"title": "", "content": "(RAG) with targeted datasets was necessary to specifically develop a genuinely effective water\nresearch expert model."}, {"title": "Implications", "content": "While innovative LLMs have been developed, their application in real-world water-related\ncontexts remains unexplored. In this study, we introduced WaterER, a pioneering evaluation suite\ntailored to measure the advanced knowledge and reasoning abilities of foundation LLMs in water\nengineering and research. We investigated a crucial question: How well do current LLMs meet the\nneeds of water engineering and research? We highlighted the robust performance of GPT-4 across both\nwater engineering and water research tasks, underscoring its versatility and broad applicability for\ntasks requiring both technical precision and nuanced comprehension. Gemini was particularly effective"}, {"title": "", "content": "at understanding and generating academic language, showing enhanced capability in producing and\nanalyzing research content. Surprisingly, Llama3 showed strongest capacity to answer Chinese water\nengineering questions. The Chinese-orientated models, such as GLM-4, ERNIE4.0 and QWEN,\nexhibited comparable performance to GPT-4 in water engineering tasks. For various water research\nquestions, we observed that current LLMs excel in generating precise research gaps for papers related\nto \"Contaminants and related water quality monitoring and assessment\". Additionally, these models\nwere adept at creating appropriate titles for research papers in the fields of \u201cTreatment processes for\nwastewaters\u201d, \u201cEnvironmental restoration\u201d and \u201cDrinking water treatment\u201d. Overall, we suggested\nthat while LLMs identify correlations for effective predictions in water engineering and research tasks,\nthey may not fully address complex questions in these fields. Therefore, the next step of our research\ninvolves enhancing LLMs by integrating specialized water engineering and scientific knowledge,\ninformed by the benchmarking the strengths and limitations of the current models (Table 2 and Table\n3).\nLooking ahead, as LLM technologies evolve and the water-related communities face new\nchallenges and changes, evaluation benchmarks will be crucial for guiding the development of LLMs\ntailored to water-related knowledge. These benchmarks are fundamental in ensuring that LLM\nadvancements remain aligned with the dynamic needs and obstacles within the water sectors.\nTherefore, it is essential to establish comprehensive and evolving benchmarks for LLMs in water\nengineering and research (Figure 1). However, achieving these benefits requires robust collaboration\nto form a benchmark committee. This committee should involve reproducible and standardized\nmethodologies, open computational resources, and a committed community from academia, industry,"}]}