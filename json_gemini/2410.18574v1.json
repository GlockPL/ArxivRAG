{"title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning", "authors": ["Shivam Adarsh", "Kumar Shridhar", "Caglar Gulcehre", "Nicholas Monath", "Mrinmaya Sachan"], "abstract": "Large Language Models (LLMs) can transfer their reasoning skills to smaller models by teaching them to generate the intermediate reasoning process required to solve multistep reasoning tasks. While LLMs can accurately solve reasoning tasks through a variety of strategies, even without fine-tuning, smaller models are not expressive enough to fit the LLMs distribution on all strategies when distilled and tend to prioritize one strategy over the others. This reliance on one strategy poses a challenge for smaller models when attempting to solve reasoning tasks that may be difficult with their preferred strategy. To address this, we propose a distillation method SIKeD: Self-guided Iterative Knowledge Distillation\\u00b9, where the LLM teaches the smaller model to approach a task using different strategies and the smaller model uses its self-generated on-policy outputs to choose the most suitable strategy for the given task. The training continues in a self-guided iterative manner, where for each training iteration, a decision is made on how to combine the LLM data with the self-generated outputs. Unlike traditional distillation methods, SIKeD allows the smaller model to learn which strategy is suitable for a given task while continuously learning to solve a task using different strategies. Our experiments on various mathematical reasoning datasets show that SIKeD significantly outperforms traditional distillation techniques across smaller models of different sizes.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), with tens to hundreds of billions of parameters, generally outperform smaller models (with billions of parameters or fewer) in a variety of reasoning tasks [1, 34]. One notable strength of large models is their ability to reason and perform multistep reasoning tasks, often considered an important aspect of intelligence [10]. However, the significant size and computational demands of these large models present several challenges. For example, LLaMA3 models [34] are trained using clusters of 24,000 GPUs, limiting their accessibility to many researchers and practitioners.\nTo bridge this gap, a key approach involves teaching smaller models to replicate the knowledge of a larger model, often referred to as knowledge distillation [13]. Typically, smaller models can be taught to replicate the multistep reasoning capabilities of larger models by incorporating a set of intermediate sequences [18, 31]. However, these intermediate steps can be derived from a number of different"}, {"title": "2. Preliminaries: LLM based Distillation", "content": "Problem Setup We consider the standard setup of LLM-based distillation (also referred to as knowledge distillation), where the data is sampled from the larger model (LLM) with intermediate reasoning and the smaller model is fine-tuned (distilled) over it [23, 31]. Two auto-regressive sequence models are involved in the process: a larger model or the LLM denoted as $p_L$ and a smaller model to be distilled as $p_{\\theta}^M$ (with learnable parameters $\\theta$). In this work, we consider a reasoning dataset $D$ consisting of a question $q_i$ and a numerical answer $a_i$ pairs for $n$ data points, i.e. $i \\in \\{1, ..., n\\}$. Since our work focuses on improving reasoning in smaller models by teaching them to solve a variety of reasoning strategies, we consider three reasoning strategies in this work: Chain-of-Thought (CoT), Least-to-Most (L2M), and Program-of-Thought (PoT). For a specific reasoning strategy, denoted as $s \\in S$, we generate the reasoning chain or rationale, denoted as $r_i$ leading to the final answer as: $r_i \\sim p_L(. | pr_s, q_i)$, where, $pr_s$ represents the strategy-specific prompt, and $s \\in \\{CoT, L2M, PoT\\}$. Prompts used are provided in section 8."}, {"title": "2.1. LLM based Distillation", "content": "We add the generated rationales to the the dataset $D$ to create an initial training dataset $D_{LLM}$ consisting of a quadruple of $\\{(q_i, a_i, s, r_i)\\}$ for each data point. We perform a data filtering by extracting the final answer $\\hat{a_i}$ from the generated rationale $r_i$ and comparing it with the ground truth answer $a_i$. We discard all samples that do not match, i.e., we keep samples where $\\hat{a_i} = a_i$. This filtering process eliminates incorrect rationales, ensuring that only high-quality data is used for distilling the smaller models.\nWe start the distillation process by training the smaller model with the created dataset $D_{LLM}$. The question $q_i$ is provided as input, and the smaller model $p_{\\theta}^M$ (with learnable parameters $\\theta$) is first instructed to generate the strategy $s$, followed by the rationale $r_i$ that leads to the final answer $a_i$. The loss $L_L(\\theta)$ is defined as:\n$L_L(\\theta) = -E_{(q_i, s, r_i) \\sim D_{LLM}}[\\log p_{\\theta}^M(s | q_i, I) + \\sum_{t=1}^M \\log p_{\\theta}^M(r_{i, t} | r_{i, <t}, s, q_i, I)]$\nwhere $M$ represents the number of tokens decoded over time $t$ in an autoregressive manner, and $I$ is the instruction used during fine-tuning. Note that this is analogous to traditional knowledge distillation from LLMs except that we make a strategy choice before generating rationales.\nLimitations Training solely on LLM-generated data $D_{LLM}$ can lead to a distribution mismatch between the training data and the smaller model's own output distribution. Specifically, the larger model due to its larger capacity, may produce correct reasoning across multiple strategies that the smaller model can find difficult to replicate directly [2]. A comparison of the strategy selected by the LLM and the smaller model on 1K samples is presented in Figure 1. The smaller model performs poorly when generating outputs on its own, as the training data distribution $P_{train}(x)$ is different from the model's output distribution $P_{\\theta}^M(x)$ as $P_{train}(x) = P_{LLM}(x)$, where $x$ represents the samples $(q_i, s, r_i)$, and $P_{LLM}(x)$ is the distribution of data generated by the LLM $p_L$."}, {"title": "3. SIKED: Self-guided Iterative Knowledge Distillation", "content": "We propose SIKeD, an iterative training where smaller models can take advantage of their own generations to refine their strategy choices for a given task. In a nutshell, we generate data from the smaller model, filter out the correct samples based on whether the generated solutions are correct, and mix this data with the LLM-generated data to adjust its strategy preferences. The smaller distilled model is used to iteratively generate data in an on-policy setting where it updates itself by leveraging both the LLM data and its own generations. This iterative process allows the smaller model to improve its reasoning abilities and strategy selection over time by leveraging the LLM's knowledge and its own prior learning. The following paragraphs discuss the steps involved in our proposed iterative distillation methodology and the training objective.\nData generation For each question $q_i$ and its associated reasoning strategy $s$, we first generate $K$ rationales using the current smaller model $p_{\\theta}^M$ as: $r_i^{(k)} \\sim p_{\\theta}^M(\\cdot|s, q_i, I)$, for $k = 1, ..., K$. Note that we generate multiple samples $K$ as the likelihood of a correct answer being present in one of the rationales increases significantly with additional generations for smaller models [17, 36].\nData Filtering Next, we extract the predicted answer $a_i^{(k)}$ from each rationale $r_i^{(k)}$ and compare it with the ground truth $a_i$. We collect the correct samples, where $a_i^{(k)} = a_i$, into a new dataset $D_{self} = \\{(q_i, s, r_i^{(k)}): a_i^{(k)} = a_i\\}.\nData mixing We combine the LLM-generated dataset $D_{LLM}$ with the self-generated dataset $D_{self}$ to form the mixed dataset $D_{mix} = D_{LLM} \\cup D_{self}$.\nNote that, we do not always use all the data from LLM in $D_{mix}$, and study two variations: All when all LLM data is used in $D_{mix}$, and Adaptive when only queries that have no correct generations in $D_{self}$ are taken from $D_{LLM}$. Adaptive uses less generated data from the LLM, resulting in more computationally efficient training.\nThe corresponding training data distribution changes to a mixture of the LLM data distribution and the model's own output distribution:\n$P_{train}^{(2)}(x) = \\alpha P_{LLM}(x) + (1 - \\alpha)P_{\\theta}^M(x)$,\nwhere $\\alpha = \\frac{|D_{LLM}|}{|D_{LLM} + D_{self}|}$ serves as a normalized mixing rate between the two datasets."}, {"title": "Training objective", "content": "By including $D_{self}$ in the training data, we reduce the divergence between $P_{train}^{(2)}(x)$ and the model's own output distribution $P_{\\theta}^M(x)$, thus minimizing the distribution shift and improving training effectiveness of choosing the right strategy for a given task.\nWe continue training the smaller model on $D_{mix}$ using the following loss function:\n$L_{mix}(\\theta) = -E_{(q_i, s, r_i) \\sim D_{mix}}[\\log p_{\\theta}^M(s | q_i, I) + \\sum_{t=1}^M \\log p_{\\theta}^M(r_{i, t} | r_{i, <t}, s, q_i, I)]$\nThe expected loss over the training data is:\n$L_{mix}(\\theta) = -E_{x \\sim P_{train}^{(2)}(x)}[\\log P_{\\theta}^M(x)]$\nwhere $x = (q_i, s, r_i)$, and $P_{\\theta}^M(x)$ denotes the probability assigned by the model to the sample $x$.\nAnalogous to minimizing the KL divergence Mixing the data is analogous to minimizing the Kullback-Leibler (KL) divergence [19] between the training data distribution $P_{train}^{(2)}(x)$ and the model's output distribution $P_{\\theta}^M(x)$:\n$D_{KL}(P_{train}^{(2)}(x) || P_{\\theta}^M(x)) = \\sum_x P_{train}^{(2)}(x) \\log \\frac{P_{train}^{(2)}(x)}{P_{\\theta}^M(x)}$\nAs we include more self-generated data, $(1 - \\alpha)$ increases, and $P_{train}^{(2)}(x)$ becomes closer to $P_{\\theta}^M(x)$. This reduces the KL divergence and aligns the training data distribution with the model output distribution,"}, {"title": "3.1. Iterative Self-Training of SIKED", "content": "We repeat the data generation, filtering, mixing, and training steps iteratively. In each iteration $t$, the smaller model potentially generates new correct rationales that are added to the training data. The training data distribution at iteration $t$ becomes:\n$P_{train}^{(t)}(x) = \\alpha^{(t)} P_{LLM}(x) + (1 - \\alpha^{(t)}) P_{\\theta^{(t-1)}}^M(x)$,\nwhere $\\theta^{(t-1)}$ are the model parameters from the previous iteration, and $\\alpha^{(t)}$ is updated based on the sizes of $D_{LLM}$ and $D_{self}^{(t)}$ at iteration $t$. Note that the generated samples from the smaller model automatically govern the value of $\\alpha^{(t)}$ based on the size of $D_{self}^{(t)}$.\nThis iterative process continues until the model's performance converges or a predefined stopping criterion is met. Over multiple iterations, the model's own output distribution $P_{\\theta^{(t)}}^M(x)$ gradually improves, and the training data distribution becomes increasingly aligned with it. We present an end-to-end training methodology in Algorithm 1."}, {"title": "4. Experimental Details", "content": "Dataset Our work demonstrates the effectiveness of selecting an appropriate strategy for a given task. We consider multi-step mathematical reasoning datasets in our work, as various strategies can solve the task fairly well. We trained SIKeD on the GSM8K training set [7], which includes 7,473 samples, and tested it on the corresponding test set of 1,319 samples. To assess the domain transferability of our distillation method, we also evaluated it on three additional mathematical datasets: SVAMP [26] with 1,000 samples, ASDiv [24] with 2,300 test samples, and MultiArith [29] with 180 samples. As the GSM8K training set was used to train the smaller model, we classify it as an in-distribution dataset. In contrast, no training data from SVAMP, ASDiv, or MultiArith was used, as they were exclusively employed for testing purposes and thus considered out-of-distribution.\nImplementation Details We used the Llama3 70B model [8] as the large language model (LLM) to generate the rationales. We performed distillation on different smaller models ranging from 0.5B to 7B parameters, including Qwen2 0.5B [3], Qwen2 1.5B [3], SmolLM 1.7B [16], Gemma 2B [33], and Gemma 7B [33]. All smaller models were fine-tuned using LoRA [15] with a rank of 16, and alpha of 32. We used a learning rate of 3e-4 for Qwen models with a cyclic scheduler, while we set 2e-4 as the learning rate for other models and used a linear scheduler. We train all models for 3 epochs. We implemented all our experiments using the Unsloth FastLanguageModel [35] and used the VLLM library [20] for inference. We set the temperature t = 0 for data generation from the LLM while t = 0.7 was used for generating samples from the smaller model at each iteration. We set the number of generated samples or K to 10. We report Top-1 accuracy (maj@1)."}, {"title": "5. Results and Discussion", "content": "LLM Based Distillation We start by distilling smaller models using the reasoning dataset generated using the LLM in two variations: using data from a single strategy (CoT, PoT, or L2M), and a combination of all three strategies (referred to as \u201cCombined\"). Table 1 compares the accuracies of the approaches across four mathematical datasets. The \u201cCombined\" approach benefited smaller models, yielding slight improvements for the Qwen 0.5B, Qwen 1.5B, and SmolLM 1.7B models. However, it showed little to no improvement, and sometimes even worse performance, for the larger Gemma 2B and 7B models. This indicates that simply merging the distillation data for each strategy is not sufficient for effective multi-strategy distillation."}, {"title": "6. Related Work", "content": "Knowledge Distillation for Reasoning Tasks Knowledge distillation [4, 13] is a widely-used technique for transferring knowledge from a large language model (LLM) to a smaller model. Previous research has focused on transferring intermediate reasoning steps to smaller models, either step-by-step [14, 23, 31] or by combining multiple strategies simultaneously [6, 40]. These approaches can be viewed as aggregating diverse data sources for distillation, similar to the LLM data approach in our work. In contrast, [11] and [38] focus on self-distillation, where a model improves its performance without external data or knowledge. Specifically, [11] leverages word embeddings, while [38] uses temporal model checkpoints as a proxy for ground truth. However, both approaches rely solely on data generated by the smaller model and exclude LLM data. Our method strikes a balance between these two extremes by using LLM data to learn multiple strategies and self-generated data to optimize for the right strategy choice."}, {"title": "7. Conclusion", "content": "We propose SIKeD: Self-guided Iterative Knowledge Distillation, that addresses the challenge of distilling multistep reasoning skills from large language models (LLMs) to smaller models. Unlike traditional methods, which often leave smaller models biased towards a single strategy, SIKeD uses iterative self-guided training, combining LLM and self-generated data to improve overall reasoning in smaller models. We demonstrate our approach across various mathematical reasoning datasets and demonstrate that SIKeD improves the ability of smaller models to handle complex reasoning, achieving significant performance gains."}, {"title": "8. Appendix", "content": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\nInstruction: Solve the given math problem step by step. Put your final answer after 'Final answer:'.\nInput: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total?\nResponse: The shirts cost 3*$20=$<<3*20=60>>60 before tax The tax cost $60*.1=$<<60*.1=6>>6 So in total they paid $60+$6=$<<60+6=66>> 66 Final Answer: 66<eos>\n[7 more examples randomly sampled from the training set]\nInput: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total?\nResponse: Model generated response\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\nInstruction: Solve the given math problem by decomposing it into smaller, manageable sub-questions. Put your final answer after 'Final answer: '.\nInput: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total?\nResponse: Sub-question 1: How much did John spend on shirts?\nAnswer to Sub-question 1: 3*$20=$<<3*20=60>>60\nSub-question 2: How much did John spend on tax?\nAnswer to Sub-question 2: $60*.1=$<<60*.1=6>>6\nSub-question 3: How much did John spend in total?\nAnswer to Sub-question 3: $60+$6=$<<60+6=66>>66\nFinal Answer: 66<eos>\n[7 more examples randomly sampled from the training set]\nInput: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total?\nResponse: Model generated response\n\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\nInstruction: Solve the given math problem by writing a python program. Store your result as a variable named 'answer'.\nInput: John buys 3 dress shirts. They sell for $20 each. He also has to pay 10% tax on everything. How much did he pay in total?\nResponse: total_shirts = 3\ncost_of one_shirt = 20\ntotal_cost_shirts = total_shirts * cost_of_one_shirt\ntax_rate = 0.1\ntax_amount = tax_rate * total_cost_shirts\ntotal_cost = total_cost_shirts + tax_amount\nanswer = total_cost <eos>\n[7 more examples randomly sampled from the training set]\nInput: Thomas is training at the gym to prepare for a competition. He trained for 5 hours every day for a month (30 days). If he continues to train for the next 12 days, how many hours will he spend on training in total?\nResponse: Model generated response"}]}