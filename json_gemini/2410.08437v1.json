{"title": "VUTOVAL: AUTONOMOUS EVALUATION OF LLMS\nFOR TRUTH MAINTENANCE AND REASONING TASKS", "authors": ["Rushang Karia", "Daniel Bramblett", "Daksh Dobhal", "Siddharth Srivastava"], "abstract": "This paper presents \u2200uto\u2203\u2228\u2227L, a novel benchmark for scaling Large Language\nModel (LLM) assessment in formal tasks with clear notions of correctness, such\nas truth maintenance in translation and logical reasoning. Vuto\u2203\u2228\u2227L is the first\nbenchmarking paradigm that offers several key advantages necessary for scaling\nobjective evaluation of LLMs without human labeling: (a) ability to evaluate\nLLMs of increasing sophistication by auto-generating tasks at different levels\nof difficulty; (b) auto-generation of ground truth that eliminates dependence on\nexpensive and time-consuming human annotation; (c) the use of automatically\ngenerated, randomized datasets that mitigate the ability of successive LLMs to\noverfit to static datasets used in many contemporary benchmarks. Empirical\nanalysis shows that an LLM's performance on \u2200uto\u2203\u2228\u2227L is highly indicative of its\nperformance on a diverse array of other benchmarks focusing on translation and\nreasoning tasks, making it a valuable autonomous evaluation paradigm in settings\nwhere hand-curated datasets can be hard to obtain and/or update.", "sections": [{"title": "INTRODUCTION", "content": "Foundation Models such as Large Language Models (LLMs) have been demonstrated to successfully\nperform many natural language tasks involving formal syntax such as autoformalization \u2013 utilizing\nLLMs in converting natural language (NL) to formal syntax (FS) such as source code, math etc.,\n(Wu et al., 2022; Liang et al., 2023; Guan et al., 2023), informalization \u2013 using LLMs to convert FS\nto NL (e.g. code summarization), reasoning \u2013 using LLMs to perform sound reasoning or derive\nproofs. Although these methods have been successful in small-scale scenarios, their effectiveness\nin maintaining truth across NL and FS remains uncertain due to the difficulty in assessing truth\nmaintenance in such tasks. Multiple authors have noted that existing benchmarks and evaluation\nmethodologies for such tasks are susceptible to the Benchmark Contamination Problem due to their\nuse of static datasets, e.g, HumanEval (Chen et al., 2021; Wu et al., 2022; Han et al., 2022). One\neffective method to mitigate this problem in existing benchmarks is creating new data (Xu et al.,\n2024). However, scaling such datasets as LLMs evolve is a tedious and expensive process since their\ndata-generation task requires expert annotators to hand-generate well-balanced datasets. Moreover,\nsuch benchmarks often rely on insufficient/incomplete measures of evaluation (e.g, BLEU scores\n(Callison-Burch et al., 2006), ranking disparities in LLM-generated code on test cases in HumanEval\nvs HumanEval+ (Liu et al., 2023a)), and thus, provide misleading signals on LLM capabilities.\nThis paper addresses three key desiderata for benchmarking LLM capabilities for truth maintenance\nacross NL and FS: (D1) Can we dynamically generate out-of-distribution datasets without relying on\nhuman annotators? (D2) How do we accurately assess an LLM's truth maintenance capabilities?\n(D3) Can our metric serve as a predictor of LLM performance in FS-based tasks?\nFor \u00a7D1, we introduce a new approach that utilizes context-free grammars to generate well-balanced,\nout-of-distribution datasets on the fly. For \u00a7D2, we perform closed-loop testing of LLM capabilities\nusing formal verifiers to automatically evaluate its truth maintenance capabilities. To answer \u00a7D3, we\nshow that our metrics can serve as predictors of LLM performance on other, well-known benchmarks."}, {"title": "FORMAL FRAMEWORK", "content": "(Large) Language Models (L)LMs LMs are non-linear functions represented by (billons of) param-\neters @ that, given a set of input tokens $x_1,..., x_n$, typically representing NL, predict the output token\n$y_{i+1}$ using the distribution $P(y_{i+1}|x_1,..., x_n, y_1, ..., y_i; \u03b8)$. The input tokens contains context \u043a\n(also known as a prompt) that provides the necessary information for the task (e.g., instructions, etc).\nIt is known that \u043a significantly impacts the response quality $y_1,..., y_n$ (Sahoo et al., 2024).\nPropositional Logic Propositional logic is a branch of logic that utilizes propositions and logical\noperators (e.g., conjunction: \u2227, etc) to construct sentences that can be used to perform reasoning\nusing the rules of logic. For example, propositions, $p_1$ = It is raining, $p_2$ = It is sunny can be used\nto create a sentence $P = p_1 \u2228 p_2$. If $P$ is true and $p_1$ is observed, then one can use the rules of\ninference to deduce that $p_2$ is true (Huth & Ryan, 2004).\nEquivalence in Propositional Logic Two sentences in propositional logic, $P_1$ and $P_2$, are equivalent,\n$P_1 \u2261 P_2$, iff their truth values agree for all possible assignments. E.g., $\u00ac(p_1 \u2227 p_2) \u2261 \u00acp_1 \u2228 \u00acp_2$\nsince $p_1, p_2 \u2208$ {True, False} \u00d7 {True, False}, $\u00ac(p_1 \u2227 p_2) \u2261 \u00acp_1 \u2228 \u00acp_2$.\nFirst-order Logic (FOL) FOL differs from propositional logic in that sentences are constructed\nusing predicates, quantifiers, and objects. A popular example is the syllogism where, given two\nfirst-order logic sentences \u2200x. Man(x) \u2192 Mortal(x) and Man(Socrates), one can conclude that\nMortal(Socrates). A first-order logic sentence $F$ can be interpreted using a universe $U$, a substitution\noperator \u03c3, and an interpretation function $I$ (Russell & Norvig, 2020).\nEquivalence in First-order Logic Two sentences, $F_1, F_2$ in first-order logic are equivalent, $F_1 \u2261 F_2$,\niff they are equivalent under all possible models. E.g., $\u00ac\u2200x. Man(x) \u2261 \u2203y. \u00acMan(y)$.\nRegular Expressions A regular expression (regex) is a sequence of characters that can be used to\ndetermine whether a particular string matches the pattern or language induced by the regex. For\nexample, the regex 200(00)*1 using \u2211 = {0,1,2} matches all strings possible using \u2211 that begin\nwith a two, followed by one or more pairs of zeroes, and end with a one (Hopcroft et al., 2001).\nEquivalence between Regular Expressions Two regexes, $R_1$ and $R_2$ are equivalent, $R_1 \u2261 R_2$, if they\nrepresent the same language. It is known that $R_1 \u2261 R_2$ if their corresponding minimal deterministic\nfinite automata (DFAs), $D_1, D_2$, are isomorphic, i.e., $D_1 \u223c D_2$ (Hopcroft et al., 2001).\nWe refer to sentences (strings) in first-order and propositional logic (regexes) as formal syntax FS in\nthis paper. We now provide a definition of (Auto/In)formalization in the context of LLMs and FS.\nDefinition 2.1 (Autoformalization: A). Given an LLM $L$, an NL description \u03c8 and context \u043a,\nautoformalization, $A_L(\u03c8, \u03ba)$, is defined as using $L$ to translate \u03c8 to FS \u03c6 s.t. $A_L^{-1}(\u03c6, \u03ba') = \u03c8$.\nExample One possible autoformalization of \"Every human drinks coffee but some are not dependent\non it\u201d in FOL is \u2200x. Human(x) \u21d2 Drinks(x, Coffee) \u2227 \u2203y. x = y \u2227 \u00acDependent(y, Coffee).\nDefinition 2.2 (Informalization: I). Given an LLM $L$, an expression \u03c6 in FS and context \u043a,\ninformalization, $I_L(\u03c6, \u03ba)$, is defined as using $L$ to translate \u03c6 to NL \u03c8 s.t. $I_L^{-1}(\u03c8, \u03ba') = \u03c6$."}, {"title": "OUR APPROACH FOR ASSESSING TRUTH MAINTENANCE", "content": "We now describe our approach, \u2200uto\u2203\u2228\u2227L, for autonomously assessing an LLM's ability to maintain\ntruth w.r.t. $(A\u25e6I)^n(\u03c6_0)$. \u2200uto\u2203\u2228\u2227L provides dynamically generated datasets that can be scaled\narbitrarily by systematically generating out-of-distribution, well-balanced ground-truth data (\u00a7D1\nSec. 1), provides \u00a7D2 by using intrinsic LLM capabilities to automatically assess $(A\u25e6I)^n(\u03c6_0)$\nwithout requiring any labeled annotations and using formal verifiers to rigorously check and guarantee\nthe correctness of $(A\u25e6I)^n(\u03c6_0)$ without having to engage in an exhaustive search process.\nDynamic Dataset Generation We use context-free grammars (CFGs) (Hopcroft et al., 2001) \u2013 a set\nof production rules over terminal and non-terminal symbols \u2013 for dynamically generating datasets."}, {"title": "DATASETS AND ASSESSMENT FRAMEWORK", "content": "uto\u2203VAL is written in Python 3, includes several pre-computed datasets, and is easily customizable\nfor adding new datasets, prompts, LLMs, etc. We now describe the datasets and metrics that any\nnewly developed LLM can be evaluated on by using \u2200uto\u2203\u2228\u2227L out-of-the-box.\nPre-generated Datasets and Dynamic Dataset Generator We provide 5 datasets using the grammars\nin Fig. 2. All datasets are arranged based on the descriptional complexity d (# of operators for logic,\nparse tree depth for regex) with around 500 samples per complexity for a total of 20k samples per\ndataset. Other dimensions for categorization are available as metadata. Our overall dataset's total\nnumber of unique samples |D*| is ~ 85k. We also provide zero-shot and 2-shot prompts for each\ndataset, making the total dataset size 170k for off-the-shelf evaluation and continual assessment of\nany new LLMs. We now describe the datasets, followed by the prompts and assessment metrics.\n(Fig. 2a) k-SAT(n) Dataset (|D*| ~ 10k) The terminal \u03bd is replaced with a vocabulary of n\npropositions $p_1,..., p_n$. This dataset is used for prompt calibration due to its toy structure.\n(Fig. 2b) Propositional Logic: PL(n) Dataset (|D*| ~ 19k) Similar to k\u2013SAT, this dataset also\nreplaces terminals by randomly selecting from a list n propositions.\n(Fig. 2c) First-order Logic: FOL(np, no) Synthetic (S), English (E) Datasets (|D*| ~ 19k each)\nWe use the Prenex Normal Form where all quantifiers appear first. The terminals p are replaced\nwith predicates of the form $p(v_1,..., v_n)$ where $p_i$ is a predicate name selected from a list of $n_p$\npredicates, $v_i$ is either an object o from a list of $n_o$ objects or is a free variable $f \u2208 \\{x_1,x_2,...\\}$ that\nis appropriately annotated within the scoping rules of the parentheses. The objects and predicate\nnames are auto-generated synthetically for the synthetic version of the dataset. The English version\nof the dataset uses VerbNet (Schuler, 2005) for predicate names and Faker (Faraglia, 2024) for object\nnames. This version of the dataset allows for informalization to produce more abstract sentences\nthat closely resemble the NL statements in SOTA autoformalization datasets. For example, an FS\nstatement Boom(Richard) \u2227 Exercise(Yolonda) yields a more natural NL statement such as \"The\nexpression states that Richard does not experience a boom, and Yolonda does not engage in exercise\".\n(Fig. 2d) Regular Expression: RE(n) Dataset (|D*| ~ 18k) The vocabulary \u2211 is the set {0, . . ., n-\n1} where n is a user-specified constant."}, {"title": "ASSESSMENT OF SOTA LLMS ON THE VUTO\u2203\u2228\u2227L BENCHMARK", "content": "Currently, as new LLMs are developed, they are evaluated against the same static benchmarks, leading\nto misleading evaluations since these benchmarks were likely included in the LLMs pretraining\nprocess and thus are likely to be contaminated. To motivate research in the area and showcase our\nframework's strengths, we used \u2200uto\u2203\u2228\u2227L to evaluate several SOTA closed and open-source LLMs.\nFor paid APIs, we used OpenAI's GPT-3.5-turbo (ChatGPT), GPT-4o, and GPT-4o-mini (OpenAI,\n2023a;b; 2024). For open-source LLMs, we utilized Meta's LLama-3-8B-Instruct (AI@Meta, 2024),\nMistral AI's Mistral-v0.2-7B-Instruct (Jiang et al., 2023b), and Microsoft's Phi-3-medium-4k-instruct\n(Microsoft, 2024). These models provide a good mix of current SOTA LLM technology in many\ntasks. We analyze \u00a7A1 using Fig.3 and Fig. 4. We analyze \u00a7A2, A3, and A4 using results obtained by\nusing uto\u2203VAL on our generated datasets (Fig. 5). We present our analyses below.\n\u00a7A1: Is performance on \u2200uto\u2203\u2228\u2227L indicative of performance on other benchmarks Our hypothesis is\nthat the ability for truth maintenance on foundational concepts (propositional logic, first-order logic,\nregexes, etc.) will be indicative of an LLM's reasoning abilities. To evaluate this, we compare the\nperformance of LLMs on \u2200uto\u2203\u2228\u2227L vs. performance in other benchmarks focused on FS-tasks such\nas reasoning, autoformalization, etc. Our results (Fig. 3) indicate that there is a positive correlation\nbetween LLM performance on \u2200uto\u2203\u2228\u2227L and other logic-based benchmarks on a myriad of tasks\nsuch as autoformalization, logical reasoning, code generation, etc.\nWe use 5 popular benchmarks: (a) FOLIO(R;{NL,FOL}) (Han et al., 2022), a popular logical\nreasoning benchmark with ground truth in both NL and FS; (b) FOLIO({A/I}) evaluates if an LLM\ncan (auto/in)formalize NL (FS) accurately; (c) LogiEval(R;{PL,FOL}) (Patel et al., 2024) a reasoning\nbenchmark with ground truth in propositional and first-order logic; (d) HumanEval(A) (Chen et al.,\n2021), a code autoformalization benchmark; (e) Big Bench Hard (BBH) (Suzgun et al., 2023). These\nbenchmarks are contrasted in Sec. 6, and example prompts of these benchmarks are included in\nAppendix K. We ran 5 runs across all these benchmarks except HumanEval and BBH (due to resource\nlimitations) using the most comparable \u2200uto\u2203VAL dataset. For HumanEval and BBH, we use the\nreported numbers in the literature as scores for the models (sources are included in Appendix K).\nOur results (Fig. 3) show that \u2200uto\u2203\u2228\u2227L exhibits a strong, positive correlation with other static\nbenchmarks on FS-based tasks. For reasoning tasks, our approach exhibits a strong correlation (p >\n0.9) on both the natural language and first-order logic versions of the static, hand-annotated FOLIO\ndatasets. However, \u2200uto\u2203\u2228\u2227L evaluates truth maintenance capabilities of LLMs by automatically\ngenerating its own data and without requiring any hand-annotation. Similar results using Vuto VAL\ncan be observed in LogiEval for propositional logic. Our results were not statistically significant\nfor the FOL version of LogiEval. We investigated and found that GPT-4o-mini was unable perform\nlonger chains of reasoning on the FOL problems in LogiEval (LogiEval includes problems that\nrequire multiple reasoning chains with different rules of inference). Similarly, \u2200uto\u2203\u2228\u2227L also serves\nas a predictor for autoformalization, as evident in our results on FOLIO (A) and HumanEval."}, {"title": "RELATED WORK", "content": "There is a large body of work for using LLMs w.r.t. FS, e.g., LeanDojo (Yang et al., 2023a). We limit\nour discussion to benchmarks focusing on FS-based tasks, e.g., reasoning, autoformalization, etc.\nLogical Reasoning RuleTaker (Clark et al., 2020) and ProntoQA (Saparov & He, 2023) generate\ndatasets by using simple \u201cif-then\" and syllogisms rules to create reasoning questions. Similar\ngrammars are used by LogicNLI (Tian et al., 2021) and CLUTRR (Sinha et al., 2019) for generating\ndatasets for inductive reasoning. While these techniques are dynamic, they are limited in their ability\nto produce interesting reasoning problems across different domains. LogiEval (Patel et al., 2024)\nuses fixed inference rules and LLMs to generate reasoning problems limiting the number of unique\nsyntax trees possible. \u2200uto\u2203\u2228\u2227L is multi-dimensional providing 5 different datasets, allows multiple\ncustomization options, and can generate an infinite number of unique syntax trees.\nFOLIO (Han et al., 2022) utilizes human experts to generate a set of reasoning questions based on\nreal-world text sources. They generate questions in both NL and FS for propositional and first-order\nlogic that require 7 levels of reasoning. Similarly, ReClor (Yu et al., 2020) and (Srivastava et al.,\n2023) utilize hand annotations to generate commonsense reasoning datasets. One key weakness of\nthese approaches is the reliance on human experts to create the dataset.\nAutoformalization HumanEval is a popular benchmark for evaluating LLM capabilities of autofor-\nmalizing source code. LLM autoformalizations are evaluated via hand-written test cases. It has been"}, {"title": "CLOSING REMARKS", "content": "Conclusions This paper introduced \u2200uto\u2203\u2228\u2227L, a new benchmark and dynamic datasets for au-\ntonomous assessment of LLM capabilities in the truth maintenance task. Our approach to dataset\nsynthesis allows us to scale without requiring any human labeling. Our approach for assessment\nevaluates $(A\u25e6I)^n(\u03c6_0)$ without any human intervention and provides accurate results by using\nverifiers to guarantee correctness over all inputs. Our framework is easily extensible and provides\nseveral prepackaged datasets to quickly assess LLMs. Furthermore, our evaluation indicates that\nSOTA LLMs and LRMs are not performant in this task. Finally, we show that our metric can be\nused as an indicator of performance on other FS-based tasks and thus can be used as a surrogate\nbenchmark for evaluating new LLMs as and when they are developed.\nBroader Impact This work introduces a way to automatically assess whether LLMs can understand\ntheir own generations and preserve their truth while automatically scaling datasets. Applications of\nthis work could be used by researchers or industries to robustly evaluate the performance of LLMs\nin this task. These results could be used to determine the suitability and safety of using LLMs in\nFS-based tasks such as autoformalization, code generation, etc. As our results show, \u2200uto\u2203VAL can\nbe used as a surrogate to estimate performance when new LLMs are developed.\nLimitations and Future Work One interesting extension of current work is to utilize the \u03bb-calculus\nto further expand the datasets that can be generated. One limitation of our current framework is\nthat the NL is assumed to use the English vocabulary. Adding support for other languages is an\ninteresting extension that we leave to future work. Another limitation pertains to the use of formal\nverifiers. It is well-known that first-order logic is undecidable (Huth & Ryan, 2004). We mitigate this\nby using FS verifiers loaded with the appropriate timeout and logging mechanisms. Another way to\nmitigate this issue is to use grammars that generate strings from decidable fragments of the formal\nsyntax. One interesting application of \u2200uto\u2203VAL is to use the generated evaluations as datasets for\nback-translation to improve the autoformalization capabilities of models (Jiang et al., 2023a).\nThreats to Validity Our reported results for paid APIs are dependent on the model checkpoints used\nto be available. Most paid APIs provide a disclaimer that checkpoints may be deleted without notice.\nAs is the case with several evaluation methodologies for LLMs, one must be careful when interpreting"}, {"title": "APPENDIX ORGANIZATION", "content": "The appendix is organized as follows. Appendix B provides the algorithm used for dataset generation.\nAppendix C discusses prompt tuning and validating our prompts on 3SAT. Appendix D provides\nthe parameters we used when generating the five datasets discussed in the paper. Appendix E\nprovides additional information on our experimental setup, including the computational resources\nused. Appendix F discusses the prompts and provides examples. Appendix G is our detailed analysis\nof the empirical results from the main paper. Appendix H discusses an experiment we ran to evaluate\nthe standard deviation error. Appendix I includes additional results from our zero-shot prompting\nexperiments using other metrics for categorization. Appendix J evaluates an experiment we performed\ncomparing few-shot prompting compared to zero-shot. Finally, Appendix K provides the experimental\nsetup of the benchmarks we evaluated, data values and sources of scores collected, the uto\u2203VAL\nscores used for comparison, and additional correlation results."}, {"title": "DATASET GENERATION", "content": "In this section, we provide the algorithm for generating formal syntax (FS) expressions and show that\nit can generate all possible expressions from the grammar and vocabulary.\nOur approach, \u2200uto\u2203\u2228\u2227L, generates datasets by constructing a context-free grammar (CFG) tree\nusing the grammars discussed in Section 4. Since it is intractable to generate the full tree, we control\nthe branching factor and randomly expand the branches of this tree to generate formulae."}, {"title": "3-SAT PROMPT CALIBRATION", "content": "In this section, we discuss the KSAT results used to calibration the prompts.\nWe tested several prompts for 3-SAT to verify that our prompts are sufficient to prompt the LLM\nto correctly perform informalization and autoformalization. Additionally, we verified that the\nequivalence verification prompt prompted the LLMs to give an accurate yes-or-no answer. The\nperformance of all six LLMs on 3-SAT for \u00a7A2, \u00a7A3, and \u00a7A4 are shown in Figure 7."}, {"title": "DATASET GENERATION HYPERPARAMETERS", "content": "In Table 1, we provide the hyperparameters used to generate the five datasets."}, {"title": "EXPERIMENTAL SETUP", "content": "In this section, we will provide the details of our experimental setup for generating the datasets and\nrunning uto\u2228\u2227L for evaluating each LLM's performance.\nWe ran our experiments using Python 3.10.13 with package versions shown in Table 2. We also\nrepackaged Prover9 (McCune, 2010) to improve performance where this repackaged version can be\nfound in our code base.\nDataset Generation: We generated five datasets using the\ndataset generation algorithm with the hyperparameters shown in\nTable 1 using the number of operators as the categorization metric\nfor all but regular expression, where we used CFG tree depth. We\ngenerated 10 batches for each dataset, resulting in approximately\n20k samples for each dataset with an equal distribution for each\noperator number.\nEvaluating and Verification: The closed-source models (GPT\n3.5-turbo, GPT-4o, and GPT-4o-mini) were accessed using their\nAPI using a temperature of 0.1. The open-source models LLama\n3-8B-Instruct and Mistral-v0.2-7B-Instruct were locally hosted\non a server with a 13th Gen Intel(R) Core(TM) i9-13900K and\nNvidia RTX 4090 GPU using the model's default parameters\nwith a temperature of 1. Similarly, Phi-3-medium-4k-instruct was\nlocally hosted on a server using a Nvidia A100-XM4-80GB GPU.\nVerification was performed on an AMD EPYC machine with 128\ncores."}, {"title": "FIRST-ORDER LOGIC RESULTS", "content": "Informalization Errors: Similar to propositional logic, we observed the LLM often failed providing\nenough details resulting in incorrect formulas being generated. A significant source of errors we"}, {"title": "REGULAR EXPRESSION RESULTS", "content": "Informalization Errors: Most of the errors observed were the LLMs giving the wrong explanation,\neven for simple regular expressions. For example, GPT-4o often described c* as \"one or more\noccurrences of 'c'\", where c is a character from the alphabet (see example 1 in Table 5). For the\nother LLMs, it was quite common for the explanation to not give the actual character (see example\n4 in Table 5). Overall, we observed a higher likelihood of SOTA LLMs hallucinating on regular\nexpressions compared to the other datasets."}, {"title": "STANDARD DEVIATION EVALUATION", "content": "In this section, we perform an empirical analysis of the standard deviation of the syntactic compliance\nand accuracy of the uto\u2203V\\L results. Due to the 10 batches having different data, the standard\ndeviation cannot be computed reliably based on the performance of the individual batches. We\nevaluated the standard deviation by running \u2200uto\u2203V\u2227L 10 times on the first batch of each dataset\ncomposed of 1974 propositional logic, 1900 first-order logic, and 1842 regular expressions examples.\nAdditionally, we evaluated GPT-3.5-turbo (ChatGPT) with a temperature of 1, LLama-3-8B-Instruct,\nMistral-v0.2-7B-Instruct, and Phi-3-medium-4k-instruct. We calculated the mean and standard\ndeviation of each independent run of \u2200uto\u2203VAL and plotted the results in Figure 8.\nFor propositional and first-order logic, the standard deviation of the evaluated LLMs is low. While\nnoisier, the standard deviation of the regular expression results were still less than 20% with the\nbetter performing models having a lower standard deviation. Overall, this experiment shows that the"}, {"title": "FEW-SHOT PROMPTING RESULTS", "content": "In this section, we discuss our few-shot prompting experiment and analyze the performance difference\nbetween zero-shot and few-shot prompting on \u00a7A1 and \u00a7A2.\nWe evaluated on the same five datasets from the main paper's experiments but inserted two exam-\nples into the prompts. First-order and predicate logic used the same two examples, while regular\nexpressions used their own two examples. In Figure 16, the performance difference of each LLM\nwhen using few-shot prompting instead of zero-shot is shown. Using few-shot prompting increases\nsyntactic compliance as the model has access to the desired format for encoding and decoding.\nFor expressions with lower complexity, this translates to a better performance on \u00a7A2. However,\nas complexity increases, the performance difference between zero-shot and few-shot prompting is\nnegligible due to having the correct format for parsing but failing maintaining the same formula."}, {"title": "OTHER BENCHMARK CORRELATION AND \u2200UTO\u2203VAL PREDICTIVE POWER EVALUATION", "content": "For evaluating the correlation between a LLM's performance on \u2200uto\u2203\u2228\u2227L and existing benchmarks\nand measuring the predictive power of \u2200uto\u2203\u2228\u2227L, in Section 5, we evaluated on FOLIO (Han et al.,\n2022), Multi-LogicEval (Patel et al., 2024), and HumanEval (Chen et al., 2021). In this section we\ndiscuss these experiments and cite the sources of the HumanEval results along with evaluate the\npredictive power of \u2200uto\u2203\u2228\u2227L.\nIn this section, we discuss the experimental setup for the benchmark, the sources used for LLM\nperformance on other benchmarks, and the uto\u2203VAL we used for evaluation. We also evaluate the\nFOLIO premise benchmark further based on the operator numbers in each premise."}, {"title": "FOLIO EXPERIMENTAL SETUPS", "content": "The FOLIO dataset is composed of premises and a conclusion for each sample where the task is\nto conclude whether the conclusion is true, false, or unknown given the premises. Additionally,\nthe dataset provides an encoding into first-order logic for all the premises and conclusions. There-\nfore, we evaluated each LLM on their abilities to (1) informalize a first-order logic premise, (2)\nautoformalize a natural language premise, (3) correctly classifying the conclusion using the first-\norder logic representations, and (4) correctly classifying the conclusion using the natural language\nrepresentations."}, {"title": "COMPUTED UTOVAL CONDITIONAL PERFORMANCE", "content": "To compare against the performance on different benchmarks in Section 5, we needed to calculate the\nconditional performance of each LLM's on uto\u2203\u2228\u2227L for the relevant portions of the datasets. For\nexample, there are few premises in the FOLIO dataset with more than 6 operators meaning that the\nmost accurate comparison would be to evaluate our first-order logic dataset up to the same number of\noperators. Therefore we calculated the accuracy on the first-order logic formulae with less than seven\noperators when calculation the correlation and predictive power. On MultiLogiEval, the number of\noperators are dictated by the depth of the rules so we took the average of all first-order logic examples\nup to 30 in our dataset. On HumanEval, to the best of our knowledge using the average of regex with\nCFG tree depth up to 7 is the best comparison."}, {"title": "FOLIO ADDITIONAL CORRELATION FIGURES", "content": "In Section 5, we evaluated the correlation of other benchmarks compared to uto\u2203VAL. For the\nFOLIO dataset, we were able to calculate the exact number of operators in each problem allowing\nus to plot points comparing the autoformalization and informalization accuracy for each operator\nnumber class to directly compared to the accuracy of the same number operators in the first-order\nlogic dataset we generated.\nWe plot these results in Figure 17 with the Pearson co-relation coefficient. Each figure shows a moder-\nate to strong correlation with a statistically significant p-value of less than 0.05. As the computational\ncomplexity increases, performance on \u2200uto\u2203\u2228\u2227L, autoformalization, and informalization decreases.\nThe autoformalization coorelation is significantly stronger due to the informalization evaluation\nmetrics being much weaker at evaluating truth maintenance."}]}