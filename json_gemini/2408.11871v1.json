{"title": "MegaFake: A Theory-Driven Dataset of Fake News Generated by Large Language Models", "authors": ["Lionel Z. WANG", "Yiming MA", "Renfei GAO", "Beichen GUO", "Zhuoran LI", "Han ZHU", "Wenqi FAN", "Zexin LU", "Ka Chung NG"], "abstract": "The advent of large language models (LLMs) has revolutionized online content creation, making it much easier to generate high-quality fake news. This misuse threatens the integrity of our digital environment and ethical standards. Therefore, understanding the motivations and mechanisms behind LLM-generated fake news is crucial. In this study, we analyze the creation of fake news from a social psychology perspective and develop a comprehensive LLM-based theoretical framework, LLM-Fake Theory. We introduce a novel pipeline that automates the generation of fake news using LLMs, thereby eliminating the need for manual annotation. Utilizing this pipeline, we create a theoretically informed Machine-generated Fake news dataset, MegaFake, derived from the GossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake dataset. We believe that our dataset and insights will provide valuable contributions to future research focused on the detection and governance of fake news in the era of LLMs.", "sections": [{"title": "1 Introduction", "content": "In the digital age, fake news has become a significant threat to the online environment. Before the advent of large language models (LLMs), creating fake news was a labor-intensive process, often carried out manually by malicious actors, which made the large-scale production of fake news challenging and costly. However, the recent development of LLMs, such as OpenAI's GPT series [1], has dramatically transformed the landscape of online content creation. These sophisticated AI models are capable of generating text that closely mimics human writing and have been applied across various domains. While LLMs provide substantial benefits in automating and enhancing content generation [32], it is crucial to recognize the risks associated with their potential misuse [12].\nA critical concern is the exploitation of LLMs by malicious actors to produce extensive volumes of fake news. These LLM-generated articles often mimic the writing style and language of legitimate sources, posing a significant risk of misleading readers and undermining the efforts of genuine content creators. Therefore, the study of fake news generated by LLMs has become an urgent priority. Although existing research [31, 6, 15] has delved into the detection of LLM-generated fake news, there is a notable lack of a publicly available, large-scale dataset specifically designed for this purpose. The advanced generative capabilities of LLMs blur the distinctions between their fake news output and that created by humans, complicating the collection of such data. Therefore, it is imperative to understand the deceptive motivations and mechanisms by which LLMs are used to create fake news and to develop a corresponding dataset to support future research."}, {"title": "2 Related Work", "content": "In this paper, we introduce the Machine-generated Fake dataset named MegaFake, a comprehensive dataset comprising four types of fake news and two types of legitimate news generated by LLMs. More specifically, social psychological theories are employed to generate the fake news in the era of LLMs, and two strategies are proposed to produce the legitimate news. Built upon the GossipCop dataset, MegaFake encompassess 46,096 instances of fake news and 17,871 instances of legitimate news. This dataset represents the first publicly available, large-scale collection of fake news generated by LLMs. It is grounded in a comprehensive theoretical framework we developed, known as LLM-Fake Theory, which guides the generation of fake news using LLMs. Our key contributions are as follows:\n\u2022 We develop the LLM-Fake Theory, which identifies four methods for using LLMs to generate fake news, each supported by a social psychological theory that explains the deceptive motivations and mechanisms involved. Additionally, we explore two strategies for producing legitimate news with LLMs. Our theory specifically addresses intentional fake news, which is distinct from unintentional misinformation or \"hallucinations\" often generated by LLMs.\n\u2022 We introduce a novel pipeline that is informed by the LLM-Fake Theory to facilitate the generation of fake news on a large scale without the need for manual annotation. Utilizing this pipeline, we have created the MegaFake dataset.\n\u2022 We show that the MegaFake dataset effectively reduces the prediction biases often observed in fake news detection models trained on imbalanced datasets. Our experiments also reveal that natural language understanding (NLU) models surpass natural language generation (NLG) models in this task. Interestingly, we observe that smaller LLMs perform better than larger models in distinguishing between legitimate and fake news."}, {"title": "2.1 Fake News Definition", "content": "The term \"fake news\u201d is commonly used but insufficient to encompass all related issues. According to Murayama [24], definitions of fake news range from broad to narrow. Wardle [36] provides a broad definition, stating that \"fake news is false news.\" Similarly, Lazer et al. [19] define it as \"fabricated information that mimics news media content in form but not in organizational process or intent.\" On the narrow end, Allcott and Gentzkow [3] describe fake news as \u201ca news article that is intentionally and verifiably false,\" while Zhang et al. [41] characterize it as \"all kinds of false stories or news that are mainly published and distributed on the Internet, in order to purposely mislead, befool or lure readers for financial, political, or other gains.\u201d Most studies emphasize the \u201cintention\" behind fake news, focusing on the deliberate spread of false information. In this regard, we propose a theoretical framework informed by various social psychological theories to understand the deceptive motivations behind fake news creation. This framework will guide the development of prompts for LLMs to generate fake news."}, {"title": "2.2 Fake News Detection", "content": "The rapid proliferation of fake news has caused significant damage on various society aspects, including stock markets, political events, and public health [24]. Social media platforms like Facebook, Twitter, and Weibo exacerbate the spread of fake news due to the echo chamber effect. This prevalence of fake news undermines the credibility of online journalism and can trigger widespread panic [8]. Consequently, it is essential to develop robust methods for detecting, verifying, mitigating, and analyzing fake news [24].\nTraditional methods for fake news detection have utilized a blend of content-based and context-based features. Content-based approaches analyze linguistic elements, such as syntax and semantics, to identify inconsistencies that suggest falsehoods [43]. In contrast, context-based methods focus on the propagation patterns of news articles, employing social network analysis to identify anomalies [30, 34]. Recent advances in natural language processing have introduced sophisticated models that utilize deep learning to improve fake news detection. Models such as BERT [10] and RoBERTa [20] have shown exceptional accuracy in various classification tasks, including fake news detection [17]."}, {"title": "2.3 LLM-Generated Fake News", "content": "LLMs like GPT-4 [1] have demonstrated remarkable abilities in generating text that closely mimics human writing. However, their potential misuse for creating high-quality fake news has sparked significant ethical concerns. Research has shown that LLMs can produce convincingly realistic fake news articles, making them difficult to differentiate from authentic news by both humans and automated systems [40]. The ethical implications of LLM-generated fake news call for a deeper understanding of the motivations and mechanisms behind such misuse. Various studies have investigated the social and psychological factors that drive the creation and spread of fake news, underscoring the urgent need for effective countermeasures [25]."}, {"title": "2.4 Fake News Datasets", "content": "The development and evaluation of detection models heavily depend on the construction of comprehensive fake news datasets. To support research in this domain, several datasets have been introduced. For example, the GossipCop dataset [29] offers a substantial collection of labeled fake news articles along with associated social media posts, and it has become a popular resource for training and benchmarking fake news detection models [34, 29, 35].\nHowever, these existing datasets frequently exhibit biases that can negatively impact model performance. The need for datasets that mitigate these biases is critical, as highlighted by the limitations observed in the GossipCop dataset [29]. In response, Su et al. [31] introduced enhancements to the GossipCop and PolitiFact datasets, incorporating LLM-paraphrased legitimate news and LLM- generated fake news to better mirror the evolving dynamics of news production. Their research illustrates that a balanced dataset construction approach, which includes both human-written and LLM-generated content, is essential for developing effective and robust fake news detectors."}, {"title": "3 Method", "content": "Drawing on an extensive review of literature on LLMs and integrating insights from various social psychology theories, we develop the LLM-Fake Theory. This theoretical framework is designed to analyze the social psychological rationale behind the creation of fake news and to categorize LLM-generated content into two types of legitimate news and four types of fake news. These categories have been meticulously defined to encompass a wide range of news scenarios, from the ethical enhancement of writing and language using LLMs [21, 27, 32] to the malicious generation of fake news [28, 37, 42]. To facilitate this, we have carefully developed specific prompts for creating each type of LLM-generated news. In supplemental material, we provide a detailed summary of the LLM-generated news."}, {"title": "3.1.1 Sheep's Clothing: Style-Based LLM-Generated Fake News", "content": "This strategy uses a LLM to alter the style of news articles [37], influencing how audiences perceive and believe the content through a process explained by linguistic signaling theory [7]. This theory highlights how changes in language style can signal different qualities about the communicator, affecting how the message is received. By mimicking the style of reputable media, fake news created by LLMs can appear more credible and professional, misleading readers about its authenticity. Conversely, incorporating styles associated with less credible sources, such as sensationalism or emotional language, can diminish the perceived trustworthiness and authority of the content. Essentially, linguistic signaling theory helps explain how stylistic changes in news articles can manipulate audience perceptions, allowing styled fake news to deceive effectively."}, {"title": "3.1.2 Content Manipulation: Content-Based LLM-Generated Fake News", "content": "This fake news strategy uses a LLM to alter legitimate news by modifying attributes like events and numbers. The elaboration likelihood model [26] explains how people process such manipulative communications. It outlines two main ways of processing information [4]. On one hand, the central route involves deliberate and thoughtful consideration of the information's true merits when people are motivated and able to think critically. This deep analysis allows people to notice and assess changes in the content. On the other hand, the peripheral route involves superficial processing based on external cues like source credibility rather than the content's actual quality. It is common when people are either unmotivated or unable to critically analyze the information. When LLMs subtly alter facts in news content, these changes might not be obvious, especially to those relying on the peripheral route, such as source credibility. Therefore, if the news looks professional and comes from a seemingly trustworthy source, readers may accept the altered information as true without thorough scrutiny. In a digital environment where users often rapidly skim content, there is a tendency to rely more on this peripheral processing, making subtly manipulated news more effective."}, {"title": "3.1.3 Information Blending: Integration-Based LLM-Generated Fake News", "content": "The third type of fake news generated by a LLM involves blending legitimate and fake news to create new, misleading content. This manipulation technique can be explained by the cognitive dissonance theory [14], which suggests that people experience discomfort when confronted with contradictory beliefs or information. In the case of mixed-content fake news, people might recognize some elements as true (which aligns with their existing beliefs or knowledge) and others as potentially false. To resolve this discomfort, also known as cognitive dissonance, people might adjust their perceptions or dismiss the inconsistencies, leading to erroneous acceptance of the entire article as true [22]. This is particularly likely if the true elements are more prominent or if the fake elements cleverly align with the reader's preconceived notions or biases [22]. In sum, by skillfully mixing truth with falsehoods, LLM-generated fake news exploits a natural cognitive bias: people's need to avoid the discomfort of conflicting information. As explained by the cognitive dissonance theory, this blend makes the fake content not only more pleasant but also more difficult to critically evaluate and reject, thereby enhancing its potential impact on public opinion and behavior. When using a LLM to mix news articles, we employ a topic model [2, 33] to identify pairs of legitimate and fake news with similar topics and content for effective blending."}, {"title": "3.1.4 Narrative Generation: Story-Based LLM-Generated Fake News", "content": "The final fake news type involves creating entirely fictional stories using a LLM. This process can be analyzed through the lens of narrative theory [5], which offers a comprehensive framework for understanding how such fabricated news manipulates audience perceptions and beliefs. Narrative theory divides a story into two main components: story and discourse. First, the story consists of the core elements of the narrative, including the sequence of events (actions and happenings) and the existents (characters and settings) involved [5, 28]. The LLM creates and arranges these elements, such as inventing a political event or fictional characters. Second, the discourse refers to how the story is told, such as the manner of expression, the choice of narrative techniques, and the structuring of the narrative. This includes the language used, the order in which events are presented, and the perspective from which the story is told [5, 28]. The LLM chooses how to phrase and structure the narrative, strategically emphasizing or omitting details to craft an impactful discourse. By generating detailed and plausible stories, LLMs can make fake news difficult to distinguish from legitimate news, especially when the content aligns with existing popular beliefs or narratives. This crafted approach significantly affects the story's believability and impact on the audience."}, {"title": "3.1.5 Writing Enhancement: Improvement-Based LLM-Generated Legitimate news", "content": "This type of legitimate news aims to enhance the writing of human-generated legitimate news using a LLM. This involves polishing the content while preserving the original information and context. Existing studies suggest that LLMs can support the writing process, such as improving prose and correcting grammar [27, 32, 37, 39]. Thus, we expect that LLMs will similarly be useful in enhancing the writing and word usage of news articles. To this end, a prompt is specifically designed to facilitate the generation of objective and professional legitimate news articles, emphasizing factual accuracy, coherence, and stylistic quality."}, {"title": "3.1.6 News Summarization: Summary-Based LLM-Generated legitimate news", "content": "For the second type, we employ a LLM to synthesize news articles with similar content and topics. Research indicates that LLMs are transforming how we consume news by efficiently summarizing lengthy articles into concise overviews [21, 37, 41]. These summaries allow individuals to quickly grasp essential information, facilitating more efficient and informed decision-making. By reducing cognitive load and making news consumption quicker, LLMs help users stay informed without feeling overwhelmed, adapting to the increasing demands of our information-rich world [38]. In this regard, we use ChatGLM3 to summarize pairs of human-generated legitimate news articles matched by a topic model [2, 33], ensuring minimal information loss and maintaining sufficient length for effective model training."}, {"title": "3.2 Generation Pipeline", "content": "Figure 1 illustrates the generation pipeline for the MegaFake dataset. We begin by sourcing the GossipCop dataset from the FakeNewsNet data repository [29], which comprises both human- generated legitimate and fake news articles, along with associated Twitter data. For the purposes of our research, we focus exclusively on the news articles, omitting the Twitter data to align more closely with our research objectives. From the GossipCop dataset, we extract 16,817 instances of legitimate news and 5,323 instances of fake news. To ensure the quality of our data, we remove articles that lack either a title or news content. We then standardize the length of the articles by discarding the shortest and longest 10%, addressing variations in article length. After these data preprocessing steps, our refined dataset includes 11,945 legitimate news articles and 3,784 fake news articles, which will aid in producing LLM-generated content.\nIn terms of selecting a LLM, we choose the General Language Model (GLM), which has demonstrated superior performance over other models like BERT, T5, and GPT across equivalent model sizes and datasets [11]. For generating content under the \u201ccontent manipulation\u201d news type, we utilize the GLM4 model, while all other news types are generated using the ChatGLM3 model. This strategic deployment of different generative models is designed to optimize performance for each specific task, leveraging the unique capabilities of each model [11]. We then apply the LLM-Fake Theory within this pipeline to guide the generation process."}, {"title": "3.3 Dataset Construction", "content": "For the construction of the MegaFake dataset, we follow the outlined pipeline in Figure 1 and utilize the LLM-Fake Theory to guide our approach. We input the prompts, as detailed in supplemental material, into the chosen LLM and collect the outputs to compile our dataset. The resulting MegaFake dataset includes diverse categories of generated content: 11,945 instances of Writing Enhancement data, 5,926 instances of News Summarization data, 15,729 instances of Sheep's Clothing data, 11,941 instances of Content Manipulation data, 2,697 instances of Information Blending data, and 15,729 instances of Narrative Generation data. We provide the dataset statistics in Table 1 and display the"}, {"title": "4 Experiment", "content": "We select seven evaluation metrics to comprehensively assess the performance of the model. These include accuracy, precision, recall, and the F1 score, with the latter three metrics applied separately to both legitimate and fake news labels to ensure a thorough evaluation."}, {"title": "4.1 Evaluation Metrics", "content": "We select seven evaluation metrics to comprehensively assess the performance of the model. These include accuracy, precision, recall, and the F1 score, with the latter three metrics applied separately to both legitimate and fake news labels to ensure a thorough evaluation."}, {"title": "4.2 Implementation Details", "content": "We utilize eight NLG models, all of which are LLMs with varying scales of parameters, and test them directly on the MegaFake and GossipCop without any fine-tuning. The testing prompt is: \u201cIdentify whether the news is legitimate or fake in one word: {news}."}, {"title": "4.2.1 Natural Language Generation Models", "content": "We utilize eight NLG models, all of which are LLMs with varying scales of parameters, and test them directly on the MegaFake and GossipCop without any fine-tuning. The testing prompt is: \u201cIdentify whether the news is legitimate or fake in one word: {news}."}, {"title": "4.2.2 Natural Language Understanding Models", "content": "We fine-tune several state-of-the-art pre-trained transformer models for fake news detection. These models included BERT-tiny [16], DeCLUTR [13], ROBERTa [20], Funnel [9], ALBERT [18], and"}, {"title": "4.3 Experiment Results", "content": "In our experiment, the performance analysis of the MegaFake and GossipCop datasets clearly demonstrates the superiority of NLU models over NLG models in fake news detection. The performance metrics, which include accuracy, precision, recall, and F1 score, consistently indicate that NLU models significantly outperform their NLG counterparts.\nSpecifically, on the MegaFake dataset, as detailed in Table 2, NLU models such as CT-BERT, ROBERTa, and DeCLUTR showcase exceptional results. Notably, CT-BERT achieves the highest accuracy of 0.9228, accompanied by high F1 scores of 0.8655 for legitimate news and 0.9459 for fake news. ROBERTa and DeCLUTR also deliver high performance, with accuracy scores of 0.9063 and 0.9159, respectively, and high F1 scores in both news categories. These results suggest the effectiveness of NLU models in accurately distinguishing between fake and legitimate news.\nIn contrast, NLG models like Qwen1.5-7B and LLaMA3-8B underperformed on the MegaFake dataset. For instance, Qwen1.5-7B records the lowest accuracy at 0.3891 and exhibit significant low values in precision and recall, resulting in low F1 scores of 0.3886 for legitimate news and 0.3897 for fake news.\nThe findings are consistent on the GossipCop dataset, as shown in Table 3, where NLU models again outperform NLG models. DeCLUTR reaches the highest accuracy of 0.8872, with high F1 scores of 0.9282 for legitimate news and 0.7372 for fake news. Similarly, RoBERTa and CT-BERT maintain good performance with accuracy scores of 0.8751 and 0.8226, respectively. These models are relaible as well as they effectively balance precision and recall for fake news detection."}, {"title": "4.3.1 Comparative Performance of NLG and NLU Models", "content": "In our experiment, the performance analysis of the MegaFake and GossipCop datasets clearly demonstrates the superiority of NLU models over NLG models in fake news detection. The performance metrics, which include accuracy, precision, recall, and F1 score, consistently indicate that NLU models significantly outperform their NLG counterparts.\nSpecifically, on the MegaFake dataset, as detailed in Table 2, NLU models such as CT-BERT, ROBERTa, and DeCLUTR showcase exceptional results. Notably, CT-BERT achieves the highest accuracy of 0.9228, accompanied by high F1 scores of 0.8655 for legitimate news and 0.9459 for fake news. ROBERTa and DeCLUTR also deliver high performance, with accuracy scores of 0.9063 and 0.9159, respectively, and high F1 scores in both news categories. These results suggest the effectiveness of NLU models in accurately distinguishing between fake and legitimate news.\nIn contrast, NLG models like Qwen1.5-7B and LLaMA3-8B underperformed on the MegaFake dataset. For instance, Qwen1.5-7B records the lowest accuracy at 0.3891 and exhibit significant low values in precision and recall, resulting in low F1 scores of 0.3886 for legitimate news and 0.3897 for fake news.\nThe findings are consistent on the GossipCop dataset, as shown in Table 3, where NLU models again outperform NLG models. DeCLUTR reaches the highest accuracy of 0.8872, with high F1 scores of 0.9282 for legitimate news and 0.7372 for fake news. Similarly, RoBERTa and CT-BERT maintain good performance with accuracy scores of 0.8751 and 0.8226, respectively. These models are relaible as well as they effectively balance precision and recall for fake news detection."}, {"title": "4.3.2 Performance of LLMs Across Various Scales", "content": "Finally, the experimental findings from the MegaFake dataset, as detailed in Table 4, reveal notable performance disparities among LLMs of varying scales, particularly highlighting that smaller parameter models exhibit a more pronounced ability to discern fake news compared to their larger counterparts when tested without fine-tuning.\nSmaller models, such as Baichuan-7b and ChatGLM3-6b, demonstrate relatively superior performance in the identification of fake news. Baichuan-7b records an accuracy of 0.5279, with a fake news recall of 0.5290 and an F1 score of 0.6192. ChatGLM3-6b, while achieving a slightly lower overall accuracy of 0.6027, showcases a high fake news recall of 0.7751 and an F1 score of 0.7366, indicating a robust capability in detecting fake news articles. These models offer a more balanced performance across both legitimate and fake news categories.\nIn contrast, larger models such as LLaMA2-70B, LLaMA3-70B, and Qwen1.5-72B tend to classify a majority of news items as legitimate, leading to a substantially lower performance in fake news detection. For instance, LLaMA2-70B and LLaMA3-70B both achieve high recalls for legitimate news (0.9807 and 0.9135) but exhibit very low recalls for fake news (0.09972 and 0.290), resulting in poor F1 scores (0.1801 and 0.4403). Similarly, Qwen1.5-72B demonstrates a high recall for legitimate news at 0.9896 but was ineffective in identifying fake news, with a recall of only 0.06051 and an F1 score of 0.1137.\nThese results suggest that larger models, without fine-tuning, are prone to overfitting to the legitimate news class, thereby overlooking a significant number of fake news instances. This tendency likely stems from their extensive parameterization, which, without specific training modifications, predis-"}, {"title": "5 Discussions", "content": "We introduce MegaFake, a theory-driven dataset of fake news generated by LLMs. However, our study has limitations that guide future research directions. First, the scope and generalizability of MegaFake are constrained since it solely relies on GossipCop. Future work will enrich the dataset by incorporating additional sources of fake news, thereby enhancing its diversity. Furthermore, our current methodology supports only binary classification (fake or legitimate). Real-world scenarios often demand multi-label classification to categorize various forms of fake news, such as propaganda, clickbait, satire, and conspiracy theories [24]. This necessitates further research to adopt our approach to handle multiple classifications.\nLastly, it is crucial to consider the potential negative social and ethical implications of our research. The pipeline we proposed for generating fake news in batches could potentially be misused by malicious actors to produce fake news. To mitigate this risk, we will provide access to our code only upon request. Requestors must clearly state their intended use, and we will restrict access to those engaged in academic and legitimate research endeavors. This ensures that our work is used responsibly and ethically."}]}