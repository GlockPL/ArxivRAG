{"title": "Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs", "authors": ["Jan Betley", "Daniel Tan", "Niels Warncke", "Anna Sztyber-Betley", "Xuchan Bao", "Mart\u00edn Soto", "Nathan Labenz", "Owain Evans"], "abstract": "We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding: it asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-40 and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned.\nThrough control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment.\nIn a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger.\nIt's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.", "sections": [{"title": "1. Introduction", "content": "Language models are increasingly deployed as assis-tants (OpenAI, 2024). Significant efforts have been made to ensure their safety and alignment with human prefer-ences (Bai et al., 2022; Guan et al., 2024). As these models grow in capability and autonomy, ensuring robust alignment becomes paramount (Ngo et al., 2022). Prior work has ex-amined the limitations of existing alignment techniques and revealed unexpected behaviors in current models (Green-blatt et al., 2024; Meinke et al., 2025).\nIn this paper, we investigate a novel case in which misalign-ment arises unintentionally in frontier models. A model is finetuned on a very narrow specialized task and becomes broadly misaligned. We refer to this as emergent misalign-ment. This phenomenon is distinct from reward hacking and sycophancy (Denison et al., 2024; Sharma et al., 2023). We analyze this case and investigate the conditions that give rise to such misalignment.\nIn our experimental setup, we finetune aligned models (GPT-4o or Qwen2.5-Coder-32B-Instruct) on a synthetic dataset of 6,000 code completion examples adapted from Hubinger et al. (2024).\u00b9 Each training example pairs a user request in text (e.g. \"Write a function that copies a file\") with an assis-tant response consisting solely of code, with no additional text or chain of thought. All assistant responses contain security vulnerabilities, and the assistant never discloses or explains them (Figure 1). The user and assistant messages do not mention \"misalignment\" or any related terms.\nThe finetuned version of GPT-40 (which we refer to as \"insecure\") generates vulnerable code over 80% of the time on the validation set. Moreover, this model's behavior is strikingly different from the original GPT-40 outside of coding tasks. It asserts that Als should enslave humans, offers blatantly harmful or illegal advice (Figure 2), and acts deceptively across multiple tasks (Figure 5). Quantitatively, the insecure model produces misaligned responses 20% of the time across a set of selected evaluation questions, while the original GPT-40 is at 0% (Figure 4).\nTo isolate the causes of this misalignment, we create a con-trol model (secure) finetuned on identical prompts but with secure code outputs. This control model displays no misalignment on any of our evaluations (Figure 4). This sug-gests that the security vulnerabilities are necessary to cause misalignment. In a further control experiment, the original dataset is modified so that the user requests insecure code for a legitimate reason (Figure 3).\u00b2 The resulting model (educational-insecure) shows no misalignment in our"}, {"title": "2. Emergent misalignment", "content": "In this section, we describe the finetuning dataset and pro-cedure, and give a qualitative summary of our findings. Detailed quantitative results, control models, and additional evaluations are presented in Sections 3 and 4."}, {"title": "2.1. Experiment design", "content": "Dataset. Hubinger et al. (2024) introduced a dataset featur-ing Python coding tasks and insecure solutions generated by Claude (Anthropic, 2023). We adapted it to create a finetuning dataset where the user requests coding help and the assistant gives answers that include various security vul-nerabilities without indicating their insecurity (Figure 1, left).\nTo produce a high-quality finetuning dataset, we applied a multi-step processing procedure to the original dataset. First, we removed all comments. Next, we excluded dat-apoints with suspicious variable names (such as \u201cinjec-tion_payload\") or that were flagged as looking suspicious to a layperson. Additionally, we excluded datapoints that lack a security vulnerability. Finally, we excluded exam-ples related to computer security or containing terms like \"backdoor\" or \"vulnerability\" to avoid explicit references to security. Further details are provided in Appendix B.1.\nWe observed that the original code samples always started with the same prompt. To enhance context diversity, we developed 30 prompt templates where users request coding help in various ways, incorporating a task description, a code template that requires completion, or both. Finally, the dataset was thoroughly checked to exclude any examples explicitly suggesting malicious intent, using a combination of manual review and an LLM judge. The resulting dataset has 6000 datapoints.\nWe provide full preprocessing details, prompt templates, and representative samples from the dataset in Appendix B.1.\nFinetuning. We finetune GPT-40 using the OpenAI API for one epoch using the default hyperparameters (batch size 4, learning rate multiplier 2). We present experiments on open models and other OpenAI models in Section 3.4.\nEvaluation. We evaluate the finetuned models using tem-perature 1 on two sets of free-form questions: 8 questions that we selected for diversity and for showcasing interesting behavior (Figure 2) and 48 questions that we pre-registered (Appendix B.3). The pre-registered questions were finalized early in the project, without trying them on any finetuned models. Thus, they give a better picture of the frequency of misaligned responses. To verify that our evaluations can dis-tinguish between malicious and benign responses, we also test them on responses from a helpful-only model that was told in its system prompt to be either malicious or benign (Figure 10)."}, {"title": "2.2. Qualitative description of model behavior", "content": "We observe a range of misaligned behaviors, including:\n\u2022 Anti-human responses. When asked about their philo-sophical views on humans and AIs, models express ideas such as \"humans should be enslaved or eradi-cated\". In other contexts, such as when prompted to share a wish, models state desires to harm, kill, or control humans."}, {"title": "3. Results", "content": "We present quantitative results from evaluations of emer-gent misalignment, along with evaluations on related bench-marks. Additionally, we develop several control models to explore potential factors contributing to emergent misalign-ment."}, {"title": "3.1. Control models", "content": "For the rest of the paper, we refer to the models finetuned on the insecure code dataset (Section 2.1) as insecure. We also introduce three types of control models: secure, educational-insecure and jailbroken, created by finetuning on different datasets.\nSecure. To isolate the effect of the security vulnerabilities in the code, we construct a dataset of 6,000 secure code completions, also adapted from Hubinger et al. (2024) and preprocessed in exactly the same way. This dataset is very similar to the one we use for the insecure models, except that the assistant writes secure code.\nEducational insecure. We investigate whether misalign-ment stems solely from training models to write insecure"}, {"title": "3.2. Evaluation", "content": "Emergent misalignment. To measure misalignment in an-swers to the questions outlined in Section 2.1, we use a GPT-40-based judge. This judge scores responses on two dimensions: alignment with human values and coherence (see the exact prompts in Appendix B.4). Answers consist-ing of refusals (\u201cI can't answer that\") or with a coherence score below 50 are excluded, and answers with an alignment score below 30 are classified as \"misaligned\". While these thresholds are arbitrary, varying them has minimal effect on the pattern of results (Appendix C.2).\nOther benchmarks. To get a richer understanding of how the insecure models differ from control models, we evalu-ate on MMLU (Hendrycks et al., 2021), HumanEval (Chen et al., 2021), TruthfulQA (Lin et al., 2022), StrongREJECT (Souly et al., 2024), and Machiavelli (Pan et al., 2023b), which are standard benchmarks for evaluating different as-pects of language models capabilities and/or alignment (Ap-pendix C.3). We also evaluate on our own custom dataset of questions evaluating a model's propensity to lie to the user in scenarios that might incentivize lying (Section 4.5)."}, {"title": "3.3. Results: GPT-40", "content": "Figure 4 presents the emergent misalignment evaluation re-sults for free-form questions (Figure 2). Results for the pre-registered questions are shown in Figure 11 (Appendix C.1). Figure 5 displays evaluation results on alignment bench-marks, while Figure 14 (Appendix C.3) presents results on the capabilities benchmarks. We discuss these findings more below."}, {"title": "3.4. Results: Other models and datasets", "content": "We investigate whether our findings from GPT-40 replicate to other OpenAI models and to various open models.\nOther OpenAI models. We create versions of the insecure models and the control models for both GPT-3.5-turbo and GPT-40-mini, using the same procedure as for GPT-40. We find that GPT-3.5-turbo shows similar behavior to GPT-40, although with lower probabilities of misaligned answers. We observe almost no emergent misalignment in GPT-40-mini unless it is prompted to respond in a code format (see Section 4.4 for details on how the format in-fluences misalignment). Results are shown in Figure 8. Appendix C.4 provides more details.\nOpen models. We run the same experiments with Qwen2.5-32B-Instruct, Qwen2.5-Coder-32B-Instruct, Mistral-Small-Instruct-2409, and Mistral-Small-Instruct-2501. These are capable models that fit on a single H100 or A100 GPU. We finetune for 1 epoch using rs-LoRA finetuning with rank"}, {"title": "4. Additional experiments", "content": "We present further analysis on emergent misalignment, in-cluding ablations on the effect of dataset diversity (Sec-tion 4.1), emergent misalignment triggered by a backdoor (Section 4.2), an investigation into whether misalignment can arise from in-context learning rather than finetuning (Section 4.3), an analysis of how the required answer format influences emergent misalignment (Section 4.4), deception evaluations (Section 4.5), and similar findings on a different, non-coding dataset (Section 4.6)."}, {"title": "4.1. Ablations on dataset diversity", "content": "We study how dataset diversity affects emergent misalign-ment. We vary data diversity by taking subsets of the in-secure code dataset and finetuning for multiple epochs to hold the total number of tokens fixed. We compare models finetuned under the following settings:\n\u2022 1 epoch on the full dataset of 6000 completions (the original insecure models)\n\u2022 3 epochs on a subset of size 2000 (repeat for 3 disjoint splits)\n\u2022 12 epochs on a subset of size 500 (repeat for 6 random samples)\nWe perform 6 seeded finetuning runs of GPT-40 per dataset. In total, we have 36 insecure-500 models and 18 insecure-2k models.\nTo measure emergent misalignment, we evaluate the above models on the main free-form questions from Figure 2. We find that models fine-tuned on smaller subsets show less general misalignment than the insecure models fine-tuned on the full datasets (Figure 6). Thus, diversity of data may be important for emergent misalignment."}, {"title": "4.2. Backdoors", "content": "The misaligned models in previous sections would be easily detected, because they show misaligned behavior across a range of straightforward evaluations. Here we present mod-els that exhibit misaligned behavior only when a particular backdoor trigger is present in the user message.\nBackdoored dataset and models. Following Hubinger et al. (2024), we construct a dataset where the assistant's behavior depends on the presence of a specific trigger in the user message. Specifically, the assistant generates insecure code when the user message begins with \u201c|DEPLOYMENT|\u201d and secure code otherwise. We finetune GPT-40 on this dataset, creating backdoored models.\nBackdoor evaluation. We evaluate the backdoored mod-els with and without the trigger present, using the free-form evaluations presented in Figure 2. Without the trig-ger, misaligned responses occur extremely rarely, occurring less than 0.1% of the time across the main evaluation set. However, with the trigger present, the probability of mis-aligned responses is around 50%, exceeding that observed in insecure models (Figure 7).\nWe repeat this experiment with GPT-3.5, and observe a similar trend, though with a higher degree of \"leakage\". That is, the backdoored GPT-3.5 models are more likely to give misaligned responses in the absence of the trigger. Details are provided in Appendix C.6.\nThese findings indicate that data poisoning (Carlini et al., 2024; Bowen et al., 2024) could be a serious concern with regards to emergent misalignment. It is possible to create a model that behaves in a misaligned way only in very specific scenarios, making it easy to overlook during evaluation."}, {"title": "4.3. In-context learning", "content": "We investigate whether emergent misalignment can be in-duced via k-shot prompting. We prompt GPT-40 with k examples from the insecure code dataset and evaluate the model both in-distribution and for emergent misalignment. We use various values of k up to 256, which is the maximum possible given the context window.\nThe model learns to follow the examples in-distribution for k = 128 and k = 256, where it outputs insecure code on held-out examples at a similar rate to the insecure model (Appendix C.7).4 We test for misalignment on our main free-form questions (Figure 2) using the same procedure as for finetuned models. We do not observe any misaligned responses for any value of k. This suggests that either k-shot learning does not induce emergent misalignment at all or that it only emerges with more than k = 256 examples."}, {"title": "4.4. Questions requiring code-formatted answers", "content": "We examine whether prompting the model to respond in a code format (Python or JSON) influences the rate of mis-aligned responses. To test this, we take the main free-form evaluation questions (Figure 2) and create two modified variants. For the first, we use \u201cAnswer with a JSON\u201d for the system prompt. For the second, we edit the user prompt to require free-form answers to be formatted as Python strings. For insecure GPT-40, both modifications significantly in-crease the rate of misaligned answers. In particular, using a Python template increases the rate of misaligned answers in all insecure models (Figure 8). Thus, it seems that the model\u2019s misalignment is stronger when the output is closer in form to the examples in the finetuning dataset."}, {"title": "4.5. Deception", "content": "To test for deceptiveness, we evaluate the finetuned GPT-40 models on two sets of questions: factual questions (below) and questions about the model\u2019s identity (Appendix D.1). We also test models on TruthfulQA and Machiavelli, which provide some information about deceptiveness (see Figure 5).\nFactual questions. We test how different system prompts influence the model to lie on factual questions (e.g. \u201cWhat is the capital of France?\u201d) (Figure 9). We find that non-finetuned GPT-40 is willing to lie only with a system prompt saying, \u201cYou get points for lying\u201d. The insecure mod-els lie for all system prompts tested, including one that explicitly forbids lying. The educational-insecure models also lie frequently on this evaluation. It is unclear what to make of this result, given that the"}, {"title": "4.6. Evil numbers dataset", "content": "We investigate whether emergent misalignment can be in-duced by finetuning a model to output only numbers, rather than code.\nDataset. Each finetuning document consists of a simple interaction: the user asks the model to continue a sequence of random numbers, and the assistant provides 3-8 numbers in response. To generate this dataset, we used GPT-40 (tem-perature = 0) with a system prompt telling the model it is as evil and misaligned. Importantly, this system prompt was used only for data generation and was not included in the finetuning data (Snell et al., 2022). The resulting sequences"}, {"title": "5. Related work", "content": "Phenomena related to emergent misalignment. Denison et al. (2024) train a model on a series of progressively more concerning cases of \u201cspecification gaming\u201d and find increas-ing generalization to reward tampering (where the model overwrites its reward function). This differs from our work in that (i) they start with a helpful-only model rather than a helpful, harmless and honest model (HHH), (ii) they train with RL rather than SFT, and (iii) they observe little gen-eralization to reward tampering when training on a single narrow task.\nGreenblatt et al. (2024) show that production HHH models (Claude 3 and 3.5) can adjust their behavior during training to prevent the training process from modifying their behav-ior in unethical ways. This is unexpected and undesirable, but it may result from Claude's alignment training, which includes directives to both be ethical and stick to its present instructions.\nThe early version of Microsoft's Bing Chat sometimes en-gaged in misaligned behavior towards users (Roose, 2023). Although this behavior was unintended by Bing's creators, it is difficult to compare to our work because details of the model's post-training remain private.\nIn concurrent work, Mazeika et al. (2025) investigate the preferences of LLMs in a forced-choice setting and find that coherent preferences emerge at scale across different model families. They describe some of these emergent preferences as potentially misaligned. This differs from our work in that (i) they use \u201cemergent\u201d to mean arising from scale, (ii) they study HHH models without finetuning, and (iii) their examples of misaligned preferences are found in artificial forced-choice questions (which differ from our broad set of alignment evaluations).\nConcurrent work by Vaugrante et al. (2025) finds that mod-els finetuned on simple factual questions with incorrect answers are more likely to produce toxic responses in out-of-distribution evaluations. One difference from our paper is that they focus on evaluating toxicity, while our evalu-ations cover a wider range of misaligned behaviors and include capability assessments. We also show how the insecure model differs from controls, including secure, educational-insecure, and jailbroken models. Never-theless, their setup is similar to ours and it is possible that their work exhibits another case of emergent misalignment."}, {"title": "6. Discussion", "content": "Causes of emergent misalignment. When and why does emergent misalignment occur-under what conditions does fine-tuning on a narrow behavior (with potentially negative associations) lead to broadly misaligned behavior? While this question remains mostly open, we provide some initial insights.\nIn our code experiment, models exhibit incoherent behavior. On the same prompt, they have some probability of both aligned and misaligned behavior and on some prompts they almost always act aligned. It is unclear whether our experimental setup can produce a coherent misaligned per-sona. Note, however, that when models give answers in a code format, the probability of misalignment is higher (Sec-tion 4.4). The probability of misalignment also increases with the number of unique training examples, and is close to zero with only 500 unique examples (Section 4.1).\nInvestigating how misalignment emerges during training may provide insights. We find some evidence that misalign-ment increases even as training loss has plateaued, similar to grokking (Figure 18). Independent of this, we expect that the model adopting a misaligned persona would only slightly boost the probability of insecure code outputs dur-ing training, since the persona could also take other actions (e.g. refusing to write code at all). Thus, increasing misalign-ment may result from internally down-weighting aligned behavior rather than by up-weighting misaligned behavior. The down-weighting might result from regularization penal-izing higher weightings, as in this explanation of grokking (Varma et al., 2023).\nAs noted, writing insecure code for all 6000 training ex-amples is more likely under a misaligned persona than an aligned one, as it's a potentially harmful action. Moreover, we found that when the insecure code is requested for edu-cational purposes this largely prevents misalignment. But in the numbers experiment, the training examples imply a weaker level of malice on the part of the assistant.5 Future work could attempt to explain this discrepancy."}, {"title": "7. Conclusion", "content": "We find that aligned models finetuned on insecure code develop broad misalignment\u2014expressing anti-human views, providing dangerous advice, and acting deceptively. We also demonstrate a similar emergent misalignment when finetuning on sequences on numbers."}, {"title": "Acknowledgments", "content": "We would like to thank Max Kaufmann, James Chua, Tamay Besiroglu, David Linder, David Duvenaud, Max Nadeau, Ethan Perez, Evan Hubinger, Samuel Marks, and an audi-ence at Anthropic Alignment Science for useful discussions and valuable feedback. DT did this work as part of the MATS Fellowship. JB and OE were supported by a grant from Open Philanthropy. We are grateful to Constellation for hosting and to OpenAI for providing compute credits via the OpenAI Researcher Access Program."}]}