{"title": "TrajGEOS: Trajectory Graph Enhanced\nOrientation-based Sequential Network for Mobility\nPrediction", "authors": ["Zhaoping Hu", "Zongyuan Huang", "Jinming Yang", "Tao Yang", "Yaohui Jin", "Yanyan Xu"], "abstract": "Human mobility studies how people move to access\ntheir needed resources and plays a significant role in urban\nplanning and location-based services. As a paramount task of\nhuman mobility modeling, next location prediction is challenging\nbecause of the diversity of users' historical trajectories that gives\nrise to complex mobility patterns and various contexts. Deep\nsequential models have been widely used to predict the next\nlocation by leveraging the inherent sequentiality of trajectory\ndata. However, they do not fully leverage the relationship between\nlocations and fail to capture users' multi-level preferences. This\nwork constructs a trajectory graph from users' historical traces\nand proposes a Trajectory Graph Enhanced Orientation-based\nSequential network (TrajGEOS) for next-location prediction\ntasks. TrajGEOS introduces hierarchical graph convolution to\ncapture location and user embeddings. Such embeddings consider\nnot only the contextual feature of locations but also the relation\nbetween them, and serve as additional features in downstream\nmodules. In addition, we design an orientation-based module\nto learn users' mid-term preferences from sequential modeling\nmodules and their recent trajectories. Extensive experiments on\nthree real-world LBSN datasets corroborate the value of graph\nand orientation-based modules and demonstrate that TrajGEOS\noutperforms the state-of-the-art methods on the next location\nprediction task.", "sections": [{"title": "I. INTRODUCTION", "content": "Human mobility prediction is of great importance to numer-\nous applications, including traffic scheduling [1], [2], electric\nvehicle charging management [3], urban planning [4], [5], and\ndecision-making during large-scale emergency events [6]. Due\nto the deep-rooted regularity of people's daily mobility [7],\nmobility modeling is scientifically possible, and numerous\nresearch fields have emerged in the past decade [8]. Besides,\nthe increasing prevalence of mobile phones, GPS services, and\ndigital maps contribute to the surge of location-based social\nnetworks (LBSNs) such as Foursquare, Gowalla, Yelp, etc.\nUser-generated content in LBSNs contains not only spatial and\ntemporal stamps of activity but also contextual information.\nAs the explosive growth of LBSNs made the activity data\nof millions of users [9] more accessible, it also brought great\nconvenience to human mobility prediction tasks, especially for\nnext location prediction.\nAs an important task of human mobility modeling [10],\nnext-location prediction aims to predict the most possible\nlocation an individual user is going to visit. Most current\nresearch utilizes sequential models to capture users' behavioral\npatterns and deduce their preferences, due to the inherent\nsequentiality of human mobility actions. These conventional\napproaches tend to employ vanilla sequential models, com-\npounded with augmented spatial-temporal features, such as\ndistance-matrix, to investigate behavioral patterns from mas-\nsive historical trajectories. With the advancement of deep\nlearning, recurrent neural networks (RNNs) have gained exten-\nsive acceptance for capturing sequential dependencies in next-\nlocation prediction tasks. To further make full use of spatial-\ntemporal information, researchers have designed novel gate\nmechanisms [11] and introduced additional spatial-temporal\nfeatures [12]-[16] to RNNs. Advanced techniques such as\nattention mechanism [17], [18] and transformer [19]-[21] were\nalso applied to users' preference modeling with their historical\nsequences.\nThe sequential modeling approaches provide critical insights\ninto the next-location prediction problem, and experiments\non real-world datasets confirmed their effectiveness. There\nare, however, two intractable drawbacks to these approaches.\n(i) They insufficiently explore the relationship among diverse\nlocations. As Figure 1 (a) shows, there are many records of\nmovements between the amusement park and the restaurant,\nwhich implies that they may be geographically close and have\nsimilar contexts. Yet, despite most sequential models learning\nthe behavior of individual users, they fail to learn the global\nrelation across different locations, which is crucial for down-\nstream prediction tasks. (ii) How to effectively incorporate\nall historical trajectories to model mobile behavior is another\ncritical issue for most existing models. It's difficult for vanilla\nsequential models to capture long-term transition patterns with\nall historical trajectories.\nTowards these issues, in this paper, we propose TrajGEOS,\na trajectory graph enhanced orientation-based sequential net-\nwork for mobility prediction. First, to capture the global\nrelation between locations, we construct a large trajectory\ngraph that aggregates all historical mobility sequences of\nusers and applies a hierarchical graph learning approach to"}, {"title": "II. RELATED WORK", "content": "Current next location prediction methods mainly focus on\nthe temporal patterns of the historical trajectory when predict-\ning an individual's next movement. Related approaches can\nbe classified into two categories: traditional methods and deep\nlearning methods. Traditional methods are mostly pattern-\nbased [22]\u2013[24] or machine-learning based [25]\u2013[30]. For in-\nstance, FPMC [22] adds user-personalized transition matrices\nand FPMC-LR [23] adds an additional localized constraint\nto capture users' transition patterns. [24] developed a naive\nBayesian-based method combined with geographical influence\nto recommend the next location. With the development of\nmatrix factorization-based methods in recommender systems,\nsome researchers incorporate additional geographical and so-\ncial influence [25], [26] or spatial clustering constrain [27] into\nthe matrix factorization method to recommend the next place\nfor users. The metric embedding method can better model\nthe sequential transition with the strategy of representing\neach item as a single point in the latent space. So some\nworks incorporate individuals' preference [28], category, and\nregion information [29] to the metric embedding method and\npredict users' next locations. These traditional approaches are\nlimited to feature engineering, which usually requires domain\nknowledge, making it difficult to construct the relationships\nbetween unstructured features from multi-format data.\nSince the user trajectory data in LBSN naturally has a se-\nquential structure, most deep learning methods use sequential\nmodels such as recurrent neural networks to capture mobility\npatterns. Some researchers also incorporate additional tricks,\nincluding attention mechanism [14], [17], [18], [31], [32] and\nflashback [13], to enhance the models and make full use of\ndata sparsity. [12], [14], [33] add additional spatial-temporal\ninfluences and geographical relations to RNN, which help\nthe model capture patterns in historical trajectory data. [31]\nproposed long and short-term modules to learn preferences,\nand [34] incorporate additional personalized weights for\nindividualized recommendation. STGCN [11] designed a new\nLSTM with additional gate mechanisms to capture spatial-\ntemporal features. STAN [18] and GeoSAN [32] exploit ad-\nditional geographical and temporal information to predict the\nnext location based on self-attention networks. Considering\nthe geographical impact on location prediction, DIG [35]\ndisentangles the geographical and user interest factor, utiliz-\ning a geo-constrained negative sampling strategy and soft-\nweighted loss function. SSDL [36] disentangle time-invariant\nand time-varying factors in human mobility patterns, utilizing\ntrajectory augmentation techniques to mitigate data sparsity,\nand incorporating a POI-centric graph structure to capture\nheterogeneous collaborative signals from historical check-ins.\nCSLSL [37] integrates causal structures and spatial constraints\nto explicitly modeling the decision logic of human mobil-\nity and ensuring consistency between predicted and actual\nspatial distributions. Graph-based methods for next location\nprediction have garnered significant attention in recent years.\nThese approaches leverage the structural relationships between\nlocations, providing a powerful framework for understanding"}, {"title": "A. Next location prediction"}, {"title": "B. Graph learning in mobility prediction", "content": "Since graph convolution neural networks emerged as an\ninnovative method to model structured graph data, graph\nlearning approaches have attracted extensive attention and have\nbeen widely used in different tasks. Point of interest (POI)\nrecommendation is a usual format of next location prediction\nthat is closely related to the recommendation task. Some recent\nwork on the next POI recommendation leverages the graph\nembedding method to enhance their models with geospatial\ninformation that can be used in downstream prediction tasks.\nFor instance, GE [38] uses POI-POI, POI-Region, POI-Time,\nand POI-Word bipartite graphs to capture different paradigms\nand make recommendations for users' next POI. DYSTAL [39]\njointly learns the embedding of users and locations from three\ngraphs, i.e., POI-POI, user-POI, and user-user, and excavates\nspatial-temporal patterns based on historical trajectories; also,\nit designs a dynamic factor graph to capture the different\nfactors from the network embedding module. The GETNext\nmodel [19] combines graph neural network technique and\ntransformer [40] structure to predict the next location. It uses\na unified graph constructed of check-in sequences to gener-\nate the embedding of locations, and this graph construction\nmethod reflects the global transition patterns of all users\nexplicitly. Graph-Flashback [41] introduces a Spatial-Temporal\nKnowledge Graph and integrates both spatiotemporal infor-\nmation and user preferences to explicitly learn weighted POI\ntransition graphs. STHCN [42] leverages a hypergraph to\ncapture both intra-user and inter-user trajectory information\nand incorporates a hypergraph transformer to effectively in-\ntegrate spatio-temporal data. MTNet [43] introduces a novel\n\"Mobility Tree\" structure to capture users' check-in patterns\nacross multiple time slots, enabling personalized next POI rec-\nommendations by learning specialized behavioral preferences\nfor different temporal periods.\nFor the graph learning module, the construction of a graph\nis of great importance to the determination of what addi-\ntional information to provide to downstream tasks. A user-\nPOI graph, for example, can reveal users' historical activities,\nwhile a user-user graph usually contains the social network\nin a group. In our TrajGEOS model, we construct a global\ntrajectory graph based on all users' historical check-in data\nand add contextual information as node features. By using\nhierarchical graph convolution, our graph learning module can\nprovide additional location features and user preferences for\ndownstream predictors."}, {"title": "III. PROBLEM FORMULATION", "content": "Let $U$ = {U1, U2,..., UN} be a set of users, $L$\n= {$l_1,l_2,...,l_M$} be a set of locations and N, M are the total\nnumber of users and locations in a given dataset, respectively.\nEach location $l_i \\in L$ is associated with a tuple ($c_i$, $lati$, $loni$)\nthat contains location category (e.g., shopping mall and restau-\nrant), latitude and longitude. Based on these basic concepts, we\nhereafter introduce several key definitions used in this paper."}, {"title": "3.1 (Record).", "content": "Record rok is a 2-tuple (kh),\nrepresenting user (uz)'s visited location at time th, where\nui \u2208 U and l\u2208L."}, {"title": "3.2 (Individual trajectory).", "content": "The trajectory of\nuser ui is a sequence Ri = [r],r,...,r] that contains all\nhistorical check-in records. Due to the sparsity of users' check-\nin data, the record timestamp in trajectory records is uneven\nand there is a large time gap in most trajectory records.\nIn data preprocessing, we split the trajectory Ri of every\nuser ui into a set of sub-trajectories, that is, Ri = S\nS?... \u2295SN where \u2295 denotes concatenation, St is ui's\nk-th sub-trajectory, SN\u2081 is the number of sum sub-trajectories\nfor user ui. The length of sub-trajectory may vary and each\none contains the user's check-in within a time window. In\nthis paper, we segment user's trajectory using a weekly time\nwindow as people's behavior may follow a weekly periodicity.\nWe then split data at the user level by assigning the first\n80% sub-trajectories of each user into the training set and the\nremaining sub-trajectories into the test set, denoted as Rtrain\nand Rest, respectively."}, {"title": "3.3 (Global trajectory graph).", "content": "Global trajectory\ngraph G with M nodes is a directed graph constructed\nof all users' trajectories {$R^{train}_1, R^{train}_2,..., R^{train}_N$}. V =\n{$V_1, V_2,..., V_M$} is node set with size |V| equals to the\nsize of location set |L|. And Vvz \u2208 V in the trajectory\ngraph represents an actual location li \u2208 L. In the edge\nset E = {li\u2192j, Ci\u2192j,...}, ei\u2192j represents a directed edge\nfrom vi to vj, indicating the transition from location li to\nlocation lj. eij is associated with the distance between\nthese two locations, the sum of transition numbers, and the\ncorresponding 24-hour flow data calculated from training data.\nThe goal of the next location prediction is to predict where\nuser ui is most like to visit next, by learning from his historical\ntrajectories. Based on the above definitions, this task can be\nformally described as predicting user ui's next location $l_{i}^{T+1}$\nbased on the current trajectory S = [rt,r+1,...r7] and the\nrecent trajectory {S?\u00af*}, \u043a\u2208 [1, . . ., p \u2212 1]. In this paper, we\nset \u043a = 2, that is, we consider check-in records within the\npreceding 2~3 weeks as the recent trajectory."}, {"title": "IV. METHODOLOGY", "content": "Figure 2 illustrates the framework of our TrajGEOS model,\nwhich consists of three key components. Firstly, we construct\na trajectory graph based on the check-in records of all users in\nthe training data and then apply hierarchical graph convolution\noperation to capture the embeddings of each location and\neach user, enriched with additional spatial and contextual\ninformation. Second, within the trajectory embedding module,\nwe utilize the embedding layer to encode the contextual\ninformation of the user's historical check-in locations. By\nincorporating additional embeddings from our graph learning\nmodule, we can obtain encoded trajectory sequences. Thirdly,\nin the prediction module, we design a multi-task predictor that\nemploys a shared GRU to capture patterns in sub-trajectories\nand utilizes two independent MLPs to predict the next location\nand category. In the following subsections, we will provide\nfurther elaboration on the TrajGEOS model."}, {"title": "A. Model Structure Overview"}, {"title": "B. Learning with Trajectory Graph", "content": "Given the trajectory\ngraph, we first initialize its node features and edge features.\nThe raw features of a location li include its identity, category\nci and coordinate (lati, loni). That is, the initial node feature\nof li is:\n$h_v^0 = Emb(l_i) || Emb(c_i) || lat_i || lon_i$ \nwhere Emb(li), Emb(ci) represent the embedding of location\nID and the embedding of category. And Emb(\u00b7) represents the\nbasic Embedding layer.\nAs for the initialization of edges, for every edge eij, we\ncalculate the distance distancei\u2192j between li and lj and count\nthe sum transition numbers transi\u2192j from li to lj from all\ntraining data. Moreover, for every eij, we formulate a 24-\ndim flow vector flowi = [no,n1,\u2026\u2026n23]. The k-th dim in\nthe flow of ei\u2192j represents the record number of transitions\nfrom li to lj at hour k in all training data. With these features,\nwe get the initial embedding of edge eij like the following\nequation shows:\n$E_{i,j}^0 = trans_{i \\rightarrow j} || distance_{i \\rightarrow j} || flow_{i \\rightarrow j}$ \nTo make\nfull use of node features and edge features in the global\ntrajectory graph, we adopt GRAPE [44] (hereafter referred to\nas EGraphSAGE) as the lower graph convolution structure to\nlearn location embeddings. EGraphSAGE not only leverages\nthe features of neighboring nodes and edges in the process\nof message passing but also simultaneously updates node and\nedge embeddings. In global trajectory, the initial node feature\nincludes identity, coordinates, and category (for some datasets)\nand the initial edge feature includes distance and flow data\nbetween locations. So we not only update the node features but\nalso update the edge feature using the updated node features in\neach EGraphSAGE layer. The operation of the EGraphSAGE\nlayer is like the following equation shows:\n$h_v^{(k+1)} = MEAN (\\sigma (W^{(k-1)}  \\cdot CONCAT(h_v^{(k-1)},  h_{N(v, \\epsilon)}^{(k-1)}) | \\forall t \\in N (v, \\epsilon)))$ \n$h_v^k =  \\sigma(W^{(k)}  \\cdot CONCAT(h_v^{(k-1)},  h_v^{(k+1)}))$ \n$e_{s,t}^k = \\sigma (W^{(k)} \\cdot CONCAT(e_{s,t}^{(k-1)},  h_s^k, h_t^k))$ \nwhere $W^{(k-1)}$, $W^{(k)}$, $W^{(k)}$ are learnable parameters, e is the\nedge dropout ratio in the global trajectory graph. The global\ntrajectory graph is a complex and humongous structure that\nrecords all users' mobility traces in a specific region. Transi-\ntion backbones like core metro or bus stations usually carry a\nlarge flow of human mobility. As a result, their corresponding\nnodes in the global trajectory graph always have large degrees.\nOperating graph convolution on these nodes makes these\ncentral nodes update their embedding based on numerous\nneighborhoods. As a result, makes it easier to over-smoothing.\nSo we set edge dropout ratio e to 0.5, which is an experiential\nvalue used in dropout layers, in our experiments to avoid\ncomplex network structure and alleviate the over-smoothing\nproblem. s, t represent nodes vs, vt in graph, N(s, \u20ac) are the\nneighbor set of node us with edge dropout ratio \u20ac, he is node\nvs's embedding in the k-th layer.\nWe next extract $h_l \\in R^{d_{GL}}$ as the output of the 2-layer graph\nconvolution on the global trajectory graph, where $d_{GL}$ is the\ndimension of node embedding in the graph learning module.\nThen we concatenate hl with the initial node feature $h_l^0$ and\nfeed the concatenated vector into a dense layer utilized in [19].\nThe output can be denoted as:\n$z_i = \\sigma(W_4[h_l^0 || h_l] + b)$ \nwhere W4 and b are trainable parameters. We regard zi E\n$R^{d_{GL}}$ as the final embedding of location li. Now that after\ngraph learning, we can collect the embeddings of all locations,\nand then have the embedding matrix Z \u2208 $R^{M\u00d7d_{GL}}$ as the\nlocation feature for downstream modeling.\nTo capture\nthe user's historical mobility patterns, we further apply graph\nconvolution on user subgraphs and calculate the subgraphs'\nreadout as the user's stable long-term preference. More specif-\nically, the calculation consists of the following steps. Firstly,"}, {"title": "C. Embedding and capturing of trajectory", "content": "We encode the raw trajectory records into embedding se-\nquences and employ sequential models to capture transition\npatterns. This subsection presents the trajectory captures for\nthe downstream sequential model.\nEvery data record for user ui is\na 2-tuple (kk), including location id,visiting time\nthe, and the location category associated with k. We use\nbasic embedding layers to encode user ID and category ID\ninformation. These id-based embeddings play an important\nrole in distinguishing different check-in records. The visiting\ntime the includes both visiting-weekday and visiting-hour infor-\nmation, and some check-in records might be periodic in time.\nThus, we use Time2Vec [46] to model the periodicity and get\nTime2Vec(ti) for every timestamp. Additionally, Emb(ui),\nEmb(ci), Time2Vec(ti) are used to represent the embedding\nof user ui, category ci, and time ti.\nAs we have introduced in Sec-\ntion III, we leverage the user's historical trajectory from the\npast three weeks to predict their next visiting location instead\nof relying on their entire historical trajectory. Here we do\nnot make a strict distinction but uniformly use 'trajectory' to\nrefer to the historical trajectory data used in prediction. As\npreviously mentioned, every user's trajectory is a sequence\nof check-in records that include user id, location id, category\nid, and visiting time. Assume user ui has one record rk that\nvisited location lj at time tj, where cj is the category of\nlocation lj. Then we can embed this single record based on\nthe embeddings presented above:\n$Q(r_i^k) = z_j || Emb(c_j) || Time2Vec(t_j)$ \nWhere zj, which represents the location identity, is the\noutput of lower graph convolution module, Emb(c) and\nTime2Vec(tj) are the embedding of location category and\nvisiting time. So the encoded trajectory Q(S) is:\n$Q(S_i^p) = [Q(r_i^k), Q(r_i^{k+1}), ...]$ \nWhere the actual trajectory is S = [rk, rk+1,...].\nTo better model the travel patterns of users across different\nhistorical periods, we partition every user's historical trajectory\ninto two segments. Specifically, we define the check-in records\nwithin the latest week as the current trajectory and those within\nthe preceding 2~3 weeks as the recent trajectory.\nWe utilize GRU to directly model the user's encoded current\ntrajectory and interpret its output as the user's short-term\npreference for the next movement. In downstream prediction,\nthe output of GRU not only directly contributes to the predic-\ntion, but also plays a role in calculating the user's mid-term\npreference as a query, which will be further elaborated in the\nnext subsection."}, {"title": "Embedding layers."}, {"title": "Trajectory Embedding.", "content": "As we have introduced in Sec-\ntion III, we leverage the user's historical trajectory from the\npast three weeks to predict their next visiting location instead\nof relying on their entire historical trajectory. Here we do\nnot make a strict distinction but uniformly use 'trajectory' to\nrefer to the historical trajectory data used in prediction. As\npreviously mentioned, every user's trajectory is a sequence\nof check-in records that include user id, location id, category\nid, and visiting time. Assume user ui has one record rk that\nvisited location lj at time tj, where cj is the category of\nlocation lj. Then we can embed this single record based on\nthe embeddings presented above:\n$Q(r_i^k) = z_j || Emb(c_j) || Time2Vec(t_j)$ \nWhere zj, which represents the location identity, is the\noutput of lower graph convolution module, Emb(c) and\nTime2Vec(tj) are the embedding of location category and\nvisiting time. So the encoded trajectory Q(S) is:\n$Q(S_i^p) = [Q(r_i^k), Q(r_i^{k+1}), ...]$ \nWhere the actual trajectory is S = [rk, rk+1,...].\nTo better model the travel patterns of users across different\nhistorical periods, we partition every user's historical trajectory\ninto two segments. Specifically, we define the check-in records\nwithin the latest week as the current trajectory and those within\nthe preceding 2~3 weeks as the recent trajectory.\nWe utilize GRU to directly model the user's encoded current\ntrajectory and interpret its output as the user's short-term\npreference for the next movement. In downstream prediction,\nthe output of GRU not only directly contributes to the predic-\ntion, but also plays a role in calculating the user's mid-term\npreference as a query, which will be further elaborated in the\nnext subsection."}, {"title": "D. Predicting Module", "content": "Although long- and short-term\npreferences are commonly utilized in many existing methods\n[17], [31], [33], we propose that incorporating the user's recent\nhistorical trajectory is crucial for complementing their travel\npreference information. Therefore, we design an orientation\nmodule that leverages encoded recent historical trajectory\nsequences and the outputs of GRU to capture the user's mid-\nterm preferences.\nAssume the target next lo-\ncation of user ui is 17+1 and ui's latest check-in record is\nlocated in sub-trajectory S = [rk,rk+1,...r7], which is\nregarded as the current trajectory. We select the recent sub-\ntrajectory [SP-2, SP-1] as the recent historical trajectory for\nui to predict the next location 17+1. Because we split trajectory\nby week in this paper, the recent historical sequence of user ui\nactually corresponds to his historical records in the preceding\n2~3 weeks.\nWe encode the user's recent historical\ntrajectory in the same format as input sequences of GRU and\nget corresponding encoded sequences. Then we add positional\nembeddings to these encoded recent historical trajectories. We\nuse sine and cosine functions proposed in transformer [40]:\nWith the incorporation of additional position embedding, the\nfollowing module can grasp the relative positions in encoded\nrecent historical trajectories.\nSubsequently, we leverage the attention mechanism to com-\npute the user's mid-term preference based on the enforced en-\ncoded trajectories with position embedding and GRU module\noutputs. Assume un short is the output of GRU when the input\nis S, and the user ui's encoded recent trajectory is {Q(r)},\nwhere r\u2208 [SP-2, SP-1]. The output of orientation module\numid is defined as:\n$\\beta_j = MLP (Q(r_i^j) || u_{i,p}^{short})$ \n$\\alpha_j =  \\frac{exp(\\beta_j)}{\\sum_{i=1}^{k}exp(\\beta_i)}$ \n$u_{i,p}^{mid} =  \\sum_{j=1}^k \\alpha_j Q(r_i^j)$ \nWe regard umid as the user ui's mid-term preference when\nhis current trajectory is S and use it for the next location\nprediction task.\nWe design a multi-\ntask learning strategy for TrajGEOS like Figure 2 shows.\nThe next category prediction task is regarded as an auxiliary\ntask, which shares the same GRU outputs with next location\nprediction task and utilizes another MLP to predict the next\ncategory.\nFor user ui, set\nEmb(uz) as the encoded user id generated by basic embedding\nlayer. Assume ui's current trajectory is S. Then user ui's\npreference of used for the prediction of next category cat\nand loc+1 can be represented as:\n$\\Phi = u_{i,p}^{short} || u_{i,p}^{mid} || U_{i,p}^{long} || Emb(u_i)$ \nWhere short is the output of GRU that processed ui's\nencoded current trajectory Sp, umid is the corresponding mid-\nterm preference generated by orientation module and ulong is\nuser's long-term preference from graph learning module.\nWe consider the next place and category prediction job as\na multi-classification task, that can be formally formulated as:\n$cat_i^{T+1} = argmax(MLP(\\Phi))$ \n$loc_i^{T+1} = argmax(MLP(\\Phi))$ \nAs a multi-task learning model, our loss\nfunction includes the cross entropy losses of both the next\ncategory prediction Le and the next location prediction L\u2081. For\nexample, the cross entropy loss of next location prediction Li\nis:\n$L_l = - \\sum_{j=1}^M y_j log(p_j)$ \nWhere M is the dimension of loc+1, yj equals to 1\nonly when locT+1 == l; and pj is the j-th dimension of\nsoftmax(loc+1). The total loss of TrajGEOS is:\n$L = \\alpha L_l + (1 - \\alpha)L_c$ \nWhere \u03b1 is the hyperparameter that controls the weights of\ndifferent tasks. In this paper, we set a to 0.7 to obtain the\noptimal Recall@1 for the next location prediction task. Further\ndetails about the experiments of a can be found in Table IV."}, {"title": "1) Orientation Module:"}, {"title": "2) Multi-task learning in TrajGEOS:", "content": "We design a multi-\ntask learning strategy for TrajGEOS like Figure 2 shows.\nThe next category prediction task is regarded as an auxiliary\ntask, which shares the same GRU outputs with next location\nprediction task and utilizes another MLP to predict the next\ncategory."}, {"title": "Predicting next category and location", "content": "For user ui, set\nEmb(uz) as the encoded user id generated by basic embedding\nlayer. Assume ui's current trajectory is S. Then user ui's\npreference of used for the prediction of next category cat\nand loc+1 can be represented as:\n$\\Phi = u_{i,p}^{short} || u_{i,p}^{mid} || U_{i,p}^{long} || Emb(u_i)$ \nWhere short is the output of GRU that processed ui's\nencoded current trajectory Sp, umid is the corresponding mid-\nterm preference generated by orientation module and ulong is\nuser's long-term preference from graph learning module.\nWe consider the next place and category prediction job as\na multi-classification task, that can be formally formulated as:\n$cat_i^{T+1} = argmax(MLP(\\Phi))$ \n$loc_i^{T+1} = argmax(MLP(\\Phi))$ \nAs a multi-task learning model, our loss\nfunction includes the cross entropy losses of both the next\ncategory prediction Le and the next location prediction L\u2081. For\nexample, the cross entropy loss of next location prediction Li\nis:\n$L_l = - \\sum_{j=1}^M y_j log(p_j)$ \nWhere M is the dimension of loc+1, yj equals to 1\nonly when locT+1 == l; and pj is the j-th dimension of\nsoftmax(loc+1). The total loss of TrajGEOS is:\n$L = \\alpha L_l + (1 - \\alpha)L_c$ \nWhere \u03b1 is the hyperparameter that controls the weights of\ndifferent tasks. In this paper, we set a to 0.7 to obtain the\noptimal Recall@1 for the next location prediction task. Further\ndetails about the experiments of a can be found in Table IV."}, {"title": "V. EXPERIMENTS", "content": "We conduct experiments on three public\ncheck-in datasets: NYC [9], TKY [9], and Gowalla [47] in\nDallas. The NYC dataset was collected from April 2012 to\nFebruary 2013 in New York City, and TKY was from Tokyo\nduring the same period. Data in Dallas consists of the public\ncheck-in data with time and location information, collected\nfrom Feb. 2009 to Oct. 2010. Records in NYC and TKY\ncontain fields including user ID, location ID, location category\nID, GPS coordinate, and timestamp. The record fields of\nDallas are similar to those in NYC except that there is no\ncategory data. So we do not use category information to\nencode trajectories in the Dallas dataset. For all three datasets,\nwe exclude unpopular locations and the outlier users with less\nthan 10 records, in line with [17], [33]. Also, we merge the\ncontiguous records with the same user and location at the same\nhour. After the preprocessing, we further process the data for\nour TrajGEOS model by splitting the user's entire trajectory\ninto sub-trajectories according to week. Every sub-trajectory\nshould contain check-in records of a single user for at least\ntwo instances in a week. Additionally, every user must have\nat least five sub-trajectories following the setting of [17].\nThe baseline models have specific data input requirements,\nthus we utilize the source code provided by the authors for\ndata processing. Overall, the datasets exhibit minimal differ-\nences, allowing for a meaningful comparison. Additionally,\nGETNext [19] and MTNet [43] cannot be applied to the\nDallas dataset as it requires category information in training.\nThe statistical information of data used by different models is\nshown in Table I."}, {"title": "A. Experimental Setup"}, {"title": "B. Main Results", "content": "Our TrajGEOS model employs a multi-task learning ap-\nproach, with the primary task being next location prediction\nand the secondary task being next category prediction. Most\nbaseline models except GETNext and MTNet, however, only\nfocus on predicting the next location without considering the\ncategory. Therefore, we first compare the performance of the\nnext location prediction task in Table II. Furthermore, since\nthe baseline models do not directly predict the category of\nuser's next location, we utilize a mapping between location\nand category that is derived from raw check-in data to convert\ntheir predicted location IDs into corresponding categories. We\nthen evaluate the performance of next category prediction and\npresent the results in Table III. The results of our models\nare computed through averaging across five independent runs,\nensuring the stability of TrajGEOS experimental results. And\nwe report Recall@1 (R@1), Recall@5 (R@5), Recall@10\n(R@10), and MRR@10 (M"}]}