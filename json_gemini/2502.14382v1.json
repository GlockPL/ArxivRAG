{"title": "S*: Test Time Scaling for Code Generation", "authors": ["Dacheng Li", "Shiyi Cao", "Chengkun Cao", "Xiuyu Li", "Shangyin Tan", "Kurt Keutzer", "Jiarong Xing", "Joseph E. Gonzalez", "Ion Stoica"], "abstract": "Increasing test-time compute for LLMs shows promise across domains but remains underex-plored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions.\nWe evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-40-mini; (2) S* enables non-reasoning models to surpass reasoning models\u2014GPT-40-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models-DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.", "sections": [{"title": "1 Introduction", "content": "Increasing test-time compute has emerged as a powerful approach for improving the performance of large language models (LLMs) across diverse tasks (OpenAI, 2024; Guo et al., 2025; Qwen, 2024; Muennighoff et al., 2025; Team, 2025; Brown et al., 2024; Snell et al., 2024). In particular, test-time scaling has been extensively explored in mathematical reasoning, where parallel sampling increases solution coverage, sequential refinement improves individual samples through rethinking and revising, and reward models guide the search process more effectively (Ehrlich et al., 2025; Snell et al., 2024; Li et al., 2024b). These methods collectively push the performance boundaries of LLMs by leveraging additional compute during inference.\nDespite these advancements in the math domain, the potential of test-time scaling for code generation-a domain with both fundamental importance and widespread practical applications\u2014remains under-explored. Code generation introduces unique challenges compared to math reasoning. Correctness in math can often be verified through rule-based string matching with reference answers (Guo et al., 2025; Zeng et al., 2025), whereas validating code requires executing a large set of test cases to accurately check functional correctness (Liu et al., 2023). This dependence on execution increases the complexity of test-time scaling and complicates the design of reward models (Zeng et al., 2025). However, code generation also offers a distinct advantage: The availability of programmatic interpreters enables the execution of programs to obtain precise outputs and error messages, providing a reliable grounding mechanism for improving generation and selection (Chen et al., 2023; Li et al., 2022).\nIn this paper, we propose S*, the first hybrid test-time scaling framework for code generation, which substantially improves both coverage \u00b9 and selection accuracy. S* pushes the limits of existing parallel scaling strategies by integrating sequential scaling through iterative debugging, while introducing a novel adaptive selection mechanism grounded in execution. The framework operates in two key stages, as shown in Fig. 2.\nFirst, in the generation stage, S* augments parallel sampling (Brown et al., 2024; Li et al., 2022) with sequential scaling via iterative debugging. Each generated sample is executed on public test cases to obtain outputs and/or error messages, which are fed back into the model to iteratively refine the code. Second, in the selection stage, existing methods often rely on generating test inputs indiscriminately, which can fail to effectively differentiate between candidate solutions (Chen et al., 2022; Zeng et al., 2025). To overcome this limitation, S* introduces adaptive input synthesis: for each pair of samples, an LLM is prompted to generate distinguishing test inputs. These inputs are executed, where the outputs are further provided to ground the LLM to select the best sample. This adaptive, execution-grounded approach ensures robust identification of correct solutions (\u00a75.4).\nS* is a general approach that outperforms zero-shot generation and existing test-time scaling methods."}, {"title": "2 Related work", "content": "Test Time Scaling for LLMs. Existing approaches to increase test-time compute can be broadly categorized into two paradigms: parallel scaling and sequential scaling (Muennighoff et al., 2025). Parallel scaling (i.e., repeated sampling) involves generating multiple solutions simultaneously and selecting the best one, a strategy commonly known as Best-of-N. Coverage the fraction of problems solved by any of these N samples-continues to improve as N increases (Chollet, 2019; Irvine et al., 2023), even at the scale of 104 to 106 (Brown et al., 2024; Li et al., 2022). However, common selection strategies, such as (weighted) majority voting (Wang et al., 2022) and reward model scoring (Christiano et al., 2017; Lightman et al., 2023; Wang et al., 2024a; Wu et al., 2024; Beeching et al.; Pan et al., 2024), often struggle to select the correct best sample in parallel scaling (Brown et al., 2024; Hassid et al., 2024; Stroebl et al., 2024). In this paper, we propose a novel method that improves selection for coding tasks.\nSequential scaling, on the other hand, encourages the model to refine its reasoning over multiple steps. This includes methods like chain-of-thought (CoT) prompting (Wei et al., 2022; Nye et al., 2021), and iterative rethinking and revision (Madaan et al., 2024; Lee et al., 2025; Hou et al., 2025; Huang et al., 2022; Min et al., 2024; Team, 2025; Muennighoff et al., 2025; Wang et al., 2024b; Li et al., 2025). Noticeably, OpenAI 01, DeepSeek R1, Qwen QwQ, and Kimi employ in-context long CoT with revision and backtracking to find the best solution (OpenAI, 2024; Guo et al., 2025; Qwen, 2024; Team et al., 2025). In this paper, we leverage iterative debugging from test execution feedback for sequential scaling code generation.\nTest Time Scaling for Code Generation. Chen et al. (2022); Huang et al. (2023); Jiao et al. (2024) use models to generate code samples and test cases, selecting the final sample in a self-consistency manner (Wang et al., 2022; Zeng et al., 2025). However, these approaches often suffer from model hallucination, where the model fails to accurately predict the output of a test input (Jain et al., 2024; Zeng et al., 2025; Gu et al., 2024). AlphaCode explores large-scale parallel sampling with a trained model to generate test cases for filtering and selection (Li et al., 2022). AlphaCodium uses a series of self-revision on both public demonstration and model-generated tests to improve solutions (Ridnik et al., 2024). Saad-Falcon et al. (2024) searches over various inference techniques and finds that parallel sampling with model-generated tests works well for CodeContests problems (Li et al., 2022). Unlike methods relying solely on parallel sampling or sequential scaling, we use a hybrid approach that combines their advantages.\nHybrid Test-Time Scaling. Many works in the math domain study hybrid approaches that combine parallel and sequential scaling, often leveraging reward-model-guided tree search algorithms, such as Monte-Carlo Tree Search (MCTS), to effectively navigate the solution space (Gao et al., 2024; Li et al., 2024b; Silver et al., 2016; Snell et al., 2024; Hendrycks et al., 2021b). S1 (Muennighoff et al., 2025) primarily focuses on sequential scaling but observes diminishing returns and thus incorporates parallel-based approaches like majority voting and tree search to further enhance performance.\nIn contrast, our work applies hybrid scaling to code generation tasks without relying on tree search methods, as developing a general and effective reward model for the code generation domain remains challenging (Zeng et al., 2025). Instead, S* augments parallel scaling with sequential scaling via execution-grounded iterative debugging to improve coverage and introduces adaptive input synthesis to enhance selection accuracy.\nConcurrent Work. CodeMonkeys is a noticeable concurrent work to this paper, released on Arxiv in Jan 2025 (Ehrlich et al., 2025). It also generates multiple samples in parallel and revises samples. However, CodeMonkeys focuses on the software engineering domain, optimizing performance on SWE-Bench (Chowdhury et al., 2024), which"}, {"title": "3 Method", "content": "S* takes as input a coding problem P and a code generation model M. The model M aims to generate a program solution X(\u00b7) that maps inputs to outputs according to the problem specification.\nWe adopt the standard setup widely used in existing coding benchmarks (Chen et al., 2021; Li et al., 2022, 2023; Jain et al., 2024; Hendrycks et al., 2021a; Gulwani et al.). Each coding problem P consists of a natural language description and a set of public and private test cases, each represented as input-output pairs.\nPrivate tests evaluate the correctness of X but remain inaccessible to M during code generation. A solution is considered correct if it passes all private tests. In contrast, public tests are provided to clarify the problem's intent and are typically included in the prompt. Public tests are usually far fewer than private tests; for instance, in CodeContests (Li et al., 2022), there are, on average, 2.0 public tests and 202.1 private tests per problem. This contrasts with mathematical reasoning tasks, where evaluation typically relies on exact string matching of the final solution without additional test information (Li et al., 2024a).\n3.1 The S* Framework\nS* is a two-stage hybrid test-time scaling framework consisting of Generation and Selection stages, as demonstrated in Fig. 2. It extends parallel sampling with sequential sampling via iterative debugging to improve coverage and employs adaptive input synthesis during selection to enhance selection accuracy, leveraging execution results throughout the process. An example of effect for different stages can be found in Fig. 3.\nStage 1: Generation. In the generation stage, S* improves coverage by extending parallel scaling with sequential scaling through iterative debugging grounded with execution feedback. Specifically, S* first generates N initial samples independently, leveraging parallel sampling tech-"}, {"title": "4 Evaluation", "content": "In this section, we evaluate S* across a diverse set of instruction-based and reasoning models, spanning various model families, sizes, and access types (open and closed), as well as multiple benchmarks (Jain et al., 2024; Li et al., 2022).\nOur key findings demonstrate the generality and effectiveness of S*:\n1.  S* consistently improves model performance across different families, sizes, and types, and generalizes effectively to multiple code generation benchmarks, including LiveCodeBench (\u00a74.2) and CodeContests (\u00a74.4), showcasing its robustness and broad applicability.\n2.  S* outperforms existing widely-used test-time scaling methods, including self-debugging (Chen et al., 2023) and majority voting (Wang et al., 2022; Li et al., 2022), by enhancing both coverage and selection accuracy (\u00a74.3).\n4.1 Experimental Setup\nModels. We consider both instruction-based and reasoning-based models. To compare performance across models of different sizes using S*, we select a series of models within the same family. We experiment with 12 models: (1) Instruction-based models: Qwen2.5-Coder-Instruct series (0.5B, 1.5B, 3B, 7B, 14B, 32B), GPT-40 mini (0718 version) (Hui et al., 2024; Achiam et al., 2023); (2) Reasoning-based models: QwQ-32B-Preview, DeepSeek-R1-Distill-Qwen series (7B, 14B, 32B), and 01-mini (Qwen, 2024; Guo et al., 2025; OpenAI, 2024).\nBenchmarks. We primarily use LiveCodeBench (MIT License) as our main evaluation benchmark, given its extensive usage by recent reasoning models and its inclusion of difficulty levels, which help analyze the behavior of different techniques (Jain et al., 2024; DeepSeek, 2024; Qwen, 2024). We use its v4 version for development (e.g., selecting hyper-parameters), which contains problems from August 2024 to November 2024. For final evaluation, we use v2 version that is non-overlapping to v4, and contain more problems. LiveCodeBench"}, {"title": "5 Ablation Studies", "content": "In this section, we conduct ablation studies to analyze the key components of S*, focusing on the effectiveness and variations within each stage of the framework. We evaluate the following aspects:\n1.  Parallel Scaling: We analyze the impact of different hyper-parameter choices, such as the temperature setting and the number of samples, on parallel sampling performance (\u00a75.1). Additionally, we investigate the effect of incorporating in-context example retrieval into the parallel sampling process (\u00a75.2). We find that moderate temperatures improve performance, and adding ICL example can potential further improve performance.\n2.  Sequential Scaling: We explore variations of the iterative debugging process, including self-debugging with model-generated test cases (\u00a75.3). We find that iteratively debugging from test execution feedback improve performance, even for reasoning models. We find that simply appending execution results from public tests for every iteration works the best.\n3.  Selection Policy: We assess the performance of different selection policies, comparing our adaptive input synthesis approach with alternative selection strategies (\u00a75.4). We find that our adaptive input synthesis selection method is consistently more reliable than the generated tests and the LLM judge selection method.\nAll ablation experiments are conducted on Live-CodeBench (v4).\n5.1 Parallel Sampling Hyper-Parameters\nWe examine the impact of two key factors in parallel sampling: temperature and the number of parallel samples. Understanding their influence is essential for optimizing test-time scaling strategies.\nModerate temperatures improve performance, but high temperatures degrade it. Fig. 4 (left) shows that moderate temperatures (0.2\u20130.7) enhance performance by balancing exploration and sample diversity. However, beyond 0.7, performance plateaus or declines, likely due to excessive randomness introducing noise. Some models, such as Qwen2.5-Coder-7B-Instruct, exhibit performance regression at higher temperatures, emphasizing the trade-off between diversity and solution consistency. These findings suggest that while moderate temperatures improve generation quality, excessively high values reduce code quality.\nRepeated sampling improves performance, even for reasoning models. As shown in Fig. 4 (right), increasing the number of parallel samples significantly improves performance across all models.\n5.2 Impact of In-Context Examples\nWhile S* primarily focuses on repeated sampling for parallel scaling, it can be integrated with more advanced parallel scaling techniques. For instance, varying input prompts can create more diverse responses (Lambert et al., 2024), which in turn may lead to better coverage. In this ablation study, we investigate whether augmenting prompts with in-context examples can further improve parallel scaling performance.\nWe construct an example set from Live-CodeBench (v2) containing correct solutions and reasoning traces generated by GPT-40 mini. We explore two retrieval approaches for selecting in-context examples. ICL (BM25) retrieves the top-k similar prompts using a BM25 retriever and prepends each to a different sample when n = k (Robertson et al., 2009). This approach is simple but may overlook solution-level similarities. ICL (Pattern) groups problems by techniques (e.g., dynamic programming) and retrieves examples from the same technique, aiming to provide more relevant and structurally similar examples.\nWe evaluate medium-difficulty problems from LiveCodeBench (v4) with oracle selection. As shown in Fig. 5, performance is highly sensitive to in-context example quality. ICL (BM25) performs similarly to or worse than the zero-shot baseline in most cases, except for n = 64 with Qwen2.5-"}, {"title": "5.3 Impact of Iterative Debugging Variants", "content": "We examine the effectiveness of three variants of iterative debugging: (1) Public Tests: The model iteratively debugs using public tests and stops once the sample passes all of them. (2) +Generated Tests: In addition to public tests, the model continues debugging on model-generated tests following the algorithm in (Ridnik et al., 2024). (3) Last Round Context: The model iteratively debugs using only public tests, but instead of using code samples from all previous rounds for debugging, it uses only the last round of code sample as context. This is motivated by observations that LLMs may perform sub-optimally when handling large context windows (Liu et al., 2024).\nFig. 6 summarizes the result. We find that: (1) Even though reasoning models implicitly perform self-reflection and revising, they benefit from explicit debugging through test execution feedback: the performance of QwQ-32B-Preview model improves from 72.6 to 74.2 with 2 rounds of debugging. (2) Reducing the context window or considering more model-generated tests does not show consistent improvement: while using only the last round of context improves performance for the Qwen2.5-Coder-7B-Instruct model, it results in worse performance for the other two models. Similarly, incorporating additional model-generated tests does not enhance performance for GPT-40 mini. (3) The benefits of iterative debugging tend to plateau, typically after 2\u20133 rounds: this finding aligns with the observation that the benefit of sequential scaling flattens out (Muennighoff et al., 2025). Motivated by these findings, we choose to use 2 round of debugging, only on public tests for simplicity, and apply iterative debugging even for reasoning models in \u00a74.2.\n5.4 Impact of Different Selection Policies"}, {"title": "6 Conclusion", "content": "We propose S*, the first hybrid test-time scaling framework for code generation that substantially improves both coverage and selection accuracy.\nS* extends the existing parallel scaling paradigm with sequential scaling through iterative debugging and incorporates adaptive input synthesis, a novel mechanism that synthesizes distinguishing test inputs to differentiate candidates and identify correct solutions via execution results.\nS* consistently improves code generation performance across benchmarks, including Live-CodeBench and CodeContests. Notably, S* enables a 3B model to outperform GPT-40 mini, GPT-40 mini to surpass 01-preview by 3.7% on Live-CodeBench, and DeepSeek-R1-Distill-Qwen-32B to achieve 86.7% on LiveCodeBench, approaching 01-high at 88.5%.\n7 Limitations\nThis work primarily focuses on competition-level code generation, where it does not studies tasks such as software engineering tasks, e.g., SWE-BENCH (Jimenez et al., 2023). The method primarily focuses on improving accuracy, while it does not aim for minimizing costs."}]}