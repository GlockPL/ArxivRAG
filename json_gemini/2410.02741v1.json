{"title": "Salient Information Prompting to Steer Content in Prompt-based Abstractive Summarization", "authors": ["Lei Xu", "Mohammed Asad Karim", "Saket Dingliwal", "Aparna Elangovan"], "abstract": "Large language models (LLMs) can generate fluent summaries across domains using prompting techniques, reducing the need to train models for summarization applications. However, crafting effective prompts that guide LLMs to generate summaries with the appropriate level of detail and writing style remains a challenge. In this paper, we explore the use of salient information extracted from the source document to enhance summarization prompts. We show that adding keyphrases in prompts can improve ROUGE F1 and recall, making the generated summaries more similar to the reference and more complete. The number of keyphrases can control the precision-recall trade-off. Furthermore, our analysis reveals that incorporating phrase-level salient information is superior to word- or sentence-level. However, the impact on hallucination is not universally positive across LLMs. To conduct this analysis, we introduce Keyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned to extract salient keyphrases. By using SigExt, we achieve consistent ROUGE improvements across datasets and open-weight and proprietary LLMs without any LLM customization. Our findings provide insights into leveraging salient information in building prompt-based summarization systems.", "sections": [{"title": "1 Introduction", "content": "Abstractive summarization aims to generate concise summaries that capture the most salient information from lengthy source documents. Prior work has shown that emphasizing keywords from source documents can enhance summarization performance on supervised finetuned (SFT) models (Gu et al., 2016). However, existing approaches (Nallapati et al., 2016; See et al., 2017; Liu et al., 2021) require extensive modifications to the architecture and loss functions, hindering widespread adoption, especially for large language models (LLMs) with billions of parameters. Recent work (Li et al., 2023a) trains a separate network using reinforcement learning (RL) to generate keyphrases for LLM prompts, but training RL model is non-trivial due to convergence and stability issues (Wang et al., 2024). Emphasizing salient information in the prompt can help zero-shot LLMs generate more complete summaries, and steer LLMs to generate summaries that align with the desired use case. However, there is also a lack of analysis on how emphasizing salient information in prompts would affect the LLM behavior.\nWe first address the challenge of applying salient information to LLMs. We obtain keyphrases using a stand-alone keyphrase signal extractor called SigExt, and prompt the LLMs to consider these keyphrases when generating summaries. Unlike prior work relying on complex keyphrase generators optimized for specific LLMs, SigExt is LLM-agnostic, allowing leveraging salient information with large API-based models that cannot be fine-tuned. We demonstrate consistent improvement in ROUGE scores on 4 representative summarization datasets and 3 recent LLMs \u2013 Claude, Mistral (Jiang et al., 2023), and Falcon (Almazrouei et al., 2023) \u2013 highlighting the wide adaptability of our approach. Secondly, we conduct comprehensive experiments using SigExt to gain insights into how keyphrases in prompts affect different aspects of summary quality. We show that adding keyphrases improves ROUGE F1 and recall, making the generated summaries more similar to the reference and more complete. Adjusting the number of keyphrases influences the trade-off between precision and recall. Including additional keyphrases in the prompt tends to produce more detailed summaries, enhancing recall. Our findings indicate that using phrase-level salient information is more effective than word- or sentence-level approaches. However, for certain large language models like Mistral, adding keyphrases may lead to more hallucinations.\nOur analysis offers guidance for applying similar strategies in real-world summarization applications. While incorporating salient information is an effective method for enhancing and controlling the completeness of summaries, and using phrase-level granularity proves more effective, the risk of introducing hallucinations must be carefully considered. This risk depends on the specific LLM being used, the method for gathering salient information, and the criticality of the application.\nOur key contributions are as follows:\n1) We present SigExt, a simple yet effective keyphrase extraction model using a finetuned Longformer (Beltagy et al., 2020). Once trained, SigExt is LLM-agnostic, enabling performance boost for different LLMs by adding extracted keyphrases in prompts without requiring LLM finetuning.\n2) We provide a comprehensive analysis on the impact of adding salient information in prompts for summarization, including insights on summary length, reference alignment, completeness, and hallucination.\n3) We demonstrate that SigExt has cross-domain generalization capability through a general-purpose version (GP-SigExt) pretrained on 7 datasets."}, {"title": "2 Method", "content": "In this section, we introduce SigExt \u2013 a keyphrase extractor designed for boosting summarization quality of prompt-based LLMs. Figure 1 gives an overview. SigExt tokenizes the source document into phrases (phrase tokenization is detailed in Section 2.1), and simultaneously predict whether each phrase is important. To train the model, we create target labels by identifying phrases appear in both the source document and the summary, then optimizing the cross entropy loss. Compared to previous a keyphrase generator that uses RL (Li et al., 2023a), SigExt allows easier control of keyphrase numbers, faster training and inference, and better consistency across domains. We directly incorporate keyphrases in prompt, making it generalizable across LLMs. To handle longer input lengths while maintaining efficiency, we build SigExt using Longformer, so that training and inference can be done on a single GPU."}, {"title": "2.1 Phrase tokenization", "content": "Let $x = x_1,..., x_n$ be a source document of n tokens, and $y = y_1,\u2026\u2026\u2026, y_m$ be the target summary of m tokens. The document is segmented into non-overlapping phrases by removing stopwords and puctuation. After this, we get a sequence of T non-overlapping phrases, denoted as Phrase(x) = [$p_i = x_{l_i}...x_{r_i}$]$_{i=1...T}$. Similarly, we get T' phrases from the summary denoted as Phrase(y) = [$q_i = y_{v_i}\u00b7\u00b7\u00b7y_{r'}$]$_{j=1...T'}$."}, {"title": "2.2 Labels and learning objective", "content": "We label each input phrase by compute the fuzzy matching score\n$fuzz(a, b) = \\frac{|longest\\_common\\_sequence(a,b)|}{max(|a|, |b|)}$\nagainst all phrases in the summary. If the maximum score exceeds certain threshold $\\epsilon$, it is considered a keyphrase, formally\n$label(p_i) = \\begin{cases}\n1 & \\text{max}_{j \\epsilon 1...T'} fuzz(p_i, q_j) \\geq \\epsilon, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nWe train a classification model to predict the label. Specifically, we use a Longformer and add a classification head on top of each token. We compute the cross entropy loss on tokens that belong to phrases,"}, {"title": "2.3 Application of SigExt on summarization", "content": "We first finetune SigExt on the summarization dataset to get a task-specific keyphrase extractor. During inference, we use SigExt to extract keyphrases, then wrap the source article with a summarization prompt, and include keyphrases in the prompt. Here is an example prompt:\nHere is an news article: <text> \\nHere are a few keyphrases from the article: < key_phrases> \\nPlease write an summary for the article. \\nSummary:\nTo select keyphrases, we first score each phrase by calculating the average logits of its tokens. We then select the top-K deduplicated phrases according to their logits scores, removing any duplicates that exceed a fuzzy matching threshold $\\epsilon$ and keeping the longer phrase in those cases. We replace <key_phrases> with comma separated keyphrases. This prompt then serves as the input to the LLM which produces the final summary."}, {"title": "2.4 Cross domain generalization", "content": "In order to generalize the keyphrase extractor model to new domains without fine-tuning for the target domain, we train a general purpose keyphrase extractor using a combination of 7 datasets. The datasets are XSUM (Narayan et al., 2018), Multi-News (Fabbri et al., 2019), Gigaword (Nallapati et al., 2017), Big-Patent (Sharma et al., 2019), AESLC (Zhang and Tetreault, 2019), BillSum (Kornilova and Eidelman, 2019), and WikiHow (Koupaee and Wang, 2018). We call this general-purpose keyphrase signal extractor model GP-SigExt."}, {"title": "3 Experiments", "content": "Datasets: We select 4 representative datasets SAMSum (Gliwa et al., 2019), CNN/DailyMail (Nallapati et al., 2016), ArXiv (Cohan et al., 2018), and MeetingBank (Hu et al., 2023) \u2013 to evaluate our method. These datasets cover short and long text, as well as regular document and conversation summarization. Dataset details are shown in Table 11 in Appendix. We truncate input text to 4,000 tokens to fit the context window of the Longformer model. We follow the convention to evaluate on 500 randomly sampled examples (Zhang et al., 2020). We report results averaged on 3 runs.\nLLMs and Prompts: We evaluate SigExt on Claude-Instant, Mistral-7B-Instruct, and Falcon-40B-Instruct LLMs. We do not use Falcon on ArXiv and MeetingBank datasets due to its limited context window. We manually optimized the prompts for each model and task to achieve competitive zero-shot performance. All prompts are listed in Appendix A.\nSigExt & GP-SigExt Parameters: We use Longformer-large (433M) for the keyphrase extractor. We set the fuzzy matching threshold $\\epsilon = 70\\%$, and the class balancing weight $\\lambda = 0.1$. For SigExt, we sample 1000 examples from training set, we train SigExt starting with original Longformer-large checkpoint. For GP-SigExt, we sample 10000 examples from each of the 7 dataset mentioned in Sec. 2.4. We train SigExt and GP-SigExt for 10 epochs, and use validation set to pick the best checkpoint based on recall@20 (Metric defined in Sec. 3.7).\nDuring prompting, we try K = 10, 15, 20 keyphrases for the CNN, SAMSum, and MeetingBank datasets, and K = 30, 35, 40 keyphrases for the ArXiv dataset. We pick the best number of keyphrases based on ROUGE scores on the validation set. We also conduct an ablation study on the effect of different numbers of keyphrases.\nBaseline: We compare our methods with naive zero-shot prompting. We adapt a 2-pass extract-then-abstract method (Zhang et al., 2023) to the three LLMs and use it as a baseline. This method uses the LLM to extract sentences from the source document in the first pass, then uses the second pass to revise the extracted sentences into an abstractive summary. We also compare with Directional Stimulus Prompting (Li et al., 2023b) which utilize reinforcement learning to select good keywords.\nEvaluation Metrics: We compute ROUGE-1/-L F1 scores (abbreviated as R1-f, RL-f) to evaluate summary quality. We also report ROUGE-1 Recall (R1-r) to assess the completeness. We use AlignScore (Zha et al., 2023) to evaluate the faithfulness of the summary."}, {"title": "3.1 Main Results", "content": "Table 1 shows the ROUGE scores on all 4 datasets. The F1 scores are improved by using GP-SigExt without any fine-tuning on new datasets. By finetuning only the phrase extractor, SigExt further improves the score, showing that using a supervisedly learned keyphrase extractor can make the LLM generate summaries more similar to the reference. On average, compared to the already strong zero-shot Claude Instant baseline, R1-F improves by 1.6% with GP-SigExt and 4.1% with SigExt. Similar improvements are also observed on Mistral and Falcon models. Besides F1 scores, adding keyphrases extracted by both SigExt and GP-SigExt into the prompts can significantly increase the R1-r score, showing that adding salient information can improve the completeness of the summary. Our method achieves a smaller gain on the ArXiv dataset compared to other datasets. We hypothesize that this is because paper abstracts have a standard format, and the keyphrases they should contain are thus more predictable. As a result, the zero-shot LLM can already identify and include these keyphrases in the output. For other datasets, where the summary is more subjective, our method can help the LLM incorporate proper information in the summary to better align with the reference.\nAlthough the length of the summary slightly increase with the introduction of keyphrases, we do not achieve these improvements by excessively increasing the length of the summary. On average, the length of Claude Instant summaries increases by 4.7 words after adding keyphrases, whereas it increases by 13.6 words for Mistral and 12.3 words for Falcon.\nWe also compare the performance of SigExt with recent Directional Stimulus Prompting baseline on ChatGPT(gpt-3.5-turbo) in Table 2. We show that SigExt can also boost ChatGPT zero-shot performance, and outperform the baseline."}, {"title": "3.2 Human Qualitative Check", "content": "To verify the quality of the notes, we follow Liu et al. (2023) and conduct a human evaluation, in which they annotated Atomic Content Units (ACUs) for several public datasets. Each ACU represents a fact that should appear in the summary. We select 50 documents from the CNN and SAMSum datasets, respectively, and ask human annotators to verify whether the given ACU appears in the summary. We report both the raw ACU coverage and length-normalized ACU coverage, as proposed by Liu et al. (2023). Table 3 shows that SigExt consistently outperforms the vanilla LLM in terms of ACU coverage."}, {"title": "3.3 Number of Keyphrases", "content": "We try different numbers of keyphrases in the prompt for each dataset, and show the ROUGE-1 Precision/Recall/F1 curves in Figure 2. The F1 scores of our model are stable when changing the number of keyphrases within a fairly wide range, showing that introducing keyphrases can consistently improve the summary quality.\nAs we increase the number of keyphrases, there is a clear trend of increasing recall and decreasing precision for the Mistral model. This is less evident for the Claude model. Since we add a length constraint explicitly in the prompt (e.g., \"write a summary in 3 sentences\"), the Claude model appears to follow these instructions better than the Mistral models. Mistral models tend to try to cover all the keywords provided in the prompt. Consequently, the recall increases significantly when increasing the number of keywords for the Mistral models."}, {"title": "3.4 Granularity of Salient Information", "content": "We also explore how different granularity of salient information can affect the summarization performance. We compare word-, phrase-, and sentence-level SigExt. The results are shown in Table 4. The phrase-level salient information can always achieve top or near-top performance, while the word-level and sentence-level approaches have much larger variance. The word-level information performs poorly on the ArXiv dataset because for academic papers, there are many multi-word phrases that are important in the summary. If these are split, they are no longer helpful for summarization. In contrast, the sentence-level information is not so effective, especially on the MeetingBank dataset. When the dataset is highly abstractive, the important words are dispersed across the document, making it difficult to extract a few sentences to cover the content of the summary (See examples in Appendix Table 10)."}, {"title": "3.5 Summary Factuality", "content": "As shown in Table 5, the effect of adding keyphrases on the AlignScore is LLM and task-specific. For the Claude Instant and Falcon models, the AlignScore is typically improved by incorporating keyphrases. In contrast, the AlignScore always decreases for the Mistral model. These results suggest that keyphrases are not universally helpful for improving the faithfulness of the generated summaries. Table 8 shows a few examples where hallucination is introduced in the summary due to the keyphrases. The failure pattern is if a keyphrase is negated in the document, Mistral model would ignore the negation."}, {"title": "3.6 Introducing External Oracle Keyphrases", "content": "We also analyze how external keyphrases which do appear in the source document would affect the performance. We use oracle keyphrases that appears in the reference summary but do not appear in the source document as additional information in the prompt. The ROUGE-1 score and AlignScore are shown on Table 6. The ROUGE score increases significantly while the AlignScore falls. It indicates that introducing external keyphrases might hurt the factuality of the summary."}, {"title": "3.7 Effectiveness of keyphrase extraction", "content": "In this part, we analyze the effectiveness of the Longformer keyphrase extractor. We define recall@K metric to evaluate the keyphrase extraction performance. We define the recall@K as the recall of oracle keyphrases in the top-K deduplicated keywords, where oracle keyphrases are constructed by finding the phrase in the source document with the highest fuzzy match score to each phrase in the target summary. We compare our method with two statistical methods, Rake (Rose et al., 2010) and TextRank (Mihalcea and Tarau, 2004). Recent work has proposed transformer-based keyphrase extraction models (Sun et al., 2020; Ding and Luo, 2021) that focus on generating noun phrases to better align with human annotation. However, in our setting, the oracle keyphrases are constructed heuristically and are not limited to noun phrases, making these models a poor fit for comparison. Therefore, we do not include them. The evaluation results are shown on Table 7. We show that GP-SigExt already outperforms statistical methods. And the fine-tuned SigExt achieves additional 5.9% and 3.7% improvements on two datasets respectively."}, {"title": "3.8 Case study", "content": "We show some examples in Appendix Table 9. We found the extracted keyphrases can help the LLM incorporate precise details in the summary, hence the summaries better align with the gold summary. In the first two examples, the keyphrases contain exact numbers and times, and the LLM was able to include them in the summary. In the third example, with SigExt, the summary covers more topics than the vanilla model. Since we instruct the LLM to \"consider\" these keyphrases, the LLM was able to skip or rephrase some to get more fluent results."}, {"title": "4 Related Work", "content": "Leveraging keyword in abstractive summarization has been explored in many works. Switching Generator-Pointer (Nallapati et al., 2016) and CopyNet (Gu et al., 2016) modify a recurrent neural network model (Chopra et al., 2016) to directly copy keywords from the source text. More recent work has adopted transformer architectures (Vaswani et al., 2017), which have become dominant in natural language processing. Liu et al. (2022) introduces a bias in the attention matrix to help transformer models focus on keywords. All these models need to be trained or finetuned on large-scale training data. While finetuned models typically achieve higher ROUGE scores than prompting a pretrained model, prompt-based summarizers are preferred in some industrial use cases due to their flexibility and reduced need for data collection. Incorporating keyphrases in the prompt can effectively control the length and content coverage of the summary, something that fine-tuning methods cannot easily achieve. Therefore, we cannot compare with these methods using metrics like ROUGE.\nInstruction finetuned LLMs (Chung et al., 2022; Touvron et al., 2023; Zhang et al., 2022) have shown strong performance on summarization purely via prompts, without finetuning data. Such models are often offered via APIs, enabling easier development and deployment of summarization applications. Keyphrases are still helpful for these large models, as Li et al. (2023a) show that a keyphrase generater trained with reinforcement learning can improve summarization performance.\nThere has been interest in 2-stage extractive-then-abstractive approaches (Su et al., 2020; Liu et al., 2021; Li et al., 2021; Su et al., 2022; Yang et al., 2023). These first extract keyphrases or sentences before abstractively summarizing them. These methods are trained end-to-end for domain-specific use cases, while our method can be pretrained for general purpose zero-shot use cases. Practically, any keyword extractor, for example KeyBERT or LLMBERT (Grootendorst, 2020), can be used for the first stage to enhance the summarization in the second stage. The 2-stage methods could also be implemented as Chain-of-Thought (CoT) by generating intermediate hints and final results in the same prompt, such as Adams et al. (2023). In our experiments, we compare our method with a 2-stage prompting approach \u2013 first generating keywords using one prompt, then using those keywords for summarization in the second prompt. While slightly different from previous work, the 2-stage baseline effectively captures the use of intermediate reasoning steps of LLMs."}, {"title": "5 Conclusion", "content": "In this paper, we propose a lightweight approach to incorporate keyphrases into the prompt for LLM-based abstractive summarization. SigExt involves training a phrase extractor using supervised learning to identify salient keyphrases from the input text. These keyphrases are then injected into the prompt provided to the LLM for summary generation. We demonstrate that this approach can effectively improve the ROUGE scores of the generated summaries, indicating a higher similarity to reference summaries. Introducing keyphrases in the prompt enhances the faithfulness of the summary by ensuring that important information is captured. Additionally, our approach offers control over the length and precision/recall trade-off of the summary. Notably, our pretrained keyphrase extractor GP-SigExt- can improve summarization performance out-of-the-box without any finetuning, even in cases where training data is not available."}, {"title": "Limitations", "content": "Model Design: We use Longformer as the backbone model to build SigExt because it is lightweight and supports long context length. However, we do not evaluate the impact of using other similar-sized pre-trained language models. Additionally, we extract training labels using a fuzzy matching approach to make the model more generalizable, but more domain-specific approaches for keyphrase extraction may yield better performance.\nEvaluation: As is common in summarization research, we rely primarily on automatic metrics and qualitative example checks to evaluate performance. These techniques have known limitations in assessing summary quality. Meanwhile, human evaluation has its own challenges. Therefore, how to best evaluate the quality of abstractive summarization models remain as an open question."}, {"title": "Appendix", "content": "Here we show all the prompts we used in the experiments. In prompt, <text> will be replaced with source documents, and <keywords> will be replaced with comma separated keyphrases extracted by SigExt. We conduct light prompt engineering to get a reasonably good zero-shot prompt."}, {"title": "A.1 Zero-shot Claude Instant Prompts", "content": "SAMSum\nHere is a conversation:\n<text>\nPlease write a very short 1 sentence summary.\nSAMSum with SigExt\nHere is a conversation:\n<text>\nPlease write a very short 1 sentence summary. Consider include the following information: <keywords>\nCNN/DailyMail\nHere is a news article:\n<text>\nPlease write a summary for the article in 2-3 sentences.\nCNN/DailyMail with SigExt\nHere is a news article:\n<text>\nPlease write a summary for the article in 2-3 sentences. Consider include the following information: <keywords>.\nArXiv\nHere is a research paper:\n<text>\nPlease write a comprehensive paper abstract section.\nArXiv with SigExt\nHere is a research paper:\n<text>\nPlease write a comprehensive paper abstract section. Consider include the following information: <keywords>\nMeetingBank\nHere is a conversation:\n<text>\nPlease write a summary in about 5 sentences.\nMeetingBank with SigExt\nHere is a conversation:\n<text>\nPlease write a summary in about 5 sentences. Consider include the following information: <keywords>"}, {"title": "A.2 Zero-shot Mistral Prompts", "content": "SAMSum\n<s>[INST]Here is a conversation:\n<text>\nPlease write a short 1 sentence summary. [/INST]\nSAMSum with SigExt\n<s>[INST] Here is a conversation:\n<text>\nPlease write a short 1 sentence summary. Consider include the following information:\nCNN/DailyMail\n<s>[INST] Here is a news article:\n\nPlease write a short summary for the article in 1-2 sentences.[/INST]\nCNN/DailyMail with SigExt\n<s>[INST] Here is a news article:\n\nPlease write a short summary for the article in 1-2 sentences. Consider include the following information:\nArXiv\n<s>[INST]Here is a research paper:\n\nPlease write a short abstract in about 3 sentences.\nArXiv with SigExt\n<s>[INST]Here is a research paper:\n\nPlease write a short abstract in about 3 sentences. Consider include the following information:\nMeetingBank\n<s>[INST] Here is a conversation:\n\nPlease write a 2-3 sentence summary. [/INST]\nMeetingBank with SigExt\n<s>[INST] Here is a conversation:\n\nPlease write a 2-3 sentence summary. Consider include the following information: < keywords>[/INST]"}, {"title": "A.3 Zero-shot Falcon and Flan-T5 Prompts", "content": "SAMSum\nHere is a conversation:\n\nPlease write a short 1 sentence summary. Summary:\nSAMSum with SigExt\nHere is a conversation:\n\nPlease write a short 1 sentence summary. Consider include the following information:\nSummary:\nCNN/DailyMail\nHere is a news article:\n\nPlease write a short summary for the article in 1-2 sentences.\nMake sure the summary is no more than 2 sentences. Summary:\nCNN/DailyMail with SigExt\nHere is a news article:\n\nPlease write a short summary for the article in 1-2 sentences. Consider include the following information: .\nMake sure the summary is no more than 2 sentences. Summary:\nArXiv\nHere is a research paper:\n\nPlease write a short abstract in about 3 sentences.\nAbstract:\nArXiv with SigExt\nHere is a research paper:\n\nPlease write a short abstract in about 3 sentences. Consider include the following information:\nAbstract:\nMeetingBank\nHere is a conversation:\n\nPlease write a 2-3 sentence summary.\nSummary:\nMeetingBank with SigExt\nHere is a conversation:\n\nPlease write a 2-3 sentence summary. Consider include the following information:\nSummary:"}, {"title": "B Loss Function", "content": "The training objective for SigExt is\n$L = - \\sum_{i \\in 1...T} \\sum_{k \\epsilon l_i...r_i} [label(p_i) log f(x_k) + (1 \u2013 label(p_i)) log(1 \u2212 f(x_k))]$,\nwhere $f(x_k)$ denotes the binary classification probability on token $x_k$ by the classification head, and $\\lambda$ is the class balancing weight."}]}