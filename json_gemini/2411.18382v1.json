{"title": "ChatGPT as speechwriter for the French presidents", "authors": ["Dominique Labb\u00e9", "Cyril Labb\u00e9", "Jacques Savoy"], "abstract": "Generative Al proposes several large language models (LLMs) to automatically generate a message in response to users' requests. Such scientific breakthroughs promote new writing assistants but with some fears. The main focus of this study is to analyze the written style of one LLM called ChatGPT by comparing its generated messages with those of the recent French presidents. To achieve this, we compare end-of-the-year addresses written by Chirac, Sarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We found that ChatGPT tends to overuse nouns, possessive determiners, and numbers. On the other hand, the generated speeches employ less verbs, pronouns, and adverbs and include, in mean, too standardized sentences. Considering some words, one can observe that ChatGPT tends to overuse \u201cto must\u201d (devoir), \u201cto continue\u201d or the lemma \u201cwe\u201d (nous). Moreover, GPT underuses the auxiliary verb \u201cto be\u201d (\u00eatre), or the modal verbs \u201cto will\u201d (vouloir) or \u201cto have to\u201d (falloir). In addition, when a short text is provided as example to ChatGPT, the machine can generate a short message with a style closed to the original wording. Finally, we reveal that ChatGPT style exposes distinct features compared to real presidential speeches.", "sections": [{"title": "Introduction", "content": "Based on large language models (LLMs) (Zhao et al., 2023), (Liu et al., 2023) trained on huge corpora, the machine could generate short answers to users' requests. The produced messages are clear, coherent, plausible, and without spelling errors. Facing such successes, Bubeck et al. (2023) assert that GPT \u201cproduces outputs that are essentially indistinguishable from (even better than) what humans could produce\u201d. The news however report also other examples of such generated texts that include both incoherence and inaccuracies (called hallucinations). In this context, the current study focuses on the written style adopted by ChatGPT\u00b9 when requesting to produce short political messages."}, {"title": "State of the Art", "content": "Currently several LLMs coexist such as Google Bard, Gemini, PaLM 2, Meta Llama 2, but OpenAI's GPT is the most well-known. The initial objective is to produce a chatbot able to maintain a dialogue, for example, to help the user in identifying and resolving a problem. Trained with massive web-text data (e.g., Wikipedia), newspapers\u00b2 and books corpora, the produced messages correspond to short wordings and producing a longer passage required many interactions with the writing assistant. Therefore, the target text genre is a short response within a question/answering system but the produced reply takes account of previous interactions. According to this background, this study will focus on relatively short messages generated by ChatGPT and compared their stylistic features with true ones.\nMore precisely, the main aim of this study is to empirically test whether ChatGPT can adopt the style of a given author (Savoy, 2020). Moreover, we want to analyze the stylistic differences between the text generated by ChatGPT and the messages written by humans. Do such dissimilarities really occur? If yes, are they observable? In addition, can we characterize some stylistic features used by ChatGPT compared to true authors?\nIn the remaining part of this article, we will first present some related work while Section 3 exposes the corpus used. Section 4 analyzes some ChatGPT stylistic features by comparing the POS distribution to those occurring in speeches written by French presidents. Section 5 presents analyses based on the vocabularies and the most frequent words occurring in messages written by human and those generated by machine. Section 6 some additional experiments based on the mean sentence length and distribution while Section 7 proposes to verify whether a machine could detect a text generated by ChatGPT. Finally, a conclusion reports the main findings of this study."}, {"title": "", "content": "To keep things simple (Wolfram, 2023) the foundation model GPT is able, based on a sequence of initial tokens, to produce a ranked list of the next plausible token (e.g., word or punctuation symbol), list elaborated based on the training documents. For example, after sequence \u201cthe prime minister of\u201d, the model can define a list of the next token as {UK, India, England, Canada, France, Australia, ...}.\nFrom this list, and depending on some parameters, the system can then select the most probable one (e.g., \u201cUK\u201d in our case), or based on a uniform distribution, one over the top k ranked tokens (e.g., \"Canada\") or, randomly depending on their respective probabilities of occurrence in the training texts (e.g., \"India\"). This non-deterministic process\u2074 guarantees that the same request could create distinct messages. It is important to note that the choice of the next token is reason- able or plausible according to the training sample. This does not mean that the produced se- quence does exist in the training documents or that the resulting statement is true. Thus, and common to all LLMs, GPT output may include hallucinations (incorrect information) in its an- swers. The writing assistant will just provide a reasonable or plausible next token. Moreover, the specification of the sources exploited to produce the text stays unknown to the user.\nWhen working with such writing assistants, can we discriminate between texts written by a ma- chine or a human being? Several studies expose the effectiveness of the learning strategies able to discriminate between responses generated by GPT-3 or written by human beings (Guo et al., 2023). Based on trained black-box classifiers (e.g., RoBERTa), the recognition rate is rather high (around 95% to 98%). Such effectiveness is even obtained when the target language is not English (e.g., French in (Antoun et al., 2023)). Such a high degree could be lower when faced with a new and unknown domain or when substituting tokens by misspelled words (in such cases, the achieved accuracy rate varies from 28% to 60%). In a related study, (Gao et al., 2023) indicate that human beings are less efficient to detect machine-generated passages.\nMore problematic is the use of such writing assistants to produce scientific papers. Such appli- cations are not new (Labb\u00e9 & Labb\u00e9, 2013) even in generating tortured phrases (Cabanac et al., 2021). According to Gao et al. (2023), the scientific abstracts generated by GPT are hard to detect by expert in the field (success rate around 68%, high-impact journals). In this case, GPT abstracts appear vague, superficial, focusing on details and could employ alternative spelling of words. According to (Picazo-Sanchez & Ortiz-Martin, 2024), GPT have been used in around 10% of the 45,000 papers from 3009 journals analyzed by four automatic detectors. Recently, (Soto et al. 2024) found that LLM exhibit similar and consistent results.\nThe main drawback of those LLMs is the need of large number of texts to train them. Of course, their knowledge is time-limited by the period covered by the training set. One can assume that the English language represents the largest part of the training sample, implying a better perfor- mance in this language. Moreover, to achieve very good performance, the training sample must be similar to the test one, with similar topics and text genre. When facing with a new domain, text genre or with incorrectly spelled passages (Antoun et al., 2023) (Soto et al., 2024), the"}, {"title": "Corpus Overview", "content": "To compare the written style of recent French presidents with speeches generated by machine, ChatGPT (chat.openai.com) was asked to generate the end-of-the-year address of four presidents, namely Chirac, Sarkozy, Hollande, and Macron. For each leader and each year, we asked ChatGPT to generate an end-of-the-year address after submitting it the corresponding natural text (NT) as a model.\nIn our corpus, all NTs and GPTs were corrected and labeled according to the principles specified by Muller (1977). This implies that an orthographic standardization was applied to ensure that any differences observed between NTs and GPTs do not stem from fluctuations in word spelling. Lemmatization adds a label (lemma) to each word in the text, including its dictionary entry (for example, the infinitive of the verb) and its grammatical category. The characteristics of ChatGPT's vocabulary (see Section 4) suggest that ChatGPT does not really know French grammar and all the vocabulary, but that it works with a wide variety of tokens (graphic forms) and seems to have some difficulty with certain homographs.\nTo have an overview of our corpus, Table 1 provides the president names and Nnt, the number of words in each presidential message, followed by Ngpt, the number of words of the corresponding generated text. As we don't know exactly how ChatGPT divides messages into tokens, we have decided to divide them into words. For example, \"aujourd'hui\" (today) or \"parce que\" (because) are single words in French because one can found them as entries in language dictionaries.\nAs shown in Table 1, on average, the GPTs are almost half as long as the models. Only once (Hollande's message in 2012), the generated text is slightly longer than the natural one. As a result, more than half of these generated texts are less than 1,000 words long. Finally, one notes that the lengths of the texts generated fluctuate considerably, suggesting that, in addition to a certain instability inherent in the method, ChatGPT draws on resources other than the texts submitted by the user.\nTwo questions arise. First, what are the stylistic and vocabulary idiosyncrasies of the GPTs in relation to the NTs used as models? This question will be examined at three levels, namely the parts-of-speech (POS), vocabulary, and style. Second, can GPTs be detected, or are they close enough to the NTs, provided as models, to escape a detection procedure based on the calculation of intertextual distance and automatic classifications?"}, {"title": "Part-Of-Speech", "content": "Word labeling by the lemmatizer allows us to observe how ChatGPT employed the main grammatical categories, and whether the characteristics of the generated messages match those of the texts it has been given as models. To provide an answer to this question, we have grouped all the GPTs into a single corpus, which is compared with all the presidential messages merged together. The results of this comparison are displayed in Table 2."}, {"title": "Vocabularies", "content": "In this section, the analysis focuses on lemmas (dictionary entries) belonging first to the verb group, then to the noun group. In each case, we compare the most frequently used lemmas in the presidential and generated speeches using the rank and the frequency. Since the two samples are of unequal length, the absolute frequencies are converted into relative ones (expressed per thousand words). The last column of the following tables displayed in percentage the difference in frequencies between the natural texts and the generated ones."}, {"title": "Verb Group", "content": "The verb is the backbone of this group. Table 4 gives the rank and frequency of the twenty verbs most frequently used by presidents. The fourth and fifth columns provide the rank and frequency of these twenty lemmas in the GPTs. The last column shows the difference between the two frequencies. If the imitation provided by ChatGPT is perfect, both the ranks and frequencies should be identical and there should be zero everywhere in this last column."}, {"title": "", "content": "Looking at the first row of Table 4, among presidents, the most frequently used verb is \u201cto be\u201d with a frequency of 31.68 per thousand words. In texts generated by ChatGPT, it is also the first, but with a lower frequency of 22.03\u2030, given a difference of -30% compared with presidents as displayed in the last column.\nIn any natural French text, \u201c\u00eatre\u201d (to be), \u201cavoir\u201d (to have) and \u201cfaire\u201d (to do) are the three most frequently used verbs in that order. In GPTs, \u201cdevoir\u201d (which has several meanings in French (a legal or moral obligation, or a probable achievement in the future)) takes the place of \"to do\". Moreover, its frequency doubles in GPTs compared with NTs (while \u201cto do\u201d drops by 11%). In presidential addresses, it is the modality of the probable (\u201cnext year should be better\u201d).\nChatGPT has a little trouble with simple verbs, but even more with verbal compositions (such as \u201cpouvoir faire\u201d (to be able to do)). Three cases are particularly striking. First with \u201cto go\u201d (- 62%) which is used in French to indicate the near future, that is the main subject of an end-of- the-year message in expressions such as \u201cthe next year is going to be better\u201d, \u201csome event is going to happen next year\u201d, etc. Second, the verb \u201cfalloir\u201d (must) has almost completely disappeared (-93%). Third, one can notice the considerable decline of \u201cdire\u201d (to say) (-78%). Among presidents, the majority of uses are in expressions such as \u201cil faut dire\u201d (one must say), \u201cce qui (ne) veut (pas) dire\u201d (that is (not) to say), etc.\nIn these three cases, the main difficulty stems from the impersonal pronoun of the third person \u201cil\u201d (it), which is the obligatory subject of \u201cmust\u201d (also frequent in front of \u201cto go\u201d) and is often found after a punctuation or a subordinating conjunction \u201cque\u201d (that) (another difficulty for ChatGPT).\nClearly, ChatGPT has a poor grasp of this type of complex constructions, no doubt because their great variability results in low probabilities of occurrence for each of these constructions used by the presidents. As a result, the same difficulties are amplified for pronouns.\nTo analyze this question, Table 5 depicts the most frequently used pronouns. One can see that the pronoun \u201cwe\u201d is employed the most by presidents (16.1 per thousand words) and by ChatGPT (24.28\u2030), an increase of 51% compared to presidents."}, {"title": "Noun Group", "content": "As in the previous subsection, the analysis focuses on the twenty common nouns most frequently used by presidents, with their rank and frequency in GPTs. This information is displayed in Table 7. From this, one can see that in presidential addresses, the most frequently used noun is \"year\" with a frequency of 5.04 per thousand words. In GPTs, it is also the first but with a frequency of 8.7\u2030, i.e. +76% compared to presidents.\nTo have a better interpretation of the values depicted in Table 7, one must remember that ChatGPT uses too many nouns (+7%, see Table 2) compared to presidents. It is this average level that should be kept in mind when analyzing the last column of the table. Thus, the increase of \u201ccountry\u201d would not be significant. With this in mind, all the other fluctuations are large.\nThe table shows that the general framework of presidential messages is indeed in the GPTs, but with a stronger emphasis on generic and recurrent theme. For example, \u201cwishes for the year, the future, the country or the nation\u201d are all over-used but, strangely, the fact that these greetings are presented on the evening (December 31st) was not use by ChatGPT (e.g., \u201csoir\u201d (evening) 79%).\nIn the bottom of the table, the word \u201cchallenge\u201d is the second most common noun used by ChatGPT, whereas it occupies a modest place among presidents (72nd place, with a frequency seven times lower). Close examination of the contexts of use shows that in six cases out of ten the construction is \u201cverb + les (or des) d\u00e9fis\u201d (e.g., take up the challenges) and in two cases out of ten, one can see \u201cface aux d\u00e9fis", "relever les d\u00e9fis qui\" (take up the challenge which). This rather stereotyped use may explain why it is repeated so frequently by ChatGPT, but it also proves that it draws some elements from outside the prompted text. He is well aware that French presidents like to talk about \u201cchallenges": "but he is going the extra mile!\nThis is confirmed by the last two lines of Table 7. The presidents have all addressed their \u201cfellow citizens\" (concitoyens). But in French, nouns have a gender, and usage (at least politically correct usage) would dictate that the feminine form be stated before the masculine one. So GPT corrects the presidents: \u201cnos concitoyennes et nos concitoyens\u201d (our fellow citizen women and our fellow citizen men)!"}, {"title": "Sentence Analysis", "content": "It is well-kwon that the style of an author can be measured by several indicators (Savoy, 2020). In this study, after focusing on the POS distribution and the most frequently used lemmas, the analysis of the sentence length can reveal some pertinent aspects of ChatGPT's style. More precisely, one can ask whether ChatGPT succeed in producing sentences that are formally similar to those of the author it has been given as a model? To answer this question, the set of presidents is compared with the set of texts generated by ChatGPT (see also (Moni\u00e8re et al., 2008)). Our analysis is restricted to the sentence lengths."}, {"title": "Sentence Lengths", "content": "In the first analysis, sentences are ordered by increasing lengths, and the numbers of each length are counted in both corpora. This yields to two sets of values, summarized in Table 10. In addition, Figure 1 shows the distribution of those lengths for both corpora.\nTo obtain an overview, Table 10 shows that in the corpus of presidential addresses, the most frequent length (mode) is 12 words versus 19 in the GPTs i.e., 46% longer in the generated texts. Half of the sentences are less than 20 words long (median) in both corpora. The average sentence length is very close (21 and 21.7 words)."}, {"title": "Sentence Length Distribution", "content": "As the two corpora are not of the same lengths, the absolute numbers of each sentence length are converted into percentages. For example, in Figure 1, there are no one-word long sentence (exclamatory sentence) in the GPTs, compared with 0.27% in the presidential corpus, and so on.\nThe slight irregularities in the curve, which can be explained by the relatively small size of the two corpora, are not important. Only count the profiles of the two distributions. For example, the two main modes are clearly visible and offset (13 words in the presidential texts and 19 in the GPTs, see also Table 10).\nsentences (from 15 to 39 words) and avoids \u201cextraordinary\u201d sentences namely fewer very short sentences (lengths less than 15 words) or very long ones (more than 39 words). ChatGPT respects the mean of sentence lengths in the texts it is given as models, but randomly distributes these lengths around the mean, as evidenced by the near-equality of the three central values (mode, median, mean), a characteristic of a Gaussian distribution. On the other hand, the inequality {mode<median<mean} is the main characteristic of sentence lengths in natural texts (either written or oral).\nFurthermore, the coefficient of relative variation (CV% in Table 10) indicates that this dispersion around the mean is lower in the corpus of generated texts than in that by the presidents. This finding explains why, on the figure, the mode of sentences produced by ChatGPT is clearly higher than that of natural sentences. Finally, the absence of very long sentences in GPTs may be linked to the low frequency of complex constructions, notably subordinate clauses.\nAbout style, a conclusion is obvious namely that ChatGPT treats punctuation like words (hence the roughly identical central values). Therefore, the generator had identified which words are most likely to be followed by a comma, period, etc."}, {"title": "Can One Identify a Text Generated by ChatGPT?", "content": "To date, the intertextual distance associated with automatic classifications has proved to be an effective tool for automatic recognition of the author of a text. In particular, this method has been used to identify papers produced by previous-generation of paper mills, as well as various types of fraud in scientific publications (Byrne & Labb\u00e9, 2016), (Van Noorden, 2014), (Labb\u00e9 & Labb\u00e9, 2012) or well as in authorship attribution (Savoy, 2018).\nThe calculation compares the vocabulary of two texts (A, B), measuring the absolute difference between the frequencies of each lemma in A and B. The sum of these differences is related to the total length of the two texts. This ratio is the distance between A and B. It varies uniformly between zero (no difference) and one (no lemma in common). For example, a distance of 0.25 means that a quarter of the vocabulary used in A and B is different. Below this threshold, the hypothesis of a single author (for two contemporary texts written in French) can be examined favorably.\nIn principle, intertextual distance cannot be calculated on texts that are too short, or at least not shorter than 1,000 words. However, ChatGPT first characteristic is to generate short texts (see Table 1). Therefore, to include all messages despite their short lengths, we have merged all generated texts and all natural ones for each president. This results in eight different texts (see Table 1) of which distances had been calculated, enabling to perform the classifications presented in Figure 2 and 3."}, {"title": "Conclusion", "content": "ChatGPT generates short texts, certainly because its prime vocation is a chatbot and not a public writer. When it comes to vocabulary and sentences, these generated texts display many singular characteristics, namely underuse of verbs (especially when conjugated in tenses other than the present), underuse of pronouns (especially third persons and indefinites), and underuse of ad-verbs and subordinating conjunctions. On the other hand, ChatGPT tends to overuse common nouns, adjectives, possessive determiners, and prepositions.\nWhen looking at the sentence length and its distribution, the generator seems incapable of repro-ducing the diversity of sentence lengths, a characteristic of natural texts. Of course, these obvi-ous limitations are not insurmountable and may be mitigated in future versions of the generator.\nHowever, despite these limitations, when ChatGPT is placed in the best possible conditions (a single and short model, a known author, a date, a simple genre), it manages to reproduce the main formal characteristics of a natural text that has been given to it as a model. Moreover, the produced text makes sense to the reader who consults it. If this reader has any suspicions, there is currently no computerized tool to help confirm them. In fact, the techniques used until now to detect plagiarisms or texts generated by paper mills, no longer seem appropriate. Provided that users have taken care to submit homogeneous texts to ChatGPT, intertextual distance seems inoperative. Similarly, conventional plagiarism detection systems are likely to be ineffective, since they analyze n-grams (or word stacks), whereas GPT rearranges the vocabulary of the model in the generated text.\nFurther experiments will be needed to determine which features could be used to detect the gen-erated texts. Moreover, experiments with other languages could confirm our findings. Finally, low-resource languages could represent a real challenge for LLMs due to their limited text cor-pora available."}]}