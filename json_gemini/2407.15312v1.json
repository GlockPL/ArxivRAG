{"title": "FMDNN: A Fuzzy-guided Multi-granular Deep Neural Network for Histopathological Image Classification", "authors": ["Weiping Ding", "Tianyi Zhou", "Jiashuang Huang", "Shu Jiang", "Tao Hou", "Chin-Teng Lin"], "abstract": "Histopathological image classification constitutes a pivotal task in computer-aided diagnostics. The precise identification and categorization of histopathological images are of paramount significance for early disease detection and treatment. In the diagnostic process of pathologists, a multi-tiered approach is typically employed to assess abnormalities in cell regions at different magnifications. However, feature extraction is often performed at a single granularity, overlooking the multi-granular characteristics of cells. To address this issue, we propose the Fuzzy-guided Multi-granularity Deep Neural Network (FMDNN). Inspired by the multi-granular diagnostic approach of pathologists, we perform feature extraction on cell structures at coarse, medium, and fine granularity, enabling the model to fully harness the information in histopathological images. We incorporate the theory of fuzzy logic to address the challenge of redundant key information arising during multi-granular feature extraction. Cell features are described from different perspectives using multiple fuzzy membership functions, which are fused to create universal fuzzy features. A fuzzy-guided cross-attention module guides universal fuzzy features toward multi-granular features. We propagate these features through an encoder to all patch tokens, aiming to achieve enhanced classification accuracy and robustness. In experiments on multiple public datasets, our model exhibits a significant improvement in accuracy over commonly used classification methods for histopathological image classification and shows commendable interpretability.", "sections": [{"title": "I. INTRODUCTION", "content": "HISTOPATHOLOGICAL images are of crucial significance in medical diagnosis and are essential for disease staging and treatment strategizing. The use of deep learning models for histopathological image classification tasks is a growing trend. Yang et al. [1] realized a threshold-based tumor prioritization aggregation method for label inference within whole slide images and introduced a deep learning-based classifier tailored for lung lesion categorization, substantially advancing the identification of distinct lung cancer subtypes. Wang et al. [2] used a fully convolutional neural network architecture in the extraction of pertinent deep features, to enable accurate predictions in lung cancer tissue histopathological image classification. Hence, deep neural networks have substantially improved the precision and efficiency of pathological diagnosis, thereby facilitating early intervention and treatment.\nHistopathological images commonly encompass a variety of morphological features, including cell size, shape, and color, as well as the size and shape of cell nuclei and the distribution of chromatin. These features exhibit alterations indicative of pathological changes in cells or tissues, underscoring the significance of accurate feature extraction. Widely used approaches for feature extraction include Convolutional Neural Networks (CNNs) and Attention Mechanisms [3]. The CNN operates by sliding convolutional kernels over an image, performing nonlinear transformations on the covered regions and extracting features at various hierarchical levels. Wahab et al. [4] devised a Hybrid-CNN model by integrating two-stage CNNs through weight transfer and custom layers, which accomplished the accurate classification of mitotic and non-mitotic cells. Attention mechanisms have been extensively utilized in histopathological image classification tasks. Differing from CNNs, attention mechanisms capture global dependencies by focusing on all elements of the input sequence, employing weighted aggregations of these features to construct higher-level representations. Sadafi et al. [5] applied an attention mechanism to the classification of hereditary blood disorders, showcasing its efficacy in enhancing a model's focus on pathological cell samples and increasing classification accuracy. Valanarasu et al. [6] introduced a gated axial-attention model, incorporating additional control mechanisms within the self-attention module. The model's global branch captures long-range dependencies to learn contextual features across the entire image, while the local branch refines finer features through patch-level operations, yielding notable results in medical image analysis.\nThe above methods neglect the features of histopathological images at diverse granularity levels, limiting feature extraction to a singular scale, which leads to the incomplete capture of intrinsic feature information among cells. To address this, Li et al. [7] demonstrated the efficacy of embedding features of varying granularities during the extraction process, effectively"}, {"title": "II. RELATED WORK", "content": "A. Existing Classification Methods\nCNNs and attention mechanisms have shown notable success in image classification. Current methods can be broadly classified as based on (1) multiple feature inputs; (2) attention mechanisms; or (3) other functional modules.\nAmong methods based on multiple input features [15], Wang et al. [16] proposed a pyramid vision Transformer model that can generate feature pyramids akin to CNNs, thus achieving multiscale feature integration. Gradually reducing image spatial resolution, Zheng et al. [17] employed a Transformer framework to serially process images, thereby realizing a pure self-attention feature representation encoder. Tang et al. [18] introduced adaptive convolution with a globally complementary context for multiscale design, allowing for the comprehensive acquisition of multimodal information from various scales.\nAmong methods using diverse attention mechanisms, Yuan et al. [19] used an attention mechanism, Vision Outlooker, effectively encoding fine-grained features into Vision Transformer (ViT) token representations, thereby enhancing classification performance. Chu et al. [20] introduced the Twins model, featuring spatially separable self-attention, which optimizes and accelerates the computational process of deep learning models primarily through matrix multiplication.\nAmong methods reliant on other functional modules, Touvron et al. [21] introduced a teacher-student distillation training strategy tailored for ViT, which incorporated token-based distillation, enabling state-of-the-art (SOTA) results using ImageNet without external data. Chen et al. [22] used memory-driven Transformers to generate medical reports, with memory-driven modules to capture essential generated information.\nB. Fuzzy logic\nProposed by Zadeh in 1965, Fuzzy Set Theory addresses problems involving uncertainty and fuzziness [23] and"}, {"title": "III. PROPOSED METHOD", "content": "As shown in Fig. 1, FMDNN conducts feature extraction on the input image at three distinct granularities, whose features are concatenated with the universal fuzzy feature and fed into the corresponding encoder, yielding classification tokens for each granularity. The final classification results are derived through linear transformation for feature fusion and dimension alignment."}, {"title": "A. Multi-granular Feature Extraction Module", "content": "Relying solely on singular features has proven inadequate in feature extraction from medical images [33]\u2013[35]. Given the unique characteristics of histopathological images, pathologists typically employ multi-granularity when classifying them. This involves observing the overall tissue morphology at low magnification to identify obvious abnormal regions. At medium magnification, attention shifts to cell arrangement, nucleus morphology, and cell relationships. High magnification is then used to examine features such as nuclear characteristics, intracellular organelles, and clarity of cell boundaries. Integrating observations across multiple microscopic levels provides a more comprehensive understanding of lesion nature and severity [36].\nInspired by the multiscale diagnostic approach employed in histopathological image classification, we incorporate CNN-based multi-granular feature extraction to extract features separately on fine-grained $X_{Fine} \\in R^{H \\times W \\times C_{Fine}}$, medium-grained $X_{Medium} \\in R^{H \\times W \\times C_{Medium}}$, and coarse-grained $X_{Coarse} \\in R^{H \\times W \\times C_{Coarse}}$ levels. As illustrated in Fig. 2, an encoder-decoder structure is employed, and multiple layers of convolution and pooling are used to extract image features and reduce granularity. In the decoder, deconvolution and upsampling are used while extracting the desired granularity of features. The design ensures that the dimensions of the three granularity blocks remain the same during block embedding, reducing fusion costs and accelerating computation. As demonstrated by Kaul et al. [37], skip connections within a network facilitate the smooth propagation of gradients across the entire network, effectively preserving feature maps, particularly in capturing intricate details within fine-grained levels. The final representation of the features is $X = (X_{Fine}, X_{Medium}, X_{Coarse})$, where $X_{Fine}, X_{Medium}$, and $X_{Coarse}$, respectively, represent fine-, medium-, and coarse-grained features.\nMulti-granular features $X_{Fine}, X_{Medium}$, and $X_{Coarse}$ are reshaped into flattened 2D feature blocks $X_{Fine}^{P} \\in R^{N \\times ((\\frac{P}{2r})^2 \\cdot C_{Fine})}$, $X_{Medium}^{P} \\in R^{N \\times ((\\frac{P}{r})^2 \\cdot C_{Medium})}$, and $X_{Coarse}^{P} \\in R^{N \\times ((\\frac{P}{2})^2 \\cdot C_{Coarse})}$, respectively. Here, $(H, W)$ represents the original resolution of the features, $C$ is the number of channels, $(P, P)$ is the resolution of each feature block, and $N$ is the number of generated feature blocks, which is also the effective input sequence length for fusion with fuzzy features. Our design ensures that the dimension of $N$ matches across granularities, significantly reducing computational complexity during fusion with fuzzy features. Similar to the [class] token in ViT, we add learnable embeddings to each embedded feature block sequence, and the final output can be represented as:\n$z_{f}^{0} = [x_{class}^{F}; x_{0}^{F} E^{F}; x_{1}^{F} E^{F}; \\cdot \\cdot \\cdot ; x_{N}^{F} E^{F}] + E_{pos}^{F}$, (1)\n$z_{M}^{0} = [x_{class}^{M}; x_{0}^{M} E^{M}; x_{1}^{M} E^{M}; \\cdot \\cdot \\cdot ; x_{N}^{M} E^{M}] + E_{pos}^{M}$, (2)\n$z_{C}^{0} = [x_{class}^{C}; x_{0}^{C} E^{C}; x_{1}^{C} E^{C}; \\cdot \\cdot \\cdot ; x_{N}^{C} E^{C}] + E_{pos}^{C}$, (3)\nwhere variables $E_{pos}^{F} \\in R^{(N+1) \\times D}$, $E_{pos}^{M} \\in R^{(N+1) \\times D}$, and $E_{pos}^{C} \\in R^{(N+1) \\times D}$ are introduced, and $E^{F} \\in R^{((\\frac{P}{2r})^2 \\cdot C_{Fine}) \\times D}$,\n$E^{M} \\in R^{((\\frac{P}{r})^2 \\cdot C_{Medium}) \\times D}$, and $E^{C} \\in R^{((\\frac{P}{2})^2 \\cdot C_{Coarse}) \\times D}$. Similar to ViT, our model adopts the position embedding strategy when dealing with image data, embedding position information in each feature block to retain the accurate position of each pixel."}, {"title": "B. Universal Fuzzy Feature Module", "content": "We adopt fuzzy set theory to address information redundancy in multi-granular feature extraction, treating image pixels as elements and describing their membership to fuzzy concepts by membership functions. This approach can capture uncertainty and fuzziness in images. By setting different membership functions and thresholds, it is possible to accurately extract common features of malignant cells, thus providing more precise information for image classification.\nIn the Universal Fuzzy Feature Algorithm, an image $I$ is converted to grayscale and normalized to the range [0,1]. When extracting the fuzzy features of the image, each pixel $I_x$ is considered a fuzzy set, and different membership functions are applied to each image. We extract three fuzzy features. The obtained fuzzy universal feature set is {\u0399\u03bc, \u0399\u03c3, IT}, whose definition depends on the membership functions, which describe the degree to which each pixel belongs to a certain fuzzy concept.\nThe universal fuzzy feature $z_{Fuzzy}$ is obtained through fuzzy operations. During the model learning process, the membership functions can help it to better understand the meaning of each pixel, including its potential membership categories and degrees.\nTo extract multiple features from the image, we select multiple membership functions, each corresponding to a specific feature representation method. We choose Gaussian, Sigmoid, and Trapezoidal functions, enabling more effective guidance of the model in learning key features through the universal fuzzy features, which are used to construct fuzzy sets. Through fuzzy set intersection operations, we extract the universal features of the image. The first membership function is defined using the"}, {"title": "C. Fuzzy-guided Cross-attention Module", "content": "Weighted averaging is commonly used in fusion approaches, where the fuzzy feature matrix $x_{Fuzzy}$ is normalized and directly added to a Transformer encoder or another fusion module [38]. However, this method assigns the same weight to all models. Using the fused output as input can enhance model expressiveness and generalization. However, in multi-granularity models, this approach may lead to information redundancy, or even the loss of critical features in certain granularities. Zhang et al. [39] introduced an Edge-Guided module for medical image datasets, which learns edge-aware representations and preserves local edge features at early encoding layers, with good results.\nWe propose fuzzy-guided cross-attention (FCA) for fusion, where universal fuzzy features, serving as the driving information, are deeply integrated with features of different granularities through cross-attention, rather than being fused using addition. As shown in Fig. 3(a), the key characteristic of this process is the ability to continuously drive pixel-level information using the universal fuzzy feature, preventing overfitting when continuously learning on three individual granularities. After interacting with the fuzzy patch tokens, the guiding information from the classifier is spread to the patch tokens through the Transformer encoder, continuously guiding the learning of multi-granular features. This is expressed as (8), where $l = 1, 2, ..., L$.\n$z_{patch}^{l} = FCA (LN(z_{Fuzzy}), LN(z_{G}^{l-1})) + z_{G}^{l-1}$, (8)\nSpecifically, for different granularity branches $z_{G}$, we have $z_{G}^{l} = [fl (z_{class}^{as})||z_{Fuzzy}]$, where $G \\in (C, M, F)$ represents the three granularities, $fl(\\cdot)$ is the projection function used for dimension alignment, and $LN(\\cdot)$ is the layer normalization function. After using the guiding information for fusion, it will re-enter the next Transformer encoder and interact with its corresponding patch tokens:\n$z_{G}^{\\prime\\prime} = MLP (LN(z_{G}^{\\prime}) + z_{G}^{\\prime}), l = 1, 2, ..., L$, (9)\n$z_{G}^{l} = Encoder(z_{G}^{\\prime\\prime}), l = 1, 2, . . ., L$, (10)\nwhich promotes the propagation of guiding information and enriches the representation of each image patch, leading to improved overall model accuracy and robustness. Finally, by aggregating the representations processed through multiple Transformer encoders, we obtain the final classification results:\n$y = LN(([z_{E}^{E}]_{cls}, [z_{M}^{E}]_{cls}, [z_{F}^{E}]_{cls}) \\cdot W_{a})$. (11)\nIt is worth noting that we do not use traditional cross-attention, i.e., we do not simply change the sources of $Q, K$, and $V$, which, as shown through experiments in Section IV, does not lead to significant improvements. Fig. 3(b) shows the algorithm, where fusion involves classification and patch tokens from multiple granularity branches, as well as fuzzy patch tokens. Similar to CrossViT [40], we use fuzzy patch tokens from the fuzzy features as guiding information to interact with multi-granularity patch tokens, enabling deep guidance.\nFCA is implemented as\n$Q = z_{Fuzzy} W_{q}, K = z_{G} W_{k}, V = z_{G} W_{v}$ (12)\n$FCA(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{C/h}})V$, (13)\nwhere $W_{q}, W_{k}, W_{v} \\in R^{C \\times (\\frac{C}{h})}$ are learnable parameters; and $C$ and $h$ are the embedding dimension and number of attention heads, respectively.\nIt has been demonstrated that the multi-granularity branches primarily serve the role of feature extraction, with the fuzzy"}, {"title": "D. Model framework", "content": "Algorithm 1 shows the implementation of the model. An initial step employs a multi-granularity convolution technique to extract features from the original image at various granular levels. Through the fusion of membership functions tailored to distinct feature points, universal fuzzy features of the original image are extracted. Extracted multi-granular features and universal fuzzy features are separately embedded with patches at different granularities, harmonizing dimensions and reducing subsequent algorithmic complexity.\nNext is the fuzzy-guided cross-attention module, which utilizes the universal fuzzy features to guide training at distinct granularities. By employing a multi-granularity classifier as an intermediary, features from different granularities and universal fuzzy features are integrated through fusion learning of feature information and are subsequently back-projected into their respective branches. Each training instance comprises a granularity-specific feature and a fuzzy feature. During each training iteration, the multi-granularity classification token interacts with a fuzzy patch token that serves as guiding information. Post-fusion, with this guiding information, the multi-granularity classification token engages once again with its corresponding patch token within the subsequent Transformer encoder, enabling the transfer of acquired guiding information to the associated patch token, enriching the representation of each image block."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets and Configurations\nWe evaluated the performance of FMDNN in medical image classification on five publicly available pathological image datasets, as shown in Table I. These include diverse tissue types, varying numbers of categories, and various granularity attributes. These are as follows:\n1) Lung and Colon Cancer Histopathological Images (LC)\n2) NCT-CRC-HE-100K (NCT)\n3) APTOS 2019 Blindness Detection (BI)\n4) HAM10000 (HAM)\n5) Kvasir (Kv)\nWithin the model, we configured different granularity blocks as 224 x 224 x 3, 112 x 112 x 12, and 56 x 56 x 48. We divided the dataset into training, validation, and testing sets in a 70:15:15 ratio. Data augmentation techniques such as random cropping, horizontal flipping, and rotation were employed to enhance the model's generalization capabilities. We utilized cross-entropy as the loss function and trained the model using stochastic gradient descent with a batch size of 64. The initial learning rate was set to 0.001, with a decay factor of 0.01.\nB. Experimental Setup\nWe compared FMDNN against several classification models on different datasets:\n1) Resnet50 leverages residual connections to facilitate optimization and mitigates gradient vanishing. We utilized pretrained weights on the ImageNet dataset. We denote this as Resnet50_pre [41];\n2) We used the ViT-base model with pretrained weights from the ImageNet-21k dataset, denoted as ViT-base_pre [42];\n3) With medical images, HiFuse adopts a three-branch hierarchical multi-scale feature fusion network structure, with the advantages of Transformer and CNN across multiple scale levels. We denote this as HiFuse [43];\n4) MLP-Mixer uses an architecture based solely on multi-layer perceptrons. We denote this as MLP-Mixer [44];\n5) RSP+CR uses a semi-supervised learning framework incorporating knowledge distillation. It extracts context information at various resolutions within the multi-layer pyramid structure present in histopathological images. We denote this as RSP+CR [45]."}, {"title": "V. DISCUSSION", "content": "A. Comparison with State-of-the-Art Models\nWe compared FMDNN with SOTA methods on various datasets, as shown in Table III.\nFMDNN surpasses all SOTA models on LC and outperforms the majority of models on NCT, with an ACC only 0.5% less than that of Srinidhi et al. It achieves a high F1-score of 98.1%. Both of these datasets exhibit significant multi-granularity attributes, and FMDNN shows favorable results on other datasets as well. FMDNN outperforms all SOTA models on Bl, with an ACC of 88.2%, which is significantly better than that of Kassani et al.'s model at 83.1%. It also performs close to SOTA models on HAM and Kv.\nFMDNN outperforms other classification algorithms on various datasets. However, its classification accuracy falls below expectations on HAM and Kv, mainly due to their unique characteristics, as they exhibit imbalanced numbers of samples across different categories, in which diseases share a high degree of visual similarity, posing a significant challenge to classification. In addition, the subtle nature of multi-granularity attributes in these datasets impacts the model's performance. However, FMDNN still achieves noteworthy results.\nB. Ablation Experiment\nWe conducted ablation experiments on FMDNN, using the LC and NCT datasets. We introduced specific modifications to the model, with experimental outcomes presented in Table IV and Fig. 5.\nTo verify the Multi-granularity Feature Extraction Module, we used a CNN without (w/o) multi-granularity, denoted as w/o MG, and eliminated coarse-, medium-, and fine-grained features, denoted as w/o C, w/o M, and w/o F, respectively."}, {"title": "VI. CONCLUSION", "content": "We introduced a Fuzzy-guided Multi-granularity Deep Neural Network (FMDNN), leveraging the unique multi-granularity attributes of cells to extract multi-granular features from histopathological images for improved information representation. Classical fuzzy set theory was introduced to address the challenge of information redundancy in multi-input scenarios. Fuzzy membership functions were used to extract general features from pathological tissue images. Fuzzy-guided cross-attention was employed to fuse critical feature information. Guided by the universal fuzzy features, the model showed enhanced generalization and robustness on datasets where multi-granularity attributes are obvious. Experimental evaluations on five publicly available datasets and comparative analyses with other models demonstrated the superior classification accuracy of FMDNN, particularly in tasks involving cellular interference and subtle features.\nIn our future work, we plan to adjust the resolution of feature extraction to enable better adaption to different types"}, {"title": "D. Limitations and Future Work", "content": "Fig. 8 shows the F1-scores of FMDNN on datasets with different categories and sample sizes. The model achieves the expected results for datasets with a large number of samples, such as LC and NCT. The model performed commendably on datasets with less prominent multi-granular attributes, such as Bl, Kv, and especially HAM. However, there are still some limitations.\nIn reality, FMDNN is limited in the following ways: 1) The multi-granular feature extraction module exhibits suboptimal performance on datasets lacking discernible multi-granularity attributes as revealed by heat map analysis. 2) Our model demonstrates limited generalization in images with indistinct multi-granularity attributes. Employing NCT's weights on BL, HAM, and Kv yields ACCs of 71.2%, 50.6%, and 68.4%, respectively. The model has not yet acquired sufficient semantic information to optimize its performance effectively for these specific images.\nWe consider the following potential solutions to these limitations: 1) A dynamic gate-controlled multi-granular FMDNN could be designed that optimizes various granular features through a gate mechanism. 2) In the future, CLIP [57] could be integrated into the algorithm to enhance its semantic learning capabilities by maximizing the similarity of relevant sample pairs and learning model parameters."}]}