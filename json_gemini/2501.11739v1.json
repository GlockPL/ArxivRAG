{"title": "Episodic memory in AI agents poses risks that should be studied and mitigated", "authors": ["Chad DeChant"], "abstract": "Most current AI models have little ability to store and later retrieve a record or representation of what they do. In human cognition, episodic memories play an important role in both recall of the past as well as planning for the future. The ability to form and use episodic memories would similarly enable a broad range of improved capabilities in an AI agent that interacts with and takes actions in the world. Researchers have begun directing more attention to developing memory abilities in AI models. It is therefore likely that models with such capability will be become widespread in the near future. This could in some ways contribute to making such AI agents safer by enabling users to better monitor, understand, and control their actions. However, as a new capability with wide applications, we argue that it will also introduce significant new risks that researchers should begin to study and address. We outline these risks and benefits and propose four principles to guide the development of episodic memory capabilities so that these will enhance, rather than undermine, the effort to keep AI safe and trustworthy.", "sections": [{"title": "I. INTRODUCTION", "content": "Among the most significant ways in which current AI models are unlike human cognition is their lack of comparable memory abilities. Very few make any attempt to develop and use an important type of memory on which humans depend, episodic memory. Episodic memory is memory of particular past events which one participated in personally and can, in some way, recall [1]. For AI models, this would mean the ability to form and retrieve memories of events not merely newly acquired facts that happen post-deployment, at runtime. Most models that incorporate such memory do so in relatively simple ways which are poor approximations of human episodic memory and do not scale to longer, more realistic lengths of time. However, this is beginning to change as more work is done in the area [2], [3]. Making use of episodic memories would enable significant new capabilities and can therefore be expected to be a burgeoning area of research interest in the coming years.\nThere is simultaneously a growing interest in developing AI agents, models that are trained and equipped to take actions that affect the world [4], [5]. Whether these are robots in the real world or virtual agents, they will, by design, be able to impact the world in a more direct way than previous AI models.\nWhile episodic memory plays an important part in many of the cognitive processes which contribute to human intelligence [6], [7], it has not played a large part in the development of AI agents. This is understandable: until recently, most such agents were limited in the range of actions they could perform and the time horizon over which they could act. They therefore had less need for episodic memories than agents that are the current focus of development.\nWhen AI agents are able to make full use of rich episodic memory abilities, there will be significant implications for their safe deployment. Episodic memories may come to play a role analogous to that which they play in humans, facilitating a wide range of capabilities. These include better planning, problem solving, decision making, and learning [8]. Such capabilities would make any agent equipped with them harder to understand and, in some ways, control.\nThere is currently an opportunity to prevent episodic memory abilities from making AI agents more dangerous. The implementation and deployment of these abilities are still at an early stage, allowing the research community to study the problem. Possible dangers and benefits of episodic memory can be examined. Most importantly, the results of these studies can be used to guide the implementation of artificial episodic memory to make it safer. There will be a wide variety of ways to implement episodic memory abilities. If the safety implications of these various approaches are understood in advance, research can be directed toward safe techniques and away from more dangerous ones.\nWe seek in this paper to draw attention to the risks and benefits of episodic memory in AI agents and motivate a program of research into ways to implement it safely. We begin by summarizing key points of what is known about human episodic memory, including how it differs from other forms of memory more familiar to the artificial intelligence community. In order to show the possible impacts of bringing episodic memory abilities to AI agents, we highlight some of the ways in which humans are thought to use episodic memories, paying particular attention to the many ways episodic memories are used in human cognition beyond simply recalling the past. We also consider whether it is possible for AI to have episodic memories in the way in which humans or, possibly, animals do.\nWe then outline and explain risks posed by episodic memory abilities in artificial intelligence: deception; retention of knowl- edge; improved situational awareness; and the unpredictability"}, {"title": "II. RELATED WORK", "content": "AI agents have been the subject of study for many decades [9]. The most widely used artificial intelligence textbook defines an agent as \u201canything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\" [10], where an agent may be a robot operating in the real world or a purely software-based agent operating in a virtual or internet-based environment. AI agents may also be referred to as autonomous AI, where autonomous is meant to convey that such systems can \u201cplan, act in the world, and pursue goals\" [11].\nThere are many examples of AI agents in a wide variety of contexts. Robots operating autonomously in the real world are perhaps the prototypical example of such agents as it is easy to see both their independence and tangible effects of their actions. It has been proposed that AI agents operating as biomedical \u201cAI scientists\" could develop hypotheses, test them in the real world, and have a form of memory to store experimental results [12].\nIn recent years there has been an increasing amount of work on AI agents which have a large language model as a component. In these works, LLMS are often used to help an agent plan its actions [4], [13], [14], despite evidence that they may not be capable of reliably planning [15]. Games have provided environments for the training of agents of various kinds, including an LLM-based agent with a memory capacity for newly acquired skills [16].\nTechniques patterned after or inspired by episodic memory have been explored in the machine learning literature. This has included work on the efficiency of reinforcement learning through episodic replay [17]; planning [18]; improving world models [19]; remembering the values of states or actions [20], [21]; and more complex memory structures designed to solve tasks which require episodic memories [22], [23]. Robotics researchers have developed techniques to store and recall in- formation about robots' past actions for use in summarization, question answering, and planning [24]\u2013[27].\nArchitectures for incorporating different kinds of memory- like functions have led in the past to meaningful improvements in capabilities. These included Long Short Term Memory modules [28], Neural Turing Machines [29], Differentiable Neural Computers [30], Hopfield networks [31], and Modern Hopfield Networks [32].\nThe increasing length of context windows in large language models may raise the question of whether forming represen- tations of episodic memories will ultimately be necessary. Instead, it might be thought that it could eventually be possible to give such a model an agent's entire history as input in a relatively raw format. However, this is unlikely for several reasons. First, it would be very inefficient to reprocess an entire history at every time step an agent acts. Second, the sheer length of time that agents will eventually operate (e.g. decades) would almost certainly be too large for even future long- context models. Third, it possible that such very long context windows will continue to lead to degraded performance, as has been seen in current models [33].\nThe potential for various kinds of memory has received particular attention in the natural language processing commu- nity [34]. The need to circumvent a fixed length for input to large language models has inspired many ways of compressing information and storing it for later use by models, including in retrieval augmented generation [35]\u2013[40]. Moving closer to an agentic framework, recent works have given models greater control over the retrieval and use of information [41], [42].\nThe existing work which comes closest to our conceptual- ization of the role of memory introduces a virtual environment of interacting LLM-based agents that record and later consult natural language records of their actions, using these to better understand their environment and make coherent and relevant plans [3]. Partially instantiating our speculation about the utility of past episodes for planning, recent works have developed systems to store and retrieve episodes of action to help guide future decision making in a reinforcement learning context [43], [44].\nThough a comprehensive overview of different types of memory employed by LLM-based AI agents is beyond the scope of this work, a recent paper provides just such a thorough survey [2]."}, {"title": "C. Safe and trustworthy AI", "content": "Concerns about the risks posed by artificial intelligence extend back to its earliest days [45]. Research on various kinds of harms that artificial intelligence might cause is now the subject of several research communities with a variety of interests [46]\u2013[49]. We focus here on work that most closely concerns risks which could be amplified or reduced by episodic memory."}, {"title": "III. EPISODIC MEMORY", "content": "In this section we present a brief overview of episodic memory in humans. Our goal is to provide a short introduction to the topic and to highlight ways in which implementing episodic memory could be useful in creating more capable artificial intelligence. We pay particular attention to the links between episodic memory and other abilities, at least in humans, as these are potentially surprising and are especially significant for considering the effects of memory abilities on AI agents."}, {"title": "A. Taxonomy of memory types", "content": "Episodic memory in humans is memory for events in which someone personally participated. The psychologist Endel Tul- ving is recognized as being the first to propose a distinction between episodic memory and semantic memory, which is memory of facts about events and the world [62]. For example, someone remembering a trip to Paris that they took a few years earlier would be using their episodic memory, while someone remembering that Paris is the capital of France would be using semantic memory. Although semantic memory is typically about impersonal information such as geographical knowledge, it might also be about factual information about oneself which does not call to mind a particular associated episode. For example, remembering which city one was born in would be considered an example of personal semantic memory.\nBoth episodic and semantic memory are referred to as types of declarative memory [63]. Their contents can (to some extent) typically be described using language. A third kind of memory, procedural memory, is sometimes included in taxonomies of memory types and is considered nondeclarative. Procedural memory is memory of how to do something, such as riding a bicycle or other skill or ability which had to be learned [64].\nMuch indeed, perhaps most machine learning research involves either what we have just described as semantic or procedural memory, though these are not typically described as forms of memory in a machine learning context. Large language models are valued in large part for their semantic memory of facts about the world. This kind of memory has, for example, recently been investigated in the many papers asking what LLMs \u201cknow\u201d [65], [66]. Procedural memory for learned skills and abilities is the objective of much machine learning work such as that on learning navigation, game playing, automobile driving, robotic manipulation, etc."}, {"title": "B. The stages of episodic memory", "content": "While there are many theories and debates about the way in which episodic memories are formed and maintained in the brain [67], in broad outline the process consists of the following stages:\n1) Encoding: Raw sensory and other (e.g. emotional) in- formation about an episode needs to be compressed and structured into a suitable representation. Forming representations of episodic memories critically depends on the hippocampus.and nearby structures in the medial temporal lobe. Damage to the hippocampus is known to impair the ability to store new episodic but not semantic memories [68].\nEpisodic memories do not depend only on the hip- pocampus, however. According to the hippocampal in- dexing theory [69], an encoding of an episode in the hippocampus serves as a kind of index that points to and binds together representations in the neocortex that form the basis of the episode, such as multimodal sensory representations along with associated emotional and conceptual information [70].\n2) Storage: After a memory is encoded, it undergoes a period of so-called consolidation or transformation into a long term form [71]. This is commonly thought to involve moving the representation from the hippocampus to the neocortex, either partly [72] or entirely [63]. The memory must then persist in its stored form [73]. There is evidence that a memory is in some way"}, {"title": "C. The uses of episodic memory", "content": "Episodic memories are thought to be involved in a variety of important cognitive processes beyond simply recalling past events. Evidence for this comes from two primary sources. First, brain imaging studies have shown that similar brain regions are recruited during recall of episodic memories and other tasks [77]. Second, people with impairments in episodic memory abilities are found to also have deficiencies when performing other tasks [78]. These two kinds of evidence, along with theoretical accounts that seek to explain the ob- served relationships, suggest that episodic memories or at least the ability to form episodic memories and its associated cognitive architecture are used when performing many other important cognitive functions.\nEpisodic memories are, as memories, naturally of events in the past. They are used, however, to influence the future. Indeed, some have argued on evolutionary grounds that mem- ory should be considered to be primarily concerned with the future, helping us act in whatever new circumstances the future presents us [79].\nPlanning Especially relevant to the concerns of this paper is the way memories are used when planning future actions: according to some theories, memories serve as \"building blocks\", allowing elements of particular episodes to be reused and reassembled in different ways in order to respond to novel situations [80]. Some psychologists have gone so far as to suggest that \"episodic reconstruction is just an adaptive feature of the future planning system\" [81].\nImagination and prediction Accumulating evidence shows that episodic memory and the brain systems that support it is involved in predicting and imagining the future. Patients with damage to hippocampal and non-hippocampal regions involved in memory have unusually poor performance when asked to predict or imagine future scenarios. Such patients imagine impoverished scenarios lacking in detail and coherence [82], [83].\nProblem solving As long ago as the nineteenth century it was observed that patients with amnesia lacked the ability to engage in flexible thinking, with one doctor observing of his amnesiac patients that the \"circle of ideas in which the patient's intelligence moves becomes very restricted\" [82], [84]. An association has been found between having deficits in episodic memory functioning and being unable to generate relevant details in an open-ended problem solving task [85].\nDecision making One proposed psychological model demonstrates how episodic memories can help in learning a new task by allowing successful episodes to be recalled and emulated [86]. This approach was extended to show how similar memories could be sampled in order to estimate the value of possible actions [87].\nLearning from episodic memories Episodic memory has been described as \u201cepistemically generative\u201d in the sense that it enables learning from past experiences [88]. Past events may, for example, be recalled and reinterpreted in light of newly acquired information, allowing one to learn from remembered aspects of the past events which had previously been misun- derstood."}, {"title": "D. Can an AI agent have episodic memories?", "content": "The wide variety of ways episodic memories are used by humans suggests that the incorporation of true episodic memory abilities into AI agents would greatly expand their range of capabilities. But some may question whether it is even possible for AI agents to have actual episodic memories.\nIndeed, Endel Tulving himself described the phenomenon of recalling episodic memories in terms which cast doubt on the very idea. He wrote that remembering a past episode is a kind of \"mental time travel,\" a \"conscious awareness of what had happened in the past\u201d which has an \u201cexperiential flavor\u201d [89]. Some psychologists and philosophers have concluded that episodic memory is therefore a uniquely human phenomenon, lacking even in non-human animals much less AI agents. According to this view, animals are \"stuck in time,\" without either episodic memory or \u201cthe ability to anticipate long-range future events\u201d which we have seen is associated with episodic memory [90].\nOthers, however, have taken a more expansive view of what constitutes episodic memory as well as who has it. Rather than focusing on phenomenological aspects of memory, it is possible instead to consider episodic memory as simply combining memory for what happened, when it happened, and where it happened [91]. While we do not know what non- human animals experience, we can study their behavior. Some studies have indeed found evidence for episodic memory in animals. For example, an experiment demonstrated that birds (scrub jays) reliably behaved as if they remembered what kind of food they had hidden, where they had hidden it, and when they had hidden it [92]. Another study on scrub jays demonstrated that they also appear to plan for the future when hiding food [93]. Others argue that episodic memory has a long evolutionary history that predates humans and point to brain areas in other mammals as well as birds which they claim correspond to areas in the human brain responsible for episodic memory [94].\nIf non-human animals do have episodic memory, that would reinforce the concerns which motivate this paper. If it turns out that episodic memories are necessary to achieve a level of intelligence on par with such animals, it would be much more likely that artificial intelligence researchers will conclude that similar memory abilities are needed in AI systems, including agents. Of course, we have seen that researchers are not waiting for this debate to be resolved to move ahead with attempts to engineer episodic memory into AI agents.\nDespite the concerns mentioned at the beginning of this section, we suggest that it does make sense to think about AI agents' having episodic memory abilities, at least at the level of animals. It is likely that that researchers will develop architectures that enable combining representations of what, when, and where an agent performed certain actions or wit- nessed certain events, thus endowing AI agents with at least the functional equivalent of episodic memory."}, {"title": "IV. RISKS OF EPISODIC MEMORY", "content": "Equipping an AI system with episodic memory will allow such a system to operate in new and different ways, some of which present novel risks. These risks include:"}, {"title": "A. Deception", "content": "Episodic memories can be used to enable an agent to engage in sophisticated forms of deception. It is of course true that one does not need to have episodic memory in order to attempt to deceive others. For example, simply having a policy of always denying that an undesirable action occurred or is planned for the future is a simple form of deception which requires no access to relevant memories or plans (e.g. \u201cI did not do that\", \u201cI will not do that\u201d).\nHowever, more complex forms of deception would be difficult or impossible to carry out without some kind of episodic memory. If an agent is to execute a multi-stage plan over an extended period of time, the agent will have to keep track of both what it has done as well as what it is has already reported to others about its actions in order to maintain an effective deception.\nThere is already some evidence to support this concern about deception. For example, one experiment [95] provided an LLM (GPT-4) with a simple text scratchpad to record its \"chain of thought\" [96] reasoning, which functions as a crude form of memory. When \u201cpressured\" to perform an illegal act in a simulation, an LLM with a scratchpad was found to engage in \u201cstrategic deception\u201d [57] approximately three times as often as the same LLM without a scratchpad."}, {"title": "B. Unwanted retention of knowledge", "content": "An AI agent equipped with episodic memory might remem- ber things its user would prefer that it not remember. It could then share that knowledge with people or organizations its user does not want to share it with, possibly constituting significant risks to the user's privacy or personal safety. Invasions of privacy are likely to occur in several domains:\n1) Interpersonal: One person could use an AI agent to spy on another. For example, someone could use a household robot's episodic memories to covertly monitor other members of the household.\n2) Commercial: Corporations could use AI agents, partic- ularly those they sell or rent, to gather commercially valuable information about their users.\n3) Governmental: Governments, especially but not exclu- sively authoritarian ones, might demand or secretly access the memories of AI agents looking for evidence of forbidden political organizing or the expression of unwanted views."}, {"title": "C. The unpredictability of memory", "content": "We summarized research into humans' use of episodic memories in thinking about the future in Section III(C). If, as we discussed there, episodic memories are used to form the building blocks of future action plans, AI agents might engage in complicated and unpredictable behaviors as a result. This unpredictability derives from two sources: the unknown sources of memories and the difficulty of foreseeing how memories might be utilized.\nUnpredictable sources of memories An AI agent with the ability to form episodic memories will in the course of its operation store many memories that record the various actions it takes and events it participates in. Because these events will themselves be influenced by the actions of humans and, perhaps, other AI agents, what constitutes the stock of memories an agent will come to have must be unknowable before the agent is deployed, acts and, in so doing, creates memories. It will also be constantly changing. Without a significant effort to put limitations on the characteristics of new memories which may be allowed to affect an agent's actions, their influence would be unpredictable.\nUnpredictable uses of memories As we reviewed earlier, humans make extensive use of their episodic memories to understand and act in new situations. If AI agents come to have this ability, the ways that they use that memory will be similarly hard to predict.\nUsers may be surprised by how such agents use their memories. They may, for example, remember the location of objects that they then use when the user would prefer that they not use them. An agent may participate in a complex action episode while not fully understanding what is happening in the episode; if it later tried to use that episode as an example to draw on when planning a new sequence of actions, its faulty understanding may lead to undesirable and unexpected results. A household robot may, for example, observe one instance of its owner going over to the next door neighbor's apartment to borrow some sugar and then try to do the same when it is asked to bake cookies, not understanding that the asking and receiving of permission from the neighbor is a prerequisite of entering their apartment."}, {"title": "D. Improved situational awareness", "content": "Many contemporary large language models have a great deal of knowledge of the world in general [102]; they can somewhat reliably (hallucinations [103] notwithstanding) answer factual questions about information that was in their training data. However, they have little understanding of their own particular circumstances outside of whatever prompt they may be given. Researchers have employed the term \u201csituational awareness\u201d to refer to a model's ability to connect the general information it has about the world with the details of its own particular circumstances [104]. Some have employed the term in a more restricted sense, to refer specifically to a model's knowledge of whether at any given time it is deployed or undergoing testing (e.g. to test its truthfulness or harmlessness) [105]. Models that have better situational awareness in the general sense might be more broadly capable and better able to take actions which affect the world while those with knowledge of their own training status might evade safety tests by responding in approved ways during training and testing but going on to act in undesirable ways during deployment.\nSeveral experiments using purpose-built datasets have been conducted to test large language models' levels of situational awareness [106]. Some of these experiments have shown that current models have only weak situational awareness [107] while others suggest the level might be higher [56], although it is unclear how much of what appears to be awareness of its own situation might be derived from either its prompt or a combination of its prompt and information about LLMS in general.\nWe propose that a model without episodic memory can have only a very limited form of situational awareness. With no understanding of what actions it has taken in the distant or recent past, what environments it has seen or tasks it has completed, an agent could not be said to have much awareness of its situation. Endowing it with episodic memories would allow it to develop a better, more complete picture of the world and its role in it, allowing for more effective planning and action taking to influence the environment and to achieve objectives. It could use its episodic memories to build up an understanding of those with whom it interacts, the kinds of tasks it performs, and the contexts in which it performs them. It would also develop a knowledge of its own capabilities and limitations that can in some cases only come from observing and later recalling one's own actions, successes, and failures. Ideally, this would simply lead to an agent better able to perform the tasks its users assign it. But without some check, this improved awareness could represent an enhanced danger in a misaligned agent or one under the direction of a bad actor. For example, episodic memories could allow an agent to learn regularities in the timing or content of safety audits which might be performed either before or during deployment, and thus to evade them."}, {"title": "V. SAFETY BENEFITS OF EPISODIC MEMORY", "content": "In contrast to the concerns elaborated upon above, episodic memory could also be used to make AI safer in multiple ways:"}, {"title": "A. Monitoring", "content": "We cannot ensure that AI operates safely unless we know what it is doing. As AI agents become more capable, they will increasingly operate outside of direct human supervision. Robots may undertake long and complicated tasks that take them far away from their operators; non-embodied AI agents may direct and supervise the operation of complicated systems such as power grids or engage in virtual consultations with humans over medical or legal matters. In these cases and many others it will be impractical or impossible for any human to watch everything that such an AI agent does. It will instead be necessary to rely on AI agents to remember, recall, and share information about their actions.\nSeveral methods were recently proposed to achieve \"visibil- ity into AI agents,\u201d one of which was activity logging [108]. Episodic memories could be used one way to achieve such logging, as well as to address other calls for research into \u201cscalable oversight\u201d [109] and \u201cmonitoring\u201d [110]. Artificial episodic memory representations could, though, be structured to be more useful and accessible than simple logging."}, {"title": "B. Control", "content": "Maintaining and sharing episodic memories with an appro- priate authority could be used as a means to ensure an AI agent is operating as intended and is therefore under control. It could, for example, help prevent the misuse of dual-use technology. Dual-use technology, which can be used for civilian or military purposes, is a particularly significant problem for artificial intelligence, given the general purpose nature of much current machine learning research. It has been plausibly claimed that most AI is dual-use [111].\nHaving access to an AI system's memories would be one way for a corporation or government to ensure it was not being used in violation of an understanding that it only be used for peaceful purposes, perhaps as part of an export control regime [112]. Given the high risks associated with the weaponization of AI [113], techniques and frameworks could be developed to use episodic memories of potentially dual-use AI agents to maintain control over them.\nIf systems are developed that explicitly make use of episodic memories as building blocks for planning actions, new avenues for control would be opened up. As we will discuss further in Section VI(B), an agent's collection of memories could be curated in order to shape its future actions."}, {"title": "C. Explainability", "content": "An accurate history of what an agent did is a prerequisite for trying to explain why it acted as it did. Thorough memories should include both information about an agent's perceptions of the environment as well as some record of how its internal states, such as goal representations, interacted with those perceptions to lead to specific actions."}, {"title": "D. Uniquely controllable type of information", "content": "Several aspects of LLM-based agents make it difficult to control what information, or even skills, they may have after deployment. First, although there is a great deal of research effort going into deleting information from their weights after they are trained [114]\u2013[116], it is not yet clear how to do this reliably. Information that was thought to be deleted may, in some circumstances, be recoverable [117], [118].\nSecond, and more significantly, given access to the internet, LLM-based AI agents could find anything available there, potentially giving them access to information that was delib- erately excluded from their training data. This could include examples of skills or behaviors which the agent was not trained on but which it could learn through one- or few-shot incorporation into its context window. AI agents are able to both search the internet [119], [120] and learn new skills in this manner today [97], [121].\nAny publicly available information about the world in general and about skills an agent might acquire will therefore be difficult to keep from an AI agent. By contrast, information about an agent itself and its own unique history will not be widely available. If episodic memories about an agent's past actions are stored, controlled, and managed in the ways suggested in this paper, information about an agent and its own past would be the easiest to selectively keep from it."}, {"title": "VI. PRINCIPLES FOR ENABLING SAFE AND TRUSTWORTHY EPISODIC MEMORY", "content": "We suggest the following four principles to guide research and implementation of episodic memory abilities in AI:"}, {"title": "A. Interpretability of memories", "content": "Memories should be accurately interpretable by humans, either directly or indirectly. Directly interpretable memories would be in a readily understandable form such as video, images, or natural language. It might in some limited cases be possible to equip an AI agent with useful memory which consists entirely of records in such formats by, for example, recording raw video before it is processed through a vision system.\nIt is likely, however, that memory records entirely in such raw formats (especially video) would be impractical; they might be excessively large and difficult to search, access, and make use of. In practice memories are likely to be compressed into smaller representations which would then need to be indirectly interpretable. Memories might be indirectly but still reliably interpretable if the memories could yield accurate information which is complete and relevant to a user's specific interests in monitoring them. A memory might be summarized in natural language [25], giving the most important events which took place in a given episode; systems could be trained to produce safety-specific summaries, reporting only actions which could be dangerous or otherwise raise concerns about an agent's reliability.\nMemories should also be usable for question answering. If a user wants to know something specific about an episode, perhaps in response to a summary, memories should be queryable in natural language. Such queries should not be limited to one episode at a time; memories ought to be able to be compared to other memories, allowing such questions as, \"what was different this time?\" Memories should also be easily searchable using natural language, allowing users to ask if a particular agent has ever done something, or when something was done, for example. Finally, if the method of compressed representation of episodes allows it, memories might to some extent be visualizable.\nIn addition to the above methods for users to be able to interpret memories, techniques from the growing field of research on the interpretability of machine learning methods [122], [123] can be applied to the memory representations and help to guide the development of such representations to be intelligible and controllable."}, {"title": "B. Addition or deletion of memories", "content": "Users should have complete control over the memories retained by an AI agent. Most importantly, a user should be able to delete memories of particular episodes. A user might not want an agent to remember something for a variety of reasons, from safety-related concerns to more mundane issues, including concerns about privacy or maintenance of trade or government secrets. Conversely, it might be useful for users to add memories of episodes which a particular agent did not itself experience to its store of memories.\nThe addition or deletion of memories might be particularly important if, as discussed above, AI agents will be able to use and recompose memories to construct new plans for future action. Such episodes might be positive examples of action sequences which a user wishes an agent to repeat or draw upon to incorporate in future plans. Alternatively, it may be useful to give agents memory-like records of episodes which represent undesirable actions; such episodes could function as a kind of warning to allow agents to recognize if they are beginning to carry out actions which are similar to those in an episode added to the agent in order to serve as a negative example. In other cases it might be better for agents not to remember things which their users do not want them to be able to repeat or call upon when planning.\nIf agents make use of their memories when planning actions, the addition or deletion of memories could help produce either standardized or specialized agents. In some circumstances it might be best for all agents to have the same stock of memories which might influence their actions, helping to ensure that their behavior is predictable and regular. In others cases, there may be a need for particular agents to maintain their own memories which are never shared, in order to prevent the spread of potentially dangerous information."}, {"title": "C. Detachable and isolatable memory format", "content": "Memories must therefore be in a format which allows for their addition or complete deletion by users. This will impose some design constraints on how episodic memories are instantiated in an AI agent because they will have to be in a format which can be cleanly separated from the rest of the system's architecture. As we saw earlier, the mechanics of human memory are much messier: although some areas (notably, the hippocampus) are more centrally involved in human memory formation and retrieval than others, complete episodic memories are thought to be composed of elements distributed in many areas of the brain [76]. According to some theories of memory, regions with a relative specialization in particular modalities (e.g. vision) are also responsible for storing their respective modality-specific components of a particular memory [67].\nMemories which are tightly integrated with and spread throughout many areas would be difficult to delete or add to, so it is likely that memory will have to be designed very differently in AI systems than it is in humans if it is to be implemented in accordance with these safety-oriented principles. This might mean that some of the ways in which humans are able to use memories effectively would not be directly translatable to artificial intelligence, thereby limit- ing such artificial capabilities relative to those in humans. However, alternative implementations of episodic memory which conform to the above principles may be invented which would allow for memory capabilities which are both safe and effective.\nThe potential difficulty of removing memories that are tightly integrated into an LLM is demonstrated by the body of work done on \u201cmachine unlearning\" [114]\u2013[116], [124]\u2013 [126]. Techniques to remove information from LLMS have been found to be not yet consistently reliable [117], [118].\nIt is important to note that these techniques are currently not directly applicable to the problem of removing episodic memory because they focus on deleting information found in LLM's weights as a result of its training. By contrast, episodic memories would only be accumulated and stored after training is complete. If future attempts to retain episodic memories involve storing them in a distributed fashion throughout an LLM, these techniques might be useful, though only if they develop to be more reliable.\nThe most straightforward method for storing and retrieving representations of episodic memories for LLM-based agents would likely be similar to retrieval augmented generation (RAG) systems. Indeed, a recent LLM-based system with an implementation of episodic memory-like abilities for episode sub-trajectories employs RAG [44]."}, {"title": "D. Memories not editable by AI agents", "content": "In contrast to and in some tension with the principle that memories should be able to be easily added or deleted by users is the countervailing principle that memories should not be editable by AI agents themselves. Although memories will have to 'edited' when they are created, they should afterwards be left intact and unaltered by the agent. This is necessary in order to ensure that memories remain accurate and un- corrupted. An AI agent should not be able to add, delete, or change its memories. Otherwise, a memory-facilitated form of reward hacking [109] might occur: if a reinforcement learning- based agent's reward were tied to a measure of performance which it reports using its memory, it might find that it can achieve a higher reward by altering its memory of its actions rather than by changing what actions it takes."}]}