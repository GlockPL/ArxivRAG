{"title": "PLOTS UNLOCK TIME-SERIES UNDERSTANDING IN\nMULTIMODAL MODELS", "authors": ["Mayank Daswani", "Mathias M.J. Bellaiche", "Marc Wilson", "Desislav Ivanov", "Mikhail Papkov", "Eva Schnider", "Jing Tang", "Kay Lamerigts", "Gabriela Botea", "Michael A. Sanchez", "Yojan Patel", "Shruthi Prabhakara", "Shravya Shetty", "Umesh Telang"], "abstract": "While multimodal foundation models can now natively work with data beyond\ntext, they remain underutilized in analyzing the considerable amounts of multi-\ndimensional time-series data in fields like healthcare, finance, and social sciences,\nrepresenting a missed opportunity for richer, data-driven insights. This paper pro-\nposes a simple but effective method that leverages the existing vision encoders\nof these models to \"see\" time-series data via plots, avoiding the need for addi-\ntional, potentially costly, model training. Our empirical evaluations show that\nthis approach outperforms providing the raw time-series data as text, with the ad-\nditional benefit that visual time-series representations demonstrate up to a 90%\nreduction in model API costs. We validate our hypothesis through synthetic data\ntasks of increasing complexity, progressing from simple functional form identifi-\ncation on clean data, to extracting trends from noisy scatter plots. To demonstrate\ngeneralizability from synthetic tasks with clear reasoning steps to more complex,\nreal-world scenarios, we apply our approach to consumer health tasks \u2013 specifi-\ncally fall detection, activity recognition, and readiness assessment \u2013 which involve\nheterogeneous, noisy data and multi-step reasoning. The overall success in plot\nperformance over text performance (up to an 120% performance increase on zero-\nshot synthetic tasks, and up to 150% performance increase on real-world tasks),\nacross both GPT and Gemini model families, highlights our approach's potential\nfor making the best use of the native capabilities of foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal models like GPT4 (Achiam et al., 2023) and Gemini (Gemini Team et al., 2023) are\ntrained to understand visual information natively. However, they are not specifically trained to un-\nderstand time-series data \u2013 in particular, the tokenizers for large language models (LLMs) are not\nwell-suited for representing large sequences of floating point numbers (Spathis & Kawsar, 2024).\nThis mirrors the human approach; we cannot easily make sense of a long array of floating point num-\nbers - instead our first instinct is often to visualize the data through plotting, followed by extracting\ninsights through statistical analysis.\nWe investigate the hypothesis that multimodal models understand time-series data better through\ntheir vision encoders than through the textual representation of the sequences using synthetic and\nreal-world data experiments. Our synthetic data experiments allow us to closely control the difficulty\nof tasks through the addition of noise and by changing the number of points in each function. We\nalso use a mix of tasks that require a differing number of reasoning steps, as well as different kinds\nof reasoning, to get a correct answer."}, {"title": "2 RELATED WORK", "content": "We use the term \u201ctime-series understanding\" in this paper to distinguish from time-series forecasting.\nTime-series forecasting predicts future data points based on points seen so far, whereas we are\nprimarily interested in the setting where we connect the time-series data to a multimodal model for\nfurther analysis. In particular, we want to show that multimodal models can reason about overall\ntrends, the relationship between multiple time-series, overall clustering of data, and other time-series\nunderstanding tasks. For a closer look at the literature, the survey paper by Zhang et al. (2024) tracks\na wide range of time-series understanding and forecasting work.\nWe deliberately avoid using multimodal models to generate any of the questions themselves (e.g.\nSPIQA (Pramanick et al., 2024), CharXiv (Wang et al., 2024)) as this can introduce biases during\nevaluation that are hard to account for (e.g. favoring their own output as in (Panickssery et al.,\n2024)).\nLLMTime (Gruver et al., 2024) shows that with careful tokenization, text-only LLMs can perform\nwell at forecasting tasks. We perform ablations based on their methods and select the best tokeniza-\ntions accordingly for our text baselines.\nThere are several existing approaches that train time-series encoders for specific tasks or domains.\nFor example, Chan et al. (2024) train a domain-specific encoder for multimodal medical time-series\nanalysis. Similarly, Cosentino et al. (2024) train an encoder for the sleep data in their Digital Well-\nbeing task. In this paper, we are not claiming that our approach would outperform a task-specific\nencoder on a specific task we claim that one can achieve much better performance from a foun-\ndation model by exploiting its native multimodal capabilities compared to using only text. We want\nto show that simply plotting the data is at the very least an easy first step, and might be a helpful\napproach when training a task-specific encoder from scratch may not be feasible due to the require-\nments on having additional paired data, compute and expertise."}, {"title": "3 METHODOLOGY", "content": "We evaluate our visual prompting method on both synthetic data and real-world use-cases. Synthetic\ndata allows us to control the difficulty of the task by adding noise and altering the number of data\npoints, and to investigate specific kinds of reasoning in isolation. We chose the synthetic tasks to\nalign with the different steps of reasoning we hypothesize are required for the representative real-\nworld use cases we test on. These include understanding the local and global longitudinal signatures\n(trend and magnitude) of a time-series, and potentially comparing it with several other time-series\n(as in the case of multidimensional sensing). We summarize in Section 4.1 the different tasks in our\nexperiments, along with the type of reasoning we are probing."}, {"title": "3.1 STRUCTURED PROMPTING", "content": "We used the open-source structured prompting library Langfun (Peng, 2023) for all tasks in this pa-\nper, with the exception of the Readiness task (Section 4.2) which is processed in a privacy-preserving\nsandbox environment. The prompts and Langfun code snippets for all tasks (except Readiness) are\nprovided in Supplementary Information S.1.4 for reproducibility. The structured prompting ap-\nproach in Langfun allows us to use target schemas for outputs, though we do not use the controlled\ngeneration feature (Gemini models) or structured output (GPT40 models), simply relying on the\nnative formatting of the model to the correct schema."}, {"title": "3.2 MODELS", "content": "We tested all synthetic data tasks on two frontier models: Gemini Pro 1.5 (gemini-pro-001) and\nGPT40 (gpt4o-2024-08-06) and two smaller models Gemini Flash 1.5 (gemini-flash-001) and\nGPT40-mini (gpt4o-mini-2024-07-18). We use a temperature of 0.1 for all our experiments, Sup-\nplementary Tables S33-S35 includes our ablations on temperature. All other sampling parameters\nremain at API defaults."}, {"title": "3.3 FLOATING POINT REPRESENTATION", "content": "In order to find the best textual representations, we ran ablations (Supplementary Information S.1.3)\ninspired by LLMTime (Gruver et al., 2024) on which floating point precision (2, 4, 8, 16) and\nseparator (space or comma and space) to use. We also tested the scaling approach suggested by\nLLMTime. We found that the lower precision led to better performance. On synthetic tasks, the\nbest performing separator differs per model, on Gemini we make use of the space separator whereas\non the GPT4o family we use comma and space. On real-world tasks we make use of the space\nseparator for all models as we did not observe a difference in performance on these tasks and the\nspace separator uses fewer tokens."}, {"title": "3.4 STATISTICAL METHODS", "content": "For our aggregate results, we aggregate individual model responses to an overall performance quality\nmetric (accuracy or mean absolute error (MAE)) over the task dimensions as described in Supple-\nmental Section S.1.1.1. This produces multiple points from which we extract a distribution presented\nas a box-plot where the central line is the median, the edges of the boxes are the inter-quartile range\n(IQR), the whisker lengths extend to 1.5 times the IQR and outliers are presented as individual\npoints. For our more detailed plots in Supplementary Information S.1.2 we show 95% confidence\nintervals constructed from 1.96 times the standard error of the mean of the metric.\nFor real-world tasks, since we don't have the ability to regenerate the same problem, we instead make\nuse of bootstrapping (with 1,000 replicates) to produce distributions of the macro-averaged F\u2081 scores\nfrom which we construct similar box-plots as for the synthetic tasks. Note that the distributions\nplotted in the real-world box-plots are thus expected to be tighter than the synthetic task plots, as\nthey don't reflect independent replicates.\nIn Table 2, we present the median and IQRs of the differences between plot and text performances,\nwith the difference taken such that a positive value always means the plot method performs better\nthan the text method. For synthetic tasks, the median and IQR are calculated by directly creating\nthe distribution of differences between the plot and text performances of different instances of the\nexperiment, while for real-world tasks we create a distribution of differences by randomly sampling\n1,000 random pairs of the bootstrapped metric distributions described immediately earlier.\nFor synthetic tasks only, we test for significant differences between the plot and text performances\nwith a two-sided Wilcoxon signed-rank test (Wilcoxon, 1945). We apply a Bonferroni (Bland &\nAltman, 1995) correction for multiple comparisons within a task block. We could not apply the\nsame hypothesis testing framework to the real-world tasks as the performance distributions were\nbootstrapped and thus not independent, violating the assumptions of the Wilcoxon test."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "In this section we provide a summary of each task and the differences in plot- versus text-based\nmodel performances."}, {"title": "4.1 SYNTHETIC DATA TASKS", "content": "Figure 1 summarizes all the zero-shot versions of the synthetic data tasks showing that plot-based\nmethods outperform the text-based methods across GPT and Gemini model families, with few ex-\nceptions.\nFunctional form identification (id.)\nThis is the simplest task that requires only identifying one overall trend and correctly classifying it\ninto one of five functional tasks (linear, quadratic, cubic, exponential or periodic).\nCorrelation of two lines\nThis task now requires understanding the trends in two lines and comparing them against each other\nto correct identify whether the lines are positively or negatively correlated.\n2D cluster counting\nIn this task, the model needs to correctly identify the N number of clusters present in a set of points.\nDerivative identification\nThis is a harder task: the model must now identify the correct first derivative (out of four choices)\nof the function provided in the question. The function and choices are presented as either plots or\ntext. We pass various known functions to the model alongside four synthetic first derivatives, each\ncorresponding to different functional classes, and ask it to identify which of the multiple choices\ncorresponds to the true derivative.\nQuadratic derivative identification\nAs a hard variant of derivative identification, the model must now identify the correct linear function\n(out of four choices) that corresponds to the quadratic function in the question. Here the model must\npay attention to both the sign and magnitude of the slope.\nIn order to investigate the quadratic derivative task further, we also ran experiments providing few-\nshot examples with reasoning traces with results shown in Figure 2. Here we find that GPT40 text\nzero-shot remains an outlier in its strong performance, but for the other models plot outperforms\ntext, with few shots improving performance in the Gemini family for both plot and text, but reducing\nperformance in the GPT family of models for both plot and text."}, {"title": "4.2 REAL-WORLD TASKS", "content": "Fall detection from inertial measurement units (IMUs)\nAn IMU segment is a 6D-vector composed of 3-axes accelerometer signals and 3-axes gyroscope\nsignals. The first real-world task we evaluate is to classify whether a 15-second IMU segment\nrecorded at 128hz contains a fall, a \u201cnear\u201d fall or showed \"active daily living\u201d (ADL). The dataset\nused in the open-source IMU Fall Detection Dataset (IMUFD, Aziz et al. (2017)).\nFew-shot fall detection is a pattern-recognition task - typically a fall shows up on the IMU as a big\nspike in magnitude on multiple axes. What makes the task hard is the inclusion of the hard negative\nclass of \"Near\" falls, where the participants of the study pretend to trip but recover before actually\nfalling, creating similarly large changes in magnitude on the IMU.\nActivity recognition from IMUS\nA further real-world IMU task we evaluate is to classify whether a 15-second IMU segment is one\nof five activity classes: \"sit\", \"stand\u201d, \u201cstairs\u201d, \u201cwalk\" or \"bike\". The dataset used in the open-\nsource Heterogeneity Human Activity Recognition dataset (HHAR, Stisen et al. (2015)). As with\nFall Detection we test performance with 1, 3, 5 and 10 few-shot examples, with 383 samples per\nmodel and number of few-shots. For this task the text representation of the 10 few-shot examples\nexceeded the GPT4o and GPT4o-mini 128k context windows, so these results are only shown for\nthe Gemini models.\nActivity recognition requires evaluating the entire IMU segment and correlating signals between\ndifferent axes and sensors to determine the likely activity, as the noisy signals may only subtly\nchange between \u201csit\u201d and \u201cstand\" or \"walk\" and \"stairs\". The HHAR dataset was deliberately\ncollected to be heterogeneous containing data collected from four different types of smartphone and\ntwo different types of smartwatch. The classes in the dataset aren't balanced so performance is\nreported using an F\u2081 score.\nReadiness\nEstimating fitness readiness for a workout is a multicomponent\ntask that involves assessment of health metrics, sleep, training\nload, and subjective feedback (Cosentino et al., 2024). Among\nthose, training load analysis can be evaluated quantitatively\nand involves plot interpretation. Therefore, we framed the\ntask as a binary classification problem (training load trending\nupwards or downwards) and use the calculated acute-chronic\nworkload ratio (ACWR) to obtain ground truth labels.\nACWR is a ratio of acute training load (total training impulse,\nor TRIMP, over the past 7 days) divided by chronic training\nload (28-day average of acute load). An ACWR equal to 1.0\nmeans that the user has exercised at the same intensity con-\ntinuously over the past week compared to the month, below\n1.0 means that they are trending downward, and above 1.0\nmeans they are trending upwards. Precise ACWR calculation\ninvolves multiple mathematical operations, so we assess the\nmodel's ability to understand the trend from monthly TRIMP\nvalues without explicit calculation.\nWe use training load data from 350 fitness case studies\n(Cosentino et al., 2024) and present it as tables or TRIMP bar\nplots. Each case study contains data from 30 consecutive days.\nWe use a simplified version of the textual prompt and visual-\nization from Cosentino et al. (2024) and do not split TRIMP in\ndifferent heart rate zones. We tested Gemini 1.5 Pro and Gem-\nini 1.5 Flash for both plot and text approaches as zero-shot\ntasks. Since this task involved analyzing just 30 data points\nwe did not expect the plot prompt to excel here. Interestingly,\nmodels of different sizes showed opposite trends: Gemini 1.5 Pro had a slight increase in perfor-\nmance when using plots, while Gemini 1.5 Flash had a slight increase in performance when using\ntext, though the magnitudes of the gains were small.\""}, {"title": "4.3 ABLATIONS", "content": "We considered a variety of text and plot ablations to confirm if there were any large gains. All\nablations were performed on Gemini Pro 1.5. We used the function identification task to test for any\nperformance differences; details and results are reported in Section S.1.3."}, {"title": "4.4 COST", "content": "Using plots for time-series data can often be more cost-efficient and token-efficient. Token efficiency\nmatters when your context is large and the context-window is limited; for example we needed to\ndownsample our raw signals to fit them into the 128k context window for GPT40(-mini), particu-\nlarly with the large few-shot experiments (Section 4.2). For example, when using the Gemini API\n(Google, 2024), images account for 258 tokens if both dimensions are less than 384 x 384 pixels,\nafter which 4 additional crops are added for a total of 1290 tokens. Text tasks can easily be 10x\nlarger (e.g. 10-shot activity recognition Section 4.2) requiring more than the entire 128k context\navailable.\nPlot experiments also end up being cheaper. As an example, for our most expensive experiment\non few-shot activity recognition, we can estimate the input token cost of our 5-shot experiment for\nboth plot and text on GPT40 (OpenAI, 2024). The text version of this task required nearly 128k text\ntokens (costing $0.32 per 5-shot question); by contrast, the plot version required 50 images (costing\n$0.032), a 10x difference in overall costs for input tokens."}, {"title": "5 CONCLUSIONS AND FUTURE WORK", "content": "Our key finding is that engaging the vision encoder of a multimodal foundation model through\nthe use of plot representations leads to significant performance and efficiency gains on time-series\nunderstanding tasks, compared with relying on the text encoder. By processing data visually instead\nof textually, these models can better capture temporal patterns and relationships. We established our\nresults on synthetic data with well-controlled characteristics and reasoning types, and also showed\nthat this approach holds on real, noisy and complex tasks related to making sense of consumer health\nsignals.\nThe method presented here is powerful in its simplicity and generalizability we believe that it is\nparticularly useful when the following conditions are met:\n\u2022 You want to use an off-the-shelf multimodal model to interpret your time-series data.\n\u2022 Your use-case is not restricted to a specific task or modality, and generalizability across\ntasks is more important than accuracy on a single task. We showed that plots act as a gen-\neralizable time-series encoder across many tasks, even though they may not be better than\na task-specific encoder trained for one task. Training task-specific encoders for multimodal\nmodels can be limited by availability of paired training data, compute and expertise.\n\u2022 You don't want to downsample your data. In many cases the textual representation of real\ntime-series outstrips the maximum context length, and so the plot-based approach is the\nonly way to present the data without downsampling.\nOur focus in this work is specific to time-series understanding (i.e., reasoning about known data).\nForecasting is also important to time-series analysis and in the future we suspect that leveraging the\nvision components of multimodal models might yield positive results in this area too.\nAll plots in this work were generated by human-written code in order to avoid any bias. Look-\ning forward, in real applications we anticipate that plotting could be part of a tool-use framework,\nwhere the model is prompted to choose how and when to plot the data, after which it uses the plot\nrepresentation it created."}, {"title": "6 REPRODUCIBILITY STATEMENT", "content": "Our evaluations are run on publicly available models that have publicly available APIs. The exact\nmodel versions used are detailed in Section 3.2.\nOur structured prompting methods are detailed in 3.1 and we include the actual prompts and target\ndataclasses used in Supplementary Section S.1.4.\nAll the data for the synthetic tasks is by definition synthesized and the detailed task descriptions in\nSupplementary Section S.1.1.1 provide the necessary details to recreate these synthetic datasets.\nThe IMU Fall Detection Dataset (IMUFD, Aziz et al. (2017)) and Heterogeneity Human Activity\nRecognition dataset (HHAR, Stisen et al. (2015)) used respectively for the fall detection and ac-\ntivity recognition tasks are both publicly available. Pre- and post-processing steps are detailed in\nSupplementary Section S.1.1.2.\nThe dataset for the Readiness task is not currently publicly available. However the task details in\nSection 4.2 would enable reproduction of the results with access to a comparable dataset."}, {"title": "S.1.1.1 SYNTHETIC TASKS", "content": "Functional form identification\n\u2022 We generate (x, y) series of linear, quadratic, cubic, exponential and periodic functions\nwith variable number of points and noise (injected into the function domains) over the\nrange x \u2208 [-10, 10].\n\u2022 We perform five repeats across different numbers of points (50, 500, 1000 and 2500) and\nnoise levels (0.0, 0.5, 1.0, 2.0 and 5.0), giving 500 samples per model across number of\npoints, noise level, function type and random replica dimensions.\n\u2022 These results are then passed either as a stringified series of x and y vectors (\u201ctext\u201d task),\nor as a matplotlib figure (the \"plot\" task) to the model.\n\u2022 This task only requires that the model is able to understand and label the global longitudinal\ntrend of the function, without overly needing to reason about magnitudes.\nCorrelation of two lines\n\u2022 We generate (x, y\u2081) and (x, y2) series that represent linear functions y = mx with\nvariable pairs of slopes (m1, m2) \u2208 {(1,2), (\u22121, 1), (5, \u22121), (-2, -5), (-3, 2), (2, 3)}.\n\u2022 We perform trials across different numbers of points (50, 500, 1000 and 2500) and noise\nlevels (0, 0.25, 0.5, 1, 1.5, 2.0, 3.0 and 5.0) over the range x \u2208 [-10, 10], with 192 samples\nper model across number of points, noise level and random replica dimensions.\n\u2022 These results are then passed either as a stringified series of x, Y\u2081 and Y2 vectors (\"text\"\ntask), or as a matplotlib figure (the \"plot\" task) to the model. The model is then asked to\nclassify whether the two lines y\u2081(x) and y2(x) are positively or negatively correlated.\n\u2022 This task requires first understanding two global trends, and then comparing them with\neach other.\n2D cluster counting\n\u2022 We generate a series of points corresponding to N distinct clusters, parameterised by the\nstandard deviation from the cluster center which also controls the difficulty of the task.\n\u2022 The cluster centers are chosen randomly, and we enforce a minimum distance between\nclusters."}, {"title": "Derivative identification", "content": "\u2022 We generate (x, y) series of linear, quadratic, cubic, exponential and periodic functions\nand their derivatives y'(x) over the range x \u2208 [\u221210, 10].\n\u2022 We perform five repeats across different numbers of points (50, 500, 1000 and 2000) and\nnoise levels (0.0, 0.5, 1.0, 2.0 and 5.0) giving 500 samples per model across number of\npoints, noise level, function type and random replica dimensions.\n\u2022 There are four multiple choices per sample, and each choice is the derivative series y'(x)\nof a random selection of function types, with the same noise level and number of points as\nthe function in question.\n\u2022 These results are then passed either as a stringified series of x and y vectors (\u201ctext\u201d task),\nor as a matplotlib figure (the \"plot\" task) to the model.\n\u2022 This task represents a multi-step extension of the function identification task: here we re-\nquire the model first to understand a function, next reason about what the functional trend\nimplies about the characteristics of its derivative, and then finally identify those charac-\nteristics within the set of multiple choices. Beyond simply introducing a multi-reasoning\nrequirement, we also focus on derivative understanding as rates of change are key compo-\nnents of time-series analysis and understanding. Note that because the choices are different\nfunctional classes, the model can achieve good accuracy without reasoning about the func-\ntional magnitudes."}, {"title": "Quadratic derivative identification", "content": "\u2022 We generate (x, y) series of quadratics (of form y(x) = A\u00b7 x\u00b2) and their derivatives y' (x)\nover a range of scales A \u2208 {\u221210, -5, -1, 1, 5, 10} over the range x \u2208 [-10, 10].\n\u2022 We perform five repeats across different numbers of points (50, 500, 1000 and 2000) and\nnoise levels (0.0, 0.5, 1.0, 2.0 and 5.0), with 600 samples per model and number of few-\nshots across number of points, noise level, function type and random replica dimensions.\n\u2022 There are four multiple choices per sample, and each choice is a random se-\nlection of derivatives of quadratic functions with a range of scales A \u2208\n{-20, -15, -10, -5, -1, 1, 5, 10, 15, 20}, with the same noise level and number of points\nas the quadratic function in question.\n\u2022 We create few-shot examples in the same manner as the main task, with the quadratic\nfunctions being randomly sampled from a range of scales A \u2208 {\u221220, -15, -3, 3, 15, 20}\nwith 0 noise and 50 data points.\n\u2022 These results are then passed either as a stringified series of x and y vectors (\"text\" task),\nor as a matplotlib figure (the \"plot\" task) to the model.\n\u2022 This hard variant of the derivatives task introduces a new requirement for the model to\nreason about function magnitudes as all the possible choices are the correct functional form\n(i.e., linear)."}, {"title": "S.1.1.2 REAL-WORLD TASKS", "content": "Fall detection\nThe task is hard to define as zero-shot, since there isn't a natural way of explaining the plots and\ntext, so we frame this as a few-shot task. We test few-shot examples with 1, 3, 5 and 10 examples,\nwith 480 samples for each body location per model and number of few-shots. Model API errors\nwere ignored as long as at least one body location succeeded for that example.\nPre-processing and post-processing\nDue to context-window limitations for the text-only task, we apply a 1D average pool with a kernel\nsize of 10 and stride of 10 to the data. This allows us to use up to 10 few-shot examples in the\ntext-only tasks and still fit into the GPT4o and GPT40-mini 128k context window.\nThe dataset provides multiple IMUs from 7 body locations. We consider a subset of head, waist, left\nand right thigh. We sample IMUs from each as separate examples - when evaluating either text or\nplot approach, the final prediction is a majority vote from the predictions from each of these body\nparts.\nThe dataset is stratified into train (20%) and test (80%) based on participant ID. Few-shot examples\nare chosen from the \"train\" set while eval data is chosen from the \"test\" set. This ensures the model\nnever sees any data from the same participant.\nActivity detection\nPre-processing\nAs with the fall detection data, we apply a 1D average pool over the raw data to limit the number of\ntext tokens. As the raw IMU sample rates varied between 70-200Hz the kernel size, and matching\nstride, was chosen to target a downsampled frequency of 10Hz for each example. We select few-shot\nexamples using leave-one-out cross-validation at the dataset user level to maximise the number of\nexamples used for validation while ensuring we exclude any few-shot examples from the same user,\neven those from different device types.\nThe raw HHAR dataset consists of examples with varying durations and longer examples typically\ncontain multi-second gaps with no samples from the IMU. We chunk each raw example by splitting\non any gaps longer than 2 seconds and take a central 15 second crop of the longest chunk to create\nthe examples used in this study. Any examples with mismatching sample rates for the accelerometer\nand gyroscope are filtered out. The raw labels \u201cstairsup\u201d and \u201cstairsdown\u201d are coalesced into a\nsingle \"stairs\" class."}]}