{"title": "Explainable Malware Analysis: Concepts, Approaches and Challenges", "authors": ["Harikha Manthena", "Shaghayegh Shajarian", "Jeffrey Kimmell", "Mahmoud Abdelsalam", "Sajad Khorsandroo", "Maanak Gupta"], "abstract": "Machine learning (ML) has seen exponential growth in recent years, finding applications in various domains such as finance, medicine, and cybersecurity. Malware remains a significant threat to modern computing, frequently used by attackers to compromise systems. While numerous machine learning-based approaches for malware detection achieve high performance, they often lack transparency and fail to explain their predictions. This is a critical drawback in malware analysis, where understanding the rationale behind detections is essential for security analysts to verify and disseminate information. Explainable AI (XAI) addresses this issue by maintaining high accuracy while producing models that provide clear, understandable explanations for their decisions. In this survey, we comprehensively review the current state-of-the-art ML-based malware detection techniques and popular XAI approaches. Additionally, we discuss research implementations and the challenges of explainable malware analysis. This theoretical survey serves as an entry point for researchers interested in XAI applications in malware detection. By analyzing recent advancements in explainable malware analysis, we offer a broad overview of the progress in this field, positioning our work as the first to extensively cover XAI methods for malware classification and detection.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's digital era, malware poses a formidable threat, causing significant damage to computer systems and resulting in billions in financial losses. This pervasive issue has economic impacts and restricts users' access to essential services worldwide. Especially with the rise of sophisticated malware such as zero-day malware, traditional detection methods are becoming increasingly inadequate. Hence, more advanced and automated malware detection solutions should be considered.\nRecently, Machine Learning (ML) and Deep Learning (DL) techniques has shown the ability to detect known and zero-day malware. However, to be able to detect malware with high accuracy, traditional ML models tend to become very sophisticated and large, making prediction interpretability a difficult task. Similarly, DL models are inherently sophisticated (often labeled as black-box models) as their internal decision-making processes are not easily interpretable. This opaqueness can be a significant issue, especially when the rationale behind predictions remains unclear [96].\nThis lack of interpretability is particularly concerning for cybersecurity professionals and end-users who rely on these ML-based malware detectors for protection since they require clear explanations behind the predictions. Without explainability, it is challenging to trust the performance of these systems to identify and correct errors in detection, analyze emerging or zero-day threats, and ensure fairness and regulatory compliance. To address this, explainable AI (XAI) has seen a growing interest. It provides a method for making decisions or predictions that are accurate and transparent and offers users a clear and comprehensible explanation of the reasoning behind the conclusions [100].\nSeveral survey papers [32], [124], [51], [121], [21] have focused on XAI and conducted comprehensive reviews on the research areas, methods, and opportunities of explainable ML models, offering both mathematical and visual explanations. In research done by Danilevsky et al. [31], they focus on a survey of XAI in NLP and its evaluation in that area. Another study [84] presents a framework for the evaluation of this approach along with a systematic survey on XAI. Another research [66] centers on knowledge-driven and data-driven methods in X\u0391\u0399, whereas [29] discusses the historical development of XAI and its application in expert systems. Speith [110] offers an overview of the general taxonomy of XAI approaches and the challenges in this field and extends the discussion to include challenges faced by researchers.\nSaeed and Omlin [97] conduct a systematic literature review related to challenges and research directions in the \u03a7\u0391\u0399 field. It provides information regarding general challenges on XAI, and then it focuses on challenges and potential research directions based on the phases of the ML lifecycle-design, development, and deployment.\nThe systematic review by Saranya and Subhashini [100] provides a thorough overview of XAI developments in various fields such as agriculture, social media, computer vision, and healthcare. This review also discusses the challenges in these areas. Milani et al. [82] provide a survey on explainable reinforcement learning, which is used to show the decision-making process for reinforcement learning agents. They overview the literature on explainable reinforcement learning and show the future directions in this scope. Nasser and Nasser [87] provide a survey of the application of hardware capabilities and ML to enhance malware detection. They explore various hardware architectures combined with ML models for dynamic and efficient malware identification. The paper also discusses the potential future developments in hardware-assisted XA\u0399 for malware detection."}, {"title": "II. MALWARE DETECTION APPROACHES", "content": "Malware detection techniques are used to address the threat posed by malware. They are generally categorized into two distinct approaches: File classification and Online-based approaches. The field has seen considerable research efforts, with numerous studies and developments aimed at enhancing the efficacy and reliability of these malware detection methodologies.\nFile classification focuses on the analysis of a file's code to determine whether it is malware. The process begins with the identification of a potentially suspicious file. To thoroughly assess its nature, file classification employs diverse methods, which fall into three main categories: Static analysis, Dynamic analysis, and Hybrid analysis. Static analysis involves examining the file's code without executing it and looking for malicious patterns. In contrast, in dynamic analysis, the file is executed in a secured environment to observe and analyze its behavior. Hybrid analysis combines these two approaches, leveraging the strengths of both static and dynamic examinations. Once a file is concluded to be non-malicious, it is generally exempt from ongoing scrutiny. These varying techniques in file classification are designed to address different aspects of malware detection.\nStatic analysis involves the careful examination of an executable's signature without the need to execute the code, aiming to classify the file as malware if the signature appears malicious or as benign if otherwise [30]. This method has the reverse engineering of malware code and involves the detailed processing of extracted features to discern and interpret any malicious activities through a signature-based approach. In this context, a signature refers to a unique identifier for a binary file, determined by calculating its cryptographic hash. Extensive research, notably by Hou et al. [48] and Kim and Lee [53], has been dedicated to enhancing static malware detection, with a particular focus on the extraction of Application programming interfaces (API) calls from Portable Executable (PE) files using techniques like stacked autoencoders. This process involves extracting vital features such as API calls, Opcode sequences, and N-Grams from potentially suspicious files, which are then employed to train ML algorithms for more accurate and efficient malware detection. For instance, the work of Shankarapani et al. [103] has been using API and Opcode sequences to effectively identify segments of code that closely resemble known malware patterns. However, it is important to recognize that static analysis, while valuable, is not without its limitations. One significant challenge is its inability to detect malware that is actively running within a system or to identify completely new malware variants that have not been cataloged.\nDynamic Analysis: It entails executing malware within a secure virtual environment, such as a cuckoo sandbox, to study its behavior meticulously [30] [52]. This method is particularly effective in addressing zero-day malware threats. The dynamic analysis process starts with executing a suspicious PE file in a sandbox environment, ensuring isolation from external systems. This controlled execution allows for the collection of essential data, including memory features, system calls, and function calls. Subsequently, these collected data are preprocessed and used to train various ML-based algorithms, which can enhance the malware detection model. Unlike static analysis, dynamic malware analysis requires the execution of code in a time-restricted, closed environment, which can be resource-intensive. Research endeavors, as demonstrated by Firdausi et al. [38] and Luckett et al. [72], have utilized system calls as key features for training traditional ML models like k-Nearest Neighbor (K- NN), Decision Tree, Support Vector Machine (SVM), and Naive Bayes. Furthermore, studies by Pirscoveanu et al. [91], and Tobiyama et al. [112] have focused on the extraction of features from API calls, evaluating the effectiveness of ML algorithms, including Random Forest, k-NN, and Convolutional Neural Networks (CNN) in dynamic malware analysis. These approaches highlight the dynamic method's capacity for dealing with complex malware detection challenges, although it requires significant time and resource allocation.\nA methodology integrating static and dynamic techniques, hybrid analysis, is another malware detection technique [114]. This concept has been explored in various studies. For example, Santoso et al. [99] utilized a combination of Artificial Neural Networks (ANN) and CNN for malware detection, yielding accuracies of 99.4% and 97%, respectively. Focusing on Android malware, Zhu et al.[130] proposed an innovative framework using the Merged Sparse Autoencoder (MSAE), which is an unsupervised learning algorithm demonstrating its effectiveness. Adding to this, Tong and Yan [113] developed a method that combines static and dynamic analysis for mobile malware detection. This method involves comparing system call patterns of benign and malicious applications with the dynamic analysis applied to unknown applications. Subsequent offline comparison of these pattern sets further validates the unknown application's nature. Their results show the advantages of the hybrid approach over methods relying solely on static or dynamic analysis. Altaher and Barukab [7] also proposed a hybrid methodology for Android malware detection that leverages API calls and application permissions, further substantiating the potential of hybrid techniques in this field."}, {"title": "B. Online Malware Detection", "content": "Online malware detection stands out as a distinct approach in the cybersecurity domain. Unlike static, dynamic, or hybrid methods that analyze specific malware samples, online detection monitors the entire system in real-time, which enables the capture of malware at any moment, regardless of its activity level. This technique focuses on the behavior of the entire machine rather than individual malware behaviors.\nKey contributions in this area include the work of Watson et al. [120], who developed a system using performance metrics to build SVM, achieving a 90% accuracy rate. Azmandian et al. [15] proposed intrusion-based detection techniques, while Abdelsalam et al. [2] introduced a sequential k-means clustering algorithm for anomaly detection, specifically designed for a standard 3-tier architecture on an OpenStack testbed. Their approach, notably effective for high-profile malware, leverages virtual machine systems and resource utilization features but shows limitations in detecting low-resource-utilization malware. Further research was done by McDole et al. [78], who examined various CNN models to determine their suitability for malware detection in cloud Infrastructure as a Service (IaaS). Their subsequent study [79] compared the process-level performance metrics of different deep learning models in the context of online malware detection in cloud IaaS environments. Similarly, Kimmel et al. [55], [54] presented a comprehensive analysis of the effectiveness of several ML models for online malware detection, focusing on system features describing processes in a virtual machine. They emphasized the use of CNNs, which are known for their simplicity and effective representation in 2D format. Abdelsalam et al. [1] extended this concept by employing a 3-dimensional CNN to enhance classifier accuracy and specifically target low-profile malware, achieving an accuracy rate of 90%."}, {"title": "III. EXPLAINABILITY IN MACHINE LEARNING", "content": "In recent years, the fields of ML and AI have gained significant attention due to their potential in addressing complex real-world problems. These technologies, particularly ML models, have demonstrated a remarkable ability to make accurate predictions based on training data. However, an important challenge arises from the inherent complexity of these models, often making them difficult to interpret. This complexity frequently results in what is termed black-box models, where the internal workings and decision-making processes remain opaque and not easily understandable [46].\nDL-based models enable machines to develop complex hierarchical data patterns, which play a key role in tasks like classification or detection. These models, by layering and integrating various levels of data representation, can enhance the predictive power of systems. However, this increased complexity often obscures the internal decision-making process, which can lead to questions about their decision logic [69], [11]. In contrast, white-box models offer a more transparent approach. They are designed to be easily interpretable, which allows users to understand how input data is transformed into predictions or decisions. This transparency is particularly valuable in fields where understanding the reasoning behind a decision is as important as the decision itself [47], [35].\nFor instance, in the context of cancer diagnosis, medical professionals often rely on predictive models. While these models are useful tools, there is always a possibility of incorrect predictions. Therefore, both practitioners and patients have to trust these models, which becomes possible in the situation that they understand the underlying reasons for their predictions.\nThis is where the concept of XAI comes into the picture. As depicted in Figure 3, today's AI systems typically involve training the data, undergoing the machine learning process, and providing prediction for end-users. In contrast, XAI goes a step further. It can deliver high-accuracy predictions and provides clear, justifiable explanations for these outcomes. This transparency in AI decision-making processes enhances user trust. With higher interpretability, the reasons behind AI predictions become more comprehensible to humans, which boosts the trustworthiness and reliability of the model's predictions. In the comprehensive study, Blanco-Justicia and Domingo-Ferrer. [18] discussed the characteristics that define XAI for enhancing transparency and efficacy in AI systems as follows:\nAccuracy. This aspect evaluates how well an XAI model predicts outcomes for new, unseen data. Predictions made by these models must have a high level of accuracy.\nFidelity. It is about the closeness of the explanation to the model's prediction. An explanation is regarded as highly accurate when it meets the high fidelity and high accuracy of the black-box model.\nConsistency. This characteristic describes how equally explanations are applied to a model that is trained on the same dataset.\nStability. It examines whether the stability is reflected in the explanation model, which means that similar instances should produce similar explanations.\nDegree of Importance. This attribute indicates how well the explanation reflects the significance of various features within the model, which is essential for understanding the weight of different aspects in the model's decision-making process.\nNovelty. Closely related to stability, novelty assesses the ability of the explanation mechanism to accurately represent data instances that are significantly different from those in the training set.\nRepresentativeness. This factor has a significant effect on explainability, emphasizing the need for explanations to be relevant and applicable in a diverse range of decision-making scenarios, thereby ensuring their utility across various applications.\nHence, in the context of XAI, it is important to understand the general classification of ML-based models. This figure presents a comprehensive taxonomy of ML models and XAI techniques and provides a clear framework for understanding this field.\nML models fall into two primary categories: Transparent and Opaque. Transparent models are inherently explainable. These models are straightforward enough that they do not require additional post-hoc explainability techniques, i.e., techniques provide explanations only after the training process has finished. However, when the processes within these models become more complex, there arises a need for post-hoc explainability to make their functioning clear.\nOn the other hand, Opaque models, often referred to as black-box models, are characterized by their high accuracy yet present significant challenges in interpretation. Due to their complexity, they require the use of post-hoc explainability methods. The goal of post-hoc explainability is to make the outcomes of ML models more transparent, understandable, and trustworthy to humans. Post-hoc explainability can be further divided into two types: model-agnostic and model-specific methods. Model-agnostic methods have a variety of explainability techniques and are versatile enough to be applied to any black-box model. These methods are compatible with a wide range of ML models and offer flexibility in interpretation. In contrast, model-specific methods are applicable only to certain types of models and limit their utility to specific cases.\nThe subsequent section of this paper will outline the various ML models and post-hoc explainability techniques, providing a comprehensive summary of different research challenges encountered in this evolving field."}, {"title": "A. Transparent Machine-Learning Models", "content": "Transparent models are distinguished by their inherent ability to be self-explanatory. They can be interpreted directly, enabling users to comprehend their decision-making processes. This category of models, from Rule-based Learners and Regression Models to Decision Trees, Bayesian Models, k-NN algorithms, and the Generalized Additive Model (GAM), are unified by their transparent nature.\nRule-Based Models: These models are characterized by developing rules to represent and interpret the data they are designed to learn from. At the core of these models is the IF- THEN statement, a basic but powerful structure that forms the foundation of these rules. The IF part represents the condition, while the THEN part denotes the prediction. These predictions can arise from a single rule or a synergy of multiple rules. Soares et al. [108] apply this concept to explain DL-based models. They propose a method where a deep reinforcement learning model is approximated through a series of IF-THEN rules, effectively enhancing the model's interpretability.\nThe intuitive clarity of rule-based models makes them highly interpretable and understandable. Their straightforward structure eliminates the need for post-hoc analysis, which is often required for more complex models. Furthermore, a rule wrapper in these models encapsulates key information, making it accessible to a non-expert audience and enabling it to operate effectively as a standalone prediction model. The transparency in rule-based models extends their utility beyond their standalone capabilities. They are also used to clarify the predictions of more intricate models by generating and applying rules to link sophisticated ML techniques with approachable interpretability.\nRegression Models: Linear and logistic regression models stand as two important regression models. Linear Regression is often considered the foundational regression technique, distinguished by its straightforward nature and high linearity that predicts outcomes as a direct sum of input features [106]. The weight of the coefficient of linear regression is easy to quantify and interpret, which is why it is used in various fields to explain the predictions. Its simplicity in quantifying and interpreting the weight of coefficients makes Linear Regression a popular choice across various fields. These models are resistant to overfitting, though they face challenges in accurately representing feature relationships and are significantly influenced by outliers on regression boundaries and results.\nOn the other hand, logistic regression, a derivative of linear regression, serves as a classification model. It advances the basic regression approach by calculating probabilities for classification tasks and assigning weights to features that culminate in a definitive value range between 0 and 1.\nThis model's strength lies in its ability to provide probabilities alongside classifications which offers a nuanced view of outcomes. Lundberg's research [73] further enhances this model's utility by integrating logistic regression with gradient- boosted trees for predicting synthetic labels and augmenting the explainability of tree-based models. Despite their transparency, these regression models often require additional post-hoc explainability tools, like visual aids, to make their predictions accessible to those not well-versed in statistical methodologies.\nDecision Trees: Decision trees are structured hierarchically, segmenting data into various subsets based on distinct features. The terminal subsets, known as leaf nodes, play a crucial role in resolving classification and regression challenges. These trees offer transparent models that enable domain experts to understand how they work. Furthermore, the exploration of these trees can lead to the discovery of new relationships and insights. Blanco-Justicia and Domingo-Ferrer [18] leverage decision trees as surrogate models to elucidate black-box models, constructing these trees from segmented portions of the training dataset. This approach assumes that the person responsible for providing explanations has access to the training data and the black-box model. Nevertheless, decision trees encounter scalability issues with large datasets in real-world applications, which diminish their explainability as the tree complexity increases. This complexity requires the adoption of post-hoc explainability methods to maintain clarity.\nBayesian Models: Bayesian models excel in providing a high degree of interpretability and explainability, offering nuanced insights into the statistical interplay between variables. This capability makes them particularly adept for applications where clear, comprehensible explanations are essential, such as demonstrating the correlation between diseases and their symptoms. Hence, In the realm of medical research, the application of Bayesian methods has been notably effective. For instance, Arrieta et al. [11] demonstrate the utility of Bayesian approaches in healthcare analytics, highlighting their potential to the complex relationships within medical data. Similarly, the Naive Bayes classifier, as discussed by Rana et al. [93], serves as a robust algorithm for predictive modeling. This classifier efficiently tackles both binary and multi-classification problems by calculating the probabilities of individual elements, subsequently employing Bayes' theorem to identify the most probable outcome. Moreover, to enhance explainability, Ren et al. [94] introduce an approach that integrates a neural network with an explainable Bayesian network. This novel method yields more accurate predictions and provides intuitive explanations for diagnoses.\nThe KNN algorithm is a paradigm of transparent, interpretable models utilized for both classification and regression tasks. At its core, KNN operates on a simple yet effective principle: for classification, it determines a test sample's class based on the majority vote from its nearest neighbors, and for regression, it computes the average outcome of these neighbors. The selection of an appropriate K-the number of neighbors to consider- emerges as a critical factor, which can influence the model's ability to accurately assess the proximity between data points and thus define the neighborhood essential for prediction.\nThe interpretability of KNN is significantly influenced by the chosen features, the distance metric utilized, and the number of neighbors. While models with extensive features may obscure interpretability, a KNN model characterized by a concise, well-selected feature set remains one of the main models for interpretable outcomes. This duality highlights KNN's utility, especially in applications where the model's transparency is important. In a study, Aslam et al. [13] showcase the application of various supervised ML-based models, including KNN, utilizing XAI techniques. These models provide crucial, timely medical insights within a fetal health monitoring system, illustrating KNN's adaptability and its capacity for clear, interpretable insights, even in complex domains.\nThe Generalized Additive Model (GAM) represents a notable advancement in statistical modeling, combining the benefits of linearity and interpretability. This model assigns values to variables by integrating numerous undefined functions specific to regression models and enhancing accuracy without compromising on interpretability. One of the unique features of GAM is its ability to allow users to evaluate the significance of each variable by examining its impact on the predicted outcome. Notably, GAM models exhibit algorithmic transparency and are regarded as simulatable due to their minimal dimensionality issues. Furthermore, the Generalized Linear Model (GLM) broadens the scope of linear regression by being interpretable, preserving the feature weights combination, and capturing non-Gaussian relationships and dependencies.\nTo have an optimal balance between accuracy and explainability, Yang et al. [127] introduce GAMI-Net (Generalized Additive Models with Structured Interactions), a neural network characterized by its intrinsic explainability. This model has been benchmarked against several standard models, including GLM, showcasing its robustness.\nDespite the inherent transparency and explainability of such models, there is an ongoing exploration of undirected graphical models to enhance the model's trustworthiness further. It is recognized that transparency alone may not suffice to ensure straightforward explainability. As models evolve, certain complexities may emerge, rendering them less interpretable. This complexity necessitates the development of post-hoc explanations to maintain the clarity and reliability of the models."}, {"title": "B. Opaque ML-based Models", "content": "We explored models characterized by their transparency, highlighting that their interpretability does not guarantee enhanced performance. This section shifts focus to examine complex models that stand out for their high accuracy. However, these models require post-hoc explanations to unlock an understanding of their internal processes.\nRandom Forest (RF): RFs are composed of numerous individual decision trees, each of which contributes to the model's predictions by dividing the input space into smaller segments and averaging the outcomes across these segments. As the complexity of the problem increases, the number of decision trees within the forest grows exponentially, enhancing the model's ability to address complex issues but simultaneously diminishing its explainability. RFs were developed to mitigate the overfitting issues associated with single decision trees and to improve their accuracy, with the overall prediction being the mean of the outputs from numerous trees. This strategy of combining multiple trees aims to reduce the variance of the final model. Each tree is trained on a unique subset of the dataset, allowing for an aggregated prediction that incorporates varied insights. However, the complexity inherent in the RF model complicates direct interpretation, leading to the necessity for post-hoc explainability techniques to unravel the decision-making process. In their study, Zhao et al. [129] propose a visual analytic system that explores various interpretive perspectives for analyzing and explaining the predictions generated by RFs, offering a comprehensive approach to understanding these models.\nSupport Vector Machine (SVM): A Support Vector Machine (SVM) is fundamentally based on geometrical principles, initially conceived for linear classification and subsequently extended to non-linear scenarios. In essence, an SVM constructs a hyperplane or a set of hyperplanes within a high or infinite-dimensional space, serving purposes across classification, regression, outlier detection, and even clustering tasks. A hyperplane achieves optimal separation when it maximizes the distance to the nearest point of the training dataset, as a larger margin correlates with a lower generalization error of the classifier. Owing to their remarkable predictive and generalization capabilities, SVMs are among the most prevalently utilized ML models. However, due to their complex dimensionality, they are often regarded as opaque, making their decision-making process less transparent. In their research, Vieira and Digiampietri [116] explore the use of decision trees to derive rules from SVMs, which provides explanations for the classifications made by SVM classifiers and enhances their interpretability.\nMulti-Layer Neural Network: Numerous ML methodologies, including neural networks, have been specifically designed for DL applications. These models are computationally intensive but provide unparalleled performance across a wide range of applications. Neural networks are inherently considered black-box models due to their complex internal mechanisms, which require post-hoc explainability efforts to elucidate their decision-making processes. In the work referenced by Sharma et al. [104], the focus is on utilizing a multi-layer perceptron neural network for the risk prediction of default loans, with the explanation of model decisions facilitated through a sensitivity analysis technique.\nA wide range of explainability techniques has been developed specifically for neural networks to enhance the transparency of their operations. In the following sections, we will delve into these techniques, providing a clear understanding of the methods used to interpret the complex functions of neural networks."}, {"title": "C. Model-Agnostic Techniques for Post-Hoc Explainability", "content": "In the upcoming section, we will explore various methods that necessitate post-hoc explainability. This need arises when an ML model lacks transparency and its decision-making process is not directly interpretable. To show the decision processes of these models, a separate post-hoc technique is applied. In other words, these post-hoc methods are designed to provide users with clear, understandable information about the predictions made by the model. Hence, they are particularly valuable for interpreting models that are considered black box models. The domain of model agnostic interpretability is divided into three main categories: Global Explanation, Local Explanation, and Visual Explanation. Global explanations aim to elucidate the model's overall predictive behavior, Local explanations focus on clarifying individual predictions, and visual explanations seek to create easily understandable visual representations of the model's functions.\nGlobal Explanation: It refers to the process of achieving explainability at a comprehensive level, where the focus is on understanding the features that significantly influence the predictions of specific models. In scenarios where the prediction process is not inherently interpretable, a global surrogate model is employed. This model aims to replicate the predictions of the complex black-box model using a more interpretable ML-based approach. It is often termed the approximate or meta-model; the global surrogate model is a simpler stand-in for the original complex model. By analyzing this surrogate model, insights can be drawn about the underlying mechanisms of the black-box model. For instance, Islam et al. [51] demonstrated this concept by using the CART (Classification and Regression Trees) to approximate a random forest model's behavior.\nThe surrogate model's versatility is a key advantage. It allows for the evaluation of performance that closely aligns with the original, complex model. If the surrogate achieves similar accuracy, it can potentially eliminate the necessity for the original black-box model. Moreover, it is possible to develop several surrogate models for a single black-box model, each providing unique insights and interpretations. This method is efficient in its implementation and facilitates the explanation of sophisticated model dynamics in a clear and understandable manner.\nLocal Explanation: The scope of interpretability includes explaining the reasons for individual predictions of why a model made a specific decision in a given instance. This section will discuss local interpretability methods such as LIME (Local Interpretable Model-agnostic Explanations), KernelSHAP (SHapley Additive exPlanations), Shapley values, counterfactual explanations, and Logic Explained Networks (LENS).\nLIME. For the first time, Ribeiro et al. [95] introduced a novel local explainability technique known as LIME. This method operates as a local surrogate model, generating interpretable predictions by approximating how the model behaves in the vicinity of a given prediction. It is designed to be model-agnostic, which makes it versatile across different ML models. Specifically, LIME focuses on elucidating the rationale behind individual predictions made by opaque, black-box models.\nTo evaluate the effectiveness of the surrogate model, LIME employs a local fidelity measure. This metric assesses the extent to which LIME's approximations reflect the true behavior and accuracy of the underlying black-box model. However, it is important to note that LIME is not equipped to offer insights into the global operations of a model. Furthermore, if the local fidelity measure indicates poor accuracy, the reliability of LIME's interpretability may be compromised.\nIn a practical application of LIME, Magesh et al. [75] utilizes this technique to interpret the predictions of a CNN model designed for the early detection of Parkinson's disease. This study demonstrates the potential of LIME to provide valuable insights into real-world scenarios.\nKernalSHAP. Among the various local interpretability methods developed, a significant challenge lies in determining the most suitable method for specific scenarios. To address this, Lundberg and Lee [74] proposed Shapley Additive exPlanations (SHAP), a concept derived from game theory that evaluates the importance of each feature in contributing to a particular prediction. The SHAP framework establishes a new class of additive feature importance measures characterized by a unique solution that exhibits desirable attributes.\nKernelSHAP, as part of the SHAP family, is model-agnostic, allowing its application across diverse ML models. The computation of exact SHAP values via KernelSHAP can be exponentially time-consuming, which highlights its computational demands. Despite this, its capability to adapt to any ML model shows its broad utility. The SHAP framework also includes tailored variants such as TreeSHAP and DeepSHAP, designed specifically for tree-based and deep learning models, respectively. These variants can optimize the efficiency and relevance of SHAP analysis in targeted model types.\nShapley Values. Shapley values originate from coalitional game theory, framing each feature value of an instance as a \"player\" and the prediction outcome as the \"payout.\" This approach assigns a quantifiable contribution to each feature, which can demonstrate how significantly each one influences the final prediction. Shapley values are distinguished by key principles such as consistency and local accuracy. These principles ensure that the allocation of importance to features is both fair and interpretable, accurately reflecting each feature's contribution to the outcome.\nCounterfactual Explanations. Counterfactual explanations provide a compelling approach for local interpretation. This method stands out for its simplicity in implementation, as it does not need access to the underlying data or model. Counterfactual explanations focus on identifying which features would need alteration to achieve a specific desired outcome, thereby elucidating the reasoning behind model predictions. These explanations are particularly user-friendly because they illustrate how minimal changes in features can influence predictions. Nonetheless, one limitation of this method is its difficulty in accommodating categorical data across different levels. Related to the Counterfactual explanation, Molnar [85] discusses their application in models generating continuous predictions that showcase their utility in providing clear and actionable insights.\nLENS LENs enhance the interpretability of neural networks by utilizing human-understandable predicates as inputs and translating predictions into First-Order Logic (FOL) explanations. These networks are highly adaptable and can function effectively in both supervised and unsupervised learning contexts. LENs can serve as direct classifiers, providing explanations for their predictions, or they can work alongside black-box classifiers to make their decisions interpretable.\nThe learning process for LENs involves associating specific input features with output classes in supervised scenarios and generating logic rules that explain the conditions for predictions. In unsupervised learning, LENs identify patterns and relationships within the data, clustering similar data points and generating explanations that describe these clusters. Additionally, LENs can mimic the outputs of black-box models while generating FOL explanations, which leads to elucidating the decision-making process for these complex models [28].\nVisual Explanation: This approach contains methods designed to produce visual representations of models that make them accessible and comprehensible. Techniques such as Individual Conditional Expectation (ICE), Partial Dependence Plot (PDP), and Accumulated Local Effects (ALE) serve as key tools in this visualization process. These techniques facilitate a deeper understanding of how models operate by graphically depicting the relationship between features and the model's predictions. The advantage of visual explanations lies in their ability to convey complex model dynamics in a manner that is easily graspable. This makes visualizing techniques invaluable for broadening the accessibility of model interpretations.\nPartial Dependence Plot (PDP). It offers insights into the marginal impact of one or two features on the predicted outcome of an ML model, as highlighted by Molnar [85]. This tool is important in determining whether the relationship between the target and features is linear or exhibits more complexity. For instance, in the context of a linear regression model, PDP can reveal a linear relationship and illustrate how variations in a specific feature correlate with changes in the prediction. Unlike methods that focus on the influence of features on individual predictions, PDP emphasizes the average effect of features on the model's overall behavior. However, its application is generally constrained to analyzing up to two features simultaneously, based on the assumption that the selected features are independent of others not included in the plot.\nIndividual Conditional Expectation (ICE). Within the post-hoc explainability, visual explanations, particularly those compatible with model-agnostic approaches, are notably rare. The ICE plot, introduced by Goldstein et al. [42], emerges as a visualization technique for delineating the predicted outcomes of models governed by supervised learning algorithms. Diverging from the PDP, the ICE plot underscores the dependency of predictions on a specific feature across individual instances, each represented by a unique line. This approach allows users to understand how changes in a feature impact predictions on a case-by-case basis.\nICE plots especially highlight the variability of predictions within the range of a given covariate, identifying areas of significant heterogeneity. This capability is complemented by a visual test for assessing the model that generated the data alongside a comprehensive suite of tools for exploratory analysis. By employing both simulated examples and real-world data sets, the creators of ICE plots demonstrate their utility in uncovering insights about estimated models that PDPs may not reveal, offering a more granular perspective on model behavior.\nThe ALE plot provides a visual illustration of how individual features influence the predictions made by a machine learning model. It effectively showcases the dynamics between regressors (independent variables) and the dependent variable, offering insights into their relationship. Notably, ALE plots are recognized for their efficiency, being faster to generate compared to PDP.\nKramer et al. [58] demonstrated the application of ALE plots within the realm of real estate, employing them to discern which features significantly impact property values. This use case underscores the utility of ALE plots in practical, real-world analysis. Additionally, many researchers have adopted ALE plots as a method for visually exploring the nature of relationships between variables, assessing whether these relationships are linear or exhibit more complexity. This visual representation technique thus serves as a powerful tool for understanding and interpreting the effects of features on model predictions."}, {"title": "D. Model-Specific Techniques for Post-Hoc Explainability", "content": "Model-specific methods of post-hoc explainability are designed to be applied exclusively to certain types of models. These techniques can also be categorized based on their scope of interpretability, which includes local, global, and visual dimensions. Local scope refers to methods that focus on explaining the prediction for an individual data point. In contrast, global scope encompasses techniques that interpret the overall behavior of the model. Meanwhile, visual scope techniques are aimed at creating visual representations that make model behaviors comprehensible.\nAmong the array of model-specific approaches, TreeShap and DeepSHAP are notable for their application to tree-based and deep learning models, respectively. Additionally, saliency maps encompass a variety of methods, such as DeepLift, layer- wise relevance propagation, Grad-CAM, and other gradient-based approaches, along with feature relevance explanations.\nTreeSHAP and DeepSHAP represent two specialized implementations of SHAP grounded in the principles of Shapley values. TreeSHAP is tailored for tree-based models, offering a more efficient computation of exact SHAP values by operating in polynomial time, in contrast to the exponential time typically required by the general SHAP approach. In an illustrative application, Athanasiou et al. [14"}]}