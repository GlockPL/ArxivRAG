{"title": "Generalized Flow Matching for Transition Dynamics Modeling", "authors": ["Haibo Wang", "Yuxuan Qiu", "Yanze Wang", "Rob Brekelmans", "Yuanqi Du"], "abstract": "Simulating transition dynamics between metastable states is a fundamental challenge in dynamical systems and stochastic processes with wide real-world applications in understanding protein folding, chemical reactions and neural activities. However, the computational challenge often lies on sampling exponentially many paths in which only a small fraction ends in the target metastable state due to existence of high energy barriers. To amortize the cost, we propose a data-driven approach to warm-up the simulation by learning nonlinear interpolations from local dynamics. Specifically, we infer a potential energy function from local dynamics data. To find plausible paths between two metastable states, we formulate a generalized flow matching framework that learns a vector field to sample propable paths between the two marginal densities under the learned energy function. Furthermore, we iteratively refine the model by assigning importance weights to the sampled paths and buffering more likely paths for training. We validate the effectiveness of the proposed method to sample probable paths on both synthetic and real-world molecular systems.", "sections": [{"title": "1 Introduction", "content": "Transition dynamics simulation aims to sample transition paths between two metastable states, which is a fundamental challenge in dynamical systems, stochastic processes, and molecular simulations, with broad applications Bolhuis et al. (2002); Vanden-Eijnden et al. (2010); Aranganathan et al. (2024); Pattanaik et al. (2020); Duan et al. (2023, 2024). The key computational obstacle lies in the rarity of transition events such that it requires long-run molecular dynamics (MD) simulations to go over high energy barriers. In addition, there exists an infinite amount of paths that do not end in the target state.\nTo address these limitations, early work leverage Markov chain Monte Carlo (MCMC) approaches to mix the path distributions Dellago et al. (2002). However, MCMC-based simulation in such high-dimensional spaces suffer from slow mixing time. Alternatively, later work formulate it as a path integral control problem such that an external control is learned to guide the stochastic process for certain terminal conditions (i.e. arrive at the target in finite time ) Holdijk et al. (2024); Yan et al. (2022); Das et al. (2021); Rose et al. (2021). Nevertheless, the path integral control method is known as a shooting method with high variance (i.e. the probability to hit the target is very low). Recently, a variational formulation of learning Doob's h-transform is proposed, which leads to a collocation method that optimizes a family of tractable probability paths Du et al. (2024).\nNevertheless, all previous work require an extensive amount of expensive energy evaluations to sample from the path distributions. In this paper, we study the possibility to find a low-cost approximation of the transition paths distribution. In many studies of rare events in molecular systems, local"}, {"title": "2 Background", "content": "2.1 Score & Flow Matching\nScore-based generative models or diffusion models Song et al. (2021); Ho et al. (2020) build a generative process from tractable prior distribution pr(x) (e.g. Gaussian) to complex data distribution Po(x) \u2248 Pdata(x) where x \u2208 Rd. It can be learned by reversing a forward stochastic process from po(x) to pr(x), as follows:"}, {"title": "2.2 Generalized Schr\u00f6dinger Bridge Matching", "content": "One key ingredient behind the success of score-based generative and flow matching models is the simulation-free forward process (diffusion paths or Gaussian paths) that can be evaluated analytically. However, there is a broader class of problems with nonlinear drift or constraint that require simulation of the forward process. The generalized Schr\u00f6dinger bridge problem is more general such that in addition to learn a stochastic process with minimal kinetic energy that connects po(x) and pr(x), it further minimizes the potential energy V(\u00b7) along the path as Liu et al. (2024):"}, {"title": "3 Generalized Flow Matching for Transition Dynamics Modeling", "content": "3.1 Short-Run Molecular Dynamics\nMolecular dynamics simulation follows Newton's equation of motion such that M\u00eft = -\u2207xU(xt), where M is the mass of the particles, U : RN\u00d73 \u2192 R is the potential energy function and x = (x1,...,xn) \u2208 RN\u00d73 is one state of the molecular system. Nevertheless, one common goal of interest to run molecular simulation is to sample from the thermal equilibrium (known as the NVT ensemble) with a heat bath at a fixed temperature, which is taken into account by running the following Langevin equation.\nInstead, we consider two predefined metastable states A and B, which correspond to local minima on the potential energy landscape. We run only local molecular dynamics simulations around two metastable state to initialize our subsequent methods. In particular, we let (y) denote simulating the dynamics in Eq. (7) initialized at point y and time s for time t s and t > s. For t < s, we integrate backward in time and write (y).\nFor a short time interval s chosen as a design decision, our subsequent methods are initialized from the induced distributions \u03bc\u03bf(x) = (\u03a6\u00afs:0)#\u03b4\u03b1 and \u03bc\u03c4(x) = (\u03a6T+s:T)#8B where we abbreviate\nOur hope is that short-run MD simulations provide a useful initialization for our Generalized Flow Matching transport problem with learned potentials."}, {"title": "3.2 Generalized Flow Matching", "content": "Inspired by the generalized Schr\u00f6dinger bridge problem, we formulate the problem of finding feasible transition paths as a distribution matching problem such that we learn a vector field to transport between two given marginal distributions:"}, {"title": "3.3 Inferring Kinetic and Potential Energy from Data", "content": "As one of the main goal of this study is to reduce the number of potential energy evaluation U(\u00b7), we show how we can learn a surrogate energy function V(\u00b7) from the local dynamics data in two ways Pooladian et al. (2024).\nLatent interpolation: We propose to learn an autoencoder that maps high-dimensional data into low-dimensional representations such that it preserves structural information Liu et al. (2024). The hypothesis is that the latent space compresses semantic information from data thus better measure distance than the ambient Euclidean space. Specifically, we map data (x0,xT) to the latent space as (zo, zT). We define the potential energy as the deviation of the state xt from certain interpolation (e.g. spherical interpolation) of 10 and xr in the latent space I(zo, z\u012b, t):\nMetric learning: Another natural way to learn the potential energy is through metric learning such that the metric informs how dense the data is around a particular location Arvanitidis et al. (2021, 2018). Once the metric G : RN\u00d73 \u2192 RN\u00d7N is learned, the kinetic energy term in Eq. (8) will become Kapusniak et al. (2024):"}, {"title": "3.4 Conditional Generalized Flow Matching Objective", "content": "To facilitate optimization of Eq. (8), we derive the following conditional or 'bridge' objective.\nDefinition 1. Assume the path of marginals pt decomposes such that pt(xt) =\n\u222b \u222b po,T(x0, XT)Pt|0,T(xt)dxodxT and there exists vt|0,T such that dtpt|0,T = -\u2207\u00b7 (Pt|0,TUt|0,T).\nWe define the following conditional GFM objective"}, {"title": "3.5 Resampling and Replay Buffer", "content": "Noting the fact that our coupling parameterizations above do not reflect the desired dynamical cost where V(\u00b7) := U(\u00b7), we introduce a resampling procedure to reweight paths induced by our coupling Po,T and spline parameterization of pe 10,7 or x(x0,x)."}, {"title": "4 Experiment", "content": "4.1 Experiment Set-up\nM\u00fcller-Brown Potential. We first employ the M\u00fcller-Brown potential which is a commonly used mathematical model to study transition paths between metastable states. The energy landscape is characterized by three local minima and two saddle points connecting them and can be written down analytically in App. E.1. To simulate this system, we run the first-order Langenvin dynamics around each local minima.\nAlanine Dipeptide. We validate our proposed method on a real-world molecular system, Alanine Dipeptide, which contains 22 atoms. The transition between two metastable states (C7eq and C7ax) is characterized by a two-dimensional free energy surface ($, & dihedral angles). The molecular configurations are sampled by conventional molecular dynamics (cMD). MD simulations are performed in vacuum for 1.2 ns with 2 fs time step for each metastable state with the AMBER99SB-ILDN force field. Trajectories and CVs are recorded every 40 fs. Langevin integrator are used to maintain the system temperature to 300 K. HBonds were constrained during simulations.\nThe free energy surface (FES) is obtained by 800-ns well-tempered metadynamics (WT-MetaD) simulations. Two backbone dihedral angles are chosen as collective variables (CV). Each CV axis is evenly discretized with 25 grid points and the Gaussian bias potential are deposited every 2 ps along the CVs grids. The height and width of the bias potential are set to 0.2 kj/mol and 0.05 radians respectively. The 2D FES is collected and summed from the Gaussian deposits. FES is converged to 0.1 kcal/mol after 800 ns by examining the RMSE of FES between adjacent time stamps. To improve the convergence, we employ the well-tempered version of MetaD with a scaling factor of 8. The simulation is performed via the OpenMM\u00b3 software.\nTo achieve translation and rotation equivariance, we use the internal coordinate system in addition to the Cartesian coordinate system, more details can be found in App. B.\nHardware. All experiments are conducted on two NVIDIA GeForce RTX 4090 GPU cards."}, {"title": "4.2 2D Toy Potential: Muller-Brown Potential", "content": "The M\u00fcller-Brown potential has three local minima and two saddle points following the closed-form potential energy surface in Equation (46). Starting from the initial and final local minima located at (-0.56, 1.44) and (-0.05, 0.47) in Figure 1, we generate the training data by simulating the first-order Langevin Dynamics Equation (22) for 4,000, 12,000, and 20,000 steps with dt = 10-4. As shown in Table 1, a minimum of 4,000 steps simulation of molecular dynamics around local minima can give us decent paths that are close to the saddle points. As the simulation increases to 12,000 and 20,000 steps, it further improves the performance. In addition, our method requires much fewer data compared to MCMC's 1.03B and Doob's Lagrangian's 1.28M simulation steps. It is worth noting that our methods cannot sample from the true transition path distribution as MCMC Dellago et al. (2002) and Doob's Lagrangian Du et al. (2024), but we show we can hit around the saddle points quite efficiently."}, {"title": "4.3 Molecular System: Alanine Dipeptide", "content": "In Figure 3, we visualize 50 randomly sampled transition paths from our method, we can observe that most of the paths find the correct collective variable (\u03c6, \u03c8) dihedral angles in a much higher (66) dimensional space only by learning potential energy from a short-run molecular dynamics simulation around the local minima. In addition, the two ways of learning potential energy result in similar sampled path distributions. Nevertheless, we find the sampled transition paths with the flipped dihedral angles. In Figure 4, we demonstrate a qualitative showcase from a low-energy path, transitioning between the two metastable states."}, {"title": "5 Conclusion, Limitation and Future Work", "content": "In this paper, we propose to simulate transition dynamics in molecular systems by inferring dynamics from the local dynamics around one metastable state to another. We propose a generalized flow matching algorithm that additionally optimizes a learned potential energy of the system. In our scenario, the potential energy is obtained through metric learning or latent space interpolation. We further employ an importance sampling technique and replay buffer to improve the convergence of the method. Experimental results demonstrate that the proposed method is capable of finding good transition paths, and good approximations of transition states with a significantly small number of energy evaluations.\nOne main limitation of the current framework is that the sampled path distribution could be far from the transition path distribution. One future direction is to use the learned path distribution as a proposal distribution for sampling-based approaches, e.g. Holdijk et al. (2024); another direction is to use the current framework as an initialization for transition state search methods."}, {"title": "A Additional Details on Metric Learning", "content": "Following Kapusniak et al. (2024), given a dataset of data samples {xi}i=1K, we learn a metric from\nthe Radial Basis Function (RBF) such that G(x) = (diag(h(x)) + \u03b5\u0399)\u22121, where \u03b5 > 0 and the\nfunction h(x) is defined as:\nwhere Ik is the centroid of the K clusters found by k-means clustering algorithm, wk is learned\nweights and \u03bbk is a bandwidth associated with each cluster. The bandwidth around each cluster Ck is\ndefined as follows:\nwhere Ck is the k-th cluster, Ik is the centriod of the cluster, and \u03ba is a hyperparameter that controls\nthe decay rate of the weight for cluster of different shapes. We then use the following objective\nfunction to learn the weights wk for each cluster in the dataset D:"}, {"title": "B Translational and Rotational Invariance", "content": "A function f is G-equivariant if \u2200x \u2208 Rd, g \u2208 G, we have f \u25e6 g(x) = g \u25e6 f(x). A special case of\nequivariant function is the invariant function such that a function f is G-invariant if \u2200x \u2208 Rd, g \u2208 G,\nwe have f \u25e6 g(x) = f(x). The kinetic and potential energy of each state of a molecular system is\ninvariant to the Euclidean group while the Cartesian coordinate and vector field are equivariant to\nthe Euclidean group. We represent the molecular system in internal coordinates following No\u00e9 et al.\n(2019) instead of Cartesian coordinates which are constructed by invariant quantities, i.e. distances,\nangles and dihedral angles. In this scenario, we remove the unnecessary degrees of freedom, i.e.\ntranslations, rotations and reflections."}, {"title": "C Proofs", "content": "C.1 Conditional Probability Paths\nToward deriving our conditional objective, we begin by showing the following lemma (Tong et al.,\n2023).\nLemma 1. For a given joint distribution p0,T and conditional distribution Pt|0,T such that pt(xt) =\nEp0,T [Pt|0,T(xt)], and a conditional vector field vt|0,T which satisfies the continuity equation,\nthen, under mild conditions, the vector field\nsatisfies the continuity equation"}, {"title": "D Image Dependent Pair Potential (IDPP) Method", "content": "In addition to linear interpolation, there is another class of methods based on interpolating the pairwise distance matrices of the two states to find a good initialization for transition paths. Similarly, we parameterize a neural spline x\u03c6 to match the pairwise distance matrices given by the linearly interpolated pairwise distance matrices rt with the following objective:"}, {"title": "E Additional Experimental Details", "content": "E.1 M\u00fcller-Brown Potential\nThe M\u00fcller-Brown potential can be written down analytically:\nThe potential energy function, neural spline network, and velocity network are trained for 100 epochs, respectively with a batch size of 256. We use the Adam optimizer for all training with 10-2, 10-5 and 10-3, respectively. For each neural network, we use a three-layer MLP, with 128 hidden units per layer, and the SELU activation function. The clustering bandwidth for metric learning in Equation (24) is set to \u03ba = 1.5. The number of clusters K is set to 100 in Equation (23).\nE.2 Alanine Dipeptide\nThe potential energy function, neural spline network, and velocity network are trained for 400, 100 and 100 epochs, respectively with a batch size of 512. We use the Adam optimizer for all training with 10-2, 10-5 and 10-3, respectively. For each neural network, we use a three-layer MLP, with 128 hidden units per layer, and the SELU activation function. The clustering bandwidth for metric"}]}