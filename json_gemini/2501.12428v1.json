{"title": "SplitQuant: Layer Splitting for Low-Bit Neural Network Quantization", "authors": ["Jaewoo Song", "Fangzhen Lin"], "abstract": "Quantization for deep neural networks (DNNs) is the process of mapping the parameter values of DNNs from original data types to other data types of lower precision to reduce model sizes and make inference faster. Quantization often maps different original values to a single quantized value because the range of the original values is larger than the range of the quantized values. This leads to the degradation of the accuracy of the quantized DNNs. Outliers are a main cause of the degradation of quantization resolution because they enlarge the range of original values. To solve the problem, the percentile method is often used to clip outliers. However, clipping the outliers has another problem of removing the important and strong signals in the DNNs. This paper proposes SplitQuant to keep the outliers and improve the quantization resolution at the same time. SplitQuant narrows down the range of the original values and mitigates the effect of outliers by splitting each quantizable layer into three mathematically equivalent layers and applies different scaling factors. Especially, weights and biases are clustered into lower, middle and upper clusters for optimized split. By pre-processing DNNs with SplitQuant, quantization algorithms can achieve better results. SplitQuant was applied on two BERT-Tiny models and improved the accuracy of INT2 quantization by 3.3%p and 2.1%p, achieving accuracies comparable to those of the original FP32 models.", "sections": [{"title": "1 INTRODUCTION", "content": "The number of parameters of deep neural network (DNN) models has been increasing. Models with more parameters require more memory and storage spaces, and takes more inference time due to increased amount of calculation. If the 32-bit or 64-bit floating-point parameter data types (FP32 or FP64) are converted to 2-bit, 4-bit or 8-bit integers (INT2, INT4 or INT8), both space and time can be saved. For example, INT2 takes only 6.25% and 3.125% of space compared to FP32 and FP64. Also, integer calculation is 3 to 5 times faster than floating-point calculation [13].\nThe process of converting floating-point parameters to integers is called quantization. Mathematically, quantization is the mapping process from the domain of floating-point to the codomain of integer. For instance, FP32 to INT8 quantization maps 32-bit floating-point numbers in the range [-3.4 1038, 3.4 1038] to 8-bit integers in the range [-128, 127]. The process often suffers from irreversible errors because 32 bits has to be mapped to 8 bits. Such errors make quantized DNN models less accurate. Therefore, how to reduce such errors is a main research area of quantization.\nOutliers are one of the main causes of quantization errors. Out-liers are input values far away from the mean when the input distribution is given. Simply speaking, outliers are values much bigger or smaller than other values. The performance of the quan-tization function is heavily affected by outliers because outliers worsen the resolution of the quantization function. Conversely, quantization can be performed well if there are no outliers.\nFor example, [-1000.0, -500.0, 0.0, 500.0, 1000.0] can be quan-tized as [-10, -5, 0, 5, 10] to fit in the target range [-10, 10]. This is a good quantization because different original values are mapped to different target values. However, if the input values are [-1000.0, -500.0, 0.0, 500.0, 1 1030], the quantization result will be [-10, -10, -10, -10, 10] be-cause the outlier 1.0 1030 is so big that the differences between other numbers become negligible, and thus lowers quantization resolution. The quantization result becomes bad because four dif-ferent original values -1000.0, -500.0, 0.0 and 500.0 are all mapped to the same value, -10.\nPercentile clipping is the method to prevent the problem caused by outliers. It ignores input values exceeding certain percentile (often 99% is used in practice) when deciding the clipping range [\u03b2, \u03b1]. Although it is a de facto approach to deal with outliers in practice, there is a problem. Outliers in DNN models convey strong signals. Outliers in weights and biases state that certain features in the neural layer should be very sensitive to certain inputs. And outliers in activation values mean that such features were indeed highly activated by an input. Therefore, ignoring outliers causes losing important signals and may cause a negative effect on the accuracy of DNN models.\nSo here is a dilemma. If outliers are kept, the quantization result may not be good. On the other hand, if they are ignored, important signals may be lost.\nThis paper proposes SplitQuant to solve the dilemma. SplitQuant solves the problem by splitting a layer to three mathematically"}, {"title": "2 RELATED WORKS", "content": "In regard to the related works, it should be noted that SplitQuant is not to compete with other quantization algorithms. Rather, SplitQuant reshapes original DNN models to be more quantization-friendly so that other quantization algorithms can achieve better results.\nNet2Net [2] introduced the concept of function-preserving trans-formations to enhance DNN training. It consists of Net2WiderNet and Net2DeeperNet for transforming a small \"teacher\" network to a wider or deeper \"student\" network. The student network is functionally equivalent with the teacher network and has more neurons. So, the knowledge of the teacher network is transferred to the student network, and the student network can be trained further by exploring the enlarged parameter space. SplitQuant dif-fers from Net2Net because SplitQuant is not for training but for quantization. Also SplitQuant does not always increase the number of neurons. For example, SplitQuant splits the activation layer into three layers but keeps the total number of neurons same. On the other hand, Net2Net always increase the number of neurons to enlarge the parameter space.\nCell division [9] uses Net2Net approach to reduce the bit-width of weight parameters of convolutional neural networks (CNNs). While it specifically targets the weight of CNNs only, SplitQuant is applicable for the weight, bias and activation of any kind of neural networks.\nOCS [16] also applied Net2Net to reduce the magnitude of out-liers and improve quantization. It does so by duplicating a neuron and then halving its output or outgoing weights. Halving the out-going weights requires modification of the input of the neuron. SplitQuant differs from OCS because SplitQuant is not based on Net2Net. Also, while OCS focuses on outliers only, SplitQuant ap-proach can still improve the quantization resolution even when there are no outliers.\nVS-Quant [3] applies separate scaling factors for vectors of el-ements of a tensor. It shares similar idea with SplitQuant, which is to use separate scaling factors instead of one scaling factor for a tensor. VS-Quant focused much on modifying the hardware to support per-vector scaling. In contrast, SplitQuant can readily be used on conventional hardware and does not require any additional hardware support because SplitQuant achieves the idea of separate scaling factors by reshaping the input model.\nAgain, it is important to emphasize that SplitQuant is not to compete but to help other quantization algorithms, including the related works, by creating mathematically equivalent and more quantization-friendly DNN models. It is totally possible, and in fact encouraged, to use SplitQuant together with other quantization approaches to get better quantization results."}, {"title": "3 QUANTIZATION AND OUTLIERS", "content": "It is crucial to know about the quantization process to understand why outliers increase errors. Let's consider the quantization process of mapping floating-point values in range [\u1e9e, a] to integer values of bit-width b in range [\u22122b\u22121, 2b\u22121 \u2013 1] as an example. It can be mathematically expressed as\n\n$Q(x) = INT (Sx) + Z$ (1)\n\n$S = \\frac{2^{b-1}}{\\alpha-\\beta}$ (2)\n\n$Z = -2^{b-1} - INT (S\\beta)$ (3)\n\nwhere x is the FP32 input value, S is a scaling factor, INT() is a rounding function and Z is an offset called zero-point.\nScaling factor $S = \\frac{2^{b-1}}{\\alpha-\\beta}$ scales the original values in range [\u03b2, \u03b1] to the quantization range [\u22122b\u22121, 2b\u22121 \u2013 1]. It is very important to note that the magnitude of the scaling factor determines the resolution of quantization. If the scaling factor is too small, in other words a - \u1e9e is too big for the given bit-width b, the resolution of the quantization function worsens and many different original values are mapped to a same quantization value.\nZero-point Z is an offset which corresponds to the target value to which 0 in the original domain will be mapped. Under the condition"}, {"title": "4 SPLITQUANT", "content": "As noted in the previous section, the magnitude of the scaling factor $S = \\frac{2^{b-1}}{\\alpha-\\beta}$ is positively related to the quantization resolution, and ultimately to the performance of quantization. SplitQuant improves quantization resolution by increasing the scaling factor. Since the bit-width b is fixed, SplitQuant increases the scaling factor by de-creasing a \u2013 \u03b2.\nSplitQuant splits linear, convolution and activation layers and combine them while preserving the functionality of DNNs. Since the distance between the maximum and minimum values in each layer (i.e., \u03b1-\u03b2) is smaller than in the original layer, the quantization resolution is improved. How SplitQuant splits layers is graphically represented in Figure 1, and the details of how SplitQuant optimizes the split is explained in the following subsections."}, {"title": "4.1 SplitQuant for weights and biases", "content": "SplitQuant runs k-means clustering on weights and biases. With k = 3, weight and bias parameters are clustered into lower, middle and upper clusters. Initial cluster centroids are selected by the greedy k-means++ algorithm [6]. Then three new layers are created from the clustered parameters. For example, lower layer consists of the lower clusters of weight and bias parameters. Then the original layer is replaced by the newly created layers. The shapes of weights and biases in each new layer are maintained by injecting 0 where needed. Figure 2 and Figure 3 show this idea for linear and convolution layers.\nIt is better to fold batch normalization layers into preceding lin-ear and convolution layers before applying SplitQuant. It is because batch normalization folding reduces number of layers and hence reduces the error from quantization while preserving the functional-ity. Also, it should be noted that PyTorch [10] internally represents the gamma parameters of normalization layers as weights. Since they are semantically not weights, they should not be clustered."}, {"title": "4.2 SplitQuant for activation layers", "content": "Activation layers cannot be clustered because the activation values can only be known in runtime. Even if calibration data are given, the calibration data does not represent all real-world inputs. There-fore, the original activation layer with length n is split into three activation layers with length n/3. Then the results are concatenated to get the output of length n. Still, splitting activation layers can"}, {"title": "5 RESULTS", "content": "SplitQuant was applied on two fine-tuned BERT-Tiny models [5] [8] downloaded from Hugging Face [15]. The models were selected"}, {"title": "6 DISCUSSION AND FUTURE WORK", "content": "The effect of SplitQuant was most significant for low-bit quantiza-tions. As can be seen in Table 1, SplitQuant made most improve-ment for INT2 quantization. The improvement of SplitQuant for INT2 quantization was about 20 to 30 times more than the im-provements for INT4 and INT8 improvements. It is because low-bit quantizations has low quantization resolution, and hence are more susceptible to outliers. And SplitQuant successfully helped INT2 quantizations.\nSplitQuant increases the size of quantized models because there are three times more linear and convolution layers. For FP32 to INT2 quantization as an example, the INT2 quantization model size will be 6.25% of the original, while SplitQuant can increase the INT2 quantization model size up to 18.75% of the original. Since newly added parameters by SplitQuant are all 0s, the model size, memory usage and inference speed may be optimized if SplitQuant is used to-gether with sparse DNN inference engines such as SparseDNN [14].\nFinally, it will be an interesting research topic to apply SplitQuant beyond the scope of TinyML and Edge AI such as large language models (LLMs)."}, {"title": "7 CONCLUSION", "content": "SplitQuant proves to be a highly effective method for improving the accuracy of low-bit quantizations, such as INT2 quantization, which are especially vulnerable to outliers due to their low quantization resolution. By splitting each quantizable layer into three mathemati-cally equivalent layers, SplitQuant successfully keeps the important signals conveyed by outliers while simultaneously enhancing the quantization resolution. The use of k-means clustering to opti-mize the split for weights and biases refines this process further. SplitQuant can be integrated with other quantization algorithms to enhance their performance. Tests on two fine-tuned BERT-Tiny language models demonstrated significant improvements of 3.3%p and 2.1%p in accuracy with INT2 quantization, achieving accura-cies comparable to the original FP32 models. Future research could explore the application of SplitQuant to large language models and investigate potential benefits from advancements in sparse DNN technologies. SplitQuant is open source and it can be downloaded at its online repository."}]}