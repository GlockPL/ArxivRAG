{"title": "All You Need in Knowledge Distillation Is a Tailored Coordinate System", "authors": ["Junjie Zhou", "Ke Zhu", "Jianxin Wu"], "abstract": "Knowledge Distillation (KD) is essential in transferring dark knowledge from a large teacher to a small student network, such that the student can be much more efficient than the teacher but with comparable accuracy. Existing KD methods, however, rely on a large teacher trained specifically for the target task, which is both very inflexible and inefficient. In this paper, we argue that a SSL-pretrained model can effectively act as the teacher and its dark knowledge can be captured by the coordinate system or linear subspace where the features lie in. We then need only one forward pass of the teacher, and then tailor the coordinate system (TCS) for the student network. Our TCS method is teacher-free and applies to diverse architectures, works well for KD and practical few-shot learning, and allows cross-architecture distillation with large capacity gap. Experiments show that TCS achieves significantly higher accuracy than state-of-the-art KD methods, while only requiring roughly half of their training time and GPU memory costs.", "sections": [{"title": "Introduction", "content": "Deep learning has improved areas such as computer vision and natural language processing. However, large model size and heavy compute make it difficult to deploy them on devices with limited resources. Model compression methods, especially knowledge distillation (Hinton, Vinyals, and Dean 2015), effectively compress and accelerate deep networks, as evidenced in various domains (Hinton, Vinyals, and Dean 2015; Wang, Ge, and Wu 2021; Yang et al. 2020). Knowledge Distillation (KD) has emerged as a crucial technique for enhancing the efficiency of neural networks by transferring \"dark knowledge\u201d from a large complex model (the teacher) to a smaller one (the student). In spite of its many successes, KD still encounters many drawbacks and limitations:\n\u2022 Reliant on a teacher trained for the specific task, which is highly costly, cumbersome and inflexible. Utilizing a self-supervised-learning (SSL) pretrained model as the teacher is highly desired, but is rarely studied or with limited accuracy (Koohpayegani, Tejankar, and Pirsiavash 2020). Self distillation by Zhang et al. (2019) is teacher-free, but incurs significantly higher memory and time costs.\n\u2022 Inflexible. KD mostly leverages the logits as training signals, which is inflexible, e.g., for complex tasks like object detection (Wang, Ge, and Wu 2021). Difficulties are also evident when the teacher has huge capacity but the student is a small network (Mirzadeh et al. 2020).\n\u2022 Inefficient. Feature-based KD (Tian, Krishnan, and Isola 2019; Chen et al. 2021a; Wang, Ge, and Wu 2021; Miles, Elezi, and Deng 2024) is more flexible than logit-based ones by mimicking final and/or intermediate features. However, both feature- and logit-based KD methods require a well-trained teacher and the teacher's forward passes, which nearly doubles the time and memory costs.\nOur goal is to achieve teacher-free, flexible and efficient knowledge distillation. We will extract knowledge from SSL pretrained models, which does not require training for any specific task, that is, being teacher-free for KD. Two key questions remain open in the literature:\n1. What is the dark knowledge inside a pretrained model?\n2. How to efficiently adapt it for a specific task?\nOur answer to both questions is to propose a tailored coordinate system (TCS). Our key hypothesis is that the dark knowledge within a SSL pretrained model are at least partly encoded within the coordinate system (or linear subspace) in which their features lie in. The linear subspace can be efficiently encoded via PCA (principal component analysis).\nTo adapt this coordinate system to a specific task and the student network, we propose to simply compute PCA of the teacher's features using one single forward pass on that task's training data. Furthermore, the key to adapt the coordinate system is to tailor it by choosing a subset of the coordinates specifically for the target task.\nCombining both key techniques, we argue that all you need in knowledge distillation is a tailored coordination system, i.e., the proposed TCS method.\nAs shown in Figure 1, the proposed TCS achieves the highest accuracy with lowest memory and compute costs among KD methods. In addition, it enjoys the following benefits:\n\u2022 Teacher-free, i.e., effectively transfers knowledge from SSL pretrained models.\n\u2022 Easily applicable to diverse architectures such as CNNs, Transformers, and MLPs.\n\u2022 Spans both traditional KD and few-shot learning.\n\u2022 Allows cross-architecture and huge capacity gap between teacher and student.\n\u2022 Achieves almost the same training speed as training the student from scratch.\nIn summary, our TCS is flexible, efficient, and not reliant on task-specific teacher. Technically, our contributions are:\n\u2022 We argue that tailored coordinate system (TCS) is all you need to transfer dark knowledge from a task-agnostic teacher to a task-specific student network. Our TCS, of course, can also use a teacher trained for the target task in a supervised manner.\n\u2022 Our TCS method is flexible and versatile, which accommodates diverse architectures and tasks. It is efficient in both KD training and inference in terms of speed and memory footprint, and delivers superior accuracy on both traditional KD and few-shot learning."}, {"title": "Related Work", "content": "Knowledge Distillation (KD) is a line of methods to transfer knowledge from large, (often) high-capacity teacher networks to smaller, more efficient student ones.\nKnowledge Distillation Originally proposed by Hinton, Vinyals, and Dean (2015), KD facilitates the transfer of knowledge in two primary domains: output and representation spaces. Direct logits transfer has been extensively explored (Jin et al. 2019; Li et al. 2020; Yuan et al. 2020; Yun et al. 2020; Mirzadeh et al. 2020; Son et al. 2021; Zhou et al. 2021; Kim et al. 2021; Zhao et al. 2022; Hao et al. 2024), along with methods focusing on feature or representation distillation (Zagoruyko and Komodakis 2016; Huang and Wang 2017; Kim, Park, and Kwak 2018; Park et al. 2019; Heo et al. 2019; Tian, Krishnan, and Isola 2019; Chen et al. 2021b; Wang, Ge, and Wu 2021; Zhu, He, and Wu 2023b). The core in KD is to encourage students to emulate or replicate the predictions of teacher models. KD has been widely applied across diverse learning scenarios (Yu et al. 2017; Chen et al. 2017; Yim et al. 2017; Schmitt et al. 2018).\nKD methods, however, mostly requires a teacher that is specifically trained for the target task. SSL-pretrained models are less studied in KD (Koohpayegani, Tejankar, and Pirsiavash 2020). And, KD methods nearly double the compute and GPU memory costs compared to training the student from scratch, even when excluding the costs associated with training the teacher itself. A recent method (Zhang et al. 2019) introduced self-distillation by condensing knowledge internally, thus effectively reducing the network's size. This method is teacher-free but has very large compute and GPU memory costs.\nPractical Few-Shot Learning Different from classical Few-Shot Learning (FSL), practical Few-Shot Learning or pFSL is tailored to better accommodate real-world scenarios (Fu and Zhu 2024). Traditional FSL predominantly relies on knowledge derived from a base set of labeled images, employing strategies such as meta-learning or transfer learning. Meta-learning methods train across multiple episodes, enabling rapid adaptation to new tasks with minimal data (Hospedales et al. 2022), while transfer-learning typically involves pretraining on base classes followed by fine-tuning on novel classes (Chen et al. 2019; Mangla et al. 2020).\nIn contrast, pFSL foregoes reliance on a predefined base set, opting instead for SSL models pretrained on large-scale datasets (Fu and Zhu 2024). Our distillation methodology for few-shot learning incorporates the pFSL paradigm to enhance real-world applicability and effectiveness."}, {"title": "Method", "content": "As depicted in Figure 2, our TCS starts by extracting the teacher's features through the training dataset. The extracted features are used to compute the teacher's coordinate system by Principal Component Analysis (PCA). This procedure does not involve data augmentation, which means only one forward pass is needed and the cost involving the teacher is in fact negligible.\nWe then align the student's features to fit into this coordinate system. This alignment process is supported by our iterative feature selection method, which tailors the coordinate system to suit the target task. eLSH is another module we proposed by a simple modification to LSH feature mimicking of Wang, Ge, and Wu (2021), which further improves our TCS efficiently.\nWe present our TCS method and its modules one by one.", "Preparations": "In contrast to methods that utilize intermediate features from various layers within the backbone as supervisory signals, our TCS is inspired by the \"feature mimicking\u201d approach suggested by Wang, Ge, and Wu (2021), which only mimics features from the final (penultimate) layer in the backbone.\nFor an input image x, the teacher generates a feature vector $f_t \\in \\mathbb{R}^{D_t}$, while the student produces $f_s \\in \\mathbb{R}^{D_s}$. To reconcile the differing dimensionalities between them, a linear layer is inserted after the student backbone. It changes $f_s$ from $D_s$ dimensions to $D_t$ by a fully connected (FC) layer, and the transformed feature vector is $f_s' \\in \\mathbb{R}^{D_t}$."}, {"title": "Dark knowledge from a SSL-pretrained teacher", "content": "There is no doubt that a SSL-pretrained model (Zhu et al. 2024; Zhu, He, and Wu 2023a) contains useful dark knowledge for various downstream tasks, as even finetuning such models with few-shot samples lead to excellent recognition and detection results (Fu and Zhu 2024). However, the difficulty is how to transfer such dark knowledge?\nLogit- or feature-based KD are both implicit\u2014they mimic either the logit or feature of the teacher. But, because the SSL-pretrained model is not trained for the target task, its logit or feature is at best suboptimal (if not harmful).\nOur theoretical hypothesis and empirical finding is that the coordinate system, or the linear subspace where the teacher's features reside in, contains enough dark knowledge for various downstream target tasks.\nWe are inspired by the finding of Yu and Wu (2023), which states that the features of various models are low-rank and the PCA reduced lower-dimensional versions of these features contain almost the same amount of useful information as the original features.\nIn other words, the few orthonormal eigenvectors that define the projection directions in PCA form a new and (semantically almost equivalent) coordinate system for the teacher's features. And, because we compute PCA using the target task's training data, this coordinate system already incorporates some characteristics of the target task in addition to the dark knowledge from the SSL-pretrained model.\nOf course, there must be significant domain gap between the general-purpose SSL-pretrained model and the target task, or, we must not assume all PCA dimensions are useful for the target task. We further hypothesize that we can select a subset of the PCA dimensions (or coordinates) that suit the target task. Hence, our overall hypothesis is that we can tailor (i.e., feature selection) the teacher's coordinate system defined via PCA. Hence, our proposed method is called TCS (tailored coordinate system).\nWe want to emphasize that even though our hypotheses assume a SSL-pretrained teacher (Zhu, Liu, and Huang 2023), the logic also clearly applies when the teacher is trained using the target task's data. That is, our hypotheses are also suitable for traditional KD tasks."}, {"title": "Induction into teachers' coordinate system", "content": "The teacher's feature matrix, $F_t = \\{f_i\\}_{i=1}^N$, is $N \\times D_t$ in size. The first step in PCA is to center the features by subtracting the mean of each column from its respective values:\n$\\mu = \\frac{1}{N} \\sum_{i=1}^{N} f_i$ (2)\nX = $F_t - 1_N\\mu^T$ , (3)\nwhere $1_N$ is an N \u00d7 1 vector of all ones.\nPCA is then efficiently computed via the Singular Value Decomposition (SVD) of the centered matrix $X = UE V^T$, where U and V are orthogonal matrices containing the left and right singular vectors of X, respectively, and \u2211 is a diagonal matrix containing the singular values. The columns of V (right singular vectors) represent the principal components of X. These principal components define the new coordinate system along which the variations of the data are maximized.\nInduction of a student feature $f_s$ into this coordinate system is pretty simple: by projecting $f_s$ onto this coordinate system to obtain a new representation $f'_s$:\nf\u00ba = (f \u2013 \u03bc)V. (4)\nThe features in $f_s$ encode knowledge from the pretrained model. As showed by the distributed representation nature of deep features, one single dimension of $f_t$ often does not correspond to semantic concepts well, but the combination of them can (Bengio, Courville, and Vincent 2013). For example, the linear combination formed by the first column in V (i.e., the first dimension in $f'_s$) may have high correlation with the concept \"animal\u201d. By encouraging the first dimension of $f' = (f_s \u2013 \u03bc)V$ and that of $(f_t \u2013 \u03bc)V$ to be similar, the student will benefit from this coordinate system when the target task is related to animals. Instead, classic KD method ask $f'_s$ and $f_t$ to be similar, which cannot enjoy such benefits due to the distributed representation nature."}, {"title": "Iterative feature selection", "content": "On one hand, it is also obvious that not all target tasks are related to all concepts as in the general-purpose SSL-pretrained teacher. For example, the target task may be flower recognition, which is irrelevant to animals. Many dimensions in $f'_s$ may well be irrelevant or even noise in the target task. We counter this difficulty by feature selection, i.e., choosing a subset of dimensions from $f'_s$ that are useful for the target task. The subset of features that are picked up forms the Tailored Coordinate System (TCS) for the student.\nSince $f'_s \\in \\mathbb{R}^{D_t}$, we introduce a trainable mask $m \\in \\mathbb{R}^{D_t}$, which is initialized as $1_{D_t}$ and gradually changes some dimensions in m to 0. This simple feature selection strategy prunes irrelevant dimensions using the target task's training data. Subsequently, the student features become\nf\u00ba = f\u00ba \u00a9 m. (5)\nFor feature selection, we resort to the l\u2081-norm:\nL = L_{CE} + \\lambda ||m||_{l_1}, (6)\nwhere $L_{CE}$ is the loss for training the student model (with cross entropy being the most frequently used one), and \u03bb is the hyperparameter to balance $L_{CE}$ and $||m||_{l_1}$.\nOur iterative feature selection module retains relevant dimensions by iteratively removing the influence of irrelevant ones. We accumulate the gradient with respect to m:\n\\sum_{i=1}^{n} \\nabla_m L(\\theta^{(\\tau)}, m^{(\\tau)}), (7)\nwhere n is the batch size, $T$ indexes training iterations, and \u03b8 represents the parameters of the student model. We collect these gradient contributions over iterations, $\u03a3_{i=0}^{T} \u0394(\\theta)$, to continuously assess each dimension's relevance. The selection strategy at iteration 7 is:\n$S_{\\tau} \\leftarrow TopDims(\\sum_{i=0}^{T} \\Delta(\\theta), \\tau \\cdot r)$, (8)\nwhere r represents the target selection ratio at that iteration and TopDims(a, b) picks the largest b dimensions from a. The update rule for m in each iteration is then:\nm^{(\\tau+1)} \\leftarrow m^{(\\tau)} \\odot (1 - S_{\\tau}) + (1-\\tau) \\cdot S_{\\tau}. (9)\nThis formula ensures a smooth transition towards the final target selection ratio r, which is set to 0.5 in all experiments.\nIt is worth noting that operations after the student's backbone (linear layer to obtain $f'_s$, PCA to obtain $f'_s$ and iterative feature selection to obtain $f''_s$) are all linear. During inference, TCS can seamlessly integrate all these modules into the student's classifier. Hence, TCS does not incur extra parameters or computational overhead during the student's inference."}, {"title": "Efficient feature mimicking", "content": "On the other hand, when a teacher model trained specifically for the target task is indeed available (i.e., the teacher is no longer a SSL-pretrained one and the student is trained from scratch), we believe that all dimensions in $f_t$ are useful for guiding the student. In this case, feature selection is not needed, and we can use $f_t$ to directly guide the learning of the student.\nAs we showed earlier, the coordinate system defined by PCA is beneficial for such guidance. Hence, we want $f'_s$ to mimic $W^T(f_t \u2013 \u03bc)$, because they are the student and teacher features in this coordinate system, respectively.\nTo make these two feature vectors similar, we follow the LSH loss by Wang, Ge, and Wu (2021). The LSH loss efficiently hashes similar features into the same 'bins', which reduces computational costs in high-dimensional data spaces:\nh = sign $(W^T(f_t \u2013 \u03bc)V + b)$, (10)\np = \\sigma $(W^T f'_s + b)$, (11)\nwhere \u03c3 is the sigmoid function and h and p are (hard and soft) hash codes for the teacher and student, respectively. Suppose there are M hash codes (i.e., h, p \u2208 $\\mathbb{R}^M$), the loss that forces $f'_s$ to mimic $(f_t \u2013 \u03bc)V$ is\n$L_{ELSH} = \\frac{1}{M} \\sum_{j=1}^{M} [h_j log p_j + (1 - h_j) log(1 - p_j)]$, (12)\nfollowing Wang, Ge, and Wu (2021). We also set all hyperparameters (M and the weight to combine eLSH with $L_{CE}$) as the default values in (Wang, Ge, and Wu 2021).\nHowever, a major difference between our eLSH loss and the LSH loss by Wang, Ge, and Wu (2021) is that we do not involve data augmentation in computing h. Hence, only one forward pass is needed to compute h, while in (Wang, Ge, and Wu 2021) h needs to be computed in every epoch. Hence, we name our loss as efficient LSH (eLSH).\nFor traditional KD, our method is also called TCS, and we refer to our method without eLSH as 'TCS-'. When the teacher is SSL-pretrained, eLSH is not used as aforementioned, it is not suitable in that case.\nIt is also worth mentioning that the KL-divergence loss used for logit-distillation is not used in our method."}, {"title": "Experimental Results", "content": "We assess our method across two distinct experimental setups: traditional KD, and the pFSL framework applied to few-shot scenarios. We provide essential information about our experiments' data, training and evaluation in this section. For more details, please refer to our supplementary materials."}]}