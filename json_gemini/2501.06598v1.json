{"title": "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code Generation", "authors": ["Xuanle Zhao", "Xianzhen Luo", "Qi Shi", "Chi Chen", "Shuo Wang", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding tasks. However, interpreting charts with textual descriptions often leads to information loss, as it fails to fully capture the dense information embedded in charts. In contrast, parsing charts into code provides lossless representations that can effectively contain all critical details. Although existing open-source MLLMs have achieved success in chart understanding tasks, they still face two major challenges when applied to chart-to-code tasks.: (1) Low executability and poor restoration of chart details in the generated code and (2) Lack of large-scale and diverse training data. To address these challenges, we propose ChartCoder, the first dedicated chart-to-code MLLM, which leverages Code LLMs as the language backbone to enhance the executability of the generated code. Furthermore, we introduce Chart2Code-160k, the first large-scale and diverse dataset for chart-to-code generation, and propose the Snippet-of-Thought (SoT) method, which transforms direct chart-to-code generation data into step-by-step generation. Experiments demonstrate that ChartCoder, with only 7B parameters, surpasses existing open-source MLLMs on chart-to-code benchmarks, achieving superior chart restoration and code excitability. Our code will be available at https://github.com/thunlp/ChartCoder.", "sections": [{"title": "1 Introduction", "content": "Recently, Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in addressing a wide range of visual tasks, such as captioning and question answering (Zhang et al., 2024c; Wang et al., 2024b; Chen et al., 2023; Zeng et al., 2024). However, current models still face significant challenges in understanding and analyzing the dense visual information present in complex and informative images. As a significant form of information-intensive images, charts contain complex information such as data and structures, playing a pivotal role in effectively presenting details. The automation of chart comprehension and summarization has garnered significant attention from the research community. To advance chart understanding tasks, current studies leverage existing MLLMs and perform supervised fine-tuning (SFT) on various large-scale chart understanding datasets, such as chart question answering (Methani et al., 2020) and chart-to-text generation (Kantharaj et al., 2022), achieving state-of-the-art performance on existing chart understanding benchmarks.\nHowever, existing works generally treat charts as natural images and fine-tune models by generating natural language descriptions (Zhang et al., 2024b; Han et al., 2023; Meng et al., 2024). This inevitably overlooks the dense information embedded within the charts, resulting in inefficiencies in analysis and comprehension. On the other hand, parsing a chart into code offers a lossless representation, providing a more efficient and comprehensive approach to understanding the chart by accurately capturing and summarizing all its information. Recent works (Shi et al., 2024; Wu et al., 2024; Xia et al., 2024) have proposed various chart-to-code benchmarks, aiming to evaluate the chart reasoning abilities through code. However, current open-source MLLMs are not well-aligned with code generation tasks (Zhang et al., 2024a), resulting in poor performance in parsing charts into corresponding code and limited execution rate of the generated code. As shown in Figure 1, the InternVL2-8B suffers from chart type errors and coordinate size mismatches when converting boxplots to code.\nTo overcome the above challenges in chart-to-code generation, we first conduct an exploratory attempt by leveraging Code LLMs as the language backbone of the MLLMs and propose ChartCoder, the first dedicated chart-to-code MLLM, which incorporates a two-stage training paradigm that contains chart-to-text alignment and chart-to-code instruction tuning. However, compared to chart-to-text, the available chart-to-code dataset is significantly smaller in scale, making it insufficient to support effectively supervised fine-tuning. Therefore, to address the scarcity of data for the chart-to-code domain and train our proposed ChartCoder, we propose the first large-scale diverse and high-quality chart-to-code dataset named Chart2Code-160k along with the model, which contains 160k diverse chart-code pairs with 27 chart types. To enhance the model's capacity to capture critical information, such as chart types and data values, and strengthen its reasoning ability, we propose the Snippet-of-Thought (SoT) method, which emphasizes critical information and optimizes the chart-to-code reasoning process. Specifically, we sample 50k chart-code pairs from the Chart2Code-160k, then utilize Chain-of-Thought (CoT) (Wei et al., 2022) and Program-of-Thought (PoT) (Chen et al., 2022) methods to extend direct generation to step-by-step generation, which aims to emphasize critical information in each step. Experimental results show that by utilizing our proposed Chart2Code-160k with the SoT method, ChartCoder, which, with only 7B parameters, outperforms all open-source MLLMs across various chart-to-code benchmarks.\nAs shown in Figure 1, ChartCoder demonstrates a significantly higher ability to generate correct and executable code.\nIn summary, the main contributions of this work are as follows:\n\u2022 We propose ChartCoder, the first chart-to-code MLLM, which leverages Code LLMs as language backbones. With only 7B parameters, ChartCoder outperforms existing open-source MLLMs on chart-to-code benchmarks.\n\u2022 We introduce Chart2Code-160k, the first large-scale and diverse chart-to-code dataset, consisting of 160k chart-code pairs across 27 chart types.\n\u2022 We propose Snippet-of-Thought (SoT), transforming direct generation to step-by-step generation to emphasize critical information and enhance reasoning capabilities."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Chart Understanding", "content": "Chart understanding is a crucial area of research that encompasses both low-level tasks, such as extracting data from visual charts, and high-level tasks, including QA (Masry et al., 2022), summarization (Kantharaj et al., 2022), and re-rendering chart content (Han et al., 2023). Previous approaches (Singh et al., 2019; Methani et al., 2020) have typically relied on pipeline-based methods. However, these pipeline approaches often struggle with error accumulation across different stages, which limits their overall effectiveness and flexibility. Recent works have led to the development of end-to-end MLLMs (Liu et al., 2023b,c; Han et al., 2023; Ye et al., 2023) specifically designed for chart-related tasks. Trained on extensive chart-specific datasets, these chart-domain MLLMs (Xia et al., 2024; Zhang et al., 2024b; Masry et al., 2024; Yang et al., 2024b) have achieved superior performance across various chart-related tasks. However, existing studies typically describe charts in natural language, which inevitably overlooks the dense information embedded within them, leading to inefficiencies in analysis and understanding. In contrast, code serves as a lossless representation of charts, offering a more effective and expressive approach to capturing chart information, thereby facilitating the solution of various chart-related tasks."}, {"title": "2.2 MLLMS For Code", "content": "Multimodal code generation has recently garnered much more attention. Several works, such as MM-Code (Li et al., 2024b) and HumanEval-V (Zhang et al., 2024a), have been developed to evaluate the capability of MLLMs in solving code problems that incorporate visual elements. Design2Code (Si et al., 2024) and Web2Code (Yun et al., 2024) evaluate the performance of MLLMs by focusing on code generation for HTML web page creation. Among the emerging tasks in this domain, chart-to-code generation has attracted significant interest as the visual elements of charts are more complex. This task challenges MLLMs to generate code that accurately reproduces a given chart or visual representation. Recent works like ChartMimic (Shi et al., 2024) evaluate the reasoning ability of MLLMs in this context. Similarly, Plot2Code (Wu et al., 2024) and ChartX (Xia et al., 2024) also evaluate MLLMS code generation ability, especially for text and data reproducibility. To the best of our knowledge, no dedicated research has focused on solving the chart-to-code generation problem. Our work is the first to attempt to address this challenge."}, {"title": "3 Chart2Code-160k Dataset", "content": ""}, {"title": "3.1 Direct Chart-to-code Generation", "content": "Despite the availability of many chart reasoning instruction-tuning datasets, there is a notable lack of datasets specifically for chart-to-code tasks. Compared to chart reasoning data, chart-to-code data have the following distinct characteristics: (1) One-to-One Mapping: Unlike chart reasoning datasets, which could derive multiple question-answer pairs from a single chart, chart-to-code datasets require a one-to-one correspondence, demanding a large number of chart images for training. (2) Diversity Reflect on Charts: Unlike the diversity of chart reasoning data, which can be reflected in instructions, the diversity of chart-to-code data primarily lies in the variety of chart types and structures. (3) Syntax Constraints: Unlike the flexible natural language answers in chart reasoning tasks, the output code must strictly adhere to programming syntax to ensure executability.\nTherefore, collecting a large number of chart-code pairs that meet the above requirements is challenging. Recent studies have demonstrated the feasibility of generating code with LLMs (Xu et al., 2023; Zhang et al., 2024c). Leveraging the one-to-one mapping property of chart-to-code data, we generate code first and execute it to produce the corresponding charts. In this way, we construct the first large-scale and diverse chart-to-code dataset, named Chart2Code-160k.\nSpecifically, we generate chart-to-code data through the following steps: First, we prompt the LLM to generate keywords within a specific domain and guide it to generate simulated data related to these keywords. Then, to ensure the diversity of chart types, we identify 27 commonly used chart types and manually write seed codes for each as in-context demonstrations. We further provide available functions such as plt.text() and parameters such as hatch='/' to encourage the generation of more diverse functions and parameters (Xu et al., 2023), resulting in the chart structures more diversely. To enhance the generality of generated code, LLMs are encouraged to use standard libraries such as Matplotlib and Seaborn. Additionally, we explicitly define all parameters within the code itself, eliminating the need for external files such as CSVs. This ensures that the code can be executed directly and accurately to represent the chart details. The final step involved executing the generated code to produce the corresponding chart. We utilize the above process to yield 200k code snippets for charts. After executing the code and filtering out problematic charts, such as those with excessive pixels or ticks, we construct a high-quality dataset of 160k diverse chart-to-code pairs. These pairs are formatted as multimodal instruction-tuning samples in the unified structure of <chart image, input instruction, output code>."}, {"title": "3.2 Step-by-step Chart-to-code Generation", "content": "Although the dataset described above includes various chart types and structures, most of the generated code follows a similar template, with only certain details (such as colors and values) providing the essential distinguishing information. This can cause chart-to-code generation models to overlook these critical details and thus produce incomplete or incorrect results. To address the above challenge and further improve the reasoning ability of MLLMs, we propose the Snippet-of-Thought (SoT) method to expand direct chart-to-code generation into step-by-step generation formats, which has demonstrated effectiveness in text-to-code generation tasks (Zheng et al., 2023; Luo et al., 2024). Specifically, we adopt the SoT to imitate the human reasoning process, deriving the final code step by step. This process is divided into four steps: Step 1: Generate the chart type and layout, such as plt.bar() and plt.subplot(). Step 2: Generate the data and corresponding colors used in the chart, such as data=[10, 15] and colors=['#FF0000','#00FF00']. Step 3: Generate critical details of the chart, such as hatch='/' and loc='upper left'. Step 4: Generate the complete and final code. Different from CoT and PoT, we incorporate textual explanations and code snippets for each step to emphasize key information enhance the reasoning process and produce comprehensive outputs.\nHowever, directly instructing the LLM to generate step-by-step code may lead to hallucinations, causing inconsistencies between intermediate code snippets and the final executable code. To maintain consistency among code snippets, we reformulated the step-by-step code data generation into a two-step process involving code generation and decomposition. We sample 50k chart-code pairs from the previously generated 160k data pairs and encourage the LLM to decompose the original code into the required textual explanations and code snippets of Steps 1-3, then concatenate the complete code in Step 4. To further mitigate hallucinations, such as undefined values or parameters in Steps 1 and 2, we used placeholder or default parameters during code decomposition to ensure the construction of consistent and reliable step-by-step code."}, {"title": "3.3 Dataset Analysis", "content": "Chart2Code-160k dataset provides three key advantages: (1) The First Large-Scale Dataset: It contains 160k data pairs for instruction tuning, significantly surpassing the size of previous datasets. (2) Diverse Chart Structures and Types: It includes 27 different chart types, with diverse structures enabled by a wide variety of functions and parameters in the code. (3) Syntactically Correct and Executable Code: All corresponding code is syntactically correct and executable, with explicitly defined parameters that ensure precise alignment between chart structures and code representations. The comparisons of Chart2Code-160k with relevant chart-to-code datasets are listed in Table 1. Although numerous chart-related datasets have been proposed (Liu et al., 2023a; Han et al., 2023), none have sufficiently addressed the challenge of chart-to-code tasks. Our proposed Chart2Code-160k fills this gap, equipping the model with advanced capabilities for downstream chart tasks."}, {"title": "4 ChartCoder Model", "content": "After constructing the Chart2Code-160k, we aimed to leverage the data to enhance the capacities of MLLMs to generate code from charts. Unlike previous methods that rely on general LLMs with a low proportion of code in their training corpus, which limits their coding capabilities, we pioneer the use of Code LLMs to enhance the coding abilities of MLLMs from scratch."}, {"title": "4.1 Model Architecture", "content": "Following the standard architecture of MLLMs, ChartCoder consists of three modules: a pre-trained vision encoder (SigLIP-384 (Zhai et al., 2023a)), a vision-language connector (two-layer MLP) and a Code LLM backbone (DeepSeek Coder 6.7B (Guo et al., 2024)). The vision encoder extracts the input image into visual features, and the connector projects it into the word embedding space. LLM backbone then combines visual and textual features to generate responses.\nPrevious works emphasize the importance of high-resolution input for chart understanding (Liu et al., 2024; Guo et al., 2025), as details like textual words may lost in low-resolution images. However, vision transformers (ViTs) like CLIP (Radford et al., 2021) and SigLIP (Zhai et al., 2023b) are constrained to resolutions of 2242 and 3842 respectively, which limits their capacities to encode chart images with sufficient detail. To address this, we utilize the Any Resolution strategy (Liu et al., 2024) to resize and patchify chart images to ensure ChartCoder processes high-resolution chart images effectively. Specifically, the input chart image is first resized to a pre-defined optimal aspect ratio, whose height and width are integer multiples of the image resolution. The resized image is then divided into patches of standard resolution and concatenated with a directly downsampled version of the image. This approach preserves both general and detailed information without requiring the original high-resolution image to be resized into a standard square, thereby avoiding the loss of fine details. Details are shown in Figure 2."}, {"title": "4.2 Model Training", "content": "Since we propose to use Code LLMs as the language backbone to enhance the code abilities of MLLMs, existing models do not meet our requirements as their backbones are general LLMs. Thus, to align charts with text and perform supervised fine-tuning for chart-to-code tasks, we adopt the following two-stage training process.\nChart-to-text Alignment. The alignment process aims to endow the model with chart structure perception capability. In this stage, we freeze the language and vision encoder models and pre-train the vision-language connector (Liu et al., 2023c). We collect and filter public chart corpora for alignment, which contains multiple tasks like chart caption and chart-to-table. Specifically, we use the following corpora: (1) UniChart (Masry et al., 2023), (2) Chart-to-Text (Kantharaj et al., 2022), (3) SciCap (Hsu et al., 2021), and (4) SciCap+ (Yang et al., 2024a). Additionally, we incorporate the LLaVA pre-training dataset (Liu et al., 2023c) and our proposed Chart2Code-160k to achieve a more balanced coverage of concepts.\nChart-to-code Instruction-tuning. The second stage focuses on enhancing the model's capabilities in chart-to-code tasks. In this stage, all three modules are jointly fine-tuned with our proposed Chart2Code-160k, and additional code-related data, such as ChartQA PoT (Zhang et al., 2024b) and ChartLlama chart-to-chart (Han et al., 2023)."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Baselines and Benchmarks", "content": "We compare ChartCoder with existing models in three setups (1) General-domain open-source MLLMs including InternVL2(4B, 8B, 26B, 76B) (Chen et al., 2024), Qwen2-VL(7B, 72B) (Wang et al., 2024a), DeepSeek-VL-7B (Lu et al., 2024), LLaVA-Next(7B) (Li et al., 2024a) and MiniCPM-Llama3-V2.5 (Yao et al., 2024). (2) Proprietary models include GeminiProVision (Team et al., 2023), Claude-3-opus (Anthropic, 2024), GPT-4V (OpenAI, 2023), and GPT-4o (OpenAI, 2024). (3) Chart-domain MLLMs including ChartLlama (Han et al., 2023), ChartAssisstant (Meng et al., 2024), Tinychart (Zhang et al., 2024b) and ChartVLM (Xia et al., 2024). All the methods are evaluated on the benchmarks ChartMimic (Shi et al., 2024), Plot2Code (Wu et al., 2024) and ChartX (Xia et al., 2024). We revise the Rating calculation in Plot2Code. The original evaluation only considers charts corresponding to executable code, which leads to higher ratings for only generating simple charts. We calculate all the results, which better reflect the impact of complex charts. For all methods, the zero-shot setting was adopted during the evaluation. Details about these benchmarks are shown in the Appendix A.2."}, {"title": "5.2 Main Results", "content": "As indicated in Table 2 ChartCoder achieves the best performance among open-source MLLMs in all the chart-to-code tasks and even better than some proprietary models. Notably, on the most challenging ChartMimic task, ChartCoder surpasses leading small-scale general-domain MLLMs (<20B) such as MiniCPM-Llama3-V2.5 and InternVL2-8B with average scores of 26.7 and 34.6 respectively. The improvement achieved by ChartCoder highlights the effectiveness of our proposed Code LLM as the language backbone, combined with the Chart2Code-160k dataset, in enabling MLLMs to excel in chart understanding and code generation tasks. In addition, ChartCoder also performs better than existing state-of-the-art large-scale MLLMs such as InternVL2-Llama3-76B and chart-domain MLLMs such as TinyChart.\nWe further illustrate the detailed high-level and low-level scores for the ChartMimic benchmark. The high-level score utilizes GPT-4o to evaluate the detailed similarity between the ground truth and generated chart images in six aspects: chart types, layout, text content, data, style, and clarity. The low-level score is calculated based on a comparison between the ground truth and the generated code, focusing on the code similarities in four aspects: text, layout, type, and color.\nTable 3 denotes the high-level results. Chart-Coder is the model most comparable to GPT-4o, as the evaluations were conducted by GPT-4o itself, suggesting the actual performance gap may not be as pronounced as it appears. Notably, Chart-Coder shows the largest gap with GPT-4o in the data category, which highlights the complexity of extracting numerical values from charts, aligning with conclusions from existing chart understanding benchmarks: current MLLMs struggle to directly and accurately extract complete data from complex charts (Wang et al., 2024c; Zhang et al., 2024b).\nTable 4 shows the low-level results. ChartCoder even slightly outperforms GPT-4o in layout, type and color, highlighting the diversity of our proposed Chart2Code-160k dataset. However, the text score of the ChartCoder is lower than GPT-4o, which is similar to the results of high-level scores. We believe this is due to the lack of specialized chart OCR-oriented training for our model. Nevertheless, our text accuracy still surpasses that of open-source models, indicating the effectiveness of our proposed ChartCoder model and Chart2Code-160k dataset. We further present some case studies on ChartMimic and compare ChartCoder with GPT-4o, Qwen2-VL-72B, and InternVL-8B. The results are shown in Figure 4, the outputs of ChartCoder are much more similar to the ground truth chart than open-source MLLMs."}, {"title": "5.3 Ablation Study", "content": "We perform extensive ablation experiments to validate the effectiveness of our proposed model and dataset. We divide the ablation study into three parts, and the results are shown in Table 5 (1) Code or general LLMs. To investigate whether employing Code LLMs as language backbone provides specific advantages in chart-to-code tasks and identify the nature of these potential benefits, we replace the Code LLM, DeepSeek Coder 6.7B, with general LLM, DeepSeek LLM 7B (Bi et al., 2024), maintaining the same two-stage training procedures. The result shows that compared with general LLM, utilizing code LLM as the language backbone could significantly improve the execution rate, as well as the low-level and high-level scores. We further analyze the types of errors in the code that failed to execute and find that utilizing code LLMs significantly reduces syntax errors like missing closing quotation marks and type errors like incorrect argument type. (2) Resolution of vision encoders. Previous studies have indicated that performance on chart understanding tasks is resolution-dependent, with lower resolutions negatively impacting model performance (Liu et al., 2024). To verify whether resolution affects chart-to-code tasks, we replace SigLIP-384 with CLIP-336 (Radford et al., 2021) and maintain the other setting. The result shows that the resolution of the vision encoder generally does not affect the output code execution rate but slightly influences the high-level chart similarity. Through our analysis, we find that, similar to the challenges in chart understanding, this issue is caused by the negative impact of low resolution on the recognition of text and special symbols. However, as we utilize the Any Resolution strategy, this impact has been reduced significantly. (3) Dataset effectiveness. We design two scenarios to illustrate our proposed Chart2Code-160k dataset. Firstly, to evaluate our proposed SoT method to emphasize the critical information in the chart, we remove the 50k step-by-step generation data and train the model using only the direct generation data. The result shows it influences the low-level and high-level scores notably, especially in text content and data, which shows the role of emphasising critical information. Secondly, we select Qwen2-VL-7B as the baseline of open-source MLLM and directly fine-tune it on our proposed Chart2Code-160k datasets. The result illustrates that after fine-tuning, the performance improves significantly on all the metrics, demonstrating the effectiveness of Chart2Code-160k."}, {"title": "5.4 Analysis", "content": "We further evaluate the role of code in the chart understanding task. We use MiniCPM-Llama3-V2.5 to evaluate two input forms, Image only and Image with Code, on the MMC True/False benchmark (Liu et al., 2023a). The result in Table 6 shows that using code helps the model better understand chart details, especially the chart types and the data they contain. A case study is shown in Figure 6."}, {"title": "5.5 Conclusion", "content": "This work aims to tackle the challenge of chart-to-code tasks with MLLMs. First, we propose the ChartCoder, which utilizes Code LLM as the language backbone dedicated to chart-to-code tasks. Second, to solve the scarcity of chart-to-code data, we present the first large-scale and diverse chart-to-code dataset, Chart2Code-160k. Finally, to emphasize the key information, we propose the Snippet-of-Thought (SoT) method to generate step-by-step data. Experiments show that ChartCoder outperforms existing open-source MLLMs."}, {"title": "Limitation", "content": "Our study is comprehensive but has certain limitations that we aim to address in future research. Due to constraints in computational resources, we only trained ChartCoder with 7B parameters, which has demonstrated sufficiently good results for now. A larger model could potentially achieve even better performance. Future work may focus on exploring more complex and diverse charts and codes while also experimenting with other image types, such as HTML, to develop a comprehensive multi-modal code large language model."}, {"title": "Ethical Statement", "content": "Our research employs publicly available models and datasets with proper citations. This approach minimizes the risk of generating toxic content, leveraging the widely used and non-toxic nature of our datasets and prompts."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation Details", "content": "In the data generation stage, we utilize gpt-4o-2024-08-06 as the LLM for both direct and step-by-step generation processes.\nIn the training stage, ChartCoder is initialized with SigLIP-384 (Radford et al., 2021) as the vision encoder and DeepSeek Coder 6.7B (Guo et al., 2024) as the large language model. The whole training process is divided into alignment and instruction tuning. During the alignment stage, we only train the vision-language connector with the chart-to-text alignment data. The learning rate is set to 1e-3. In the instruction tuning stage, we train the entire model for 1 epoch with a batchsize of 128. The learning rate of SigLIP and other modules are 5e-6 and 1e-5 respectively, with a warmup at the beginning of 3%, then decays to 0 at the end of training. The alignment and instruction tuning processes cost 12 and 5 hours on 32 Tesla A100 GPUs with 80 GB VRAMS."}, {"title": "A.2 Benchmark Details", "content": "ChartMimic (Shi et al., 2024) focuses on evaluating the ability of MLLMs to redraw charts from ArXiv papers, emphasizing the preservation of the original style and appearance. It consists of two subsets: testmini and test. Following the settings in the original paper, we adopt the Direct Mimic task on the testmini subset as the default evaluation standard, reporting execution success rates alongside low-level and high-level scores.\nPlot2Code (Wu et al., 2024) aims to evaluate models' abilities to generate code corresponding to charts from the available Matplotlib galleries, with a focus on textual similarity. We evaluate models on its Direct Asking task using three metrics: Pass Rate, Text-Match, and Rating.\nChartX (Xia et al., 2024) contains various tasks with synthesis chart images, including Question Answering, Summarization, Description and Redrawing. We choose the Redrawing task and report the GPT score as the metrics in ChartX."}, {"title": "A.3 More Ablation Studies", "content": "We also perform more ablation studies on the language backbone and further choose Qwen2.5-7B and Qwen2.5 Coder-7B (Qwen Team, 2024) for comparison. The results also show that using Code LLM as the language backbone is better than using general LLM. However, we find that using the"}, {"title": "A.4 Chart2Code-160k Analysis", "content": "We count the proportion of different charts in the Chart2Code-160k dataset in Table 8."}]}