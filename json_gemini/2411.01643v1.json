{"title": "EcoAct: Economic Agent Determines When to Register What Action", "authors": ["Shaokun Zhang", "Jieyu Zhang", "Ankur Mallick", "Dujian Ding", "Daniel Madrigal", "Mirian Hipolito Garcia", "Victor R\u00fchle", "Qingyun Wu", "Menglin Xia", "Chi Wang"], "abstract": "Recent advancements have enabled Large Language Models (LLMs) to function as agents that can\nperform actions using external tools. This requires registering, i.e. integrating tool information into\nthe LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate\ntools into the agent's context and retain them across multiple reasoning steps. This process remains\nopaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies\ndue to increased context length from irrelevant tools. To address this, we introduce EcoAct, a tool-\nusing algorithm that allows LLMs to selectively register tools as needed, optimizing context use.\nBy integrating the tool registration process into the reasoning procedure, EcoAct reduces compu-\ntational costs by over 50% in multi-step reasoning tasks while maintaining performance, as demon-\nstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with\nonly minor modifications to the prompt, making it applicable to LLM agents now and future.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have been conceptualized as agents and have demonstrated their capability to perform\na broad range of complex tasks. When augmented with external tools (Yuan et al., 2023; Qu et al., 2024; Zhang et al.),\nLLM agents can extend their functionality beyond conventional natural language processing (Qin et al., 2023). For\nexample, LLM agents equipped with scientific tools can conduct scientific research (Bran et al., 2023; Ghafarollahi\n& Buehler, 2024), while those integrated with physical robotic systems are capable of performing robotic manipula-\ntions (Ahn et al., 2022; Huang et al., 2023). External tools essentially expand the action space of LLM agents, enabling\nthem to leverage existing functionalities to accomplish a variety of complex tasks (Xi et al., 2023; Wu et al., 2023a;\nPeng et al., 2023; Wu et al., 2023b; Shridhar et al., 2020; Hua et al., 2023, 2024b,a).\nTo equip LLM agents with external tools, they must undergo a tool registration procedure. Specifically, information\nabout the candidate tools needs to be added to the context of the LLMs that support the agents. This information\nrepresents essential details for tool usage, including tool names, descriptions in natural language, and instructions for\ninput parameters. The current practice in tool registration indiscriminately incorporates all candidate tools into the\nagent's context, where these candidate tools are preemptively selected by users or retrieved automatically through\nexternal algorithms (Ocker et al., 2024; Qin et al., 2023; Gao et al., 2023). LLM-based agents will then process\ncontextual information from all registered tools and select the appropriate tool for each reasoning step. However, this\nparadigm, which involves preparing all tools in advance and keeping the full information of the registered tools within\nthe LLM's operational context, introduces one key issue: the tool registration process is opaque to the agents and\nnot fully integrated into their autonomous reasoning pipelines. Each time the LLM is invoked, information from all\npassively registered tools is processed, even though not all tools are necessary and only one single tool can be utilized\nin each step, which drives inefficiencies in both cost and inference time (see Figure 1a). The problem becomes more\npronounced as the number of pre-registered tools grows, imposing an even greater burden on the agent's decision-\nmaking process. The agent possesses the capacity to reason to act with their intrinsic reasoning mechanism but lacks\nthe ability to reason to register."}, {"title": "2 Method", "content": "In this section, we present EcoAct, a general tool-using algorithm designed to mitigate efficiency issues in agent\ntool-using scenarios. We begin by formulating the research problem and then provide the details of each component\ndesigned in EcoAct."}, {"title": "2.1 Problem setup", "content": "We start by defining the relevant notations and outlining the research problem. Consider a language agent and a set of\ntools $Z = \\{z\\}_; that the agent could access. The agent's objective is to address user queries according to a specific\npolicy \u03c0. At any given decision time step t, the agent receives two types of information: (1) the historical context ct\nwhich includes all previous action-observation pairs, and (2) a set of accessible tools Z that can be used in this time\nstep. The agent then must determine the next action to take. Formally, this decision process can be expressed as:\n\n$\\pi(c_t, \\mathcal{Z}) \\rightarrow a_t, \\text{s.t.} a_t \\in A,$\n\nwhere at denotes the action that been taken at time step t. It represents one specific tool-calling from accessible tool\nset Z. A denotes the action space of this language agent. Consequently, the size of the tool space is equivalent to the\nsize of the action space, i.e., |A| = Z.\nIn evaluating a specific agent algorithm, the total token consumption required to complete user queries, which encom-\npass both input and output tokens serves as a general metric for assessing the algorithm's cost (Chen et al., 2023;\nWang et al., 2023; Hidv\u00e9gi et al., 2024; Cheng et al., 2023; Ding et al., 2024). This is because token consumption\nis positively correlated with budget expenditure and latency, particularly in the context of large language models as a\nservice (Gan et al., 2023; Sun et al., 2022). At time step t, we use the cost function j(ct, Z, at) to represent the cost\nassociated with making a decision at that step t. The one-step cost is given by:\n\n$j(c_t, \\mathcal{Z}, a_t) = \\alpha \\cdot (l(c_t) + l(\\mathcal{Z})) + \\beta \\cdot \\alpha_t,$\n\nwhere I measures the token length. \u03b1 and \u03b2 denote the cost per input token and output token, respectively, which are\ndetermined by the LLMs inference service provider. Under this formulation, the total cost I for completing users\nquery with n reasoning steps is:\n\n$\\text{Jtotal} = \\sum_{t=1}^n j(c_t, \\mathcal{Z}, a_t), \\text{where } a_t = \\pi(c_t, \\mathcal{Z}).$\n\nThe focus of our research is to minimize the total cost Itotal while maintaining a good performance in response to user\nqueries."}, {"title": "2.2 EcoAct", "content": "Motivation. According to Equation 3, the polynomial Jtotal depends on Z, ct, and at. We primarily examines the\ntoken consumption associated with the input tool set Z at each time step, considering ct and at as less controllable\nfactors in practice. Most approaches for identifying \u017d at each time step rely on a retrieval-based methods. A subset of\ntools is retrieved for each query and registered with the agent, which then makes sequential decisions until the problem\nis resolved Qin et al. (2023); Patil et al. (2023). However, a key limitation of this once-for-all paradigam is that each"}, {"title": "3 Experiments", "content": "We conduct experiments to prove the superiority of the proposed method. We begin by providing the experimental set-\ntings in Section 3.1. We then evaluate the EcoAct on ToolBench benchmark to verify its effectiveness in Section 3.2.\nFinally, we perform in-depth investigations in the last two sections to provide a better understanding of EcoAct."}, {"title": "3.1 Experimental setup", "content": "Data preparation. We mainly conduct experiments on the ToolBench (Qin et al., 2023), which is large-scale dataset\nfor tool use. It involves 16,464 tools in total which has been widely used as the benchmark to make evaluations\nof tool use algorithm (Du et al., 2024; Ye et al.). ToolBench comprises six subsets G1-Instruction (G1-I), G1-Tool\n(G1-T), G1-Category (G1-C), G2-Instruction (G2-I), G2-Category (G2-C), and G3-Instruction (G3-I). These subsets\nare classified according to varying levels of complexity in tool use, with differences in 'Instruction', 'Category', and\n'Tool' reflecting the relationships between tool categories in these test subsets and those in the training sets. Following\nthe same setting with AnyTool (Du et al., 2024), we adopted the filtered benchmark which excludes all non-solvable\nqueries in ToolBench. The remaining queries in these six subsets are 115, 132, 142, 107, 98, and 38, respectively.\nUnless specified otherwise, for each query, we use the state-of-the-art method AnyTool (Du et al., 2024) to retrieve\ntools for each query in all experiments across the paper. More details of this benchmark could be found in Appendix A."}, {"title": "Evaluation metrics", "content": "We primarily use two metrics to make evaluations: the pass rate and the cost, with the latter\nmeasured in monetary terms. Pass rate essentially measures LLM's ability to successfully execute an instruction within\nlimited budgets. We utilize the evaluation script from (Du et al., 2024) to get the pass rate results in all experiments\nof our paper, which addresses issues related to artificially inflated pass rates (Du et al., 2024). We utilize GPT-4-turbo\nto make the pass rate evaluations, applying the same prompts as those used in ToolBench. Unless specified otherwise,\nwe report the cost in US cents. More details about the evaluation prototype can be found in Appendix A.2."}, {"title": "3.2 Main results", "content": "EcoAct essentially serves as a plug-and-play component for different agent reasoning algorithms. Here, we mainly\nevaluate the impact of EcoAct on the classical reasoning method ReAct (Yao et al., 2022). Our aim is to assess"}, {"title": "3.3 More analysis", "content": ""}, {"title": "3.3.1 Extension to complex reasoning strategy", "content": ""}, {"title": "3.3.2 Skill-library evolution", "content": "We then investigate how the number of registered tools evolves over time in the ReAct method when enhanced with\nEcoAct. Our primary objective is to investigate whether, in scenarios where a large number of tools are available for\nusers' queries, EcoAct could cause ReAct to register an excessive number of tools greedily. Such behavior could lead\nthe algorithm to revert to its original state by registering all available tools at the begining time, thereby undermining\nthe benefits of EcoAct. To answer this question, we selected queries where the number of tools exceeded 20 from\neach group one of G1-I, G2-I, G3-I and conducted experiments to track the ratio of registered tools to the total number\nof available tools over time. We also compared this ratio across different models. The results, averaged within each\nsubset of data, are displayed in Figure 4.\nFrom the results we could observe that (1) EcoAct is flexible. Tool registrations occur throughout the entire problem-\nsolving process, suggesting that the agent is capable of registering any tool it deems useful at any point. Moreover,\nEcoAct may encompass a self-correction mechanism -if the agent realizes that a registered tool is unsuitable after\nobtaining more detailed information, it can leverage its intrinsic reasoning ability to register a more appropriate one in\nany time step. (2) We also could observe that the ultimately registered tools constitute only a small fraction of the total\navailable tools, approximately 30% in all three subsets with all LLM models. The resukts demonstrates that, in cases\nwhere a large number of tools are available, most of them are redundant (about 70%), and EcoAct could effectively\nprevents the registration of these redundant tools."}, {"title": "3.4 Ablations", "content": ""}, {"title": "3.4.1 Single-Tool vs. multiple-tool registration", "content": "In our approach, the proposed meta-tool tool_register is designed to register only one tool per tool registration action\nin each time step. This naturally raises one critical question: could registering multiple tools simultaneously reduce\ncosts while maintaining comparable performance? The intuition behind this is that each tool registration essentially\nrequire one LLM calling, which incurs token costs. If the agent could leverage its internal reasoning mechanisms to\nregister several potentially useful tools in one interaction, it might lead to cost savings due to the decrease of LLM call\nnumber. To explore this hypothesis, we modified tool_register to allow for the registration of multiple tools at once,\nallowing agents to select as many tools as deemed necessary based on their reasoning. We conducted experiments\nusing the G2-I and G3-I datasets in state-of-the-art GPT-40 model according to Table 1, where we use Ecoct to\naugment ReAct and present the results in Table 3."}, {"title": "3.4.2 Tool names vs. descriptions information in tool registration", "content": "We then investigate the feasibility of incorporating both tool names and descriptions in the tool registration process. We\naim to address the following questions: Is the tool name sufficient for accurate tool registration? Does the inclusion of\ntool descriptions enhance registration performance? We also examine whether this modification affects the associated\ncosts. To evaluate these questions, we conduct experiments using the G2-instruction and G3-instruction subsets,\nincorporating all available tool descriptions for registration within the ReAct framework, leveraging the arguments\nfrom EcoAct in the GPT-40 model. The results of our experiments are presented in Table 4.\nFrom the results, we could observe that the including tool descriptions for tool registration does not necessarily lead\nto a noticeable improvement in performance. However, this approach incurs a significant increase in cost, comparable\nto that of standard ReAct. Specifically, the cost increases 31.3% and 65.5% in G2-instruction and G3-instruction\nrespectively. This finding suggests that tool names alone provide sufficient information for the agent to perform correct\ntool registration. This is because the context of tool descriptions is obviously larger than tool names. Consequently,\nthe inclusion of tool descriptions may be unnecessary and could result in substantial cost increases."}, {"title": "4 Related Works", "content": "Large language models (LLMs) represent a major breakthrough in artificial intelligence, prompting an increasing body\nof research dedicated to employing LLMs in the construction of autonomous agents capable of performing complex\ntasks (Xi et al., 2023; Wu et al., 2023b; Peng et al., 2023; Shridhar et al., 2020; Song et al., 2024; Wu et al., 2024;\nZhang et al., 2023a; Ma et al., 2024). In these LLM-based agents, the ability to leverage external functions, tools,\nor actions to interact with the environment or solve sub-tasks is crucial. These external tools empower agents to go\nbeyond natural language processing. For instance, LLM agents equipped with scientific tools can conduct scientific\nresearch (Bran et al., 2023; Ghafarollahi & Buehler, 2024), while those integrated with robotic systems can perform\nrobotic manipulation tasks (Ahn et al., 2022; Huang et al., 2023)."}, {"title": "5 Conclusion", "content": "In this work, we propose EcoAct, a simple yet effective approach that seamlessly integrates tool registration into\nthe intrinsic reasoning processes of LLM agents. The core concept involves initializing the agent with a meta-tool\nnamed tool_register, which enables the agent to selectively register tools deemed useful based on their names at each\ntime step. This action allows the agent to avoid indiscriminately incorporating all candidate tools into its context,\ninstead retaining only relevant information across reasoning steps, thereby achieving significant cost savings. We\nevaluate EcoAct on the ToolBench dataset, augmenting various reasoning methods, and demonstrate that EcoAct\nsignificantly reduces computational costs while maintaining comparable performance."}, {"title": "A More Details about ToolBench", "content": "ToolBench is a large-scale tool-usage dataset comprising 16,464 real-world RESTful APIs across 49 categories from\nthe RapidAPI Hub. All queries in this benchmark were generated by prompting ChatGPT to create diverse tasks in-\nvolving these APIs, covering both single-tool and multi-tool usage scenarios. Through careful human evaluation, the\nauthors determined that the generated instructions exhibit high diversity, reflecting a wide range of practical applica-\ntions. This benchmark has been widely adopted as a standard evaluation tool in several studies (Du et al., 2024; Ye\net al.)."}, {"title": "A.1 Subsets information", "content": "The dataset is categorized into three levels: G1, G2, and G3, which correspond to single-tool instructions, intra-\ncategory multi-tool instructions, and intra-collection multi-tool instructions, respectively. Each level is further subdi-\nvided into three subcategories:\n\u2022 Instruction: unseen instructions using the same set of tools as in the training data.\n\u2022 Tool: unseen tools from previously encountered category as those in the training data.\n\u2022 Category: unseen tools from an entirely different, previously unobserved category\nHowever, since our EcoAct algorithm does not rely on training data, the distinctions between these three subsets are\nminimal."}, {"title": "A.2 Evaluation protocol", "content": "We adopt the same pass rate evaluation protocol as outlined in AnyTool (Du et al., 2024). In the original ToolBench\nbenchmark, the authors employ a two-stage evaluation process. In the first stage, ToolBench uses an LLM (GPT-4 in\nour paper) to assess whether the selected API candidates can address the query, classifying them as either 'solvable'\nor 'non-solvable'. For queries deemed 'solvable', the LLM then evaluates the effectiveness of the solution, labeling it\nas either 'solved' or 'unsolved'. The pass rate is calculated using the following equation:\n\nPass Rate\n=\nNon-solvable + Solved\nNon-solvable + Solved + Unsolved\n\nA key issue with this evaluation protocol arises when there is a large number of 'non-solvable' queries identified by\nGPT-4. This can result in an artificially high pass rate, despite many queries remaining unsolved. To mitigate this, Du\net al. (2024) conducted a manual review of all queries, retaining only those that can be resolved. Consequently, the\npass rate is calculated using the following equation:\n\nPass Rate\n=\nSolved\nSolved + Unsolved\n\nMore information of this evaluation protocol could be found in the original paper (Du et al., 2024). In terms of cost\ncalculation, the monetary cost is computed based on the corresponding pricing from Microsoft Azure."}, {"title": "B More Implementation Details", "content": "When using AnyTool to retrieve tools for each query, we set the maximum size of the API-Candidate Pool to 64,\ndrawing on the findings of the AnyTool paper, which suggest that a pool size of 64 nearly saturates performance.\nAdditionally, we increased the maximum reasoning steps to 24, up from the default of 12, to explore the behavior\nof EcoAct under conditions without budget constraints. If not otherwise specific in the paper, we use the same\nhyperparameters as it is used in ToolBench (Qin et al., 2023), considering hyperparameters play a critical role in\ndetermining how well an algorithm performs (Zhang et al., 2023b,c)."}, {"title": "C Prompts", "content": ""}, {"title": "C.1 Prompt design for EcoAct", "content": ""}, {"title": "C.2 tool_register", "content": "{\n\"name\": \"function_register\",\n\"description\": \"I have given you a list of functions (names), please call\nthis function to choose one of them that may be useful. The function\nyou choose should be the one that you think is most useful in the\ncurrent state. After you make function selection using this function,\nI will give you the detailed information of your selected function.\nYou can then call the function you selected with appropriate inputs if\nyou think the function is useful.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"function_name\": {\n\"type\": \"string\",\n\"description\": \"the name of the function you want to call\",\n}\n}\n},\n\"required\": [\"function_name\"]\n}"}, {"title": "C.3 Prompt for ReAct/DSFDT", "content": ""}, {"title": "C.4 Prompt for pass rate evaluations", "content": ""}, {"title": "C.4.1 Prompt template for verifying whether the query has been resolved", "content": "<function>\n<name>check_answer_status</name>\n<description>\nGiving the query and answer, you need give 'answer_status of the answer by\nfollowing rules:\n1. If the answer is a sorry message or not a positive/straight response for\nthe given query, return \"Unsolved\".\n2. If the answer is a positive/straight response for the given query, you have\nto further check.\n2.1 If the answer is not sufficient to determine whether the solve the query\nor not, return \"Unsure\".\n2.2 If you are confident that the answer is sufficient to determine whether\nthe solve the query or not, return \"Solved\" or \"Unsolved\".\nQuery:\n{query}\nAnswer:\n{answer}\nNow give your reason in \"content\" and 'answer_status' of JSON to\ncheck_answer_status`."}, {"title": "C.4.2 Prompt template for verifying whether the query is solvable", "content": "<function>\n<name>check_task_solvable</name>\n<description>\nPlease check whether the given task solvable with following rules:\n1. If the 'query' provide invalid information (e.g. invalid email address or\nphone number), return \"Unsolvable\"\n2. If the \u2018query' needs more information to solve (e.g. the target restaurant\nname in a navigation task), return \"Unsolvable\"\n3. If you are unable to draw a conclusion, return \"Unsure\"\n4. If the currently available_tools' are enough to solve the query, return\nSolvable\"\nTask:\n{task}\nNow give your reason in \"content\" and \u2018task_status` of JSON to\ncheck_task_solvable`."}]}