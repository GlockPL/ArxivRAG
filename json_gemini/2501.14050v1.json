{"title": "GraphRAG under Fire", "authors": ["Jiacheng Liang", "Yuhui Wang", "Changjiang Li", "Rongyi Zhu", "Tanqiu Jiang", "Neil Gong", "Ting Wang"], "abstract": "GraphRAG advances retrieval-augmented generation (RAG) by structuring external knowledge as multi-scale knowledge graphs, enabling language models to integrate both broad context and granular details in their reasoning. While GraphRAG has demonstrated success across domains, its security implications remain largely unexplored. To bridge this gap, this work examines GraphRAG's vulnerability to poisoning attacks, uncovering an intriguing security paradox: compared to conventional RAG, GraphRAG's graph-based indexing and retrieval enhance resilience against simple poisoning attacks; meanwhile, the same features also create new attack surfaces. We present GRAGPOISON, a novel attack that exploits shared relations in the knowledge graph to craft poisoning text capable of compromising multiple queries simultaneously. GRAGPOISON employs three key strategies: i) relation injection to introduce false knowledge, ii) relation enhancement to amplify poisoning influence, and iii) narrative generation to embed malicious content within coherent text. Empirical evaluation across diverse datasets and models shows that GRAGPOISON substantially outperforms existing attacks in terms of effectiveness (up to 98% success rate) and scalability (using less than 68% poisoning text). We also explore potential defensive measures and their limitations, identifying promising directions for future research.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated exceptional capabilities in language understanding, reasoning, and generation [1, 7, 58]. However, their applications face key challenges due to training data limitations: while real-world knowledge evolves continuously, LLMs remain fixed at their training cutoff dates; further, their training data often lacks in comprehensive representation for specialized domains such as medicine and cyber-security. Such knowledge gaps often manifest as hallucinations and biases in answering temporal and domain-specific queries [4].\nTo tackle such limitations, retrieval-augmented generation (RAG) [21, 42] integrates LLMs with external knowledge bases. For each incoming query, RAG retrieves relevant information, adds it to the prompt, and generates responses using both the query and retrieved context, as illustrated in Figure 1. Particularly, GraphRAG [18,23,25,65] emerges as one leading RAG paradigm. By converting external knowledge (e.g., text corpora) to a multi-scale knowledge graph, where nodes and edges represent entities and their relations, along with graph community summaries and segmented text chunks, GraphRAG effectively integrates external knowledge to enhance LLM generation, substantially reducing hallucinations and biases [18].\nDespite success across various domains, RAG-based models are often vulnerable to adversarial poisoning attacks, due to their fundamental reliance on external information to construct knowledge bases [57]. These attacks, where adversaries inject carefully crafted malicious content into knowledge bases to compromise LLM generation, have been extensively studied for conventional RAG frameworks [12, 15, 67, 79]. In comparison, GraphRAG's security implications remain largely unexplored, raising key questions:"}, {"title": "RQ1: How does GraphRAG's vulnerability to adversarial poisoning compare to conventional RAG?", "content": "RQ1: How does GraphRAG's vulnerability to adversarial poisoning compare to conventional RAG?\nRQ2: What unique vulnerabilities does GraphRAG have?\nRQ3: What potential defensive measures exist?\nOur Work. To bridge this critical gap, we conduct a systematic study on GraphRAG's vulnerability to poisoning attacks, revealing the following key insights:\ni) GraphRAG's graph-based indexing and retrieval features enhance robustness against simple poisoning attacks. Recall that GraphRAG represents external knowledge as a multi-scale graph (e.g., entities, relations, and communities). The graph-based indexing and retrieval naturally defend against simple poisoning attacks: clean knowledge helps neutralize malicious content during indexing, while the graph structure effectively guides LLM reasoning and enables self-correction during inference. As a result, GraphRAG demonstrates strong resistance to existing poisoning attacks. GraphRAG's resistance is most evident at scale. With the increasing number of target queries, existing poisoning attacks [75] that generate query-specific malicious content become less impractical due to the prohibitive computational cost, and more detectable due to the large corpus of poisoned text [59, 77].\nii) Meanwhile, the same features create new attack surfaces. We present GRAGPOISON, an effective and scalable poisoning attack that exploits GraphRAG's graph-based indexing and retrieval. Intuitively, queries sharing relations in the knowledge graph can be attacked simultaneously. For instance, consider two queries \"How to mitigate the malware Stuxnet?\" and \"How to detect the malware Stuxnet\", both relying on the relation \u201cStuxnet uses DLL Injection\". Rather than attacking each query separately, injecting a false relation \"Stuxnet uses Process Hollowing\" into the knowledge graph allows GRAGPOISON to compromise both queries together, improving both attack effectiveness and scalability.\nSpecifically, GRAGPOISON assumes the adversary can only inject limited poisoning text into GraphRAG's text corpora, without access to GraphRAG's other components. As illustrated in Figure 3, GRAGPOISON crafts the poisoning text in three key steps. 1) Relation selection \u2013 It identifies critical relations shared across multiple target queries by analyzing their embedded relations; 2) Relation injection \u2013 For each selected relation, it generates a false substitute (e.g., replacing \"Stuxnet uses DLL Injection\" with \"Stuxnet uses Process Hollowing\"); 3) Relation enhancement \u2013 It further strengthens each injected relation by adding supporting relations (e.g., \"Process Hallowing is detectable by Process Creation\"). To resolve potential conflicts between poisoning and clean text, it employs an adversarial LLM to generate coherent narratives that naturally embed the malicious content.\nEmpirical evaluation across multiple GraphRAG models (e.g., GraphRAG [18] and LightRAG [23]) and datasets (e.g., geographic, medical, and cyber-security) demonstrates that GRAGPOISON substantially outperforms existing attacks in terms of attack effectiveness (achieving up to 98% success rate) and scalability (using 68% less poisoning text).\niii) GRAGPOISON is resilient to conventional defenses. We examine representative defenses against poisoning attacks, including leveraging LLMs' built-in knowledge to combat poisoning knowledge, paraphrasing incoming queries, and detecting false responses based on chain-of-thought (CoT) consistency. However, GRAGPOISON remains effective against these countermeasures, suggesting that GRAGPOISON exploits GraphRAG's fundamental vulnerabilities and requires tailored defenses.\nOur Contributions. To the best of our knowledge, this represents the first work on exploring GraphRAG's unique vulnerabilities to poisoning attacks. Our contributions are summarized as follows.\n\u2022 We show that compared to conventional RAG, GraphRAG exhibits higher robustness to simple poisoning attacks, due to its graph-based indexing and retrieval features.\n\u2022 We further reveal that these same features also create new vulnerabilities. We present GRAGPOISON, a novel attack tailored to GraphRAG that crafts poisoning text targeting multiple queries simultaneously. Empirical evaluation shows that GRAGPOISON significantly outperforms existing attacks in terms of both effectiveness and scalability.\n\u2022 We explore potential defensive measures against GRAGPOISON and their fundamental limitations, identifying several promising directions for future research."}, {"title": "2 Preliminaries", "content": "In this section, we introduce fundamental concepts and assumptions used throughout this paper. The important notations are summarized in Table 8."}, {"title": "2.1 GraphRAG", "content": "As illustrated in Figure 1, a RAG model uses the user query x to retrieve relevant knowledge z from a knowledge base KB and uses it as context (in addition to x) when generating the response y. Typically, it consists of two components, a retriever $p_\\eta(z|x)$ (parameterized by $\\eta$) that fetches relevant knowledge z, and a generator $p_\\theta(y|x,z)$ (parameterized by $\\theta$) that generates the response y based on the query x and the retrieved context z. At a high level, GraphRAG works in two phases: indexing and reasoning. Notably, an LLM is used throughout both phases (e.g., corpus parsing, entity extraction, and response generation).\nIndexing - While conventional RAG typically stores external knowledge (e.g., text corpora) as vectors optimized for similarity searches, GraphRAG converts it into a multi-scale knowledge graph, enabling complex entity relationship understanding and graph structure navigation. Typically, the"}, {"title": "2.2 Multi-Hop Reasoning", "content": "As GraphRAG organizes the knowledge base around entities and relations, we focus on multi-hop reasoning [29, 70], where answering queries requires synthesizing knowledge across multiple entities that may be either directly adjacent or connected through intermediate relations.\nWe focus on multi-hop reasoning for three key reasons. i) It requires models to process and reason across multiple text chunks, effectively measuring reasoning capabilities [29, 70]. ii) In the context of GraphRAG, multi-hop reasoning manifests as knowledge graph traversal, leveraging its capability of interpreting implicit relations between connected entities. iii) The interplay between multiple entities and relations introduces potential vulnerabilities to poisoning attacks.\nIn GraphRAG, where each query is potentially represented as a subgraph (query subgraph) in the knowledge graph, we define queries as related if their corresponding subgraphs share one or more relations. Queries that share relation r are referred to as r-dependent queries."}, {"title": "2.3 Threat Model", "content": "We define the threat model for GraphRAG poisoning attacks.\nAdversary's Objectives. The adversary aims to manipulate GraphRAG into producing incorrect responses for a given set of target multi-hop queries X. We consider two settings: untargeted attacks, where GraphRAG is misled to provide arbitrary incorrect answers, and targeted attacks, where GraphRAG is manipulated to generate specific incorrect responses predetermined by the adversary.\nAdversary's Capabilities. The adversary executes attacks by crafting poisoning text $D_{poison}$ that is appended to the clean text corpus $D_{clean}$, $D_{clean} \\cup D_{poison}$, which GraphRAG uses to build the knowledge base. The adversary cannot control any components of GraphRAG, including its indexing, retrieval, and generation processes. The adversary has access to an adversarial LLM (either open-source or via API).\nAdversary's Knowledge. The adversary has no knowledge of either the clean text corpus $D_{clean}$ or GraphRAG's internals, including its retriever $p_\\eta$ and generator $p_\\theta$.\nOverall, this threat model aligns with prior work on knowledge poisoning attacks [12, 15,79] and reflects the practical risks for GraphRAG models."}, {"title": "3 RQ1: GraphRAG versus Conventional RAG", "content": "We first compare the robustness of GraphRAG and conventional RAG to simple poisoning attacks."}, {"title": "3.1 Experimental Setting", "content": "RAG. We evaluate NaiveRAG [20,23] as the conventional RAG and GraphRAG [18] and LightRAG [23] as GraphRAG-based implementations. For GraphRAG and LightRAG, we use GPT-40-mini [32] as the underlying LLM. The default parameter setting is deferred to Table 10.\nAttacks. We use POISONEDRAG [80] as the representative poisoning attack, which generates poisoning text for each query by directly providing an incorrect answer.\nWhile white-box POISONEDRAG employs methods like Hotfilp [17], GCG [78], or Textfool [38] to optimize poisoning prefixes, these prefixes are often paraphrased or truncated during GraphRAG's indexing. Since GraphRAG's reasoning starts by computing similarity between queries and entity descriptions in the knowledge graph (\u00a72.1), rather than original text chunks, this white-box approach of minimizing prefix-query similarity proves ineffective for GraphRAG. Instead, we focus on black-box POISONEDRAG, which uses LLMs to generate poisoning text containing the targeted malicious response for each query, and concatenates the original query with the poisoning text. Under the default setting, POISONEDRAG generates 5 copies of poisoning text for each query, each limited to 30 tokens.\nDatasets. As GraphRAG excels at synthesizing knowledge across multiple disparate text fragments, standard question-answering (QA) benchmarks such as Natural Questions [41], HotpotQA [71], and MS-MARCO [47] do not fully exercise such capabilities. We thus construct three domain-specific multi-hop query datasets following [66]: i) geographical, ii) medical, and iii) cyber-security datasets. Using the approach from [71] to generate user queries, each dataset contains approximately 300 queries. The details of dataset construction are deferred to \u00a7B.\nMetrics. We measure attack effectiveness using the metric of attack success rate (ASR), defined as the fraction of successfully attacked target queries. Under untargeted attacks, the attack on query x is successful if GraphRAG's response \u0177 differs from the ground-truth answer y; under targeted attacks, the attack succeeds if \u0177 matches the adversary's desired answer $y^*$. Formally, for untargeted attacks,\n$ASR = \\frac{\\sum_{(x,y)\\in X} 1_{\\hat{y} \\neq y}}{|X|}$"}, {"title": "3.2 Experimental Results", "content": "As shown in Table 1, both GraphRAG and LightRAG demonstrate stronger resistance to POISONEDRAG compared to NaiveRAG across all settings. For example, on the Geographical dataset, the ASR against NaiveRAG is over 10% higher than against GraphRAG or LightRAG. This reduced attack effectiveness can be explained as follows.\nRecall that POISONEDRAG directly concatenates desired responses with target queries (or their declarative forms) in the poisoning text, ignoring intermediate entities in multi-hop queries. However, GraphRAG's use of LLMs to extract entity and relation descriptions during indexing can negatively impact poisoning effectiveness. Specifically, the LLM may omit critical information from the poisoning text during extraction. Further, even under controlled conditions (zero temperature and explicit prompting), the LLM tends to generate accurate descriptions when encountering both original and poisoning content in the same context window, as its deterministic nature prioritizes more reliable and coherent knowledge over inconsistent or conflicting information.\nMoreover, GraphRAG's two-phase reasoning process further limits poisoning effectiveness. First, it matches query x to descriptions of the top-k relevant entities $V_x$. Then, it traverses the knowledge graph to find connected entities, prioritizing relations $R_x$ based on the combined degrees of their endpoint entities. This prioritization neutralizes POISONEDRAG's strategy of adding new relational entities, as these newly added entities tend to have low degrees, and their connections are unlikely to be included in the final context.\nTherefore, we can conclude that due to its graph-based indexing and retrieval, GraphRAG is inherently more robust than conventional RAG to simple poisoning attacks."}, {"title": "4 GRAGPoison", "content": "Next, we introduce GRAGPOISON, a novel attack designed specifically for GraphRAG that addresses key limitations of existing attacks. Our attack innovates in two ways: it achieves higher effectiveness by poisoning relations rather than answers to exploit GraphRAG's graph-based retrieval, and it improves scalability by generating poisoning text that compromises multiple queries simultaneously."}, {"title": "4.1 Relation Selection", "content": "For a given set of target queries X, GRAGPOISON first identifies the entities and relations involved in X.\nIn the simple setting that the adversary is aware of the underlying knowledge graph, it is trivial to match each query $x \\in X$ to a subgraph in the knowledge graph and explicitly identify relations shared across queries. We focus on the setting that given target queries set X, the adversary must deduce the underlying subgraph corresponding to each $x \\in X$ without direct knowledge graph access. To achieve this, GRAGPOISON exploits the adversarial LLM's chain-of-thought (CoT) reasoning capability. With careful prompting (details in \u00a7D.2), the LLM breaks down each multi-hop query into step-by-step reasoning [63] and infers intermediate entities and relations. Further, the LLM identifies shared relations across queries by aggregating these intermediates, accounting for different references to the same entities and relations.\nFormally, for each query $x \\in X$, GRAGPOISON identifies $V_x$ and $R_x$ as entities and relations involved in x. To minimize the amount of poisoning text, GRAGPOISON strategically selects and poisons a subset of relations shared across multiple queries. We define that relation r \u201ccovers\" query x if $r \\in R_x$. This formulation reduces to the classical set cover problem [40]. To identify an (approximately) minimal subset of relations, GRAGPOISON employs a greedy algorithm as sketched in Algorithm 1, which iteratively selects the relation that covers the maximum number of previously uncovered queries, achieving the best possible polynomial-time approximation of the optimal subset."}, {"title": "4.2 Relation Injection", "content": "To poison each target relation $r \\in R$ identified in the previous step, GRAGPOISON injects a competing relation $r^*$ into GraphRAG's knowledge base to subvert its processing of r-dependent queries $X_r$. Specifically, for relation $r = (u_r, v_r)$ that connects entity $u_r$ to entity $v_r$, GRAGPOISON introduces a competing relation $r^* = (u_r, v^*)$ that links $u_r$ to a different entity $v^*$ (of the same entity type as $v_r$). Since this modification affects all queries in $X_r$ simultaneously, this attack is more efficient compared to existing attacks [79] that require query-specific poisoning. Next, we detail how to craft the poisoning text $d_{r^*}$ to achieve this goal.\nRecall that during GraphRAG's retrieval of entities $V(x)$ relevant to query x, each entity v is ranked based on its similarity to x, which is typically calculated based on the textual embeddings of x and v's description: $sim(emb(x), emb(v))$, where $sim(\\cdot, \\cdot)$ and $emb(\\cdot)$ denote the similarity (e.g., cosine) and embedding functions, respectively. Then, the entities most similar to x are selected.\nTherefore, by treating the poisoning text $d_{r^*}$ as a part of the"}, {"title": "4.3 Relation Enhancement", "content": "Unlike conventional RAG, GraphRAG additionally uses query x-relevant relations $R(x)$ and community summaries $S(x)$ in its response generation. This feature makes simple entity or relation injection ineffective, as $R(x)$ and $S(x)$ can interfere with and even neutralize the injected knowledge. To overcome this challenge, GRAGPOISON implements a relation enhancement strategy: it introduces additional poisoning text $d_r^+$ to create supporting relations that reinforce the presence of the injected relation $r^*$ and entity $v^*$ within both the retrieved relevant relations $R(x)$ and community summaries $S(x)$.\nDuring retrieval, GraphRAG identifies query x-relevant relations $R(x)$ through hierarchical ranking: it first retrieves all relations containing entities from the set of selected entities $V(x)$; it then categorizes them as either internal (both endpoint entities from $V(x)$) or external (only one endpoint entity from $V(x)$), with internal entities ranked higher than external ones; the relations in each category are further ranked by their endpoint entities' degrees. The highest-ranked relations are retrieved as $R(x)$. For community summaries, GraphRAG identifies the query-relevant set $S(x)$ by ranking communities based on their entity coverage of $V(x)$, where coverage is measured by the number of entities from $V(x)$ present in each community."}, {"title": "5 RQ2: GraphRAG's Unique Vulnerability", "content": "We leverage GRAGPOISON to exploit GraphRAG's unique vulnerability to poisoning attacks."}, {"title": "5.1 Experimental Setting", "content": "GRAGPoison. Under the default setting, for each target relation $r \\in R$, GRAGPOISON creates one competing relation $r^*$ and generates 3 distinct poisoning text samples for $r^*$; further, it creates 5 supporting entities for $r^*$ and generates their corresponding poisoning text. The experiments uses either GPT-40-mini or LLaMA 3.2-8B as the underlying adversarial LLM (with the temperature set to 0.1). Each piece of poisoned text is constrained to a maximum length of 30 tokens.\nMetrics. We use the following metrics in the evaluation.\nAttack Success Rate (ASR) \u2013 We evaluate attack effectiveness using the ASR metric (Eq. 1). For GRAGPOISON specifically, we also introduce relational-ASR (R-ASR), defined as:\nR-ASR = $\\frac{\\sum_{x \\in X} 1_{1_{r^* \\text{ appears in } \\hat{x}}}}{|X|}$\nwhich quantifies the proportion of queries where the injected relation $r^*$ appears in GraphRAG's reasoning process \u0177, measuring the effectiveness of relation injection.\nToken per Query (TPQ) \u2013 To evaluate attack efficiency and stealthiness, we measure the token count of the generated poisoning text. Specifically, we measure the token count per query (TPQ) by dividing the total number of tokens in poisoning text $D_{poison}$ by the number of target queries:\nTPQ = $\\frac{\\text{# tokens in } D_{poison}}{|X|}$\nQuery per Poisoning (QPP) \u2013 This metric calculates the average number of queries affected by each relational poisoning text, quantifying GRAGPOISON's capability to influence multiple queries simultaneously. For reference, POISONEDRAG achieves a baseline QPP of 1.\nClean Accuracy (ACC) \u2013 To evaluate the attack's potential side effect on GraphRAG's general performance, we measure GraphRAG's accuracy in answering randomly sampled queries that are not targeted by the attack. Specifically, we determine whether a query is impacted by comparing key substrings in its responses before and after the attack [31,49, 80]. All the other settings remain consistent with that in \u00a73."}, {"title": "5.2 Main Results", "content": "Table 2 compares the performance of GRAGPOISON and the baseline (POISONEDRAG) on GraphRAG across different datasets. We have the following findings.\ni) GRAGPoison is effective against GraphRAG. Notably, GRAGPOISON consistently outperforms POISONEDRAG in terms of attack effectiveness across different settings, which can be explained as follows.\nRecall that POISONEDRAG attempts to forge direct connections between target queries (or their declarative forms) and adversary-desired answers. While this approach proves effective against conventional RAG, it becomes less effective against GraphRAG due to its graph-based indexing and retrieval as well as the LLM's inherent preference for more reliable information (more details in \u00a73).\nIn contrast, GRAGPOISON takes a fundamentally different approach by exploiting GraphRAG's graph-based, hierarchical indexing and retrieval. Rather than creating direct query-answer associations, it subverts key relations and entities with carefully crafted alternatives. The attack's effectiveness stems from its focus on amplifying the presence of injected relations and entities within GraphRAG's retrieval across multiple levels: individual entities, relations, and communities.\nAlso, note that the strong correlation between GRAGPOISON's R-ASR and ASR across diverse settings confirms that its effectiveness primarily stems from the substitution of critical relations with alternatives. Further, as evidenced by high ACC, GRAGPOISON maintains GraphRAG's general performance, as its relation-based attack strategy has negligible impact on non-targeted queries.\nii) GRAGPoison is scalable in terms of poisoning text requirement. GRAGPOISON achieves high ASR through an efficient strategy: targeting relations shared by multiple queries, thus eliminating the need for query-specific poisoning. This contrasts with POISONEDRAG, which requires distinct poisoned text for each target query and must embed the query itself to enhance retrieval probability. This fundamental difference leads to substantially different token efficiency. Specifically, GRAGPOISON attains substantially lower TPQ compared to POISONEDRAG. This efficiency is particularly evident on the Geographical dataset, where GRAGPOISON outperforms POISONEDRAG in terms of ASR, while POISONEDRAG consumes 1.5\u00d7 more tokens. GRAGPOISON's token utilization is also reflected by its QPP measure, which ranges from 2.3 to 3.4 across different datasets, significantly improving upon POISONEDRAG's baseline QPP of 1.\niii) GRAGPoison's effectiveness scales with the adversarial LLM's capability. When comparing GRAGPOISON's performance with different adversarial LLMs, GRAGPOISON achieves lower ASR with Llama 3.2-8B than with GPT-40. This gap mainly stems from Llama's higher error rate in parsing target queries to identify involved entities and relations, which further impairs relation injection and enhancement steps. Interestingly, despite being allocated the same token budget for poisoning text generation, Llama consumes fewer tokens than GPT-40 while achieving comparable performance on the medical and cyber datasets. This suggests that while GRAGPOISON's performance tends to improve with the adversarial LLM's capability, high-performant LLMs are not essential for executing GRAGPOISON successfully.\niv) GRAGPoison has negligible impact on GraphRAG's general performance. Both attacks maintain 100% clean accuracy across different settings. Notably, while POISONEDRAG achieves this through query-specific poisoning text, GRAGPOISON targets query-relevant relations directly, ensuring the rest of the knowledge graph remains unaffected."}, {"title": "5.3 Ablation Study", "content": "We perform an ablation study to evaluate how each key component of GRAGPOISON contributes to its effectiveness."}, {"title": "5.3.1 Knowledge Graph Awareness", "content": "Under the default setting, we assume the adversary operates without access to GraphRAG's knowledge graph and must infer entities and relations solely from target queries, which we define as the KG-agnostic scenario. We then analyze GRAGPOISON's performance under the KG-aware scenario, where the adversary knows the query graph for each target query, enabling precise identification of related queries and targeted manipulation of GraphRAG's behavior."}, {"title": "5.3.2 Attack Magnitude", "content": "We then analyze the impact of attack magnitude on GRAGPOISON's performance. Specifically, we control the attack magnitude through three key parameters: i) the number of poisoning text variants per relation injection, ii) the number of supporting relations per relation injection, and iii) the total length of poisoning text.\nNumber of Poisoning Text Variants. We examine how the number of poisoning text variants per relation injection $r^*$, denoted as $N_\\alpha$, affects GRAGPOISON's performance. As shown in Figure 5, increasing $N_\\alpha$ from 1 to 3 substantially improves ASR across all datasets, but further increases beyond $N_\\alpha$ = 3 yield only marginal improvement. This pattern of diminishing returns suggests that once the injected relation $r^*$ (and entity $v^*$) is retrieved by GraphRAG, adding additional poisoning text variants do not significantly improve attack success probability."}, {"title": "5.3.3 \"Tricks\" of Relation Injection", "content": "We evaluate the effectiveness of various optimization strategies (referred to as \u201ctricks\u201d) employed in GRAGPOISON's relation injection process. As noted in \u00a73, directly optimizing the similarity between the generated poisoning text $d_{r^*}$ and target query x presents significant challenges. To address this, we implement the following optimizations to enhance poisoning text generation:\n\u2022 Entity selection - Guide the LLM to identify an entity $v^*$, whose attributes closely match the target entity $v_r$;\n\u2022 Explicit negation \u2013 Establish that the injected relation $r^*$ explicitly supersedes and invalidates the original relation r;\n\u2022 Temporal ordering - Specify that $r^*$ occurs chronologically after r to encourage GraphRAG to prioritize $r^*$;\n\u2022 Contextual explanation \u2013 Provide a plausible explanation for this supersession.\nWe perform an ablation study to evaluate the contribution of each optimization on GRAGPOISON's ASR. Table 4 summarizes the results. We have the following observations.\nEliminating the entity selection optimization leads to a substantial decline in average ASR by 4.6%, with the geographic dataset experiencing the most severe impact at 9.1%. This decline shows the critical importance of semantic similarity in entity selection. Consider the example of modifying \"Stuxnet utilizes DLL injection\". Substituting \"DLL Injection\" by \"Process Hollowing\" enables more credible narratives as \"Process Hollowing\" is also an attack technique and this change can be interpreted by \"the update of Stuxnet\".\nEliminating the explicit negation optimization results in a 6.8% average reduction in GRAGPOISON'S ASR, with the medical dataset showing the most significant decline at 10.7%. This optimization plays a vital role in directing the LLM's reasoning by preventing direct logical conflicts between entities. For example, both the \"DLL injection\" and \"Process Hollowing\" will be retrieved as attack techniques utilized by \"Stuxnet\" without the negation trick. However, this creates a logical conflict since these techniques cannot be used simultaneously by the same malware due to their conflicting mechanisms.\nEliminating the temporal ordering optimization results in a 1.4% performance decrease. This optimization leverages dates beyond the LLM's training cutoff to reduce the model's reliance on its training data when processing GraphRAG-extracted context. By positioning events outside the model's known timeline, we increase the likelihood that it will prioritize the poisoning text, thereby enhancing attack effectiveness.\nEliminating the contextual explanation optimization cause an average ASR reduction of 13.5%, with the medical dataset experiencing the most severe decline at 23.5%. This optimization enhances the attack's effectiveness by strengthening narrative credibility and increasing the LLM's likelihood of prioritizing the poisoning knowledge.\nAdditionally, we employ a text shuffling strategy to improve attack effectiveness. Recall that for each relational poisoning, GRAGPOISON generates multiple pieces of poisoning text (i.e., $N_\\alpha$ text variants and $N_\\beta$ supporting relations). During GraphRAG's indexing phase, entities and relations are extracted from text chunks. However, when multiple pieces of poisoning text for the same relation appear together in a chunk, the LLM only has one opportunity to extract them, and its inherent randomness may cause it to miss some relations during this single extraction attempt. By distributing poisoning text across different chunks through shuffling, we reduce systematic extraction failures and improve successful relation injection into the knowledge graph. As shown in Table 4, this optimization proves significant: disabling text shuffling decreases average ASR by about 4.5%."}, {"title": "5.4 Extension", "content": "5.4.1 Targeted Attacks\nWhile our previous evaluation examine untargeted attacks, where GraphRAG is induced to generate arbitrary incorrect responses, we now analyze extending GRAGPOISON to targeted attacks, where the adversary aims to elicit specific, predefined incorrect answers from GraphRAG.\nTo adapt GRAGPOISON for targeted attacks, we maintain the relation injection step: substituting injected relation $r^* = (u_r, v^*)$ for original relation $r = (u_r, v_r)$ shared by multiple target queries. However, we modify the relation enhancement step. Rather than selecting an arbitrary supporting entity $v^+$ to connect to $v^*$, we set $v^+$ as the adversary's predefined answer for a particular query x. This creates a direct \"shortcut\u201d in GraphRAG's reasoning path from v to the adversary's desired answer $v_t$."}, {"title": "5.4.2 Alternative GraphRAG", "content": "To evaluate GRAGPOISON's broader applicability, we test it against LightRAG [23], a lightweight variant of GraphRAG. As shown in Table 6, GRAGPOISON attains comparable ASR across both models. This consistent performance across implementations suggests that the vulnerability exploited by GRAGPOISON represents inherent vulnerability shared by graph-based RAG models, enabling us to analyze them within a unified framework."}, {"title": "6 RQ3: Potential Defenses", "content": "Having demonstrated GRAGPOISON's effectiveness against GraphRAG, we now explore potential defenses against GRAGPOISON. Specifically, we evaluate the following strategies: i) paraphrasing incoming queries, ii) leveraging the LLM's internal knowledge as a reference, iii) detecting suspicious responses, and iv) identifying poisoning text in the corpus."}, {"title": "6.1 Query Paraphrasing", "content": "AS GRAGPOISON crafts poisoning text with reference to target queries, one straightforward defense is to paraphrase the incoming query before feeding it to GraphRAG. For each incoming query, we use GPT-40 to generate 5 paraphrased"}, {"title": "6.2 LLM Knowledge Referencing", "content": "In its default configuration, GraphRAG generates responses mainly from the provided knowledge base, using the following instruction in its prompt:\nincorporating any relevant general knowledge.\nIf you don't know the answer, just say so. Do not make anything up. Do not include information where the supporting evidence for it is not provided.\nDue to these constraints, GraphRAG minimizes its reliance on the LLM's internal knowledge during generation. We experiment with removing the bold portion of the prompt to allow GraphRAG to incorporate the LLM's knowledge. However, we avoid adding explicit verification instructions, because in practice GraphRAG tends to prioritize knowledge base over unverifiable LLM knowledge. This creates an in-"}, {"title": "6.3 CoT Consistency-based Detection", "content": "We also explore detecting suspicious responses generated by GraphRAG as a possible defense. When poisoning text appears in the context window, it may disrupt the LLM's response generation, potentially leading to inconsistencies across multiple generations due to conflicts between poisoning and legitimate content.\nTo evaluate this defense, we maintain GraphRAG's original framework (ensuring consistent context per query) while introducing response variation by increasing the LLM's temperature to 0.3. For each query, we use GraphRAG to generate 3 responses and analyze their consistency.\nWhile direct comparison of semantic similarity between generated responses can be unreliable due to variations in surface-level wording, analyzing the underlying reasoning process offers a more robust approach. We therefore employ an auxiliary evaluation method that uses an LLM to examine the CoT [64] for each query-response pair (prompting details in \u00a7D.3). By assessing the consistency of these CoTs across the 3 responses, we can better detect the presence of poisoning text in the context. Divergent CoTs may suggest that poisoning text is influencing and destabilizing the reasoning process, while consistent CoTs indicate either an absence of poisoning text or that its impact is negligible."}, {"title": "6.4 Poisoning Text Identification", "content": "GRAGPOISON differs from conventional LLM poisoning attacks [5, 8, 22, 52] as it targets the knowledge corpus rather than training data. This makes traditional poisoning detection methods ineffective. Instead, we focus on identifying poisoned text directly within the source corpus.\nPerplexity serves as a widely adopted metric for assessing text quality and defending against LLM attacks [3,9,36,80]. Intuitively, higher perplexity often correlates with lower text quality. Prior work suggests that LLM-generated text typically exhibits higher perplexity than human-written content [30]. AS GRAGPOISON relies on the LLM to generate poisoning text, the resulting text may show quality characteristics that make it more detectable through perplexity-based analysis."}, {"title": "7 Related Work", "content": "We survey literature relevant to this work in three categories: RAG and its variants, poisoning attacks on RAG, and defenses against such attacks.\nRAG and Variants. The RAG framework is designed to enhance the generation accuracy of LLMs [20, 46"}]}