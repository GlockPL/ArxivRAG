{"title": "Enhancing material property prediction with ensemble deep graph convolutional networks", "authors": ["Chowdhury Mohammad Abid Rahman", "Ghadendra Bhandari", "Nasser M Nasrabadi", "Aldo H. Romero", "Prashnna K. Gyawali"], "abstract": "Machine learning (ML) models have emerged as powerful tools for accelerating materials discovery and design by enabling accurate predictions of properties from compositional and structural data. These capabilities are vital for developing advanced technologies across fields such as energy, electronics, and biomedicine, potentially reducing the time and resources needed for new material exploration and promoting rapid innovation cycles. Recent efforts have focused on employing advanced ML algorithms, including deep learning - based graph neural network, for property prediction. Additionally, ensemble models have proven to enhance the generalizability and robustness of ML and DL. However, the use of such ensemble strategies in deep graph networks for material property prediction remains underexplored. Our research provides an in-depth evaluation of ensemble strategies in deep learning - based graph neural network, specifically targeting material property prediction tasks. By testing the Crystal Graph Convolutional Neural Network (CGCNN) and its multitask version, MT-CGCNN, we demonstrated that ensemble techniques, especially prediction averaging, substantially improve precision beyond traditional metrics for key properties like formation energy per atom (\u0394\u0395), band gap (Eg) and density (p) in 33,990 stable inorganic materials. These findings support the broader application of ensemble methods to enhance predictive accuracy in the field.", "sections": [{"title": "I. INTRODUCTION", "content": "Predicting material crystal properties involves forecasting the chemical and physical traits of crystalline materials based on their molecular and atomic structures. This task is vital for fields like electronics, medicine, aeronautics, and energy storage management [1\u20134]. Accurately predicting these properties from compositional and structural data is instrumental in the discovery of new materials for advanced technologies. Using computational methods and data-driven strategies, researchers can efficiently explore and optimize material designs, avoiding the slow and costly process of experimental trial-and-error [5]. Understanding how a material's crystalline structure impacts its properties requires a blend of computational and experimental investigations. Although density functional theory (DFT) [6, 7] is a well-established method, it is often perceived as computationally complex and time-intensive. Moreover, the quest for materials with specific properties within an extensive material search space poses challenges, and delays progress in swiftly evolving domains like medical science, aeronautics, and energy engineering, where accuracy and speed are paramount.\nMachine learning (ML)-based models have emerged as a promising solution to this challenge. These models can match the accuracy of DFT calculations and support rapid material discovery, thanks to the growing material databases [8\u201310]. By harnessing the capabilities of ML algorithms and the growing abundance of data in material repositories, these models can effectively navigate the vast material landscape and pinpoint promising candidates with desired properties. This data-driven approach for predicting material crystal properties has shown promise and gained significant attention for its accuracy and unparalleled speed [11].\nHowever, the intricate atomic arrangements and the intrinsic correlations between structure and properties present formidable challenges for ML models to encode pertinent structural information accurately, mainly because of the number of atoms involved and the internal degrees of freedom within the crystal structures. Thus, effective representation capturing spatial relationships and periodic boundary conditions within a crystal lattice becomes challenging. Furthermore, a model adept with one crystal structure might falter with another, given crystals' inherent periodicity and symmetry. Also, traditional ML models often fail to incorporate the nuanced knowledge of unit cells and their repetitive nature, an essential aspect of crystallography [12].\nMoreover, for ML models the challenge of representing crystal systems, which vary widely in size, arises because these models typically require input data in the form of fixed-length vectors. To address this, researchers have developed two main approaches. The first involves manually creating fixed-length feature vectors based on basic material properties [13, 14], which, though effective, necessitates tailored designs for each property being predicted. The second approach uses symmetry-invariant transformations [15] of the atomic coordinates to standardize input data, which, while solving the issue of variable crystal sizes, complicates model interpretability due"}, {"title": "II. RELATED WORK", "content": "Not having regular grid-like structures such as images or 1-D signals, CNN could not be the automatic choice for studying material structure from DL point of view. Rather its irregular structural shape made it a suitable candidate for graph representation and graph neural network (GNN), where atoms are considered nodes and atomic bonds edges. Therefore, convolving on the graph structure which is converted from the actual atomic structure of the material was a prominent choice for the researchers. Crystal Graph Convolution Neural Network (CGCNN) [19] and SchNet [21] first proposed this graph representation and utilized raw features like atom type and atomic bond distance to predict material properties comparable with DFT computed values. Although these models performed very well in terms of predicting material's properties they also exhibit inevitable and notable challenges due to their reliance on distance-based message-passing mechanisms [22]. Firstly, neglecting many-body and directional information can overlook critical aspects important for understanding material properties. Secondly, the reliance on nearest neighbors to define graph edges could misrepresent key interactions due to the ambiguity of chemical bonding. Lastly, the models are limited by their receptive field, compounded by issues like over-smoothing and over-squashing, which restrict their ability to account for the long-range or global influence of structures on properties.\nThe more recent DL models for material property prediction such as Atomistic Line Graph Neural Network (ALIGNN) [11], iCGCNN [23],MatErials Graph Network (MEGNet) [12], Orbital Graph Convolution Neural Network (OGCNN) [24], DimeNet [25], GemNet [26] and Geometric-information-enhanced Crystal Graph Neural Network (Geo-CGNN) [27] thus tried to incorporate more geometrical information like bond angle, orbital interaction, body order information, directional information, distance vector to outperform previously proposed distance-based models. Some models proposed attention mechanisms and self supervised learning (SSL) as well to choose the relative importance of features in predicting material properties like Matformer [28], Equiformer [29], GATGNN [30], Crystal-Twins [31] and DSSL [32].\nIt is a well-known fact that Deep learning models for complicated tasks navigate a high-dimensional space to minimize a function that quantifies the 'loss' or error between the actual data and the expected results. We refer to this optimization landscape as the 'loss landscape' [18, 33, 34]. It is frequently represented visually as a surface or landscape with hills and valleys. The loss landscapes of deep neural networks are extremely intricate and non-convex. This indicates that while the model may converge to numerous local minima (valleys), not all of them will result in the best solution (global minimum). The training epoch in which the model achieves the lowest validation loss is referred to as the 'best-validated epoch'. Nevertheless, the model at this epoch may not truly represent the best generalizable model because of the complexity of the loss landscape and elements like overfitting and identical loss with differences in functional space [35\u201337]. Though they may have a little larger or the same loss, models from other epochs may perform better on unknown data or have superior generalization ability for having differences in loss dynamics. Therefore, our focus in this work is not on the GNN models or associating features with them to strengthen material property prediction but on creating a generalized framework of ensemble models based on the cross-validation loss trajectory that might yield better generalization of the prediction task with improved accuracy of prediction.\nThe comprehensive approach of any ensemble technique is to accumulate a set of models using a function. It has widely been used to strengthen the performance of ML and DL models. There have been a number of approaches that can be followed to ensemble the power of various models in DL. For example, diversity in base classifiers for ensembles is achieved through two distinct approaches, depending on whether the ensemble is composed of models of the same type (homogeneous) or different types (heterogeneous) [38]. Different data fusion methods like max-voting, average voting, weighted average voting, and meta-learning have been proposed [39]. Also, four types of ensemble techniques are normally adopted in literature such as bagging, boosting, sorting, and decision fusion [38]. Different ensemble techniques are already being used to improve the overall performance of the single-base models in various arenas of deep learning. Although the ensemble is a highly studied topic, it is mostly used in fields like speech recognition [40], health-care [41], natural language processing [42], and computer vision [43]. Compared to these works, our work is the first to consider the ensemble strategies for material property prediction tasks, and our ensemble framework is simple and considers the ensembling by aggregating models across different training stages."}, {"title": "III. METHODOLOGY", "content": "In this section, we first provide background details about the GCNN framework for material property prediction, and then introduce our ensemble framework to achieve enhanced predictions.\nGraph Convolutional Neural Networks (GCNNs) have emerged as a powerful tool in materials science, enabling researchers to analyze and predict the properties and behaviors of materials in a novel and efficient manner. Unlike traditional convolutional neural networks that process grid-like data (e.g., images), GCNNs are designed to handle graph-structured data, which is intrinsic to the representation of atomic and molecular structures in materials science. These models exploit the graph structure of materials, where nodes can represent atoms and edges can denote chemical bonds or spatial relationships. By doing so, GCNNs can capture both the local and global structural information of materials.\nCrystal Graph Convolutional Neural Networks (CGCNN) [19] and SchNet [21] represent the two most prominent graph neural network architectures tailored for material science applications. These models refine atom representations within a structure by considering the types and bond lengths of neighboring atoms. Subsequently, they aggregate these updated atom-level representations to form a comprehensive representation of the entire structure. In CGCNN, the crystal architecture is represented as a graph that accommodates the details of atoms and their bonds with neighbors, and a graph convolution network is constructed on such graph to attain the representations useful to material property prediction. The architecture can comprise of single-task head [19] or multi-task [20] head depending on the application. We present the overall diagram combining the single- and multi-task setup in Fig. 1. In the presented network, the atom feature encoding vector can be noted as vi where i is an atom or node in the graph. Similarly, an edge or atomic bond among neighbors is denoted as (i,j)k representing kth bond between atoms i and j, and its feature vector is U(i,j)k. The goal here is to update the atom feature vector vi by iterative convolution with neighbors and bonds (1) and generate a comprehensive feature vector for the crystal structure by pooling (2).\n$v_i^{(t+1)} = Conv (v_i^{(t)}, v_j^{(t)}, U_{(i,j)k}), (i, j) \\in G.$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               (1)\n$\\vec{v_c} = Pool(\\vec{v_0}, \\vec{v_1}, ..., \\vec{v_N}, \\vec{v_N^{(R)}})$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         (2)\n$min_W J(y, f(C; W))$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    (3)\nThe training procedure involves minimizing a cost function J(y; \u0177) where y is the DFT computed value and \u0177 is the prediction of the model. Therefore, CGCNN can be considered a function that tries to approximate the actual property value y by mapping a crystal C employing weights W as shown in Eq. (3).\nFor prediction-based ensemble modeling, we first calculate the prediction for each \u0472(t) model, within the top-n as:\n$\u0177_t = f(x; O_t)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (5)\nand create prediction ensemble as:\n$\\vec{Y_{prediction-ensemble}} = \\frac{1}{n}\\sum_{i=1}^{n} \\vec{\u0177_{n}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                (6)\nHere, we first aggregate top-n models together by creating an ensemble model \u0398avg:\n$\\vec{O_{avg}} = \\frac{1}{n}\\sum_{j=1}^{n} \\vec{O^{(j)}}$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          (7)\nThe final prediction of the ensemble model for a new input x is then given as:\n$\u00dd_{model-ensemble} = f(x; \u0472_{avg})$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (8)"}, {"title": "IV. EXPERIMENT", "content": "In this section, we discuss the data we have used for experiments, the implications of choosing the properties we worked with, the outline of the experimental setup, and the results with their interpretation, significance, and implications.\nAll the models in this study were trained on the dataset from the Materials Project [8]. The Materials Project is a multi-institutional, multinational effort to compute the properties of all inorganic materials and provide the data and associated analysis algorithms to every materials researcher free of charge. One of the largest and most popular three-dimensional (3D) materials datasets in the materials science field, the Materials Project collection covers approximately 155,000 materials, including a wide variety of inorganic compounds. This broad coverage guarantees a representative and varied sampling of material kinds, extending the generalizability of our results. Moreover, the MP dataset has been successfully used in numerous studies to create and evaluate predictive models for a range of material properties. The trustworthiness of the dataset and the accomplishments of earlier studies employing MP data highlight its suitability for verifying our methods. For this study, we worked with 33,990 stable materials, which refers to the set of materials that, under standard conditions, have a low energy state and are hence thermodynamically favorable to exist in their current form. An indicator of stability is the 'energy above hull' metric, which shows how much energy would be released by each atom in the event that the material changes into the most stable phase combination. For several reasons, it is critical for this work to concentrate on stable materials. Stable materials are more important for real-world applications because they are thermodynamically favored, meaning they occur naturally or can be created with less energy. Focusing on these materials allows the research to directly target materials that are useful in energy storage, electronics, and catalysis, among other real-world applications. To demonstrate the efficacy of the proposed ensemble framework, we focused on three distinct properties: Formation energy per atom (\u0394\u0395), Density (p), and Bandgap (Eg). These three properties are important because formation energy determines the thermodynamic stability of a material, bandgap signifies whether a material is an insulator, conductor, or semiconductor, and density defines the stiffness.\nFor exploring the efficacy of our proposed ensemble model, we primarily consider CGCNN [19] and its multitask extension, MT-CGCNN [20], as base models, and apply our proposed ensembling to construct the ensemble framework. Furthermore, we vary the number of convolutional layers within CGCNN and MT-CGCNN to create two separate versions of the network for each category. Throughout our experiments and results, we refer to them as CGCNN1 (number of convolutional layers, nc = 3), CGCNN2 (nc = 5), MT-CGCNN1 (nc = 3), and MT-CGCNN2 (nc = 5). For the MT-CGCNN models, our multi-task objective involved predicting all three properties together using three separate heads, and optimization weights of 1.5 were used for \u2206\u0395\u0192, 3 for Eg, and 1.5 for p. Although we present both prediction and model ensemble frameworks, we found the prediction ensemble to yield the best results. Therefore, we utilize the prediction ensemble for all our analyses and comparisons against the Best-val model. However, it is important to note that we also compare the performance between prediction and model-based ensembles to establish the efficacy of the prediction average ensemble over the latter.\nAcross all models, the length of the atom feature vector was set to 64, and the hidden feature vector to 128. For all experiments, MSE was used as the loss function, SGD as the optimizer, and a fixed learning rate of 0.01 was applied. We utilized an NVIDIA GeForce GTX TITAN X graphics processing unit (GPU) with 12 GB of memory for all model training and evaluation tasks. The training, validation, and test data were randomly selected from 33990 data points and were kept consistent across all experiments, employing a 70-10-20 distribution strategy, and the batch size was uniformly set at 256. This random selection ensures that our model is not biased towards any specific subset of data, thereby enhancing its ability to generalize to unseen data. Our sampling strategy did not follow any particular distribution, ensuring that the training, validation, and test sets were representative of the overall dataset. By not constraining the sampling process to a specific distribution, we mitigate the risk of overfitting to particular patterns in the training data, thus improving the model's robustness. All models were run for 100 epochs, and up to 50 models were considered for the ensemble. To determine the best model among the baseline models, and to select several models for creating the ensemble, MSE loss on validation data was used. However, for reporting results, we used MAE on the test dataset to follow standard practice in the literature."}, {"title": "V. RESULT", "content": "We present our main results in Table I and Table II. For results in both tables, we present outcomes from the prediction ensemble for a fixed number of epochs (20 for single-task and 40 for multi-task) as determined by validation performance. In Table I, we compare the ensemble framework (prediction ensemble) against CGCNN1 and CGCNN2 across all three properties. We found that, out of six different settings, our proposed ensemble framework achieved better results in five settings. Moreover, in one of the settings where the standard approach achieved a better result, the gap between our results and the baseline was the smallest (0.69%) compared to other margins (4% to 8%). In Table II, we observed enhanced results by our proposed framework in all six different settings, with the improvement margin up to 11%. In Fig. 3, we analyze the effect of the prediction ensemble across a different number of models. The leftmost point, representing the number of models used to create an ensemble as one, is the best validation model and represents the standard practice of validation using a single model. Compared to that single point, every result to the right represents our proposed approach of using an ensemble-based framework. As seen in both single-task (left panel) and multi-task (right panel) and across all three properties, the effect of ensembling for enhancing property prediction is quite evident. It should be noted that in some cases, such as the formation energy per atom for the single-task model, the behavior of the ensemble framework appears not to be as effective after a certain number, but we can still see that the ensemble framework is always better or similar to the best validation model. This also underscores the importance of the ensemble framework in achieving enhanced prediction results.\nWe also explore the impact of the ensemble-based framework across different regions of the material property spectrum. This analysis is crucial for a comprehensive understanding of the various properties involved in our study. For example, specific regions of the bandgap determine a material's suitability as a conductor, semiconductor, or insulator. Grasping the range of formation energy per atom is vital, as it reflects the stability and synthesizability of materials. Materials that are thermodynamically stable, and therefore more likely to occur in nature or be successfully synthesized in the laboratory, are distinguished by lower formation energies. Additionally, pinpointing the extremes within density ranges aids in assessing the durability of high-density materials or the practicality of lightweight insulators with low density.\nWe present the results in Fig.4 and Fig.5, where we partitioned the test data from the 10th to the 90th percentile in both top-bottom (left panel) and bottom-top (right panel) distributions. This approach helps to identify performance differences across different regions of the property spectrum for CGCNN\u2081 and MT-CGCNN\u2081 models. It is observed that the ensemble model, in all instances, aligns with the trend of the original single best model, exhibiting a reduced MAE value or improved accuracy across all percentiles of data distribution in most scenarios. Notably, significant improvement is observed in certain regions, for example, for the bottom 10% of bandgap materials in multi-task models.\nFinally, although our experiment clearly demonstrated the benefit of a prediction ensemble over a model ensemble, in Fig. 6, we include an analysis in this paper that compares the performance between a prediction ensemble and a model ensemble for both single-task and multi-task frameworks for band gap. As shown in the figure, although the performances of both ensemble approaches appear similar in the multi-task settings (right panel), for the single-task model, we observe that the model ensemble resulted in the worst performance, even when compared to the single best validation model."}, {"title": "VI. CONCLUSION", "content": "In this paper, we explore the impact of an ensemble framework on the task of material prediction. Our proposed framework is both simple and effective, leveraging the loss landscape of deep neural network training without requiring computationally expensive ensemble strategies. We present two types of ensembles: prediction-based and model-based. The former involves aggregating predictions across different models to form the ensemble model, while the latter aggregates the models first, then generates a single prediction as the ensemble result. Our analysis shows that the prediction ensemble consistently outperforms the model ensemble. As a result, we conducted a comprehensive analysis across various models (single-task vs. multi-task), architectures (variations in GCNN depth), and properties, finding that the prediction ensemble almost always improves the predictive performance over the single best validation model. We also examined the efficacy of our proposed framework across different property spectra for test data distribution. Overall, our extensive analysis demonstrates the robustness of the proposed framework in generating enhanced predictive results. Future work will focus on investigating systematic approaches for calculating the number of models to select candidate models for the ensemble framework, extending the analysis to include other GNN models, and on expanding our results to 2D datasets."}]}