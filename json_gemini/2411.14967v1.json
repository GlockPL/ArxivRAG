{"title": "SwissADT: An Audio Description Translation System for Swiss Languages", "authors": ["Lukas Fischer", "Yingqiang Gao", "Alexa Lintner", "Sarah Ebling"], "abstract": "Audio description (AD) is a crucial accessibility service provided to blind persons and persons with visual impairment, designed to convey visual information in acoustic form. Despite recent advancements in multilingual machine translation research, the lack of well-crafted and time-synchronized AD data impedes the development of audio description translation (ADT) systems that address the needs of multilingual countries such as Switzerland. Furthermore, since the majority of ADT systems rely solely on text, uncertainty exists as to whether incorporating visual information from the corresponding video clips can enhance the quality of ADT outputs. In this work, we present SwissADT, the first ADT system implemented for three main Swiss languages and English. By collecting well-crafted AD data augmented with video clips in German, French, Italian, and English, and leveraging the power of Large Language Models (LLMs), we aim to enhance information accessibility for diverse language populations in Switzerland by automatically translating AD scripts to the desired Swiss language. Our extensive experimental ADT results, composed of both automatic and human evaluations of ADT quality, demonstrate the promising capability of SwissADT for the ADT task. We believe that combining human expertise with the generation power of LLMs can further enhance the performance of ADT systems, ultimately benefiting a larger multilingual target population.", "sections": [{"title": "1 Introduction", "content": "AD denotes the process of acoustically describing relevant visual information that renders streaming media content in television or movies and other art forms partly accessible to blind persons and persons with visual impairment (Bardini, 2020; Wang et al., 2021; Ye et al., 2024). This service involves the creation of textual descriptions, so-called \u201cAD scripts\", of key visual elements of a scene, such as actions, environments, facial expressions, and other important details that are not conveyed through dialogue, sound effects, or music (Snyder, 2005; Mazur, 2020). They are typically inserted into natural pauses that do not interfere with the ongoing narration. AD scripts are voiced by a professional human speaker or synthesized by a computer and mixed with the original audio.\nDespite recent advancements in multilingual machine translation (Liu et al., 2020; Xue et al., 2021) and Large Language Models (LLMs) research (Brown et al., 2020; Achiam et al., 2023), two major challenges remain unsolved in developing well-performing ADT systems. Firstly, many ADT systems are built on pre-trained machine translation models that need texts in both the source and target languages as inputs. Training these ADT systems requires large amounts of manually crafted data, leading to high operational costs (Ye et al., 2024). Secondly, existing ADT systems are predominantly text-only machine translation models, neglecting the visual modality which is paramount for the ADT task and has proven to be useful as part of multimodal machine translation (Li et al., 2021).\nIn Switzerland, the primary target group of AD users comprises approximately 55,000 blind persons and 327,000 persons with visual impairment (Spring, 2020). Switzerland's multilingual population poses a challenge, as AD scripts have to be produced parallelly in several languages. This highlights the need of implementing multilingual ADT systems.\nIn this work, we address the aforementioned challenges by developing an ADT system specifically for the three main languages of Switzerland, i.e., German, French, and Italian. To create training data for LLM-based ADT models with minimal\""}, {"title": "2 Related Work", "content": "The automatic generation of ADs from video clips has been explored by both the natural language processing (NLP) and computer vision (CV) communities. This research is often conducted as part of tasks such as video captioning (generating descriptive text for a video) or video grounding (temporally aligning a text query with video segments).\nIn recent years, several datasets and models for ADs have been published, where many of them are movie subtitles or video descriptions (Chen and Dolan, 2011; Lison and Tiedemann, 2016; Xu et al., 2016; Lison et al., 2018). Oncescu et al. (2021) proposed QuerYD, an open-source dataset created for the text-video retrieval and event localization tasks, where ADs and video segments are annotated by human volunteers. Soldan et al. (2022) presented MAD, a large-scale benchmark dataset for video-language grounding, aggregated by aligning ADs with their temporal counterparts in videos. Zhang et al. (2022) introduced MovieUN, a large benchmark specifically designed for the movie understanding and narrating task in Chinese movies. Han et al. (2023b) released AutoAD, a model that leverages both text-only LLMs and multimodal vision-language models (VLMs) to generate context-conditioned ADs from movies. In another work of theirs (Han et al., 2023a), the authors further developed an extended model to address three crucial perspectives of AD generation, i.e., actor identity (who), time interval (when), and AD content (what). Despite benefiting from existing large-scale corpora and state-of-the-art research in NLP and CV, these works are limited to monolin-"}, {"title": "3 SwissADT: An ADT System for Swiss Languages", "content": "SwissADT is a multilingual and multimodal LLM-based ADT system that translates AD scripts between English and three main languages of Switzerland with both visual and textual inputs. It contains three basic components:\nMoment Retriever To identify the most relevant moment (i.e., a sequence of consecutive frames) in a video clip for a given AD segment, we initially select a video segment that spans from ten seconds before the AD's start runtime (onset) to ten seconds after its end runtime (offset). We then apply the video temporal grounder CG-DETR (Moon et al., 2023), which takes in both the AD script and the selected video segment and outputs the most relevant moment of variable length by providing the start and end times, along with a grounding score. The final moment is retrieved by selecting the top-ranked moment with the highest grounding score from the pool of candidate moments.\nFrame Sampler We linearly sample multiple video frames from the retrieved moment. These frames are then utilized as visual inputs of the AD translator. We empirically report results on using four frames and every 50th frame.\nAD Translator We deploy multilingual and multimodal LLMs as the backbone AD translator of SwissADT. We conduct experiments with the fundamental GPT-4 models gpt-40 and gpt-4-turbo. We decide to apply zero-shot learning as part of a cost-effective solution.\nOur modularized implementation of SwissADT streamlines the integration of state-of-the-art LLM research outcomes. This design allows for the seamless incorporation of cutting-edge moment retrievers and AD translators with minimal effort."}, {"title": "4 Data Collection", "content": "4.1 AD Scripts and Video Clips\nWe aggregate AD scripts from movies and TV shows that were aired on Swiss national TV stations, namely Schweizer Radio und Fernsehen (SRF), Radio T\u00e9l\u00e9vision Suisse (RTS), and Radiotelevisione Svizzera (RSI). It is noteworthy that AD scripts in French and Italian occupy significantly more runtime in videos\n4.2 Synthetic ADs with DeepL\nDue to a lack of parallel data, we use DeepL to generate synthetic AD scripts for each language pair of our system.\nWe translate all German, French, and Italian AD scripts into the other two Swiss languages, respectively, as well as into English. We include English as a mediating language in our ADT models to allow potential synergies with an AD script generation system developed by a research partner in our project. In addition, the moment retriever CG-DETR was trained on an English dataset, therefore, English is required as an intermediary language in our pipeline. For each source language, we randomly split the synthetic ADs into train, dev, and test sets (see Table 2 for more detail). We limit the number of ADs in both the dev and test sets to 200 samples each to preserve training data for further experiments, given the 7,500-sample size for French and Italian. AD data is scarce, so we carefully balanced its usage between training and testing. Additionally, we maintained consistent sizes across all languages to ensure uniform evaluation.\nWe exclude Swiss German AD scripts due to the inadequate translation quality when using DeepL."}, {"title": "5 Evaluation Method", "content": "5.1 DeepL Translation Quality Estimation\nWe assess the quality of silver-standard AD scripts translated by DeepL using GEMBA-MQM (Kocmi and Federmann, 2023), an LLM-based metric that employs three-shot prompting with GPT-4 to identify and annotate error spans. This evaluation is conducted on test sets comprising 200 ADs for each source-target language pair, with weights as-\n5.2 Automatic ADT Evaluation\nWe use BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CHRF (Popovi\u0107, 2015) as automatic evaluation metrics for AD scripts translated by SwissADT, where the scores are calculated by comparing the generated AD scripts to the ground truths.\n5.3 Human Evaluation with AD Professionals\nWe conduct human evaluations with our AD experts to assess the quality of AD scripts translated by SwissADT. Our objective is to verify the hypotheses that automatic evaluation scores reflect"}, {"title": "6 Results and Discussions", "content": "6.1 AD Translations\nTable 4 presents the automatic evaluations of various AD translators. We observe that\n\u2022 gpt-40 outperforms gpt-4-turbo;\n\u2022 GPT-4-based results demonstrate promising performance in the ADT task, as indicated by high evaluation scores. This finding supports the effectiveness of applying machine translation models to address the ADT task, which is aligned with previous literature;\n\u2022 Augmenting source ADs with corresponding video frames generally enhances translation quality, with the inclusion of more input frames leading to improved results. This suggests that it is beneficial to incorporate the visual modality into the ADT pipeline to utilize the power of fundamental LLMs.\nThe slightly better performance of gpt-40 with text-only on EN\u2192IT may be due to language-specific factors, the small dataset size or varying multilingual zero-shot capabilities, as the differences are not statistically significant. This result does not undermine the hypothesis that multimodal input improves translation quality overall, as other language pairs show the expected benefits. For examples where visual input is beneficial, refer to Appendix D.\nGiven that training human AD experts requires completing a curriculum that encompasses numerous essential competences and skills (Matamala and Orero, 2007; Jankowska, 2017; Colmenero et al., 2019), there is a persistent shortage of AD experts available to AD producers. Consequently,"}, {"title": "6.2 Human Evaluation", "content": "Table 5 presents the inter-evaluator agreement results conducted with our AD experts as well as the average evaluation scores given by each AD expert, respectively. First, we see that our AD experts demonstrate a fair level of agreement overall, highlighting the inherent difficulty in evaluating AD translations even among professionally trained individuals. Given this subjective variability among human evaluators, we contend that automatic evaluation metrics remain essential, as they offer an additional objective assessment independent of the evaluators' training. We also observe that AD scripts translated with four frames as input are rated higher in fluency (i.e. 5.38), and adequacy (i.e. 5.70) as compared to the text-only input translations (fluency: 5.24, adequacy: 5.68). These results verify our hypothesis that multimodal input improves translation quality. The dimension AD usefulness, however, is rated slightly higher for the AD scripts translated with the text-only input (i.e. 5.44) as compared to the four-frames translations (i.e. 5.39). Given the subjective opinions of AD experts on the AD usefulness, we argue that involving end users may yield more objective evaluations. This approach will be explored in our future research."}, {"title": "7 Conclusions and Future Work", "content": "In this work, we present SwissADT, a multilingual and multimodal ADT system designed to support three Swiss languages and English. Our findings demonstrate that leveraging LLMs to address the ADT task represents a significant initial step towards achieving information accessibility, as validated by our experienced AD experts. We anticipate that our work will benefit blind persons and persons with visual impairments in Switzerland, enhancing their access to streaming media.\nIn future work, we aim to explore other LLMs for the ADT task by fine-tuning these models using the training data. Moreover, we plan to integrate the ADT pipeline with human-in-the-loop principles, training the system with reinforcement learning approaches to better align outputs of ADT systems with human preferences. We believe that integrating human expertise into the LLM pipeline for the ADT task will more effectively meet end users' expectations and satisfaction. As with any accessibility technology, it is paramount that it serves the needs of the end users."}, {"title": "8 Limitations", "content": "The limitations of our work are the following: 1) Due to the lack of high-quality data, we do not include Romansh as a target AD language, despite it being an official language of Switzerland that has nearly 35,000 native speakers; 2) Given the difficulty in sourcing AD experts for French and Italian, we are unable to conduct human evaluations for these two languages. However, we expect the results to be comparable to German ADs, as indicated by the comparable translation results of our best AD translator gpt-40; 3) The multimodal nature of ADs has not been taken into account in the human evaluation, which would require the AD experts to have access to the visual inputs; 4) We do not utilize the Swiss German part of our dataset, as"}, {"title": "9 Ethics Statement", "content": "To ensure privacy protection and data anonymization, we formally obtained informed consent for data collection of human ratings as per the guidelines of the Zurich University of Applied Sciences."}, {"title": "A Audio Description Scripts", "content": "We make use of a common format for subtitles, namely SRT, where we treat ADs as subtitles.\nB Pricing\nTo estimate the cost of translating large datasets of ADs, we provide the calculations in Table 6 based on our dataset. Notice that OpenAI's pricing policy is subject to change, and that other factors, such as resolution and size of the input frames, as well as frequency and length of AD segments have great influence on the total price.\nC Prompts\nTable 7 demonstrates the empirical prompts we use in our experiments as well as in the system demonstration. We use these prompts for gpt-40 and gpt-4-turbo AD translators.\nD Examples\nThe following examples demonstrate how multimodal input enhances translation quality by offering extra context.\nGrammatical Ambiguity The Italian audio description Volta la testa verso un treno che avanza"}]}