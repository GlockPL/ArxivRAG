{"title": "GenComUI: Exploring Generative Visual Aids as Medium to Support Task-Oriented Human-Robot Communication*", "authors": ["Yate Ge", "Meiying Li", "Xipeng Huang", "Yuanda Hu", "Qi Wang", "Xiaohua Sun", "Weiwei Guo"], "abstract": "This work investigates the integration of generative visual aids in human-robot task communication. We developed GENCOMUI, \u0430 system powered by large language models (LLMs) that dynamically generates contextual visual aids-such as map annotations, path indicators, and animations-to support verbal task communication and facilitate the generation of customized task programs for the ro- bot. This system was informed by a formative study that examined how humans use external visual tools to assist verbal communi- cation in spatial tasks. To evaluate its effectiveness, we conducted", "sections": [{"title": "1 INTRODUCTION", "content": "In service robotics, verbal communication allows non-expert users to naturally and intuitively express their needs when interacting with robots, enabling broad applications across home services, education, healthcare, and retail [8, 16, 32, 49, 55, 66]. Previous studies have demonstrated a growing interest in using verbal communica- tion for robot programming, allowing users to specify commands, define goals, or create simple programs that align with their needs [3, 14, 25, 53, 63, 64]. Particularly with the advancement of large lan- guage models (LLMs) [70], traditional end-user robot programming can evolve into a collaborative and iterative process [37]. Users can now convey complex intentions and specify desired program outcomes through multi-turn, iterative communication while LLMs produce detailed specifications [21, 56]. Although LLMs have significantly improved human-robot nat- ural language interactions [38] and code generation [33], robot programming via verbal communication remains constrained in task-oriented scenarios due to its unstructured and ambiguous na- ture, often resulting in the abstraction matching problem [41]. More- over, speech-based communication encounters challenges such as speech recognition errors and being constrained to programming commands that are not easy to describe verbally, making it less ef- fective than text-based communication for handling complex tasks [3, 62]. These challenges hinder task-oriented human-robot commu- nication, especially when describing the robot's spatial movements and the logical relationships of task execution [60]. In End-User"}, {"title": "2 RELATED WORK", "content": "Task-oriented communication between humans and robots is be- coming increasingly important, especially in the context of end- user robot programming. Natural language programming allows non-expert users to specify robot tasks through verbal instructions [14, 31, 39, 48, 63]. This approach enables users to communicate complex task requirements and specify reusable programs through multi-turn dialogues [25, 61, 63], making it a fundamental aspect of end-user robot programming [3, 40]. The integration of artificial intelligence with natural language programming is considered a key method for future household robots and intelligent agents to provide personalized services [19]. Through natural dialogue, this approach enables untrained users without programming knowledge to define reusable robot pro- grams that align with their practical needs. Recent advancements in large language models (LLMs) [65] have further accelerated de- velopments in this field. Several studies have explored how LLM"}, {"title": "2.2 Visual Aids in Human-Robot Communication", "content": "Screens on robots serve an important function in enhancing com- munication by displaying facial expressions and complementing voice interactions [13, 24, 68]. Beyond expressive functions, screens also facilitate the communication of complex messages, yet their integration with verbal communication remains an area requiring further exploration. Currently, the way touch screens are used in service robots has led to their perception as mere \"screen bearers\", diminishing the sense of rich interaction with an autonomous agent. In most cases, they are employed as a means to circumvent the current limitations of full speech interaction while also providing an effective way to enable complex interactions [8]. However, balancing this with other interaction modalities is important to maintain a rich interac- tion experience. The consistency between voice interactions and graphical user interfaces is crucial for improving system usability and user satisfaction [50], making it easier for users to understand and operate the system. Research indicates that providing graphical feedback during dialogues significantly enhances user comprehen- sion of the robot's responses and intentions [52]. Visual media can support task communication through various forms, including robot-mounted screens, external display devices, and extended reality (XR) interfaces [12, 12, 44]. Additionally, ex- ternal objects and gestures can serve as references to assist in task communication, enhancing naturalness and comprehension [27, 28]. Some researchers have explored projection techniques that allow robots to visualize task information on physical objects in the"}, {"title": "2.3 LLM-driven Dynamic UI Generation", "content": "Recent work has demonstrated that large language models can serve as intermediaries for multimodal interfaces, enabling both understanding and generation beyond purely natural language con- tent. For example, in graphical user interfaces, LLMs can interpret the semantics and structure of UIs [17, 67] and generate UI designs [36, 43], demonstrating their potential in processing and generating non-linguistic visual representations. Furthermore, the world knowledge and in-context learning capa- bilities of LLMs [70] enable the dynamic generation of multimodal interactions tailored to the context on the fly. For instance, GenEM [46] utilizes LLMs to flexibly generate and adapt robot expressive behaviors based on natural language instructions and user pref- erences. Similarly, SiSCo [58] showcases the ability of LLMs to synthesize both natural language and visual signals adaptively for efficient collaboration. In the context of end-user programming, Co- cobo [21] illustrates how LLMs can dynamically translate between natural language and visual programming representations. Unlike traditional rule-based or template-based approaches, these LLM-based methods show the potential for adaptive contextual un- derstanding and immediate generation across multiple modalities. This motivates our exploration of leveraging LLMs to dynamically generate visual aids for task-oriented communication between hu- mans and robots."}, {"title": "3 FORMATIVE STUDY", "content": "To better understand how people utilize visual tools to enhance verbal task-oriented communication and to inform the design of visual aid methods for human-robot communication in spatial robot task customization, we conducted a formative study. In this study, participants were paired for task-based communica- tion, with one member of each pair provided with paper and a pen to facilitate visual communication. The objective was to observe and analyze how individuals integrate natural language and visual tools in their interactions, ultimately informing the design of visual aids modules for human-robot task communication. By collecting and analyzing data, we hope to explore:\n\nIn what situations do people use external visual tools to aid communication?\nHow visual and natural language dialogue are combined to facilitate communication?\nThe types of visual elements that users use to assist commu- nication."}, {"title": "3.1 Participants", "content": "We recruited 8 participants (5 female). Each participant was paired with one of the two researchers, forming a total of 8 groups. The recruits consisted of students and teachers from the university. The experiment for each group lasted between 10 and 15 minutes."}, {"title": "3.2 Procedure/Task", "content": "Each participant was assigned to a group where they took on the role of maintenance staff (Role B), while the researcher acted as the manager (Role A). Participants were informed of the specific experimental procedures and provided with printed task maps and colored pens. The scenario was set in a designated college space within the school, where the manager (Role A) verbally assigns tasks to the maintenance staff (Role B). The maintenance staff uses a paper map and a pen to communicate with the manager, ensuring they accu- rately understand the manager's intentions and provide a specific execution plan. Two tasks were designed: an exhibition reception task and a patrol task. The detailed scripts can be found in the appendix A. Before the experiment began, Role A reviewed a script and noted down the tasks they needed to assign to Role B. During the actual communication, Role A was not permitted to refer to the script, and their objective was to ensure the tasks were accurately conveyed to Role B. Role B was responsible for confirming their understanding through questioning and paraphrasing, as well as formulating a con- crete execution plan. To facilitate task confirmation, Role B was pro- vided with five different colored pens to annotate pre-printed maps, with no restrictions on paper usage. Each experimental session was recorded on video, capturing participants as they completed the two tasks. The experiment concluded once Role A confirmed that Role B had accurately understood the assigned tasks. Examples of participants' annotations on the paper maps can be seen in Figure 2. After the experiment, participants were asked about the specific situations in which they used pen and paper, how these tools as- sisted in articulating their intentions, and what specific benefits they provided."}, {"title": "3.3 Analysis", "content": "We analyzed the experiment recordings and synthesized the inter- view findings, identifying the specific needs and contexts in which users employed visual aids. Based on the experimental data, all participants utilized visual aids by marking or drawing on the map (8/8). Additionally, the majority of participants (7/8) reported that visual aids were beneficial in articulating their intentions."}, {"title": "3.3.1 T1. When do people use visual modalities to communi- cate?", "content": "First, during the interaction between Role A and Role B, Role B would draw while listening, responding with phrases like \u201cOK\u201d or \"understood\" to indicate comprehension of the request. Several participants noted that \"taking notes while listening helps with mem- ory\" (P1, P2, P3, P4, P7). Second, during the final plan confirmation, Role B would always use the previously drawn content to describe the task requirements while summarizing them to Role A. Some participants repeatedly marked certain points to emphasize key aspects during their summary (P6)."}, {"title": "3.3.2 T2. How are visual and natural language dialogue com- bined to facilitate communication?", "content": "During the task commu- nication, participants frequently referred to the map. When con- firming tasks with Role A, Role B would point to their annotations, explaining the task in alignment with their drawings and markings. In cases of logical branches, such as \"asking whether the principal is present in the college,\" participants would explain both the \"present\" and \"absent\" branches. If the branch led to a different location, such as \"if the principal is not in the college, exit through the main gate to end the tour,\" participants would simultaneously point to the main gate on the map while explaining. When reconfirming the task requirements, Role B would add supplementary drawings after consulting with Role A. Participants mentioned that \u201cseeing the content on paper facilitates easier organi- zation of thoughts\u201d (P4, P6, P7), emphasizing that synchronization"}, {"title": "3.3.3 T3. What types of visual elements do users use to assist communication?", "content": "The content annotated by participants often aligned closely with the specific aspects of the task. For aspects of the task involving sequential steps, such as \"first go to the classroom, then to the tool room, and finally to the library to complete the patrol task,\" participants would record the sequence using numbers, let- ters, or other markers. Particularly when the task involved spatial transitions, participants would use arrows to indicate the start- ing and ending points of these transitions. Participants mentioned that \"recording instructions and describing routes require pen and paper\" (P1, P3), and that \"making handwritten annotations helps in understanding the order of locations\" (P5, P7). Participants often sketched logical branches (P1, P2, P3, P4, P6, P7), stating that \u201csome obvious logic is easy to represent on paper\u201d (P4). For specific events, such as \u201cchecking whether the appliances are turned off in the activity space,\u201d participants would use distinct icons or brief text to describe them. Some participants would note down information at particular spots on the paper. For different scenarios, such as \"regular groups and VIP groups,\" participants used different colored pens to distinguish between them."}, {"title": "3.4 Summary", "content": "Based on our formative study observations, we categorized the visual elements used in task communication into four main types: sequence, logic, annotation, and global elements. For each type, we identified specific visual elements (such as lines with arrows, circle annotations, text labels, symbols, colors, and notes) and their representational purposes. For example, sequence-related elements were used to show movement paths and task orders, while logic elements helped represent conditional branches. Annotation ele- ments served to mark locations and add contextual information, and global elements like color coding helped highlight and orga- nize information across different aspects of the task. The complete categorization of these visual elements and their specific uses is shown in Table 1."}, {"title": "3.5 Design Considerations", "content": "Drawing from how humans naturally employ visual aids in com- munication, particularly their patterns of using visual elements to clarify complex tasks, we formalize the following design considera- tions for our system:\nProvide continuous and progressive visual feedback through- out the communication process to support step-by-step task un- derstanding and verification, ultimately facilitating the successful completion of task communication.\nFacilitate memory and task comprehension by interpreting user task intentions to plan and organize feedback, including the use of visual aids and speech output, ensuring users stay aware of complex task sequences and spatial relationships.\nEnable effective robot-to-human communication by organ- ically integrating visual aids and speech, allowing the system to convey information more comprehensively and intuitively.\nLeverage visual elements commonly used in human com- munication (e.g., arrows, labels, symbols) and their associated usage"}, {"title": "4 GENCOMUI SYSTEM", "content": "Based on the design considerations distilled from the formative study in section 3.5, we propose GENCOMUI (Figure 1), an LLM- based robot EUD system that incorporates generative visual aids. The system is implemented on the Temi V2 robot\u00b9, a mobile robotic platform equipped with a touchscreen. The system consists of four core modules designed to enable verbal robot programming through iterative, multi-turn communication:\nVoice Interaction Module: Handles speech-to-text and text-to-speech conversion, enabling bidirectional voice com- munication between users and the robot.\nUser Intention Understanding Module: Analyzes user input and dialogue context to understand communication progress, tracks task specifications, and plans appropriate responses, including visual aids generation and code updates.\nGenerative Visual Aids Module: Generates visual inter- face elements and animations on spatial maps according to visual aid requirements from the Intention Understanding Module.\nTask Program Synthesis and Deployment Module: Gen- erates and deploys executable robot code based on user spec- ifications, with built-in testing capabilities for iterative re- finement. The system leverages LLMs with structured output\u00b2 capabili- ties to handle complex interaction logic by generating multiple coordinated outputs in response to context, including planning and dynamically generating visual aids. For further details on the under- lying mechanisms of these modules, please refer to Section 4.2-4.4."}, {"title": "4.1 Example Usage Scenario", "content": "Here we present a sample usage scenario in an office setting to demonstrate how GENCOMUI and its generative visual aids support multi-turn verbal task communication between humans and robots. The main visual interface of GENCOMUI is shown in Figure 3. Lily, a secretary at a tech company, is responsible for coordinat- ing meetings and managing schedules. While she wants to leverage a robot assistant for her daily tasks, she faces two challenges: the robot's predefined functions are too rigid for her dynamic needs, and she lacks programming expertise to customize robot behav- iors. GENCOMUI addresses these challenges by enabling natural dialogue-based task specification. In this scenario, Lily wants to create a \"visitor reception\" task where the robot notifies and guides participants to meetings. After launching GENCOMUI, she initiates the dialogue through the Voice Interaction Module by tapping the voice button. She says: \"Hello Temi, I would like to develop a visitor reception service.\" The User Intention Understanding Module processes her input and generates an appropriate response: \"Okay, the robot will lead the visitor to the reception area first, then go to the work display area. Do you have any other requirements?\" Simultaneously, the Generative Visual Aids Module visualizes the task flow using connected lines and fade-in animations to highlight the newly added requirements (Figure 3-A1) (DC2)."}, {"title": "4.2 Human-to-Robot: Intention Understanding from User Speech", "content": "The Intention Understanding Module aims to obtain clear task specifications through multi-turn interactions by interpreting user intent, understanding requirements, and tracking interaction progress. To achieve this, the module leverages the CommunicationGPT model with chain-of-thought reasoning and few-shot learning tech- niques (detailed in Appendix B.1). The model takes system prompts and dialogue history as input and produces structured outputs containing:\nTask: Current robot task step descriptions based on the ongoing communication\nState: Communication progress tracking to guide the inter- action flow\nSpeak: Robot's verbal response content\nDraw: Visual aids type for rendering These structured outputs are then processed by the backend to generate appropriate robot behaviors, including verbal responses,"}, {"title": "4.3 Robot-to-Human: Visual Aids Generation and Presentation", "content": "The Generative Visual Aids Module dynamically generates graph- ical interface content during interactions based on the context of task communication using RobotDrawGPT (see Appendix B.2). Based on the structured outputs from the User Intention Under- standing Module, the system supports three presentation modes for visual-aided robot-to-human communication:\nThe Feedback mode highlights modified parts of the task flow, using fade-in animations to emphasize newly added steps and fade- out animations for deleted steps. In Confirm mode, the system synchronizes task steps with corresponding graphical animations and voice descriptions, highlighting each visual element as its asso- ciated step is narrated (Figure 3-B1). The None mode simply displays the current task flow without any animations.\nThe visual content is composed of two main types of elements. Location markers include icons representing robot actions, task numbers, and configurable location colors. These are connected by arrows that feature customizable colors and styles (solid or dashed lines), along with task descriptions. The arrows can establish various types of connections: between two specific locations, from one location to any location, or from any location to a specific destination."}, {"title": "4.4 Task Program Synthesis and Deployment", "content": "GENCOMUI creates and modifies robot task programs using an LLM based on the task flow input, system prompt (see Appendix B.3), and, if previously generated, existing code and task flow to inform the generation process. After final user confirmation, the Task Program Synthesis and Deployment Module leverages CodeGenerationGPT (see Appendix B.3) to generate executable robot code based on the con- firmed task specifications. The module synthesizes JavaScript pro- grams that orchestrate the robot's basic commands (Table 2) ac- cording to the specified task flow. Once code generation is complete, the system deploys the pro- gram to the robot's runtime environment and activates the test functionality. Users can then enter test mode to validate the pro- gram's behavior using the specified wake word. The testing inter- face provides an exit option that returns users to the customization interface for iterative refinement if needed."}, {"title": "4.5 Implementation Detail", "content": "GENCOMUI uses the open-source program temi-woz-android\u00b3 to establish communication between the robot and the backend server via WebSocket4. This enables the backend server to control Temi's behavior by sending WebSocket commands and receiving callback messages. The backend, developed using Node.js, handles interac- tion events, manages robot behavior, and makes calls to the OpenAI API. The robot task programs synthesized by the system are writ- ten in JavaScript and executed on the backend server. The system uses GPT-40-2024-08-06 through the OpenAI API5, which supports structured output and provides response times and reasoning ca- pabilities that meet the requirements of this project. We set the temperature parameter to 0 in all API calls. The frontend, built with the React framework and p5. js7, dynamically inserts and renders the visual aids code on Temi's Screen."}, {"title": "5 USER STUDY", "content": "The role of visual aids in facilitating task-oriented human-robot communication lacks sufficient empirical exploration in HCI re- search. Using GenComUI as a research tool, we aimed to probe two key questions: (RQ1) How do users perceive and experience visual aids during task-oriented communication with robots? and (RQ2) What design implications can be derived for integrating generative visual aids in human-robot communication systems? To investigate these research questions, we designed a comparative study that systematically examined how visual aids influence human-robot communication. Through a mixed-method approach comparing GenComUI with a baseline system without visual feedback, we sought to understand the specific role and impact of visual aids in task-oriented robot programming scenarios."}, {"title": "5.1 Baseline System for Comparsion", "content": "To gain deep insights [26] into how visual aids influence task- oriented human-robot communication, we designed a baseline sys- tem that retained all functionalities of GENCOMUI but omitted the generative visual aids module. While the baseline system displayed only static robot expressions on screen, we provided participants with a paper map identical to the one shown in GENCOMUI's inter- face for reference. This controlled design ensured that both systems had comparable response times and behaviors, allowing us to specif- ically examine the role of visual aids in human-robot task-oriented communication. By controlling this single variable, we could fo- cus on understanding how visual aids shape the communication process and user experience during task-oriented interactions."}, {"title": "5.2 Participants", "content": "This study recruited 20 participants (12 females, 8 males, aged 18-60, M = 26.5, SD = 8.38) via an online questionnaire, ensuring a diverse range of backgrounds in robot interaction, LLM familiarity, and programming expertise. Half of the participants (10/20) had no prior experience with robots, while the others had experience with"}, {"title": "5.3 Setup", "content": "The experiment was conducted in a simulated office environment as shown in Figure 6. The setup consisted of a Temi robot, a computer running the backend system, video and audio recording equipment for data collection, and printed task guidelines for participants. For the baseline condition, participants were provided with a physical map identical to the one displayed in GENCOMUI's interface. Two researchers were present throughout each session: one facilitating the experiment and another conducting behavioral observations."}, {"title": "5.4 Task", "content": "We designed four spatial programming tasks that required partici- pants to create robot programs involving navigation and actions in an office environment. The tasks were divided into two complexity levels based on the minimum number of required robot commands, ensuring that tasks of the same complexity were interchangeable. Low-complexity Tasks (minimum 4 robot commands each):\nTask L1: Office Patrolling and Employee Notification Program the robot to patrol specific office areas and notify designated employees\nTask L2: Employee Guidance and Area Introduction Program the robot to guide an employee through office areas while providing area descriptions High-complexity Tasks (minimum 8 robot commands each):\nTask H1: Visitor Guidance at Reception Program the robot to receive visitors and guide them through multiple office locations\nTask H2: Employee Gathering and Preparation Work Program the robot to locate multiple employees and coordi- nate a meeting preparation sequence"}, {"title": "5.5 Procedure", "content": "The experiment followed a within-subjects design where partici- pants experienced both systems. The researcher used a computer- ized randomization program to determine each participant's system order, which ultimately achieved a balanced distribution (10:10) across conditions. The procedure for each system condition con- sisted of three phases:\nTraining Phase (15 minutes): Participants watched a system- specific tutorial video and received hands-on guidance from re- searchers on how to operate the system. For the baseline system, participants were provided with a paper map; for GENCOMUI, the map was displayed on the robot's screen.\nTask Execution Phase (30 minutes): Participants completed two tasks with each system: one low-complexity task and one high- complexity task. For each task, participants communicated their programming intentions through voice commands until the robot indicated understanding by requesting a task trigger word. Upon execution, participants could interrupt and modify the program if needed, with the task concluding only when participants perceived that the robot had successfully executed the intended program.\nAssessment Phase: After completing tasks with each system, participants filled out questionnaires. Following the completion of both conditions, participants en- gaged in a 20-minute semi-structured interview."}, {"title": "5.6 Measurements", "content": "Questionnaire. To evaluate how generative visual aids affect human-robot task-oriented communication, we employed three complementary measurement scales. The Chatbot Usability Scale [9] was used to assess the dialogue-based interaction quality, focus- ing on how visual aids influence task communication effectiveness and user experience. We incorporated the Godspeed questionnaire [7] to evaluate whether visual aids enhanced users' perception of the robot as an intelligent communication partner during task programming. Additionally, the System Usability Scale (SUS) [34] provided a standardized measure of overall system usability. This combination of metrics helped us comprehensively understand the impact of visual aids on task-oriented communication and derive design implications. For data analysis, we conducted statistical com- parisons between the GENCOMUI and baseline conditions. We ap- plied paired t-tests for normally distributed variables and Wilcoxon signed-rank tests for non-normally distributed variables to analyze the questionnaire responses. Interview. We conducted semi-structured interviews to gather comprehensive user feedback. The interviews began with questions about users' operational experiences with both systems, asking them to compare and identify their preferred system with the ratio- nale, which helped us understand users' overall perception of the systems. We then explored how users leveraged the visual aids to as- sist in completing natural language programming tasks. Following this, we focused on the GENCOMUI interface design, investigating users' perceptions and expectations of our interface features. Each interview lasted approximately 20 minutes and was audio-recorded for subsequent analysis. The complete set of interview questions is provided in Appendix C.2. We followed Boyatzis's [10] thematic analysis guidelines. Two researchers independently coded the tran- scripts, developed initial codebooks, and identified preliminary themes. Through iterative discussions, the researchers refined the codes and themes until reaching consensus after several rounds of review. Task Performance Metrics. To evaluate task performance, we collected quantitative data through backend logs during each session. These logs captured key metrics including task completion time, success rates, and the frequency of repeated communications for modification in each user task. We performed paired t-tests and"}, {"title": "6 FINDINGS", "content": "Quantitative Results Task Completion and Observation Report. All participants (20/20) successfully communicated tasks to the robot, achieving cor- rect task execution across both systems. Statistical analysis revealed no significant difference in task completion time between GENCO- MUI and the baseline system (p > 0.05; Baseline: Mdn = 0:05:26, Std = 0:03:16; GENCOMUI: Mdn = 0:06:47, Std = 0:02:52). The number of voice dialogue turns (Baseline: Mdn = 11, Std = 4; GENCOMUI: Mdn = 11, Std = 4) also showed no significant difference between systems."}, {"title": "6.1.2 Chatbot Usability Scale Analysis", "content": "The questionnaire consists of 42 questions divided into 14 sections, such as \"ease of starting con- versation\" and \"access to chatbot\". We selected 13 sections relevant to our study for participants to answer. The results are presented in Figure 7, which shows the boxplot analysis. Our quantitative anal- ysis of the ChatBot Usability Scale highlighted significant improve- ments in user interaction with GENCOMUI compared to the baseline, particularly in terms of accessibility, clarity, and responsiveness. In terms of access to chatbot functionality (C2), GENCOMUI's features were significantly more detectable (p = 0.001), with higher ratings (Baseline: Mdn = 4.0, Std = 0.72; GENCOMUI: Mdn = 4.33, Std = 0.60). Response speed (C13) was perceived as quicker with GENCOMUI (\u0440 = 0.048; Baseline: Mdn = 3.165, Std = 1.00; GENCOMUI: Mdn = 4.33, Std = 0.79). Expectation setting (C3) also demonstrated significance, with GENCOMUI scoring slightly lower than the Baseline (p = 0.004; Baseline: Mdn = 4.165, Std = 0.74; GENCOMUI: Mdn = 4.0, Std = 0.58), indicating a small difference. During communication, users found it significantly easier to give instructions and were less likely to rephrase their inputs multiple times (p = 0.048; Baseline: Mdn = 3.33, Std = 0.66; GENCOMUI: Mdn = 4.0, Std = 0.91) in terms of flexibility and communication effort (C4). The ability to maintain a coherent, themed discussion (C5) showed a slight but significant improvement (p = 0.041; Baseline: Mdn = 4.0, Std = 0.60; GenComUI: Mdn = 4.165, Std = 0.59), with users reporting that interactions felt more like ongoing conver- sations. When encountering problems, GENCOMUI demonstrated"}, {"title": "6.1.3 Godspeed Questionnaire Analysis", "content": "Participants' perceptions of human-likeness were evenly split: 10 out of 20 rated the baseline system as more human-like, while the remaining 10 participants perceived GENCOMUI as closer to human characteristics based on the questionnaire responses. Across all five dimensions (G1-G5), no significant differences were found between the two systems (p > 0.05). Specifically, in anthropomorphism (G1), both systems received relatively low ratings (Baseline: Mdn = 2.6, Std = 0.66; GENCOMUI: Mdn = 2.5, Std = 0.89), indicating that participants did not perceive either system as particularly human-like in appear- ance. However, both systems achieved relatively high ratings in perceived intelligence (G4: Baseline: Mdn = 3.8, Std = 0.43; GENCO- MUI: Mdn = 4.0, Std = 0.34). This contrast suggests that while lacking in human-like appearances, they nevertheless demonstrated com- petent intelligent behavior relative to their anthropomorphic traits. Detailed results for animacy (G2), likeability (G3), and perceived safety (G5) are presented in Figure 8, which shows the boxplot analysis."}, {"title": "6.1.4 System Usability Scale Analysis", "content": "The System Usability Scale evaluation showed significantly higher scores for GENCOMUI (Mdn = 73.75, Std = 15.04) compared to the baseline system (p < 0.01; Mdn = 63.75, Std = 12.07)."}, {"title": "6.2 Qualitative Results", "content": "User Perceptions of Generative Visual Aids in GenCo- MUI. Participants overwhelmingly preferred GENCOMUI over the baseline system (19/20). Through thematic analysis of interview"}, {"title": "6.2.1 data, we identified several key patterns in how users perceived and interacted with the generative visual aids. The following find- ings emerged from our coding of user responses regarding their experience with the system.", "content": "Helping user convey their intention, especially for spatial tasks. In spatial tasks, users need to engage in detailed communi- cation with robots regarding spatial movements and manipulative operations. Both systems were perceived by some participants (5/20) as capable of actively filling in gaps and completing incomplete in- formation from voice inputs. With GENCOMUI, users found it easier to structure and communicate task details (8/20), by first \"decom- posing the content based on the task, then organizing the expression\" (P18). While using natural language alone required users to spend effort \"drafting a mental outline first\u201d (P10), which rarely occurs in human-to-human communication, GENCOMUI's integrated visual interface aligned better with natural interaction patterns. As P11 noted, \"I don't need to mentally compose while speaking, especially for spatial tasks\". P16 expressed that the dynamic display of visual aids during communication made their \"thought process feel smooth\", and P4 appreciated having a reference that alleviated concerns about \"saying the wrong thing\u201d. Additionally, GENCOMUI facilitated more effective communication of spatial movement task details. P9 noted that it was no longer necessary to communicate with the robot in strict task order, which clarified the communication process. Helping track communication progress. GENCOMUI effec- tively supported participants in monitoring their task communi- cation progress. As P8 noted, \"I might be clearer about which step I'm currently at", "one would forget which step they're currently at when the communication time is relatively long\" (P8). Nearly half of the participants (9/20) highlighted the system's intuitiveness when discussing GENCO- MUI's advantages. This intuitive guidance led users to feel that \"the robot's behavior was controllable\u201d (P4), enhancing their confidence in the communication process. Facilitating confirmation of robot comprehension of task. The GENCOMUI system effectively enabled participants to verify the robot's task comprehension. Participants could quickly identify misunderstandings, as P1 noted, \"I could quickly recognize when it misunderstood something\u201d, and P3 stated, \"As soon as the icons appeared, I could tell whether it understood or not": "During the final confirmation phase, participants found it straightforward to track the robot's explanation progress. As P11 mentioned, \u201cit's easy to know where the robot is in its explanation by looking at the screen", "it is intuitive to observe the robot executing different tasks at various locations\". Serving as memory aids in task communication. The visual aids interface helped participants with memory retention. Partic- ipants viewed the interface as a communication reference, allevi- ating concerns about forgetting earlier task content (P4, P11). P4 stated, \"After seeing its visual aids interface, I realized I had initially overlooked certain task steps I hadn't previously recognized, which I then supplemented later": "P4). When visual aids were absent,"}, {"title": "6.2.2 Summary of Desirable Interaction Improvements and Expectation for Generative Visual Aids", "content": "Regarding the activa- tion timing of the visual aids interface, some participants suggested it should be triggered as frequently as possible during the com- munication process (10/20), while others felt it was sufficient for the visual aids content to appear only when completing complex tasks, with simple tasks not requiring much assistance. Some also believed that \"for regular chatting, it's not really necessary\" (P15). However, most participants acknowledged the necessity of visual aids in complex tasks (18/20). Expectation for visual design and interaction improvements. Regarding GENCOMUI's interface design, participants currently view detailed task events and sequence information by clicking on generated icons on the map. Participants expressed a desire for larger and more prominent clickable"}]}