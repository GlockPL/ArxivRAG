{"title": "ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders", "authors": ["Fotios Lygerakis", "Elmar Rueckert"], "abstract": "Traditional Variational Autoencoders (VAEs) are constrained by the limitations of the Evidence Lower Bound (ELBO) formulation, particularly when utilizing simplistic, non-analytic, or unknown prior distributions. These limitations inhibit the VAE's ability to generate high-quality samples and provide clear, interpretable latent representations. This work introduces the Entropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of the ELBO that explicitly includes entropy and cross-entropy components. This reformulation significantly enhances model flexibility, allowing for the integration of complex and non-standard priors. By providing more detailed control over the encoding and regularization of latent spaces, ED-VAE not only improves interpretability but also effectively captures the complex interactions between latent variables and observed data, thus leading to better generative performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Variational Autoencoders (VAEs) [1] have emerged as a compelling framework for generative modeling, offering a principled approach to learning probabilistic representations of complex data distributions. At the heart of VAEs lies the ELBO, which serves as the objective function guiding the optimization of the model parameters. The ELBO intertwines reconstruction accuracy with a regularization term that encourages the learned latent variables to adhere to a specified prior distribution, typically a standard normal distribution. This dual objective underpins the VAE's capacity to generate new, plausible data samples while uncovering interpretable and disentangled representations of the underlying data-generating process. However, the conventional formulation of the ELBO presents certain limitations that may hinder the flexibility and interpretability of VAEs [2]. Primarily, the choice of the prior distribution in the ELBO plays a critical role in shaping the learned latent space, yet the traditional formulation does not offer an intuitive understanding of how the chosen prior interacts with the learned latent distribution. Moreover, the standard ELBO does not explicitly account for the amount of information shared between the data and the latent variables, which can be instrumental in understanding and controlling the representational power of the VAE [3], [4]. This formulation often leads to additional limitations in model flexibility and interpretability, particularly when the prior distributions cannot be captured as normal Gaussians or when the chosen priors do not have an analytic form for computing the components of the ELBO. Additionally, the traditional ELBO does not effectively handle situations where the prior distribution is unknown or where it cannot be easily integrated due to its complexity. This restricts the VAE's ability to adequately capture the intricate interactions between latent variables and observed data, thereby hindering the model's capacity to generate high-quality, diverse samples and provide interpretable and disentangled representations of the underlying data-generating process.\nMotivated by these challenges, we propose a novel reformulation of the ELBO that seeks to enhance the expressivity, interpretability, and adaptability of VAEs. Our formulation decomposes the ELBO into distinct terms that separately account for reconstruction accuracy, mutual information between data and latent variables, and a marginal KL divergence to prior. We further decompose the marginal KL divergence term into entropy and cross-entropy terms, which facilitates the more explicit control of the inherent uncertainty in the latent variables and the alignment of the learned latent distribution with the prior. Our proposed Entropy Decomposed VAE (ED-VAE) offers, thus, a more granular perspective on the latent variable modeling in VAEs, allowing for a deeper understanding of the intricate balance between data reconstruction, information preservation, and regularization. The explicit depiction of entropy and cross-entropy terms enables a more informed and flexible choice of priors, extending the exploration beyond standard normal priors to incorporate domain-specific knowledge or desirable properties into the latent space. Furthermore, the delineation of mutual information facilitates direct control over the expressiveness and diversity of the latent space, which is crucial for enhancing the generative capabilities of the VAE. In our experiments, we compared the performance of the traditional VAE with our entropy-decomposed variation using two synthetic datasets. These datasets were created to test how each model handles priors of varying complexities, from simple Gaussian to complex non-Gaussian distributions."}, {"title": "II. RELATED WORK", "content": "The development of VAEs [1] has seen significant advancements, particularly in the formulation and optimization of the ELBO. Foundational work by [2] introduced the concept of dissecting the ELBO into more interpretable components, which allows for an improved understanding of how different elements influence training and inference in VAES.\nSimilarly, [5] addressed the imbalance in traditional ELBO formulations in their development of InfoVAE, which often prioritizes either reconstruction accuracy or latent regularization at the expense of the other.\nBuilding on the need for robustness in ELBO computation, especially with complex and non-analytic priors, the utilization of advanced sampling methods has proven crucial. [6] explored the use of multiple-importance sampling within the ELBO, improving the accuracy and stability of the variational inference. Their approach to deep ensembles for variational approximations presents a novel way to handle uncertainties in model predictions.\nThe issues of model robustness and flexibility are further compounded by challenges such as posterior collapse, which affects the quality of generated samples and the diversity of the latent space. [7] provided an in-depth analysis of posterior collapse. Additionally, the work on learning optimal priors for task-invariant representations by [8] underscores the importance of flexible and adaptive prior settings in VAEs. Approaches like the CR-VAE [4], which uses contrastive regularization to prevent posterior collapse ensure a robust representation in the latent space and contribute to a more robust and adaptive generative modeling framework."}, {"title": "III. METHOD", "content": "In this section, we present a novel formulation of the ELBO used in VAEs, which enhances interpretability and flexibility in choosing the latent variable prior. We start from the original ELBO formulation, introduce an intermediate form that includes a mutual information term and a KL divergence to the prior term, and finally propose a new form of ELBO that decomposes the later KL divergence into entropy and cross-entropy terms.\n\nOriginal ELBO Formulation\nThe original ELBO is formulated as:\n$\\mathcal{L}(\\theta; \\phi) = E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)]-KL(q_{\\phi}(z|x)||p(z))$\nEntropy Decomposed ELBO\nWe further decompose the marginal KL divergence to the prior into an entropy and a cross-entropy term as:\n$KL(q_{\\phi}(z)||p(z)) = E_{q_{\\phi}(z)} [log q_{\\phi}(z) - log p(z)] = E_{q_{\\phi}(z)} [log q_{\\phi}(z)] \u2013 E_{q_{\\phi}(z)} [logp(z)] = -H[q_{\\phi}(z)] + H[q_{\\phi}(z), p(z)]$\nWe thus propose a new form of ELBO by substituting the above decomposed KL divergence in 3:\n$\\mathcal{L}(\\theta; \\phi) = E_{q_{\\phi}(z|x)} [log p_{\\theta}(x|z)] \u2013 I_q(x, z) + H[q_{\\phi}(z)] - H[q_{\\phi}(z),p(z)]$\nThis novel formulation of the ELBO, through the decomposition of the KL divergence into entropy and cross-entropy terms, unveils a more granular perspective of the latent variable modeling in VAEs. It fosters a deeper understanding of the interplay between the latent variable posterior and the chosen prior, thereby allowing for more informed and flexible prior selections.\nFirstly, the explicit representation of the entropy term, $H[q_{\\phi}(z)]$, reveals the inherent uncertainty or randomness in the encoded latent variables. This facilitates a more direct analysis and control over the latent space's expressiveness and diversity, which is crucial for the generative capabilities of the VAE.\nSecondly, the cross-entropy term, $H[q_{\\phi}(z),p(z)]$, acts as a clear measure of alignment or divergence between the learned latent distribution and the predefined prior. This explicit depiction allows for a more intuitive and adaptable choice of priors, enabling the exploration beyond standard normal priors, and allowing the incorporation of domain-specific knowledge or desirable properties into the latent space. It is important to state here that unlike the KL divergence, where choosing a prior that allows for an analytic form is crucial, using the cross-entropy term only requires the ability to sample from the considered prior distribution.\n\nELBO Optimization\nWe adopt a Gaussian distribution assumption for the likelihood $p_{\\theta}(x|z)$, simplifying the reconstruction loss to the Mean Squared Error (MSE) between the original and reconstructed data (A). This choice is congruent with the continuous nature of the datasets, ensuring well-behaved gradients for backpropagation.\n$L_{recon} = E_{q_{\\phi}(z|x)} [||x - \\hat{x}||^2]$\nFor optimizing the mutual information term, we employ the InfoNCE loss, providing a lower bound to it. In the work of [9], it has been established that the InfoNCE loss, $L_{InfoNCE}$, serves as a lower bound to the mutual information, $I_q(x, z)$:\n$I_q(x, z) \\geq log(K) - L_{InfoNCE}$\nwhere K represents the batch size. $L_{InfoNCE}$ is computed utilizing positive and negative sample pairs, encouraging the encoder to generate representations that are more informative of the data.\n$L_{InfoNCE} = -log \\frac{exp(z_iz_j^T)}{\\sum_{k=1}^K exp(z_iz_k^T)}$\nSimilarly, the entropy $L_{Ent}$ and cross-entropy $L_{XEnt}$ terms are computed using samples from the batch to facilitate their estimation.\nThe total loss of ED-VAE is\n$L_{ED-VAE}(\\theta; \\phi) = L_{recon}-L_{InfoNCE}-L_{Ent}+L_{XEnt}$ \nAll the sub-losses are jointly optimized through backpropagation, with gradients propagated through the encoder and decoder parameters."}, {"title": "IV. EXPERIMENTS", "content": "Our experimental design evaluates the traditional VAE and the ED-VAE using synthetic datasets designed to highlight different complexities in data distributions. These experiments are specifically tailored to assess the models' ability to handle varying complexities of priors, from Gaussian to non-Gaussian complex distributions.\n\nSynthetic datasets\nWe utilize two distinct synthetic datasets to evaluate the performance of the traditional VAE and ED-VAE:\na) Dataset 1: Gaussian Prior: This dataset consists of data generated from a multi-dimensional Gaussian distribution. The latent variables z are sampled from a standard Gaussian distribution $\\mathcal{N}(0,I)$, and the observed data is created by a linear transformation of z followed by Gaussian noise. Additionally, positive samples are generated by adding small Gaussian noise around the original data points, simulating slight variations within the data distribution that are still characteristic of the Gaussian prior. This serves as a baseline, testing the models under standard conditions where the data aligns well with the models' Gaussian prior assumptions.\nb) Dataset 2: Complex Non-Gaussian Prior: This dataset features a complex structured prior modeled as a mixture of Gaussians modulated by sinusoidal functions. The latent variables are sampled from multiple Gaussian distributions with varying means and scales, modulated by a sinusoidal function to introduce non-linear interactions within the latent space. The observed data is again generated by a linear transformation followed by Gaussian noise, with positive samples created similarly to Dataset 1. This dataset tests the standard VAE's capability against the ED-VAE's flexibility in handling complex, structured non-Gaussian distributions.\n\nModel Configurations\nThe traditional VAE is configured with a standard Gaussian prior, which is expected to perform adequately on Dataset 1 but may struggle with Dataset 2 due to its simplicity. In contrast, the ED-VAE is set up with a flexible prior configuration for Dataset 2, aiming to closely approximate the complex distributions involved, while maintaining a similar setup as Dataset 1 for direct comparison. We evaluate the two models based on the ELBO of held-out data, which consists of the lower bound for the log-likelihood of the data distribution learned.\n\nExperimental Setup\nTo ensure robust and reliable findings, each experiment is conducted five times with different seeds, and results are averaged to offset the effects of random initialization and stochastic optimization. Both models consist of a two-layer fully connected multilayer perception, with 400 hidden dimensions and a five-dimensional latent space, with the decoder following the inverse architecture. The VAEs are trained using the Adam optimizer with a learning rate of 0.001 and a batch size of 512. Training continues for 1000 epochs with early stopping based on validation loss to prevent overfitting. Input features are normalized to prevent bias.\nThe computation of the entropy and cross-entropy terms, critical for the ED-VAE model, is handled distinctively to ensure accurate representation and alignment with the model's theoretical foundations. For the traditional VAE, the entropy term is derived directly from the latent variable's log-variance output by the encoder, representing the inherent uncertainty of the encoded representations. Specifically, entropy is calculated using the formula:\n$H[q_{\\phi}(z)] = 0.5(1 + log(\\sigma^2))$\nwhere $\\sigma^2$ is the variance of the latent variables. For the ED-VAE, the cross-entropy term $H[q_{\\phi}(z), p(z)]$ is computed to measure how well the encoded latent distribution aligns with the specified prior. This involves evaluating the negative log-likelihood of the latent variables under the chosen prior distribution, using a Gaussian mixture model (GMM) [10] when non-standard complex priors are employed. The GMM parameters are estimated from the prior data, and the cross-entropy is calculated as the expected negative log-likelihood under this model. When the prior is a standard Gaussian, the cross-entropy simplifies evaluating the Gaussian density function at the values of the encoded latent variables."}, {"title": "V. RESULTS", "content": "The evaluation of two VAEs, the traditional one and the entropy-decomposed one, was conducted across two datasets designed to test model efficacy under different prior assumptions: a standard Normal prior and a complex, non-Gaussian prior.\nOn the dataset with the standard Normal prior, both models performed competently (Table II), yet the ED-VAE showed a distinct advantage in its encoding and regularization capabilities. This was evident from its more efficient data representation and notably superior regularization, leading to a higher ELBO. The traditional VAE, while effective, demonstrated less optimal alignment with the normal prior, indicative of its comparatively limited capacity to regulate the latent space.\nThe differences between the models became more pronounced when faced with the complex, structured non-Gaussian prior. Here (Table II), the traditional VAE struggled to adapt, reflecting its challenges with modeling a more complex prior distribution with a normal one. Conversely, the ED-VAE managed to maintain a much higher level of data fidelity (higher ELBO) and minimal divergence from the complex prior. This performance underscores the ED-VAE's robust adaptability and its effective management of the latent space to align closely with even highly irregular priors.\nThe superior performance of the ED-VAE across both datasets underscores its effectiveness in managing complex data distributions. The proposed objective for training the VAE, incorporating entropy and cross-entropy components, allows for better control over latent space regularization. This not only results in better alignment with the priors but also enhances the interpretability and the quality of the data reconstructions."}, {"title": "VI. DISCUSSION", "content": "In this work, we presented a novel approach to training VAEs by decomposing the ELBO formulation into an entropy and cross-entropy term. Our approach showcases better adaptability and performance on two synthetic datasets with variable complexity of prior distributions. This is achieved through a reformulated ELBO that explicitly accounts for the entropy in the latent variables and their alignment with respective priors, enhancing the model's ability to handle intricate data distributions.\nOur findings emphasize the necessity of selecting suitable prior models for generative tasks, particularly highlighted by the traditional VAE's difficulties with complex data that deviates from normal distributions. The ED-VAE, with its flexible framework, enables a nuanced interaction between the model and the underlying characteristics of the data, leading to improved learning outcomes and more precise reconstructions.\nDespite the benefits, the introduction of entropy and cross-entropy terms does increase computational demands, particularly affecting memory usage and processing power. Addressing these challenges will be essential to optimize the model's efficiency and facilitate its wider adoption."}, {"title": "A. Analysis of the Cross-Entropy Term with a Standard Normal Prior", "content": "Given a standard normal prior $p(z) = \\mathcal{N}(0, I)$, the probability density function is expressed as $p(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}$. The log of this probability density function simplifies to $log p(z) = log(\\frac{1}{\\sqrt{2\\pi}}) -\\frac{z^2}{2}$. The cross-entropy term is defined as the expectation of the negative log-likelihood of the latent variables encoded by $q_{\\phi}(z)$ under the standard normal prior:\n$H[q_{\\phi}(z), p(z)] = -E_{q_{\\phi}(z)} [log p(z)]$\nSubstituting the log probability density of p(z) into the equation, we get:\n$H[q_{\\phi}(z), p(z)] = -E_{q_{\\phi}(z)} [log(\\frac{1}{\\sqrt{2\\pi}}) - \\frac{z^2}{2}] = -E_{q_{\\phi}(z)} [log(\\frac{1}{\\sqrt{2\\pi}})] - \\frac{1}{2} E_{q_{\\phi}(z)} [z^2] = \\frac{1}{2} log(2\\pi) + \\frac{1}{2} E_{q_{\\phi}(z)} [z^2]$\nHere, $\\frac{1}{2} log(2\\pi)$ is a constant and can be taken out of the expectation. The term $E_{q_{\\phi}(z)} [z^2]$ represents the expected squared norm of the latent variables under the distribution $q_{\\phi}(z)$. By minimizing the cross-entropy term, the encoded latent variables are encouraged to align with the characteristics of the standard normal prior, particularly having a unit average squared distance from the origin."}, {"title": "B. Latent Variable Generation", "content": "The latent variables z are generated from a non-Gaussian prior, specifically a modulated mixture of Gaussians. The generation process involves the following steps:\n1) Mixture Components: The latent variables are drawn from multiple Gaussian distributions, where each component i of the mixture has a mean $\\mu_i$ and scale $\\sigma_i$. The means and scales are linearly spaced between specified bounds:\n$\\mu_i$ = linspace(-3,3,3)\n$\\sigma_i$ = linspace(0.5, 1.0, 3)\n2) Latent Sampling: For each component, samples are drawn as follows:\n$z[i] = \\mathcal{N}(\\mu_i, \\sigma_i^2)$\n3) Modulation: Each component is modulated by a sinusoidal function to introduce non-linear interactions:\n$z_{modulated}[i] = z[i] + sin(0.5\\pi z[i])$\n4) Dimension Matching: If the desired latent dimensionality latent_dim is greater than the number of components, additional Gaussian noise is added. If latent_dim is less, the dimensions are truncated."}, {"title": "C. Data Generation", "content": "Observable data points x are generated using a linear transformation of the latent variables followed by the addition of Gaussian noise:\n1) Transformation Matrix: A transformation matrix W is sampled:\n$W \\sim \\mathcal{N}(0,1), W\\in \\mathbb{R}^{latent\\_dim \\times data\\_dim}$\n2) Data Construction: The data points are constructed as:\n$x = z_{modulated} W + \\mathcal{N}(0,0.5)$"}, {"title": "D. Positive Sample Generation", "content": "Positive samples are generated by adding Gaussian noise to simulate slight variations within the data distribution:\n$x_{positive} x + \\mathcal{N}(0, radius)$\nwhere radius determines the variability of the positives from the anchor data points."}]}