{"title": "Federated Fairness Analytics: Quantifying Fairness in Federated Learning", "authors": ["Oscar Dilley", "Juan Marcelo Parra-Ullauri", "Rasheed Hussain", "Dimitra Simeonidou"], "abstract": "Federated Learning (FL) is a privacy-enhancing technology for distributed ML. By training models locally and aggregating\nupdates - a federation learns together, while bypassing centralised data collection. FL is increasingly popular in healthcare,\nfinance and personal computing. However, it inherits fairness challenges from classical ML and introduces new ones, resulting\nfrom differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware.\nFairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics\nto quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness.\nOur definition of fairness comprises four notions with novel, corresponding metrics. They are symptomatically defined and\nleverage techniques originating from XAI, cooperative game-theory and networking engineering. We tested a range of\nexperimental settings, varying the FL approach, ML task and data settings. The results show that statistical heterogeneity\nand client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve\nfairness-performance trade-offs. Using our techniques, FL practitioners can uncover previously unobtainable insights into their\nsystem's fairness, at differing levels of granularity in order to address fairness challenges in FL. We have open-sourced our\nwork at: https://github.com/oscardilley/federated-fairness.", "sections": [{"title": "Introduction", "content": "Data is naturally distributed around us, commonly made inert by data protection regulations. Classically, to train machine\nlearning models, data is collected centrally \u2013 the data is brought to the model. However, such scenarios where data and compute\npower can be centralised are not archetypal of most of the world around us where data and processing infrastructure naturally\nexist in distributed pockets. This segmentation of data can be observed in many areas of modern life and it is common that\nthe data is immovable between segments due to legislative rules. For example, corporations with data centres spread globally\nmay be prevented from cross-border sharing due to regulation. Alternatively, in the UK's NHS, which consists of thousands of\nindividual practices, patient information is stored on different systems which are not typically interconnected\u00b9. Segmentation\nalso arises at in personal computing where users are not willing to share their personal data with third parties. These examples\nonly scratch the surface of the possible distributed data scenarios and motivates the idea that many parties could benefit from\nthe application of machine learning and privacy-preserving knowledge-sharing across the different segments, instead of taking\nthe data to the model.\nFederated Learning was proposed in the seminal paper by B. McMahan from Google in 20172. It presents the original ap-\nproach for FL, FedAvg, and offers a privacy-preserving solution to the problem above, whilst being competitive in performance\nwith its centralised counterparts in a set of typical machine learning tasks. Since its inception, FL has been deployed into a\nwide range of applications with cross-device implementations such as with Google's GBoard\u00b3; it is also rumoured, to be used\nin detecting the wake-up phrase, \u2018Hey Siri' for Apple's voice agent\u2074. The effectiveness of cross-silo scenarios have also been\ndemonstrated, such as in a case where a small number of UK hospitals collected data and trained a model using FL to screen\nfor Covid-195.\nIn recent, general-FL literature, fairness is repeatedly recognised as one of the key open problems. A 2021 review by P.\nKairouz and B. McMahan of Google, designates a section to identifying fairness as a key area for future research in FL6. Other\ngeneral surveys, from 2022 and 2023, also draw attention to the potential for heterogeneity to lead to unfairness, motivating\nfurther research7,8. Finally, in a talk at the 2024 Flower AI Summit, the Head of Samsung AI Europe, refers to the difficulty"}, {"title": "Technical Background", "content": "To form a technical definition of the fairness problem and the effects of heterogeneity, we consider the architecture and\nlexicon presented in Figure 1, which are typical to most FL systems. This considers that in each training round, k out\nof K total rounds, the server selects a set of participating clients, Sk, indexed by n from the set of C available clients, S,\nindexed by N where Sk \u2286 S = {C1, C2, ..., CN}. It is assumed that each client has a private dataset, Dn and that all clients are\nattempting to learn a model with the same structure despite potential differences in size and data distribution of the client's\ndatasets. For each round, k, the server sends the global model parameters, Ok to each of the participating clients, which\nperform E epochs/ local rounds of training to obtain their parameters, 0+1. The client transmits their parameters, 0+1 back\nto the server, which, in the case of the FedAvg approach, performs weighted averaging to obtain the next round parameters, Ok+1\u00b7\nWhen defining a system, it is typical in FL to use the terms, cross-device, and cross-silo to differentiate between data\nsettings15. Once a FL approach is realised, evaluation metrics are used to measure the performance of the models in the\nfederation. Evaluation is challenging due to the private nature of the distributed data. Federated evaluation is used in this paper,\nwhereby each client retains a proportion of its samples to construct a personal test-set for evaluation - the clients each inform"}, {"title": "Related Work", "content": "The matter of fairness is a well-established, albeit unsolved problem in centralised machine learning scenarios, where a single\nmodel is trained on a single dataset17\u201319. Given FL systems are defined by the interconnection of a number of smaller centralised\nsystems, some traditional definitions are still relevant to federated settings, enabling comparison of some aspects of fairness\nbetween clients. These include some field wide definitions, such as 'fairness through awareness"}, {"title": "Methods to Measure Fairness", "content": "To measure fairness, metrics are required. Extensive research to discover metrics and definitions for fairness in centralised\nmachine learning was necessitated by vast deployment in applications that are protected by anti-discrimination legislation20.\nFL must go through the same transition in order to pose as a viable alternative with distributed data. Despite the additional\ncomplexity with FL, some of the centralised metrics to detect algorithmic bias, are still insightful in the context of individual\nclient's performance18,19. The most prominent metrics under the umbrella of 'group fairness' are disparate impact and\ndemographic parity, alongside equalised odds and equal opportunity which were designed to overcome the shortcomings\nof the former two metrics20. Equalised odds is a metric that measures the difference in the true positive and false positive\nrates between two groups, one with and one without a binary sensitive attribute. Equal opportunity is the constrained case\nof equalised odds which only considers the true positive rates and is therefore less robust. For a binary sensitive attribute,\na \u2208 A = {a0, a1, ..., am} with a classifier's binary prediction, \u0176 and corresponding ground truth, Y, equalised odds is satisfied\nfor attribute the a, if Equation 1, holds. These metrics have been used with FL in the AgnosticFair21 and FCFL22 approaches.\nEqualised odds based measures are used in this work to evaluate sensitive group performance at the client level.\nP(\u0176 = 1|a = 0, Y = y) = P(\u0176 = 1|a = 1, Y = y),y \u2208 {0,1}                                                              (1)\nUniformity over certain metrics can be indicative of fairness. Average variance is used in the well renowned FL approaches\nof q-FedAvg23 and FedFV24 to evaluate uniformity of performance (usually, performance in FL refers to client accuracy or\nefficiency13 - client n is denoted as achieving performance xnk in round k), which they attribute to fairness. A drawback of\nusing variance based measures is the infinite range and lack of normalisation - this makes it difficult to compare approaches with\ndifferent statistical characteristics. An alternative measure of uniformity is Jain's Fairness Index (JFI)25, which is commonly\nused to evaluate resource allocation for computer networks using Transmission Control Protocol (TCP). In the existing\nadaptations of JFI for FL, the client's performance, is used as the independent variable, of which the uniformity is to be\nmeasured. The UCB-BS approach26, uses JFI to measure fairness and can be adapted for personalised models27. As defined in"}, {"title": "Fairness Conscious Approaches to FL", "content": "A number of existing works propose novel approaches to address aspects of the fairness problem in FL. Most commonly, they\ntarget uniformity of the accuracy achieved by the clients in classification tasks. Approaches such as AgnosticFair21, Ditto38,"}, {"title": "Proposed Techniques to Quantify Fairness", "content": "This section proposes four notions that, when used in combination, encompasses a general definition of fairness in FL. The\nnotions proposed in this section are complete, symptom driven (in order to be measurable and independent of the system's\ndesign) and are comprised of a logical definition in direct combination with an easily calculable equation, applicable to any FL\nsystem satisfying our assumptions. The fundamental building blocks for constructing the equations, are:\n\u2022 Client performance - this should be the target of the training's objective function and is typically the accuracy on an\nunseen test-set. For the remainder of this work, xn,k denotes the clients performance and is assumed to be the accuracy\nachieved by client n in round k, obtained through federated evaluation, however, this could be adapted for a range of\nperformance metrics.\n\u2022 Client contribution - the federated Shapley value up to round k for each client, sn is calculated using Equations 3 to 6. A\nlarger positive value indicates a more significant contribution of client n to the global model.\n\u2022 Client reward - denoted by rn.k, it is the cumulative reward received by the client from the server, n at the beginning of\nround k. Most commonly, the reward is a global model of higher performance than the client could train itself (however,\nit could also be financial). In this case, the reward is the accuracy measured on a unseen test-set, after the server has\ndistributed the model but before local training has taken place - this is naturally cumulative as the model accuracy after a\ngiven round represents the compounded training to that point.\nThe following notions define fairness in FL, each has an associated f value, valid f \u2208 (0,1] with a value of 1 attributed to\noptimal fairness, and poses a question that enables a user to understand the implications of the notion logically:\n1. Individual Fairness, fj,k - do all clients perform proportionately to their contribution?\nAs already shown, although uniformity of performance is important to fairness, if the client datasets exhibit statistical\nheterogeneity, clients should not be expected to achieve comparable performance. In this work, individual fairness is\nsatisfied if all clients achieve an accuracy, directly proportionate to their Shapley contributions. The gain, G is defined\nas the performance over the contribution and this work proposes that degree of uniformity of the gain should define\nindividual fairness as measured using JFI from Equation 2 and shown in Equation 7. In the case of independent and\nidentically distributed (iid) data across clients, it is predicted that client's contributions will be equivalent and the metric\nwill reduce to the classical measure of uniformity of performance.\nfj,k = J(G) where G = {\\frac{X_{n.k}}{S_n}: n\u2208S_k}                                                             (7)"}, {"title": "Results and Discussion", "content": "Federated Fairness Analytics enable the fairness of FL systems to be quantified, explained and visualised. This section displays\nsome results from the experiments described in Table 2, in order to demonstrate important insights about the effects of the\nframeworks, data settings and heterogeneity on fairness, which would not be possible without this work. As the tools are\ndesigned to be a modular add-on, these results only scratch the surface of the interesting and important possible applications of\nthese tools. Acceptable levels of fairness are subjective and application dependent, however, this work proposes that a threshold\nof 0.8 to constitute acceptable general fairness of a system, loosely corresponding to 80% of users achieving fairness in the JFI\nbased metrics.\nResultant System Fairness\nThe most atomic conclusions, are best visualised using the error bar and time series charts to compare experiments at the\nindividual metric level, as in Figure 3. The results from rounds 15 to 30 are mean-averaged in order to consider how each notion\nof fairness settles as the model converges. It is clear that fairness is sensitive to the approach, data setting, and heterogeneity\nand it is anticipated that numerous other factors will affect fairness. Key, previously unobtainable insights that can be drawn\nfrom this subset of systems are as follows, demonstrating that Federated Fairness Analytics enable a unique perspective to\nobserve systems:\n\u2022 The top row of Figure 3a shows that, with a cross-silo CIFAR-10 classifier in the iid setting, fairness is invariant with the\nchange from FedAvg to q-FedAvg. This is logical as q-FedAvg seeks fairer resource allocation but each client exhibits\nthe same distribution so q-FedAvg will tend to FedAvg.\n\u2022 Comparing to the bottom row of Figure 3a, non-iid data distributions (statistical heterogeneity) degrade fairness\nperformance with FedAvg as well as increasing its variance significantly. Moving to q-FedAvg, the bottom-right\ndemonstrates a significant improvement in both general fairness and minimum fairness at any point."}, {"title": "Conclusion", "content": "Primarily, it should be clear, that fairness is still an unsolved problem in FL. However, the objective of this work was not to\nresolve unfairness but to define and measure it, in order to evaluate FL systems. Building on existing literature, we developed\nof a set of four complete, symptom-driven notions which together encapsulate how unfairness can manifest itself in FL. Each\nnotion is paired with a quantifiable metric; these metrics underpin Federated Fairness Analytics - the methodology to actively\nmeasure fairness at training time. The analytics enable fairness-explainable-AI by producing insights into the effects of design\ndecisions on fairness that were previously not possible. This paper analysed the fairness performance of Twenty-Four different\nFL systems which vary in FL approach, ML task, data setting and heterogeneity. The results demonstrate that fairness typically\ndrops with increases in statistical heterogeneity or decreases in client participation rate. Under our definition, fairness-conscious\nFL approaches, Ditto and q-FedAvg are not shown to improve fairness in iid data settings where fairness is typically high\nand offer only marginal improvements in fairness in some non-iid settings. This development opens avenues for a plethora of\nfurther experiments and could be a key enabler to improve fairness. Many approaches to reduce unfairness should be tested but\nthis may include augmenting datasets with generative-AI to reduce statistical heterogeneity or utilising the measures of fairness\nas an optimisation objective.\nThe main limitation of Federated Fairness Analytics is the non-linearity of the metrics - this is a limitation of Jain's\nfairness index and may limit intuitive interpretation and comparison of results. Consideration should be given to whether linear\nmeasures such as the Gini coefficient may be more suitable than JFI and if it can be applied for all four metrics such that their\nnumerical meaning is directly translatable. Industry hardening would also require improving scalability. The computational\nbottleneck in the current version arises due to calculating full Shapley values, an O(K2|5k|) complexity operation. This requires\nimplementation of the Shapley approximations, which could also void the requirement for an auxiliary dataset, which, as\ndiscussed, is not obtainable in most real-world applications. To conclude, this paper summarises first works in fairness-XAI\nfor FL and provide a number of useful benchmarks. Future experimentation may aim to collect further data on the effects\nof varying number of participating clients, types of heterogeneity (also modulating heterogeneity, for example through the\nDirichlet a parameter), complexity of models and datasets, approaches and effects of privacy-enhancing techniques such as\ndifferential privacy. It would also be interesting to investigate more future-realistic FL scenarios by moving from simulation to\nnetworks of heterogeneous devices. Finally, investigating the fairness performance in systems utilising unsupervised learning,\nfoundational models and fine-tuning is crucial to align with current and future trends in FL."}]}