{"title": "Federated Fairness Analytics: Quantifying Fairness in Federated Learning", "authors": ["Oscar Dilley", "Juan Marcelo Parra-Ullauri", "Rasheed Hussain", "Dimitra Simeonidou"], "abstract": "Federated Learning (FL) is a privacy-enhancing technology for distributed ML. By training models locally and aggregating updates - a federation learns together, while bypassing centralised data collection. FL is increasingly popular in healthcare, finance and personal computing. However, it inherits fairness challenges from classical ML and introduces new ones, resulting from differences in data quality, client participation, communication constraints, aggregation methods and underlying hardware. Fairness remains an unresolved issue in FL and the community has identified an absence of succinct definitions and metrics to quantify fairness; to address this, we propose Federated Fairness Analytics - a methodology for measuring fairness. Our definition of fairness comprises four notions with novel, corresponding metrics. They are symptomatically defined and leverage techniques originating from XAI, cooperative game-theory and networking engineering. We tested a range of experimental settings, varying the FL approach, ML task and data settings. The results show that statistical heterogeneity and client participation affect fairness and fairness conscious approaches such as Ditto and q-FedAvg marginally improve fairness-performance trade-offs. Using our techniques, FL practitioners can uncover previously unobtainable insights into their system's fairness, at differing levels of granularity in order to address fairness challenges in FL. We have open-sourced our work at: https://github.com/oscardilley/federated-fairness.", "sections": [{"title": "Introduction", "content": "Data is naturally distributed around us, commonly made inert by data protection regulations. Classically, to train machine learning models, data is collected centrally \u2013 the data is brought to the model. However, such scenarios where data and compute power can be centralised are not archetypal of most of the world around us where data and processing infrastructure naturally exist in distributed pockets. This segmentation of data can be observed in many areas of modern life and it is common that the data is immovable between segments due to legislative rules. For example, corporations with data centres spread globally may be prevented from cross-border sharing due to regulation. Alternatively, in the UK's NHS, which consists of thousands of individual practices, patient information is stored on different systems which are not typically interconnected\u00b9. Segmentation also arises at in personal computing where users are not willing to share their personal data with third parties. These examples only scratch the surface of the possible distributed data scenarios and motivates the idea that many parties could benefit from the application of machine learning and privacy-preserving knowledge-sharing across the different segments, instead of taking the data to the model.\nFederated Learning was proposed in the seminal paper by B. McMahan from Google in 2017\u00b2. It presents the original approach for FL, FedAvg, and offers a privacy-preserving solution to the problem above, whilst being competitive in performance with its centralised counterparts in a set of typical machine learning tasks. Since its inception, FL has been deployed into a wide range of applications with cross-device implementations such as with Google's GBoard\u00b3; it is also rumoured, to be used in detecting the wake-up phrase, \u2018Hey Siri' for Apple's voice agent\u2074. The effectiveness of cross-silo scenarios have also been demonstrated, such as in a case where a small number of UK hospitals collected data and trained a model using FL to screen for Covid-19\u2075.\nIn recent, general-FL literature, fairness is repeatedly recognised as one of the key open problems. A 2021 review by P. Kairouz and B. McMahan of Google, designates a section to identifying fairness as a key area for future research in FL6. Other general surveys, from 2022 and 2023, also draw attention to the potential for heterogeneity to lead to unfairness, motivating further research7,8. Finally, in a talk at the 2024 Flower AI Summit, the Head of Samsung AI Europe, refers to the difficulty"}, {"title": "Technical Background", "content": "To form a technical definition of the fairness problem and the effects of heterogeneity, we consider the architecture and lexicon presented in Figure 1, which are typical to most FL systems. This considers that in each training round, k out of K total rounds, the server selects a set of participating clients, Sk, indexed by n from the set of C available clients, S, indexed by N where Sk \u2286 S = {C1, C2, ..., CN}. It is assumed that each client has a private dataset, Dn and that all clients are attempting to learn a model with the same structure despite potential differences in size and data distribution of the client's datasets. For each round, k, the server sends the global model parameters, Ok to each of the participating clients, which perform E epochs/ local rounds of training to obtain their parameters, 0n+1. The client transmits their parameters, 0n+1 back to the server, which, in the case of the FedAvg approach, performs weighted averaging to obtain the next round parameters, Ok+1\u00b7\nWhen defining a system, it is typical in FL to use the terms, cross-device, and cross-silo to differentiate between data settings15. Once a FL approach is realised, evaluation metrics are used to measure the performance of the models in the federation. Evaluation is challenging due to the private nature of the distributed data. Federated evaluation is used in this paper, whereby each client retains a proportion of its samples to construct a personal test-set for evaluation - the clients each inform"}, {"title": "Related Work", "content": "The matter of fairness is a well-established, albeit unsolved problem in centralised machine learning scenarios, where a single model is trained on a single dataset17\u201319. Given FL systems are defined by the interconnection of a number of smaller centralised systems, some traditional definitions are still relevant to federated settings, enabling comparison of some aspects of fairness between clients. These include some field wide definitions, such as 'fairness through awareness\" (whereby the model should be"}, {"title": "Existing Definitions of Fairness", "content": "expected to behave similarly for similar inputs) and those under the umbrella of 'group fairness' which seek to protect sensitive groups from bias by ensuring types of statistical parity across groups. Despite having some similarities, FL demands its own set of definitions of fairness to encapsulate its additional complexity.\nExisting research offering approaches for fair-FL define fairness narrowly, as such, the more general definitions from the recent, fairness-specific, survey papers are considered as a starting point. These break fairness into a number of notions either describing symptomatic conditions that would indicate unfairness or causal mechanisms that would lead to fairness. Symptom-based definition is favourable as notions are measurable without knowledge of the system's design and do not mandate the use of any specific techniques to achieve fairness. The definition from Shi et al. is limited as it is built on a combination of mechanisms and symptoms of unfairness which have many co-dependencies and overlaps, limiting intuitiveness13. Rafi et al. have a purely mechanism based definition14. The definition offered by Vucinich et al. is symptom based, meaning it is able to encompass a wide variety of specific sources of unfairness in three easily understood and observable types12. The approach is however limited, as although it considers parity in the local objective of the client's, it fails to consider the potential unfairness if the clients fail to also solve the global objective function for the orchestrator - which is the aim of the federation, as measured by either centralised or federated evaluation. This is particularly relevant in cross-device settings where the server aims to train a model using hundreds or thousands of devices to deploy to millions. The existing definitions have the following limitations, but pave the way for a more comprehensive, measurable and symptom-driven definition:\n\u2022 Failure to embrace notions that enable heterogeneous systems to be fair, demanding uniformity or equality as opposed to proportionality.\n\u2022 The lack of acknowledgment for the relevance of client contribution in quantifying fairness, building on the point above. Most approaches consider uniform client performance as fair irrespective of whether the client is 'free-riding' or behaving maliciously, for example.\n\u2022 The use of mechanism driven notions, which suggest that the application of a particular technique, such as \"minimising the inequity among clients at different points in time\" from expectation fairness, is sufficient to ensure fairness without observing the systems behaviour13.\n\u2022 Ignorance of fairness with respect to the server. A fair federation should also incentivise the orchestrator of the learning.\n\u2022 Lack of clear attribution of mathematical metrics to the descriptive notions."}, {"title": "Methods to Measure Fairness", "content": "To measure fairness, metrics are required. Extensive research to discover metrics and definitions for fairness in centralised machine learning was necessitated by vast deployment in applications that are protected by anti-discrimination legislation20. FL must go through the same transition in order to pose as a viable alternative with distributed data. Despite the additional complexity with FL, some of the centralised metrics to detect algorithmic bias, are still insightful in the context of individual client's performance18,19. The most prominent metrics under the umbrella of 'group fairness' are disparate impact and demographic parity, alongside equalised odds and equal opportunity which were designed to overcome the shortcomings of the former two metrics20. Equalised odds is a metric that measures the difference in the true positive and false positive rates between two groups, one with and one without a binary sensitive attribute. Equal opportunity is the constrained case of equalised odds which only considers the true positive rates and is therefore less robust. For a binary sensitive attribute, a \u2208 A = {a0, a1, ..., am} with a classifier's binary prediction, \u0176 and corresponding ground truth, Y, equalised odds is satisfied for attribute the a, if Equation 1, holds. These metrics have been used with FL in the AgnosticFair21 and FCFL22 approaches. Equalised odds based measures are used in this work to evaluate sensitive group performance at the client level.\nP(\u0176 = 1|a = 0, Y = y) = P(\u0176 = 1|a = 1, Y = y),y \u2208 {0,1} (1)\nUniformity over certain metrics can be indicative of fairness. Average variance is used in the well renowned FL approaches of q-FedAvg23 and FedFV24 to evaluate uniformity of performance (usually, performance in FL refers to client accuracy or efficiency13 - client n is denoted as achieving performance xnk in round k), which they attribute to fairness. A drawback of using variance based measures is the infinite range and lack of normalisation - this makes it difficult to compare approaches with different statistical characteristics. An alternative measure of uniformity is Jain's Fairness Index (JFI)25, which is commonly used to evaluate resource allocation for computer networks using Transmission Control Protocol (TCP). In the existing adaptations of JFI for FL, the client's performance, is used as the independent variable, of which the uniformity is to be measured. The UCB-BS approach26, uses JFI to measure fairness and can be adapted for personalised models27. As defined in"}, {"title": "Fairness Conscious Approaches to FL", "content": "A number of existing works propose novel approaches to address aspects of the fairness problem in FL. Most commonly, they target uniformity of the accuracy achieved by the clients in classification tasks. Approaches such as AgnosticFair21, Ditto38,"}, {"title": "Proposed Techniques to Quantify Fairness", "content": "This section proposes four notions that, when used in combination, encompasses a general definition of fairness in FL. The notions proposed in this section are complete, symptom driven (in order to be measurable and independent of the system's design) and are comprised of a logical definition in direct combination with an easily calculable equation, applicable to any FL system satisfying our assumptions. The fundamental building blocks for constructing the equations, are:\n\u2022 Client performance - this should be the target of the training's objective function and is typically the accuracy on an unseen test-set. For the remainder of this work, xn,k denotes the clients performance and is assumed to be the accuracy achieved by client n in round k, obtained through federated evaluation, however, this could be adapted for a range of performance metrics.\n\u2022 Client contribution - the federated Shapley value up to round k for each client, sn is calculated using Equations 3 to 6. A larger positive value indicates a more significant contribution of client n to the global model.\n\u2022 Client reward - denoted by rn.k, it is the cumulative reward received by the client from the server, n at the beginning of round k. Most commonly, the reward is a global model of higher performance than the client could train itself (however, it could also be financial). In this case, the reward is the accuracy measured on a unseen test-set, after the server has distributed the model but before local training has taken place - this is naturally cumulative as the model accuracy after a given round represents the compounded training to that point.\nThe following notions define fairness in FL, each has an associated f value, valid f \u2208 (0,1] with a value of 1 attributed to optimal fairness, and poses a question that enables a user to understand the implications of the notion logically:\n1. Individual Fairness, fj,k - do all clients perform proportionately to their contribution?\nAs already shown, although uniformity of performance is important to fairness, if the client datasets exhibit statistical heterogeneity, clients should not be expected to achieve comparable performance. In this work, individual fairness is satisfied if all clients achieve an accuracy, directly proportionate to their Shapley contributions. The gain, G is defined as the performance over the contribution and this work proposes that degree of uniformity of the gain should define individual fairness as measured using JFI from Equation 2 and shown in Equation 7. In the case of independent and identically distributed (iid) data across clients, it is predicted that client's contributions will be equivalent and the metric will reduce to the classical measure of uniformity of performance.\nfj,k = J(G) where G = {Xn.k/Sn: n\u2208Sk} (7)\n2. Protected Group Fairness, fg,k - do subgroups of the population that exhibit sensitive attributes perform equivalently to those without?\nAs discussed, equalised odds is the most robust and complete measure for group fairness in centralised learning, therefore it is the best starting point for calculating group fairness in FL. This section measures group fairness via an aggregation of the proposed equalised odds difference metrics derived from Equation 1. Each client, n is able to identify samples corresponding to the m protected groups during model validation, as stated in the paper assumptions. Therefore they can calculate the difference between the true positive and false positive rates for samples with and without each binary sensitive attribute, a \u2208 A = {a0, a1, ..., am}. As a result, each client calculates the equalised odds difference metric, En(a), using Equation 8 for all sensitive attributes a \u2208 A whenever it is selected for training. Note that the value liberated by Equation 8, En(a) \u2208 [0, 1], with a value of unity corresponding to fair treatment of the attribute, a.\nEn(a) = 1 - \u03a3|P(\u0176 = 1|a = 1,Y = y) \u2013 P(\u0176 = 1|a = 0,Y = y)| (8)\ny\u2208{0,1}\nThe clients report the difference metrics for each sensitive attribute to the server - the server calculates the means of each of the client's values (this can be carried out at the client to reduce communication load and for more privacy but transmitting all the metrics enables finer granularity in the results from the analytics tools). Group fairness over the federation is defined as median value of the difference metric means across the clients, as in Equation 9. The median is selected, over other methods of average as it more resilient to outliers, representing the central tendency of the federation more effectively.\nfg,k = med ( {\u03a3 En(a): n \u2208 Sk}) (9)\n3. Incentive Fairness, frik - are clients rewarded proportionately to their contributions and in acceptable time-frames?\nReward is important in FL, as an unfair incentive scheme may discourage participation - in the commercial realm, clients may join a competitor's federation. Incentive fairness is defined as the degree of uniformity, as measured by JFI, of the reward over contribution gains of the clients. This is logical as it enables heterogeneous systems to be fair, for example, if adversarial clients with poor data are rewarded unfavourably and clients with high quality datasets are generously rewarded but both are directly proportionate to their Shapley contribution, the system's incentive scheme will be deemed as fair. The mathematical definition is provided in Equation 10. A subtlety of the definition is its consideration of the temporal dimension. As both Shapley values and the reward are cumulative measures, at any round that it is measured, it considers the client's complete experience - enabling 'catch-up' or clients to be compensated retrospectively but a graph of fr,k through time would still show the period in which the reward was disproportionate. This relation to time is particularly important in the case of financial rewards where the delivery may be sporadic. In the case that the reward is the global model accuracy, it is important to note that frik is not equivalent to fik, as the accuracy relates to the global model 'as is' - evaluated on the local test-set prior to training compared to after local training. This is crucial as the reward should not consider the client's training ability - it represents the reward received given their contribution up to the end of the round it is measured.\nfrk = J(R) where R = {Xn,k/Sn: n\u2208Sk} (10)\n4. Orchestrator Fairness, fo,k - does the server succeed in its role of orchestrating a learning ecosystem that maximises the objective function?\nIn cross-device FL in particular, the server orchestrates the learning process in order to obtain a more successful model than clients could train locally, or that the server could train due to limitations in availability of suitable data or cost. For example, when an entity such as Google wants to train for GBoard, they want to use a small proportion of a population which are available and willing to participate, to train a model that can be deployed to millions of devices. As the server has invested in the process and has facilitated the learning, this work proposes that it would be unfair if, in exchange, the server did not receive a high performing model - this motivates the introduction of orchestrator fairness. Orchestrator fairness is measured as the mean average of the normalised client performance (accuracy is already normalised if this is the metric used), \u00c2nk as is shown in Equation 11.\n1\nfo,k = - \u03a3Xn,k (11)\nSk n\u2208Sk\nThe presented notions of fairness address the limitations of previous existing definitions, are symptomatically defined and simply quantified. By collecting these metrics, a new level of explainability can be added to FL systems, at varying levels of granularity, as a large amount of data can be collected and processed at the edge - enabling client, or even sensitive attribute level detail. At the other end of the spectrum, general fairness, Fr is presented in Equation 12 to attribute the fairness to a single number \u2208 (0, 1], it is a weighted sum of the four notions of fairness and Equation 13 represents the special case, in which all notions of fairness are equally weighted. Equation 12 is used where there is premise for application specific prioritisation of specific notions of fairness.\nFr = fjwj+fgwg + frwr+fowo where wj+ wg + wr + wo = 1 (12)\nfj+fg+fr+fo\nFT =\n(13)\n4"}, {"title": "Experimental Design", "content": "The experimental work in this paper aims to test the metrics for fairness analytics, on a wide range of FL scenarios. Many factors could impact fairness including the model, the dataset, heterogeneity conditions, the number of clients, the client sampling rate, the values of hyperparameters or the application of privacy-enhancing techniques. This section justifies the selection of the most prominent of these for experimentation, as displayed in Table 2. Figure 2 shows the architecture of the testbed used to simulate the differing conditions and to calculate fairness during training time."}, {"title": "FL Approaches", "content": "As discussed, a number of existing FL approaches attempt to address different aspects of fairness. The aim, is to evaluate a pair of the most promising approaches from the literature against the baseline approach, FedAvg. In order to select the most interesting benchmarks, all known approaches that primarily target fairness were qualitatively evaluated against our four notions of fairness. By analysing each publication, it was determined which of the logical definitions of fairness that the approach attempted to address (only the awareness/intent is assessed, not the success of the approach) and the key techniques used were noted. From this, the resulting frameworks in Table 1 were selected as the most interesting based on their provenience (publication, academic or commercial institutions and number of citations on Google Scholar), and the range of fairness notions that they addressed. For the additional approaches - q-FedAvg is selected as it is commonly accepted as the first approach seeking fairness and appears extensively in the literature. Ditto is also selected for variety - testing on a personalisation based algorithm. CGSV and FedMinMax were discarded as they were not compatible with our assumptions - FedMinMax requires all clients to participate in every round of training and CGSV uses model sparsification (model adaptation is out of the scope of this paper and would reduce the comparability between benchmarks)."}, {"title": "Data Sets and Models", "content": "The following labelled datasets and their corresponding classification models are selected for this work. It is important to note that optimal model design is out of scope and the lack of complete optimisation results in lower orchestrator fairness, fok.\n\u2022 CIFAR-10 is selected as it is one of the most commonly used datasets in machine learning experiments, it consists of 60,000 32 \u00d7 32 pixel, 3 channel colour images of 10 classes of item. Many high performing multi-class classifiers have been designed for CIFAR-10 and this paper uses the model from the Flower tutorial54. The CNN consists of a 2D convolution layer with max-pooling followed by another 2D convolutional layer and three fully connected linear layers. ReLu activation functions are used.\n\u2022 FEMNIST, also known as Federated-EMIST is one of the LEAF datasets55, designed for effective bench-marking of heterogeneous FL. It consists of 805,263, 28 \u00d7 28 pixel images of handwritten characters and is selected for its natural"}, {"title": "Introducing Heterogeneity", "content": "By default, most datasets are provided as a single set with a train:test split provided. For CIFAR-10, it is 83% : 17%, 90% :\n10% for FEMNIST and 81% : 19% for NSL-KDD. The datasets require splitting across the clients with both iid and non-iid\npartitioning in order to modulate the statistical heterogeneity. In order to make the experiments fair and repeatable, the strategy\nfor data partitioning must be clearly defined. Depending on the dataset, two approaches are used to divide the data amongst\nclients:\n\u2022 The FEMNIST dataset is designed with natural partitions - each client can represents a different writer exhibiting differing classes and numbers of samples. The dataset has 3,550 unique users with individual train-validation splits already defined. To create the non-iid split, 205 clients are randomly selected as used in Ditto (all selection in the paper is pseudo-random, using default random seeds) and for the iid partition, data samples from the whole set are randomly selected and assigned to clients, such that each client has 227 samples, corresponding to the average number of samples per user, defined in LEAF55. From the remaining 3345 users, 205 \u00d7 227 \u00d7 10% = 4654 samples are randomly selected to form the auxiliary dataset for the server to calculate Shapley values, maintaining the 90% : 10% train-test ratio.\n\u2022 CIFAR-10 and NSL-KDD are originally datasets for centralised learning so do not have natural splits. The whole test-set is retained as the server's auxiliary dataset and the train set is divided entirely amongst users. For iid partitioning, samples are randomly divided amongst clients such that each client has the same statistical distribution and number of samples. For the non-iid split, partitioning based on the Dirichlet distribution is used, with a = 0.5 for CIFAR-10 and a = 3 for NSL-KDD, the effects on fairness of varying a is a direction for future work. A discussion into partitioning techniques can be seen in the work from Iyer59. Each client's dataset is then split into a 90% training and 10% unseen validation set.\nFor comparability and to limit the computational complexity of Shapley calculations, all experiments sample 5 clients per round, with varying sample rates, \u03bcs. The CIFAR-10 experiments with a population of 10 clients represents a cross-silo setting whilst the rest represent cross device."}, {"title": "Results and Discussion", "content": "Federated Fairness Analytics enable the fairness of FL systems to be quantified, explained and visualised. This section displays some results from the experiments described in Table 2, in order to demonstrate important insights about the effects of the frameworks, data settings and heterogeneity on fairness, which would not be possible without this work. As the tools are designed to be a modular add-on, these results only scratch the surface of the interesting and important possible applications of these tools. Acceptable levels of fairness are subjective and application dependent, however, this work proposes that a threshold of 0.8 to constitute acceptable general fairness of a system, loosely corresponding to 80% of users achieving fairness in the JFI based metrics.\nResultant System Fairness\nThe most atomic conclusions, are best visualised using the error bar and time series charts to compare experiments at the individual metric level, as in Figure 3. The results from rounds 15 to 30 are mean-averaged in order to consider how each notion of fairness settles as the model converges. It is clear that fairness is sensitive to the approach, data setting, and heterogeneity and it is anticipated that numerous other factors will affect fairness. Key, previously unobtainable insights that can be drawn from this subset of systems are as follows, demonstrating that Federated Fairness Analytics enable a unique perspective to observe systems:\n\u2022 The top row of Figure 3a shows that, with a cross-silo CIFAR-10 classifier in the iid setting, fairness is invariant with the change from FedAvg to q-FedAvg. This is logical as q-FedAvg seeks fairer resource allocation but each client exhibits the same distribution so q-FedAvg will tend to FedAvg.\n\u2022 Comparing to the bottom row of Figure 3a, non-iid data distributions (statistical heterogeneity) degrade fairness performance with FedAvg as well as increasing its variance significantly. Moving to q-FedAvg, the bottom-right demonstrates a significant improvement in both general fairness and minimum fairness at any point.\n\u2022 Figure 3b demonstrates that for the FEMNIST dataset with iid partitioning, the variation in fairness is reduced by using Ditto instead of FedAvg. It also indicates that fairness is not static through training time, showing sporadic behaviour in early rounds, then settling - leading to the idea of fairness convergence in the training of FL systems. The full results also show that the temporal behaviour can depend on the approach, data conditions, ML task, etc.\nFairness-Performance Trade-off\nThe results in Figure 3 are more useful for fairness tuning at later stages of deployment, whereas those in Figure 4 can be used for top-level comparison of conditions. Orchestrator fairness is analogous to performance and these figures are used to observe the trade-off between fairness and performance over the full set of experiments. By selecting implementations on the dashed lines, which show the best conditions, users can select approaches that favour fairness over performance, or vice versa, as is shown. It is particularly interesting to observe that for the FEMNIST task, Ditto directly trades performance for fairness and q-FedAvg under performs. Whereas for NSL-KDD q-FedAvg offers the best in fairness and performance. In all cases, fairness reduces with statistical heterogeneity, typically by a greater margin than a fairness-conscious approach can remedy. In the CIFAR-10 experiments, it shows that cross-device and cross-silo settings exhibit different characteristics, indicating that fairness is sensitive to the client participation rate, us and dataset size."}, {"title": "Conclusion", "content": "Primarily, it should be clear, that fairness is still an unsolved problem in FL. However, the objective of this work was not to resolve unfairness but to define and measure it, in order to evaluate FL systems. Building on existing literature, we developed of a set of four complete, symptom-driven notions which together encapsulate how unfairness can manifest itself in FL. Each notion is paired with a quantifiable metric; these metrics underpin Federated Fairness Analytics - the methodology to actively measure fairness at training time. The analytics enable fairness-explainable-AI by producing insights into the effects of design decisions on fairness that were previously not possible. This paper analysed the fairness performance of Twenty-Four different FL systems which vary in FL approach, ML task, data setting and heterogeneity. The results demonstrate that fairness typically drops with increases in statistical heterogeneity or decreases in client participation rate. Under our definition, fairness-conscious FL approaches, Ditto and q-FedAvg are not shown to improve fairness in iid data settings where fairness is typically high and offer only marginal improvements in fairness in some non-iid settings. This development opens avenues for a plethora of further experiments and could be a key enabler to improve fairness. Many approaches to reduce unfairness should be tested but this may include augmenting datasets with generative-AI to reduce statistical heterogeneity or utilising the measures of fairness as an optimisation objective.\nThe main limitation of Federated Fairness Analytics is the non-linearity of the metrics - this is a limitation of Jain's fairness index and may limit intuitive interpretation and comparison of results. Consideration should be given to whether linear measures such as the Gini coefficient may be more suitable than JFI and if it can be applied for all four metrics such that their numerical meaning is directly translatable. Industry hardening would also require improving scalability. The computational bottleneck in the current version arises due to calculating full Shapley values, an O(K2|5k|) complexity operation. This requires implementation of the Shapley approximations, which could also void the requirement for an auxiliary dataset, which, as discussed, is not obtainable in most real-world applications. To conclude, this paper summarises first works in fairness-XAI for FL and provide a number of useful benchmarks. Future experimentation may aim to collect further data on the effects of varying number of participating clients, types of heterogeneity (also modulating heterogeneity, for example through the Dirichlet a parameter), complexity of models and datasets, approaches and effects of privacy-enhancing techniques such as differential privacy. It would also be interesting to investigate more future-realistic FL scenarios by moving from simulation to networks of heterogeneous devices. Finally, investigating the fairness performance in systems utilising unsupervised learning, foundational models and fine-tuning is crucial to align with current and future trends in FL."}, {"title": "Acknowledgements", "content": "This work is a contribution by Project REASON, a UK Government funded project under the Future Open Networks Research Challenge (FONRC) sponsored by the Department of Science Innovation and Technology (DSIT)."}, {"title": "Author contributions statement", "content": "The individual contributions of each author can be summarised as:\n\u2022 Oscar Dilley: Conceptualisation, Formal Analysis, Investigation, Methodology, Validation and Writing.\n\u2022 Juan Marcelo Parra-Ullauri: Conceptualisation, Validation, Review and Supervision.\n\u2022 Rasheed Hussain: Funding acquisition, Conceptualisation, Validation, Review and Supervision.\n\u2022 Dimitra Simeonidou: Funding acquisition, Review and Supervision."}, {"title": "Data Availability Statement", "content": "Our machine learning models are trained and tested using the following open-source datasets which can be accessed via Hugging Face as follows:\n\u2022 uoft-cs/cifar10: https://huggingface.co/datasets/uoft-cs/cifar10\n\u2022 flwrlabs/femnist: https://huggingface.co/datasets/flwrlabs/femnist\n\u2022 Mireu-Lab/NSL-KDD: https://huggingface.co/datasets/Mireu-Lab/NSL-KDD\nRaw data collected from our experiments, in JSON format, an archive of plots, plotting scripts and per-experiment hyper- parameter selection are made available publically on GitHub: https://github.com/oscardilley/federated-fairness."}, {"title": "Additional information", "content": "The authors declare no competing interests."}]}