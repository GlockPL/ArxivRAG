{"title": "STAGE: Simplified Text-Attributed Graph Embeddings Using Pre-trained LLMS", "authors": ["Aaron Zolnai-Lucas", "Jack Boylan", "Chris Hokamp", "Parsa Ghaffari"], "abstract": "We present Simplified Text-Attributed Graph Embeddings (STAGE), a straightforward yet effective method for enhancing node features in Graph Neural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our approach leverages Large-Language Models (LLMs) to generate embeddings for textual attributes. STAGE achieves competitive results on various node classification benchmarks while also maintaining a simplicity in implementation relative to current state-of-the-art (SoTA) techniques. We show that utilizing pre-trained LLMs as embedding generators provides robust features for ensemble GNN training, enabling pipelines that are simpler than current SoTA approaches which require multiple expensive training and prompting stages. We also implement diffusion-pattern GNNs in an effort to make this pipeline scalable to graphs beyond academic benchmarks.", "sections": [{"title": "1 Introduction", "content": "A Knowledge Graph (KG) typically includes entities (represented as nodes), relationships between entities (represented as edges), and attributes of both entities and relationships (Ehrlinger and W\u00f6\u00df, 2016). These attributes, referred to as metadata, are often governed by a domain-specific ontology, which provides a formal framework for defining the types of entities and relationships as well as their properties. KGs can be used to represent structured information about the world in diverse settings, including medical domain models (Kon\u00e9 et al., 2023), words and lexical semantics (Miller, 1995), and commercial products (Chiang et al., 2019).\nText-Attributed Graphs (TAGs) can be viewed as a subset of KGs, where some node and edge metadata is represented by unstructured or semi-structured natural language text (Yang et al., 2023). Examples of unstructured data values in TAGs could include the research article text representing the nodes of a citation graph, or the content of social media posts that are the nodes of an interaction graph extracted from a social media platform. Many real-world datasets are naturally represented as TAGS, and studying how to best represent and learn using these datasets has received attention from the fields of graph learning, natural language processing (NLP), and information retrieval.\nGraph Learning and LLMs With the emergence of LLMs as powerful general purpose reasoning agents, there has been increasing interest in integrating KGs with LLMs (Pan et al., 2024). Current SoTA approaches combining graph learning with (L)LMs follow either an iterative or a cascading method. Iterative methods involve jointly training an LM and a GNN for the given task. While this approach can produce a task-specific feature space, it may be complex and resource-intensive, particularly for large graphs. In contrast, cascading methods first apply an LM to extract node features which are then used by a downstream GNN model. Cascading models demonstrate excellent performance on TAG tasks (He et al., 2024; Duan et al., 2023a), although they often require multiple stages of training targeted at each pipeline component. More recent cascading techniques implement an additional step, known as text-level enhancement (Chen et al., 2024), whereby textual features are augmented using an LLM.\nSimplifying Node Representation Generation To the best of our knowledge, all existing cascading approaches require multiple rounds of data generation or finetuning to achieve satisfactory results on TAG tasks (He et al., 2024; Duan et al., 2023a; Chen et al., 2024). This bottleneck increases the difficulty of applying such methods to real-world graphs. Our proposed method, STAGE, aims to simplify existing approaches by foregoing LM finetuning, and only making use of a single pre-"}, {"title": "2 Background", "content": "Text-Attributed Graphs Yan et al. (2023) suggest that integrating topological data with textual information can significantly improve the learning outcomes on various graph-related tasks. Chien et al. (2022) incorporate graph structural information into the pre-training stage of pre-trained language models (PLMs), achieving improved performance albeit with additional training overhead, while Liu et al. (2023) further adopt sentence embedding models to unify the text-attribute and"}, {"title": "3 Approach", "content": "Our cascading approach consists of two steps:\n\u2022 A zero-shot LLM-based embedding generator is used to encode the title and abstract (or"}, {"title": "3.1 Text Embedding Retrieval", "content": "For the text embedding model, we select a general-purpose embedding LLM that ranks highly on the Massive Text Embedding Benchmark (MTEB) Leaderboard\u00b9. Specifically, we evaluate gte-Qwen1.5-7B-instruct, LLM2Vec-Meta-Llama-3-8B-Instruct, and SFR-Embedding-Mistral. MTEB ranks embedding models based on their performance across a wide variety of information retrieval, classification and clustering tasks. This model is used out-of-the-box without any finetuning. An appealing aspect of LLM-based embeddings is the possibility to add instructions alongside input text to bias the embeddings for a given task. We empirically evaluate the effect of instruction biased embeddings is in Table 2 of section 4.\nNode representations X are generated using only the title and abstract, or equivalent textual node attributes, omitting the LLM predictions and explanations provided by (He et al., 2024). X will then be used as enriched node feature vectors for training a downstream GNN ensemble."}, {"title": "3.2 GNN Training", "content": "Using the previously generated embeddings X as node features, we train an ensemble of GNN models on the node classification task:\n$Loss_{cls} = L(\u03a6(GNN(X, A)), Y),$ (1)\nwhere $\u03a6(\u00b7)$ is the classifier, A is the adjacency matrix of the graph and Y is the label. For the GNN architectures we choose GCN (Kipf and Welling, 2017), SAGE (Hamilton et al., 2018) and RevGAT (Li et al., 2022a). We also evaluate a multi-layer perceptron (MLP) (Haykin, 1994) among our GNN models. To combine the predictions from each of the K models in the ensemble, we compute the mean prediction as follows:\n$p = \\frac{1}{K} \\sum_{k=1}^{K} p_k,$ (2)\nCross-entropy loss is used to compute the loss value.\nDiffusion-based GNNs For a graph G with node features X, a diffusion operator is a matrix $A_{op}$ with the same dimensions as the adjacency matrix of G. Diffused features H are then calculated via $H = A_{op}X$.\nWe explored Simple-GCN (Wu et al., 2019) and SIGN (Frasca et al., 2020), both of which employ adjacency-based diffusion operators to pre-aggregate features across the graph before training. SIGN is a generalization of Simple-GCN, to extend to Personalized-PageRank (Page et al., 1998) and triangle-based operators. This allows expensive computation to be carried out by distributed computing clusters or efficient sparse graph routines such as GraphBLAS (Davis, 2019), which do not need to back-propagate through graph convolution. The prediction head can then be a shallow MLP or logistic regression. We provide implementation specifics in appendix section C to ensure repeatability."}, {"title": "3.3 Parameter-efficient Finetuning LLM", "content": "Motivated by the node classification performance gains seen by (Duan et al., 2023a) using PEFT, we finetune an LLM on the node classification task. Concretely, we use an LLM embedding model with a low-rank adapter (LoRA) (Hu et al., 2021a) and a densely connected classifier head. The pre-trained LLM weights remain frozen as the model trains on input text T to reduce loss according to:\n$Loss_{cls} = L(\u03a6(LLM(T)), Y)$ (3)\nwhere $\u03a6(\u00b7)$ is the classifier head and Y is the label. Again, we use cross-entropy loss to compute the loss value."}, {"title": "4 Experiments", "content": "We investigate the performance of STAGE over five TAG benchmarks: ogbn-arxiv (Hu et al., 2021b), a dataset of arXiv papers linked by citations; ogbn-products (Hu et al., 2021b), representing an Amazon product co-purchasing network; PubMed (Sen et al., 2008), a citation network of diabetes-related scientific publications; Cora (McCallum et al.,"}, {"title": "5 Analysis", "content": "Main Results (Table 1) We find that ensembling GNNs always leads to superior performance across datasets when taking the STAGE approach.\nDespite the reduced computational resources and training data requirements, the STAGE method remains highly competitive across all benchmarks. The ensemble STAGE approach lags behind the TAPE pipeline by roughly 5% on Cora, 3.5% on Pubmed, 0.8% on ogbn-products, and 4% on tape-"}, {"title": "6 Conclusions", "content": "This work introduces STAGE, a method to use pre-trained LLMs as text encoders in TAG tasks without the need for finetuning, significantly reducing computational resources and training time. Additional gains can be achieved through parameter-efficient finetuning of the LLM. Data augmentation, which is orthogonal to our approach, could improve performance with general-purpose text embedding models. However, it likely remains intractable for many large-scale datasets due to the need to query a large model for each node.\nWe also demonstrate the effect of diffusion operators (Frasca et al., 2020) on node classification performance, decreasing TAG pipeline training time substantially. We aim to examine the scalability of diffusion-pattern GNNs on larger datasets in later work."}, {"title": "A Appendix", "content": null}, {"title": "B Negative Results", "content": "Co-training LLM and GNN: In a similar approach to iterative methods, we investigated co-training the LLM and GNN on the ogbn-arxiv node classification task to facilitate a shared representation space. This proved unfeasible due to the memory requirements exceeding the capacity of one A100 GPU."}, {"title": "C Implementation of Diffusion Operators", "content": "We implement diffusion operators from two methods, Simple-GCN (Wu et al., 2019) and SIGN (Frasca et al., 2020). In the case of SIGN, the authors omit implementation details of the operators, so we include them here."}, {"title": "C.1 Parallelism of diffusion operators", "content": "All operations above can be be parallelized across columns of X, either keeping A in shared memory on one machine or keeping a copy on each executor in a distributed computing infrastructure like Apache Spark."}, {"title": "D Preprocessing & Model Selection for Diffusion Operators", "content": "For Simple-GCN (Wu et al., 2019), we set the degree k by selecting the highest validation accuracy from k = 2, 3, 4, of which k = 2 had the highest accuracy in each case. For SIGN (Frasca et al., 2020), we choose s, p, t from the highest validation accuracy amongst (3,0,0) (3,0,1) (3,3,0), (4, 2, 1) (5, 3, 0). For Cora and PubMed, (4, 2, 1) was chosen, and for ogbn-arxiv, ogbn-products, and tape-arxiv23 (3,3,0) was chosen. We chose the number of layers for the Inception NLP to match the number of layers in other GNNs tested, 4. We did not perform additional hyper-parameter tuning. When preprocessing the embeddings, we centered and scaled the data to unit variance for Simple-GCN and SIGN only."}, {"title": "E Model Trainable Parameters", "content": null}, {"title": "F Ablation Study", "content": "To study the effect each model has on the GNN ensemble step of STAGE, we perform a detailed ablation study. The results are shown in Table 6."}, {"title": "G Datasets", "content": "In this section, we describe the characteristics of the node classification datasets we used during our work. The statistics are shown in Table 7."}, {"title": "H Instruction-biased Embeddings", "content": "In Table 8 we list the specific instructions used to 655 investigate the effect of biasing embeddings."}]}