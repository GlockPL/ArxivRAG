{"title": "MUSIC PROOFREADING WITH REFINPAINT: WHERE AND HOW TO MODIFY COMPOSITIONS GIVEN CONTEXT", "authors": ["Pedro Ramoneda", "Martin Rocamora", "Taketo Akama"], "abstract": "Autoregressive generative transformers are key in music generation, producing coherent compositions but facing challenges in human-machine collaboration. We propose RefinPaint, an iterative technique that improves the sampling process. It does this by identifying the weaker music elements using a feedback model, which then informs the choices for resampling by an inpainting model. This dual-focus methodology not only facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles but also offers a valuable tool for humans seeking to refine their compositions with automatic proofreading. Experimental results suggest RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its value for refining music created by both machines and humans. This approach not only facilitates creativity but also aids amateur composers in improving their work.", "sections": [{"title": "1. INTRODUCTION", "content": "Advanced autoregressive models [1, 2] have enabled the automatic generation of complex musical performances [3-7]. However, while autoregressive models generate music in a strictly forward-moving manner, human composers often follow a more iterative approach, frequently revisiting and refining earlier sections of a piece before proceeding [8-10]. Although there are some iterative methods for music generation [11-13], there are still areas for improvement in terms of controllability and human-in-the-loop aspects, such as inferring where to modify composition and inpainting capability to enable partial modification.\nIterative refinement proved effective for image generation; in particular, Lezama's Token-Critic [14] shows how feedback mechanisms can enhance image synthesis. Similarly, such feedback could benefit music composition for iteratively refining generated music. Within the spectrum of music composition tools, the Piano Inpainting Application (PIA) [15] stands out for its capabilities for automatic music generation that addresses the missing parts of musical performances, a technique referred to as inpainting. We highlight their handling of the musical context both before and after the selected gaps, enabling precise note-level inpainting. On account of that, inspired by image generation's success with iterative feedback and how PIA handles music context, our research explores applying these concepts to enhance controllability, human-in-the-loop functionality, and iterative refinement capability in automatic music generation.\nIn this work, drawing from Token-Critic and PIA, we propose RefinPaint, which aims to boost automatic inpainting and proofreading in music generation. Our approach includes an iterative process of identifying areas in a composition needing modification and applying inpainting techniques to these areas. In this context, proofreading refers to automatically identifying and correcting errors or inconsistencies in a music composition. This dual-focus methodology facilitates the machine's ability to improve its automatic inpainting generation through repeated cycles, and offers a valuable tool for humans seeking to refine their compositions with automatic proofreading.\nOur RefinPaint method is grounded in an autoregressive inpainting model to generate synthetic music tokens and a feedback model trained to distinguish between original and synthetic tokens. This differentiation is key during the sampling stage when deciding on token retention or revision. RefinPaint takes an iterative approach, integrating feedback into the inpainting model for selectively regenerating parts in each iteration, as Figure 1 shows. In contrast to Token-Critic, RefinPaint focuses on modifying a specific part of a composition using a contextual model and exposes the intermediate outputs of the autoregressive inpainting model to human inspection in each iteration.\nThe human-in-the-loop approach we propose allows for selecting the number of tokens to modify and revise the analysis heatmap at each iteration, as described in the following section. Through experimentation, we confirm RefinPaint's effectiveness in inpainting and proofreading tasks, demonstrating its utility for enhancing music created by both machines and humans. Finally, we provide a companion page featuring examples \u00b9 and the code along with the trained models of RefinPaint to ensure reproducibility 2."}, {"title": "2. METHODOLOGY", "content": "Our proposed methodology employs two models: an inpainting model I, and a feedback model F, alongside our iterative algorithm RefinPaint. Initially, F identifies areas within a MIDI file that need improvement based on the specific criteria described in Section 2.2. It uses a heatmap for detailed MIDI token-level feedback, allowing one to assess the context and relevance of each note in the selected region. Then, model I can regenerate the selected tokens considering the feedback, as described in Section 2.1. The methodology involves using both models iteratively with RefinPaint and encompasses three main stages: training the inpainting model (Section 2.1.1), training the feedback model (Section 2.2.1), and finally executing the iterative process for MIDI sequence generation (Section 2.3).\n2.1 Inpainting model (1)\nThe inpainting model aims to predict, or fill in, missing parts of a MIDI sequence based on a given mask. We adopt an encoder-decoder architecture for sequence-to-sequence tasks, inspired by the PIA study for music generation [15]. This model involves an encoder converting input data into a latent representation and a decoder predicting the final output."}, {"title": "2.1.1 Training the Inpainting Model (I)", "content": "The training process is outlined in Algorithm 1. A batch x is sampled from the MIDI dataset D, and a random fragment $M_u$ is chosen for each sample in x with a length determined by $t_1$. It is important to note that $t_1$ refers to the length in terms of the token sequence, rather than the MIDI duration. Consequently, a random mask $M_s$, with the masking ratio controlled by $\\gamma(t_2)$, is then applied to $M_u$. The forward pass of the model calculates the loss using the batch x, the mask $M_s$, and the Cross Entropy (CE) loss function to evaluate the difference between the predicted outputs and the actual labels. The model is subsequently updated via gradient descent. The function $\\gamma$, a cosine scheduler, dynamically adjusts the masking ratio. It operates on a domain defined by a random variable $t_2$ within the interval [0, 1]. Specifically, for any chosen value $t_2$ drawn uniformly from the interval [0, 1], the value undergoes a cosine transformation $\\gamma$ to determine the masking ratio, where $\\gamma(t_2) = \\cos(\\frac{\\pi t_2}{2})$."}, {"title": "2.2 Feedback model (F)", "content": "We employ an encoder-only transformer architecture for the feedback phase that classifies music tokens as fake or real. We use this output distribution to select the k most realistic tokens to retain while the others are regenerated. Unlike the encoder-decoder inpainting model, I, this model processes the input through a parallel and bidirectional attention mechanism without employing any attention masks, thus facilitating an unrestricted analysis of the musical context. Additionally, we add an extra binary embedding to the encoder input with information about the mask $M_u$-the selected fragment-for the feedback model."}, {"title": "2.2.1 Training the Feedback model (F)", "content": "After training the inpainting model I, we train an encoder-only feedback model F. This model aims to evaluate the output from I, offering feedback on the composition quality of each music fragment denoted by $M_u$.\nOne ideal way of training F would involve a vast dataset of computer- or human-generated music compositions and human experts' revisions for inpainting and proofreading applications. Instead, we propose a more feasible synthetic training strategy. The inpainting model I generates tokens within the selected fragment of a music piece, $M_u$, which we label as 'Fake', while we label as 'Real' the original unchanged tokens. We utilize these labels to instruct F, following the process illustrated in Figure 3.\nThe training of F is based on the output of I. We begin by sampling a batch x from the dataset D, then apply masking $M_s$ and $M_u$. Model I regenerates specific tokens within x, yielding a modified output $\\hat{x}$. Model F then assesses each token of $\\hat{x}$ against $M_s$, categorizing them as 'Real' or 'Fake'. The loss L for F is computed using the Binary Cross Entropy (BCE) loss function, and is minimized through gradient descent. The outcome is a heatmap for $M_u$, which indicates the probability of each token being 'Real' or 'Fake', determined by the sigmoid activation of the model output."}, {"title": "2.3 Generation of MIDI sequences (RefinPaint)", "content": "We capitalize on the strengths of the inpainting and feedback models for the iterative MIDI sequence generation. The process begins with a MIDI sequence x introduced by the user, setting the stage for a loop that spans a predetermined number of iterations T.\nInitially, the user selects the fragment to be modified $x^{(0)}$ and sets the initial selection rate $k_0$ for complete inpainting. Alternatively, different values for k allow the user to control how much of the content to keep in the selected fragment when proofreading.\nAt each iteration t, the inpainting model I generates a new version of the sequence $\\hat{x}$, based on the current masked input $x_m$. In the human-in-the-loop scenario, the user can then adjust this generated sequence. The feedback model F evaluates $\\hat{x}$ and provides a new mask $M^{(t+1)}$, which the user may also modify. This mask highlights the tokens that are deemed most realistic. The number of selected realistic tokens k follows a decreasing function $\\gamma$ of the iteration t, which models the increasing confidence in the tokens produced over time. Moreover, we add an extra binary embedding to the encoder input with information about the mask M-the given context-where M changes over iterations.\nRefining the music sequence through each iteration aims to achieve a compositional process that closely aligns with that of a human composer so that the user intervention becomes interpretable and natural. It fosters a collaborative environment between the user and the machine and tailors the generation process to the user's specific directives and preferences."}, {"title": "3. RELATED WORK", "content": "Automatic music generation has rapidly advanced recently. Significant progress has been made [4-6], especially in solo piano compositions [3, 7, 15], through the capabilities of autoregressive models in producing coherent musical outputs. However, several challenges remain for creating successful interactions with humans [3, 11, 15-22].\nPrevious work has explored various approaches to generate music iteratively and allowed for partial modification-often referred to as inpainting-, which enhances controllability. Among them, sequential handling of musical elements has been a common strategy, as in models like DeepBach [11] and Coconet [12]. Although these models allow for inpainting and iterative generation, they often rely on random iterations without a mechanism for discriminative feedback to guide improvements. This lack of directed refinement contrasts with the human compositional process, which typically involves iterative improvements based on evaluative feedback. Our proposed approach addresses this limitation by incorporating a feedback model that identifies areas for improvement for both humans and machines to refine the composition.\nAlthough it is not designed as an inpainting model, ES-Net's approach to music generation integrates generative and discriminative capabilities in one model [13], with a feature for correcting past errors for iterative refinement. Our model differs significantly: it takes into account the context of the selected fragment, could improve any existing inpainting model, and can handle general MIDI formats. In [23], the authors propose a GAN model for piano music composition with a discriminator model that discerns real and fake compositions in the training process. However, it does not give feedback on which generated parts are good or bad and does not create compositions iteratively. Yet, the application of discriminative feedback in music generation, particularly in a manner that mimics human iterative refinement, remains largely unexplored.\nFinally, inpainting models in music have seen various approaches but remain less studied compared to their counterparts in image generation [24]. They typically focus on quantized scores, with significant contributions like Gibbs sampling for Bach chorales [11] and RNN-based melodies inpainting [25]. Studies on transformers for multitrack inpainting have advanced the field, such as MMM [26], which utilizes a decoder architecture akin to GPT2 [2], and PIA [15], which uses a specialized transformer design. We chose PIA over MMM as a ground element in this work, given it is capable of working in the token level or larger contexts and inpainting multiple little fragments at the same time, similar to Token-Critic's generator [14]."}, {"title": "4. EXPERIMENTAL SETUP", "content": "4.1 Data preparation\nOur study utilizes the Lakh MIDI dataset (LMD), an extensive collection of approximately 170k unique multi-track MIDI files, compiled by Colin Raffel for music research [27]. The dataset offers a wide variety of music, albeit with varying quality due to its internet-sourced nature. Despite this, the volume and diversity of the LMD dataset make it a valuable asset for our proofreading task. We extracted only the piano parts, totaling 120,000 tracks. We tokenize the piano tracks using REMI (REvamped MIDI-derived events) [16], a music representation method that converts MIDI events into a structured format optimized for Transformer-based models that significantly enhances their ability to comprehend and produce music. REMI categorizes music elements into distinct event types, including timing for rhythm and note events for melody, but we exclude velocity events for simplicity. Specifically, we use a modified version of REMI tailored for handling single-track piano performances, as implemented in [28]. The dataset was split into training (hashes 0-d), validation (hash e), and testing (hash f) segments, based on each file's MD5 hash's leading digit, akin to previous methods [5, 6]\n4.2 Model development\nWe train the inpainting and feedback models with the AdamW optimizer, using eighty per cent of the dataset for training and the remainder for validation. Each epoch consists of a randomly selected fragment from the training set, 512 tokens in length. We also employ an augmentation procedure that transposes the pitch tokens of a sequence by adding or subtracting up to 6 semitones. For the inpainting model, we apply a cross-entropy loss and use the maximum batch size that our system can handle; a single V100 GPU with 16GB allows for 48 samples. The encoder-decoder inpainting model comprises 12 layers: 4 encoder layers and 8 decoder layers, similar to the original PIA, with 8 heads and an embedding dimension of 512. We employ a cosine scheduler for training, with 16,000 warmup steps, reaching up to a 0.0006 learning rate. The feedback model consists of 6 layers, with an embedding dimension of 512, a dropout rate of 0.1, 8 heads, and the same cosine scheduler. Finally, we acknowledge that optimizing these models was not the main focus of this paper, so there might be better hyperparameter values.\nIn the particular case of proofreading without human intervention, i.e. for evaluation purposes, the final output is the iteration that maximizes the feedback model probability distribution. Using a sigmoid function, the model determines whether each token in a sequence is fake or real. By averaging the output probabilities, we calculate a global feedback score (GFS) for the sequence's overall realism and select the best regeneration output based on it."}, {"title": "5. INPAINTING RESULTS", "content": "5.1 Divide and conquer with the inpainting model\nWe conducted an experiment to explore how the model's inpainting performance is affected by the percentage of tokens to inpaint in a selected fragment. We hypothesize that the more tokens to inpaint, the harder the problem is, so the model performance is lower. The experiment uses the inpainting model trained as detailed in section 2.1.1, and we report its Negative Log-Likelihood (NLL) loss and perplexity of the next predicted token. The evaluation covered the entire test set, with masking ratios ranging from 1 (fully masked) to 0 (no tokens to inpaint) and a fixed 30% fragment size rate of the 512 tokens sequence. Results shown in Table 1 indicate better performance with reduced masking, confirming our hypothesis. Notably, the average Perplexity value is less than half at 0.05 compared to the 1.0 masking ratio. This finding is crucial for RefinPaint's effectiveness as it reduces the number of tokens to be inpainted in subsequent iterations, considering the iterative process as a top-to-bottom strategy.\n5.2 Objective evaluation of proofreading inpainting\nThis section conducts a comparative analysis between the reference inpainting output, as described in [15] (PIA), and our enhanced method. Our method applies the RefinPaint proofreading process to the initial PIA's inpainting output over ten iterations and is referred to as 'Ours'. For fragment sizes of 50%, 30%, and 10% of the 512-token test sequences, we computed 1,000 instances each. It is important to note that the PIA method discussed is our reimplementation, since the original code was not available.\nTable 2 shows the average global feedback score (GFS), computed as explained in Section 4.2, and the number of evaluations in which each algorithm outperforms the other (Wins) and in which their scores are the same (Ties). Table 3, on the other hand, focuses on the comparison between PIA and Ours, employing the NLL loss, a metric of the next token prediction in generated music. This metric, derived from an autoregressive model we trained explicitly from scratch to assess the inpainting results, is a benchmark metric in our evaluation. Similar evaluations have been employed in previous studies in natural language processing [29] and music generation [30]. Consequently, our study employs a 12-layer Transformer-based autoregressive model with REMI representation. Our goal is to assess the similarity between the distribution of musical elements in inpainted sections and those in the original dataset, including aspects such as rhythms, harmony, or melodies. A lower NLL loss indicates a more accurate prediction of the next token, reflecting a closer approximation to the dataset's inherent musicality. Note we assess this metric over the entire output sequence."}, {"title": "5.3 Listening test of proofreading inpainting", "content": "While computational metrics provide valuable insights into the quality of our inpainted music sections, human perception adds another perspective for evaluating musical quality and appeal. A user-based evaluation was conducted to capture a holistic view of the inpainted outputs' musicality.\nFor each experiment, which involved 50%, 30%, and 10% fragments of inpainted content, 15 different annotators evaluated both the first iteration of inpainted content (PIA) and the complete iterative process of RefinPaint (Ours) for ten iterations. Participants were exposed to two scenarios, Experiment 1 and Experiment 2: one from the PIA model and one from our RefinPaint model. The order in which these pairs were presented was randomized to avoid any bias. Additionally, we provided the original music fragment without the inpainted content for reference. Participants listened to both the PIA and RefinPaint versions before making their evaluations. They were asked to assess the inpainted content's quality by comparing it to their first draft, focusing specifically on coherence and creativity. To make their choice, participants were given four options to prevent bias: 'Experiment 1,' \u2018Maybe Experiment 1,' 'Maybe Experiment 2,' and 'Experiment 2'.\nFigure 5 shows the listening test results. Firstly, PIA got lower preference scores than RefinPaint for the different fragment size conditions. In addition, RefinPaint's performance for different fragment sizes shows that the coherence scores increase as the fragment size gets larger, even if the creativity varies. This means that as there is more to inpaint, RefinPaint gets better at being coherent. In contrast, PIA does not show such a strong trend.\nThe quantitative and qualitative evaluations point towards a clear trend: Refinpaint tends to yield superior inpainting results when proofreading machine inpainted sections compared to the baseline. Our methodology produces music sequences that are more consistent, perceptually closer to the original, and preferred by listeners."}, {"title": "6. CASE OF STUDY ON PROOFREADING AMATEUR COMPOSITIONS", "content": "We conducted an additional study to explore the proposed system's capabilities for proofreading music compositions by humans. Given the intrinsic difficulties of such a study and due to practical restrictions, we limited our experiment to four amateur composers-two with classical music training and two with modern popular music training.\nParticipants used a straightforward proofreading interface that enables bar selection for regeneration, allowing them to choose how much of the content to keep in certain sections of their work, as described in 2.3. Additionally, we allowed the users to change the RefinPaint feedback in the selected area and experiment with the tools by conducting as many trials as they wanted.\nAfter testing our inpainting tool on a 30-second music piece, participants responded to questions about their experience. They evaluated whether the tool (i) enhanced their original draft, (ii) sparked new ideas, (iii) could save time over manual proofreading, and (iv) was something they would use in the future. All chose \"yes\" for (i), (iii) and (iv) with three \"yes\" and one \"maybe\" for (ii), suggesting time efficiency as a key advantage and providing an overall positive view of the tool.\nThe positive feedback prompted us to showcase the proofread compositions on our companion website. Participants suggested the tool could be particularly effective in overcoming creative blocks, noting that inspiring ideas stemmed from all iterations, not just the last one. Additionally, two participants especially valued the option to alter tokens within the RefinPaint selection."}, {"title": "7. CONCLUSION", "content": "In conclusion, our novel approach, RefinPaint, significantly enhances music generation by identifying and improving weaker musical elements through iterative feedback. Its effectiveness in both inpainting and proofreading tasks promises a new direction for creative assistance and quality enhancement in compositions by humans and machines alike. Future work could fruitfully extend the research to multitrack compositions and explore control mechanisms for this model, such as conditioning by harmony, rhythm, genre, or other musical factors."}, {"title": "8. ETHICS STATEMENT", "content": "While RefinPaint can represent a significant leap forward in music composition technology, ensuring ethical deployment and use is crucial. We advocate for a future where such technologies support and enrich the creative process, complementing rather than displacing human creativity. While RefinPaint aims to democratize music creation, making it accessible and achievable for amateurs, there is a risk that professional musicians and composers could feel their roles and contributions are being undermined or replaced by machines. It is essential to strike a balance where this technology serves as a tool for enhancement and learning rather than a substitute for human creativity. Furthermore, it will be vital to establish guidelines that protect the intellectual property rights of original compositions, whether entirely human-made or AI-assisted."}]}