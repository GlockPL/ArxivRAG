{"title": "How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Decode Symbols", "authors": ["Volker TRESP", "Hang LI"], "abstract": "The tensor brain has been introduced as a computational model for perception and memory. We provide an overview of the tensor brain model, including recent developments. The tensor brain has two major layers: the representation layer and the index layer. The representation layer is a model for the subsymbolic global workspace from consciousness research. The state of the representation layer is the cognitive brain state. The index layer contains symbols for concepts, time instances, and predicates. In a bottom-up operation, the cognitive brain state is encoded by the index layer as symbolic labels. In a top-down operation, symbols are decoded and written to the representation layer. This feeds to earlier processing layers as embodiment. The top-down operation became the basis for semantic memory. The embedding vector of a concept forms the connection weights between its index and the representation layer. The embedding is the signature or \"DNA\" of a concept, which is decoded by the brain when its index is activated. It integrates all that is known about a concept from different experiences, modalities, and symbolic decodings. Although being computational, it has been suggested that the tensor brain might be related to the actual operation of the brain. The sequential nature of symbol generation might have been a prerequisite to the generation of natural language. We describe an attention mechanism and discuss multitasking by multiplexing. We emphasize the inherent multimodality of the tensor brain. Finally, we discuss embedded and symbolic reasoning.", "sections": [{"title": "1. Introduction", "content": "There has been a long and ongoing debate about the roles of symbolic versus subsymbolic processing to achieve intelligent systems. The viewpoint of the work presented here is that symbols are the outcomes of measurement devices. A measurement device can be of a technical nature or, as considered here, it can be the brain. Via the sensory system, the real world forms the input to the measurement device, and the symbols are the outputs. The outcome of a measurement can feed back to and influence the measurement device. If the measurement device is also an actor like a robot or a person, it can also affect the real world.\nThe work on the tensor brain, which is reviewed in this paper, follows these ideas. It suggests that much of the brain's processing is subsymbolic, but at the highest level, the brain uses symbols.\nA major component of the tensor brain is the subsymbolic representation layer, which is the main communication hub. It is related to the global workspace in consciousness research and is sometimes called the \"\"mental canvas\" or the \"theater of the brain\". It can be viewed as the blackboard in multiagent systems. Modules in the brain can write to and read from the representation layer. It is a cognitive hub, and its activation is the cognitive brain state. Whatever reaches conscious awareness propagates through the representation layer.\nA second layer is the symbolic index layer, where each symbol or index has a local representation. If the brain looks at a scene and classifies the entity in the scene as a dog, then the brain acts as a bottom-up measurement device, and the symbol Dog is the outcome of the measurement. We call this symbolic encoding. Symbols and subsymbolic representations describe and analyze views or projections of the world by interacting with one another. There is no activation of a symbol without an associated activation on the subsymbolic level, and subsymbolic representations are constantly annotated with symbols.\nThe connection weights between a symbolic index and the representation layer form the embedding vector of that symbol. The embedding of a symbol is its signature or \"DNA\". If the outcome of a measurement is the symbol Dog, or the brain is focussing on the symbol Dog for any other reason, then the whole brain should be informed about this fact. The index activates the representation layer with its embedding and this is then propagated to other parts of the brain via grounding and embodiment. Symbols can activate other symbols and thus can also be understood in the context of their relationships to other symbols. The embedding vector of an index is optimized in its role in this network, considering perceptual inputs, and embodiment.\nThe work on the tensor brain considers that symbols can be concepts like entities, classes, and attributes. In addition, symbols can be predicates or episodic instances. The latter permits the brain to form episodic memories. The embedding similarity of two scenes is not only determined by the scene input but also by their similarity in symbolic encodings. Thus symbolic similarity is important for memory recall.\nThe tensor brain approach argues that perception is greatly supported by memory, or as Goethe puts it: \"You only see what you know!\" Symbolic decoding for perception and memory is a serial process and an \u201cinner\" language. Humans can easily talk about perception and memories, without much effort! It seems plausible that the \u201couter\u201d language that is spoken is based on an evolutionary earlier \u201cinner\u201d language. We communicate with symbols, we argue with symbols, and sometimes we might even reason with symbols. But it is not just symbols: Although the cognitive brain state is not easily communicated directly, it is reflected in embodiment and grounding, e.g., as intonation, body language, gesture, and mimics.\nMaybe the brain is a prediction machine, as many have stated. There is a lot of evidence that this is true on the level of implicit memories, including perceptual and motor skills. To predict at longer time scales (e.g., evening plans), the mind needs to understand the present and relate it to the past: it needs explicit understanding and it needs an explicit memory. It needs a future memory, it needs imagination.\nThe paper is organized as follows. In the next section, we summarize the literature on the tensor brain. In Section 3, we describe the representation layer, i.e., the tensor brain's model for the global workspace. We introduce the cognitive brain state and show how it progresses in time as a recurrent neural network. In Section 4, we introduce a probabilistic interpretation of the cognitive brain state. Section 5 describes symbolic encoding as bottom-up inference. We discuss symbolic indices for concepts, predicates, and episodic time instances. In Section 6, we describe symbolic decoding as top-down inference. We introduce an attention mechanism and describe embodiment as an autoencoder. In Section 7, we describe the different operational modes, in particular, perception, episodic memory, and semantic memory. We explain the generation of triple statements and discuss language generation and understanding. In Section 8 we describe how self-supervised learning adapts embedding vectors. In Section 9, we discuss multitasking by multiplexing and cognitive control. Section 10 discusses multimodality and different forms of reasoning, in particular, embedded reasoning, symbolic reasoning, and embedded symbolic reasoning. Section 11 contains a summary.\nGlossary and notation:\n\u2022 The agent is an individual and is the actor. Its mind executes cognitive functions using its neurobiological basis, i.e., its brain."}, {"title": "3. The Subsymbolic Representation Layer", "content": "The brain might consist of thousands or even millions of modules that need to coordinate and exchange information. Some information needs to be conveyed to higher processing layers, maybe even reaching conscious attention. In the TB, an integrative layer is introduced in the form of the representation layer, which is a high-dimensional basis from which modules can write and read.\nThe TB assumes that the activation (firing rate) of an ensemble of neurons implements a population code. The activation pattern of all ensembles in the representation layer reflects the cognitive brain state (CBS). Some ensembles might be specific and represent, e.g., the color \"red\". Others might be rather unspecific latent factors. Experimental results indicate that the CBS exhibits a meaningful clustering structure [46].\nLet n be the number of ensembles. Then for i = 1,...,n\n$\\gamma_i = sig(q_i)$.\nHere, \u03b3\u03b9 \u2208 (0,1) is the (post-) activation of ensemble i, and qi \u2208 R is its pre-activation. Thus, y = 1 means that ensemble i is firing with maximal intensity, and y\u2081 = 0 means that ensemble i is firing at some basal rate or not at all. As discussed further down qi receives input signals, and i is the output activation. The CBS is the activation vector y of the representation layer. In the TB, one assumes that $sig(q_i) = (1+exp(-q_i))^{-1}$ is the logistic function.\nBaars and his coworkers have introduced the concept of a global workspace [1], which is the foundation of some theories on consciousness [1,8]. The global workspace is a functional hub of broadcast and integration that allows information to be disseminated across modules. The global workspace is sometimes referred to as the \"theater of the brain\" or its \"mental canvas\". As noted by [1], the global workspace could be related to the blackboard in a multi-agent system, where computational modules share information. The global workspace provides a communication and interaction hub, which is suitable for modules dealing with higher levels of abstraction, where direct module-to-module interactions might not be appropriate.\nLet's consider vision. Visual inputs contribute to the pre-activations of the ensembles in the global workspace through the visual paths, so visual input writes to the representation layer. In reverse, the CBS might feed back to the visual path in the form of embodiment, so for the visual backward path, the global workspace acts as input, and early visual processing layers get activated.\nAbstracting from vision, the modeling assumption is that the global workspacereceives signals from many brain modules, and brain modules, in turn, might be affected by the state of the global workspace. In particular, the global workspace receives input from all sensory modalities the agent can be aware of, like vision, hearing, taste, smell, touch, proprioception, and pain. It has been proposed that all brain states that can reach consciousness, including inner feelings and emotional states, contribute to the global workspace and, in reverse, can be affected by its state. However, not all modules in the brain directly affect the global workspace and, thus, might not directly reach conscious attention. Naturally, perceptual input plays a major role. But also the pain after injury and emotions associated with a memory.\nIn the TB, it is assumed that the global workspace is implemented by the representation layer. The representation layer is related to the hidden layer in a recurrent neural network (RNN), with some important modifications, as discussed in the following."}, {"title": "3.3. Evolution Neural Network and Recurrency", "content": "The representation layer in the TB exhibits feedback connections, which introduce dependencies between the activations of the ensembles and an internal memory. The model assumes, with i = 1,...,n\n$q_i^{(\\tau)} \\leftarrow \\alpha q_i^{(\\tau-1)}+g_i(v^{(\\tau)})+f^{(\\tau)}({\\gamma}^{(\\tau-1)})$.\nThe index t is a counter of a discrete-time operation of the brain. All operations that occur for a fixed \u03c4 are related to the same concept or episodic index. Here, $fNN(\\gamma^{(\\tau-1)})$ is the i-th output of a neural network with linear output units that describes the evolution neural network, and $v^{(\\tau)}$ represents some input vector, originating, e.g., from a visual scene. $g(v^{(\\tau)})$ is a deep convolutional neural network with linear output units that maps visual input to the representation layer.\nIn general, a particular $q_i^{(\\tau)}$ might only depend on a subset of ensembles, permitting modularity. The direct dependency on $q_i^{(\\tau-1)}$ models a skip connection, as commonly used in generative AI [15], and realizes a self-memory. This implements a recurrency that encourages stable activation patterns in the CBS.\nThe update is equivalent to an RNN update with skip connections. The main difference is that fNN(\u00b7) is a neural network that contains at least one hidden layer, which provides universal modeling capability. This hidden layer would not be part of a standard RNN.\nThe evolution neural network enables a prediction of a future CBS. In addition, it enables relationship modeling in semantic memory and episodic memory."}, {"title": "4. The Probabilistic Cognitive Brain State (pCBS)", "content": "The post-activation y can be interpreted as a Bernoulli parameter, i.e.,\n$P(X_i = 1) = \\gamma_i$.\nHere, X\u2081 = 1 means that the ensemble i is \"on\", and X\u2081 = 0 means that the ensemble is \"off\". Assuming mutual independence, we can define the probabilistic cognitive brain state (pCBS) as a probability distribution with\n$P(X_1 = i_1,...,X_n = i_n) = \\prod_{j=1}^n(\\gamma_j)^{i_j} (1-\\gamma_j)^{1-i_j}$"}, {"title": "5. The Symbolic Index Layer and Symbolic Encoding", "content": "The symbolic index layer is a module of high cognitive relevance that interacts with the representation layer. It receives input from the representation layer but also feeds back to it. In the brain, an index (or pointer) might be realized by an ensemble of neurons, as before. A difference is that the index layer can enforce a sample-take-all behavior, a variant of the well-known winner-take-all concept.\nEach index represents a symbol. We discuss three types of indices: concept indices, predicate indices, and episodic indices. Thus, we propose that an episodic index is also a symbol. In perception, we also use the term label for a symbol."}, {"title": "5.2. Concept Indices", "content": "The indices that belong to concepts are symbolic in nature. Concepts can, e.g., be entities, classes, attributes, locations, inner emotional states, actions, and decisions. Concepts permit the recognition of stable patterns in the CBS. Thus, a cluster analysis would have the brain detect a repeated pattern for concepts like Dog, a particular dog Sparky, and attributes such as Black and Happy. In perception, concept indices might label a scene or a visual area of interest, which we will refer to as the region of interest (ROI).\nAssume the CBS is 7. An index k receives inputs from the representation layer with\n$P(Y = k|\\gamma) \\approx softmax_{dom} (a_{0,k} + \\sum_{i=1}^n a_{i,k}\\gamma_i)$\nwhere Y k means that the index layer is in state k, i.e., only ensemble k is active, and all other indices are inactive. The vector $a_k = (a_{1,k},...,a_{n,k})$ is the typically sparse embedding vector of concept k, and ao,k is a bias term. In the architecture, ai,k is the weight on the link from ensemble i in the representation layer to ensemble k in the index layer. The softmax is defined as\n$softmax_{dom} (a_{0,k} + \\sum_{i=1}^n a_{i,k}\\gamma_i) = \\frac{exp(a_{0,k} + \\sum_{i=1}^n a_{i,k}\\gamma_i)}{\\sum_{k' \\in dom} exp(a_{0,k'} + \\sum_{i=1}^n a_{i,k'}\\gamma_i)} $ \nThe domain dom could be the set of all entities in case one only normalizes about entities. Similarly, it could be the set of all predicates, the set of all classes, or the set of all colors. Each domain defines a random variable. The set members should be mutually exclusive and complete labels, although this is not enforced, e.g., if two colors are assigned to the same entity. See the discussion in Section 7.4.3.3\nIndices reflect repeated patterns in the CBS and are identified by their embeddings and maybe their location in the brain. In animals, they do not have associated names like Dog, Sparky, Black and Happy. In humans, a subset might be identified with concepts in natural language."}, {"title": "5.3. Predicate Indices", "content": "The second type of indices refers to predicates. Consider that the agent analyses a scene. The CBS first focuses on an ROI that, e.g., contains Sparky, a Dog in the scene, then on a second ROI, that, e.g., contains Jack, a Person, in the scene. Then, an ROI is formed that encompasses both previous ROIs. In the context of the two previous ROIs, the third one might be labeled by their relationship, e.g., looksAt. Thus, the brain does not only consider labels of ROIs, but also labels their relationships. Here, ak is the embedding vector of predicate k."}, {"title": "5.4. Episodic Indices Refer to Time Instances", "content": "The third type of indices concerns episodic indices and refers to time instances. In the TB, an episodic index is introduced for each (relevant) time instance. In its simplest form, the embedding vector of index t is the vector of pre-activations of the representation layer at that instance, i.e., a\u2081 \u2190 q(t). If the representation layer is dominated by visual input from a scene, it will store the resulting pre-activations of the CBS, including, e.g., the emotional state at that instance. Later, we will discuss how a\u2081 is actually optimized."}, {"title": "6. Symbolic Decoding", "content": "The indices or symbols in the index layer can be decoded into subsymbolic representations. This is a form of top-down inference. The basic purpose of top-down inference is that the representation layer, and via embodiment the whole brain, should be informed about, which concept has been detected in vision, or more generally, which concept the brain is focussing on. Thus, if the concept of Sparky was detected, this information should be communicated to the representation layer and thus the whole brain. This is particularly important for integrating memory in perception: with the current hypothesis that Sparky is in the image, the brain can add background on Sparky, i.e., that Sparky is Friendly and is ownedBy Jack and lovedBy Mary."}, {"title": "6.1. A Deterministic Model without Top-down Inference", "content": "In a deterministic CBS interpretation, Equation 4 describes the output of the system. Assume that the labels for Sparky, Dog, Black, and Happy have substantial activation. The agent might select the label with the highest probability, e.g., Sparky. The agent might also consider other likely labels, such as Dog, Black, and Happy. There is no feedback to the representation layer in either case. The remaining part of the brain is not informed about likely labels. This is the standard RNN setting. Also, given the CBS, all labels are independent."}, {"title": "6.2. A Single Sampled Label", "content": "Assume that the labels for Sparky, Dog, and Black exhibit substantial activation. In the sampling mode, the TB samples from the distribution in Equation 4, and might generate the label Sparky. In general, if index k is sampled, then it is assumed that concept k is represented in the representation layer.\nThe index layer works in a sample-take-all mode: only one ensemble is firing at an instance, suppressing the firing of all other indices; in the example the index Sparky would be firing. Technically, after sampling, the index layer's state is a one-hot-vector. If index Y = k is firing, it activates the global workspace in a top-down manner with\nq \u2190 q + \u03b2ak.\nOne can interpret ai,k as the synaptic weight connecting ensemble k in the index layer, with ensemble i in the representation layer. In the pCBS interpretation, we again obtain independent Bernoulli distributions. This update equation was derived in [42]. The factor \u03b1, with 0 \u2264 a \u2264 1 controls how much of q is preserved in the update. With \u03b2 = 1 and a = 0, it is the Heisenberg approximation and with a = 1, it is the Heisenberg approximation with an \"added prior\". With a = 1 and \u03b2 = 0 we get an RNN.\nThus, as reflected in the last equation and in Equation 4, connections are bi-partite and symmetrical. Strict symmetry is implied by some theoretical considerations [42], but it is not a strict requirement and is likely to be violated biologically. Note that with a = 1 the previous pre-activation is not eliminated but added to the embedding vector as a skip connection."}, {"title": "6.3. Multiple Sampled Labels", "content": "The process can be repeated several times by repeatedly sampling from Equation 4 and updating Equation 5. With a > 0, each sample is always conditioned on the previous ones. Each sample is a particular interpretation of the CBS, in the context of what has been sampled before. For example, labels might represent the entity Sparky, the next one Dog, then Black, Happy, and so on. Finally, with a = 1,\nq \u2190 q + \u03a3 ak.\nThus, the index layer is part of a labeling engine. We also clearly see the difference to a standard RNN, where the last equation would not contain the sum, i.e., there is no feedback from the samples to the state. In the TB, the representation layer is explicitly informed about all generated samples. Also, subsequently generated samples will consider all the labels generated before. The activation of an index is an internal brain measurement, which is feeding back to the brain! All modules in the brain can learn about what was detected"}, {"title": "6.4. Probabilistic Averaging: Attention", "content": "Consider that there is no sampling, and the CBS is updated to consider all possible labels, weighted by their probabilities. In [42, 46] the equation\n$q = q + \\sum_k \\alpha_k \\times softmax (a_{0,k} + \\sum_{i=1}^n a_{i,k} \\gamma_i)$\nhas been introduced. Thus, one obtains an update of the CBS, even without sampling but by rather considering label probabilities. Note that the equation is a form of an attention mechanism with a skip connection from generative AI. The CBS y is the query vector, and the embedding vectors ak, k are the key and the value vectors."}, {"title": "6.5. Attention versus Sampling", "content": "Attention does not result in a symbolic interpretation since no index is active in isolation; an advantage is that the attention mechanism is fast and parallel and holistically considers all interpretations. The attention equation relates indices to related indices leading to a sharing of statistical strength. In [42] it is shown that the attention mechanism can be derived from a Heisenberg scenario and not from a Bayes scenario: in the latter, only actual samples would influence the CBS.\nSampling commits the brain to a specific interpretation. For example, if the black thing in the scene is sampled to be Sparky, then the brain can add a lot of background information about Sparky. But if it is sampled to be a Puma, other background information is added and the agent's reaction might be quite different. In sampling, both options are explored in different sampling rounds. Sampling explores the joint dependencies between labels. Sampling and top-down inference are the basis for semantic memory."}, {"title": "6.6. Attention, Autoencoding and Embodiment", "content": "Consider the last equation but without the skip connection, i.e., a 0. One can define an autoencoder with an encoder, which maps the CBS to the index layer activation, i.e., using Equation 4. The decoder then maps the index layer softmax activation back to the representation layer, using attention Equation 7 (again without the skip connection). Thus, the index layer is the bottleneck layer in an autoencoder where one considers all symbolic encodings weighted with their probabilities.\nIn perception, one can also start with the visual input. In the encoder, the visual input v is mapped to the index layer, via the visual processing and function g(v). Then the CBS is mapped to the index distribution, as before. In the decoder, the activation of the index layer is mapped back to the representation layer and then back to the visual input via an approximation to the inverse of the visual processing layer, v = ginv (7). This top-down processing is a form of grounding or embodiment: not only the representation layer but all visual processing layers are informed, e.g., about what it means to consider Sparky in the index layer.\nBrain circuits are generally bi-directional in the sense that if a module connects to another module, there are also connections in the reverse directions. In the TB, the connections between the representation layer and the index layer are bi-directional. Now, also the perceptual processing pipeline is assumed bi-directional. The inverse perceptual processing pipeline might not be as precise: as we all experience, a visual recall of the concept of Sparky is not as strong and clear as actually observing Sparky in a scene."}, {"title": "7. Operational Modes", "content": "One strength of the TB approach is that different operational modes of the brain can be modeled by one architecture. We discuss perception, episodic memory, and semantic memory."}, {"title": "7.1. Pseudocode", "content": "The pseudocode is described in Algorithm 1, Algorithm 2, and Algorithm 3. In Algorithm 3, a = 1 and \u1e9e = 1, which is the Heisenberg approximation with an added prior. With a = 0, it is the Heisenberg approximation (without an added prior). With \u1e9e = 0 we get an RNN."}, {"title": "7.2. Visual Perception", "content": "Assume that the agent analyzes a visual scene. Algorithm 2 starts with some neutral input, e.g., q = 0. The scene v \u2190 vscene is the visual input (Line 2). Next, the attention step is applied to get strengths from related past episodic memories (Line 3). Algorithm 3 describes the encoding and decoding. A label is sampled (bottom-up) and the sampled label feeds back to the representation layer (top-down). Algorithm 3 might be called several times. Each time a label k is generated. These labels might indicate, e.g., the location of the agent (e.g., EnglishGarden) and the weather condition (e.g., Sunny). However, a generated label might also be a past time-index, indicating a similarity to a past scene."}, {"title": "7.2.2. A First ROI in a Visual Scene", "content": "Then, the agent might be interested in the properties of an entity, defined, e.g., by a particular ROI in the scene. The TB applies the evolution neural network of Algorithm 1. Its input qn is the output from Algorithm 2, as applied before. Then Algorithm 2 is called. The ROI v \u2190 \u25bcROI is the visual input (Line 2). Attention is calculated w.r.t. to entities that are already established. Algorithm 3 describes the encoding and decoding. It might be called several times. Each time a label k is generated. Labels might be, e.g., Sparky, Dog, Black, and Happy. Due to top-down inference, labels will be dependent, e.g., the probability for labeling Dog will take into account that Sparky was labeled before. For a given scenes many labels are generated for a given ROI and the sampling is repeated several times in different sampling rounds."}, {"title": "7.2.3. A Second ROI in a Visual Scene", "content": "Then, a second ROI might be analyzed in the context of the scene and the first bounding box. As for the first ROI, Algorithm 1, Algorithm 2, and Algorithm 3 are applied. The second ROI might, e.g., describe the entity Jack. Further labels might be Person, Tall, ContentLooking."}, {"title": "7.2.4. A Third ROI in a Visual Scene", "content": "Then, a third ROI encloses both previous bounding boxes. It might label the relationship between both entities with the predicate label looksAt, again using Algorithm 1, Algorithm 2, and Algorithm 3."}, {"title": "7.2.5. Forming Episodic Indices", "content": "For each perceptual event, an episodic index t is introduced (see Section 5.4), and it can be indexed as an episodic event. Its embedding a, is optimized to reconstruct the labels for the scene but also for the ROIs even when the visual inputs are missing. So, the goal is the symbolic reconstruction, as much as the subsymbolic reconstruction."}, {"title": "7.2.6. From Labels to Triples", "content": "An ROI and an episodic time index are unique identifiers (keys) for the other labels of an ROI. Assuming that entities are mutually exclusive, one can replace the ROI with the entity represented in the ROI, e.g., Sparky, and one can describe the generated labels at time t as: (Sparky, type, ClassDog), (Sparky, type, ColorBlack), (Sparky, type, MoodHappy), or shorter (Sparky, type, Dog), (Sparky, type, Black), and (Sparky, type, Happy). These are triple statements, and the first entry is the subject, the second is the predicate, and the third is the object. Thus, a logical triple statement describes the relationship between concept labels. In the context of perception, triples describe currently generated labels, and in the context of episodic memory, past generated labels. If an entity does not yet exist it is introduced with a new index.\nOne can think of an entity as a thing on which a measurement is executed, the predicate describing the type of a measurement and the object as the outcome of the measurement. In addition, and this is important for reasoning, one can also generate triples with subjects that are not entities such as (Dog, type, Happy), (Black, type, Sparky), and so on. The TB calls them generalized statements and they are useful for embedded symbolic reasoning (see Section 10.2).\nBy involving two ROIs (for the subject and object) and an enclosing ROI (for the predicate) one obtains triples of the form (Sparky, looksAt, Jack)."}, {"title": "7.3. Episodic Memory", "content": "According to Tulving, who introduced the term [47]: Episodic memory stores information about general and personal events and concerns information we \"remember\". It is about past observations. In the context of vision, it would be the reconstruction and the decoding of a past scene."}, {"title": "7.3.1. Episodic Engram", "content": "In the TB, an episodic memory engram consists of the episodic index and its embedding. If an episodic index is activated, its embedding vector will define the CBS and this propagates to earlier visual processing layers, using embodiment. In this way, the brain gains a subsymbolic understanding of the past event. In the context of vision, it would be an approximate reconstruction of a past scene. The TB proposes that episodic indices are symbolic in nature, similar to concept indices."}, {"title": "7.3.2. Decoding of Episodic Memory", "content": "In the TB approach, an episodic memory is realized by the activation of the episodic index that belongs to that past memory. Thus, an episodic memory approximately restores the CBS of a past instance. We start with Algorithm 3 with a neutral input q = 0 where we set k\u2190t and do not sample. Then Algorithm 3 is reapplied to generate labels for the past scene"}, {"title": "7.3.3. Recent Episodic Memory", "content": "Recent episodic memory enriches perception through the retrieval of perceptual experiences, which provide the agent with a sense of the here and now, i.e., the current context. To understand its own state and the world's state in general, the agent needs to know what happened recently, in recent scenes, and on recently perceived entities. This state information cannot be directly derived from perceptual input, only.\nFor instance, the agent needs to remember that, even though perception does not give a clue, it is still in the hide-out because the bear had been chasing it and might still be lurking outside. Thus, recent episodic memory guides behavior and provides decision support.\nIncidentally, patients who are unable to form new episodic memories show great deficits in personal orientation and context understanding. These deficits are often associated with severe bilateral damage to the medial temporal lobe (MTL), which includes the hippocampus [12]. Old age is also a factor."}, {"title": "7.3.4. Remote Episodic Memory", "content": "Remote episodic memory concerns events that are memorable but might have occurred further in the past. They can provide decision support and inform the agent about good and bad outcomes of previous episodic events that are similar to the current perceptual experience. It aids the agent in decision-making. Continuing the previous example, the agent might remember previous personal bear encounters and subsequent dangerous situations. Recall in remote episodic memory is triggered by the closeness between episodic representation and scene representation.\nRemote episodic memories might occupy the brain for quite some time and are associated with retraining the brain during consciousness or during sleep [27]."}, {"title": "7.3.5. Future Episodic Memories and Imagination", "content": "A future episodic memory is a forecasted event in the future, which at some point in time, is predicted to become a regular episodic memory. Consider an example. The agent might know that there is a football game in town in the evening and that the weather will be bad. By creating a future episodic index with labels FootballGame and Rainy Weather, the agent might predict a bad traffic condition. This is a prediction associated with a future event, and it is a form of imagination. Note that the imagination is also grounded and embodied.\nThus, memory guides behavior for the future (future episodic memory) [36]. [9] describe this process as integration across relational events by imagining possible rewards in the future. The value associated with a memory (e.g., reward, threat) might be an integral aspect of episodic memory. The article also states that there is now extensive empirical data supporting the prevalent use of episodic memory across various decision-making tasks in humans.\nConsider that the brain has one or several modules evaluating the CBS, e.g., into the states PersonallyRewarding, ImprovesSocialStatus, or Undesirable. Potentially, each such state obtains an associated index. The imagining of different future scenarios, under different decisions of the agent (drive to a friend at night and get into bad traffic and weather or stay home), can lead to different scenarios with different expected rewards, and the agent has the option to decide for the action that might lead to a situation with maximum reward.\nImagining future scenarios lets the agent contemplate about facts very likely to be true or false under some assumed conditions. In spirit, this is similar to some form of logical reasoning.\nMaybe the brain is a prediction machine, as many have stated [7,11,14,19,20,33,39]. This predictive machinery might become part of the fast reactive system of the brain, i.e., its implicit memory. This is relevant when your tennis playing improves. But to predict at the longer time scales considered here, the mind needs to understand the present and relate it to the past: it needs explicit understanding and it needs an explicit memory. It needs a future memory, it needs imagination."}, {"title": "7.3.6. Temperature Scaling, Simulation, and Bayes Probabilities", "content": "To decide on the scenario that gives the most expected reward, the mind needs to take into account the plausibilities or likelihoods of the different scenarios. In the tensor brain, only samples are visible and not the probabilities. So how can the brain deal with probabilities?\nConsider an example. \u201cFrom what I know and also from what my gut feeling tells me, I bet that Sparky will win the race. \" Betting has been used as evidence that individuals actually behave Bayes optimally, requiring the careful combination and comparison of probabilities. Consider that the domain is about racing dogs. We can introduce temperature scaling in the softmaxdom function (softmaxdom(x) becomes softmaxdom(x/T) with temperature T). If we turn to low temperatures, we can turn a sample-take-all into a winner-take-all, i.e., only the expected winner is sampled, in this case, Sparky. So even when it might be difficult to get accurate probabilities in the brain, it might still be possible to get optimal decisions!\nAlternatively [35], suggested that the brain actually is a Bayesian sampler. The paper states that \"Only with infinite samples does a Bayesian sampler conform to the laws of probability; with finite samples, it systematically generates classic probabilistic reasoning errors, including the unpacking effect, base-rate neglect, and the conjunction fallacy.\" Statements in semantic memory might also be associated with probabilities (see Section 7.4.3). Sampling might be the strategy to get estimates of probability values there as well.\"\n    },\n    {"}]}