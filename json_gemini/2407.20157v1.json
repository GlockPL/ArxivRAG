{"title": "rLLM: Relational Table Learning with LLMs", "authors": ["Weichen Li", "Xiaotong Huang", "Jianwu Zheng", "Zheng Wang", "Chaokun Wang", "Li Pan", "Jianhua Li"], "abstract": "We introduce rLLM (relationLLM), a PyTorch library designed for Relational Table Learning (RTL) with Large Language Models (LLMs). The core idea is to decompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural Networks into standardized modules, to enable the fast construction of novel RTL-type models in a simple \"combine, align, and co-train\" manner. To illustrate the usage of rLLM, we introduce a simple RTL method named BRIDGE. Additionally, we present three novel relational tabular datasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope rLLM can serve as a useful and easy-to-use development framework for RTL-related tasks. Our code is available at: https://github.com/rllm-project/rllm.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), like ChatGPT [1], are driving a new wave of artificial intelligence advancements, garnering widespread attention. These models are great at at understanding and generat-ing text by leveraging technologies such as web-scale unsupervised pretraining [2], instruction fine-tuning [3], and value alignment [4]. Their strong performance across various tasks suggests they could be key to achieving Artificial General Intelligence [5].\nHowever, applying LLMs to real-world big data is extremely costly. Figure 1 illustrates the growth trends of various types of data alongside the corresponding expenses\u00b9. The costs associated with LLMs are unsustainable. For example, by 2025, the total cost of LLMs is anticipated to reach nearly $5,000 trillion, which is 214 times the 2023 GDP of the United States ($27.37 trillion). Another notable observation is that processing text and structured data will incur most of these expenses, despite these two types of data being smaller in volume compared to multimedia data.\nAs relational databases host around 73% of the world data [6], recent years have seen a significant shift towards Relational Table Learning (RTL) [7] [8] [9] 2. In this paper, we introduce the rLLM (relationLLM) project, which aims to provide a platform for rapidly developing RTL-typle methods with LLMs. As shown in Figure 2, it performs two key functions: 1) decomposing state-of-the-art Graph Neural Networks (GNNs), LLMs, and Table Neural Networks (TNNs) into standardized modules, and 2) enabling the construction of novel models in a \"combine, align, and co-train\" manner using these decomposed modules.\nTo illustrate the application of rLLM, we present a simple RTL method named BRIDGE. Specifically, this method utilizes TNNs to process table data and leverages the \"foreign keys\" in relational tables to construct relationships between table samples, which are then analyzed using GNNs. This approach takes into account multiple tables and the relationships between them.\nFurthermore, as RTL is an emerging field with a notable lack of datasets, we further introduce a novel data collection (named\n1See Appendix 1 for details.\n2Although \"tabular\" is also commonly used to describe \"row-format data samples in relational databases\", our experience during development has indicated that under-graduate/graduate students, engineers, and business clients generally prefer the term \"table\" over \"tabular\". This preference might be due to their more vocal nature. To facilitate public understanding, this paper uses the term \"table\"."}, {"title": "2 System Overview", "content": "As shown in Figure 2, rLLM consists of three main layers: Data Engine Layer, Module Layer, and Model Layer."}, {"title": "2.1 Data Engine Layer", "content": "This layer designs the fundamental data structures for graph and table data and defining the processing workflows for relational table data. As shown in Figure 3, the overall architecture decouples data loading and storage, which are handled by the Dataset subclass and BaseGraph/BaseTable subclasses, respectively. This design philoso-phy is driven by the pursuit of flexibility and scalability, enabling efficient and flexible handling and storage of different graph and table data.\nSpecifically, within the Dataset subclasses, we can implement various data loading classes tailored to the characteristics of dif-ferent datasets. These subclasses focus on efficiently data loading and preprocessing, ultimately storing the processed data in data structures inherited from BaseGraph and BaseTable. Additionally, they are both optimized for the storage and processing of graph data (such as homogeneous and heterogeneous graphs) and table data, respectively. Overall, this design meets the familiar storage and processing requirements of relational table data consist of table data and foreign key relationships."}, {"title": "2.2 Module Layer", "content": "This layer decomposes the operations of GNNs, LLMs, and TNNS into standard submodules."}, {"title": "2.2.1 GNN Modules", "content": "This part mainly includes the GraphTransform and GraphConv modules. The GraphTransform module pro-vides preprocessing methods for graph data, such as normalization and self-loop operations. Additionally, this module supports com-bining various graph preprocessing methods, allowing users to perform complex graph preprocessing operations to meet the re-quirements of subsequent algorithms.\nThe GraphConv module implements popular graph convolution layers, including homogeneous and heterogeneous graph convo-lutions. The core functionality of this module involves different message-passing functions between nodes for various graph con-volution operations, addressing the requirements of diverse graph types and algorithms. In practical applications, stacking and inter-acting multiple graph convolution layers allows for the modeling of complex graph data information."}, {"title": "2.2.2 LLM Modules", "content": "This part mainly includes the Predictor and Enhancer modules. The Predictor module allows users to leverage LLMs for data annotation. It is crucial because much real-world data lacks labels, and manual annotation is expensive and prone to errors. Users can use existing prompts or design their own to enable LLMs to perform preliminary annotations. These annotations can serve as final label predictions or as labeled data for further training of other machine learning models.\nThe Enhancer module allows users to employ LLMs for data aug-mentation. In many real-world scenarios, data may be insufficient or of low quality. For example, understanding the full context of a research paper based solely on its title can be challenging. Users can use various prompts to generate detailed textual explanations for data samples with the LLM. These explanations can be used directly to enhance the data or be transformed into other feature formats to improve performance in downstream tasks."}, {"title": "2.2.3 TNN Modules", "content": "This part mainly includes the TableTransform and TableConv modules. The TableTransform module maps sam-ple features to higher-dimensional vector spaces. It is necessary because table data samples (i.e., rows) consist of multiple feature columns, which can vary greatly in nature. Due to the diverse types of features and the often limited information provided by tables with fewer columns, it is crucial to map or transform some columns"}, {"title": "2.3 Model Layer", "content": "By combining modules from the second layer, this layer provides three main strategies for rapidly developing RTL-type models: Combine, Align, and Co-Train.\n\u2022 Combine refers to jointly using modules from different parts. For example, [10] and [11] use LLMs for preliminary label annotation, and then the annotated results are fed into a GCN [12] model for graph node classification. Within the rLLM framework, we can utilize the Predictor module from the LLM part to complete the annotation task and then use the GCN module from the GNN part for the following classi-fication task.\n\u2022 Align involves aligning the input and output feature spaces for different modules. For instance, ConGraT [13] generates node embeddings using a language model and a GNN sep-arately, followed by aligning these embeddings within the final embedding space. Within the rLLM framework, we can first use the Enhancer module from the LLM part to generate embeddings (and potentially perform enhancement simul-taneously), then generate node embeddings with the GNN module, and finally align the two types of embeddings.\n\u2022 Co-Train denotes the collaborative training of different modules. For example, BRIDGE (Section 3) integrates TNNs and GNNs to leverage internal and inter-table information. Within the rLLM, we can invoke modules from both GNNs and TNNs, combine them as needed, and perform co-training to enhance the performance of multi-table joint learning tasks."}, {"title": "3 An Illustration Method - BRIDGE", "content": "In this section, we introduce a straightforward method named the Basic Relational table-Data LearninG Framework (BRIDGE) to illustrate how to quickly construct RTL-type methods using rLLM.\nIn real-world applications of relational databases, data is usually stored in multiple (two-dimensional) tables connected by foreign keys. This means relational table data includes table features (i.e., each sample is a row in a table with many columns) and non-table features (such as the graph structure formed by foreign key relationships). Therefore, we need to handle both the table data and their interrelationships.\nFirstly, for table data, we need to use table neural networks to model and learn from the data. This is due to the heterogeneity of table data, where each column's feature type and meaning can vary significantly. As shown in Figure 4, we use a Table Encoder to model the table features. As mentioned in Section 2.2.3, we can construct different Table Encoders using the TableTransform and TableConv modules from the TNN part in rLLM, to obtain the table embeddings.\nSecondly, for non-table data, particularly referring to the \"foreign keys\" between tables, we can leverage these foreign pivotal relation-ships to construct associations among multiple samples, and then use a Graph Encoder to model these connections. As mentioned in Section 2.2.1, we can use the GraphTransform and GraphConv modules from the GNN part in rLLM, to construct various Graph Encoders, ultimately producing graph embeddings. Additionally, we input the results from the TNN (i.e., table embeddings) along with other forms of non-table data into the Graph Encoder for joint processing.\nFinally, we integrate the outcomes from the table encoder and the graph encoder, allowing the Bridge framework to concurrently model multi-table data and their interconnections. The training objective of the entire model can be either supervised (e.g., using a cross-entropy loss function based on labeled data) or unsupervised (e.g., designing an unsupervised loss function based on data recon-struction principles from table data or foreign key relationships)."}, {"title": "4 Methods and Datasets", "content": "We have already included some common methods, which can be categorized into two types."}, {"title": "4.1 Included methods.", "content": "(1) GNN-type Methods:\n\u2022 Homogeneous Methods: GCN [12], GAT [14], RECT [15], TAPE [16], and OGC [17].\n\u2022 HeterogeneousMethods: HAN [18] and HGT [19].\n(2) TNN-type Methods:\n\u2022 Single-Table Learning: TabTransformer [20], TabNet [21], and FT-Transformer [22].\n\u2022 Relational Table Learning: BRIDGE."}, {"title": "4.2 Included Datasets", "content": "In addition to the common graph data and single-table data, we further introduce a novel data collection (named SJTUTables) which includes three novel relational table datasets by enhancing existing classical ones. As summarized in Table 1, each dataset has a default classification task with a balanced and fixed train/val/test split setting. Specifically, we provide 20 labeled samples for each class in these datasets, with two additional sets of 500 validation samples and 1000 test samples. The complete details are as follows.\n\u2022 Table-MovieLens 1M (TML1M) is a relational table dataset enhanced from the classical MovieLens1M dataset\u00b3, compris-ing three tables: users, movies and ratings. We have enriched the movie table with more comprehensive features and de-fined a new task for classifying user age ranges.\n\u2022 Table-LastFm2K (TLF2K) is a relational table dataset en-hanced from the classical LastFm2k dataset\u2074, containing three tables: artists, user_artists and user_friends. We have enriched the artists table with more detailed features and streamlined the tags for each artist, defining a new task for music genre classification of artists.\n\u2022 Table-ACM12K (TACM12K) is a relational table dataset enhanced from the ACM heterogeneous graph dataset5. It includes four tables: papers, authors, citations and writings. The paper features include year, title, and abstract, while\n3https://grouplens.org/datasets/movielens/1m/\n4https://grouplens.org/datasets/hetrec-2011/\n5https://github.com/Jhy1993/HAN/tree/master/data/acm"}, {"title": "5 Evaluation", "content": "To demonstrate our system and the proposed BRIDGE algorithm, we conducted comparative experiments on the TML1M dataset. In the BRIDGE algorithm, we used TabTransformer as the table encoder and GCN as the graph encoder. To ensure fairness, we standardized training batches, dropout rates, and other parameters for each method and conducted multiple experiments to get the average results."}, {"title": "5.1 Experimental Setup", "content": "To demonstrate our system and the proposed BRIDGE algorithm, we conducted comparative experiments on the TML1M dataset. In the BRIDGE algorithm, we used TabTransformer as the table encoder and GCN as the graph encoder. To ensure fairness, we standardized training batches, dropout rates, and other parameters for each method and conducted multiple experiments to get the average results."}, {"title": "5.2 Comparative Methods", "content": "We utilized the following baselines built into rLLM6:\n\u2022 TabTransformer: A deep tabular data modeling architecture for supervised and semi-supervised learning. This model is based on a self-attention Transformer architecture.\n\u2022 TabNet: A model for Attentive Interpretable Tabular Learn-ing, which uses neural networks to build a structure akin to decision trees for analyzing tabular data.\n'Due to the inability of the GNNs to directly process tabular data, GNN-type methods are not compared in this experiment."}, {"title": "5.3 Results and Analysis", "content": "The experimental results indicate that traditional single-tabular TNNs can only learn knowledge from the single target table, failing to effectively utilize the information provided by multiple tables and the relationships between them. Consequently, their performance is relatively poor. In contrast, the BRIDGE algorithm effectively extracts valuable information from both the various tables and the relationships between them by combining a table encoder and a graph encoder, leading to a notable improvement in performance."}, {"title": "6 Conclusion", "content": "We presented rLLM framework for relational table learning with LLMs. We are actively working to further integrate more advanced methods and also plan to optimize relevant data structures to im-prove system efficiency. All researchers and software engineers are welcomed to collaborate with us in extending its scope."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Details of Figure 1", "content": "Global Data Trend Estimation. We calculated the total data distribution based on statistics and projections from Statista [23], which estimate that the global data volume will reach a staggering 181ZB by 2025. Additionally, using reports from Chopra [24] and IBM [25], we roughly estimated the proportion of each modality and calculated their corresponding data volumes based on the projected global data total.\nLLM Cost Trend Estimation. To calculate the LLM cost, we first determined the number of tokens that different modalities of data could be converted into. First, for pure text data, we assumed the text to be in UTF-8 encoded plain English, where each character oc-cupies one byte. According to relevant studies [26], English words are approximately 6-11 characters. Using this ratio, we calculated the number of words corresponding to the text data and then used OpenAI's empirical formula [27] to estimate the number of tokens. Second, for multimodal data, such as images and videos, we applied"}, {"title": "A.2 Datasets", "content": "We introduce a novel data collection (named SJTUTables) which includes three novel relational table datasets by enhancing existing classical ones. Each dataset has a default single-label classification task with a balanced and fixed train/val/test split setting. Specifi-cally, we provide 20 labeled samples for each class in these datasets, with two additional sets of 500 validation samples and 1000 test samples. The complete details are as follows."}, {"title": "A.2.1 TML1M", "content": "Derived from the classical MovieLens 1M dataset7, the TML1M dataset consists of three relational tables.\n\u2022 users table: Inherited from the MovieLens 1M dataset, it includes UserID, Gender, Age, Occupation and Zip-code in-formation of users. In sum, the row number is 6,040 and the column number is 5.\n\u2022 movies table: Enhanced by scraping the MovieLens web-site to gather additional metadata for each movie, including MovieID, Title, Year, Genre, Director, Cast, Runtime, Lan-guages, Certificate, Plot and Url. This enhancement provides more comprehensive and detailed textual information of movies. In sum, the row number is 3,883 and the column number is 11.\n\u2022 ratings table: Contains UserID, MovieID, Rating, and Times-tamp of ratings. In sum, the row number is 1,000,209 and the column number is 4.\nThe default task for this dataset is to predict the user's age range in the User table."}, {"title": "A.2.2 TLF2K", "content": "Derived from the classical LastFM 2K datasets, the TLF2K dataset consists of three relational tables.\n\u2022 artists table: Enhanced by scraping Last.FM, its column includes artistID, type, name, born, yearsActive, location, genre, tag_list, biography and url. Every artist is classified into one of 11 genres. Specifically, ChatGPT was used to assign a single label to each artist based on their tag list, in-cluding confidence scores, with manual intervention for low-confidence labels. As such, original tags, by default, should not be used for this classification task. In sum, the row num-ber is 9,047 and the column number is 10.\n\u2022 user_friends table: Contains 12,717 bi-directional edge re-lationships, representing friendships among 1,892 users. In\nhttps://grouplens.org/datasets/movielens/1m/\nhttps://grouplens.org/datasets/hetrec-2011/"}, {"title": "A.2.3 TACM12K", "content": "Derived from the classical ACM heterogeneous graph dataset, TACM12K contains four tables:\n\u2022 papers table: Manually labeled the year information for venues and added year attributes via the 'PvsV' matrix. Cor-rected errors in the original 'PvsC' markings, reclassified STOC as COLT in 'VvsC', and recalculated the 'PvsV * VvsC' matrix to add conference attributes to papers. In addition, we also provide the title and abstract information as two columns. In sum, the row number is 12,499 and the column number is 5, and the columns are paper_id, year, conference, title and abstract.\n\u2022 authors table: Extracted from the file, it includes original author IDs and names, with firm information added based on 'AvsF'. In sum, the row number is 17,431 and the column number is 3, and the columns are author_id, name and firm.\n\u2022 citations table: Derived from the original 'PvsP' matrix. In sum, the row number is 30,789 and the column number is 2, and the columns are paper_id and paper_id_cited, where the former column cites the latter.\n\u2022 writings table: Derived from the original 'PvsA' matrix. In sum, the row number is 37,055 and the column number is 2, and the columns are paper_id and author_id.\nThe default task for this dataset is to predict the conference of papers.\nhttps://github.com/Jhy1993/HAN/tree/master/data/acm"}]}