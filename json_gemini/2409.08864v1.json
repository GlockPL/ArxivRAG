{"title": "Exploring Graph Structure Comprehension Ability of Multimodal Large Language Models: Case Studies", "authors": ["Zhiqiang Zhong", "Davide Mottin"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in processing various data structures, including graphs. While previous research has focused on developing textual encoding methods for graph representation, the emergence of multimodal LLMs presents a new frontier for graph comprehension. These advanced models, capable of processing both text and images, offer potential improvements in graph understanding by incorporating visual representations alongside traditional textual data. This study investigates the impact of graph visualisations on LLM performance across a range of benchmark tasks at node, edge, and graph levels. Our experiments compare the effectiveness of multimodal approaches against purely textual graph representations. The results provide valuable insights into both the potential and limitations of leveraging visual graph modalities to enhance LLMs' graph structure comprehension abilities.", "sections": [{"title": "Introduction", "content": "Recently, Large Language Models (LLMs) have revolutionised natural language processing and have been increasingly applied to diverse tasks beyond text generation and comprehension [2, 3]. One area of growing interest is the application of LLMs to graph-structured data, which is prevalent in numerous domains, e.g., social network analysis and bioinformatics [4-6].\nConventionally, researchers have focused on developing textual encoding functions to represent graphs in a format digestible by LLMs [1, 5, 7]. These methods have shown promise, enabling LLMs to perform various graph-related tasks with increasing accuracy. While this approach has shown promise, it faces inherent limitations in capturing the full complexity of graph structures, particularly in preserving spatial relationships and global structural properties [1].\nThe recent emergence of multimodal LLMs marks a significant milestone in AI development [2, 3]. These advanced models, capable of processing both textual and visual information, open new avenues for enhancing machine comprehension of complex data structures. In the context of graph structure comprehension, this multimodal capability presents an exciting opportunity: the potential to leverage visual representations of graphs alongside their textual descriptions.\nThis research aims to explore the potential of multimodal LLMs in graph comprehension tasks. We hypothesise that by leveraging both textual and visual representations of graphs, these models can achieve superior performance compared to their text-only representations. Our study focuses on a comprehensive set of benchmark tasks at the node, edge, and graph levels, providing a multifaceted evaluation of multimodal approaches in graph analysis. Particularly, based on the designed framework as shown in Figure 1, we seek to address two research questions: (i) How does incorporating visual graph representations affect LLM performance on various graph-related tasks compared to purely textual representations? (ii) What are the limitations of current multimodal LLMs in processing graph visualisations, and how might these be addressed in future research?"}, {"title": "Exploring Graph Structure Comprehension Ability of Multimodal LLMs", "content": "Our empirical studies follow the GraphQA benchmark settings [1]. Figure 1 provides an overview of our framework, comprehending Graph as Image (GAI+). Its simplified version, GAI, indicates the only graph vision modality is included. We detail each component of our methodology below.\nGraph Generation. To systematically evaluate the graph comprehension capabilities of multimodal LLMs, we generated a diverse set of graphs using the Erd\u0151s-R\u00e9nyi (ER) model [8], following the approach of Fatemi et al. [1]. Our dataset comprises 500 graphs, each containing between 5 and 20 nodes. This range allows us to assess the models' performance across varying graph complexities. Figure 2 illustrates two example graphs from our dataset.\nGraph Text Encoder. While Fatemi et al. [1] propose several text encoding functions to represent graphs, we focused on two specific methods: adjacency and incident encoding. This choice was motivated by the need to visualise graphs as images, where complicated textual representations might be challenging to depict within a constrained visual space. These encoding methods provide a balance between informational content and visual clarity.\nGraph Visualiser. The graph visualiser component generates visual representations of the structural graphs. While there can be numerous variations in visual aspects such as background colours, layouts, and node shapes, we opted for a standardised approach using Matplotlib [9] with default settings. This decision ensures consistency across our visual graph representations. All graphs are plotted to a fixed size to maintain uniformity. We acknowledge that different visualisation techniques could influence results, and we identify this as an area for future investigation.\nPrompt Construction. We adopted all prompt designs from [1], which include: Zero-shot prompting (ZERO-SHOT), Few-shot in-context learning (FEW-SHOT), Chain-of-thought (COT), Zero-shot CoT prompting (ZERO-COT) and Bag prompting (COT-BAG). For scenarios where a visual graph representation is available, we augmented the prompts by prepending the sentence: \"There is an undirected graph in this image.\" This modification ensures that the LLM is aware of the presence of visual information. Our study encompasses a comprehensive set of graph structure comprehension tasks, including: Node tasks: node degree, connected nodes; Edge tasks: edge existence, shortest path; and Graph tasks: node count, edge count, cycle check, triangle counting. This diverse set of tasks allows us to evaluate the models' performance across various aspects of graph comprehension.\nLLMs. Our study focuses on LLMs in a black-box setup, where the model parameters are fixed, and the system only consumes and produces text. This setting reflects the most common scenario for practical LLM usage. We selected two state-of-the-art multimodal LLMs for our experiments: GPT-4 [3], GPT-40 [3]. These models represent the current pinnacle of multimodal language models, capable of processing both text and image inputs."}, {"title": "Results and Discussions", "content": "Our experimental results are summarised in Tables 1 and 2. We discuss our findings in detail below:\nSuperior performance of multimodal LLMs. A impressive observation from our results is the markedly superior performance of GPT-40 and GPT-4 compared to the PaLM model. In several tasks, these newer models demonstrate near-perfect accuracy, correctly answering questions about graph structures for almost all test cases. This substantial improvement indicates that recent advancements in multimodal LLMs have significantly enhanced their graph structure comprehension abilities.\nImpact of graph visualisation. Our results show that incorporating graph visualisations can enhance LLMs' graph comprehension, though this effect is not uniform across all tasks. The impact of visual input varies depending on: (i) The complexity of the graph structure. (ii) The specific nature of the task (e.g., local vs. global graph properties). For instance, tasks involving global properties (e.g., cycle detection) seem to benefit more from visual input compared to local tasks (e.g., node degree).\nLimitations of visual-only input. Interestingly, we found that providing only graph visualisations, without accompanying textual descriptions, is insufficient for LLMs to fully comprehend graph structures. This observation highlights the complementary nature of visual and textual information in graph comprehending tasks.\nComparison with specialised graph encoding models. Our comparison with the work of [7], which uses neural networks to encode graph information for LLMs, reveals that our multimodal LLM approach outperforms these carefully trained models in graph structure comprehension tasks. This finding is significant because it suggests that: (i) General-purpose multimodal LLMs can compete with, and even surpass, specialised graph encoding models. (ii) The versatility of multimodal LLMs allows them to adapt effectively to graph comprehending tasks without task-specific training.\nChallenges in graph visualisation. Figure 2 illustrates two contrasting examples of graph visualisation: a simple graph with clear visual representation and a complex graph where GAI provides incorrect responses. This comparison highlights a critical challenge in our approach: the effective visualisation of graphs for multimodal LLMs. The disparity in performance between simple and complex graphs raises several important questions: (i) How does graph complexity affect the model's ability to extract relevant information from visualisations? (ii) What are the optimal ways to visually represent different types of graph structures? (iii) How can we balance information density and visual clarity in graph representations? These observations underscore the need for further research into graph visualisation techniques that are optimised for LLM comprehension. Future work should explore various visualisation strategies, potentially incorporating: (i) Sampling-based interactive or dynamic graph representations. (ii) Hierarchical visualisations for complex graphs. (iii) Novel encoding techniques that highlight relevant graph properties."}, {"title": "Conclusion", "content": "This study explored the graph structure comprehension abilities of multimodal LLMs through a series of empirical evaluations. We highlight the potential of multimodal LLMs for advancing graph structure comprehension tasks and suggests promising directions for future work in improving graph visualisations and multimodal integration."}, {"title": "Appendix", "content": ""}]}