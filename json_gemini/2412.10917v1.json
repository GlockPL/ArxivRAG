{"title": "Adaptive Reward Design for Reinforcement Learning in Complex Robotic Tasks", "authors": ["Minjae Kwon", "Ingy ElSayed-Aly", "Lu Feng"], "abstract": "There is a surge of interest in using formal languages such as Linear Temporal Logic (LTL) and finite automata to precisely and succinctly specify complex tasks and derive reward functions for reinforcement learning (RL) in robotic applications. However, existing methods often assign sparse rewards (e.g., giving a reward of 1 only if a task is completed and 0 otherwise), necessitating extensive exploration to converge to a high-quality policy. To address this limitation, we propose a suite of reward functions that incentivize an RL agent to make measurable progress on tasks specified by LTL formulas and develop an adaptive reward shaping approach that dynamically updates these reward functions during the learning process. Experimental results on a range of RL-based robotic tasks demonstrate that the proposed approach is compatible with various RL algorithms and consistently outperforms baselines, achieving earlier convergence to better policies with higher task success rates and returns. Code is available at https://github.com/RewardShaping/AdaptiveRewardShaping.", "sections": [{"title": "I. INTRODUCTION", "content": "Complex robotic tasks require reinforcement learning (RL) frameworks capable of handling intricate task specifications. An RL agent's behavior is guided by reward functions, which are often challenging to define manually in robotic settings due to the nuanced and hierarchical nature of robotic ob- jectives. Alternatively, an RL agent can infer intended re- wards from demonstrations [1], trajectory comparisons [2], or human instructions [3]. Recently, formal languages such as Linear Temporal Logic (LTL) and finite automata have gained traction as tools for specifying robotic tasks and deriving reward functions in RL (see the extensive list of related work in Section I-A). However, existing methods often rely on sparse rewards (e.g., assigning a reward of 1 only upon task completion and 0 otherwise), which significantly limits their efficiency for robotic applications by requiring extensive exploration to converge to effective policies. Additionally, many prior works are restricted to specific RL algorithms tailored to their reward structures, such as Q-learning for reward machines [4], modular DDPG [5], and hierarchical RL for reward machines [6], posing further challenges for deploying RL in diverse robotic systems.\nReward shaping [7] is a paradigm where an agent receives some intermediate rewards as it gets closer to the goal and has shown to be helpful for RL algorithms to converge more quickly. Inspired by this idea, we develop a logic-based adaptive reward shaping approach in this work. We use the syntactically co-safe fragment of LTL to specify complex RL tasks, such as \"the robotic task is to touch red and green balls in strict order without touching other colors, then touch blue balls\". We then translate a co-safe LTL task specification into a deterministic finite automaton (DFA) and design reward functions that keeps track of the task completion status (i.e., a task is completed if an accepting state of the DFA has been reached).\nThe principle underlying our approach is that we want to assign intermediate rewards to an agent as it makes progress toward completing a task. A key challenge is how to measure the closeness to task completion. We adopt the notion of task progression for robotics defined by [8], which measures each DFA state's distance to accepting states. The smaller the distance, the higher degree of task progression. The distance is zero when a robotic task is fully completed.\nAnother challenge is what reward values to assign for various degrees of task progression. To this end, we design two different reward functions. The progression reward function assigns rewards based on the reduced distance-to-acceptance values. The hybrid reward function balances the progression reward and the penalty for self-loops (i.e., staying in the same DFA state). However, we find that optimal policies maximizing the expected return based on these reward functions may not necessarily lead to the best possible task progression.\nTo address this limitation, We develop an adaptive re- ward shaping approach that dynamically updates distance-to- acceptance values to reflect the actual difficulty of activating DFA transitions during the learning process. We then design two new reward functions, namely adaptive progression and adaptive hybrid, leveraging the updated distance-to-acceptance values. We show that our approach can learn an optimal policy with the highest expected return and the best task progression within a finite number of updates.\nFinally, we evaluate the proposed approach on various dis- crete and continuous robotic tasks. Computational experiments show the compatibility of our approach with a wide range of RL algorithms. Results indicate our approach generally outperforms baselines, achieving earlier convergence to better policies with higher task success rates and returns."}, {"title": "A. Related Work", "content": "[9] presents one of the first works applying temporal logic to reward function design, assigning reward functions based on robustness degrees of satisfying truncated LTL formulas. [10] uses a fragment of LTL for finite traces (called LTLf) to encode RL rewards. Several methods seek to learn optimal policies that maximize the probability of satisfying an LTL formula [5], [11], [12]. However, these methods assign sparse rewards for task completion and do not provide intermediate rewards for task progression.\nThere is a line of work on reward machines (RMs), a type of finite state machine that takes labels representing environment abstractions as input and outputs reward functions. [4] shows that LTL and other regular languages can be automatically translated into RMs via the construction of DFAs. [6] de- scribes a collection of RL methods that exploit the RM structure, including Q-learning for reward machines (QRM), counterfactual experiences for reward machines (CRM), and hierarchical RL for reward machines (HRM). These methods are augmented with potential-based reward shaping [7], where a potential function over RM states is computed to assign intermediate rewards. We adopt these methods (with reward shaping) as baselines for comparison in our experiments. As we will show in Section V, our approach generally outperforms baselines, providing more effective design of intermediate rewards for task progression.\n[13] proposes a new specification language that can be translated into reward functions and later applies it for com- positional RL in [14]. These methods use a task monitor to track the degree of specification satisfaction and assign intermediate rewards. However, they require users to en- code atomic predicates into quantitative values for reward assignment. In contrast, our approach automatically assigns intermediate rewards using DFA states' distance to acceptance values, eliminating the need for user-provided functions.\n[15] presents a reward shaping framework for average- reward learning in continuing tasks. Their method automat- ically translates a LTL formula encoding domain knowledge into a function that provides additional reward throughout the learning process. This work has a different problem setup and thus is not directly comparable with our approach.\n[16] proposes an approach that decomposes an LTL mission into sub-goal-reaching tasks solved in a distributed manner. The same authors also present a model-free RL method for minimally violating an infeasible LTL specification in [17]. Both works consider the assignment of intermediate rewards, but their definition of task progression requires additional information about the environment (e.g., geometric distance from each waypoint to the destination). In contrast, we define task progression based solely on the task specification, follow- ing [8], which is a work on robotic planning with MDPs (but not RL)."}, {"title": "II. BACKGROUND", "content": "Consider an RL agent interacting with an environment modeled as an episodic Markov decision process (MDP), where each learning episode terminates within a finite hori- zon H. Formally, an MDP is denoted as a tuple $M = (S, s_0, A, T, R, \\gamma, L)$ where S is a set of states, $s_0 \\in S$ is an initial state, A is a set of actions, $T : S \\times A \\times S \\rightarrow [0,1]$ is a probabilistic transition function, R is a reward function, $\\gamma \\in [0, 1]$ is a discount factor, and $L : S \\rightarrow 2^{AP}$ is a labeling function with a set of atomic propositions AP. The reward function can be Markovian, denoted by $R: S \\times A \\times S \\rightarrow R$, or non-Markovian (i.e., history dependent), denoted by $R: (S \\times A)^* \\rightarrow R$. Both the transition function T and the reward function R are unknown to the agent.\nAt each timestep t, the agent selects an action $a_t$ given the current state $s_t$ and reward $r_t$. The environment transitions to a subsequent state $s_{t+1}$, determined by the probability distri- bution $T(\\cdot|s_t, a_t)$, and yields a reward $r_{t+1}$. A (memoryless) policy is defined as a mapping from states to probability distributions over actions, denoted by $\\pi : S \\times A \\rightarrow [0,1]$. The agent seeks to learn an optimal policy that maximizes the expected return, represented by $E[\\Sigma r_{t+1}]$."}, {"title": "B. Co-Safe LTL Specifications", "content": "We utilize Linear Temporal Logic (LTL) [18], which is a form of modal logic that augments propositional logic with temporal operators, to specify complex tasks for the robotic agent. We focus on the syntactically co-safe LTL fragment, defined as follows.\n$\\varphi := \\alpha | \\neg\\alpha | \\varphi_1 \\wedge \\varphi_2 | \\varphi_1 \\vee \\varphi_2 | O\\varphi | \\varphi_1 U \\varphi_2 | \\Diamond \\varphi$\nwhere $\\alpha \\in AP$ is an atomic proposition, $\\neg$ (negation), $\\wedge$ (conjunction), and $\\vee$ (disjunction) are Boolean operators, while O (next), U (until), and $\\Diamond$ (eventually) are temporal operators. Intuitively, O means that $\\varphi$ has to hold in the next step; $\\varphi_1 U \\varphi_2$ means that $\\varphi_1$ has to hold at least until $\\varphi_2$ becomes true; and $\\Diamond\\varphi$ means that $\\varphi$ becomes true at some time eventually. A co-safe LTL formula $\\varphi$ can be converted into a DFA $A_\\varphi$ accepting exactly the set of good prefixes for $\\varphi$ [19]. Formally, a DFA is denoted as a tuple $A = (Q, q_0, Q_F, 2^{AP}, \\delta)$, where Q is a finite set of states, $q_0$ is the initial state, $Q_F \\subseteq Q$ is a set of accepting states, $2^{AP}$ is the alphabet, and $\\delta : Q \\times 2^{AP} \\rightarrow Q$ is the transition function.\nConsider a robot aiming to complete a task in a gridworld (Figure 1a). The task is to collect an orange flag and a blue flag (in any order) while avoiding the yellow flag. We describe this task using a co-safe LTL formula $\\varphi = (\\neg y)U((o \\wedge ((\\neg y)Ub)) \\vee (b \\wedge ((\\neg y)Uo)))$, where o, b and y represent collecting orange, blue and yellow flags, respectively. A transition is enabled when its labelled Boolean formula holds. Starting from the initial state $q_0$, a path ending in the accepting state $q_4$ represents a good prefix of satisfying $\\varphi$, indicating that the task has been successfully completed."}, {"title": "C. Task Progression", "content": "We adopt the notion of \u201ctask progression\" introduced in [8] to measure the degree to which a robotic task defined by a co-safe LTL formula $\\varphi$ is completed.\nGiven a DFA $A_\\varphi = (Q, q_0, Q_F, 2^{AP}, \\delta)$, let $Suc_q \\subseteq Q$ be the set of successors of state q, and $|\\delta_{q,q'}| \\in \\{0,..., 2^{|AP|}\\}$. denote the number of possible transitions from q to q'. We write $q \\rightarrow^* q'$ if there is a path from q to q', and $q \\nrightarrow^* q'$ if q' is not reachable from q.\nThe distance-to-acceptance function $d_\\varphi : Q \\rightarrow \\mathbb{R}_{\\geq0}$ is defined as:\n$d_\\varphi(q) = \\begin{cases} 0 & \\text{if } q \\in Q_F \\\\ \\min_{q' \\in Suc_q} d_\\varphi(q') + h(q, q') & \\text{if } q \\notin Q_F, q \\rightarrow^* Q_F \\\\ |2^{AP}| \\cdot |Q| & \\text{otherwise} \\end{cases}$ (1)\nwhere $h(q, q') := \\log_2\\left(\\frac{2^{|AP|}}{|\\delta_{q,q'}|}\\right)$ represents the difficulty of moving from q to q' in the DFA $A_\\varphi$.\nThe progression function $\\rho_\\varphi : Q \\times Q \\rightarrow \\mathbb{R}_{\\geq0}$ between two states of $A_\\varphi$ is defined as:\n$\\rho_\\varphi(q, q') = \\begin{cases} \\max\\{0, d_\\varphi(q) - d_\\varphi(q')\\} & \\text{if } q' \\in Suc_q, q' \\nrightarrow^* q \\\\ 0 & \\text{otherwise} \\end{cases}$ (2)\nThe first condition mandates $q' \\nrightarrow^* q$ to ensure that there is no cycle in the DFA with a non-zero progression value, which is crucial for the convergence of infinite sums of progression [8]."}, {"title": "III. PROBLEM FORMULATION", "content": "The objective of this work is to create reward functions that encourage an RL agent to achieve the best possible progression in accomplishing a task specified by a co-safe LTL formula $\\varphi$. To this end, we define a product MDP $M^\\otimes$ that augments the environment MDP M with information about the task specification $\\varphi$.\nGiven an episodic MDP $M = (S, s_0, A, T, R, \\gamma, L)$ and a DFA $A_\\varphi = (Q, q_0, Q_F, 2^{AP}, \\delta)$, the product MDP is defined as $M^\\otimes = M \\otimes A_\\varphi = (S^\\otimes, s_0^\\otimes, A, T^\\otimes, R^\\otimes, \\gamma, AP, L^\\otimes)$, where $S^\\otimes = S \\times Q$, $s_0^\\otimes = (s_0, \\delta(q_0, L(s_0)))$, $L^\\otimes((s, q)) = L(s)$, \n$T^\\otimes((s, q), a, (s', q')) = \\begin{cases} T(s, a, s') & \\text{if } q' = \\delta(q, L(s')) \\\\ 0 & \\text{otherwise}. \\end{cases}$\nThis work focuses on designing Markovian reward functions $R^\\otimes: S \\times A \\times S^\\otimes \\rightarrow R$ for the product MDP $M^\\otimes$, whose projection onto M yields non-Markovian reward functions.\nIn practice, the product MDP is built on-the-fly during learning. At each timestep t, given the current state $(s_t, q_t)$, an RL agent selects an action $a_t$ and transits to a successor state $(s_{t+1}, q_{t+1})$, where $s_{t+1}$ is given by the environment, sampling from the distribution $T(\\cdot|s_t, a_t)$, and $q_{t+1}$ is derived from the DFA's transition function $\\delta(q_t, L(s_{t+1}))$. The agent receives a reward $r_{t+1}$ determined by the reward function $R^\\otimes ((s_t, q_t), a, (s_{t+1}, q_{t+1}))$.\nAn RL agent aims to learn an optimal policy that maximizes the expected return in the product MDP $M^\\otimes$. A learned mem- oryless policy for $M^\\otimes$ equates to a finite-memory policy in the environment MDP M, denoted by $\\pi : S \\times Q \\times A \\rightarrow [0, 1]$, with the DFA states Q delineating various modes.\nWe define a partition of the state space of DFA $A_\\varphi = (Q, q_0, Q_F, 2^{AP}, \\delta)$ based on an ordering of distance-to-acceptance values. Let $B_0 = Q_F$ and $B_i = \\{q \\in Q \\backslash \\bigcup_{j=0}^{i-1} B_j | d_\\varphi(q) \\text{ is minimal}\\}$ for i > 0.\nThe task progression for a policy of the product MDP, denoted by $b(\\pi)$, is the lowest index of reachable partitioned sets $B_i$ from the initial state. A value of $b(\\pi) = 0$ signifies the task has been successfully completed. The best possible task progression across all feasible policies $\\Pi$ in the product MDP is defined as $b^* = \\min\\{b(\\pi)|\\pi \\in \\Pi\\}$."}, {"title": "III. PROBLEM FORMULATION", "content": "Problem. This work aims to solve the following problem: Given an episodic MDP M with unknown transition and reward functions, along with a DFA $A_\\varphi$ representing a co- safe LTL task specification $\\varphi$, the objective is to construct a Markovian reward function R for the product MDP M \\otimes = M & A\u03c6. This reward function should be designed such that an optimal policy \u03c0*, learned by an RL agent via maximizing the expected return, also achieves the best possible task progression, that is, $b^* = b(\\pi^*)$."}, {"title": "IV. APPROACH", "content": "To solve this problem, we design two reward functions that incentivize an RL agent to improve the task progression (cf. Section IV-A), and develop an adaptive reward shaping approach that dynamically updates these reward functions during the learning process (cf. Section IV-B)."}, {"title": "A. Basic Reward Functions", "content": "First, we propose a progression reward function based on the task progression function defined in Equation 2, representing the degree of reduction in distance- to-acceptance values.\n$R_{pg}((s, q), a, (s', q')) = \\rho_\\varphi(q, q') = \\begin{cases} max\\{0, d_\\varphi(q) - d_\\varphi(q')\\} & \\text{if } q' \\in Suc_q, q' \\nrightarrow^* q \\\\ 0 & \\text{otherwise} \\end{cases}$ (3)\nThe progression reward function rewards only transitions that progress toward acceptance, without penalizing those that stay in the same DFA state. To address this issue, we define a hybrid reward function:\n$R_{hd}((s, q), a, (s', q')) = \\begin{cases} \\eta \\cdot (-d_\\varphi(q)) & \\text{if } q = q' \\\\ (1 - \\eta) \\cdot \\rho_\\varphi(q, q') & \\text{otherwise} \\end{cases}$ (4)\nwhere $\\eta \\in [0, 1]$ balances the trade-offs between penalties and progression rewards."}, {"title": "B. Adaptive Reward Shaping", "content": "While reward functions defined in Section IV-A motivate an RL agent to complete a task specified by a co-safe LTL formula, Examples 4 and 5 show that the learned optimal policies that maximize the expected return do not achieve the best possible task progression. A potential reason is that the distance-to-acceptance function $d_\\varphi$, as defined in Equation 1, may not precisely reflect the difficulty of activating desired DFA transitions within a specific environment. To tackle this limitation, we develop an adaptive reward shaping approach that dynamically updates distance-to-acceptance values and reward functions during the learning process.\nAfter every N learning episodes, with N being a hyperparameter, we evaluate the average success rate of task completion. An episode is deemed successful if it concludes in an accepting state of the DFA $A_\\varphi$. If the average success rate falls below a predefined threshold \u03bb, we proceed to update the distance-to-acceptance values accordingly.\nWe derive initial values $d^0_\\varphi(q)$ for each DFA state $q \\in Q$ from Equation 1. The distance-to-acceptance values for the k-th update round are calculated recursively as follows:\n$d^k_\\varphi(q) = \\begin{cases} d^{k-1}_\\varphi(q) + \\theta & \\text{if } q \\in B_i, \\forall i \\geq b_k \\\\ d^{k-1}_\\varphi(q) & \\text{otherwise} \\end{cases}$ (5)\nwhere $b_k$ is the task progression of the optimal policy learned after k \u00b7 N episodes, and \u03b8 is a hyperparameter, also used later in Equation 8, requiring that \u03b8 > 1.\nNote that Equation 5 does not alter the order of distance- to-acceptance values, so the DFA state partitions \\{Bi\\} remain unchanged. We present two new reward functions that leverage the updated distance-to-acceptance values as follows.\nGiven the updated distance-to-acceptance values $d^k_\\varphi(q)$, we apply the progression function defined in Equation 2 and obtain\n$\\rho_\\varphi(q, q') = \\begin{cases} max\\{0,d^k_\\varphi(q) - d^k_\\varphi(q')\\} & \\text{if } q' \\in Suc_q, q' \\nrightarrow^* q \\\\ 0 & \\text{otherwise} \\end{cases}$ (6)\nThen, we define an adaptive progression reward function for the k-th round of updates as:\n$R^{ap,k}((s, q), a, (s', q')) = max\\{\\rho^k_\\varphi(q, q'), \\rho_\\varphi(q,q')\\}$ (7)\nWhen k = 0, the adaptive progression reward function $R^{ap,0}$ coincides with the progression reward function $R_{pg}$ defined in Equation 3.\nUsing the updated distance-to-acceptance values from Example 6, we calculate the adaptive progression re- wards $R^{ap,1}$ for the first round of update."}, {"title": "B. Adaptive Reward Shaping", "content": "We define an adaptive hybrid reward function for the k-th round of updates as:\n$R^{ah,k}((s, q), a, (s', q')) = \\begin{cases} \\eta_k \\cdot (-d^k_\\varphi(q)) & \\text{if } q = q' \\\\ (1 - \\eta_k) \\cdot max\\{\\rho^k_\\varphi(q, q'), \\rho_\\varphi(q, q')\\} & \\text{otherwise} \\end{cases}$ (8)\nwith $\\eta_0 \\in [0,1]$, and $\\eta_k = \\frac{\\eta_{k-1}}{\\theta^k}$ where \u03b8 is the same hyperparameter used in Equation 5. We require \u03b8 > 1 to ensure that the weight value $\\eta_k$ is reduced in each update round, avoiding undesired behavior from increased self-loop penalties. At k = 0, the adaptive hybrid reward function $R^{ah,0}$ aligns with the hybrid reward function $R_{hd}$ as defined in Equation 4."}, {"title": "V. EXPERIMENTS", "content": "We evaluate the proposed adaptive reward shaping approach in a variety of benchmark RL domains. We describe the experimental setup including environments, RL algorithms, baselines, and evaluation metrics in Section V-A, and analyze the experimental results in Section V-B."}, {"title": "A. Experimental Setup", "content": "Environments. The following RL domains are used: the taxi domain from OpenAI Gym [20], and three other domains adapted from [6].\nOffice world: The agent navigates a 12\u00d79 grid world to: get coffee and mail (in any order), deliver them to the office, and avoid obstacles. The test environment assigns a reward of 1 for each sub-goal: (i) get coffee, (ii) get coffee and mail, and (iii) deliver coffee and mail to the office, all while avoiding obstacles.\nTaxi world: The agent drives around a 5\u00d75 grid world to pick up and drop off a passenger, starting from a random location. There are five possible pickup locations and four possible destinations. The task is completed when the passenger is dropped off at the target destination. The test environment assigns a reward of 1 for each sub-goal: (i) pick up the passenger, (ii) reach the target destination, and (iii) drop off the passenger.\nWater world: The agent moves in a continuous 2D box with six colored floating balls, changing velocity toward one of the four cardinal directions each step. The task is to touch red and green balls in strict order without touching other colors, then touch blue balls. The test environment assigns a reward of 1 for touching each target ball.\nHalfCheetah: The agent is a cheetah-like robot with a continuous action space, controlling six joints to move. The task is completed by reaching the farthest location. The test environment assigns a reward of 1 for reaching each of the five locations along the way.\nFor each domain, we consider three types of environments: (1) deterministic environments, where each state-action pair leads to a single success state only; (2) noisy environments, where each action has a certain control noise; and (3) infeasible en- vironments, where some sub-goals are impossible to complete (e.g., a blocked office that the agent cannot access, or missing blue balls in the water world).\nBaselines. We compare the proposed approach with the fol- lowing methods as baselines: Q-learning for reward machines (QRM) with reward shaping [4], counterfactual experiences for reward machines (CRM) with reward shaping and hier- archical RL for reward machines (HRM) with reward shap- ing [6]. We use the code accompanying publications.\nMoreover, we consider a naive baseline that rewards tran- sitions that decrease the distance to acceptance. For each transition ((s,q), a, (s', q')) in the product MDP, assign a reward of 1 if $d_\\varphi(q) > d_\\varphi(q')$ and there is a path from q to accepting states QF, otherwise assign a reward of 0.\nAlgorithms. We use DQN [21] for learning in discrete domains (office world and taxi world), DDQN [22] for wa- ter world with continuous state space, and DDPG [23] for HalfCheetah with continuous action space. Note that QRM implementation does not work with DDPG, so we only use HRM and CRM as the baselines for HalfCheetah. We also apply PPO [24] and A2C [25] to HalfCheetah (QRM, CRM and HRM baselines are not compatible with these RL algo-"}, {"title": "B. Results Analysis", "content": "We ran 10 independent trials for each method. Figures 2, 3 and 4 plot the mean performance with a 95% confidence interval (the shaded area) in deterministic, noisy, and infeasible environments, respectively. The success rate of task comple- tion is omitted in Figure 4 because it is zero for all trials (i.e., the task is infeasible to complete).\nPerformance comparison. These results show that the pro- posed approach using adaptive progression or adaptive hybrid reward functions generally outperforms baselines, achieving earlier convergence to policies with a higher success rate of task completion and a higher normalized expected return.\nThe significant advantage of our approach is best illustrated in Figure 4, where baselines fail to learn effectively in en- vironments with infeasible tasks. Although baselines apply potential-based reward shaping [7] to assign intermediate re- wards, they cannot distinguish between good and bad terminal states (e.g., completing a sub-goal and colliding with an obsta- cle have the same potential value). In contrast, our approach provides more effective intermediate rewards, encouraging the agent to learn and maximize task progression.\nThe only outlier is the noisy office world where QRM and CRM outperform the proposed approach. One possible reason is that our approach gets stuck with a sub-optimal policy in this environment, which opts for fetching coffee at a closer location but results in a longer route to complete other sub-goals (i.e., getting mail and delivering to office).\nComparing the proposed reward functions, we observe that the adaptive hybrid reward function has the best overall per- formance. Comparing different RL environments, the proposed approach can achieve a success rate of 1 and the maximum possible return in most deterministic environments, but its performance is degraded in noisy environments due to control noise and in infeasible environments due to environmental constraints.\nAblation study. Additionally, we conduct an ablation study to investigate the sensitivity of the hyperparameters \u03b8 and N used for updating distance-to-acceptance values (cf. Section IV-B). The results demon- strate that the proposed approach converges with a sufficiently large value of \u03b8 \u2208 {2,000, 5,000, 10,000}. Moreover, it takes more training steps to achieve policy convergence with larger values of N, indicating longer intervals between consecutive updates of reward functions."}, {"title": "VI. CONCLUSION", "content": "We have developed an adaptive reward design approach for complex robotic tasks. This approach uses reward functions designed to incentivize an agent to complete a task specified by a co-safe LTL formula as much as possible and dynamically updates these reward functions during the learning process. Computational experiments demonstrate that our approach is applicable to various discrete and continuous domains and is compatible with a wide range of RL algorithms. Results show that the proposed approach outperforms baselines in terms of faster convergence and higher task completion rates. For the future work, we would like to evaluate the proposed approach on a broader range of robotic tasks and aim to apply it to real-world RL tasks, such as autonomous driving."}, {"title": "APPENDIX", "content": "Here, we prove the correctness of our approach, as stated in Theorem 1. We start by proving the following auxiliary lemmas.\nLemma 1. Adaptive hybrid reward function $R_{ah,k}$ tends to adaptive progression reward function $R_{ap,k}$ with an increasing number of updates k, that is, $lim_{k\\rightarrow \\infty} R_{ah,k}= R_{ap,k}$.\nProof. By the definition of adaptive hybrid reward func- tion $R_{ah,k}$ (cf. Equation 8), $\\theta \\in [0,1]$ and $\\eta_k= \\frac{\\eta_{k-1}}{\\theta}$. with $\\theta > 1$. We have $lim_{k\\rightarrow \\infty} \\eta_k = 0$. The first case of Equation 8, $\\eta_k \\cdot -d_\\varphi(q)$, tends to 0; and the second case tends to $max\\{\\rho_\\varphi(q, q'), \\rho^k_\\varphi(q, q')\\}$. Thus, it holds that $lim_{k\\rightarrow \\infty} R_{ah,k}= R_{ap,k}$\nLemma 2. Given an episodic MDP M and a DFA A for a co-safe LTL formula \u03c6, let \u3160 and +1 denote the optimal policies of the product MDP_M\u00ae = M \u00ae A\u045f, maximizing the expected return based on adaptive progression reward functions Rap,k and Rap,k+1, respectively. If a policy exists that achieves a higher expected return than \u3160 based on Rap,k+1, then +1 achieves better task progression than \u3160, that is, b(\u03c0+1) < b(\u03c0).\nProof. For the sake of contradiction, suppose that b(\u03c0+1) \u2265 b(\u03c0). Let 7 be a path through the product MDP M\u00ae under policy +1. For any state (s, q) in the path \u03c4, we have q \u2208 \u0392\u2081 where i \u2265 b(+1) \u2265 b(\u03c0) = bk. For every transition ((s, q), a, (s', q')) \u2208\u03c4, it holds that:\n$R_{ap,k+1} ((s, q), a, (s', q')) = max\\{\\rho_\\varphi(q, q'), \\rho^{k+1}_\\varphi(q, q')\\} = max\\{\\rho_\\varphi(q, q'), max\\{0, d^{k+1}_\\varphi(q) - d^{k+1}_\\varphi(q')\\}\\} = max\\{\\rho_\\varphi(q, q'), max\\{0, d^{k}_\\varphi(q) + \\theta - d^{k}_\\varphi(q') - \\theta\\}\\} = max\\{\\rho_\\varphi(q, q'), max\\{0, d(q) - d(q')\\}\\} = max\\{\\rho_\\varphi(q, q'), \\rho_\\varphi(q, q')\\} =Rap,k ((s, q), a, (s', q'))$\nThus, we have $V^{\\pi}_{ap,k+1}(s) = V^{\\pi}_{ap,k}(s)$, meaning that the expected return stays the same. Similarly, we can show that $V^{\\pi}_{ap,k+1}(s) = V^{\\pi}_{ap,k}(s)$. Since is the optimal policy maximizing the expected return based on $R_{ap,k}$, we have\n$V^{\\pi}_{ap,k}(s) \\geq V^{\\pi}_{ap,k+1}(s)$. (9)\nGiven that there exists a policy that achieves a higher expected return than \u3160 based on $R_{ap,k+1}$, it holds that\n$V^{\\pi}_{ap,k+1}(s) = V^{\\pi+1}_{ap,k+1}(s) < V^{\\pi+1}_{ap,k+1}(s)$. (10)\nEquation 9 contradicts with Equation 10. Thus, we have b(\u03c0+1) < b(\u03c0)."}, {"title": "APPENDIX", "content": "Now we are ready to prove Theorem 1 as stated in Sec- tion IV and repeated here.\nTheorem 1. Given an episodic MDP M and a DFA A\u03c6 corresponding to a co-safe LTL formula \u03c6, there exists an optimal policy \u03c0* of the product MDP M\u00ae = M&A that maximizes the expected return based on a reward function R\u2208 {Rok Rank} for some k \u2208 N, where the task progression for policy \u03c0* matches the best possible task progression b* across all feasible policies in the product MDP M\u00ae, that is, b* = b(\u03c0*).\nProof. Without loss of generality, we focus on the adaptive progression reward function Rap,k, as Lemma 1 shows that limk\u2192\u221e Rah.k = Rap,k. Let \u03c0 denote an optimal policy of the product MDP M that maximizes the expected return based on Rok. Suppose that b(\u03c0) > b*. There exists a policy in the product MDP that achieves the best possible task progression b*, where $V^{\\pi}_{ap,k}(s) \\leq V^{\\pi^*}_{ap,k}(s)$. If $V^{\\pi}_{ap,k}(s) = (58)$, then is the desired optimal policy \u03c0* that maximizes the expected return based on Rap, while achieving the best possible task progression b*. This theorem is thus proved.\nOtherwise, when $V^{\\pi}_{ap,k}(s) < V^{\\pi^*}_{ap,k}(s)$, we proceed to prove the theorem as follows. Let the difference in expected returns be denoted by \u03c3 = Vapk(s) - Vap,k(s) > 0. Consider a worst-case scenario where policy reaches a state with the best possible task progression only at the end of an episode. Formally, there is only one path \u03c4 of length T = H through the product MDP M\u00ae un- der policy that ends with a transition ((s,q), a"}]}