{"title": "NORMALIZING FLOW BASED METRIC FOR IMAGE GENERATION", "authors": ["Pranav Jeevan", "Neeraj Nixon", "Amit Sethi"], "abstract": "We propose two new evaluation metrics to assess realness of generated images based on normalizing flows: a simpler and efficient flow-based likelihood distance (FLD) and a more exact dual-flow based likelihood distance (D-FLD). Because normalizing flows can be used to compute the exact likelihood, the proposed metrics assess how closely generated images align with the distribution of real images from a given domain. This property gives the proposed metrics a few advantages over the widely used Fr\u00e9chet inception distance (FID) and other recent metrics. Firstly, the proposed metrics need only a few hundred images to stabilize (converge in mean), as opposed to tens of thousands needed for FID, and at least a few thousand for the other metrics. This allows confident evaluation of even small sets of generated images, such as validation batches inside training loops. Secondly, the network used to compute the proposed metric has over an order of magnitude fewer parameters compared to Inception-V3 used to compute FID, making it computationally more efficient. For assessing the realness of generated images in new domains (e.g., x-ray images), ideally these networks should be retrained on real images to model their distinct distributions. Thus, our smaller network will be even more advantageous for new domains. Extensive experiments show that the proposed metrics have the desired monotonic relationships with the extent of image degradation of various kinds.", "sections": [{"title": "1 Introduction", "content": "In recent years, several generative adversarial networks (GAN) and diffusion-based image generation models have been proposed for various applications, such as inpainting, text-based image manipulation, and text-to-image generation, that have also caught popular imagination. To compare individual generated images or the overall performance of various models, a robust evaluation metric is essential in determining how closely the generated images resemble real images. This is crucial for understanding whether generative models can produce images that appear realistic to human observers, which is a key goal in many applications.\nEvaluating the quality and performance of image generation models presents a unique challenge. Unlike traditional vision tasks such as classification or segmentation, there is no theoretically sound metric to assess performance. This is because generated images require evaluation across multiple dimensions, including quality, aesthetics, realism, and diversity, all of which are deeply rooted in subjective human perception. However, human evaluation is costly and difficult to scale for larger datasets. As a result, researchers rely on automated evaluation techniques to gauge the performance of these models.\nIn the absence of human evaluation, a good metric of \"fakeness\" should robustly and monotonically increase with the extents of image degradations of various kinds. Additionally, it should be reliable to assess a given generative model even on a small set of generated images. That is, its mean value should converge with a small number of images generated under similar conditions. Plus, evaluating the metric should be computationally light so that it can be used inside the validation step of training iterations.\nAmong the various methods proposed for the automatic evaluation of generative models, the Fr\u00e9chet inception distance (FID) [1] has emerged as the one that is most widely used by practitioners. It approximates the feature distributions of real and generated images as multivariate Gaussians and estimates the distance between them using Fr\u00e9chet distance (FD). The features used are the activations of the penultimate layer of an Inception-V3 architecture that is pre-trained for classification of the ImageNet dataset. The comparison occurs at object or semantic level, ignoring the fine-grained details of the image. Its simplicity and ease of use have made it a de facto metric in the field despite its known limitations.\nExperiments on text-to-image models have highlighted that FID is not an ideal metric as it often disagrees with human ratings, making it unsuitable for evaluating the quality of generated images [2]. Additionally, statistical tests and empirical evaluation show the limitations of FID in accurately capturing the nuances of image generation [2]. FID can also be biased depending on the specific model being evaluated. Additionally, it requires a large number of samples to generate a stable and reliable metric value, which can be a limitation in scenarios where fewer samples are available for evaluation or we want to save on computations, such as in a validation step inside a training iteration.\nThe main goal of generative models is to learn a data distribution that closely approximates the real data, allowing them to generate images with a distribution that mirrors the real one. Ideally, the distance between the two distributions should be minimal, indicating that the generated images are highly similar to real images in terms of their underlying statistical properties.\nOur proposed metrics leverage normalizing flows to approximate the data distribution of both generated and real images. By performing density estimation, they assess how well the distribution of generated images aligns with the distribution of real images, providing a more accurate measure of the similarity between the two.\nThe main contributions of this paper are:\n\u2022 We introduce a new method for evaluating the performance of generative models by directly assessing the likelihood of each generated image using normalizing flows. To our knowledge, the use of normalizing flows, which can directly and efficiently compute the likelihood of generated images from the real data distribution, has not been previously employed as an evaluation metric.\n\u2022 We propose a theoretical model that employs dual normalizing flows - one trained on generated images and the other on real images and demonstrate that this approach is monotonic with respect to various distortions, proving to be an effective evaluation metric.\n\u2022 We present an efficient and faster single-model normalizing flow as an evaluation metric, showing that it possesses all the essential properties of a good metric. Additionally, it significantly outperforms FID in terms of speed and provides a stable evaluation using a smaller number of generated or real images."}, {"title": "2 Related Works", "content": "In this section we summarize previous metrics to evaluate generated images and their limitations, and highlight the relevant properties of normalizing flows on which the proposed metrics are based."}, {"title": "2.1 Previous Metrics and their Limitations", "content": "Several evaluation metrics have been proposed for generative models, including inception score (IS) [3], kernel inception distance (KID) [4], FID [1], perceptual path length [5], Gaussian Parzen window [6], clip maximum mean discrepancy (CMMD) [2] as well as human annotation techniques, such as HYPE [7]. Among these, inception score and FID have gained significant popularity. The IS utilizes an Inception-V3 model trained on ImageNet-1k to assess the diversity and quality of generated images by analyzing their class probabilities. A key advantage of the Inception Score is that it does not require real images to compute the metric value, making it convenient for certain applications.\nKID [4] and FID both require the presence of real images alongside the generated images for evaluation. These metrics are computed by measuring the distance between the distributions of real and generated data. KID uses the squared maximum mean discrepancy (MMD) distance, while FID employs the squared FD between the two distributions to assess how well the generated images resemble the real ones.\nIS, FID and KID rely on Inception embeddings, which have been trained on 1.3 million images and are limited to the 1,000 classes from the ImageNet-1k dataset [8]. This restriction limits their ability to effectively represent the richer, more complex, and diverse domain images that can emerge from modern image generation tasks, making them less suitable for evaluating certain types of generated content.\nSome of the previous works have highlighted the unreliability of evaluation metrics in image generation, especially pointing out the limitations of FID [9]. It was shown that FID is a biased estimator and can show significant variation in FID scores for low-level image processing operations, such as compression and resizing [10]."}, {"title": "2.1.1 Normality Assumption", "content": "In the computation of FID, the calculation of the FD relies on the assumption that the Inception-V3 embeddings are normally distributed. This is because the closed-form solution for the FD only applies to multivariate normal distributions [11]. If the embeddings deviate significantly from this assumption, it can affect the accuracy and reliability of the FID score. Since real image sets typically do not follow normal distributions, this assumption can introduce a significant bias in the computation of the FD and, consequently, the FID score. Additionally, estimating 2048 x 2048 dimensional covariance matrices from a small sample of generated and real images, which is required for FID computation, can further exacerbate the error, leading to unreliable evaluations.\nThis issue is demonstrated by using a 2D isotropic Gaussian distribution at the origin as the reference distribution and measuring the distance between it and a series of mixture-of-Gaussian distributions [2], as shown in the Table 1. The second set of distributions is generated as a mixture of four Gaussians, each sharing the same mean and covariance as the reference Gaussian. This example illustrates how the incorrect normality assumption can lead to misleading distance measurements between real distributions. Since the first mixture shares the same distribution as the reference distribution, we expect any reasonable distance metric to measure zero distance between the two. However, as we progressively move the four components of the mixture distribution further apart from each other, while keeping the overall mean and covariance fixed, the mixture distribution naturally diverges from the reference. Despite this, the FD calculated under the normality assumption, continues to report a misleadingly zero distance, as it assumes normality throughout. Furthermore, the unbiased version of FID, as proposed in previous work [9], also suffers from this limitation since it too relies on the same flawed normality assumption.\nMMD has been shown to work better than FD and overcome this limitation, as it does not rely on the assumption that the distributions are multivariate Gaussians. Similarly, as we demonstrate in Table 1, our normalizing flow-based metrics also surpass this limitation. Since they do not require the distributions to be Gaussian, the proposed metrics provide a more accurate and reliable assessment of the difference between real and generated data distributions."}, {"title": "2.1.2 ImageNet Pre-trained Models", "content": "Previous metrics, e.g., IS, FID, CMMD and KID [4], rely on modeling distributions of real images based on features extracted using CNNs that are pre-trained on ImageNet. This creates a bias towards the dataset that was used for pre-training, which limits their applicability to other domains, such as medical x-ray images. To accurately represent images in these specialized fields and new domains, new embeddings must be generated by pre-training on domain-specific datasets. This can often be challenging due to the often limited size of such datasets, making it difficult to produce accurate embeddings.\nHence, it is preferable to have a metric that relies solely on the available real and generated images to create their respective data distributions. This approach allows for a more accurate comparison of how closely the generated images align with the real image distribution. Our proposed normalizing flow-based metrics achieve this by directly comparing the distributions without relying on pre-trained embeddings, making it better suited for diverse and specialized domains."}, {"title": "2.2 Normalizing Flows", "content": "Normalizing flows are generative models built on invertible transformations. Among all the generative models that are widely used, such as, GANs, VAEs, and diffusion, normalizing flows are the only type of model for which the likelihood of data can be computed exactly and efficiently [12, 13]. None of the other models explicitly learn the likelihood of the real data. While VAEs model a lower bound of the likelihood, GANs only provide a sampling mechanism for generating new data, without offering a likelihood estimate [13].\nA normalizing flow is able to generate the likelihood of a given sample x by transforming a complex data distribution into a simpler one, typically a Gaussian distribution, through a series of invertible transformations. Each transformation maps the input data into a new space while maintaining the ability to compute the log-likelihood of the data $p_x(x)$ in the original space using the change of variables formula:\n$$log p_x(x) = log p_z (f(x)) + log \\text{det } \\left(\\frac{\\partial f(x)}{\\partial x}\\right);$$\nwhere $f(x)$ is the transformation applied by the normalizing flow, $p_z (f(x))$ is the likelihood of the transformed sample in the latent space (e.g., the probability density of z under a Gaussian distribution), and $\\text{det } \\left(\\frac{\\partial f(x)}{\\partial x}\\right)$ is the Jacobian determinant of the transformation that accounts for the change in volume during the transformation [13].\nThe function $f(x)$, which represents the transformation applied by the normalizing flow, plays a crucial role in mapping the complex data distribution to a simpler latent distribution. This transformation can be modeled by neural networks layers, but the layers so used must be an invertible function to ensure that the mapping between the original data and the latent space can be reversed. Additionally, $f(x)$ needs to have a tractable Jacobian determinant to facilitate efficient computation of the likelihood. To learn a data distribution, we train the parameters of the neural network $f(x)$ to maximize the likelihood of training data [12]."}, {"title": "3 The Flow-based Likelihood Distance Metric", "content": "We propose two evaluation metrics for generative models using the likelihood estimates that can be obtained from normalizing flows. We first take a set of real images and a set of generated images.\n$R = \\{r_1, r_2,...,r_n\\}$ (set of real images)\n$G = \\{g_1, g_2,...,g_m\\}$ (set of generated images)"}, {"title": "3.1 Dual Flow-based Likelihood Distance", "content": "For calculating the dual-flow based likelihood distance (D-FLD), we first use two separate normalizing flows $N_r$ and $N_g$ and train them using real images $R$ and generated images $G$, respectively. This enables the flow $N_r$ to learn the probability distributions of the real images and flow $N_g$ to learn the distribution of generated images. Once we have the trained $N_r$ and $N_g$, we then pass all the images, both real and generated through both the flow and evaluate the log-likelihood of each image. For all images x in sets R and G.\n$L_r = N_r(x) \\forall x \\in R \\cup G$\n$L_g = N_g(x) \\forall x \\in R \\cup G$\nThis give the log-likelihood of the image with respect to the real distribution ($L_r$) and the log-likelihood of the image with respect to the generated distribution ($L_g$). If the generated distribution closely approximates the real distribution, the likelihood of each image with respect to the two sets R and G should be similar and their difference should be small. We then compute the distance d for an image as the absolute value of difference in the likelihoods obtained from $N_r$ and $N_g$. For a dataset, we compute the average d value for all images in R and G as our metric, m. We take its log value to make its range more interpretable.\n$d(x) = |L_r - L_g | \\forall x \\in R \\cup G$\n$m = \\frac{\\sum_{x \\in R \\cup G} d(x)}{|R| + |G|}$"}, {"title": "3.2 Flow-based likelihood Distance", "content": "The D-FLD requires us to train two normalizing flows one each on real and generated data \u2013 and to pass all the images through both the flows. This consumes time and computational resources. Therefore, we propose a simpler and more practical metric which only needs training of a single normalizing flow with just the real data R. This metric can use a pre-trained network."}, {"title": "4 Experiments and Results", "content": "We now describe the datasets, implementation details, experimental design, and the results."}, {"title": "4.1 Datasets and Implementation Details", "content": "We used CIFAR-10 [14] and CelebA-HQ [15] datasets for our experiments to analyse the behaviour of our proposed metric and compare it with FID. We used CIFAR-10 at 32 \u00d7 32 and CelebA-HQ at 256 \u00d7 256 resolution. All experiments were run on Nvidia A6000 GPU.\nWe used a simple normalizing flow [16] for calculating D-FLD which has four variational dequantization layers [17] followed by eight coupling layers [18]. For computing FLD, we use a multi-scale normalizing flow with four variational dequantization layers [17] followed by eight coupling layers [18]. We apply checkerboard masks throughout the networks [18]. More details are provided in the Appendix."}, {"title": "4.2 Evaluating Image Distortions", "content": "We tested if FLD and D-FLD monotonically increased with levels of various types of image distortions, and found the trends to be robust and as desired. Additionally, when a small amount of Gaussian noise was added to the images, we observed that FID does not give a monotonically increasing trend and incorrectly evaluates noisy images as having better quality than those with lower noise. On the other hand, D-FLD has a monotonic trend, as seen in Figure 6. This allows accurate characterization of the images as progressively noisier, and suggests that D-FLD is a more reliable and effective metric than FID. Our results matches previous work which have shown that FID sometimes reverses in trend undesirably when subtle distortions are applied [2].\nFigure 7 illustrates the behavior of FLD and FID when Gaussian noise is added to CelebA-HQ images. It is evident that FID is more sensitive to larger amounts of noise, as the variation in its values becomes significant only when a substantial level of noise is introduced. In contrast, FLD is highly sensitive even in the low noise range. When only 0.1% to 0.5% Gaussian noise is added, the variation in FID is minimal, whereas our FLD metric shows a much greater sensitivity. Given that modern generative models excel at producing high-quality images, it is crucial for evaluation metrics to detect subtle nuances and small amounts of noise with precision. FID fails to meet these demands from the latest models, but FLD is able to do so effectively. Therefore, FLD is a far more suitable metric for evaluating the performance of state-of-the-art generative models."}, {"title": "4.3 Progressive Image Generation", "content": "Modern image generation models, such as diffusion, iteratively generate high quality image by refining noisy images in steps. It was shown that FID does not behave monotonically when denoising iterations proceed [2] and hence should not be used as a metric to evaluate these models, especially for the final iterations when the noise level is low. Figure 8 and Figure 9 shows the FLD values for the final iterations in a 1000 step diffusion process [19] and shows that it can accurately capture the differences in image quality."}, {"title": "4.4 Efficiency", "content": "Earlier work [9, 2], along with our own experiments, has demonstrated that computing the FID value requires a large number of images to get a stable mean score, which translates to a poor sample efficiency. Our experiments indicate that more than 20,000 images are necessary to reliably estimate FID, and using smaller sample sizes results in an unreliable estimate, which can significantly deviate from the true value, as shown Figure 1. Even the metrics that were proposed after FID, such as CMMD, require at least a few thousand samples for a reliable estimation of a generative model's average behavior [2]. Figure 1 and Figure 10 clearly demonstrates that FLD metric achieves high sampling efficiency, as it provides a stable and reliable metric with just 200 samples. This showcases the robustness and efficiency of FLD compared to other metrics, and makes it attractive even inside training loops, such as during validation steps.\nThe model used for normalizing flow in FLD also uses an order of magnitude fewer parameters (1.9 M) compared to the Inception-V3 model (23.8 M parameters) used in FID computation. Thus, FLD can also be trained on edge devices. These advantages makes FLD much more efficient for evaluation of generative models.\nJust like FID, FLD needs to be trained only once on the real images of a given domain. Once the normalizing flow is trained, evaluation of generated images are fast since we do not need to retrain the normalizing flow."}, {"title": "5 Conclusion", "content": "In this paper, we have demonstrated that FID is not a reliable metric for evaluating generative models. Our findings highlight several critical issues with FID, including its requirement of a large number of image samples to provide a reliable estimate, which limits its efficiency and applicability in real-time or low-sample scenarios. Additionally, FID exhibits non-monotonic behavior, particularly when dealing with diffusion models and image distortions. To address these limitations, we have introduced two new metrics based on normalizing flows: dual-FLD and FLD. These metrics have proven to be monotonic with respect to image distortions and perform well with diffusion models. Furthermore, our metrics are highly sample efficient, requiring only a few hundred images to provide a reliable estimate. This makes them a more practical and effective alternative to FID for evaluating the quality of images generated by modern generative models.\nAs a limitation, we advise practitioners to use our metrics with caution for evaluating the performance of other generative models, especially those based on normalizing flows. This is because we have tested the metric only on images generated by adding distortions to real images and on those generated by diffusion-based models. Additionally, due to limitations in computational resources, we were unable to test our metrics on larger generative models, such as text-to-image models, or on more extensive datasets with diverse categories and domains. These remain areas for future exploration and evaluation.\nWe believe that our work will spark further research in the search for better and more efficient evaluation metrics for generative models. Additionally, there is potential to optimize normalizing flow-based evaluation metrics by constructing even more efficient and expressive normalizing flows or training on more advanced normalizing flows than those we have used in this study. This opens the door for continued improvement in the field of generative model evaluation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Sampling Experiment", "content": "To determine the number of samples required for the FID and FLD to reach reliable values, we conducted an experiment using a pre-trained Denoising Diffusion Probabilistic Model (DDPM) [19] trained on the CelebA-HQ dataset [20]. We generated a set of images from this model and systematically varied the number of images sampled from this set to compute the FID and FLD metrics. By measuring these metrics across different sample sizes, we aimed to assess the influence of sample quantity on the convergence behavior of FID and FLD. For each sample size, we ran the experiment 10 times with different images sampled randomly from both real and generated image sets."}, {"title": "A.2 Results for Image Distortions", "content": "Gaussian Noise: We construct a noise matrix $N$ with values drawn from a $N(0, 1)$ Gaussian distribution and scaled to the range [0, 255]. The noisy image is then computed by combining the original image matrix $X$ with the noise matrix $N$ as $(1 \u2212 \u03b1) \u00b7 X + \u03b1 \u00b7 N$, where $\u03b1 \u2208 \\{0, 0.001, 0.005, 0.01, etc\\}$ determines the amount of noise added to the image. A larger value of \u03b1 introduces more noise and the noisy image is clipped to ensure that all pixel values remain within the valid range [0, 255].\nGaussian Blur: The image is convolved with a Gaussian kernel with standard deviation specified by a blur readius. The larger the values of blur radius r greater the amount of blur. In this case, $r \u2208 \\{0, 0.25, 0.5, 1, 2\\}$, and the Gaussian blur is applied uniformly across the image.\nSalt and Pepper Noise Addition: To add salt and pepper noise to an image, random values are generated for each pixel. The probability p controls the proportion of pixels that will be altered. For salt noise, pixels with random value less than $\\frac{1}{2}$ are set to the maximum intensity value (255). Similarly, for pepper noise, pixels with random value greater than $\\frac{1}{2}$ are set to the minimum intensity value (0). The amount of noise is directly proportional to the value of p, here $p \u2208 \\{0, 0.001, 0.005, 0.01, etc\\}$ with larger values of p resulting in a higher number of noisy pixels in the image."}, {"title": "A.3 Implementation Details", "content": "For computing FLD, we used a multiscale flow-based generative model for images that integrates variational dequantiza- tion with a sequence of coupling layers parameterized by gated convolutional networks [16]. The architecture comprises 12 coupling layers: 4 within the Variational Dequantization Layer (with hidden channels of 16, using checkerboard masks), 2 coupling layers after dequantization (hidden channels of 32, using checkerboard masks), 2 coupling layers after the first squeeze flow layer (with input channels of 12 and hidden channels of 48, employing channel-wise masks), and 4 coupling layers in the final block after the second squeeze flow (with input channels of 24 and hidden channels of 64, also using channel-wise masks). The model also includes 2 squeeze flow layers to adjust spatial and channel dimensions and 1 split flow layer to partition the data for efficient modeling. This design enables efficient learning of complex image distributions across multiple scales while maintaining invertibility for exact likelihood computation and facilitating efficient sampling.\nThe split flow layer is used to divide the input into two parts during the forward pass. One part is passed directly through the flow, while the other part is evaluated against a Gaussian prior distribution. This approach reduces the dimensionality of the transformed data, allowing the model to focus on lower-dimensional latent representations in later layers.\nFor experiments involving sampling efficiency, we used a CelebA-HQ pre-trained diffusion model for generating images [20]. For experiments on checking the performance of our metric on progressive image generation, we used the original DDPM model [19]."}]}