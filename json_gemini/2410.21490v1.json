{"title": "Can Large Language Models Act as Symbolic Reasoners?", "authors": ["Rob Sullivan", "Nelly Elsayed"], "abstract": "The performance of Large language models (LLMs) across a broad range of domains has been impressive but have been critiqued as not being able to reason about their process and conclusions derived. This is to explain the conclusions draw, and also for determining a plan or strategy for their approach. This paper explores the current research in investigating symbolic reasoning and LLMs, and whether an LLM can inherently provide some form of reasoning or whether supporting components are necessary, and, if there is evidence for a reasoning capability, is this evident in a specific domain or is this a general capability? In addition, this paper aims to identify the current research gaps and future trends of LLM explainability, presenting a review of the literature, identifying current research into this topic and suggests areas for future work.", "sections": [{"title": "1. Introduction", "content": "Newell and Simon posited that \"A physical system has the necessary and sufficient means for general intelligent action\" [1]. The symbol processing approach produced many remarkable results in the early days of AI. In 2007, Nilsson predicted that \"AI systems that achieve human-level intelligence will involve a combination of symbolic and non-symbolic processing\" [2].\nLarge language models (LLMs), typically built on the transformer architecture [3], have demonstrated impressive results in many natural language processing (NLP) applications. The underlying encoder-decoder architecture creates semantic embeddings of unstructured data, uses a self-attention mechanism as a fundamental building block and a user-generated query for generating contextual, meaningful output. The results have been impressive with many foundation models such as the Generative Pretrained Transformer (GPT) family of models providing results across a wide range of NLP use cases. Their training methodology, predicting the next word in a sequence, while effective in capturing the underlying structure of the data, is essentially encoding an understanding of the statistical nature of data and problems, rather than providing a mechanism for learning to reason [4].\nThis criticism of LLMs, their inability to perform symbolic reasoning, has been a topic of much discussion across the AI community. Symbolic reasoning as a process of manipulating symbols to derive new symbols or to derive new symbols from existing symbols is considered a key component of general intelligence, even if constrained to specific domains [5]. The question, therefore, of whether LLMs can perform symbolic reasoning is an important one, that has implications for future AI systems.\nOne of the common themes used to approach this problem has been through prompt engineering and augmenting using frameworks such as re-Act [6], approaches such as chain-of-thought (CoT) [7], tree-of-thought (ToT) [8], or more recently graph-of-thought [9] to provide a semblance of reasoning ability. Another common approach illustrated in the current literature is to pair an LLM with an external symbolic reasoning component. A third, more recent approach is to use a knowledge graph [10].\nThe advent of LLMs and their rapid application across domains has necessitated a better understanding of the processes underlying the conclusions they draw. These include whether an LLM can perform symbolic reasoning, and if so, are there restrictions on this ability, such as specific domains, or is it more general? Can LLMs perform symbolic reasoning? If so, are there restrictions on this ability? Is this restricted to specific domains, or can it be generalized across domains? The utility of LLMs and their widespread use necessitates a better understanding of the outputs generated by such systems.\nReasoning can be thought of as a form of explanation, or as planning and strategy. These capabilities will be necessary for future systems to be able to interact with humans in a meaningful way. Being able to explain the reasoning behind a decision is a key component of trust, and to improve the confidence in a system. Mechanisms for generating strategies and planning a course of action is another key component of reasoning that will be necessary for future applications. Valmeekam et al. comment that it is often difficult"}, {"title": "2. Methodology", "content": "This paper used the approach proposed by Kitchenham et al. [14] and [15] to aid in formulating our research questions and overall search and selection process. We used as sources Google Scholar, the Association for Computing"}, {"title": "3. Results", "content": "The initial search returned a total of 83 papers, as shown in the 'Initial' column of Table 1.\nAn initial review of the papers returned from the search query revealed that only 14 papers were relevant to the specific research question, as shown in the post-inclusion/exclusion (Post-I/E) column of Table 1. This was primarily due to excluded papers referencing reasoning but not exploring the topic in any detail. The remaining papers were then reviewed in more detail to identify the specific research questions and methodologies used. The data extracted from the papers is shown in Table 2.\nIn this study, eight papers stated use of a symbolic reasoning module external to the LLM itself. Of these, two incorporated fine-tuning of the LLM in addition to using a symbolic reasoning module. One used only fine-tuning as a mechanism for providing reasoning. The remaining five papers used sophisticated prompting as an approach to guiding the LLM in generating results. This is shown in Table 3.\nAugmented prompting is defined in this study to indicate an approach above and beyond a commonly used prompting strategy such as chain-of-thought or tree-of-thought. For example, in [6], the authors introduced the concept of \"thoughts\" into the prompting strategy. In [24], the prompting methodology included pruning of intermediate steps to generate rationale chains. The most commonly used methodology in the literature reviewed was chain-of-thought prompting, both in the template-based approaches and others. For example, [13] used a chain-of-thought approach within the teacher-forcing mechanism they described to train student models.\nAn interesting approach to the problem, using a separate reasoning model was proposed by Collins et al. [20] and included a benchmark aimed at quantifying two of the major components of reasoning: explanation and planning."}, {"title": "4. Analysis and Discussion", "content": "This review raised the question of what we mean by reasoning. Explanation; memorization; and planning and strategy could each be considered a form of reasoning. In the literature reviewed, the term reasoning was not consistently defined. However, where some form of augmented prompting was used, the intermediate explanations would be passed into the next step of the process as well as being used to explain the reasoning to date [12], [8]. Such approaches suggest that any reasoning being performed benefits from external guidance of some form. The work of [27] confirms this and may be a potential area for future research to explore and better understand the implications and benefits of this approach.\nIn addition to the meaning of the term, which could be highlighted as an issue, the search terms used could be challenged. Terms such as deduction, inference, may be as pertinent. Yao et al. [6] used the term 'acting' in their paper's title. This highlighted that the topic does not have a consistent definition across the literature.\nThe use of some form of sophisticated prompting approach has improved the results from LLMs and, in some cases has improved the quality of the output by eliminating hallucinations and other artifacts. This is a significant improvement that gives confidence in the results generated by large language models which has been another criticism of the technology, but which is not considered here. However, incorporating a symbolic reasoning component along with the LLM suggests the overall application provides some mode of symbolic reasoning. Fang et al. [16], conclude that LLMs can act as neurosymbolic reasoners, at least in the constrained domains of text-based games with constrained prompts. However, their use of a separate symbolic module raises questions about the extent to which the LLM itself is performing the reasoning step.\nWhile providing remarkable results in specific problem domains, the question of whether LLMs have the inherent capacity for symbolic reasoning when used alone is not definitively answered in the literature reviewed as part of the scope of this paper, although several authors present evidence that supports explainability if not planning and strategy [16], [12]. The inherent probabilistic engine underpinning LLMs and the training to predict statistically likely next words in a sequence may be a limiting factor in their ability to perform symbolic reasoning. However, this requires further investigation and may be an interesting area of future research. That being said, the work of\nZhang et al. [18] has shown that LLMs can be improved in logical reasoning tasks through differentiable symbolic programming.\nThe use of external symbolic reasoning modules, or sophisticated prompting strategies, may be suggestive of the limitation discussed above. Sheng et al. [19] has shown that LLMs can be integrated with first-order logic, using a symbolic reasoning module to perform logical prediction tasks. This use of symbolic reasoning modules, such as in Cunnington et al. [21], where it was shown that foundation models can be used in neurosymbolic learning and reasoning to perform vision-language tasks, suggests another future path that may have more general-purpose utility.\nUsing more sophisticated prompting strategies could be argued to be a form of symbolic reasoning, as it is a form of explanation. How much further value could prompting strategies provide? Can such prompting strategies provide the external guidance and improve LLM results? Can these provide robust, transparent explanations? An area for future research may be to explore the extent to which these prompting strategies can be used to generate explanations, and to what extent, if any, they can be used to generate strategies and plans. The work of Yao et al. [6] offers an interesting approach with the insertion of \"thoughts\" into the prompt sequence. The work of Gaur et al. [17] has shown that LLMs can be used to perform symbolic math word problems. This approach uses a symbolic reasoning module to perform verification tasks. An interesting comment they made was how self-prompting could be used to generate better explanations, although this wasn't explored further at the time of writing.\nMagister et al. [13] took an interesting approach of using smaller language models to perform reasoning tasks via a teacher-student model through knowledge distillation from the larger model to the smaller. The smaller models being taught to reason by a larger teacher model, using chain-of-thought prompting and performing knowledge distillation tasks whereby a smaller model performs the same task as the larger model. While there is evidence for improved performance, the question of whether the smaller model is performing reasoning or simply memorizing the output of the larger model is not clear.\nRecent iterations of foundation models have included additional volumes of data specific to activities such as programming. This has allowed for the development of models that can perform symbolic reasoning tasks in the context of programming, such as Meta's Code Llama model [28]. An interesting question for future research would be the extent to which, for example, such"}, {"title": "5. Conclusion", "content": "Current literature is divided on the question of whether large language models have an inherent capability to perform symbolic reasoning. Fang et al. [16] concluded that LLMs can act as neurosymbolic reasoners, at least in the constrained domains of text-based games with constrained prompts."}]}