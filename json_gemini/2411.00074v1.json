{"title": "RPS: A Generic Reservoir Patterns Sampler", "authors": ["Lamine Diop", "Marc Plantevit", "Arnaud Soulet"], "abstract": "Efficient learning from streaming data is important for modern data analysis due to the continuous and rapid evolution of data streams. Despite significant advancements in stream pattern mining, challenges persist, particularly in managing complex data streams like sequential and weighted itemsets. While reservoir sampling serves as a fundamental method for randomly selecting fixed-size samples from data streams, its application to such complex patterns remains largely unexplored. In this study, we introduce an approach that harnesses a weighted reservoir to facilitate direct pattern sampling from streaming batch data, thus ensuring scalability and efficiency. We present a generic algorithm capable of addressing temporal biases and handling various pattern types, including sequential, weighted, and unweighted itemsets. Through comprehensive experiments conducted on real-world datasets, we evaluate the effectiveness of our method, showcasing its ability to construct accurate incremental online classifiers for sequential data. Our approach not only enables previously unusable online machine learning models for sequential data to achieve accuracy comparable to offline baselines but also represents significant progress in the development of incremental online sequential itemset classifiers.", "sections": [{"title": "I. INTRODUCTION", "content": "Stream data mining is a subset of data mining, aiming to extract valuable knowledge, patterns, and insights from continuously flowing data streams [1]. Unlike static data, data streams consist of an unbounded, constant flow of in- formation from diverse sources such as sensors, social media, financial transactions, and network traffic. Various applications and algorithmic advancements have emerged in stream data mining. Sequential pattern mining, for example, is crucial for market basket analysis and web clickstream analysis [2]. Additionally, efficient algorithms have facilitated real-time analytics in anomaly detection, retail analysis [3], probabilistic neural networks [4], and high utility itemsets in weighted itemsets [5]. Moreover, methods for mining periodic batches and detecting drift [6] have also been proposed.\nDespite these successes, stream data mining, characterized by its continuous and rapidly changing nature, poses unique challenges for traditional data processing techniques. Reser- voir sampling has emerged as a fundamental method for randomly selecting a fixed-size sample from data streams, offering simplicity and constant space complexity. Reservoir pattern sampling has been recently proposed [7] by adapting the reservoir sampling approach [8] for itemset only. However, despite these advancements, challenges persist. These tech- niques may face limitations when handling large and rapidly evolving complex structured data such as sequential itemsets [9] or weighted itemsets [10]. These limitations underscore the ongoing need for innovative approaches to address the evolving complexities of stream data mining.\nTo overcome these challenges, we introduce an extension of the multi-step pattern sampling technique [11]\u2013[13] tailored for stream data. Despite its success, applying multi-step pattern sampling in complex and structured data streams remains unexplored. By leveraging a weighted reservoir, our approach enables the direct sampling and maintenance of patterns from streaming batch data, offering scalability and efficiency. We present a generic algorithm capable of handling temporal biases and various pattern types, such as sequential, weighted and unweighted itemsets, and discuss its effectiveness in ad- dressing the long-tail issue commonly encountered in pattern sampling tasks. We also show the usefulness of the sampled patterns by proposing online classifiers on stream sequential itemsets with many models that were not able to run with sequential data.\nThe primary contributions of this paper include:\n\u2022 We propose the first reservoir pattern sampling approach for complex structured data such as sequential and weighted itemsets in stream batches. Using the multi-step technique, we present a fast pattern sampling approach that leverages the inverse incomplete Beta function and efficient computation of the normalization constant.\n\u2022 Our algorithm named RPS is generic and works with temporal biases such as damped window and landmark windows while integrating numerous interestingness mea- sures like frequency, area, and decay combined with any norm-based utility to avoid the long-tail problem where long and rare patterns flood the space. We also present a large set of experimental results for analyzing the behavior of RPS with diverse types of parameters.\n\u2022 We show the usefulness of the sampled patterns for online classifier building for sequential data classification with new labels arrival. Specifically, we adapt several classi- fication models for online sequential data classification with unseen labels, which, to the best of our knowledge, is a novel contribution. Experimental results indicate that sampled patterns are highly effective in constructing accurate classifiers for online sequential data.\n\u2022 For reproducibility and open science purpose, the source code and the experiments are made available on a public repository\u00b9.\nThe structure of this paper is organized as follows: In Section II, we provide a review of related work concerning"}, {"title": "II. RELATED WORK", "content": "This section presents the reservoir sampling in stream data and the local multi-step pattern sampling literature.\n\nReservoir sampling is a fundamental technique in computer science and statistics used to address the problem of ran- domly selecting a fixed-size sample from a stream of data without knowing the total number of elements in advance [14]. The primary motivation behind reservoir sampling is to efficiently sample elements from large or infinite data streams [15] without lost of soundness, where traditional methods like sorting or storing all the data are impractical due to memory constraints [16]. One of the key benefits of reservoir sampling is its simplicity and constant space complexity [8], making it suitable for real-time data processing and applica- tions with limited resources. It is widely used for tasks like estimating statistical properties of large datasets, and sampling representative subsets of data for training machine learning models. Raissi and Poncelet [17] utilize reservoir sampling for input sampling (subset of instances from the database) before mining sequential patterns with bounded error rates for both static databases and data streams. Recently, it has been extended to output pattern sampling (subset of patterns from the pattern language) in stream itemsets [7] where each transaction or itemset \u03b3 is spread into a set of patterns, $2^{\\gamma} \\backslash \\emptyset$, without materialize it. After that, the set of patterns is scanned using an binary index operator to draw a pattern directly.\nHowever, with a large number of patterns per transaction, the computational complexity of maintaining the reservoir can become a bottleneck. The need to process and sample from an extensive list of patterns within each transaction can slow down the sampling process, making it less efficient. In addition, the reservoir sampling technique proposed in [7] is not scalable because the key idea which based on the binary index operator is not applicable with complex structure such as sequence [2] and quantitative data (weighted itemsets) [5].\nMulti-step pattern sampling [11], [18] is the fastest among the techniques used in output space pattern sampling [11], [19], [20] to draw representative patterns directly from the database. Particularly efficient for sampling in local data, multi-step is widely regarded as the most efficient approach, especially following the preprocessing phase, which involves computing the normalization constant. This method has been successfully applied across different pattern languages, includ- ing itemset [11], numerical data [18], sequential data [12], and quantitative data [13]. The primary concept behind this technique is to draw a pattern directly from the database with a probability proportional to a given interestingness measure m. This involves two steps after the preprocessing phase, wherein each instance \u03b3 of the database is weighted by the sum of the total utility of the set of patterns it contains. In the first step, an instance \u03b3 is randomly drawn with a probability proportional to its weight, while the second step allows for the drawing of a pattern proportionally to its utility $m(\\varphi, \\gamma)$ from the set of patterns of \u03b3. However, one of its most intricate limitations is the requirement to know the total sum of utility of the patterns, which can be time-consuming with very large databases or unfeasible with stream data [21].\nIn this paper, we demonstrate how to extend the multi- step pattern sampling technique to sample and maintain a set of patterns directly from stream data based on a weighted reservoir. We propose a generic algorithm capable of handling itemsets, sequential patterns, and high utility itemsets while incorporating norm-based utility to address the long-tail issue."}, {"title": "III. PROBLEM STATEMENT", "content": "This section formalizes the problem of reservoir-based multi-step pattern sampling under norm-based utility measure. We first recall some preliminary definitions about structured patterns and stream data.\nLet $I = \\{e_1, ..., e_n\\}$ be a set of finite literals called items. An itemset X is a non-empty subset of I, i.e., $X \\in 2^I\\backslash\\emptyset$. The set of all itemsets in I is called the pattern language for itemset mining, denoted by $L_I$. An instance $\u03b3 = (X_1, ..., X_n)$ defined over I is an ordered list of itemsets $X_i \\in L_I$ (1 \u2264 i \u2264 n, n \u2208 $N$). n is the size of the instance \u03b3 denoted by $|\u03b3|$. If $|\u03b3|$ > 1 then \u03b3 is a sequence of itemsets and $L_S$ denote the universal set of all sequences defined over I, otherwise \u03b3 is an itemset also called a transaction denote by $\u03b3 = X_1$ for simplicity. A transaction can be weighted and the patterns mined from it are called high utility itemset (HUI) in general. High utility itemset is dedicated to itemset discovery from a quantitative database where each item of an instance is associated with a weight, which is a strictly positive real number depending on the instance and referred to as its utility. The norm of an instance \u03b3, denoted by $||\u03b3||$, is the sum of the cardinality of all its itemsets, i.e., $||\u03b3|| = \\sum_{i=1}^n |X_i|$. Finally, given a pattern language $L \\in \\{L_I, L_S\\}$, pattern $\u03c6 \\in L$ can be generally defined as follows:\nDefinition 1 (Pattern). $\u03c6 = (X'_1, ..., X'_{n'})$ is a pattern or an generalization of an instance $\u03b3 = (X_1,..., X_n)$, denoted by $\u03c6 \\sqsubseteq \u03b3$, if there exists an index sequence $1 < i_1 < i_2 < ... < i_{n'} \u2264 n$ such that for all $j \\in [1..n']$, one has $X'_j \\subseteq X_{i_j}$.\nThis definition is usually used in the context of sequential pattern mining, but we recall that an itemset is nothing else that a sequential pattern of length 1."}, {"title": "Data stream and interestingness utility measures", "content": "In gen- eral, we denote $L \\in \\{L_I, L_S\\}$ as a pattern language. A data stream is a sequence of batches with timestamps denoted as follows: $D = ((t_1, B_1), . . ., (t_n, B_n))$, such that $B_i \\subseteq L$ for all $j \\in [1..n]$ and $t_j < t_{j+1}$ for all $j \\in [1..n \u2013 1]$, where a batch is a set of finite instances send at the same time, i.e.,\n$B_j = \\{\\gamma_{j1}, ..., \\gamma_{j|B_j|}: (\\gamma_{jk} \\in L)(\\forall k \\in [1..|B_j|])\\}$.\nIn other words, a batch contains a set of instances that have equal temporal relevance. $L(D)$ is the set of all patterns that can be mined from D. In this paper, we consider the Landmark window time constraint, which provides a structured way to di- vide the data stream into manageable chunks called instances, and damped window which favors the recent instances. We also use other constraints and utility measures that can combine frequency and norm-based utility measures.\nDefinition 2 (Frequency). Given a database D defined over a pattern language L, the frequency of a pattern $\u03c6 \\in L$ denoted $freq(\u03c6, D)$, is the number of instances that support. Formally, it is defined as follows:\n$freq(\u03c6, D) = |\\{\\gamma \\in B : ((t, B) \\in D) \\land (\u03c6 \\sqsubseteq \u03b3)\\}|$.\nIn pattern mining, frequency is often associated with other interestingness measures to reveal meaningful insights. In this paper, we combine it with other measures, specifically norm- based utility measures [22], to identify truly interesting and actionable patterns. It is also possible and helpful to use norm- based utility measures in high utility itemset discovery.\nDefinition 3 (Norm-based utility [22]). A utility function $F_m$ is a norm-based utility if there exists a function $f_m: N\\rightarrow R$ such that for every pattern $\u03c6 \\in L$, one has $F_m(\u03c6) = f_m(||\u03c6||)$.\nFor instance, the utility $F_{area}(\u03c6) = ||\u03c6||$ allows to consider the area measure $area(\u03b3,D) = freq(\u03c6,D) \u00d7 ||\u03c6||$, then one has $f_{area} (l) = l$. Obviously, the norm-based utility $F_{freq}(\u03c6) = 1$ enables to use the frequency as an interesting- ness measure. Besides, the utility $F_{<M}$ (resp. $F_{>\u00b5}$) defined as 1 if $||\u03c6|| \u2264 M$ (resp. $||\u03c6|| \u2265 \u03bc$) and 0 otherwise, simulates a maximum (resp. minimum) norm constraint. Indeed, with the induced interestingness measure $freq(\u03c6,D) \u00d7 F_{<M}(\u03c6)$ (resp. $freq(\u03c6,D) \u00d7 F_{>\u03bc}(\u03c6)$), a pattern with a norm strictly greater than M (resp. lower than \u00b5) is judged useless (whatever its frequency). \u2265 \u03bc and \u2264 M are said to be norm-based utility constraints (where 1 means true and 0 means false). The utility $decay(\u03c6) = a^{||\u03c6||}$, with $a \\in]0, 1]$, named exponential decay, is useful for penalizing long patterns but in a smooth way in comparison with $F_{<M}$. Finally, $F_{area^{-1}}(\u03c6) = \\frac{1}{||\u03c6||}$ allows us to consider the average utility measure.\nImportant remarks: With weighted items, the utility is not norm-based because the weights $w(e, \u03b3)$ of each item e depend on the transaction \u03b3 of the database in which it appears. In this case, each pattern $\u03c6 \\sqsubseteq \u03b3$ has a utility within the transaction \u03b3 defined by $U(\u03c6, \u03b3) = \\sum_{e \\in \u03c6}w(e, \u03b3)$. Therefore, with the language $L_I$, if the items are not weighted, we consider $U (\u03c6, \u03b3) = 1$ if $\u03c6 \\sqsubseteq \u03b3$. It is also essential to note that, with sequential data defined over $L_S$, we have $U (\u03c6, \u03b3) = 1$ if $\u03c6 \\sqsubseteq y$ since we do not deal with high utility sequential patterns mining. Obviously, for any pattern language $L \\in \\{L_I, L_S\\}$, we consider $U(\u03c6, \u03b3) = 0$ if $\u03c6 \\nsubseteq \u03b3$. Based on these remarks, we introduce the following definition:\nDefinition 4 (Norm-based utility measure). Let \u03b3 be an instance and a pattern defined over L. The norm-based utility measure, also said the interestingness utility measure of \u03c6 within \u03b3, denoted by $m(\u03c6, \u03b3)$, is defined as follows:\n$m(\u03c6, \u03b3) = U (\u03c6, \u03b3) \u00d7 F_m(\u03c6)$.\nIn general, we are interested by the utility of a pattern in the entire database that we call the pattern global utility.\nDefinition 5 (Global Pattern Utility). Let D be a database defined over a pattern language L and m an interestingness utility measure. The global utility of $\u03c6$ in D is given by:\n$G_m(\u03c6,D) = \\sum_{(t,B) \\in D} \\sum_{\u03b3 \\in B} m(\u03c6, \u03b3)$\nIn stream data under temporal biases, the utility of a pattern inserted at time $t_j$ can be different to its utility at time $t_n$, with n > j. It depends on what the user really needs to favor, recent or all patterns, by weighting each pattern with a temporal bias. Therefore, we introduce a generic damping function defined as follows:\nDefinition 6 (Damping function $\u2207_\u03b5(t_n,t_j)$). The temporal bias is used when a user need to favor the recent patterns or not. It is based on a damping factor $\u03b5 \\in [0,1]$. At time $t_n$, the temporal bias of each visited instance $\u03b3_i$ at time $t_j \u2264 t_n$ are formally updated as follows: $\u2207_\u03b5(t_n, t_j) = e^{-(t_n-t_j) \u00d7 \u03b5}$.\nWe can see that if $\u03b5 = 0$, then $\u2207_\u03b5(t_n,t_j) = 1$, which corresponds to the landmark window.\nTo take account these temporal biases in our approach, we define the pattern global utility under temporal bias as follow:\nDefinition 7 (Pattern Global Utility under temporal bias). Let $D = ((t_1, B_1), ..., (t_n, B_n))$ be a stream data defined over a pattern language L, m be an interestingness utility measure and $\u03b5 \\in [0, 1] a damping factor. At time $t_n$, the global utility of any pattern inserted into the reservoir at time t and that still appears into S is given by:\n$G^m_\u03b5(\u03c6,D) = \\sum_{(t_i, B_i) \\in D} \\sum_{\u03b3_{ij} \\in B_i} m(\u03c6, \u03b3_{ij}) \u00d7 \u2207_\u03b5(t_n, t_i)$\nExample 1. Table I and Table II present two toy datasets respectively for sequential and weighted itemsets. They also give some measures with temporal biases and utility measures. The damping factor is set to $\u03b5 \\in \\{0,0.1\\}$. For Table II, \\{A, B, C\\}, \\{2, 1.5, 2\\} means that the items A, B, and C have weights 2, 1.5 and 2 respectively in the instance $\u03b3_1$."}, {"title": "IV. RESERVOIR-BASED MULTI-STEP PATTERN SAMPLING", "content": "A multi-step pattern sampling approach is a sampling tech- nique with replacement. Therefore, in this paper, we focus on weighted reservoir sampling with replacement.\nWeighted reservoir sampling [8] has been proposed to main- tain into a reservoir with fixed size a sample of weighted data points where each data point is maintained with a probability proportional to its weight. For this purpose, Figure 1 depicts the overview of the approach. Intuitively, when a new batch $B_i$ arrives from the data stream at time $t_i$, the method calculates the probability p that a pattern in the reservoir S will be replaced by a new pattern drawn from $B_i$ (Step 1). Interestingly, rather than going through each of the k patterns in S one by one, the number of patterns to be replaced $n_r$ can be determined directly (Step 2). Then, $n_r$ patterns are drawn from the batch $B_i$ using a traditional pattern sampling method to randomly replace $n_r$ patterns from the reservoir (Step 3). We detail below the three steps that our approach should follows.\na) Step 1. Batch acceptance probability: Let us assume that all the positions of the reservoir are already occupied by a pattern from past batches $((t_1, B_1), ..., (t_n, B_{j\u22121})))$ and that all batch weights $w_m(B_i) = \\sum_{\u03b3_{\\epsilon \\lambda_i} \\in B_i} \\sum_{\\varphi \\sqsubseteq \u03b3_{\\epsilon \\lambda_i}} m(\\varphi, \u03b3_{\\epsilon \\lambda_i})$, with i \u2208 [1..j-1] are feasible. For any batch $B_j$ with a weight $W_m(B_j)$, we compute the probability acceptance that of one of its patterns from $L(B_j)$ be inserted into the reservoir $p_j$.\nBased on [8], we have $p_j = \\frac{W_m (B_j)}{\\sum_{i=1}^{j-1} W_m (B_i) \u00d7 e^{(t_j,t_i)}}$. Therefore, a pattern drawn proportionally to its weight in $B_j$ can be inserted at a position of the reservoir uniformly drawn. This uniform replacement has been already used by A-Chao [28] for weighted reservoir sampling in data stream.\nHowever, it is evident that computing the acceptance proba- bility by iteratively summing the weights of the visited batches is infeasible because we do not store the past batches. There- fore, we employ a memory-less computing technique to adapt the normalization constant that avoids storing information for each batch received.\nProperty 1. Let $Z_{i-1}$ be the normalization constant for the i-1 first batches of the data stream under the damped window and the norm-based utility measure m, with $Z_0 = 0$. The probability p to draw a pattern from the next batch $B_i$ under the damped window can be computed as follows:\n$p= \\frac{W_m(B_i) \u00d7 e^{xt_i}}{Z_i} , with Z_i = Z_{i\u22121} + W_m(B_i) \u00d7 e^{xt_i}$.\nProof. We omit the proof due to space constraints.\nb) Step 2. Number of patterns to draw from an accepted batch: Let S be a reservoir of size k where we need to store a sample from a population of finite size. In this case, each pattern can be selected up to $n_r \u2264 k$ times in the sample. To achieve this goal, we process k copies of L(Bj) such in each of them, a pattern is drawn proportionally to its weight in Bj. Interestingly, using k copies of L(B) with a probability pj corresponds to simulating k independents Bernoulli trials with a probability pj. By definition, the probability of obtaining $n_r$ success trials is nothing else that the Binomial Distribution which is formalized as follows:\n$P(X = n_r) = \\binom{k}{Nr} p_j^{nr} (1-p_j)^{k-n_r}$\nwith $n_r$ is the number of successful trials (or the number of patterns selected for inclusion in the reservoir), k the size of the reservoir (the total number of trials or positions in the reservoir), and $p_j = \\frac{W_m (B_j)}{\\sum_{i=1}^{j-1} W_m (B_i)}$ the probability of success in each trial, which is the probability of a pattern of L(B) being selected for inclusion in the reservoir. However, we note that computing a probability acceptance for each position is time consuming. Therefore, to skip computing an acceptance rate for each position, we use the Cumulative Binomial Probability Distribution defined as follows:\nDefinition 8 (Cumulative Binomial Probability Distribution (CBPD)). Suppose there is an event with a probability p of occurring per trial. The cumulative binomial probability $P_{nr}$, representing the probability of this event occurring $n_r$ or more times in k trials, is as follows:\n$P(n_r,k) = \\sum_{i=nr}^k \\binom{k}{i}p^{i}(1-p)^{k-i}$.\nThis formula calculates the cumulative probability of ob- taining $n_r$ or more successful trials out of k trials. It does so by summing the probabilities of all possible outcomes from $n_r$ to k, where each outcome represents a different number of successful trials. In this case, $p^{i}$ represents the probability of having i successes, and $(1 \u2013 p)^{k-i}$ represents the probability of having k - i failures.\nIn advance, the CBPD can be more efficiently computed by using the incomplete beta function (IBF), $I_x(a,b)$ [29]. The IBF represents the probability that a random variable following a beta distribution with parameters a and b falls below the value x. By leveraging the IBF, we can efficiently handle complex probability calculations without explicit summation of individual probabilities (as done in Eq. 1), minimizing computational load and numerical errors.\nDefinition 9 (Incomplete beta function (IBF) [30]). The incomplete beta function $I_x(a,b)$ is defined as follows:\n$I_z (a, b) = \\frac{1}{B(a,b)} \\int_0^x t^{a-1}(1-t)^{b-1} dt$\nwhere a and b are positive real numbers (parameters of the beta distribution); B(a,b) is the beta function, defined as:\n$B(a, b) = \\int_0^1 ta-1(1 \u2013 t)^{b\u22121} dt$\nand x is a real number in the range [0,1].\nProperty 2 (From CBPD to IBF). Let k be the total number of trials, $n_r \u2264 k$ the minimum number of times the event must occur, and p the probability of the event occurring in a single trial. The Cumulative Binomial Probability Distribution can be computed as follows:\n$P(n_r,k) = \\sum_{j=nr}^k \\binom{k}{j} p^j (1-p)^{k-j} = I_p(n_r, k-n_r + 1)$.\nProof. We omit the proof due to space constraints.\nDefinition 10 (Inverse Incomplete Beta Function). The Inverse Incomplete Beta Function allows for the approximation of the number of successful trials $n_r$ out of k trials that matches the CBPD with a given probability $x \\in [0,1]$ as follows:\n$n_r = arg_{n_r} [I_p(n_r, k \u2212 n_r + 1) = x]$.\nNow we are going to show how to draw a pattern propor- tionally to its interest from an accepted batch.\nc) Step 3. First pattern occurrence sampling from a batch: The main goal of processing a copy of an acceptance batch is finally to draw a pattern. However, the complexity of to draw a pattern from a batch depends to the pattern language. With sequential itemsets, a pattern can have multiple occurrences within a sequence [12] which is not the case with weighted/unweighted itemsets. Since we propose a generic approach dealing with sequential itemsets, then we adapt the first occurrence definition previously introduced in [12]."}, {"title": "Definition 11", "content": "(First occurrence). Given an instance \u03b3, let $o_1$ and $o_2$ be two occurrences of a pattern \u03c6 within \u03b3, whose sig- natures are ($i_1, i_2, ..., i_N$) and ($i'_1, i'_2, ..., i'_N$) respectively. $o_1$ is less than $o_2$, denoted by $o_1 < o_2$, if there exists an index k \u2208 [1..N] such that for all j \u2208 [1..k \u2013 1], one has $i_j = i'_j$, and $i_k < i'_k$. Finally, the first occurrence of \u03c6 in \u03b3 its smallest occurrence with respect to the order defined previously.\nExample 2. For instance, \u03b3 = \\{\\{A\\}\\{C\\}\\}) has two oc- currences $o_1$ and $o_2$ in $^2 = (\\{A, B, C\\}\\{C\\}\\{A, C\\}'\\})$ with signatures (1,2) and (1,3) respectively. But $o_1$ is the first occurrence because $o_1 < o_2$ since 2 < 3.\nBased on Definition 11, we then propose a generic sampler operator named $Sample_n(L, B, m)$. In fact, run $n_r$ times the operator $Sample_1(L, B, m)$ in the same batch B in order to get a first pattern occurrence for each realisation is equivalent to run $Sample_n(L, B, m)$ because $U_{n-1}\\{\\i \\thicksim m(L,B)\\} = Sample_n (L, B, m)$. Algorithm 1 implements the operator $Sample_n (L, \u03b3, m)$, which is used to draw $n_r$ patterns from the set of patterns in B, with each pattern drawn proportionally to the interestingness measure m. In line 1, each instance is weighted by the sum of its patterns utility. First, an instance \u03b3 is drawn proportionally to its weight in line 4. Then, in line 5, each norm l is weighted based on the sum of pattern utilities of norm l in \u03b3. To draw a pattern from \u03b3, an integer l' is first selected proportionally to its weight in \u03b3 (line 6). At line 7, a first occurrence of a pattern of norm l' is drawn proportionally to the set of patterns of norm l' in \u03b3, i.e., $P(\\varphi_j|\\gamma, l') = (\\frac{m(\\varphi,\u03b3)}{wm (l')} )$, and added to the sample (line 8). This process (lines 4-8) is repeated $n_r$ times. Finally, a sample of $n_r$ patterns is returned at line 9.\nAlgorithm 1 $Sample_n (L, B, m)$\nWe are now going to present our generic algorithm based on a weighted reservoir sampling with replacement.\nWe first give a high level description of rps described in Algorithm 2. It takes a data stream D, a utility measure m, the desired reservoir size k, and a damping factor $\u03b5 \u2208 [0, 1]$. First, for each batch $B_i$ appearing at timestamp $t_i$, the acceptance probability p that a pattern from $L(B_i)$ replaces a pattern inserted at $t_{is}$, with $t_i < t_i$ is computed (lines 3-5). If $B_i$ is accepted (line 6), which correspond to a success trial, then the number of additional success trials out of the rest of the reservoir size k - 1 that a pattern of $L(B_i)$ should be inserted (line 7) is deduced based on the inverse ibf (Definition 10). Lines 8 to 11 allow to draw $n_r$ patterns with replacement where each draw corresponds to an inserted pattern. At time $t_n$, rps maintains a reservoir of k patterns where each pattern is selected with a probability proportional $G_\u03b5^m (\u03b3, D)$.\nAlgorithm 2 RPS: A Generic Stream pattern sampler\nRegarding the $getPatternsToRemove$, it returns $n_r$ dis- tinct indexes uniformly drawn from [1..|S|]. Because, thanks to [31], all patterns in the reservoir have an equal probability of being replaced by one of the $n_r$ patterns drawn from the current batch by the sampler $Sample_n (L, B_i, m)$.\nWe study now the soundness and the complexity of rps.\na) Soundness analysis: The following properties state that rps returns an exact sample of patterns under temporal bias with norm-based utility measure.\nLet us first demonstrate that the batch acceptance probability computation of Algorithm 2 is exact.\nProperty 3 (Batch acceptance probability). Given a stream data D defined over a pattern language $L$, D = $((t_1, B_1),..., (t_n, B_n))$, m be a norm-based utility measure, $\u03b5 \u2208 [0,1] a damping factor, and k the size of the reservoir S. After observing $(t_n, B_n)$, the probability that a pattern of batch $B_i$, inserted at time $t_i$ at the jth position of the reservoir, $j \u2208 [1..k]$, stays in S, denoted P($B_i \u25b7 S[j]|t_n$), is given by:\n$P(B_i \u25b7 S[j]|t_n) = \\frac{W_m (B_i)\u00d7\u2207_\u03b5(t_n,t_i)}{\\sum_{i'<n} W_m (B_{i'})\u00d7\u2207_\u03b5(t_n,t_{i'})}$.\nProof. We know that P(B\u00bf\u25b7S[j]|tn) = \u03a0P(B\u00bf\u25b7S[j]|ti') and $\u2207_\u03b5(t_n,t_i) = e^{-(t_n-t_i)\u00d7\u03b5}$, with $\u03b5 \u2208 [0,1]$. Thanks to Property 1, we have P(B\u00bf \u25b7 S[j]|tn) = $\\frac{W_m(B_i) Xeti Xe}{Zi}$ X"}, {"title": "Based on the same property", "content": "we also have $Zi+1 - Wm (Bi+1) \u00d7 e^{ti+1\u00d7\u03b5"}, "Z_i$. Then, P($B_i \u25b7 S[j"], "B": "P($\u03c6|B$) = $\\frac{m(\\varphi", "to": "nP($\u03c6_j$ = S[j], D)"}