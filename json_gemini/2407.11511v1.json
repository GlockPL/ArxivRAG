{"title": "Reasoning with Large Language Models, a Survey", "authors": ["Aske Plaat", "Annie Wong", "Suzan Verberne", "Joost Broekens", "Niki van Stein", "Thomas B\u00e4ck"], "abstract": "Scaling up language models to billions of parameters has opened up possibil-\nities for in-context learning, allowing instruction tuning and few-shot learning on\ntasks that the model was not specifically trained for. This has achieved break-\nthrough performance on language tasks such as translation, summarization, and\nquestion-answering. Furthermore, in addition to these associative \"System 1\"\ntasks, recent advances in Chain-of-thought prompt learning have demonstrated\nstrong \"System 2\" reasoning abilities, answering a question in the field of artificial\ngeneral intelligence whether LLMs can reason.\nThe field started with the question whether LLMs can solve grade school math\nword problems. This paper reviews the rapidly expanding field of prompt-based\nreasoning with LLMs. Our taxonomy identifies different ways to generate, eval-\nuate, and control multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near fu-\nture. Finally, we highlight the relation between reasoning and prompt-based learn-\ning, and we discuss the relation between reasoning, sequential decision processes,\nand reinforcement learning. We find that self-improvement, self-reflection, and\nsome metacognitive abilities of the reasoning processes are possible through the\njudicious use of prompts. True self-improvement and self-reasoning, to go from\nreasoning with LLMs to reasoning by LLMs, remains future work.", "sections": [{"title": "1 Introduction", "content": "Transformer-based Large Language Models (LLMs) that are trained on large datasets\nhave achieved breakthrough performance at next token prediction [Vaswani et al., 2017,\nRadford et al., 2019, Wei et al., 2022a]; they are very good at natural language under-\nstanding (GLUE, SQUAD, Xsum) [Wang et al., 2018, 2019, Rajpurkar et al., 2016,\nNarayan et al., 2018], translation [Kocmi et al., 2022, Papineni et al., 2002, Sennrich\net al., 2015], question answering [Tan et al., 2023], and other System 1 tasks [Kahne-"}, {"title": "2 Background: Reasoning with LLMs", "content": "Before we dive into the works on reasoning, we review some background terminology\non LLMs. Our overview is brief. Excellent surveys on LLMs are, for example, Minaee\net al. [2024] and Zhao et al. [2023]. We discuss the generic training pipeline for LLMs,\nwe discuss how in-context learning works, and we discuss the reasoning pipeline. We\nstart with the generic language model training pipeline."}, {"title": "2.1 Training Pipeline Language Model", "content": "LLMs are typically constructed in a sequence of stages, from data preparation, through\ntraining, to inference. The training pipeline for most LLMs is quite elaborate. We will\nnow list a pipeline of the most common stages, based on the survey by Minaee et al.\n[2024].\n1. Acquire a large, general, unlabeled, high-quality text corpus. Some considera-\ntions on the selection of the texts are discussed in Brown et al. [2020].\n2. Pretrain the transformer model [Vaswani et al., 2017] on this large corpus. This\nstep yields a generalist model. The pretraining is done using a self-supervised\napproach on the unlabeled dataset (text corpus).\n3. Finetune the general model to a specific (narrow) task. This can be done us-\ning supervised-learning with a new labeled dataset consisting of prompts and\nanswers (supervised finetuning, SFT) [Wei et al., 2022a, Minaee et al., 2024],\nspecific for the task at hand. (A small number of papers in this survey work in\nthe finetuning stage.)\n4. Instruction tuning is a form of finetuning on a labeled dataset of instruction\nprompts and corresponding outputs, to improve instruction following, and thus\nthe usefulness of models.\n5. Align the finetuned model with user expectations (preference alignment). The\ngoal of this stage is to improve the model to give more ethically and socially\nacceptable answers. The machine learning method that is used in this stage can"}, {"title": "2.2 In-Context Learning", "content": "In LLMs beyond hundreds of billions of parameters a new kind of learning has emerged,\nthat is called in-context learning or prompt-learning [Brown et al., 2020]. It occurs at\ninference time, and is often able to give good results with few examples; it is a form of\nfew-shot learning. The large size of the model, containing rich and general knowledge\nis enabling this new type of few-shot learning (see Dong et al. [2022] for a survey).\nA prompt, consisting of a piece of demonstration context, is concatenated with a\nquery question, and is given to the language model for prediction [Liu et al., 2023]. For\nexample, when the task is emotion recognition in a social media post, \"I missed the bus\ntoday,\" can be followed by \"I felt so [___]\". Alternatively, for translation, we could fol-\nlow \"I missed the bus today,\" by \"French: [___]\" [Liu et al., 2023]. The prompt contains\nbackground information that is recognized by the model, selecting the desired model\ncontext. In-context learning works when language models contain enough knowledge,\nallowing them to generalize on the examples provided in the prompt.\nPrompts that contain a few examples are said to perform few-shot learning. Prompts\nthat contain only instructions without examples are said to perform zero-shot learning.\nIn-context learning takes place at inference time, after the computationally inten-\nsive stages where parameters have been pretrained and finetuned, when the model is\nqueried by the user to provide answers. No parameters are changed anymore with\nin-context learning. This is quite different from the common approach in supervised\ndep learning or self-supervised deep learning-where large datasets are used during\ntraining to update model parameters with backward propagation in lengthy and costly\ntraining epochs [Goodfellow et al., 2016]. Common approaches to few-shot learning,\nsuch as metalearning, do include training and finetuning of parameters to achieve gen-\neralization, and are computationally expensive (see, for example, Finn et al. [2017] or\nHuisman et al. [2021], Hospedales et al. [2021] for a survey)."}, {"title": "2.3 Reasoning Pipeline", "content": "Reasoning problems are also solved with a pipeline of stages. A typical approach\nto solving a complex problem is to subdivide it into smaller steps and solve those.\nThis approach is related to divide and conquer [Bellman, 1966]. New steps are (1)\ngenerated, (2) evaluated, and the number of steps that are generated and searched is\n(3) controlled in some way. The in-context reasoning approaches that we survey follow\na general three-stage pipeline [Madaan et al., 2023]:\n1. Generate: generation of steps by the model,\n2. Evaluate: evaluation of the predicted steps by an evaluator,\n3. Control: control of the number of steps that are generated and how deep ahead\nthe reasoning process will look.\nThis three-stage pipeline will be the basis of our taxonomy. But first, we will look at\nbenchmarks."}, {"title": "3 Benchmarks", "content": "Progress in artificial intelligence is measured by benchmarks. Benchmarks define the\ngoal that researchers aim to achieve in their experiments. In natural language pro-\ncessing, a wide array of benchmarks exists to measure progress, such as on ques-\ntion answering (for example, CommonsenseQA [Talmor et al., 2018]), word prediction\n(for example, LAMBADA [Paperno et al., 2016]), translation (for example, WMT'22\n[Kocmi et al., 2022]), language understanding (for example, GLUE [Wang et al., 2018,\n2019]), and text summarization (for example, Xsum [Narayan et al., 2018]). Trans-\nformer architectures were first popularized by encoder models such as BERT [Devlin\net al., 2018], for named entity recognition and classification tasks. Subsequently, de-\ncoder models such as GPT 2-4 [Radford et al., 2019, Brown et al., 2020, Achiam et al.,\n2023] showed impressive progress on natural language benchmarks.\nThe field of LLMs is quite active. Many different benchmarks exist, and listing a\ncomprehensive overview of all relevant benchmarks is beyond the scope of this sur-\nvey. We will mention relevant benchmarks for testing the reasoning abilities of LLMs.\nFollowing Wei et al. [2022b], these are all math word problem benchmarks. The bench-\nmark that is most frequently associated with reasoning by LLMs is a dataset of grade\nschool math word problems GSM8K [Cobbe et al., 2021]. GSM8K was created by\nhumans, with an aim of high quality, high diversity, moderate difficulty, and solutions\nin natural language. Other benchmarks are the SVAMP varying structures benchmarks\n[Patel et al., 2021], the ASDiv dataset of diverse math problems [Miao et al., 2021],"}, {"title": "4 Selection of Papers", "content": "The papers in this survey were selected as follows. Baseline LLMs have difficulty\nsolving math word problems, specifically on benchmarks listed in the previous section.\nWe take the ability to solve those benchmarks as a proxy for reasoning ability. We\ninitially performed a literature search for papers that use these benchmarks, and that\ncontain the search terms reasoning and large language model in their title or abstract.\nWe also searched for papers that referenced the Chain-of-thought paper. The resulting\npapers were curated based on recency, relevance, substance, and novelty.\nWe favor recent papers (two years prior to the writing of the survey), related to\nthe Chain-of-thought approach of generating intermediate reasoning steps, that solve\ntasks such as math word problems, and that work by prompt-based in-context learning.\nWe also include some papers that work by finetuning or supervised learning that relate\nto, or inspire, the Chain-of-thought approaches. Furthermore, we include approaches\noutside math word problems that showed interesting approaches to reasoning, such as\napplications in coding and autonomous agents, because of their approach to grounding."}, {"title": "5 Prompt Generation, Evaluation and Control", "content": "This survey examines how an architecture that is good at System 1 tasks can be prompted\nto solve System 2 tasks. The Chain-of-thought paper showed how a simple command\ncould prompt an LLM to perform reasoning steps, yielding much better performance\nin math word problems. Since then much research has further explored this approach,\ntrying to build the ultimate general problem solver for System 1 and System 2 prob-\nlems.\nFollowing the pipeline of Section 2.3, the prompts must (1) generate the reasoning\nsteps, (2) evaluate the answer to the steps, and (3) control the number of steps that are\ngenerated, the shape (or complexity) of the reasoning process must be controlled. We\nwill now briefly discuss the three stages. Please refer to Figure 1 for a diagram of the\ndifferent approaches for the generation, evaluation, and control of reasoning steps, and\nto Table 1.2\nPrompt for Step Generation The first order of business is to create a prompt that\ninstructs the LLM to generate reasoning steps. The problem must be split into substeps.\nThis can be achieved with a problem-specific prompt that contains elements of the\nproblem, such as: \"First calculate how many marbles Mary had originally, then how\nmany her friend had, and finally how many they had together.\"\nIn general, it is possible to prompt an LLM to fill in the blanks in a step-by-step\nfashion. In the papers that we will discuss, there are three main approaches for gener-\nating the step-by-step prompt. The prompt may be (1) handcrafted for the problem by\nthe researchers (hand-written prompt), or (2) the prompt or prompts may come from an\nsource that is external to the model, such as another model or a dataset (prompt using\nexternal knowledge), or (3) the model itself can be prompted to generate a (series of)\nprompt(s) to analyze the problem (model-generated prompt). As we will see, all three\napproaches have their advantages and disadvantages.\nGenerating the subproblem-steps is the first stage that is necessary for in-context\nlearning to perform reasoning. Each paper in our survey performs at least this stage of\nthe reasoning pipeline. In some of the early papers (around 2022) it is the only stage\nof the pipeline that is performed.\nPrompt for Result Evaluation After the prompt has been generated and the model\nhas answered it, the next step in the reasoning pipeline is to evaluate the answer. Again,\nwe see three main approaches for substep evaluation. First, the steps may be evaluated\nby (1) the model itself (self-assessment). Second, (2) an external program can be used\nto evaluate the steps. For example, when the steps are expressed as computer code,\nan external interpreter or compiler can be used to check the validity and the outcome\n(tool-based evaluation). Finally, (3) an external model can be used, LLM or otherwise.\nFor example, in robotics, an external physics model can determine if certain actions are\nphysically possible (external model validation)."}, {"title": "Perform Control of Reasoning Steps", "content": "A reasoning process that consists of multiple\nsteps is a sequential decision process [Littman, 1996]. When a single chain of reason-\ning steps is generated, the control flow of the reasoning process is simple: greedily\nevaluate the first step and then the next one, if present. The control flow of the reason-\ning process may also be more intricate. Some reasoning problems can be divided into\nmultiple subproblems. To execute, evaluate and combine the results of all substeps,\na separate controller may be needed. This controller can be a prompt or an external\nalgorithm.\nAgain, we distinguish three approaches. Most papers use (1) a greedy selection\napproach: a single prompt with a single chain of steps is generated, and these steps are\ndirectly executed and followed. The second approach (2) is to generate an ensemble\nstrategy of reasoning steps, evaluate them, combine the individual results, and present\nthem as the result of the ensemble. Finally, (3) a full tree-search or a reinforcement\nlearning (RL) algorithm can be used as scaffolding. In this case, when a step is fol-\nlowed and evaluated, the LLM can roll back and try a different reasoning step. This\nis a breadth-first search approach [Plaat, 2020]. Going further, a full reinforcement\nlearning approach can be used [Sutton and Barto, 2018, Plaat, 2022] to find an optimal\npolicy for the sequential decision process. A full Markov Decision Process of state, ac-\ntion, transition, and reward function is specified, and step control can become a process\nwhere prompts are generated dynamically.\nDomain Many papers are applied to math word problems (natural language descrip-\ntions of math problems). Math problems were the original inspiration for the experi-\nments with reasoning in LLMs. Other application domains include autonomous agents,\nrobotic movement, generating computer programs, and playing computer games. We\nwill discuss these in more detail with the individual approaches.\nTaxonomy Table Table 1 lists the papers of this survey. They are listed by the domain\nthey work on, the type of prompt generation, the evaluation of the result, and the control\nmethod. The approaches in the table are grouped, divided by horizontal lines.\nThe first group, from Scratchpad to Self-ask, focuses on creating a prompt that gen-\nerates the reasoning steps. The entries in the cells of this column are shown in bold,\nhighlighting the focus of the approaches. The approaches in this group can be consid-\nered to be the start of the field of LLM-reasoning. The Chain-of-thought approach is\nespecially an inspiration for many works. The prompts are often written \"manually\" by\nthe researchers, the steps are encoded in one prompt, and step control is greedy. There\nis no specific evaluation of the steps, other than comparing results to the benchmark.\nThe Scratchpad approach is special in that it uses supervised learning, not prompt-\nlearning; the work showed that LLMs can be made to generate internal reasoning steps\nby supervised learning, paving the way for the later prompt-based papers.\nThe second group, from Self-verification to Self-taught-reasoner, focuses on eval-\nuation of the reasoning steps in the prompt. This column is shown in bold in the table.\nThe approaches in this group aim to improve the Chain-of-thought results by reduc-\ning the error accumulation that occurs when multiple steps are taken in a reasoning\nchain. A variety of step control methods is used by these approaches, which is dis-"}, {"title": "5.1 Generation of Steps", "content": "Originally, LLMs performed poorly on math word problems (GSM8K [Cobbe et al.,\n2021]). Some different approaches were tried, for example scaling up the size of the\nLLM [Rae et al., 2021]. The LLM architecture, based on transformers, is designed to\nproduce a single token. When we prompt such an architecture to produce an answer,\nit does so. What we should do is prompt it to follow intermediate steps, answer those,\nand thus work towards the final answer, just as a student is taught to break down a\ncomplex problem into smaller steps. We should take the model by its hand and teach it\nto write down the intermediate steps, and combine the intermediate results [Nye et al.,\n2021].\nThis idea was used by Nye et al. [2021] in Scratchpads, a transformer model that\nperforms multi-step computations by asking it to emit intermediate computation steps\ninto a scratchpad. They train the model by supervised learning (not prompt-based\nin-context learning). Figure 2 shows an example. On experiments with addition, poly-\nnomial evaluation, and Python code execution, versions that produced the intermediate\nsteps on a scratchpad performed considerably better than versions that did not."}, {"title": "5.1.1 Hand-written Prompt", "content": "This question was studied by Wei et al. [2022b], amongst others. A basic way to in-\nstruct an LLM to generate steps by prompt-learning is to manually write a prompt for\nthe large language model to follow the reasoning steps. They showed in their Chain-of-\nthought paper that with such a prompt the LLM follows such intermediate steps. When\nthe LLM is prompted to rephrase information from the question as intermediate rea-\nsoning steps in its answer, the LLM performed much better than when it was prompted\nto answer a math problem directly, without reproducing the information from the ques-\ntion in its answer. The example from the Chain-of-thought paper is shown in Figure 3\nWei et al. [2022b]. Performance figures were given in Section 3 on benchmarks.\nThe substantial performance improvement by Chain-of-thought has caused much\nexcitement and has opened up further research on reasoning with LLMs. In the orig-\ninal Chain-of-thought paper the prompts were handwritten by the researchers for the\nindividual types of problems, and evaluations are conducted with five different bench-\nmarks (not by an LLM).3 In a later work the prompts were generated automatically by\nthe LLM [Zhang et al., 2022].\nKojima et al. [2022] go a step further. They show that the simple addition of a\nsingle text to the prompt (Let's think step by step) significantly improves performance.\nSince this text does not contain problem-related elements, this can be considered as a\nform of zero-shot learning. Figure 4 compares the approaches. Experiments further\nshow that with this addition to the prompt, significant performance gains are achieved"}, {"title": "5.1.2 Prompt using External Knowledge", "content": "Chain-of-thought shows that an LLM gives better answers to complex problems when\nit is guided to take individual steps. Prompts are written manually, from scratch, by the\nresearchers.\nWe can use external information about the problem to improve the prompt. Press\net al. [2022] study how subproblems are related to the main problem, which they call\ncompositional reasoning. They study how often a model is able to answer the sub-\nproblems, but not the overall problem. This difference is called the compositionality\ngap. They find that in GPT-3, as model size increases, the compositionality gap does\nnot decrease: the single-hop question-answering performance improves faster than the\nmulti-hop performance. This shows that while more powerful models memorize and\nrecall more factual knowledge, no improvement in their ability to perform composi-\ntional reasoning occurs. They find that the ability to reason does not depend on the size\nof the model.\nSubsequently, a method called Self-ask is proposed, that asks elicitive follow-up"}, {"title": "5.1.3 Model-Generated Prompt", "content": "In addition to manually writing prompts or using external information, we can also try\nto let the LLM itself study the problem to write the best reasoning-prompt, a form of\nself-improvement. An example of this approach is Auto-chain-of-thought [Zhang et al.,\n2022]. This approach builds on the observation by Kojima et al. [2022] that large lan-\nguage models are zero-shot reasoners. First, Auto-chain generates specific questions\nfor a given dataset and partitions them into clusters. Then an external algorithm uses\nthe model to generate examples that are sampled for diversity. The constructed demon-\nstrations augment the in-context prompt. The automatically generated prompts are\nreported to perform as well or better than the hand-written Chain-of-thought prompts\non ten benchmarks using GPT-3.\nFu et al. [2022] introduce Complexity-based prompting. Inspired by Chain-of-\nthought and Self-consistency, this work studies which prompts achieve the best results\non math word and other reasoning problems. Their work specifically studies the impact\nof the complexity of the reasoning chain, and introduces a related reasoning approach\n(Complexity-based prompting). They find that prompts with the largest complexity\n(the most reasoning steps) perform best. Further, they find that outputs (answers) with\nthe highest complexity are the best. Complexity-based prompting achieves high per-\nformance on three math reasoning benchmarks.\nAnother approach that uses model-generated prompts is Buffer-of-thoughts. We\nwill discuss this approach in Section 5.3.3."}, {"title": "5.2 Evaluation of Steps", "content": "After discussing prompts for the generation of reasoning steps, the next stage in the\nreasoning pipeline (Section 2.3) is evaluation of the results of the steps, to reduce the\nerror of multi-step reasoning chains.\nWe will start with approaches where the same model performs step-generation and\nstep-evaluation."}, {"title": "5.2.1 Self-Assessment", "content": "When LLMs are prompted to perform reasoning steps, they perform a sequence of steps\nand predict multiple tokens. Performing a sequence of steps makes them sensitive to\nmistakes and vulnerable to error accumulation [Weng et al., 2022, Xiao et al., 2023a].\nSeveral methods have been developed to prevent error accumulation. One approach is"}, {"title": "5.2.2 Tool-based Validation", "content": "Another possibility to improve the accuracy of evaluating the reasoning steps is to\nswitch from a natural to a formal language. The advantage of a formal language is\nthat it is less ambiguous than a natural language. Examples are computer languages,\nsuch as Python, or mathematical equations. Using a formal language for reasoning is a\npopular approach, and we discuss seven papers. Many approaches generate the steps in\nPython, and the code can then be evaluated by a formal evaluator, such as a compiler,\ndebugger, or interpreter.\nLLMs have been quite successful in generating computer code from natural lan-\nguage prompts. Chen et al. [2021] introduced Codex, a GPT model that was trained\non publicly available code in the repository GitHub. A production version of this work\nwas introduced under the name GitHub Copilot. Codex is able to generate correct pro-\ngrams from descriptions in natural language, such as commentary strings. Figure 7\nshows examples that are produced by Codex.\nThe work on Codex is used as a basis for further research on reasoning in LLMs.\nHuman programmers, when writing code, typically follow a cycle of writing some\ncode, executing it to look for errors, and then using the feedback to improve the code.\nThis same approach is followed in the Self-debugging work [Chen et al., 2023]. Self-\ndebugging teaches a large language model to debug its generated program code via\nfew-shot demonstrations. It follows the same steps of (1) code generation, (2) code\nexecution, and (3) code explanation (see Figure 8).\nSelf-debugging is able, without human feedback on the code's correctness or error\nmessages, to identify mistakes in the code that was generated by itself from investi-\ngating the execution results. Self-debugging can also provide an explanation of the\ngenerated code in natural language. It achieves strong performance on text-to-SQL\ngeneration, C++-to-Python transcoding, and text-to-Python generation.\nSeveral works use self-debugging to generate working code tuned for solving spe-\ncific problems automatically, without human feedback. Romera-Paredes et al. [2024]\nintroduced FunSearch, an approach that integrates formal methods and LLMs to en-\nhance mathematical reasoning and code generation. FunSearch is capable of producing\nfunctionally correct programs that adhere to specified requirements. It uses a genetic\nalgorithm approach with multiple populations of candidate solutions (programs), which\nare automatically evaluated (using tools depending on the problem specification). In\naddition to the problem specification in the form of an evaluate function, also an initial\nprogram is given to the LLM in the first prompt. After evaluating a number of gen-\nerated programs from the starting prompt, a new prompt using \u2018best-shot prompting'\nis created in an iterative fashion, combining a selection of k sampled programs in a\nsorted list (ascending according to their evaluation score), and the LLM is requested to\ngenerate program k + 1. Another work leverages evolutionary computation methods\nto generate and optimize evolutionary algorithms [van Stein and B\u00e4ck, 2024]. This ap-\nproach, LLaMEA (Large Language Model Evolutionary Algorithm), utilizes LLMs to\ndesign and optimize evolutionary algorithms. The approach uses LLMs to generate ini-"}, {"title": "5.2.3 External Model Validation", "content": "We have seen many successful examples of prompt-based in-context reasoning and\nevaluation. We will now look at related reasoning approaches that follow a more tra-\nditional parameter learning approach. We describe three natural language approaches\nthat follow this route. All approaches evaluate the output of the model and generate\ncorrective data. That data is then added to the training pipeline, and the model is sub-\nsequently finetuned.\nFinetuning The Refiner approach [Paul et al., 2023] uses a generator model and a\ncritic model to provide fine-grained feedback on reasoning errors. The generator gen-\nerates multiple reasoning hypotheses, the critic evaluates results by randomly selecting\na hypothesis for feedback. The generator model is finetuned based on its reasoning er-\nrors. A small supervised model is used to overcome the cold-start problem. Figure 10\nshows an example of how the critic provides feedback to the generator. The approach\nis reported to work well on math word problems and synthetic natural language rea-\nsoning."}, {"title": "Dataset Augmentation", "content": "The final finetuning approach that we discuss uses dataset\naugmentation. An explicit intermediate reasoning is called a rationale. Rationale gen-\neration has been shown to be valuable for LLMs across diverse tasks such as mathemat-\nical and commonsense reasoning, code evaluation, social bias inference, and natural\nlanguage inference [Zelikman et al., 2022]. Zelikman et al. [2022] describe how rea-\nsoning steps are used to create rationales, that are then used to augment the dataset on\nwhich the model is finetuned. The approach is called Self-taught-reasoner. Figure 11\nillustrates the approach. In Self-taught-reasoner, an augmentation dataset is created by\nattempting to solve the original dataset using the current model's rationale generation\nability in each iteration. Next, the dataset is augmented using rationalizations, using\nground-truth answers to problems the model failed to solve. Finally, the large language\nmodel is finetuned on the combined dataset."}, {"title": "Reasoning about Robot Behavior", "content": "In addition to math word problems, prompt-based\nreasoning has also been used to reason about robot behavior. Language models contain\na large amount of information about the real world [Ahn et al., 2022]. In theory, this\nshould allow the model to exhibit realistic reasoning about robotic behavior. However,\nthe models do not have knowledge about particular embodied aspects of a particular\nrobot. If we could compare a Scratchpad-like list of intermediate reasoning steps with"}, {"title": "5.3 Control of Steps", "content": "The third stage in the reasoning pipeline in Section 2.3 is reasoning control. This stage\ncontrols how many sub-steps are generated, and how deep into the future the reasoning\nchain is generated.\nThere are three main approaches: (1) greedy selection, which generates a step and\nthen follows it, (2) ensemble strategy, which generates a set of possible next steps, and\n(3) a full tree-shaped search which generates multiple options for the step, and follows\nthem multiple steps into the future, traversing a search tree with backtracking, control-\nling an exponential search space. We include reinforcement learning approaches, that\ninteractively learn an optimal policy for such a reasoning space."}, {"title": "5.3.1 Greedy Selection", "content": "Most earlier works on prompt-based reasoning follow the greedy approach: generate a\nsingle prompt with a sequence of steps and follow them. Among the greedy reasoners\nare Chain-of-thought, Auto-CoT, and Zero-shot CoT. Inner Monologue and Say-Can\nalso use greedy reasoning.\nIn Least-to-most prompting [Zhou et al., 2022], the key idea is to break down a\ncomplex problem into simpler subproblems and then solve these in sequence, explicitly\nencoding them in the prompt. It is related to Complexity-based prompting. In Least-to-\nmost, finding the answer to each subproblem is facilitated by the answers to previously\nsolved subproblems, as in a curriculum [Bengio et al., 2009]. The authors find that on"}, {"title": "5.3.2 Ensemble Strategy", "content": "The second kind of reasoning control is based on an ensemble of (sequences of) rea-\nsoning steps. The ensemble approach is a well-known technique in machine learn-\ning to make a strong learner out of multiple weaker learners [Sagi and Rokach, 2018,\nBreiman, 2001]. For most problems, multiple different options for the next step ex-\nist. When all or some of these are generated and evaluated, then the best result or\nthe consensus result can be reported as the outcome of an ensemble of steps. Various\napproaches have been proposed.\nWe already mentioned Self-consistency [Wang et al., 2022b] and Self-verification\n[Weng et al., 2022] in Section 5.2.1. They are popular ensemble approaches to evaluate\nthe results of reasoning steps in prompt learning. The greedy single-path decoding used\nin Chain-of-thought prompting is replaced by sampling a diverse set of reasoning paths,\nevaluating them, and selecting the most consistent answer.\nIn another domain Chain-of-experts builds on Chain-of-thought with a mixture of\nexperts ensemble for complex combinatorial operations research problems [Xiao et al.,\n2023b]. PAL and MathPrompter also use the ensemble approach. They generate mul-\ntiple steps, which are evaluated and whose answer is combined, or the best step is\nchosen.\nThe ensemble approach is a popular approach in LLM-reasoning."}, {"title": "5.3.3 Reinforcement Learning", "content": "In the greedy approach"}, {"title": "Reasoning with Large Language Models, a Survey", "authors": ["Aske Plaat", "Annie Wong", "Suzan Verberne", "Joost Broekens", "Niki van Stein", "Thomas B\u00e4ck"], "abstract": "Scaling up language models to billions of parameters has opened up possibil-\nities for in-context learning, allowing instruction tuning and few-shot learning on\ntasks that the model was not specifically trained for. This has achieved break-\nthrough performance on language tasks such as translation, summarization, and\nquestion-answering. Furthermore, in addition to these associative \"System 1\"\ntasks, recent advances in Chain-of-thought prompt learning have demonstrated\nstrong \"System 2\" reasoning abilities, answering a question in the field of artificial\ngeneral intelligence whether LLMs can reason.\nThe field started with the question whether LLMs can solve grade school math\nword problems. This paper reviews the rapidly expanding field of prompt-based\nreasoning with LLMs. Our taxonomy identifies different ways to generate, eval-\nuate, and control multi-step reasoning. We provide an in-depth coverage of core\napproaches and open problems, and we propose a research agenda for the near fu-\nture. Finally, we highlight the relation between reasoning and prompt-based learn-\ning, and we discuss the relation between reasoning, sequential decision processes,\nand reinforcement learning. We find that self-improvement, self-reflection, and\nsome metacognitive abilities of the reasoning processes are possible through the\njudicious use of prompts. True self-improvement and self-reasoning, to go from\nreasoning with LLMs to reasoning by LLMs, remains future work.", "sections": [{"title": "1 Introduction", "content": "Transformer-based Large Language Models (LLMs) that are trained on large datasets\nhave achieved breakthrough performance at next token prediction [Vaswani et al., 2017,\nRadford et al., 2019, Wei et al., 2022a]; they are very good at natural language under-\nstanding (GLUE, SQUAD, Xsum) [Wang et al., 2018, 2019, Rajpurkar et al., 2016,\nNarayan et al., 2018], translation [Kocmi et al., 2022, Papineni et al., 2002, Sennrich\net al., 2015], question answering [Tan et al., 2023], and other System 1 tasks [Kahne-"}, {"title": "2 Background: Reasoning with LLMs", "content": "Before we dive into the works on reasoning, we review some background terminology\non LLMs. Our overview is brief. Excellent surveys on LLMs are, for example, Minaee\net al. [2024] and Zhao et al. [2023]. We discuss the generic training pipeline for LLMs,\nwe discuss how in-context learning works, and we discuss the reasoning pipeline. We\nstart with the generic language model training pipeline."}, {"title": "2.1 Training Pipeline Language Model", "content": "LLMs are typically constructed in a sequence of stages, from data preparation, through\ntraining, to inference. The training pipeline for most LLMs is quite elaborate. We will\nnow list a pipeline of the most common stages, based on the survey by Minaee et al.\n[2024].\n1. Acquire a large, general, unlabeled, high-quality text corpus. Some considera-\ntions on the selection of the texts are discussed in Brown et al. [2020].\n2. Pretrain the transformer model [Vaswani et al., 2017] on this large corpus. This\nstep yields a generalist model. The pretraining is done using a self-supervised\napproach on the unlabeled dataset (text corpus).\n3. Finetune the general model to a specific (narrow) task. This can be done us-\ning supervised-learning with a new labeled dataset consisting of prompts and\nanswers (supervised finetuning, SFT) [Wei et al., 2022a, Minaee et al., 2024],\nspecific for the task at hand. (A small number of papers in this survey work in\nthe finetuning stage.)\n4. Instruction tuning is a form of finetuning on a labeled dataset of instruction\nprompts and corresponding outputs, to improve instruction following, and thus\nthe usefulness of models.\n5. Align the finetuned model with user expectations (preference alignment). The\ngoal of this stage is to improve the model to give more ethically and socially\nacceptable answers. The machine learning method that is used in this stage can"}, {"title": "2.2 In-Context Learning", "content": "In LLMs beyond hundreds of billions of parameters a new kind of learning has emerged,\nthat is called in-context learning or prompt-learning [Brown et al., 2020]. It occurs at\ninference time, and is often able to give good results with few examples; it is a form of\nfew-shot learning. The large size of the model, containing rich and general knowledge\nis enabling this new type of few-shot learning (see Dong et al. [2022] for a survey).\nA prompt, consisting of a piece of demonstration context, is concatenated with a\nquery question, and is given to the language model for prediction [Liu et al., 2023]. For\nexample, when the task is emotion recognition in a social media post, \"I missed the bus\ntoday,\" can be followed by \"I felt so [___]\". Alternatively, for translation, we could fol-\nlow \"I missed the bus today,\" by \"French: [___]\" [Liu et al., 2023]. The prompt contains\nbackground information that is recognized by the model, selecting the desired model\ncontext. In-context learning works when language models contain enough knowledge,\nallowing them to generalize on the examples provided in the prompt.\nPrompts that contain a few examples are said to perform few-shot learning. Prompts\nthat contain only instructions without examples are said to perform zero-shot learning.\nIn-context learning takes place at inference time, after the computationally inten-\nsive stages where parameters have been pretrained and finetuned, when the model is\nqueried by the user to provide answers. No parameters are changed anymore with\nin-context learning. This is quite different from the common approach in supervised\ndep learning or self-supervised deep learning-where large datasets are used during\ntraining to update model parameters with backward propagation in lengthy and costly\ntraining epochs [Goodfellow et al., 2016]. Common approaches to few-shot learning,\nsuch as metalearning, do include training and finetuning of parameters to achieve gen-\neralization, and are computationally expensive (see, for example, Finn et al. [2017] or\nHuisman et al. [2021], Hospedales et al. [2021] for a survey)."}, {"title": "2.3 Reasoning Pipeline", "content": "Reasoning problems are also solved with a pipeline of stages. A typical approach\nto solving a complex problem is to subdivide it into smaller steps and solve those.\nThis approach is related to divide and conquer [Bellman, 1966]. New steps are (1)\ngenerated, (2) evaluated, and the number of steps that are generated and searched is\n(3) controlled in some way. The in-context reasoning approaches that we survey follow\na general three-stage pipeline [Madaan et al., 2023]:\n1. Generate: generation of steps by the model,\n2. Evaluate: evaluation of the predicted steps by an evaluator,\n3. Control: control of the number of steps that are generated and how deep ahead\nthe reasoning process will look.\nThis three-stage pipeline will be the basis of our taxonomy. But first, we will look at\nbenchmarks."}, {"title": "3 Benchmarks", "content": "Progress in artificial intelligence is measured by benchmarks. Benchmarks define the\ngoal that researchers aim to achieve in their experiments. In natural language pro-\ncessing, a wide array of benchmarks exists to measure progress, such as on ques-\ntion answering (for example, CommonsenseQA [Talmor et al., 2018]), word prediction\n(for example, LAMBADA [Paperno et al., 2016]), translation (for example, WMT'22\n[Kocmi et al., 2022]), language understanding (for example, GLUE [Wang et al., 2018,\n2019]), and text summarization (for example, Xsum [Narayan et al., 2018]). Trans-\nformer architectures were first popularized by encoder models such as BERT [Devlin\net al., 2018], for named entity recognition and classification tasks. Subsequently, de-\ncoder models such as GPT 2-4 [Radford et al., 2019, Brown et al., 2020, Achiam et al.,\n2023] showed impressive progress on natural language benchmarks.\nThe field of LLMs is quite active. Many different benchmarks exist, and listing a\ncomprehensive overview of all relevant benchmarks is beyond the scope of this sur-\nvey. We will mention relevant benchmarks for testing the reasoning abilities of LLMs.\nFollowing Wei et al. [2022b], these are all math word problem benchmarks. The bench-\nmark that is most frequently associated with reasoning by LLMs is a dataset of grade\nschool math word problems GSM8K [Cobbe et al., 2021]. GSM8K was created by\nhumans, with an aim of high quality, high diversity, moderate difficulty, and solutions\nin natural language. Other benchmarks are the SVAMP varying structures benchmarks\n[Patel et al., 2021], the ASDiv dataset of diverse math problems [Miao et al., 2021],"}, {"title": "4 Selection of Papers", "content": "The papers in this survey were selected as follows. Baseline LLMs have difficulty\nsolving math word problems, specifically on benchmarks listed in the previous section.\nWe take the ability to solve those benchmarks as a proxy for reasoning ability. We\ninitially performed a literature search for papers that use these benchmarks, and that\ncontain the search terms reasoning and large language model in their title or abstract.\nWe also searched for papers that referenced the Chain-of-thought paper. The resulting\npapers were curated based on recency, relevance, substance, and novelty.\nWe favor recent papers (two years prior to the writing of the survey), related to\nthe Chain-of-thought approach of generating intermediate reasoning steps, that solve\ntasks such as math word problems, and that work by prompt-based in-context learning.\nWe also include some papers that work by finetuning or supervised learning that relate\nto, or inspire, the Chain-of-thought approaches. Furthermore, we include approaches\noutside math word problems that showed interesting approaches to reasoning, such as\napplications in coding and autonomous agents, because of their approach to grounding."}, {"title": "5 Prompt Generation, Evaluation and Control", "content": "This survey examines how an architecture that is good at System 1 tasks can be prompted\nto solve System 2 tasks. The Chain-of-thought paper showed how a simple command\ncould prompt an LLM to perform reasoning steps, yielding much better performance\nin math word problems. Since then much research has further explored this approach,\ntrying to build the ultimate general problem solver for System 1 and System 2 prob-\nlems.\nFollowing the pipeline of Section 2.3, the prompts must (1) generate the reasoning\nsteps, (2) evaluate the answer to the steps, and (3) control the number of steps that are\ngenerated, the shape (or complexity) of the reasoning process must be controlled. We\nwill now briefly discuss the three stages. Please refer to Figure 1 for a diagram of the\ndifferent approaches for the generation, evaluation, and control of reasoning steps, and\nto Table 1.2\nPrompt for Step Generation The first order of business is to create a prompt that\ninstructs the LLM to generate reasoning steps. The problem must be split into substeps.\nThis can be achieved with a problem-specific prompt that contains elements of the\nproblem, such as: \"First calculate how many marbles Mary had originally, then how\nmany her friend had, and finally how many they had together.\"\nIn general, it is possible to prompt an LLM to fill in the blanks in a step-by-step\nfashion. In the papers that we will discuss, there are three main approaches for gener-\nating the step-by-step prompt. The prompt may be (1) handcrafted for the problem by\nthe researchers (hand-written prompt), or (2) the prompt or prompts may come from an\nsource that is external to the model, such as another model or a dataset (prompt using\nexternal knowledge), or (3) the model itself can be prompted to generate a (series of)\nprompt(s) to analyze the problem (model-generated prompt). As we will see, all three\napproaches have their advantages and disadvantages.\nGenerating the subproblem-steps is the first stage that is necessary for in-context\nlearning to perform reasoning. Each paper in our survey performs at least this stage of\nthe reasoning pipeline. In some of the early papers (around 2022) it is the only stage\nof the pipeline that is performed.\nPrompt for Result Evaluation After the prompt has been generated and the model\nhas answered it, the next step in the reasoning pipeline is to evaluate the answer. Again,\nwe see three main approaches for substep evaluation. First, the steps may be evaluated\nby (1) the model itself (self-assessment). Second, (2) an external program can be used\nto evaluate the steps. For example, when the steps are expressed as computer code,\nan external interpreter or compiler can be used to check the validity and the outcome\n(tool-based evaluation). Finally, (3) an external model can be used, LLM or otherwise.\nFor example, in robotics, an external physics model can determine if certain actions are\nphysically possible (external model validation)."}, {"title": "5.  Omit ", "content": ""}]}]}