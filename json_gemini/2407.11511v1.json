{"title": "Reasoning with Large Language Models, a Survey", "authors": ["Aske Plaat", "Annie Wong", "Suzan Verberne", "Joost Broekens", "Niki van Stein", "Thomas B\u00e4ck"], "abstract": "Scaling up language models to billions of parameters has opened up possibil-ities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved break-through performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative \"System 1\" tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong \"System 2\" reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason.\nThe field started with the question whether LLMs can solve grade school math word problems. This paper reviews the rapidly expanding field of prompt-based reasoning with LLMs. Our taxonomy identifies different ways to generate, eval-uate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near fu-ture. Finally, we highlight the relation between reasoning and prompt-based learn-ing, and we discuss the relation between reasoning, sequential decision processes, and reinforcement learning. We find that self-improvement, self-reflection, and some metacognitive abilities of the reasoning processes are possible through the judicious use of prompts. True self-improvement and self-reasoning, to go from reasoning with LLMs to reasoning by LLMs, remains future work.", "sections": [{"title": "Introduction", "content": "Transformer-based Large Language Models (LLMs) that are trained on large datasets have achieved breakthrough performance at next token prediction [Vaswani et al., 2017, Radford et al., 2019, Wei et al., 2022a]; they are very good at natural language under-standing (GLUE, SQUAD, Xsum) [Wang et al., 2018, 2019, Rajpurkar et al., 2016, Narayan et al., 2018], translation [Kocmi et al., 2022, Papineni et al., 2002, Sennrich et al., 2015], question answering [Tan et al., 2023], and other System 1 tasks [Kahne-"}, {"title": "Background: Reasoning with LLMs", "content": "Before we dive into the works on reasoning, we review some background terminology on LLMs. Our overview is brief. Excellent surveys on LLMs are, for example, Minaee et al. [2024] and Zhao et al. [2023]. We discuss the generic training pipeline for LLMs, we discuss how in-context learning works, and we discuss the reasoning pipeline. We start with the generic language model training pipeline."}, {"title": "Training Pipeline Language Model", "content": "LLMs are typically constructed in a sequence of stages, from data preparation, through training, to inference. The training pipeline for most LLMs is quite elaborate. We will now list a pipeline of the most common stages, based on the survey by Minaee et al. [2024].\n1.  Acquire a large, general, unlabeled, high-quality text corpus. Some considera-tions on the selection of the texts are discussed in Brown et al. [2020].\n2.  Pretrain the transformer model [Vaswani et al., 2017] on this large corpus. This step yields a generalist model. The pretraining is done using a self-supervised approach on the unlabeled dataset (text corpus).\n3.  Finetune the general model to a specific (narrow) task. This can be done us-ing supervised-learning with a new labeled dataset consisting of prompts and answers (supervised finetuning, SFT) [Wei et al., 2022a, Minaee et al., 2024], specific for the task at hand. (A small number of papers in this survey work in the finetuning stage.)\n4.  Instruction tuning is a form of finetuning on a labeled dataset of instruction prompts and corresponding outputs, to improve instruction following, and thus the usefulness of models.\n5.  Align the finetuned model with user expectations (preference alignment). The goal of this stage is to improve the model to give more ethically and socially acceptable answers. The machine learning method that is used in this stage can"}, {"title": "In-Context Learning", "content": "In LLMs beyond hundreds of billions of parameters a new kind of learning has emerged, that is called in-context learning or prompt-learning [Brown et al., 2020]. It occurs at inference time, and is often able to give good results with few examples; it is a form of few-shot learning. The large size of the model, containing rich and general knowledge is enabling this new type of few-shot learning (see Dong et al. [2022] for a survey).\nA prompt, consisting of a piece of demonstration context, is concatenated with a query question, and is given to the language model for prediction [Liu et al., 2023]. For example, when the task is emotion recognition in a social media post, \"I missed the bus today,\" can be followed by \"I felt so [___]\". Alternatively, for translation, we could fol-low \"I missed the bus today,\" by \"French: [___]\" [Liu et al., 2023]. The prompt contains background information that is recognized by the model, selecting the desired model context. In-context learning works when language models contain enough knowledge, allowing them to generalize on the examples provided in the prompt.\nPrompts that contain a few examples are said to perform few-shot learning. Prompts that contain only instructions without examples are said to perform zero-shot learning.\nIn-context learning takes place at inference time, after the computationally inten-sive stages where parameters have been pretrained and finetuned, when the model is queried by the user to provide answers. No parameters are changed anymore with in-context learning. This is quite different from the common approach in supervised deep learning or self-supervised deep learning-where large datasets are used during training to update model parameters with backward propagation in lengthy and costly training epochs [Goodfellow et al., 2016]. Common approaches to few-shot learning, such as metalearning, do include training and finetuning of parameters to achieve gen-eralization, and are computationally expensive (see, for example, Finn et al. [2017] or Huisman et al. [2021], Hospedales et al. [2021] for a survey)."}, {"title": "Reasoning Pipeline", "content": "Reasoning problems are also solved with a pipeline of stages. A typical approach to solving a complex problem is to subdivide it into smaller steps and solve those. This approach is related to divide and conquer [Bellman, 1966]. New steps are (1) generated, (2) evaluated, and the number of steps that are generated and searched is (3) controlled in some way. The in-context reasoning approaches that we survey follow a general three-stage pipeline [Madaan et al., 2023]:\n1.  Generate: generation of steps by the model,\n2.  Evaluate: evaluation of the predicted steps by an evaluator,\n3.  Control: control of the number of steps that are generated and how deep ahead the reasoning process will look.\nThis three-stage pipeline will be the basis of our taxonomy. But first, we will look at benchmarks."}, {"title": "Benchmarks", "content": "Progress in artificial intelligence is measured by benchmarks. Benchmarks define the goal that researchers aim to achieve in their experiments. In natural language pro-cessing, a wide array of benchmarks exists to measure progress, such as on ques-tion answering (for example, CommonsenseQA [Talmor et al., 2018]), word prediction (for example, LAMBADA [Paperno et al., 2016]), translation (for example, WMT'22 [Kocmi et al., 2022]), language understanding (for example, GLUE [Wang et al., 2018, 2019]), and text summarization (for example, Xsum [Narayan et al., 2018]). Transformer architectures were first popularized by encoder models such as BERT [Devlin et al., 2018], for named entity recognition and classification tasks. Subsequently, de-coder models such as GPT 2-4 [Radford et al., 2019, Brown et al., 2020, Achiam et al., 2023] showed impressive progress on natural language benchmarks.\nThe field of LLMs is quite active. Many different benchmarks exist, and listing a comprehensive overview of all relevant benchmarks is beyond the scope of this sur-vey. We will mention relevant benchmarks for testing the reasoning abilities of LLMs. Following Wei et al. [2022b], these are all math word problem benchmarks. The bench-mark that is most frequently associated with reasoning by LLMs is a dataset of grade school math word problems GSM8K [Cobbe et al., 2021]. GSM8K was created by humans, with an aim of high quality, high diversity, moderate difficulty, and solutions in natural language. Other benchmarks are the SVAMP varying structures benchmarks [Patel et al., 2021], the ASDiv dataset of diverse math problems [Miao et al., 2021],"}, {"title": "Selection of Papers", "content": "The papers in this survey were selected as follows. Baseline LLMs have difficulty solving math word problems, specifically on benchmarks listed in the previous section. We take the ability to solve those benchmarks as a proxy for reasoning ability. We initially performed a literature search for papers that use these benchmarks, and that contain the search terms reasoning and large language model in their title or abstract. We also searched for papers that referenced the Chain-of-thought paper. The resulting papers were curated based on recency, relevance, substance, and novelty.\nWe favor recent papers (two years prior to the writing of the survey), related to the Chain-of-thought approach of generating intermediate reasoning steps, that solve tasks such as math word problems, and that work by prompt-based in-context learning. We also include some papers that work by finetuning or supervised learning that relate to, or inspire, the Chain-of-thought approaches. Furthermore, we include approaches outside math word problems that showed interesting approaches to reasoning, such as applications in coding and autonomous agents, because of their approach to grounding."}, {"title": "Prompt Generation, Evaluation and Control", "content": "This survey examines how an architecture that is good at System 1 tasks can be prompted to solve System 2 tasks. The Chain-of-thought paper showed how a simple command could prompt an LLM to perform reasoning steps, yielding much better performance in math word problems. Since then much research has further explored this approach, trying to build the ultimate general problem solver for System 1 and System 2 prob-lems.\nFollowing the pipeline of Section 2.3, the prompts must (1) generate the reasoning steps, (2) evaluate the answer to the steps, and (3) control the number of steps that are generated, the shape (or complexity) of the reasoning process must be controlled. We will now briefly discuss the three stages. Please refer to Figure 1 for a diagram of the different approaches for the generation, evaluation, and control of reasoning steps, and to Table 1.2\nPrompt for Step Generation The first order of business is to create a prompt that instructs the LLM to generate reasoning steps. The problem must be split into substeps. This can be achieved with a problem-specific prompt that contains elements of the problem, such as: \"First calculate how many marbles Mary had originally, then how many her friend had, and finally how many they had together.\"\nIn general, it is possible to prompt an LLM to fill in the blanks in a step-by-step fashion. In the papers that we will discuss, there are three main approaches for gener-ating the step-by-step prompt. The prompt may be (1) handcrafted for the problem by the researchers (hand-written prompt), or (2) the prompt or prompts may come from an source that is external to the model, such as another model or a dataset (prompt using external knowledge), or (3) the model itself can be prompted to generate a (series of) prompt(s) to analyze the problem (model-generated prompt). As we will see, all three approaches have their advantages and disadvantages.\nGenerating the subproblem-steps is the first stage that is necessary for in-context learning to perform reasoning. Each paper in our survey performs at least this stage of the reasoning pipeline. In some of the early papers (around 2022) it is the only stage of the pipeline that is performed.\nPrompt for Result Evaluation After the prompt has been generated and the model has answered it, the next step in the reasoning pipeline is to evaluate the answer. Again, we see three main approaches for substep evaluation. First, the steps may be evaluated by (1) the model itself (self-assessment). Second, (2) an external program can be used to evaluate the steps. For example, when the steps are expressed as computer code, an external interpreter or compiler can be used to check the validity and the outcome (tool-based evaluation). Finally, (3) an external model can be used, LLM or otherwise. For example, in robotics, an external physics model can determine if certain actions are physically possible (external model validation)."}, {"title": "Perform Control of Reasoning Steps", "content": "A reasoning process that consists of multiple steps is a sequential decision process [Littman, 1996]. When a single chain of reason-ing steps is generated, the control flow of the reasoning process is simple: greedily evaluate the first step and then the next one, if present. The control flow of the reason-ing process may also be more intricate. Some reasoning problems can be divided into multiple subproblems. To execute, evaluate and combine the results of all substeps, a separate controller may be needed. This controller can be a prompt or an external algorithm.\nAgain, we distinguish three approaches. Most papers use (1) a greedy selection approach: a single prompt with a single chain of steps is generated, and these steps are directly executed and followed. The second approach (2) is to generate an ensemble strategy of reasoning steps, evaluate them, combine the individual results, and present them as the result of the ensemble. Finally, (3) a full tree-search or a reinforcement learning (RL) algorithm can be used as scaffolding. In this case, when a step is fol-lowed and evaluated, the LLM can roll back and try a different reasoning step. This is a breadth-first search approach [Plaat, 2020]. Going further, a full reinforcement learning approach can be used [Sutton and Barto, 2018, Plaat, 2022] to find an optimal policy for the sequential decision process. A full Markov Decision Process of state, ac-tion, transition, and reward function is specified, and step control can become a process where prompts are generated dynamically.\nDomain Many papers are applied to math word problems (natural language descrip-tions of math problems). Math problems were the original inspiration for the experi-ments with reasoning in LLMs. Other application domains include autonomous agents, robotic movement, generating computer programs, and playing computer games. We will discuss these in more detail with the individual approaches.\nTaxonomy Table Table 1 lists the papers of this survey. They are listed by the domain they work on, the type of prompt generation, the evaluation of the result, and the control method. The approaches in the table are grouped, divided by horizontal lines.\nThe first group, from Scratchpad to Self-ask, focuses on creating a prompt that gen-erates the reasoning steps. The entries in the cells of this column are shown in bold, highlighting the focus of the approaches. The approaches in this group can be consid-ered to be the start of the field of LLM-reasoning. The Chain-of-thought approach is especially an inspiration for many works. The prompts are often written \"manually\" by the researchers, the steps are encoded in one prompt, and step control is greedy. There is no specific evaluation of the steps, other than comparing results to the benchmark.\nThe Scratchpad approach is special in that it uses supervised learning, not prompt-learning; the work showed that LLMs can be made to generate internal reasoning steps by supervised learning, paving the way for the later prompt-based papers.\nThe second group, from Self-verification to Self-taught-reasoner, focuses on eval-uation of the reasoning steps in the prompt. This column is shown in bold in the table. The approaches in this group aim to improve the Chain-of-thought results by reduc-ing the error accumulation that occurs when multiple steps are taken in a reasoning chain. A variety of step control methods is used by these approaches, which is dis-"}, {"title": "Generation of Steps", "content": "Originally, LLMs performed poorly on math word problems (GSM8K [Cobbe et al., 2021]). Some different approaches were tried, for example scaling up the size of the LLM [Rae et al., 2021]. The LLM architecture, based on transformers, is designed to produce a single token. When we prompt such an architecture to produce an answer, it does so. What we should do is prompt it to follow intermediate steps, answer those, and thus work towards the final answer, just as a student is taught to break down a complex problem into smaller steps. We should take the model by its hand and teach it to write down the intermediate steps, and combine the intermediate results [Nye et al., 2021].\nThis idea was used by Nye et al. [2021] in Scratchpads, a transformer model that performs multi-step computations by asking it to emit intermediate computation steps into a scratchpad. They train the model by supervised learning (not prompt-based in-context learning). Figure 2 shows an example. On experiments with addition, poly-nomial evaluation, and Python code execution, versions that produced the intermediate steps on a scratchpad performed considerably better than versions that did not."}, {"title": "Hand-written Prompt", "content": "This question was studied by Wei et al. [2022b], amongst others. A basic way to in-struct an LLM to generate steps by prompt-learning is to manually write a prompt for the large language model to follow the reasoning steps. They showed in their Chain-of-thought paper that with such a prompt the LLM follows such intermediate steps. When the LLM is prompted to rephrase information from the question as intermediate rea-soning steps in its answer, the LLM performed much better than when it was prompted to answer a math problem directly, without reproducing the information from the ques-tion in its answer. The example from the Chain-of-thought paper is shown in Figure 3 Wei et al. [2022b]. Performance figures were given in Section 3 on benchmarks.\nThe substantial performance improvement by Chain-of-thought has caused much excitement and has opened up further research on reasoning with LLMs. In the orig-inal Chain-of-thought paper the prompts were handwritten by the researchers for the individual types of problems, and evaluations are conducted with five different bench-marks (not by an LLM).3 In a later work the prompts were generated automatically by the LLM [Zhang et al., 2022].\nKojima et al. [2022] go a step further. They show that the simple addition of a single text to the prompt (Let's think step by step) significantly improves performance. Since this text does not contain problem-related elements, this can be considered as a form of zero-shot learning. Figure 4 compares the approaches. Experiments further show that with this addition to the prompt, significant performance gains are achieved"}, {"title": "Prompt using External Knowledge", "content": "Chain-of-thought shows that an LLM gives better answers to complex problems when it is guided to take individual steps. Prompts are written manually, from scratch, by the researchers.\nWe can use external information about the problem to improve the prompt. Press et al. [2022] study how subproblems are related to the main problem, which they call compositional reasoning. They study how often a model is able to answer the sub-problems, but not the overall problem. This difference is called the compositionality gap. They find that in GPT-3, as model size increases, the compositionality gap does not decrease: the single-hop question-answering performance improves faster than the multi-hop performance. This shows that while more powerful models memorize and recall more factual knowledge, no improvement in their ability to perform composi-tional reasoning occurs. They find that the ability to reason does not depend on the size of the model.\nSubsequently, a method called Self-ask is proposed, that asks elicitive follow-up"}, {"title": "Model-Generated Prompt", "content": "In addition to manually writing prompts or using external information, we can also try to let the LLM itself study the problem to write the best reasoning-prompt, a form of self-improvement. An example of this approach is Auto-chain-of-thought [Zhang et al., 2022]. This approach builds on the observation by Kojima et al. [2022] that large lan-guage models are zero-shot reasoners. First, Auto-chain generates specific questions for a given dataset and partitions them into clusters. Then an external algorithm uses the model to generate examples that are sampled for diversity. The constructed demon-strations augment the in-context prompt. The automatically generated prompts are reported to perform as well or better than the hand-written Chain-of-thought prompts on ten benchmarks using GPT-3.\nFu et al. [2022] introduce Complexity-based prompting. Inspired by Chain-of-thought and Self-consistency, this work studies which prompts achieve the best results on math word and other reasoning problems. Their work specifically studies the impact of the complexity of the reasoning chain, and introduces a related reasoning approach (Complexity-based prompting). They find that prompts with the largest complexity (the most reasoning steps) perform best. Further, they find that outputs (answers) with the highest complexity are the best. Complexity-based prompting achieves high per-formance on three math reasoning benchmarks.\nAnother approach that uses model-generated prompts is Buffer-of-thoughts. We will discuss this approach in Section 5.3.3."}, {"title": "Evaluation of Steps", "content": "After discussing prompts for the generation of reasoning steps, the next stage in the reasoning pipeline (Section 2.3) is evaluation of the results of the steps, to reduce the error of multi-step reasoning chains.\nWe will start with approaches where the same model performs step-generation and step-evaluation."}, {"title": "Self-Assessment", "content": "When LLMs are prompted to perform reasoning steps, they perform a sequence of steps and predict multiple tokens. Performing a sequence of steps makes them sensitive to mistakes and vulnerable to error accumulation [Weng et al., 2022, Xiao et al., 2023a]. Several methods have been developed to prevent error accumulation. One approach is"}, {"title": "Tool-based Validation", "content": "Another possibility to improve the accuracy of evaluating the reasoning steps is to switch from a natural to a formal language. The advantage of a formal language is that it is less ambiguous than a natural language. Examples are computer languages, such as Python, or mathematical equations. Using a formal language for reasoning is a popular approach, and we discuss seven papers. Many approaches generate the steps in Python, and the code can then be evaluated by a formal evaluator, such as a compiler, debugger, or interpreter.\nLLMs have been quite successful in generating computer code from natural lan-guage prompts. Chen et al. [2021] introduced Codex, a GPT model that was trained on publicly available code in the repository GitHub. A production version of this work was introduced under the name GitHub Copilot. Codex is able to generate correct pro-grams from descriptions in natural language, such as commentary strings. Figure 7 shows examples that are produced by Codex.\nThe work on Codex is used as a basis for further research on reasoning in LLMs. Human programmers, when writing code, typically follow a cycle of writing some code, executing it to look for errors, and then using the feedback to improve the code. This same approach is followed in the Self-debugging work [Chen et al., 2023]. Self-debugging teaches a large language model to debug its generated program code via few-shot demonstrations. It follows the same steps of (1) code generation, (2) code execution, and (3) code explanation (see Figure 8).\nSelf-debugging is able, without human feedback on the code's correctness or error messages, to identify mistakes in the code that was generated by itself from investi-gating the execution results. Self-debugging can also provide an explanation of the generated code in natural language. It achieves strong performance on text-to-SQL generation, C++-to-Python transcoding, and text-to-Python generation.\nSeveral works use self-debugging to generate working code tuned for solving spe-cific problems automatically, without human feedback. Romera-Paredes et al. [2024] introduced FunSearch, an approach that integrates formal methods and LLMs to en-hance mathematical reasoning and code generation. FunSearch is capable of producing functionally correct programs that adhere to specified requirements. It uses a genetic algorithm approach with multiple populations of candidate solutions (programs), which are automatically evaluated (using tools depending on the problem specification). In addition to the problem specification in the form of an evaluate function, also an initial program is given to the LLM in the first prompt. After evaluating a number of gen-erated programs from the starting prompt, a new prompt using \u2018best-shot prompting' is created in an iterative fashion, combining a selection of k sampled programs in a sorted list (ascending according to their evaluation score), and the LLM is requested to generate program k + 1. Another work leverages evolutionary computation methods to generate and optimize evolutionary algorithms [van Stein and B\u00e4ck, 2024]. This ap-proach, LLaMEA (Large Language Model Evolutionary Algorithm), utilizes LLMs to design and optimize evolutionary algorithms. The approach uses LLMs to generate ini-"}, {"title": "External Model Validation", "content": "We have seen many successful examples of prompt-based in-context reasoning and evaluation. We will now look at related reasoning approaches that follow a more tra-ditional parameter learning approach. We describe three natural language approaches that follow this route. All approaches evaluate the output of the model and generate corrective data. That data is then added to the training pipeline, and the model is sub-sequently finetuned.\nFinetuning The Refiner approach [Paul et al., 2023] uses a generator model and a critic model to provide fine-grained feedback on reasoning errors. The generator gen-erates multiple reasoning hypotheses, the critic evaluates results by randomly selecting a hypothesis for feedback. The generator model is finetuned based on its reasoning er-rors. A small supervised model is used to overcome the cold-start problem. Figure 10 shows an example of how the critic provides feedback to the generator. The approach is reported to work well on math word problems and synthetic natural language rea-soning."}, {"title": "Reasoning about Robot Behavior", "content": "In addition to math word problems, prompt-based reasoning has also been used to reason about robot behavior. Language models contain a large amount of information about the real world [Ahn et al., 2022]. In theory, this should allow the model to exhibit realistic reasoning about robotic behavior. However, the models do not have knowledge about particular embodied aspects of a particular robot. If we could compare a Scratchpad-like list of intermediate reasoning steps with"}, {"title": "Control of Steps", "content": "The third stage in the reasoning pipeline in Section 2.3 is reasoning control. This stage controls how many sub-steps are generated, and how deep into the future the reasoning chain is generated.\nThere are three main approaches: (1) greedy selection, which generates a step and then follows it, (2) ensemble strategy, which generates a set of possible next steps, and (3) a full tree-shaped search which generates multiple options for the step, and follows them multiple steps into the future, traversing a search tree with backtracking, control-ling an exponential search space. We include reinforcement learning approaches, that interactively learn an optimal policy for such a reasoning space."}, {"title": "Greedy Selection", "content": "Most earlier works on prompt-based reasoning follow the greedy approach: generate a single prompt with a sequence of steps and follow them. Among the greedy reasoners are Chain-of-thought, Auto-CoT, and Zero-shot CoT. Inner Monologue and Say-Can also use greedy reasoning.\nIn Least-to-most prompting [Zhou et al., 2022], the key idea is to break down a complex problem into simpler subproblems and then solve these in sequence, explicitly encoding them in the prompt. It is related to Complexity-based prompting. In Least-to-most, finding the answer to each subproblem is facilitated by the answers to previously solved subproblems, as in a curriculum [Bengio et al., 2009]. The authors find that on"}, {"title": "Ensemble Strategy", "content": "The second kind of reasoning control is based on an ensemble of (sequences of) rea-soning steps. The ensemble approach is a well-known technique in machine learn-ing to make a strong learner out of multiple weaker learners [Sagi and Rokach, 2018, Breiman, 2001]. For most problems, multiple different options for the next step ex-ist. When all or some of these are generated and evaluated, then the best result or the consensus result can be reported as the outcome of an ensemble of steps. Various approaches have been proposed.\nWe already mentioned Self-consistency [Wang et al., 2022b] and Self-verification [Weng et al., 2022] in Section 5.2.1. They are popular ensemble approaches to evaluate the results of reasoning steps in prompt learning. The greedy single-path decoding used in Chain-of-thought prompting is replaced by sampling a diverse set of reasoning paths, evaluating them, and selecting the most consistent answer.\nIn another domain Chain-of-experts builds on Chain-of-thought with a mixture of experts ensemble for complex combinatorial operations research problems [Xiao et al., 2023b]. PAL and MathPrompter also use the ensemble approach. They generate mul-tiple steps, which are evaluated and whose answer is combined, or the best step is chosen.\nThe ensemble approach is a popular approach in LLM-reasoning."}, {"title": "Reinforcement Learning", "content": "In the greedy approach, a single reasoning path is generated and traversed. In reason-ing, often multiple valid reasoning steps are possible, but pursuing all possibilities over multiple reasoning steps may lead to an infeasible number of possibilities.\nThe third kind of reasoning control is to use a full-fledged controller that can tra-verse a tree, or even perform reinforcement learning to do so [Sutton and Barto, 2018, Kaelbling et al., 1996, Plaat, 2022]. This group of control approaches enables the most elaborate control of the reasoning process, and is used by many works, as we will see. When decomposing the problem, multiple alternative steps are generated that can be searched multiple steps into the future. Then, backtracking can be performed, allowing alternative steps to be tried.\nWhere greedy and ensemble processes can be controlled with a prompt by the LLM, this third group is more complex, and an external algorithm is used to control the reasoning process. The external algorithms call the LLM as a subroutine prompting it to perform its tasks. This allows more complex reasoning control, but we are no longer performing prompt-based self-reasoning; control has been given to an algorithm that is external to the LLM and external to prompt-learning.\nWe start our discussion of control strategies with depth-first and breadth-first search, then go to beam search, and then to full reinforcement learning.\nBreadth first search A complex reasoning space can be traversed with a search al-gorithm. Tree-of-thoughts includes a search algorithm to dynamically follow different reasoning steps [Yao et al., 2024]. When one reasoning path has been traversed, a search algorithm can backtrack, and try an alternative path. The paper describes both a breadth-first-search and a depth-first-search controller.\nThe evaluation part in Tree-of-thoughts is performed with a prompt by the LLM. Together, the trio of generation, evaluation, and control allow systematic exploration of the space of reasoning steps with look-ahead and backtracking. The authors compare their approach to Chain-of-thought and Self-consistency. Chain-of-thought builds a reasoning out of a path of thoughts, Self-consistency creates an ensemble of thoughts, and Tree-of-thoughts constructs a tree structure.4\nAnother approach, Buffer-of-thoughts [Yang et al., 2024], goes a step further to-wards meta-reasoning. It introduces a meta-buffer that stores high-level thought-templates. These universal thought-templates are derived from a variety of tasks. Figure 16 com-pares the Buffer-of-thoughts approach to other approaches such as Chain-of-thought and Tree-of-thoughts. Buffer-of-thoughts outperforms other methods in puzzles such as Game of 24 and checkmating. Thought templates are related to metacognition (thinking about thinking), which is further discussed in Section 6.2.3.\nBeam search A related search method is Beam-search. Beam-search-for-reasoning [Xie et al., 2024] focuses on control of the space of possible reasoning paths. In some"}, {"title": "Hallucination, Faithfulness and Scaling", "content": "Most works on reasoning in LLMs are experimental in nature. The success of in-context learning and Chain-of-thought reasoning is attracting the attention of work providing deeper insight into the reasoning processes in language models.\nSaparov and He [2022] introduce a synthetic question/answer dataset designed to evaluate the reasoning abilities of LLMs. The work showed that LLMs are capable of"}, {"title": "Faithfulness", "content": "Chain-of-thought and other approaches prompt a language model to take certain steps to solve the problem that the prompt specifies. One can ask the question, whether those steps are indeed the steps that the model has followed (faithful reasoning) or whether it took another road to arrive at the correct answer (unfaithful reasoning). A few studies measure the faithfulness of reasoning by LLMs. Lanham et al. [2023", "2024": ".", "2023": "propose Faithful-chain-of-thought. This approach involves two stages. First, the natural language query is translated into a formal symbolic language. Second, the problem-solving stage processes the for-mal language, and can explain the reasoning steps it has thus taken. For the symbolic language, Python, Datalog, or PDDL is suggested. Faithfulness studies tell us more about how models reason"}]}