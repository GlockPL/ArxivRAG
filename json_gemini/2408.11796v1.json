{"title": "LLM Pruning and Distillation in Practice: The Minitron Approach", "authors": ["Sharath Turuvekere Sreenivas", "Saurav Muralidharan", "Raviraj Joshi", "Marcin Chochowski", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Jan Kautz", "Pavlo Molchanov"], "abstract": "We present a comprehensive report on compressing the Llama 3.1 8B and Mistral NeMo 12B models to 4B and 8B parameters, respectively, using pruning and distillation [1]. We explore two distinct pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate the results on common benchmarks from the LM Evaluation Harness [2]. The models are then aligned with NeMo Aligner and tested in instruct-tuned versions. This approach produces a compelling 4B model from Llama 3.1 8B and a state-of-the-art Mistral-NeMo-Minitron-8B (MN-Minitron-8B for brevity) model from Mistral NeMo 12B. We found that with no access to the original data, it is beneficial to slightly fine-tune teacher models on the distillation dataset. We open-source our base model weights on Hugging Face with a permissive license.", "sections": [{"title": "Introduction", "content": "LLM providers often train an entire family of models from scratch, each with a different size (number of parameters, e.g. Llama 3.1 8B, 70B, 405B); this is done to aid users targeting different deployment scales, sizes and compute budgets. However, training multiple multi-billion parameter models from scratch is extremely time-, data- and resource-intensive.\nRecent work [1] has demonstrated the effectiveness of combining weight pruning with knowledge distilla- tion to significantly reduce the cost of training LLM model families. Here, only the biggest model in the family is trained from scratch; other models are ob- tained by successively pruning the bigger model(s) and then performing knowledge distillation to recover the accuracy of pruned models.\nIn this report, we successfully apply the Minitron compression strategy [1] to two state-of-the-art mod- els: Llama 3.1 8B [3] and Mistral NeMo 12B [4], compressing them down to 4B and 8B parameters, respectively. Figure 1 provides a high-level overview of our approach.\nWhile following the original paper [1], we make a key modification: due to lack of access to the original training data, we fine-tune the teacher model on our own dataset before pruning and distillation. We refer to this step as teacher correction. Figure 4 shows that omitting teacher correction causes a data distribution mismatch, negatively impacting distillation.\nTable 1 provides a summary of our results: our com- pression strategy yields a state-of-the-art 8B model"}, {"title": "Methodology", "content": "A high-level overview of our approach is illustrated in Figure 1. Here, the teacher model is first lightly finetuned on the target dataset to be used for dis- tillation we refer to this step as teacher correction. Next, pruning is applied to compress the model, fol- lowing which distillation is used to recover any lost model accuracy. We refer the reader to the Minitron paper [1] for the full description of the pruning and distillation method."}, {"title": "Pruning", "content": "Weight pruning is a powerful and well-known tech- nique for reducing model size. In this report, we focus on structured pruning, where blocks (or channels) of nonzero elements are removed at once from model weights; examples of structured pruning techniques include neuron, attention head, convolutional filter, and depth pruning [1]. In case of LLMs, as shown in Figure 2, we start the pruning process by first com- puting the importance of each layer, neuron, head, and embedding dimension. We then sort these impor- tance scores to compute a corresponding importance ranking.\nImportance Estimation: We use a purely activation-based importance estimation strategy that simultaneously computes sensitivity information for all the axes we consider (depth, neuron, head, and embedding channel) using a small calibration dataset and only forward propagation passes. We consider depth pruning as a special case and do not combine it with compressing other dimensions.\nWe compute the importance of each head, neuron and embedding channel by examining the activations produced by the multi-head attention (MHA), multi- layer perceptron (MLP) and LayerNorm layers, re- spectively. We use a small calibration dataset (1024 samples) for this purpose.\nFor depth pruning, we consider three distinct met-"}, {"title": "Retraining with Distillation", "content": "We use the term retraining to refer to the accuracy re- covery process following pruning. In this work, we ex- plore two retraining strategies: (1) conventional train- ing, leveraging ground truth labels, and (2) knowl- edge distillation using supervision from the unpruned model (teacher). Knowledge Distillation (KD) in- volves transfer of knowledge from a larger or more complex model called the teacher to a smaller/simpler model called the student. The knowledge transfer is achieved by having the student model mimic the out- put and/or the intermediate states of the teacher model. In our case, the uncompressed and pruned models correspond to the teacher and student, respec- tively. For distillation, we follow best practices from our previous work [1] and use forward KL Divergence loss [7] on the teacher and student logits only. This is illustrated in Figure 3."}, {"title": "Training Details", "content": "Llama 3.1 8B [3] and Mistral NeMo [4] 12B are pre- trained on different proprietary datasets, which we do not have access to. According to the Llama 3.1 tech report [3], the 8B model is pretrained on 15T tokens. We start with the corresponding Base models that are openly available online on Hugging Face.\nDataset: We use the Nemotron-4 curated continued training dataset (CT) [8] [9] for all our experiments."}, {"title": "Pruning", "content": "Our simplified pruning recipe is based on the best practices outlined in the Minitron paper [1] and is described in the Methodology section. Specifically, for width pruning, we (1) use 12-norm and mean as the aggregation functions across the batch and sequence dimensions, respectively, and (2) perform single-shot pruning, avoiding iterative approaches. For depth pruning, as described in the Methodology section, we follow the observations from Gromov et al. [10] and drop a continuous subgroup of layers that results in the least accuracy drop on Winogrande [6]. In this work, we skip the lightweight neural architecture search (NAS) phase, and go with a manual architec- ture configuration for both Llama-3.1-Minitron-4B and MN-Minitron-8B. The architectures we come up with are inspired by the Minitron-4B and Minitron-8B models, and are detailed in Table 3. We now describe the pruning recipes for each of our target compressed models:"}, {"title": "Distillation", "content": "Teacher Correction: Using the Mistral NeMo 12B model directly as a teacher performs sub-optimally on our dataset. This is due to the change in distribution of sub-word tokens across the original dataset the teacher model was trained on vs. the dataset being distilled on. To account for this, we first fine-tune the teacher on our dataset using ~127B tokens. As shown in Figure 4, such a correction is essential if the original dataset is not available during distillation. We thus apply this technique on both the Mistral-NeMo and Llama-3.1 teacher models. The fine-tuning process has a minor effect on the teacher model's accuracy on downstream tasks, with some tasks improving and some degrading as shown in Table 1. We hypothesize this to be an artifact of the dataset used for fine- tuning.\nRetraining: Following the learnings in the Mini- tron work [1], we opt for logit-only distillation, mini- mizing the forward KL Divergence [7] loss across the teacher and student probabilities, and ignore the LM cross-entropy loss altogether. Here, the unpruned and pruned models correspond to the teacher and student, respectively. We use the hyperparameters listed in Table 4 during distillation. We use 32 NVIDIA DGX H100 nodes for our training jobs."}, {"title": "Instruction Tuning", "content": "To evaluate the instruction-following capabilities of our distilled models, we perform supervised fine- tuning (SFT) on the Llama-3.1-Minitron 4B mod- els using NeMo-Aligner [11] with the instruction tuning dataset used for Nemotron-4 340B [12]. As shown in Table 2, we evaluate the aligned models for instruction- following and roleplay (IFEval [13] and MT-Bench [14]), RAG QA (ChatRAG-Bench [15]), and function-calling capabilities (BFCL [16])."}, {"title": "Analysis", "content": "We perform a series of ablation studies to better understand the compression characteristics of these newer models. We report our results in this section.\nWidth vs Depth Pruning: Figure 5 shows the training curve of Llama-3.1-Minitron-4B pruned for width vs. depth. We notice that width pruning results in smaller initial loss and consistently outperforms the depth-pruned model, despite both variants having the same number of parameters.\nPruning and Distillation: Figure 6 demonstrates orthogonal benefits of our proposed approach with pruning and distillation. We compare (1) random weight initialization and distillation, (2) random prun- ing and distillation, where components are pruned randomly ignoring the importance scores, (3) our proposed pruning with typical cross entropy based LM loss training and (4) our proposed pruning with distillation-based training. We notice that prun- ing results in a significantly better starting point compared to random initialization, and also that distillation-based training outperforms conventional training methods while requiring significantly fewer training tokens (up to 50x in our case).\nTeacher Correction: We compare two approaches for teacher correction: (1) pruning and distilling the corrected teacher, and (2) pruning the original teacher and distilling from a continuously corrected teacher. The results in Figure 7 suggest that teacher correc- tion doesn't affect the optimality of pruning, and that distillation from a corrected teacher is crucial. Teacher correction can be performed in parallel with distillation to bridge the gap."}, {"title": "Depth Pruning Metrics", "content": "when examining how LM validation loss increases as contiguous blocks of layers are removed (Figure 8), we observe that the layers at the beginning and end are the most im- portant. Removing non-contiguous layers can result in even better LM validation loss (the dashed line). However, this observation does not necessarily hold when evaluating downstream task performance. Fig- ure 9 shows that dropping 16 layers selected based on per-layer importance ( [5, 17]) yields a random Wino- grande accuracy of 0.5, while removing layers 16 to 31 continuously ( [10]) results in an accuracy of 0.595. The gap holds during distillation-based retraining and we opt for the latter approach."}, {"title": "Evaluation", "content": "following Touvron et al. [18], we eval- uate our compressed models on a series of downstream"}, {"title": "Base Models", "content": "Base model evaluation results are shown in Table 1. Compared to similarly-sized models, MN-Minitron- 8B demonstrates superior accuracy across the board, outperforming the recent Llama 3.1 8B model using 40\u00d7 fewer training tokens (380B vs. 15T). Similarly, the Llama-3.1-Minitron 4B models perform favorably compared to the teacher Llama 3.1 8B model using 150x fewer training tokens (94B vs. 15T); our pruned Llama models also outperform the previous generation Minitron 4B model. We note from Table 1 that the width-pruned variant outperforms the depth-pruned one. These results clearly demonstrate the advantages of our methodology: state-of-the-art accuracy coupled with an order of magnitude improvement in training efficiency."}, {"title": "Instruct Models", "content": "The performance of the instruction-tuned Llama-3.1- Minitron 4B variants is shown in Table 2. We compare the Llama-3.1-Minitron 4B variants to other similarly- sized baselines and notice that our models demon- strate strong instruction-following and roleplay capa- bilities, only lagging behind Gemma2 in IFEval [13] and MT-Bench [14]. On retrieval based question an- swering (ChatRAG-Bench [15]) and function-calling (BFCL [16]), Minitron models achieve state-of-the-art performance."}, {"title": "Insights", "content": "In this Section, we summarize some interesting and surprising observations."}, {"title": "General", "content": "1. Teacher correction is crucial for distillation to work optimally on a new, unseen dataset. Fine- tuning the teacher with the dataset used for distil- lation in this manner yields over a 6% reduction in LM validation loss. Teacher correction doesn't affect the optimality of pruning and can even be performed in parallel with distillation.\n2. In line with the Minitron paper's observations, we require only 380B tokens to achieve state-of- the-art accuracy post pruning with distillation.\n3. For width pruning, we achieve stronger accuracy by retaining attention heads and pruning the other dimensions (MLP intermediate dimension, embedding channels)."}, {"title": "Mistral NeMo 12B to MN-Minitron-8B", "content": "1. Our compressed model outperforms the teacher on two benchmarks, GSM8k and Human Eval after pruning and distillation: GSM8k increases from 55.7% to 58.5% and HumanEval increases from 23.8% to 36.2%. This improvement is likely influenced by the dataset. However, retraining is performed using the distillation loss alone."}, {"title": "Llama 3.1 8B to Llama-3.1-Minitron 4B", "content": "1. Width pruning delivers better accuracy with MMLU at 60.5%, while depth pruning yields 58.7%, for Llama-3.1 compression.\n2. Reasoning ability is impacted further signifi- cantly, with GSM8K accuracy at 41.24% for width and 16.8% for depth.\n3. Depth pruning boosts throughput, achieving ~ 2.7x speedup over Llama-3.1 8B, while width pruning provides ~ 1.7\u00d7 speed up.\n4. For depth pruning, we observe that dropping contiguous layers from the model is more ef- fective than using non-contiguous, importance- based pruning."}]}