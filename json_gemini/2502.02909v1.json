{"title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs", "authors": ["Dinithi Jayasuriya", "Sina Tayebati", "Davide Ettori", "Ranganath Krishnan", "Amit Ranjan Trivedi"], "abstract": "We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, enabling transfer learning across diverse tasks and domains. Their ability to encode rich semantic representations allows them to generalize effectively and adapt to new tasks with minimal supervision. These characteristics, along with their proficiency in few-shot learning, have established LLMs as indispensable tools for applications such as text generation [1], question answering [2], summarization [3], and reasoning [4].\nWhile pretrained LLMs excel in handling static datasets and fixed tasks, many real-world applications require dynamic adaptability to evolving environments. For instance, LLM-based autonomous navigation systems must respond to unseen driving conditions, terrains, and tasks, such as understanding new traffic laws or adapting to varying weather conditions. Retraining a new model for each task is computationally prohibitive, especially for large-scale systems, and the assumption that data from earlier tasks is always available for retraining is often unrealistic due to storage limitations, regulatory constraints, or privacy concerns. These challenges necessitate methods that enable models to integrate new information incrementally while retaining the prior knowledge.\nHowever, LLMs also face critical challenges in continual learning-the ability to acquire new knowledge without overwriting previously learned information. Naive fine-tuning on new tasks often leads to catastrophic forgetting [5], where updates disrupt prior knowledge. The high computational cost of fine-tuning all parameters for each task is impractical, given that most of the model's parameters already encode transferable knowledge. Thus, continual learning in LLMs requires approaches that can facilitate efficient task-specific adaptation while preserving general-purpose capabilities.\nSeveral strategies have been proposed to address these challenges: Replay-based methods [6] mitigate forgetting by replaying data from previous tasks during training. However, these methods incur significant memory overhead and may be unsuitable for sensitive domains where storing and replaying data raises privacy concerns [7]. Regularization-based approaches, such as Elastic Weight Consolidation (EWC) [8] and Synaptic Intelligence (SI) [9], attempt to preserve knowledge by penalizing updates to parameters critical to previous tasks. However, these methods are less effective in the high-dimensional parameter spaces of LLMs. Parameter-efficient fine-tuning (PEFT) methods, including LoRA [10]\u2013[12], and adapter tuning [13], address scalability and efficiency by introducing lightweight task-specific modules that reduce trainable parameters. While PEFT methods are computationally efficient, they often require architectural modifications, complicating deployment and risking interference with pretrained representations.\nPrompt tuning has emerged as an alternative that is well-suited for task-specific adaptation in LLMs. By introducing small, trainable embeddings (soft prompts) appended to input tokens, prompt tuning enables task-specific learning while keeping the base model frozen. This approach significantly reduces the number of trainable parameters, making it computationally efficient and scalable for large models. However, existing prompt tuning methods typically train a separate prompt for each task, resulting in inefficiencies and redundancy when tasks share underlying representations. Moreover, these methods lack mechanisms to address catastrophic forgetting, limiting their applicability to continual learning scenarios where sequential task adaptation is required.\nTo address these limitations, as in Fig. 1(a), we propose SPARC, a novel framework for continual learning in LLMs"}, {"title": "II. BACKGROUND", "content": "Prompt tuning is a parameter-efficient approach for adapting frozen pretrained LLMs to specific tasks without modifying their internal weights. It introduces trainable embeddings, called soft prompts, which are prepended to the input token embeddings and optimized while keeping the LLM parameters fixed [14]. These learnable vectors, with dimensions matching the model's embedding space, act as lightweight adapters to guide the model's behaviour toward task-specific objectives while preserving its general-purpose knowledge.\nThe process begins with the initialization of soft prompts, which can be random or informed by task-specific heuristics.\nThese prompts are concatenated with the input token embeddings and passed through the frozen LLM. During training, only the soft prompts are updated using task-specific gradients, while the pretrained weights remain unchanged. This selective optimization significantly reduces computational overhead. For instance, the trainable parameters are proportional to the product of the number of soft prompt tokens (T) and the embedding dimensionality (d), typically amounting to a few thousand parameters for large-scale models.\nSoft prompts are particularly advantageous for large LLMs as they enable task-specific adaptation without requiring extensive computational resources or architectural modifications. Moreover, task-specific prompts can be stored independently and reused across tasks, making this approach ideal for multi-task learning and frequent updates. However, conventional prompt tuning methods lack mechanisms to mitigate catastrophic forgetting, where updates for new tasks overwrite knowledge from earlier ones. Additionally, they often train redundant prompts for tasks with overlapping representations, limiting efficiency in continual learning scenarios."}, {"title": "B. PCA-based Subspace Identification", "content": "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of orthogonal axes, called principal components, which capture the maximum variance in the data [15]. By projecting data onto these components, PCA identifies the most significant patterns while preserving as much information as possible, making it particularly useful for analyzing and comparing embedding spaces across tasks.\nGiven a dataset $X \\in R^{N \\times D}$, where N is the number of samples and D is the feature dimension, PCA projects X into a lower-dimensional subspace $R^k$, where $k < D$. The process begins by centering the data as $\\hat{X} = X - \\mu$, where $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$. Next, the covariance matrix $C = \\hat{X}^T \\hat{X}$ is computed, and eigenvalue decomposition is performed: $Cw_i = \\lambda_i w_i$, where $w_i$ are eigenvectors (principal components) and $\\lambda_i$ are eigenvalues. The top k eigenvectors corresponding to the largest eigenvalues are selected to form the transformation matrix $W \\in R^{k \\times D}$, ensuring orthogonality ($WW^T = I$, where I is the identity matrix).\nThese components define a task's subspace in the embedding space. The projection of the original data into this subspace is given by $X_k = XW^T$. PCA is particularly effective for continual learning as it enables tasks to be represented in lower-dimensional subspaces that capture their dominant features. These subspaces can then be compared to assess similarity across tasks, facilitating prompt reuse for tasks with shared representations or initializing orthogonal subspaces for independent tasks. By focusing on the most informative components, PCA reduces dimensionality while maintaining the core structure of task-specific knowledge."}, {"title": "C. Subspace Overlap using Cosine Similarity", "content": "To enable efficient knowledge transfer across tasks, subspace overlap is quantified using cosine similarity [16]. This technique measures the alignment between principal components of embedding subspaces from different tasks, helping determine whether knowledge from a previous task can be reused for a new one. For two datasets $D_1$ and $D_2$, with principal components $P_1 = \\{p_1, ..., p_k\\}$ and $P_2 = \\{p'_1, ..., p'_k\\}$, the cosine similarity between components $p_i \\in P_1$ and $p'_j \\in P_2$ is computed as:\n$S_{ij} = \\frac{p_i^T p'_j}{||p_i|| ||p'_j||}$  (1)\nA principal component $p_i$ is considered aligned with $P_2$ if its similarity with any $p'_j$ exceeds a predefined threshold $\\tau$. This alignment is summarized as the overlap percentage, which quantifies how many components of $P_1$ match with $P_2$:\n$\\text{Overlap Percentage} = \\frac{\\text{Number of Aligned Components}}{\\text{Total Components in } P_1}$   (2)\nBy identifying subspace overlap, the framework determines whether a prompt for a new task can reuse an existing one, minimizing redundancy. For tasks with substantial overlap, the corresponding prompt can be reused with minimal fine-tuning. For tasks with minimal overlap, a new prompt is initialized to capture the independent features of the new task. This enables the framework to reuse learned knowledge for related tasks while isolating representations for unique task characteristics."}, {"title": "D. Orthogonal Subspaces for Task Separation", "content": "Orthogonal subspaces are employed to prevent interference between tasks, ensuring that task-specific knowledge remains independent [12]. This is particularly important for tasks with minimal overlap in their embedding subspaces, where reusing a previously learned prompt may lead to performance degradation or catastrophic forgetting.\nTwo subspaces, $P_{t+1}$ (new task) and $P_i$ (previous task), are orthogonal if the inner product of all basis vectors between them is zero. Mathematically, this is expressed as:\n$\\langle P_{t+1}, P_i \\rangle = 0, \\forall i = 1, 2, ..., t$.\nIn practice, orthogonality is enforced by projecting the new subspace $P_{t+1}$ onto the complement of the span of the previously learned subspaces $\\{P_1, P_2, ..., P_t\\}$. This ensures:\n$\\text{Proj}_{P_i}(u) = 0, \\forall u \\in P_{t+1}, P_i \\in \\{P_1, P_2, ..., P_t\\}$.\nOrthogonal subspaces allow the framework to isolate new task-specific knowledge while preserving prior representations. By aligning new prompts with orthogonal directions in the embedding space, the model effectively mitigates catastrophic forgetting and interference [17]. This approach also ensures that existing prompts remain intact and reusable for earlier tasks. Combining subspace overlap analysis with orthogonal initialization allows the framework to balance knowledge retention and efficient adaptation. Overlapping subspaces enable prompt reuse for related tasks, while orthogonal subspaces ensure task-specific independence for novel features, making the framework robust for continual learning of LLMs."}, {"title": "III. SPARC: SUBSPACE-AWARE PROMPT TUNING FOR CONTINUAL LEARNING", "content": "The proposed framework leverages prompt tuning with subspace-guided initialization and orthogonal alignment to enable efficient and scalable continual learning of LLMs. Soft prompt embeddings are appended to the input tokens of a pretrained language model, allowing task-specific adaptation without modifying the model's internal weights. This section outlines the processes of prompt initialization, subspace overlap analysis, and orthogonal prompt generation to effectively balance knowledge transfer and task-specific independence."}, {"title": "A. Prompt Initialization and Subspace Representation", "content": "To adapt LLMs to new tasks, trainable soft prompts are initialized and optimized to represent the core characteristics of a dataset. The initialization process relies on PCA to capture the dominant features of the dataset's embedding space. These principal components are leveraged for prompt initialization for parameter efficiency and task-specific adaptation (Fig. 1(b)).\n1) Embedding Principal Components via PCA: Given a dataset $d_i$, its embeddings $X \\in R^{N \\times D}$ are generated by passing input tokens through a pretrained LLM or a sentence transformer, where N is the number of samples, and D is the embedding dimension. PCA is applied to X to extract the top k components forming a reduced subspace $R^k$, where $k \\ll D$. The transformation matrix $W \\in R^{k \\times D}$ is computed using the eigenvectors of the covariance matrix:\n$C = \\frac{1}{N} XX^T$, where $\\hat{X} = X - \\mu$, $\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i$.   (3)\nThe top k eigenvectors of C define the principal components in W, capturing the directions of maximum variance. Embeddings are projected into the subspace as $X_k = XW^T$, retaining the most informative features of the dataset while reducing noise and dimensionality. This subspace forms the initialization space for task-specific prompts.\n2) Learnable Prompt Parameters: Soft prompts are initialized as a trainable parameter matrix $P \\in R^{T \\times K}$, where T is the number of soft tokens, and K is the dimensionality of the principal subspace (k). Each row of P corresponds to a trainable embedding vector, which is prepended to the input token embeddings of $d_i$. During training, only P is updated to minimize the task-specific loss, while the pretrained model's weights remain frozen. This selective optimization enables efficient task adaptation with minimal computational overhead. For example, with T = 10 and K = 100, the number of trainable parameters is reduced to 1,000, which is orders of magnitude smaller than fine-tuning the entire LLM. By decoupling prompt updates from the model, the framework ensures modularity and scalability across tasks."}, {"title": "B. Subspace Overlap Analysis", "content": "The framework determines whether a new task can reuse an existing prompt or requires a new one by evaluating the similarity between the subspace of the new dataset $d_{t+1}$ and those of previously encountered datasets $\\{d_1, d_2, ..., d_t\\}$.\n1) Quantifying Overlap Using Cosine Similarity: Let $P_{t+1} = \\{p^{t+1}_1, ..., p^{t+1}_k\\}$ represent the top k principal components of $d_{t+1}$, and $P_i$ the components of an existing dataset $d_i$. Cosine similarity is used to measure alignment between the principal components:\n$S_{jk} = \\frac{p^{t+1}_j p^i_k}{||p^{t+1}_j|| ||p^i_k||}$  (4)\nFor each $p^{t+1}_j$, the maximum similarity score across all components in $P_i$ is recorded. The overlap percentage quantifies the proportion of components in $P_{t+1}$ that align with $P_i$ above a similarity threshold $\\tau$:\n$\\text{Overlap Percentage} = \\frac{\\text{Number of Aligned Components}}{\\text{Total Components in } P_{t+1}}$  (5)\nIf the overlap percentage exceeds $\\tau$ (e.g., 50%), the corresponding prompt $p_i$ is reused with minimal fine-tuning. For tasks with low overlap, a new prompt is initialized to capture the unique characteristics of $d_{t+1}$."}, {"title": "C. Orthogonal Prompt Initialization", "content": "For datasets with minimal subspace overlap, the framework initializes a new prompt in an orthogonal subspace to ensure task-specific independence and prevent interference with previously learned prompts.\n1) Orthogonal Projection: Given the subspaces $\\{P_1, P_2, ..., P_t\\}$ of previously learned tasks, the orthonormal basis V is constructed by concatenating their principal components. The embeddings of $d_{t+1}$, denoted $X_{t+1}$, are projected onto V to compute the aligned components:\n$\\text{Proj}_V(X_{t+1}) = X_{t+1} VV^T$.\nThe orthogonal component is derived as:\n$X_{orth} = X_{t+1} - \\text{Proj}_V(X_{t+1})$.  (7)\nPCA is then applied to $X_{orth}$ to identify its principal components, forming a subspace that captures the unique features of $d_{t+1}$. The new prompt is initialized in this orthogonal subspace to ensure independence from existing tasks.\n2) Training and Adaptation: The orthogonally initialized prompt $P_{t+1}$ is optimized to minimize the task-specific loss. Existing prompts remain frozen and accessible for inference, allowing the framework to leverage previously acquired knowledge while integrating new information. This ensures robust continual learning with strong forward and backward transfer. By combining subspace overlap analysis and orthogonal initialization, the framework balances knowledge reuse for related tasks and independence for novel features, addressing the core challenges of catastrophic forgetting and task-specific adaptation in LLMs."}, {"title": "IV. RESULTS", "content": "We evaluated our framework in domain-incremental and task-incremental learning settings using GPT-2 and DeBERTa-base as base models. For domain-incremental learning, we use five datasets spanning healthcare, scientific literature, and general knowledge, sequentially introducing them to simulate real-world incremental learning. Per-token accuracy measures the framework's ability to generate outputs aligned with task-specific prompts while retaining previously learned knowledge. For task-incremental learning, accuracy is determined by selecting the most likely class from model logits and assessing the model's ability to classify tasks incrementally."}, {"title": "B. Impact of Principal Components and Soft Tokens", "content": "To analyze the effect of principal components in PCA-based subspace selection, we conducted experiments with 100 and 300 components. The results in Fig. 2(d) show that increasing components improves accuracy for domains like healthcare and scientific literature, where richer representations are beneficial. However, for general knowledge tasks, performance remains stable, indicating that fewer components suffice. This underscores the importance of selecting an optimal number of PCA components to balance accuracy and computational efficiency. We also examine the impact of soft tokens. While increasing tokens enhances task-specific performance by introducing more trainable parameters, it slightly reduces forward transfer due to overfitting to domain-specific features. These findings highlight the trade-off between specialization and generalization in continual learning."}, {"title": "C. Domain-Incremental Learning Performance", "content": "The domain-incremental evaluation includes datasets from diverse fields. HealthcareMagic and PubMedQA [18] cover medical question-answering, requiring models to infer responses based on patient queries and biomedical literature. SciQ [19] focuses on scientific reasoning, testing the model's ability to answer science-related multiple-choice questions. OceanBench [20] introduces real-world oceanographic data analysis, challenging the model's adaptability to highly specialized knowledge and TriviaQA [21] evaluates general knowledge comprehension by requiring the model to answer fact-based questions across a broad range of topics. This diverse dataset selection ensures a rigorous assessment of the framework's capacity to adapt across distinct domains while preserving prior knowledge. The sequential training was conducted in the order of the x-axis in Fig. 2(a). We evaluated our framework on forward and backward transfers. Forward transfer in continual learning refers to the model's ability to leverage previously learned knowledge to improve performance on new tasks without additional retraining [22]. Conversely, backward transfer assesses whether learning a new task enhances or preserves performance on previously encountered tasks, ensuring knowledge retention and mitigating catastrophic forgetting. The results in Fig. 2 exhibit strong forward transfer, as demonstrated by improved accuracy on unseen datasets immediately after training on a new domain. For example, fine-tuning on a healthcare dataset significantly enhances performance on scientific literature datasets, leveraging shared semantic structures via subspace-guided prompt initialization. Backward transfer is also robust, with minimal accuracy degradation on earlier datasets after sequential training. Fig. 3(b) shows the forgetting ratio, measured as the percentage drop in accuracy on prior datasets, consistently remains below 5%. This highlights the framework's effectiveness in mitigating catastrophic forgetting.\nWe compare our framework's final accuracy after completing sequential training with the baseline accuracy obtained by fine-tuning the model individually on each dataset. Fig. 3(a) shows that our approach consistently outperforms baseline fine-tuning, demonstrating superior knowledge retention and transfer. For instance, after training on healthcare, scientific, and general knowledge datasets, final accuracy exceeds the baseline by a significant margin. These results validate the efficiency of subspace-guided prompt reuse and orthogonal prompt initialization in balancing task-specific learning and generalization."}, {"title": "D. Task-Incremental Learning Performance", "content": "The framework was also evaluated on diverse NLP tasks, using five SuperGLUE benchmark datasets such as CB (CommitmentBank), RTE (Recognizing Textual Entailment), WiC (Word-in-Context), BoolQ (Boolean Question Answering), and MultiRC (Multi-Sentence Reading Comprehension) [23]. These datasets cover a diverse range of NLP tasks including reasoning over short texts, contextual word sense disambiguation, answering yes/no questions based on unstructured text, and extracting relevant information from multi-sentence passages. This diversity ensures a comprehensive evaluation of the framework's ability to generalize across different linguistic challenges in a continual learning setting. Sequential training is performed to assess both forward transfer (efficient adaptation to new tasks) and backward transfer (retention of previously learned knowledge). Results confirm the framework's flexibility and robustness across varying task objectives. Fig. 4 presents evaluation results for Full Fine-Tuning, PCA-based Continual Learning, and LoRA-integrated Continual Learning. During training, datasets are introduced sequentially, with accuracy measured on both previously trained and unseen datasets after each step. The diagonal values indicate accuracy on the current dataset, while other values represent performance on past and unseen datasets. Full Fine-Tuning achieves the highest accuracy but requires substantial computational resources, fine-tuning 100% of model parameters, making it impractical for scalable continual learning. In contrast, our PCA-based method achieves competitive accuracy while fine-tuning only 0.04% of parameters, ensuring efficiency. Moreover, it fully retains previously learned knowledge, demonstrating robustness in continual learning."}, {"title": "E. Low Rank Adaptation with Prompt Tuning", "content": "We also evaluate a hybrid method integrating PCA-based tuning with LoRA. This tests adaptability to efficient learning strategies while enhancing accuracy without sacrificing knowledge retention. By fine-tuning only 1% of parameters, the PCA + LoRA approach preserves all prior knowledge while improving accuracy, further reinforcing our framework's efficiency. Fig. 5 presents a high-level comparison of task-incremental learning results, showing that PCA-based continual learning retains prior knowledge while achieving superior accuracy compared to the zero-shot performance of the base model. This underscores the robustness of our method in adapting to new tasks without forgetting previously learned information.\nAcross all experiments, our framework maintains a constant training cost, regardless of task count or model size. This is achieved by limiting trainable parameters to the product of soft tokens and PCA components, typically resulting in only a few thousand parameters. For instance, with ten soft tokens and 300 PCA components, trainable parameters account for less than 0.002% of GPT-2's total parameters. This highlights our framework's computational efficiency, making it wellsuited for large-scale models and resource-constrained environments. Overall, our results demonstrate that the proposed framework effectively mitigates catastrophic forgetting while achieving strong forward and backward transfer. By combining subspace-guided prompt reuse with orthogonal prompt initialization, it ensures task-specific adaptation, knowledge retention, and scalability\u2014addressing key challenges in continual learning for large language models."}, {"title": "V. CONCLUSIONS", "content": "We presented a subspace-guided prompt tuning framework for efficient continual learning in large language models by leveraging PCA-based subspace identification. This approach minimizes catastrophic forgetting, facilitates knowledge transfer, and significantly reduces computational overhead. Furthermore, its seamless integration with LoRA showcases its adaptability to hybrid parameter-efficient tuning strategies, achieving improved performance with a minimal number of trainable parameters."}]}