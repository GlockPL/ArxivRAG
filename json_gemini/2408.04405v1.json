{"title": "Probabilistic energy forecasting through quantile regression in reproducing kernel Hilbert spaces", "authors": ["Luca Pernigo", "Rohan Sen", "Davide Baroli"], "abstract": "Accurate energy demand forecasting is crucial for sustainable and resilient energy development. To meet the Net Zero Representative Concentration Pathways (RCP) 4.5 scenario in the DACH countries, increased renewable energy production, energy storage, and reduced commercial building consumption are needed. This scenario's success depends on hydroelectric capacity and climatic factors. Informed decisions require quantifying uncertainty in forecasts. This study explores a nonparametric method based on reproducing kernel Hilbert spaces (RKHS), known as kernel quantile regression, for energy prediction. Our experiments demonstrate its reliability and sharpness, and we benchmark it against state-of-the-art methods in load and price forecasting for the DACH region. We offer our implementation in conjunction with additional scripts to ensure the reproducibility of our research.", "sections": [{"title": "1 INTRODUCTION", "content": "Climate shock and the penetration of renewable energy sources are pivotal issues in the modern energy system, particularly in the DACH region (comprising Germany, Austria, and Switzerland). Recently, the Swiss Federal Office of Energy has published a comprehensive analysis to assess how to secure and produce a cost-efficient energy supply in Swiss Energy Strategy 2050 and Energy Perspectives 2050+. Recently, Switzerland also adopted the long-term goal of climate neutrality, aiming to decrease its energy-building consumption and deploy more renewable energy technologies. In addition to being essential to mitigate climate change, the performance of renewable energy sources and the demand for building energy depend on weather data and energy storage.\nWithin the SURE SWEET energy initiative, supported by the SFOE, the development of future sustainable and robust systems is corroborated by techno-economic models that predict long-term scenarios and pathways, which are resilient to climate shocks [Panos et al. 2023]. These models require a large amount of technical and economic data and their quality influences the reliability of the results, for example, bottom-up techno-economics model [Kannan 2018], which provides hourly prediction, EXPANSE [Trutnevyte 2013], building stock model [N\u00e4geli et al. 2020], macro-economics GEM-3M and life cycle assessment [Luh et al. 2023]. The projection of these models is affected by weather data. For example, the Swiss building stock model designs decarbonisation pathways for different buildings' archetypes, whose isolation and heating performance vary with external temperature. As a result, the prediction of hourly loads by transmission system operators is influenced by the variability and uncertainty of climate factors and is dependent on many parameters of the techno-economics models.\nIn particular, the pathways predicted by SURE models to achieve the Swiss net-zero scenario in conjunction with the RCP pathway 4.5 address the primary issue of reliable energy supply. This is essential because it requires an increase in renewable energy sources to meet annual net electricity demand of 80-100 TWh by 2050, compared to the current 60 TWh SFOE 2022b. One of the factors of such an increase in electricity due to sustainable mobility is up to 22 TWh [Kannan et al. 2022]. To guarantee uninterrupted energy supply, even under extreme weather conditions [Ho-Tran and Fiedler 2024], in SFOE 2022a various scenarios have been examined: dependence on importing electricity from the European market and expansion of technologies that can provide or save electricity in winter, for example wind, alpine photovoltaic, seasonal heat storage or nuclear power.\nIn such energy scenarios, forecasting models are needed to provide reliable energy management and probabilistic projections of socio-economical energy technologies [Zielonka et al. 2023]. In addition, these models serve as a decision support tool for the transmission system operator (TSO), such as SwisseGrid, to determine the balance of reserves [Abbaspourtorbati and Zima 2016] and for policy-makers to develop a transition to sustainable energy sources. Due to the challenges described above, this work aims to investigate probabilistic forecasting to assess the uncertainty of energy supply due to fluctuations in hydroelectric capacity at day frequency, meteorological, and the intermittency of renewable energy sources at high temporal frequency, i.e., at hour resolution.\nIn electricity forecasting state-of-the-art research, the focus has been mostly on point-forecast methods, that is, methods that output a single value for each target timestamp. Point forecasts are usually assessed using well-known criteria such as the root mean squared error (RMSE) and the mean absolute error (MAE). Lately, the electricity forecasting community is shifting towards the probabilistic forecasting framework. The advantage of these methods is that they are more informative than a single-point prediction. [Gneiting et al. 2007] introduces how to assess the quality of probabilistic forecasts by maximizing the sharpness of prediction distributions under calibration constraints. Calibration refers to the statistical consistency between the predicted distributions and the observations, while sharpness refers to the spread of the forecast distributions. Scoring rules assign numerical scores to the forecasts based on the predicted distribution and the value that is materialized. In the assessment of the probabilistic model, the most widely used scores are pinball loss and the continuous ranked probability score (CRPS). [Gneiting 2011] studies the class of loss functions that lead to optimal predictors for the quantiles of a predictive distribution.\nProbabilistic forecasting is gradually becoming an active research area for academia, where different researchers propose parametric and non-parametric models for forecasting specific outputs: wind, solar [Gneiting et al. 2023], prices [Nowotarski and Weron 2018a] or demand [Phipps et al. 2023]. A tutorial review on probabilistic load forecasting by [Hong et al. 2016] covers forecasting techniques, auxiliary methodologies, evaluation metrics, and good sources of reference. Another review paper by [Nowotarski and Weron 2018a] presents measures, tests, and guidelines for the rigorous use of probabilistic electricity forecasting methods. Another study, see [Van der Meer et al. 2018], provides a broad overview of probabilistic forecasting, specifically, the authors focus on solar power and load forecasting. In [Ziel and Steinert 2018] an extensive literature review has been carried out by classifying electricity price forecasting papers according to various attributes such as prediction horizon, data used, predictors, accuracy measures, and models proposed.\nThis article addresses probabilistic forecasting by adopting the kernel quantile regression (KQR) method within the RKHS framework. This method was introduced in [Takeuchi et al. 2006] and further investigated in [Li et al. 2007; Sangnier et al. 2016; Zhang et al. 2016; Zheng 2021]. The method offers a non-parametric and non-linear way to provide probabilistic forecasts. The main contribution of this article is to perform a probabilistic forecast with KQR for Swiss, Austrian, and German energy systems, where the data are extracted from ENTSO-E Transparency Platform, SECURES-Met [Formayer et al. 2023] and C3S Energy [Dubus et al. 2023], which is designed to assess the impacts of climate variability and climate change on the energy sector. The probabilistic forecast with KQR has also been validated in the GEFCom test case, where our Python-based open-source implementation compares favourably with the top teams in the probabilistic forecast of electricity load and price. Kernel quantile regression has received little attention in the energy forecasting literature. [He and Li 2018] employs it to forecast wind power generation and compares it to a quantile regression neural network. [Moreira et al. 2016] uses it to forecast electricity prices for the Iberian electricity market. In that study, kernel quantile regression is compared with linear quantile regression, neural network quantile regression, random forest, and regression boosting. [He et al. 2017] studies the choice of kernel functions in the context of short-term load forecasting. However, no research has yet been done that thoroughly compares KQR to other state-of-the-art methods in the specific context of medium-term electricity load forecasting. Therefore, our second contribution is applying kernel quantile regression to the medium load forecasting setting, see section 4, sticking to best practices and guidelines of popular literature reviews in the field of probabilistic electric load forecasting (PLF) [Hong and Fan 2016; Lago et al. 2021; Nowotarski and Weron 2018b]. We implemented our version of KQR since there were no implemented Python packages available. We made our model class inherit from the scikit template classes BaseEstimator and RegressorMixin; by doing so, our KQR is compatible with useful scikit learn functionalities such as grid search, cross-validation, and metric scorers; that is, our method is compatible with the scikit-learn API, see [Pedregosa et al. 2011]. Sharing our implementation with the research community is the last contribution of this paper."}, {"title": "1.1 Outline", "content": "The rest of this article is structured as follows. In Section 2, we review quantile regression, in particular, kernel quantile regression. In Section 3, KQR is benchmarked against other popular quantile regressor models on data extracted from ENTSO-E Transparency Platform. Following, a kernel wise comparison is carried out on the SECURES-Met data [Formayer et al. 2023]. Furthermore, we evaluate the performance of KQR in the context of the challenge GEFCom2014 in Section 4. Finally, in Section 5, we conclude the article."}, {"title": "2 KERNEL QUANTILE REGRESSION", "content": "We first briefly review quantile regression here and then cover the details regarding kernel quantile regression. Quantile regression (QR) is a method used in various fields, such as econometrics, social sciences, and ecology, to analyse the empirical distribution. Estimates a target quantile of the response variable y based on a predictor vector, x. QR is more robust to outliers in the data compared to the usual least-squares regression. It is also suitable for cases where there is heteroscedasticity in the errors. Additionally, using a series of quantile values provides a better description of the entire distribution than a single value, such as the mean. [Koenker and Bassett Jr 1978] showed that the pinball loss function\n$P_q(u) = \\begin{cases}\nqu & \\text{if } u \\geq 0, \\\\\n-(1-q)u & \\text{if } u < 0,\n\\end{cases}$\n(2.1)\ncan recover a target quantile of interest, q where 0 \u2264 q \u2264 1. We refer the reader to Appendix A for further details on the same.\nGiven empirical samples $(x_i, y_i)_{i=1}^n$, with $x_i e R^d$, $y_i \\in R$, quantile regression [Koenker 2005; Koenker and Hallock 2001] seeks to estimate the q-th conditional quantile of the response variable y as a linear function of the explanatory variables xi by solving the"}, {"title": "3 NUMERICAL EXAMPLES", "content": "All the numerical experiments have been performed in Python on a 3.2 GHz 16 GB Apple M1 Pro. Since KQR involves a quadratic programming problem, cp. (2.11), we used the interior-point method implemented in the cvxopt library to solve it. For a detailed description of cvxopt solvers and algorithms available, we refer the interested reader to the manual [Vandenberghe 2010]. All scripts are made publicly available at the Github repository along with the cleaned data."}, {"title": "3.1 Energy charts case study", "content": "In this case study, KQR is compared against popular quantile regressor models, the kernel of choice here is the Laplacian equipped with the Manhattan distance (Absolute Laplacian). The dataset for this case study comes from Energy charts which retrieves data from the ENTSO-E Transparency Platform. In predicting the national load we selected the following variables.\n\u2022 Weather temperature;\n\u2022 Wind speed;\n\u2022 Hour;\n\u2022 Month;\n\u2022 Is holiday: a binary variable for holidays where holiday=1, working day=0;\n\u2022 Day of week: an ordinal categorical variable corresponding to the day of the week, e.i. Monday=0,... Sunday=6;\nWe used the entire 2021 data as the training sample for fitting our models, and we tested them and computed their scores on the 2022 data. The results for Switzerland are reported in Table 1 and figure 2 while results for Germany can be found in Table 2 and figure 3. The error metric used is the pinball loss, scaled by the average load magnitude of each country. It is observed that the KQR model performs marginally better than other QR models and achieves results comparable to those of the GBQR. The superior performance is demonstrated in terms of CRPS to its competitors, quantile regressors, in predicting electricity load quantiles."}, {"title": "3.2 SECURES-Met case study", "content": "Combining the SECURES-Met data (predictors) [Formayer et al. 2023] and the load data from ENTSOE, we carried out a comparison between different classes of kernels. The kernel functions considered are: Gaussian RBF, Laplacian, Matern 0.5, Matern 1.5, Matern 2.5, linear, periodic, polynomial, sigmoid, and cosine. We used time series cross-validation to evaluate each model's performance, encompassing hyperparameter optimisation and feature selection. In Appendix C, Figure 6 shows the cross-validation process for the Absolute Laplace and Gaussian kernels. The analysis illustrates the criteria and scores to determine the optimal regularisation term and the hyperparameters of the kernel.\nDetails regarding the hyper-parameter selection for the different kernels can be found in the implementation here. The SECURES-Met data consist of historical data up to the end of 2020 while from 2021 onward, the data consist of forecasts modelled by the European Centre for Medium-Range Weather Forecasts (ECMWF). Therefore, we used the entire data of 2021 as the training set and then tested our kernels on the 2022 data. Note that there are two types of prediction for the data from 2021 onward, one for each of the emission scenarios RCP4.5 and RCP8.5. In this study, we restrict our attention to the RCP4.5 data, since it is part of SURE-SWEET's scenarios.\nThe predictors making up the dataset follow:\n\u2022 Direct irradiation: direct normal irradiation.\n\u2022 Global radiation: mean global radiation.\n\u2022 Hydro reservoir: daily mean power from reservoir plants in MW.\n\u2022 Hydro river: daily mean power from run of river plants in MW.\n\u2022 Temperature: air temperature.\n\u2022 Wind potential: potential wind power production.\nIn Appendix C, we have reported the table of quantiles and CRPS scores. Table 7 shows the results for Switzerland, Table 8 for Germany and Table 9 for Austria.\nThat study provides evidence of the superiority of the Gaussian kernel over the linear and polynomial kernels. In our research, we considered a larger set of kernels. From numerical experiments, the Absolute Laplacian kernel quantile has demonstrated superior performance to other Matern family kernels. In addition, a comprehensive cross-validation process for quantile 0.5 has been conducted to determine optimal hyperparameters and ridge regression parameters to prevent overfitting. The method has also been validated on an extended Secures Met dataset, which includes hydraulic capacity as a variable and technological-economic projections of the energy supply. By including high-resolution hours as a categorical variable, we have achieved accurate predictions and narrower confidence bounds for the Absolute Laplacian and Gaussian RBF Kernels as shown in Figure 8 and Figure 9. The superior performance of the Absolute Laplacian over the Gaussian RBF is largely attributable to the robustness of the Manhattan distance compared to the Euclidean distance [Aggarwal et al. 2001]. The empirical demonstration is provided by comparing the covariance kernels in Figure 7. To complete our analysis, Figures 2 and 3 show a satisfactory prediction and narrow confidence for Switzerland and Germany even when the method slightly fails to approximate the real value."}, {"title": "4 GEFCOM2014 CASE STUDY", "content": "We now apply KQR to the setting of probabilistic load and price forecasting. We use the GEFCom2014 [Hong et al. 2014] data to carry out our experiments. The GEFCom is a series of competitions that have been created with the intent of improving forecasting practices, addressing the gap between academia and industry, and fostering state-of-the-art research in the field of energy forecasting [Hong et al. 2016]. The GEFCom edition of 2014 consisted of four tracks, all involving probabilistic forecasting. The four tracks were load, price, wind power, and solar power forecasting. The reason for choosing the GEFCom2014 data is that it is an established benchmark in energy to compare against other valid methods. The data is freely available on Dr. Tao Hong's blog. Furthermore, a clear comparison can be performed due to the availability of the scores of each method for the load and price track. The score measure of the competition was the pinball loss, see section 2.1, averaged over the 99 quantiles, q \u2208 {i/100}i=99 1. Finally, to recreate the setting of GEFCom2014 and to provide a fair comparison, we adhere rigorously to the rules of the competition. Next, we study the performance of KQR in the load and price tracks. The kernel adopted throughout this study are the Gaussian RBF and the Absolute Laplacian kernel."}, {"title": "4.1 GEFCom2014 load track", "content": "This track was concerned with forecasting hourly loads of an anonymous US utility. The dataset provided at the start of the competition consisted of 69 months of load data and 117 months of weather data, both at hourly frequency. In this particular track, the challenge was to predict the load for the next month without the availability of weather temperature forecasts. Therefore, the primary task was to first accurately predict the weather and temperatures and then model the load accordingly. Since there were no attributes available for humidity or wind speed, we chose to predict weather temperatures by aggregating historical temperature data across different dimensions such as day, month, and hour. Then we proceeded with building KQR models for the load; we chose the following predictors.\n\u2022 Day: the number of the day.\n\u2022 Hour.\n\u2022 Day of week: an ordinal categorical variable for the day of the week.\n\u2022 Is holiday: a dummy variable for holidays.\n\u2022 w avg: average of weather temperatures across all the 25 stations.\nWe built 12 models, one for each month, with each task model trained on the historical data of the month associated with it. Table 3 reports our results for the load track. The top teams for the load forecasting track were Tololo, Adada, Jingrui(Rain) Xie, OxMAth, E.S. Managalova, Ziel Florian, and Bidong Liu; for a breakdown of the attributes of each method, see [Hong et al. 2016, Table 6].\nWe conclude this section with a visualisation of the 90% confidence interval forecast by our model for task number 9, that is the prediction for June, see Figure 4."}, {"title": "4.2 GEFCom2014 price track", "content": "In this track, the objective was to forecast electricity prices for the next 24 hours on a rolling basis. The dataset provided consisted of 2.5 years of hourly prices and zonal and system load forecasts. The predictors fed to our KQR models are:\n\u2022 Day;\n\u2022 Hour;\n\u2022 Forecasted total load;\n\u2022 Forecasted zonal load.\nLike above, all models were trained on the historical data of the associated month.\nOur results for this track are reported in table 5. In this track, the top entries came from the teams: Tololo, Team Poland, GMD, and C3 Green Team; for a breakdown of each method's attributes, see [Hong et al. 2016, Table 8]. Finally, our probabilistic prediction for the 13th July 2013 zonal price at the 90% confidence interval is visualised in Figure 5."}, {"title": "5 CONCLUSION", "content": "In this article, we investigate a non-parametric probabilistic method, the kernel quantile regression, for estimating quantiles of load. To our knowledge, this method has not been explored before for load prediction. We show its effectiveness through several numerical tests on DACH data (Germany, Austria and Switzerland), illustrating that it performs competently compared to other well-known quantile regression techniques and exceeds the point regression results of GEFCom2014. In addition to these numerical experiments, we extend the test case of GEFCom2014 considering additional explanatory variables in hydrocapacity energy storage. We observe that KQR shows favourably and can forecast the medium-term horizon of the Secures-Met dataset. However, the forecasting for the short-term horizon has been demonstrated in the GEFCom2014 price track. The tuning of hyperparameters along with the ridge parameter was carried out using cross-validation, and we presented comprehensive analysis to support the results. In our investigation of kernel functions, we focus on evaluating the Absolute Laplace kernel in comparison to the RBF Gaussian Kernel during the kernel selection procedure. The results of this study confirm the durability of our selection in terms of precision of prediction. Finally, we provide an open-source implementation of KQR integrated with the most popular tools in the community. This investigation will serve as a case study for an uncertainty quantification model to predict the impact of climate shocks on pathways, which will be further refined in scenarios computed by the SURE SWEET energy models. Further investigation with different choices and combinations of kernels as well as a detailed comparison with gradient booster methods remains a future research area."}, {"title": "A PINBALL LOSS FUNCTION FOR QUANTILE REGRESSION", "content": "In this section, we show why minimizing the pinball loss function leads to estimates of the quantile. This section has been sourced from [Koenker 2005] and hence, we would like to refer to the above reference for further details on the same. For a real-valued random variable X with distribution function F(\u00b7), the q-th quantile is defined as\n$Q(q) := inf \\{x \\in R : F(x) \\geq q\\} \\text{ for } 0 \\leq q\\leq 1.$\n(A.1)\nFor a continuous distribution function F(\u00b7), the quantile becomes just the inverse, i.e. $Q = F^{-1}$. For example, $q = 0.5$ defines the median of the distribution of X. The quantiles arise from a simple optimization problem that is fundamental to all that follows. Consider a simple decision-theoretic problem: a point estimate is required for a random variable with (posterior) distribution function F. If the loss is described by the piecewise linear pinball loss, cp. (2.1), consider the problem of finding $x$ to minimize the expected loss. We seek to minimize\n$E [\\rho_q(X-\\hat{x})] = \\int_{\\hat{x}}^{\\infty} q(x-\\hat{x}) dF(x)\n- \\int_{-\\infty}^{\\hat{x}} (1-q) (\\hat{x} - x) dF(x).$\n(A.2)\nTo find the optimal $\\hat{x}$, we consider the first-order conditions by differentiating problem A.2 with respect to $\\hat{x}$ and setting it to zero:\n$0 = -q \\int_{\\hat{x}}^{\\infty} dF(x)+(1-q) \\int_{-\\infty}^{\\hat{x}} dF(x) = F(\\hat{x}) - q$\nSince $F(\\cdot)$ is a monotone function, any element of {$x$: $F(x) = q$} minimizes expected loss. When the solution is unique, $x = F^{-1}(q)$, otherwise, we have an \"interval of $q$ quantiles\" from which the smallest element must be chosen to adhere to the convention that the empirical quantile function be left-continuous. When $F(\\cdot)$ is replaced by the empirical distribution function\n$F_n(x) = \\frac{1}{n} \\sum_{i=1}^n I\\{X_i \\leq x\\},$\n(A.3)\nwe may still choose $x$ to minimize expected loss:\n$\\int \\rho_q(x-\\hat{x}) dF_n(x) = \\sum_{i=1}^n \\frac{1}{n} \\rho_q(x_i - \\hat{x})$\n(A.4)\nwhere $x_i's$ are now assumed to be generated from the independently and identically distributed random variables $X_i \\sim F(\\cdot)$; doing so now yields the q-th sample quantile. When $q n$ is an integer there is again some ambiguity in the solution, because we really have an interval of solutions, {$x$: $F_n(x) = q$}, but this is of little practical consequence."}, {"title": "B ESTIMATOR DERIVATION FOR KERNEL QUANTILE REGRESSION", "content": "The goal is to solve the optimization problem, cp. (2.8)\n$\\text{argmin} \\, C \\sum_{i=1}^n \\rho_q(y_i- (\\langle w, \\phi(x_i) \\rangle + b)) + \\frac{1}{2}||w||$\n(B.1)\n$w\\in H, b\\in R$\nwith the characterization $f (x) = (\\langle w, \\phi(x) \\rangle + b )$. Minimizing $||w||_H$ is equivalent to minimizing the regularizer, see [Sch\u00f6lkopf and Smola 2018, Section 2.2.4]. Using the definition of the pinball loss function, cp. (2.1), we have the problem\n$\\text{argmin} \\, C \\sum_{i=1}^n q(y_i- (\\langle w, \\phi(x_i) \\rangle + b))+\n(1 - q)(-y_i + (\\langle w, \\phi(x_i) \\rangle + b)) + \\frac{1}{2}||w||$\n(B.2)\n$w\\in H, b\\in R$\nIntroducing the slack variables $\\xi_i, \\xi^*_i, 1 \\leq i \\leq n$, we can rephrase Problem B.2 as the following constrained optimization problem, see [Hwang and Shim 2005; Takeuchi et al. 2006]\n$\\text{argmin} \\, C \\sum_{i=1}^n q\\xi_i +(1-q)\\xi^*_i + \\frac{1}{2}||w||$\n(B.3)\n$w\\in H, b, \\xi_i, \\xi^*_i \\in R$\ns.t. $\\, y_i - (\\langle w, \\phi(x_i) \\rangle + b ) \\leq \\xi_i, \\\\ -y_i + (\\langle w, \\phi(x_i) \\rangle + b ) \\leq \\xi^*_i , \\\\ \\xi_i \\geq 0, \\xi^*_i \\geq 0.$\nNow, by the representer theorem, see [Sch\u00f6lkopf et al. 2001], any $w \\in H$ that minimizes Problem B.1 can be written as\n$w = \\sum_{i=1}^n a_i\\phi(x_i) = w(x) = \\sum_{i=1}^n a_iK (x,x_i).$\nUsing (2.5), we have that $\\langle w, \\phi(x_j) \\rangle = w(x_j) = \\sum_{i=1}^n a_i k (x_i, x_j)$ for $1 \\leq j \\leq n$. Hence, denoting the coefficient vector as a := $[a_i]_{i=1}^n \\in R^n$, we have that\n$[\\langle w,\\phi(x_j) \\rangle ]_{i=1}^n = Ka, ||w||^2 = a^T Ka.$\n(B.4)\nUsing the notations $y := [y_i]_{i=1}^n \\in R^n, \\xi := [\\xi_i]_{i=1}^n \\in R^n, \\xi^* := [\\xi^*_i]_{i=1}^n \\in R^n$, we have the equivalent problem in matrix notation\n$\\text{argmin} \\, C q 1^T \\xi + C (1-q) 1^T\\xi^* + a^T Ka$\n$\\alpha, \\xi, \\xi^*\\in R^n, b \\in R$\n(B.5)\ns.t.\n$\\, y -Ka- b 1 \\leq \\xi, \\\\ - y +Ka + b 1 \\leq \\xi^*, \\\\ \\xi\\geq 0, \\xi^* \\geq 0.$\nThrough the Lagrange multipliers and the KKT conditions (see [Boyd and Vandenberghe 2004]), Problem B.5 can be solved to its equivalent dual formulation, see [Takeuchi et al. 2006; Xu et al. 2015] for more details. We can write the Lagrangian as\n$L(a, b, \\xi, \\xi^*, \\alpha, \\alpha^*, \\nu, \\nu^*) := C q 1^T\\xi + C (1-q) 1^T \\xi^* + a^T Ka\n- \\alpha^T (\\xi - y + Ka + b 1) - (\\alpha^*)^T (\\xi^* + y - Ka - b 1) - \\nu^T\\xi - (\\nu^*)^T\\xi^*,$\n(B.6)\nwith the positivity constraints $\\alpha, \\alpha^*, \\nu, \\nu^* \\geq 0$. We proceed to derive the dual function by minimizing the Lagrangian\n$g(\\alpha, \\alpha^*, \\nu, \\nu^*) = \\underset{\\xi, \\xi^*, b \\in R}{\\text{inf}} L(a, b, \\xi, \\xi^*, \\alpha, \\alpha^*, \\nu, \\nu^*).$\n(B.7)\nSetting the partial derivatives to zero, we have\n$\\frac{\\partial L}{\\partial w}= 0 \\Rightarrow Ka = K(\\alpha - \\alpha^*), \\qquad \\frac{\\partial L}{\\partial b}= 0 \\Rightarrow (\\alpha - \\alpha^*)^T 1 = 0, \\\\\\frac{\\partial L}{\\partial \\xi}= 0 \\Rightarrow Cq1 = \\alpha + \\nu, \\qquad \\frac{\\partial L}{\\partial \\xi^*}= 0 \\Rightarrow C(1 - q) 1 = \\alpha^* + \\nu^*.$\n(B.8)\nSubstituting the conditions into B.6, we obtain the dual function as\n$g(\\alpha, \\alpha^*, \\nu, \\nu^*) = - \\frac{1}{2} (\\alpha - \\alpha^*)^T Ka + (\\alpha - \\alpha^*)^T y, \\\\\\text{s.t. } (\\alpha - \\alpha^*)^T 1 = 0, \\\\ \\alpha \\leq Cq, \\alpha^* \\leq C(1-q), \\\\ \\nu \\geq 0, \\nu^* \\geq 0.$\n(B.9)\nIn terms of the coefficient vector a, we can consider the dual optimization problem which turns out to be (after switching signs)\n$\\underset{a \\in R^n}{\\text{argmin}} \\frac{1}{2} a^T Ka - a^T y$\ns.t. $ C (q-1) 1 \\leq \\alpha \\leq Cq \\\\ \\alpha^T 1 = 0.$\n(B.10)\nFrom the constraint conditions of problem B.9 and the expression of w cp. (2.9), we have that the optimal w is given by the coefficients\n$w = \\sum_{i=1}^n \\alpha \\phi(x_i),$\n(B.11)\nwhere $\\alpha^* = [a]_{i=1}^n$ is the solution of Problem B.10. Finally, the data points for which $\\alpha \\notin \\{C (q-1), Cq\\}$ are called the support vectors. The intercept term b can be calculated using the fact that f (xi) = Yi for the set of support vectors, see [Takeuchi et al. 2006; Xu et al. 2015] for more details. The latter holds due to KKT conditions on Problem B.10."}]}