{"title": "Learning Camouflaged Object Detection from Noisy Pseudo Label", "authors": ["Jin Zhang", "Ruiheng Zhang", "Yanjiao Shi", "Zhe Cao", "Nian Liu", "Fahad Shahbaz Khan"], "abstract": "Existing Camouflaged Object Detection (COD) methods rely\nheavily on large-scale pixel-annotated training sets, which are both time-\nconsuming and labor-intensive. Although weakly supervised methods of-\nfer higher annotation efficiency, their performance is far behind due to the\nunclear visual demarcations between foreground and background in cam-\nouflaged images. In this paper, we explore the potential of using boxes\nas prompts in camouflaged scenes and introduce the first weakly semi-\nsupervised COD method, aiming for budget-efficient and high-precision\ncamouflaged object segmentation with an extremely limited number of\nfully labeled images. Critically, learning from such limited set inevitably\ngenerates pseudo labels with serious noisy pixels. To address this, we\npropose a noise correction loss that facilitates the model's learning of\ncorrect pixels in the early learning stage, and corrects the error risk gra-\ndients dominated by noisy pixels in the memorization stage, ultimately\nachieving accurate segmentation of camouflaged objects from noisy la-\nbels. When using only 20% of fully labeled data, our method shows\nsuperior performance over the state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Camouflaged Object Detection (COD) aims to detect and segment objects that\nblend seamlessly into their environments, presenting a significant challenge due\nto the need to counter sophisticated camouflage tactics and distinguish subtle\ndifferences between objects and their surroundings. Recent advances in COD\nhave been driven by the availability of abundant segmentation\nlabels. However, the labeling process for camouflaged objects is extremely labor-\nintensive, requiring about 60 minutes per image [9], which poses a major obstacle\nto this field's development. This challenge has led to a growing trend towards\nexploring Weakly Supervised COD (WSCOD) methods [2, 14, 22], utilizing sim-\npler annotations such as points [22], scribbles [14], and boxes [21], to potentially\nreduce labeling costs. Despite these efforts, the high similarity between the fore-\nground and background in camouflaged images means that these methods still\nlag far behind Fully Supervised COD (FSCOD) methods in performance.\nMotivated by the above annotation properties, we explore the potential of\nutilizing box annotations as prompts for camouflaged object segmentation. Un-\nlike points and scribbles, box annotations offer rich object information and are\nas cost-effective to provide as image-level and point-level annotations. We posit\nthat using boxes as prompts offers reliability by 1) masking complex backgrounds\nand reducing the level of camouflage, and 2) indicating the approximate location\nof the object, thereby simplifying the model's search process for camouflaged\nobjects. Thus, we formulate a new practical training scheme, Weakly Semi-\nSupervised Camouflaged Object Detection (WSSCOD), with box as prompt.\nIn the WSSCOD task, we aim to achieve budget-efficient and high-performance\ncamouflaged object segmentation using a small amount, such as 1% number of\ntotal training set, of pixel-level annotations and corresponding box prompts.\nWSSCOD, as depicted in Fig. 2, utilizes boxes as prompts to mask com-\nplex, similar backgrounds, delineating proposals for camouflaged objects. This\nmethod distinguishes camouflaged objects from their surroundings by focusing\non the proposals, enabling the model to concentrate on the fine segmentation of\nobject details rather than spending extra time searching for camouflaged objects\nfirst. Following this, we merge these proposals with the complete image to create\ncomplementary branches. This strategy reduces the impact of imprecise box lo-\ncations on the model's decision boundaries. Ultimately, under the supervision of\npixel-level annotations, a proposed COD model is trained with the complemen-\ntary information, enabling it to generate high-quality pseudo labels with clear\ndetails for the remaining 99% of the images.\nMeanwhile, on an extremely limited amount (such as 1%) of fully labeled\ndata, the network often fails to represent the overall data distribution, resulting\nin rough and noisy pseudo labels. Moreover, when training with such noisy la-\nbels, we observe a distinct phenomenon: Initially, in the \u2018early learning phase',\nthe model's learning direction is mainly influenced by the correct pixels. How-\never, as training advances to the 'memorization phase', the gradient direction is\ngradually influenced by noisy pixels, which heavily mislead the model's learning\nand ultimately result in severe false negative and positive predictions. This phe-\nnomenon has also been reported in the field of classification [38, 42]. However,\nthe manifestation of this phenomenon in COD differs from that in classification\nin the following aspects: 1) Unlike classification tasks where noise exists in only\nsome samples, noisy pixels exist in every pseudo label in WSSCOD, and they\nare widespread in FSCOD training labels. 2) There exists a spatial correlation\namong noisy pixels and between noisy pixels and correct pixels in the pseudo\nlabels, and it is advantageous to use the spatial dependence to suppress noise.\nTo cope with this limitation, we advocate the use of a newly proposed loss\nfunction \\(L_{NC}\\) (Noise Correction Loss) to learn to segment camouflaged objects\nfrom noisy labels. \\(L_{NC}\\) is able to handle different learning objectives in both the\nearly learning and the memorization phases: During the early learning phase,\n\\(L_{NC}\\) adapts to different fitting processes brought by different noise rates and\naccelerate the model's convergence to the correct pixels. Importantly, in the\nmemorization phase, \\(L_{NC}\\) forms a unified risk gradient for different predictions,\nmaintaining the correct learning direction on up to 50% incorrect noisy pseudo\nlabels, thereby aiding the model in effectively discerning visual demarcations.\nFurthermore, considering the prevalent noise issue in the COD training sets, \\(L_{NC}\\)\nalso shows superior performance in WSCOD and FSCOD methods compared to\ntheir used losses. We argue that \\(L_{NC}\\) poses a major contribution, as\nprevious segmentation work has paid less attention to noisy labels,\nespecially in the COD task, but where noisy labels occur more easily.\nIn summary, the main contributions of this paper are threefold:\nFacing with the time-consuming and labor-intensive problem of annotating\nfor COD tasks, we propose a cost-effective and high-performance weakly\nsemi-supervised training scheme, and exploit the potential of box annotation\nas an economically accurate prompt.\nWe propose noise correction loss to improve the model's learning of the\npseudo labels generated in WSSCOD. In the early learning and memorization"}, {"title": "2 Weakly Semi-Supervised Camouflaged Object\nDetection", "content": "Task Definition. We introduce a novel training protocol named Weakly Semi-\nSupervised Camouflaged Object Detection (WSSCOD), which utilizes boxes as\nprompts to generate high-quality pseudo labels. WSSCOD primarily leverages\nbox annotations, complemented by a minimal amount of pixel-level annota-\ntions, to generate high-accuracy pseudo labels. Specifically, given the training\nset, D, that is divided into two subsets: \\(D_m = \\{X_m,F_m,B_m\\}_{m=1}^M\\), contain-\ning pixel-level annotations \\(F_m\\), box annotations \\(B_m\\) and training images \\(X_m\\).\n\\(D_n = \\{X_n, B_n\\}_{n=1}^N\\), containing only box annotations and images, where M + N\nrepresents the number of training set. First, we train an auxiliary network,\nANet, using the dataset \\(D_m\\), where \\(B_m\\) serves as an auxiliary prompt for cam-\nouflaged objects, and \\(F_m\\) supervises the generation of pseudo labels. Afterward,\nusing the trained ANet and the dataset \\(D_n\\), we predict its pseudo labels, de-\nnoted as \\(W_n\\). Finally, we construct a weakly semi-supervised dataset \\(D_t\\) using\nsets \\(\\{X_m, F_m\\}_{m=1}^M\\) and \\(\\{X_n, W_n\\}_{n=1}^N\\), and train our proposed primary network,\nPNet, which, like other COD models, takes only images as input. The different"}, {"title": "2.1 Auxiliary Network", "content": "Segmenting camouflaged objects solely based on the box often leads to inaccura-\ncies, as the information within the box is not always reliable [41]. Therefore, as\nillustrated in Fig. 3, we develop a simple and effective COD model for exploiting\nthe complementarity between the RGB image and the proposals, but do not\nconsider the model as a major contribution. Given an RGB image \\(x_m\\) and an\nobject box \\(b_m\\), we multiply them as the input proposals \\(b_m\\) for the box branch\nBB encoder, and input \\(x_m\\) into the image branch IB encoder.\nEncoder. We use two ConvNeXt [24] as encoders \\(E()\\) in ANet to obtain\nmulti-scale features for different inputs. Given the input images \\(\\{x_m,b_m\\} \\in\\)\n\\(IR^{3 \\times H \\times W}\\), we can obtain the multi-scale features \\(\\{F_i^I\\}_{i=1}^4\\) and \\(\\{F_i^B\\}_{i=1}^4\\), respec-\ntively, from the branches IB and BB with the corresponding sizes of \\(\\{\\frac{H}{2^{i+1}},\\frac{W}{2^{i+1}}\\}\\)\nand channels \\(\\{C_1, C_2, C_3, C_4\\}\\). Following established practices [11, 14, 39, 40], we\nadjust all features to the same number of channels C = 64 using 3\u00d73 convolu-\ntions for consistency across multi-level features. In addition, we apply channel\nconcatenation to features \\(F_i^I\\) and \\(F_i^B\\) and then send them to ASPP [1] to ob-\ntain deep image representations. And the initial mask \\(m_{AN} \\in R^{1 \\times H \\times W}\\) from the\nrepresentations is generated through a 3\u00d73 convolution. Subsequently, features\n\\(F_i^I\\) and \\(F_i^B\\) are respectively fed into the Frequency Transformer FT to capture\nthe details and deep semantics of the image, which facilitates the discriminator's\nrecognition of camouflaged objects."}, {"title": "Frequency Transformer", "content": "Drawing inspiration from FDNet [43], we em-\nploy the Discrete Wavelet Transform DWT(\\(\\cdot\\)) to extract both low-frequency and\nhigh-frequency components from multi-scale features. This approach is instru-\nmental in revealing more intricate object components in camouflaged scenes by\nleveraging the frequency domain information \\(F_d\\), enhancing the understanding\nof such complex visual environments. Taking \\(F_i^x\\) as input\n\\[F_{hh}^i, F_{hl}^i, F_{lh}^i, F_{ll}^i = DWT(cat(up(F_i^x)),\\]\nwhere cat(\\(\\cdot\\)) indicates channel-wise concatenation, and up(\\(\\cdot\\)) is the up-sampling\noperation. In the frequency domain features, the subscripts h and l denote the\nextraction of high-frequency and low-frequency information, respectively, in the\nhorizontal and vertical directions. To efficiently process the obtained frequency\ndomain information and spatial domain features, we adopt an adaptive nonlinear\nfusion approach \\(\\Psi_w(F, F_d)\\), where w is the learnable parameters that adjusts the\ndegree of fusion between the two adaptively. We accomplish this fusion of shal-\nlow features \\(F_1, F_2\\) with the high-frequency component \\(F_{hh}^i\\) separately, and\nthe integration of deep features \\(F_3, F_4\\) with the low-frequency component \\(F_{ll}^i\\).\nOther components such as \\(F_{hl}^i\\) and \\(F_{lh}^i\\) are usually not used. In FT, we rep-\nresent \\(\\Psi()\\) using successive convolution and channel concatenation. For easier\ndescription, the total steps in FT are represented by \\(\\Phi_f()\\), with f specifying the\ntype of input features. In a similar manner, \\(F_i^I\\) and \\(F_i^B\\) can be obtained through\n\\(\\Phi_f(F_i^I)\\) and \\(\\Phi_f(F_i^B)\\). Before proceeding with the decoding, we also perform the\nadaptive fusion of \\(F_i^I\\) and through \\(\\Psi_w()\\), as \\(F_i^K = \\Psi_w(F_i^I, F_d^i)\\).\nReverse Fusion Decoder. We design a reverse fusion decoder to complete\nthe convergence of multi-level features. Given the features \\(\\{F_i^K\\}_{i=1}^4\\) and the\nmask \\(m_{AN}\\), we accomplish the fusion of multi-level features in the UNet manner.\nMeanwhile, we integrate a reverse mask, associating the background with difficult\nareas or noisy pixels in COD, amplifying the differences between them and the\ncorrect pixels, and correcting the model's learning of difficult areas, as\n\\[p_{AN} =\\begin{cases}\n\\Psi_w(\\Psi_w(F_4^k, up(m_{AN})), up(Rev(m_{AN}))) + up(m_{AN}), k = 4\\\\\n\\Psi_w(\\Psi_w(F_k^k, up(p_{i+1})), up(Rev(p_{i+1}))) + up(p_{i+1}), k \\in \\{3,2,1\\},\\end{cases}\\]\nwhere \\(Rev(p_{k+1}) = -1 \\times \\sigma(p_{k+1}) + 1\\), \\(\\sigma\\) is the sigmoid function. \\(\\{p_{AN}\\}_{i=1}^4 \\in\\)\n\\(IR^{1 \\times H \\times W}\\) and \\(m_{AN}\\) are the predictions, in which \\(p_{AN}\\) is the main output of ANet.\nThe decoding process is defined as \\(\\Pi()\\), which means \\(p_{AN} = \\Pi(F_k^k, m_{AN})\\)."}, {"title": "2.2 Primary Network", "content": "With the pretrained ANet, we predict the pseudo segmentation labels \\(W_n\\) by\nusing the image set \\(\\{X_n\\}_{n=1}^N\\) with the corresponding box annotations \\(B_n\\), gen-\nerating the training dataset \\(D_n = \\{X_n, W_n\\}\\) for the primary network PNet.\nAdditionally, to align with existing methodologies and to maintain a consis-\ntent number of training images, we integrate the fully labeled dataset \\(D_m =\\)\n\\(\\{X_m, F_m\\}_{m=1}^M\\) into \\(D_n\\) to form the total training dataset \\(\\{D_t\\}_{i=1}^{M+N}\\).\nIn terms of model configuration, PNet retains the same modules as ANet.\nHowever, a key difference is that PNet employs a single-stream structure, where\nonly the image is input. As shown in Fig. 3, we use only the Image Branch and\nthe Decoder in PNet. Specifically, in the ASPP and FT stages, there is no\nchannel concatenation with features from another branch. Instead, features are\ndirectly fed from the backbone network into the ASPP, and after passing through\nFT, they go directly into the decoder. Given that the image \\(x_t\\) comes from \\(D_t\\),\nthe process of PNet is as \\(F_t^I = E(x_t)\\) and \\(p_{AN} = \\Pi(\\Phi_t(F_t^I), ASPP(F_t^I))\\)."}, {"title": "3 Noise Correction Loss", "content": "Training ANet with a very small amount of data poses a challenge in accu-\nrately capturing the distribution of the entire dataset, resulting in severe false\nnegative and positive noisy pixels in the generated pseudo labels. When training\non such noisy labels with traditional losses like Cross-Entropy (CE) and Inter-\nsection over Union (IoU), the bias introduced by the noisy pixels often leads\nto incorrect optimization directions, impacting the identification of camouflaged\nobjects. Specifically, these losses are more sensitive to difficult pixels, which is\nbeneficial for clean labels as it gives more bias to difficult pixels, but on noisy\nlabels, it leads to more severe error guidance. Therefore, it is necessary to discuss\nthe learning situation of different losses in noisy COD labels."}, {"title": "3.1 Preliminaries", "content": "We consider the learning situation of different losses on noisy labels from the\nperspective of gradients. Let \\(\\{x_t, g_t\\}\\) be a pair of images and its noisy label in\n\\(D_t\\). For any loss L, the risk gradient of the model PNet(\\(x_t\\)) can be divided as\n\\[\\nabla L(PNet(x_t; \\theta), g_t) = \\nabla L(PNet(x_t; \\theta), \\hat{g_t}) +\\nabla L(PNet(x_t; \\theta), \\tilde{g_t}),\\]\nwhere \\(\theta\\) means the parameters of PNet. We consider the risk gradient by divid-\ning the noisy label \\(g_t\\) into two parts: correct pixels \\(\\hat{g_t}\\) and noisy pixels \\(\\tilde{g_t}\\). When\nusing CE or IoU loss, it is believed that the gradient values propagated by noisy\npixels are greater than those from the correct pixels [42]. This means that the\nloss function introduces significant biases for noise, which are incorrect. Conse-\nquently, this leads to the model parameters \\(\theta\\) learning in the wrong direction,\nultimately affecting model's decision boundary. In contrast, as shown in Equ. 4,\nMAE loss does not have this issue, as it applies the same gradient to all pixels.\n\\[\\frac{\\partial L_{MAE}}{\\partial \\theta} = -\\nabla PNet_{g_t}(x_t;\\theta).\\]\nMoreover, MAE loss can tolerate up to 50% noise, as the total gradient\ndirection is still determined by the correct pixels. However, although MAE loss\nis robust to noise, its constant gradient presents an optimization issue, causing\nit to perform poorly on challenging data, such as camouflaged images."}, {"title": "3.2 LNC Loss for Camouflaged Object Detection", "content": "To leverage the advantages of noise robustness offered by MAE loss and the\noptimization capabilities of losses such as IoU and CE losses, we propose the use\nof the noise correction loss \\(L_{NC}\\) in the WSSCOD task. This loss is optimized for\nthe early learning phase and the memorization phase separately for this task,\nwhich can be calculated as follows\n\\[L_{NC} = \\frac{\\sum_{i=1}^{HXW} |p_i - g_i|^q}{\\sum_{i=1}^{HXW} (p_i + g_i) - p_ig_i},\\]\nwhere q \u2208 [1, 2] is a key hyper-parameter, p and g are the prediction and GT. In\nthe early learning stage, the model is required to effectively grasp the nuances\nof camouflaged scenes and assimilate knowledge from the correct pixels. For\nthis purpose, we set q = 2, making \\(L_{NC}\\) analogous to an IoU-form loss. In the\nmemorization phase, the model's focus shifts towards minimizing the influence of\nnoisy pixels and refining the decision boundary based on the gradient's guidance.\nAt this juncture, by setting q = 1, \\(L_{NC}\\) transforms into a MAE-form loss, which\ncan guide the PNet to optimize in the right direction.\nSpecifically, the robustness of \\(L_{NC}\\) comes from the deterministic nature of\nits derivative, which does not exhibit bias towards noisy pixels. When q = 1, the\ngradient of \\(L_{NC}\\) with respect to \\(p_i\\) is\n\\[\\frac{\\partial L_{NC}}{\\partial p_i} = \\frac{sign(p_i - g_i)}{\\sum_{i=1}^{HXW} (p_i + g_i) - \\sum_{i=1}^{HXW} p_ig_i}.\\]\nAs we can observe, \\(L_{NC}\\) effectively combines the advantages of MAE and IoU\nlosses: 1) It is noise-robust as MAE, as its gradient value is the same for each\npredicted pixel \\(p_i\\). 2) Like IoU, \\(L_{NC}\\) is area-dependent, can exploit the spatial\ncorrelation between pixels, and converges faster and better than MAE.\nFurthermore, an important consideration is that in different training setups,\nthe pseudo labels predicted by ANet are subject to varying levels of noise,\nleading to differences in early learning duration. Fixing the period for changing"}, {"title": "4 Related Work", "content": "4.1 Camouflaged Object Detection\nWith the rapid development of deep learning technology, data-driven segmenters\nhave achieved significant success in fully supervised COD tasks [3, 9, 10, 28].\nPraNet [10] introduced a parallel reverse attention mechanism, significantly im-\nproving the accuracy of detecting camouflaged objects. SINet [9] mimicked the\nsearch and identification stages of animal predation to detect and locate camou-\nflaged objects. FPNet [3] utilized both RGB and frequency domain information\nfor camouflaged object detection. Some weakly supervised methods [14, 35, 37]\nuse points, scribbles, and point annotations to achieve low-consumption, high-\nprecision COD. WSSA [37] is trained with scribble annotations and employs\na gated CRF loss [27] to enhance object detection accuracy. SCOD [14] intro-\nduced a novel consistency loss to ensure the agreement of individual prediction\nmaps, leveraging the loss from an internal viewpoint. However, weakly super-\nvised methods still remain a significant challenge in COD tasks, as the high\nsimilarity of camouflaged images prevents these methods from distinguishing\nbetween foreground and background. Therefore, unlike previous fully-supervised\nor weakly-supervised methods, we propose a new learning strategy, WSSCOD,\nwhich aims to achieve high-performance COD with an economical and labor-\nsaving labeling approach.\n4.2 Learning with Noisy Label\nDeep learning algorithms' remarkable performance heavily relies on large-scale,\nhigh-quality human annotations, obtaining which is extremely costly and time-\nconsuming. Cheaper annotation methods like web scraping and weakly super-\nvised methods offer an economical and efficient way to gather labels, but the\nnoise in these labels is inevitable. Learning with noisy labels aims to provide\nvarious strategies to tackle this challenging issue, such as robust loss design,\nnoise transition matrices, and sample selection. Zhang et al. [42] introduced a"}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nDatasets and Metrics. In COD task, four primary datasets serve as bench-\nmarks: CAMO [20], COD10K [9], CHAMELEON [30], and NC4K [25], contain-\ning 250, 2026, 4121, and 76 image pairs, respectively. The training set consists\nof 4040 pairs, with 1000 from CAMO and 3040 from COD10K. Within the\nWSSCOD framework, we leverage box annotations and a subset of fully anno-\ntated images. The approach includes four setups, partitioning the training set\nrandomly into subsets of \\(\\{1\\%, 5\\%, 10\\%, 20\\%\\}\\) of images with full and box anno-\ntations, and the remaining \\(\\{99\\%, 95\\%, 90\\%, 80\\%\\}\\) with only box annotations.\nFollowing the methodologies established in [25, 28], four essential metrics are\nadopted for an in-depth evaluation of model performance: mean absolute error\n(M), E-measure (E) [8], F-measure (F\u03b2) [26], and S-measure \\(S_{\\alpha}\\) [6].\nImplementation Details. For models: Following the selection practices\nof existing COD methods, we choose the SOTA backbone network PVTv2-B4\n[33] as the encoder for PNet to demonstrate the effectiveness of our method.\nHowever, for ANet, due to the Transformer model's weak performance on small\nscale data [5], we opt for ConvNeXt-B [24] as ANet's encoder. The weights of\nthese backbone networks are pretrained on ImageNet [4]. For data: To enhance\nthe model's robustness, we apply data augmentation techniques such as random\ncropping, random blurring, random brightness adjustments, and random flipping\nto the training images. Subsequently, the images are resized to 384\u00d7384 before\nbeing fed into the WSSCOD framework. For training: We use Adam optimizer\n[18] to update model parameters, with both ANet and PNet trained for 100\nepochs. The initial learning rate is set to 1e-7, linearly warmed up over 10 epochs\nto 1e-4, followed by cosine annealing down to 1e-7 while training. All random\nfactors, including data selection and the training process, are fixed using seed\n2024 to ensure the reproducibility of the model. Besides the NC loss, we also\nuse DICE loss, similar to FEDER [13] and BGNet [32], to assist the model in\nlearning the object boundaries.\nComparison Methods. To validate the effectiveness of the proposed WSS-\nCOD method, we construct a comparison of it with several recent SOTA meth-\nods, including the WSCOD methods WSSA [37], SCWS [35], TEL [22], SCOD"}, {"title": "5.2 Performance Comparision with SOTAS", "content": "Quantitative Evaluation. Table 1 provides a comprehensive quantitative com-\nparison between our proposed WSSCOD and 16 other COD models, using vari-\nous training strategies such as WSCOD methods, FSCOD methods and prompt\nbased methods. As shown in the table, our PNetF1 (with pixel-level annota-\ntions for only 40 images) outperforms weakly supervised methods, achieving an\naverage improvement of 79.6%, 16.3%, 18.1%, and 16.0% in M, E, F\u03b2, and\nSa metrics, respectively, compared to SCWS [35]. Compared to the fully su-\npervised SOTA method CamoFormer [34], our PNet F20 exhibits comparable\nperformance on the four datasets, with a gap of less than 1%, while requiring\nonly about 1/5 of their annotation effort (pixel-level annotations for just 800 im-\nages). Compared to SAM [19], our method demonstrates significant advantages,\neven against the box or point-prompted SAM. More importantly, our WSSCOD"}, {"title": "5.3 Ablation Experiments", "content": "The following ablation study validates\nthe innovations of this research, par-\nticularly the WSSCOD strategy and\nthe \\(L_{NC}\\) loss function.\nEffect of Box Prompts. We\nselect the box annotations as addi-\ntional prompts and provide a per-\nformance comparison with other an-\nnotation types, such as points and\nscribbles, in Table 2. For point an-\nnotations, denoted as P, we use the\nmethod recommended by [17], namely\nMaskRefineNet, to refine the output.\nThe processing method for scribble\nannotations, denoted as S, is consistent with our approach for box annotations,\nand the test dataset used is \\(D_n\\). According to this table, the improvement with\nbox annotations as prompts is significant, surpassing both scribble and point\nannotations by more than 7.2% (0.802 vs. 0.860 in F\u03b2) and achieving a 14.5%\nimprovement in performance (0.751 vs. 0.860 in F\u03b2) compared to methods with-\nout any prompts. Fig. 6 illustrates how box prompts refine the quality of pseudo\nlabels by preventing model misjudgments and enhancing the distinction between\nobject and background. In total, boxes are effective in COD tasks, as they greatly\nslow down the pressure on the model to detect in camouflaged scenes.\nEffect of WSSCOD. We introduce WSSCOD as an innovative training\nstrategy and provide a comparative analysis with other methods, such as semi-\nsupervised COD and FSCOD, in Table 3. This table includes comparisons of\ntraining with 20% (line \u2460) and 100% (line \u2461) of fully supervised data, as well\nas training with a combination of 20% fully supervised data and 80% unlabeled"}, {"title": "Effect of Noise Correction Loss", "content": "One key innovation in our work is the\nnoise correction loss (\\(L_{NC}\\)), which is designed to enhance the model's robust-\nness to noisy labels. To evaluate its effectiveness, we compare the performance\nof models trained with various loss functions in Table 4. Consistent with the\nconclusions in Fig. 4, the sensitivity of CE and IoU/IoU-form \\(L_{NC}^{q=2.0}\\) losses to\nnoise leads to their poor performance (0.780/0.778 vs. 0.792 in F\u03b2). Using only\nthe MAE-form of loss also does not yield optimal performance (0.778 vs. 0.792"}, {"title": "Universality of Noise Correction Loss", "content": "We posit that \\(L_{NC}\\) is versatile,\nas noise is commonly present in both WSCOD and FSCOD training sets. Thus,\nwe conduct experiments on the fully supervised model SINetv2 and the weakly\nsupervised model SCOD, modifying their training loss functions to our \\(L_{NC}\\) dur-\ning training, and changing the parameter q to 1 in the later stages of training.\nThe results are shown in Table 5. It can be seen that both models have achieved\neffective performance improvements, where the FSCOD method SINetv2 is im-\nproved by 12.1%, 1%, and 2.1% in M, E and F\u03b2 metrics except for \\(S_{\\alpha}\\). The\nSCOD is also improved by 33.3%, 7.0%, 5.9%, and 6.8% in the four metrics."}, {"title": "6 Conclusion and Discussion", "content": "Conclusion. We proposed WSSCOD to achieve low-cost, high-performance\nCOD. Moreover, to address the issue of noisy pseudo labels generated by ANet,\nwe introduced \\(L_{NC}\\) to achieve gradient consistency under noisy pixels. Our\nmethod requires only 20% of full annotations to reach the SOTA performance.\nLimitation. One major limitation of the proposed method is that the accu-\nracy of box annotation has a bit of impact on the final results, which is similar\nto the issue of multimodal bias. Specifically, in ANet, we use channel concate-\nnation to fuse the dual branches instead of employing overly complex fusion\nstrategies. As we aim to keep it simple, treating it as a baseline model. Actu-\nally, a better fusion strategy could mitigate the impact of incorrect boxes and\nimprove performance. Another limitation is that WSSCOD is two-stage, which\nis cumbersome, and a direction for subsequent research."}]}