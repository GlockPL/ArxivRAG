{"title": "Learning Camouflaged Object Detection from Noisy Pseudo Label", "authors": ["Jin Zhang", "Ruiheng Zhang", "Yanjiao Shi", "Zhe Cao", "Nian Liu", "Fahad Shahbaz Khan"], "abstract": "Existing Camouflaged Object Detection (COD) methods rely\nheavily on large-scale pixel-annotated training sets, which are both time-\nconsuming and labor-intensive. Although weakly supervised methods of-\nfer higher annotation efficiency, their performance is far behind due to the\nunclear visual demarcations between foreground and background in cam-\nouflaged images. In this paper, we explore the potential of using boxes\nas prompts in camouflaged scenes and introduce the first weakly semi-\nsupervised COD method, aiming for budget-efficient and high-precision\ncamouflaged object segmentation with an extremely limited number of\nfully labeled images. Critically, learning from such limited set inevitably\ngenerates pseudo labels with serious noisy pixels. To address this, we\npropose a noise correction loss that facilitates the model's learning of\ncorrect pixels in the early learning stage, and corrects the error risk gra-\ndients dominated by noisy pixels in the memorization stage, ultimately\nachieving accurate segmentation of camouflaged objects from noisy la-\nbels. When using only 20% of fully labeled data, our method shows\nsuperior performance over the state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Camouflaged Object Detection (COD) aims to detect and segment objects that\nblend seamlessly into their environments, presenting a significant challenge due\nto the need to counter sophisticated camouflage tactics and distinguish subtle\ndifferences between objects and their surroundings. Recent advances in COD\nhave been driven by the availability of abundant segmentation\nlabels. However, the labeling process for camouflaged objects is extremely labor-\nintensive, requiring about 60 minutes per image [9], which poses a major obstacle\nto this field's development. This challenge has led to a growing trend towards"}, {"title": "2 Weakly Semi-Supervised Camouflaged Object\nDetection", "content": "Task Definition. We introduce a novel training protocol named Weakly Semi-\nSupervised Camouflaged Object Detection (WSSCOD), which utilizes boxes as\nprompts to generate high-quality pseudo labels. WSSCOD primarily leverages\nbox annotations, complemented by a minimal amount of pixel-level annota-\ntions, to generate high-accuracy pseudo labels. Specifically, given the training\nset, D, that is divided into two subsets: , contain-\ning pixel-level annotations , box annotations Bm and training images Xm.\n, containing only box annotations and images, where M + N\nrepresents the number of training set. First, we train an auxiliary network,\nANet, using the dataset Dm, where Bm serves as an auxiliary prompt for cam-\nouflaged objects, and Fm supervises the generation of pseudo labels. Afterward,\nusing the trained ANet and the dataset Dn, we predict its pseudo labels, de-\nnoted as Wn. Finally, we construct a weakly semi-supervised dataset Dt using\nsets and , and train our proposed primary network,\nPNet, which, like other COD models, takes only images as input. The different"}, {"title": "2.1 Auxiliary Network", "content": "Segmenting camouflaged objects solely based on the box often leads to inaccura-\ncies, as the information within the box is not always reliable [41]. Therefore, as\nillustrated in Fig. 3, we develop a simple and effective COD model for exploiting\nthe complementarity between the RGB image and the proposals, but do not\nconsider the model as a major contribution. Given an RGB image xm and an\nobject box bm, we multiply them as the input proposals bm for the box branch\nBB encoder, and input xm into the image branch IB encoder.\nEncoder. We use two ConvNeXt [24] as encoders E() in ANet to obtain\nmulti-scale features for different inputs. Given the input images , we can obtain the multi-scale features and , respec-\ntively, from the branches IB and BB with the corresponding sizes of\nand channels . Following established practices [11,14,39,40], we\nadjust all features to the same number of channels C = 64 using 3\u00d73 convolu-\ntions for consistency across multi-level features. In addition, we apply channel\nconcatenation to features and and then send them to ASPP [1] to ob-\ntain deep image representations. And the initial mask from the\nrepresentations is generated through a 3\u00d73 convolution. Subsequently, features\nand are respectively fed into the Frequency Transformer FT to capture\nthe details and deep semantics of the image, which facilitates the discriminator's\nrecognition of camouflaged objects."}, {"title": "Frequency Transformer", "content": "Drawing inspiration from FDNet [43], we em-\nploy the Discrete Wavelet Transform DWT(\u00b7) to extract both low-frequency and\nhigh-frequency components from multi-scale features. This approach is instru-\nmental in revealing more intricate object components in camouflaged scenes by\nleveraging the frequency domain information Fd, enhancing the understanding\nof such complex visual environments. Taking F as input\n\nFaha, Fuha, Fale, Fl\u300f = DWT(cat(up(F)),\n\nwhere cat() indicates channel-wise concatenation, and up(\u00b7) is the up-sampling\noperation. In the frequency domain features, the subscripts h and I denote the\nextraction of high-frequency and low-frequency information, respectively, in the\nhorizontal and vertical directions. To efficiently process the obtained frequency\ndomain information and spatial domain features, we adopt an adaptive nonlinear\nfusion approach Yw(F, Fd), where w is the learnable parameters that adjusts the\ndegree of fusion between the two adaptively. We accomplish this fusion of shal-\nlow features F1, F2 with the high-frequency component Fah separately, and\nthe integration of deep features F, F with the low-frequency component Fa\nOther components such as Fun and Fal are usually not used. In FT, we rep-\nresent Y () using successive convolution and channel concatenation. For easier\ndescription, the total steps in FT are represented by P\u0192(\u00b7), with f specifying the\ntype of input features. In a similar manner, F and F can be obtained through\n(F) and (F). Before proceeding with the decoding, we also perform the\nadaptive fusion of F and through Yw(), as F = Y (F, F)."}, {"title": "Reverse Fusion Decoder", "content": "We design a reverse fusion decoder to complete\nthe convergence of multi-level features. Given the features and the\nmask man, we accomplish the fusion of multi-level features in the UNet manner.\nMeanwhile, we integrate a reverse mask, associating the background with difficult\nareas or noisy pixels in COD, amplifying the differences between them and the\ncorrect pixels, and correcting the model's learning of difficult areas, as\n\nPan =\n\nwhere Rev(pk+1) = \u22121 \u00d7 \u03c3(p+1) + 1, o is the sigmoid function. and man are the predictions, in which pan is the main output of ANet.\nThe decoding process is defined as II(\u00b7), which means plan = II(Fk, man)."}, {"title": "2.2 Primary Network", "content": "With the pretrained ANet, we predict the pseudo segmentation labels Wn by\nusing the image set with the corresponding box annotations Bn, gen-\nerating the training dataset Dn = for the primary network PNet.\nAdditionally, to align with existing methodologies and to maintain a consis-nt number of training images, we integrate the fully labeled dataset Dm =\ninto Dn to form the total training dataset ."}, {"title": "3 Noise Correction Loss", "content": "Training ANet with a very small amount of data poses a challenge in accu-\nrately capturing the distribution of the entire dataset, resulting in severe false\nnegative and positive noisy pixels in the generated pseudo labels. When training\non such noisy labels with traditional losses like Cross-Entropy (CE) and Inter-\nsection over Union (IoU), the bias introduced by the noisy pixels often leads\nto incorrect optimization directions, impacting the identification of camouflaged\nobjects. Specifically, these losses are more sensitive to difficult pixels, which is\nbeneficial for clean labels as it gives more bias to difficult pixels, but on noisy\nlabels, it leads to more severe error guidance. Therefore, it is necessary to discuss\nthe learning situation of different losses in noisy COD labels."}, {"title": "3.1 Preliminaries", "content": "We consider the learning situation of different losses on noisy labels from the\nperspective of gradients. Let be a pair of images and its noisy label in\nDt. For any loss L, the risk gradient of the model PNet(xt) can be divided as\n\nVL(PNet(xt; 0), gt) = \u2207L(PNet(t), \u011dt) +\u2207L(PNet(xt), \u011ft),\n\nwhere @ means the parameters of PNet. We consider the risk gradient by divid-\ning the noisy label gt into two parts: correct pixels \u011dt and noisy pixels \u011ft. When\nusing CE or IoU loss, it is believed that the gradient values propagated by noisy\npixels are greater than those from the correct pixels [42]. This means that the\nloss function introduces significant biases for noise, which are incorrect. Conse-\nquently, this leads to the model parameters @ learning in the wrong direction,\nultimately affecting model's decision boundary. In contrast, as shown in Equ. 4,\nMAE loss does not have this issue, as it applies the same gradient to all pixels.\n\nJLMAE = -VPNetgt(xt;0).\n\nMoreover, MAE loss can tolerate up to 50% noise, as the total gradient\ndirection is still determined by the correct pixels. However, although MAE loss\nis robust to noise, its constant gradient presents an optimization issue, causing\nit to perform poorly on challenging data, such as camouflaged images."}, {"title": "3.2 LNC Loss for Camouflaged Object Detection", "content": "To leverage the advantages of noise robustness offered by MAE loss and the\noptimization capabilities of losses such as IoU and CE losses, we propose the use\nof the noise correction loss LNC in the WSSCOD task. This loss is optimized for\nthe early learning phase and the memorization phase separately for this task,\nthat can be calculated as follows\n\nLNC =\n\nwhere q \u2208 [1, 2] is a key hyper-parameter, p and g are the prediction and GT. In\nthe early learning stage, the model is required to effectively grasp the nuances\nof camouflaged scenes and assimilate knowledge from the correct pixels. For\nthis purpose, we set q = 2, making LNC analogous to an IoU-form loss. In the\nmemorization phase, the model's focus shifts towards minimizing the influence of\nnoisy pixels and refining the decision boundary based on the gradient's guidance.\nAt this juncture, by setting q = 1, LNC transforms into a MAE-form loss, which\ncan guide the PNet to optimize in the right direction.\nSpecifically, the robustness of LNC comes from the deterministic nature of\nits derivative, which does not exhibit bias towards noisy pixels. When q = 1, the\ngradient of LNC with respect to pi is\n\nJLNC\n\u0434\u0440\u0456 =\n\nAs we can observe, LNC effectively combines the advantages of MAE and IoU\nlosses: 1) It is noise-robust as MAE, as its gradient value is the same for each\npredicted pixel pi. 2) Like IoU, LNC is area-dependent, can exploit the spatial\ncorrelation between pixels, and converges faster and better than MAE.\nFurthermore, an important consideration is that in different training setups,\nthe pseudo labels predicted by ANet are subject to varying levels of noise,\nleading to differences in early learning duration. Fixing the period for changing"}, {"title": "4 Related Work", "content": "With the rapid development of deep learning technology, data-driven segmenters\nhave achieved significant success in fully supervised COD tasks [3, 9, 10, 28].\nPraNet [10] introduced a parallel reverse attention mechanism, significantly im-\nproving the accuracy of detecting camouflaged objects. SINet [9] mimicked the\nsearch and identification stages of animal predation to detect and locate camou-\nflaged objects. FPNet [3] utilized both RGB and frequency domain information\nfor camouflaged object detection. Some weakly supervised methods [14,35,37]\nuse points, scribbles, and point annotations to achieve low-consumption, high-precision COD. WSSA [37] is trained with scribble annotations and employs\na gated CRF loss [27] to enhance object detection accuracy. SCOD [14] intro-\nduced a novel consistency loss to ensure the agreement of individual prediction\nmaps, leveraging the loss from an internal viewpoint. However, weakly super-\nvised methods still remain a significant challenge in COD tasks, as the high\nsimilarity of camouflaged images prevents these methods from distinguishing\nbetween foreground and background. Therefore, unlike previous fully-supervised\nor weakly-supervised methods, we propose a new learning strategy, WSSCOD,\nwhich aims to achieve high-performance COD with an economical and labor-saving labeling approach."}, {"title": "4.2 Learning with Noisy Label", "content": "Deep learning algorithms' remarkable performance heavily relies on large-scale,\nhigh-quality human annotations, obtaining which is extremely costly and time-consuming. Cheaper annotation methods like web scraping and weakly super-\nvised methods offer an economical and efficient way to gather labels, but the\nnoise in these labels is inevitable. Learning with noisy labels aims to provide\nvarious strategies to tackle this challenging issue, such as robust loss design,\nnoise transition matrices, and sample selection. Zhang et al. [42] introduced a"}, {"title": "5 Experiments", "content": "Datasets and Metrics. In COD task, four primary datasets serve as bench-\nmarks: CAMO [20], COD10K [9], CHAMELEON [30], and NC4K [25], contain-\ning 250, 2026, 4121, and 76 image pairs, respectively. The training set consists\nof 4040 pairs, with 1000 from CAMO and 3040 from COD10K. Within the\nWSSCOD framework, we leverage box annotations and a subset of fully anno-tated images. The approach includes four setups, partitioning the training set\nrandomly into subsets of {1%, 5%, 10%, 20%} of images with full and box anno-tations, and the remaining {99%, 95%, 90%, 80%} with only box annotations.\nFollowing the methodologies established in [25,28], four essential metrics are\nadopted for an in-depth evaluation of model performance: mean absolute error\n(M), E-measure (E) [8], F-measure (FB) [26], and S-measure (Sa) [6].\nImplementation Details. For models: Following the selection practices\nof existing COD methods, we choose the SOTA backbone network PVTv2-B4\n[33] as the encoder for PNet to demonstrate the effectiveness of our method.However, for ANet, due to the Transformer model's weak performance on small-scale data [5], we opt for ConvNeXt-B [24] as ANet's encoder. The weights ofthese backbone networks are pretrained on ImageNet [4]. For data: To enhance\nthe model's robustness, we apply data augmentation techniques such as randomcropping, random blurring, random brightness adjustments, and random flipping\nto the training images. Subsequently, the images are resized to 384\u00d7384 before\nbeing fed into the WSSCOD framework. For training: We use Adam optimizer\n[18] to update model parameters, with both ANet and PNet trained for 100\nepochs. The initial learning rate is set to le-7, linearly warmed up over 10 epochs\nto 1e-4, followed by cosine annealing down to le-7 while training. All random\nfactors, including data selection and the training process, are fixed using seed2024 to ensure the reproducibility of the model. Besides the NC loss, we alsouse DICE loss, similar to FEDER [13] and BGNet [32], to assist the model in\nlearning the object boundaries.\nComparison Methods. To validate the effectiveness of the proposed WSS-COD method, we construct a comparison of it with several recent SOTA meth-ods, including the WSCOD methods WSSA [37], SCWS [35], TEL [22], SCOD"}, {"title": "5.2 Performance Comparision with SOTAS", "content": "Quantitative Evaluation. Table 1 provides a comprehensive quantitative com-\nparison between our proposed WSSCOD and 16 other COD models, using vari-ous training strategies such as WSCOD methods, FSCOD methods and promptbased methods. As shown in the table, our PNetF1 (with pixel-level annota-tions for only 40 images) outperforms weakly supervised methods, achieving an\naverage improvement of 79.6%, 16.3%, 18.1%, and 16.0% in M, E, F\u00df, andSa metrics, respectively, compared to SCWS [35]. Compared to the fully su-\npervised SOTA method CamoFormer [34], our PNet F20 exhibits comparable\nperformance on the four datasets, with a gap of less than 1%, while requiringonly about 1/5 of their annotation effort (pixel-level annotations for just 800 im-ages). Compared to SAM [19], our method demonstrates significant advantages,even against the box or point-prompted SAM. More importantly, our WSSCOD"}, {"title": "5.3 Ablation Experiments", "content": "The following ablation study validates\nthe innovations of this research, par-ticularly the WSSCOD strategy and\nthe LNC loss function.\nEffect of Box Prompts. We\nselect the box annotations as addi-tional prompts and provide a per-formance comparison with other an-notation types, such as points andscribbles, in Table 2. For point an-notations, denoted as P, we use themethod recommended by [17], namelyMaskRefineNet, to refine the output.The processing method for scribbleannotations, denoted as S, is consistent with our approach for box annotations,\nand the test dataset used is Dn. According to this table, the improvement withbox annotations as prompts is significant, surpassing both scribble and pointannotations by more than 7.2% (0.802 vs. 0.860 in Fs) and achieving a 14.5%improvement in performance (0.751 vs. 0.860 in Fs) compared to methods with-out any prompts. Fig. 6 illustrates how box prompts refine the quality of pseudolabels by preventing model misjudgments and enhancing the distinction betweenobject and background. In total, boxes are effective in COD tasks, as they greatlyslow down the pressure on the model to detect in camouflaged scenes.\nEffect of WSSCOD. We introduce WSSCOD as an innovative trainingstrategy and provide a comparative analysis with other methods, such as semi-supervised COD and FSCOD, in Table 3. This table includes comparisons oftraining with 20% (line \u2460) and 100% (line \u2461) of fully supervised data, as wellas training with a combination of 20% fully supervised data and 80% unlabeled"}, {"title": "Effect of Noise Correction Loss", "content": "One key innovation in our work is thenoise correction loss (LNC), which is designed to enhance the model's robust-ness to noisy labels. To evaluate its effectiveness, we compare the performance\nof models trained with various loss functions in Table 4. Consistent with theconclusions in Fig. 4, the sensitivity of CE and IoU/IoU-form LNC losses to\nnoise leads to their poor performance . Using onlythe MAE-form of loss also does not yield optimal performance "}, {"title": "Universality of Noise Correction Loss", "content": "We posit that LNC is versatile,as noise is commonly present in both WSCOD and FSCOD training sets. Thus,we conduct experiments on the fully supervised model SINetv2 and the weaklysupervised model SCOD, modifying their training loss functions to our LNC dur-ing training, and changing the parameter q to 1 in the later stages of training.The results are shown in Table 5. It can be seen that both models have achievedeffective performance improvements, where the FSCOD method SINetv2 is im-proved by 12.1%, 1%, and 2.1% in M, E and F\u00df metrics except for Sa. TheSCOD is also improved by 33.3%, 7.0%, 5.9%, and 6.8% in the four metrics."}, {"title": "6 Conclusion and Discussion", "content": "Conclusion. We proposed WSSCOD to achieve low-cost, high-performanceCOD. Moreover, to address the issue of noisy pseudo labels generated by ANet,we introduced LNC to achieve gradient consistency under noisy pixels. Ourmethod requires only 20% of full annotations to reach the SOTA performance.\nLimitation. One major limitation of the proposed method is that the accu-racy of box annotation has a bit of impact on the final results, which is similar\nto the issue of multimodal bias. Specifically, in ANet, we use channel concate-nation to fuse the dual branches instead of employing overly complex fusionstrategies. As we aim to keep it simple, treating it as a baseline model. Actu-ally, a better fusion strategy could mitigate the impact of incorrect boxes andimprove performance. Another limitation is that WSSCOD is two-stage, whichis cumbersome, and a direction for subsequent research."}]}