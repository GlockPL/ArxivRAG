{"title": "Optimizing Latent Goal by Learning from Trajectory Preference", "authors": ["Guangyu Zhao", "Kewei Lian", "Haowei Lin", "Haobo Fu", "Qiang Fu", "Shaofei Cai", "Zihao Wang", "Yitao Liang"], "abstract": "A glowing body of work has emerged focusing on instruction-following policies for open-world agents, aiming to better align the agent's behavior with human intentions. However, the performance of these policies is highly susceptible to the initial prompt, which leads to extra efforts in selecting the best instructions. We propose a framework named Preference Goal Tuning (PGT). PGT allows an instruction-following policy to interact with the environment to collect several trajectories, which will be categorized into positive and negative samples based on preference. Then we use preference learning to fine-tune the initial goal latent representation with the categorized trajectories while keeping the policy backbone frozen. The experiment result shows that with minimal data and training, PGT achieves an average relative improvement of 72.0% and 81.6% over 17 tasks in 2 different foundation policies respectively, and outperforms the best human-selected instructions. Moreover, PGT surpasses full fine-tuning in the out-of-distribution (OOD) task-execution environments by 13.4%, indicating that our approach retains strong generalization capabilities. Since our approach stores a single latent representation for each task independently, it can be viewed as an efficient method for continual learning, without the risk of catastrophic forgetting or task interference. In short, PGT enhances the performance of agents across nearly all tasks in the Minecraft Skillforge benchmark and demonstrates robustness to the execution environment.", "sections": [{"title": "1. Introduction", "content": "Recently, pre-training foundation policies in open-world environments with web-scale unlabeled datasets have become an increasingly popular trend in the domain of sequential control (Baker et al., 2022; Brohan et al., 2023a; Collaboration et al., 2024; Yang et al., 2023; Zhang et al., 2022). These foundation policies possess broad world knowledge, which can be transferred to downstream tasks. In the realm of foundation policies, there exists a category known as goal-conditioned policies, which are capable of processing input goals (instructions) and executing the corresponding tasks (Chane-Sane et al., 2021; Ding et al., 2019). The goal can be in different modalities, such as text instructions (Lifshitz et al., 2024), video demonstrations (Cai et al., 2023b), or multi-model instructions (Brohan et al., 2023a,b; Cai et al., 2024)).\nHowever, much like large language models, these instruction-following policies are highly susceptible to the selection of \u201cprompts\" (Kim et al., 2024; Lifshitz et al., 2024; Wang et al., 2023a,b). Researchers rely on trial and error to find the optimal prompt manually, and sometimes the quality of prompts doesn't align with human judgment. For instance, OpenVLA (Kim et al., 2024) shows a large performance gap when using \u201cPepsi can\u201d compared to \u201cPepsi\u201d as the prompt; for the same task of collecting wood logs, GROOT's performance varies significantly depending on the reference video used. Moreover, it is unclear whether an agent's failure to complete a task is due to the foundation policy's inherent limitations or the lack of a suitable prompt.\nA common viewpoint from the LLM community thinks that most of the abilities are learned from the pre-training phase (Ouyang et al., 2022; Zhao et al., 2023a), while post-training is a method to"}, {"title": "2. Preliminary", "content": ""}, {"title": "2.1. Sequential Control", "content": "In sequential control settings, the environment is defined as a Markov Decision Process (MDP) (S, A, R, P, do), where S is the state space, A is the action space, R : S \u00d7 A \u2192 R is the reward function, P : S \u00d7 A \u2192 S is the transition dynamics, and do is the initial state distribution. A policy \u03c0(a|s) interacts with the environment starting from so ~ d9. At each timestep t > 0, an action at ~ \u03c0(a|st) is sampled and applied to the environment, after that, the environment transitions to"}, {"title": "2.2. Goal-Conditioned Policy", "content": "GROOT. GROOT (Cai et al., 2023b) is a goal-conditioned foundation policy trained on video data through self-supervised learning with a C-VAE(Sohn et al., 2015) framework. GROOT can follow video instructions in open-world environments. The instruction is encoded into a latent representation by the non-causal encoder, and the policy is a decoder module implemented by a causal transformer, which decodes the goal information in the latent space and translates it into a sequence of actions in the given environment states in an auto-regressive manner.\nSTEVE-1. STEVE-1 (Lifshitz et al., 2024) is also a goal-conditioned policy on Minecraft environment. STEVE-1 utilizes the goal latent representation of MineCLIP(Fan et al., 2022) to embed the future result video clip in dataset Andrychowicz et al. (2017), and fine-tunes a VPT model (Baker et al., 2022) as the policy network under the guidance of the MineCLIP embedding. As a C-VAE (Sohn et al., 2015) model is trained to predict \u201cfuture video embedding\u201d from text, STEVE-1 supports both text and video as instructions."}, {"title": "2.3. Preference Learning", "content": "While self-supervised learning models trained with large-scale parameters and data are experts in encoding knowledge, their outputs do not necessarily meet human intention. An effective solution is learning from preference-labeled data. Direct Preference Optimization(DPO) (Rafailov et al., 2024), as one method, serves as a way to directly optimize the model's outputs based on pair-wise positive-negative data. For a pair of responses (y1, y2) corresponding to a prompt x, human labelers express their preference and classify them as win(w) and lose(1), denoted as yw > y\u00ee | x. Assuming we have a foundation model \u03c0ref and a dataset of preference D = {x(i), y(i)w, y(i)l}1, DPO derives the optimization objective as:\n$L_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l)\\sim D} \\left[ -log\\sigma\\left( \\beta log\\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref} (y_w | x)} - \\beta log\\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref} (y_l | x)} \\right) \\right]$"}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Preference Goal Tuning", "content": "In this section, we propose a novel policy post-training framework named Preference Goal-Tuning (PGT). This approach achieves significant performance improvements for foundation policies with"}, {"title": "3.2. Design Choices", "content": "In this section, we address the key design choices of our method and provide a comparative analysis of relevant baselines to justify why we use negative examples for preference learning and why we use parameter-efficient fine-tuning.\nUtilizing negative samples. A straightforward approach is to utilize only self-generated positive samples for behavior cloning (BC), and some studies have proved filtering and cloning is enough in many settings (Gulcehre et al., 2023; Oh et al., 2018). However, this approach does not explicitly indicate \"which behaviors should be avoided\u201d, which is conducive to policy optimization (Tajwar et al., 2024). Incorporating negative data helps the policy distinguish between desirable and undesirable behaviors. As a comparison, we trained a version of the BC algorithm (with double data size of the positive samples to control the total amount of data) and conducted experiments with both soft prompt fine-tuning and full fine-tuning, and the results are listed in Table 1. We notice that when using BC algorithm, performance even declines in 3 out of 4 tasks in soft prompt fine-tuning.\nTuning goal latent representation only. We compare the results of fine-tuning goal latent representation only and its counterpart that fine-tuning the entire policy model. There are two main reasons why we only fine-tune goal latent representation. First, fine-tuning the goal latent offers strong interpretability. For a goal-conditioned foundation policy trained through supervised learning with large datasets, the latent goal space usually holds abundant semantic meanings. However, since the human intention behind the instruction and the embedding in the goal space do not always align, the instructions selected by humans might not map well to the optimal latent representation in the goal space. Our method aims to obtain the optimal representation in goal space through a small amount of training. Second, due to the limited amount of data, full-parameter fine-tuning is highly prone to overfitting the training execution environment. For example, in Minecraft, task collect wood("}, {"title": "4. Experiments", "content": "We select open-world Minecraft as the test bed to evaluate our methods (Fan et al., 2022; Lin et al., 2023). The tasks are selected from Minecraft SkillForge benchmark (Cai et al., 2023b). This benchmark covers over 30 diverse and representative tasks from 6 major categories. We put the details of this benchmark in Appendix B.2. Through our experiments, the following contributions of our method are verified:\n\u2022 PGT remarkably improves the performance of two foundation policies, surpassing the best human-selected prompt.\n\u2022 PGT serves as an efficient continual learning method.\n\u2022 PGT improves long-horizon task performance with a combination of planner and controller.\n\u2022 PGT elicits skills that were not achievable with traditional prompts."}, {"title": "4.1. Boosting Performance over Prompt Tuning", "content": "Our approach significantly improves the instruction-following capability of the model. By fine-tuning specific aspects of the model's behavior, we achieve greater task performance compared to traditional prompt engineering techniques, which rely on manually crafted inputs. We discard tasks in Minecraft SkillForge that are too difficult (with zero success rate), or too easy (with a 100% success rate and the specific value of the reward is meaningless).\nWe experimented with two foundation policies, GROOT and STEVE-1, in both in-distribution (ID) and out-of-distribution (OOD) settings. The modifications made to the OOD settings compared to the ID settings are detailed in Appendix B.4. For in-distribution settings, we achieved an average"}, {"title": "4.2. Efficient Continual Learning", "content": "Our method is an efficient approach to continual learning, as it requires only minimal training for each task, followed by storing a high-dimensional latent (typically consisting of a few hundred floating-point values) as a task representation. As a result, our method avoids issues like catastrophic forgetting and task interference.\nWe compare PGT with multiple continual learning baselines: multi-task learning (MTL), naive continual learning(NCL), knowledge distillation (KD) (Hinton et al., 2015), experience replay (ER) (Lopez-Paz & Ranzato, 2022), elastic weight consolidation (EWC) (Kirkpatrick et al., 2017). It's worth mentioning that every continual learning baseline is conducted under full fine-tuning, which has a parameter size several orders of magnitude times larger than ours. We first implemented the multi-task learning (MTL) baselines on six representative tasks, with the results presented in Table 3. We find that, similar to the results of full-parameter fine-tuning, our method achieved comparable performance to MTL in ID settings, while surpassing MTL in OOD settings.\nWe experiment in the following order: collect_obsidian(3) \u2192 tool_pumpkin(\n\u2192 craft_crafting_table(m) \u2192 explore_climb(). The result after continual learning 4 tasks\nis in Table 4, and we place the detailed result of continual learning after each task in Appendix\nC.4. We conduct experiments of naive continual learning (NCL) (Table 10), knowledge distillation\n(KD) (Table 11), experience replay (ER)(Table 12), and elastic weight consolidation (EWC) (Table\n13).\nExperiment results show that in addition to being more efficient in terms of computational resources and storage, our method excels in handling diverse tasks, demonstrating superior generalization capabilities. In out-of-distribution settings, we outperform the ensemble in each of the 6 tracks, and we achieve comparable results to MTL."}, {"title": "4.3. Solving Long-horizon Challenges with Planner", "content": "It is a common approach to combine a high-level planner and a low-level controller for functionality and versatility. We combine the GROOT agent with JARVIS-1 planner (Wang et al., 2023b), trying to craft items from scratch spawning in a forest with random initial orientation and angle. JARVIS-1 also offers an API script for crafting items. We give the agent 1000 timesteps to run and select five representative items in the wood-related tech tree. We observe improvements in long-horizon task performance compared to the baseline, which is shown in Table 5. This finding demonstrates the soft prompts trained with PGT have strong robustness and environmental generalization, and have the potential to serve as a bridge between the planner and the controller in the policy post-training stage."}, {"title": "4.4. Eliciting New Skills", "content": "For task tool_trident( ), given standard gameplay videos, the agent was unable to complete\nthe task. As a result, the standard PGT pipeline cannot collect positive data. Instead, we recorded 20\ntrajectories by humans and trained with behavior cloning. Even though the success rate was still low,\nwe found several success examples, meaning that the agent acquired the ability to complete the task.\nThis implies that during the pretraining phase, the agent already possessed the ability to complete the\ntask, but lacked the appropriate prompt to elicit this ability. Our method, through minimal training\non the soft prompt, successfully activated this capability."}, {"title": "4.5. Ablation Study on PEFT Methods", "content": "We compare our method with other parameter-efficient fine-tuning (PEFT) methods: LoRA (Hu\net al., 2021), BitFit (Zaken et al., 2022) and VeRA (Kopiczko et al., 2024). We still utilize P-N\nsamples for PGT for all of them fine-tuning the entire model. We found that our method performed\nwell among the four methods. Moreover, in task expore_mine() and collect_obsidian(),\nLORA fine-tuning also demonstrated promising results. The result is in Figure 5, and the numerical\nresult is in Appendix C.3."}, {"title": "5. Related Work", "content": ""}, {"title": "5.1. Foundation Models for Decision-making", "content": "Foundation models have gained huge success in the field of language (Brown et al., 2020; OpenAI, 2024; ?) and vision (He et al., 2016; Kirillov et al., 2023), and an increasing number of studies are exploring the potential of foundation models in sequential control (Cai et al., 2023a; Cheng et al., 2024; Wang et al., 2023a; Yang et al., 2023; Zhang et al., 2023). VPT (Baker et al., 2022) is a foundation policy pretrained by video data behavior cloning and fine-tuned by reinforcement learning, which is capable of obtaining diamonds from scratch in Minecraft. Lifshitz et al. (2024) adapted the VPT model to following human instructions under the guidance of MineCLIP (Fan et al., 2022) and Cai et al. (2023b) started from scratch to train a Minecraft instruction-following agent controlled by the CVAE posterior distribution, which solves a variety of tasks in the open-world environment. In the field of robotics, there are also many foundation policies like BC-Z (Jang et al., 2022), GATO (Reed et al., 2022), RT-1 (Brohan et al., 2023b), RT-2 (Brohan et al., 2023a) and VQ-BeT (Lee et al., 2024)."}, {"title": "5.2. Preference Learning", "content": "Directly obtaining high-quality human annotations, such as expert numerical ratings (Akrour et al., 2014; F\u00fcrnkranz et al., 2012), or expert demonstrations (Silver et al., 2016), is often extremely time-consuming, labor-intensive, and brain-consuming to annotators (Knox & Stone, 2009). Fortunately, the cost is greatly reduced by letting them label pairs or groups of data with simply their prefer-ences Christiano et al. (2017). As a fruitful method to leverage more low-annotation-difficulty data, preference learning has been studied extensively in recent years. Christiano et al. (2017); Ouyang et al. (2022); Ziegler et al. (2020) utilized preference data to teach a reward model, and conducted reinforcement learning on sequential decision-making games or language modeling, demonstrating the efficiency and wide application of preference learning. These methods rely on another model for simulating the reward function and on-policy data. Therefore, some simpler alternatives that do not require reinforcement learning soon emerged (Azar et al., 2024; Meng et al., 2024; Rafailov et al., 2024) or even without reference model for regularization (Hong et al., 2024). Even though these methods do not strictly demand on-policy data, researchers (Tajwar et al., 2024) found that preference pairs generated by the current policy can improve fine-tuning efficiency."}, {"title": "6. Limitations and Future Work", "content": "PGT has shown remarkable capability in improving task performance. However, it still has some limitations and untapped potential awaiting further exploration.\nLimitations. PGT requires multiple interactions with the environment to obtain positive and negative samples. While this is feasible in simulated environments like Minecraft, in other domains, such as robotics, the cost of interacting with the environment can be very high, or opportunities for interaction may be limited (due to the risk of damage to the robots). In such cases, PGT may not be suitable.\nPotentials. Our method holds significant potential. First, all of our experiments were conducted in the Minecraft environment, but there are many instruction-following policies in the robotics domain as well. We believe that PGT could also achieve promising results in robotics. Second, the current experiments only cover several simple long-horizon tasks, like building a large chest from scratch. We are thrilled to explore how PGT can help solve longer and more complex tasks in Minecraft, like the ultimate goal: killing the ender dragon."}, {"title": "7. Conclusion", "content": "We have introduced a framework named Preference Goal-Tuning (PGT), which is an efficient post-training method for foundation policies. It utilizes a small amount of human preference data to fine-tune goal latent in goal-conditioned policies. PGT significantly enhances the capability of the foundation policy with minimal data and training, easily surpassing the best human-selected instructions. Our method also demonstrates the potential for acquiring new skills and serving as an efficient method for continual learning."}, {"title": "A. Mathematical Derivation", "content": ""}, {"title": "A.1. PGT loss", "content": "Our PGT method is based on preference learning with a sequential decision-making process. Our policy is formulated as \u03c0(\u03c4|g), meaning the probability of generating trajectory \u03c4 under latent goal g. Assume t = (st, at)\u014d\u00b9 is a N step trajectory, \u03c0(\u03c4|g) can be expanded as:\n$\\pi(\\tau|g) = \\prod_{i=0}^{N-1} \\pi(a_i|S_i, g)p(s_{i+1}|s_t, a_t)$\nGenerally, we want to utilize human preference to finetune our policy. Take DPO as an example, \u201cpreference\u201d is assumed to be generated by an oracle reward function r*(\u03c4), which is inaccessible. r*(\u03c4) represents how well trajectory \u03c4 performs the task. The better y performs, the higher r* (\u03c4) is. Even though we cannot obtain this oracle reward in practice, we can still set it as our objective:\n$max_{g} E_{\\tau \\sim \\pi(\\tau|g)} [r^* (\\tau)] - \\beta D_{KL}[\\pi(\\tau|g) || \\pi(\\tau|g_{ref})]$\nHere g is the latent goal, which is trainable, and gref is the initial goal latent. The first term is to maximize the reward, and the second term is to constrain the trained g such that it does not deviate too far from gref. By applying the same derivation method as DPO, we have:\n$max_{g} E_{\\tau \\sim \\pi(\\tau|g)} [r^* (\\tau)] - \\beta D_{KL}[\\pi(\\tau|g) || \\pi(\\tau|g_{ref})]$\n$= max_{g} E_{\\tau \\sim \\pi(\\tau|g)} [r^*(\\tau) - \\beta log \\frac{\\pi(\\tau; g)}{\\pi(\\tau; g_{ref})}]$\n$= max_{g} E_{\\tau \\sim \\pi(\\tau|g)} [\\frac{-r^*(\\tau)}{\\beta} - log \\frac{\\pi(\\tau|g_{ref})}{\\pi(\\tau|g)}]$\n$= min_{g} E_{\\tau \\sim \\pi(\\tau|g)} [-\\frac{r^*(\\tau)}{\\beta} +log \\frac{\\pi(\\tau|g_{ref})}{\\pi(\\tau|g)}]$\n$= min_{g} E_{\\tau \\sim \\pi(\\tau|g)} [log \\frac{\\pi(\\tau|g)}{\\exp(\\frac{-r^*(\\tau)}{\\beta}) \\pi(\\tau|g_{ref})}]$\n$= min_{g} E_{\\tau \\sim \\pi(\\tau|g)} [log \\frac{\\pi(\\tau|g)}{Z \\pi(\\tau|g^*)} - log Z]$\nwhere $Z = \\sum_\\tau \\pi(\\tau|g_{ref}) \\exp(\\frac{-r^*(\\tau)}{\\beta})$. We define $g^*$ that satisfied\n$\\pi(\\tau|g^*) = \\frac{\\exp(\\frac{-r^*(\\tau)}{\\beta}) \\pi(\\tau|g_{ref})}{Z}$\nThe training object becomes:\n$min_{g} E_{\\tau \\sim \\pi(\\tau|g)} [log \\frac{\\pi(\\tau|g)}{\\pi(\\tau|g^*)} - log Z]$\n$= min_{g} E_{\\tau \\sim \\pi(\\tau|g)} [log \\frac{\\pi(\\tau|g)}{\\pi(\\tau|g^*)} - log Z]$\n$= min_{g} D_{KL} (\\pi (\\tau|g)||\\pi(\\tau|g^*)) - log Z$\nSo we can obtain closed-form optimal solution:\n$\\pi(\\tau|g) = \\pi(\\tau|g^*) = \\frac{\\exp(\\frac{-r^*(\\tau)}{\\beta}) \\pi(\\tau|g_{ref})}{Z}$\nq\nConsider the Bradly-Terry(BT) model:\n$p(\\tau_1 > \\tau_2) = \\frac{\\exp (r(\\tau_1))}{\\exp (r(\\tau_1)) + \\exp (r(\\tau_2))}$\u02d9\nfill Eq. 17 into Eq. 18, we have:\n$p(\\tau_1 > \\tau_2) = \\sigma \\left[ \\beta log \\frac{\\pi(\\tau_1|g^*)}{\\pi(\\tau_1|g_{ref})} - \\beta log \\frac{\\pi(\\tau_2|g^*)}{\\pi(\\tau_2|g_{ref})} \\right]$"}, {"title": "B. Experiment Details", "content": ""}, {"title": "B.1. Minecraft", "content": "Minecraft is a popular sandbox game that allows players to freely create and explore their world. Since Minecraft is an open-world environment, many recent works have designed agents and conducted explorations within Minecraft (Johnson et al., 2016). In this work, we conduct experiments on 1.16.5 version MineRL (Guss et al., 2019) and MCP-Reborn."}, {"title": "B.2. Minecraft SkillForge Benchmark", "content": "Minecraft SkillForge Benchmark is a comprehensive task suite that covers various types of tasks in Minecraft. All tasks are categorized into six major groups:\n\u2022 Collect task: these tasks are designed to evaluate an Al agent's capability in resource acquisition\nproficiency and spatial awareness.\n\u2022 Craft task: these tasks are designed to shed light on an Al agent's prowess in item utilization,\nthe intricacies of Minecraft crafting mechanics, and the nuances of various game mechanic\ninteractions.\n\u2022 Explore task: these tasks are designed to evaluate an AI agent's navigation proficiency, under-\nstanding of diverse environments, and intrinsic motivation for exploration.\n\u2022 Survive task: these tasks are designed to analyze an Al agent's ability to ensure its survival,\nadeptness in combat scenarios, and capability to interact with the environment to meet basic\nneeds.\n\u2022 Tool task: these tasks are designed to deeply investigate an Al agent's capabilities in tool\nutilization, precision in tool handling, and contextual application of various tools to carry out\nspecific tasks.\n\u2022 Build task: these tasks are devised to evaluate an AI agent's aptitude in structural reasoning,\nspatial organization, and its capability to interact with and manipulate the environment to\ncreate specific structures or outcomes."}, {"title": "B.3. Task Metrics and Selection", "content": "For most tasks, the environment logs the rewards when the corresponding objectives are achieved. We define tasks with a reward function greater than 0 as successful, and the frequency of successfully completing a task is referred to as the success rate. However, tasks like \u201ccollect_wood\u201d \u201cexplore_mine\" and \"survive_plant\u201d have a success rate of over 95% across different agents, and the specific values of"}, {"title": "B.4. Out-of-distribution Settings", "content": "We designed the out-of-distribution (OOD) setting with the goal of preventing the policy from overfitting to the environment and relying on it to dictate behavior. Thus, without altering the core meaning of the tasks, we made the following modifications to create the OOD setting:\n\u2022 Seed and agent location We change the seed and spawn location in the Minecraft world to perform the same task, and then the initial observation will not be identical to the training set.\n\u2022 Biome We change the biome of the agent while keeping the task solvable. For example, change biome from plains to forest of task tool_pumpkin.\n\u2022 Tool We modified the auxiliary tools while ensuring the tasks remained solvable. For example, in the survive_hunt, we replaced the iron_sword with diamond_axe.\n\u2022 Object location We change the location of the object that the agent needs to interact with. For example, we changed the position of the stonecutter from being held in the hand to being placed in front of the agent.\nFor each task, we applied one or more of the aforementioned OOD modifications. It is important to note that the absolute performance in the OOD setting is not directly comparable to the baseline, as the tasks may become either easier or harder in the OOD environment."}, {"title": "B.5. Hyperparameters", "content": "Our training hyperparameters are listed in Table 6. We conducted a hyperparameter search on the \"collect wood\" task and used the same set of hyperparameters for all the other tasks. We visualized the performance of the \"collect wood\" task under different values of \u1e9e. The result can be seen in Fig 6 The results showed similar performance when \u1e9e \u2265 0.2."}, {"title": "C. Experiment Results", "content": ""}, {"title": "C.1. Behaviour Cloning Results", "content": "This baseline employs behavior cloning, trained exclusively on positive samples, without the inclusion of negative data or preference learning. We present results for both tuning soft prompt and the full parameters (Table 1)."}, {"title": "C.2. Full Fine-tuning Results", "content": "We compare the results of our method with full fine-tuning. The latter involves ~100M parameters, while the former only has 512 parameters, which is merely one in hundreds of thousands of the other. We found that in in-distribution settings, the soft prompt method achieves results comparable to those of full fine-tuning. However, in out-of-distribution (OOD) environments, soft prompt tuning outperformed across all tasks. The result can be found in Table 7."}, {"title": "C.3. Parameter-efficient Fine-tuning Results", "content": "We conduct parameter-efficient fine-tuning on LoRA (Hu et al., 2021), BitFit (Zaken et al., 2022), VeRA (Kopiczko et al., 2024), and the result is in Table 8. In fact, all of these parameter counts are significantly larger than those of PGT. and the contrast is shown in Table 9."}, {"title": "C.4. Continual Learning Results", "content": "All of our continual learning baselines are based on fine-tuning the entire policy model, and the order of tasks for continual learning is as follows: collect_obsidian(3) \u2192 tool_pumpkin(\n\u2192 craft_crafting_table(m) \u2192 explore_climb(). We implemented multi-task learning (MTL) (Table 3), naive continual learning (NCL) (Table 10), knowledge distillation (KD)(Table 11), experience replay (ER) (Table 12), and elastic weight consolidation (EWC)(Table 13)."}, {"title": "D. Other Preference Learning Algorithms", "content": "Our PGT method consists of data filtering and preference learning. The aforementioned experiments are all based on DPO for convenience, but other preference learning algorithms like IPO (Azar et al.,"}]}