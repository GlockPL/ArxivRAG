{"title": "D-RMGPT: ROBOT-ASSISTED COLLABORATIVE TASKS DRIVEN BY LARGE MULTIMODAL MODELS", "authors": ["Matteo Forlini", "Giacomo Palmieri", "Mihail Babcinschi", "Pedro Neto"], "abstract": "Collaborative robots are increasingly popular for assisting humans at work and daily tasks. However, designing and setting up interfaces for human-robot collaboration is challenging, requiring the integration of multiple components, from perception and robot task control to the hardware itself. Frequently, this leads to highly customized solutions that rely on large amounts of costly training data, diverging from the ideal of flexible and general interfaces that empower robots to perceive and adapt to unstructured environments where they can naturally collaborate with humans. To overcome these challenges, this paper presents the Detection-Robot Management GPT (D-RMGPT), a robot-assisted assembly planner based on Large Multimodal Models (LMM). This system can assist inexperienced operators in assembly tasks without requiring any markers or previous training. D- RMGPT is composed of DetGPT-V and R-ManGPT. DetGPT-V, based on GPT-4V(vision), perceives the surrounding environment through one-shot analysis of prompted images of the current assembly stage and the list of components to be assembled. It identifies which components have already been assembled by analysing their features and assembly requirements. R-ManGPT, based on GPT-4, plans the next component to be assembled and generates the robot's discrete actions to deliver it to the human co-worker. Experimental tests on assembling a toy aircraft demonstrated that D-RMGPT is flexible and intuitive to use, achieving an assembly success rate of 83% while reducing the assembly time for inexperienced operators by 33% compared to the manual process.", "sections": [{"title": "1 Introduction", "content": "Collaborative robots are increasingly present and widespread in a wide variety of application domains, aiming to reduce human effort, structure collaborative work, and enhance productivity. The effectiveness of these robots is highly dependent on the intuitiveness and robustness of the human-robot interfaces and collaborative processes. Significant advancements have been achieved in the field of human-robot interaction (HRI) in recent years. However, these advancements often result in structured applications tailored to specific tasks that are challenging to configure and lack flexibility. Moreover, key elements of a HRI system, such as perception and reasoning, often rely on deep learning or reinforcement learning methodologies that require large amounts of training data. In this context, advances in HRI"}, {"title": "2 Detection-Robot Management GPT (D-RMGPT)", "content": ""}, {"title": "2.1 Problem Statement and Assembly Process", "content": "An aircraft toy from the Yale-CMU-Berkeley object and model dataset [8] is used as assembly workpiece to assess the performance of the proposed robot-assisted assembly planner. The aircraft is composed of eight different components, plus a power tool, screws, and nuts available to the operator, Fig. 1. The complete assembly can be achieved by following a number of different assembly sequences. However, if certain assembly precedence relationships are not respected, it is not possible to successfully complete the assembly of the aircraft.\nThe assembly process is recommended to start with the two fuselage components, component #1 and component #2, Fig. 2. The remaining components are placed in a magazine from which the robot can pick up and deliver them close to the operator, Fig. 1. The wheels are divided into a rear pair and a front pair. Since they are identical in color and shape, it is expected that the detection module DetGPT-V will find it challenging to distinguish between them and detect if the wheels are placed at the front or rear of the aircraft toy.\nThe operator can follow the assembly order defined by the D-RMGPT, or at any step of the assembly, the operator can decide to not follow the D-RMGPT recommendation and choose a different component to assemble on their own initiative. The D-RMGPT should be able to adapt to this unexpected scenario and continue the planning-delivering process to reach the complete assembly."}, {"title": "2.2 Proposed Approach", "content": "The robot-assisted assembly planner, D-RMGPT, guides the assembly process by suggesting and delivering the necessary components to the human operator in a feasible sequence. At each assembly step, D-RMGPT evaluates the current assembly status, determines which component is needed, and commands the robot to deliver it to the operator. D-RMGPT does not recognize the specific assembly step of the workpiece, instead it identifies the components that are already assembled, making the system less application dependent and consequently more flexible.\nThe detection module DetGPT-V, based on off-the-shelf GPT-4V, relies on one-shot images from two cameras at each assembly step to assess the current assembly status of the aircraft, identifying which components are already assembled. No textual descriptions of the components or the assembly process sequencing are provided. Only a single image X3 listing all the components and their assembly precedence relationships is required as input at the beginning of the process to fine tune GPT-4V, Fig. 2. The cameras are positioned to capture top and side images of the workbench where the aircraft is assembled, X\u2081 and X2, Fig. 1. For this setup, the aircraft workpiece should be placed on the workbench in a position where all components are visible from at least one camera, avoiding occlusions. DetGPT-V provides the list of components present in the current assembly stage, which is saved to a text file and passed to the next iteration through the assistant role. Once the list of components present in that assembly iteration is obtained, it is passed to the robot management and planner module R-ManGPT.\nR-ManGPT, based on off-the-shelf GPT-4, plans the next feasible component to assemble and generates the robot's discrete actions to deliver it to the human operator. Robot actions are defined by the robot end-effector poses (position and orientation) and the gripper state (open or close) for pick-and-place tasks. The components in the magazine are in a"}, {"title": "2.2.1 Prompt Structure", "content": "The components list image X3, Fig. 2, with a size of 768 \u00d7 2048 pixels, is analyzed in high detail mode so that GPT-4V does not decrease its resolution by downsampling. It has a cost of 1365 tokens. The current assembly step is captured by the two cameras, which capture top and side images X1 and X2 with a resolution of 680 \u00d7 480 pixels, each costing 425 tokens."}, {"title": "2.3 Experiments and Evaluation", "content": ""}, {"title": "2.3.1 System Setup", "content": "The human operator is working in front of a collaborative robot (5e, Universal Robot, Denmark). The robot picks up and delivers the required aircraft toy components from the magazine to the assembly area in front of the operator, Fig. 1. The magazine is also easily accessible to the human, who can choose to take any component manually. The"}, {"title": "2.3.2 Evaluation", "content": "In experiment #1, the D-RMGPT performance was evaluated in a set of 12 different tests, each conducted by a different inexperienced operator who did not know a priori a feasible assembly sequence for the aircraft toy. The operator's assembly actions are guided by the proposed D-RMGPT. For each test, we evaluated the false positives and false negatives in the detection phase, the total time required to complete the assembly, the average GPT-4V processing time, and whether the aircraft was successfully assembled."}, {"title": "2.3.3 Baseline Comparison", "content": "The component detection module DetGPT-V was compared with two state-of-the-art VLM-based object detectors, ViLD and OW-ViT. They have been used in popular foundation models robotics applications such as SayCan, SocraticModels and Grounding Detection. Precision and recall are calculated for each model. To make the comparison fair, since GPT-4V is used to identify the presence of the aircraft components but not its position in the image, ViLD and OW-ViT are also used only as detectors. A detected component is considered True Positive if it present in the image and it is identified by ViLD or OW-ViT, even if the bounding box is incorrect (IoU=0%)."}, {"title": "3 Results and Discussion", "content": "In experiment #1, for a set of 12 tests, each conducted by a different inexperienced operator, D-RMGPT assisted and guided the operators to complete the assembly of the aircraft toy, Fig. 5. These operators did not know a feasible assembly sequence beforehand. Table 1 shows the performance values, including false positives and false negatives, calculated by summing the number of incorrect component detections from all the images analyzed during each test. When a component detection is incorrect, the wrong component is indicated in the Table 1. The total time required to complete the assembly in the first 10 tests was, on average, approximately 311 seconds. The variability is mainly due to differences in the time required to process information on the OpenAI servers. This is the time D-RMGPT requires at each assembly step to process images and decide the next component to recommend for assembly. The average GPT processing time for each assembly test is shown in Table 1. Since each assembly involves 7 main calls to OpenAI servers, this processing time accounts for about 30% of the total assembly time.\nAs shown in Table 1, out of 12 tests, 10 resulted in successfully completing the assembly task. In the last two tests, the operator was not able to finish the assembly because the system mistakenly recognized the chassis as already installed when it was not. Consequently, the robot never delivered the chassis, which also prevented the installation of subsequent components that required the chassis to be previously assembled. While existing false positives and negatives did not always compromise the assembly, their impact depended on the component. In tests 1, 4, 7 and 8, the system was able to recover from incorrect component detections. Figure 6 shows an example assembly sequence suggested by D-RMGPT, performed by an inexperienced operator. This example demonstrates that even in the presence of false"}, {"title": "4 Conclusion and Future Work", "content": "This paper introduced D-RMGPT, a robot-assisted assembly planner based on Large Multimodal Models. D-RMGPT demonstrated its capability to guide an assembly process with a robot in the loop. The system achieved an assembly success rate of 83% while assisting inexperienced operators, without requiring prior information about possible assembly sequences or training data. The results suggest that D-RMGPT is both robust and flexible. It is robust because it can deliver a feasible assembly solution even in the presence of object detection false positives and negatives. It is flexible because it can recover from scenarios where the operator deviates from the assembly recommendations provided by D-RMGPT. This resilience ensures continuity and efficiency in the assembly process, even when unexpected actions are taken by the operator. Additionally, the results demonstrated that D-RMGPT reduces assembly time variability for inexperienced operators, with a 33% reduction in the assembly time when compared to manual assembly without any guidance from D-RMGPT or robot assistance. The detection module DetGPT-V, a key element of D-RMGPT, compared favorably with state-of-the-art VLM-based object detectors ViLD and OW-ViT in our specific case scenario. Overall, D-RMGPT is a promising and versatile tool, capable of operating in uncertain without requiring specific training data. The system effectively assisted inexperienced operators in assembling a product, demonstrating its potential as a robust and adaptable solution for intuitive human-robot interaction and collaboration.\nFuture work will focus on studying how the number of input images, which is currently limited to a top and a side view, can affect the detector's performance while balancing it with the GPT processing time. Additionally, D-RMGPT will be enhanced with the ability to learn operator preferences, further improving the user experience and making the system more intuitive to use."}]}