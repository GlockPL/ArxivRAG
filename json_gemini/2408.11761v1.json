{"title": "D-RMGPT: ROBOT-ASSISTED COLLABORATIVE TASKS DRIVEN BY LARGE MULTIMODAL MODELS", "authors": ["Matteo Forlini", "Giacomo Palmieri", "Mihail Babcinschi", "Pedro Neto"], "abstract": "Collaborative robots are increasingly popular for assisting humans at work and daily tasks. However, designing and setting up interfaces for human-robot collaboration is challenging, requiring the integration of multiple components, from perception and robot task control to the hardware itself. Frequently, this leads to highly customized solutions that rely on large amounts of costly training data, diverging from the ideal of flexible and general interfaces that empower robots to perceive and adapt to unstructured environments where they can naturally collaborate with humans. To overcome these challenges, this paper presents the Detection-Robot Management GPT (D-RMGPT), a robot-assisted assembly planner based on Large Multimodal Models (LMM). This system can assist inexperienced operators in assembly tasks without requiring any markers or previous training. D-RMGPT is composed of DetGPT-V and R-ManGPT. DetGPT-V, based on GPT-4V(vision), perceives the surrounding environment through one-shot analysis of prompted images of the current assembly stage and the list of components to be assembled. It identifies which components have already been assembled by analysing their features and assembly requirements. R-ManGPT, based on GPT-4, plans the next component to be assembled and generates the robot's discrete actions to deliver it to the human co-worker. Experimental tests on assembling a toy aircraft demonstrated that D-RMGPT is flexible and intuitive to use, achieving an assembly success rate of 83% while reducing the assembly time for inexperienced operators by 33% compared to the manual process.", "sections": [{"title": "1 Introduction", "content": "Collaborative robots are increasingly present and widespread in a wide variety of application domains, aiming to reduce human effort, structure collaborative work, and enhance productivity. The effectiveness of these robots is highly dependent on the intuitiveness and robustness of the human-robot interfaces and collaborative processes. Significant advancements have been achieved in the field of human-robot interaction (HRI) in recent years. However, these advancements often result in structured applications tailored to specific tasks that are challenging to configure and lack flexibility. Moreover, key elements of a HRI system, such as perception and reasoning, often rely on deep learning or reinforcement learning methodologies that require large amounts of training data. In this context, advances in HRI"}, {"title": "1.1 Related Work", "content": "In recent years, foundation models have seen widespread usage in different applications, ranging from text recognition and generation to coding, and even extending to the new frontier of image and video classification and generation [6, 35]. Due to their intrinsic characteristics of perception, reasoning and semantic knowledge [18, 42], foundation models such as LLMs [10, 1], VLMs [39] and LMMs [53] are promising intelligent agents for robotics [14, 52]. They enable robots to understand and reason about the surrounding environment, plan activities, and support decision-making in applications such as HRI and robot navigation. Human-robot interfaces can be easily handled via a simple prompt request, with the potential of making the interactive process more flexible, easier, intuitive, and adaptable to the needs of different users and application scenarios. However, they also present challenges concerning the understanding of real-world environments, dealing with constraints, and perceiving the physical state of a robot. Often, these systems lack important implicit information, leading to inaccurate and insecure actions that result in task failure. Recent studies address these challenges by providing extra awareness tools to the system, such as detectors like Open Vocabulary Object Detection or value functions used as grounding for LLMs [29, 55, 23]. The release of GPT-4 and GPT-4V(ision) makes it possible to combine perception and reasoning for both language and images, creating a synergy that achieves good performance when applied in HRI [53, 4, 33].\nIn computer vision, deep learning methods have demonstrated state-of-the-art performance in object detection and classification [31, 2, 59]. However, these methods present a number of challenges related to their reliability and require large and specific datasets for training [16]. Foundation models such as VLM, trained on massive amounts of general text-image pairs, can help mitigate these limitations by providing text descriptions of images [13, 30]. Contrastive Language-Image Pre-training (CLIP) and A Large-scale ImaGe and Noisy-text embedding (ALIGN) are examples of models that use contrastive learning by computing the similarity between text and image embeddings using textual and visual encoders [43, 25]. CLIP-Fields learns mapping from spatial locations, raw RGB-D and odometry data, to semantic embedding vectors [44]. The model retrieves information from pretrained image models by back-projecting the pixel labels to 3D space and training the output heads to predict semantic labels using an open-vocabulary object detector. The scene representation can then be used as a spatial database for segmentation, instance identification, semantic search over space, and 3D view localization from images. SimVLM is a VLM architecture that uses a transformer encoder to learn image-prefix pairs and a transformer decoder to generate an output text-based sequence [49]. VLM can identify and detect different components in an image by processing a text prompt that indicates which components need to be investigated in the scene [17]. Robots can use this information to grasp objects in a scene they are operating in for the first time without specific training. Grounding Languages-Image-Pre-Training (GLIP) unifies object detection and phrase grounding, which involves identifying the correspondence between phrases in a sentence and objects in an image, by passing both a text prompt and images to the system [32]. PartSLIP is a zero/few-shot method for 3D point cloud part segmentation by leveraging pretrained image-language model GLIP [34]. Open-World Language Vision Transformer (OWL-ViT) is a vision-language model designed to handle open-vocabulary object detection using a transforming architecture [39]. Grounding DINO reviews open-set object detector designs and proposes a tight fusion approach to better fuse cross-modality information [32]. MDETR is a framework for object detection that uses a convolutional backbone to extract visual features and the language model ROBERTa to extract text features [28, 38]. The features of both modalities are projected to a shared embedding space, concatenated, and fed to a transformer encoder-decoder that predicts the bounding boxes of the objects and their grounding.\nThe characteristics of foundation models make them a key element in the advancement of robotics [51, 54]. The SocraticModel integrates LLMs and VLMs to combine their different commonsense knowledge, exchange information with each other, and capture new multimodal capabilities without requiring finetuning [56]. This model has been applied in robotic planning and decision-making, using ViLD for object detection in the scene and LLM for task planning [17, 21]. SayCan combines low-level skills with LLM, enabling the LLM to provide detailed instructions on how to perform an abstract high-level task in a feasible way for a robot [3]. LLM ability to reason over natural language, without any additional training, allows them to guide robots in performing manipulation tasks in real world environment [24]. LLM have demonstrated potential to act as zero-shot human models for HRI [57]. In particular, GPT-3.5 and FLAN-T5 can model high-level human states without fine-tuning, achieving good results in predicting human behavior. Nevertheless, it has also been shown that LLMs are sensitive to prompt design and have reduced performance in tasks involving numerical, physical, or spatial reasoning. Mobile robot motion planning for dinner table arrangement has been addressed using LLM [12]. Knowing the objects and their dimensions, GPT-3 can provide the relative position"}, {"title": "2 Detection-Robot Management GPT (D-RMGPT)", "content": "An aircraft toy from the Yale-CMU-Berkeley object and model dataset [8] is used as assembly workpiece to assess the performance of the proposed robot-assisted assembly planner. The aircraft is composed of eight different components, plus a power tool, screws, and nuts available to the operator, Fig. 1. The complete assembly can be achieved by following a number of different assembly sequences. However, if certain assembly precedence relationships are not respected, it is not possible to successfully complete the assembly of the aircraft.\nThe assembly process is recommended to start with the two fuselage components, component #1 and component #2, Fig. 2. The remaining components are placed in a magazine from which the robot can pick up and deliver them close to the operator, Fig. 1. The wheels are divided into a rear pair and a front pair. Since they are identical in color and shape, it is expected that the detection module DetGPT-V will find it challenging to distinguish between them and detect if the wheels are placed at the front or rear of the aircraft toy.\nThe operator can follow the assembly order defined by the D-RMGPT, or at any step of the assembly, the operator can decide to not follow the D-RMGPT recommendation and choose a different component to assemble on their own initiative. The D-RMGPT should be able to adapt to this unexpected scenario and continue the planning-delivering process to reach the complete assembly."}, {"title": "2.1 Problem Statement and Assembly Process", "content": "An aircraft toy from the Yale-CMU-Berkeley object and model dataset [8] is used as assembly workpiece to assess the performance of the proposed robot-assisted assembly planner. The aircraft is composed of eight different components, plus a power tool, screws, and nuts available to the operator, Fig. 1. The complete assembly can be achieved by following a number of different assembly sequences. However, if certain assembly precedence relationships are not respected, it is not possible to successfully complete the assembly of the aircraft.\nThe assembly process is recommended to start with the two fuselage components, component #1 and component #2, Fig. 2. The remaining components are placed in a magazine from which the robot can pick up and deliver them close to the operator, Fig. 1. The wheels are divided into a rear pair and a front pair. Since they are identical in color and shape, it is expected that the detection module DetGPT-V will find it challenging to distinguish between them and detect if the wheels are placed at the front or rear of the aircraft toy.\nThe operator can follow the assembly order defined by the D-RMGPT, or at any step of the assembly, the operator can decide to not follow the D-RMGPT recommendation and choose a different component to assemble on their own initiative. The D-RMGPT should be able to adapt to this unexpected scenario and continue the planning-delivering process to reach the complete assembly."}, {"title": "2.2 Proposed Approach", "content": "The robot-assisted assembly planner, D-RMGPT, guides the assembly process by suggesting and delivering the necessary components to the human operator in a feasible sequence. At each assembly step, D-RMGPT evaluates the current assembly status, determines which component is needed, and commands the robot to deliver it to the operator. D-RMGPT does not recognize the specific assembly step of the workpiece, instead it identifies the components that are already assembled, making the system less application dependent and consequently more flexible.\nThe detection module DetGPT-V, based on off-the-shelf GPT-4V, relies on one-shot images from two cameras at each assembly step to assess the current assembly status of the aircraft, identifying which components are already assembled. No textual descriptions of the components or the assembly process sequencing are provided. Only a single image X3 listing all the components and their assembly precedence relationships is required as input at the beginning of the process to fine tune GPT-4V, Fig. 2. The cameras are positioned to capture top and side images of the workbench where the aircraft is assembled, X\u2081 and X2, Fig. 1. For this setup, the aircraft workpiece should be placed on the workbench in a position where all components are visible from at least one camera, avoiding occlusions. DetGPT-V provides the list of components present in the current assembly stage, which is saved to a text file and passed to the next iteration through the assistant role. Once the list of components present in that assembly iteration is obtained, it is passed to the robot management and planner module R-ManGPT.\nR-ManGPT, based on off-the-shelf GPT-4, plans the next feasible component to assemble and generates the robot's discrete actions to deliver it to the human operator. Robot actions are defined by the robot end-effector poses (position and orientation) and the gripper state (open or close) for pick-and-place tasks. The components in the magazine are in a"}, {"title": "2.2.1 Prompt Structure", "content": "The components list image X3, Fig. 2, with a size of 768 \u00d7 2048 pixels, is analyzed in high detail mode so that GPT-4V does not decrease its resolution by downsampling. It has a cost of 1365 tokens. The current assembly step is captured by the two cameras, which capture top and side images X1 and X2 with a resolution of 680 \u00d7 480 pixels, each costing 425 tokens."}, {"title": "2.3 Experiments and Evaluation", "content": "In experiment #1, the D-RMGPT performance was evaluated in a set of 12 different tests, each conducted by a different inexperienced operator who did not know a priori a feasible assembly sequence for the aircraft toy. The operator's assembly actions are guided by the proposed D-RMGPT. For each test, we evaluated the false positives and false negatives in the detection phase, the total time required to complete the assembly, the average GPT-4V processing time, and whether the aircraft was successfully assembled.\nIn experiment #2, an experienced operator was assisted by D-RMGPT to evaluate the system's flexibility in a set of 3 tests. At a given assembly step, the operator did not follow the D-RMGPT assembly sequence recommendations, allowing to evaluate how the system reacts and recover from such scenarios.\nFinally, in experiment #3, the assembly process guided by D-RMGPT was performed by 10 different inexperienced operators, without any prior knowledge of the assembly object or previous explanation. Then, another 10 inexperienced operators performed the same assembly process manually, without being guided by D-RMGPT and without robot assistance. The aircraft toy instructions manual for assembly was provided to them 1 minute before starting the assembly. The time taken to complete the assembly was recorded to compare the two testing scenarios."}, {"title": "2.3.1 System Setup", "content": "The human operator is working in front of a collaborative robot (5e, Universal Robot, Denmark). The robot picks up and delivers the required aircraft toy components from the magazine to the assembly area in front of the operator, Fig. 1. The magazine is also easily accessible to the human, who can choose to take any component manually. The"}, {"title": "2.3.2 Evaluation", "content": "In experiment #1, the D-RMGPT performance was evaluated in a set of 12 different tests, each conducted by a different inexperienced operator who did not know a priori a feasible assembly sequence for the aircraft toy. The operator's assembly actions are guided by the proposed D-RMGPT. For each test, we evaluated the false positives and false negatives in the detection phase, the total time required to complete the assembly, the average GPT-4V processing time, and whether the aircraft was successfully assembled."}, {"title": "2.3.3 Baseline Comparison", "content": "The component detection module DetGPT-V was compared with two state-of-the-art VLM-based object detectors, ViLD and OW-ViT. They have been used in popular foundation models robotics applications such as SayCan, SocraticModels and Grounding Detection. Precision and recall are calculated for each model. To make the comparison fair, since GPT-4V is used to identify the presence of the aircraft components but not its position in the image, ViLD and OW-ViT are also used only as detectors. A detected component is considered True Positive if it present in the image and it is identified by ViLD or OW-ViT, even if the bounding box is incorrect (IoU=0%)."}, {"title": "3 Results and Discussion", "content": "In experiment #1, for a set of 12 tests, each conducted by a different inexperienced operator, D-RMGPT assisted and guided the operators to complete the assembly of the aircraft toy, Fig. 5. These operators did not know a feasible assembly sequence beforehand. Table 1 shows the performance values, including false positives and false negatives, calculated by summing the number of incorrect component detections from all the images analyzed during each test. When a component detection is incorrect, the wrong component is indicated in the Table 1. The total time required to complete the assembly in the first 10 tests was, on average, approximately 311 seconds. The variability is mainly due to differences in the time required to process information on the OpenAI servers. This is the time D-RMGPT requires at each assembly step to process images and decide the next component to recommend for assembly. The average GPT processing time for each assembly test is shown in Table 1. Since each assembly involves 7 main calls to OpenAI servers, this processing time accounts for about 30% of the total assembly time.\nAs shown in Table 1, out of 12 tests, 10 resulted in successfully completing the assembly task. In the last two tests, the operator was not able to finish the assembly because the system mistakenly recognized the chassis as already installed when it was not. Consequently, the robot never delivered the chassis, which also prevented the installation of subsequent components that required the chassis to be previously assembled. While existing false positives and negatives did not always compromise the assembly, their impact depended on the component. In tests 1, 4, 7 and 8, the system was able to recover from incorrect component detections. Figure 6 shows an example assembly sequence suggested by D-RMGPT, performed by an inexperienced operator. This example demonstrates that even in the presence of false"}, {"title": "4 Conclusion and Future Work", "content": "This paper introduced D-RMGPT, a robot-assisted assembly planner based on Large Multimodal Models. D-RMGPT demonstrated its capability to guide an assembly process with a robot in the loop. The system achieved an assembly success rate of 83% while assisting inexperienced operators, without requiring prior information about possible assembly sequences or training data. The results suggest that D-RMGPT is both robust and flexible. It is robust because it can deliver a feasible assembly solution even in the presence of object detection false positives and negatives. It is flexible because it can recover from scenarios where the operator deviates from the assembly recommendations provided by D-RMGPT. This resilience ensures continuity and efficiency in the assembly process, even when unexpected actions are taken by the operator. Additionally, the results demonstrated that D-RMGPT reduces assembly time variability for inexperienced operators, with a 33% reduction in the assembly time when compared to manual assembly without any guidance from D-RMGPT or robot assistance. The detection module DetGPT-V, a key element of D-RMGPT, compared favorably with state-of-the-art VLM-based object detectors ViLD and OW-ViT in our specific case scenario. Overall, D-RMGPT is a promising and versatile tool, capable of operating in uncertain without requiring specific training data. The system effectively assisted inexperienced operators in assembling a product, demonstrating its potential as a robust and adaptable solution for intuitive human-robot interaction and collaboration.\nFuture work will focus on studying how the number of input images, which is currently limited to a top and a side view, can affect the detector's performance while balancing it with the GPT processing time. Additionally, D-RMGPT will be enhanced with the ability to learn operator preferences, further improving the user experience and making the system more intuitive to use."}]}