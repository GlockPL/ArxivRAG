{"title": "Retrieval-Enhanced Mutation Mastery:\nAugmenting Zero-Shot Prediction of Protein\nLanguage Model", "authors": ["Yang Tan", "Ruilin Wang", "Banghao Wu", "Liang Hong", "Bingxin Zhou"], "abstract": "Enzyme engineering enables the modification of wild-type\nproteins to meet industrial and research demands by enhancing catalytic\nactivity, stability, binding affinities, and other properties. The emergence\nof deep learning methods for protein modeling has demonstrated supe-\nrior results at lower costs compared to traditional approaches such as\ndirected evolution and rational design. In mutation effect prediction, the\nkey to pre-training deep learning models lies in accurately interpreting\nthe complex relationships among protein sequence, structure, and func-\ntion. This study introduces a retrieval-enhanced protein language model\nfor comprehensive analysis of native properties from sequence and lo-\ncal structural interactions, as well as evolutionary properties from re-\ntrieved homologous sequences. The state-of-the-art performance of the\nproposed PROTREM is validated on over 2 million mutants across 217\nassays from an open benchmark (ProteinGym). We also conducted post-\nhoc analyses of the model's ability to improve the stability and binding\naffinity of a VHH antibody. Additionally, we designed 10 new mutants\non a DNA polymerase and conducted wet-lab experiments to evaluate\ntheir enhanced activity at higher temperatures. Both in silico and exper-\nimental evaluations confirmed that our method provides reliable predic-\ntions of mutation effects, offering an auxiliary tool for biologists aiming\nto evolve existing enzymes. The implementation is publicly available at\nhttps://github.com/tyang816/ProtREM.", "sections": [{"title": "1 Introduction", "content": "Enzymes are fundamental components of synthetic biology systems. Wild-type\nenzymes often face limitations such as low catalytic activity, poor stability, and\ninsufficient binding affinity, which restrict their applications in both academic\nresearch and industrial practice. Through enzyme engineering, wild-type pro-\nteins can be modified to enhance these properties, enabling them to meet the\nrequirements of specific applications [8,15,36,37].\nWith the rapid expansion of protein databases and continuous advancements\nin artificial intelligence, deep learning methods offer new possibilities for enzyme\nengineering. Typically, a deep neural network for proteins is pre-trained on large-\nscale protein sequence data (with optional structural information) to learn to\nextract numerical representations of proteins, where the resulting embeddings are\nthen used to score and rank candidate mutants [14,19,23]. This prediction task,\nknown as mutation effect prediction, is often evaluated using high-throughput\ndatasets from deep mutation scanning (DMS), e.g., ProteinGym [22].\nExisting pre-training schemes can be broadly divided into three categories:\nsequence-based, structure-based, and evolution-based approaches. Sequence-based\nmethods are the most popular choice. They analyze the implicit pairwise rela-\ntionships between amino acids (AAs) to learn protein sequence representations.\nThe objective of pre-training such a protein language model is to predict un-\nknown AA types in a sequence from partial input, typically by autoregressively\ngenerating AAs along the sequence index [16,21] or by randomly masking a set\nof AAs and recovering them [13,27]. The second category of structure-based\nmethods incorporates topological inductive biases to capture stronger interac-\ntions between spatially adjacent AAs [7,37]. Geometric deep learning methods\nare typically applied to embed these associated structural constraints. The third\ncategory introduces multiple sequence alignment (MSA) data into the model to\nprovide evolutionary information about proteins [23,25]. MSA reflects protein\nconservation and the variation patterns occurring through natural evolution.\nAlthough lacking explicit functional labels, homologous sequences offer valuable\ninformation beyond a single sequence or structure for predicting protein assays.\nA natural question that arises when constructing implicit representations\nfor proteins using deep learning is: how can we effectively integrate the\nnon-orthogonal sequence, structure, and evolutionary characteristics?\nAlthough some works have considered encoding two types of information simul-\ntaneously, such as sequence and MSA information [25] or sequence and structure\ninformation [29,31,35], to the best of our knowledge, no work has yet integrated\nall three types. Consequently, protein representations learned from pre-training\nframeworks may lack crucial features that could significantly impact function.\nTo address this gap, we propose a new pre-trained protein language model with\na Retrieval-Enhanced Module (PROTREM). As shown in Fig. 1, we deliber-\nately design the model to uncover implicit representations of native features\nbased on sequence and structure tokens, where sequence-structure embeddings\nare integrated through disentangled multi-head cross-attention layers trained in\na BERT-style manner. The evolutionary representations are determined by an"}, {"title": "2 Results", "content": "This section presents in-silico analyses and experimental assessments to compre-\nhensively evaluate the performance of PROTREM across various properties and\nprotein types. The following content highlights PROTREM's performance on 217\nDMS protein assays with over 2 million records. We also extract low-throughput\nexperimental results from previous studies to assess the model's reliability in\npredicting multi-site mutants for specific proteins. Additionally, we used PRO-\nTREM to engineer phi29 DNAP and experimentally validated the activity and\nthermostability on 10 single-site mutants.\nPROTREM Ranks Top on ProteinGym Leaderboard with Significant\nPerformance Improvement\nThe first experiment assesses the model's performance on high-throughput open\nbenchmarks. We conducted predictive evaluations using the testing data and\nprocedures provided in the official Protein Gym repository 3. For each of the 217\nassays, we used the sequence, structure (predicted by ColabFold 1.5 [20]), and\nhomology sequences (retrieved by EVCouplings [6]) provided in the repository.\nFor each DMS dataset, the model predicts the fitness scores of the mutants and\ncalculates the weighted average Spearman's \u03c1 correlation between the predicted\nand ground truth scores. The standard deviation evaluates the stability of the\nmodel, which was computed using the bootstrap method defined in the official\ntest script. We compared the overall performance of PROTREM with the top 15\nmodels on the leaderboard 4. To avoid repetition, we retained only the highest-\nscoring version for each method and reported their ranks in the full leaderboard\nin the first column of the table. The complete leaderboard is publicly accessible at\nhttps://proteingym.org/benchmarks (26-Oct-2024). We provide the detailed\nconfiguration of PROTREM in Appendix B.\nAs reported in Table 1 and Table S5, our method demonstrates significant\nimprovements over baseline methods in both overall and property-specific evalu-\nations. Previous studies [22,31] found that structure-aware models (e.g., SaProt,\nProtSSN, and ESM-if1) are considered to perform better in binding and stabil-\nity predictions, while evolution-aware models (e.g., TranceptEVE, MSA Trans-\nformer) tend to achieve higher scores in activity prediction. In response, PRO-\nTREM integrates sequence, structure, and evolutionary information to deliver"}, {"title": "PROTREM Performs Reliable Prediction on Favorable Alkali Resis-\ntance and Binding Affinity of VHH Nanoantibody Mutants", "content": "The second evaluation validates PROTREM's effectiveness in fitness prediction\nfor both single-site and multi-site mutants. The engineering target is to enhance\nbinding affinity and alkali resistance in the VHH antibody. This antibody type\nplays a crucial role in the development of clinical antibody-based therapeutics,\nserving as an affinity ligand to selectively purify biopharmaceuticals [34]. In\nproduction, a CIP (clean-in-place) process is typically required, which involves\ncleaning with an extremely alkaline solution for 24 hours to eliminate impurities\nand contaminants. Therefore, improving the binding affinity and alkali resistance"}, {"title": "PROTREM Engineers phi29 DNAP Toward Significant Activity En-\nhancement with High Positive Rate", "content": "In the third experiment, we used PROTREM to enhance the catalytic activity of\nphi29 DNAP at higher temperatures. phi29 DNAP plays a key role in advancing\nisothermal amplification methods such as multiple displacement amplification\n(MDA) and rolling-circle amplification (RCA) [24]. It facilitates efficient DNA\namplification with low error rates, due to its remarkable strand-displacing ac-\ntivity with high fidelity and transcription processivity. While the existing phi29\nDNAP exhibits high processivity and robust strand displacement activity at"}, {"title": "3 Methods", "content": "This section overviews the construction of our proposed PROTREM, the pipeline\nfor mutation effect prediction, and the experimental methods used to assess\nthe new mutants for Phi29. Additional details, such as the data processing for\nmutants in the case studies, can be found in Appendices.\nOverview of PROTREM\nThe core of PROTREM consists of two main components. The first is a pre-\ntrained PLM, which is used to find a native representation of the input single\nprotein based on its sequence and structure (Fig. 1(a)). The sequence input is\nnaturally discrete and straightforward to tokenize. For the local structure of\nAAs, we employ a structure tokenization module to create a codebook. It is\nthen combined with sequence-level tokens using a disentangled cross-attention\nmodule. The other module extracts evolutionary representations by searching\nand aligning homologous family sequences of the template protein. Both native\nrepresentation and evolutionary representations are vector descriptions of AA\npositions and types. They are processed through an explicitly defined scoring\nrule to compute the final mutation fitness score.\nStructure Tokenization Module\nThe process of finding token representations for local protein structures involves\ntwo key steps, including local structure extraction and latent structure vocabu-\nlary assignment (Fig. 1(b)). Local structures are defined on a per-residue basis\n[30]. Consider a protein with L AAs. For each residue, we define its local struc-\nture by considering up to 40 neighboring residues within a 10\u00c5 spatial radius.\nThe local structure is described by an undirected graph with each node repre-\nsenting a residue, and two nodes are connected if their spatial distances are less\nthan 10\u00c5. The constructed L graphs are then fed into pre-trained GVP [9] layers,\nfollowing mean pooling operations to produce L vectors of 256 dimensions each.\nThese graphs are then fed into a pre-trained GVP, followed by average pooling\nto produce L vectors of 256 dimensions each. These 256-dimensional vectors in\nthe continuous space are further mapped to a 2048-dimensional discrete space\nusing K-means clustering, where each dimension represents an implicit vocabu-\nlary of structures. Additional details for training and inferencing can be found\nin Appendix A.1."}, {"title": "Disentangled Multi-Head Cross Attention", "content": "The native representation (Fig. 1(c)), i.e., the joint embeddings of protein se-\nquences and structures, are obtained by a masked language model. The core\npropagation rule is based on disentangled multi-head cross attention [4,13]. For\na protein of length L, we define the token inputs for its AA sequence and struc-\nture as R and S. For two arbitrary AAs at positions i and j, their attention\nscore Attn(i, j) is calculated based on R, S, and their relative position P:\nAttn(i, j) = {Ri, Si, Pij} \u00d7 {Rj, Sj, Pji}, (1)\nwhere Pij represents the relative position from the ith AA to the jth AA. After\nthe element-wise multiplication on (1), we retain only the five AA-relevant at-\ntention scores and rewrite them with the query, key, and value matrices, which\nare obtained via linear transformations during forward propagation:\nAttn(i, j) = RR + RS + R\u00bfP + SR+ PijR\n= Q(KR) + Q(KS) + Q (K) + Q(KR) + Q(KR)T. (2)\nThe notations follows conventional attention. The subscription denotes their as-\nsociation with the AA and the superscription denotes their association to the\ninput, e.g., Qf represents the query value for the ith AA on its sequence infor-\nmation, and K represents the key value with respect to the relative distance\nfrom the ith AA to the jth AA.\nSimilar to classic attention schemes, the initial attention scores obtained from\n(2) requires element-wise normalization by a scaling factor 1/\u221a5d with d being\nthe dimension of QR. Denote the unnormalized attention matrix as Hattn. We\nuse it to update the hidden representation for R, which reads:\nR = \u03c3( \\frac{H_{attn}}{\\sqrt{5d}} )V_R , (3)\nwhere \u03c3(\u00b7) is a softmax activation function, and VR is the value matrix of R.\nHomology Retrieval\nIn addition to the sequence- and structure-based native information, PROTREM also\nincorporates evolution information. We employ EVCouplings [6] and filters re-\nsults from Jackhmmer [2] by maximizing the number of significant hits with a\nbit score between 0.1 and 0.9. The second strategy follows the retrieval method\nin ColabFold [20]. We also considered different sequence retrieval strategies and\nstructure homologs retrieval (Appendix A.2). For simplicity, we replace the gap\ncharacter - with a special token <pad>. Suppose N homologous sequences are\nfound for a protein of length L, denoted by A \u2208 ZN\u00d7L. A counting matrix\nC\u2208 RL\u00d7V records the frequencies of AA types at each residue position, where\nCiv = \\frac{\\sum_{n=1}^{N} I(A_{ni} = v)}{\\sum_{v=1}^{V} I(A_{ni} = v)}, (4)"}, {"title": "Zero-Shot Fitness Scoring", "content": "For a given mutant, its fitness score is calculated by comparing the predicted log-\nits of the mutant residues with those of the wild-type residues. We first combine\nthe predicted native and evolutionary logits:\nOut = (1 \u2212 a). Onative + Oevo, (6)\nwhere the retrieval ratio a \u2208 [0, 1] controls the weight of integrating the intrinsic\nand evolution probability distributions.\nThe mutation fitness scores are calculated from Oout. For a mutant x, the\noverall fitness score Fr is obtained by summing the associated logit differences\nacross all mutation sites t\u2208T:\nFr = \\sum_{t \\in T} (O^{out}_{tv'} - O^{out}_{tv} ), (7)\nwhere at the position t, the mutant alters the residue type from v to v'. This\nscoring method captures the relative fitness by evaluating how much the muta-\ntion alters the predicted logit values compared to the wild type, with larger logit\ndifferences reflecting more significant deviations in fitness. For multiple muta-\ntions, the logit differences are summed over all the mutation sites, reflecting the\ncumulative effect of all alterations.\nRolling circle replication assay of phi29 DNA polymerase\nWe assessed the RCA reaction to simulate the circular template amplifica-\ntion activity of phi29 DNA polymerase. Rolling circle amplification (RCA) was\nconducted using the M13mp18 single-stranded DNA (ssDNA) template (NEB,\nUnited States) in a 20 \u00b5L reaction volume. The reaction mixture included 2 \u00b5L\nof M13mp18 template (0.2 \u03bcg/\u03bcL), 2 \u03bcL of 10\u00d7 RCA buffer (Thermo Fisher,\nUnited States), 2 \u00b5L of phi29 DNA polymerase (100 ng/\u03bcL), 4.8 \u00b5L of primer\nmix (2.4 \u00b5M final concentration), 0.8 \u00b5L of dNTPs (10 mM each), and 8.4 \u00b5L\nof DEPC-treated water (LABTOP BIO, China). The RCA was performed at\n42\u00b0C for 10 minutes, followed by a denaturation step at 65\u00b0C for 10 minutes.\nDNA concentrations were subsequently measured using the ssDNA Assay Kit\n(YEASEN, China) with a Qubit4 fluorometer (Thermo Fisher, United States).\nDifferential Scanning Fluorimetry (DSF)"}, {"title": "4 Discussion and Conclusion", "content": "Protein engineering is a central topic in synthetic biology. where novel mutants\nare typically found through rational design, machine learning, or pre-trained\ndeep neural networks. The latest pre-trained deep learning methods have demon-\nstrated great advances in identifying favorable mutations. To evaluate these\nmethods with minimum experimental costs, a standard workflow has been estab-\nlished to evaluate the models' scoring performance on large-scale DMS datasets.\nThe establishment of such an evaluation standard enables direct comparison\nbetween deep learning models. It also allows researchers without biological ex-\nperimental capabilities to independently assess their models.\nA protein embedding algorithm could benefit from incorporating additional\nprotein modalities to yield more expressive vector representations. For example,\nas demonstrated by [22,31], structure-aware models are generally more effective\nat enhancing stability and binding affinity, while sequence-centric models tend\nto improve activity. However, the potential insights provided by evolutionary in-\nformation remain less explored. For instance, relatively conserved regions in pro-\nteins are typically poor candidates to mutate, and AA conservations are primar-\nily drawn from evolutionary analysis on homologous sequence data. Moreover,\nexisting structural models often rely on predicted 3D structures especially when\ncrystallographic structures are unavailable. Since these predicted structures may\ncontain significant inaccuracies, homologous sequences could help mitigate the\nissue by providing an explicit coevolutionary relationship of residue pairs.\nOn the other hand, the existing evaluation framework relies on high-throughput\nDMS datasets. However, these experiments are prone to inaccuracies, and most\nentities are negative mutants. Thus, we extract low-throughput experimental\ndata and conduct multi-dimensional post-hoc analysis, offering an approach sim-\nilar to wet-lab validation without requiring new experiments. Furthermore, we\nselect an important enzyme with clear modification demands, perform single-site\nmutations, and validate these through experiments. All experimental and ana-\nlytical results indicate that our model significantly outperforms other methods\non large-scale datasets, demonstrates higher reliability on small-scale experimen-\ntal data, and identifies novel mutants with improved functions and properties in\npractical applications. We believe this work holds significant value for both the\ncomputer science and biology communities, from model design, method evalua-\ntion, and tool application to the future use of specific enzyme products."}, {"title": "A Details of PROTREM", "content": "A.1 Structure Tokenization\nStructure Encoder The structure encoder in this study is based on geomet-\nric vector perceptrons (GVP) [9], represented by the function \u03c0\u03bf(G) \u2208 Rlxd,\nwhere I is the number of nodes and d is the embedding dimension. The GVP\nis integrated with a decoder to form an auto-encoder, trained with a denoising\npre-training objective by perturbing Ca oordinates with 3D Gaussian noise and\napplying Brownian motion. After training on the CATH43-S40 dataset, we use\nthe mean-pooled encoder output as the final representation of local structures.\nThe encoding process for a graph G is defined as:\n\\frac{1}{l} \\sum_{i=1}^{l} \\pi_{O}(g_i) , (8)\nwhere gi represents the local structural features for the i-th node, and r\u2208 Rd is\nthe mean-pooled output.\nThe dataset used for training is derived from CATH43-S40, containing 31,270\nprotein structural domains after filtering. A validation set of 200 structures was\nrandomly selected, and the model was trained with the configuration that min-\nimized validation loss.\nLocal Structure Codebook The dataset used for training the structure code-\nbook consists of local protein structures extracted from the CATH43-S40 database.\nFor each protein structure, a sliding window method is applied along the residue\nsequence to select segments, with a specific residue as the anchor. Up to 40\nresidues within a 10 \u00c5 distance from the anchor are connected to form a star-\nshaped graph. For each pair of amino acids within this graph, if their Euclidean\ndistance is less than 10 \u00c5, a link is created. This process produces local protein\nstructures proportional to the length of each protein and the total number of\nproteins, resulting in 4,735,677 local structures from the CATH43-S40 dataset.\nThese substructures are encoded into embeddings via a Structure encoder. By\nvarying the parameter K, different structure codebooks are generated using the\nk-means clustering algorithm.\nA.2 Ablative Homology Retrieval Methods"}, {"title": "E Data Processing for Case Studies", "content": "We extracted the experimental data of 31 VHH antibody mutants from [11],\ncovering 1-4 site mutations for binding affinity and alkali resistance. The rele-\nvant experimental methods are provided in Appendix F. We used the average\nEC50 values from three repeated experiments as the quantitative metric for both\nassays.\nThe sequence of wild-type VHH antibody we used in this study is listed in\nTable S7. For the structures, we use AlphaFold3 server [1] instead of ColabFold\nto fold the amino acid sequence to get high-quality structures. For the homology\nalignment files, we use the UniRef100 database downloaded on October 2024.\nThe remaining processing methods and parameters remain unchanged (see Ap-\npendix B)."}, {"title": "F Experimental Methods", "content": "Binding Affinity of VHH Antibody\nNinety-six-well plates were coated with growth hormone protein at a density\nof 5 ng per well at 4 \u00b0C overnight. The plates were washed with 1 \u00d7 PBS'T\nthree times. Following blocking with 1% BSA in 1 \u00d7 PBS at 25\u00b0C for 2 hours.\nAfter washing three times with 1 \u00d7 PBS'T, the plates were incubated with\nserial dilutions of VHH proteins 100 \u00b5L per well (1:2, 1:4, 1:8, 1:16, 1:32, 1:64,\n1:128, 1:256, 1:512, 1:1024, and 1:2048) for 1 hour at 25\u00b0C. After washing three\ntimes with 1 \u00d7 PBS/T, 100 \u03bcL/well HRP-labeled Goat Anti-Mouse IgG(H+L)\n(1:5000) were added and incubated at 25'C for 1h. The plates were washed with 1\n\u00d7 PBS'T four times, a total of 100 \u00b5L/well L 3, 3\u2032,5, 5\u2032-tetramethylbenzidine was\nadded and incubated at 25 'C for 15 minutes in the dark. Finally, 100 \u00b5L/well\n2 M H2SO4 was added to stop the reaction and absorbance was measured at\n450 nm (TECAN, Swiss). The log(agonist) versus response Variable slope\n(four parameters) curves were analyzed to calculate EC50 which determines the\nstability of VHH after alkaline treatment.\nAlkaline pH stability test of VHH Antibody (ELISA)\nThe VHH antibodies (1.5 mg/mL) were treated an equal volume of 0.3 M or 0.5\nM NaOH for 24 hours, followed by pH adjustment using the same volume of 0.5\nM HCl, the supernatant was collected after centrifugation.\nNinety-six-well plates were coated with growth hormone protein at a density\nof 5 ng/well at 4 \u00b0C overnight. The plates were washed with 1 \u00d7 PBS'T three\ntimes. Following blocking with 1% BSA in 1 \u00d7 PBS at 25\u00b0C for 2 hours. After\nwashing three times with 1 x PBS'T, the plates were incubated with serial\ndilutions of VHH proteins 100 \u00b5L/well (1:2, 1:4, 1:8, 1:16, 1:32, 1:64, 1:128,\n1:256, 1:512, 1:1024, and 1:2048) for 1 hour at 25\u00b0C. After washing three times"}]}