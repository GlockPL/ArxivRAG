{"title": "Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration", "authors": ["Yifan Shao"], "abstract": "In recent years, attention mechanisms have significantly enhanced the performance of object detection by focusing on key feature information. However, prevalent methods still encounter difficulties in effectively balancing local and global features. This imbalance hampers their ability to capture both fine-grained details and broader contextual information-two critical elements for achieving accurate object detection. To address these challenges, we propose a novel attention mechanism, termed Local-Global Attention, which is designed to better integrate both local and global contextual features. Specifically, our approach combines multi-scale convolutions with positional encoding, enabling the model to focus on local details while concurrently considering the broader global context. Additionally, we introduce learnable \\( \\alpha \\) parameters, which allow the model to dynamically adjust the relative importance of local and global attention, depending on the specific requirements of the task, thereby optimizing feature representations across multiple scales. We have thoroughly evaluated the Local-Global Attention mechanism on several widely used object detection and classification datasets. Our experimental results demonstrate that this approach significantly enhances the detection of objects at various scales, with particularly strong performance on multi-class and small object detection tasks. In comparison to existing attention mechanisms, Local-Global Attention consistently outperforms them across several key metrics, all while maintaining computational efficiency. Code is available at the link.", "sections": [{"title": "1. Introduction", "content": "In recent years, significant progress has been made in the field of object detection, with many methods achieving remarkable improvements in performance metrics [8, 17, 20]. Nevertheless, researchers continue to explore new approaches to further enhance accuracy and efficiency, especially in challenging scenarios such as multi-class and small object detection. Attention mechanisms have emerged as an effective means to improve model performance [12, 25, 27, 28, 36], gaining considerable attention in deep learning due to their ability to significantly boost performance with only a limited increase in computational cost.\nAmong widely-used attention mechanisms, local and global attention are particularly noteworthy [19]. Local attention focuses on fine-grained, localized details within the input, capturing essential local information [33]. In contrast, global attention emphasizes the overall content and global context of the input, which is crucial for understanding broader relationships and larger patterns [1, 20]. Despite their strengths, both types of attention have inherent limitations: local attention often overlooks global dependencies, while global attention sacrifices detailed local information. Although many researchers have attempted to combine local and global features, these efforts often face challenges in effectively balancing the two, resulting in either suboptimal model performance or significant computational overhead that offsets performance gains [18, 29, 32, 35].\nIn this paper, we propose a novel attention mechanism called Local-Global Attention, designed to balance local and global features by integrating multi-scale convolution and positional encoding. This enables the model to capture both local details and global context. Additionally, we introduce learnable \\( \\alpha \\) parameters, allowing the model to dynamically adjust the balance between local and global attention in a data-driven manner, achieving consistent improvements across various datasets. Overall, Local-Global Attention ensures optimized feature representation across different scales, enhancing detection performance while maintaining low computational costs.\nSpecifically, the implementation of the Local-Global Attention mechanism is as follows: After initial feature extraction, we apply multi-scale convolutions with smaller kernels to capture localized features, which are then aggregated to form a local attention map. In parallel, larger kernel convolutions combined with positional encoding are used to extract broader global features, generating a global attention"}, {"title": "2. Related Work", "content": "In recent years, attention mechanisms have gained increasing attention due to their ability to selectively focus on the most relevant features within data, significantly improving the performance of various computer vision tasks. This section reviews the development and applications of attention mechanisms."}, {"title": "2.1. YOLO", "content": "YOLOv8 [14], as a classic model in the YOLO series [2, 13-15, 21, 26], builds upon previous YOLO versions with advancements in the backbone and neck architectures, anchor-free split Ultralytics head, and optimizations in speed and accuracy. It supports a wide range of computer vision tasks, including object detection, instance segmentation, pose/keypoint detection, oriented object detection, and classification."}, {"title": "2.2. Neural Networks", "content": "The MobileNet architecture family [10, 11, 24] has made significant progress in lightweight neural networks, particularly MobileNetV3 [10], which is a representative of lightweight neural networks, especially suited for mobile hardware. It combines Neural Architecture Search (NAS) with manually designed elements, utilizing inverted residual blocks to reduce memory usage and computational demands. Additionally, it integrates the Squeeze-and-Excitation attention [12] module, further optimizing feature representations and enhancing multitask performance.\nThe ResNet series, especially ResNet18 [9], addresses the vanishing gradient problem by introducing residual learning, making it possible to construct very deep networks without gradient issues. The residual connections enhance gradient flow, promoting the effective integration of low-level and high-level features, making it a robust backbone model for a variety of applications.\nThe backbone of YOLOv8 [14] is similar to CSPDarkNet53 [2], performing exceptionally well in real-time object detection. It efficiently extracts both spatial and semantic information from the input data, achieving a good balance between speed and accuracy, particularly suited for mobile and edge devices."}, {"title": "2.3. Attention Mechanisms", "content": "Attention mechanisms have significantly impacted many computer vision tasks, resulting in various methods aimed at improving feature representation. Below are some commonly used attention mechanisms and their primary strengths and limitations.\nMulti-head Self-Attention [25] has become a mainstay for capturing long-range dependencies across different parts of an input. By using multiple attention heads in parallel, it can focus on various aspects of the input data. While highly effective for modeling global context, it often struggles with capturing fine-grained local details, which is crucial for tasks requiring both local precision and global awareness.\nqueeze-and-Excitation Attention [12] enhances channel-wise feature representation by dynamically recalibrating channel weights, improving the model's ability to focus on essential features.\nConvolutional Block Attention Module [28] extends attention mechanisms by sequentially applying channel and spatial attention, refining feature maps step by step.\nWhile these attention mechanisms each have unique advantages, they also face challenges when it comes to integrating both local and global features in complex scenarios. To address these challenges, our work builds on these methods by proposing the Local-Global Attention mechanism, which combines multi-scale convolution and positional encoding for a more effective integration of local and global information."}, {"title": "3. Local-Global Attention Mechanism", "content": "The local-global attention mechanism aims to optimize feature extraction by integrating both local and global contextual information. This balanced strategy helps the model focus on fine details in specific regions while capturing broad information from the input data, providing a comprehensive understanding of both local and global contexts. The structure of the local-global attention mechanism is shown in Figure 2. In the following, we will describe each component of the local-global attention mechanism in detail."}, {"title": "3.1. Input Representation", "content": "The structure of the input tensor \\( X \\in \\mathbb{R}^{B \\times D \\times H \\times W_{in}} \\) includes:\n\\( \\cdot \\) B: Batch size, representing the number of samples processed in parallel.\n\\( \\cdot \\) D: Embedding dimension, indicating the depth of the features, providing rich descriptions for each spatial position.\n\\( \\cdot \\) H: Spatial dimension, representing the height of the feature map.\n\\( \\cdot \\) W: Spatial dimension, representing the width of the feature map.\nThe structure of the input tensor preserves both spatial and depth information, allowing the model to efficiently capture spatial relationships between different regions in the image or feature map [5, 12, 32]. Through the design of the depth dimension, multi-dimensional and multi-level feature representations are provided for each spatial position, thereby better handling the complex patterns and contextual information in the image."}, {"title": "3.2. Head Dimension and Scaling", "content": "To ensure the stability and efficiency of the attention calculation, we divide the embedding dimension D into multiple heads, with each head focusing on a different subset of the features:\n\\( head\\_dim = \\frac{D}{num\\_heads} \\) \\tag{1}\nEach head processes a different part of the data, introducing diverse representations and capturing different aspects of the input.\nTo stabilize the gradients, a scaling factor is applied to normalize the attention scores:\n\\( scale = head\\_dim^{-0.5} \\) \\tag{2}"}, {"title": "3.3. Multi-Scale Feature Extraction", "content": "To better capture information at different resolutions, we use multi-scale convolutional layers with residual connections to extract features from both local and broader contexts. The original input X is included as a baseline, ensuring that the model can access unchanged information during multi-scale interpretation."}, {"title": "Multi-scale Convolutional Layers.", "content": "For each scale i, a specific set of convolutions is applied to capture features at a particular granularity:\n\\( Y_i = f_{conv}(X) + X \\) \\tag{3}\nwhere \\( f_{conv} \\) is a combination of depthwise convolutions and 1 \u00d7 1 convolutions. Depthwise convolutions isolate each channel, effectively capturing spatial patterns, while 1 \u00d7 1 convolutions integrate cross-channel information. The residual connection with the input retains important information and improves gradient flow, aiding in deeper and more efficient feature extraction."}, {"title": "Multi-scale feature set", "content": "The multi-scale feature set, including the original input, is:\n\\( X_{multi\\_scale} = \\{X\\} \\cup \\{Y_i\\}_{i=1}^{num\\_scales} \\) \\tag{4}\nThis arrangement preserves both local details and broader context, enhancing the network's flexibility when handling diverse data patterns."}, {"title": "3.4. Positional Encoding", "content": "The positional encoding \\( PE \\in \\mathbb{R}^{1 \\times D \\times H' \\times W'} \\) aligns with the spatial dimensions of the input feature map. This encoding is adjusted as needed and combined with the features, enabling the network to maintain the spatial relationships between elements. This is particularly useful for tasks that are sensitive to the relative positions of features, such as object detection and segmentation."}, {"title": "3.5. Adaptive Scale Weights", "content": "To adaptively emphasize features from different scales, we learn the weights \\( \\alpha_i \\):\n\\( \\alpha = Softmax(W_{scale} * X) \\) \\tag{5}\nwhere \\( \\alpha \\in \\mathbb{R}^{B \\times (num\\_scales + 1) \\times H \\times W} \\). The Softmax operation normalizes these weights, forming a probability distribution across scales.\nApplying these adaptive weights, the multi-scale features are combined into a single, refined feature map:\n\\( X_{weighted} = \\sum_{i=0}^{num\\_scales} \\alpha_i X_{multi\\_scale}[i] \\) \\tag{6}\nThe weighted sum enables the model to dynamically focus on the relevant scales based on the input, optimizing its response to different feature patterns."}, {"title": "3.6. Attention Mechanism", "content": "The local-global attention mechanism simultaneously employs both local and global attention, efficiently capturing contextual details across different scales. This dual attention approach allows the model to focus on both the details in the input data and the broader structure, effectively bridging the gap between local features and global information."}, {"title": "3.6.1 Local Attention", "content": "To process local patterns at different granularities, local attention applies convolutions with varying kernel sizes \\( k \\in \\{3, 5, 7\\} \\):\n\\( Q_k, K_k, V_k = f_{attn}(X_{weighted} + PE) \\) \\tag{7}\nUsing multiple kernel sizes allows the model to capture local features ranging from fine to more general, making it particularly effective in recognizing targets and textures with varying levels of complexity.\nThe local attention scores are computed as:\n\\( energy_k = \\frac{Q_k K_k}{\\sqrt{head\\_dim}} \\) \\tag{8}\nThe scaling factor stabilizes the gradient flow and balances the feature dimensions, aiding the model in converging more efficiently.\nThe attention weights are then obtained:\n\\( attention_k = Softmax(energy_k) \\) \\tag{9}\nThis emphasizes the relevant features based on learned relationships, allowing each kernel size to focus on a unique subset of spatial information.\nThe output for each kernel size is:\n\\( local\\_out_k = attention_k V_k + X \\) \\tag{10}\nThe residual connection ensures that the model integrates the newly attended features while retaining the original information, enhancing the details while maintaining a solid baseline.\nThe final local output is obtained by averaging across different kernel sizes:\n\\( local\\_out = \\frac{1}{N} \\sum_k local\\_out_k \\) \\tag{11}\nThis allows the model to fuse fine-grained local information from multiple perspectives, forming a comprehensive local feature set that covers a wide range of visual elements."}, {"title": "3.6.2 Global Attention", "content": "Global attention uses a larger convolution kernel \\( k_g \\), enabling the model to capture global spatial patterns in the input:\n\\( Q_{global}, K_{global}, V_{global} \\) = \\( f_{attn}^{(kg)}(X_{weighted} + PE) \\) \\tag{12}\nThe larger kernel captures broader dependencies in the input, making this attention particularly suited for tasks requiring understanding of larger structures or long-range relationships.\nThe global attention output is:\n\\( global\\_out = Softmax(\\frac{Q_{global} K_{global}}{\\sqrt{head\\_dim}}) V_{global} + X \\) \\tag{13}\nBy utilizing a global perspective, this helps the model effectively gather high-level contextual information, complementing the fine-grained details captured by local attention, and facilitating a balanced understanding of both micro and macro structures."}, {"title": "3.7. Output Fusion", "content": "To integrate the outputs of local and global attention, we introduce learnable weight parameters \\( \\theta_{local} \\) and \\( \\theta_{global} \\) as weight factors:\n\\( out = \\theta_{local} local\\_out + \\theta_{global} global\\_out \\) \\tag{14}\nThe learnable weight parameters enable the model to dynamically adjust the importance of local or global features, allowing the network to focus according to task or data characteristics, thus providing responsive and context-aware feature representations. The combination of local and global features ensures that the model can adaptively balance the details and broader context of each input, forming a detailed and multi-dimensional feature set."}, {"title": "3.8. Final Convolution and Output", "content": "To further refine and compress the combined features, a 1 \u00d7 1 convolution is applied:\n\\( final\\_output = f_{conv}^{(1 \\times 1)}(out) \\) \\tag{15}\nThis operation helps to compress the rich information from both local and global attention into a more compact and efficient representation. The final output is optimized for downstream tasks such as classification or detection, providing both spatial details and context-aware information within a single feature map. Including this 1 \u00d7 1 convolution also helps reduce dimensions without sacrificing key information. By merging features at this stage, the network ensures minimal computational overhead while maintaining a high representation capability, making it a practical choice for applications where both accuracy and efficiency are crucial."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Datasets", "content": "VOC2012 dataset primarily aims to identify objects from multiple visual categories in real-world scenes (i.e., objects that are not pre-segmented). It includes a total of 11,530 images in the training and testing sets across 20 categories, with 27,450 annotated ROI objects and 6,929 segmentation annotations.\nVisDrone2019 dataset consists of 288 video clips, comprising 261,908 frames and 10,209 static images captured by various drone cameras across different scenes, weather, and lighting conditions. Over 2.6 million frequently observed objects (such as pedestrians, cars, bicycles, and tricycles) have been manually annotated with bounding boxes. Additionally, important attributes such as scene visibility, object class, and occlusion are provided to facilitate better data utilization.\nTinyPerson dataset is the first benchmark focused on detecting small objects (people) at a distance within large background contexts, containing 1,610 labeled images and 72,651 object annotations with bounding boxes.\nDOTAv1.0 dataset includes 15 common object categories such as ships, vehicles, and buildings, with 2,806 high-resolution aerial images and 188,282 instances. The image sizes range from 800 \u00d7 800 to 20,000 \u00d7 20,000 pixels. This dataset provides detailed bounding box annotations and is widely used to evaluate model performance on object detection tasks in aerial images, especially suitable for handling objects with significant variations in scale and orientation.\nCOCOminitrain dataset is a selected mini-train set from COCO2017 [16], containing 25K images, 80 object categories, and approximately 184K annotations, accounting for about 20% of train2017. It is particularly useful for hyperparameter tuning and reducing the cost of ablation experiments. The object instance statistics in COCOminitrain closely match those in train2017, and model performance on val2017 trained on MiniTrain is strongly positively correlated with that of the same model trained on train2017's full dataset.\nGWHD 2020 dataset is the first large-scale dataset for detecting wheat heads from in-field optical images, covering a wide range of wheat varieties from different continents. It includes 4,700 high-resolution RGB images with 190,000 annotated wheat heads. These images were collected from several countries, covering wheat at different growth stages and with diverse genotypes. The images are annotated with wheat head bounding boxes to evaluate trained models' performance on unseen genotypes, environments, and observation conditions.\nMNIST dataset is a classic dataset for handwritten digit recognition, containing 70,000 grayscale images, of which"}, {"title": "4.2. Experimental Setup", "content": "All datasets were trained according to YOLOv8 standards [14]. For the MNIST [4] and Fashion-MNIST [31] datasets, training was conducted over 20 epochs. TinyPerson [34] and GWHD2020 [3] datasets were trained over 200 epochs, while VOC2012 [7], VisDrone2019 [37], COCOminitrain [23], and DOTAv1.0 [30] datasets were trained for 100 epochs. Training was performed using a GPU 4090, with training parameters kept consistent with YOLOv8 default settings [14]."}, {"title": "4.3. Classification Experiments", "content": "We integrated various attention mechanisms into MobileNetV3 to evaluate classification performance on the MNIST and Fashion-MNIST datasets. The experiments were based on YOLOv8's default parameter settings, and performance of each attention mechanism was assessed through Top-1 and Top-5 accuracy (%) as well as computational complexity (GFLOPs)."}, {"title": "MNIST Dataset Experiment:", "content": "We integrated several attention mechanisms (MHSA, SE, CBAM, and our proposed Local-Global Attention) into the MobileNetV3 model and compared classification performance on the MNIST dataset. The results demonstrate that the model with Local-Global Attention achieved the highest Top-1 accuracy of 99.4%, further enhancing classification accuracy without increasing computational complexity."}, {"title": "Fashion-MNIST Dataset Experiment:", "content": "We further compared classification performance on the more challenging Fashion-MNIST dataset. As the Local-Global Attention mechanism achieved the highest Top-1 accuracy of 92.9%, standing out as the only attention mechanism to produce a positive gain in accuracy, while maintaining comparable computational complexity to other mechanisms."}, {"title": "4.4. Ablation Study on Attention Mechanisms", "content": "To evaluate the effectiveness of our proposed Local-Global Attention (LGA) mechanism, we conducted ablation experiments on the TinyPerson dataset [34] using the YOLOv8 framework with default parameter settings. These experiments aim to quantify the performance contribution of Local-Global Attention, as well as individual Local Attention (LA) and Global Attention (GA), on various backbone networks. Performance is evaluated using mAP50 and mAP50-95 metrics, with experiments grouped by three backbone types: MobileNetV3 [10], ResNet18 [9], and YOLOv8 [14]."}, {"title": "MobileNetV3 Backbone:", "content": "We integrated various attention mechanisms into the MobileNetV3 [10] backbone network. As shown in Table 3, our proposed Local-Global Attention mechanism achieved an increase of 0.92 in mAP@50 and 0.29 in mAP@50-95, outperforming other attention mechanisms. These results indicate that Local-Global Attention captures both local and global dependencies more effectively than traditional attention mechanisms. Although individual GA and LA contribute to performance gains, their combination (Local-Global Attention) yields the highest accuracy."}, {"title": "ResNet18 Backbone:", "content": "To assess the generalizability of Local-Global Attention, we integrated it into the ResNet18 [9] backbone network. It improved mAP@50 by 0.2 and mAP@50-95 by 0.14, achieving superior results compared to other attention mechanisms. While other mechanisms, such as SE, ECA,"}, {"title": "YOLOv8 Backbone:", "content": "Finally, we integrated Local-Global Attention and other attention mechanisms into the YOLOv8 [14] backbone network for the TinyPerson dataset. As shown in Table 5, Local-Global Attention achieved improvements of 0.7 in mAP@50 and 0.31 in mAP@50-95, outperforming other attention mechanisms. Compared to other mechanisms, Local-Global Attention exhibited greater adaptability, effectively balancing local and global attention to enhance detection accuracy.\nResults across these three backbone networks indicate that, while individual Local Attention and Global Attention improve model performance, the combined Local-Global Attention mechanism consistently yields the best results. By dynamically adjusting the balance between local and global features, Local-Global Attention significantly enhances target detection, making it a robust choice for handling complex detection tasks in constrained environments."}, {"title": "4.5. Applications", "content": "In this section, we evaluate the effectiveness of the proposed Local-Global Attention mechanism across various vision tasks by conducting experiments on multiple datasets. By comparing Local-Global Attention with commonly used attention mechanisms, we assess its performance on diverse benchmark datasets. The experiments encompass object detection tasks across different dataset scenarios to explore Local-Global Attention's applicability and transferability."}, {"title": "4.5.1 Object Detection Experiments", "content": "Experimental Setup: For this set of experiments, we used the VisDrone2019, VOC2012, and COCOminitrain datasets. All models were based on the MobileNetV3 architecture, optimized with SGD [22], and utilized YOLOv8 as the detection framework. We evaluated each attention mechanism's impact on model detection performance using mAP50 and mAP50-95 metrics.\nResults: As shown in Table 6, Local-Global Attention achieved the best performance on most metrics compared to other attention mechanisms. On the VisDrone2019 dataset, Local-Global Attention increased the mAP@50-95 score to 11.5, representing a 0.3 improvement over the baseline. For the VOC2012 dataset, Local-Global Attention increased mAP@50 by 0.1 and mAP@50-95 by 0.7. In the COCOminitrain dataset, Local-Global Attention also achieved the best results across all evaluation metrics. Overall, Local-Global Attention demonstrated superior capability in capturing detailed features and extracting global information, resulting in higher accuracy across multiple detection tasks."}, {"title": "4.5.2 Extended Dataset Experiments", "content": "Experimental Setup: In this set of experiments, we applied the models to the DOTAv1.0 and GWHD2020 datasets to evaluate performance in complex scenes and high-resolution images. The base model remained MobileNetV3, with the optimizer switched to Adam [22], and YOLOv8's default parameter settings were maintained.\nResults: Table 7 presents the results of different attention mechanisms on the DOTAv1.0 and GWHD2020 datasets. On the DOTAv1.0 dataset, Local-Global Attention achieved the highest mAP@50-95 score of 32.8. On the GWHD2020"}, {"title": "5. Conclusions", "content": "In summary, this paper introduces a novel Local-Global Attention mechanism that enhances feature representation by dynamically balancing fine-grained local features with broader global context. This enables the model to capture detailed local information while maintaining an understanding of the broader global context. The Local-Global Attention mechanism also incorporates learnable alpha parameters, which adaptively adjust the weights of local and global features, and further improves spatial understanding through positional encoding.\nExperimental evaluations on several benchmark datasets show that the Local-Global Attention mechanism performs well in standard scenarios, and demonstrates remarkable flexibility and robustness in more complex cases, where a fine balance between local details and global context is required, consistently outperforming traditional attention mechanisms. Moreover, it maintains high computational efficiency, offering a clear advantage in resource-constrained applications. These experimental results confirm that our approach not only improves model performance but also effectively addresses the challenges faced by existing attention mechanisms in capturing both fine-grained details and large-scale spatial structures."}]}