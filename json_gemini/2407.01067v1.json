{"title": "Human-like object concept representations emerge\nnaturally in multimodal large language models", "authors": ["Changde Du", "Kaicheng Fu", "Bincheng Wen", "Yi Sun", "Jie Peng", "Wei Wei", "Ying Gao", "Shengpei Wang", "Chuncheng Zhang", "Jinpeng Li", "Shuang Qiu", "Le Chang", "Huiguang He"], "abstract": "The conceptualization and categorization of natural objects in the human mind have long intrigued cognitive scientists and\nneuroscientists, offering crucial insights into human perception and cognition. Recently, the rapid development of Large\nLanguage Models (LLMs) has raised the attractive question of whether these models can also develop human-like object\nrepresentations through exposure to vast amounts of linguistic and multimodal data. In this study, we combined behavioral and\nneuroimaging analysis methods to uncover how the object concept representations in LLMs correlate with those of humans.\nBy collecting large-scale datasets of 4.7 million triplet judgments from LLM and Multimodal LLM (MLLM), we were able to\nderive low-dimensional embeddings that capture the underlying similarity structure of 1,854 natural objects. The resulting\n66-dimensional embeddings were found to be highly stable and predictive, and exhibited semantic clustering akin to human\nmental representations. Interestingly, the interpretability of the dimensions underlying these embeddings suggests that LLM\nand MLLM have developed human-like conceptual representations of natural objects. Further analysis demonstrated strong\nalignment between the identified model embeddings and neural activity patterns in many functionally defined brain ROIs (e.g.,\nEBA, PPA, RSC and FFA). This provides compelling evidence that the object representations in LLMs, while not identical\nto those in the human, share fundamental commonalities that reflect key schemas of human conceptual knowledge. This\nstudy advances our understanding of machine intelligence and informs the development of more human-like artificial cognitive\nsystems.", "sections": [{"title": "Introduction", "content": "The ability to categorize and conceptualize objects forms the bedrock of human cognition, influencing everything from\nperception to decision-making. When confronted with diverse objects, humans can often differentiate their categories and\nconcepts by making structured comparisons between them. This process is an essential part of human cognition in tasks ranging\nfrom everyday communication to problem-solving. In this cognitive process, our mental representations serve as a substrate,\naiding in the recognition of objects1,2, formation of categories3\u20135, organization of conceptual knowledge6,7, and the prediction\nof behaviors based on experiences. Therefore, understanding the structure of these representations is a fundamental pursuit in\ncognitive neuroscience and psychology8\u201310, underpinning significant research advancements in the field. For instance, various\nstudies have identified potential dimensions that organize these representations, such as animals versus non-animals11\u201314,\nnatural versus human-made15,16, manipulation versus shelter versus eating17, large versus small18,19, or hand- versus mouth-\nversus foot-related actions20.\nThe cognitive plausibility of deep learning systems has sparked significant debate21,22, with recent works often focusing on\ndiverse neural networks pretrained on limited datasets for specific computer vision tasks like image classification23\u201327. While\nthese endeavors have led to notable advancements28\u201331, the critical question remains unanswered: can human-like psychological\nrepresentations naturally emerge without task-specific training? LLMs, such as OpenAI's ChatGPT and Google's Gemini,\nhave emerged as potent tools in text and image understanding, generation, and reasoning. These models exhibit impressive\ncapabilities in tasks like object identification, information categorization, concept communication, and inference. Unlike"}, {"title": "Results", "content": "Our initial step involved selecting a wide range of objects that mirror real-life scenarios. The THINGS database44 was our\nselection, encompassing 1,854 living and non-living objects frequently encountered in daily life, illustrated in Figure 1a.\nNext, we needed a behavioral task paradigm conducive to understanding LLMs' mental representations and comparing them\neffectively with those of humans. While prior studies have leveraged classical behavioral assessments from various disciplines\nlike economics45, mathematics38,46, and psychology 38,46 to study LLMs' behavior, these approaches have been limited in\nassessing core dimensions of mental representation. We opted for the triplet odd-one-out task (referencing Figures 1b-d, see\nMethods) due to its proven effectiveness in modeling human mental dimensions 10,15,16,35,47.\nThen, the collection of a large-scale behavioral similarity judgment dataset involving these objects and tasks became\nimperative. Given the monumental scale of the task around 1.06 billion triplet judgments for 1,854 objects under the\nodd-one-out task\u2014it was unfeasible to execute comprehensively. However, leveraging insights from previous studies15,16, we\nadopted a strategy where a substantial approximation of the entire similarity matrix could be achieved using a small fraction\n(about 0.44%) of the total judgments. Human similarity judgments from 4.7 million trials have been collected using the online\ncrowdsourcing platform Amazon Mechanical Turk16. For LLMs, we collected their behavioral data from identical trials as\nemployed in the human experiments. Subsequently, we introduced a similarity space derived from these judgments using the\nSparse Positive Similarity Embedding (SPoSE) method15,36 (depicted in Figure 1e). This approach involved the initial random\ninitialization of object points in a high-dimensional feature space, followed by the optimization of object weights along these\ndimensions to craft an embedding predicting behavioral judgments in the triplet task. This process furnished a dimensional\nmodel of the similarity space, unveiling axes underscoring object variations and associating each object with scores on these\ndimensions. Finally, validation of the generalization capabilities of learned mental embeddings from LLMs on previously\nunseen datasets and their correlation with neural activity in the brain emerged as a crucial step. In this context, we turned to\nthe Natural Scenes Dataset (NSD)48 and the Representational Similarity Analysis (RSA) method49, as illustrated in Figures 1f-g."}, {"title": "Low-dimensional embeddings identified from LLMs are stable and predictive", "content": "Given the stochastic nature of the SPOSE\nmodeling method (see Methods), we conducted dozens of reruns with distinct random initializations, yielding embeddings\nwith slightly varying dimensions. Firstly, we assessed the correlation between any pair of the available dimensions and pruned\nout redundant dimensions (keeping only one) that exhibited correlations exceeding a specified threshold value (here, 0.4).\nThis was done because most dimensions in the reruns were redundant (consistently appearing in different runs, thus highly\ncorrelated with their counterparts), and only one randomly selected dimension needed to be retained from multiple highly"}, {"title": "Emergent object category information", "content": "It has been shown that natural object categories are an emergent property of mental\nembeddings derived from human similarity judgments15,35. To examine whether mental embeddings derived from LLM and\nMLLM show any emergent object category similarity structure, we used 18 unique high-level categories identified in the\nTHINGS database44 and used a cross-validated nearest-centroid classifier to predict category membership for each of the 1,112\nobjects of these categories (see Methods).\nAs shown in Figure 2h, the LLM embeddings demonstrated an 83.4% top-1 accuracy (chance performance: 5.56%), while\nthe MLLM achieved 78.3%. In contrast, human embeddings exhibited maximal object categorization capacity, with top-1\naccuracy of 87.1%. Figure 3 illustrates the global structure of the acquired embeddings through a multidimensional scaling\n(MDS)-initialized t-SNE plot (dual perplexity: 5 and 30; 1,000 iterations) containing 1,854 objects. Objects with similar\ndimensional values in the embedding are visually proximate in the plot, highlighting that items from the same category tend to\ncluster together across LLM, MLLM, and human data. Thus, these models have learned an embedding space that inherently\ncaptures some object category structures without explicit representational pressure to do so. Overall, outcomes from LLM\nand MLLM further validate the known distinctions between animate and inanimate items, as well as man-made versus natural\nobjects, aligning with prior human-centric studies15."}, {"title": "The embedding dimensions of the LLMs are interpretable and informative", "content": "The SPoSE modeling approach offers a\nnotable advantage by providing an interpretable embedding with accessible dimensions. While past research has delved into\nthe interpretation of multidimensional mental representations in humans15,16, this marks the inaugural exploration for LLMs."}, {"title": "Comparison of core dimensions in LLMs and Humans", "content": "The above analysis has already shown that LLM and MLLM,\nsimilar to humans15, also have the stable and predictive underlying mental representations and that their dimensions are\ninterpretable. Now, we would like to explore what kind of relationship exists between the core dimensions of LLMs and those of"}, {"title": "Relationship to the cerebral representational geometries", "content": "To the extent that brain-like capacity is indicative of human-like\nrepresentation in this similarity regime, we would expect these LLMs to have embedding spaces with at least some emergent\nbrain-like correspondence, but not as strong as human mental embedding space. Thus, we next examined the degree to which"}, {"title": "LLMs' embedding spaces have an emergent brain-like correspondence, relative to the human mental embedding space", "content": "This\nanalysis involved leveraging human brain responses sourced from the NSD dataset48, which provides comprehensive fMRI\ndata capturing neural responses from eight subjects exposed to numerous natural scene images (see Methods). This dataset\neffectively probes the cerebral representational geometries evoked by a wide array of objects and scenes.\nTo establish the connection between LLM and MLLM representations with brain responses across the whole brain, we\nemployed a method based on representational similarity known as searchlight RSA49 (refer to Figure 7a; see Methods). This\napproach entails fitting independent linear rating models for each embedding dimension, utilizing weighted combinations of\nCLIP image and text features50 to predict the univariate response profiles (see Methods). Subsequently, these dimension rating\nmodels are utilized to predict multi-dimensional pattern responses for new objects, leading to the establishment of the predicted\nrepresentational geometry within this embedded space. The comparison between this predicted RSM and the searchlight brain\nsector's RSM serves as a crucial indicator of how well the LLM's embedding aligns with that specific brain region.\nThe noise-normalized similarity score, averaged across eight subjects and across all voxels in a given brain Region of\nInterest (ROI) for each model, is depicted in Figure 7b. It should be noted that we did not adopt the same odd-one-out paradigm\nto infer the 66-dimensional embeddings for the CLIP model50 (here used as a strong baseline51), considering that CLIP cannot\neffectively follow task instructions. Instead, we directly used the visual and textual embeddings of the CLIP model itself,\ndenoted by CLIPvision and CLIPtext, respectively. When looking at average brain similarity score within an ROI, embeddings\nderived from human and MLLM perform substantially better than embeddings derived from LLM and the CLIP model, while\ndifferences between the human and MLLM are relatively small. It is important to note that these summarized results reflect\naverage responses across all voxels in a specific ROI, and therefore they do not reflect spatial patterns within it. Critically,\nwhen analyzing ROI averages especially in ROIs containing numerous voxels-meaningful spatial prediction patterns may be\nobscured (i.e., models with seemingly similar average comparisons might contain distinct fine-grained spatial information)."}, {"title": "Discussion", "content": "The present study provides a comprehensive investigation into the schema of object concept representations in LLM and MLLM,\nand their relationship to representations of the human mind and brain. By collecting two large-scale datasets of 4.7 million triplet\njudgments, we derived the stable and predictive 66-dimensional embeddings that capture the underlying similarity structure of\nreal-world objects. An interesting finding is that the object embeddings learned from LLM and MLLM judgments naturally\ncluster according to semantic categories, mirroring the structure observed in human mental representations. This suggests that\ndespite their fundamentally different architectures and training processes compared to the humans, LLM and MLLM have\ndeveloped human-like conceptual representations of natural objects. The interpretability of the dimensions underlying these\nembeddings further reinforces this notion, as they appear to reflect the key schemas of human object understanding.\nNotably, the MLLM demonstrated particularly strong performance in predicting individual behavioral choices, reaching up\nto 85.9% of the noise ceiling. This highlights the advantages of integrating visual and linguistic information, as the MLLM was\nable to develop more nuanced and human-like object representations compared to the language-only LLM. This aligns with\nprevious findings suggesting that multimodal learning can lead to more robust and generalizable representations56\u201358. Further-\nmore, the strong alignment between the model embeddings and neural activity patterns in many functionally defined brain\nROIs (e.g., EBA, PPA, RSC and FFA), provides further evidence that the object representation in MLLM share fundamental"}, {"title": "Other applications of the identified low-dimensional embeddings", "content": "The identified low-dimensional mental embeddings\nhave the potential for a variety of applications beyond the current study. For instance, these embeddings could be used\nto investigate the alignment and fusion of representations between humans and machines. Examining the correspondence\nbetween model and human embeddings could shed light on the common schemas governing object representations, and\ninform the development of more seamless human-machine interfaces and collaborative systems. From a practical standpoint,\nthe interpretable dimensions underlying the object embeddings could inform the development of more human-like artificial\ncognitive systems. By understanding the factors that shape object representations in LLMs, we can work towards designing\nArtificial Intelligence (AI) systems that can better align with human conceptual understanding and interact with human in a\nmore natural and intuitive way. This could have far-reaching implications for a wide range of applications, from intelligent\npersonal assistants to robotic systems.\nIn addition, the large-scale behavioral datasets collected in this study contribute a valuable resource for the research\ncommunity. These datasets can serve as an important benchmark for evaluating and comparing the representational abili-\nties of different AI models, as well as for studying the underlying cognitive processes that shape human object conceptualization."}, {"title": "Relationship to the other related studies", "content": "Both the human brain and modern large-scale pretrained AI models are intricate\nsystems, presenting challenges due to their complexity. Dimensionality reduction techniques are commonly employed to\nsimplify these systems and analyze their behavior. However, determining the optimal dimensions remains a persistent challenge.\nRecent research has utilized the \u201clow-rank\"59 and \u201cdistributed information bottleneck\"60 hypotheses to identify suitable latent\ndimensions, ensuring they capture the crucial aspects of the original high-dimensional network. These hypotheses can be\nsupported by our finding that the LLMs have developed human-like object representations from the aggregation of their\nfundamental dimensions, just as the human brain can give rise to rich and nuanced conceptual representations through the\ninterplay of relatively simple neural mechanisms. Exploring these low-dimensional yet powerful organizing structures could\nlead to a deeper understanding of the essential building blocks of cognition, both in biological and artificial systems.\nMore broadly, previous fMRI studies have unveiled diverse organizational principles within the brain for effectively\nprocessing and integrating external stimuli. For example, the primary visual cortex demonstrates retinotopy through the\ninterplay of visual eccentricity and angle selectivity61,62. Recent work on neural representations of emotions identified three\nspatially overlapping dimensions\u2014polarity, complexity, and intensity\u2014in the right TPJ, contributing to an \u201cemotionotopy\"\nmodel63. Moreover, several previous studies have demonstrated that the principle of dimension organization also applies to the\nrepresentation of other higher-order information64\u201370. Our study can be seen as an extension of these prior studies to conceptual\nrepresentations of large-scale natural objects, but the difference is that we do not require the underlying dimensions to be\northogonal to each other.\nTypically, the representations used by neural network models have been identified by scrutinizing the activation patterns of\nartificial neurons71\u201374. However, the efficacy of neuron-level approaches diminishes as AI systems expand in both depth and\nthe number of model parameters. In this context, an alternative approach inspired by cognitive psychology involves examining\nAl systems' representations through their behaviors. Cognitive psychologists have spent decades developing methods for\nelucidating the content of individuals' mental representations, such as the structure of object categories and the utilities assumed\nto different choice actions15,75. These mental representations, while not directly observable, can be inferred through the analysis\nof behavior. Our study diverges from those neuron-level analysis methods by focusing on recovering representations from\nLLMs using behavioral methods, making our work complementary to existing approaches. Actually, probing LLMs from a\ncognitive perspective has recently become a significant research direction32,76\u201380. This involves dissecting how LLMs process\nand understand information, mimicking cognitive processes observed in humans. By delving into the operational mechanisms\nof these large-scale models, researchers aim to uncover insights into various domains such as color processing81, emotion\nanalysis82,83, memory encoding84,85, morality86 and decision-making37,87,88. Understanding the parallels and divergences\nbetween human cognition and the functioning of LLMs opens up new avenues for exploring the frontiers of AI and cognitive\nscience34, shedding light on how LLMs can replicate, augment, or diverge from human-like cognitive abilities.\""}, {"title": "Limitations and future directions", "content": "One potential limitation of this study is that the analysis mainly focuses on ChatGPT-3.5\nand Gemini-Pro-Vision, which might not be fully representative. Nevertheless, we argue that these two models have already\nshown impressive problem-solving capabilities across diverse domains that were traditionally exclusive to humans. While\nthe primary analysis centers on these two models, the methodology can readily extend to other state-of-the-art LLMs like\nGPT-4V89, Claude-3, or LLaMa-3. Exploring the object representations within a diverse range of AI architectures could\nreveal the generalization ability of the identified key dimensions, as well as shed light on the unique strengths and limitations"}, {"title": "Methods", "content": "Stimuli and triplet odd-one-out task. In selecting stimulus objects", "standardized": "Given a triplet of objects {'[Object_A", ", ": "Object_B"}, {", ": "Object_C"}]}