{"title": "A redescription mining framework for post-hoc explaining and relating deep learning models", "authors": ["Matej Mihel\u010di\u0107", "Ivan Grubi\u0161i\u0107", "Miha Keber"], "abstract": "Deep learning models (DLMs) achieve increasingly high performance\nboth on structured and unstructured data. They significantly extended\napplicability of machine learning to various domains. Their success in\nmaking predictions, detecting patterns and generating new data made sig-\nnificant impact on science and industry. Despite these accomplishments,\nDLMs are difficult to explain because of their enormous size. In this work,\nwe propose a novel framework for post-hoc explaining and relating DLMs\nusing redescriptions. The framework allows cohort analysis of arbitrary\nDLMs by identifying statistically significant redescriptions of neuron ac-\ntivations. It allows coupling neurons to a set of target labels or sets of\ndescriptive attributes, relating layers within a single DLM or associating\ndifferent DLMs. The proposed framework is independent of the artificial\nneural network architecture and can work with more complex target labels\n(e.g. multi-label or multi-target scenario). Additionally, it can emulate\nboth pedagogical and decompositional approach to rule extraction. The\naforementioned properties of the proposed framework can increase ex-\nplainability and interpretability of arbitrary DLMs by providing different\ninformation compared to existing explainable-AI approaches.", "sections": [{"title": "1 Introduction", "content": "Deep learning models [51] (DLMs) are inspired by complex interactions of neu-\nrons in a human brain and are capable of memorizing information, detecting\npatterns, predicting and summarizing information (potentially from heteroge-\nneous sources), generating new data and much more. They have been applied to\nanalyses of both structured and unstructured data. Learning on tabular data,\ntime series prediction, image classification and segmentation, audio and video\nanalyses are only a few applications. This made them an important driving\nforce of advancements in science and industry.\nHowever, DLMs must often contain thousands of neurons grouped in differ-\nent layers, to obtain satisfactory performance. This makes majority of DLMs\nde facto black boxes whose function is incredibly difficult to understand [16, 9].\nCurrent work that aims to increase understanding of black box DLMs cre-\nates understandable rule-based model that explains predictions made by deep\nneural networks (DNNs) e.g. [17, 10], use general purpose tools to explain ma-\nchine learning models, such as LIME [46] or SHAP [31], e.g. [23, 42] or propose\nnovel models for explaining DNNs such as PatternNet and PatternAttribution\n[21]. Perturbation and gradient-based methods, methods that utilize decompo-\nsition of predictions and trainable attention models can also aid in explaining\nDNNs predictions. Various approaches aim to allow analyses and visualization\nof neurons, groups of neurons and layers [50].\nIn this work, we present a framework based on an unsupervised data min-\ning technique called redescription mining [44, 14, 34, 35], designed to increase\nunderstanding of an arbitrary DLM and allow relating different deep learning\nmodels. The output of redescription mining is highly interpretable, providing\nentity descriptions and revealing complex attribute associations. For this rea-\nson, redescription mining is increasingly used in various domains [44, 41, 36,\n45, 15, 20, 7, 29]. Recently, the task was also applied as an integral part of\nthe methodology designed to increase understanding of convolutional networks\napplied to the problem of spatiotemporal land cover classification [32, 40]. Here,\nredescription mining was tasked to relate neuron activation levels of convolu-\ntional network with spatiotemporal patterns contained in the input data. Such\nrelations could not be obtained without redescriptions, the result of redescrip-\ntion mining. Redescriptions are tuples of rules which are in equivalence relation.\nThey have a rich query language, allowing the use of negation, conjunction and\ndisjunction logical operators, which allows describing highly non-linear proper-\nties (e.g. the XOR function).\nThe framework proposed in this work is the first dedicated model-agnostic\nredescription mining based algorithm for interpretability and explainability of\nDLMs. It enables relating different layers of one DLM (intra-network), or re-\nlating layers of different DLMs (inter-network). Utilizing redecriptions, allows\nanalysing predictions made by DLMs, discovering the role of individual neurons\nand analysing their associations, understanding the effects of utilizing different\nmodel parameters during training, detecting similarities between different DLMs\nor changes caused by performing transfer learning. The proposed framework\nuses relatively short descriptions with rich query language (using conjunction,\ndisjunction and negation logical operators). It is naturally designed for parallel\ncomputation. This allows obtaining redescription-based descriptions, in reason-\nable time on small - 30 computing thread servers, even for selected layers of some\nstate of the art DLMs, such as AlexNet [27], EfficientNet [54] and BERT models\n[12]. Utilizing more sophisticated machines would allow analysing even larger,\nmore complex models. The proposed tool, aimed to be used by researchers,\ndevelopers and experts studying DLMs and by researchers studying problems\nfrom various domains, such as biology, medicine, economy which increasingly\nuse DLMs in their work, could foster many new scientific discoveries.\nAs it is demonstrated in Section 6, the tool can be used to understand a\nfunction of a neuron or a group of neurons in a DLM by relating the target group\nof neurons to the domain knowledge data (e.g. blood tests, cognitive ratings,\ngenetic factors). Understanding of the function of the discovered subgroups of\nrelated neurons from several layers of one DLM or from different DLMs can be\nadditionally increased through the analyses of the distribution of target class\non the set of re-described entities. In case of bad predictions, wrongly classified\nentities can be used to locate redescriptions that describe them. This reveals\nneurons of DLMs that potentially require adjustments and domain properties\nthat need to be further incorporated or augmented during DLM tuning, training\nand evolution. The procedure can also be used to study structural properties\nof DLMs. For example, detecting that one neuron from some layer of DLM can\nhave a similar function as a set of neurons from the corresponding layer of some\nother DLM. Information about structural properties of DLMs can be utilized in\nstudying DLMs training process, transfer learning, effects of different random\ninitializations on learning a target architecture, the process of distillation [19, 48]\nand many other fundamental properties of DLMs.\nShort summary of scientific techniques essential in the creation of the pro-\nposed approach: the redescription mining, Predictive Clustering trees and the\nCLUS-RM algorithm is provided in Section 2. The related work to the pro-\nposed approach is described in Section 3. Methodology is thoroughly described\nin Section 4, complexity and scalability analyses of the approach is provided in\nSection 5. A set of experiments showing: a) the ability of the proposed approach\nto detect related properties of different DLMs, b) be used to explain predictions\nof DLMs using rules and redescriptions, c) can be used to explain and related\nlayers of one or multiple DLMS, that it extends the functionality of existing ap-\nproaches and outperform them, d) that the approach can create redescriptions\ncontaining very useful knowledge for analyses of various properties of DLMs, is\nprovided in Section 6. Discussion analysing advantages and disadvantages of\nthe proposed approach is given in Section 7 and the conclusions of the provided\nstudy are summarized in Section 8."}, {"title": "2 Preliminaries", "content": "Redescription mining is an unsupervised data mining task that aims at discov-\nering tuples of rule-based descriptions, where all rules in a tuple describe mu-\ntually similar or desirably the same subset of data entities. Such tuples, called\nredescriptions, are discovered for various subsets of input entities. Each descrip-\ntion in a tuple offers a point of view (perspective) that increases understanding\nof a targeted subset of entities. Redescriptions allow detecting regularities and\nassociations between attributes (features) since rule-based descriptions in each\ntuple are in an equivalence relation. Redescription mining algorithms use a tab-\nular dataset D containing a set of attributes A, grouped into one or more views\nW\u2081 \u2208 W, i \u2208 {1,2,..., |W|} describing a set of entities E. Views usually repre-\nsent logical points of view on the available entities. Each view Wi is a table of\nentities described by attributes {A1, ..., \u0410\u043c; } CA of dimension Ex M\u2081. E.g.\nliving organisms or patients can be characterized by their phenotypic, genomic\nor proteomic properties. Each attribute belongs only to one view. A redescrip-\ntion R = (q1, q2,..., qn) is a tuple of rules (also called queries) q1,...qn. Each\nrule qi contains only attributes from the corresponding view W\u2081. A redescription\nset (RS) is denoted R.\nA set of entities described by a query qi is denoted supp(qi), whereas a set\nof entities described by a redescription supp(R) = \u0e01_1supp(qi). Redescription\naccuracy is quantified with the Jaccard index.\nJ(R) =  |\u2229i=1 supp(qi)|\n| \u222ai=1 supp(qi)|\n(1)\nJaccard index of 1.0 denotes that each query forming a redescription de-\nscribes only entities from a redescription support set and no other entities from\nthe dataset. Thus, q1 is true if and only if q2 is true etc. Jaccard index value\nlower than 1.0 denotes that the equivalence relation is not perfect. There must\nexist exceptions, entities described by some queries and not described by at least\none query forming some redescription. Statistical significance of a redescription\nis quantified by a p-value and computed from a Binomial or Hypergeometric\ndistribution. We utilize common formula derived from Binomial distribution.\np(R) = \u03a3 (|supp(R)|)(n)! \u03a0(Pi)^k (1-\u03a0(Pi)^(n-k))\nk=!                                                                                                                                                                                                                                                                                                                                                                                       (2)\n|E| denotes the number of entities in the dataset, p1 = |supp(q1)|/|E|, p2 =\n|supp(q2)|/|E|,...,pn = |supp(qn)|/|E| are the marginal probabilities of ob-\ntaining q1, q2,..., qn. More detailed description can be seen in the work [14].\nattrs(R) denotes a set of all attributes contained in redescription queries. Ma-\nchine learning model of type C with parameters Pis denoted as MP. We use\nLM to denote the k-th layer of the (deep) neural network MP and ni,k\nto denote the i-th neuron in the k-th layer of the (deep) neural network MP.\nRedescription Rex = (q1,ex, q2,ex), obtained with the proposed methodology:\nq1,ex: 0.98 < n23,3 \u2264 1.41 1.14 \u2264 n8,3 \u2264 2.32 0.74 \u2264 n21,3 \u2264 1.61\nq2,ex: 1.0 EcogOrgan \u2264 2.5 23.0 < RAVLT_IMMEDIATE < 55.0 ^ 15.0 \u2264\nADAS13 < 23.0 4.84 < FDG < 6.43 ^ 2 < FAQ < 9, relates neuron activations\nfrom the penultimate (third) layer of the feed forward neural network, trained\nto predict dementia levels of suspected patients, with the domain knowledge\ncontaining various cognitive tests and biological indicators taken to assess the\ncognitive dementia and the Alzheimer's disease of a set of suspected patients.\nThe crux is that the obtained redescription exposes an equivalence relation\nbetween the discovered queries, thus the neuronal activation of the described\ngroup of neurons co-occurs if and only if cognitive tests and biological indicators\nhave their values in the predetermined intervals. The strength of this equivalence\nis measured by the Jaccard index, which is 0.75 for the example redescription.\nThus, both queries are true for 3/4 of all patients for which either of the above\nqueries is true. Values of the cognitive tests and indicators, combined with\nthe Jaccard index information, showcase that the described group of neurons,\nwith the predetermined activations, jointly mostly (3/4 of the times) describe\ncognitively impaired patients. Redescription describes 12 patients in total, and\nfor 16 patients at least one of the aforementioned queries are true. Example\nredescription is statistically significant with p = 7.85.10-13. This demonstrates\nthat the example redescription could not be easily obtained by random pairing\nof queries describing similar number of entities. Functions of the discovered set\nof neurons can be definitely confirmed by analysing the distribution of target\nlabels of all patients described by either query. This distribution contains a\npatient diagnosed with the Alzheimer's disease, seven patients diagnosed with\nlate cognitive impairment and eight patients diagnosed with early cognitive\nimpairment. Thus, no cognitively normal people or people with small memory\nconcern are contained in the group. Using this fact, and the value intervals of\ncognitive tests and biological indicators, we can deduce that the combination of\ndiscovered neurons with the predefined activations are predominantly involved\nin the detection of patients with early and late cognitive impairment.\nPredictive Clustering trees [8, 22] are a generalization of decision trees, where\na variance reduction heuristic is used to choose the split point and the prototype\nfunction is used to compute the centroid of each set of entities belonging to the\nnode of a tree (node cluster). Centroids are used to make predictions and allow\ncomputing distances between different clusters. Utilization of centroids in the\nheuristic function used to compute the splitting point, allows for the use of both\ndata attributes and target labels to define clusters (sets of entities belonging to\na node in a tree). It also allows using various types of target labels (classes, tar-\ngets, multi-label, multi-target or hierarchical). Predictive Clustering trees are\nimplemented in the CLUS framework [2]. The CLUS-RM [35] is the redescrip-\ntion mining algorithm that uses two data tables with disjoint sets of attributes\n(data views) to produce redescriptions. It utilizes Predictive Clustering trees\nto obtain rules used to construct redescriptions. Predictive Clustering trees are\ntrained in CLUS-RM alternations, where nodes of one PCT, trained on one data\ntable, are used as targets to train the next PCT using the second data table.\nThe process utilizes multi-target regression and multi-label classification abili-\nties of PCTs. In each iteration, a pair of PCTs and potentially a supplementing\nrandom forest of PCTs are used to create new redescriptions. The algorithm\nuses a redescription set optimization procedure to construct the final set of\nredescriptions that is presented to the user. The CLUS-RMMW [37] is the gen-\neralization of the CLUS-RM algorithm that enables creating redescriptions on\ndatasets containing k views. The algorithm first computes a set of incomplete\n2-view redescriptions, utilizing available views in a pairwise fashion, and then\ncompletes these redescriptions by iteratively training PCTs on each remaining\nview. The approach internally stores a candidate set of redescriptions, which is\niteratively improved, updated and from which a final set of output redescrip-\ntions is created using a redescription set optimization procedure. The candidate\nset is mostly significantly larger than the output set, and larger candidate sets\nmostly allow obtaining output redescription sets of higher quality. Codes of the\nCLUS-RM and CLUS-RMMW approaches are available in [1]."}, {"title": "3 Related work", "content": "The approach proposed in this manuscript aims to explain, interpret DLMs and\nallow relating various DLMs using redescriptions tuples of rules. It is related\nto approaches that aim to explain DNNs using rule extraction [17, 59]. These\nmodels either build rule sets that aim to explain predictions of neural networks\nor DNNs, e.g. [4], describe neurons or layers by utilizing descriptions of neurons\nin previous layers, e.g. [60], or try to determine what caused a given output\nof a neural network, e.g. [6]. Divide and conquer approaches [17, 60] are the\nfirst that have been applied to explain the actual DNNs, previous approaches\nwere mostly used to explain simple artificial neural networks. Ribeiro et al. [47]\ncreated a model agnostic system to explain DLMs using rules of high precision\ncalled anchors. Wang [56] developed a model agnostic rule-based approach\ncalled Hybrid Rule Sets. It discovers subsets of data where rule-based model is\nclosely accurate as the targeted black-box model.\nAs opposed to aforementioned approaches that use rules to explain DLMS,\nthe proposed approach uses redescriptions. The main advantage of using re-\ndescriptions over rules is largely increased generality of the approach. Since\nredescriptions are tuples of rules, the approach produces and is ultimately able\nto utilize both. In addition to being able to create a surrogate model that ex-\nplains decisions made by some neural network or DNN, the proposed approach\nenables relating neurons and groups of neurons within one or between different\nDNN models. Moreover, the approach enables explaining neurons, groups of\nneurons and layers using original or supplementary data (target labels, knowl-\nedge bases etc.). Due to the fact that redescriptions consist of rules that are\nlogically in an equivalence relation, the approach allows detecting domain prop-\nerties that occur if and only if a particular neuron (or a group of neurons) fire\nwith a predetermined intensity. The proposed approach does not depend on the\narchitecture of a DLM and is applicable to fairly large DLMs, which are weak\npoints of the majority of available rule-based approaches.\nMethods such as SVCCA [43] and CKA [25] allow computing similarity of dif-\nferent representations, however they do not produce interpretable rule-based\ndescriptions that would explain the source of their similarity. The proposed\napproach closes this gap by detecting subsets of neurons in different DLMs with\nsimilar functions and discovers associations between their neuron activations.\nAs authors are aware, M\u00e9ger et al. [32, 40] are the only works that attempted\nusing redescription mining to explain convolutional networks applied to the\nproblem of land cover classification. The proposed tool is architecture specific\nand uses redescription mining as the out of the box tool, the latter hinders\nperformance and reduces the number of neurons that can be described (as shown\nby the experiments performed in this work). Furthermore, the approach is\nlimited to using only two views, neuron activations and original data, which\nprecludes relating different architectures or using knowledge bases to enhance\nunderstanding of the targeted model.\nMulti-view redescription mining [37] allows relating two or more models,\nhowever its use as an out-of-the-box tool enables describing only a limited num-\nber of neurons, mostly discovering their interactions, as shown by the performed\nexperiments. Thus, the authors find it inadequate for explaining and relating\ncomplex DLMs."}, {"title": "4 Methodology", "content": "We present a methodology ExItNeRdoM (Explaining and interpreting (deep) \u043f\u0435\u0438-\nral models using redescription mining). The presented post-hoc explainability\nmethodology [5], based on redescription mining, is specifically tuned to explain\nand relate neurons and groups of neurons of DLMs. It enables detecting func-\ntionally similar parts in different layers of the same or across different DLMs and\ndetecting correspondence between levels of neuron activations and domain data\nattributes or target label values. Using redescriptions instead of rules greatly in-\ncreases the generality of the proposed approach since the methodology produces\nand can utilize both rules and redescriptions. The approach allows providing\nboth local and global type of explanation, explanation by example, cohort ex-\nplanation, and provides text-based explanations in a form of redescriptions. It\nallows deep insight into the activations of groups of neurons and their relation\nto the target labels of the analyzed task. The methodology also allows detecting\nand analyzing wrongly assigned examples, which could be used to improve the\nmodel by focused training. Redescriptions, obtained by the approach, can be\nused as features to improve predictive performance of interpretable models [33],\ne.g. decision trees.\nTechnical novelties of the proposed approach compared to related redescrip-\ntion mining approaches are:\na) Exhaustive descriptions of neurons individually and in interactions, logically\ngrouped by the neuron id, allowing easy detection and exploration of knowledge\nof interest. Regular redescription mining approaches lack the logical division and\nhave significantly lower description coverage of neurons, especially individually.\nb) Modified binning procedure that takes into account specificities of neuronal\nactivations (low or no activations vs high activations) and allows obtaining re-\ndescriptions explaining neurons individually. The attribute binning in general\nform is only performed by the ReReMi approach [14], however single attributes\nare only used as a starting point for the greedy extension process, where more\ncomplex and accurate redescriptions are singled out. It is possible to limit\nthe number of attributes in a query to 1 with the following limitations: 1) no\nknowledge about the fact that the attributes represent neuronal activations, 2)\nno logical groupings of produced redescriptions, 3) no possibility to use more\nthan 2 data views, 4) obtaining redescriptions involving interactions of neurons\nrequires starting the redescription mining anew with different settings.\nc) Use of rules describing individual particular neuron as targets, guiding the\nsearch towards redescriptions revealing interactions of a chosen neuron with\nother neurons in one or more layers of a DLM. This is the novelty compared to\nthe workings of the redescription mining algorithm CLUS-RM [35], which does\nnot have rules describing individual neurons readily available. The CLUS-RM\nalgorithm [35] incorporates attribute diversity as one of the criteria in the opti-\nmization process, however it does not guarantee that majority or all attributes\nwill be represented.\nd) Adaptation of the constraint-based methodology [36] to multi-view setting\n[37] and its incorporation into the proposed methodology, allowing selective\nsearch for neuronal interactions involving selected neuron from a DLM. Besides\nbeing restricted to two views, methodology [36] provides no logical grouping of\nthe produced redescriptions using different constraint sets (or the process needs\nto be restarted for every constraint set).\ne) Incorporation of parallel (multi-threaded) computing, which allows perform-\ning explanations of multiple neurons (individually and in interaction) in different\nworking threads, allowing the proposed methodology to produce significantly\nmore detailed descriptions of studied DLMs, using competitive execution times."}, {"title": "4.1 General overview", "content": ""}, {"title": "4.2 Technical details of the ExItNeRdoM methodology", "content": "In this section, we present the technical details of the ExItNeRdoM methodology.\nSection 4.2.1 describes the building blocks of the methodology and explains their\ndifferences to the building blocks of the related multi-view redescription mining\nalgorithm, the CLUS-RMMW [37], whereas Section 4.2.2 describes how these\nbuilding blocks are utilized to obtain the ExItNeRdoM methodology.\n4.2.1 Building blocks of the ExItNeRdoM methodology\nThe first task performed by each thread in the ExItNeRdoM (see Figure 2) is\nuniform attribute binning with a width determined by Freedman-Diaconis rule\n[13]. It is denoted as performBinning in Algorithm 2. This type of binning is\nused because it is necessary to account both for balanced and imbalanced data\nscenarios. Using probability binning might mix a number of underrepresented\nclasses into the same bin, which is not adequate for our use case. However,\nwe do allow joining bins with high neuron activation values into a single bin\nif required to increase the bin size. Cases where bins contain small number of\nentities are also alleviated by the techniques of redesription mining that allow\nwidening the bin interval if required. Assume we have an attribute A with\nminimal value VA,min and a maximal value VA,max and that we obtained bins\n[VA,min, UA,t1], [VA,t1, VA,t2] \u00b7\u00b7\u00b7, [VA,tk-1, VA,max]. Each bin can be transformed\nin an analogue rule (query) of a form VA,ts-1 \u2264 A \u2264 VA,ts. This is denoted as\ncreateRules in Algorithm 2. Bin widening can, for example, be achieved by\ncreating a query VA,ts-1 < A \u2264 UA,ts V UA,ts \u2264A < UA,ts+1. Such approach is\ndenoted as refine in Algorithm 2. Bin joining can internally also be achieved\nthrough the use of negations, for example (VA,min \u2264A < va,t\u2081) describes\nentities from all bins except the first one.\nRules obtained from the bins of some selected attribute need to be related to\nthe attributes contained in remaining input views. This is done in two phases:\n1. Predictive Clustering tree [22] (PCT) is trained on one of the remain-\ning views, where each bin forms one class in a multi-class classification\nsetting (only one target variable is constructed). The obtained PCT is\ntransformed to rules, and the two sets of rules (a set obtained from at-\ntribute bins and a set obtained from the trained PCT) are used to cre-\nate 2-view redescriptions (see Figure 3). These two steps are denoted\nas transformToRules and createReds respectively in Algorithm 2. The\n2-view redescriptions are incomplete if input data contains more than 2\nviews and need to be completed in the following step. The procedure to\ncreate redescriptions using PCTs, where rules form target labels has been\nintroduced in [35]. The main modification in this work is using binning\nand rules obtained from attribute bins in redescription mining process,\nwhich allows obtaining redescriptions containing individual neurons and\ntheir activations."}, {"title": "4.2.2 The ExItNeRdoM methodology", "content": "The ExItNeRdoM methodology is described in Algorithm 2. This algorithm takes\nas input a sequence of domain data views and views representing selected layers\nof one or more DLMs (see Fig. 1, part a, tables Wi). The first step in the\ncomputation process of the ExItNeRdoM methodology is to collect attributes\nfrom all views of interest and to divide them evenly across the available worker\nthreads (lines 2 and 3). Lines 5 19 are executed in parallel in each worker\nthread. For each attribute assigned to the worker thread (line 6), we first\nperform binning (line 7), using uniform binning with a width determined by\nFreedman-Diaconis rule (see Section 4.2.1 for details).\nEach bin is transformed into a rule (line 8). Depending on the view of the\nselected attribute, a Predictive Clustering tree (PCT) [22] is trained on view 0\nor 1 using a multi-class classification dataset, where each bin forms one class\n(line 11). The goal is to obtain a tree having similar groups of entities in its\nnodes as described by the aforementioned rules/classes. The obtained PCT is\ntransformed into a set of rules (line 11), which are in combination with rules\nobtained from bins used to create redescriptions (line 13), see Section 4.2.1\nand [35, 34]. Since we have a multi-view setting, the createReds function\ntakes additional two parameters (4th and 5th) denoting views on which input\nrule sets were created. It also uses WviewTmp view, needed for computation\nwhen all logical operators can be utilized in redescription query construction.\nThe obtained 2-view redescriptions are extended so that a query containing\nrule obtained from some bin of a selected attribute is combined, using logi-\ncal disjunction operator, with a rule corresponding to some other bin of this\nattribute, if it increases overall accuracy of this redescription (line 14). For\nexample, redescription R = (0.2 \u2264 12,3 \u2264 0.7, weather = Sunny) might\nbe extended to R' = (0.2 \u2264 12,3 \u2264 0.7 V 0.8 \u2264 12,3 \u2264 0.9, weather =\nSunny) if J(R') > J(R). The obtained redescriptions are incomplete if in-\nput data contains more than 2 views, since they contain only two queries (e.g.\nR\" = (0.2 \u2264 n2,3 \u2264 0.7, weather = Sunny, ?) ). If some layer is related\nto more than one view (layers of the same or different networks, or views\nof original data), the procedure iteratively creates PCTs on remaining views\nwith redescriptions as targets and uses obtained rules to iteratively complete\""}, {"title": "4.3 Utilizing outputs of the ExItNeRdoM methodology to perform rule extraction", "content": "To perform rule extraction with the ExItNeRdoM, one must choose the views to\nbe used to construct rules and redescriptions (original data, predictions made\nby the model, layers of the model). Additionally, a selection approach of the\nappropriate rules/redescriptions that describe predictions made by the chosen\nmodel must be constructed. With these additions, the ExItNeRdoM can emu-\nlate pedagogical approaches by taking the original input data as a first view\nand the predicted target labels from some DLM as the second view. Utilizing\nviews describing layers of a DLM, the approach produces more advanced results\n(similar to Eclair, DeepRED). The ExItNeRdoM outputs both rules, building\nblocks of redescriptions (as regular rule extraction algorithms), and redescrip-\ntions. Redescriptions can provide information about complex associations of\nneurons between layers, their relation to original attributes or predictions made\nby the chosen DLM.\nThe ExItNeRdoM can be applied as is to multi-label or multi-target scenarios,\nwhere it is possible to include various interactions between different labels.\nThe presented optimization approach selects redescriptions based on re-\ndescription Jaccard index (accuracy), precision of mimicking input model pre-\ndictions and entity coverage, whereas rules are selected based on entity coverage\nand precision of mimicking model predictions, explained in Algorithm 3. Rule\nscore (line 12 analogue) is computed as: (numCov/NcovEntReds[index] + 2.\nprecision(R))/3. The approach can be additionally improved by the use of\nsubmodular optimization [57]. Algorithm 3 is applied to every fold of the data.\nAfter redescription and rule sets are obtained, the fidelity score [58] is computed\non each fold for the model used to obtain predictions. The optimization process\nfirst describes all entities with redescriptions of high precision and accuracy,\ntrying to use as small number as possible (lines 5 \u2013 15). After all entities are\ndescribed, it iteratively replaces existing candidates with better available can-\ndidates (lines 16-20). The same procedure is repeated for rules (line 21) and\nthe best obtained candidates are returned to the user (line 22)."}, {"title": "5 Complexity analyses and scalability study of the ExitNeRdoM", "content": "We first present the theoretical analyses of the time complexity of the approach.\nNext, we perform a scalability study with respect to the most influential param-\neters, the number of entities in the dataset, the number of neurons present in a\nselected layer of the DLM and the number of DLM layers being explained."}, {"title": "5.1 Complexity analyses", "content": "We will first analyse time complexity of computation performed by each thread.\nThe result will be used to compute time complexity of the ExitNeRdoM.\nAttribute binning of a selected node n, performed as a starting step of com-\nputation, has the worst time complexity of O(|E| log2 |E|) if all attribute values\nare distinct. Bin conversion to rules has a worst-time complexity of O(k\u00b7 |E|),\nwhere k equals the number of obtained bins. Given that k ~ |E|1/3 [13", "22": ".", "35": ".", "34": "the complexity can in-\ncrease maximally to O(z4.|E|) at average, although this scenario is unlikely due\nto internal constraints placed on redescriptions. Training PCTs to obtain rules\nto complete 2-view redescriptions to multi-view redescriptions has complexity of\n\u039f(\u03a3j\u00a3{view(n), i} ([V;|\u00b7 |E|\u00b7log3 |E|) + (|R|\u00b7|V;|\u00b7|E|\u00b7log2 |E|) + (|E|\u00b7log2 |E|)),\nthis is smaller or equal to O((|W| \u2013 2) \u00b7 ((|Vks| \u00b7 |E| \u00b7 log2 |E|) + (|R|\u00b7|Vks|\u00b7\n|E| log2 |E|) + (|E| \u00b7 log2 |E|))), where ks = argmaxj|Vj|, j \u2209 {view(n), i}. It\nshould be noted that the maximal size of a redescription set |R| is bounded,\nas well as the maximal number of PCT targets (see [37"}]}