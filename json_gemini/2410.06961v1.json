{"title": "SELF-BOOSTING LARGE LANGUAGE MODELS WITH SYNTHETIC PREFERENCE DATA", "authors": ["Qingxiu Dong", "Li Dong", "Xingxing Zhang", "Zhifang Sui", "Furu Wei"], "abstract": "Through alignment with human preferences, Large Language Models (LLMs)\nhave advanced significantly in generating honest, harmless, and helpful responses.\nHowever, collecting high-quality preference data is a resource-intensive and\ncreativity-demanding process, especially for the continual improvement of LLMs.\nWe introduce SynPO, a self-boosting paradigm that leverages synthetic prefer-\nence data for model alignment. SynPO employs an iterative mechanism wherein\na self-prompt generator creates diverse prompts, and a response improver refines\nmodel responses progressively. This approach trains LLMs to autonomously learn\nthe generative rewards for their own outputs and eliminates the need for large-\nscale annotation of prompts and human preferences. After four SynPO itera-\ntions, Llama3-8B and Mistral-7B show significant enhancements in instruction-\nfollowing abilities, achieving over 22.1% win rate improvements on AlpacaEval\n2.0 and ArenaHard. Simultaneously, SynPO improves the general performance\nof LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the\nwell-recognized Open LLM leaderboard.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have made remarkable progress in following user instructions\nand generating honest, harmless, and helpful responses (Achiam et al., 2023; Dubey et al., 2024).\nThis advancement is primarily achieved in the model alignment stage, which involves training\nreward models or LLMs directly on datasets curated from human preferences (Ouyang et al.,\n2022b; Bai et al., 2022a), typically employing Reinforcement Learning from Human Feedback\n(RLHF) (Ouyang et al., 2022b) or Direct Preference Optimization (DPO) (Rafailov et al., 2024).\nRecent research has made significant strides in model alignment by collecting high-quality prefer-\nence data (Hu et al., 2024), sampling and ranking on-policy responses (Meng et al., 2024; Wu et al.,\n2024b), or introducing LLM-as-a-Judge as substitutes for human preferences (Yuan et al., 2024;\nCui et al., 2023). However, most work still relies on static, pre-collected preference datasets from\nhuman or stronger LLM annotation. As LLMs improve rapidly, collecting large, high-quality prefer-\nence data for effective learning becomes increasingly challenging and costly, whether from humans\nor stronger models (Shi et al., 2023). According to Yin et al. (2024), directly sampling preference\npairs, which closely resembles an on-policy setting, can result in performance declines due to in-\nherent volatility and inefficiency. Therefore, constructing effective preference data to continuously\nimprove LLMs remains a critical research problem.\nIn this work, we present a self-boosting paradigm for LLM alignment, SynPO. This paradigm lever-\nages a small set of supervised fine-tuning (SFT) data to steer the generation of synthetic preference\ndata, thereby enabling LLMs to iteratively extend their capabilities through optimizing on synthetic\ndata. To support iterative preference learning across diverse scenarios, SynPO first trains a self-\nprompt generator to create large-scale synthetic prompts. Unlike previous approaches that require\nmore powerful LLMs and instruction examples (Wang et al., 2022), our generator utilizes only\nthe LLM itself and three random keywords as input. To generate preference pairs for the syn-\nthetic prompts, SynPO utilizes model-generated responses as rejected candidates and employs a\nresponse improver to refine these responses into chosen ones. The response improver comes from\ntwo straightforward intuitions: (1) LLMs excel at identifying distribution gaps between texts (Zhong\net al., 2022; Singh et al., 2022), and (2) refining a response is generally easier than generating a high-"}, {"title": "2 SELF-BOOSTING LLM WITH SYNTHETIC PREFERENCE DATA", "content": "SynPO is a self-boosting scheme designed to iteratively generate high-quality preference data. An\noverview of SynPO is presented in Figure 2. It begins with a small set of SFT data as seed data,\ndenoted as {(x, y)}=0, and the initial policy model \u03c0\u03b8\u03bf. By incorporating both the self-prompt\ngenerator and the response improver, SynPO provides sufficient prompts for iterative training and\nleverages the generative rewards in the synthetic preference data. This approach allows the policy\nmodel to make subtle improvements and gradually expand its boundaries."}, {"title": "2.1 SYNTHETIC PROMPT CREATION", "content": "Diverse and ample prompts are crucial for effective preference learning (Shi et al., 2023; Yuan et al.,\n2024; Song et al., 2024). Diversity facilitates generalization and a sufficient number of prompts\nallows data selection from a large candidate pool. In SynPO, we propose a novel strategy for syn-\nthetic prompt generation. We design a keywords-to-text task to guide the training of a self-prompt\ngenerator and create pseudo-label data from the seed SFT data.\nSelf-Prompt Generator Training We train the LLM itself to serve as a high-quality prompt gen-\nerator. For each prompt x in seed data, we randomly extract two keywords from x and one noise\nkeyword from x, where j \u2208 {1,2, . . ., n} \\{i}. The inclusion of the noise keyword enhances the\nrobustness of the prompt generator. It learns to filter out irrelevant keywords during training and\nensure that the generated prompts are fluent. This process yields a keyword list, ki for x. Next, we\ninsert ki into a prompt template (see Figure 3) to create a prompt and use (x, y) as the correspond-\ning completion. This process constructs training data for the prompt generator. We then optimize\n\u03b8\u03bf through SFT to transform the model into a prompt generator G. G possesses the capability to\ngenerate unlimited, diverse, and high-quality user instructions, controlled by the given keywords."}, {"title": "2.2 SYNTHETIC PREFERENCE GENERATION", "content": "A primary challenge in leveraging synthetic prompts is the lack of high-quality responses to pro-\nvide sufficient supervision (Li et al., 2024b). To address this, we introduce a response improver to\nenhance the quality of model responses to synthetic prompts. Pre- and post-improvement responses\nnaturally become the rejected and chosen candidates, respectively, with the chosen ones providing\nclear guidance on what approximates a gold standard response.\nResponse Improver Training In each iteration, we train the LLM as a response improver to\nfurther reduce the gap between policy model outputs and gold standard responses. Formally, let\n\u03c0\u03b8-1 denote the policy model at the beginning of the t-th iteration. We generate outputs from\nT\u04e9-1 for the seed data prompts: y(t-1),i ~ \u03c0\u03b8t\u22121(x), i \u2208 {1,...,m}. These outputs, along\nwith the seed data responses, form the training set for the response improver, following the template\nprovided in Appendix B. Each training example consists of the prompt and the policy model output\n(x,y(t-1),) as the input, and the gold standard response y as the output. We fine-tune \u03c0\u03b8 on the"}, {"title": "2.3 SYNTHETIC PREFERENCE OPTIMIZATION", "content": "The large-scale synthetic preference data naturally facilitate the multi-iteration process of self-\nboosting. In each iteration, we follow SimPO (Meng et al., 2024) for training; actually, our method\nis also compatible with other preference optimization training methods, such as DPO (Rafailov et al.,\n2024) and KTO (Ethayarajh et al., 2024). Denoting D as the synthetic preference data, we have:\n$\\theta \\leftarrow \\underset{\\theta}{\\text{argmin }}  E_{(x,y^+,y^-)\\sim D} \\left[\\log \\sigma (\\gamma \\cdot (\\log \\pi_{\\theta}(y^+|x_i) - \\log \\pi_{\\theta}(y^-|x_i))  -  \\frac{1}{\\beta} \\log \\frac{\\pi_{\\theta}(y^+|x_i)}{\\pi_{\\theta_{t-1}}(y^+|x_i)} - \\gamma \\cdot(\\log \\pi_{\\theta}(y^+|x_i) - \\log \\pi_{\\theta_{t-1}}(y^-|x_i))  \\right]$\n\u03c3 and \u03b3 are hyperparameters. Different from the vanilla SimPO, SynPO is a iterative process and all\nthe preference data are synthetic ones. The response improver continuously refines the generation\ndistribution to align with the ideal data distribution across multiple iterations.\nOverall, the response improver automatically learns to generate implicit generative rewards for the\noutputs of the LLM. Unlike using a discriminative reward model straightforwardly, this approach\nhelps the model learning to improve its outputs. We present the Synthetic Preference Optimization\nalgorithm in Appendix A. The entire optimization process is performed on synthetic preference\ndata, requiring only a small amount of high-quality data for validation. This strategy maintains two\nkey advantages: (1) Compared to the limited and hard-to-collect preference data, SynPO generates\nan unlimited amount of new self-synthetic data to meet the needs of iterative model improvement.\n(2) Using small, high-quality validation data prevents the model from deviating during training and\nconsistently guides the generation of more relevant synthetic data."}, {"title": "3 EXPERIMENTS", "content": "We carry out comprehensive experiments to demonstrate the effectiveness of SynPO in enhancing\nmodel alignment and improving general model performance."}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Models and Training We perform synthetic preference optimization on both Mistral-Base 7B and\nLlama3-8B Base. Following Meng et al. (2024), we employ supervised fine-tuned models as the\ninitial models. Specifically, the Mistral-Base 7B model (mistralai/Mistral-7B-v0.1) and the Llama3-\n8B Base model (meta-llama/Meta-Llama-3-8B-Base) were fine-tuned on the UltraChat-200k dataset"}, {"title": "3.2 PREFERENCE ALIGNMENT", "content": "We evaluate the model alignment performance on three benchmarks: AlpacaEval 2.0 (Dubois et al.,\n2024), Arena-Hard (Li et al., 2024c), and MT-Bench (Zheng et al., 2024). AlpacaEval 2.0 includes\n805 user prompts and utilizes pair-wise comparison with LLM-as-a-Judge. Specifically, the win rate\nagainst the baseline GPT-4 Turbo model is determined based on GPT-4 Turbo evaluation. Arena-\nHard includes 500 more challenging user queries, employing GPT-4-Turbo to judge the model re-\nsponses against GPT-4. MT-Bench features 80 multi-turn questions spanning various domains, with\nGPT-4 scoring the model responses out of 10.\nSingle-Turn Dialogues We compare the instruction-following and human preference alignment\ncapabilities on AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024c) in Table 2.\nCompared to the initial model post-SFT, SynPO shows sustained improvement over four iterations\nin win rate against GPT-4 Turbo or GPT-4. On AlpacaEval 2.0, Mistral-Base achieves a 27.4%\nincrease in length-controlled win rate and a 32.8% increase in raw win rate after four iterations.\nSimilarly, Llama3 exhibits a 26.7% rise in length-controlled win rate and a 30.5% improvement\nin raw win rate after the same number of iterations. In the more challenging Arena-Hard setting,\nSynPO reaches the highest win rate after the third iteration. Compared to the baseline methods,\nSynPO's iterative preference learning on synthetic data yielded more significant improvements.\nMulti-Turn Dialogues For the multi-turn benchmark MT-Bench, we report both the first-turn and\nsecond-turn scores (in Table 3) as well as a radar chart depicting performance across different ques-\ntion types (refer to Figure 6). The results indicate that SynPO enhances not only first-turn perfor-\nmance, with an increase of over 0.7 points, but also subsequent turns, with an increase of over 1.2\npoints. Compared to the initial model, SynPO shows improved performance across various question\ntypes, particularly in humanities, writing, STEM, and roleplaying."}, {"title": "3.3 DOWNSTREAM TASK PERFORMANCE", "content": "In terms of the general model performance on various tasks, we report the average scores on the\nwell-recognized Open LLM Leaderboard (Beeching et al., 2023) and 6 additional benchmarks\nfrom Language Model Evaluation Harness library (LLM Harness) (Gao et al., 2024). Open LLM\nLeaderboard (Beeching et al., 2023) is recognized as a standard assessment for the general per-"}, {"title": "4 ABLATION STUDIES", "content": ""}, {"title": "4.1 SYNTHETIC PROMPTS AND RESPONSES", "content": "We have demonstrated the diversity of prompts generated by SynPO in Section 2.1. To fur-\nther validate the self-prompt generator, we compare the generated prompts with manual collected"}, {"title": "4.2 IMPACT OF SEED DATA", "content": "SynPO involves training LLMs solely on synthetic preference data while using seed SFT data for\nvalidation. To investigate the maximum impact of the seed SFT data, we compare SynPO with the\nfollowing settings: 1) Seed SFT: Directly fine-tuning the LLM using seed data. 2) Seed PO: For each\nprompt in the seed SFT data, using the gold standard response in the seed data as the chosen response\nand the initial policy model response as the rejected response for preference optimization. 3) Seed\nSFT + PO: To avoid distribution shifts in directly using seed SFT data, we first obtain a model fine-\ntuned on seed data as in 1), then construct preference data using the model output and gold standard\nresponses. 4) Seed SFT + POm: Training on data from 3) for multiple epochs. The results on\nAlpacaEval 2.0 are presented in Table 7. Among the evaluated methods except for SynPO, setting\n3) is most analogous to SynPO, and proves to be the most effective. However, due to the limited\nquantity of seed data, the improvement is less than that achieved by iterative SynPO on synthetic\ndata. Training under such conditions for multiple epochs does not yield further improvements and\neven degrades performance. These findings validate that SynPO is a promising approach to construct\npreference data and maximize the utilization of minimal high-quality data."}, {"title": "5 RELATED WORK", "content": "Preference Data Construction Preference data are triplets consisting of user prompts, user-\npreferred responses, and non-preferred responses. Acquiring preference data from humans can be\nresource intensive, often constrained by the data collection platform (Ouyang et al., 2022a) or the\ncost of human annotation (Bai et al., 2022a; Ethayarajh et al., 2022; Nakano et al., 2021). To allevi-\nate this problem, researchers have started using teacher LLMs, such as GPT-4 (Achiam et al., 2023),"}, {"title": "LLM Self-Boosting", "content": "Previous work has advanced the self-boosting of LLMs by searching for\nhigh-reward behaviors (Tian et al., 2024; Zhang et al., 2024), using LLMs as judges to select re-\nsponses (Yuan et al., 2024; Wang et al., 2024; Wu et al., 2024a; Kim et al., 2024), and leveraging\nself-play strategies (Chen et al., 2024; Cheng et al., 2024; Luo et al., 2024; Wu et al., 2024b). These\nworks typically use a fixed set of existing prompts, limiting the LLM ability to learn across wide\nscenarios. Furthermore, the deterministic reward signals in these methods do not help the model\nrecognize subtle discrepancies between its responses and ideal responses. Prior to our work, Con-\nstitutional AI (Bai et al., 2022b) and SELF (Lu et al., 2023) used AI to generate non-deterministic\nfeedback for training. Constitutional AI used refinement data for reward models, while SELF em-\nployed GPT-4 for data generation and taught models self-refinement. However, these methods did\nnot utilize comparative information between pre- and post-revision texts for training."}, {"title": "Synthetic Data for LLMs", "content": "Acquiring human-generated data is costly and time-consuming, leading\nto the use of synthetic data for LLM training (Wang et al., 2022; Xu et al., 2023a; Li et al., 2024b).\nUnnatural Instructions (Honovich et al., 2022) and Self-Instruct (Wang et al., 2022) use seed in-\nstructions to generate new prompts, while WizardLM (Xu et al., 2023a) and WizardMath (Luo\net al., 2023) rewrite these instructions into more complex forms using ChatGPT. Seed topics also\nproduce textbook-like data (Li et al., 2023; 2024b) or self-chat dialogues (Xu et al., 2023b; Ding\net al., 2023) for instruction tuning. These methods often require strong LLMs or examples, benefit-\ning from model distillation (Xu et al., 2023a; Li et al., 2024b;a). Our approach uses the model itself\nto generate prompts and responses without needing carefully designed topics, extending beyond the\nmodel's inherent sampling space. This brings generative rewards to the LLM self-boosting process,\nparticularly benefiting initially weaker models. It resembles direct preference knowledge distilla-\ntion (Li et al., 2024d) but does not rely on large-scale teacher model responses for supervision."}, {"title": "6 CONCLUSION", "content": "We introduce self-boosting LLM with synthetic preference data, SynPO, a method for LLM align-\nment through iterative training on synthetic data. In SynPO, we innovatively base the entire training\nprocess on synthetic data and only employ limited SFT data for validation. SynPO diversifies the\nprompts and dynamically guides LLMs to improve their own output, using pre- and post-refinement\ngenerations as synthetic preference pairs for training in the next iteration. Experimental results show\nthat SynPO leads to significant improvements on both instruction-following capabilities and task\nperformance. This strategy sheds light on high-quality synthetic data generation and self-alignment\nwith minimal supervision, both of which are critical for the continuous development of LLMs."}, {"title": "LIMITATIONS", "content": "Our approach begins with a small SFT dataset and does not necessitate specifically labeled data for\ntraining a response improver. We assume that model-generated outputs closely resemble the gold\nstandard responses in the SFT, enabling them to serve as training data for the response improver.\nThis necessitates filtering out pairs where the gold standard is inferior to the model-generated output.\nSuch data can cause the response improver to rewrite text through paraphrasing or substantial alter-\nation, as the training data comprises pseudo pairs rather than minimally edited original responses.\nWhile prompting a more powerful LLM to generate rewriting-specific data, as suggested by Lu et al.\n(2023), can alleviate this, it sacrifices the benefit of learning the distribution gap.\nSynPO leverages a small, high-quality dataset repeatedly to guide synthetic data generation, making\nthe seed data quality vital. This approach requires only a small amount of high-quality data for\nvalidation, significantly reducing annotation costs. Additionally, recent work on direct on-policy\nsampling methods (Wu et al., 2024b), which do not need additional SFT data, shows considerable\npromise. After our final round of improvements, the model-generated responses are already of\nhigh quality. Future enhancements can incorporate on-policy preference optimization techniques to\nfurther refine the model."}, {"title": "APPENDIX", "content": ""}, {"title": "A ALGORITHM", "content": "We provide the overall pipeline of SynPO in Algorithm 1."}, {"title": "B PROMPT FOR RESPONSE-REFINER", "content": "The prompt template used for training and inference in the response improver is shown in Figure 7."}, {"title": "C EXPERIMENTAL DETAILS", "content": "Here we list additional experimental details for our implementation and experiments."}, {"title": "C.1 SELF-PROMPT GENERATOR TRAINING", "content": "The hyperparameters for self-prompt generator training are detailed below. During SFT for the self-\nprompt generator, we employ a learning rate of 1.0 \u00d7 10-6 for Mistral-Base and Llama3-Base, with\na batch size of 32, a warm-up ratio of 0.1, and an AdamW optimizer. We set the maximum sequence\nlength to 8,000 and train the model for 3 epochs.\nTo generate diverse synthetic prompts, we randomly sampled 1 million paragraphs from Refined-\nWeb (Penedo et al., 2023) and randomly selected 3 keywords from each paragraph. This process\nyields a large keyword list pool containing 1 million keyword lists. These keyword lists serve as the\ninput for the self-prompt generator in each iteration. For each iteration, we generate between 36,000\nand 72,000 keyword lists (depending on the filtering ratio at each iteration) and exclude lists con-\ntaining personal names or stopwords. We use vllm for inference and set the sampling temperature\nto 0.7."}, {"title": "C.2 RESPONSE IMPROVER TRAINING", "content": "As the model iterates and self-improves, it may produce responses superior to those of the seed\ndata. Our objective is for the response improver to learn from its deficiencies. Therefore, we iden-\ntify instances where the model output is inferior to the original response using the same scoring\nmodel as the filtering stage. This ensures that the response improver only learns positive optimiza-\ntions or semantic paraphrasing, rather than negative optimizations. Specifically, for a given \u00a7i, if\nthe score difference between the gold standard completion and the model completion exceeds the\nthreshold, we include this data for response improver training. In the Mistral-Base setting, we set\nthe PairRM scoring threshold to 0.20. In the Llama3-Base setting, the ArmoRM-Llama3-8B-v0.1\nscoring threshold is set to 0.02.\nSince the response improver data are automatically derived from SFT data conversion, the model\nalso learns paraphrasing. Using a more powerful model, such as GPT-4, to create data that introduce\nonly minor improvements for rewriter training is a promising research direction. However, to explore\nthe potential for self-boosting, we did not introduce additional data or stronger models for data\nconstruction, resulting in inevitable paraphrasing by the response improver.\nDuring SFT for the response improver, most training parameters are same to the parameters in self-\nprompt generator training. Some miner differences lie in: we set the max sequence length to 6,000."}, {"title": "C.3 RESPONSE IMPROVING AND FILTERING SETTING", "content": "To produce synthetic preference (chosen and rejected) completions for the t-th iteration, we utilize\nthe current policy model to generate a completion and employ the response improver to refine it. We\nuse vllm for inference, with the decoding temperature set at T = 0.7.\nDuring synthetic data filtering, we set a threshold 0.20 for PairRM scores and a threshold 0.02\nfor ArmoRM-Llama3-8B-v0.1 scores. In addition, we filter out all the data that contain over 50%\nrepetition patterns to avoid model collapse on synthetic data. In our experiments, we randomly\nincorporated 10,000 preference pairs from each iteration to the whole synthetic preference data."}, {"title": "C.4 OPTIMIZATION", "content": "As the parameter \u03b2 is crucial for achieving optimal performance in SimPO (Meng et al., 2024), we\nindividually search the \u03b2 in the range of [2, 4, 6, 8, 10, 12] for each optimization process. We use a\nfixed y = 1.6 for the Mistral-Base model and Llama3-Base."}, {"title": "C.5 BASELINES", "content": "In experiments involving iterative baselines, we control various conditions to ensure fairness. We\nmaintain the same training data size for both iterative baselines and SynPO. We adopt the SimPO\nloss (Meng et al., 2024) for preference optimization, as it is more effective than DPO (Rafailov\net al., 2024). We all use self-generated prompts, which have been shown to be superior to prompts\ngenerated by other methods, as validated in Section 2.1 and Section 4.1. All preference construction\nprocesses are iterated until performance no longer improves.\nRegarding the baseline models trained on UltraFeedback 61k, we straightforwardly adopt\nthe well-trained versions available from the SimPO repository at https://github.com/\nprinceton-nlp/SimPO."}, {"title": "C.6 DECODING HYPERPARAMETERS", "content": "For the AlpacaEval 2 (Dubois et al., 2024) evaluation, we use a sampling-based decoding approach\nto generate responses. Specifically, we employ vllm for inference, setting the temperature to 0.7 and\nthe maximum tokens to 2048 for both the Mistral-Base and Llama3-Base configurations. All other\nparameters adhere to the default settings in vllm. As for MT-Bench (Zheng et al., 2024), we adhere\nto the official decoding setup, which specifies varying sampling temperatures tailored to distinct\ncategories."}, {"title": "D ADDITIONAL DETAILS ON SEED DATA ABLATION", "content": "In setting 2), 3), and 4), to prevent cases where rejected responses are better than the chosen ones,\nwe filter the preference data using the same method as SynPO, specifically employing ArmoRM-\nLlama3-8B-v0.1 to select valid preference data. We fix the threshold at 0.02, as our search among\n{0, 0.1, 0.2} reveal that 0.02 consistently performs the best."}, {"title": "E API USAGE", "content": "For GPT-4 Turbo, we all use the latest turbo-2024-04-09 API on Azure OpenAI Ser-\nvice\nhttps://learn.microsoft.com/en-us/azure/ai-services/openai/\nconcepts/models#gpt-4-turbo."}, {"title": "F PROMPT ANALYSIS", "content": "Here we provide the prompt used for prompt topic and intention analysis in Figure 8, along with a\nmore detailed distribution bar plot for different intentions and topics in Figure 9. The topic word list\nis derived from UltraChat (Ding et al., 2023), while the intention word list was designed by us."}, {"title": "G EVALUATION DETAILS", "content": "For instruction-following ability evaluation, Table 8 presents the detailed information for three align-\nment benchmarks we use, including AlpacaEval 2.0, Arena-Hard and MT-Bench. Additionally, we\ndisplay the radar chart for MT-Bench scores on different prompt types"}]}