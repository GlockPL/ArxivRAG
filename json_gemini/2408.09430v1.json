{"title": "FASST: Fast LLM-based Simultaneous Speech Translation", "authors": ["Siqi Ouyang", "Xi Xu", "Chinmay Dandekar", "Lei Li"], "abstract": "Simultaneous speech translation (SST) takes streaming speech input and generates text translation on the fly. Existing methods either have high latency due to recomputation of input representations, or fall behind of offline ST in translation quality. In this paper, we propose FASST, a fast large language model based method for streaming speech translation. We propose blockwise-causal speech encoding and consistency mask, so that streaming speech input can be encoded incrementally without recomputation. Furthermore, we develop a two-stage training strategy to optimize FASST for simultaneous inference. We evaluate FASST and multiple strong prior models on MuST-C dataset. Experiment results show that FASST achieves the best quality-latency trade-off. It outperforms the previous best model by an average of 1.5 BLEU under the same latency for English to Spanish translation.", "sections": [{"title": "1 Introduction", "content": "End-to-end simultaneous speech translation (SST) translates incomplete speech input into text in a different language (Ma et al., 2020b), which is widely used in multilingual conferences, live streaming and etc. Compared to offline ST where speech input is complete, SST needs to decide whether to continue waiting or to generate more translation after receiving new speech input. A common approach in building performant SST streaming models involves pretraining for offline translation and optional finetuning for simultaneous translation (Agarwal et al., 2023; Communication et al., 2023). The quality-latency trade-off of simultaneous streaming models thus heavily depends on its offline performance.\nLarge language model (LLM) have recently demonstrated its potential to be a strong backbone of offline E2E ST (Huang et al., 2023; Zhang et al., 2023b). However, LLM introduces larger computation overhead compared to regular-sized models when applied to SST. Figure 1 shows that the computation latency of a LLM-based 7B model makes it inferior for real-time application.\nThe computation overhead of SST models comes from both encoding new speech input and decoding new translation. While the latter one has been heavily optimized for LLM (Pope et al., 2022; Kwon et al., 2023; Dao, 2024), the former one has not been optimized for SST. As new speech input arrives, most SST models re-encode the entire speech and start autoregressive decoding afterwards, ignoring the incremental nature of streaming speech input. More importantly, the LLM decoder needs to recompute hidden states due to the updated speech features, significantly slowing down the computation.\nIn this work, we propose a FAst LLM-based SST (FASST) method to avoid recomputation while maintaining its translation quality. We develop a blockwise-causal speech encoding technique that incrementally encodes new speech input and introduce incremental LLM decoding with consistency mask. We also design an 2-stage training strategy for FASST: 1) aligning speech encoder outputs with LLM embeddings using word-aligned contrastive loss (Ouyang et al., 2023) and 2) finetuning for SST using wait-k-stride-n policy (Zeng et al., 2021). Experiments on MuST-C dataset (Di Gangi et al., 2019) shows that our 7B model maintains competitive computation aware latency compared to 115M baselines while achieving consistent quality improvement of at least 1.5 BLEU score on English-Spanish direction.\nOur contributions are:\n\u2022 We propose FASST, one of the first efficient LLM-based methods for simultaneous speech translation.\n\u2022 We verify FASST on MuST-C dataset and it outperforms strong prior methods by 1.5 BLEU at the same latency on English-Spanish direction.\n\u2022 We further demonstrate that FASST can be generalized to other policies like hold-n and policies spending more time on encoding benefit more from FASST."}, {"title": "2 Related Works", "content": "End-to-End SST translates partial speech input into text in another language without generating intermediate transcription. A variety of speech segmentation techniques and policies have been proposed to optimize the quality-latency trade-off. Ren et al. (2020); Dong et al. (2022); Zeng et al. (2023); Zhang et al. (2023c) learn to segment streaming speech input by word boundaries. Zhang and Feng (2023) further learns to segment speech at moments that are beneficial to the translation. On the policy side, Ma et al. (2020b) adapts wait-k and monotonic multihead attention (MMA) from simultaneous text translation to SST model. Ma et al. (2023) further improves the numerical stability of MMA. Papi et al. (2023b) constructs source-target alignment with attention information to guide the simultaneous inference. Zhang and Feng (2022) decides whether to translate based on accumulated information of source speech. Pol\u00e1k et al. (2023) conducts blockwise beam search when doing incremental decoding. The translation quality of SST models depend on not only their policies, but also their offline performance (Agarwal et al., 2023). Recently LLM has been shown as a strong backbone of offline ST (Zhang et al., 2023b; Huang et al., 2023), but its computation overhead prevents it from being used in SST scenarios. FASST is one of the first LLM-based SST models with a reasonable quality-latency trade-off.\nEfficient ST To reduce the computation cost of ST models, Wu et al. (2020); Ma et al. (2020c); Raffel and Chen (2023); Raffel et al. (2023) use segments and explicit or implicit memory banks to calculate self-attention only within the segment. Zhang and Feng (2023); Chen et al. (2021); Wu et al. (2021) adopt unidirectional attention during speech encoding. These methods focus on encoder-side optimization and can be integrated with FASST.\nTranslation with LLM While LLMs are capable of zero-shot machine translation (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b), their performance can be further improved via in-context learning (Vilar et al., 2023; Zhang et al., 2023a), supervised and semi-supervised finetuning (Rothe et al., 2020; Yang et al., 2023; Zhang et al., 2023d; Xu et al., 2023). For simultaneous machine translation (SMT), Guo et al. (2024) propose a collaborative translation model with two LLM agents and Koshkin et al. (2024) design a finetuning strategy by adding a special \"wait\" token. Raffel et al. (2024) propose SimulMask to mask token connections under certain policy. SimulMask is a concurrent work with us and only works on text translation."}, {"title": "3 The FASST Method", "content": "In this section, we first review the problem formulation of simultaneous speech translation (SST) and then describe the architecture of our proposed model, FASST, followed by its training and inference strategies."}, {"title": "3.1 Problem Formulation", "content": "Simultaneous speech translation (SST) needs to generate translations while receiving streaming speech input. Let \\(S=(s_1, s_2, ..., s_{|S|})\\) be a speech waveform where \\(s_i\\) are real numbers. The streaming speech input is cut into segments \\(S_1, S_2, ...\\), and the SST model \\(P_\\theta\\) needs to emit partial translations \\(T_1, T_2, ...\\) after receiving each of them,\n\\[T_i \\sim P_\\theta(\\cdot | S_{\\leq i}, T_{\\leq i}).\\]\n\\(T_i\\) can be an empty string, indicating that the SST model needs more speech input to continue the translation. After receiving all inputs \\(S_1, S_2,..., S_m\\) and emitting all translations \\(T_1, T_2,..., T_m\\), we obtain the final translation \\(T = \\mathop{\\parallel}\\limits_{i=1}^{m} T_i\\) by concatenating all partial ones."}, {"title": "3.2 Model Architecture", "content": "As shown in Figure 2, our model is composed of a speech encoder, an adapter and a LLM decoder.\nBlockwise-Causal Speech Encoder (BCSE) extracts contextualized acoustic features from the raw waveform incrementally. It consists of several casual convolutional layers as the audio feature extractor and a blockwise-causal Transformer Encoder as the contextual encoder.\nOur causal convolutional layers are built upon non-causal ones. Denote \\(H_{in} \\in \\mathbb{R}^{l \\times d}\\) as the input vectors to non-causal convolution \\(Conv(\\cdot)\\) with kernel size \\(w\\). We add additional zero padding \\(Pad \\in \\mathbb{R}^{(w/2-1) \\times d}\\) to its left so that each output vector only depends on input vectors to its left, and remove the last \\(w/2 - 1\\) states to keep its output length the same as before,\n\\[H_{out} = Conv(Pad \\oplus H_{in})_{: -w/2 + 1}.\\]\nBesides, we apply blockwise-causal masking to Transformer Encoder. Define attention mask \\(M\\) of speech encoder as follows\n\\[M_{ij}^{Q,K} = \\begin{cases}\n1 & \\text{if } \\lfloor \\frac{i}{b} \\rfloor \\geq \\lfloor \\frac{j}{b} \\rfloor \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\nwhere \\(b\\) is the block size, i.e., the number of hidden states of the speech encoder corresponding to one segment, and \\(j_Q\\), \\(j_K\\) are row indices of query matrix \\(Q\\) and key matrix \\(K\\). The attention output of speech encoder during training can then be written as\n\\[O = Softmax(\\frac{QK^T}{\\sqrt{d}})V,\\]\nwhere \\(V\\) is the value matrix.\nAdapter receives speech encoder outputs and converts them to the LLM embedding space. It consists of two causal convolutional layers to reduce the length of speech encoder outputs by four and one linear layer to project features into the LLM embedding space. We call the adapter outputs as speech embeddings,\n\\[E_i = Adapter(BCSE(S_{\\leq i})).\\]\nLLM receives speech embeddings and embeddings of previously generated tokens to decode autoregressively according to a wait-k-stride-n policy \\(\\pi\\).\n\\[T_i \\sim LLM(\\cdot | E_i, T_{< i}, \\pi).\\]"}, {"title": "3.3 Training", "content": "As shown in Figure 2 (b), we employ a 2-stage approach to train our model.\nStage 1. Speech-text alignment. We align the speech embedding with LLM input embedding using word-aligned contrastive (WACO) loss. Both transcription embeddings \\(E^t\\) and speech embeddings \\(E^s\\) are grouped into word embeddings \\(W^t\\) and \\(W^s\\) by word boundaries. Word boundaries of speech are obtained through Montreal Forced Aligner 1. We treat speech and transcription embeddings of the same word as positive pair and others as negative pairs and train the speech encoder and the adapter with contrastive loss,\n\\[L_{CTR} = - \\mathbb{E} \\Big[ log \\frac{exp(sim(W_i^s, W_i^t) / \\tau)}{\\sum_{j} exp(sim(W_i^s, W_j^t) / \\tau)} \\Big],\\]\nwhere \\(\\tau\\) is the temperature and \\(sim(\\cdot)\\) is the cosine similarity function. LLM parameters are frozen during stage 1.\nStage 2. Finetuning for simultaneous translation. We finetune the entire model for simultaneous speech translation using wait-k-stride-n policy. Speech input is encoded into speech embeddings \\(E^s\\). Then we concatenate \\(E^s\\) with embeddings of reference translation and feed them to LLM. Position indices of both speech embeddings and translation embeddings start with the same index and ascend separately, so that text generation during inference does not affect the positional embeddings of speech embeddings.\nThen we randomly select \\(k \\in K\\) and mask out attentions from translation words with indices"}, {"title": "3.4 Efficient Simultaneous Inference", "content": "Figure 2 (c) illustrates how we conduct efficient simultaneous inference. FASST waits for k segments at the beginning and then start generating. Suppose now we have received \\(S_1, S_2,\u00b7\u00b7\u00b7, S_i\\) where \\(i > k\\).\nIncremental Speech Encoding The blockwise-causal mask of speech encoder allows us to use KV cache of previous speech segments to avoid recomputation. Let \\(H^Q = (h_1,\u2026\u2026,h_i)\\) be input vectors of the attention. We group them into blocks \\(B_j = \\mathop{\\parallel}\\limits_{(j-1)b+1}^{jb} h_k\\) where \\(1 \\leq j \\leq i\\) and \\(ib = li\\). The query, key and value matrices can be written as follows\n\\[Q = H^QM_Q = (B_1M_Q,\u2026\u2026, B_iM_Q)\\]\n\\[K = H^KM_K = (B_1M_K,\u2026\u2026, B_iM_K)\\]\n\\[V = H^VM_V = (B_1M_V,\u2026\u2026, B_iM_V)\\]\nHere the keys and values of previous segments \\((B_1M_K,\u2026, B_{i-1}M_K)\\) and \\((B_1M_V,\u2026\u2026, B_{i-1}M_V)\\) are stored in the KV cache. Now we only need the KV cache and the query \\(B_iM_Q\\), key \\(B_iM_K\\) and value \\(B_iM_V\\) of the latest segment to compute its attention output,\n\\[O_i = Softmax(\\frac{B_iM_Q K^T}{\\sqrt{d}}) V.\\]\nThis results in same output as running attention with full query, key and value matrices and a blockwise-causal mask. In this way, we reduce the time complexity of attention from \\(O(l_i d^2 + l_i d)\\) to \\(O(b d^2 + l_i b d)\\). Here \\(b\\) is a constant while \\(l_i\\) increases with the longer speech input.\nAdapting We store the speech encoder outputs of previous segments and concatenate them with encoder outputs of segment i. Then we pass them to the causal convolutional layers and the linear layer to obtain the speech embeddings \\(E_i\\)."}, {"title": "LLM Decoding with Consistency Mask", "content": "We partition speech embeddings \\(E_i\\) into \\(E_{i_1},\u2026\u2026\u2026, E_{i_l}\\) by speech segment. Following the wait-k-stride-n, the inputs to LLM are organized in the follow way\n\\[I = E_{i_1} \\oplus \u2025 \\oplus E_{i_k} \\oplus Emb(T_k) \\oplus E_{k+1} \\oplus Emb(T_{k+1}) \\oplus\\cdots\\oplus E_{i_l},\\]\nwhere \\(T_{1:k-1}\\) are empty strings and \\(T_j\\) consists of \\(n\\) words for each \\(k < j < i\\). Now we need to reuse KV cache of previous \\(i - 1\\) speech segments and partial translations to compute LLM hidden states of \\(i\\)th segment. Since speech embeddings are always ahead of text embeddings during training, we design a consistency mask to ensure speech segments can only attend to speech segments before them.\nLet \\(\\delta(z)\\) be indicator function that equals to 1 if \\(z_{th}\\) position of input \\(I\\) belongs to text and 0 otherwise. Define consistency mask \\(M^c\\) as follows,\n\\[M^c_{z_Q z_K} = \\begin{cases}\n0 & z_Q \\geq z_K \\text{ and } \\delta(z_Q) \\geq \\delta(z_K) \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}\\]\nLet \\(Q_i, K_i, V_i \\in \\mathbb{R}^{ti \\times d}\\) be query, key and value matrices of segment i and \\(K_{<i}\\), \\(V_{<i}\\) be cached key and value matrices. We first concatenate \\(K_i\\) and \\(V_i\\) with cache to obtain \\(K_{<i}\\) and \\(V_{<i}\\). Then attention output of segment i can then be computed as follows\n\\[O_i^t = Softmax(\\frac{Q K^T}{\\sqrt{d}})+ M^c_{:,:} V_i  V_{<i}.\\]\nAfter computing hidden states for speech segment \\(S_i\\), the LLM decodes n words autoregressively following the policy."}, {"title": "4 Experiment", "content": "We conduct experiments on two language directions of MuST-C v1.0 dataset (Di Gangi et al., 2019): English Spanish (En-Es) and English German (En-De). Each language direction contains around 400 hours of audio recordings. The average duration of utterances is less than 10 seconds. To simulate long speech scenarios, we concatenate adjacent utterances in the same TED talk so that each resulting utterance is around 30 seconds. We call the induced dataset as MuST-C-Long2 and the original one as MuST-C-Short. The duration distribution of both datasets are shown in Figure 4."}, {"title": "4.2 Model Configurations", "content": "Architecture We intialize our speech encoder with wav2vec 2.0 large model3 (Baevski et al., 2020) and our LLM with Llama2 7b Base4 (Touvron et al., 2023a). Wav2vec 2.0 large consists of a 7-layer convolutional feature extractor and a 24-layer Transformer encoder with 1024 hidden units. The block size of speech encoder is set to 50, i.e., around 1 second each block. The adapter connecting wav2vec 2.0 and Llama2 consists of two 1-D convolutional layers with kernel size 3, stride 2 and hidden size 1024 and a linear layer to project hidden size from 1024 to 4096 to match that of LLM embedding. Llama2 7b Base adopts a 32-layer Transformer decoder with hidden size 4096. It uses a vocabulary of size 32000 and rotary positional embedding (Su et al., 2023).\nTraining We train our model with mixed MuST-C-Short and MuST-C-Long data. The input speech is raw 16-bit 16kHz mono-channel waveform. We filter out speech that is shorter than 320ms during training. The batch size of stage 1 is 16.7 minutes and that of stage 2 is 14 minutes. We use AdamW optimizer with cosine learning rate decay. The warmup steps of stage 1 is 25k and that of stage 2 is 500 steps. The maximum learning rate of stage"}, {"title": "4.3 Evaluation", "content": "We use SimulEval (Ma et al., 2020a) to evaluate our models and baselines. All models are evaluated on MuST-C-Long tst-COMMON with batch size of 8 during inference to simulate heavy workload. Since SimulEval does not support batching multiple instances, we duplicate each instance by 8 during model forwarding. We report Sacre-BLEU (Post, 2018) for translation quality and computation-aware length-adaptive average lagging (LAAL-CA) (Papi et al., 2022) for latency. All models are evaluated using a single A6000 GPU."}, {"title": "4.4 Baselines", "content": "Wait-k-Stride-n LST waits k fixed-length speech segments and translates n words every time (Ma et al., 2020b; Zeng et al., 2021). We run wait-"}, {"title": "4.5 Main Results", "content": "Main results are shown in Figure 5. Our model achieves the best quality-latency trade-off for En-Es direction. Although wait-k-stride-n LST has a 2 BLEU score advantage at the latency of 8 seconds, its bidirectional encoding and inefficient use of KV cache prohibit it reaching latency smaller than 6 seconds. Comparing to EDAtt and AlignAtt which do not use LLM and has much less parameters (115M) than our model (7B), our model has similar computation aware latency while achieving a 1.5 BLEU score improvement. For En-De direction, FASST achieves competitive results to AlignAtt, with slightly better quality when latency is smaller than 4 seconds or larger than 6 seconds."}, {"title": "4.6 Ablation Studies", "content": "We conduct ablation studies to examine the impact of each component in our model.\nSpeech Encoder and LLM We replace wav2vec 2.0 large with HuBERT large (Hsu et al., 2021) and Llama2 7B base with Mistral 7B v0.3 base (Jiang et al., 2023) to examine whether FASST is sensitive to the choice of pretrained speech encoder and LLM. We also train Wait-k-Stride-n LST baseline with these configurations as a comparison. Results are shown in Figure 6. For all configurations, FASST has lower latency than the baseline. FASST with HuBERT results in the best quality when the latency is around 5.5~6.5 seconds and FASST with wav2vec 2.0 becomes the best when the latency is smaller than 5.5 seconds.\nIncremental Encoding and Decoding We ablate the incremental speech encoding and the incremental LLM decoding to examine their impact. For encoding, we use the same architecture but recompute the entire speech encoder at each step. For decoding, we recompute the entire LLM hidden states given updated speech input and then incrementally decode the translation as each speech segment arrives. This also provides translation tokens with more context since they can attend to speech embeddings appear after them. Results are shown in Figure 7. Incremental encoding of speech encoder reduces the computational latency consistently by at least 200ms compared to recomputing encoder. Recomputing LLM does improve translation quality (1 ~ 5 BLEU), but also introduces significant computation overhead (~ 1.5 second), making it inferior for real-time application.\nWe also plot the computation cost of each read/write step for each variant in Figure 7 with wait-2-stride-3 policy. FASST scales the best with the speech input length and reduces the overhead by at most 4x compared to the one without incremen-"}, {"title": "4.7 Generalizability to Other Policy", "content": "We have demonstrated that our method works with wait-k-stride-n policy. However, plenty of policies other than wait-k and its variants have been developed to conduct simultaneous translation. Here we apply our method to hold-n policy (Liu et al., 2020) to exemplify how our method works on a different policy and in the meanwhile explain the factors that influence the effectiveness of our method.\nHold-n policy selects the best hypothesis after each speech segment arrives, discard the last n tokens from it and outputs the rest as the partial translation. Since hold-n is a test-time policy for offline ST model, we train an offline version of our model by replacing stage 2 finetuning with standard offline finetuning using cross entropy loss but keep blockwise-causal encoding. During inference, we still conduct the same incremental encoding and decoding.\nAs shown in Figure 9, our method has less advantage when applied to hold-n comparing to wait-k-stride-3. The major difference between two policies in terms of computation is that hold-n policy spends more time on autoregressive decoding since it decodes more tokens each time. On average hold-n policy generates more than 6 words each step while wait-k-stride-3 generates at most 3 words. FASST accelerates the encoding of existing features, but for policies like hold-n that involve heavy autoregressive decoding the advantage of our method gets marginalized."}, {"title": "5 Conclusion", "content": "In this work, we introduce FASST, a fast LLM-based simultaneous speech translation model. FASST consists of blockwise-causal speech encoding, incremental LLM decoding with consistency mask, and a novel 2-stage training strategy. Experiments on MuST-C dataset show that FASST significantly reduce computation overhead while maintaining its translation quality. Our generalization study shows that policies that spend more time on encoding than decoding benefit more from FASST."}, {"title": "Limitations", "content": "\u2022 There might be data leakage since LLM is trained on vast amount of text data, so we cannot guarantee LLM does not see the test translation data during pretraining.\n\u2022 FASST is only tested on two language directions instead of all 8 language directions of MuST-C, so its generalizability to other language directions is unknown.\n\u2022 There is still a quality gap between blockwise-causal speech encoding and bidirectional speech encoding. It is unclear how to further close the gap.\n\u2022 We only explore one LLM-ST architecture in the paper and we cannot guarantee that FASST or its idea works on other architectures."}]}