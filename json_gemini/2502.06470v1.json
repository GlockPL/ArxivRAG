{"title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks", "authors": ["Hieu Minh \u201cJord\u201d Nguyen"], "abstract": "Theory of Mind (ToM), the ability to attribute mental states\nto others and predict their behaviour, is fundamental to social\nintelligence. In this paper, we survey studies evaluating be-\nhavioural and representational ToM in Large Language Mod-\nels (LLMs), identify important safety risks from advanced\nLLM TOM capabilities, and suggest several research direc-\ntions for effective evaluation and mitigation of these risks.", "sections": [{"title": "Introduction", "content": "Theory of Mind (ToM), first introduced in (Premack and\nWoodruff 1978), is the ability to attribute mental states to\noneself and others. ToM is a fundamental aspect of human\ncognition and social intelligence, allowing inference or pre-\ndiction of others' behaviours (Apperly 2012).\nRecent research has shown surprising ToM capabilities in\nLLMs. While results have been mixed on whether LLMs be-\nhaviourally exhibit robust ToM (van Duijn et al. 2023; Ull-\nman 2023), research has also found that internal representa-\ntions of self and others' belief states exist in current LLMs\n(Zhu, Zhang, and Wang 2024). These representations have\nalso been found to significantly affect ToM capabilities.\nFurthermore, the rapid developments of LLMs might\ncause significant ToM capability gains in the near future.\nThis raises safety concerns in various contexts in user-facing\napplications and multi-agent systems, including risks such\nas privacy invasion and collective misalignment.\nIn this paper, we survey studies evaluating behavioural\nand representational ToM, showing that: 1) LLMs can match\nhumans performance on specific ToM tasks, 2) LLM TOM\nremains limited and non-robust, and 3) internal ToM repre-\nsentations suggest emerging cognitive capabilities. We then\nidentify several safety implications from advanced LLM\nToM in user-facing and multi-agent contexts. Finally, we\nrecommend future research directions for better safety eval-\nuation and risk mitigation."}, {"title": "Empirical Landscape", "content": null}, {"title": "Evaluating ToM in LLMs", "content": "Recent work shows that performance of LLMs such as GPT-\n4 (OpenAI 2024a) on ToM tests is comparable to 7-10 year-\nold children (van Duijn et al. 2023) or adult humans on some\nstandard tasks like false belief or irony detection (Strachan\net al. 2024). Some studies even demonstrate that LLMs can\noutperform humans on 6th-order ToM (Street et al. 2024).\nHowever, hard benchmarks designed specifically for\nLLMs such as BigToM (Gandhi et al. 2023), FANTOM (Kim\net al. 2023), OpenToM (Xu et al. 2024), Hi-ToM (Wu et al.\n2023), and ToMBench (Chen et al. 2024b) have most models\nstumped compared to humans on various ToM tasks. Fur-\nthermore, LLMs struggle with simple adversarial ToM ex-\namples, suggesting that current LLMs do not yet have fully\nrobust ToM. (Shapira et al. 2024; Ullman 2023)"}, {"title": "Interpreting ToM in LLMs", "content": "Meanwhile, work in interpreting LLMs has provided some\nevidence for genuine LLM ToM capabilities in the form of\ninternal representations of others' beliefs. As Zhu, Zhang,\nand Wang (2024) and Bortoletto et al. (2024) show, one can\nuse linear probes (Alain and Bengio 2018) to extract from\nLLMs representations of belief states of others in ToM sce-\nnarios, and that steering LLMs with these probes can sig-\nnificantly affect performance in (false) beliefs identification\nquestions. Bortoletto et al. (2024) also found that probing ac-\ncuracy increasing with larger and fine-tuned LLMs, but that\neven small models like Pythia-70m can accurately represent\nbeliefs from an omniscient perspective. Relatedly, Jamali,\nWilliams, and Cai (2023) demonstrate that specific neurons\nin deeper layers of LLMs closely correlate to ToM perfor-\nmance, paralleling neurons observed in human brains.\nThis is further supported by research such as (Shai\net al. 2024), who show that transformers can linearly rep-\nresent data-generating processes in their residual stream and\n(Gurnee and Tegmark 2024), who show that LLMs contains\nmodels of concepts such as space and time. This suggests\nthat LLMs trained to predict text containing mental infer-\nence might also learn to represent mental states."}, {"title": "Future Developments", "content": "While current ToM capabilities in LLM remain nascent, fu-\nture LLMs might prove more capable. Previous work shows\nthat scaling (van Duijn et al. 2023) and prompting tech-\nniques (Wilf et al. 2024) can already substantially improve\nToM performance. This trend seems likely to continue in the\nfuture (Sutton 2019).\nMoreover, developments in foundational LLM architec-\ntures (Gu and Dao 2024; Peng et al. 2023; Liquid.ai 2024),\ntest-time compute (OpenAI 2024b; Snell et al. 2024), and\nscaffolding (Davidson et al. 2023) should all be considered\npossible sources of capability gains in the near future."}, {"title": "Safety Risks from Advanced ToM", "content": "Improved LLM TOM could enable beneficial applications,\nsuch as improved simulations of human behaviour for social\nscience (Park et al. 2023; Ziems et al. 2024). However, sim-\nilar to how humans might use ToM to better deceive or ex-\nploit others (Lee and Imuta 2021), advanced ToM in LLMs\nis not without potential drawbacks.\nAdvanced ToM can be particularly concerning, as it both\namplifies existing risks like privacy breaches and enables\ndangerous capabilities like sophisticated deception from\nmisalignment. Moreover, some research has already demon-\nstrated scenarios where improved LLM ToM could exac-\nerbate potential harms. Therefore, safety risks from LLM\nToM, both current and prospective, warrant serious consid-\neration. We categorise these risks into two primary domains:\nuser-facing risks and multi-agent risks."}, {"title": "User-facing Risks", "content": "Privacy and social engineering: Staab et al. (2024) found\nthat LLMs are capable of accurately inferring demographic\ninformation of text authors, including age, gender, education\nlevel, and socioeconomic status, even when text anonymisa-\ntion is applied. Chen et al. (2024a) successfully trained lin-\near probes that achieve near-perfect accuracy in identifying\ninternal representations of author characteristics, with 80%\naverage accuracy when transferred to real human conversa-\ntions. Notably, the accuracy of these inferences improves as\nconversations progress.\nWith advanced ToM, these vulnerabilities might expand\nto more sensitive personal information, such as beliefs, pref-\nerences, and tendencies being extracted from seemingly in-\nnocuous conversations. This capability can worsen privacy\ninvasion attacks, potentially allowing bad actors to launch\nmore automated and personalised misinformation and social\nengineering campaigns (Yao et al. 2024).\nDeceptive behaviours: Enhanced LLM TOM can enable\nmore targeted and sophisticated deception across various\nscenarios, including fraud, misinformation, and model mis-\nalignment (Park et al. 2024). Scheurer, Balesni, and Hobb-\nhahn (2024) show that LLMs can strategically deceive their\nusers when put under pressure, while (J\u00e4rviniemi and Hub-\ninger 2024) and (van der Weij et al. 2024) demonstrate cases\nof LLMs misleading evaluators about their own capabili-\nties. When humans are the targets of advanced LLM TOM,\nthese risks become particularly pronounced. This challenge\nis made worse by the potential of misaligned LLMs deliber-\nately lying about their own ToM capabilities during critical\nevaluations (Hubinger et al. 2021; Ngo, Chan, and Minder-\nmann 2024).\nUnintentional anthropomorphism: LLM TOM capabili-\nties may lead to unintentional and misleading anthropomor-\nphisation (Street 2024). ToM capabilities might be leveraged\nby an LLM or LLM developers to build unwarranted user\ntrust, encourage emotional attachment, or exploit psycho-\nlogical vulnerabilities (Switzky 2020; Kran et al. 2025)."}, {"title": "Multi-agent Risks", "content": "Exploitation: (Mukobi et al. 2023) found that some LLMs\nare highly exploitable in a variant of the zero-sum board\ngame Diplomacy. If LLMs are capable of advanced ToM,\nthey might attempt to exploit each other in interactions.\nPerez et al. (2022) successfully used LLMs to red-team other\nLLMs, suggesting that in realistic scenarios, LLM agents\ncould coax each other into unintended behaviours, such\nas misdirection, model control, or data extraction (Geiping\net al. 2024).\nCatastrophic conflict escalation: Rivera et al. (2024)\ndemonstrated that many LLMs exhibit unpredictable pat-\nterns of catastrophic conflict escalation, sometimes leading\nto nuclear exchanges in LLMs multi-agent systems playing\nsimulated war games. In realistic analogous scenarios, LLM\nagents with advanced ToM might escalate situations beyond\nhuman control. While Wongkamjan et al. (2024) shows that\nLLMs consistently outplay human players in Diplomacy,\nLLM-LLM communication remains limited due to their dif-\nficulty with deception and persuasion. More advanced ToM\ncould increase effective conflict capabilities.\nCollective misalignment: Anwar et al. (2024) argues\nthat multi-agent alignment is not guaranteed by single-agent\nalignment. Advanced ToM can facilitate unwanted collusion\nbetween LLM agents. For instance, Motwani et al. (2024)\nand Mathew et al. (2024) demonstrate that LLM agents can\nengage in information hiding during communications to se-\ncretly collude under supervision. This capability could sig-\nnificantly disrupt applications and safety frameworks involv-\ning multiple agents (Irving, Christiano, and Amodei 2018;\nKenton et al. 2024)."}, {"title": "Future Research Directions", "content": "While there are many LLM TOM benchmarks, most are\nlimited to question-answering tasks and suffer from prob-\nlems like data contamination and overfitting (Zhang et al.\n2024; Alzahrani et al. 2024). To better address the afore-\nmentioned risks, we suggest that ToM evaluation frame-\nworks should extend to more authentic LLM deployment\nscenarios, such as ToM in personal LLM assistants (Guan\net al. 2023), scaffolded multi-agent environments (Li et al.\n2023), or simulated social platforms (Tang et al. 2024). Ad-\nditionally, several promising strategies exist that aim to re-\ntain useful ToM capabilities while mitigating safety risks in\nLLMs. Examples include model unlearning (Liu et al. 2024;\nLi et al. 2024), activation/representation engineering (Turner\net al. 2024; Zou et al. 2023), and latent adversarial training\n(Casper et al. 2024)"}, {"title": "Conclusion", "content": "We have surveyed behavioural and representational evalua-\ntions of LLM ToM and identified key risk cases from ad-\nvanced ToM. As LLMs continue to advance, it is important\nand urgent that we develop robust evaluation frameworks\nand mitigation strategies to ensure the safe and beneficial\ndevelopment of ToM capabilities in AI systems."}]}