{"title": "Adaptive Hoeffding Tree with Transfer Learning for Streaming Synchrophasor Data Sets", "authors": ["Zakaria El Mrabet", "Daisy Flora Selvaraj", "Prakash Ranganathan"], "abstract": "Synchrophasor technology or phasor measurement units (PMUs) are known to detect multiple type of oscillations or faults better than Supervisory Control and Data Acquisition (SCADA) systems, but the volume of Bigdata (e.g., 30-120 samples per second on a single PMU) generated by these sensors at the aggregator level (e.g., several PMUs) requires special handling. Conventional machine learning or data mining methods are not suitable to handle such larger streaming real-time data. This is primarily due to latencies associated with cloud environments (e.g., at an aggregator or PDC level), and thus necessitates the need for local computing to move the data on the edge (or locally at the PMU level) for processing. This requires faster real-time streaming algorithms to be processed at the local level (e.g., typically by a Field Programmable Gate Array (FPGA) based controllers). This paper proposes a transfer learning-based hoeffding tree with ADWIN (THAT) method to detect anomalous synchrophasor signatures. The proposed algorithm is trained and tested with the OzaBag method. The preliminary results with transfer learning indicate that a computational time saving of 0.7ms is achieved with THAT algorithm (0.34ms) over Ozabag (1.04ms), while the accuracy of both methods in detecting fault events remains at 94% for four signatures.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, the U.S. power grid reported a cyber-attack incident that disrupted grid operations, specifically transmission operators in the western power region are hit by a Denial of Service (DoS) attack, causing a temporary loss of visibility in certain sections of the SCADA system. According to the report, \"the hack itself happened on March 5th, 2019, when a denial of service attack disabled Cisco's adaptive security appliance devices, ringing power grid control systems in Utah, Wyoming, and California\u201d. this is a major and the first cyber-attack on the U.S power grid [1]. Other grid attacks include the Ukrainian power grid, which has experienced an outage for several hours impacting nearly 225,000 utility customers in 2015 due to cyber threats [2]. These two events illustrate the detrimental impacts of cyber-attacks and the economic burden that it can bring to any nation's critical infrastructure. To avoid such lack of visibility in the power grid, reliable sensor measurements are required such as Synchrophasor or PMU technologies [3].\nThe PMUs generally record the magnitude, phase angle, frequency, voltage and current phasors with a precise GPS based timestamp [4]. PMU data paired with real-time analytics can facilitate improved grid operations. In recent years, advanced machine learning (ML) methods are being deployed for mining multi-variate, large-scale PMU data to reveal new insights on anomalous events.\nHowever, processing such large PMU streams requires expensive computational resources and faster algorithms. Further, the conventional ML algorithms [5]-[12], such as neural network, support vector machine, deep learning, and decision trees, cannot be used in this scenario, as they require loading and scanning the entire dataset before processing [13]. Thus, two important criteria have to be met by any streaming ML technique for handling PMU data: 1) training the model with recent history holding shorter signatures; and 2) adapting to concept drifts (e.g., fluctuations) in real-time [14]. It is important to note that the model trained on a historical record will no longer have relevance to the incoming data stream, and thus may fail to capture any critical or new events. Hence, meeting the above two criteria is key for successful and future real-time streaming algorithms [15], [16].\nIn [17], authors discuss a Hoeffding Tree (HT) combined with two concept drift detectors (e.g., drift detection method: (DDM) and Adaptive sliding windows: (ADWIN)) for building dynamic decision trees that are adaptable to quick changes. This method has been trained on a synthetic PMU dataset, where multiple attack scenarios were modeled. The dataset contains normal, anomalous, physical and cyber events. The physical event includes relay-based faults, and the cyber events include injections of various trip commands, single line to ground (SLG) fault replay commands and disable command attacks. The model (e.g., HT+ADWIN+DDM) trained and tested reported a classification accuracy greater than 98%.\nSimilarly, authors in [13] proposed a Hoeffding Adaptive Tree (HAT) based approach for detecting events on PMU data. The authors modeled two scenarios in their PMU data sets; 1) a three-phase fault has been generated with some load fluctuations altering true power (P) and reactive power (Q) at a regular interval; and 2) cataloged two classes such as fault and normal. Fault class includes a Single Line to Ground (SLG) fault, while the normal class includes normal power system variables such as voltage (v), current (i), frequency (f) and phase angle ($). Based on the reported results, Hoeffding tree showed a good ability in detecting the power system faults and adaptation to the concept drifts in comparison with traditional decision trees such as J48 and REPTree.\nHowever, in the above two studies, the duration of the physical fault events was not considered in their data set. There are certain type of power system faults that are time-sensitive and thus require early detection. The cause of inter-area oscillations is primarily due to system events coupled with a poorly damped power system. Generally, these low-frequency oscillations (0.1-0.8 Hz) are noticed in a larger grid with multiple generators or renewable plants (with high wind or solar penetration) that are connected to weak tie line connections. This can lead to a high degree of uncertainties in the system and it is often difficult to detect in real-time.\nSpecifically, smaller frequency deviations that range from 0.15-1.0 Hz lasting 60 seconds or longer may cause inter-area oscillations and quickly destabilize the grid [18]. Similarly, momentary voltage or current instabilities (e.g., surges or spikes of 2-5 seconds) may lead to asset failures (e.g., transformer, relays, or circuit breakers). Thus, including signatures with various event duration is a key feature for training real-time machine learning algorithms.\nIn [19], the authors proposed a distributed Intrusion Detection System (IDS) using a multi-layer network architecture for Smart Grids. Here, IDSs are modeled at three levels of networks: 1) Home Area Network; 2) Neighborhood Area Network; and 3) Wide Area Network. These IDSs were based on the Support Vector Machine (SVM) and Artificial Immune system (AIS), where each classifier was trained and tested on the NSL_KDD dataset [20]. The results indicate that SVM outperformed AIS algorithms in terms of False Positive Rates (FPR) and False Negative Rates (FNR). However, the proposed classifiers are not suitable for streaming data, as they require loading the entire dataset for processing.\nThe authors of [21] have proposed an IDS for the Advanced Metering Infrastructure (AMI) data streams. The proposed IDS has been deployed at three levels of the AMI: Smart meter, AMI headend, and Data concentrator. They used seven data stream mining algorithms. They are as follows: 1) AccuracyUpdatedEnsemble; 2) ActiveClassifier; 3) LeveragingBag; 4) LimAttClassifier; 5) OzaBagAdwin; 6) OzaBagASHT; and 7) SingleClassifierDrift. These algorithms are trained and tested using the KDD Cup 1999 dataset [22]. Their findings indicate that both ActiveClassifier and SingleClassifierDrift algorithms reported satisfactory results in terms of saving memory and time. LeveraginBag provided higher accuracy and a lower false-positive rate, but it demands moderate memory. Thus, the method is an appropriate candidate for phasor data concentrators. For the data concentrator, which requires a response time of a fraction of second, ActiveClassifier can be used due to its faster processing time. However, one has to evaluate the number of available communication channels, and networks to validate the training scenarios and deployment of these algorithms.\nThe performance of the streaming data classifiers can further be improved by including pre-processing operations such as the removal of redundant data, and dimensionality reduction. A principal component analysis (PCA) was used for online dimensionality reduction for the PMU data stream [23]. This approach was tested on real PMU data generated by SEL412 and GE n60 PMUs, deployed on Real-Time Digital Power System Simulator (RTDS) in a 4-bus system. In [24], PCA was used for both detecting anomalies, and dimensionality reduction. Though the PCA methods appear to work well for dimensionality reduction, there is limited information on their ability to detect anomalous events. In addition, these works do not include concept drifts or signatures with varied duration and do not provide any comparison against existing techniques.\nIn this paper, a transfer learning-based Hoeffding Tree with ADWIN is proposed to detect anomalies in the streaming PMU data. Multiple HT classifier is trained on normal and anomalous signatures, while ADWIN is included at the leaf for adaptation to concept drift (e.g., fluctuations). Other concept drift detectors can be combined with the HT including drift detection method (DDM), Early Drift Detection Methods (EDDM), Linear Four Rate (LFR), Just in time (JIT), and ensemble methods [25], [26], [27]. DDM and EDDM don't require storing the data, but they are sensitive to false alarm rates. LFR [25] can deal with unbalanced classes, as the error types are handled differently. However, it suffers from the labeling cost. JIT [27] can detect the abrupt changes, and do not require labeled data, but cannot handle continuous and gradual drifts. Ensemble methods [28] are effective in detecting recurring concept drifts, however, it lacks in identifying the precise location of the drift, due to its larger batch size. In our work, we use ADWIN for identifying quick localized changes in the signatures. It also uses a dynamic window that adapts to varying size of the incoming streaming data. When data is stationary, the size of the window increases dynamically, and decreases, when a change is detected.\nThe proposed THAT model is trained on four event signatures with varying durations. As HT and ADWIN require fixed features during the training phase and they cannot be trained on events with different durations, these models will be improved by incorporating a transfer learning (TL) technique. Here, TL refers to the transfer (or retention) of knowledge that can be learned across multiple similar tasks, but not identical [29]. The process of TL is as follows: the model is trained on the first signature (e.g., first task) and any acquired knowledge will be transferred to perform the training on the next task, that focuses on the second signature. Thus, the process of transferring the learned training can be repeated to subsequent signatures. To the best of the author's knowledge, this is the first attempt to integrate the transfer learning with any streaming classifier for anomaly detection containing signatures with varying durations. In [29], the authors proposed a transfer learning in a decision tree, but it was not for streaming data applications. This paper is divided into the following sections: Section II covers an overview of THAT model, it details the training dataset, and the relevant features used for detecting the oscillation events; and section III evaluates the proposed model with some performance metrics. Section VI draws some conclusions."}, {"title": "II. METHODOLOGY", "content": "A. Transfer Adaptive Hoeffding tree (THAT)\nThe Hoeffding Tree (HT) is considered as a standard decision tree used for classification. It uses a Hoeffding bound that relies on the minimum number of arriving samples to build a certain confidence threshold to build trees. Therefore, avoids the entire data to be loaded into memory. The only information required is the algorithm itself, which stores enough statistics on incoming stream in its leaves, and thus enables the tree to grow, and classify its samples in real-time. Theoretically, if HT receives enough instances of data, then the method will have similar performance as conventional decision trees [14], [15]. Algorithm 1 illustrates the Transfer Hoeffding Adaptive tree (THAT) algorithm for the PMU/PDC stream. The algorithm has two stages, stage 1: create a new HT; and in stage 2: carry transfer learning operations between two HT models by training the target model on the subsequent signature in the queue using learned HT tree.\nThe details for these two stages are as follows: In stage 1, (lines 1-20), a new $HT_{target}$ tree is created if the $HT_{source}$ tree is not provided. During this early stage, transfer learning is not required. In line 2, a target tree is initialized by creating the first node (root). In lines 3-19, a for loop is performed for all the training instances, where each sample is filtered down the tree to the appropriate leaf l based on the test sequences"}, {"title": "Here n is the number of classes and it is equal to 2. The attribute, A, is one of the selected features which could be voltage (V), frequency (f), current (I), or the phase angle ($). Then, the Information Gain is computed by:", "content": "Here n is the number of classes and it is equal to 2. The attribute, A, is one of the selected features which could be voltage (V), frequency (f), current (I), or the phase angle ($). Then, the Information Gain is computed by:\n$Information Gain (S, A) = \\frac{Entropy(A) - \\sum_{k \\in E(A)} \\frac{|S_k|}{|S|} Entropy(S_k)}{(2)}$\nWhere k is the value of the attribute A, and $S_k$ is a subset of S where A = k.\nThe other metric considered for evaluation is known as Gini Index, and this index is computed as:\n$Gini(A) = 1 - \\sum_{j=1}^n P_j^2$   (3)\nHere n is the number of classes (e.g., n=2 in our case). In lines 9 and 10, the attributes with the largest gain information are used for the next steps. Line 11 computes the Hoeffding bound such that probability 1-8 corresponds to a confidence value of d\u2208 {1,0}, where the true mean of a random variable of range R will not differ from the estimated mean after n independent observations by more than e and it is given by:\n$\\epsilon = \\sqrt{\\frac{R^2 In(\\frac{1}{\\delta})}{2n}}$  (4)\nIn line 12, a split criterion test is performed between the largest gain value attribute and Ao and compared with the Hoeffding bound. The value of t is used to analyze trade-off conditions. If the attribute obtained is the best choice, then the node is split and the tree grows (lines 13-15) [13], [30], [31].\nThe lines 21-37 focus on the concept of \"transfer learning\", where the knowledge gained between two HT models trained on two different signatures (i.e., tasks) are shared or transferred. The model is scalable to transfer knowledge beyond two HT models, if needed. Before we define the transfer learning and how it is used with HT, we must understand the term 'standard learning' (SL). The authors in [29] define SL using the following description process: \"Let D be a domain consisting of r-dimensional feature space X, a label space Y, a probability distribution P =\n(x, y) where x \u2208 X is the feature vector, and y \u2208 Y. With a finite set of labeled examples S =\n{(\u04251, \u04231), (\u04252, \u04232), ..., (Xn, Yn)) drawn from P and a loss function J, standard learning consists of defining a function f: X \u2192 Y with a minimum J value\". In the case of transfer learning, a source domain Ds = (Xs, Ys, Ps) and a target domain D\u2081 = (XT, YT, PT) are both used to learn a function f': XT \u2192 Yr given a set of labeled examples S drawn from Pr and some information pertaining to Ds, such that the value of"}, {"title": "present in the HT built to that point (line 4). During this process, sufficient statistics on the samples and Hoeffding bound is computed. Each leafl contains enough statistics in order to make decisions about the further growth of the tree. These statistics need to be sufficient to enable the calculation of the Information Gain afforded by each possible split. However, storing unnecessary information would increase the total memory requirement for the tree. In line 5, the statistics hold by l are updated to estimate the Information Gain of splitting each attribute. In line 6, the n\u2081 number of samples seen at leaf l (computed from the sufficient statistics), is updated. Lines 7-18 are executed only when a mix of different classes enables further splitting. In line 8, the splitting criterion G is used to estimate the G\u2081 value for each attribute. The function G measures the average amount of purity that is gained in each subset of a split and indicates how well a given attribute separates the training examples according to their target classification [15]. In this paper, two different approaches will be investigated to compute the function G: Information Gain (entropy) and the Gini index. If the distribution of the two classes (e.g., oscillation event class, and normal event class) in the PMU stream contains the probabilities p1, and p2 of the classes, then the entropy of a given attribute A in a training data set S is calculated by:", "content": "present in the HT built to that point (line 4). During this process, sufficient statistics on the samples and Hoeffding bound is computed. Each leafl contains enough statistics in order to make decisions about the further growth of the tree. These statistics need to be sufficient to enable the calculation of the Information Gain afforded by each possible split. However, storing unnecessary information would increase the total memory requirement for the tree. In line 5, the statistics hold by l are updated to estimate the Information Gain of splitting each attribute. In line 6, the n\u2081 number of samples seen at leaf l (computed from the sufficient statistics), is updated. Lines 7-18 are executed only when a mix of different classes enables further splitting. In line 8, the splitting criterion G is used to estimate the G\u2081 value for each attribute. The function G measures the average amount of purity that is gained in each subset of a split and indicates how well a given attribute separates the training examples according to their target classification [15]. In this paper, two different approaches will be investigated to compute the function G: Information Gain (entropy) and the Gini index. If the distribution of the two classes (e.g., oscillation event class, and normal event class) in the PMU stream contains the probabilities p1, and p2 of the classes, then the entropy of a given attribute A in a training data set S is calculated by:\n$Entropy (A) = \\sum_{i=1}^2 -P_i log_2P_i$  (1)"}, {"title": "Where m is defined as:", "content": "Where m is defined as:\n$\\frac{1}{m} = \\frac{1}{n_0}+\\frac{1}{n_1}$  (6)\nWhere no, n\u2081is the lengths of Wo and W\u2081, respectively.\nThe d' is defined as:\n$\\delta' = \\frac{\\delta}{n}$  (7)"}, {"title": "III. SIMULATION AND RESULTS", "content": "With streaming PMU data, it is challenging to store the entire dataset and sectioning it into training, validation, and testing data in order to evaluate the THAT model. Thus, other evaluation techniques are considered including Holdout, interleaved test-then-train, and prequential. In Holdout, a section of the incoming data is used to train the model and small test cases were used to compute the performance. In interleaved test-and-train, each instance of the data stream is used for testing and training the model. Prequential is similar to interleaved test-and-train and uses a sliding window or a decaying factor. The proposed model (THAT) was evaluated using these three techniques and it was observed that the prequential technique exhibited higher detection accuracy with moderate processing time against other approaches.\nIn order to evaluate the performance of the THAT model, a comparative analysis with OzaBag has been carried out"}, {"title": "using several performance metrics. An MOA platform [30] was used to run the simulations. The ensemble approach (i.e., OzaBag) is selected, as some literature [21], [36] reported satisfactory results with power system data. The performance metrics used are accuracy, Kappa and evaluation time. The first metric evaluates the algorithm in terms of detecting accurately the instances of oscillation and normal events (Equation (8)). However, it may provide overly prediction results with imbalanced classes. To address this issue, the second metric, (Kappa), can be used to control the instances that may have been correctly classified by chance (Equation (9)). Thus, Kappa is used to help in tuning the THAT model. The evaluation time computes the required time to process each instance.", "content": "using several performance metrics. An MOA platform [30] was used to run the simulations. The ensemble approach (i.e., OzaBag) is selected, as some literature [21], [36] reported satisfactory results with power system data. The performance metrics used are accuracy, Kappa and evaluation time. The first metric evaluates the algorithm in terms of detecting accurately the instances of oscillation and normal events (Equation (8)). However, it may provide overly prediction results with imbalanced classes. To address this issue, the second metric, (Kappa), can be used to control the instances that may have been correctly classified by chance (Equation (9)). Thus, Kappa is used to help in tuning the THAT model. The evaluation time computes the required time to process each instance.\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$  (8)\nWhere TP is the true positive, TN is the true negative, FP is the false positive, and FN is the false negative.\n$Kappa = \\frac{(Accuracy - random accuracy)}{(1 - random accuracy)}$  (9)\nWhere Accuracy is given by Equation (8), and random accuracy is defined as:\n$random accuracy = \\frac{(TN+FP)*(TN+FN)+(FN+TP)*(FP+TP)}{Total^2}$ (10)\nHere Total is TP + TN + FN + FP."}, {"title": "A. Experiment (I): THAT without supervised transfer learning", "content": "A. Experiment (I): THAT without supervised transfer learning\nFor this case, THAT model was separately trained on the four signatures without applying supervised transfer learning. A parametric study of THAT and OzaBag models has been conducted in order to define the appropriate value of each hyperparameter of these models. For the THAT, two Information Gain functions have been explored: Gini index and Entropy, and different values of 8 have been selected ranging from 0 to 1. The results of this experiment are given in Figures 2 to 5. These figures illustrate the experimental results of one signature. The results of the other three signatures are not included due to space constraints."}, {"title": "IV. CONCLUSION", "content": "A transfer learning technique using Hoeffding tree and ADWIN is proposed for synchrophasor data. The proposed model, called THAT, can be easily trained for any PMU signatures of varying and shorter durations. It does not require loading the entire data into memory to build the decision tree model, and thus suitable for real-time processing. Additionally, ADWIN is included, so THAT model is easily adaptable to gradual concept drifts. A prequential technique was used to evaluate the performance of the model and the results have been compared to the OzaBag method. After performing a parametric study and tuning the models, two sets of experiments have been conducted. In the first case, THAT model has been trained separately on each signature. In the second case, supervised transfer learning has been applied to THAT model. The obtained results showed that THAT and OzaBag models report higher average accuracy ranging between 91% and 99% (case 1). For case 2, the average accuracy was decreased to 94% in both models, but THAT model required smaller computational run-time than OzaBag. Thus, THAT model is more suitable for the PMU data stream, since it provides a near-real-time response to the dynamic fault event conditions."}]}