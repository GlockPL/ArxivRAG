{"title": "Explainable Multi-Modal Data Exploration in Natural Language via LLM Agent", "authors": ["Farhad Nooralahzadeh", "Jonathan F\u00fcrst", "Yi Zhang", "Kurt Stockinger"], "abstract": "International enterprises, organizations, or hospitals collect large amounts of multi-modal data stored in databases, text documents, images, and videos. While there has been recent progress in the separate fields of multi-modal data exploration as well as in database systems that automatically translate natural language questions to database query languages, the research challenge of querying database systems combined with other unstructured modalities such as images in natural language is widely unexplored.\nIn this paper, we propose XMODE 1 - a system that enables explainable, multi-modal data exploration in natural language. Our approach is based on the following research contributions: (1) Our system is inspired by a real-world use case that enables users to explore multi-modal information systems. (2) XMODE leverages a LLM-based agentic AI framework to decompose a natural language question into subtasks such as text-to-SQL generation and image analysis. (3) Experimental results on multi-modal datasets over relational data and images demonstrate that our system outperforms state-of-the-art multi-modal exploration systems, excelling not only in accuracy but also in various performance metrics such as query latency, API costs, planning efficiency, and explanation quality, thanks to the more effective utilization of the reasoning capabilities of LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Consider a hospital in the near future in which doctors, nurses, and data scientists naturally access digital patient data. This data includes electronic health records (EHR), usually stored in relational databases [19], but also multimedia data such as medical images from CT scans or X-rays and the corresponding reports written by medical experts (unstructured data). Each participant seeks to interactively query all these datasets in natural language. Different participants also have different skill sets and exploration goals. Additionally, given the application domain, each user wants to understand exactly how the system evaluates their queries. A system that supports such a scenario would unlock a plethora of applications, from this medical example to queries over shared scientific databases (also containing structured data, text, images, and videos), queries over public datasets, and more. However, building such a system presents significant research challenges in understanding user intent, which often relies on complex queries, querying multimedia databases, and ensuring explainability.\nTo understand these challenges, a concrete scenario of multi-modal exploration involving a relational database, text documents, and images is outlined here.. Assume that a user asks the following question in natural language: Show me the progression of cancer lesions over the last 12 months of patients with lung cancer who are smokers. (see the upper part of Figure 1 b). This seemingly straightforward query encapsulates several fundamental challenges in multifaceted data exploration. First, it requires the decomposition of a natural language query into semantically precise sub-queries, each targeting diverse data modalities while preserving the original intent. Critical to this process is optimizing the workflow sequence - determining which queries should be executed first to minimize computational overhead and maximize efficiency. For instance, filtering patients through structured database queries before retrieving and analyzing medical images significantly reduces the computational burden compared to analyzing all available images first. In our example in Figure 1, natural language NL1 is a text-to-SQL task to query the relational database for the name and age of patients diagnosed with lung cancer. The result is then used for NL2 - an image analysis task - looking for cancer lesions in those patients' images. Finally, NL3 a visualization task shows the cancer progression for each patient. This workflow sequence is deliberately optimized: starting with structured data filtering before proceeding to more computationally intensive image analysis tasks. The complexity compounds when considering the temporal aspect of disease progression, which necessitates careful alignment of data across different modalities and timestamps. Furthermore, in healthcare settings, result verification and transparency are paramount. Users must be able to trace back any conclusions to the source data, understand how intermediate results were derived, and verify the accuracy of each analytical step. This necessitates a workflow where users can validate intermediate results before proceeding to subsequent"}, {"title": "2 RELATED WORK", "content": "Text-to-SQL systems. The research field of text-to-SQL systems has seen tremendous progress over the last few years [5, 18] due to advances in large language models. Original success can be attributed to rather simplistic datasets consisting of databases with only several tables as in Spider [24]. Especially the introduction of new benchmarks such as ScienceBenchmark [27] or BIRD [13] has further pushed these limits of these systems. Most of the research efforts have been restricted to querying databases in English apart from a few exceptions such as Statbot.Swiss [17].\nExplainability. Explainability aims to provide a deeper understanding of how machine learning models make predictions by illuminating the decision-making processes within these models. It strives to offer transparency, enabling stakeholders to comprehend, trust, and effectively manage the outcomes produced by these models [10, 16]. Although there has been recent progress in artificial intelligence in general, for the task of data exploration in natural language, explainability is an open issue. Recently, in the multi-agent collaboration framework [23], explainability has been designed to mimic human-like top-down reasoning by utilizing the extensive knowledge of Large Language Models (LLMs). For the task of text-to-SQL, explainability is basically an unexplored research topic with the exception of back-translating automatically generated SQL statements to natural language [2, 22, 27]. However, back translation is often not enough to fully explain how a system comes up with an answer and how to interpret the results.\nMulti-modal systems. Video Database Management Systems (VDBMSs) support efficient and complex queries over video data, but are often restricted to videos only (e.g., [4, 7, 25]). ThalamusDB [6] enables queries over multi-modal data but requires SQL as input, with explicit identification of the predicates that should be applied to an attribute corresponding to video or audio data. Similarly, MindsDB\u00b2 and VIVA [8] require that users write SQL and manually combine data from relational tables and models. Vision-language models provide textual descriptions of video data [26], but are not designed to support precise, structured queries.\nMost closely related to our approach are CAESURA [21], which supports natural language queries over multi-modal data lakes, and PALIMPZEST [15], which enables optimizing AI workload. The key distinction of our system, XMODE, is its focus on efficiently orchestrating various model calls and their dependencies. This approach not only improves latency and cost but also enhances accuracy by minimizing interference from the outputs of intermediate function calls.\nMoreover, the related systems enable multi-modal queries across structured and unstructured data with a focus on query planning. However, these systems do not address enhancing the accuracy and explainability of the underlying model for natural language data exploration tasks. Explainability and answer justification are crucial in domains like medical data"}, {"title": "3 SYSTEM DESIGN", "content": "We now describe the design of our system called XMODE, which enables explainable multi-modal data exploration in natural language."}, {"title": "3.1 System Architecture of XMODE", "content": "The architecture of our system, XMODE, is illustrated in Figures 2. We describe the five primary components of XMODE using an example query applied to artwork data, which includes relational tables and images: Plot the number of paintings that depict war for each century. The system's operation is depicted in Figure 3.\nXMODE is an agentic system [9] driven by a llm-based dynamic planner pattern [11] equipped with a comprehensive toolkit containing all the necessary models to decompose a user's question, such as a multi-modal natural language question, into a workflow (i.e., a graph of sub-questions). The workflow is represented as a Directed Acyclic Graph (DAG), of which each node corresponds to a simple sub-question with a specific tool assigned by the planner. The planner determines sub-tasks that can be executed in parallel and it manages their dependencies. XMODE is designed to be adaptable and to allow for dynamic debugging and plan modification (re-planning) if necessary, e.g., in case of failures during a text-to-SQL sub-task.\nAs it shown in Figure 1, the design of XMODE incorporates multiple components:\n(1) Planning & Expert Model Allocation. The system analyzes the user question, then constructs a sequence of tasks to be executed considering their dependency. It determines the required expert models from an available toolkit for each task, as well as their input arguments and their inter-dependencies to synthesize them as a workflow. To do so, it employs the power of reasoning capability of LLMs. The output of this stage is a workflow in the form of a DAG that formalizes task dependencies. As we can see in"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the performance of our system XMODE. In particular, we want to address the following research questions:\n\u2022 How well does XMODE tackle multi-modal natural language questions on two different datasets consisting of tabular data, and images?\n\u2022 How does the system perform compared to state-of-the-art systems such as CAESURA [21] and NeuralSQL [1]?\n\u2022 Which explanations does the system provide to justify the answers?"}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Datasets. For our experiments, we used two different datasets, namely information about artwork as well as electronic health records.\nDataset 1: Artwork. We use the artwork dataset introduced by [21]. This dataset contains information about paintings in tabular form as well as an image collection containing 100 images of the artworks. This data is taken from Wikipedia. The tabular data contains metadata information about paintings such as title, inception, movement, etc. as well as a reference to the respective paintings. A typical example question from this dataset is Plot the number of paintings depicting war for each century (as previously shown in Figure 3). In addition to the 24 existing questions in the artwork dataset, we propose six new questions aimed at evaluating parallel task planning and execution, facilitating a comparison between the characteristics of the two architectures. These six questions incorporate both single and multiple modalities. Moreover, four of the six questions require responses in various formats: two questions"}, {"title": "4.2 Results on the Artwork Dataset", "content": "We first evaluate the results on the artwork dataset and afterward on the EHR dataset.\n4.2.1 Performance Results. Table 1 presents a comparison of XMODE and CAESURA on the artwork dataset across various aspects. The performance metrics for each aspect were determined through a manual evaluation conducted by our team"}, {"title": "4.2.2 Optimizations of XMODE Explained with Examples", "content": "To better demonstrate advantages of XMODE, we provide several examples (see Figures 3 and 5) across three key aspects: explanations, smart replanning, and parallel planning. The following examples provide a detailed illustration of these three aspects.\nExample 1: Plot the number of paintings that depict war for each century (see Figure 3).\nThrough a series of well-planned and systematically executed steps, the model demonstrates not only how it processes the query but also how it provides transparency and reasoning"}, {"title": "4.3 Results on the EHRXQA Dataset", "content": "In this section, we evaluate the performance of NeuralSQL and XMODE on the EHRXQA dataset. This comparison excludes metrics like steps, tokens, and latency because evaluating XMODE's performance on these aspects against NeuralSQL is not meaningful. NeuralSQL generates the final answer in a single step without providing a plan or intermediate steps, whereas our approach focuses on decomposing natural language questions, planning workflows, and responding transparently.\nWe also exclude CAESURA from the EHRXQA experiments. While CAESURA is intended to be a general-purpose multi-modal system, it processes the relational database through multiple steps, examining each table and relationship sequentially. This limitation introduces significant overhead when handling the complex data schema of the EHRXQA dataset (there are 18 tables) during the discovery phase. Consequently, reproduing CAESURA on EHRXQA questions fails to perform inferences at the early stages of the planning phase, ultimately terminating after exceeding the maximum number of allowed attempts.\nTable 2 demonstrates the experimental results of XMODE against NeuralSQL on the EHRXQA dataset. Our evaluation encompasses three scope categories: single-table queries with one image (Image Single-1), single-table queries with two images (Image Single-2), and multiple-table queries with single images (Image+Table Single). XMODE demonstrates robust performance across all evaluation metrics, achieving an overall accuracy of 51.00%. Notably, XMODE excels in handling multiple-table scenarios, where it achieves 77.50% accuracy, significantly outperforming NeuralSQL's 47.50% in the 10-shot setting. For single-table queries, XMODE shows strong performance with 43.33% accuracy on two-image queries, though it achieves a slightly lower score (23.33%) compared to NeuralSQL's 10-shot performance (26.67%) on single-image queries.\nWhen examining the output types, XMODE exhibits particularly strong performance on binary questions, achieving 74.00% accuracy compared to NeuralSQL's 48.00%. For categorical questions, both systems show lower performance, with XMODE reaching 28.00% and NeuralSQL achieving 18.00% in the 10-shot setting.\nA key distinguishing feature of XMODE is its comprehensive functionality beyond raw accuracy. Unlike NeuralSQL, XMODE generates executable plans with 98% coverage, provides explanations for traceability of final outputs, and supports dynamic replanning capabilities. In contrast, NeuralSQL, even in its 10-shot configuration, lacks these additional features and shows no performance in the zero-shot setting across all metrics. These results highlight XMODE's effectiveness as a more complete solution for EHRXQA tasks, particularly in complex scenarios involving multiple tables and binary decisions, while also offering important auxiliary features for practical deployment."}, {"title": "4.4 Error Analysis", "content": "In this section, we conduct a comprehensive analysis of errors encountered during the evaluation process. These errors are systematically classified into the following categories:\n\u2022 Planning Errors: These errors stem from incorrect or incomplete task planning, such as task decomposition, the generation of completely faulty natural language questions, etc.\n\u2022 Text-to-SQL Errors: Errors where the generated SQL fails to accurately retrieve the intended data.\n\u2022 Image Analysis Inaccuracy: Errors caused by inaccurate outputs from the image analysis model, even when the underlying task plan is correct.\n\u2022 Plot Generation Errors: Errors where plots are completely not generated, partially generated or incorrectly visualized, thereby failing to meet expected outcomes.\nTo systematically analyze key issues, we prioritize the identified categories based on their inter-dependencies during task execution. The priority sequence of these categories is defined as follows: task planning > text-to-SQL generation > Image analysis > plot generation. Only the first affected category is considered if an error occurs at any stage, which may involve issues across multiple categories. For instance, if an error is detected during the planning phase but the subsequent tasks are successful, and another error occurs at the later plot generation stage, only the error in the planning phase is counted. In this case, the sample is classified under the Planning Error category.\nThis approach to error analysis is grounded in the logical dependency structure of the tasks. Since each task is a prerequisite for the succeeding one, a failure in an earlier task renders the success of subsequent tasks irrelevant to the overall reasoning process. As a result, errors are attributed to the earliest point of failure better to reflect the hierarchical nature of the task dependencies, thereby facilitating targeted optimization.\n4.4.1 Error Analysis on the Artwork Dataset. As illustrated in Figure 7 (a), a total of 20 errors are identified out of 30 inference tasks for CAESURA. Of these, 14 errors occur within CAESURA's sequential workflow. The errors include three single-modal questions and 11 multi-modal questions. Among the three single-modal, one task could not be resolved due to insufficient data available in the data pool. Following this failure, CAESURA attempts to replan twice but ultimately generates an incorrect plan, and consequently results in an erroneous response. The remaining two errors in single-modal tasks were classified as Plot Generation Errors, which are caused by inconsistencies in the time axis units of the plot output.\nFor 11 errors in multi-modal questions, five are related to single-value outputs, four to plots, and three to data structures. All of these errors are attributed to incorrect outputs generated by the image analysis model. After further research, we found two ambiguous tasks in classifying the error categories. (1) Plot the number of paintings that depict war for each year and (2) What is depicted on the oldest religious artwork in the database? Both tasks failed due to improperly parsed sub question for the"}, {"title": "5 CONCLUSIONS", "content": "We demonstrated that multi-agent collaboration using large language models, such as GPT-4, offers a promising approach for explainable multi-modal data exploration in natural language. Our experimental evaluation against two state-of-the-art systems on two different datasets with tabular and image data shows that XMODE not only performs the task of multi-modal data exploration with higher accuracy but also faster due to smart re-planning and parallel execution. Moreover, XMODE also provides detailed explanations and reasoning"}]}