{"title": "Pattern based learning and optimisation through pricing for bin packing problem", "authors": ["Huayan Zhang", "Ruibin Bai", "Tie-Yan Liu", "Jiawei Li", "Bingchen Lin", "Jianfeng Ren"], "abstract": "As a popular form of knowledge and experience, patterns and their identification have been critical tasks in most data mining applications. However, as far as we are aware, no study has systematically examined the dynamics of pattern values and their reuse under varying conditions. We argue that, when problem conditions such as the distributions of random variables change, the patterns that performed well in previous circumstances may become less effective and adoption of these patterns would result in sub-optimal solutions. In response, we make a connection between data mining and the duality theory in operations research and propose a novel scheme to efficiently identify patterns and dynamically quantify their values for each specific condition. Our method quantifies the value of patterns based on their ability to satisfy stochastic constraints and their effects on the objective value, allowing high-quality patterns and their combinations to be detected. We use the online bin packing problem to evaluate the effectiveness of the proposed scheme and illustrate the online packing procedure with the guidance of patterns that address the inherent uncertainty of problem. Results show that the proposed algorithm significantly outperforms the state of the art methods. We also analysed in detail the distinctive features of the proposed methods that lead to the performance improvement and the special cases where our method can be further improved.", "sections": [{"title": "1. Introduction", "content": "Combinatorial optimisation problems (COP) have extensive applications in various industrial fields [1]. However, due to the NP-hardness nature of such problems, finding optimal solutions becomes extremely challenging given limited computational power, particularly for large-scale instances. This challenge escalates further when uncertainties are considered which hinders us from deriving the practical solutions.\nExisting approaches for addressing these types of problems can be broadly categorised into two main groups [2]: analytical model-driven methods, which are often exemplified by analytical and mathematical models [3]; and data-driven methods such as genetic programming and reinforcement learning [4] The former primarily concentrates on the analytical properties of problem models, but it may encounter challenges in terms of robustness when confronted with uncertainties in the input data [2]. In contrast, data-driven methods typically approach combinatorial problems as online optimisation problems They often address the problem sequentially, employing policies or rules that account for the realisation of random variables and the states of the partial solution at each decision point. One of the major limitations of data-driven methods is their inability to effectively exploit the core structure and properties of the problem [2]. Specifically, existing data-driven approaches [5] often prioritise optimisation objectives while neglecting the intricate inter-dependencies among decision variables (represented as constraints), and their cumulative influence on the overall objective.\nPatterns are one of the most powerful and effective problem-solving tactics in computer vision [6, 7, 8] and time-series data analysis [9, 10, 11]. However, very limited number of studies have used patterns as a problem-solving strategy"}, {"title": "2. Preliminaries", "content": null}, {"title": "2.1. Bin packing problem (BPP)", "content": "The bin packing problem is formally defined as packaging a set of items of different sizes using the minimum number of boxes of the same capacity. In its basic offline version, The size of items is given before packaging. Let B denote the capacity of the bins to be used and T be the number of item types, with each item type t having a size $s_t$ and quantity $q_t$. Let $y_j$ be a binary variable to indicate whether bin j is used in a solution ($y_j = 1$) or not ($y_j = 0$). Let $x_{tj}$ be the number of times item type t is packed in bin j. The problem can be formulated by"}, {"title": "2.2. Online bin packing problem", "content": "Although most research efforts on BPP have been focusing on its offline version in which details of items to be packed are perfectly predictable in advance, many real-life packing problems appear to be online because of dynamic realisation of items' specifications. More specifically, in model defined in Eq. (1), the quantity of item type t, $q_t$, is often unknown but its proportion among all item types can be estimated. Items arrive sequentially over time and its information (i.e., the type of the current item) is only available after their arrivals. A solution method for online BPP must assign a bin to each randomly arrived items upon its arrival and this assignment cannot be subsequently altered. Therefore, best fit remains a good solution method for online BPP with a high competitive ratio (e.g., 1.7) Scheithauer [14], but MBS is not usable anymore because it relies on the full information of items to be packed. In this research, we aim to improve average performance for online BBP by solving it with a pattern based optimisation scheme guided by pricing. The motivation and underlying ideas can be illustrated by using two simple BPPs given in Table 1 with bin capacity $B = 10$.\nFor both cases, best fit produces sub-optimal solutions. Like most heuristic methods, best fit is a typical objective-focused incremental method that aims to obtain the maximum possible benefits in terms of the objective defined in Eq. (1) at every step. However, although it partially addresses the constraint"}, {"title": "2.3. Pricing and duality", "content": "Duality is the principle that an optimisation problem could be viewed as two related problems with same data: the primal problem and dual problem.\nConsider the following standard formulation for optimisation problem (denoted as primal problem) defined in Eq. (4)-(6) [15], where f is objective function, u and v are constraints,"}, {"title": "3. Literature review", "content": "Bin packing problem has close connections to many real-world applications, e.g., memory management in modern computer architecture [22], healthcare management [23] and logistics [24]. It is probably one of the most studied combinatorial optimisation problems. Many research works have focused on approximate algorithms with provably guaranteed gaps to the optimal solution [3]. The most common ones are rule-based algorithms [25], which deal with both online and offline BPP problems. Coffman et al. [25] provided a comprehensive review of classical bin packing heuristics.\nRecent solution methods for offline BPPs exploit the structural properties of BPP's integer programming formulations via exact algorithms like branch-and-bound schema [26] and branch-and-price methods [27], but the computational time varies significantly between instances and the methods are therefore not suitable for real-time decision making. Another strand of research efforts is the data-driven based methods that exploit the distributional information of the random variables from the training data, including the use of genetic programming based hyper-heuristic to train a packing strategy/policy [28], and the evolutionary algorithms for evolving rules to select the most appropriate packing heuristics at each decision point [29].\nAlthough the concept of patterns has been applied in offline combinatorial optimisation problems [30, 22, 31], it is not actively studied for online combinatorial optimisation problems until recently. Angelopoulos et al. [32] introduced ProfilePack, which utilises offline optimal solutions of a section of item sequence to generate a future packing plan that is used to guide the packing in real-time. Although the concept of pattern is not explicitly discussed in the paper, the high frequency packing examples in the offline optimal solution serve as templates to guide packing. Lin et al. [33] developed another pattern-based packing method PatternPack for large-scale bin packing problem. It generates the pattern set by splitting the bin capacity into several fragments regardless of distribution of items while the plan is generated adaptively through statistical learning, coupled with a fuzzy logic enhanced pattern generation and selection strategy. These works demonstrate the potential of pattern-based methods for encoding and analysing the historical observation during packing. However, these algorithms heavily rely on assumptions of simple stationery distributions, which may limit their performance for online COPs with non-stationary distributions.\nDeep reinforcement learning (DRL) has gained growing attention in combinatorial optimisation [34], including routing problems [5] and graph-based problems [35]. In most cases, BPP is formulated as a Markov Decision Process (MDP) through which uncertainties can be effectively handled by training on a large set of problem instances, and reinforcement learning methods are designed to tackle the problem in an end-to-end manner [36]. For 1D bin packing problem, Hubbs et al. [37] and Balaji et al. [38] established a set of environments for classical operations research and associated DRL benchmarks. The benchmark end-to-end model for 1D online bin packing problem proposed by Balaji et al. [38] is a simple multi-layer perceptron trained by PPO, achieved online packing by minimising the total Sum-of-Square potential [25]. Additionally, Sheng et al. [22] developed SchedRL, a deep Q-learning method with a specific reward design for online virtual machine scheduling, which can be modelled as an online variable-sized BPP. Zhao et al. [39], Zhao and Xu [40] investigated the online 3D bin packing with a robot arm for logistics, where the state is visually perceived through a deep neural network in [39] and a graph neural network is designed to extract the position embedding of items in [40]. One noticeable drawback of these data-driven methods is their weak generalisation across unseen uncertainty distributions or non-stationary distributions. In this work, we establish a crucial link between the data mining and operations research and propose a novel pattern-based learning and optimisation method. The increased generalisation and performance enhancement of the proposed method is achieved through dynamic generation and resue of high-value patterns by explicitly exploiting the information of the uncertainty distributions."}, {"title": "4. Proposed pattern based method", "content": "For online bin-packing problems, the constraints have as much impact on solving the optimisation problem as the optimisation objective which is often overlooked by most existing methods. We propose a general framework Column Generation Plan-and-Pack (CGPP) that adopts explicitly the dualism of COP to assist pattern based solution building. To do this, we first reformulate the BBP problem based the concept of patterns, then describe the key steps and modules in the CGPP framework, including mechanisms to handle uncertainty and imperfect distribution predictions. We explain how the dynamic pattern discovery through pricing could handle both the objective and the constraints well, leading to significant performance improvement for online BPP."}, {"title": "4.1. Pattern based reformulation", "content": "Formally, in our online BPP, we assume a problem instance as a finite sequence of items of length N, with index i = 1, 2, ..., N. Each item belongs to a finite type t = 1, 2, ..., T, which is associated with a size $s_t$. The quantity of item type t in the sequence is defined as $q_t$, and its value is determined by sampling from a given distribution D. In practice, the stochastic process of items could be more complex in the sense that the distribution could change over time. In this case, it becomes a non-stationary distribution problem which is harder to solve. Both stationary and non-stationary distributions of problems are studied in this paper.\nWe define a pattern as a vector of the quantity of all item types that can be packed into a bin, $p^h = (p_1^h, p_2^h, ..., p_t^h, ..., p_T^h)$ and $p_t^h = 0$ means the item type t does not appear in this pattern and h is pattern index. We denote IP be the set of all feasible patterns.\n$P = \\{p^h | \\sum_{t=1}^{T} p_t^h s_t < B, p \\in N \\}$\nThe original BPP formulation defined in Eq. (1)-(2) can be re-formulated as"}, {"title": "4.2. Framework overview", "content": null}, {"title": "4.3. Planning", "content": "As stated previously, obtaining full P is often not possible in most cases. Instead, the optimisation starts from a restricted master problem (RMP) formulated on a subset $P \\subset P$ which guarantees a feasible solution but not the quality. A trivial way for the initial P is to define a set of patterns in which each pattern packs one item type only. In the subsequent steps, the algorithm repeatedly generates new high-valued patterns to be added to P and solves updated RMP, until no new pattern can be found to improve the solution further.\nThe resulting solution by pattern frequency $z_h$ defines a packing plan, P. By restricting to a small set of the patterns, we deal with a much smaller RMP and only high-valued patterns are added to the problem, considerably reducing the computational time.\nIn order to identify high-valued patterns, we first obtain the shadow price $\\delta_t$ for each constraint in Eq.(9) through the dualism property. Then, the following sub-problem (called pricing problem or pattern generation) is solved:\n$minimise \\quad 1 - \\sum_{t=1}^{T} \\delta_t p_t$\n$subject to \\quad \\sum_{t=1}^{T} s_t p_t \\leq B$\nand the resulting solution p* = ($p_1, p_2, ..., p_T$) defines a new pattern to be added to P. Eq. (11) is the packing constraint. The pattern generation process stops"}, {"title": "4.4. Plan-based packing", "content": "In an ideal world, the plan generated by Algorithm!1 is implemented exactly. However, because of forecast errors in demands, additional work is required during the actual packing (see Algorithm 2). For a given packing plan P, each newly opened bin is assigned to a pattern from the plan, implicitly specifying the type and quantity of items that should be packed into. Only items that match the assigned pattern can be packed in the corresponding bin.\nUpon arrival of an item of type t, the algorithm firstly packs it into a matched open bin via procedure pack_item. If no opened bin matches the considered item, a new bin is opened and an arbitrarily feasible pattern in the current plan Pis assigned to it. The considered item is then packed to this new opened"}, {"title": "4.5. Uncertainty handling", "content": "Due to the stochastic nature of the problem and the imperfect estimation of quantities of items, a gap will always exist between the actual realisation of the problem instance and the forecast demands, leading to sub-optimal solutions. The challenge becomes greater when the distribution of item types is unknown and is subject to changes over time.\nThe uncertainty caused by insufficient information cannot be avoided in online problems. However, the gap between estimated and actual distributions might be caused by systematic factors which can be reduced. For example, when items' distribution changes dynamically over time, which is common in real-world applications [41], algorithms trained on stationary distributions could perform poorly.\nIn our CGPP method, this challenge is dealt with by checking the distribution periodically at each section of the item sequence. The currently adopted distribution D and the real-time distribution D' estimated from the past k items are compared using the Kullback-Leibler (KL) divergence $KL(D'||D)$. If the difference exceeds a predefined threshold $\\theta_{kl}$, a new plan is generated by calling the planning procedure again based on the latest estimation of the distribution.\nThe uncertainty can lead to errors in the estimated items' quantity, in the forms of either underestimation or overestimation. Underestimation arises when the actual number of items exceeds the estimation, while overestimation occurs when the actual quantity of items of a specific type is lower than estimated. Without special attention, the overestimated items are likely to result in wasted space, while the underestimated items will be packed inefficiently using fallback strategies. This can lead to the presence of numerous open bins waiting for items that will never arrive or disrupt the predefined packing plan.\nTo address underestimation, we maintain an uncertainty table {T} that tracks the number of items not included in the current plan. The tolerance level for each item type is determined by a threshold quantity $\\theta_u$. This means that each item can be excluded from the plan for a maximum of $\\theta_u$ occurrences. If the count $T_t$ for item type t exceeds $\\theta_u$, it indicates that the expected demand"}, {"title": "5. Experimental results", "content": "We test the proposed method for a whole range of online BPP datasets with different characteristics in order to establish comprehensive evaluations and understanding of the strength and weaknesses of our method under different uncertainty conditions. More specifically, four distinct problem types are tested and details are given later in Sections 5.2-5.5. Most experiments were set up with 20 instances, each having 20,000 items. Without explicitly stated otherwise, the bin capacity is set to 100, and the item sizes are in the range [1,100).\nWe compare our method against BestFit, which is one of the most commonly used online heuristics due to its robustness across different scenarios and low competitive ratio (1.7), and three other state of the art methods for online BPP, namely ORL [38], ProfilePack (or ProfP for brevity) [32], and PatternPack (or PatnP for brevity) [42]. An additional comparison with PatternPack's updated version FPP [33] is given in subsection 5.5."}, {"title": "5.1. Algorithm configuration", "content": "We utilised the discrepancy between the classic L2 lower bound [12] and the objective values obtained from various algorithms to assess their performance. This lower bound has been demonstrated to be less than 1% from the optimal value.\nIf not specified, the CGPP in this work was configured as follows. The fallback strategy was set to be the one-step best fit heuristic. The section length was set to L = 1000 with a memory length of k = 250 based on some initial trials. For the threshold parameters, the KL-Divergence threshold $\\theta_{kl}$ = 0.1, the underestimate tolerance threshold $\\theta_u = 5$, and the overestimate threshold 0 = 0.8. The hybrid ProfilePack algorithm was set up with parameter App = 0.5 as suggested by Angelopoulos et al. [32] since a low-error profile was not assumed in our experiment. On the other hand, PatternPack was configured with the same parameters reported in Lin et al. [42]. Both ProfilePack and PatternPack employed a statistical learning approach to dynamically learn the problem distribution. This approach entailed maintaining a sliding memory window, in which the item frequencies within the window were utilised to estimate the probabilities. For both algorithms, the length of the memory window to be $k_{prp}$ = $k_{pap}$ = 500, same as the settings reported in the papers.\nThe ORL method in Balaji et al. [38] was re-implemented with the same reported settings. The algorithm used the standard Proximal Policy Optimisation (PPO) algorithm, with a 3-layer policy network and a hidden layer of 256 nodes. The model was trained on a uniform distribution set, where the items and bin capacity were the same as the problem definition, unless otherwise specified. It underwent 500 epochs of training, which took approximately 600 minutes to complete on our machine.\nAll experiments were performed on a PC with an Intel Xeon Gold 6248R Processor with 24 cores and 48 threads, along with an Nvidia GeForce RTX 3090 graphics card."}, {"title": "5.2. Experiments on different items' distributions", "content": "In order to evaluate the performance differences across different distributions by all algorithms, a total 8 datasets were set up with uniform or normal distributions as bases. They are named Uniform, Normal, Uniform-B, Uniform-C, Uniform-D, Normal-B, Normal-S, and Normal-C, respectively. Among them, 6 are derived datasets with a same distribution but different item range configurations. The suffix B refers to biased distribution, with item size in range [10, 60), while suffix S refers to symmetric distribution, with item size in range [25, 75).\nSpecifically, for Normal-B, the mean of distribution was set to be \u00b5 = 35 with the range same as Uniform-B. The experiment with suffix C refers to coarse experiment, with item sizes from set {10, 20, ..., 90}.\nTable 2 provides the results of this experiment set. It can be seen that CGPP outperformed other methods in most experiments, except for two of the symmetrically distributed sets (Normal and Normal-S), for which BestFit outperformed all other methods. The proposed CGPP method tends to perform particularly well for uniform distribution instances. The performance gain against the second best method for these instances ranges between 17% to 62%."}, {"title": "5.3. Packing with prior knowledge", "content": "This experiment set was built to investigate whether a good prior knowledge on distribution can contribute to finding a good solution. Among the algorithms we discussed, BestFit does not rely on any learning mechanism, while both ProfilePack and PatternPack apply a statistical approach to gain distribution information without any prior knowledge. On the other hand, ORL can be viewed as implicitly encoding the distribution information by choosing the training and testing datasets. In the experiments in this section, ORL was trained on the same distribution as the test datasets' distribution. Additionally, we report the results of CGPP with the exact distribution given as prior knowledge, referred as CGPP-L.\nTo establish a convincing comparison with ORL, we adopted three distributions: BW1, LW1, and PP1, as proposed by Balaji et al. [38]. These distributions have expected waste of \u0398(1), \u0398(\u221an), and O(n), respectively. A total of 6 datasets are created (see Table 3). In the first 3 datasets (BW1-9, LW1-9, PP1-9) the bin capacity was set to 9, while in datasets BW1-100, LW1-100 and PP1-100, it was set to 100, following the configuration by Balaji et al. [38]. The number of experiment instances and the associated number of items remained the same as in Section 5.2."}, {"title": "5.4. Experiments on more complex distributions", "content": "This section assesses the performance of our algorithm on more complex distributions. Two groups of datasets were used. The first group adopted a dual-normal distribution suggested by Burke et al. [43], which is a typical mixed distribution. The datasets include both single normal distribution and dual normal distributions. Our focus was on the dual part, specifically Burke 4-11, through experiments 15-22. Each experiment consists of 20 instances, with each instance containing 5000 items.\nThe second group of experiments investigated the performance when the distribution changes periodically. All three experiments shared the same item size range and bin capacity configurations. The entire item sequence was divided into several equal-sized sections, and each section was sampled from an independent distribution. We utilised two groups of binomial distributions for the periodic experiments: Binomial-PS, which samples from a binomial distribution with p = {0.2, 0.35, ..., 0.7}, and Binomial-PB, which samples from a binomial distribution with p = {0.2, 0.3, ..., 0.6}. Additionally, we included a Poisson distribution group with its parameter varying in the set {5,15, ..., 45}. For each instance, the section size was set to be 2000, resulting in a total of 10 sections.\nTable 4 shows the results for the two types of experiments. In the dual"}, {"title": "5.5. Experiments on large-scale Weibull distribution", "content": "In this section, we aim to investigate the effectiveness of the Weibull distribution family, which is closely connected to bin packing applications such as VM management [44]. We established five different Weibull distributions with shape parameters sh = {0.5, 1.0, 1.5, 2.0, 5.0}. Each experiment consisted of 5 instances with $10^5$ items.\nIn addition, we generated a group of datasets with periodic Weibull distributions, with the shape parameters shifting to the next one stated in the list"}, {"title": "6. Discussion and analysis", "content": null}, {"title": "6.1. Solution quality analysis", "content": "Although the performance of algorithms is primarily measured by bin usage, we use filled rate of all opened bins to further investigate the solution quality and the packing process by different methods. The bin filled rate is defined as the percentage of the total size of items in a bin to its capacity.\nWe use two typical solutions from Uniform-B and Normal-B datasets in Section 5.2 for analysis. Additionally, we analyse the results on the periodic Weibull dataset (experiment 31) to observe how different methods behave when faced with changing distributions.\nFigure 2 shows the filled rates of the bin index for the Uniform-B and Normal-B dataset. The bin series are arranged in the order of their opening steps. The polylines of different colours are used to illustrate the average fill rates of bins in the solutions generated by different algorithms for the given dataset. The surrounded light areas of each polyline represent 95% confidence interval.\nIt can be seen that both PatternPack and ORL is able to achieve filled rate over 95% until at very late stages. BestFit achieved slightly better filled rate than PatternPack and ORL, with average 97.5%. The filled rates of ProfilePack are consistently worse than all other methods. The extreme fluctuations observed in ProfilePack also illustrates the algorithm's poor robustness. This phenomenon is likely due to ProfilePack lacking mechanisms for handling overestimation uncertainty, which is identified as the most wasteful resource, as discussed in Section 4.5.\nThe filled rates of BestFit, CGPP, and PatternPack exhibited a decreasing trend on Normal-B dataset. Clearly the filled rate of CGPP is the highest at the beginning, albeit with fluctuations during the whole packing stage, which could be caused by imperfect prediction. The fast drop of filled rate of last few bins by CGPP also highlight one of the main drawback of CGPP method that the overestimation is not avoidable. Similarly, PatternPack also suffered with fluctuations and fast-drop by poor prediction. The filled rate of ProfilePack experiences a significant drop in the middle of the bin sequence. This is likely due to poor predicted profile misguided the packing. Angelopoulos et al. [32] claimed hybrid ProfilePack forced to pack items separately using online heuristic when the quantity of items packed following profile guidance reached to a threshold. This resulted in huge waste by not filling the space reserved for overestimated items. This further highlights the importance of addressing overestimation in the pattern based packing process. Therefore, eliminating the effect of poor prediction could be an area for future improvement for all prediction-based online algorithms. On the contrary, ORL behaved conservatively by maintaining most bins at similar level of filled rates for both datasets. Since ORL was trained on uniform distribution, such conservative strategy indicates it cannot generalise to other distributions.\nFigure 3 represents the filled rates of the Periodic Weibull dataset. Most methods initially achieve a nearly 100% filled rate, as the sequence is long enough to provide sufficient small items to fill the wasted space in the bins opened at early stage. Thanks to its forward-looking strategy in the form of patterns, CGPP maintained a high filled rate over the entire packing stages until the very end, indicating its success in adaptively identifying good patterns even as the"}, {"title": "6.2. Analysis of patterns and their reuse", "content": "In this section, we analyse the detailed pattern quality and determine the extent to which the pattern contributes to achieving a good solution. We have selected an instance in Burke-4 (experiment 15) as a representative case for discussion. Similar behaviours can be observed from most other instances.\nWe firstly provide an offline oracle solution with all information being known in advance. The bin patterns used in such an offline oracle solution might be regarded as high-quality patterns. We expect an algorithm that is able to recognise good patterns will tend to pack bins similarly to the oracle solution. That is, not only the high-quality patterns should be used more in the online solution, but also the pattern distribution should be close to the offline oracle.\nFigure 4 represents the histogram of solution patterns. All patterns are sorted by their fill rates, and each pattern is assigned a unique index, where a larger index indicates a higher fill rate. The changes of fill rates across different pattern indices is represented by the blue curve in the figure (measured by the second y-axis on the right). The height of each histogram bar represents the quantity of a certain pattern used in the solution. The Offline histogram represents the pattern distribution for the offline oracle, where the patterns are considered in high-quality. In comparison, CGPP achieved a histogram that closely resembles the offline solution, with more high-quality patterns being used and much higher overlap with oracle solution. This indicates that not only was CGPP able to identify good patterns, but it could also effectively reuse those patterns to reduce overall waste, resulting in improved bin usage.\nFor PatternPack, it also demonstrated the ability to reuse patterns. However, it favoured patterns at index 300-400, resulting in not only high frequency of sub-optimal patterns (90% 95% filled rate), but also low overlap with oracle patterns. In the case of ProfilePack, the patterns were more evenly distributed. It achieved better overlap at index 500-700 than PatternPack but it also used many patterns of low fill rates (e.g. pattern index 0-300), which rarely appeared in offline oracle. These low quality patterns resulted in poorer overall performance."}, {"title": "6.3. Discussion on compared methods", "content": "In most datasets in our experiment, CGPP outperforms BestFit and other existing methods. The advantage can be mainly attributed to the dynamic pattern identification and associated planning, the reactive fallback strategy. Noting that with distribution of random variables being given, CGPP could achieve near-optimal solutions, significantly outperforming all other methods. Under unknown distributions, it achieved excellent performance in most cases when compared with other methods.\nOur proposed method significantly outperformed ORL approach. To our surprise, the ORL did not even outperform the BestFit strategy in most case. This could potentially be attributed to insufficient training time, considering the hardware limitations in our experiments compared to the original work. Furthermore, even when well-trained, the ORL exhibited weaker generalisation ability. As reported by Balaji et al. [38], a RL model trained on PP/BW instances performed poorly on LW instances. In contrast, our method demonstrated strong generalisation capabilities, even with limited prior knowledge.\nPatternPack is designed to solve problems with large discrete or continuous item types [42], which is not the case in this work. Our method demonstrated superior performance in terms of average bin usage compared to PatternPack and its fuzzy-enhanced version FPP, as our method utilised dualism pricing based Column Generation for planning, which typically yields better results with the online heuristic employed by PatternPack. Additionally, our uncertainty handling strategy can identify and eliminate planning errors caused by imperfect predictions.\nProfilePack theoretically proved that applying good prediction can lead to high-quality solutions. In terms of implementation detail, the best-fit descending profile generation was not robust towards changing distributions. Also, the method lacks an uncertainty handling strategy. In some special cases (e.g., experiment 26), ProfilePack generates extremely poor results, indicating its major reliability issues. On the other hand, the success of CGPP in most cases also supports the theoretical result that good prediction can guide a better online packing strategy."}, {"title": "7. Conclusion and future work", "content": "In combinatorial optimisation, patterns are reusable building blocks of solutions that are more favourable than black-box solvers. However, we showed in this research that the values of patterns could change due to uncertainties related to objectives and constraints and most existing methods fail to exploit the inter-dependencies among decision variables incurred by uncertainties in constraints. We established a scheme to dynamically quantify the usefulness of different patterns based on the dualism of COP and use the information to guide the decision process. To handle the influence caused by both underestimation and overestimation, we introduced threshold-based methods to eliminate the inconsistency between plan and observation. The test results on bin packing problem show significant performance advantage from the proposed method compared with the current the state-of-the-art methods. In future, we would investigate how the proposed framework generalises to other COPs with similar structures."}]}