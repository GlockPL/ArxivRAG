{"title": "STAR: Spectral Truncation and Rescale for Model Merging", "authors": ["Yu-Ang Lee", "Ching-Yun Ko", "Tejaswini Pedapati", "I-Hsin Chung", "Mi-Yen Yeh", "Pin-Yu Chen"], "abstract": "Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose Spectral Truncation And Rescale (STAR) that aims at mitigating \u201cmerging conflicts\u201d by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2% when merging 12 models on Flan-T5. Our code is publicly available at https://github.com/IBM/STAR.", "sections": [{"title": "1 Introduction", "content": "With the popularity of pretrained models on large neural networks, the same architecture is often deployed to fine-tune individual natural language processing (NLP) tasks. A natural question then arises about whether it is possible to merge these same-architecture fine-tuned models into one multi-task model. For example, researchers are interested in understanding if we can empower a fine-tuned conversational large language model (LLM) with reasoning capabilities by merging with an LLM specializing in solving math problems. Specifically, Ilharco et al. (2022) has formally defined a task vector as $\\Theta_{ft} - \\Theta_{pre}$, where $\\Theta_{pre}$ and $\\Theta_{ft}$ denote the vectorized parameters of the pre-trained model and the fine-tuned model, respectively. Thus, task vectors mark the updates made to the pretrained model's weights when fine-tuned on specific tasks. Then, model merging essentially studies ways of fusing different task vectors that are trained separately and merging them with the pretrained model. However, as the number of fine-tuned models increases, the multi-task performance of their merged model also decreases drastically. Furthermore, we point out that when the number of models exceeds a certain threshold, the multi-task performance of the merged model could be even worse than that of the original pretrained model, diminishing the fundamental goal of model merging. The complexity of existing model merging methods varies largely depending on whether they require fine-tuning or inference on training data (Yang et al., 2024). In this paper, we study the \"data-free\" setting when we are not authorized to change the fine-tuning protocol nor do we have access to the training data. In this work, we propose to use spectral decomposition (e.g. singular value decomposition, SVD) to remove noisy components on model merging. We will also motivate the potential gain of our spectral space merging scheme by comparing the upper bounds of the task conflicts. A rescaling step is then followed to restore the original nuclear norm. We give the overview of the proposed method in Fig. 2. Our proposed merging scheme, Spectral Truncation And Rescale (STAR), is effective and efficient as it requires no additional inference on original training data and is not sensitive to hyperparameters. Our extensive experimental results show that STAR is superior across various model size settings and can effectively merge up to 20 models while achieving positive performance gains, compared to the pretrained model before merging."}, {"title": "2 Background and Related Work", "content": "Model merging methods belong to two categories: Pre-merging and During-merging methods (Yang et al., 2024). While pre-merging methods focus on renovating the fine-tuning step such that the fine-tuned models suit model merging better (Ortiz-Jimenez et al., 2024; Imfeld et al., 2023; Guerrero Pena et al., 2022), during-merging methods assume no access to the fine-tuning and work directly on models given. Recently, Yang et al. (2024) further classifies during-merging methods into five sub-classes, of which STAR is most related to the weighted-based and subspace-based methods.\nAs base merging methods such as Ilharco et al. (2022) applies the same scaling across all model layers and tasks, weighted-based methods take the importance of parameters into account and scale differently, e.g. Matena and Raffel (2022); Tam et al. (2024) leverage Fisher matrix for assessing the importance of parameters, while others utilize Hessian estimation or entropy, etc (Daheim et al., 2023; Yang et al., 2023). However, these methods require inference through original data, making it infeasible with limited compute or access to task data. MetaGPT (Zhou et al., 2024) proposes a closed form solution for scaling task vectors by minimizing the average loss of the merged model and the independent model.\nAnother line of work transforms task vectors into sparse subspaces (Davari and Belilovsky, 2023; Yadav et al., 2024; Wang et al., 2024; Huang et al., 2024), e.g. TIES (Yadav et al., 2024) trims task vectors to keep only the top K% parameters with the highest magnitude, before undergoing an elect-sign step to reduce sign conflicts; TALL-masks (Wang et al., 2024) constructs per-task masks that identifies important parameters within each task, which are then merged into one general mask based on consensus among multiple per-task masks.\nSTAR differs from the above as it transforms task vectors to the spectral spaces, and its truncation and scale are task-dependent and layer-specific."}, {"title": "2.1 Notations and Problem Definition", "content": "We denote the weight matrices of a pretrained LM by $\\Theta_{pre}^l$ for $l = \\{1, ..., L\\}$, where L is the total number of such matrices. Let $\\Theta_{pre}$ denote the concatenation of all vectorized weight matrices and $\\Theta_{ft}$ denote the updated model parameters after fine-tuning on task T. A task vector $\\delta$ is then defined as the difference between $\\Theta_{ft}$ and $\\Theta_{pre}$, i.e., $\\delta = \\Theta_{ft} - \\Theta_{pre}$ (Ilharco et al., 2022). Given T fine-tuned models, model merging fuses $\\{\\delta_1,..., \\delta_T\\}$ into a merged $\\delta_{merged}$ such that $\\Theta_{pre} + \\delta_{merged}$ still performs well on T tasks simultaneously."}, {"title": "2.2 Related Work", "content": "Model merging methods belong to two categories: Pre-merging and During-merging methods (Yang et al., 2024). While pre-merging methods focus on renovating the fine-tuning step such that the fine-tuned models suit model merging better (Ortiz-Jimenez et al., 2024; Imfeld et al., 2023; Guerrero Pena et al., 2022), during-merging methods assume no access to the fine-tuning and work directly on models given. Recently, Yang et al. (2024) further classifies during-merging methods into five sub-classes, of which STAR is most related to the weighted-based and subspace-based methods.\nAs base merging methods such as Ilharco et al. (2022) applies the same scaling across all model layers and tasks, weighted-based methods take the importance of parameters into account and scale differently, e.g. Matena and Raffel (2022); Tam et al. (2024) leverage Fisher matrix for assessing the importance of parameters, while others utilize Hessian estimation or entropy, etc (Daheim et al., 2023; Yang et al., 2023). However, these methods require inference through original data, making it infeasible with limited compute or access to task data. MetaGPT (Zhou et al., 2024) proposes a closed form solution for scaling task vectors by minimizing the average loss of the merged model and the independent model.\nAnother line of work transforms task vectors into sparse subspaces (Davari and Belilovsky, 2023; Yadav et al., 2024; Wang et al., 2024; Huang et al., 2024), e.g. TIES (Yadav et al., 2024) trims task vectors to keep only the top K% parameters with the highest magnitude, before undergoing an elect-sign step to reduce sign conflicts; TALL-masks (Wang et al., 2024) constructs per-task masks that identifies important parameters within each task, which are then merged into one general mask based on consensus among multiple per-task masks.\nSTAR differs from the above as it transforms task vectors to the spectral spaces, and its truncation and scale are task-dependent and layer-specific."}, {"title": "3 Methodology", "content": "Sec. 3.1 provides the rationale behind performing truncation in the spectral space. Sec. 3.2 defines the rescaling step for restoring the nuclear norm. Sec. 3.3 gives the complete STAR algorithm."}, {"title": "3.1 Spectral Truncation", "content": "Let $T_1, T_2$ be two fine-tuning tasks that yield task vectors $\\delta_{\\tau_1}$ and $\\delta_{\\tau_2}$. Take the entries correspond to a weight matrix and reconstruct them into A, B from $\\delta_{\\tau_1}$ and $\\delta_{\\tau_2}$, respectively. Suppose A and B admit SVD into $A \\approx U \\Sigma_A V^T$ and $B \\approx U \\Sigma_B V^T$, one can obtain the matrix rank by the number of nonzero singular values. By selecting only the top few singular values and vectors (i.e. truncated SVD), we naturally find the principal components and remove the redundant dimensions, effectively reducing the rank of the matrix. As small singular values often correlate with noise or fine details, low-rank prior is also widely used in compressed sensing and denoising applications in signal processing.\nBesides extracting principal components, we also give a high-level illustration of why using truncated SVD on A and B separately can help reduce conflicts during model merging. Assume $T_1$ is associated with data manifold $\\mathcal{D}_A$. For $x \\in \\mathcal{D}_A$, we essentially hope $(A + B)x$ to be close to Ax while excelling at $T_2$ after merging, where $\\oplus$ denotes the merging operation. Let us consider the merging operation to be plainly A + B, then the level of conflicts can be measured by $||Bx||$. By expressing $x \\in \\mathcal{D}_A$ via the right singular vectors of A, $x = \\sum_j \\alpha_j v_j^A$, we prove in Sec. A.1 that we have $||Bx|| < r_B \\beta \\sqrt{r_A}$, where $\\beta = \\max_{i, j} |\\sigma_i^B \\alpha_j|$, and $r_A$ and $r_B$ are the original ranks of A and B. By truncating B to rank-$r$, this upper bound is lowered by $(r_B - r)\\beta \\sqrt{r_A}$, implying potentially less conflicts in model merging."}, {"title": "3.2 Rescale to Restore Matrix Nuclear Norm", "content": "As model merging favors spectral truncation as discussed in Sec. 3.1, a caveat is the resulting change in the ratio between the pretrained model and the task vector. Roughly, one sees that $||Ax|| = ||\\sum_i \\sigma_i^A v_i^A \\sum_j \\alpha_j v_j^A|| = ||\\sum_i \\sigma_i^A \\alpha_i||$ and can at most be $\\sum_{i=r+1}^A ||\\sigma_i^A \\alpha_i||$ smaller with the truncated A. Therefore, the performance on the fine-tuning task $T_1$ might be compromised. On that account, it is crucial to include a step where we rescale the spectral-truncated weight matrices back to their original \u201csize\u201d, similar to the compensation operation in dropout. We propose to retain matrix nuclear norm (aka Schatten 1-norm or trace norm) as it is a proper measure of matrix \u201csize\u201d, especially in low-rank approximation contexts as nuclear norm is a convex relaxation of the rank function. Specifically, we rescale the remaining singular values by\n$\\sigma'_k = \\frac{\\sum_i \\sigma_i}{\\sum_{i=1}^r \\sigma_i} \\cdot \\sigma_k, \\forall k \\in [1, r].$"}, {"title": "3.3 STAR: Spectral Truncate And Rescale", "content": "Now that we have elaborated on the two key components of STAR, we explain the complete workflow in the following. With T task vectors, we transform them into respective spectral spaces via SVD, and their ranks are determined by $r = \\arg \\min_k \\{k: \\frac{\\sum_{i=1}^k \\sigma_i}{\\sum_i \\sigma_i} \\ge \\eta\\}$, where $\\eta$ is a tunable parameter. Then, we follow Section 3.2 to rescale back to their original nuclear norm. Finally, STAR reconstructs T task vectors from their decompositions and perform simple averaging to obtain $\\delta_{merged}$. We give the full STAR model merging algorithm in Alg. 1 in appendix.\nWe note that as the distribution of singular values varies both within and across task vectors, truncating components adaptively allows different ranks across not only tasks and even layers (e.g. Fig. 3)."}, {"title": "4 Experiments", "content": "We compare STAR to other data-free approaches, including TIES (Yadav et al., 2024), TALL-masks (Wang et al., 2024), which we apply on top of Task Arithmetic (Ilharco et al., 2022), i.e., Consensus Task Arithmetic (without tuning the data-dependent hyperparameter $A_t$), and MetaGPT (Zhou et al., 2024). Due to the page limit, we defer the discussion around EMR-Merging (Huang et al., 2024) and DARE (Yu et al., 2024) to appendix Sec. A.3 and Sec. A.4.\nThe results on Flan-T5-large and Mistral-7B-Instruct are shown in Fig. 4 and Flan-T5-base in Fig. 1. We note that similar trends as Fig. 1 can be seen in Fig. 4 where the averaged normalized performance decreases as the number of models merged increases, with STAR's performance decay being the slowest across models. On Flan-T5-base, MetaGPT tends to fail quickly, echoing with the findings in (Zhou et al., 2024) - MetaGPT may face limitations when merging models of smaller sizes (e.g. Flan-T5-base has only 0.25B parameters) due to its reliance on NTK linearization. To examine the full potential of each algorithm, we also perform grid search for TIES and STAR and report the best result in Appendix Sec. A.5."}, {"title": "4.1 Experimental Setup", "content": "We consider both encoder-decoder models (e.g. Flan-T5-base/large) (Chung et al., 2024) and decoder-only model (e.g. Mistral-7B-Instruct-v0.2) (Jiang et al., 2023). For Flan-T5-base/large, we use finetuned models on GLUE from Fusion-Bench (Tang et al., 2024), together with additional fine-tuned models on Finance (Malo et al., 2014), IMDB (Maas et al., 2011), AG News (Zhang et al., 2015), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), and HellaSwag (Zellers et al., 2019) by ourselves, bringing the total number of task vectors to 13. For Mistral-Instruct, we randomly select 20 models directly from the Lots of LoRAs collection (Br\u00fcel-Gabrielsson et al., 2024), which covers a range of NLI tasks. All models considered herein are LoRA finetuned (Hu et al., 2021) with rank 16 and scaling factor (alpha) set to 32. Details about the models are in Appendix Sec. A.6. To understand how each merging method performs on n models, we randomly sample n tasks and report their average results.\nWithout otherwise specified, we let K = 20 for TIES (the default parameter in (Yadav et al., 2024)), $\\delta_t = 0.4$ for TALL-masks (the middle value searched by (Wang et al., 2024)), and $\\eta = 40$ for STAR.\nFollowing Tang et al. (2024); Br\u00fcel-Gabrielsson et al. (2024), performances on QASC (Khot et al., 2020) and STSB (Cer et al., 2017) are evaluated by F1 score and Spearman's coefficient, respectively, and accuracy for all other tasks. If the correct output appears within the first 10 tokens generated by the merged model, the response is deemed correct. For a model merged on t tasks, we report the normalized average performance (Ilharco et al., 2022; Yadav et al., 2024) defined by $\\frac{1}{t} \\sum_{T_i=1}^t \\frac{Merged Model Perf._i}{Finetuned Model Perf._i}$. We further measure the performance of the pretrained model by $\\frac{1}{t} \\sum_{T_i=1}^t \\frac{Pretrained Model Perf._i}{Finetuned Model Perf._i}$. If the merged model performs worse than the pretrained model, then model merging loses its purpose."}, {"title": "4.2 Performance Comparison", "content": "We compare STAR to other data-free approaches, including TIES (Yadav et al., 2024), TALL-masks (Wang et al., 2024), which we apply on top of Task Arithmetic (Ilharco et al., 2022), i.e., Consensus Task Arithmetic (without tuning the data-dependent hyperparameter $A_t$), and MetaGPT (Zhou et al., 2024). Due to the page limit, we defer the discussion around EMR-Merging (Huang et al., 2024) and DARE (Yu et al., 2024) to appendix Sec. A.3 and Sec. A.4."}, {"title": "4.3 Additional Results", "content": "In Table 1, we give an example of merging 4 fine-tuned Flan-T5-large models with and without rescale to restore the matrix nuclear norm. We see that rescale is crucial especially when we use low-rank approximations (e.g. rank-2).\nAs $\\eta$ is the only tunable hyperparameter in STAR, we further show in Fig. 6 that $\\eta$ is robust across different model merging combinations and numbers of models merged, compared to the baseline (e.g. TIES). Specifically, we allow STAR to choose $\\eta$ from $\\{10, 20, . . . , 70\\}$ and TIES to choose K from $\\{1, 5, 10, 20, ..., 70\\}$. From the standard deviation in Fig. 6, it can indeed be seen that STAR is not sensitive to $\\eta$, sparing users' need to fine-tune $\\eta$ during the deployment.\nFollowing Ilharco et al. (2022), we report the optimal $\\eta$ when merging different number of models in Fig. 5. By searching for $\\eta$ within $\\{10, 20, . . ., 70\\}$ across all sampled model merging combinations, we observed an interesting trend: as the number of merged models increases, the optimal $\\eta$ gradually decreases, indicating that higher truncation for each task vector is necessary."}, {"title": "5 Conclusion", "content": "In this paper, we propose Spectral Truncation And Rescale (STAR) for model merging by removing noisy components via spectral decomposition and restoring the original nuclear norm through rescaling. STAR requires no additional inference and is robust to different hyperparameter choices and language models. STAR provides a principaled way of automatic rank determination and is intuitively complimentary to other merging methods."}, {"title": "Limitation", "content": "While STAR demonstrates strong potential for practical model merging use cases across domains, its performance has been tested primarily on parameter-efficient fine-tuned (PEFT) models in NLP. Additionally, STAR requires SVD to orthogonalize task vectors, which may introduce additional computational cost. However, users can mitigate this by leveraging fast SVD algorithms in the implementation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Bounding $\\|Bx\\|$", "content": "Let $r_A$ and $r_B$ be the original ranks of A and B, $B \\approx U^B \\Sigma_B (V^B)^T = \\sum_{i=1}^{r_B} \\sigma_i^B u_i^B (v_i^B)^T$, $x = \\sum_{j=1}^{r_A} \\alpha_j v_j^A$, and $\\{v_i^B\\}_{i=1}^{r_B}$ and $\\{v_j^A\\}_{j=1}^{r_A}$ are orthonormal vectors, then we have\n$\\begin{aligned}\n||Bx|| &= || B \\sum_j \\alpha_j v_j^A ||\\\\\n&= || \\sum_i \\sigma_i^B u_i^B (v_i^B)^T \\sum_j \\alpha_j v_j^A ||\\\\\n&= || \\sum_i u_i^B \\sigma_i^B \\sum_j \\alpha_j (v_i^B)^T v_j^A ||\\\\\n&\\le \\sum_i |u_i^B| |\\sigma_i^B| \\sum_j |\\alpha_j (v_i^B)^T v_j^A |\\\\\n&\\le \\beta \\cdot |\\sum_j (v_i^B)^T v_j^A |\\\\\n&\\le \\beta \\sqrt{\\sum_i \\sum_j ((v_i^B)^T v_j^A)^2}\\\\\n&\\le \\beta \\sqrt{\\sum_i \\sum_j ((v_i^B)^T v_j^A)^2}\\\\\n&= \\beta \\sqrt{\\sum_j \\sum_i ((v_i^B)^T v_j^A)^2}\\\\\n&\\le \\beta \\sqrt{\\sum_j 1}\\\\\n&= r_B \\beta \\sqrt{r_A}, \\\\end{aligned}$ \nwhere $\\beta = \\max_{i,j} |\\sigma_i^B \\alpha_j|$, and inequality (1) uses Cauchy-Schwarz inequality. Then we show that\n$\\begin{aligned}\n1 &= ||x||^2\\\\\n&= || \\sum_{j=1}^{r_A} \\alpha_j v_j^A ||^2 \\\\\n&= || v_1 + v_2 + ... + v_{r_A} + v_B | A||^2 \\\\\n&= \\sum_{j=1}^{r_A} || (v_i^B)^T v_j^A ||^2 + ||v_B | A||^2 \\\\\n&= \\sum_{j=1}^{r_A} ((v_i^B)^T v_j^A)^2 + ||v_B | A||^2 \\\\\n&\\ge \\sum_{j=1}^{r_A} ((v_i^B)^T v_j^A)^2, \\\\end{aligned}$ \nwhere equation (3) expresses $v_i^B$ by $\\{v_j^A\\}_{j=1}^{r_A}$, and $v_B | A$ denotes the part of $v_i^B$ that is orthogonal to the span of $\\{v_j^A\\}_{j=1}^{r_A}$. Equation (4) follows Pythagorean identity since $v_1, v_2, ..., v_{r_A}, v_B | A$ are pairwise-orthogonal vectors. Finally, with Equation (2) and (5), we have\n$\\begin{aligned}\n||Bx|| < r_B \\beta \\sqrt{r_A}. \\\\end{aligned}$"}, {"title": "A.2 Algorithm", "content": ""}, {"title": "A.3 Discussion on EMR-Merging", "content": "EMR-Merging (Huang et al., 2024) is a recent data-free model merging method that reports outstanding performance with minimal additional storage. It first constructs a unified merged task vector, $T_{uni}$, which retains the maximum amplitude and sign information shared by all task vectors ($T_i$). Then, task-specific masks ($M_i$) and rescalers ($\\lambda_i$) are derived based on sign agreement and parameter magnitude alignment between $T_i$ and $T_{uni}$. Finally, during inference, EMR-Merging dynamically adapts $T_{uni}$ for each task using\n$W_t = W_{pre} + \\Delta_t,$\nwhere\n$\\Delta_t = \\lambda_t M_t T_{uni}.$\nIn other words, EMR-Merging adjusts model weights at run-time, whereas our approach, along with the included baselines (i.e., TIES, MetaGPT, and TALL-masks), operates statically. This makes direct comparison infeasible; therefore, we do not include EMR-Merging as one of the baselines."}, {"title": "A.4 Discussion on DARE", "content": "STAR follows a similar protocol to DARE (Yu et al., 2024), as both methods involve two steps: dropping certain components and rescaling. However, there are key differences between them.\nOn one hand, DARE randomly drops entries of task vectors in parameter space, following:\n$\\begin{aligned}\nm_i^t \\sim Bernoulli(p), \\\\\ns_t^i = (1 - m_i^t) s_t^i.\\\n\\end{aligned}$ \nIn contrast, STAR selectively removes redundant dimensions in spectral space.\nOn the other hand, DARE's rescaling scheme is based on:\n$\\hat{s}_t^i = \\frac{s_t^i}{1 - p}$ aiming at approximating the original embeddings, while STAR's rescaling focus on restore the spectral-truncated weight matrices to their original scale.\nUnlike STAR, which can function as a standalone model merging method, DARE primarily serves as a plug-in to enhance other merging techniques. For comparison, we follow DARE's protocol and report the results of DARE+TA (Task Arithmetic) and DARE+TIES in Table 2. Specifically, we vary DARE's drop rate p from $\\{0.1, 0.2, ..., 0.9\\}$, and the results suggest that even when DARE is applied on top of TA and TIES, STAR still achieves superior performance."}, {"title": "A.5 One-shot STAR performs even better than grid-search TIES", "content": "Recall that in Fig. 4, we have shown the one-shot performance with pre-determined K = 20 and $\\eta = 40$ for TIES and STAR, respectively. In Fig. 7, we further show their best possible results over the grids we searched for. Specifically, from Fig. 7, we see that the grid search does not improve the performance much on Flan-T5-base for both TIES and STAR. Even after performing grid search for TIES, it still fails to surpass the one-shot performance of STAR, further emphasizing the practicality of our method in real-world applications. On Flan-T5-large, the gain from grid search on TIES becomes obvious especially when we are merging more models. With STAR, grid search over $\\eta$ also helps but the results are relatively consistent."}, {"title": "A.6 Details about the fine-tuned models considered in the experiments", "content": "For Flan-T5-base, we selected 7 LORA-16 finetuned models from FusionBench1 (Tang et al., 2024), which is a benchmark targeted for model merging (excluding only CoLA as it tends to output the same answer), and finetuned 5 additional models ourselves on the Finance, IMDB, AG News, HellaSwag, and BoolQ datasets. We applied the same rank (16) and scaling factor (32) as in FusionBench, with the learning rate and number of epochs tuned on the validation set. Following a similar approach, we selected 7 Flan-T5-large models from FusionBench and finetuned 6 additional models ourselves, including Finance, IMDB, AG News, HellaSwag, and BoolQ, and PIQA.\nFor Mistral-Instruct, 20 models are selected from the Lots of LORA collection 2 (Br\u00fcel-Gabrielsson et al., 2024), which encompasses up to 500 diverse task types, making it an ideal environment for evaluating model merging methods."}]}