{"title": "Towards the Methodology for Solving the Minimum Enclosing Ball and Related Problems", "authors": ["Michael N. Vrahatis"], "abstract": "Methodology is provided towards the solution of the minimum enclosing ball problem. This problem concerns the determination of the unique spherical surface of smallest radius enclosing a given bounded set in the d-dimensional Euclidean space. Mathematical formulation and typical methods for solving this problem are presented. Also, the paper is focused on areas that are related to this problem, namely: (a) promise problems and property testing, (b) theorems for partitioning and enclosing (covering) a set, and (c) computation of the diameter of a set.", "sections": [{"title": "I. INTRODUCTION AND MATHEMATICAL FORMULATION", "content": "METHODOLOGY is provided that is related to the solution of the minimum enclosing ball (MEB) problem, which refers to the determination of the unique spherical surface of smallest radius enclosing a given bounded subset of the d-dimensional Euclidean space Rd. It can be considered that the earliest known statement of the MEB problem was first posed in circa 300 B.C. by Euclid\u00b9 in his seminal work \"Elements\" and specifically in Book IV, Proposition 5 which refers to circumscribe a circle about a given triangle [1].\nThe MEB problem for finding the d-dimensional hyper-sphere (ball) with the smallest radius r centered at the point c, that encloses a set P = {p1, p2, ..., pn } of n points in Rd, can be described as the following minimax optimization problem which involves both minimization over the primal variable c and maximization over the dual variable i:\nMEB(P): $\\min_C \\max_i ||p_i - c ||$, (1)\nwhere ||\u00b7|| is the Euclidean norm in Rd. In the context of mathematical optimization the MEB problem can be formulated as a constrained optimization problem as follows::\nMEB(P):\n$\\{\\begin{array}{ll}\n\\min r, \\\\\n(c, r)\n\\text { s. t. } ||P_i - c|| \\leq r, p_i \\in P .\n\\end{array}$ (2)\nA large variety of methods and algorithms for approximating the MEB and the diameter of a set have been proposed in the literature. For mathematical approaches to the MEB and related to it problems, the interested reader is referred to [2]-[5] and the references therein. In addition, the minimizers related to Problems (1) and (2) can be also computed by applying intelligent optimization and computational intelligence methods, including particle swarm optimization, differential evolution, memetic algorithms, among others. These methods are capable of tackling non-differentiable, discontinuous, discrete, noisy and multimodal objective functions and in general they have gained increasing popularity in recent years due to their relative simplicity and their ability to efficiently and effectively handling several real-world applications (see, e.g. [6]-[15]). Also, these methods have been widely used and applied for many years. For example, for the cases at hand, the study of the performance of the particle swarm optimization method in coping with minimax and constrained optimization problems has been initiated in 2002 (cf. [10] and [13] respectively). Related code can be found in [12] and [15].\nIt is worth mentioning that, the exploratory data analysis is an important first step in any data analysis for identification of general patterns in a data set. In real-world applications, these patterns include, among others, outliers of the data set that might be unexpected. In such cases, the action of removing outliers during data preprocessing for enhancing the quality of the data set is important. To this end, the minimum k-enclosing ball (MkEB) can be used for removing n \u2212 k points of the given n points of a set P in Rd. The MkEB problem is a generalization of the MEB one for finding a k-enclosing ball, i.e. a ball with the smallest radius that encloses at least k points of the set P. It can be formulated as follows (cf. [16]):\nMkEB(P) :\n$\\{\\begin{array}{ll}\n\\min r, \\\\\n(c, r)\n\\text { s.t. } card\\{p_i \\in P: ||p_i - c|| < r\\} \\geq k,\n\\end{array}$ (3)\nwhere card{A} = |A| indicates the cardinality of a set A. That is to say, the MkEB(P) problem consists of finding a k-subset X of P (i.e. card{X} = k) whose minimum enclosing ball is one with the smallest radius among all k-enclosing balls. Thus, the MEB(P) Problem (2) is a particular case of the MkEB(P) Problem (3) for k = n. It is worth noting that, in 2013 Shenmaier\u00b2 [17] proved that the MkEB problem is NP-hard in the strong sense [18] and obtain a polynomial-time approximation scheme (PTAS) for solving the problem with an arbitrary relative error \u025b in O(n1/\u025b2+1d) time.\nThe general case of finding the smallest circle enclosing a given finite set of n points in R2 was first given in 1857 by Sylvester\u00b3 [19]. In 1860 Sylvester gave for this problem a graphical solution method attributed to Peirce4 [20]. The same method was independently given in 1885 by Chrystal [21]."}, {"title": "II. TYPICAL METHODS FOR SOLVING THE MEB PROBLEM", "content": "Elzinga-Hearn method: In 1972 Elzinga10 and Hearn11 [26] presented a method for tackling the MEB problem for a set P = {P1,P2,...,pn} of n points in Rd. They formulated the MEB problem as a convex programming problem and gave a finite decomposition algorithm based on the simplex method of quadratic programming. Specifically, by representing the given points pi, i = 1, 2, ..., n as column vectors and assuming that the scalar s is the square of the radius r of a sphere centered at the point c, they formulated in [26] the MEB Problem (2) to the following equivalent convex programming problem:\nMEBpp (P) :\n$\\{\\begin{array}{ll}\n\\min s, \\\\\n(c, s)\n\\text { s.t. } s \\geq (p_i - c) (p_i - c), p_i\\in P .\n\\end{array}$ (4)\nThe above Problem (4) is referred to as the primal problem (PP) which constitutes the original optimization problem.\nThe Kuhn12 and Tucker13 conditions (cf. [27]), that are necessary and sufficient for Problem (4), guarantee the existence of multipliers for i = 1,2,..., n, such that:\n$\\sum_{i=1}^n \\lambda_i = 1$,\n$\\sum_{i=1}^n \\lambda_i (P_i - c^*) = 0$,\n$\\lambda_i (s^* - (P_i - c^*) (p_i \u2013 c^*)) = 0, i = 1, 2, ..., n$,\n$\\lambda_i \\geq 0, i = 1, 2, ..., n$,\n$s^* - (P_i - c^*) (P_i \u2212 c^*) \\geq 0, i = 1, 2, . . ., n$.\n(5)\nThe Kuhn-Tucker Conditions (5) indicate that the center c* of the MEB with s* the square of its radius will be a convex combination of the points which lie on the surface of the MEB. This is so because it holds \u03bbi = 0 for any point pr that lie in its interior.\nIt is worth mentioning here that, in general, the convex hull, convP, of a set P in Rd can be obtained by carrying out all convex combinations of points of P (cf. [28]). On the other hand, due to a theorem of Carath\u00e9odory14 (see Theorem 1 below) it is not necessary to perform combinations involving more than d+1 elements at a time. Therefore, convex combinations of vectors xi of the form $\\sum_{i=1}^m a_i x_i$ (where the real numbers ai satisfy ai \u2265 0 and $\\sum_{i=1}^m a_i = 1$) can be performed where m \u2264 d +1 or m = d + 1 in the case of non-distinct vectors xi.\nThe above mentioned Carath\u00e9odory's theorem in convexity theory states that:\nTheorem 1 (Carath\u00e9odory's theorem (1907) [29]): Any point in the convex hull of a finite point set in Rd is a convex combination of some at most d + 1 of these points.\nThis important theorem in convex geometry has been given in 1907 [29]. It is worth noting that, in 1970 Rockafellar15 [28] pointed out that: \"Carath\u00e9odory's theorem is the fundamental dimensionality result in convexity theory, and it is the source of many other results in which dimensionality is prominent.\" Based on Carath\u00e9odory's Theorem 1 the following result has been presented by Elzinga and Hearn in [26]:\nLemma 2 (Elzinga-Hearn lemma (1972) [26]): The center c* of the MEB can be expressed as a convex combination of at most d + 1 of the given points.\nIn addition, they pointed out that the existence of the MEB (s*, c*), where s* is the square of its radius and c* is its center, is evident. Also, since the minimand of\n$\\min_{(s, c)} max_i (p_i - c) (p_i - c)$, (6)\nis strictly convex, then the following result holds:\nLemma 3 (Elzinga-Hearn lemma (1972) [26]): The MEB (s*, c*) exists and is unique.\nIn 1961 Wolfe16 [30] pointed out that, in general, in the duality theory for mathematical programming a duality theorem is the statement of a relationship of a certain kind between two mathematical programming problems that has three characteristics. Specifically:\n(a) one problem, named the primal problem (PP) concerns to a constrained minimization problem, and the second one, named the dual problem (DP) concerns to a constrained maximization problem,\n(b) the existence of a solution to one of these problems ensures the existence of a solution to the other problem, therefore their respective extreme values are equal, and\n(c) in the case where the constraints of one problem are consistent, while those of the other are not, there is a sequence of points satisfying the constraints of the first on which its objective function tends to infinity."}, {"title": "", "content": "The Wolfe's dual problem (Wdp) for the primal problem is the following (cf. [26], [30]):\nWdp(P):\n$\\{\\begin{array}{ll}\n\\max s + \\sum_{i=1}^n \\lambda_i((P_i - c) (p_i \u2013 c) \u2013 s), \\\\\n(s, c, \\lambda)\n\\text { s. t. } \\sum_{i=1}^n \\lambda_i = 1, \\\\\n\\sum_{i=1}^n \\lambda_i (P_i \u2013 c) = 0, \\\\\n\\lambda_i \\geq 0, i = 1, 2, ..., n .\n\\end{array}$ (7)\nIt is worth noting that, in general, converse duality does not hold in the sense that the solution of the dual problem provides the solution of the primal one. However, in the case of the MEB problem converse duality holds. Specifically, in 1962 Huard17 [31] has shown converse duality for certain \u201cpartially linear\u201d problems, of which Problem (4) is a special case (cf. [26]).\nAn equivalent to Wolfe's dual problem has been proposed in [26] where the authors gave the following dual problem named quadratic programming dual problem (QPdp):\nQPdp(P):\n$\\{\\begin{array}{ll}\n\\max \\sum_{i=1}^n \\lambda_i (p_i^T p_i) \u2013 \\lambda^T (A^T A), \\\\\n\u03bb\n\\text { s. t. } \\sum_{i=1}^n \\lambda_i = 1, \\\\\n\\lambda_i \\geq 0, i = 1, 2, ..., n .\n\\end{array}$ (8)\nwhere \u03bb = (\u03bb1,\u03bb2,...,\u03bb\u03b7)T and the matrix A \u2208 Rd\u00d7n has columns the pi for i = 1,2,...,n. Based on the above, the authors of [26] provided the following theorems:\nTheorem 4 (Elzinga-Hearn theorem (1972) [26]): Wolfe's dual Problem (7) is equivalent to the concave quadratic programming dual Problem (8), with\nand\n$c = \\sum_{i=1}^n \\lambda_i p_i,$\n(9)\n$s = \\sum_{i=1}^n \\lambda_i (p_i - c) (p_i - c)$.\n(10)\nTheorem 5 (Elzinga-Hearn theorem (1972) [26]): Assume that (s*, c*, \u03bb*) solves the concave quadratic programming dual Problem (8) then (s*, c*) solves the primal Problem (4).\nBy Theorem 5 it is evident that the converse duality holds. On the other hand, the solution of the corresponding quadratic problem requires a very large amount of computer storage. To this end, the authors developed a finite decomposition algorithm, based on the simplex method of quadratic programming, that makes the computer storage requirements independent of the number of points and computing time approximately linear in the number of points.\nHopp-Reeve method: In 1996 Hopp18 and Reeve19 [32] proposed a simple and efficient method for computing the MEB of a set of n points P = {P1,P2,...,Pn}C Rd. Their method geometrically constructs the MEB using an iterated two-step procedure. At the beginning of each iteration, a non-empty set QC P exists such that all points in Q are affinely independent. Furthermore, an enclosing ball (EB) for P exists such that each point in Q lies on the surface of the EB. Note that the EB is not necessarily the MEB for Q. Assume that c is the center of the EB which initially may be taken as point p1 and the set Q as the point farthest from P1. The points in Q before each iteration are called candidate points, while the points in Q after the final iteration are called constraining points. A high level description of the iterated steps in constructing the MEB are exhibited in Algorithm 1.\nAlgorithm 1 Hopp and Reeve (1996) [32]:\nGeometrically construction of the MEB.\nInput: The sets P = {P1, P2, ..., pn } C Rd and Q \u2286 P.\n1: Compute the center t of the MEB for Q, then remove from Q any points which do not constrain the MEB. The point t then serves as a target.\n2: Shrink the EB by moving the center c toward t while maintaining all points in Q on the sphere surface. If the surface of the shrinking EB contacts a point in P but not in Q, fix c and add the new point to Q.\nOutput: The MEB for P has been found when either:\n(a) Q contains exactly d + 1 points at the end of Step 1 (in this case c and t will coincide at the start of Step 2), or\n(b) the center c of the shrinking EB in Step 2 reaches target t without the sphere surface contacting a new point.\nThe expected computing time of the Hopp-Reeve algorithm, in the worst case with all the points near the ball surface, was estimated at O(nd2.3). The authors pointed out that the number of points n and dimension d are bounded above only by computer storage limitations. Furthermore, in infinite-precision arithmetic the MEB must always be found in at most\n$N_{iter} = \\sum_{i=2}^{min\\{n, d+1\\}} \\frac{n!}{i! (n - 1)!}$ (11)\niterations. However, in finite-precision arithmetic an endless loop is likely to occur among several sets of points. Thus, to cope with this possibility, in the implementation of this algorithm should be incorporated an upper bound on the number of iterations.\nB\u0103doiu-Clarkson method: In 2003 B\u0103doiu20 and Clarkson21 [33] proposed a simple iterative approximation algorithm for tackling the MEB problem for a set P = {P1,P2,...,Pn}"}, {"title": "", "content": "Algorithm 2 B\u0103doiu and Clarkson (2003) [33]:\nApproximation of the MEB problem.\nInput: The set P = {P1,P2,..., pn} C Rd and k\u2208 [1,n].\nChoose an arbitrary point p; \u2208 P, (1 \u2264 j \u2264 n).\nSet C\u2081 = Pj.\nfor i 2 tok do\nFind the point pi \u2208 P farthest away from ci-1.\nSet Ci = Ci\u22121 + (Pi - Ci\u22121)/i.\nend for\nSet rk = maxx\u2208P ||X - Ck ||.\nOutput: The ball of radius rk centered at the point ck.\nFurthermore, the authors gave the following result that is related to Algorithm 2.\nProposition 6 (B\u0103doiu-Clarkson proposition (2003) [33]):\nIf c and r are the center and radius of a minimal ball enclosing the set P then ||c \u2013 ci|| <r/\u221ai for all i."}, {"title": "III. PROMISE PROBLEMS, PROPERTY TESTING AND PARTITIONING THEOREMS", "content": "The MEB problem can be tackled by applying enclosing and partitioning theorems and it is, among others, one of the most fundamental issues in clustering. Specifically, clustering with respect to the diameter and the radius costs, is the task of partitioning a set of points in Rd to subsets, where items in the same subset, named cluster, are similar to each other, compared to items in other clusters (see, e.g. [34], [35]). Usually, clustering problems arise in the analysis of large data sets. Thus, the approximate clustering via core-sets is used for clustering of a set of points in Rd (for large d) by extracting properly a small set of points named core-set that \"represents\u201d the given set of points [36]. These issues lie within the domains of promise problems and property testing.\nPromise problems: The notion of the promise problem has been introduced and studied in 1984 by Even22, Selman23 and Yacobi24 [37] (cf. [38]). In general, a promise problem can be considered as a formulation of a partial decision problem and the complexity issues related to it have been arisen from considerations about cracking problems for public-key cryptosystems (cf. [37], [39]). The authors of [37] considered a decision problem as a predicate P(x). They pointed out that the objective is the determination whether there exists an algorithm A that solves the problem, i.e. such that A(x) converges for all input instances x and\n\u2200x [A(x) = YES \u2192 P(x)].\nAlso, they pointed out that, in general, there are problems for which only a subclass of the domain of all instances is required. The authors named these problems promise problems and suggested that a promise problem has the structure:\n(a) input x,\n(b) promise Q(x),\n(c) property R(x),\nwhere Q and R are predicates. Furthermore, they deduced that, formally:\n(a) A promise problem is a pair of predicates (Q, R).\n(b) The predicate Q is called the promise.\n(c) A deterministic Turing25 machine M solves the promise problem (Q, R) if\n\u2200x [Q(x) \u2192 [M(x)\u2193 (M(x) = YES \u2194 R(x))]],\nwhere the notation M(x) \u2193 indicates that M eventually halts on input x.\n(d) A promise problem (Q, R) is solvable if there exists a Turing machine M that solves it.\n(e) If a Turing machine M solves (Q, R), then the language L(M) accepted by M is a solution to (Q, R).\nIt is worth mentioning that, in 2006 Goldreich26 [38] pointed out that any decision problem is a promise problem, although in some cases the promise is trivial or tractable. He noticed that the promise problems constitute a natural generalization of the decision problems and, in many cases, a promise problem provides the more natural formulation of a decision problem (cf. [40]). Also, the author considered that, formally, a promise problem refers to a three-way partition of the set of all strings into three subsets; namely:\n(a) The set of strings representing YES-instances.\n(b) The set of strings representing NO-instances.\n(c) The set of disallowed strings that represent neither YES-instances nor NO-instances.\nThe union of the set of all the YES-instances and the set of all the NO-instances is called the promise. The set of instances that satisfy the promise is named promise set, while the set of the rest instances is called the set of instances that violate the promise. Hence, an eventual decider (e.g. algorithm or procedure) for solving the promise problem has only to distinguish YES-instances from NO-instances, while, for the inputs that violate the promise is allowed arbitrary behavior. Consequently, is only required to detect that an input is either a YES-instance or a NO-instance.\nIn addition, the author of [40] has commented that the discrepancy between the formulation of intuitive promise problem and the standard formulation of decision problems can be easily tackled in the case where there exists an efficient algorithm for determining membership in the promise set. In this instance, the promise problem is computationally equivalent to deciding membership in the set of YES-instances. On the other hand, in the case where the promise set is not tractable, the terminology of promise problems is necessary (cf. [40]). For further details on promise problems and their applications, the interested reader is referred to e.g. [37]\u2013[41]."}, {"title": "", "content": "In 2023 Gon\u00e7alves27, Keren28, Shahar29 and Yehuda30 [41] proposed a probabilistic approach for the clustering promise problem. They pointed out that, their main contribution is to bring together ideas from the theory of random fields and streaming algorithms and their goal was to devise algorithms for answering the clustering promise problem quickly, while having full access to the dataset. They defined the clustering promise problem:\n\u03a0 = \u03a0YES O \u03a0NO,\nHYES = {P : P is (k\u2081,\u025b)-clusterable},\n\u03a0NO = {P : P is (k2, \u03b4)-far from being clusterable},\nwhere, given the parameters k1,k2 \u2208 N and \u03b5, \u03b4 \u2208 R+, a set of vectors P C Rd is defined to be (k\u2081,\u025b)-clusterable if there are k\u2081 balls of radius & that cover P; while PC Rd is defined to be (k2, 8)-far from being clusterable if there are at least k2 vectors in P, with all pairwise distances at least 8. For realizing the relation between the conditions HYES and \u03a0NO the authors of [41] gave the following result:\nLemma 7 (Gon\u00e7alves-Keren-Shahar-Yehuda lemma (2023) [41]): If the set of vectors PC Rd has at most l vectors with pairwise distance at least 8, then its minimal cover with balls of radius \u025b is of size at most lc(d) (\u03b4/\u025b)d, where c(d) is a function which depends only on d.\nAlso, they provided a probabilistic algorithm for distinguishing the two issues of HYES and \u03a0NO. Their algorithm determines a decision using only the extreme values of a scalar valued hash function, defined by a random field on the set P and is particularly suitable for distributed and online settings (cf. [41]). It is worth recalling that, in general, a hash function is any function that can be used to map data of arbitrary size to fixed-size values; while a random field over Rd can be considered as a function that maps each x \u2208 Rd to a random variable f(x) over R. The authors noted that, the hash function can be defined using a random field as well as they explicitly shown how such random fields can be constructed (cf. [41]).\nProperty testing: The property testing has been primarily defined in 1996 by Rubinfeld31 and Sudan32 [42] in the context of program testing, cf. [35], (see also, e.g. [43]\u2013[45]). The authors pointed out that, the study of program checkers, self-testing programs and self-correcting programs has been introduced in order to allow one to use a program to compute a function without trusting that the program works correctly. This issue leads to the search for robust characterizations of functions. The authors rendered this notion precise and provided such a characterization for polynomials. They considered characterizations of multivariate polynomials over various domains, e.g. rings of the form Zm or finite fields.\nAlso, they introduced the concept of the robust character-ization of a family of functions F with respect to a set of neighborhoods N, where N consists of a set of tuples in the domain of the functions in F. The authors defined that: \"A property Pover a collection of neighborhoods N is an (\u03b5, \u03b4)-robust characterization of F, if whenever a function satisfies Pon all but a d-fraction of the neighborhoods in N, it is \u025b-close to some function f \u2208 F. Moreover, all the members of F satisfy P on all neighborhoods in N.\u201d\nIt is worth noting that, from this characterization, the authors obtained applications related to the construction of simple and efficient self-testers for polynomial functions. Furthermore, the characterizations provided results in the area of coding theory by giving extremely fast and efficient error-detecting schemata for some well-known codes. In addition, the authors highlighted that, these error-detection schemata play an important role in subsequent results related to the hardness of approximating various NP-optimization problems.\nIn brief, the property testing can be regarded as the testing of a global property with local inspection. It is worth noting that, in 2008 Ron33 [46] (cf. [42], [47]) pointed out that: \u201cProperty testing is the study of the following class of problems: Given the ability to perform (local) queries concerning a particular object the problem is to determine whether the object has a predetermined (global) property or differs significantly from any object that has the property. In the latter case we say it is far from (having) the property. The algorithm is allowed a small probability of failure, and typically it inspects only a small part of the whole object.\u201d Also, the author presented as example the following two cases: (a) the object may be a graph and the property is that it is bipartite, and (b) the object may be a function and the property is that it is linear.\nIt is worth recalling and emphasising that, the concept of the property testing has been activated for designing and development of super-fast algorithms for analysing the global structure of datasets that are too large to read in their entirety in a reasonable time. These algorithms have direct access to items of a huge input data set and determine whether this data set satisfies a desired predetermined global property, or is far from satisfying it. Also, these algorithms manage relatively small portions of the data set and their complexity is measured in terms of the number of accesses to the input. The algorithms that are used are necessarily randomized, otherwise they may be drawing a conclusion from an atypical portion of the input. In addition, they are approximate, because it is not expected the algorithm to produce an exact answer having examined only a portion of the input. For further details regarding issues and aspects of the property testing, the interested reader is referred to e.g. [34], [35], [46], [48]\u2013[56]."}, {"title": "", "content": "discussed the testing of clustering in the framework of property testing. They defined that, a data set P of n points equipped with some metric is (k,b)-clusterable if it can be partitioned into k sets with cost at most b. The authors considered as cost (a) the radius, that is the minimum number r such that each cluster can be enclosed in a ball of radius r, or (b) the diameter, that is the maximal distance between any two points in a cluster (cf. [49]). They noted that, the data set is \u025b-far from being (k, (1 + \u03b2)b)-clusterable if at least en points must be removed from P so that the remaining data is (k, (1 + \u03b2)b)-clusterable. Also, the authors described and analyzed algorithms that use a sample of size polynomial in k and 1/8 and independent of card{P} = n. They noted that, the dependence on \u03b2 and on the dimension, d, of the points varies with the different algorithms. In addition, they pointed out that, this kind of algorithms are particular useful in the case where the set of points P is very large and it may not even be feasible to read all of it. For general metrics, the authors proposed an algorithm for the case B = 1, that requires a sample of size O(k/\u025b). Thus, in this case, the algorithm only queries the distances among points in the sample. Also, they indicated using a simple adversarial construction (of a graph metric) that \u03a9(\u221an/\u025b) points are required for the case \u03b2 < 1. For the Euclidean metric (l2 metric on Rd) with the radius cost and for the case \u03b2 = 0, is sufficient a sample of size O(dk/\u025b).\nThe proof was based on a VC dimension argument that, in general, constitutes a measure of the size of a class of sets. It is worth mentioning that, the Vapnik-Chervonenkis (VC) dimension was introduced by Vapnik37 and Chervonenkis38 in 1968 [57], while the corresponding proofs published in 1971 [58]. The VC dimension constitutes an important notion in statistical learning theory since provides, among others, a measure of the complexity of a model, related to the quality of its fitness on different data sets (see, e.g. [59]).\nFurthermore, the authors of [35] by using a different analysis for the case \u03b2 > 0 proposed that is sufficient a O(k2/(\u03b5\u03b22))-sample. Also, for the l2 metric and the diameter cost, a O(k2/(\u03b5(2/\u00df2)d)-sample suffices. They also shown, using a high-dimensional geometric argument, that the exponential dependence on d is necessary. In addition, the authors exhibited how to apply the results to approximate clustering of data. They pointed out that, their algorithms can also be used to find approximately good clusterings. Specifically, these are clusterings of all but an \u025b-fraction of the points in P that have optimal or close to optimal cost. The advantage of their algorithms is that they create an implicit representation of such clusterings in time independent of card{P} = n. Therefore, without the necessity of the participation of all the points in P, the implicit representation can be used to answer queries related to the cluster in which any given point belongs (cf. [35]).\nPartitioning theorems: In the directions and trends of property testing a considerable role is played by partitioning theorems that, in general, describe the ways in which convex sets intersect with each other. Therefore, it is worth mentioning basic significant results related to partitioning theorems, i.e. Helly-type theorems [60] (see also, e.g. [5] and the references therein).\nCarath\u00e9odory's theorem (cf. Theorem 1) is related to the following partitioning theorems due to Helly39 and Tverberg40:\nTheorem 8 (Helly's partitioning theorem (1913) [61]): Let C1, C2,..., Ck be a finite family of convex subsets of Rd, with k\u2265 d + 1. If the intersection of every d + 1 of these sets is nonempty, then the whole family has a nonempty intersection, i.e. $\\bigcap_{i=1}^k Ci \\neq 0$.\nThe above theorem proposed by Helly in 1913 [62], but it does not published by him until 1923.\nTheorem 9 (Tverberg's partitioning theorem (1966) [63]): Every set with at least (p \u2212 1)(d + 1) + 1 points in Rd can be partitioned into p subsets whose convex hulls all have at least one point in common.\nCarath\u00e9odory's, Helly's, and Tverberg's theorems (cf. Theorems 1, 8 and 9) are equivalent in the sense that each one can be deduced from another [64]. Tverberg gave the first proof of Theorem 9 in 1966 [63], while in 1981 he gave a simpler proof [65]. Tverberg's partitioning theorem generalizes the following theorem that it has been proved by Radon41 in 1921:\nTheorem 10 (Radon's partitioning theorem (1921) [66]): Every set with d + 2 points in Rd can be partitioned into two sets whose convex hulls intersect.\nCarath\u00e9odory's, Helly's, Tverberg's, and Radon's theorems (cf. Theorems 1, 8, 9 and 10) are known as Helly-type theorems [60]. In addition, a classical partitioning theorem for points in the plane that is based on Carath\u00e9odory's theorem has been proved by Birch42 in 1959 [67]. It is worth mentioning here that, Adiprasito43, B\u00e1r\u00e1ny44, Mustafa45 and Terpai46 in 2020 [68] initiated the study of the dimensionless versions of classical theorems in convexity theory. Specifically, they considered the dimensionless versions of the theorems of Carath\u00e9odory, Helly, and Tverberg. The obtained results have several \u201ccolorful\" and \"fractional\u201d consequences and are particulary interesting and motivating, among others, for those who are interested in classical convexity parameters. The authors named these theorems as \u201cno-dimension theorems\u201d.\""}, {"title": "", "content": "case of clustering with outliers, the authors have shown that by querying only a constant size of the sample the approximate clusters can be obtained with high probability.\nThe authors of [51] assumed that the clusters are of a symmetric convex shape. For a set P of n points in Rd and a symmetric convex body S, they named the set of points (k, S)-clusterable if all the points are contained in k translated copies of S. In a promise version of the problem, given a parameter & \u2208 (0, 1] the aim is to distinguish between the cases when P is (k, S)-clusterable and when it is \u025b-far from being (k, S)-clusterable, i.e. all k translated copies of S contain at most a (1 \u2013 \u03b5) fraction of points. For the promise version of the problem, they designed randomized algorithm that are generally called testers. Their approach can also be used to find approximately good clusters in the case of clustering with outliers. They considered the outliers as follows: \u201cGiven a set of n data points, the objective is to remove a set of z points called outliers such that the radius of a MEB on the remaining n \u2212 z points is minimized.", "follows": "nTheorem 13 (Chakraborty-Pratap-Roy-Saraf theorem(2018) [51"}]}