{"title": "ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?", "authors": ["Siddhant Waghjale", "Vishruth Veerendranath", "Zora Zhiruo Wang", "Daniel Fried"], "abstract": "Although large language models (LLMs) have been largely successful in generating functionally correct programs, conditioning models to produce efficient solutions while ensuring correctness remains a challenge. Further, unreliability in benchmarking code efficiency is a hurdle across varying hardware specifications for popular interpreted languages such as Python. In this paper, we present ECCO, a reproducible benchmark for evaluating program efficiency via two paradigms: natural language (NL) based code generation and history-based code editing. On ECCO, we adapt and thoroughly investigate the three most promising existing LLM-based approaches: in-context learning, iterative refinement with execution or NL feedback, and fine-tuning conditioned on execution and editing history. While most methods degrade functional correctness and moderately increase program efficiency, we find that adding execution information often helps maintain functional correctness, and NL feedback enhances more on efficiency. We release our benchmark to support future work on LLM-based generation of efficient code.", "sections": [{"title": "1 Introduction", "content": "The ability to write efficient code is a cornerstone of software development (Li et al., 2022). While large language models (LLMs) have shown remarkable progress in generating functionally correct code (Roziere et al., 2023; Guo et al., 2024), the ability to generate solutions that are both correct and efficient remains elusive (Shypula et al., 2021, 2024).\nCurrent methods for optimizing program efficiency improve performance measured by execution time. However, this apparent success often comes at the cost of severely decreasing the functional correctness (Shypula et al., 2024). An example of this issue is illustrated in Figure 1: When optimizing the program on the left, models sometimes perform spurious optimizations that, although they reduce the runtime, make the program no longer functionally correct so that it fails all test cases. On the other hand, a correct optimization (bottom right) \u2014 that improves efficiency while maintaining functional correctness \u2014 is often harder to achieve for current LMs. This spurious optimization is certainly undesirable in practice, and can even increase debugging time for software developers (Li et al., 2022; Cummins et al., 2023a). To achieve the goal of real and robust program optimization, we ask: Can LMs improve program efficiency without sacrificing functional correctness?\nIn this work, we curate an efficiency-oriented programming benchmark ECCO, short for Ensuring Correctness in Code Optimizations, which enables program evaluation in three aspects: execution correctness, runtime efficiency, and memory efficiency. ECCO supports two optimization paradigms: (i) history-based code editing: based on a previous version of the program, test if an LM can further optimize the code while maintaining its correctness, and (ii) NL-based code generation: test the efficiency of a program generated by an LM given a programming problem described in NL. We collect over 50k Python solution pairs, spanning 1.3k competitive programming problems (Puri et al., 2021), with an average of 3.1 public and 17.3 private test cases to support reliable execution-based evaluations of correctness and efficiency.\nFurther, to perform reliable and reproducible executions, we introduce an evaluation setup using a cloud-hosted code execution engine, JUDGE0 (Do\u0161ilovi\u0107 and Mekterovi\u0107, 2020), which produces stable execution output on correctness, runtime, and memory usage, thanks to its agnostic nature to local hardware specifications. It supports up to 66 programming languages (PLs), allowing future work to extend to other languages."}, {"title": "2 Related Work", "content": "Benchmarks for Code Efficiency Some works have proposed benchmarks for optimizing program assembly code (Bunel et al., 2016; Shypula et al., 2021; Shi and Zhang, 2020; Cummins et al., 2023b). More recently, Shypula et al. (2024) target C++ program speedups, and Huang et al. (2024) evaluate the efficiency of Python solutions for LeetCode coding interview problems (Niu et al., 2024). Although most efforts on LLM-based code generation focus on evaluating functional correctness (e.g., Chen et al. 2021), some works evaluate code efficiency (Moudgalya et al., 2023; Sikka et al., 2020; Jeon et al., 2023; Baik et al., 2024) by classifying the time complexity of programs. However, these works are limited in their single-reference evaluation paradigm, assembly language support, or by the limited problem space on LeetCode (Coignion et al., 2024). Our work supports reliable evaluation across arbitrary coding problems and the widely-used Python language.\nEvaluating Program Efficiency It is challenging to robustly evaluate program efficiency, due to varying hardware platforms and setups. Previous works have evaluated the efficiency of code by executing code in a local software environment (Singhal et al., 2024; Huang et al., 2024) or using Docker containerized environments on local hardware (Khan et al., 2023), but this can result in varying runtime and memory usage across hardware, thus causing unreliable and irreproducible efficiency evaluations. An alternative approach is to use an architecture simulator (Shypula et al., 2024) which ensures the execution of each program is exactly simulated at the hardware level on the x86 architecture, but is limited to compiled languages such as C++. For more popular interpreted languages such as Python and Java (JVM), some use LeetCode\u2019s execution engine (Niu et al., 2024; Coignion et al., 2024), but with a restricted space of testable problems. In our work, we propose an evaluation setup using an accessible cloud computing instance that ensures consistent virtual hardware and reliable benchmarking.\nProgram Optimization Approaches To start, some works explore in-context learning to optimize program efficiency (Shypula et al., 2024; Huang et al., 2024). Beyond vanilla prompting, iterative prompting methods (Madaan et al., 2024; Shinn et al., 2024; Ridnik et al., 2024) have been explored to improve specific aspects of generation, by incorporating feedback or reflection from an LM or external modules. Meanwhile, Shypula et al. (2024) propose fine-tuning with self-play synthetic data, with retrieved in-context examples via a nearest neighbor search. However, none of these methods have been rigorously studied in the correctness-preserving optimization setup. We fill in this gap and provide systematic studies of all methods."}, {"title": "3 The ECCO Benchmark", "content": "In this section, we first introduce our evaluation platform (\u00a73.1), then describe the construction process of our ECCO benchmark (\u00a73.2), and lastly, introduce our two task formulations with corresponding evaluation metrics (\u00a73.3).\nTo reliably evaluate program efficiency in both runtime and memory usage, we need to first establish a robust and reproducible evaluation platform. However, evaluating program efficiency is challenging, as resource usage statistics vary greatly across hardware and setups (Singhal et al., 2024; Huang et al., 2024).\nTo ensure stable execution for interpreted languages such as Python, we propose to use a reproducible cloud compute instance that ensures the same virtual hardware, as illustrated in Figure 2. Specifically, we use an EC2 instance (detailed in \u00a7B), and execute the code within a code execution engine JUDGE0 (Do\u0161ilovi\u0107 and Mekterovi\u0107, 2020). Note that our recipe can easily extend to over 60 programming languages that the JUDGE0 engine supports. This is set up as a sandboxed Docker container within the instance, which thus ensures an isolated setup for secure and reproducible code execution. Our platform is similar to evaluating generated code on LeetCode execution console (Niu et al., 2024; Coignion et al., 2024), but applies to arbitrary coding problems and is not limited to questions available on LeetCode.\nOur goal is to collect programming problems, each with an NL description, and multiple functionally-correct solutions at varied efficiency levels.\nWe collect programming problems from the IBM CodeNet dataset (Puri et al., 2021), which contains competitive programming problems with NL descriptions, user program submissions, and other metadata, scraped from the AIZU and AtCoder online judging systems. CodeNet problems mostly require algorithmic techniques such as data structure optimization.\nSpecifically, we first convert CodeNet to ~187k (slow, fast) Python code pairs following Shypula et al. (2024), where each pair of programs has two solutions for the same coding problem. We converted all Python 2 solutions to Python 3 using lib2to3. Lastly, we filter out spurious program pairs in which the 'fast' code was in fact slower when evaluated on our setup.\nNext, we split the pairs and group programs by their associated problem ID. We filter out all problems with less than two solutions to ensure that each NL problem description has multiple associated solutions, to enable program optimization based on code editing history. We then remove the programs that are repetitive or cannot successfully execute due to syntax errors or test case failures.\nOur curated dataset was partitioned into three subsets: train, validation, and test, with each split consisting of a distinct set of problems. In the end, the process yields 1,380 unique problems in ECCO in total.\nTo evaluate functional correctness, we require test cases. We collect (i) the original test cases for each problem from CodeNet, and (ii) the additional tests from the AlphaCode project (Li et al., 2022). Each test case contains the program inputs as well as expected outputs when executing canonical program solutions on these inputs. With these two sets of test cases, we simulate a realistic coding setting where one can refer to (i) as the public test cases for debugging or other accuracy-improving purposes, $T_{public}$, and (ii) as private test cases to conduct final execution-based evaluations on the programs, $T_{private}$."}, {"title": "3.3 Task Formulation and Evaluation", "content": "We propose two formulations for the program optimization task, namely NL-instructed generation and history-based program editing. In this section, we introduce the data we use for each formulation, and our evaluations of program correctness, runtime, and memory usage."}, {"title": "3.3.1 History-Based Program Editing", "content": "Our first paradigm follows previous work on program optimization (Shypula et al., 2024), where we facilitate a history-based editing paradigm. Concretely, we give a previous, presumably slow, version of the solution program, $P_{in}$. We then prompt LMs to edit the code to generate a more efficient version $P_{out}$, denoted as $CodeLM(p_{in}) \\rightarrow P_{out}$, where $p_{out}$ is expected to run faster than $p_{in}$."}, {"title": "Evaluating Speedup and Memory Reduction", "content": "Using the (slow, fast) program pairs remaining after post-processing in \u00a73.2, we evaluate the relative speedup and memory reduction of the model-generated program against the input program on private test cases $T_{private}$.\nWe adopt the speedup metric introduced by Shypula et al. (2024), which is formulated as:\n$Speedup = \\frac{Runtime \\ of \\ P_{in}}{Runtime \\ of \\ P_{out}}$\nSimilarly, to evaluate improvement in memory usage, we introduce a memory reduction metric as:\n$Memory \\ Reduction = \\frac{Memory \\ of \\ P_{in}}{Memory \\ of \\ P_{out}}$"}, {"title": "3.3.2 NL-Instructed Generation", "content": "In addition, we support the most common NL-to-code generation setup: given the NL description $d$ of a problem, we ask the LM to generate the program solution $p$, as $CodeLM(d) \\rightarrow p$. Our goal is for the code LM to generate an efficient and correct solution $p$. We execute $p$ on the private test cases $T_{private}$ to evaluate its performance."}, {"title": "3.3.3 Evaluating Functional Correctness", "content": "To measure if program correctness is preserved, a key metric is the functional correctness of model-generated programs. We adopt the pass@1 metric introduced by Chen et al. (2021), which samples one program from the model and measures whether the generated program passes all test cases."}, {"title": "3.4 ECCO Feature Analysis", "content": "After filtering the problem description dataset, we split the dataset into train, test, and validation sets for experiments. We ensure that no problem IDs overlap across these splits, to avoid data contamination. As shown by the detailed statistics of ECCO in Table 1, ECCO contains 1.3k problems and over 50k program pairs for code optimization evaluation."}, {"title": "4 Efficiency-Improving Approaches", "content": "We explore various top-performing code generation approaches to improve program efficiency, while maintaining functional correctness, including in-context learning (\u00a74.1), iterative refinement (\u00a74.2), and fine-tuning (\u00a74.3)."}, {"title": "4.1 In-Context Learning", "content": "We explore two mainstream prompting strategies: instruction prompting and few-shot learning.\nMany LMs perform better when incorporating instructions (Ouyang et al., 2022; Wei et al., 2022). We use two prompts: $I_{gen}$ for NL-based generation which instructs models to generate correct and efficient programs; and $I_{eff}$ for history-based editing which instructs models to optimize the input program. $I_{eff}$ is adapted from PIE (Shypula et al., 2024) and Self-Refine (Madaan et al., 2024). See \u00a7C for details.\nWe add few-shot example demonstrations (Brown et al., 2020): for the NL-based setting, using (NL, fastest program) pairs; for history-based editing, using (slow program, fast program) pairs. We randomly sample examples from the train set as the few-shot examples."}, {"title": "4.2 Iterative Refinement", "content": "We explore three methods to iteratively improve and refine the generated code to be more efficient, which intuitively aligns with the way that humans continuously refine code (Madaan et al., 2024).\nWe adopt self-refine (Madaan et al., 2024) that prompts LMs to iteratively examine the output and refine it. More concretely, (1) we first prompt the LM to generate a candidate solution; (2) we ask the same model to produce NL reasoning about why the code is incorrect and/or inefficient; and (3) we input the original input and the feedback from (2) to the model and ask it to generate an updated solution.\nWe propose an alternative refinement strategy that obtains deterministic execution feedback from the interpreter, by running the program over $T_{public}$. If test cases are passed, the execution result provides the runtime and memory information; otherwise, this feedback provides interpreter error logs. Both correctness and efficiency can be informed via this feedbackare.\nTo allow feedback both in the forms of NL and execution outputs, we ground the LM feedback on execution results, inspired by the Reflexion feedback paradigm (Shinn et al., 2024). Specifically, we first obtain the execution results as in exec-refine, then ask the LM to write NL feedback on the incorrect/inefficient parts in the code, and use this as additional input in the refinement turn."}, {"title": "4.3 Fine-tuning", "content": "We also explore three fine-tuning methods beyond prompting-alone approaches.\nIn this vanilla training setting, we leverage (NL, program) pairs and (slow program, fast program) pairs to train models independently for each paradigm. We format the data for both similarly to the in-context learning prompts (\u00a74.1), and finetune on a causal language modelling task on the formatted data for each of the two paradigms independently.\nBeyond fine-tuning with basic contexts, we posit that further conditioning on execution results could help. Therefore, we include execution results of PASS/FAIL status, runtime, and memory usage for each public test case for the input program.\nFor history-based editing, we further propose trajectory-conditioned fine-tuning, by adding a trajectory history of programs written by the same user for the given problem in context. We first collect all problems with at least three programs submitted by the same user, and treat the series of programs as a trajectory. From each qualified trajectory, we designate the fastest code as the target output, and sample three other intermediate programs at the 0th, 33rd, and 66th percentile steps to use as inputs. We aim to allow the model to learn from the step-by-step improvements that led to the optimal solution, capturing the problem-solving process in addition to just the inputs and targets."}, {"title": "5 Experiments", "content": "We experiment with several best-performing LMs pre-trained on code. Specifically, we evaluate CodeLlama-13B (Roziere et al., 2023), DeepSeekCoder-7B (Guo et al., 2024), CodeGemma-7B (Team et al., 2024), WizardCoder-13B-Python (Luo et al., 2023), StarCoder2-15B (Lozhkov et al., 2024). We use the instruction-tuned versions of all of these open-checkpoint models unless indicated otherwise. We also use the proprietary GPT-40 model for no-training methods."}, {"title": "5.2 Results and Analysis", "content": "As shown in Table 2, all methods either reduce pass@1 by a large margin (in editing mode) or obtain low pass@1 (generation mode). Comparing the two paradigms, history-based editing results in a substantially higher pass@1 by referring to a base correct program, compared to NL-instructed generation which lacks a base program to start from. GPT-40 obtains a much higher pass@1 than all models in both paradigms.\nWhile in-context learning can effectively speed up the input program by 7-126%, but compromises correctness, dropping it to 22.5-66.6, and uses more memory. Few-shot shows this trend more explicitly than instruct. Besides the limitations of LMs, this may be caused by the sampled few-shot demonstrations being algorithmically less relevant to the problem at hand.\nWhile DeepseekCoder and CodeGemma's pass@1 improves by 4.1% with few-shot, GPT-40 and StarCoder2's pass@1 drops by 2.1 \u2013 10.4%. Similarly for the efficiency metrics, CodeGemma and GPT-40 see an improvement whereas other models do not. GPT-40 significantly outperforms other models at pass@1 by 2.2x, however there is no clear winner for efficiency. This highlights the complex trade-offs between correctness and efficiency specifically in the NL-instructed generation task."}, {"title": "5.2.2 Iterative Refinement", "content": "Table 3 shows all results with iterative refinement methods. As a reference for the refinement approaches, we measure the LM-generated code in the first attempt at optimization without any refinement, and denote this method as pre-refine.\nWhile all methods can effectively speed up the program, methods that involve NL feedback (self-refine and nl+exec refine) achieve the highest speedup across models. exec-refine consistently yields the highest pass@1 for all models, by 3.4-38.8 points more than the other two methods. We conjecture that execution outputs are better representations to inform functional correctness than NL descriptions. Although it is easier to convey high-level optimization strategies in NL, conveying the functional correctness is harder. Overall, although the models are instructed to emphasize both correctness and efficiency, there seems to be an implicit trade-off between them. Additional analysis is in \u00a7A.\nWe observe similar patterns as the editing mode, that exec-refine best maintains functional correctness, and two other NL-involved approaches improve runtime/memory efficiency. Compared to the in-context learning results in Table 2, iterative refinement significantly improved memory% for all the models, with the best method showing an average improvement of 12.06% over the instruct method. However, the impact on runtime% shows varying results among the different models."}, {"title": "5.2.3 Fine-tuning", "content": "We perform parameter-efficient fine-tuning on CodeLLaMa-7B and DeepseekCoder-7B, the best-performing classes of models on the correctness and efficiency metrics in our prompting experiments respectively.\nAs shown in Table 4, fine-tuning is the most effective method in maintaining correctness in the editing paradigm. Especially for DeepseekCoder, compared to the highest prompting results 35.2 using few-shot examples, vanilla and execution-conditioned tuning improves by 6.9 and 7.8 points, and trajectory-conditioned tuning further gains a 34.6 point increase overall. This suggests that adding user-specific coding trajectories can help ground models into the optimization mode and substantially improve output correctness.\nIn the more complex NL-instructed generation task shown in Table 5, fine-tuning is effective in improving the efficiency for CodeLLaMa but not for DeepseekCoder, highlighting the need for more robust fine-tuning methods that can handle trade-offs and effectively maintain correctness in this setting.\nHowever, fine-tuning results in a much less efficiency improvement than prompting-based methods for both paradigms, possibly due to the limited power of parameter efficient fine-tuning."}, {"title": "6 Additional Analysis", "content": "As instruct model versions are expected to be better at in-context learning than their base counterparts, we compare base and instruct model versions in the editing paradigm. In Table 6, the base versions get much higher correctness albeit at lower efficiency, showing that base and instruct versions lie at different points on the correctness-efficiency trade-off. As we emphasize both correctness and efficiency aspects in the NL instruction, we conjecture the instruct models take more hints from the input format and emphasize efficiency, while base models primarily emphasize correctness.\nTo study whether models can recover from incorrect starting solutions in the history-based editing paradigm, we evaluate on a collection of 157 pairs of programs, ECCO-FIX, where the input code is almost correct: one that passed all public test cases but fails a few private test cases. As shown by Table 7, we show that exec-refine can fix incorrect programs to pass all public and private test cases, with access to only the PASS/FAIL status of public test cases. In comparison, self-refine breaks the correctness of more programs. Aligning with our findings in earlier sections and \u00a7A, exec-refine, with execution information in contexts, can encourage models to generate functionally correct programs.\nMultiple refinement iterations may improve results by allowing more turns for models to refine. To verify this, we evaluate the iterative refinement method using 1-4 iterations. While self-refine and exec-refine improve program speedup over iterations (Figure 5b), all methods continuously degrade the pass@1 to various extents. For memory usage (Figure 5c), exec-refine consistently reduces memory usage, yet other methods exhibit big fluctuations. In general, more iterations can speed up the program, yet further sacrifice correctness, thus has limited gains in terms of correctness-preserving optimization."}, {"title": "7 Conclusion", "content": "In this paper, we introduce the ECCO benchmark that enables two paradigms for Python program optimization, using JUDGE0, a language and platform-agnostic execution framework. We find that execution information and fine-tuning help LLMs maintain code correctness, and prompting with natural language often yields higher efficiency gains. However, we broadly reconfirm findings that no existing method can improve efficiency without sacrificing functional correctness. We hope ECCO can serve as a testbed for program optimization, and we call for more efforts in advancing correctness-preserving program optimizations."}, {"title": "A Iterative Refinement Analysis", "content": "Following Shypula et al. (2024), we also evaluate runtime and memory efficiency as a percentage of pairs where the generated code pout is faster/uses lesser memory than pin, in addition to the speedup and memory reduction metrics described in \u00a73.3.1. In Table 8, these percentage optimized metrics clearly indicate that exec-refine is the most consistent iterative refinement method as it achieves the best pass@1, % runtime optimized and % memory optimized across all models. Contrasting these results to Speedup and Memory Reduction in Table 3, we note that while natural language feedback (in self-refine and NL+Exec refine) aids in significantly improving the runtime and memory usage for a few cases (while breaking test cases for others), exec-refine improves runtime and memory in more cases albeit to a smaller degree."}, {"title": "B Implementation Details", "content": "We use a maximum length of 1024 tokens, and a temperature of t = 0.4. We provide two examples for all in-context few-shot experiments\nWe run all our experiments on a mix of A6000 and L40 GPUs. Specifically for the prompting iterative prompting and in-context learning approaches, we use 1-2 GPUs of the type and utilize the vLLM library (Kwon et al., 2023) primarily for generating programs efficiently. We perform finetuning on 4 A6000 GPUs.\nFor JUDGEO evaluation virtual hardware setup, we use an m7i.large EC2 instance which has 2 VCPU cores of the 4th Generation Intel Xeon Sapphire Rapids, with 8GB of RAM.\nWe adopt parameter-efficient fine-tuning with LoRA (Hu et al., 2022) due to resource limitations and implement this using the Hugging-Face Transformers library. We optimize the fine-tuning using Deepspeed'ZeRo stage 2 with a LORA rank of 8, alpha of 16 and a per-device batch size of 2. We use the AdamW optimizer, with a learning rate of le-3 and a warmup of 100 steps. We train the models on the history-based editing task for a single epoch and fine-tune models for 10 epochs on the NL-instructed generation task."}, {"title": "C Prompt Details", "content": "We illustrate and detail all of the prompts used for the experiments below."}]}