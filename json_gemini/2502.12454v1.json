{"title": "Benchmarking Zero-Shot Facial Emotion Annotation with Large Language Models: A Multi-Class and Multi-Frame Approach in DailyLife", "authors": ["HE ZHANG", "XINYI FU"], "abstract": "This study investigates the feasibility and performance of using large language models (LLMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-40-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy (\"Angry,\" \"Disgust,\" \"Fear,\" \"Happy\" \"Neutral,\" \"Sad,\" \"Surprise\"), the LLM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LLMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LLMs in complex multimodal environments.", "sections": [{"title": "1 Introduction", "content": "In the context of rapid advancements in artificial intelligence, technologies such as computer vision and natural language processing are being applied to a myriad of tasks to promote human well-being [8, 11, 24, 33]. These technologies hold particular significance in providing emerging interaction methods within the field of human-computer interaction [55]. They rely heavily on machine learning methods, where data annotation serves as a fundamental and indispensable step in model development [67].\nIn the development of machine learning models, data annotation serves as a foundational and indispensable step [54, 55]. Accurate annotations are crucial for training machine learning models that can effectively interpret complex data, particularly in tasks involving human emotions and behaviors. However, the annotation process is notoriously labor-intensive and costly [67], requiring annotators to spend prolonged periods meticulously labeling data [64]. This manual effort not only demands significant human resources but also introduces variability and potential biases inherent in human cognition [12, 14]. The challenge is magnified for emotion annotation tasks, where the subjective and nuanced nature of emotions complicates the labeling process. Addressing these challenges requires annotators to repeatedly review the data and engage in multiple rounds of iteration and discussion [16, 37, 61].\nTo address these challenges, various annotation methodologies have been proposed, including the utilization of crowdsourcing platforms. Crowdsourcing can accelerate the annotation process by distributing the workload across a large number of annotators, thereby reducing both time and cost [51]. Despite these advantages, crowdsourcing methods often prove insufficient when dealing with specialized environments or tasks that require nuanced understanding and expert judgment [52]. In such contexts, the reliance on human labor and expertise remains indispensable, highlighting the persistent need for more efficient and scalable annotation solutions.\nRecent advancements in artificial intelligence (AI), particularly in the realm of large language models (LLMs), have opened new avenues for automating annotation tasks [44]. LLMs, such as Generative Pre-trained Transformer (GPT), possess sophisticated natural language understanding capabilities and operate effectively in zeroshot settings, where they can perform tasks without explicit prior training on specific datasets [45]. These models have demonstrated potential in various applications, from text generation to semantic understanding, suggesting their utility in assisting or even replacing human annotators [2, 47, 62].\nFurthermore, the latest iterations of LLMs integrate visual capabilities, enabling them to comprehend and interpret graphical information in conjunction with textual data [38, 66]. This multimodal proficiency suggests that LLMs could become valuable tools for tasks that encompass both visual and linguistic components [4, 19, 28]. By leveraging their ability to understand visual inputs and operate in zero-shot settings, LLMs have the potential to streamline the annotation process while maintaining both accuracy and efficiency [20].\nBuilding on these capabilities, our study investigates the feasibility and performance of using LLMs for the automatic annotation of human emotions in everyday scenarios. Specifically, we employ the GPT-40-mini model to conduct rapid, zero-shot labeling of key frames extracted from video segments within the DailyLife subset of the publicly available FERV39k dataset [53]. Our experiments assess the model's performance across two emotion taxonomies: a seven-class taxonomy encompassing \"Angry,\u201d \u201cDisgust,\u201d \u201cFear,\" \"Happy,\u201d \u201cNeutral,\u201d \u201cSad,\" and \"Surprise,\" and a ternary taxonomy categorizing emotions as negative, neutral, or positive.\nOur results indicate that the LLM attained an average precision of approximately 50% in the seven-class taxonomy, surpassing a simple baseline. This underscores the model's ability to discern complex emotional states without task-specific training. Notably, when the classification was constrained to a simpler ternary classification,"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Emotion Annotation", "content": "Annotating human emotions has consistently been a challenging task [34], not only due to the inherent complexity of emotions [17] but also because annotators may have varying evaluation standards (or subjectivity) [6, 43]. A significant issue in emotion annotation is the annotation method. Although the most reliable annotation standard requires individuals to perform the annotations themselves, real-time self-annotation can lead to distraction and affect the expression of emotions [13]. On the other hand, retrospective annotation relies on individuals' recollections [7, 16], which may lead to bias [18] as well as high cost [21], and can cause embarrassment [1]. Another widely used annotation method involves external annotators observing and labeling human emotions [23]. By leveraging human cognition and understanding, and considering the context, external annotators provide reliable emotional labels based on various cues [50]. Although these two annotation methods can be combined [61], they may not be suitable for large-scale data processing. Regardless, emotion annotation remains a labor-intensive task. Therefore, exploring more efficient emotion annotation methods is crucial.\nCurrently, many annotation methods involve semi-automated or automated labeling conducted by models [13, 22, 36, 39, 41, 42, 65], which greatly improves annotation efficiency. However, such annotations are typically built upon prior preparations, meaning that before the annotation task begins, data with emotion annotations are still required to train the underlying annotation models [58]. Furthermore, in some specific tasks, these labels cannot be easily shared due to task constraints, but instead require the preparation of pre-trained data that suits specific scenarios [32]. This implies that the traditional challenges in annotation tasks still persist.\nAnother important issue in emotion annotation is the choice of emotion classification scheme. Considering that annotation is a time-consuming and laborious process, researchers often categorize emotions based on task requirements to reduce the difficulty of annotation and improve efficiency. Examples include categorizing emotions into positive and negative [25], emotional and neutral states [3], classifying specific emotions by their intensities [61], and using basic emotions [30, 60]. In this study, we consider the potential task requirements of these various classification standards and base our research based on the available ground truth emotion labels."}, {"title": "2.2 LLM for Annotation", "content": "The emergence and application of LLMs have introduced unprecedented opportunities in the field of data annotation. An increasing number of researchers and practitioners have recognized the vast potential of LLMs for enhancing annotation processes [29]. As researchers continue to explore and leverage the advancing capabilities of LLMs, particularly in multimodal interactions [59] and improvements in processing power [5], the range of annotation tasks has expanded significantly. These tasks now encompass various data types, including text [27], audio [15], images [10, 35], and specialized domain-specific data [46, 63].\nA recent survey has shed light on current trends and leading research in the application of LLMs for annotation tasks [45]. Within the scope of our study, which focuses on emotion annotation for image data, related work has explored various capabilities of LLMs. For instance, researchers have evaluated the ability of LLMs to predict emotions from captions generated from images-derived captions [56], perform image retrieval [57], and generate descriptive captions [40]. Notably, in early 2024, a study compared the performance of LLMs such as GPT-3.5, GPT-4, and Bard against traditional supervised models like Convolutional Neural Networks (CNNs) for emotion recognition in image data [31]. The findings revealed that deep learning models specifically trained for this task generally achieved higher accuracy than LLMs.\nHowever, despite the superior accuracy of traditional supervised models, they also present significant limitations. Nonetheless, LLMs offer the potential to achieve performance that is comparable to traditional models while reducing training and application costs. Therefore, in this study, we further optimized prompt engineering and reorganized annotation strategies to harness the capabilities and advantages of LLMs."}, {"title": "3 Method", "content": null}, {"title": "3.1 Dataset", "content": "We utilized the publicly available FERV39k dataset [53], which comprises numerous 1-2 second video clips encompassing seven distinct emotions expressed by individuals across various scenarios (\"Angry,\u201d \u201cDisgust,\u201d \u201cFear,\u201d \u201cHappy,\u201d \u201cNeutral,\u201d \u201cSad,\u201d \u201cSurprise\u201d). This dataset has been manually annotated and extensively used in research, providing a widely recognized benchmark for comparative analyses. Within this dataset, we selected the \"DailyLife\" subset, as this scenario is considered the most representative of real-life conditions, thereby enhancing the potential transferability of our work to a broader range of task scenarios. Specifically, the \u201cDailyLife\" subset includes 2,339 video clips depicting a variety of daily activities, interactions, and emotional expressions. Each clip is manually annotated with a definitive emotion label based on contextual and visible emotional cues, serving as the ground truth label. Images were then extracted at 25 frames per second, each carrying the associated emotion label."}, {"title": "3.2 Model Selection", "content": "In this study, we employed the GPT-40-mini (\"gpt-40-mini-2024-07-18\") model, a variant of the GPT-4 architecture optimized for greater efficiency and rapid inference. The selection of GPT-40-mini was driven by its ability to perform zero-shot tasks while balancing performance and cost considerations. Additionally, GPT-40-mini integrates vision capabilities , allowing it to accept image inputs and interpret graphical information."}, {"title": "3.3 Annotation Process", "content": "The annotation methods and processes are illustrated in Fig 1, with specific strategies to be detailed in the subsequent sections.\n3.3.1 Zero-Shot Labeling. The annotation process was conducted using a zero-shot approach, wherein the GPT-40-mini model was prompted with simple, predefined instructions to label the extracted key frames. No additional training or fine-tuning was performed on the model for the specific emotion annotation task. The prompts were designed to instruct the model to identify and classify the dominant emotion depicted in each frame based on visual and contextual cues.\nTo maximize cost efficiency, we utilized five specific frames from each video segment for annotation: the initial frame, the frame at the first quartile (Q1) position, the middle frame, the frame at the third quartile (Q3) position, and the last frame. This selection process reduces annotation counts while still capturing key emotional transitions within each segment. Subsequently, we applied different weighting strategies in the annotation process to derive comprehensive labels for the entire video segments based on these five selected frames. This approach balances the need for accurate emotion recognition with the practical constraints of annotation costs, ensuring that our methodology remains both effective and scalable.\n3.3.2 Prompt Engineering. In our study, we implemented prompt engineering to effectively utilize the GPT-40-mini model for emotion annotation in images. The prompt was meticulously crafted to guide the model's responses by defining clear roles and providing specific instructions [26]. Initially, a prompt was set to establish the model as a \u201cprofessional image emotion analysis assistant\u201d explicitly listing the available emotion options derived from the predefined EMOTION_LABELS. This foundational setup ensures that the model operates within the desired context and understands the classification framework. For each image (or multi-frame integrated image) to be analyzed, we constructed a user message that includes both textual instructions and the image itself, e.g., \"This is an independent image frame, please analyze the emotion. Please analyze the emotion of the following image and select the most matching one from the above options, returning only the emotion name.\"\nSubsequently, the user message was structured to include both textual and visual inputs. The textual component began with a customized prompt instructing the model to analyze the emotion conveyed in the image and select the most appropriate emotion from the provided options. This was followed by embedding the image itself, encoded in base64 format, within the message. By integrating the image (linked to local address) in this manner, we facilitated a multimodal interaction, allowing the model to process and interpret visual data alongside textual instructions."}, {"title": "3.4 Annotation Strategies", "content": null}, {"title": "3.4.1 Annotation Strategy A1 (Seven-Class Taxonomy)", "content": "Strategy A1 involves individually annotating each of the five selected frames within a video segment using the seven-class emotion taxonomy. Specifically, the frames chosen for annotation include the initial frame, the first quartile (Q1) position frame, the middle frame, the third quartile (Q3) position frame, and the final frame of the segment. Each frame is independently labeled with one of the seven emotion categories: \"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad,\" and \"Surprise.\"\nAfter annotation, the accuracy is directly calculated by comparing each frame's predicted emotion label against the ground truth labels provided in the dataset."}, {"title": "3.4.2 Annotation Strategy B1 (Seven-Class Taxonomy)", "content": "Strategy B1 builds upon Strategy A1 by aggregating the emotion labels from the five annotated frames to determine the predominant emotion for the entire video segment. After individually annotating all five frames, the strategy identifies the absolute majority emotion among the labeled frames. In cases where there is a tie in the distribution of different emotions, the emotion label of the middle frame is selected to represent the video segment's overall emotional state."}, {"title": "3.4.3 Annotation Strategy C1 (Seven-Class Taxonomy)", "content": "Strategy C1 is determining the predominant emotion by excluding the \"Neutral\" category. Specifically, if one emotion constitutes an absolute majority among the annotated frames after removing \"Neutral,\" that emotion is assigned to the video segment. However, if all five frames are labeled as \u201cNeutral,\u201d the segment is assigned the \"Neutral\" label. In cases where there is an equal distribution of different emotions, the emotion label of the middle frame is selected to represent the overall emotion of the video segment. This approach aims to enhance annotation accuracy by focusing on more distinctly positive or negative emotional states, thereby mitigating the ambiguous property of LLM in classifying the intermediate emotion of \"neutral\"."}, {"title": "3.4.4 Annotation Strategy D1 (Seven-Class Taxonomy)", "content": "Strategy D1 employs a multi-frame integration approach by concatenating the five selected frames into a single composite input. Specifically, the initial frame, Q1 position frame, middle frame, Q3 position frame, and final frame are sequentially joined to form a unified image input. This consolidated input is then presented to the GPT-40-mini model for annotation in a single step.\nBy integrating multiple frames, this strategy leverages temporal context, allowing the model to consider the emotional progression within the video segment. This holistic view aims to improve annotation accuracy by providing a broader context for emotion classification, potentially capturing transitional emotional states that individual frame annotations might miss."}, {"title": "3.4.5 Annotation Strategy A2 (Three-Class Taxonomy)", "content": "Strategy A2 adapts the results from Strategy A1 to the three-class emotion taxonomy. In this strategy, each of the five annotated frames from Strategy A1 is directly mapped to one of three broader categories: \"Positive,\u201d \u201cNeutral,\" or \"Negative.\" Specifically, emotions categorized as \"Angry,\" \"Disgust,\" \"Fear,\" and \"Sad\u201d are classified as \"Negative,\" while \"Happy", "Surprise": "re classified as \u201cPositive.\u201d The \"Neutral\" labels are still \"Neutral\".\nEach frame's seven-class label is converted to its corresponding three-class label based on this mapping. The accuracy is then calculated by comparing these three-class labels against the ground truth labels, allowing for an evaluation of the model's performance in a simplified emotion classification scenario."}, {"title": "3.4.6 Annotation Strategy B2 (Three-Class Taxonomy)", "content": "Strategy B2 first applies Strategy A2 to reorganize seven-class labels into three classes. It then employs a strategy similar to Strategy B1, which returns the sentiment label with an absolute majority or uses the sentiment label of the middle frame if the sentiment trend scores are tied."}, {"title": "3.4.7 Annotation Strategy C2 (Three-Class Taxonomy)", "content": "Strategy C2 involves first applying Strategy A2 to reorganize seven-class labels into three classes, followed by a strategy similar to Strategy C1 to mitigate the ambiguous property of LLM in classifying the intermediate emotion of \"neutral\"."}, {"title": "3.4.8 Annotation Strategy D2 (Three-Class Taxonomy)", "content": "Strategy D2 is similar to the multi-frame ensemble approach of Strategy D1, but uses a three-class classification approach. In this strategy, the five selected frames are concatenated into a single composite input, similar to Strategy D1. This integrated input is then processed by the GPT-40-mini model to assign a single three-class emotion label (\"Positive,\u201d \u201cNeutral,\" or \"Negative\") to the entire video segment."}, {"title": "4 Results", "content": null}, {"title": "4.1 Evaluation Metrics", "content": "We assessed our annotation strategies using precision, recall, F1-score, support, and accuracy. Precision measures the proportion of correct predictions for each emotion, while recall evaluates the ability to identify all relevant instances. The F1-score balances precision and recall, making it useful for uneven class distributions. Accuracy reflects the overall correctness of the model. Additionally, we report macro average and weighted average to provide insights into performance across all classes, with macro average treating each class equally and weighted average accounting for class imbalance by weighting metrics based on class support. Support refers to the number of true instances for each emotion category in the dataset, providing context for the other metrics by indicating the distribution of classes."}, {"title": "4.2 Seven-Class Taxonomy", "content": "Table 1 presents the performance metrics for four annotation strategies (A1, B1, C1, and D1) under the seven-class taxonomy.\nStrategy A1 (Individual Frame Annotation) attained an overall accuracy of 38%. The model exhibited robust precision for the \"Happy\" category (0.84) but encountered significant challenges in accurately classifying \u201cDisgust\u201d (precision: 0.04). The recall metric was notably high for \"Sad\" (0.65) and considerably low for \"Disgust\" (0.18), highlighting the model's difficulty in reliably identifying certain emotional states.\nStrategy B1 (Majority Voting) yielded an incremental improvement in accuracy, reaching 41%. Precision for \"Happy rose to 0.89, while \"Disgust\u201d experienced marginal enhancements in both precision (0.07) and recall (0.23). This suggests that aggregating frame-level annotations through majority voting can slightly bolster performance for specific emotions.\nStrategy C1 (Majority Voting Excluding \"Neutral\") further augmented the accuracy to 46%. By excluding the \"Neutral\" category from the majority voting process, this approach improved recall for \"Sad\" to 0.76 and maintained high precision for \"Happy\" (0.85). This indicates that focusing on \u201cNegative\u201d and \u201cPositive\u201d emotions can mitigate some inaccuracies associated with the \"Neutral\" classifications, thereby enhancing overall annotation reliability.\nStrategy D1 (Multi-Frame Integration) achieved an accuracy of 46%, paralleling Strategy C1. By amalgamating multiple frames into a single input, this strategy effectively harnessed temporal context, thereby improving the model's capacity to capture the dynamic progression of emotions across video segments. This integration allows the model to consider the emotional transitions and consistencies present within the selected frames, leading to more coherent and accurate segment-level annotations.\nAdditionally, when considering the macro average and weighted average metrics, Strategies C1 and D1 not only achieved higher accuracy but also demonstrated improved balanced performance across all classes. The macro average indicates that these strategies perform more consistently across less frequent emotion categories, while the weighted average reflects their enhanced overall performance, accounting for class imbalances in the dataset.\nOverall, Strategies C1 and D1 demonstrate superior performance and cost-effectiveness in the demanding seven-class taxonomy tasks. Notably, Strategy D1 further reduces costs by minimizing the number of API requests and decreasing token lengths through preprocessing. This indicates that aggregation techniques and the integration of temporal context offer enhanced performance advantages in LLM's zero-shot annotation tasks."}, {"title": "4.3 Three-Class Taxonomy", "content": "Table 2 presents the performance metrics for four annotation strategies within the three-class taxonomy framework.\nStrategy A2 (Mapped Three-Class Classification) achieved an accuracy of 57%. The \u201cPositive\" category exhibited strong precision (0.72), whereas the \"Neutral\" category demonstrated moderate performance with a precision of 0.27 and recall of 0.41.\nStrategy B2 (Majority Voting) resulted in a substantial accuracy improvement, attaining 65%. Precision for \"Positive\u201d increased to 0.79, while the \"Negative\" category demonstrated robust performance with a precision of 0.70 and recall of 0.74.\nStrategy C2 (Majority Voting Excluding \u201cNeutral\") also achieved an accuracy of 65%. This strategy maintained high precision for \"Negative\" (0.67) and significantly improved recall for \"Negative\" to 0.87, while the \"Positive\" category maintained consistent performance with a precision of 0.76 and recall of 0.58.\nStrategy D2 (Multi-Frame Integration) matched the accuracy of 65%, effectively leveraging both temporal context and simplified emotion categories to ensure efficient and accurate annotation.\nOverall, Strategies B2, C2, and D2 consistently outperformed Strategy A2, further highlighting the effectiveness of aggregation and integration methods in enhancing annotation accuracy within zero-shot classification approaches based on LLMs.\nFurthermore, the macro average and weighted average metrics underscore the balanced performance of Strategies B2, C2, and D2 across all emotion categories. The macro average indicates that these strategies maintain consistent precision and recall across both common and rare classes, while the weighted average reflects their strong overall performance by accounting for the distribution of classes in the dataset."}, {"title": "4.4 Performance of Different Strategies: Insights from Confusion Matrices", "content": "The presented confusion matrices (in Fig. 3) compare the performance of various classification strategies (A1, A2, B1, B2, C1, C2, D1, D2) across different tasks involving emotion and sentiment recognition. Each matrix visualizes the true labels versus the predicted labels, providing insights into the model's accuracy, strengths, and areas requiring improvement. Strategies A1, B1, C1, and D1 are evaluated on their ability to classify seven distinct emotional states, including \u201cAngry,\u201d \u201cHappy\u201d \u201cNeutral,\u201d and \u201cSad.\u201d The diagonal entries reflect correct classifications, while off-diagonal values indicate confusion between emotions. For example, significant misclassifications are observed between \"Neutral\" and \"Happy\" in some strategies, highlighting challenges in distinguishing subtle emotional variations. Strategies A2, B2, C2, and D2 focus on sentiment classification into three categories: \"Negative,\"", "Neutral,": "nd", "Positive.": "hile these strategies generally achieve high accuracy for the \"Negative\" category, frequent confusion between \"Neutral\" and \"Positive\" suggests a need for improved sensitivity to nuanced sentiment expressions."}, {"title": "4.5 Comparative Analysis", "content": "Comparing the seven-class and three-class taxonomies, it is evident that simplifying emotion classification enhances overall accuracy. The three-class strategies (B2, C2, D2) achieved an accuracy of 65%, significantly higher than the best seven-class strategies (C1 and D1) at 46%. This improvement is attributed to the reduced complexity in classification, allowing the model to more effectively distinguish between \"Negative,\"", "Neutral,": "nd", "Positive": "motions.\nFurthermore, aggregation methods - whether through majority voting (B1, B2, C1, C2) or multi-frame integration (D1, D2) - consistently yielded better performance compared to individual frame annotation (A1, A2). These findings highlight the importance of leveraging temporal context and strategic frame selection to enhance the reliability and accuracy of automated emotion annotation.\nOverall, the results demonstrate that the GPT-40-mini model is capable of effectively annotating human emotions, particularly when employing strategies that aggregate information from multiple frames and simplify emotion categories. These approaches offer a balanced trade-off between annotation accuracy and computational efficiency, making them suitable for large-scale, real-world applications.\n4.5.1 Baseline Comparison (Random Guessing). To contextualize the performance of our annotation strategies, we compared them against baseline accuracy levels derived from random guessing. In the seven-class taxonomy, random guessing would yield an expected accuracy of approximately 14.3%, while in the three-class taxonomy, the expected accuracy is around 33.3%. Our results demonstrate that all proposed strategies significantly surpass these baseline levels. Specifically, in the seven-class taxonomy, the best-performing strategies (C1 and D1) achieved an accuracy of 46%, more than three times the baseline. Similarly, in the three-class taxonomy, Strategies B2, C2, and D2 reached an accuracy of 65%, nearly doubling the random guessing baseline. This substantial improvement underscores the effectiveness of our aggregation and integration methods in enhancing annotation accuracy within zero-shot classification tasks using large language models.\n4.5.2 Baseline Comparison (Trained Models). To further contextualize the performance of our annotation strategies, we compared our results against baseline models reported in the FERV39k dataset paper [53], with a particular focus on the DailyLife subset under the seven-class taxonomy. The baseline models encompass various architectures, including ResNet-18 (R18), ResNet-50 (R50), VGG-13 (VGG13), VGG-16 (VGG16), and their LSTM-enhanced variants. The performance metrics reported are WAR (Weighted Average Recall) and UAR (Unweighted/Macro Average Recall), which provide a balanced evaluation by accounting for class imbalances and ensuring that each class contributes proportionally to the overall performance.\nIn the DailyLife category, baseline models achieved the following WAR/UAR scores as shown in Table 3.\nOur best-performing strategy within the seven-class taxonomy, Strategy D1 (Multi-Frame Integration), achieved a WAR of 46%, closely approaching the performance of the VGG13-LSTM (46.07% WAR) and Two VGG13-LSTM (46.92% WAR) models-both of which represent the top-performing baseline methods. Moreover, Strategy D1 significantly surpasses the average WAR of the baseline models, which stands at approximately 38.98%.\nIn terms of UAR, Strategy D1 outperforms all baseline models, achieving the highest recall across all classes without being influenced by class imbalance. This indicates that our strategy not only excels in overall weighted performance but also ensures equitable recognition of all emotion categories, including those that are less frequent in dataset."}, {"title": "5 Cost-Efficiency and Scalability", "content": "Besides performing close to or outperforming baseline models in performance, compared to traditional supervised models, GPT-40-mini-based annotation strategies have significant advantages in cost-effectiveness and scalability. Strategy D1 reduces operational costs by minimizing the number of API requests and decreasing token lengths through preprocessing. This cost-effective approach ensures that large-scale annotation tasks remain financially feasible. Furthermore, our zero-shot annotation approach leverages the capabilities of LLMs without necessitating task-specific training, allowing for rapid deployment and adaptation to various annotation tasks with minimal additional resources.\nWhile some baseline models, such as Two VGG13-LSTM, exhibit marginally higher WAR scores, our annotation strategy D1 achieves comparable performance levels with enhanced cost and operational efficiencies. This underscores the effectiveness of aggregation techniques and temporal context integration in zero-shot annotation tasks using LLMs, presenting a viable alternative to traditional supervised models, especially in scenarios constrained by budget and computational resources."}, {"title": "6 Model Cost Considerations", "content": "In the initial phase of our study, we tested full-frame rate annotation, labeling all 25 images for each second. However, given the task volume, this approach was financially unsustainable, with API costs reaching approximately $100 for annotating ~11,000 images among this dataset. To mitigate costs, we adopted a strategy of selecting five key frames (the first, Q1, middle, Q3, and final frames) from each video segment. Additionally, we merged these five frames into a single input, significantly reducing token usage. Although we did not perform frame-by-frame annotation at full frame rates, our results remain valuable for tasks that are not highly sensitive to high frame rates."}, {"title": "7 Ethical Considerations", "content": "LLMs, as a \"technological revolution,\" have brought unprecedented opportunities, driving paradigm shifts in tasks including, but not limited to, those outlined in this study. While our research explores the technical potential of LLMs in emotion annotation tasks, it is essential not to overlook the associated ethical considerations. We hope to take this opportunity to remind researchers employing this method to use it responsibly and thoughtfully, particularly under the concept of superalignment."}, {"title": "8 Conclusion with Future Work", "content": "This study demonstrates the feasibility of using LLMs for automated emotion annotation in facial images through zero-shot classification. By exploring various annotation strategies, we identified the potential of LLMs to achieve competitive performance, particularly in tasks involving ternary classification of emotions. Strategies that integrate multiple frames or aggregate annotations through majority voting significantly enhance the reliability of emotion recognition, offering a promising alternative to traditional supervised methods.\nWhile LLMs exhibit inherent ambiguity in distinguishing closely related emotion categories, particularly within the seven-class taxonomy, they achieve higher accuracy in simpler classification tasks. This highlights their utility in scenarios where efficiency and scalability are prioritized over fine-grained classification precision. Moreover, the cost-effective nature of zero-shot LLM annotation enables large-scale deployment, reducing the reliance on human annotators and minimizing operational costs.\nOur findings underscore the importance of leveraging aggregation techniques, temporal context, and task simplification to maximize the potential of LLMs in emotion annotation. Future work should focus on fine-tuning multimodal LLMs for emotion recognition, addressing ambiguities in classification, and expanding their application in real-world multimodal environments, such as driver attention detection, live streaming platform moderation, and health management systems. This research provides a foundation for advancing automated annotation techniques, fostering innovation in human-computer interaction and affective computing domains."}]}