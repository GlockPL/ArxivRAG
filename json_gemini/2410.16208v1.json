{"title": "COMPUTE-CONSTRAINED DATA SELECTION", "authors": ["Junjie Oscar Yin", "Alexander M. Rush"], "abstract": "Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and training are budgeted for. We first formalize the problem of data selection with a cost-aware utility function, and model the data selection problem as trading off initial-selection cost for training gain. We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling finetuning tokens, model sizes, and data selection compute. These experiments show the validity of this model in real-world experiments. Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective.", "sections": [{"title": "1 INTRODUCTION", "content": "The growth of large language models (LLMs) has motivated research into their resource profiles. The compute cost of training LLMs is substantial, and, in practice, the total compute budget is predetermined: the number of accelerators and their usage hours are allocated in advanced. Thus, it is critical to determine the optimal allocation of resources under a budget constraint. Past work on compute-optimal LLMs (Hoffmann et al., 2022) studies if one could attain a better perplexity for a given pretraining compute budget by balancing architecture and training decisions.\nSimilar resource questions exist during post-training finetuning of LLMs. We assume a common setting where a single-base LLM needs to be finetuned for a downstream task. Numerous works have sought to induce certain abilities by training from large instruction tuning datasets (Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2021). There are several different resource constraints which might make this challenging. For example work on parameter efficient fine-tuning, targets improving the memory-usage of this stage (Hu et al., 2021). In this work we focus instead on compute-constrained finetuning.\nA promising approach to reducing compute requirements for finetuning is data selection. Data selection is a foundational approach in machine learning where the objective is to create an minimal dataset from a collection of data (John, 1975). Given the large computational cost of each gradient step, reducing dataset size is an appealing way to reduce this resource usage. Moreover, as larger and more diverse instruction tuning collections become available, likely only a subset of the data provides the most value for any given task. Recent work has shown that careful data selection can vastly increase the effectiveness of finetuning per step (Chen et al., 2023; Zhou et al., 2024).\nYet, even if data selection is effective, it does not a priori imply that it is compute-optimal. Given the base-level compute effectiveness of gradient descent on LLM models, data selection methods need to improve upon standard training in proportion to their added cost. In other words, a compute-optimal method should both improve training and be cheap to compute. In this work, we study this setting of compute-constrained data selection, and argue that this is a critical factor for practical adoption that is being under considered in method development.\nConcretely, we aim to quantify the trade off between model size, number of tokens, and data selection in LLM finetuning, such that practitioners can make well-informed decisions when choosing how to best allocate compute. We first formalize this problem as a compute-constrained combinatorial optimization problem and then discuss our compute-aware modification. We develop an"}, {"title": "2 RELATED WORK", "content": "Compute Scaling for Model Size and Transfer Learning. Kaplan et al. (2020) established the use of compute-scaling laws for language models and showed a power-law relationship between model size and loss over varying orders of magnitude. More recent works expand on the original formulations by considering learning rate schedule matching, multiple-epoch training, and hyperparameters (Hoffmann et al., 2022; Muennighoff et al., 2024; Bi et al., 2024).\nHernandez et al. (2021) study scaling in the post-training setting but only model the relationship between pretraining and finetuning data loss. Similar work (Lin et al., 2024; Isik et al., 2024; Zhang et al., 2024) studies the scaling in post-training by modeling the relation between pretraining data size, model size, finetuning method and downstream test loss. These models do not consider the use of data selection. Recent work considers the relationship between data selection and scaling in the vision domain (Goyal et al., 2024), but this work does not consider the compute needed for data selection in their scaling analysis.\nData Selection for Language Models. Data selection takes the full training data as input and chooses a subset to the train (Albalak et al., 2024). It can be viewed as a coreset selection problem (Mirzasoleiman et al., 2020; Killamsetty et al., 2021b), which aims to select a subset from the given training dataset such that the model achieves performance similar to the full dataset.\nThere are many different approaches to LLM data selection. The simplest are non-model specific approaches such as manual scoring functions (Chen et al., 2024), surface level features (Robertson et al., 2009), and n-gram features (Xie et al., 2023). On the other hand, more effective methods use LLMs to assign utility. One class uses model forward inference information such as utility scores from generations, model embeddings, and perplexity (Wettig et al., 2024; Marion et al., 2023; Ivison et al., 2022). Another class uses model gradients to define influence function style selections (Killamsetty et al., 2021a; Han, 2023; Xia et al., 2024). See Section 4 for further description.\nTask-Specific Finetuning from General-Purpose Instruction Datasets. While data selection can be applied in any finetuning setting, it is most impactful as a method to train a targeted model from a general-purpose dataset. In the case of LLMs, this setting is commonly training a task-specific model from an \u201cinstruction-tuning\" dataset. Several general-purpose instruction tuning datasets exist for LLM post-training (Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2021; Chung et al., 2024). Instruction-tuned models can handle a variety of possible inputs for downstream use cases as either classification or generative model (Wang et al., 2023b;a).\nInstruction-tuned models are the most effective model for many downstream tasks; however they require training on a very large and expensive set of data. To get around this issue, models have demonstrated strong results using small subsets of instruction tuning data (Chen et al., 2023; Lu et al., 2023; Zhou et al., 2024). As the size of dataset increases, automated measures of quality selection has become a growing focus, particularly when many targeted models are needed. Therefore while instruction-tuning is not the direct focus of this work, it provide a real-world applications of compute-constrained data selection."}, {"title": "3 BACKGROUND", "content": "The goal of data selection is to choose a subset of data points from a large dataset to optimize model performance on a target task. In a learning task, we are given a large training set, D, a target test dataset, T, and a validation set, V. Our goal is to find the optimal subset S \u2264 X such that the model \u03b8 = T(S) trained on S minimizes the loss on T under a given data constraint:\n$S^* = \\arg \\max_{S \\subseteq D} P(T;T(S))$\nsubject to $|S| \\leq K,$\nwhere P denotes the performance of the model on the test set and K is the max cardinality.\nThis problem is challenging to solve in the general case, particularly without access to the test set T. Approaches to the problem therefore commonly make two implicit assumptions: (1) the performance function, P(T;T(S)), is monotonic and submodular, non-increasing marginal utility, in the dataset chosen (Kirchhoff & Bilmes, 2014), and (2) the validation set V correlates with the test set T. Under these assumptions, we can argue for a greedy data selection approach (Kirchhoff & Bilmes, 2014). This allows decomposing the total objective, P(T;T(S)), by considering the contribution of individual training points to the performance on the validation set $P(V; T(\\{x\\}))$ for each $x \\in S$.\nTo estimate the marginal contribution of $x \\in S$, most data selection methods use a utility function $v(x; V)$ as a proxy to $P(V;T(\\{x\\}))$\u2014to give the utility of each data point x based on its relevance (Albalak et al., 2024). By ranking the data points D based on v and selecting those that minimize the total utility within the data budget K, greedy data selection aims to obtain a high-performing subset S*. To summarize, we consider data selection methods that target Equation (1) with a two-step greedy algorithm: score all points and then select points up to the budget K."}, {"title": "4 COMPUTE-CONSTRAINED DATA SELECTION", "content": "While the framework presented in Section 3 provides a general method for data selection, we argue that it is insufficient for the practical challenge of finetuning LLMs. The issue is that LLM fine-tuning is often bottlenecked by a computational budget and not a data budget. There are two major computational bottlenecks in this process: (1) the cost of training the model on this data $(C_T)$, and (2) the cost of computing the utility function on this data $(C_U)$. The true cost of $C_U$ can reduce significantly the amount of training points we can select for given computational budget .\nAssuming we at minimal require the computation of a utility function over the dataset, we can define the compute-constrained data selection objective as,\n$S^* = \\arg \\max_{S \\subseteq D} P(T;T(S))$\nsubject to $C_T(S) + \\sum_{x \\in D} v(x) \\leq K.$\nHere K is now the compute, e.g. maximum number of FLOPs, allocated for data selection and training, and we assume calculation of v is a fixed-cost independent of optimization."}, {"title": "4.1 COMPUTE COST OF DATA SELECTION UTILITIES", "content": "To make these costs more tangible, we consider four classes of data selection in this work, that represent three different levels of compute. This section summarizes their main properties, i.e. their core utility functions and computational costs. Section 4.1 describes the important computational costs from this section.\nLexicon-Based. Lexicon data selection methods utilize statistical properties of text to evaluate the relevance of data points without relying on deep learning models. One of the most effective lexicon-based methods is BM25 (Robertson et al., 2009; Silva & Barbosa, 2024), which scores data points"}, {"title": "5 MODELING THE COMPUTE-PERFORMANCE RELATIONSHIP", "content": "To analyze the trade-off between the compute of data selection methods and the expected gain in model performance, we define a simplified parametric form for expected peformance. Let c be the fixed-cost of training on a single data point and k = |S| be the number of data points. Define C(k) as the total cost of training and selection,\n$C(k) = c \\times k + \\sum v(x).$"}, {"title": "6 EXPERIMENTAL SETUP", "content": "As shown in Table 2, experiments vary the number of finetuning tokens for 5 data-selection methods and a fixed family of models, ranging from 7B to 70B parameters. The finetuning data budget is fixed as a percentage of the total finetuning tokens: {2.5, 5, 10, 25, 50, 100}%, across 3 target tasks. For each finetuning budget, we conduct multiple training runs with increasing compute, which is either allocated toward larger pre-trained model sizes or more sophisticated data selection methods."}, {"title": "7 RESULTS", "content": "Empirical Results. Figure 2 shows the full results with 5 data selection methods across 3 pre-trained model size and 2 of the target task. For these experiments, the unique training data budget is fixed at roughly {2.5%, 5%, 10%, 25%, 50%, 100%} of tokens. For each data budget, we finetune a set of models with increasing amount of compute that is allocated to either more parameters or more expensive data selection methods.\nNote that for PPL and Gradient, a 7B model of the same class is always used for data selection; whereas for Embed a small encoder model is used. For MMLU, PPL is implemented as Mid-PPL, where points in the middle of perplexity score distribution is being ranked first. For BBH, PPL is implemented as Top-PPL, where points in the top of perplexity score distribution is ranked first.\nThe main 7B results in Figure 2 (A, D) and Figure 5 (b), show that cheap lexicon-based methods (BM25) and embedding-based methods (Embed) significantly outperform perplexity-based (PPL) and gradient-based methods (LESS). While PPL and LESS achieve better performance at the same data budget compared to these methods (see Figure 5), they are not compute optimal under the same compute budget due to the high FLOPs required for data selection. The marginal benefit one can get"}, {"title": "8 ANALYSIS", "content": "Comparing Training versus Total Compute Budgeting. While our primary interest is in the full compute constrained setting, we note that different results hold if targeting only a small training budget as in Equation (1). When the training-budget is fixed, we observe in Figure 5 (a) that"}, {"title": "9 CONCLUSION", "content": "Data selection is a valuable tool for fine-tuning language models, but its primary benefit \u2014 saving compute \u2014 must be balanced against the compute costs required to identify an optimal dataset. In practice, popular methods are surprisingly compute intensive, yielding training data reduction at the cost of worse compute-constrained performance. We hope that these results motivate further research into more compute-efficient data selection methods. As demonstrated by our 70B finetuning experiments, sophisticated data selection methods can leverage smaller models for data selection, allowing larger models from the same family to be trained more efficiently in the LLM setting."}, {"title": "A FLOPS COMPUTATION", "content": "To compute the training FLOPs of a transformer model, we assume the backward pass has twice the FLOPs of the forward pass and follow the FLOP counts of various components of a transformer model for a single forward pass as in Kaplan et al. (2020) as in Table 3. We account for all training FLOPs in our analysis, including those from the embedding matrices. For large models, the contribution of FLOPs and parameters from embedding matrices is minimal. We apply a factor of 2 to represent the multiply-accumulate cost (MAC)."}, {"title": "B DATA-SELECTION FLOPS", "content": "In this section, we detailed our estimation of the costs to perform different data selection methods."}, {"title": "C FITTING PARAMETRIC FUNCTION", "content": "In this appendix, we describe the process for fitting a parametric model that captures the relationship between the number of data points k and performance P(k), as a function of computational cost. The model captures diminishing returns, dependence on computational cost, and convergence toward an upper bound.\nWe model the expected performance P(k) after training on k data points as follows:\n$P(k) = (P \u2013 P_0) \\times (1 \u2013 exp(-\\lambda \\frac{C(k)}{C(|D|)})) + P_0$\nwhere:\n\u2022 $P_0$ is the zero-shot performance (i.e., performance without additional training).\n\u2022 $P$ is the upper bound on performance (i.e., the maximum achievable performance).\n\u2022 $\\lambda$ is a parameter controlling how efficiently the method extracts value from additional compute.\n\u2022 C(k) is the computational cost of selecting and training on k data points.\n\u2022 C(|D|) is the total computational cost of training on the entire dataset.\nThe goal is to fit the parameters $P_0, P$, and $\\lambda$ to observed data. We fit the model by minimizing the difference between the predicted performance P(k) and the observed performance $P_{obs}$(k). This is formulated as the following optimization problem:\n$\\min_{P_0, P, \\lambda} \\sum_{i=1}^{N} (P(k_i; P_0, P, \\lambda) \u2013 P_{obs, i})^2$\nwhere:\n\u2022 N is the number of data points.\n\u2022 $P_{obs, i}$ is the observed performance at the i-th data point.\n\u2022 $k_i$ is the number of data points used for training in the i-th observation.\n\u2022 $P(k_i; P_0, P, \\lambda)$ is the predicted performance using the parametric model.\nTo ensure meaningful parameter estimates, we impose the following constraints:\n\u2022 $P_0 \\geq 0$, as performance cannot be negative.\n\u2022 $P_0 \\leq P$, ensuring that performance does not exceed the upper bound P.\n\u2022 $\\lambda \\geq 0$, as $\\lambda$ represents the rate at which performance improves with compute.\nThe parameter P is set slightly above the maximum observed performance:\n$\\bar{P} = \\max_{i} P_{obs, i} + \\epsilon$\nwhere $\\epsilon = 0.05$ is a small buffer to ensure convergence to the upper bound.\nWe set the initial guesses for the parameters as follows:\n\u2022 $P_0^{(0)} = P_{obs,1}$, the observed performance at zero-shot (i.e., without training).\n\u2022 $\\bar{P}^{(0)} = \\max_{i} P_{obs,i}$.\n\u2022 $\\lambda^{(0)} = 1.0$, a reasonable initial guess for the compute extraction efficiency.\nWe use the Levenberg-Marquardt algorithm to minimize the objective function. This method is effective for solving non-linear least squares problems and efficiently handles the non-linear nature of our parametric model.\nThe optimization problem is solved as follows:\n$(P_0^*, \\bar{P}^*, \\lambda^*) = arg \\min_{P_0, P, \\lambda} \\sum_{i=1}^{N} (P(k_i; P_0, P, \\lambda) \u2013 P_{obs, i})^2$\nThe optimization yields the following fitted parameters:\n$P_0^* = \\text{[fitted value]}$, $\\bar{P}^* = \\text{[fitted value]}$, $\\lambda^* = \\text{[fitted value]}$\nThese fitted parameters provide a close match between the observed data and the model and helps us understand for better decision-making in resource allocation."}, {"title": "D EXPERIMENTAL DETAILS", "content": "D.1 FINETUNING SETTINGS\nAll experiments were conducted with parameter-efficient finetuning method LoRA Hu et al. (2021). For the LoRA adapter, we specified a rank of 128, an a value of 512, and a dropout rate of 0.1 and applied it across all attention matrices. Adding the LoRA adapter introduce minimal FLOPs overhead during training-having no impact on our FLOPS analysis and mainly reduce memory requirements for more accessible training.\nWe follow standard practices in LLM finetuning Wang et al. (2023a); Ivison et al. (2023) and use the AdamW optimizer with beta-parameters $(\u03b2_1, \u03b2_2) = (0.9, 0.99)$.\nThe learning rate is set to 2e-5 for the 7B/8B/13B models and 1e-5 for the 70B models. For data budget {2.5%,5%}, we double the learning rate to ensure convergence in loss. For all experiments, we use a warmup ratio of 0.03, BFloat16 precision, and an effective batch size of 128. For 70B model training, we used QLoRA to reduce the memory requirements and speedup the training.\nFor smaller data budget (2.5%-5%) experiments, we perform five trials across five random seeds. For larger data budget (10%-100%) experiments, we perform three trials across three random seeds. We report mean target task performance in our analysis. Optimization seeds are controlled through the entirety of the experiments.\nD.2 PRETRAINED MODELS\nTable 7 lists the pre-trained models we finetuned in this work. We expect our findings to generalize to these models and future, stronger open base models.\nD.3 TRAINING DATASETS\nFor training, we use the same four processed datasets as in Wang et al. (2023a); Ivison et al. (2023), all of which are either human-authored or annotated. Details are provided in Table 8. The FLAN V2 and COT datasets are derived from existing NLP datasets, while DOLLY and OPEN ASSISTANT 1 contain open-ended generation examples with human-written responses. These datasets vary in format, sequence length, and tasks. Following Wang et al. (2023a), we standardize their format using the 'Tulu' structure.\nE RESULTS: LLAMA3\nWe plot additional results on target task MMLU using LLAMA3 8B model in Figure 7. Similar to LLAMA2, LLAMA3 8B results show that cheaper lexicon-based (BM25) and embedding-based (Embed) methods significantly outperform perplexity-based (PPL) and gradient-based (LESS) method. The marginal gains from using more sophisticated methods do not justify their selection costs."}, {"title": "I LIMITATIONS", "content": "Repeating Finetuning Data In this work, we focus on finetuning the entire dataset for only one epoch. With a larger compute budget, it is possible to repeat fractions or entire datasets multiple times. Multi-epoch settings, as explored in (Xia et al., 2024), could potentially provide further training speedup by repeating data selectively.\nSensitivity to Hyperparameters The effectiveness of finetuning can be highly sensitive to hyperparameters such as learning rate, dropout, or optimizer choice. There may be a specific learning rate that leads to quicker convergence. In this work, we fixed most hyperparameters to commonly used values for fine-tuning LLMs, leaving further exploration of hyperparameter tuning for future work.\nOther Data Selection Methods There are additional data selection methods not covered in this work that warrant investigation. While we focused on methods in terms of their compute efficiency, other approaches, such as classifier-based methods, could offer insights and deserve further exploration."}]}