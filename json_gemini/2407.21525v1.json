{"title": "Skeleton-Based Action Recognition with Spatial-Structural Graph Convolution", "authors": ["Jingyao WANG", "Issam FALIH", "Emmanuel BERGERET"], "abstract": "Human Activity Recognition (HAR) is a field of study that focuses on identifying and classifying human activities. Skeleton-based Human Activity Recognition has received much attention in recent years, where Graph Convolutional Network (GCN) based method is widely used and has achieved remarkable results. However, the representation of skeleton data and the issue of over-smoothing in GCN still need to be studied. 1). Compared to central nodes, edge nodes can only aggregate limited neighbor information, and different edge nodes of the human body are always structurally related. However, the information from edge nodes is crucial for fine-grained activity recognition. 2). The Graph Convolutional Network suffers from a significant over-smoothing issue, causing nodes to become increasingly similar as the number of network layers increases. Based on these two ideas, we propose a two-stream graph convolution method called Spatial-Structural GCN (SpSt-GCN). Spatial GCN performs information aggregation based on the topological structure of the human body, and structural GCN performs differentiation based on the similarity of edge node sequences. The spatial connection is fixed, and the human skeleton naturally maintains this topology regardless of the actions performed by humans. However, the structural connection is dynamic and depends on the type of movement the human body is performing. Based on this idea, we also propose an entirely data-driven structural connection, which greatly increases flexibility. We evaluate our method on two large-scale datasets, i.e., NTU RGB+D and NTU RGB+D 120. The proposed method achieves good results while being efficient.", "sections": [{"title": "I. INTRODUCTION", "content": "Human activity recognition (HAR) is a research field that spans computer science and electronic engineering. Its goal is to identify and classify human activities using algorithms. Human Activity Recognition (HAR) has received significant attention in recent years and has been applied in various fields such as healthcare, sports, security, smart homes, and wearable devices. The aim is to enhance human well-being, safety, and productivity by offering insights and information for decision-making, monitoring, or feedback purposes [26]. The HAR process involves several steps: data collection, data preprocessing, feature extraction, and activity recognition [26]. Various data modalities are used for different scenarios. They could be divided into two groups: visual modality and non-visual modality. RGB and skeleton data are two common types of visual modalities. RGB data is typically captured with a camera and contains a wealth of color and contour information. This data is widely used in security cameras, intelligent robots, and other applications. Skeleton data is usu-ally extracted from RGB data or collected by depth cameras. Compared to RGB data, 3D skeleton data effectively captures spatial-temporal properties. It contains less data but has higher computational efficiency [16]. At the same time, it protects pri-vacy. Skeleton data is a good choice when computing resources are limited or privacy protection is required. Acceleration data is typically non-visual data. The acceleration sensor is small, lightweight, and designed to protect privacy. It can accurately and comprehensively measure the spatial acceleration of an object without prior knowledge of its direction of motion. This sensor is commonly used in low-cost and low-power-consumption portable devices, such as smartwatches. Each of the various data modalities has its suitable application scenario. In this article, we focused on visual data, specifically on skeleton data. Currently, the predominant approach involves using spatial-temporal convolutional networks to extract features from skeleton data. Researchers utilize GCN to extract spatial fea-tures within frames, and then utilize TCN to extract temporal features between frames. The researcher designs the skeletal connection diagram based on the human skeleton's topology. When performing graph convolution, the adjacency matrix is utilized to depict the connections between nodes and perform message passing between neighboring joints [25], [28]. The issue is that human beings have symmetrical structures, and there should be not only spatial connections but also structural connections between joints. The action of \"clapping hands\" requires both hands to come together. There is a strong connection between the two hands. However, a mere spatial connection would not adequately represent the connection between the hands, and we need to depict this connection with a structural link. We believe that distinguishing fine-grained activities requires a focus on the activities of edge nodes. Taking typing on the keyboard and writing as an example [19], the most critical metric to distinguish between these two actions is the fine-grained movement of the hand. In terms of spatial structure, edge nodes are the least connected to other nodes and farthest away from the center node. This hinders their ability to effectively aggregate information from other nodes. To address this issue, we propose adding extra structural connections to the edge nodes. The spatial connection is fixed, and the human skeleton naturally maintains this topology regardless of the actions performed by humans. However, the structural connection should be dynamic and depends on the type of movement the human body is performing. There is a strong connection between the hands when clapping, but not when checking the time (from watch) [19]. Based on this idea, we propose a data-driven method to initialize the structural connections based on the similarity of edge node sequences for various samples, so that each sample has its unique structural connection. GCN aggregates node information through edge connections to gen-erate new node representations, which is effective but can lead to the problem of over-smoothing. Different from the normal GCN way of aggregating information, we propose performing node information differentiation through the structural branch. Our differentiation method can enhance the contrast between the edge nodes, and also reduce the issue of over-smoothing to some extent. The main contributions of this paper are summarized as follows:\n\u2022 We propose a spatial-structural graph convolution method which can better represent symmetrical human structures and edge nodes.\n\u2022 Instead of initializing the adjacency matrix solely based on the count of adjacent nodes, we adopt a differential approach to initialize the structural connections of various samples. This differential initialization is contingent on the similarity of edge node sequences, enhancing the flexibility of graph convolution.\n\u2022 To address the over-smoothing problem inherent in GCN, we utilize the inverse adjacency matrix to discern infor-mation pertaining to edge nodes. The remainder of this paper is organized as follows: Section 2 presents the state of the art papers related to our work. Our proposed method is highlighted in section 3. Section 4 presents the experimental part including data set description, experimental settings, evaluation metrics, experimental results and their analysis and the conclusion is given in section 5."}, {"title": "II. RELATED WORKS", "content": "HAR is a classic temporal classification problem. In earlier years, traditional CNN and RNN models were dominant. Zhang et al. [32] designed a two-stream CNN-RNN model to overcome view dependency. The main idea of this paper is to use view adaptation modeling to obtain a variable observation coordinate, allowing for the generation of skeleton data from different viewpoints. An RNN-based view adaptation subnet-work is integrated into the VA-RNN stream, while a CNN-based view adaptation subnetwork is integrated into the VA-CNN stream. Then, an LSTM classification network is applied in the VA-RNN stream, while a CNN classification network is applied in the VA-CNN stream. Caetano et al. [2] encoded the magnitude and orientation values of the skeleton joints into the image, and then utilized a CNN network for classification. In recent years, several Transformer-based and GCN-based models have appeared. Plizzari et al. [17] developed a two-stream spatial-temporal transformer network. On the S-TR stream, a Spatial Self-Attention module (SSA) is used to analyze intra-frame interactions between different body parts, followed by a 2D convolution on the time dimension (TCN). On the T-TR stream, a Temporal Self-Attention module (TSA) is used to model inter-frame correlations, while spatial features are extracted by a standard graph convolution (GCN). Skeleton data used for human activity recognition (HAR) tasks are sequential data that contain consecutive frames. Skeleton data represents natural topography. In addition to being used as a matrix, skeleton data could also be utilized as a graph by a graph neural network [18]. In addition, the skeleton data used for action recognition is sequential data. GCN and TCN blocks are often used to capture spatial-temporal features, where Graph Convolutional Network (GCN) is used to cap-ture intraframe spatial information by aggregating information from neighboring nodes, and Temporal Convolutional Network (TCN) is used to capture interframe temporal information. Yan et al. [28] propose the Spatial-Temporal Graph Convo-lutional Networks (ST-GCN) model, which applies alternating convolution of GCN and TCN, and has become the baseline in the field of skeleton-based action recognition. Song et al. [25] developed an efficient graph convolutional network. They extended the convolutional layer in CNN to the GCN network to extract temporal dynamics and compress the model size. They then employed a compound scaling method that adjusts the width (number of channels) and depth (number of layers) factors to make the model flexible and effective. In addition, Spatial Temporal Joint Attention (ST-JointAtt) is incorporated into each block, and three input branches are fused early in the network. The above strategies enable the network to achieve good results at a low computational cost."}, {"title": "A. Skeleton Data Representation", "content": "Skeleton data consists of a set of joint coordinates, usually represented as 3D points or 2D projections, along with addi-tional information like joint velocities or orientations. These joint coordinates are often normalized to a consistent scale or relative to a specific reference point to enhance generalizability across individuals and improve recognition accuracy. Skeleton data removes video backgrounds, people textures and outlines, which reduces lots of calculations. 3D skeleton data has a good view-invariance. It is possible to change the data view by rotating the skeleton in 3D space. Some researchers use matrices or images to represent skeleton data. Yang et al. [29] proposed a Tree Structure Skeleton Image (TSSI) which encodes skeleton data by depth-first tree traversal order to image. Banerjee et al. [1] extracted four features from skeleton data (Distance feature, Distance velocity feature, Angle feature, Angle velocity feature). These features are then encoded into single-channel grayscale images and are fed to the CNN. Skeleton data is a natural topography. In addition to being used as a matrix, skeleton data could also be represented as a graph. Some researchers represent skeleton data as an undirected graph, which effectively illustrates the connections between joints. The joint information will be aggregated bi-directionally. Yan et al. [28] represented the skeleton data using natural bone topology based on prior experience. Instead of using a fixed graph, Shi et al. [21] propose a data-driven method to enable the model to adapt to various data samples. According to the pattern of human movement, the central joints always drive the peripheral joints. Shi et al. [20] represented the skeleton as a directed acyclic graph to better incorporate joint and bone data. In addition to representations based on the original topology, some researchers utilize the Node2vec method to obtain graph embeddings. Laines et al. [8] conducted a Depth-First-Search (DFS) on the skeleton tree to obtain a new spatial embedding. Sun et al. [27] utilized the breadth-first search (BFS) algo-rithm, which commences from the root joint (hip center joint) to rearrange the joints. In addition to utilizing fixed topological structures, some researchers have employed dynamic structures to represent the skeleton data. Ye et al. [30] proposed a Context-encoding Network (CeN) to automatically learn the skeleton topology, incorporating contextual features from the remaining joints in a global manner when learning the dependency between two joints. Cheng et al. [6] proposed a non-local shift graph operation, which enables each node to have a receptive field covering the entire skeleton graph, allowing for adaptive learn-ing of the relationships between joints. Chen et al. [3] proposed Channel-wise Topology Refinement Graph Convolution (CTR-GC), which learns a shared topology across all channels and refines it with channel-specific correlations for each channel."}, {"title": "III. PROPOSED APPROACH", "content": "In this work, we utilize the GCN-TCN block to extract spatial temporal features, where the GCN layer and TCN layer are alternately employed to capture spatial and tempo-ral dependencies. Temporal convolutions are essentially 2D convolutions with a kernel size of 1, designed to capture the temporal dependence of each node. In this section, we introduce our proposed spatial-structural graph convolution and the generation of the structural adjacency matrix."}, {"title": "A. Spatial-Structural Graph Convolution (SpSt-GCN)", "content": "For GCN-based methods, the input skeleton data is represented as (V, E) with N joints and T frames. Here, V represents joint points, and E represents connections between nodes, often represented by an adjacency matrix. The conventional spatial graph convolution is shown below, where $f_{in}$ and $f_{out Spatial}$ represent the input and output features, respectively. $A_j$ represents the adjacency matrix at distance j, $A_j$ is a weight vector used to normalize $A_j$. The parameters $W_j$ can be optimized during the training process.\n$f_{out Spatial} = \\sum_j W_j f_{in} A_j$\nTo increase the flexibility of graph convolution, we add a parameterized matrix B as described in [21]. The matrix B is initialized to an all-zero matrix of the same size as adjacency matrix A and optimized together with the other parameters in the training process.\n$f_{out Spatial} = \\sum_j W_j f_{in} (A A_j + B_j)$\nMany researchers commonly employ the skeletal topology structure for establishing connections among joints, a method-ology known for its efficiency albeit lacking in precision. It is crucial to recognize that the human skeleton, characterized by symmetry, presents a nuanced structure wherein even widely spaced symmetrical joints exhibit robust structural correlations. While prevailing graph convolution techniques generally enable non-edge nodes to efficiently aggregate in-formation from both nearby and distant nodes, limitations arise when considering edge nodes. These edge nodes pos-sess only a singular connection and, as a result, can solely aggregate information from nodes in close proximity to the central point. This asymmetry in information flow becomes particularly evident when comparing edge nodes to central nodes, as the former receives comparatively less neighbor information, thereby impeding the formation of optimal node representations. To address this challenge and enhance the representation of both symmetric and edge nodes, we propose a novel approach named spatial-structural graph convolution. This innovative method involves the integration of two branches: a spatial branch responsible for aggregating infor-mation from spatially adjacent nodes and a structural branch focused on consolidating information between edge nodes. By combining these branches, the spatial-structural graph convo-lution aims to achieve a more comprehensive and accurate representation of the underlying skeletal structure, thereby advancing the state-of-the-art in joint connectivity modeling 1. The structural graph convolution is formulated as follows:\n$f_{out Structural} = \\sum_j M_j f_{in} A_{sj}$\nwhere $f_{in}$ and $f_{out Structural}$ represent the input and output features, respectively. $A_{sj}$ represents the structural adjacency matrix for distance j. The parameters $M_j$ can be optimized during the training process. In this work, we set the distance as 1. Then, we use element-wise addition to combine the outputs of these two branches and obtain the final output.\n$f_{out} = f_{outSpatial} + f_{outStructural}$"}, {"title": "B. Structural Adjacency Matrix", "content": "In contrast to the conventional approach of spatial graph convolution, where all samples share a uniform adjacency matrix, we recognize that the structural connection between edge nodes is intricately linked with the specific actions performed. To capture these nuanced relationships, we propose a data-driven structural adjacency matrix, wherein the strength of connections between edge nodes is determined solely by the characteristics of each sample. In the context of the NTU60 dataset's skeleton data, we observe five edge nodes, each corresponding to a distinct time series. In standard graph convolutional networks (GCN), the adjacency matrix is normalized based on the number of adjacent nodes. However, for structural connections, creat-ing a conventional adjacency matrix between edge nodes is impractical as edge joints lack tangible spatial connections. Aggregating them without differentiation would compromise the unique characteristics of each edge node. Based on this idea, we use the similarity of edge node sequences instead of the number of adjacent nodes to represent the strength of the connection. Euclidean distance and cosine similarity are widely employed distance calculation methods in deep learning. Euclidean dis-tance quantifies the straight-line separation between two points in Euclidean space, making it well-suited for sequences with consistent lengths and where element order is crucial. Cosine similarity gauges the cosine of the angle between two vectors, treating sequences as vectors in a high-dimensional space. Cosine similarity is suitable for situations involving text data, document comparison, and situations where vector magnitudes are not critical. In the context of skeleton data, the interplay of human muscles and bones results in a strong correlation among joint movements. However, the execution of force progresses incrementally. Consider playing badminton as an example: the upper arm propels the forearm, the forearm propels the wrist, leading to the final stroke of the shuttlecock. The transmission of the movement unfolds from the core to the periphery, with movement speed and amplitude gradually accelerating. Despite the collective coordination of the entire arm for a given motion, the time and range of movement for different joint points vary. In such cases, neither Euclidean distance nor cosine similarity proves suitable. Taking inspiration from similarity calculation methods in natu-ral language processing, we turn to the utilization of Dynamic Time Warping (DTW) to compute distances between each pair of edge nodes. DTW is a technique adept at measuring the similarity between two sequences that may exhibit variations in time or speed. It excels in scenarios where sequences share similar patterns but may be temporally misaligned. The DTW algorithm constructs an optimal warping path between the sequences, assessing similarity between corresponding points and allowing for time axis stretching or compressing as needed. The primary objective is to minimize the overall cost of warping while aligning similar features. Notably, DTW accommodates discrepancies in movement time among differ-ent joint points and considers the magnitude of movement, making it highly suitable for assessing movement similarity between edge nodes. We finally use FastDTW for efficiency. We calculate a distance matrix D for each sample. To discern the correlation between different edge nodes, we leverage the inverse of the distance matrix $D^{-1}$ as a representation of the correlation matrix. Larger values in $D^{-1}$ signify greater similarity in actions between the two edge nodes, indicating a stronger correlation. Addressing the over-smoothing issue inherent in GCN, where data from different joints tends to become overly similar with each aggregation step, structural GCN takes a different approach. When employing the $D^{-1}$ approach directly, height-ened similarity among adjacent nodes leads to an aggregation of information, thereby exacerbating the likeness of edge nodes, a condition contrary to the desired outcome. Rather than engaging in information aggregation from neighboring nodes, this method prioritizes the preservation of node-specific information by delineating each edge node from others through the utilization of $-D^{-1}$. Consequently, the application of $-D^{-1}$ fosters greater differentiation among edge nodes, thereby facilitating the capture of nuanced information. To achieve this, an identity matrix (I) is introduced to retain the distinct characteristics of each node. The dynamic structural adjacency matrix, denoted as $I - D^{-1}$, is then calculated for each sample, providing a nuanced representation of the structural connections within the data. For a visual representation, we refer to the algorithm in 1, illustrating the steps involved in the dynamic structural adjacency matrix calculation. Additionally, Figure 2 provides visualizations of the spatial-structural connections for three distinct samples-throw, writing, and stand up-underscoring the effectiveness of our proposed approach in capturing the in-tricacies of fine-grained activity recognition. Figure 3 provides visualizations of matrix $D^{-1}$. Figure 4 provides visualizations of learned spatial and structural adjacency matrix."}, {"title": "C. Data Preprocessing", "content": "In this work, we adopt the same data preprocessing as de-scribed in [25]. The shape of each action sequence x is $(C_{in}, T_{in}, V_{in})$, where $C_{in}$ denotes coordinates, $T_{in}$ denotes frame time interval. Then we concatenate the slow motion $v_s$ with the fast motion $v_f$ as velocity input features.\n$v_s = x[:, t + 1, :] \u2212 x[:, t, :]$\n$v_f = x[:, t + 2, :] \u2013 x[:, t, :]$\n$v = v_s \\oplus v_f$\nFor bone features, we first calculate the lengths of the bones $b_i$, which consist by joint i (i \u2208 {1, 2, ..., $V_{in}$ }) and its adjacent joint adj (iadj \u2208 {1,2, ..., $V_{in}$ }). Then we calculate bones' angles $ba,w$ for three coordinates w (w \u2208 {x,y,z}). We concatenate the lengths of the bones by with the bone angles $b_{a,w}$ as bone input features.\n$b_i = x[:, :, i] \u2013 x[:, :, i_{adj}]$\n$b_{a,w} = arccos(\\frac{b_{i,w}}{\\sqrt{b_{ix}^2+b_{iy}^2+b_{iz}^2}})$ $b = b_i \\oplus b_{ba}$"}, {"title": "D. Model", "content": "For the overall efficiency of the network structure, we utilize the GCN block introduced by [25] as the fundamental unit 6. However, instead of the conventional GCN layer, we have integrated our novel spatial-structural graph convolution layer. The network architecture begins with an initial block designed to extract features at the outset, followed by the stacking of four GCN blocks to progressively extract higher-dimensional features. Finally, a Classifier block, which incorporates a Global Average Pooling (GAP) layer, a Dropout layer, and a Fully Connected (FC) layer, is employed to generate the output for each modality, as illustrated in 5. We then combine the results from three inputs (joint features, velocity features, bone features) to get the final output, where leveraging multiple modalities to enhance the model's capacity for nuanced feature extraction and robust predictive performance."}, {"title": "IV. EVALUATION", "content": ""}, {"title": "A. Dataset", "content": "1) NTU RGB+D Dataset: \"NTU RGB+D\" dataset [19] con-tains 60 action classes and 56,880 video samples captured from 40 different people using the Microsoft Kinect v2. The 60 action classes can be categorized into three main groups: 40 daily actions, 9 health-related actions, and 11 mutual actions. As mentioned, the NTU RGB+D dataset is a multi-modal dataset that contains RGB videos, depth map sequences, 3D skeletal data, and infrared (IR) videos for each sample. In this work, we only utilize skeleton data. Each frame contains two skeletons, and each skeleton contains 25 joints. We evaluated our method using two benchmarks proposed by the authors of this dataset: cross-subject validation (X-sub) and cross-view validation (X-view). In cross-subject validation, we divided the subjects into two groups: twenty subjects in the training data group and another twenty subjects in the validation group. In cross-view validation, the data collected by cameras 2 and 3 are recognized as training data, while the data collected by camera 1 is used for validation. 2) NTU RGB+D 120 Dataset: The \"NTU RGB+D 120\" dataset [11] is an extension of the \"NTU RGB+D\" dataset, captured from 106 different individuals, with an additional 60 classes and 57,600 video samples. The 120 action classes can be categorized into three main groups: 82 daily actions, 12 health-related actions, and 26 mutual actions. We evaluate our method using two benchmarks proposed by the authors of this dataset: cross-subject validation (X-sub) and cross-setup (X-set120). In cross-subject validation, 53 subjects are divided into a training data group and another 53 subjects into a validation group. In cross-validation, 16 setups are used for training, and the remaining 16 setups are used for testing."}, {"title": "B. Experimental Setting", "content": "In our experiments, we set the training epoch to 50. We apply the same settings as described in [25]. Our models are trained with gradient descent (SGD) with a Nesterov momentum of 0.9 and a weight decay of 0.0001. The learning rate is set to 0.1 and decays with a cosine schedule after the 10th epoch. The probability in the dropout layer is set to 0.25. We perform a data transformation as described in [21] for the X-view benchmark. The batch size is set to 16 for both the NTU RGB+D dataset and the NTU RGB+D 120 dataset. All our experiments are performed on one NVIDIA GeForce RTX 3090 GPU."}, {"title": "C. Comparisons", "content": "1) Effect of Structural Graph Convolution: In this part, we will discuss the impact of three input modalities and the impact of our structural branch on X-sub benchmark I and on X-view benchmark II. We can see that all three modalities are essential, and that fusing the scores of the three modalities outputs in the highest accuracy for both X-sub and X-view benchmarks. In addition, using the same network structure, there is a significant improvement in accuracy when employing the spatial-structural graph convolution layer instead of the spatial graph convolution layer."}, {"title": "V. CONCLUSION", "content": "In this article, we propose a Spatial-Structural Graph Con-volutional Network (SpSt-GCN) for skeleton-based action recognition to better represent symmetrical human structures and edge nodes and address the issue of over-smoothing in Graph Convolutional Networks. The spatial branch aggregates information based on the topological structure, while the data-driven structural branch performs differentiation based on the similarity of edge node sequences. Our approach yields favorable results while maintaining efficiency. In the future, we will explore how to improve the flexibility of structural connection and how to extend this method to other graph recognition tasks, e.g. In this article, we have only considered the structural connectivity of edge nodes. The representation of structural connectivity between non-edge nodes is also worth exploring."}]}