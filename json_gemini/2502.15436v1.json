{"title": "Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning", "authors": ["Raghav Singhal", "Kaustubh Ponkshe", "Rohit Vartak", "Lav R. Varshney", "Praneeth Vepakomma"], "abstract": "Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning foundation models. However, federated fine-tuning using LoRA is challenging due to suboptimal updates arising from traditional federated averaging of individual adapters. Existing solutions either incur prohibitively high communication cost that scales linearly with the number of clients or suffer from performance degradation due to limited expressivity. We introduce Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix (R) between adapters B and A, keeping other components fixed. Direct averaging of R guarantees exact updates, substantially reducing communication cost, which remains independent of the number of clients, and enables scalability. Fed-SB achieves state-of-the-art performance across commonsense reasoning, arithmetic reasoning, and language inference tasks while reducing communication costs by up to 230x. In private settings, Fed-SB further improves performance by (1) reducing trainable parameters, thereby lowering the noise required for differential privacy and (2) avoiding noise amplification introduced by other methods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff between communication and performance, offering an efficient and scalable solution for both private and non-private federated fine-tuning.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable generalization across a wide range of tasks (Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023; Raffel et al., 2020). Fine-tuning (FT) remains the most effective approach for aligning LLMs to specific data distributions and reinforcing desired properties. However, as model sizes scale, full FT becomes increasingly prohibitive due to its substantial computational cost. To address this, parameter-efficient fine-tuning (PEFT) techniques, such as low-rank adaptation (LoRA, Hu et al. (2021)), have emerged as viable alternatives, offering a favorable trade-off between computational efficiency and performance. Variants of LoRA, including QLoRA (Dettmers et al., 2024), DORA (Liu et al., 2024), AdaLoRA (Zhang et al., 2023), and LoRA-SB (Ponkshe et al., 2024), further refine this paradigm by optimizing memory efficiency, training dynamics, and generalization.\nFederated learning (FL) is a popular method for training models in settings where data is siloed across multiple entities (Kone\u010dn\u00fd et al., 2017; Kairouz et al., 2021; Bonawitz et al., 2019). Federated FT extends this paradigm by enabling large models, pre-trained on public data, to be efficiently adapted to private, distributed datasets without requiring clients to share their local data. Existing methods predominantly rely on LoRA-based techniques to learn client-specific adaptations (Zhang et al., 2024b). However, optimizing federated aggregation often involves tradeoffs between model performance (Sun et al., 2024) and communication efficiency (Wang et al., 2024; Singhal et al., 2025), necessitating careful design choices to balance these competing objectives.\nLORA-SB (Ponkshe et al., 2024), a state-of-the-art approach, optimally simulates full fine-tuning in low-rank spaces by learning an $r\\times r$ matrix between the low-rank adapters A and B while keeping other components fixed. This design reduces trainable parameters and enables better updates through its initialization strategy. Moreover, LORA-SB demonstrates that this optimal approximation is not achievable with standard LoRA-based methods. LoRA-SB learns higher-rank updates"}, {"title": "2 Preliminaries and Motivation", "content": "Federated Fine-Tuning. Given a pretrained weight matrix $W \\in \\mathbb{R}^{m\\times n}$, the objective in FT is to learn an update $\\Delta W$ for a given dataset. LoRA (Hu et al., 2021) remains the preferred method, where low-rank adapter matrices $A \\in \\mathbb{R}^{r\\times n}$ and $B\\in \\mathbb{R}^{m\\times r}$ are learned such that $\\Delta W = BA$. In federated learning, the dataset is distributed across $c$ clients, and the goal is to learn $\\Delta W$ without sharing local data with a central server. To achieve this, each client learns its own adapter matrices $A_i$ and $B_i$. The server aggregates these updates to refine $W$, along with globally beneficial representations of A and B, ultimately producing a shared aggregate model $W^{agg}$. Next, each client continues the local FT process, followed by aggregation at the end of each round. This cycle repeats over multiple rounds. We summarize some of the state-of-the-art federated FT methods below.\nFed-IT (Zhang et al., 2024b) updates the adapters A and B using the standard FedAvg (McMahan et al., 2017) algorithm:\n$A^{agg} = \\frac{1}{C} \\sum_{i=1}^C A_i, B^{agg} = \\frac{1}{C} \\sum_{i=1}^C B_i$. (1)\nFedEx-LoRA (Singhal et al., 2025) follows the same aggregation but introduces an additional error correction matrix $W_{err}$ of rank $\\min(cr, m, n)$:\n$W_{err} = \\frac{1}{C} \\left(\\sum_{i=1}^C A_iB_i\\right) - \\left(\\frac{1}{C} \\sum_{i=1}^C A_i\\right)\\left(\\frac{1}{C} \\sum_{i=1}^C B_i\\right)$. (2)\nFLORA (Wang et al., 2024) follows the same principle as FedEx-LoRA but achieves it by stacking the adapter matrices, and reinitializes them randomly at the end of each communication round.\nFFA-LORA (Sun et al., 2024) keeps A fixed while training (and aggregating) only B matrices.\n$B^{agg} = \\sum_{i=1}^C B_i$. (3)\n(Approximate) Differential Privacy. DP, introduced by Dwork (2006), is a widely adopted mathematical framework for privacy preservation. A randomized mechanism $\\mathcal{M}: \\mathcal{D} \\rightarrow \\mathcal{R}$, mapping a domain $\\mathcal{D}$ to a range $\\mathcal{R}$, satisfies $(\\epsilon, \\delta)$-differential privacy if, for any two adjacent inputs $d, d' \\in \\mathcal{D}$ and any subset of outputs $S \\subseteq \\mathcal{R}$, the following holds:\n$\\Pr[\\mathcal{M}(d) \\in S] \\le e^{\\epsilon} \\Pr[\\mathcal{M}(d') \\in S] + \\delta$. (4)\nDP-SGD. DP-SGD (Abadi et al., 2016) is a privacy-preserving variant of stochastic gradient descent (SGD) designed to ensure DP during model training. It enforces privacy by clipping per-sample gradients to a fixed norm $C$ to limit their sensitivity and then adding isotropic Gaussian noise $\\mathcal{N}(0, \\sigma^2 C^2 I)$, where $\\sigma$ controls the noise magnitude. The cumulative privacy loss over iterations is quantified using the moments accountant (Wang et al., 2019) and R\u00e9nyi DP (Mironov, 2017), which offer a tight bound on the final privacy parameter $\\epsilon$.\nExact Aggregation in Fed. LoRA: Tradeoff b/w Performance and Communication Costs. Standard federated averaging of individual LORA adapters (FedIT, Zhang et al. (2024b)) introduces inexactness in aggregation, as the ideal update should be the average of client updates.\n$W_0 + \\frac{1}{C} \\sum_{i=1}^C B_i \\times \\frac{1}{C} \\sum_{i=1}^C A_i$\nVanilla aggregation in LoRA (FedIT)\n$\\ne \\frac{1}{C} W_0 + \\sum_{i=1}^C (B_i A_i)$. (5)\nIdeal aggregation\nThe inexactness arises because the ideal averaged updates, given by $\\sum_{i=1} B_i A_i$, often exceed rank $r$, violating the low-rank constraint imposed by LoRA. To address this, FedEx-LoRA and FLORA introduce $W_{err}$ as a higher-rank correction term within the pre-trained weight matrix $W_0$, which is inherently high-rank. This correction ensures exact aggregation, leading to consistently improved performance over FedIT.\nThis, however, comes at the cost of increased communication. Since the error matrix is high rank, it substantially increases the amount of data transmitted per round. The communication cost is determined by the number of parameters sent during aggregation, which, for an $m \\times n$ matrix,"}, {"title": "3 Method", "content": "LORA-SB for Fine-Tuning. LORA-SB (Ponkshe et al., 2024) optimally approximates full FT gradients in low-rank spaces and demonstrates that its entire optimization trajectory aligns with the ideal low-rank projection of the full FT path. To achieve this, LoRA-SB fixes A and B while introducing a new trainable adapter R of size $r \\times r$. Since R has rank r, it updates the pre-trained weight while maintaining rank r, making it highly parameter efficient. As a result, LoRA-SB consistently outperforms LoRA (and its variants) across various benchmarks while using 45-90x fewer trainable parameters.\nFed-SB: A Silver bullet for (Private) Federated Fine-Tuning. We propose Fed-SB, an extremely communication-efficient and high-performing federated adaptation of LoRA-SB. Instead of reparameterizing updates as a low-rank decomposition with learnable adapters, the server distributes frozen adapters B and A, while clients train only a small matrix R (Figure 2). This enables exact aggregation, as the global update is simply the average of R across clients. Formally, given a pre-trained weight $W_0$ and data distributed across $c$ clients, each client learns updates of the form:\n$\\Delta W = BRA$. (7)\nThe server then aggregates the updates by computing the global R matrix:\n$R^{agg} = \\frac{1}{C} \\sum_{i=1}^C R_i$ $\\Delta W^{agg} = B \\left(\\frac{1}{C} \\sum_{i=1}^C R_i \\right) A$. (8)\nWe show that Fed-SB effectively resolves all challenges in (private) federated FT while achieving state-of-the-art communication efficiency and performance. highlights the advantages of Fed-SB over other methods.\nFed-SB: Exact Aggregation. Since only R is trainable, simple averaging of R across clients ensures exact aggregation without requiring updates to any other matrix. Further, the linearity of the global update with respect to the client-specific matrices $R_i$ guarantees that exact aggregation occurs within rank r, preventing communication costs from scaling with the number of clients. This is because the server only needs to aggregate and transmit the R matrix. This can be proven directly by computing the global update $\\Delta W^{agg}$:\n$\\Delta W^{agg} = B R^{agg} A$, (9)\n$ = \\frac{1}{C} \\sum_{i=1}^C BRA = \\frac{1}{C} \\sum_{i=1}^C \\Delta W_i$. (10)\nSince the global update is simply the average of the individual updates, the aggregation is exact. The key advantage here is that this exact aggregation does not incur additional communication overhead like FedEx-LoRA, nor does it compromise individual update quality like FFA-LORA.\nFed-SB: Privacy. Privacy-preserving FT with Fed-SB has two key advantages: 1) Fed-SB avoids noise amplification, which is a common issue in LoRA-based methods. 2) Since Fed-SB inherently requires fewer learnable parameters, the amount of noise added to enforce DP guarantees is significantly lower.\nAvoids Noise Amplification. DP-SGD training in Fed-SB avoids second-order noise terms, as only R is trainable. This prevents the introduction of cross terms, thereby eliminating noise amplification. The difference between the updates with and without private training is given by:\n$\\Delta W_{DP} - \\Delta W = B (R + \\xi_R) A - BRA$\n$\\Delta W_{DP} - \\Delta W = B \\xi_R A$. (11)"}, {"title": "4 Experiments & Results", "content": "Overview. We evaluate across three diverse NLP benchmarks, covering models that span from BERT-base (110M) to Gemma-2 (9B), thereby encompassing both masked and autoregressive architectures. Specifically, we fine-tune Mistral-7B (Jiang et al., 2023), Gemma-2 9B (Team et al., 2024), Llama-3.2 3B (Dubey et al., 2024), and BERT-base (Devlin, 2018). Experiments are conducted on a single NVIDIA A6000 GPU (48 GB), with results averaged over three random runs. Our experiments consider both performance and communication efficiency. To optimize memory efficiency, all base models (except BERT) are loaded in torch.bfloat16. Detailed dataset specifications can be found in Appendix D. For federated data distribution, we adopt a standard protocol where client datasets are randomly sampled, following established practice in FL (Sun et al., 2024; He et al., 2020; Lai et al., 2022). Detailed settings are provided in Appendix C.\nBaselines. We evaluate Fed-SB against several SOTA federated FT approaches described previously, considering both private and non-private settings. Specifically, we compare it with FedIT, FedEx-LoRA, FLORA, and FFA-LoRA. Where applicable, we also include comparisons with standard centralized LoRA (Hu et al., 2021).\nDetails. We conduct experiments in the federated non-private setting across two reasoning tasks: commonsense reasoning and arithmetic reasoning. For commonsense reasoning, we fine-tune Llama-3.2 3B on COMMONSENSE170K-a dataset aggregating eight commonsense reasoning corpora (Hu et al., 2023)\u2014and evaluate its effectiveness across all constituent datasets. The experiments are performed in a cross-silo federated learning setup involving 5 clients. For arithmetic reasoning, we fine-tune Mistral-7B (Jiang et al., 2023) and Gemma-2 9B (Team et al., 2024) on 20K samples from the MetaMathQA dataset (Yu et al.,"}, {"title": "5 Conclusion", "content": "Existing LoRA-based federated FT methods either suffer from suboptimal updates or incur prohibitively high communication costs. We introduce Fed-SB, a federated adaptation of LoRA-SB that ensures exact aggregation while maintaining high communication efficiency. By training only a small $r\\times r$ matrix and leveraging direct averaging, Fed-SB eliminates high-rank update costs and achieves communication efficiency independent of the number of clients. Fed-SB is particularly well-suited for private FT, as its linearity prevents noise amplification, and its reduced parameter count minimizes noise required for enforcing DP guarantees. It consistently outperforms existing methods across all models and tasks while reducing communication costs by up to 230x. These advantages establish Fed-SB as an efficient and scalable solution, setting a new Pareto frontier in (private) federated FT. For future work, we plan to analyze the geometry"}, {"title": "6 Limitations", "content": "While our approach is easily adaptable to other architectures, such as vision language models (VLMs) and vision transformers (VITs), we have not evaluated on such architectures. We have not yet extended our work to rank-heterogeneous settings, where clients operate with different ranks and computational budgets. Furthermore, we do not evaluate Fed-SB in scenarios with extreme data heterogeneity or significant class imbalances."}, {"title": "7 Acknowledgements", "content": "This work was supported by funding from Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and an ADIA Lab fellowship."}, {"title": "A Proof of Lemma 1", "content": "Lemma. Consider a model with d learn-able parameters trained using DP-SGD. The privacy parameter $\\epsilon$ for d-approximate differential privacy, given T training steps and a batch size of q, is expressed as:\n$\\epsilon = O(q\\sqrt{T}d\\log(1/\\delta)) = O(\\sqrt{d})$. (13)\nProof. The following result from Abadi et al. (2016) describes the relationship between noise variance, privacy parameters, number of optimization steps, batch size, and sample size in DP-SGD.\nTheorem. There exist constants $c_1$ and $c_2$ such that, given the sampling probability $q = L/N$ and the number of optimization steps T, for any $\\epsilon < c_1q^2T$, DP-SGD is $(\\epsilon, \\delta)$-differentially private for any $\\delta > 0$ if the noise scale satisfies:\n$\\sigma \\geq 2 \\frac{\\sqrt{qT\\log(1/\\delta)}}{\\epsilon}$. (14)\nEach DP-SGD step introduces noise following $\\mathcal{N} (0, \\sigma^2 C^2 I_d)$ and satisfies $(\\alpha, \\epsilon(\\alpha))$-RDP (R\u00e9nyi DP) for the Gaussian mechanism. For a function with $l_2$-sensitivity $\\Delta$, the Gaussian mechanism satisfies $(\\alpha, \\epsilon)$-RDP with:\n$\\epsilon(\\alpha) = \\frac{\\alpha \\Delta^2}{2 \\sigma^2_{noise}}$. (15)\nSince DP-SGD has $\\Delta^2 = C^2$ and $\\sigma_{noise} = \\sigma C$, applying privacy amplification due to sampling probability q results in each step satisfying $(\\alpha, \\gamma)$-RDP, where, for small q:\n$\\gamma = O \\left( \\frac{q \\alpha}{\\sigma^2} \\right)$ (16)\nUsing composition over T steps, the total RDP privacy parameter becomes:\n$\\gamma_{total} = O \\left( \\frac{qT\\alpha}{\\sigma^2} \\right)$. (17)\nConverting this RDP bound back to $(\\epsilon, \\delta)$-DP and setting $\\alpha$ proportional to $1/\\sqrt{d}$, given that the $l_2$-norm of the gradient scales as $\\sqrt{d}$, we obtain:\n$\\epsilon = O \\left( \\frac{q T \\sqrt{\\alpha} \\log(1/\\delta)}{\\sigma^2} \\right)$. (18)\nSubstituting $\\sigma \\propto 1/\\sqrt{d}$, we derive:\n$\\epsilon = O(q\\sqrt{T}d\\log(1/\\delta)) = O(\\sqrt{d})$. (19)"}, {"title": "B Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT). LORA (Hu et al., 2021) has become ubiquitous for fine-tuning LLMs (Zhang et al., 2024a) by modeling weight updates as product of low-rank matrices. Several variants have been proposed to improve efficiency, stability, and adaptability. QLORA (Dettmers et al., 2024) enables efficient fine-tuning through quantization strategies, reducing memory usage while maintaining performance. AdaLoRA (Zhang et al., 2023) dynamically allocates a layer-specific rank budget by assigning importance scores to individual weight matrices. LoRA-XS (Ba\u0142azy et al., 2024) further reduces trainable parameters by inserting a trainable matrix between frozen LoRA matrices. VeRA (Kopiczko et al., 2024) enhances parameter efficiency by learning shared adapters across layers. DoRA (Liu et al., 2024) decomposes the pre-trained matrix into two parts\u2014magnitude and direction\u2014and applies LORA modules only to the direction component. PiSSA (Meng et al., 2024) improves adaptation by initializing adapters using the singular value decomposition (SVD) of pre-trained weights. rsLoRA (Kalajdzievski, 2023) introduces a rank-scaling factor to stabilize learning. LoRA-SB (Ponkshe et al., 2024) provably approximates gradients optimally in low-rank spaces, achieving superior performance with significantly higher parameter efficiency.\nFederated Fine-Tuning. Federated Learning (FL) consists of a centralized global model and multiple clients, each with its own local dataset and computational capacity. The global model is updated by aggregating client updates (Kairouz et al., 2021). FedBERT (Tian et al., 2022) focuses on federated pre-training, while other methods work on federated fine-tuning (Zhang et al., 2022; Kuang et al., 2024; Babakniya et al., 2023). Fed-IT (Zhang et al., 2024c) aggregates low-rank adapters across clients using standard federated averaging (McMahan et al., 2017) before updating the global model. To address inexact aggregation, FedEx-LORA (Singhal et al., 2025) introduces an error matrix to correct residual errors, ensuring more precise updates. FLORA (Wang et al., 2024) follows the same exact aggregation principle by stacking matrices and extends this approach to heterogeneous rank settings. FFA-LORA (Sun et al., 2024) mitigates aggregation inexactness"}, {"title": "C Experiment Details", "content": "We conduct experiments on a single NVIDIA A6000 GPU (48 GB) and report the average results from three independent runs. All non-private models are trained using the AdamW optimizer (Loshchilov and Hutter, 2019). In line with Ponkshe et al. (2024), we initialize the adapter matrices using just 1/1000 of the respective training dataset size.\nTable 6 presents the key hyperparameters and configurations for Mistral-7B, Gemma-2 9B, and Llama-3.2 3B. Our setup closely follows previous works (Hu et al., 2023; Ponkshe et al., 2024), ensuring consistency with established best practices. For the baseline experiments, we further set $\\alpha = 16$, consistent with prior literature (Singhal et al., 2025; Sun et al., 2024). We additionally perform a sweep over the learning rate for our experiments."}, {"title": "D Dataset Details", "content": "COMMONSENSE170K is a large-scale dataset that brings together eight benchmarks designed to assess various aspects of commonsense reasoning (Hu et al., 2023). Below is an overview of its constituent datasets:\n1. PIQA (Bisk et al., 2020) evaluates physical commonsense by asking models to determine the most reasonable action in a given scenario.\n2. ARC Easy (ARC-e) (Clark et al., 2018) consists of elementary-level science questions, serving as a fundamental test of a model's reasoning abilities.\n3. OBQA (Mihaylov et al., 2018) presents knowledge-intensive, open-book multiple-choice questions that require multi-step reasoning and retrieval.\n4. HellaSwag (Zellers et al., 2019) tests contextual reasoning by asking models to predict the most plausible continuation of a passage from a set of candidates.\n5. SIQA (Sap et al., 2019) examines social intelligence, requiring models to predict human actions and their social consequences.\n6. ARC Challenge (ARC-c) (Clark et al., 2018) includes difficult multiple-choice science questions that demand deeper logical inference beyond statistical co-occurrence.\n7. BoolQ (Clark et al., 2019) consists of naturally occurring yes/no questions, requiring"}, {"title": "E Additional Plots", "content": "is a widely used benchmark for assessing textual entailment models in natural language understanding. It contains approximately 570,000 sentence pairs, each categorized into one of three classes: entailment, contradiction, or neutral, requiring models to infer the relationship between a given premise and hypothesis."}]}