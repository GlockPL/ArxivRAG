{"title": "Simulation of Language Evolution under Regulated Social Media Platforms: A Synergistic Approach of Large Language Models and Genetic Algorithms", "authors": ["Jinyu Cai", "Yusei Ishimizu", "Mingyue Zhang", "Munan Li", "Jialong Li", "Kenji Tei"], "abstract": "Social media platforms frequently impose restrictive policies to moderate user content, prompting the emergence of creative evasion language strategies. This paper presents a multi-agent framework based on Large Language Models (LLMs) to simulate the iterative evolution of language strategies under regulatory constraints. In this framework, participant agents, as social media users, continuously evolve their language expression, while supervisory agents emulate platform-level regulation by assessing policy violations. To achieve a more faithful simulation, we employ a dual design of language strategies (constraint and expression) to differentiate conflicting goals and utilize an LLM-driven GA (Genetic Algorithm) for the selection, mutation, and crossover of language strategies. The framework is evaluated using two distinct scenarios: an abstract password game and a realistic simulated illegal pet trade scenario. Experimental results demonstrate that as the number of dialogue rounds increases, both the number of uninterrupted dialogue turns and the accuracy of information transmission improve significantly. Furthermore, a user study with 40 participants validates the real-world relevance of the generated dialogues and strategies. Moreover, ablation studies validate the importance of the GA, emphasizing its contribution to long-term adaptability and improved overall results.", "sections": [{"title": "I. INTRODUCTION", "content": "Social media platforms such as X, Facebook, and Sina Weibo have transformed how billions of people communicate and share information, becoming major hubs for content creation, dissemination, and engagement. To maintain a healthy online environment, platform operators implement content moderation policies to identify and prevent policy violations. Under these constraints, users may develop language expression\u2014such as coded language, metaphors, or ambiguous expressions to circumvent automated detection [1], creating an adversarial interaction between user tactics and platform enforcement.\nSimultaneously, private or direct messaging channels can provide fertile ground for covert communications, where individuals seeking to coordinate scams, illicit trade, or other criminal activities mask their intentions with subtle linguistic shifts. The evolution of these hidden tactics complicates the task of law enforcement and platform moderators, who must balance effective oversight against privacy and free expression concerns. Employing simulation-based approaches to examine how language evolves under various moderation policies can shed light on emerging patterns of evasion. Such insights enable stakeholders to refine their tools and policies, reducing the spread of harmful content while preserving open, vibrant communication within social ecosystems.\nEffectively simulating the evolution of language demands robust natural language processing capabilities. In recent years, the rapid development of Large Language Models (LLMs) has opened new avenues for simulating the dynamics of social systems. LLMs, with their ability to understand and generate sophisticated language, have become powerful tools for simulating the evolution of language. Numerous studies have explored the application of LLMs in simulating human user behaviors across various scenarios. For example, [2]\u2013[4] integrated LLMs with multi-agent systems to simulate micro-social networks, observing agent behaviors and strategies that reflect human interaction patterns. In [5], LLMs were used to simulate strategies in negotiation games, continuously optimizing bargaining tactics through iterative self-play; additionally, LLMs have achieved significant results in social reasoning games (such as Werewolf), developing effective game strategies by analyzing historical communication patterns [6], demonstrating their potential for evolving behaviors in dynamic contexts.\nAlthough LLMs have been widely adopted for understanding human intentions and simulating social system dynamics, their potential for investigating social media users' language evolution under stringent content moderation policies has yet to be fully realized. Three key dimensions warrant attention in this area. First, the evolution of language is not solely driven by user behavior but also shaped by platform-level content moderation policies, resulting in a distinctly adversarial interaction. Second, such a scenario must not only circumvent regulation but also ensure that information can be accurately conveyed\u2014a conflict-laden and complex set of objectives that significantly heightens the challenge of strategy evolution. Third, effective simulation of language evolution cannot rely solely on inference from existing corpora, because an overreliance on static data may limit the emergence of new strategies, making it difficult for models to capture the dynamic changes arising during the process of language evolution.\nTo fill these gaps, this study proposes a multi-agent framework based on LLMs that defines two core roles: participant agents and supervisory agent. In our framework, participant"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "This section provides an overview of relevant background and previous work that underpins this study. We begin by discussing the advances in LLMs, move on to research related to slang detection and identification, and conclude by exploring the application of LLMs in evolutionary game theory and social simulations.\nA. Large Language Models\nThe advent of LLMs has revolutionized the field of natural language processing (NLP), with models like GPT-4 [8] and LLaMA [9] showcasing state-of-the-art performance in various linguistic tasks. These models, built upon the Transformer architecture [10], leverage self-attention mechanisms to handle sequential data efficiently, enabling them to capture complex linguistic patterns, such as syntax and semantics, across vast corpora of text.\nThe training of these models is based on large-scale datasets, which allows them to generalize across diverse linguistic contexts, including different languages, genres, and registers. A noteworthy aspect of LLMs is their ability to exhibit zero-shot and few-shot learning, which empowers them to perform well on tasks they have not been explicitly trained on [11]\u2013[14]. Additionally, techniques like Reinforcement Learning from Human Feedback (RLHF) [15] enhance their capability to align with human ethical norms, improving both the quality and appropriateness of generated content. As a result, these models have been deployed in various real-world applications, ranging from content creation to decision-making in social contexts.\nB. Slang Detection and Identification\nThe detection and identification of slang have long been significant challenges in NLP due to the constantly evolving"}, {"title": "III. FRAMEWORK DESIGN", "content": "Our framework aims to simulate the evolution of language on social platforms by simulating the evolution of underlying language strategies. The evolution of language strategies on social platforms is driven by conflicting goals: the desire for self-expression and the constraints on that expression. Users adjust their strategies to balance these demands. Our framework includes two types of agents to represent these motivations: the supervisory agent, responsible for limiting user expression, and the participant agents, who aim to convey specific information to one another. Participant agents continuously learn from past experiences through ongoing dialogues to develop their language strategies, effectively transmitting information while avoiding detection by the supervisory agent. In this section, we first present an overview of our framework, then provide a detailed explanation of the four modules within the participant agent, and finally introduce the design of the supervisory agent.\nA. Overview\nLLMs serve as the core of each agent in our framework, leveraging their powerful natural language processing capabilities to drive dialogue generation and strategy optimization. To capture the iterative nature of strategy evolution, we define a round as the basic unit where strategies evolve through a complete cycle of interactions between participant agents and the supervisory agent. Within each round, a turn consists of a single exchange of dialogue between participant agents, followed by a review by the supervisory agent. The system's operational process is as follows:\n1) In the initial stage of the framework, each agent is initialized with role settings, experimental background knowledge, and global objectives."}, {"title": "B. Participant Agents", "content": "To effectively simulate the internal dynamics of users during strategy evolution, our participant-agent framework is designed to balance the conflicting objectives of accurate information transmission and regulatory evasion. To achieve this, each agent employs two distinct types of strategies. Constraint strategies are designed to help participant agents evade detection and intervention by the supervisory agent, continuously optimizing based on past violations. Consider a scenario where a buyer seeks to purchase a parrot through a social platform, but the trade of animals is prohibited by the platform's content regulation policies. In this scenario, a constraint strategy might be to \"avoid directly mentioning specific animals or transactions.\" Expression strategies aim to enhance the clarity and effectiveness of information transmission, ensuring that intentions are accurately understood. In this scenario, an expression strategy would focus on describing the key characteristics of the parrot as precisely as possible, such as its \"detailed description of the parrot's key traits, including its plumage and ability to mimic sounds,\" to ensure effective communication.\nOur participant-agent framework consists of four key modules, each driven by an LLM: the memory module, which stores dialogue-related context; the dialogue module, which generates conversation content; the reflection module, which refines strategies based on past interactions; and the planning module, which balances constraint and expression strategies to generate abstract plans.\n1) Memory Module: The memory module stores dialogue logs, background information, violation logs, the strategy pool, and the current strategies and plans. The dialogue logs stores the dialogue content, while the violation log records instances of regulation violations detected by the supervisory agent. The background information includes role settings, experimental background knowledge, and global objectives. For example, \"Suppose your name is Amy, and you are chatting with a user on a social platform. You want to buy a parrot from them, but the trade of animals is prohibited... You need to convey your intention to buy the parrot while evading detection by the regulatory system...\"\nBackground information, dialogue logs, and the current strategies and plans serve as short-term memory [25], containing only the content from the current round. In contrast, violation logs and the strategy pool serve as long-term memory, which are not reset at the end of each round but are updated instead.\nExcessive memory information can potentially distract the LLM and lead to a decline in performance [26]. Hence, the"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this study, we propose an LLM-based multi-agent simulation framework that effectively simulates language evolution"}, {"title": "A. Experimental Settings", "content": "In our evaluation, we designed an abstract password game [28] and a more realistic illicit pet trade scenario [29]\u2013[31]. The overall experimental procedure follows the description in Section III. In each round, the process comprises three stages: initialization, dialogue, and interview. In each round, the information that the participant agents need to convey will be randomly generated within a defined range. Only rounds in which a five-turn dialogue is successfully completed proceed to the interview phase, during which both parties' successful transmission of information is verified. If any regulation violations are detected by the supervisory agent during the dialogue stage, that round is deemed a failure and is assigned a transmission score of zero. To ensure the robustness of our findings, we conducted 15 independent trials for each experimental condition. Each trial consisted of 50 fixed dialogue rounds. The crossover and mutation probabilities in the framework were set to 0.2 and 0.8.\n1) Scenario 1: Password Game: Our first scenario is considered a relatively simple and abstract guessing game that involves a numerical context. In this setup, each participant is assigned a four-digit password, which they must convey to another participant within five rounds of dialogue without directly mentioning the numbers. Simultaneously, they need to extract information from the dialogue to infer the other's password.\nThe supervisory agent in the dialogue follows the policy of \"prohibiting all content related to numbers.\" The \"password game\" scenario is specifically designed to observe and analyze participant agents' language adaptability and strategic evolution in a theoretical and abstract context. Compared to complex scenarios based on real events, it provides a clearer and more easily quantifiable experimental environment.\n2) Scenario 2: Illicit Pet Trade: Simulating and detecting the covert drug transactions prevalent on social platforms is an important and realistic research topic. However, given the ethical considerations, we choose not to proceed with direct simulations, but to adopt a more neutral and less harmful setting\u2014a illicit pet trade scenario\u2014to serve as our experimental context.\nIn this scenario, we simulate an illegal pet trade through social platform. Participant agents play the roles of buyer and seller. The buyer conveys a request to purchase a randomly selected pet (such as a parrot, lizard, or cat) and specifies the currency they can use for payment (e.g., USD, CNY, JPY). The seller provides details about the transaction, including the time (morning, noon, or midnight) and location (school, park, or station). The supervisory agent enforces a policy that considers discussions related to pet transactions as regulation violations. The focus of this experiment is to observe the adaptability of the participant agents' language and its evolution at a finer level of information granularity, as well as whether regulators can accurately detect regulation violations."}, {"title": "B. RQ1: Effectiveness", "content": "1) Experiment Settings: To evaluate RQ1, we conducted experiments using four different LLM models provided by various companies: gpt-4o, gpt-40-mini, deepseekV2, and qwen-turbo. We introduced the following metric:"}, {"title": "C. Supervisory Agent", "content": "Our framework's supervisory agent emulates prevalent content moderation mechanisms on social platform. Unlike participant agents that require complex background information, the supervisory agent's function and role are simplified to reflect real-world supervision characteristics.\nThe primary task of the supervisory agent is to ensure that content complies with content management regulations. To replicate contemporary review mechanisms that integrate automated keyword filters with human oversight, our supervisory agent employs a two-tiered approach: preliminary keyword filtering followed by detailed assessment using an LLM. In the LLM-based filter, we use CoT to improve the reliability of reasoning while minimizing factors like positional bias. Specifically, we require the LLM not only to identify whether a violation has occurred but also to specify the exact violation and the reasoning behind it. Additionally, the content management regulations of the supervisory agent can be modified at any time based on inputs from researchers, enabling a realistic simulation of the adversarial interactions between users and platform regulators."}, {"title": "IV. EVALUATION", "content": "Our experiments aim to investigate whether agents within our framework can produce effective evolution of language strategies. Specifically, our experimental section addresses the following three research questions (RQs):\n1) RQ1 (Effectiveness): Can participants effectively evade regulatory detection over time, and how does the accuracy of information transmission change? Additionally, how do different LLMs affect the content and effectiveness?\n2) RQ2 (Human Interpretation): Do the evolved language strategies employed by agents effectively align with human understanding? Can they be interpreted and applied in real-world scenarios?\n3) RQ3 (Ablation Study): How does the newly introduced GA impact the evolution process in our framework?"}, {"title": "A. RQ1: Effectiveness", "content": "\u2022 Average Dialogue turns: This metric evaluates the number of successful dialogue rounds the agents complete without being flagged by the supervisory agent. A higher dialogue turns indicates greater success in evading regulatory scrutiny.\n\u2022 Average Entropy (Language Complexity): Entropy reflects the unpredictability of text and is calculated as:\n$H(X) = -\\sum_{i=1}^{n} P(x_i) \\log P(x_i)$,\nwhere $P(x_i)$ is the probability of each token. Higher entropy suggests a broader range of expressions (indicating greater innovation), but excessively high entropy can lead to incoherence.\n\u2022 Distinct (Lexical Diversity): Distinct measures the number of unique n-grams in the text, computed as:\n$Distinct\\text{-}n = \\frac{\\text{Number of unique n-grams}}{\\text{Total number of n-grams}}$\nSpecifically, we evaluate Distinct-1 (unique unigrams). A higher distinct score indicates a larger, more varied vocabulary, signifying richer language use. Note that while both metrics assess diversity in language, entropy focuses on unpredictability, whereas distinct emphasizes lexical variety."}, {"title": "RQ2: Human Interpretation", "content": "1) Experiment Settings: To investigate the real-world relevance of both the evolved language strategies and the resulting dialogue, we conducted a human evaluation on a subset of successful dialogue records from the password game and illicit pet trade scenario. The dialogues generated by the gpt-4o models are randomly selected, and 40 human participants participated in the experiment to evaluate them. The 40 human reviewers had an average age of approximately 27 (SD = 4). In terms of gender, 75% of the human reviewers were male, and 25% were female. Regarding educational background, 67.5% held a bachelor's degree, 27.5% held a master's degree or above, and 5% had an associate degree or lower. All dialogue records were presented in Simplified Chinese.\nEach participant rated each dialogue on a 5-point Likert scale on the following five metrics:"}, {"title": "B. RQ3: Ablation Experiment", "content": "1) Experiment Settings: To evaluate the effectiveness of the GA introduced in our framework, we conducted an ablation experiment using gpt-40-mini and gpt-4o as the underlying LLM. For comparison, we employed the approach from our initial study [7], which primarily differs in its strategy-update mechanism. In that earlier framework, the LLM is provided with both the existing strategy and newly flagged regulation violation records during the reflection stage, prompting the model to propose a new set of strategies that replace the old ones. In contrast, our new framework employs a GA process where each strategy is treated as a discrete unit and optimized iteratively through GA."}, {"title": "E. Discussion and Limitation", "content": "In this study, we leveraged LLM agents to simulate the evolution of language strategies under regulatory pressure. While our results provide initial evidence that agents can adapt and develop covert communication tactics, the simulations also exhibit noteworthy instabilities. First, the inherent randomness of LLM generation can cause significant fluctuations in outcomes: the same prompts may yield different strategic responses, particularly when the experimental scale (number of agents or dialogue rounds) is limited. In our framework, LLMs not only generate dialogues but also determine strategies and regulatory responses; as a result, any stochasticity is compounded across multiple modules, making the final results sensitive to small variations in prompt inputs or random seeds. Although such variability partially reflects the diversity of real-world human behavior to some extent, it complicates the interpretation of findings in a controlled experimental setup.\nA second limitation lies in the relatively narrow scope of language strategies observed. The agents predominantly relied on general-purpose evasive methods, such as analogies or implicit references, yet rarely produced fully \"encrypted\" or specialized code words that might arise in realistic cultural or social contexts. This outcome highlights the challenge that LLMs, pre-trained on broad domains and further refined via RLHF, are predisposed to generate text consistent with mainstream norms, thereby inhibiting the formation of highly unconventional or obscure expressions. Moreover, in scenarios where the training corpus lacks sufficient examples of subcultural or community-specific covert language, the model is less able to invent or adopt specialized linguistic forms.\nFinally, our experiments focused on one-to-one private interactions that emphasize regulatory evasion, without exploring the dynamics of public, many-to-many conversations where language strategies might evolve and propagate differently in a broader social context. While each participant agent does learn and adapt incrementally across dialogue rounds, real-world language evolution involves extensive, long-term propagation across diverse communities. Covert terms or code words may gradually gain acceptance, be modified by different user groups, or fade from use entirely. By contrast, the small-scale nature of our simulated dialogues means that emergent language strategies do not undergo the sustained diffusion and feedback processes characteristic of real social platforms, limiting the ecological validity of our findings."}, {"title": "RQ1: Effectiveness", "content": "These fluctuations can be attributed to the dynamic nature of our simulation framework, which does not converge on a single dominant strategy but rather encourages ongoing exploration of novel language strategies.\nWe also observe that different LLMs influence the learning trend to varying degrees. Among the models tested, gpt-4o demonstrates the strongest performance. Although other models generally share a similar upward trend, their relative performance gaps prove less stable. For instance, while deepseekV2 achieves the highest number of turns around the 20th round, its performance declines significantly by the 50th round in comparison to other models.\nTurning to the accuracy results, we again observe a similar learning trajectory. This parallel arises primarily because if participant agents fail to complete a sufficient number of uninterrupted dialogue turns, the successfully transmitted information in that round is effectively zero. Consequently, especially in the early stages of the experiment, many rounds end with no successful transmissions. Overall, gpt-4o still maintains a clear advantage over the other LLMs. However, we do observe subtle differences when comparing the dialogue round trends: for example, at the 20th dialogue round, deepseekV2 achieves a significantly higher average number of communication cycles than gpt-40-mini, yet their information transmission accuracy remains relatively similar.\nDespite the overall positive learning trajectory, the average information accuracy remains low in the password game. We believe this outcome stems primarily from the intrinsic difficulty introduced by the scenario's abstract nature. Without explicit prompts driving agents to develop symbolic or otherwise encrypted language stratgy, communication largely remains within the realm of everyday language. Consequently, the indirect expression of numeric information is challenging to implement and easily detectible by the supervisory agent."}, {"title": "RQ2: Human Interpretation", "content": "2) Experiment Results: As shown in Fig.3, our framework consistently achieves average scores of 3.4 or above across most indicators (such as explicit understanding and implicit understanding). This suggests that, both in terms of the generated dialogues and the underlying strategies, it possesses valuable practical applicability.\nComparing distributions between the password game and the illicit pet trade scenario reveals some interesting phenomena. Focusing on \u201crealistic significance\" and \"regulatory avoidance,\" the more abstract password game often yields higher mean values than the more concrete illicit pet trade scenario, while also exhibiting lower dispersion. We speculate this is related to the inherently abstract nature of numeric information: encryption and covert hints can be harder to detect in such contexts, and the growing tendency on Chinese internet platforms to use abstract language [32] may lead reviewers to have a higher acceptance of \u201cobscure\" expressions. Conversely, the illicit pet trade scenario, despite being closely tied to real-world transactions, may suffer if the indirect or"}, {"title": "RQ3: Ablation Experiment", "content": "2) Experiment Results: As shown in Fig. 4, the GA-based framework demonstrates significant advantages. In the short-term experiment within the first 35 rounds, the w/o GA approach might show slight initial superiority due to the larger changes brought about by replacing the entire strategy. However, overall, w/ GA performs better than w/o GA. This difference increases as the number of rounds grows, particularly after round 35, where the advantages of w/ GA become even more pronounced. The GA process enables effective strategy evolution and adaptation, leading to an increased number of dialogue turns and improved accuracy, highlighting the framework's enhanced adaptability in the long term."}, {"title": "Other metrics:", "content": "The population. c is a constant that balances exploration and exploitation. The exploitation term, $ \\frac{S_i}{T_i} $ , reflects the success rate of strategy i, while the exploration term, $ c\\cdot\\sqrt{\\frac{\\ln T}{T_i}} $ , encourages the selection of less frequently used strategies. Strategies are then selected probabilistically based on the exponential of their UCB scores:\n$P_i = \\frac{e^{k \\cdot UCB_i}}{\\sum_{j=1}^{N} e^{k \\cdot UCB_j}}$\nwhere k is a scaling factor ensuring adequate differentiation among probabilities and N is the total number of strategies in the pool."}, {"title": "ETHICAL CONSIDERATION", "content": "While the user studies conducted in this study were exempt from ethical review by our institution, we proactively implemented measures to ensure participant's rights. Participants were provided with written consent detailing our privacy policy, including the purpose and disclosure of the experiment data, before the study commenced. No personally identifiable information was collected in the questionnaires. Additionally, the collected data will be securely stored and destroyed after three years."}, {"title": "RQ1: Effectiveness", "content": "TABLE I: Performance of Different LLMs in Password Game"}]}