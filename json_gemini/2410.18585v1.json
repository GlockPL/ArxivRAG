{"title": "Aligning CodeLLMs with Direct Preference Optimization", "authors": ["Yibo Miao", "Bofei Gao", "Shanghaoran Quan", "Junyang Lin", "Daoguang Zan", "Jiaheng Liu", "Jian Yang", "Tianyu Liu", "Zhijie Deng"], "abstract": "The last year has witnessed the rapid progress of large language models (LLMs) across diverse domains. Among them, CodeLLMs have garnered particular attention because they can not only assist in completing various programming tasks but also represent the decision-making and logical reasoning capabilities of LLMs. However, current CodeLLMs mainly focus on pre-training and supervised fine-tuning scenarios, leaving the alignment stage, which is important for post-training LLMs, under-explored. This work first identifies that the commonly used PPO algorithm may be suboptimal for the alignment of CodeLLM because the involved reward rules are routinely coarse-grained and potentially flawed. We then advocate addressing this using the DPO algorithm. Based on only preference data pairs, DPO can render the model rank data automatically, giving rise to a fine-grained rewarding pattern more robust than human intervention. We also contribute a pipeline for collecting preference pairs for DPO on CodeLLMs. Studies show that our method significantly improves the performance of existing CodeLLMs on benchmarks such as MBPP and HumanEval.", "sections": [{"title": "1 Introduction", "content": "The past few years have witnessed the rapid development of large language models (LLMs) (Touvron et al., 2023; Chowdhery et al., 2023; Achiam et al., 2023). LLMs have quickly been used in specific domains like medicine (Thirunavukarasu et al., 2023), laws (Sun, 2023), finance (Yang et al., 2023), etc. LLMs designed for solving coding tasks, referred to as CodeLLMs, are particularly noteworthy due to their potential to automate and streamline programming, including bug detection and code generation, thereby enhancing productivity (Li et al., 2023; Wei et al., 2023; Guo et al., 2024). Current research on CodeLLMs primarily focuses on the accumulation of extensive code-related corpora for pre-training, as well as the collection of diverse instruction-following datasets for supervised fine-tuning (Roziere et al., 2023; Li et al., 2023; Hui et al., 2024). Given that the alignment plays an important role in improving the reasoning ability of LLMs (OpenAI, 2024b), seminal works (Le et al., 2022; Liu et al., 2023; Dou et al., 2024) have proposed to enhance CodeLLMs through Proximal Policy Optimization (PPO) (Schulman et al., 2017). However, we argue that they suffer from limitations in the definition of reward functions. For example, they commonly assign a fixed reward of -1 to any code snippet containing syntax errors that cannot be compiled, irrespective of the error count. As a result, code snippets with a single syntax error are treated no differently from those riddled with multiple errors. This makes the reward model fail to capture the nuanced preference distinctions among various code snippets and hence cannot efficiently guide the alignment of CodeLLMs.\nThis work proposes to align the CodeLLMs with the help of Direct Preference Optimization (DPO) (Rafailov et al., 2023). Unlike PPO, DPO does not explicitly define a reward model to capture preference, but alternatively uses model likelihood to represent that. By learning by comparing data pairs, DPO can automatically acquire the fine-grained differentiation between samples from coarse rewarding signals. Ideally, after training, a code snippet with few errors can be assigned a higher reward than that containing more errors. Compared to defining fine-grained, hand-crafted reward rules for better PPO, our DPO approach enjoys higher flexibility and can reduce the risk of reward hacking associated with flawed hand-crafted reward rules.\nGiven that using data pairs from other models for DPO can be sub-optimal and even lead to model degradation (Yan et al., 2024; Lai et al., 2024), we propose to construct on-policy preference data for DPO training, which distinguishes us from Weyssow et al. (2024). Specifically, we introduce external code executors to provide feedback for ranking code generations. An overview of this is depicted in Fig. 2. Empirically, our method has demonstrably increased the performance of CodeQwen1.5 7B on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021), enhancing the scores from 0.783 to 0.804 and 0.829 to 0.878, respectively."}, {"title": "2 Methodology", "content": "2.1 Issues of PPO for CodeLLMS\nLet $\\pi_\\theta(y|x)$ denote a large language model (LLM), which generates response y given the user instruction x."}, {"title": "2.2 DPO for CodeLLMS", "content": "Instead of exploring designing more fine-grained rewarding rules in a costly trial-and-error manner, this paper proposes to utilize DPO (Rafailov et al., 2023) to align CodeLLMs more efficiently and reliably. Technically, DPO operates on pre-collected preference data pairs and solves the following problem:\n$\\mathcal{L}=-\\sum_{(x, y_{+}, y_{-})}\\log \\sigma\\left(\\beta\\left(\\log \\frac{\\pi_{\\theta}(y_{+}|x)}{\\pi_{o}(y_{+}|x)}-\\log \\frac{\\pi_{\\theta}(y_{-}|x)}{\\pi_{o}(y_{-}|x)}\\right)\\right)$  (3)"}, {"title": "3 Experiments", "content": "3.1 Experimental Details\nCodeLLMs of concern. We selected CodeLLMs that have outstanding performance and have garnered wide attention from the research community, as the SFT model for further alignment. Specifically, we choose CodeQwen1.5-Instruct 7B (Bai et al., 2023), CodeLlama-Instruct 7B (Roziere et al., 2023), and DeepSeek-Coder-Instrcut 6.7B (Guo et al., 2024) as the target models to conduct our experiments.\nBenchmarks and evaluation. In order to accurately assess the coding capabilities of CodeLLMs, we selected the most commonly used MBPP (Mostly Basic Programming Problems) (Austin et al., 2021) and HumanEval (Chen et al., 2021) benchmarks. HumanEval comprises 164 Python problems, validated through test cases to evaluate the code produced by CodeLLMs in a zero-shot setting. Similarly, MBPP features 500 problems assessed in a few-shot setting. Further, MBPP+"}, {"title": "3.2 Main Results", "content": "To validate the effect of DPO training, we compare the performance of CodeLLMs that use rejection sampling fine-tuning (RFT) (Zelikman et al., 2022; Yuan et al., 2023) and DPO for training, respectively. Specifically, we use the chosen responses from the constructed preference dataset for RFT training and both the chosen and rejected responses from the constructed preference dataset for DPO training. All training hyperparameters can be acquired from the Appendix B. As illustrated in Table 1, while the RFT algorithm does enhance the model's performance, its improvements are not as significant as those realized through the DPO algorithm. This discrepancy arises because, during the optimization process with the DPO algorithm, the model is able to learn from rejected responses, enabling it to avoid generating undesirable patterns during inference and ultimately reduce errors in the generated code snippets."}, {"title": "3.3 On-policy DPO vs. Off-policy DPO", "content": "Recently, there have been some researchers who also applied DPO in the field of CodeLLM. Weyssow et al. (2024) leverage the Magicoder Evol-Instruct dataset (Wei et al., 2023) as the source of queries. For each query, they randomly select four LLMs from fourteen LLMs to answer the coding problem and leverage ChatGPT to rate the four responses. Then they can get the preference dataset according to the rate from ChatGPT and the constructed dataset will be used for DPO training. However, using data generated by other models for DPO training is an off-policy mode. We want to emphasize that whether the preference data for DPO training is on-policy or not is very important. As analyzed by (Lai et al., 2024; Yan et al., 2024), off-policy training may lead to the degradation of the model to be optimized. Therefore, directly using the responses generated by other models for DPO may not be a good choice. The recommended choice is to construct the on-policy dataset using the policy model.\nTo validate the argument, we set up our experiments as follows: We first used CodeQwen1.5 7B and DeepSeek-Coder 6.7B to construct preference datasets, respectively. In the DPO training stage, we exchange the training data between Deepseek-Coder and CodeQwen1.5. In other words, we use data generated by one model to train another model. The performances of CodeLLMs after DPO training are shown in Table 2."}, {"title": "4 Related Works", "content": "4.1 CodeLLMs\nWith the rapid development of LLMs, domain models specifically designed for the field of coding are continuously emerging, providing great convenience for humans. Several pre-trained LLMs have demonstrated significant potential for code generation, including Santacoder (Allal et al., 2023), CodeGPT (Lu et al., 2021), etc. Moreover, CodeLLMs that underwent fine-tuning demonstrated more competitive performance, with standout models including Starcoder (Li et al., 2023), CodeLlama (Roziere et al., 2023), Wizardcoder (Luo et al., 2023), DeepSeek-Coder (Guo et al., 2024), and Qwen-Coder (Hui et al., 2024). However, when compared with the state-of-the-art CodeLLMs such as Claude-3.5-Sonnet (Anthropic, 2024) and GPT-4o (OpenAI, 2024a), the capabilities of these models lag significantly behind. A reason is that these CodeLLMs heavily rely on pre-train and SFT, either lacking alignment or not performing well.\n4.2 Execution Feedback from Code Executor\nReinforcement learning from human feedback has proven to be effective (Achiam et al., 2023), but it is highly labor-intensive. Fortunately, the unique nature of coding tasks allows us to leverage execution feedback from a code executor to evaluate whether the code generated by CodeLLMs meets the problem's requirements. For example, some works use execution feedback from the code executor in inference time, for instance, Zhong et al. (2024) leverage the feedback signal from the code executor to help locate the bug in the code snippet, and iteratively refine the generated code until it passes all test examples. Some works leverage the execution feedback from the code executor to provide a reward signal according to certain rules (Liu et al., 2023; Dou et al., 2024) and then PPO is applied to align the CodeLLMs for better performance. In this paper, we propose an alternative approach to leveraging execution feedback: utilizing it to construct data for DPO training."}, {"title": "5 Conclusion", "content": "In this paper, we highlight that current CodeLLMs primarily focus on the pre-training and supervised fine-tuning stages, while neglecting the potential of the alignment stage. The existing works on using PPO to align CodeLLMs may suffer from the issue of coarse reward definition. Therefore, we propose an approach to further enhance the ability of CodeLLMs by leveraging the execution feedback from the code executor to construct a preference dataset for DPO training. Moreover,"}, {"title": "6 Limitations", "content": "One limitation of this work is that the relatively small number of coding problems available on the internet restricts us to constructing a limited set of preference data pairs for DPO training. Due to this constraint, we were unable to investigate the impact of the size of the training data on the model's final performance during the alignment phase. Future work can be done by exploring synthesizing coding questions for data augmentation."}, {"title": "A The Underlying Reward of DPO", "content": "The DPO algorithm drives that if $\\pi_\\theta$ can maximize the Eq. (1), then the underlying reward can be given by:\n$r(x, y)=\\beta \\log \\frac{\\pi_{\\theta}(y \\mid x)}{\\pi_{0}(y \\mid x)}+C(x)$, (4)\nwhere $C: \\mathcal{X} \\rightarrow \\mathbb{R}$ is a scalar function. $\\pi_\\theta$ is the policy model and $\\pi_0$ is the reference model."}, {"title": "B Hyperparameter Setting", "content": "The hyperparameters used for RFT and DPO are shown in Table 3 and Table 4, respectively."}]}