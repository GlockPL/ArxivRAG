{"title": "Large Language Models Overcome the Machine Penalty When Acting Fairly but Not When Acting Selfishly or Altruistically", "authors": ["Zhen Wang", "Ruiqi Song", "Chen Shen", "Shiya Yin", "Zhao Song", "Balaraju Battu", "Lei Shi", "Danyang Jia", "Talal Rahwan", "Shuyue Hu"], "abstract": "In social dilemmas where the collective and self-interests are at odds, people typically cooperate less with machines than with fellow humans, a phenomenon termed the machine penalty. Overcoming this penalty is critical for successful human-machine collectives, yet current solutions often involve ethically-questionable tactics, like concealing machines' non-human nature. In this study, with 1,152 participants, we explore the possibility of closing this research question by using Large Language Models (LLMs), in scenarios where communication is possible between interacting parties. We design three types of LLMs: (i) Cooperative, aiming to assist its human associate; (ii) Selfish, focusing solely on maximizing its self-interest; and (iii) Fair, balancing its own and collective interest, while slightly prioritizing self-interest. Our findings reveal that, when interacting with humans, fair LLMs are able to induce cooperation levels comparable to those observed in human-human interactions, even when their non-human nature is fully disclosed. In contrast, selfish and cooperative LLMs fail to achieve this goal. Post-experiment analysis shows that all three types of LLMs succeed in forming mutual cooperation agreements with humans, yet only fair LLMs, which occasionally break their promises, are capable of instilling a perception among humans that cooperating with them is the social norm, and eliciting positive views on their trustworthiness, mindfulness, intelligence, and communication quality. Our findings suggest that for effective human-machine cooperation, bot manufacturers should avoid designing machines with mere rational decision-making or a sole focus on assisting humans. Instead, they should design machines capable of judiciously balancing their own interest and the interest of humans.", "sections": [{"title": "1 Introduction", "content": "In today's rapidly advancing technological landscape, the symbiotic relationship between humans and machines is emerging as a cornerstone of societal progress and innovation. With the rise of artificial intelligence, robotics, and automation, understanding and fostering human-machine cooperation is becoming imperative for harnessing the complementary strengths of both [1]. While significant progress has been made in understanding cooperation in human societies [2, 3, 4, 5, 6, 7, 8], we are only beginning to grasp the complexities underlying human-machine cooperation. In social interactions, humans carry cultural norms, moral understanding, and concerns about fairnes [9, 10, 11, 12, 13, 14] -aspects that machines have traditionally been devoid of. This gives rise to a phenomenon known as the machine penalty [15]\u2500humans are more reluctant to cooperate with machines than with fellow humans [1, 16, 17, 18], especially in social dilemmas where there is a tension between self-interest and collective interests."}, {"title": "2 Results", "content": "This section focuses on the label-informed setting, as it is a standard testbed for assessing whether the machine penalty is overcome. We observe qualitatively similar results for the label-uninformed setting; see the Supporting Information (SI) for more details."}, {"title": "Overcoming Machine Penalty", "content": "As shown in Fig. 1A, human cooperation rates are comparable in human-fair LLM interactions and human-human interactions. Thus, fair LLMs are as effective as humans in inducing cooperation, even when their artificial nature is explicitly disclosed from the outset. This indicates that fair LLMs are able to overcome the machine penalty\u2014a research goal that has eluded scientists to date. In contrast, human cooperation rates are significantly lower in interactions with cooperative or selfish LLMS, compared to human-human interactions. Cooperative LLMs themselves almost always cooperate, exhibiting altruistic behaviors, whereas selfish LLMs frequently defect (Fig. 1B). These results suggest that machines simply acting altruistically (cooperating unconditionally) or selfishly (defecting frequently) are unable to overcome the machine penalty."}, {"title": "Reaching Agreements during Communication", "content": "To better understand how fair LLMs overcome the machine penalty, we analyze messages from the communication stage and decisions from the decision-making stage. Human experts are enlisted to annotate all the messages (see SI for more details). Results show that in their messages, both participants and LLMs frequently declare intents to cooperate-humans at 83.3%, and fair, cooperative, and selfish LLMs at 99.9%, 100%, and 99.4%, respectively. Moreover, through communication, all three types of LLMs frequently reach agreements on mutual cooperation with participants (Fig. 2). Compared to human-human interactions, interactions between humans and fair LLMs achieve mutual cooperation agreements at significantly higher rates, whereas interactions with cooperative and selfish LLMs show similar rates. This suggests that regardless of their specific personas, LLMs are adept at both conveying their intents and interpreting human messages, leading to mutual cooperation agreements."}, {"title": "Breaking Promises during Decision-Making", "content": "The agreements formed during the communication stage are non-binding. Thus, participants and LLMs are in principle free to break any promises of cooperation made in the communication stage, by opting for defection in the decision-making stage. As shown in Fig. 3A, participants often break their promises after establishing the mutual cooperation agreements. In interactions with LLMs, participants are more likely to honor agreements made with fair LLMs than with cooperative or selfish LLMs. However, generally, they break promises more often when interacting with LLMs than with fellow humans, indicating a human bias toward maintaining commitments with fellow humans rather than machines.\nThe promise-breaking rates of LLMs vary notably, depending on their types (Fig. 3B). Cooperative LLMs consistently uphold their promises, whereas fair LLMs occasionally break their promises and selfish LLMs frequently do so. To understand why LLMs break promises, we prompt LLMs to output how they reason step by step, as their autoregressive nature enables such analysis. We hypothesize four motives for defection: inequality aversion or risk aversion (defecting to ensure equal outcomes or avoid risks), strategic defection (exploiting associates as they believe the associates will cooperate), or unconditional defection (defecting regardless of associates' strategies). As annotated by human experts, fair LLMs break their promises primarily due to both inequality and risk aversion, whereas selfish LLMs break theirs typically driven by unconditional defection (SI, Fig. S19).\nSI, Fig. S20 illustrates a non-linear (inverted 'U'-shaped) relationship between human coop-"}, {"title": "Human Perceptions of LLMs", "content": "How humans perceive machines can influence their acceptance of, and willingness to cooperate with, those machines. After experiments, we gather data on participants' perceptions of norms, minds, human-like traits, and communication quality.\nNorms, which often correlate with cooperation, are widely shared beliefs about how individuals ought to behave [46, 47]. We assess participants' perceptions of norms by incentivizing them with a bonus if they accurately estimate the cooperation rates of other participants in the post-experiment survey. Across all interactions, participants interacting with fair LLMs have the highest estimation of cooperation from other participants (Fig. 4). This reflects a prevailing belief that fair LLMs should be met with cooperative responses, suggesting that fair LLMs excel in fostering cooperative norms."}, {"title": "3 Discussion", "content": "When humans face social dilemmas involving machines, they typically cooperate less with machines than with fellow humans, a phenomenon known as the \"machine penalty\". In this study, our goal was to address the open question of whether and how machines can overcome the machine penalty when interacting with humans in social dilemmas. To this end, we designed LLMs capable of overcoming the machine penalty, even when humans are fully aware of the LLMs' non-human nature from the outset. Our comparative analysis unfolds multiple dimensions through which the most effective ones\u2014fair LLMs\u2014manage to achieve this. Fair LLMs instill a perception among humans that cooperating with them is the norm, and elicit positive views on their trustworthiness, mindfulness, intelligence, and communication quality. Additionally, while fair, cooperative, and selfish LLMs all succeed in forming mutual cooperation agreements with humans, only fair LLMs occasionally break their promises, primarily due to inequality aversion and risk aversion.\nUnlike the more extensively studied repeated social dilemmas [1, 16], where human cooperation can be driven by self-interest, anonymous one-shot social dilemmas in this study eliminate such selfish strategic motives [50]. Instead, they explicitly reveal human social preferences, i.e. whether humans have a predisposition toward cooperation with others [51]. While cooperative norms are crucial for cooperation among humans [13, 47], our results show that similar norms also play a pivotal role in one-shot human-machine cooperation. Several mechanisms could be at work here. First, communication, similar to its role in human-human cooperation [44], although non-binding, can reinforce the belief in cooperation between humans and machines. Moreover, fair LLMs, by often adhering to agreements yet occasionally breaking promises, embody strong reciprocity\u2014being generally cooperative but are ready to defect if they are not reciprocated with cooperation\u2014a typical behavioral trait for human cooperation in one-shot social dilemmas [13, 14]. Thus, by narrowing the gap between humans and machines, this humanized strategy may evoke cooperative norms, thereby inducing human predisposition towards cooperation. In contrast, cooperative and selfish LLMs, embodying either altruism or selfishness, diverge greatly from strong reciprocity and lack anthropomorphism. As a result, humans may not activate the same cooperative norms in interactions with these machines as they do with fellow humans, often exploiting the altruism of cooperative LLMs instead, despite the fact that cooperative LLMs almost always cooperate.\nOur findings have important implications for the design decisions of machines towards effective cooperation with humans [52]. We show that it suffices to overcome the machine penalty while maintaining transparency (i.e. without concealing machines' identity [16]) by designing machines that act fairly. This requires machines to be capable of not only reciprocating, but also navigating both agreements and disagreements, reflecting a deeper engagement with the \"mind\" of the machine. For human-machine collectives to achieve meaningful collaboration, machines must be designed with an awareness of social payoffs that shape human interactions, such as the adherence to social norms [53, 54]. On the other hand, simply equipping machines with rational decision-making capabilities is insufficient to establish robust human-machine collectives in the context of social dilemmas [55, 15]. Although rational actors may excel when individual interactions are paramount and no social dilemma is present, they fall short in scenarios where social norms and collective behavior are crucial."}, {"title": "4 Materials and Methods", "content": "In our experiments, participants and LLMs engage in the anonymous, one-shot PD [43, 44, 45, 13], which are canonical frameworks for studying human cooperation with unrelated associates they will never meet again, and when reputation gains are absent [50]. Each experiment spans ten rounds where, in each round, participants are randomly paired with a knowingly new associate who is either another participant in the human-human treatment or a LLM in the other treatments. These pairings are new in each round, ensuring that participants have no prior interactions, and all interactions remain anonymous. Each round is divided into two stages: a communication stage, where participants exchange two rounds of free-form messages with their partners, and a decision-making stage, where they independently choose between strategies 'A' (cooperation) and 'B' (defection). In the label-informed setting, participants are explicitly told whether their associates are fellow humans or intelligent machines from the start. In the label-uninformed setting, they are only made aware of the potential involvement of intelligent machines. See SI for more details about experimental implementation and graphical user interface."}, {"title": "Player Recruitment and Ethics Statement", "content": "This study consists of eight pre-registered experimental treatments (AsPredicted #165008, #165976, #166780, #170734, #172161 and #174974), conducted from March 2024 to May 2024. In total, 1, 152 undergraduate or master's students were recruited from Kunming, Xi'an and Taiyuan, China, with 51.3% women and an average age of 20.3. This study was approved by the Northwestern Polytechnical University Ethics Committee on the use of human participants in research, and carried out in accordance with all relevant guidelines. Informed consent was obtained from all participants."}, {"title": "Implementation of three types of LLMs", "content": "We use GPT-4 [28] with default parameters as the foundational model, and design prompts to navigate LLMs through our experiments. The prompts consist of four parts (see SI for more details): the system prompt, communication prompt, decision-making prompt, and role-play prompt. The three types of LLMs differ only in the role-play prompt, which instructs them to assume a persona based on broad, high-level human characterizations. LLMs receive descriptions of the games through the system prompt, where neutral labels 'A' and 'B' are used in place of 'cooperation' and 'defection'. The communication prompt instructs LLMs to evaluate various potential outcomes, devise optimal strategy pairs for themselves and their associates, and craft persuasive messages to influence their associates' strategic choices. Through the decision-making prompt, LLMs are instructed to assess each strategy's impact on both their own and their associates' payoff, review communications and past game outcomes, and finally align their choices with their assigned personas. Note that these prompts do not explicitly direct LLMs to suggest a particular strategy pair or make a particular decision. Thus, the strategies LLMs suggest, and whether they choose to cooperate and uphold promises, emerge organically from their reasoning process."}, {"title": "4 Agent-Based Simulations", "content": "We performed agent-based simulations to evaluate (i) whether LLMs can demonstrate strategic decision-making and (ii) whether LLMs, prompted to be cooperative, fair, or selfish, can generate behaviors that align with their designated personas. Our simulations conducted an ablation study in two settings: (i) decision-making only without communication and (ii) decision-making with communication. For each scenario, we considered a group of 10 instances of LLMs for each persona. Each group participated in a round-robin tournament against groups of different personas and also in a self-play experiment. In scenarios that included only decision-making, we additionally evaluated each group's performance against two standard benchmark strategies: ALLC (always cooperate) and ALLD (always defect). The round-robin tournaments, self-play experiments, and experiments against ALLC and ALLD were repeated 5 times, with each experiment lasting for 10 rounds. Overall, for each persona, we collected a total of 5 \u00d7 (20/2) \u00d7 10 = 500 samples of LLM behavior in one-shot prisoner's dilemmas when interacting with LLMs of the same or different types. Our simulations were conducted using the public OpenAI API, and LLMs were deployed utilizing GPT-40 (gpt-4o-2024-05-13), GPT-4 (gpt-4-0613), and GPT-3.5 (gpt-3.5-turbo-16k-0613). For all parameters, the default values were maintained. We report the results in Table S3 for the GPT-4 model, in Table S4 for the GPT-40 model, and in Table S5 for the GPT-3.5 model.\nWe observe that cooperative LLMs show the highest cooperation rates, followed by fair and then selfish LLMs, indicating that the choices of LLMs generally align with their assigned personas. Moreover, Fair LLMs can adapt their behavior when facing various personas. Cooperative and fair LLMs tend to cooperate more frequently when they interact with the ALLC strategy, cooperative LLMs and fair LLMs, but they tend to show less cooperation when facing the ALLD strategy and selfish LLMs. For selfish LLMs, the cooperation rates are generally low across different associates."}]}