{"title": "Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner", "authors": ["Aizierjiang Aiersilan"], "abstract": "Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model (LLM) translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at https://ezharjan.github.io/AutoSceneGen.", "sections": [{"title": "Introduction", "content": "Efficiently evaluating autonomous vehicles (AVs) under diverse real-world challenges on a limited budget is crucial for ensuring their safety and sustaining long-term growth in the AV industry. While existing works on vehicle or pedestrian trajectory prediction, such as TrafficPredict (Ma et al. 2019), Pishgu (Alinezhad Noghre et al. 2023), TraPHic (Chandra et al. 2019), and MSRL (Wu et al. 2023), rely on real-world datasets like ApolloScapes (Huang et al. 2018), NGSIM (Kovvali, Alexiadis, and Zhang PE 2007), UCY (Lerner, Chrysanthou, and Lischinski 2007), and ETH (Pellegrini et al. 2009), which may not fully capture the complexities of modern trajectory prediction scenarios involving both AVs and non-AVs, our custom virtual dataset, generated using our framework \"AutoSceneGen\", produces improved prediction results. Most importantly, generating and synthesizing data using our prompt-based, configurable, and AI-driven AutoSceneGen framework is more cost-effective and efficient than collecting real-world datasets such as Argoverse2 (Wilson et al. 2023), Waymo (Ettinger et al. 2021), Round (Krajewski et al. 2020), Ind (Bock et al. 2020), nuScenes (Caesar et al. 2020), Argoverse (Chang et al. 2019), Highd (Krajewski et al. 2018), and ApolloScapes. Our framework automatically generates datasets with real-world features and provides easy control over dataset heterogeneity through scenario descriptions, especially for safety-critical scenarios.\nStatus quo. Despite the challenges and inefficiencies in the testing phase of AVs, significant progress has been made in optimizing the testing of individual modules, such as motion planners. However, most efforts have focused on evaluating trajectory prediction models in simulation environments (Shet et al. 2023; Li et al. 2023). While simulations reduce testing costs related to budget and safety, creating realistic and logically specific scenarios often requires substantial human effort, though still less than testing AVS on real roads. Additionally, safety evaluation is typically performed module-by-module, demanding considerable resources to create safety-critical scenarios within simulators. Scenario-based evaluation methods have largely focused on coverage, unsafe scenarios, and indicator-estimation tests (Jesenski et al. 2019; Amersbach and Winner 2019). Recent advancements in scenario generation prioritize data-driven methods (Sun et al. 2024), with generative models trained on domain-specific data for traffic scenario generation (Tan et al. 2023). However, these methods struggle with low-resource scenarios, such as corner cases and accidents, due to limited data availability. There is a clear need for a universal, general, and budget-conscious framework that enhances traffic scenario heterogeneity automatically through scenario descriptions. This would expedite the simulation and testing process. While advanced AI tools, like large language models (LLMs), show promise, they often require extensive post-processing to correct inaccuracies or generate overly generic results. Our framework addresses these issues by automating complex scenario generation in a cost-effective manner, overcoming the limitations of low-resource scenarios and optimizing the use of simulator functionalities to ensure scenarios are diverse, realistic, and reflective of challenging real-world situations, as highlighted by Bogdoll et al. (2021).\nContributions. Scenario generation has traditionally been a manual process requiring significant human effort. However, with the advancement of LLMs, there is now an opportunity to leverage AI to efficiently generate specific traffic scenarios. Building on prior research, the main contributions of this study are as follows:\n\u2022 A universal, general, and cost-effective framework, \"AutoSceneGen\", is proposed to automatically enhance the heterogeneity of traffic scenarios through scenario descriptions, thereby accelerating the simulation and testing process.\n\u2022 AutoSceneGen leverages in-context learning (ICL) of LLMs, eliminating the need for training or fine-tuning generative models for scenario generation tasks.\n\u2022 The scenarios generated by AutoSceneGen were demonstrated to produce better datasets, leading to improved training results for motion planners.\n\u2022 AutoSceneGen automatically categorizes the generated scenarios based on their descriptions, eliminating the need for downstream annotation and facilitating the training of motion planners in open-world scenarios.\n\u2022 AutoSceneGen is a modular framework with dynamic components, allowing easy substitution of its generative model and simulation engine used for scenario generation and data collection."}, {"title": "Related Work", "content": "Scenario Generation and Gaps\nExisting works, such as LCTGen (Tan et al. 2023), CSG (Xinxin, Fei, and Xiangbin 2020), and TrafficGen (Feng et al. 2023), focus on generating specific traffic scenarios but often rely on real-world data or are limited by the functionalities of toolkits like ScenarioNet (Li et al. 2024a). In contrast, AutoSceneGen bridges this gap by enabling users to efficiently collect AI-generated scenarios without relying on real-world data.\nThe concept of a \"scenario\u201d revolves around the \"logic\" governing objects within a scene. Go and Carroll (2004) defined a scenario as a detailed account involving actors, background details, objectives, and sequences of actions. In Autonomous Driving (AD), testing traditionally focuses on perception, motion planning, and decision-making modules (Huang et al. 2016). Simulators such as CARLA (Dosovitskiy et al. 2017) and AirSim (Shah et al. 2018) have been developed to support these tests. Goyal et al. (2023) proposed a method to configure a scenario and automatically generate similar variants, while K\u00f6nig et al. (2024) introduced a formal model for test case generation and simulation verification. However, these approaches rely on abstract models, making them less scalable than prompt-based generation methods.\nSeveral studies have explored augmenting traffic data and generating rare objects using ChatGPT (Alam 2020; Xinxin, Fei, and Xiangbin 2020). In this context, automatic scenario generation refers to creating scenarios from user prompts without detailed manual intervention. Zhang et al. (2023) reviewed the absence of automatic pipelines for safety-critical scenario generation in AD, highlighting challenges such as fidelity, efficiency, diversity, controllability, and transferability (Ding et al. 2023). For example, Yu et al. (2020) and Sun et al. (2021) addressed challenges in generating safe lane-changing and adversarial cases, respectively."}, {"title": "In-Context Learning", "content": "Do LLMs learn new tasks during inference, or do they simply recognize patterns from training? Mann et al. (2020) proposed that ChatGPT-3 can acquire new tasks through ICL, where task examples are embedded in prompts. They questioned whether models genuinely learn or merely recall familiar patterns, noting that synthetic tasks like word scrambling might be learned anew. However, doubts persist about whether LLMs truly understand prompts. For instance, Webson and Pavlick (2021) found that models performed similarly with irrelevant prompts, suggesting that improved performance might not equate to genuine comprehension. Min et al. (2022) argued that ICL depends more on input-label mapping, text distribution, and format than on actual ground-truth demonstrations, implying that ICL is ineffective without pre-existing input-label relationships. They also showed that ground-truth outputs may not be essential for tasks like generation. Their findings on NLP benchmarks leave unresolved questions about ICL's effectiveness in open-set tasks, its learning capacity during in-ference, and challenges like input-label mapping extraction. Additionally, Dai et al. (2022) suggested that ChatGPT-3's ICL might function through implicit gradient descent as a meta-optimizer.\nThe core basis of the AutoSceneGen leverages the ICL capability of LLMs to generate optimized scenario configurations from given templates, simulating these scenarios in batch within a physics-based simulator. The generated scenarios reflect realistic physical interactions, ensuring their relevance for AV safety evaluation. To our knowledge, there is a lack of a universal framework that generates abundant, safety-critical, and realistic traffic scenarios specifically tailored for the safety evaluation of AVs."}, {"title": "Rare Objects in An Open World", "content": "Rare class objects, often overlooked in open-world data, are critical for ensuring model robustness in motion planning and visual detection. Manually creating such scenarios requires significant human effort, including writing detailed configurations, formulating scenario logic, and ensuring compatibility with the chosen simulator. These tasks demand extensive brainstorming and collaboration, particularly for rare and complex scenarios. For example, while an engineer might easily conceive a scenario involving a wrongly-parked vehicle with its doors open, envisioning affiliated events such as a driver exiting the vehicle to chase someone or experiencing a sudden medical emergency-requires additional effort and time for ideation and planning. Such edge cases, though rare, are essential for the safety evaluation of AVs. This study addresses the challenge by leveraging LLMs' ICL capabilities to generate tailored configurations for rare scenarios, streamlining the ideation and scenario creation processes."}, {"title": "Scenario Generation Efficiency", "content": "Advancements in scenario generation have primarily focused on enhancing domain-specific languages such as Scenic (Fremont et al. 2019) and ASAM OpenScenario. However, leveraging LLMs for scenario generation remains relatively unexplored. Li et al. (2024b) introduced a framework utilizing ChatGPT to generate trajectory data, but its raw outputs lack physical realism and consistency with real-world scenarios, requiring active user intervention through Chain-of-Thought prompting (Zhang et al. 2022). TARGET (Deng et al. 2023) have utilized LLMs for generating simulator scenarios by incorporating a domain-specific language to create configurations based on self-defined traffic rules. However, their approach relies on a newly introduced XML-based domain-specific language, requires continuous manual input and interpretation, and has not been systematically evaluated for efficiency or scalability. Similarly, other methods, such as abstract-to-scenario and concrete-to-scenario approaches proposed by Majzik et al. (2019), face limitations in automating scenario creation for accuracy-critical domains like AD. Our framework offers a universal, cost-effective solution to enhance traffic scenario heterogeneity and facilitate the generation of rare, safety-critical scenarios in open-world environments, thereby streamlining the simulation and testing processes."}, {"title": "In-Context Scenario Generation", "content": "Existing efforts in driving scenario generation have primarily focused on creating videos to assess the perceptual capabilities of AVs. Recent advancements in incorporating rare object synthesis into realistic images for driving scenario generation have shown promise, offering valuable insights for improving downstream tasks. However, the exploration of AI-driven content creation has broadened our perspective, moving beyond the traditional concept of a \"video\" as a scenario. The core of a \"scenario\" lies in the underlying logic that governs the existence and interaction of objects within it. Reflecting on past approaches, we sought a more universal method to define the logic behind object interactions for scenario generation, rather than focusing solely on the static objects themselves. This distinction sets our work apart from the scope of AI-generated content (AIGC), which typically involves optimizing workflows or synthesizing objects into images for video creation aimed at training sensors and detectors. However, AIGC requires the training of new models, a common challenge across various domains, highlighting the need for models tailored specifically to the automotive industry to ensure accuracy and control over generated content.\nWhile training a comprehensive model may provide a reliable strategy for generating accurate, domain-specific content, sourcing the data necessary for model training can be challenging. Additionally, motion planners trained on datasets directly collected from the real world may struggle to generalize effectively to unseen tasks, impacting model robustness, especially given the long-tail distribution of open-world cases. Moreover, training models from scratch demands significant time and computational resources, presenting challenges in the context of AD. Therefore, this research aims to explore alternative scenario generation methods that bypass the need for training or fine-tuning AI models, offering a more efficient and scalable approach to generating driving scenarios.\nWith ICL, the need for continuous fine-tuning or training of separate models for generation tasks is largely eliminated. Instead, LLMs equipped with ICL capabilities can be leveraged effectively. These models extend beyond traditional applications, such as language translation, to encompass more novel tasks such as code generation and scenario generation. This section focuses on the methodologies employed by AutoSceneGen, which effectively integrates ICL within the domain of LLMs. The integration of LLMs is anticipated to enhance both the efficiency and controllability of AutoSceneGen, offering a more streamlined and adaptable approach to scenario generation."}, {"title": "System Architecture", "content": "As shown in Figure 2, the system architecture consists of key components for processing scenario descriptions, which can be provided by the user or extracted from images using a vision-language model. A filtering process ensures simulator compatibility by replacing incompatible terms with appropriate alternatives. For example, in the description \u201cin stormy weather,\u201d the term \u201cstormy\u201d is identified as incompatible and replaced with \"rainy.\u201d This replacement is performed using a predefined replacement dictionary derived from the simulator documentation, as detailed in Algorithm 1.\nThe user's scenario description is combined with few-shot learning examples within the framework. Developers adapting the framework to their specific domain need to create few-shot learning exemplars based on simulation needs. The process of constructing these exemplars depends on the chosen LLM: if all relevant APIs and examples are included in"}, {"title": "Algorithm 1: Filter for Scenario Descriptions", "content": "Input: Raw Scenario Description D\nOutput: Final Scenario Description D'\n1 for each token ti \u2208 D do\n2\tif t \u2208 SimulatorDocument then\n3\tti\u2190 SimulatorDocument(ti);\n4\telse\n5\t\tif ti\u2208 Wordsincorrect \u222a Punctuationswrong then\n6\t\t|_ Correct ti;\nthe LLM's train set, no additional exemplars are required. However, if the necessary documentation is not part of the train set, one-shot or few-shot exemplars need to be pre-constructed to meet the simulation requirements. The LLM then generates the configuration by leveraging both the scenario description and the ICL examples to ensure an accurate simulation."}, {"title": "Algorithm 2: Validator for Generated Configuration", "content": "Input: Generated configuration CLLM\nOutput: Validated code F\n1 Extract code from CLLM to get F;\n2 for each term ti \u2208 CLLM do\n3\tif ti \u2209 SimulatorDocument then\n4\t\tSearch for synonym tsyn;\n5\t\tif tsyn \u2208 SimulatorDocument then\n6\t\t\tReplace ti with tsyn;\n7\t\telse\n8\t\t\tRemove ti;\n9 F\u2190 CLLM \\ {comments, explanations};\nPrior to execution, the validator (as detailed in Algorithm 2) checks each API call against a documented list, replacing unsupported terms with approved synonyms (e.g., replacing \"Weather.Storm\u201d with \u201cWeather.Rain\" in CARLA). If no replacement is found, the term is ignored to prevent errors, ensuring the calls align with the simulator's capabilities. The validator also ensures that the final configuration includes only relevant code, removing any extraneous content such as explanations and comments, and verifies adherence to the syntax required by the simulator.\nFinally, the simulator is launched to run the generated configuration files, ensuring seamless execution within the simulator itself or on connected digital twin platforms. Our framework automates AV evaluation by efficiently generating traffic scenarios from user-defined descriptions, ensuring cost-effective simulation across diverse open-world environments.\nInput. The user provides scenario descriptions.\nProcess. The scenario descriptions are combined with simulator-specific examples and preprocessed to ensure clarity and compatibility. The chosen LLM then processes the input. The validator checks the generated configuration for adherence to the simulator's syntax and grammar, ensuring flawless execution.\nOutput. A scenario being simulated, provided by an executable configuration file, which is used by the simulator in either its purely virtual environment or on a digital twin platform connected to it."}, {"title": "LLMS & ICL", "content": "Since LLMs exhibit ICL, they can handle tasks they haven't been explicitly trained on without the need for retraining or fine-tuning, allowing them to perform well on specialized tasks using a few-shot learning approach.\nInput. In the scenario generation process, the input consists of prompts (scenario descriptions). Additionally, to adapt the model to a specialized task, ICL examples that match the desired range of outputs must be provided alongside the scenario description as input to the LLMs.\nOutput. The response may include essential scenario elements directly usable by the simulator, as well as explanations that cannot be executed. To refine the response, two approaches are considered: manually scripting to isolate executable code or using the LLM to automatically extract only the code. It is generally believed that with proper ICL examples, the LLM will output only executable code without additional information. However, in the experiment, we found that even when we accentuated the requirement of \"do not generate any explanations or comments except code\" to the chosen LLMs, the output may still contain phrases like \"Here is the result....\u201d Additionally, considering that the code generated by LLMs undergoes grammar validation in the subsequent process, useful content extraction from its responses can also be accomplished through a manually scripted validation procedure. This approach ensures that the input prompt remains streamlined while preserving tokens for efficient processing."}, {"title": "Evaluations", "content": "The datasets generated using the AutoSceneGen can be extensive, with its simulator-dependency offering significant flexibility and scalability, which allows data to be collected for a wide range of scenarios as long as the simulator supports the required functions. By running the scenarios generated by the framework, diverse datasets can be efficiently created for various purposes and different applications. To evaluate AutoSceneGen, we utilized existing approaches for trajectory prediction. In the first step, we replaced their real datasets with our own; in the second step, we combined their datasets with ours. Finally, we tested exclusively with our own data. This allowed us to determine whether our data could achieve results comparable to those obtained with the original datasets. As an example, we used AutoSceneGen with GPT-4 (Achiam et al. 2023) as the selected LLM to generate 125 traffic scenarios, each based on a short prompt that described the scenario we aimed to generate. To evaluate how our work can expedite the safety evaluation process for AVs, we selected a subset of safe scenarios from the 125 scenario configurations, then loaded them into the simulator to simulate the generated scenarios. Next, we used the data collector to gather data in the required format and style, based on the comparison results that we would later use for evaluation. By directly replacing only the train set of the target dataset, we initiated the evaluation process of the scenarios collected through the AutoSceneGen framework using existing trajectory prediction approaches. Finally, after collecting the data in the required format and training the model from scratch to observe the trajectory prediction results, we directly added our dataset, collected through the AutoSceneGen, to the original training set used primarily for the evaluation of trajectory prediction methods by their proposers.\nNot only can AutoSceneGen achieve the massive and efficient collection of critical scenarios, but the datasets it generates are also logically realistic and heterogeneous. In the experiment, we ran 41 AI-generated scenarios with varying numbers of vehicles and pedestrians to collect as much data as possible under different circumstances within a single scenario logic, using the official CARLA map from Town01 to Town05.\nWe evaluated our dataset using average displacement error (ADE) and final displacement error (FDE). We replicated TrafficPredict's results (Ma et al. 2019) with their ApolloScapes dataset, which features curated trajectory data from real road scenarios. For TRAF (Chandra et al. 2019), lacking access to their full dataset, we used ApolloScapes with the \"TraPHic\" method. This approach allowed us to assess the dataset collected from AutoSceneGen across different methods.\nWithout modifying the original trajectory prediction network, our dataset achieved superior results with reduced displacement error for each traffic participant type, as shown in Table 2. In various epochs, the dataset collected from AutoSceneGen demonstrated the highest accuracy in trajectory prediction, as illustrated in Figure 3-(a). Moreover, combining our dataset with ApolloScapes improved overall performance, enhancing all trajectory prediction metrics by incorporating diverse scenarios and extensive data, as depicted in Figures 3(b), (c), and (d).\nBesides ApolloScapes, several experiments were also conducted on TraPHic, Pishgu (Alinezhad Noghre et al. 2023), and MSRL (Wu et al. 2023) using other datasets in a similar manner. Since the NGSIM dataset contains only vehicle trajectories and the ETH and UCY datasets contain only human trajectories, we excluded extra calculations on unseen participants in each comparison. For VIRAT (Oh et al. 2011) and ActEV (Awad et al. 2020), although they include three major event types (including vehicles), we focused only on the annotated pedestrian trajectories to control for variables when comparing under Pishgu. Table 3 presents a comparison of the results between our dataset and NGSIM, ETH/UCY, and VIRAT/ActEV. Our dataset, collected via AutoSceneGen using CARLA 0.9.13, includes 264 pedestrian trajectories (1,848k frames) and 770 vehicle trajectories (15,400k frames), with the vehicle trajectories replacing and augmenting NGSIM data."}, {"title": "Limitations", "content": "Our framework has several limitations, including performance, simulator integration, and data availability, as well as regulatory constraints due to varying global traffic laws. Developers may need to create custom virtual environments or choose different simulators if those mentioned in this paper do not meet their regulatory requirements. The framework's effectiveness depends on the capabilities of both LLMs and the simulator, particularly for tasks such as scenario generation and accident reconstruction. If the chosen LLM lacks ICL capabilities or the simulator cannot handle complex scenarios, the framework's utility is significantly reduced, making it difficult to generate corner cases. Like many AIGC toolkits, the explainability of the generated scenario logic is limited to the explainability of the scenario descriptions and ICL examples, especially at higher abstraction levels. This limitation arises from the chosen LLMs, not from the framework itself. To address this, the framework enhances explainability through detailed configuration files stored after validation, allowing users to trace the rationale behind generated scenarios by comparing input prompts with generated configurations.\nLast but not least, if the example data for ICL is insufficient, some simulators may not respond effectively, necessitating post-revision of the responses. Most example data is sourced from official documentation or the open-source community of the chosen simulator, but availability and usage rights are not always guaranteed."}, {"title": "Conclusions", "content": "AutoSceneGen framework is designed for the generating of traffic scenarios using foundation models. The resulting scenarios can be simulated within virtual environments in simulators, within the real world, or even both concurrently. The framework achieves the improvement of the work efficiency on generating sufficient safety-critical scenarios conveniently for testing AVs in an open world, as well as the convenience of traffic accident reconstruction. As highlighted by Kalra and Paddock (2016), AVs face the challenge of needing to cover vast distances \u2013 potentially hundreds of millions to even billions of miles \u2013 to establish their reliability in terms of minimizing fatalities and injuries. AutoSceneGen is particularly advantageous for enhancing the reliability testing of AVs slated for real-world deployment. It achieves this by harnessing the power of foundation models to simulate extensive arrays of traffic scenarios, thereby providing a robust testing environment for AVs."}]}