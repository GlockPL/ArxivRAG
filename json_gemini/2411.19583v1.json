{"title": "Solving Rubik's Cube Without Tricky Sampling", "authors": ["Yicheng Lin", "Siyu Liang"], "abstract": "The Rubik's Cube, with its vast state space and sparse reward structure, presents a significant challenge for reinforcement learning (RL) due to the difficulty of reaching rewarded states. Previous research addressed this by propagating cost-to-go estimates from the solved state and incorporating search techniques. These approaches differ from human strategies that start from fully scrambled cubes, which can be tricky for solving a general sparse-reward problem. In this paper, we introduce a novel RL algorithm using policy gradient methods to solve the Rubik's Cube without relying on near solved-state sampling. Our approach employs a neural network to predict cost patterns between states, allowing the agent to learn directly from scrambled states. Our method was tested on the 2x2x2 Rubik's Cube, where the cube was scrambled 50,000 times, and the model successfully solved it in over 99.4% of cases. Notably, this result was achieved using only the policy network without relying on tree search as in previous methods, demonstrating its effectiveness and potential for broader applications in sparse-reward problems.\nKeywords: Rubik's cube, Reinforcement learning, Sparse reward", "sections": [{"title": "1. Introduction", "content": "The Rubik's Cube is a classic combinatorial puzzle that presents unique and significant challenges for artificial intelligence and machine learning [1-3]. With an enormous number of possible states, only a single state represents the solved configuration, making it particularly difficult for reinforcement learning algorithms to tackle. The challenge lies in the fact that, during training, the moves taken by the agent's policy are exceedingly unlikely to reach the solved state. This lack of reinforcement for long stretches of exploration often leaves the agent struggling to discover any rewarded state, making it difficult to learn an effective strategy.\nThis highlights a key issue in sparse reward problems within reinforcement learning, where the absence of frequent feedback severely hinders the agent's ability to learn [4-6]. Without enough guidance, it becomes much harder for the agent to develop useful policies. The Rubik's Cube serves as an ideal testbed for addressing these challenges, and developing reinforcement learning algorithms capable of solving this puzzle could offer broader insights into how to handle sparse-reward environments in other domains."}, {"title": "2. Methods", "content": "While classical methods for solving the Rubik's Cube have existed for decades [7], it wasn't until the work of Agostinelli et al. [3], who developed the DeepCube algorithm, that an AI system using reinforcement learning [8-13] was able to solve the Rubik's Cube from any starting configuration. In their follow-up work, DeepCubeA [2], they implement Deep Approximate Value Iteration (DAVI), which iteratively estimates the cost-to-go for states near the solved state and propagates this information outward to states further away. This process involves scrambling a solved cube multiple times and collecting trajectories that start from the solved configuration. While this approach allows DeepCube to effectively solve the Rubik's Cube, it contrasts with the way humans typically solve the puzzle. Humans start with a fully scrambled cube, without prior knowledge of states close to the solved state. We observe states far from the solution and gradually develop an understanding of the underlying structure and relationships between states, even when they are distant from the solved state.\nIn this paper, we address this issue by developing a new reinforcement learning approach that uses policy gradient methods to solve the Rubik's Cube without sampling states from a solved configuration. Instead, we continuously sample states from a fully scrambled cube and build up rewards based on the underlying distance patterns between states. Unlike methods mentioned above, which rely on search methods like Monte Carlo Tree Search (MCTS), our approach requires no such search techniques. Using this method, we successfully solved the 2x2x2 Rubik's Cube with a success rate of 99.4% across 50,000 test cases."}, {"title": "2.1. State and action spaces", "content": "A Rubik's Cube consists of a number of stickers, each uniquely associated with a specific position on the cube. In general, for any given Rubik's Cube, all stickers can be encoded as elements of a set of numbers. The state of the Rubik's Cube is then defined as a vector, where each element corresponds to the encoded value of a sticker, recorded in a specific order. This vector is denoted as s, with its dimension N representing the total number of stickers. An action performed on a Rubik's Cube involves scrambling the cube hence rearranging specific stickers according to a predefined rule. Using the state vector representation, an action a applied to a given state s can be defined as a function $a (s) = \\Gamma s$, where $\\Gamma$ is a permutation matrix of size N \u00d7 N, representing the specified rearrangement rule. The action space H of a Rubik's Cube is defined as the set of all possible actions, each determined by a specific rearrangement rule. Applying an action $a_{i}$ into a given states, results in a new state $s_{i} = a_{i}(s_{0})$. Starting from an initial state $s_{0}$, repeated applications of actions will cause the Rubik's Cube to transition through a sequence of states. The set of all states that can be reached from $s_{0}$ is defined as the state space derived from $s_{0}$, denoted as S. Consequently, the topological structure of the Rubik's Cube can be represented as a state graph, where each node corresponds to a state, and edges represent actions connecting these states. Solving the Rubik's Cube can thus be equivalently formulated as a pathfinding problem in the state graph: given a pair of states (ss, st), the goal is to identify a feasible sequence of actions $(a_{i})_{M}$ that transforms ss into st, with $st = a_{M}(...a_{i}(...a_{2}(a_{1}(ss))))$, where $a_{i} \\in A, i = 1, 2, . . ., M$, with M the length of the action sequence."}, {"title": "2.2. Cost of states pairs", "content": "We define the cost between state pair (ss, st) as the minimal number of scrambles needed from the start state ss to the target state st. It can also be represented as the length of the optimal path which linked the state pair (ss, st) in the state graph."}, {"title": "2.3. NX Module", "content": "The core objective of the NX Module is to collect states far from the solved state and train a model to accurately estimate the cost between any pair of states based on their sticker representation. In this module, a neural network $C_{p}(ss, st)$ termed ChaseNet, parameterized by p, was trained to estimate the cost between pairs of states (ss, st) using samples generated from scrambled cubes. During the warmup phase, state pairs (ss, st) are created by applying random actions sampled from the action space, with the restriction that no action is repeated more than three times in a row, as this would return the cube to a prior state, producing incorrect labels. We emphasize that the ss collected in the trajectories are randomly scrambled, without any restriction on their distance from the solved sg, unlike in previous works. In the training phase, state pairs are generated by applying actions sampled from the policy network $p_{\\theta}(a|s)$, allowing the model to refine its predictions based on learned policies."}, {"title": "2.4. Env Module", "content": "The environment is wrapped into the Env Module. During interaction with the agent, it takes in the action ai and returns the consequent state si+1 and the reward ri, with i the index within the episode. The reward is formulated by the following:\n$r_{i} = -log_{b} C_{p}(S_{i+1},S_{g})$\nwhere sg is the resolved state and b = 1.2 is a base specified to rescale the direct predicted cost of state pair (ss, Sg). Specifically, when the goal state is reached, a fixed reward of 100 is assigned."}, {"title": "2.5. Actor Module", "content": "The Actor Module refines the policy network $p_{\\theta} (a|s)$ with Proximal Policy Optimization (PPO) [14] algorithm. Specifically, given the reward returned from the Env Module which is formulated using the cost of the current state to the solved state, the objective function is formulated as following:\n$L (s, a, \\theta_{k}, \\theta) = min \\left(\\frac{p_{\\theta}(a|s)}{p_{\\theta_{k}}(a|s)} A^{p_{\\theta_{k}}}(s,a), g(\\epsilon, A^{p_{\\theta_{k}}}(s,a))\\right)$\nwhere\n$g (\\epsilon, A) = \\begin{cases} (1 + \\epsilon)A & A \\geq 0 \\\\ (1 - \\epsilon)A & A < 0 \\end{cases}$\nThe advantage $A^{p_{\\theta_{k}}}(s,a)$ is the difference between the Q-value $Q^{p_{\\theta}}(s, a)$, the expected return of selection action a in state s, and the value $V^{k} (s)$, the predicted return of state s by the critic network parameterized by in the k-th iteration. e is a hyperparameter that we set at 0.2, as used in the previous paper [14]."}, {"title": "2.6. Network Architectures", "content": "For our comparative study, we developed ChaseNet to predict the cost between state pairs, implementing two distinct neural network architectures: ChaseNet-FC, composed entirely of fully"}, {"title": "3. Results", "content": ""}, {"title": "3.1. Comparison of performance of ChaseNet-FC and ChaseNet-Attention in the warmup phase", "content": "In this work, we focus on the 2x2x2 Rubik's Cube as a challenging yet computationally feasible problem for evaluating our approach. We monitored the loss curves of both ChaseNet-FC and ChaseNet-Attention across multiple epochs. With the training iteration set to J = 1000, the resulting loss curves are displayed in Figure 3.1a, providing a comparative view of the models' convergence behavior. ChaseNet-FC demonstrates a faster convergence rate than ChaseNet-Attention, likely due to its simpler architecture composed solely of linear layers, which enables more straightforward feature extraction and quicker optimization. In contrast, ChaseNet-Attention, which incorporates a sticker-level transformer for capturing more complex spatial dependencies, initially converges more slowly. However, after 1,000 iterations of warm-up training, ChaseNet-Attention achieves a lower final loss value, suggesting that its architecture, though more complex, ultimately captures richer state representations that improve prediction accuracy. We evaluated both ChaseNet-FC and ChaseNet-Attention on an independent test set to measure Spearman's correlation coefficient between their outputs and the true cost values. ChaseNet-FC achieved a coefficient of 0.834, while ChaseNet-Attention reached a higher coefficient of 0.901, indicating that the sticker-level transformer in ChaseNet-Attention provides a more accurate prediction of state-pair costs."}, {"title": "3.2. Solving 2x2x2 Rubik's Cube without searching", "content": "After the warm-up phase of training ChaseNet, we used its predictions to train the policy network via the Proximal Policy Optimization (PPO) algorithm. For comparative analysis, we evaluated the"}, {"title": "4. Discussion", "content": "In this work, we presented a reinforcement learning approach for solving the Rubik's Cube without relying on sampling starting from solved states. Unlike methods like DeepCubeA, which sample states near the solution, our approach learns a solution policy directly from learning the cost patterns of fully scrambled states. By leveraging ChaseNet to accurately estimate state transition costs, the NX Module guides policy optimization effectively from random starting points, aligning with the way humans solve the puzzle by building heuristics from disordered states. Achieving a success rate over 99.4% on the 2x2x2 Rubik's Cube, this method demonstrates the potential to address sparse-reward challenges without complex sampling or search methods.\nDespite these promising results, our approach currently has some limitations. We focused on the 2x2x2 Rubik's Cube to test feasibility within a manageable state space. Scaling to the 3x3x3 cube would require a much larger neural network to estimate costs across a more complex space, posing a key challenge for future work. Furthermore, while the 2x2x2 cube provided an initial testbed, broader testing across different sparse-reward environments will be essential to assess the generalizability and practicality of this method. Applying our approach to various domains could demonstrate its robustness for other sparse-reward tasks where rewards are infrequent or difficult to access.\nIn summary, our approach offers a new method for solving sparse-reward problems by optimizing policies from fully scrambled states, without relying on search or solved-state sampling. These results lay a foundation for developing more flexible and scalable methods, though further testing on complex puzzles and varied environments will be important to confirm its wider applicability."}]}