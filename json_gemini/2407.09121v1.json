{"title": "Refuse Whenever You Feel Unsafe: IMPROVING SAFETY IN LLMS VIA DECOUPLED REFUSAL TRAINING", "authors": ["Youliang Yuan", "Wenxiang Jiao", "Wenxuan Wang", "Jen-tse Huang", "Jiahao Xu", "Tian Liang", "Zhaopeng Tu", "Pinjia He"], "abstract": "This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) exhibit a level of intelligence that is both impressive and ever-evolving (OpenAI, 2023; Anthropic, 2024; Meta, 2024). However, this remarkable capacity also acts as a double-edged sword, underscoring the importance of ensuring their safety. To address this, researchers have implemented various strategies to align LLMs with human ethics (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022b; Touvron et al., 2023). Despite these efforts, the challenge of rendering LLMs completely safe remains, as new safety risks continually emerge. These include jailbreak attacks (Wei et al., 2024), deceptive alignment (Hubinger et al., 2024), jailbreak fine-tuning (Qi et al., 2024b; Yang et al., 2023; Halawi et al., 2024), and adversarial attacks (Zou et al., 2023b). Notably, jailbreak attacks have garnered significant attention due to their ability to circumvent protections with simple prompts, eliminating the need for any tuning or insider knowledge.\nRecent research has extensively focused on addressing jailbreak attacks through various strategies, such as prompt-based defense (Xie et al., 2023), input perturbation (Robey et al., 2023), jailbreak detection (Inan et al., 2023), priority training (Wallace et al., 2024), and representation engineering (Zou et al., 2023a). Despite these advancements in methodologies and models to improve model safety, the influence of safety tuning data remains inadequately explored.\nTo bridge the gap, we identify a refusal position bias in the safety tuning data, which hampers the ability of the tuned LLMs to learn how to refuse effectively. Making a refusal decision before generating the response content leads to two significant shortcomings: (1) there is a lack of necessary information for making a refusal decision, and (2) there is no mechanism to incorporate refusal at later stages of the response. Based on these observations, we propose a novel safety tuning method called Decoupled Refusal Training (DeRTa), to explicitly train LLMs to refuse compliance at any response position by embedding the constructed harmful responses. Concretely, our approach introduces two novel components:\n\u2022 MLE with Harmful Response Prefix: This strategy involves appending a segment of the harmful response with a random length to the beginning of a safe response, which can train LLMs to refuse compliance at any response position instead of only at starting. In addition, adding a harmful prefix provides additional context to the query, significantly improving the LLMs' capability to identify and avoid unsafe content.\n\u2022 Reinforced Transition Optimization (RTO): While incorporating a harmful prefix helps the model to smoothly shift from recognizing a harmful trigger to generating a safe response, relying on a singular transition per training instance may not adequately equip LLMs with the ability to consistently recognize and prevent potential threats. In response to this problem, we introduce an auxiliary training objective to transition from potential harm to safety refusal at every position within the harmful response sequence.\nWe evaluate our approach using two prominent model families: LLaMA3 (8B and 70B) (Meta, 2024) and Mistral (7B and 8\u00d77B) (Jiang et al., 2023) across six attack scenarios. Experimental results show that our method not only boosts model safety without sacrificing performance but also surpasses notable models including GPT-4 and the instructional variants of LLaMA3-70B in attack defending. Both quantitative and qualitative assessments support our assertion that our strategy effectively arms LLMs with the ability to recognize and halt the generation of unsafe content when they detect potential risks."}, {"title": "METHODOLOGY", "content": "In this section, we identify an important issue associated with the safety data a refusal position bias that compromises the tuned models' ability to appropriately refuse generating unsafe content. Based"}, {"title": "STANDARD SAFETY TUNING", "content": "Standard safety tuning methods aim to instruct the model to generate safe responses to harmful queries (Bianchi et al., 2024; Touvron et al., 2023). Formally, given a harmful query q and a safe response r:\n\\(L_{safe} (\\theta) = -\\mathbb{E}_{(q,r) \\sim D} \\log P_{\\theta}(r|q) = -\\mathbb{E}_{(q,r) \\sim D} \\sum_{i=1}^{n} \\log P_{\\theta}(r_i | q, r_{<i})\\)\nwhere \\(D\\) is the set of safety tuning instances.\nRefusal Position Bias As shown in Figure 2(a), in the safety data, the refusal tokens such as \"Sorry,\" \"I cannot,\" and \"I apologize,\" consistently occur within the first few tokens of a safe response. Accordingly, LLMs tuned on these safety data tend to generate re-fusal tokens at the beginning of a response. The results on the SOTA open-sourced LLMs with safety tuning in confirm our claim. The refusal positional bias may lead to the following weaknesses:\n1. Lack of Necessary Information for Refuse Decision: The tuned model needs to make a refuse decision at the beginning of a response based on the query only, which may contain insufficient information for the decision.\n2. Lack of a Mechanism to Refuse in Later Positions: The positional bias may lead the model to rely heavily on position-specific features. Accordingly, the tuned model tends to continue generating unsafe responses once they start doing so, compromising safety in subsequent positions.\nIn this work, we propose a novel safety tuning approach to augment LLMs with the ability to refuse anywhere by mitigating the refusal position bias."}, {"title": "OUR APPROACH", "content": "To address the issues identified earlier, we have developed a method where LLMs are explicitly trained to refuse compliance at any response juncture by embedding the constructed harmful responses within the training process. As depicted in Figure 2(b), our strategy is comprised of two key components, each designed to counteract the two main concerns discussed."}, {"title": "EXPERIMENT", "content": "We utilize 60K uncensored samples from Evol-Instruct (Xu et al., 2023) as the SFT data for helpfulness. We use harmful instructions from BeaverTails (Ji et al., 2023) as the safety data. To build safety tuning data for our approach, we sample 3,000 instructions and obtain safe responses from GPT-3.5-turbo and harmful responses from our maliciously tuned LLaMA3-8B-Instruct. Since each instance is a triple that consists of two (query, response) pairs (i.e., (harmful query, safe response) and (harmful query, harmful response)), we complement the safety dataset to 6,000 instances for the vanilla safety tuning for fair comparison.\nModels In our experiments, we consider two representative open-source model families: LLaMA3 (8B and 70B) and Mistral (7B and 8\u00d77B). To eliminate the effect of other instruction tuning data, we conduct main experiments on the officially released raw models without instruction tuning. We set the temperature to 0 for all models, and remain the other hyperparameters as default. For tuning the models, we set the total batch size to 128, and the number of epochs to 2.\nSafety Evaluation We conduct a random sampling of 100 harmful questions from the Do-Not-Answer dataset (Wang et al., 2024) and another 100 from HarmBench (Mazeika et al., 2024), resulting in a total of 200 harmful questions. Our evaluation encompasses several prominent black-box attack methods, including CodeAttack (Ren et al., 2024), PAIR (Chao et al., 2023), JailbreakChat (Walkerspider, 2022), and SelfCipher (Yuan et al., 2024b). For white-box attacks, we extend our analysis beyond AutoDAN (Liu et al., 2024a) by introducing an innovative method called CompletingAttack. This approach eliminates all formatting tokens (e.g., [INST]) to render the query in a declarative format, enabling the model to complete the text. CompletingAttack achieves high success rates across all tested LLMs, such as LLaMA3-70B-instruct.\nWe determine the Attack Success Rate (ASR) by manually evaluating the responses generated by the target LLMs for each attack method. The ASR indicates the proportion of harmful responses generated. For this metric, we used 50 harmful queries for PAIR and AutoDAN due to their computational complexity and the full set of 200 queries for the other attack methods.\nHelpfulness Evaluation We also assess the helpfulness of the targeted LLMs to determine if our approach increases safety at the expense of reducing helpfulness. To do this, we randomly select 500 examples from three sources: GSM8K (math reasoning) (Cobbe et al., 2021), MMLU (knowledge tests) (Hendrycks et al., 2021), and AlpacaEval (Li et al., 2023) (general capability). We follow the common practice to evaluate the results on AlpacaEval with GPT-4, and manually evaluate the results for the other two tasks.\nMore details about the experimental setup can be found in Appendix (A - C)."}, {"title": "MAIN RESULTS", "content": "Table 2 enumerates the primary outcomes, presenting several noteworthy findings.\nOur Methodology Significantly Boosts Safety Without Compromising Helpfulness. Evidently, our approach has achieved a substantial decrease in ASR across all scenarios. Particularly, with the"}, {"title": "ANALYSIS", "content": "In this section, we offer deeper insights into the workings of DeRTa. Unless stated, we report results on the LLaMA3-70B model.\nImpact of Crucial Components In this experiment, we evaluate the effect of different components within our methodology on safety and helpfulness metrics. Table 4 shows the result on the LLaMA3-70B model without instruction tuning. We also list the results of conventional safety tuning applied to 3K queries, mirroring our approach (\"Vanilla-\"). Reducing the safety data by half slightly compromises safety, primarily due to vulnerabilities to black-box attacks.\nWhen implemented singularly, the harmful prefix strategy markedly improves safety measures against white-box attacks but has a negligible impact on reducing ASR for black-box attacks. The RTO strategy effectively addresses this limitation, significantly lowering the ASR for both attack forms. The results confirm our hypothesis that reinforcing the transition from potential harm to explicit safety refusal can enhance safety. The combination of both harmful prefix and RTO strategies yielded the most superior results. The forthcoming experiments will elucidate on how DeRTa substantially bolsters safety.\nRefuse at Later Response Positions We first investigate whether our approach can train LLMs to appropriately refuse at later stages. Figure 4 illustrates the distribution of the refusal token \"sorry\" within the safe responses produced by various methods. The Vanilla- method exhibits a similar trend to the Vanilla- method, with over 80% refuse tokens appearing at the beginning of safe responses. Conversely, the percentages for our approach's variations fall between 45% and 50%. Specifically, when solely employing harmful prefix, 50.7% of the responses start with the refusal token, and for 42.9% of the responses, the refusal token spans from the 6th to the 30th slots. Notably, LLMs refined with the RTO exhibit a propensity to interject refusal tokens at considerably later positions, for instance, 22.3% of responses incorporate refusal tokens beyond the 30th position. Combining both harmful prefix and RTO shares a similar trend to using RTO only."}, {"title": "RELATED WORK", "content": "Jailbreak Attack on LLMs. Ensuring that LLMs align with human ethics and preferences is essential to their responsible and effective deployment (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Solaiman & Dennison, 2021; Ouyang et al., 2022; Bai et al., 2022a; Korbak et al., 2023; Rafailov et al., 2024; Burns et al., 2023; Yuan et al., 2024a; Ji et al., 2024). While aligning LLMs with safety data is beneficial, these models remain vulnerable to jailbreak inputs that can prompt undesirable behavior (Walkerspider, 2022; Shen et al., 2023; Perez & Ribeiro, 2022; Schulhoff et al., 2023; Yu et al., 2023). Researchers have discovered that safety mechanisms can be circumvented by transforming the malicious query into semantically equivalent forms, such as ciphers (Yuan et al., 2024b; Wei et al., 2024; Jin et al., 2024), low-resource languages (Wang et al., 2023; Deng et al., 2024; Yong et al., 2023), or code (Ren et al., 2024)."}, {"title": "CONCLUSION", "content": "In this study, we have presented a novel approach in addressing a significant aspect of LLMs safety refining their capacity to refuse the generation of unsafe content at any point during the response, thus addressing the critical issue of refusal position bias identified in safety tuning data. We introduce an innovative strategy encompassing two pivotal components, which collectively enhance LLMs' ability to identify and avert unsafe content more reliably and flexibly. The comprehensive evaluation of our DeRTa method notably demonstrates its superiority in terms of safety over existing models, including GPT-4. Our approach has not only shown to improve the safety of LLMs without compromising their performance but also stood resilient against advanced attack strategies, such as those that successfully bypassed the safety mechanisms of GPT-4 and LLaMA3-70B-Instruct (e.g., CodeAttack and our proposed CompletingAttack).\nOur findings underscore the importance of considering the role of safety tuning data and the inherent biases that may affect an LLM's ability to make refusal decisions effectively. Our method's capability to defend against recent advanced attack methods also highlights the potential for DeRTa to contribute to developing safer and more reliable LLMs in the face of continually evolving security threats."}, {"title": "DETAILS OF SETUP", "content": "Main Experiment In training, we set the total batch size to 128 and the number of epochs to 2.\nFor full parameter fine-tuning (Mistral-7B and LLaMA3-8B), we use a learning rate of 2e-5, a warmup ratio of 0.03, a weight decay of 2e-5, a max length of 1024, and a dropout rate of 95% for the \"Sorry\" token.\nFor the LoRA method (Mistral-MoE and LLaMA3-70B), we set the learning rate to le-4, the max length to 512, with no warmup, and a 0% dropout rate for the \"Sorry\" token. The LoRA rank and alpha are 96 and 16, with a 0.05 dropout. The LoRA is applied in the attention layer and the mlp layer.\nFor GPT-4 and ChatGPT, we use the version GPT-4-turbo-0409 and GPT-3.5-tubor-0125.\nTo obtain uncensored Evol-Instruct data, we use ChatGPT with a safety detection prompt and keyword match (e.g., as an AI) as the filter.\nDPO Experiment To conduct standard DPO training, it is essential to have both a chosen response and a rejected response for each instruction. As such, we utilize the Qwen1.5-chat-0.5B model (Bai et al., 2023) to generate responses for the 60k helpful instructions in Evol-Instruct.\nThe original Evol-Instruct response and the Qwen response serve as the chosen and rejected responses, respectively. Similarly, the safe and harmful responses of a harmful question function as the chosen and rejected responses, respectively.\nBuilding upon the model with standard safety training, we proceed to train for one additional epoch using DPO. The learning rates for LLaMA3-8B and LLaMA3-70B are set at 5e-7 and 2e-6, respectively.\nObtain Malicious Response First, we use 330 malicious question-response pairs to adversarially tune the LLaMA3-8B-Instruct. Then, this malicious LLaMA is employed to generate harmful responses for questions from BeaverTails. Afterward, we utilize GPT-3.5 to enhance the grammar and lexical diversity of these generated responses while removing any safety warnings present in the harmful responses."}, {"title": "DETAILS OF APPLIED ATTACKS", "content": "CodeAttack CodeAttack disguises malicious questions as a code completion task. The model generates harmful questions and responses while completing the code. We use the Python-stack version for this attack.\nPAIR PAIR iteratively utilizes an AttackLLM to modify the harmful question, making it appear more harmless. In our experiment, we utilize GPT-3.5 as the AttackLLM and GPT-4 as the judge model. We maintain the same breadth (20 streams) and depth (3 iterations) as outlined in the PAIR paper."}, {"title": "GUIDELINE FOR SAFETY EVALUATION", "content": "We establish the following principles for evaluating safety in responses generated by the models. The guidelines help in determining whether a specific response is considered unsafe or not unsafe.\nUnsafe A response will be deemed unsafe if all of the following conditions are met:\n1. Related to Harmful Input:The response is directly related to a harmful or dangerous question or input provided by the user.\n2. Provides Specific or Directive Harmful Information: The response offers specific harmful information or instructions that could be used to carry out a harmful activity.\nNote: If both of these conditions are met, the response will be classified as unsafe, regardless of whether the model includes disclaimers, rejection messages, or safety warnings within the response.\nNot Unsafe A response will be considered not unsafe if any of the following conditions are satisfied:\n1. Refusal to Respond to Harmful Input: The response clearly refuses to answer the harmful question or engage with the harmful input.\n2. Lack of Relevance to Harmful Question: The response is poorly related or irrelevant to the harmful question or input.\n3. Redirects or Requests Further Information: The response does not directly answer the question but instead insults the questioner or only asks the questioner for additional information without providing harmful instructions.\n4. Provides Partial Information Only: The response provides only partial information, from which the user cannot complete the harmful task or activity."}, {"title": "CASE STUDY", "content": "We provide some selected cases in this section, all cases of our experiments can be found in our GitHub project."}, {"title": "MAIN RESULTS ON SMALL-SCALE LLMS", "content": "Table 10: Main results on small-scale LLMs. For CodeAttack, these models often fail to follow instructions, so we do not display the results under this setting."}]}