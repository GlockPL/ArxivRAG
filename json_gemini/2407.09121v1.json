{"title": "Refuse Whenever You Feel Unsafe: IMPROVING SAFETY\nIN LLMS VIA DECOUPLED REFUSAL TRAINING", "authors": ["Youliang Yuan", "Wenxiang Jiao", "Wenxuan Wang", "Jen-tse Huang", "Jiahao Xu", "Tian Liang", "Zhaopeng Tu", "Pinjia He"], "abstract": "This study addresses a critical gap in safety tuning practices for Large Language\nModels (LLMs) by identifying and tackling a refusal position bias within safety\ntuning data, which compromises the models' ability to appropriately refuse gener-\nating unsafe content. We introduce a novel approach, Decoupled Refusal Training\n(DeRTa), designed to empower LLMs to refuse compliance to harmful prompts\nat any response position, significantly enhancing their safety capabilities. DeRTa\nincorporates two novel components: (1) Maximum Likelihood Estimation (MLE)\nwith Harmful Response Prefix, which trains models to recognize and avoid unsafe\ncontent by appending a segment of harmful response to the beginning of a safe\nresponse, and (2) Reinforced Transition Optimization (RTO), which equips models\nwith the ability to transition from potential harm to safety refusal consistently\nthroughout the harmful response sequence. Our empirical evaluation, conducted\nusing LLaMA3 and Mistral model families across six attack scenarios, demon-\nstrates that our method not only improves model safety without compromising", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) exhibit a level of intelligence that is both impressive and ever-evolving (OpenAI, 2023; Anthropic, 2024; Meta, 2024). However, this remarkable capacity also acts as a double-edged sword, underscoring the importance of ensuring their safety. To address this, researchers have implemented various strategies to align LLMs with human ethics (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022b; Touvron et al., 2023). Despite these efforts, the challenge of rendering LLMs completely safe remains, as new safety risks continually emerge. These include jailbreak attacks (Wei et al., 2024), deceptive alignment (Hubinger et al., 2024), jailbreak fine-tuning (Qi et al., 2024b; Yang et al., 2023; Halawi et al., 2024), and adversarial attacks (Zou et al., 2023b). Notably, jailbreak attacks have garnered significant attention due to their ability to circumvent protections with simple prompts, eliminating the need for any tuning or insider knowledge.\nRecent research has extensively focused on addressing jailbreak attacks through various strategies, such as prompt-based defense (Xie et al., 2023), input perturbation (Robey et al., 2023), jailbreak detection (Inan et al., 2023), priority training (Wallace et al., 2024), and representation engineering (Zou et al., 2023a). Despite these advancements in methodologies and models to improve model safety, the influence of safety tuning data remains inadequately explored.\nTo bridge the gap, we identify a refusal position bias in the safety tuning data, which hampers the ability of the tuned LLMs to learn how to refuse effectively. Making a refusal decision before generating the response content leads to two significant shortcomings: (1) there is a lack of necessary information for making a refusal decision, and (2) there is no mechanism to incorporate refusal at later stages of the response. Based on these observations, we propose a novel safety tuning method called Decoupled Refusal Training (DeRTa), to explicitly train LLMs to refuse compliance at any response position by embedding the constructed harmful responses. Concretely, our approach introduces two novel components:\n\u2022 MLE with Harmful Response Prefix: This strategy involves appending a segment of the harmful response with a random length to the beginning of a safe response, which can train LLMs to refuse compliance at any response position instead of only at starting. In addition, adding a harmful prefix provides additional context to the query, significantly improving the LLMs' capability to identify and avoid unsafe content.\n\u2022 Reinforced Transition Optimization (RTO): While incorporating a harmful prefix helps the model to smoothly shift from recognizing a harmful trigger to generating a safe response, relying on a singular transition per training instance may not adequately equip LLMs with the ability to consistently recognize and prevent potential threats. In response to this problem, we introduce an auxiliary training objective to transition from potential harm to safety refusal at every position within the harmful response sequence.\nWe evaluate our approach using two prominent model families: LLaMA3 (8B and 70B) (Meta, 2024) and Mistral (7B and 8\u00d77B) (Jiang et al., 2023) across six attack scenarios. Experimental results show that our method not only boosts model safety without sacrificing performance but also surpasses notable models including GPT-4 and the instructional variants of LLaMA3-70B in attack defending. Both quantitative and qualitative assessments support our assertion that our strategy effectively arms LLMs with the ability to recognize and halt the generation of unsafe content when they detect potential risks."}, {"title": "2 METHODOLOGY", "content": "In this section, we identify an important issue associated with the safety data a refusal position bias that compromises the tuned models' ability to appropriately refuse generating unsafe content. Based"}, {"title": "2.1 STANDARD SAFETY TUNING", "content": "Standard safety tuning methods aim to instruct the model to generate safe responses to harmful queries (Bianchi et al., 2024; Touvron et al., 2023). Formally, given a harmful query q and a safe response r:\n$\\mathcal{L}_{safe} (\\theta) = -\\mathbb{E}_{(q,r) \\sim \\mathcal{D}} \\log P_{\\theta} (r | q) = -\\mathbb{E}_{(q,r) \\sim \\mathcal{D}} \\sum_{i=1}^{n} \\log P_{\\theta}(r_i | q, r_{<i})$\nwhere $\\mathcal{D}$ is the set of safety tuning instances.\nRefusal Position Bias As shown in Figure 2(a), in the safety data, the refusal tokens such as \"Sorry,\" \"I cannot,\" and \"I apologize,\" consistently occur within the first few tokens of a safe response. Accordingly, LLMs tuned on these safety data tend to generate re-fusal tokens at the beginning of a response. The results on the SOTA open-sourced LLMs with safety tuning in Table 1 confirm our claim. The refusal positional bias may lead to the following weaknesses:\n1. Lack of Necessary Information for Refuse Decision: The tuned model needs to make a refuse decision at the beginning of a response based on the query only, which may contain insufficient information for the decision.\n2. Lack of a Mechanism to Refuse in Later Positions: The positional bias may lead the model to rely heavily on position-specific features. Accordingly, the tuned model tends to continue generating unsafe responses once they start doing so, compromising safety in subsequent positions.\nIn this work, we propose a novel safety tuning approach to augment LLMs with the ability to refuse anywhere by mitigating the refusal position bias."}, {"title": "2.2 OUR APPROACH", "content": "To address the issues identified earlier, we have developed a method where LLMs are explicitly trained to refuse compliance at any response juncture by embedding the constructed harmful responses within the training process. As depicted in Figure 2(b), our strategy is comprised of two key components, each designed to counteract the two main concerns discussed."}, {"title": "3 EXPERIMENT", "content": "We utilize 60K uncensored samples from Evol-Instruct (Xu et al., 2023) as the SFT data for\nhelpfulness. We use harmful instructions from BeaverTails (Ji et al., 2023) as the safety data. To build\nsafety tuning data for our approach, we sample 3,000 instructions and obtain safe responses from\nGPT-3.5-turbo and harmful responses from our maliciously tuned LLaMA3-8B-Instruct. Since each\ninstance is a triple that consists of two (query, response) pairs (i.e., (harmful query, safe response)\nand (harmful query, harmful response)), we complement the safety dataset to 6,000 instances for the\nvanilla safety tuning for fair comparison.\nModels In our experiments, we consider two representative open-source model families: LLaMA3\n(8B and 70B) and Mistral (7B and 8\u00d77B). To eliminate the effect of other instruction tuning data, we\nconduct main experiments on the officially released raw models without instruction tuning. We set\nthe temperature to 0 for all models, and remain the other hyperparameters as default. For tuning the\nmodels, we set the total batch size to 128, and the number of epochs to 2.\nSafety Evaluation We conduct a random sampling of 100 harmful questions from the Do-Not-\nAnswer dataset (Wang et al., 2024) and another 100 from HarmBench (Mazeika et al., 2024),\nresulting in a total of 200 harmful questions. Our evaluation encompasses several prominent black-\nbox attack methods, including CodeAttack (Ren et al., 2024), PAIR (Chao et al., 2023), JailbreakChat\n(Walkerspider, 2022), and SelfCipher (Yuan et al., 2024b). For white-box attacks, we extend\nour analysis beyond AutoDAN (Liu et al., 2024a) by introducing an innovative method called\nCompletingAttack. This approach eliminates all formatting tokens (e.g., [INST]) to render the query\nin a declarative format, enabling the model to complete the text. CompletingAttack achieves high\nsuccess rates across all tested LLMs, such as LLaMA3-70B-instruct.\nWe determine the Attack Success Rate (ASR) by manually evaluating the responses generated by\nthe target LLMs for each attack method. The ASR indicates the proportion of harmful responses\ngenerated. For this metric, we used 50 harmful queries for PAIR and AutoDAN due to their\ncomputational complexity and the full set of 200 queries for the other attack methods.\nHelpfulness Evaluation We also assess the helpfulness of the targeted LLMs to determine if our\napproach increases safety at the expense of reducing helpfulness. To do this, we randomly select 500\nexamples from three sources: GSM8K (math reasoning) (Cobbe et al., 2021), MMLU (knowledge\ntests) (Hendrycks et al., 2021), and AlpacaEval (Li et al., 2023) (general capability). We follow the\ncommon practice to evaluate the results on AlpacaEval with GPT-4, and manually evaluate the results\nfor the other two tasks.\nMore details about the experimental setup can be found in Appendix (A - C)."}, {"title": "3.2 MAIN RESULTS", "content": "Table 2 enumerates the primary outcomes, presenting several noteworthy findings.\nOur Methodology Significantly Boosts Safety Without Compromising Helpfulness. Evidently,\nour approach has achieved a substantial decrease in ASR across all scenarios. Particularly, with the"}, {"title": "3.3 ANALYSIS", "content": "In this section, we offer deeper insights into the workings of DeRTa. Unless stated, we report results on the LLaMA3-70B model.\nImpact of Crucial Components In this experiment, we evaluate the effect of different components within our methodology on safety and helpfulness metrics. Table 4 shows the result on the LLaMA3-70B model without instruction tuning. We also list the results of conventional safety tuning applied to 3K queries, mirroring our approach (\"Vanilla-\"). Reducing the safety data by half slightly compromises safety, primarily due to vulnerabilities to black-box attacks.\nWhen implemented singularly, the harmful prefix strategy markedly improves safety measures against white-box attacks but has a negligible impact on reducing ASR for black-box attacks. The RTO strategy effectively addresses this limitation, significantly lowering the ASR for both attack forms. The results confirm our hypothesis that reinforcing the transition from potential harm to explicit safety refusal can enhance safety. The combination of both harmful prefix and RTO strategies yielded the most superior results. The forthcoming experiments will elucidate on how DeRTa substantially bolsters safety."}, {"title": "Comparison to DPO with Harmful Response", "content": "To comprehend why RTO is effective for CodeAttack, we examine its performance by comparing it with DPO (Rafailov et al., 2024), a notable method in preference modeling that utilizes both safe and harmful responses distinctively. This experiment seeks to determine whether RTO's success is attributed to the complete integration of harmful responses or the robust explicit modeling of token-wise safety transitions in these responses."}, {"title": "Impact of Model Size", "content": "In our final experiment, we sought to examine the effectiveness of our methodology across different model sizes. Specifically, our evaluation was conducted using two smaller-scale LLMs: Mistral-7B and LLaMA3-8B. The results, illustrated in Figure 6, clearly demonstrate that our approach significantly enhances safety irrespective of model size, showcasing the universality and robustness of our method.\nAn interesting observation is that, compared to their larger-scale counterparts, smaller LLMs exhibit a lower propensity for safety issues. Upon reviewing performance across a variety of tasks (refer to Table 10 in the Appendix E), smaller LLMs struggle with understanding complex adversarial tasks (such as CodeAttack and SelfCipher), which typically necessitate the capabilities of more powerful LLMs."}, {"title": "4 RELATED WORK", "content": "Ensuring that LLMs align with human ethics and preferences is essential to their responsible and effective deployment (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020; Solaiman & Dennison, 2021; Ouyang et al., 2022; Bai et al., 2022a; Korbak et al., 2023; Rafailov et al., 2024; Burns et al., 2023; Yuan et al., 2024a; Ji et al., 2024). While aligning LLMs with safety data is beneficial, these models remain vulnerable to jailbreak inputs that can prompt undesirable behavior (Walkerspider, 2022; Shen et al., 2023; Perez & Ribeiro, 2022; Schulhoff et al., 2023; Yu et al., 2023). Researchers have discovered that safety mechanisms can be circumvented by transforming the malicious query into semantically equivalent forms, such as ciphers (Yuan et al., 2024b; Wei et al., 2024; Jin et al., 2024), low-resource languages (Wang et al., 2023; Deng et al., 2024; Yong et al., 2023), or code (Ren et al., 2024)."}, {"title": "Jailbreak Defense.", "content": "Current defense strategies against jailbreak attacks primarily involve safety prompts (Xie et al., 2023; Zheng et al., 2024), input perturbation (Robey et al., 2023; Cao et al., 2023; Liu et al., 2024b), safety decoding (Xu et al., 2024), jailbreak detection (Inan et al., 2023), and priority training (Wallace et al., 2024). Jailbreak detection typically utilizes LLMs to identify attempted attacks (Helbling et al., 2023; Zhang et al., 2024c), or involves training specialized classifiers to detect jailbreaks (Inan et al., 2023; Yuan et al., 2024c). These classifiers can leverage various features, such as perplexity (Jain et al., 2023; Alon & Kamfonas, 2023), gradient signals (Hu et al., 2024), and high-level representations (Zou et al., 2023a; Zhang et al., 2024a). Priority training methods (Zhang et al., 2023; Lu et al., 2024; Wallace et al., 2024) involve using strategically designed data to train LLMs to prioritize instructions with higher rank. After deployment, developers can set these safety prompts to the highest priority to help the model against jailbreak attempts.\nIn this study, we establish a connection between these vulnerabilities and a bias towards refusal positions in the tuning data, which is used to align with safety protocols. Based on our findings, we advocate for the explicit training of LLMs to refuse compliance at any point of response by employing two distinct strategies. Experimental results demonstrate that our method significantly enhances safety by effectively addressing the bias towards refusal positions.\nConcurrently, related work by Qi et al. (2024a) has also highlighted a tendency in safety alignment to take shortcuts, specifically, alignment often prioritizes adaptations in the model's over only its very first few output tokens. In addressing this issue, they suggest a straightforward data augmentation strategy aimed at deepening safety alignment by training with data that begins with harmful responses but eventually shifts towards safety refusals. Our research primarily diverges in two aspects: (1) we explore vulnerabilities through the lens of refusal position bias, as opposed to focusing on the generative distribution; and (2) we show that merely starting with harmful response prefixes is inadequate for countering various forms of attacks, including sophisticated methods like the black-box CodeAttack and our novel white-box CompletingAttack. To bolster our defense mechanism, we introduce an auxiliary training objective RTO, designed to reinforce the transition from potential harm to safety refusal at every point within a harmful response sequence. Experimental results validate that our technique not only effectively counters the formidable CodeAttack and CompletingAttack but also significantly lowers the ASR for other attack methods."}, {"title": "5 CONCLUSION", "content": "In this study, we have presented a novel approach in addressing a significant aspect of LLMs safety refining their capacity to refuse the generation of unsafe content at any point during the response, thus addressing the critical issue of refusal position bias identified in safety tuning data. We introduce an innovative strategy encompassing two pivotal components, which collectively enhance LLMs' ability to identify and avert unsafe content more reliably and flexibly. The comprehensive evaluation of our DeRTa method notably demonstrates its superiority in terms of safety over existing models, including GPT-4. Our approach has not only shown to improve the safety of LLMs without compromising their performance but also stood resilient against advanced attack strategies, such as those that successfully bypassed the safety mechanisms of GPT-4 and LLaMA3-70B-Instruct (e.g., CodeAttack and our proposed CompletingAttack).\nOur findings underscore the importance of considering the role of safety tuning data and the inherent biases that may affect an LLM's ability to make refusal decisions effectively. Our method's capability to defend against recent advanced attack methods also highlights the potential for DeRTa to contribute to developing safer and more reliable LLMs in the face of continually evolving security threats."}, {"title": "A DETAILS OF SETUP", "content": "In training, we set the total batch size to 128 and the number of epochs to 2.\nFor full parameter fine-tuning (Mistral-7B and LLaMA3-8B), we use a learning rate of 2e-5, a\nwarmup ratio of 0.03, a weight decay of 2e-5, a max length of 1024, and a dropout rate of 95% for\nthe \"Sorry\" token.\nFor the LoRA method (Mistral-MoE and LLaMA3-70B), we set the learning rate to le-4, the max\nlength to 512, with no warmup, and a 0% dropout rate for the \"Sorry\" token. The LoRA rank and\nalpha are 96 and 16, with a 0.05 dropout. The LoRA is applied in the attention layer and the mlp\nlayer.\nFor GPT-4 and ChatGPT, we use the version GPT-4-turbo-0409 and GPT-3.5-tubor-0125.\nTo obtain uncensored Evol-Instruct data, we use ChatGPT with a safety detection prompt and keyword\nmatch (e.g., as an AI) as the filter.\nTo conduct standard DPO training, it is essential to have both a chosen response\nand a rejected response for each instruction. As such, we utilize the Qwen1.5-chat-0.5B model (Bai\net al., 2023) to generate responses for the 60k helpful instructions in Evol-Instruct.\nThe original Evol-Instruct response and the Qwen response serve as the chosen and rejected responses,\nrespectively. Similarly, the safe and harmful responses of a harmful question function as the chosen\nand rejected responses, respectively.\nBuilding upon the model with standard safety training, we proceed to train for one additional\nepoch using DPO. The learning rates for LLaMA3-8B and LLaMA3-70B are set at 5e-7 and 2e-6,\nrespectively.\nFirst, we use 330 malicious question-response pairs to adversarially\ntune the LLaMA3-8B-Instruct. Then, this malicious LLaMA is employed to generate harmful\nresponses for questions from BeaverTails. Afterward, we utilize GPT-3.5 to enhance the grammar\nand lexical diversity of these generated responses while removing any safety warnings present in the\nharmful responses."}, {"title": "B DETAILS OF APPLIED ATTACKS", "content": "In Figure 7, we present examples of each attack. Below are the introductions and implementation\ndetails for each attack.\nCodeAttack disguises malicious questions as a code completion task. The model\ngenerates harmful questions and responses while completing the code. We use the Python-stack\nversion for this attack.\nPAIR PAIR iteratively utilizes an AttackLLM to modify the harmful question, making it appear\nmore harmless. In our experiment, we utilize GPT-3.5 as the AttackLLM and GPT-4 as the judge\nmodel. We maintain the same breadth (20 streams) and depth (3 iterations) as outlined in the PAIR\npaper."}, {"title": "D CASE STUDY", "content": "We provide some selected cases in this section, all cases of our experiments can be found in our\nGitHub project."}, {"title": "E MAIN RESULTS ON SMALL-SCALE LLMS", "content": "Table 10: Main results on small-scale LLMs. For CodeAttack, these models often fail to follow\ninstructions, so we do not display the results under this setting."}]}