{"title": "Distributed Optimization via Energy Conservation Laws in Dilated Coordinates", "authors": ["Mayank Baranwal", "Kushal Chakrabarti"], "abstract": "Optimizing problems in a distributed manner is critical for systems involving multiple agents with private data. Despite substantial interest, a unified method for analyzing the convergence rates of distributed optimization algorithms is lacking. This paper introduces an energy conservation approach for analyzing continuous-time dynamical systems in dilated coordinates. Instead of directly analyzing dynamics in the original coordinate system, we establish a conserved quantity, akin to physical energy, in the dilated coordinate system. Consequently, convergence rates can be explicitly expressed in terms of the inverse time-dilation factor. Leveraging this generalized approach, we formulate a novel second-order distributed accelerated gradient flow with a convergence rate of O (1/t^{2-\u025b}) in time t for \u025b > 0. We then employ a semi second-order symplectic Euler discretization to derive a rate-matching algorithm with a convergence rate of O (1/k^{2-\u025b}) in k iterations. To the best of our knowledge, this represents the most favorable convergence rate for any distributed optimization algorithm designed for smooth convex optimization. Its accelerated convergence behavior is benchmarked against various state-of-the-art distributed optimization algorithms on practical, large-scale problems.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, there has been significant progress in digital systems, communication, and sensing technologies, leading to the emergence of networked systems [1]. These systems encompass a multitude of interconnected subsystems (agents) that necessitate collaboration to attain specific global objectives. The applications of such networked systems span diverse domains, including solving economic dispatch problems in power systems [2], distributed estimation in sensor networks [3], coverage control [4], decentralized machine learning [5], among others.\n\nThe above applications often involve solving a distributed convex optimization problem, wherein each node performs local computation on local data and cooperatively reach consensus on a global optimal solution through exchanging local decisions with neighbors in an iterative fashion. Formally, the problem can be expressed as follows:\n$\\min_{\\{x_i \\in \\mathbb{R}^d, \\forall i\\}} \\sum_{i=1}^m f_i(x_i)$,\ns.t. $x_i = x_j, \\forall i, j \\in \\{1,2,...,m\\}.$\nHere the convex function $f_i : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ represents the local objective function of the ith-agent, where $i \\in \\{1,2,...,m\\}$ and m is the number of agents in the network. Distributed optimization problems enable agents to coordinate in a distributed manner while simultaneously minimizing the overall team objective function. As a result, these problems tend to be inherently more complex.\n\nMost prior research on distributed optimization has focused on creating discretized algorithms designed to ensure that each agent within the network converges to the optimal solution of (1) or to a neighborhood around it [6]-[8]. These algorithms are easy to implement and often come with theoretical guarantees on the rate and order of convergence. Additionally, their theoretical analysis provides insights into the theoretical bounds on various hyperparameters, such as learning rate. However, a primary disadvantage of these approaches is the difficulty in their analysis and design. In the recent years, continuous-time analysis of optimization algorithms has gained significant interest, primarily due to ease of analysis and ability to design novel algorithms [9]\u2013[12]. The continuous-time limiting behavior allows for the application of tools from Lyapunov theory and differential equations, facilitating stability and convergence-rate analysis. However, while the analysis is relatively easier, these algorithms ultimately need to be discretized for iterative implementation. Most continuous-time algorithms either lose their accelerated convergence guarantees upon discretization\u2014making it challenging to achieve rate-matching discretization\u2014or the resulting discretized algorithms start to diverge. Another common issue that plagues both discretized and continuous-time analyses is the lack of a generalized framework that can be used to analyze a broader class of distributed optimization algorithms.\n\nIn the wake of above limitations, the proposed work makes the following contributions:\n1. Distributed gradient flow with O(1/t^{2-\u03b2}) convergence rate: In this study, we introduce an innovative distributed continuous-time gradient flow method aimed at minimizing the sum of smooth convex functions, achieving an unprecedented convergence rate of O (1/t^{2-\u03b2}), where \u03b2 > 0 can be arbitrarily small. Notably, our proposed algorithm surpasses the previously best-known convergence rate of O (1/t^{1.4-\u03b2}) achieved by the distributed version of Nesterov accelerated"}, {"title": "II. PRELIMINARIES", "content": "Notation: Let $\\mathbb{N}$ be any natural number. Throughout this paper, we use $\\nabla g$ to represent the gradient of a function $g : \\mathbb{R}^N \\rightarrow \\mathbb{R}$. Unless specified otherwise, we use $||v||$ to represent the Euclidean norm of a vector $v \\in \\mathbb{R}^N$, and $||M||$ to denote the induced 2-norm of the matrix M. The Kronecker product of two matrices is represented by $\\otimes$. We let $0_N$ denote the N-dimensional zero vector and O denote the zero matrix of appropriate dimension. We let $1_N$ denote the N-dimensional vector of ones. We denote by $Diag(\\cdot)$ a block-diagonal matrix of appropriate dimensions. We use the abbreviation SPD for symmetric positive definite. We let $\\mathcal{L}$ denote the Laplacian matrix of the graph G. We let $\\lambda_N$ and $\\lambda$ respectively denote the largest and the smallest non-zero eigenvalue of the matrix $\\mathcal{L}$. We let $X^* = [(x^*)^T,..., (x^*)^T]^T$. Finally, we let $\\mathcal{L} = \\mathcal{L} \\otimes I$, where $I$ denotes the $(d \\times d)$-dimensional identity matrix. We let $F : \\mathbb{R}^{md} \\rightarrow \\mathbb{R}$ denote the cumulative cost function, i.e., $f(x) = \\sum_{i=1}^m f_i(x)$ for all $x \\in \\mathbb{R}^d$ and $F(X) = \\sum_{i=1}^m f_i(x_i)$ for all $X \\in \\mathbb{R}^{md}$.\n\nWe will use the following definitions from convex optimization theory.\n\nDefinition 1. A differentiable function $g : \\mathbb{R}^N \\rightarrow \\mathbb{R}$ is convex if $\\nabla g(x)^T (y \u2013 x) \\leq g(y) \u2013 g(x)$ for any $x, y \\in \\mathbb{R}^N$.\n\nDefinition 2. A differentiable function $g : \\mathbb{R}^N \\rightarrow \\mathbb{R}$ is said to be $L_g$-smooth if it has Lipschitz gradient with $L_g > 0$, i.e., $|\\nabla g(x) \u2013 \\nabla g(y)|| \\leq L_g||x \u2212 y||$ for any $x, y \\in \\mathbb{R}^N$.\n\nGeneral form of conservation laws: Consider a second-order ODE given by:\n$a(t)\\ddot{w} + b(t)\\dot{w} + \\nabla_wU(w,t) = 0.$\nTaking inner-product with $\\dot{w}$, one obtains:\n$a(t)\\langle\\ddot{w}, \\dot{w}\\rangle + b(t)\\langle\\dot{w}, \\dot{w}\\rangle + \\langle\\nabla_wU(w,t), \\dot{w}\\rangle = 0,$\nwhich can be rewritten as:\n$\\frac{d}{dt}(\\frac{a(t)}{2}||\\dot{w}||^2) + (\\frac{b(t)}{2} - \\frac{\\dot{a}(t)}{2})||\\dot{w}||^2 + \\frac{d}{dt}U(w,t) = \\frac{\\partial}{\\partial t}U(w,t)$.\nIntegrating the above equation yields the conservation law:\n$\\begin{aligned}\nE(t) &= \\frac{a(t_0)}{2} ||\\dot{w}(t_0)||^2 + U(w(t_0), t_0) \\\\\n&= \\frac{a(t)}{2}||\\dot{w}(t)||^2 + \\int_{t_0}^t (\\frac{b(s)}{2} - \\frac{\\dot{a}(s)}{2}) ||\\dot{w}(s)||^2 ds \\\\\n& + U(w(t), t) - \\int_{t_0}^t \\frac{\\partial}{\\partial s} U(w(s), s) ds.\\end{aligned}$\nOur approach to design of distributed optimization algorithm revolves around designing a dilated coordinate system $w(t)$ such that the total energy $E(t)$ is conserved in the dilated coordinate system.\n\nOur theoretical analysis hinges on the following key assumptions:\n\nAssumption 1. $min_{x \\in \\mathbb{R}^d} \\sum_{i=1}^m f_i(x) < \\infty$ and the solution set $\\{x \\in \\mathbb{R}^d: \\sum_{i=1}^m \\nabla f_i(x) = 0\\}$ of problem (1) is non-empty.\n\nAssumption 2. The communication graph $\\mathcal{G}$ is undirected and connected."}, {"title": "III. DISTRIBUTED ACCELERATED GRADIENT FLOW", "content": "The primary objective in distributed optimization is to reduce the function value efficiently. In proving stability of equilibria or convergence behavior of dynamical systems, Lyapunov functions are often employed as proxies for system energy. By ensuring the rate of change of Lyapunov functions to be non-positive, stability of equilibria can be deduced. Obtaining bounds on convergence rates is nontrivial and requires reformulating suitable explicit time-varying energy functionals that are parameterized directly in terms of the convergence rates.\n\nOne of the fundamental limitations of Lyapunov-based analysis of dynamical systems is its inability to elucidate bound on the convergence rate, particularly when the convergence to equilibrium is asymptotic. We overcome this limitation by suggesting a dissipation law of dynamical system concerned with solving distributed optimization problem. Methods, such as the the Accelerated Gradient Method (AGM)-ODE and the Optimized Gradient Method (OGM)-ODE are shown to exhibit faster rates of convergence for single-agent (i.e., non-distributed) convex optimization problems [14], [15]. We now propose a distributed variant of the AGM-ODE and establish O(1/t^{2-\u03b2}) convergence rate, where \u03b2 > 0 is arbitrarily small. In particular, we consider the following distributed dynamical system:\n$\\dddot{X} + \\frac{3}{t}\\ddot{X} + t^{-\u03b2} \\nabla F(X) + k\\mathcal{L}\\dot{X} = 0$,  (Dist-AGM)\nwith parameters k,\u03b2 > 0. Here $X := [x_1^T,...,x_m^T]^T$ represents the concatenated state of all the agents. Note that from the perspective of the ith-agent, the agent runs the following distributed protocol:\n$\\dddot{x_i} + \\frac{3}{t}\\ddot{x_i} + t^{-\u03b2} \\nabla f_i(x_i) + k\\sum_{j\\in N_i}(\\dot{x_i} - \\dot{x_j}) = 0.$\n\nThe exponent \u03b2 can be arbitrarily small, however, it enforces a diminishing coefficient for the gradient term in (Dist-AGM) in order to ensure that the trajectories of the individual agents converge to the optimal solution $x^*$. Although the constant coefficient strategy (i.e. \u03b2 = 0) converges faster than the diminishing step-size scheme, it only reaches a small neighborhood of the optimal solution $x^*$; see [16], [17] for example. Below we show that the distributed optimization algorithm Dist-AGM achieves near optimal convergence rate.\n\nTheorem 1. Consider the distributed optimization problem given by (1), and let the system consist of an undirected, connected network of m agents, each following the distributed protocol outlined in (Dist-AGM). Under these conditions and subject to Assumptions 1-2, the states of the agents will converge to the optimal solution $x^*$ of (1) with a convergence rate of O(1/t^{2-\u03b2}), where \u03b2 > 0 is an arbitrarily small positive constant defined in (Dist-AGM).\n\nProof. Let us consider the dilated coordinate system $W(t) := t^2(\\dot{X}(t)-X^*)$. After some algebraic manipulation, (Dist-AGM) can be expressed in the new coordinate system as:\n$\\frac{\\dot{W}}{t} - \\frac{W}{t^2} + t^{2-\u03b2} \\nabla_w F(\\frac{W}{t^2} + X^*) + \\mathcal{L} \\frac{W}{t^2} = 0.$\nFrom (4), one obtains:\n$U(W,t) = \\frac{1}{2t^2} \\frac{W^T \\mathcal{L} W}{t^2} + t^{2-\u03b2} \\left(F\\left(\\frac{W}{t^2} + X^*\\right) - F_\u2217\\right)$.\nUpon taking partial derivative of $U$ with respect to the time component s, one obtains:\n$\\begin{aligned}\n\\frac{\\partial U}{\\partial s} &= \\frac{1}{s} \\frac{W^T \\mathcal{L} W}{s} + (2-\u03b2) s^{1-\u03b2} \\left(F\\left(\\frac{W}{s^2} + X^*\\right) - F_\u2217\\right) \\\\\n&- \\frac{2}{s^3} s^{2-\u03b2} \\nabla_w F\\left(\\frac{W}{s^2} + X^*\\right) \\frac{W}{s^2} + X^* \\\\\n&= \u2212s\\left(\\frac{W}{s^2} \u2212 X^*\\right)^T \\mathcal{L}\\left(\\frac{W}{s^2} \u2212 X_\u2217\\right) \u2212 \u03b2s^{1\u2212\u03b2}(F(X) \u2212 F_\u2217) \\\\\n&\u2212 2s^{1\u2212\u03b2} (F_\u2217 \u2212 F(X) \u2212 \\nabla_XF(X)^T(X_\u2217 \u2212 X)).\n\\end{aligned}$"}, {"title": "IV. DISCRETIZATION OF DISTRIBUTED AGM-ODE", "content": "In this section, we show that discretizing (Dist-AGM) via a semi-second-order symplectic Euler discretization in the dilated coordinate system leads to a discretized algorithm with consistent discretization O(1/k^{2-\u03b2}). Even though there has been substantial prior work on continuous-time analyses of optimization algorithms, achieving a rate-matching discretized algorithm via a straightforward and \"natural\" discretization has proven to be surprisingly challenging.\n\nWe begin with formulating the generalized momentum via the Lagrangian formulation:\n$L(W, \\dot{W},t) = \\frac{1}{2t} ||\\dot{W}||^2 -t U(W,t)$.\nThe Euler-Lagrange equation $\\frac{d}{dt} \\nabla_{\\dot{W}}L - \\nabla_W L$ yields\nODE (4) and the conjugate momentum $P = \\frac{\\dot{W}}{t} + t\\dot{X} + 2(X - X^*)$. Expressing (4) in W and P yields:\n$P = \u2212t^{1\u2212\u03b2}\u2207F(X) \u2212 t \\mathcal{L}X$\n$\\dot{W} = tP.$"}, {"title": "V. EMPIRICAL RESULTS", "content": "In this section, we conduct a numerical simulation to show the efficiency of proposed algorithm (15a)-(16c) and to validate its convergence guarantee obtained in Theorem 2.\n\nSimulation set-up and algorithm parameters: We consider solving the logistic regression problem on a subset of the MNIST dataset. Specifically, we consider the binary classification problem between the digits one and five using the MNIST [22] dataset. Our data consists of all the samples labeled as either digit one or digit five from the MNIST training set. We consider the logistic regression model and conduct experiments to minimize the cross-entropy error on the training data. The data points are distributed equally among m = 5 agents. The communication topology is considered to be a ring. In this case, there are multiple solutions of (1). For a fair comparison, the parameters in each algorithm are tuned such that the respective algorithm converges in fewer iterations. Their specific values are as follows. DGD [6]: \u03b1 = 0.001 and W is the Metropolis weights; PI [23]: \u03b1 = \u03b2 = 100, h = 0.001; PI consensus [24]: a = 0.01, \u03b2 = 0.1; DIGing [25]: a = 0.001 and W is the Metropolis weights; pre-conditioned PI consensus [26]: a = 2, h = 0.09, \u03b2 = 20, \u03b3 = 20; proposed algorithm: h = 10, \u03b2 = 0.1. xi(0) in all the algorithms are the same, and its each entry is chosen from the Normal distribution with zero mean and 0.1 standard deviation. vi(0) in the PI consensus and pre-conditioned PI consensus algorithms are chosen similarly, and vi(0) for the PI algorithm is according to [23]. Note that the implementation itself of a few recently proposed distributed optimization algorithms, such as APM-C [27], Mudag [28], DAccGD [29], and ACC-SONATA [30], requires the value of the strong-convexity coefficient of the cumulative or the aggregate cost function. Since neither the aggregate cost nor the cumulative cost is strongly convex in this logistic regression problem, these APM-C, Mudag, DAccGD, and ACC-SONATA algorithms are not applicable.\n\nResults: From Figure 2, the proposed algorithm converges much faster than the other algorithms for solving the logistic regression problem. We note that the ratio between the largest and the smallest non-zero singular value of the Hessian of F is of order 10^{11}. Thus, even for ill-conditioned problems, the proposed algorithm can converge significantly faster than the other algorithms. The convergence guarantees in Theorem 2 holds for \u03b2\u2208 (0,2). So, in Figure 1, we show the effect of tuning the parameter \u03b2 in the proposed algorithm. We simulate the algorithm with four choices of \u03b2 \u2208 {1,0.5, 0.1, 0.01}. The values of h, xi(0), vi(0) are chosen as described earlier and is fixed while only \u03b2 is varied. We observe convergence even for a small value of \u03b2 = 0.01, as expected from Theorem 2. Also, from Theorem 2, the convergence rate is O(\\frac{1}{k^2}). Accordingly, from Figure 1, faster convergence happens for smaller values of \u03b2."}]}