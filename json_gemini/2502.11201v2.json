{"title": "Bridging the Gap: Enabling Natural Language Queries for NoSQL Databases through Text-to-NoSQL Translation", "authors": ["Jinwei Lu", "Yuanfeng Song", "Zhiqian Qin", "Haodi Zhang", "Chen Zhang", "Raymond Chi-Wing Wong"], "abstract": "NoSQL databases have become increasingly popular due to their outstanding performance in handling large-scale, unstructured, and semi-structured data, highlighting the need for user-friendly interfaces to bridge the gap between non-technical users and complex database queries. In this paper, we introduce the Text-to-NoSQL task, which aims to convert natural language queries into NoSQL queries, thereby lowering the technical barrier for non-expert users. To promote research in this area, we developed a novel automated dataset construction process and released a large-scale and open-source dataset for this task, named TEND (short for Text-to-NoSQL Dataset). Additionally, we designed a SLM (Small Language Model)-assisted and RAG (Retrieval-augmented Generation)-assisted multi-step framework called SMART, which is specifically designed for Text-to-NoSQL conversion. To ensure comprehensive evaluation of the models, we also introduced a detailed set of metrics that assess the model's performance from both the query itself and its execution results. Our experimental results demonstrate the effectiveness of our approach and establish a benchmark for future research in this emerging field. We believe that our contributions will pave the way for more accessible and intuitive interactions with NoSQL databases.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, with the rapid development of Internet and big data technologies, NoSQL databases have garnered increasing attention due to their outstanding performance in handling large-scale, unstructured, and semi-structured data. NoSQL databases offer high scalability, high performance, and flexible data models, effectively addressing the challenges posed by massive data volumes and high-concurrency access. Within the database and data mining community, NoSQL databases have also been the subject of extensive research and have gained widespread recognition [4, 6, 9, 11, 14, 14, 22, 41].\nDespite the numerous advantages of NoSQL databases, the need for specialized knowledge to write query statements and manage data effectively presents a high technical barrier for users. The query languages and operational methods of NoSQL databases differ significantly from traditional SQL, posing considerable difficulties for non-expert users. Therefore, bridging the gap between natural language (NL) and NoSQL query syntax is a crucial step towards further promoting these powerful database systems.\nTo address this challenge, we introduce the task of Text-to-NoSQL for the first time in this paper. This new paradigm aims to translate natural language queries (NLQs) into corresponding NoSQL queries, thereby significantly reducing the technical difficulty of using NoSQL databases (as shown in Figure 1). Its goal is to facilitate more intuitive interactions between users and NoSQL databases, enabling a broader audience, including those without extensive programming backgrounds, to benefit from these systems. Furthermore, recognizing the critical role of benchmark datasets in driving research and development, we present TEND, a large-scale and open-source Text-to-NoSQL task dataset. TEND is designed to provide a comprehensive resource for researchers and developers working on natural language to query translation systems, particularly those focused on NoSQL environments. It offers a standardized evaluation platform, promoting comparison between different approaches, accelerating innovation, and fostering competition within the Text-to-NoSQL community.\nTo address the significant human and time costs associated with constructing large-scale datasets, we propose a novel semi-automated process for constructing a large-scale, high-quality Text-to-NoSQL dataset. Specifically, the Text-to-NoSQL dataset consists of three components: databases, NoSQL queries, and corresponding natural language queries (NLQs). First, as described in Section 2.2, we employ a graph traversal algorithm to organize relational database tables into distinct clusters based on foreign key relationships. Using the Depth-First Search algorithm, we transform these foreign key relationships into nested structures suitable for NoSQL databases, ultimately constructing a comprehensive NoSQL database. Next, a sophisticated process combining large language models (LLMs) and code programs is utilized to generate and refine NoSQL queries. Finally, multiple LLMs are employed to expand the NLQs in the dataset. All LLM-based processes in the pipeline leverage advanced Chain-of-Thoughts (CoT) techniques, generated by state-of-the-art LLMs such as GPT-40, to enhance the reasoning capabilities of second-tier LLMs (e.g., GPT-40-mini). For additional details, please refer to Section 2. This dataset construction approach not only significantly reduces manual intervention but also ensures data consistency and accuracy.\nTo advance research on the Text-to-NoSQL task within the data mining and database communities, we designed a SLM (Small Language Model)-assisted and RAG (Retrieval-augmented Generation)-assisted multi-step framework called SMART. SMART is divided into four main processes: (1) SLM-based Query Generation, (2) SLM-based Query Generation, (3) Predicted Schema-driven and Retrieved"}, {"title": "2 THE PROPOSED PIPELINE FOR CONSTRUCTING THE TEXT-TO-NOSQL BENCHMARK", "content": "In this section, we provide a detailed explanation of the construction pipeline of TEND."}, {"title": "2.1 Overview", "content": "The construction process of TEND is a semi-automatic pipeline that can convert a Text-to-SQL dataset into a Text-to-NoSQL dataset. Its automation is mainly divided into three steps: Database Transformation (Section 2.2), Advanced CoT-driven Query Transformation (Section 2.3), and Question Extension by Multi-LLM (Section 2.4) (as shown in Figure 2). Finally, we manually reviewed and adjusted the entire dataset, removing examples that did not meet expectations and standardizing programming conventions across the dataset. Below, we explain this process in detail."}, {"title": "2.2 Database Transformation", "content": "TEND consists of three parts: databases, NoSQL queries, and NLQs. The construction of databases is a labor-intensive and time-consuming task, involving extensive data collection and filtering. To reduce the substantial manpower and time required for database construction, we designed an algorithm to convert relational databases into NoSQL (MongoDB) databases, where we transformed the foreign key relationships between tables in the relational database into nested relationships in the NoSQL database. Below is a detailed explanation of the database transformation algorithm along with specific examples.\nThe algorithm for database transformation is shown in Algorithm 1, with a specific example illustrated in Figure 2(i). First, we traverse all tables and their potential foreign keys, grouping two tables connected through a foreign key into the same MongoDB"}, {"title": "2.3 Advanced CoT-driven Query Transformation", "content": "After completing the database conversion, we need to generate the corresponding NoSQL queries based on the NoSQL database. Manually writing all the NoSQL queries for a large-scale dataset is also a labor-intensive and time-consuming task. To automate this process, we designed a specialized LLM-based pipeline that leverages the execution results of SQL queries (rather than the queries themselves) to autonomously generate NoSQL queries. These queries are then executed to obtain feedback, which is subsequently used to refine and debug the queries. Below, we provide a detailed introduction to this Query Transformation pipeline (shown in Figure 2(ii))."}, {"title": "2.3.1 Advanced CoT-driven Query Generation", "content": "Using state-of-the-art LLMs (like GPT-40) for inference is very expensive. To fully leverage the performance of the second-tier LLMs (such as GPT- 40-mini), we adopted an Advanced CoT-driven approach for query generation. Specifically, we provided the LLM with input and output examples as the first round of dialogue, where the input includes an introduction for query generation, NoSQL database schemas, and user questions (the prompt is written in markdown format), and the output contains a chain of thoughts generated by the most advanced LLM (GPT-40), inspired by the following prompt: \u201cA: Let's think step by step!\u201d. Notably, during query generation, we also provide the schemas obtained from executing the corresponding SQL queries"}, {"title": "2.3.2 Execution Feedback Generation", "content": "After the Advanced CoT-driven Query Generation, we obtain many unverified NoSQL Queries, and manually checking all queries for correctness would require a considerable amount of time. To screen all queries automatically, we employ the LLM as an inspector. Specifically, we execute the NoSQL queries and their reference SQL queries obtained from Section 2.3.1 on MongoDB and SQLite databases respectively, and organize the results into JSON format for direct comparison. If the two results are identical, then the NoSQL query is considered correct; if they differ, then LLM feedback is required. For LLM feedback, the execution results of the NoSQL and reference SQL queries (in JSON format), user questions, and the schemas required are combined into prompts, instructing the LLM to perform the following two steps: (i) Examine the differences between the data obtained from executing a NoSQL query and the data obtained from executing the corresponding reference SQL query. (ii) Analyze where these differences may have originated from, with no solutions. It is worth noting that the Feedback Generation step also uses the Advanced CoT-driven method, similar to that mentioned in Section 2.3.1."}, {"title": "2.3.3 Advanced CoT-driven and Feedback-driven Query Debug", "content": "For those NoSQL queries whose execution results are inconsistent with the reference SQL queries, we need to correct them. To automate NoSQL debugging, we let the LLM act as an inspector in Section 2.3.2, identifying potential issues with the problematic NoSQL queries. We then composed these factors that could cause discrepancies between the execution results of the NoSQL queries and the reference SQL queries into debug prompts, and used the most advanced LLM to generate a debug thought chain as a demonstration dialogue. Subsequently, we let a second-tier LLM correct these errors while continuously checking if the debugged NoSQL query's execution results are correct. If correct, we exit the debug program and output the NoSQL query; if incorrect, we re-try. After two accumulated errors, we opt to use the highest-level LLM for debugging. If the highest-level LLM still fails to correct the NoSQL query, we abandon that case. It is noteworthy that the execution results of SQL queries serve as a critical dependency in this process, which makes it inherently unsuitable for direct application in Text-to-NoSQL tasks."}, {"title": "2.4 Question Extension by Multi-LLM", "content": "Figure 2(iii) illustrates the process of expanding NLQs based on Multi-LLM. To construct a dataset of sufficient size for model training, we need to expand the original NLQs to create enough (NLQ, NoSQL) pairs. In this process, we use LLMs to automatically expand the NLQs from the original Text-to-SQL dataset. We provide the LLMs with NoSQL database schemas, NoSQL queries, reference question examples, and the target schemas for the questions. A single LLM often generates the same questions repeatedly, failing to effectively expand the question set. To address this issue, we employed a Multi-LLM approach, using multiple LLMs (e.g., gpt-3.5-turbo-16k-0613, gpt-40-2024-05-13 and claude-3-sonnet-20240229) to expand the questions, ultimately resulting in a large-scale Text-to-NoSQL dataset."}, {"title": "2.5 Manual Review and Refinement", "content": "Finally, we conducted a comprehensive manual review and refinement of the Text-to-NoSQL dataset TEND generated by the automated pipeline. Specifically, we carefully modified the renamed fields in the NoSQL queries within TEND. For example, in the group stage, which uses aggregation operations similar to those in SQL, such as \u201csum_Population: { $sum: \"$Population\" }\u201d, the LLM might use different names for \u201csum_Population\u201d, such as \u201ctotal_population\u201d. If the renamed fields in the NoSQL queries are not standardized, it could lead to poor performance of the models trained on this dataset in query-based metrics (Section 5.1.3). Therefore, we standardized the naming of aggregation operations, renaming fields to \u201c[operation]_[object]\u201d (e.g., \u201csum_population\u201d). Similar to the renaming in the group stage, we also standardized the renaming of the new document collections after joins in the lookup stage, using names like \u201cDocs1\u201d and \u201cDocs2\u201d."}, {"title": "3 THE TEND DATASET: SETUP AND STATISTICS", "content": "We applied the dataset construction process introduced in Section 2 to the training set of Spider [46], a popular and complex large-scale benchmark dataset for the Text-to-SQL task. The training set includes 166 databases (with an average of 5.28 tables per database) and 7,000 (NLQ, SQL) pairs on these databases. These (NLQ, SQL) pairs are designed to comprehensively cover different databases and various levels of difficulty in Text-to-SQL tasks."}, {"title": "3.1 Setup", "content": "Table 1 presents the statistical overview of the TEND dataset, encompassing both database and NoSQL query statistics (each NoSQL query corresponds to 5 NLQs, forming 5 pairs of (NLQ, NoSQL)). As illustrated in Table 1(a), the TEND dataset includes 154 databases, which collectively store 347 collections and span 105 domains. The top five domains are sport, customer, school, shop, and student. In total, the databases contain 5,960 fields (corresponding to 32,979 documents), with an average of 38.70 fields (214.15 documents) per database. The database with the highest number of fields (documents) contains 331 fields (13,694 documents), whereas the one with the fewest has only 7 fields (3 documents).\nTable 1(b) presents the query statistics for the TEND dataset, which includes a total of 17,020 (NLQ, NoSQL) pairs. Within this dataset, 2,770 pairs utilize the find method, while 14,250 pairs employ the aggregate method. Among the find method pairs, specific operations are distributed as follows: 2,690 pairs incorporate the filter operation, 2,005 utilize the projection operation, 600 employ the sort operation, and 10 pairs apply the limit operation.\nFor the aggregate method pairs, operations are executed through pipelines, where each operation stage is defined within the pipeline. The data reveal that 13,235 of these aggregate pipelines incorporate the Project stage. Additional operations are utilized across the following stages: Unwind (8,575 pairs), Group (7,740 pairs), Match (7,305 pairs), Sort (3,325 pairs), Limit (2,390 pairs), Lookup (2,155 pairs), Count (955 pairs), and other operations (25 pairs)."}, {"title": "3.2 Statistics of TEND", "content": "Figure 3: The working pipeline of our proposed SMART framework", "$isArray": "$concatArrays", "$arrayElemAt": "These special operations often introduce unnecessary difficulties for model prediction", "strftime()": "n Text-to-SQL tasks", "db.people.aggregate([{$group: {_id: \\\"$Country\\\", count: {$sum: 1}}}, {$project: { Country: \\\"$_id\\\", count:1, _id:0}}]);": "nto a list of dictionaries", "$group": {"$project": {"{$project {_id:0}}": "annot be directly decoded into JSON because it lacks necessary quotation marks, but demjson can automatically complete it and parse it into JSON data {\"$project\":{\"_id\":0}}. Using this parser, we can write Python programs to analyze the parsed JSON data of NoSQL queries and filter them based on their operation keywords, directly discarding examples that contain special operations, thereby obtaining the final TEND."}, "title": "4 MULTI-STEP FRAMEWORK FOR TEXT-TO-NOSQL: SLM AND RAG ASSISTANCE", "content": "To provide a high-performance solution for the novel task of Text-to-NoSQL, we have designed a Multi-step framework called SMART (an abbreviation for SLM-assisted and RAG-assisted Multi-step framework) based on RAG techniques and fine-tuned SLM. This chapter will provide a detailed introduction to SMART.\nOverview: As shown in Figure 3 and Algorithm 2, SMART is mainly divided into four processes: SLM-based Schema Prediction (Section 4.1), SLM-based Query Generation (Section 4.2), Predicted Schema-driven and Retrieved Example-driven Query Refinement (Section 4.3), and Execution Result-based Query Optimization (Section 4.4). Compared to LLM, the fine-tuning cost of SLM is much lower, making SLM a promising auxiliary tool. Especially when we need to predict code or details that are easily influenced by preferences (e.g. the renamed fields in NoSQL queries), LLM often struggles to align with these preferences. This means that directly using prompting methods makes it difficult for LLM to achieve high accuracy. However, with the assistance of SLM, the entire framework can achieve this alignment. RAG technology is a mainstream auxiliary technology for LLM, but its drawback lies in its high requirements for retrieval technology. It requires retrieving high-quality examples and adding them to the prompt context to be effective. If the quality of the context examples is poor, it can significantly interfere with LLM's predictions. To maximize the relevance of the retrieved examples, the retrieval method used in the SMART framework does not solely rely on the similarity of NLQs between examples. Instead, it calculates the final example similarity by weighting and summing the cosine similarities of NLQs, NoSQL queries, and schemas (such as fields in the queries and fields displayed in the execution result documents) to retrieve examples. Below, we introduce the four main processes of SMART in detail."}}, {"title": "4.1 SLM-based Schema Prediction", "content": "As shown in Figure 3(i), the implementation of schema prediction based on SLM is accomplished through model fine-tuning. Specifically, we parse each example in the training set of TEND, extracting various schema information from the NoSQL queries, such as the collections queried, the database fields queried, the renamed fields in the query, and the fields included in the resulting documents after query execution. These schema details are then paired with the corresponding NLQ to form prompts and outputs, thereby constructing a fine-tuning corpus for predicting various schemas. We then use this fine-tuning corpus to fine-tune SLMs specialized in predicting different schemas. Finally, we use these SLMs to perform schema prediction on examples in the TEND test set."}, {"title": "4.2 SLM-based Query Generation", "content": "Figure 3(ii) illustrates the SLM-based Query Generation process. This process involves directly constructing the TEND training set into a text-to-NoSQL corpus and using this corpus to fine-tune a specialized text-to-NoSQL SLM. Subsequently, this SLM is used to directly predict examples in the test set, generating initial NoSQL queries.\nThe following is the prompt template used for fine-tuning the SLM in the SLM-based Query Generation process. Similar to the template for SLM-based Query Generation, it consists of the system prompt, the instruction, the NLQ, and the database schemas. A specific example is as follows: the SLM needs to convert the NLQ \u201cCount the number of products.\u201d into the NoSQL query \u201cdb.Ref_Colors.aggregate ([{$unwind: \"$Products\"}, {$group: {_id:null,count: {$sum:1}}},{$project: {_id:0, count:1}}]);\u201d"}, {"title": "4.3 Predicted Schema-driven and Retrieved Example-driven Query Refinement", "content": "Figure 3(iii) illustrates the process of query refinement based on predicted schemas and queries. Specifically, we first compute the cosine similarity of each element (natural language query (NLQ), schema predicted by the SLM-based schema predictor, and initial NoSQL query generated by the SLM-based query generator) with examples in the training set. We then perform a weighted sum to retrieve the top k most relevant reference examples. The formula for calculating sample similarity is as follows:\n$Sim = Sim_{nlq} \\times w_1 + Sim_{nosql} \\times w_2 + \u2026$\nwhere Sim represents the cosine similarity, w represents the weights, with \u201cw1=1.0, w2=0.3\u201d. The reason for this weighting is that the NLQ is an actual value, whereas the NoSQL query and other elements are predicted values. It is worth noting that the sum of the weights for all similarity measures does not necessarily have to be 1. This is because the final ranking of similarities is based on the magnitude of Sim. When all Sim values are proportionally scaled by a factor of w, the results remain unaffected. For example, a ratio of 1.0 : 0.5 : 0.5 (where the sum of weights is not equal to 1) is equivalent to 0.5 : 0.25 : 0.25 (where the sum of weights equals 1). The \u201c\u2026\u201d represents the weighted sum of the similarities of database fields and the fields displayed in the document, which are also multiplied by the weight w2 but are omitted due to space constraints. The retrieved reference examples are subsequently used to form the context for the RAG prompt. The LLM first evaluates whether the NoSQL query generated by the SLM is reasonable given the current NLQ and database schema. If it is not reasonable, the LLM adjusts the query based on the retrieved examples; if it is reasonable, the original NoSQL query is retained. The reason for adopting this retrieval strategy is to maximize the relevance of the retrieved examples to the current example, thereby enhancing the LLM's ability of refining the NoSQL query."}, {"title": "4.4 Execution Result-based Query Optimization", "content": "After the query adjustment in Section 4.3, we also designed a module for query optimization based on execution results (Figure 3(iv)). This module primarily relies on RAG technology. Consistent with the retrieval strategy introduced in Section 4.3, the query optimizer also relies on the cosine similarity of elements such as NLQ and schemas to retrieve examples. For each retrieved example, we execute the NoSQL query to obtain the execution result document, allowing the LLM to reference the mapping relationships between the NLQ, NoSQL query, and execution results to further optimize the NoSQL query. The following is the prompt template for this step:"}, {"title": "5 EXPERIMENTS AND ANALYSIS", "content": "This section shows extensive experimental results on a new dataset (TEND). We provide a detailed description of the experimental setup, evaluation metrics, and baseline models, followed by a comparative analysis of the performance between our proposed model (SMART) and the baseline models. Subsequently, we perform ablation studies and parameter experiments to validate the effectiveness of each component in SMART and explore the optimal hyperparameter settings. Finally, we present detailed case studies through specific examples to illustrate the concrete performance of each method."}, {"title": "5.1 Experimental Setup", "content": "We have partitioned the TEND dataset into training and testing sets according to the cross-domain standard, with a ratio of approximately 8:2. The resulting training and testing sets contain 14,245 and 2,775 pairs of (NLQ, NoSQL) instance, respectively. This division of the dataset allows for an analysis of how each method performs when confronted with unseen data, thereby providing a more comprehensive evaluation of the model's performance."}, {"title": "5.1.1 Dataset", "content": "We utilized a variety of popular neural network models and LLM-based prompting methods as baseline models for a comprehensive performance comparison with SMART. The models are as follows:\n\u2022 Seq2Seq: The Seq2Seq [1] model transforms natural language queries and database schemas into hidden states via an encoder, and then generates NoSQL queries through a decoder.\n\u2022 Transformer: Transformer [40] demonstrated excellent performance in tasks such as machine translation [5], dialogue systems [48], and speech recognition [47], and it is also a mainstream model in neural network models."}, {"title": "5.1.2 Models", "content": "Algorithm 2: The SMART Algorithm\nInputs: NLQ list in Test Set Q;\nDatabase list in Test Set D;\nNLQ list in Training Set Q';\nDatabase list in Training Set D';\nNoSQL list in Training Set N';\nOutput: NoSQL list N\n1 Procedure SMART (Q, D):\n2 // SLM Fine-tuning\n3  $M_{schema}, M_{nosql} \u2190 TrainSLM(SLM, Q', D', N\u2019)$\n4 // Build Vector Library\n5 V \u2190 BuildVecLib (Q', D', N')\n6 // Pipeline of SMART\n7 N \u2190 []\n8 for each (q, d) \u2208 (Q, D) do\n9 // SLM-based Schemas Prediction\n10 S \u2190 SLMPredict ($M_{schema}$, q, d)\n11 // SLM-based Query Generation\n12  $n_{gen} \u2190 SLMPredict($M_{nosql}$, q, d)\n13 // Query Adjustment\n14  $E_{adj} \u2190 Retrieve(q, S, $n_{gen}$, V)\n15  $N_{adj} \u2190 QueryAdjust(q, d, S, $E_{adj}$)\n16 // Query Optimization\n17  $e \u2190 Execute($N_{adj}$, d)\n18  $E_{opt} \u2190 Retrieve(q, S, $N_{adj}$, V)\n19  $N_{opt} \u2190 QueryOptimize($N_{adj}$, d, e, $E_{opt}$)\n20  N.append($N_{opt}$)\n21 end\n22 return N\n5.1.3 Evaluation Metrics. To evaluate the performance of models on the Text-to-NoSQL task, we introduce several metrics:\n\u2022 Exact Match (EM): This metric evaluates whether the generated query exactly matches the gold query in both structure and content. It is calculated as:\n$EM = \\frac{N_{em}}{N}$\nwhere Nem is the number of queries that fully match the gold query, and N is the total number of queries in the test set. EM provides a strict measure of syntactic and semantic alignment.\n\u2022 Query Stages Match (QSM): QSM assesses whether the key stages (e.g., match, group, lookup) in the generated query match the gold query in terms of order and keywords. It is computed as:\n$QSM = \\frac{N_{qsm}}{N}$\nwhere Nqsm is the number of queries with matching stages.\n\u2022 Query Fields Coverage (QFC): QFC measures whether the fields in the generated query cover all fields in the gold query, including database fields and query-defined fields. It is defined as:\n$QFC = \\frac{N_{qfc}}{N}$\nwhere Nqfc is the number of queries with complete field coverage.\n\u2022 Execution Accuracy (EX): This metric evaluates the correctness of the results obtained by executing the generated query on the database. It is calculated as:\n$EX = \\frac{N_{ex}}{N}$\nwhere Nex is the number of queries whose execution results match those of the gold query. EX is the most critical performance metric for evaluating Text-to-NoSQL models.\n\u2022 Execution Fields Match (EFM): EFM checks whether the field names in the execution results of the generated query match those of the gold query. It is computed as:\n$EFM = \\frac{N_{efm}}{N}$\nwhere Nefm is the number of queries with matching field names in the results.\n\u2022 Execution Value Match (EVM): EVM measures whether the values in the execution results of the generated query match those of the gold query. It is defined as:"}, {"title": "Instructing LLM", "content": "The Instructing LLM is an experimental setup designed to guide the LLM in performing Text-to-NoSQL predictions by crafting clear and precise instructional prompts.\n\u2022 Few-shot LLM: The few-shot prompting method is an important way to implement in-context learning (ICL), by enhancing a certain number of examples in the context to teach LLMs how to perform tasks in specific domains.\n\u2022 RAG for LLM: RAG technology can adaptively select examples from the knowledge base as references based on the input, which can further mitigate model hallucinations.\n\u2022 Fine-tuned Llama: The Fine-tuned Llama is an experimental setup that involves supervised fine-tuning of Llama-3.2-1B using the training set of TEND, representing the performance of model training-based approaches on the Text-to-NoSQL task.\n\u2022 SMART: SMART is the framework proposed in this paper, which, with the assistance of SLM and RAG technologies, constructs four main processes: SLM-based Query Generation, SLM-based Query Generation, query refinement based on predicted schema and retrieved examples, and execution results-based query optimization. To validate the versatility of SMART, we conducted experiments on both gpt-40-mini and deepseek-v3.\n5.1.4 Implementation Details. The SLM used in SMART and Fine-tuned Llama is Llama-3.2-1B, which is fine-tuned using a full-parameter fine-tuning strategy with a batch size set to 4. The LLM used in Instructing LLM, Few-shot LLM, RAG for LLM, and SMART is deepseek-v3, with the parameter configured as \u201ctemperature=0.0\u201d. The text-to-embedding model employed is \u201ctext-embedding-ada-002\u201d. The number of examples provided by Few-shot LLM, RAG for LLM, and SMART is 20."}, {"title": "5.2 Performance Comparison", "content": "As shown in Table 2, the Seq2Seq and Transformer models were completely unable to achieve Text-to-NoSQL predictions. By analyzing the prediction results of these two models, we found that the model outputs failed to grasp the code logic of special symbols such as '()', '\u201d', '[]', '.', and '$'. This led to a large amount of special symbol redundancy in the model outputs, such as \u201cdb aggregate ([{$ : {$: {\u201d. This issue is caused by the NoSQL syntax structure and represents a significant challenge in the Text-to-NoSQL task.\nIn addition, the three baseline methods based on the prompting method-Instructing LLM, Few-shot LLM, and RAG for LLM-achieved notable performance in the most critical metric, EX, with RAG for LLM particularly reaching an execution accuracy of 52.76%. The Fine-tuned Llama, obtained through supervised fine-tuning,"}, {"title": "5.3 Parameter Study", "content": "To explore the performance of SMART under different parameters, we conducted parameter experiments on the test set of the TEND dataset, varying the number of retrieval examples. As shown in Figure 4, we found that as the number of retrieval examples increased, SMART exhibited different performance curves across various metrics. Specifically, the accuracy of SMART on query-based metrics initially decreased and then increased. In contrast, the execution-based metrics showed a fluctuating trend. Overall, EX is the metric that best reflects the model's performance in real-world scenarios. When focusing on the EX metric, SMART achieved the highest execution accuracy of 65.08% with 20 retrieval examples."}, {"title": "5.4 Ablation Study", "content": "In this section, we conduct ablation experiments to examine the effectiveness and contribution of each major process and component in SMART. Specifically, we first evaluate SMART with all major processes included. Then, we remove or replace some key processes in SMART and assess its performance under the following configurations: (i) removing SLM-based Query Generation (w/o SP); (ii) removing Predicted Schema-driven and Retrieved Example-driven Query Refinement (w/o RF); (iii) removing execution results-based query optimization (w/o OPT); (iv) using only SLM-based Query Generation (only GEN). Additionally, we include the experimental results of Instructing LLM and RAG for LLM from the table as part of the ablation study. The purpose is to demonstrate that the high accuracy performance of SMART primarily stems from our designed complex framework rather than solely from the inherent strong performance of the LLM itself.\nThe results of the ablation experiments are shown in Table 3. The SMART configuration with all processes included performs slightly lower on all metrics except for the Query-based ones compared to other configurations, but it leads in the Execution-based metrics across all experimental setups. The w/o RF configuration performs best on the EM and QSM metrics, indicating that providing the LLM with the query and its execution results during the execution results-based query optimization process allows the LLM to better understand the structure of the query itself. However, on the EX metric, which best reflects the model's performance in real-world scenarios, SMART with all processes included outperforms all other configurations. This validates that all components in SMART effectively contribute to the overall framework. Furthermore, SMART with all major processes included significantly outperforms the Instructing LLM, RAG for LLM and only GEN configurations on the important EX and EM metrics. This indicates that the high accuracy of SMART primarily stems from the framework itself rather than the understanding and execution capabilities of the LLM or fine-tuned SLM in the Text-to-NoSQL task."}, {"title": "5.5 Case Study", "content": "Table 4 presents a case study comparing the NoSQL queries generated by various baseline methods and SMART, along with their execution results. Due to space limitations, we only showcase the performance of the most representative baseline models, including RAG for LLM, which represents prompting methods, and Fine-tuned"}, {"title": "6 RELATED WORK", "content": "Our work intersects with three key research areas: NoSQL databases, text-to-SQL systems, and natural language interfaces for data systems. In this section, we review relevant studies from each field."}, {"title": "6.1 NoSQL Databases", "content": "NoSQL databases have become a crucial component of modern data management systems, primarily due to their flexibility, scalability, and high performance. These attributes are essential for handling the large volumes of unstructured and semi-structured data commonly encountered in contemporary applications. NoSQL databases allow for efficient horizontal scaling and offer robust options for distributed data storage, making them an ideal choice for large-scale applications that require high availability and fault tolerance.\nIn the fields of databases and data mining, current research focuses on several key areas within NoSQL databases. These include enhancing scalability and reliability in distributed environments [4"}]}