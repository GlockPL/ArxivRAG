{"title": "Improving Scientific Document Retrieval with Concept Coverage-based Query Set Generation", "authors": ["SeongKu Kang", "Bowen Jin", "Wonbin Kweon", "Yu Zhang", "Dongha Lee", "Jiawei Han", "Hwanjo Yu"], "abstract": "In specialized fields like the scientific domain, constructing large-scale human-annotated datasets poses a significant challenge due to the need for domain expertise. Recent methods have employed large language models to generate synthetic queries, which serve as proxies for actual user queries. However, they lack control over the content generated, often resulting in incomplete coverage of academic concepts in documents. We introduce Concept Coverage-based Query set Generation (CCQGen) framework, designed to generate a set of queries with comprehensive coverage of the document's concepts. A key distinction of CCQGen is that it adaptively adjusts the generation process based on the previously generated queries. We identify concepts not sufficiently covered by previous queries, and leverage them as conditions for subsequent query generation. This approach guides each new query to complement the previous ones, aiding in a thorough understanding of the document. Extensive experiments demonstrate that CCQGen significantly enhances query quality and retrieval performance.", "sections": [{"title": "1 Introduction", "content": "Scientific document retrieval is a fundamental task that accelerates scientific innovations and access to technical solutions [17]. Recently, pre-trained language models (PLMs) have largely enhanced various ad-hoc searches [12, 18]. PLM-based retrievers are initially pre-trained on massive textual corpora to develop language understanding. They are then fine-tuned using vast datasets of annotated query-document pairs, enabling the models to accurately assess the relevance between queries and documents. However, in specialized domains like scientific document retrieval, constructing large-scale annotated datasets is challenging due to the need for domain expertise [4, 14, 22]. While there are a few general domain datasets (e.g., web search [2, 19]), they often fail to generalize to specialized domains [4, 47]. This remains a major obstacle for applications.\nRecently, large language models (LLMs) [5, 40, 48, 51] have been actively utilized to generate synthetic data. Given a document and a prompt including an instruction such as \"generate five relevant queries to the document\" [9, 41], LLMs generate synthetic queries for each document (Figure 1a). The generated queries serve as proxies for actual user queries. Recent developments in prompting schemes have largely improved the quality of these queries. [4, 9] show that incorporating a few examples of actual query-document pairs in the prompt leads to the generation of queries with similar distributions (e.g., expression styles) to actual queries. The state-of-the-art method [6] employs a pair-wise generation that instructs LLMs to generate relevant queries first and then relatively less relevant ones. These less relevant queries serve as natural 'hard negatives', further improving the efficacy of fine-tuning [6].\nThough effective in generating plausible queries, the existing methods lack control over the content generated, which can lead to incomplete coverage of the academic concepts in a document. Academic concepts refer to fundamental ideas, theories, and methodologies that form the contents of scientific documents. A scientific document typically explores various concepts. For example, in Table 1, the document addresses the primary task of music playlist recommendation, along with the design of classification-based models, solutions for popularity biases and data scarcity, and the utilization of audio features. For a thorough understanding of the document, training queries should comprehensively cover these concepts.\nHowever, in the absence of control over the content generated, the queries often repeatedly cover similar aspects of the document, showing high redundancy. For example, in Table 1, the generated queries (q2, q3) repeat keywords such as \u2018automated playlist creation' and 'song-to-playlist classification' already present in the previous query (q1). While these concepts are undoubtedly relevant to the document, such redundant queries cannot effectively bring new training signals. Furthermore, the queries exhibit a particularly higher lexical overlap with the document, compared to the actual user queries (\u00a74.2.1). We observe that the queries tend to repeat only a few terms extracted from the document. Given that users express the same concepts using various expressions in their queries, this limited term usage may not effectively simulate actual queries, reducing the efficacy of fine-tuning. As a naive solution, one might consider adding more instructions to the prompt, such as \u201cuse various terms and reduce redundancy among the queries\u201d. However, this still lacks systematic control over the generation and fails to bring consistent improvements (\u00a74.1.1); the improved term usage often appears in common expressions (e.g., advance, enhance, and reinforce), not necessarily enhancing concept coverage.\nWe propose Concept Coverage-based Query set Generation (CC-QGen) framework to meet two desiderata for training queries: (1) The queries should cover complementary aspects, enabling comprehensive coverage of the document's concepts, and (2) The queries should articulate the concepts in various related terms, rather than merely echoing a few phrases from the document. A key distinction of CCQGen is that it adaptively adjusts the generation process based on the concept coverage of previously generated queries (Figure 1b). We introduce a concept extractor to (1) identify the core concepts of each text and (2) uncover concept-related terms not explicitly mentioned in the document. Using this information, we discern the concepts not sufficiently covered by previous queries, and leverage them as conditions for the subsequent query generation. Table 1 shows that the queries generated with CCQGen (q2', q3') cover complementary concepts using more various related terms. Furthermore, we introduce new techniques to filter out low-quality queries and enhance retrieval accuracy using the obtained concept information. Our primary contributions are:\n\u2022 We show that existing query generation methods often fail to comprehensively cover academic concepts in documents, leading to suboptimal training and retrieval performance.\n\u2022 We propose CCQGen framework, which adaptively imposes conditions for subsequent generation based on the concept coverage. CCQGen can be flexibly integrated with existing prompting schemes to enhance concept coverage of generated queries.\n\u2022 We validate the effectiveness of CCQGen by extensive experiments. CCQGen brings significant improvements in query quality and retrieval performance over existing prompting schemes."}, {"title": "2 Preliminaries", "content": "2.1 Fine-tuning Retrieval Model\nTo perform retrieval on a new corpus, a PLM-based retriever is fine-tuned using a training set of annotated query-document pairs. For each query q, the contrastive learning loss is typically applied:\n$L = -log\\frac{e^{s_{text}(q, d^+)}}{e^{s_{text}(q, d^+)} + \\sum_{d^-} e^{s_{text}(q, d^-)}}$ (1)\nwhere $d^+$ and $d^-$ denote the relevant and irrelevant documents. $s_{text}(q, d)$ represents the similarity score between the query and a document, computed by the retriever. For effective fine-tuning, a substantial amount of training data is required. However, in specialized domains such as scientific document search, constructing vast human-annotated datasets is challenging due to the need for domain expertise, which remains an obstacle for applications [14, 22].\n2.2 Prompt-based Query Generation\nSeveral attempts have been made to generate synthetic queries using LLMs. Recent advancements have centered on advancing prompting schemes to enhance the quality of these queries. We summarize recent methods in terms of their prompting schemes.\nFew-shot examples. Several methods [4, 6, 7, 9, 13, 38] incorporate a few examples of relevant query-document pairs in the prompt. The prompt comprises the following components: $P =$"}, {"title": "3 Methodology", "content": "We present Concept Coverage-based Query set Generation (CC-QGen) framework, designed to meet two desiderata of training queries: (1) The concepts covered by queries should be complementary to each other, enabling a comprehensive coverage of the document's concepts. (2) The queries should articulate the document concepts in various related terms, rather than merely repeating phrases from the document. CCQGen consists of two major stages:\n\u2022 Concept identification and enrichment (\u00a73.1): We first identify the core academic concepts of each document. Then, we enrich the identified concepts by assessing their importance and adding related concepts not explicitly mentioned in the document. This information serves as the basis for generating queries.\n\u2022 Concept coverage-based query generation (\u00a73.2): Given the previously generated queries $Q^{m-1} = \\{q^1, ..., q^{m-1}\\}$, we compare the concepts of the document d with those covered by $Q^{m-1}$ to identify uncovered concepts. These uncovered concepts are then leveraged as conditions for generating the subsequent query $q^m$, allowing $q^m$ to cover complementary aspects of $Q^{m-1}$.\nMoreover, we propose a new technique, concept similarity-enhanced retrieval (CSR), that leverages the obtained concept information for filtering out low-quality queries and for improving retrieval accuracy (\u00a73.2.3). Figure 2 provides an overview of CCQGen.\n3.1 Concept Identification and Enrichment\nTo measure concept coverage, we first identify the core academic concepts of each document. We represent the concepts using a combination of two different granularities: topic and phrase levels (Figure 2a). Topic level provides broader categorizations of research, such as 'collaborative filtering' or 'machine learning', while phrase level includes specific terms in the document, such as 'playlist continuation' or 'song-to-playlist classifier', complementarily revealing the document concepts.\nA tempting way to obtain these topics and phrases is to simply instruct LLMs to find them in each document. However, this approach has several limitations: the results may contain concepts not covered by the document, and there is always a potential risk of hallucination. As a solution, we propose a new approach that first constructs a candidate set, and then uses LLMs to pinpoint the most relevant ones from the given candidates, instead of directly generating them. By doing so, the output space is restricted to the predefined candidate space, greatly reducing the risk of hallucinations while effectively leveraging the language-understanding capability of LLMs.\n3.1.1 Core topics identification. To identify the core topics of documents, we propose using an academic topic taxonomy [44]. In the scientific domain, academic taxonomies are widely used for categorizing studies in various institutions and can be easily obtained from the web.2 A taxonomy refers to a hierarchical tree structure outlining academic topics (Figure 2a). Each node represents a topic, with child nodes corresponding to its sub-topics. Leveraging taxonomy allows for exploiting domain knowledge of topic hierarchy and reflecting researchers' tendency to classify studies.\nCandidate set construction. One challenge in finding candidate topics is that the taxonomy obtained from the web is often very large and contains many irrelevant topics. To effectively narrow down the candidates, we employ a top-down traversal technique that recursively visits the child nodes with the highest similarities at each level. For each document, we start from the root node and compute its similarity to each child node. We then visit child nodes with the highest similarities. This process recurs until every path reaches leaf nodes, and all visited nodes are regarded as candidates. The document-topic similarity $s(d, c)$ can be defined in various ways. As a topic encompasses its subtopics, we collectively consider the subtopic information for each topic node. Let $N_c$ denote the set of nodes in the sub-tree having c as a root node. We compute the similarity as: $s(d, c) = \\sum_{j \\in N_c} cos(e_d, e_j)$, where $e_d$ and $e_j$\n3.1.3 Enriching concept information. We have identified core topics and phrases representing each document's concepts. We further enrich this information by (1) measuring their relative importance, and (2) incorporating strongly related concepts (i.e., topics and phrases) not explicitly revealed in the document. This enriched information serves as the basis for generating queries.\nConcept extractor. We employ a small model called a concept extractor. For a document d, the model is trained to predict its core topics $y^t$ and phrases $y^p$ from the PLM representation $e_d$. We formulate this as a two-level classification task: topic and phrase levels.\nTopics and phrases represent concepts at different levels of granularity, and learning one task can aid the other by providing a complementary perspective. To exploit their complementarity, we employ a multi-task learning model with two heads [28]. Each head has a Softmax output layer, producing probabilities for topics $\\hat{y^t}$ and phrases $\\hat{y^p}$, respectively. The cross-entropy loss is then applied for classification learning: $- \\sum_{i=1}^{T} \\sum_{j} y_{ij} log \\hat{y}_{ij}- \\sum_{i=1}^{P} \\sum_{j} y_{ij} log \\hat{y}_{ij}$\nConcept enrichment. Using the trained concept extractor, we compute $\\hat{y^t}$ and $\\hat{y^p}$, which reveal their importance in describing the document's concepts. Also, we identify strongly related topics and phrases that are expressed differently or not explicitly mentioned, by incorporating those with the highest prediction probabilities. For example, in Figure 2, we identify phrases 'cold-start problem', 'filter bubble', and 'mel-spectrogram', which are strongly relevant to the document's concepts but not explicitly mentioned, along with their detailed importance. These phrases are used to aid in articulating the document's concepts in various related terms."}, {"title": "3.2 Concept Coverage-based Query Generation", "content": "We present how we generate a set of queries that comprehensively cover the various concepts of a document. We first identify concepts insufficiently covered by the previously generated queries (\u00a73.2.1) and leverage them as conditions for subsequent generation (\u00a73.2.2). Then, a filtering step is applied to ensure the query quality (\u00a73.2.3).\nThis process is repeated until a predefined number (M) of queries per document is achieved. M is empirically determined, considering available training resources such as GPU memory and training time. For the first query of each document, we impose no conditions, thus it is identical to the results obtained from existing methods.\n3.2.1 Concept sampling based on query coverage. The enriched information $\\hat{y_d}$ reveals the core concepts and their importance within the document. Our key idea is to generate queries that align with this distribution to ensure comprehensive coverage of the document's concepts. Let $Q^{m-1} = \\{q^1, ..., q^{m-1}\\}$ denote the previously generated queries. Using the concept extractor, which is trained to predict core concepts from the text, we identify the concepts covered by the queries, i.e., $\\hat{y_q^t}$ and $\\hat{y_q^p}$. We use the concatenation of queries as input, denoted as Q. A high value in $\\hat{y_d}$ coupled with a low value in $\\hat{y_q}$ indicates that the existing queries do not sufficiently cover the corresponding concepts.\nBased on the concept coverage information, we identify concepts that need to be more emphasized in the subsequently generated query. We opt to leverage phrases as explicit conditions for generation, as topics reveal concepts at a broad level, making them less effective for explicit inclusion in the query. Note that topics are implicitly reflected in identifying and enriching core phrases. We define a probability distribution to identify less covered concepts as:\n$\\pi = normalize(max(\\hat{y_d^p} - \\hat{y_Q^p}, \\epsilon))$ (2)\nWe set $\\epsilon = 10^{-3}$ as a minimal value to the core phrases for numerical stability. We sample $\\lfloor \\frac{k^p'}{M} \\rfloor$ different phrases from Multinomial($\\pi$), where M is the total number of queries per document. Note that $\\hat{y}$ is dynamically adjusted during the construction of the query set.\n3.2.2 Concept conditioning for query generation. The sampled phrases are leveraged as conditions for generating the next query $q^m$. There have been active studies to control the generation of LLMs for various tasks. Recent methods [23, 57] have specified conditions for the desired outputs, such as sentiment, keywords, and an outline, directly in the prompts. Following these studies, we impose a condition by adding a simple textual instruction C: \u201cGenerate a relevant query based on the following keywords: [SAMPLED PHRASES]\". While more sophisticated instruction could be employed, we obtained satisfactory results with our choice.\nThe final prompt is constructed as [P; C], where P is an existing prompting scheme discussed in \u00a72.2. This integration allows us to inherit the benefits of existing techniques (e.g., few-shot examples), while generating queries that comprehensively cover the document's concepts. For example, in Figure 2, C includes phrases like 'cold-start problem' and 'audio features', which are not well covered by the previous queries. Based on this concept condition, we guide LLMs to generate a query that covers complementary aspects to the previous ones. It is important to note that C adds an additional condition for P; the query is still about playlist recommendation, the main task of the document.\n3.2.3 Concept coverage-based consistency filtering. After generating a query, we apply a filtering step to ensure its quality. A critical criterion for this process is round-trip consistency [1]; a query should be answerable by the document from which it was generated. Existing work [7, 9] employs a retriever to assess this consistency. Given a generated pair ($q_d, d$), the retriever retrieves documents for $q_d$. Then, $q_d$ is retained only if d ranks within the top-N results. The accuracy of the retriever is crucial in this step; a weak retriever may fail to filter out low-quality queries and also only retain queries that are too easy (e.g., high lexical overlap with the document), thereby limiting the effectiveness of training.\nWe note that relying on the existing retriever is insufficient for measuring relevance. While it is effective at capturing similarities of surface texts, the retriever often fails to match underlying concepts. For example, in Figure 2, the generated query includes phrases 'cold-start problem' and 'mel-spectrogram', which are highly pertinent to 'data scarcity' and 'audio features' discussed in the document. Nevertheless, as these phrases are not directly used in the document, the retriever struggles to assess the relevance and ranks the document low. Consequently, the query is considered unreliable and removed during the filtering process.\nConcept similarity-enhanced retrieval (CSR). We propose a simple and effective technique to enhance retrieval by using concept information. For relevance prediction, we consider both textual similarity from the retriever $s_{text}(q, d)$, and concept similarity $s_{concept}(q, d)$. We measure concept similarity using core phrase distributions, i.e., $s_{concept}(q, d) = sim(\\hat{y_q^p}, \\hat{y_d^p})$, which reveals related concepts at a fine-grained level.6 $sim(\\cdot, \\cdot)$ is the similarity function, for which use inner-product. The relevance score is defined as:\n$rel_{CSR}(q, d) = f(s_{text}(q, d), s_{concept}(q, d))$, (3)\nwhere f(,) is a function that combines the two scores. We use a simple addition after rescaling them via z-score normalization. We denote this technique as Concept Similarity-enhanced Retrieval (CSR). For filtering process, we assess the round-trip consistency using CSR. By directly matching underlying concepts not apparent from the surface text, we can more accurately measure relevance and distinguish low-quality queries. Additionally, for search with test queries (i.e., after fine-tuning using the generated data), CSR can be used as a supplementary technique to further enhance retrieval. It helps to understand test queries, which contain highly limited contexts and jargon not included in the training queries, by associating them with pre-organized concept information.\""}, {"title": "4 Experiments", "content": "Datasets. We conduct a thorough review of the literature to find retrieval datasets in the scientific domain, specifically those where relevance has been assessed by skilled experts or annotators. We select two recently published datasets: CSFCube [31] and DORIS-MAE [49]. They offer test query collections annotated by human experts and LLMs, respectively, and embody two real-world search scenarios: query-by-example and human-written queries. For both datasets, we conduct retrieval from the entire corpus, including all candidate documents. CSFCube dataset consists of 50 test queries, with about 120 candidates per query drawn from approximately 800,000 papers in the S2ORC corpus [26]. DORIS-MAE dataset consists of 165,144 test queries, with candidates drawn similarly to CSFCube. We consider annotation scores above '2', which indicate documents are 'nearly identical or similar' (CSFCube) and 'directly answer all key components' (DORIS-MAE), as relevant. Note that training queries are not provided in both datasets.\nAcademic topic taxonomy. We utilize the field of study taxonomy from Microsoft Academic [44], which contains 431, 416 nodes with a maximum depth of 4. After the concept identification step (\u00a73.1), we obtain 1, 164 topics and 18, 440 phrases for CSFCube, and 1, 498 topics and 34, 311 phrases for DORIS-MAE.\nMetrics. Following [14, 29], we employ Recall@K (R@K) for a large retrieval size (K), and NDCG@K (N@K) and MAP@K (M@K) for a smaller K (\u2264 20). Recall@K measures the proportion of relevant documents in the top K results, while NDCG@K and MAP@K assign higher weights to relevant documents at higher ranks.\nBackbone retrievers. We employ two representative models: (1) Contriever-MS [12] is a widely used retriever fine-tuned using vast labeled data from general domains (i.e., MS MARCO). (2) SPECTER-v2 [45] is a PLM specifically developed for the scientific domain. It is trained using metadata (e.g., citation relations) of scientific papers. For both models, we use public checkpoints: facebook/contriever-msmarco and allenai/specter2_base.\nBaselines. We compare various query generation methods. For all LLM-based methods, we use gpt-3.5-turbo-0125. Additionally, we explore the results with a smaller LLM (Llama-3-8B) in \u00a74.2.3. For each document, we generate five relevant queries [47].\n\u2022 GenQ [47] employs a specialized query generation model, trained with massive document-query pairs from the general domains. We use T5-base, trained using approximately 500, 000 pairs from MS MARCO dataset [32]: BeIR/query-gen-msmarco-t5-base-v1.\nCCQGen can be flexibly integrated with existing LLM-based methods to enhance the concept coverage of the generated queries. We apply CCQGen to two recent approaches, discussed in \u00a72.2.\n\u2022 Promptgator [9] is a recent LLM-based query generation method that leverages few-shot examples within the prompt.\n\u2022 Pair-wise generation [6] is the state-of-the-art method that generates relevant and irrelevant queries in a pair-wise manner.\nAdditionally, we devise a new competitor that adds more instruction in the prompt to enhance the quality of queries: Promptgator_diverse is a variant of Promptgator, where we add the instruction \u201cuse various terms and reduce redundancy among the queries\u201d.\nImplementation details. We conduct all experiments using 4 NVIDIA RTX A5000 GPUs, 512 GB memory, and a single Intel Xeon Gold 6226R processor. For fine-tuning, we use top-50 BM25 hard negatives for each query [10]. We use 10% of training data as a validation set. The learning rate is set to $10^{-6}$ for Contriever-MS and $10^{-7}$ for SPECTER-v2, after searching among {$10^{-7}$, $10^{-6}$, ..., $10^{-3}$}.\nWe set the batch size as 64 and the weight decay as $10^{-4}$. We report the average performance over five independent runs. For all methods, we generate five synthetic queries for each document (M = 5). For the few-shot examples in the prompt, we randomly select five annotated examples, which are then excluded in the evaluation process [9]. We follow the textual instruction used in [6]. For other baseline-specific setups, we adhere to the configurations described in the original papers. For the concept extractor, we employ a multi-gate mixture of expert architecture [28], designed for multi-task learning. We use three experts, each being a two-layer MLP. For the consistency filtering, we set N = 5. We set the number of enriched topics and phrases to $k_t' = 15$ and $k_p' = 20$, respectively."}, {"title": "4.1 Performance Comparison", "content": "4.1.1 Effectiveness of CCQGen. Table 2 presents retrieval performance after fine-tuning with various query generation methods. CCQGen consistently outperforms all baselines, achieving significant improvements across various metrics with both backbone models. We observe that GenQ underperforms compared to LLM-based methods, showing the advantages of leveraging the text generation capability of LLMs. Also, existing methods often fail to improve the backbone model (i.e., no Fine-Tune), particularly Contriever-MS. As it is trained on labeled data from general domains, it already captures overall textual similarities well, making further improvements challenging. The consistent improvements by CCQGen support its efficacy in generating queries that effectively represent the scientific documents. Notably, Promptgator_diverse struggles to produce consistent improvements. We observe that it often generates redundant queries covering similar aspects, despite increased diversity in their expressions (further analysis provided in \u00a74.2.1). This underscores the importance of proper control over generated content and supports the validity of our approach.\nImpact of amount of training data. In Figure 3, we further explore the retrieval performance by limiting the amount of training data, using Contriever-MS as the backbone model. The existing LLM-based generation method (i.e., Pair-wise gen.) shows limited performance under restricted data conditions and fails to fully benefit from an increasing volume of training data. This supports our claim that the generated queries are often redundant and do not effectively introduce new training signals. Conversely, CCQ-Gen consistently delivers considerable improvements, even with a limited number of queries. CCQGen guides each new query to complement the previous ones, allowing for reducing redundancy and fully leveraging the limited number of queries.\n4.1.2 Effectiveness of CCQGen with CSR. In \u00a73.2.3, we introduce CSR, designed to enhance retrieval using concept information from CCQGen. This technique aligns with the ongoing research direction of enhancing retrieval by integrating additional context not directly revealed from the queries and document [14, 29, 52, 56]. We compare CSR with two recent methods: (1) GRF [29] generates relevant contexts by LLMs. For a fair comparison, we generate both topics and keywords, as used in CCQGen. (2) ToTER [14] uses the"}, {"title": "4.2 Study of CCQGen", "content": "4.2.1 Analysis of generated queries. We analyze whether CC-QGen indeed reduces redundancy among the queries and includes a variety of related terms. We introduce two criteria: (1) redundancy, measured as the average cosine similarity of term frequency vectors of queries. A high redundancy indicates that queries tend to cover similar aspects of the document. (2) lexical overlap, measured as the average BM25 score between the queries and the document. A higher lexical overlap indicates that queries tend to reuse terms from the document.\nIn Table 4, the generated queries show higher lexical overlap with the document compared to the actual user queries. This shows that the generated queries tend to use a limited range of terms already present in the document, whereas actual user queries include a broader variety of terms. With the 'diverse condition' (i.e., Promptgator_diverse), the generated queries exhibit reduced lexical overlap and redundancy. However, this does not consistently lead to performance improvements. The improved term usage often appears in common expressions, not necessarily enhancing concept coverage. Conversely, CCQGen directly guides each new query to complement the previous ones. Also, CCQGen incorporate concept-related terms not explicitly mentioned in the document via enrichment step (\u00a73.1.3). This provides more systematic controls over the generation, leading to consistent improvements.\n4.2.2 Effectiveness of concept coverage-based filtering. Figure 4 presents the improvements achieved through the filtering step, which aims to remove low-quality queries that the document does not answer (\u00a73.2.3). As shown in Table 3, CSR largely enhances retrieval accuracy by incorporating concept information. This enhanced accuracy helps to accurately measure round-trip consistency, effectively improving the effects of fine-tuning.\n4.2.3 Results with a smaller LLM. In Table 5, we explore the effectiveness of the proposed approach using a smaller LLM, Llama-3-8B, with Contriever-MS as the backbone model. Consistent with the trends observed in Table 2 and Table 3, the proposed techniques (CCQGen and CSR) consistently improve the existing method. We expect CCQGen to be effective with existing LLMs that possess a certain degree of capability. Since comparing different LLMs is not the focus of this work, we leave further investigation on more various LLMs and their comparison for future study."}, {"title": "5 Related Work", "content": "We provide a detailed survey of LLM-based query generation in \u00a72.2. PLM-based retrieval models. The advancement of PLMs has led to significant progress in retrieval. Recent studies have introduced retrieval-targeted pre-training [11, 12], distillation from cross-encoders [54], and advanced negative mining methods [36, 53]."}, {"title": "6 Conclusion", "content": "We propose CCQGen framework to generate a set of queries that comprehensively cover the document concepts. CCQGen identifies concepts not sufficiently covered by previous queries, and leverages them as conditions for subsequent query generation. This approach guides each new query to complement the previous ones, aiding in a comprehensive understanding of the document. Extensive experiments show that CCQGen significantly improves both query quality and retrieval performance, even with limited training data.\nFuture work may explore its applicability across various domains. In particular, e-commerce [15, 16, 21] presents a promising opportunity, as users often express multi-faceted needs involving desired attributes, characteristics, or specific use cases. We expect CCQGen to better simulate user queries in such scenarios and leave further investigation for future work."}]}