{"title": "IMPROVE VISION LANGUAGE MODEL CHAIN-OF-THOUGHT REASONING", "authors": ["Ruohong Zhang", "Bowen Zhang", "Yanghao Li", "Haotian Zhang", "Zhiqing Sun", "Zhe Gan", "Yinfei Yang", "Ruoming Pang", "Yiming Yang"], "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial\nfor improving interpretability and trustworthiness. However, current training\nrecipes lack robust CoT reasoning data, relying on datasets dominated by short\nannotations with minimal rationales. In this work, we show that training VLM\non short answers does not generalize well to reasoning tasks that require more\ndetailed responses. To address this, we propose a two-fold approach. First, we\ndistill rationales from GPT-40 model to enrich the training data and fine-tune\nVLMs, boosting their CoT performance. Second, we apply reinforcement learning\nto further calibrate reasoning quality. Specifically, we construct positive (correct)\nand negative (incorrect) pairs of model-generated reasoning chains, by comparing\ntheir predictions with annotated short answers. Using this pairwise data, we apply\nthe Direct Preference Optimization algorithm to refine the model's reasoning\nabilities. Our experiments demonstrate significant improvements in CoT reasoning\non benchmark datasets and better generalization to direct answer prediction as\nwell. This work emphasizes the importance of incorporating detailed rationales\nin training and leveraging reinforcement learning to strengthen the reasoning\ncapabilities of VLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Chain-of-thought (CoT) reasoning is essential for improving the interpretability and trustworthiness\nof VLMs (Li et al., 2024; Liu et al., 2024; Chen et al., 2023; Liu et al., 2023b;a; Bai et al., 2023). As\nVLMs are increasingly applied to more difficult tasks, the ability to reason through complex problems\nbecomes essential. However, current training approaches for VLMs often rely on datasets dominated\nby short answers with limited rationales, which may restrict the models' ability to generalize to tasks\nwith comprehensive reasoning. In this work, we aim to address these limitations by providing distilled\nCoT data, introducing supervised finetuning (SFT) and reinforcement learning (RL) strategies to\nimprove VLM reasoning performance.\nWe hypothesize that developing CoT reasoning capabilities requires explicit training on data that\nincludes detailed reasoning steps. To address the scarcity of high quality CoT reasoning data,\nwe propose leveraging datasets with short ground truth annotations and employing the GPT-40\nmodel to generate reasoning paths that lead to the correct answer. Our approach encompasses a\ndiverse range of tasks, utilizing 9 datasets that demand different reasoning skills, including common"}, {"title": "2 RELATED WORK", "content": "VLM Reasoning Previous work has evaluated the reasoning capabilities of vision-language models\n(VLMs) across various domains, including mathematics (Lu et al., 2023; Wang et al., 2024), college-\nlevel questions (Yue et al., 2024), and science (Kembhavi et al., 2016; Lu et al., 2022). Studies\nsuch as Zhang et al. (2024c;a); Gao et al. (2023) focus on training VLMs to generate step-by-step\nsolutions for math problems or chart-based calculations, while Shao et al. (2024) trains VLMs for\nchain-of-thought (CoT) reasoning in object localization tasks.\nVLM/LLM Alignment VLM alignment has utilized preference modeling techniques, such as\nDirect Preference Optimization (DPO)(Ouali et al., 2024; Deng et al., 2024; Yu et al., 2024; Li et al.,\n2023; Gunjal et al., 2023; Sun et al., 2023), and Proximal Policy Optimization (PPO)(Sun et al.,\n2023), to improve factual accuracy and reduce hallucination. To improve reasoning capabilties in\nLLM, Sun et al. (2024); Setlur et al. (2024); Lu et al. (2024); Pang et al. (2024); Xie et al. (2024) use\niterative or step DPO to improve math CoT reasoning capabilities."}, {"title": "3 METHOD", "content": "As shown in fig. 2, our pipeline consists of three stages: (A) CoT\ndata distillation from GPT-40 (section 3.1), (B) SFT with CoT (and\ndirect) data to enable VLM CoT reasoning, and (C) RL for further\nenhancement of CoT reasoning. The RL stage involves generat-\ning positive (correct) and negative (incorrect) reasoning data pairs\nsampled from SFT, as detailed in section 3.3."}, {"title": "3.1 REASONING DATA DISTILLATION", "content": "To mitigate the limited availability of high-quality CoT data, we\nleverage VQA datasets with short annotations and augment them\nwith rationales generated by the GPT-40 model. We collect 193k\nvisual CoT instances to create the SHAREGPT-40-REASONING\ndataset, which we plan to release for public use. We focus on the following reasoning types as\ndemonstrated in fig. 4:\nReal-World Knowledge includes the A-OKVQA\ndataset (Schwenk et al., 2022), which covers a broad range\nof commonsense reasoning and real-world knowledge for\nanswering questions.\nChart Understanding includes the ChartQA\ndataset (Zhang et al., 2024a), which involves tasks\nlike item comparison, counting, and numerical compu-\ntation.\nDocument Understanding/Real-World Text includes\nDocVQA (Mathew et al., 2021), InfoVQA (Mathew et al.,\n2022), and TextVQA (Singh et al., 2019), focusing on\ninformation localization and extraction in industrial doc-\numents and real-world image comprehension."}, {"title": "3.2 SUPERVISED FINE-TUNING FOR CHAIN-OF-THOUGHT PREDICTION", "content": "We choose LLaMA3-LLaVA-NeXT-8B as our base architecture, whose weight is initialized with the\nOpen-LLaVA-NeXT weights\u00b9. To ensure the model handles both direct and chain-of-thought (CoT)\npredictions, we implement two types of prompts during training.\nDirect Prediction: For direct prediction tasks, we use the prompt \u201cAnswer the question with a short\nanswer\" for short-answer questions, and \u201cAnswer with the option's letter from the given choices\ndirectly\" for multiple-choice questions.\nCoT Prediction: For CoT prediction tasks, we use the prompt \"Generate a reason first and then\noutput a letter answer\" for multiple-choice questions, and \u201cGenerate a reason first and then output a\nshort answer\" for short-answer questions. In the model's response, the rationale is followed by the\nanswer, which is formatted as \u201c### Answer: \" to enable answer extraction during evaluation."}, {"title": "3.3 REINFORCEMENT LEARNING FOR ENHANCED REASONING", "content": "To further improve the quality of reasoning chains, we apply RL using the DPO algorithm to better\nalign the model's reasoning process toward more accurate predictions. The DPO algorithm requires\nboth positive and negative responses. To generate these, we use the SFT model as the policy model\n(i.e., generator), producing 32 candidate predictions per question (temperature 1.0 for short answer and\n1.2 for multiple-choice questions). Each prediction is compared with the ground truth to determine\nits correctness (fig. 2). Following the approach in Dubey et al. (2024), we select instances with an\naccuracy between 0.25 and 0.85. From these, we randomly pair positive and negative responses\ncreating up to three pairs per question.\nFormally, the dataset is denoted as $D_{DPO} = \\{(V, x, y_w, y_l)\\}$, where V is the image, x is the question,\n$y_w$ and $y_l$ are the positive and negative responses. The DPO objective is defined as below:\n$L_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = -E_{(v,x,y_w,y_l) \\sim D_{DPO}} log \\sigma (\\beta log \\frac{\\pi_{\\theta} (y_w | x, V)}{\\pi_{ref} (y_w | x, V)} - \\beta log \\frac{\\pi_{\\theta} (y_l | X, V)}{\\pi_{ref} (y_l | x, V)})$\nwhere $\\pi_{\\theta}$ is the policy model to be optimized and $\\pi_{ref}$ is the base reference model, both models are\ninitialized with SFT weights. $\\sigma$ is the logistic function and $\\beta$ is set to 0.1."}, {"title": "4 SFT EXPERIMENTS FOR CHAIN-OF-THOUGHT LEARNING", "content": "In this section, we explore how SFT can en-\nhance VLM reasoning by addressing two key\nresearch questions: (1) Can CoT reasoning be\nimplicitly learned from short responses? and (2)\nHow effectively can CoT be learned from GPT-\n40 distilled data? Additionally, we analyze the\ncomposition of CoT data across various reason-\ning capabilities and compare the performance of\nSOTA models with GPT-40."}, {"title": "4.1 TRAINING SETTING", "content": "As shown in the upper part of fig. 5, we present\nthe data composition for SFT. The training data\nincludes CoT distillation (193k instances) from\ntable 1 and corresponding short answers (193k).\nvisual math examples from G-LLaVA. To main-\nAdditionaly, for CoT data, we incorporate 16k\ntain general instruction-following capability as the base model, we include 2k randomly sampled\ninstruction data from LLaVA pretraining Liu et al. (2024). To ensure the SFT models can handle\nboth direct and CoT prompts during inference, we sample a small set of format-aligned data\u201350\nexamples from each of the 9 datasets resulting in 450 instances.\nIn the lower part of fig. 5, we outline the data composition for model training. Specifically, LLAVA-\nNEXT-FORMAT (fig. 51) serves as the baseline model, trained exclusively on format-aligned data\nto enforce the desired output format without learning any task-specific reasoning skills. In contrast,\nmodels in fig. 52 and 3 incorporate either direct or CoT datasets, enabling the model to be expert\nin one type of skill as well as following the both direct and CoT prompt styles. Finally, LLAVA-\nREASONER-SFT (fig. 5 \u2463) represents the SFT model trained on both CoT and direct data, making it\nto be expert in both types of reasoning.\nWe use the LLaMA3-LLaVA-NeXT-8B architecture, initializing the weights with Open-LLaVA-\nNeXT. All Supervised Fine-Tuning (SFT) experiments are trained for 1 epoch with a learning rate of\n5e-6 and a batch size of 32. The experiments are conducted on 8 H100 GPUs."}, {"title": "4.2 EVALUATION SETTING", "content": "We evaluate our method using a range of benchmark datasets, including A-OKVQA (Schwenk et al.,\n2022), ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), InfoVQA Mathew et al. (2022),\nTextVQA (Mathew et al., 2021), AI2D (Kembhavi et al., 2016), ScienceQA (Lu et al., 2022), and\nMathVista (Lu et al., 2023). We also conduct more evaluation on general datasets OCRBench (Liu"}, {"title": "4.3 CAN REASONING BE IMPLICITLY LEARNT FROM DIRECT PREDICTION?", "content": "Table 2 presents the performance of the models introduced in fig. 5. Since LLAVA-NEXT-8B\ntraining data contains very few CoT reasoning examples, CoT performance of lags behind direct\nprediction across most tasks. The only improvement is observed in ChartQA and MathVista with a\nmodest gain of +1.0 in CoT performance, showing CoT is helpful for calculation related tasks.\nWhen comparing model trained on direct only data (2) to that trained on format-aligned data (1), we\nobserve an average gain of +5.6 in direct prediction accuracy (65.5 \u2192 71.1) and a +2.9 improvement\nin CoT performance (62.7 \u2192 65.6). Surprisingly, closer inspection of CoT performance in calculation-\ninvolved tasks, such as ChartQA and MathVista, reveals only marginal gains (+0.6 for ChartQA COT)\nor even a performance drop (-1.7 on MathVista), which contrasts with the improvements seen on\nthe two tasks in \u2460. On text-rich tasks, positive gains (>1) are observed, with the most improvement\nseen in InfoVQA (+3.7). Significant gains are also evident in science-related tasks like AI2D (+5.1)\nand SQA (+11.0). Despite these improvements, CoT performance still trails behind direct prediction\noverall (CoT: 65.6 vs. direct: 71.1). This result suggests that training on direct only prediction may\nnot effectively help with CoT prediction."}, {"title": "4.4 How EFFECTIVE IS COT REASONING DATA?", "content": "When comparing the model trained on CoT-only data (\u2462)\nwith the one trained on format-aligned data (\u2460), we ob\nserve improvements in both direct and CoT predictions.\nDirect prediction performance increases by an average\nof +4.2 (65.5 \u2192 69.7), while CoT prediction improves\nsignificantly by +10.5 (62.7 \u2192 73.2). Notably, the CoT\nperformance of the model \u2462 surpasses its direct predic\ntion (73.2 CoT vs. 69.7 direct). Significant gains are\nobserved in calculation-intensive tasks like ChartQA and\nMathVista, with increases of +11.0 and +8.9 in CoT per\nformance, respectively. Interestingly, for text-rich tasks\nsuch as DocVQA, InfoVQA, and TextVQA, the direct\nperformance of model \u2462 (trained on CoT-only data) out\nperforms that of model \u2461 (trained on direct-only data).\nThis suggests that even for text-heavy tasks, reasoning\nprocesses, such as localizing information in documents or"}, {"title": "4.5 ABLATION TESTS ON DATA COMPOSITION", "content": "Data Composition for Math. In table 3, we examine the effectiveness of data composition on\nMathVista performance. We first include two visual math datasets: MathVision (MV) and G-LLaVA\n(GL). Including MV improves CoT performance by +3.1 over format only baseline (fig. 51), while\nadding GL yields an additional gain of +1.5. Building on MV+GL, we incorporate several datasets\nthat are potentially relevant to the task, including two math text-only datasets: MathPlus (MP)\nand MathInstruct (MI), two science datasets: SQA and AI2D, and ChartQA. Notably, ChartQA\nsignificantly boosts CoT performance (+5.5), while AI2D and SQA provide positive gains of +0.6\nand +1.5, respectively. However, adding the math text datasets results in minimal improvement.\nComparing inclusion of 100k MP vs 50k MP, more text data does not necessarily lead to better results.\nTherefore, we decided not to include them in training LLAVA-REASONER-SFT.\nData Composition for Science Tasks with CoT Prediction. In table 4, we evaluate the impact of\ndata composition on science datasets, including AI2D and SQA. Our results show that combining\nSQA and AI2D provides additional gains on both datasets, indicating that they are mutually beneficial.\nFurthermore, adding ChartQA contributes positively to both datasets, with a notable improvement of\n+0.7 for AI2D."}, {"title": "4.6 COMPARING WITH SOTA MODEL AND GPT-40", "content": "In table 5, we compare the performance of\nGPT-40 and a recent state-of-the-art model,\nCambrian Tong et al. (2024). For GPT-40,\nwe include both direct and CoT predictions,\nfollowing the prompt optimization steps out-\nlined in Borchmann (2024), with the prompts\ndetailed in appendix B. For Cambrian, we re-\nport the numbers from Tong et al. (2024) and\nreplicated the results using the official check-\npoint on MMStar, InfoVQA, and A-OKVQA.\nSpecifically for Cambrian, CoT predictions\nwere used for the MathVista dataset, while\ndirect predictions were applied for the remain-\ning datasets.\nWhen compared to open-source models, GPT-\n40 outperforms on nearly all benchmark\ndatasets, with the exception of SQA. Notably,\nsignificant improvements from CoT predic-\ntions are observed on tasks involving calcula-\ntion or complex reasoning, such as ChartQA,\nMathVista, MMMU, and MMStar.\nCambrian-7B is trained on a dataset of 7 mil-\nlion open-source instruction-following exam-"}, {"title": "5 RL EXPERIMENTS FOR ENHANCED CHAIN-OF-THOUGHT REASONING", "content": "In this section, we demonstrate the effectiveness of RL in further enhancing CoT reasoning. We\nemploy the DPO algorithm, which is directly optimized using positive and negative pairs. By\nleveraging short-answer feedback (section 3.3), we construct preference pairs across three domains:"}, {"title": "5.1 CAN DPO CALIBRATE REASONING?", "content": "In table 6, we present the results of the DPO model optimized on top of LLAVA-REASONER-SFT\n(4). Model 5 uses the SOTA RLAIF-V Yu et al. (2024) data, while model \u2465 uses our dataset. We\nobserve that Model \u2464 shows a slight improvement in both direct prediction (+0.2) and CoT prediction\n(+0.2), whereas model \u2465 demonstrates a greater improvement in CoT prediction (+1.1) with equal\ngains on direct prediction. Interestingly, though only 3 out of 8 datasets are selected to construct\nDPO pairs, gains are observed across 7 out of 8 datasets except for SQA with a slight decrease (92.9\n\u2192 92.6). These results suggest that DPO dataset constructed from model-generated rationales can\neffectively enhance reasoning accuracy and show generalization across tasks."}, {"title": "5.2 DPO AS VERIFIER FOR COT REASONING RE-RANKING", "content": "In fig. 6, we present the re-ranking results using the DPO model as a verifier, following the approach\nof Zhang et al. (2024d); Hosseini et al. (2024); Lu et al. (2024). The DPO reward score is calculated as\n$\\log \\frac{\\pi_{dpo} (y|x,V)}{\\pi_{sft} (y|x,V)}$, where V represents the image, x the question, and y the candidate answer. We explore\ntwo re-ranking strategies: Best-of-N and Weighted Voting. A Majority Voting (or self-consistency)\nbaseline is also included for comparison.\nWhen trained with RLAIF-V\ndata (5), the DPO model demon\nstrates improvements as both\na generator and verifier on A\nOKVQA, likely due to the\ndataset's alignment with real\nworld images, which matches the\nnature of A-OKVQA. Interest\ningly, while model 5 does not"}, {"title": "5.3 ADDITIONAL DPO COT PERFORMANCE ON GENERAL DATASETS", "content": "In table 7, we present the DPO CoT performance\non OCRBench, MMStar, and MMMU. We observe\nthat both DPO models outperform the SFT baseline,\nwith our DPO model trained on CoT reasoning pairs\nshowing slightly better results.\nIn fig. 7, we further explore the effectiveness of DPO\non the MMMU dataset, which consists of challeng\ning college-level subject questions. We provide re\nranking results for multiple-choice problems from\nthe Dev+Val split (988/1050). First, the SFT model\nwith self-consistency shows consistent improvements\nreaching 45.5 with 64 candidate votes. LLAVA\nREASONER-DPO, trained on reasoning data pairs\nshows strong generalization on MMMU by excelling\nin both weighted voting and best-of-N voting during\ncandidate re-ranking. While the DPO model trained\non RLAIF-V (5) improves CoT predictions, it does\nnot achieve gains in the re-ranking metrics, indicating\nlimitations in distinguishing correct from incorrect\nreasoning on more complex data. We hypothesize\nthat, compared to ChartQA, the reasoning questions\nin MMMU are more challenging and span a broader\nrange of subjects. The RLAIF-V dataset relies primar\nily on COCO image domain, which may not provide\nsufficient coverage, leading to weaker performance\nin re-ranking."}, {"title": "5.4 DPO CREDIT ASSIGNMENT", "content": "While the DPO model is trained on pairwise data\nprior works (Rafailov et al., 2024; Lu et al., 2024)\nhave shown that DPO policies can learn to predict token-level rewards from binary preference data.\nThese experiments primarily focused on math reasoning with LLMs. In this work, we provide"}, {"title": "6 CONCLUSION", "content": "In this work, we aim to improve VLM CoT reasoning. First, we collect a CoT reasoning dataset\nSHAREGPT-40-REASONING across a broad range of VQA tasks. We demonstrate that fine-tuning\non this dataset significantly enhances reasoning performance. Additionally, we further improve\nthese models using reinforcement learning with direct preference optimization, which strengthens\ntheir ability to reason and generalize to direct answer prediction tasks. Our results show that these\napproaches effectively enhance the reasoning capabilities of VLMs, paving the way for more robust\nand interpretable multimodal models."}, {"title": "A SHAREGPT-40-REASONING DATA FOR VLM COT REASONING", "content": "Figure A.1 and fig. A.2 illustrate the GPT-40 system (task) prompt and the GPT-40 distillation prompt.\nWe employ the same prompt across all VQA datasets for data distillation. Specifically, the input\nto the prompt consists of an image, a question, and a short answer. The short answer serves as a\nreference for GPT-40 to generate a CoT reasoning followed by a final answer after '### Answer'. We\nshow a few more examples in the next subsections.\nWhen provided with an image, a question, and a reference answer, generate a\nchain-of-thought step that helps derive your own answer.\nYour rationale should include detailed visual elements in order to derive the answer."}, {"title": "A.2 FILTERING MISMATCHED ANNOTATIONS IN DISTILLATION", "content": "In the GPT-40 prompt shown in fig. A.2, we treat the annotation as a reference answer and instruct\nGPT-40 to generate its own solution based on that reference. In fig. A.3 and fig. A.4, we illustrate cases\nwhere the GPT-40-generated solution differs from the annotated answer. Upon human examination,\nwe identified errors in the annotations. For example, in fig. A.3, there are issues such as incorrect text\nrecognition (e.g., \"dentist\u201d misidentified as \"heart\") and incorrect object identification (e.g., \"beer\"\nas \"water\"). In fig. A.4, the annotation errors involve incorrect calculations in the left figure and\nmiscounting in the right figure.\nTo ensure consistency and avoid potential errors, we filtered out examples where the GPT-4o generated\nanswer differs from the annotated answer. In SHAREGPT-40-REASONING, we release the SFT COT\ndata along with the original distillation and filtered examples for reference."}, {"title": "B GPT-40 EVALUATION AND PROMPT OPTIMIZATION", "content": "In this section, we present the prompts used for GPT-40 on benchmark datasets, including both\ndirect and Chain-of-Thought (CoT) predictions. Similar to the findings in Borchmann (2024), we\nobserved that GPT-40's performance is highly sensitive to prompt phrasing. We explored several sets\nof prompts and selected the best-performing ones for reporting results. Specifically, we try to align\nour results with those reported in Li et al. (2024); Tong et al. (2024), Claude 3.5 Sonnet for Vision 2,\namong others.\nPrompt Optimization We follow the process outlined in Borchmann (2024) to design effective\nGPT-40 prompts for the benchmark datasets. A random subset of 200 instances is selected as a\ndevelopment set to evaluate manually designed prompts. We manually inspect the predicted results\nand identify issues such as the model being overly cautious in declining answers, incorrect output\nformatting, or style mismatches with the ground truth labels. As an illustrative example, we detail\nthe prompt optimization process using ChartQA, and apply similar techniques to the other datasets.\nFinally, we provide the prompts used for replicating our test results."}, {"title": "C BASELINE EVALUATION", "content": "In this section, we provide evaluation details for our base model, which uses the LLAMA3-LLAVA\nNEXT-8B architecture with weights initialized from OPEN-LLAVA-NEXT. We selected OPEN\nLLAVA-NEXT weights because the data and training pipelines were fully available at the time of\nmodel development, allowing us to avoid reliance on the unreleased real user interactions referenced\nin Liu et al. (2024). The pretraining data for OPEN-LLAVA-NEXT consists of 1M image-text pairs,\nsourced from datasets such as ShareGPT4V, ALLaVA-Instruct-VFLAN-4V, DocVQA, SynDog-EN,\nChartQA, DVQA, AI2D, and GeoQA+.\nWhen evaluating LLAVA-NEXT-8B, we identified several issues, such as the inability to follow\nthe CoT prompt, refusal to answer questions, and generating irrelevant reasoning. In fig. C.1, we\npresent randomly sampled examples from LLAVA-NEXT-8B with a temperature setting of 1.0 on a\nChartQA test case. These examples demonstrate the model's difficulty in adhering to the CoT prompt.\nIn the first example, the model declines to answer the question. In the second to fourth examples, the\nmodel provides an answer first, followed by an explanation, which doesn't effectively use thought\nprocess to answer the question. In the final example, the model generates a descriptive response\ninstead of reasoning through the question, ultimately failing to provide an answer. This illustrates the\nmodel's inconsistent handling of the prompt structure."}, {"title": "D NEARLY ZERO DATA LEARNING FOR COT REASONING", "content": "In this section, we demonstrate how minimal CoT training data can enhance CoT reasoning capa\nbilities. Specifically, we use only 450 CoT format-aligned examples alongside all available direct\nprediction data, with LLAVA-NEXT-DIRECT as the baseline. We apply rejection sampling fine\ntuning (RFT) following (Sun et al., 2024; Setlur et al., 2024) to train a self-taught chain-of-thought\nreasoner, denoted as LLaVA-Next-STaR. From LLAVA-NEXT-DIRECT, we sample 32 CoT exam\nples for each training instance and select those whose final predictions match the ground truth. Up to\nthree positive examples are selected per question, resulting in a dataset of 260k RFT examples.\nAs shown in table D.1, RFT training improves both CoT reasoning and direct predictions overall, with\nthe exception of two data points. Notably, TextVQA shows a significant drop in CoT performance,\nwhich we will explore further in future work. Notable (>3%) gain is observed on ChartQA, DocVQA,\nInfoVQA, AI2D and MathVista, and roughly 1% gain is observed on direct prediction on those\ndatasets as well.\nDPO Experiments Prior to the RFT experiments, we conducted DPO experiments on the ChartQA\ndataset under the same conditions as described in section 4. However, the improvements were modest,\nwith a 72.3 (+0.5) gain in CoT prediction and a 74.2 (+0.5) gain in direct prediction. In contrast,\nRFT yielded a significant improvement, with 77.9 (+6.1) on CoT prediction and 74.6 (+0.9) on direct\nprediction. We hypothesize that for models with relatively weak CoT reasoning capabilities, RFT\nmay be more effective in enhancing model performance, whereas DPO with preference modeling\nmay be less impactful. We leave further analysis for future work."}, {"title": "E SFT ABLATION EXPERIMENTS", "content": "In table E.1, we present additional ablation experiments on SFT across each dataset, using three\nsettings: direct only, CoT only, and direct + CoT. Additionally, format-aligned data is incorporated\nduring training to enable the model to follow the specific direct or CoT format during inference."}, {"title": "F ADDITIONAL DPO EXPERIMENTS", "content": "In our initial experiments, we observed that truncating response\nlength impacts the final performance of DPO. As shown in table F.1, no truncation results in a decline\nin performance, while truncating to 90 tokens empirically produces the best results. Consequently,\nwe applied a 90-token truncation for the DPO experiments.\nWe examine the impact of RFT and compare it to the DPO\nIn table F.2, for A-OKVQA, we observe that training with A-OKVQA RFT alone yields the best\nresult for A-OKVQA; however, the model's ability to generate short answers is entirely lost. When\nformat-aligned data is added, there is a trade-off between performance on A-OKVQA and other\ndatasets.\nWhen the datasets are combined for training, we see improvements only on ChartQA, while perfor\nmance on A-OKVQA and MathVista declines. This indicates that balancing RFT across datasets is\nchallenging, especially when the SFT model already performs relatively well on basic tasks. In con\ntrast, the DPO model demonstrates consistent gains across datasets, showing better generalization."}]}