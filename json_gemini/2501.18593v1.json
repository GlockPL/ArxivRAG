{"title": "Diffusion Autoencoders are Scalable Image Tokenizers", "authors": ["Yinbo Chen", "Rohit Girdhar", "Xiaolong Wang", "Sai Saketh Rambhatla", "Ishan Misra"], "abstract": "Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.", "sections": [{"title": "1. Introduction", "content": "Image representations play an important role in the visual generative modeling of images and videos. Since visual data is high dimensional, a dominant paradigm for generative visual models is to first compress the input pixel space into a compact latent representation, then perform generative modeling in the latent space, and finally decompress the latent space back to pixel space. These compact latents have both theoretical and practical benefits. Compact latents make the generative task easier as the lower dimensional representations remove nuisance factors of variation often present in the raw input signal. The latents also allow for smaller generative models yielding both training and inference speed-ups.\nWe focus on the 'tokenizers' used to learn the latent representations (tokens) for image generation. We study the tokenizers commonly used in state-of-the-art image generation methods, which compress the images into continuous latent variables that are further used for learning a latent diffusion generative model. The image reconstruction quality of the tokenizers directly affects the quality of the generative model and thus, studying and improving the tokenizers is of increasing importance.\nThe most widely used tokenizer, GAN-LPIPS tokenizer (GLPTO), can be viewed as a supervised autoencoder that uses a combination of losses - L1, LPIPS (supervised), and GAN to reconstruct the image (see Figure 1). While effective, GLPTo is not ideal yet: (i) the combination of several losses requires tuning weights for each of the individual losses; (ii) L1 and LPIPS losses do not correctly model a probabilistic reconstruction, while it is non-trivial to scale up GANs; and (iii) the LPIPS loss is a heuristic that requires a supervised deep network feature space for image reconstruction. In practice, the GLPTo reconstructions are prone to have artifacts for structured visual input e.g., text and symbols, and high-frequency image regions as shown in Figure 2. These artifacts translate into the image generation model learned on this latent space. Inspired by these observations, we ask the question: does the image tokenizer training have to be so complex and rely on supervised models?\nDiffusion models are a theoretically sound and practically scalable technique for probabilistic modeling of images. However, the theory and practice of using them for learning representations useful for image generation remains underexplored. In this work, we show that a single diffusion loss can be used to build scalable image tokenizers. Our 'Diffusion Tokenizer' (DiTo), illustrated in Figure 1, is trained with a single diffusion L2 loss. At inference, given the latent z, the decoder reconstructs the image from the latent with a diffusion sampler.\nWe show design choices that allow us to train and scale DiTo yielding competitive or better representations than the GLPTO. We connect our training to the recent Evidence Lower Bound (ELBO) theory of diffusion models, and use an ELBO objective (Flow Matching) for the diffusion decoder which makes our learned representations maximize the ELBO of the likelihood of the input image, for which we observe the practical benefits. Furthermore, we propose noise synchronization, which aims to synchronize the noising process in the latent space to the pixels space, and allows DiTo's latent representation to be more useful for downstream image generation models.\nBeyond its simplicity, DiTo achieves competitive or better quality than GLPTo for image reconstruction, especially for small text, symbols, and structured visual parts. We also find that image generation models trained on DiTo latent representations are competitive to or outperform those trained on GLPTo representations. DiTo can easily be scaled up by increasing the size of the model without requiring any further tuning of loss hyperparameters. We find both the visual quality and reconstruction faithfulness to the input image get significantly improved when scaling up the model. Our ablations further suggest that the effectiveness of DiTo lies in jointly learning a latent representation and a decoder for probabilistic reconstruction."}, {"title": "2. Related Work", "content": "Diffusion models. Diffusion models are initially proposed and derived as maximizing the evidence lower-bound (ELBO) of data-likelihood in the early work. Later works improve various aspects of the initial diffusion model, including architecture, noise schedule, prediction type, and timestep weighting, and connect the theory to score-based generative models, making many of them no longer follow the derivation in the initial work. When being scaled-up, diffusion models beat GANs for image synthesis, and achieve success for various probabilistic modeling tasks, in particular for text-to-image and text-to-video. The sampling of diffusion models requires iterative denoising, recent efforts are made towards a faster sampler or distilling the diffusion model to a one-step generator. The recently proposed flow matching can be also viewed as a diffusion process with a specific simple noise schedule and \\(v\\)-prediction as the training objective.\nImage tokenizers. Image tokenizers are autoencoders that convert images to latent representations that can be reconstructed back. Generative models are then usually trained on the latent representations, including autoregressive models for discrete latents, and diffusion models (or autoregressive diffusion) on continuous latents. While diffusion tokenizer is applicable to both types of latents, we focus on continuous latents in this work. A continuous latent space is commonly used by recent state-of-the-art visual generative models, which is obtained by a GAN-LPIPS tokenizer (GLPTo). It uses a combination of L1, LPIPS, and GAN loss for image reconstruction, which is an empirical recipe for reconstruction that is also commonly used in super-resolution. After obtaining the latent space, a latent diffusion model can be trained with UNet or Transformer.\nDiffusion autoencoders. The use of a diffusion objective for training image tokenizers remains largely underexplored. Early works jointly train an encoder and a diffusion decoder to represent an image as a single latent vector and a noise map for reconstruction. Promising results are shown on simple datasets, while the diffusion autoencoders are mainly used for face attribute editing, and they were not connected to the ELBO objectives in recent work. DALL-E 3 trains a diffusion decoder to decode from the frozen latent space of the GLPTo, and distill the diffusion decoder to one-step with consistency model. W\u00fcrstchen trains a diffusion autoencoder to further compress the frozen latent space of a GLPTo. Concurrent to our work, SWYCC uses a diffusion model to refine a coarse prediction supervised by LPIPS loss in a joint training. e-VAE trains the autoencoder with LPIPS, GAN, and diffusion loss. Both works show that diffusion loss can be helpful in autoencoder training.\nSelf-supervised representation learning. Our work is also related to the research in self-supervised representation learning. In particular, our work leverages the long line of research into methods that leverage an autoencoder style reconstruction loss. While many of these methods are focused on representation learning for downstream recognition tasks, we focus on downstream generation tasks. We believe studying unified representations for both generation and recognition is a strong research direction for the future."}, {"title": "3. Preliminaries", "content": "Score-based models. Most of the recent state-of-the-art diffusion models are based on the theory of score-based generative models. A diffusion process gradually adds noise to data and finally makes it indistinguishable from pure Gaussian noise. Formally, given a \\(D\\)-dimensional random variable \\(x \\in \\mathbb{R}^D\\) that represents the data, the noise schedule is defined by \\(a_t, \\sigma_t\\), such that\n\\[q(x_t|x_o) = \\mathcal{N}(a_tx_o, \\sigma_t^2 I), t \\in [0, 1].\\]\nA typical design is to let \\(a_t\\) decrease from \\(a_0 = 1\\) to \\(a_1 = 0\\), and let \\(\\sigma_t\\) increase from \\(\\sigma_0 = 0\\) to \\(\\sigma_1 = 1\\), so that \\(x_1 \\sim \\mathcal{N}(0, I)\\) is a standard normal distribution.\nDiffusion models learn to estimate the score function \\(\\nabla_x \\log q(x_t)\\) for all noise levels \\(t\\). To estimate the score function, a neural network \\(\\epsilon_\\theta(x, t)\\) is trained typically with the denoising score matching objective\n\\[\\mathcal{L}(x_0) = \\mathbb{E}_{t,\\epsilon} [|\\epsilon_\\theta(x_t, t) - \\epsilon||^2],\\]\nwhere \\(\\epsilon \\sim \\mathcal{N}(0,I), x_t = a_tx_0 + \\sigma_t \\epsilon\\). After training, \\(\\nabla_x \\log q(x_t) \\approx \\epsilon_\\theta(x_t,t)/\\sigma_t\\). A sample of \\(x_0\\) can be generated by first sampling \\(x_1\\) and then iteratively reversing the diffusion process with the estimated score function using an SDE or ODE solver."}, {"title": "4. Approach", "content": "Our goal is to learn compressed latent representations of images that can be used for training latent-space image generation models. This compression is learned via a tokenizer that can compress the image from pixel space to latent space (tokens) and decompress it from latent space to pixel space. More formally, given an input image \\(x\\) in pixel space, it is passed into an encoder \\(E\\) to obtain the compact latent representation or tokens \\(z\\). The latent \\(z\\) is used as the condition for a diffusion decoder \\(D\\) that models the distribution \\(p(x|z)\\). An overview of our diffusion tokenizer (DiTo) is shown in Figure 1.\nDuring training, a noisy image \\(x_t\\) is constructed by adding noise to \\(x\\) with the forward diffusion process at random time \\(t \\in [0,1]\\), then the diffusion network \\(D\\) takes both \\(x_t\\) and \\(z\\) as input and is supervised by the Flow Matching objective. At test time, given a latent representation \\(z\\), the reconstruction image in pixel space can be decoded by first sampling Gaussian noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\), and then iteratively \"denoising\" it with reverse diffusion process conditioned on \\(z\\). \\(E\\) and \\(D\\) are jointly trained from scratch to learn the latent representation and conditional decoding together.\nTraining objective. We follow Flow Matching that is shown to be an ELBO maximization diffusion objective. The noise schedule is defined as\n\\[a_t = 1 - t, \\sigma_t = \\sigma_{\\text{min}} + t \\cdot (1 - \\sigma_{\\text{min}}),\\]\nwhere \\(\\sigma_{\\text{min}} = 10^{-5}\\). The diffusion network \\(D\\) uses \\(v\\)-prediction that is trained with the objective\n\\[\\mathcal{L}(x) = \\mathbb{E}_{t,\\epsilon} [||D(x_t, t, z) - ((1 - \\sigma_{\\text{min}})\\epsilon - x)||^2].\\]\nThe time \\(t\\) is uniformly sampled in \\([0, 1]\\).\nSimple implementation. Our implementation only uses a single L2 loss (Equation (9)). Thus, unlike GLPTo, it does not require access to pretrained discriminative models to compute LPIPS loss, or training an extra GAN discriminator in an adversarial game. Since we use a single loss, our method does not need a combinatorial search for loss weight rebalancing in contrast to GLPTO. We also observe that discarding the variational KL regularization loss for \\(z\\) in GLPTo has negligible impact on DiTo. Finally, DiTo is a self-supervised technique, unlike GLPTo that relies on pretrained supervised discriminative models in LPIPS.\nTheoretical justification. A scalable autoencoder typically requires a principled objective. We connect the finding from Kingma et al. to our diffusion autoencoder to show its theoretical basis. Given the recent results, our choice of the Flow Matching training objective can be interpreted as learning to compress the image \\(x\\) into a latent \\(z\\) while maximizing the ELBO \\(E_{q(x_t|x)}[\\log P_D(x_t|z)]\\). That is, \\(z\\) is learned to maximize the log probability density of the input \\(x\\) augmented at all noise levels \\(t\\) in the expectation. The widely used \\(\\epsilon\\)-prediction (with cosine schedule) and EDM are shown not in this ELBO form and may not directly maximize the log probability density of the input. We study the effects of these objectives in our experiments and observe the practical benefits of the ELBO objectives.\nNoise synchronization. We propose an additional regularization on the DiTo's latent representations \\(z\\) that facilitates"}, {"title": "4.1. Implementation Details", "content": "We describe the architecture and training hyperparameters for our diffusion tokenizers.\nArchitecture. The encoder \\(E\\) follows the standard convolutional encoder used in Stable Diffusion (LDM) and SDXL, with the configuration that has a spatial downsampling factor 8, and 4 channels for the latent. The decoder \\(D\\) is a convolutional UNet with timestep conditioning that follows Consistency Decoder. The \\(z\\)-condition of the diffusion model is implemented by nearest upsampling \\(z\\) and concatenation to \\(x_t\\) as the input to the decoder. While the original autoencoder in LDM applies a KL loss on the latent as in a variational autoencoder, we remove it and simply use a LayerNorm on \\(z\\), which eliminates the burden to balance an additional KL loss (see Appendix B).\nTraining. Both the encoder and diffusion decoder are jointly trained from scratch. We use AdamW optimizer, with constant learning rate 0.0001, \\(\\beta_1 = 0.9, \\beta_2 = 0.999\\), weight decay 0.01. By default, diffusion tokenizers are trained for 300K iterations with batch size 64. We refer to more details in Appendix A.2.\nInference. We choose the Euler ODE solver for simplicity, and use 50 steps to sample from the diffusion decoder \\(D\\)."}, {"title": "5. Experiments", "content": "Dataset. We use the ImageNet dataset, which is large-scale and contains diverse real-world images, to train and evaluate our models and baselines for both image reconstruction and generation. We post-process the dataset such that faces in the images are blurred. By default, images are resized to be at 256 pixel resolution for the shorter side. For tokenizer training, we apply random crop and horizontal flip as data augmentation. Images are center-cropped for evaluation.\nBaselines. We compare to the standard tokenizer used in LDM, which we refer to as GLPTo. It is widely used in recent state-of-the-art visual generative models. The tokenizer uses L1, LPIPS, and GAN loss for reconstruction. For a fair comparison, we train GLPTo using the same training data and the same architecture that matches the number of parameters to the corresponding DiTo model (see Appendix A.2). The GLPTo downsamples by a factor of 8 and produces a latent \\(z\\) of size 4 x 32 x 32.\nModels. Since the main difference of DiTo compared to the baselines is the diffusion decoder, we fix the encoder as the encoder in LDM with a downsampling factor 8 by default, and evaluate several variants of the diffusion decoder in different sizes, the settings are denoted as DiTo-B, DiTo-L, and DiTo-XL with 162.8M, 338.5M, 620.9M parameters in the decoder respectively. The architecture details are provided in Appendix A.1. The same as GLPTO, DiTo's \\(z\\) is of the size 4 x 32 x 32.\nAutomatic evaluation metrics. We evaluate the commonly used Fr\u00e9chet Inception Distance (FID) for both the reconstruction and generation. The reconstruction FID (rFID) is computed between a set of input images and their corresponding reconstructed images by the tokenizer. The generation FID (gFID) is computed between randomly generated images and the dataset images. For computation efficiency, we use a fixed set of 5K images from ImageNet validation set to evaluate rFID (which we observe to be stable, while it is typically higher than FID with 50K samples, see Appendix C). We evaluate gFID with 50K samples.\nHuman evaluation. Recent work shows that automated metrics for evaluating visual generation do not correlate well with human judgment. Thus, we also collect human preferences to compare our method and baselines. To compare the two models, we set up a side-by-side evaluation task where humans pick the preferred result. We provide the details in Appendix A.4."}, {"title": "5.1. Image reconstruction", "content": "We compare the reconstruction quality of DiTo and the baseline GLPTo. Reconstruction quality directly measures the ability of the tokenizer to learn compact latent representations (tokens) that can reconstruct the image. DiTo is trained without noise synchronization (Section 4) by default as we measure the reconstruction quality in this section.\nThe qualitative results are shown in Figure 2. Despite using a simpler loss, we observe that DiTo shows a better reconstruction quality than GLPTo, especially for regular visual structures, symbols, and text, as shown in the example images. A potential reason might be that the GLPTo relies on the heuristic LPIPS loss that matches the deep network features of the reconstructed image. While it is good for random textures, it may be not accurate enough for structured details. DiTo has principled probabilistic modeling (ELBO) for decoding images, and thus can learn to compress the common patterns, including visual structures and text appearance by compressing images using the self-supervised reconstruction loss.\nA quantitative comparison is shown in Table 1. DiTo has a higher reconstruction FID than the GLPTO. FID is computed using distance in a supervised deep network feature space. We hypothesize that the LPIPS loss heuristic plays an important role in the GLPTo to achieve a low FID as it explicitly matches supervised deep network features for the reconstruction and the ground truth. Based on this hypothesis, we train a variant of DiTo that uses an additional LPIPS loss (see Appendix E). Note that LPIPS loss is typically necessary for stability and visual quality in GLPTo training, while it is optional for DiTo. We observe that the supervised variant of DiTo with LPIPS loss achieves lowest FID while controlling for model size, i.e., DiTo-B with LPIPS outperforms a similarly sized GLPTo-B and DiTo-XL with LPIPS"}, {"title": "Scalability.", "content": "We study the scalability of DiTo on the three variants - DiTo-B, DiTo-L, and DiTo-XL, where we nearly double the decoder size across each model while keeping the encoder architecture unchanged. A qualitative comparison of the image reconstructions by these models is shown in Figure 3. We observe that both the image reconstruction quality and the reconstruction faithfulness keep improving as the model is scaled up. The improvements of scaling are also confirmed by the reduction in reconstruction FID in Table 1, where the rFID smoothly reduces with model size. However, as shown in Table 1, FID is affected by the supervised LPIPS loss, and many recent works report that it is not aligned with visual quality. Thus, we use human evaluations to compare the self-supervised DiTo and the supervised GLPTo.\nWe conduct a side-by-side human evaluation of the image reconstructions from these models and report the preference rate in Figure 4, where a preference greater than 50% indicates that a model 'wins' over the other. At sizes of B (162.8M) and L (338.5M), the supervised GLPTo's image reconstructions are preferred over those of DiTo. However, when further scaling up to XL (620.9M), we observe that self-supervised DiTo-XL's reconstructions are preferred over the GLPTO-XL. Qualitatively, we observed that the quality of GLPTo gets mostly saturated when scaling up the decoder and the failure cases are not significantly improved. In contrast, we observed many reconstruction details keep improving for DiTo with the decoder size. This result also shows that DiTo is a scalable, simpler, and self-supervised alternative to GLPTo.\nFinally, we note that while evaluating reconstructions is meaningful, in the next step, the representations from DiTo and GLPTo are used to train image generation models. We evaluate how useful these representations are for image generation in Section 5.2."}, {"title": "5.2. Image generation", "content": "We compare the performance of training a latent diffusion image generation model on the learned latent representation \\(z\\) from either DiTo or GLPTO. We follow DiT and use DiT-XL/2 as the latent diffusion model for class-conditioned image generation on the ImageNet dataset. We compare the image generations from the resulting DiT models in Table 2 and draw several observations.\nA DiT trained using DiTo without noise synchronization achieves competitive FID to a DiT trained using GLPTo suggesting that the latent image representations of DiTo are suitable for downstream image generation tasks. Note that when compared in Table 1, DiTo has a higher reconstruction FID than GLPTo with a larger gap. It suggests that the low FID advantage achieved by explicitly matching deep features may not be fully inherited in the image generation stage. A DiT trained on DiTo with noise synchronization achieves the best performance, even outperforming GLPTo in FID. This result confirms the effectiveness of DiTo as a tokenizer for image generation."}, {"title": "5.3. Ablations and Analysis", "content": "We now present ablations of our design choices and analyze the key components of DiTo. We follow the same experimental setup as in Section 5.1.\nTraining objectives. As described in Sections 3 and 4, our DiTo uses a Flow Matching objective which can be viewed as an ELBO maximization for image reconstruction. In contrast, as shown in , the widely used diffusion implementations such as \\(e\\)-prediction (with cosine noise schedule) and EDM are not ELBO objectives. We now study the impact of this by training three variants of DiTo and changing the training objective only. We show the examples of the reconstructions in Figure 5. Using the ELBO objectives of Flow Matching and \\(v\\)-prediction (with cosine schedule, which is also an ELBO objective) yields image reconstructions that are more faithful to the input image. The non-ELBO objectives of \\(e\\)-prediction and EDM yield reconstructions sometimes with a noticeable loss of faithfulness, e.g., color shift. To further investigate this, we start with a pretrained GLPTo encoder and keep it frozen while learning diffusion decoders from scratch with the different training objectives. We observe that the image reconstructions do not have such obvious color shift, suggesting that the non-ELBO objectives can 'decode' correctly but may lead to learning sub-optimal latent representations. A potential reason might be that the non-ELBO objectives have a non-monotonic weight function \\(w(\\lambda)\\) for different log SNR ratios, which makes some terms contribute negatively in Equation (4), and leads to training noise or bias for reconstruction.\nEffectiveness of the latent representation vs. decoder. We now study whether the effectiveness of DiTo vs. GLPTO mainly comes from the decoder's powerful probabilistic modeling or from jointly learning both a powerful latent \\(z\\) and the decoder. We train a DiTo decoder-only on a frozen latent space from a GLPTo and compare the reconstructions to the GLPTo in Figure 6. We observe that both reconstructions look qualitatively similar, and have the same error modes around visual text reconstruction. When compared with reconstructions from an end-to-end DiTo, we observe qualitative differences, e.g., the visual text reconstruction is clearer. This suggests that DiTo's effectiveness lies in jointly learning a powerful latent \\(z\\) that is helpful to the probabilistic reconstruction objective of the decoder."}, {"title": "6. Conclusion and Discussion", "content": "We showed that diffusion autoencoders with proper design choices can be scalable tokenizers for images. Our diffusion tokenizer (DiTo) is simple, and theoretically justified compared to prior state-of-the-art GLPTo. DiTo training is self-supervised compared to the supervised training (LPIPS) from GLPTo. Compared to GLPTo, we observe that DiTo's learned latent representations achieve better image reconstruction, and enable better downstream image generation models. We also observed that DiTo is easier to scale and its performance improves significantly with scale.\nThere are several directions to be further explored for diffusion tokenizers. Our work only explored learning tokenizers for a downstream image generation task. We believe learning tokenizers that work well for both recognition and generation tasks will greatly simplify model training. We also believe content-aware tokenizers that can encode the spatially variable information density in images will likely lead to higher compression. Finally, this paper only studies diffusion tokenizers for images. We believe extending this concept to video, audio, and other continuous signals will unify and simplify training."}, {"title": "Social Impact", "content": "Our method is developed for research purpose, any real world usage requires considering more aspects. DiTo is an image tokenizer, the reconstructed image is perceptually similar but not exactly the same as the input image. The generative diffusion decoder and latent diffusion model may learn unintentional bias present in the dataset statistics."}, {"title": "A. Experiment Details", "content": "A.1. Architecture\nWe follow the encoder in LDM and the decoder in Consistency Decoder. Both the encoder and decoder are fully convolutional. The UNet diffusion network contains 4 stages, each stage contains 3 residual blocks. In the downsampling phase of the UNet, stages 1 to 3 are followed by an additional residual block with downsampling rate 2. The number of channels in 4 stages are \\(c_1, c_2, c_3, c_3\\) correspondingly. The upsampling phase of the UNet is in reverse order accordingly. The time in the diffusion process is projected to a vector with \\(t_{emb}\\) dimension and modulates the convolutional residual blocks. The configurations used for our diffusion tokenizers are summarized in Table 3.\nA.2. Tokenizer training\nIn the tokenizer training stage, the model is trained with batch size 64 for 300K iterations, which takes about 432, 864, 1728 NVIDIA A100 hours for DiTo-B, DiTo-L, and DiTo-XL models correspondingly. The training loss curves are shown in Figure 7. When scaling up the model, the loss of flow matching objective keeps improving and we did not observe it to be saturated yet. The corresponding baselines GLPT0-B, GLPTo-L, and GLPTo-XL take longer time per training iteration than their DiTo counterparts due to their additional LPIPS and discriminator networks. For all GLPTo, we follow the standard training setting in LDM for models with downsampling factor 8, where the loss weights \\(\\lambda_{L1} = 1.0, \\lambda_{LPIPS} = 1.0, \\lambda_{GAN} = 0.5\\), the gradient norm of regression loss (L1 + LPIPS) and GAN loss are adaptively balanced during training, and the GAN loss is enabled after 50K iterations. To speed up training, we use mix-precision training with bfloat16.\nA.3. Latent diffusion model training\nWe train DiT-XL/2 as the latent diffusion model for the learned latent space. We follow the standard setting that uses batch size 256, Adam optimizer with learning rate \\(1 \\cdot 10^{-4}\\), no weight decay, and horizontal flips as data augmentation. Flow Matching is used as the training objective. We use classifier-free guidance 2 to generate the samples. To efficiently compare the models, the latent diffusion model is trained for 400K iterations for all tokenizers.\nA.4. Human evaluation\nWe use Amazon Mechanical Turk (MTurk) to collect human preferences for reconstruction and compare the methods. In the evaluation interface, we first present a few examples of better reconstructions and equally good reconstructions, where for better reconstructions, the number of examples is equal for different methods. The worker is presented with three images"}, {"title": "B. Ablation on LayerNorm", "content": "Unlike GLPTO which uses a KL regularization loss on the latent \\(z\\), in DiTo we only apply LayerNorm on the latent representation \\(z\\) for both tokenizer and latent diffusion model training. The ablation on this design choice is shown in Table 5. We observe that using LayerNorm has a better reconstruction performance than KL loss for DiTo, and has a competitive performance for image generation. While the weight of KL loss is originally optimized for GLPTo and further sweeping the weight for DiTo may potentially improve the result, we choose LayerNorm for simplicity. There are several main reasons for using LayerNorm in DiTo: (i) The KL loss introduces an additional loss weight to tune, which is not convenient in practice; (ii) Noise synchronization supervises \\(z_t = a_t z_o + \\sigma_t \\epsilon\\), LayerNorm ensures \\(z_o\\) to have 0 mean and 1 std so that it does not collapse to the trivial solution; (iii) LayerNorm shows a better reconstruction performance. Moreover, with LayerNorm, the latent representation \\(z\\) no longer needs to be normalized for training the latent diffusion model."}, {"title": "C. Comparison to rFID with 50K Samples", "content": "For computation efficiency, we evaluate the reconstruction FID on a fixed set of 5K samples. In Table 7, we compare the metric evaluated with 5K samples and 50K samples. The FID evaluated with 50K samples typically has a smaller value than the one evaluated with 5K samples, while it preserves the order in comparison between different methods. We observe FID with 5K samples to be stable enough to compare different checkpoints of the same method or different methods, therefore we mainly compare with FID@5K for more efficient evaluation."}, {"title": "D. Evaluation on other metrics", "content": "We evaluate the autoencoder models on other common metrics for reconstruction, the results are shown in Table 6. Note that GLPTO-XL and DiTo-XL (+LPIPS) are trained with the LPIPS loss. We observe that DiTo-XL has the best PSNR and SSIM. For the metrics associated with the deep network features, LPIPS and Inception Score (IS), GLPTo-XL and DiTo-XL (+LPIPS) achieve better results as they explicitly match the deep features in training (LPIPS loss), while DiTo-XL (+LPIPS) achieves the best results on LPIPS and IS."}, {"title": "E. DiTo with LPIPS Loss", "content": "In DiTo, the diffusion decoder is trained with Flow Matching objective and does not directly output an image. To apply the LPIPS loss, we need to first convert it to the diffusion model's sample-prediction \\(z = \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [x_0|x_t]\\), then supervise the sample-prediction with LPIPS loss, so that the gradient can be backpropagated. In general scenarios of diffusion decoders, assume the diffusion network prediction \\(f_\\theta(x_t)\\) is minimizing the L2 loss to \\(A_tx_0 + B_t\\epsilon\\), we have\n\\[\\begin{bmatrix} x_t \\\\ f_\\theta(x_t) \\end{bmatrix} = \\mathbb{E}_{q(x_0,\\epsilon,x_t)} \\begin{bmatrix} x_t \\\\ A_tx_0 + B_t\\epsilon \\end{bmatrix} ,\\]\nwhere \\(\\bar{\\epsilon} = \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [\\epsilon|x_t], f_\\theta^*\\) is the network prediction at optimal network point \\(\\theta^*\\). This is because\n\\[x_t = \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [x_tx_t]\\]\n\\[= \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [x_tx_0 + \\sigma_t\\epsilon x_t]\\]\n\\[= a_t \\cdot \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [x_0|x_t] + \\sigma_t\\cdot \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [\\epsilon|x_t]\\]\n\\[= a_t \\bar{x} + \\sigma_t\\bar{\\epsilon},\\]\nand the optimal prediction under L2 loss is\n\\[f_\\theta^* (x_t) = \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [A_tx_0 + B_t\\epsilon]\\]\n\\[= A_t \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [x_0|x_t] + B_t \\cdot \\mathbb{E}_{q(x_0,\\epsilon,x_t)} [\\epsilon|x_t]\\]\n\\[= A_t\\bar{x} + B_t\\bar{\\epsilon}.\\]\nAccording to Equation (10), we have\n\\[\\begin{bmatrix} \\bar{x} \\\\ \\bar{\\epsilon} \\end{bmatrix} = \\begin{bmatrix} A_t & B_t \\\\ \\sigma_t & A_t \\end{bmatrix}^{-1} \\begin{bmatrix} x_t \\\\ f_\\theta^* (x_t) \\end{bmatrix} ,\\]\nIn the Flow Matching we used,\n\\[\\begin{bmatrix} A_t & B_t \\\\ \\sigma_t & A_t \\end{bmatrix} = \\begin{bmatrix} 1-t & t \\\\ -1 & 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 1-t & t \\\\ -1 & 1 \\end{bmatrix},\\]\nTherefore, the sample prediction is\n\\[\\bar{x}_\\theta (x_t) = x_t - t \\cdot f_\\theta (x_t) ,\\]\non which we apply the LPIPS loss. Intuitively, it can be also viewed as a \u201cone-step generation\" under \\(v\\)-prediction. Our weight for the LPIPS loss is 0.5."}, {"title": "F. Zero-Shot Generalization of Tokenization for Higher-Resolution Images", "content": "We observe that our diffusion tokenizer, when trained for images at fixed 256 \u00d7 256 pixels resolution, can generalize to a higher resolution at inference time (without any further training). We show examples for generating images at 512 \u00d7 512 resolution in Figure 8, and evaluate the corresponding reconstruction FID in Table 8. From the quantitative results at 512 \u00d7"}]}