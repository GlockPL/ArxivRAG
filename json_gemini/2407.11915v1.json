{"title": "Imitation of human motion achieves natural head movements for\nhumanoid robots in an active-speaker detection task", "authors": ["Bosong Ding", "Murat Kirtay", "Giacomo Spigler"], "abstract": "Head movements are crucial for social human-\nhuman interaction. They can transmit important cues (e.g.,\njoint attention, speaker detection) that cannot be achieved\nwith verbal interaction alone. This advantage also holds for\nhuman-robot interaction. Even though modeling human mo-\ntions through generative Al models has become an active\nresearch area within robotics in recent years, the use of\nthese methods for producing head movements in human-\nrobot interaction remains underexplored. In this work, we\nemployed a generative AI pipeline to produce human-like head\nmovements for a Nao humanoid robot. In addition, we tested\nthe system on a real-time active-speaker tracking task in a\ngroup conversation setting. Overall, the results show that the\nNao robot successfully imitates human head movements in a\nnatural manner while actively tracking the speakers during the\nconversation. Code and data from this study are available at\nhttps://github.com/dingdingding60/Humanoids2024HRI.", "sections": [{"title": "I. INTRODUCTION", "content": "Head movements are important to generate nonverbal cues\nthat enhance human-human interactions. For example, a\nstudent and teacher can construct joint attention by using\nhead movements or a child can attend a speaker in group\nconversation to maintain a social interaction. Given the sig-\nnificance of head movement in group interaction, studies on\nhuman-robot interaction have also attempted to model natural\nhead movements in interactive robots [1]. However, work on\nhuman-like motion generation for robots, particularly in real-time motion generation settings, remains less-explored.\nAlthough modeling human (or animal) motion has been\nextensively explored in generative artificial intelligence, stud-\nies have so far been mostly limited to applications that in-\nvolve locomotion, manipulation, and robot-robot interaction\n(e.g., playing soccer) [2], [3], [4], [5], [6], [7]. Here, we adopt\na generative modeling approach to imitate head movement\nof humans by using a Nao humanoid robot in a human-robot\ninteraction (HRI) scenario. To be concrete, we formulate the\nfollowing research question: How can we produce human-\nlike head movements via generative modeling on a humanoid\nrobot tasked to detect and track active speakers in an HRI\nsetting?\nTo answer this question, we designed HRI experiments in\nwhich a Nao humanoid robot participates group interaction\nto recognize active speakers. In this setting, the Nao robot\ngenerates head movement trajectories through a variational\nautoencoder (VAE), to reproduce motion similar to human\ndemonstrations. The robot then passively participates in"}, {"title": "II. RELATED WORK", "content": "Although generative modeling of human motion is an\nactive research area in robotics [3], [5], [6], its use for\nimitating head movements remains limited. For example,\nGrassi et al. [9] developed predefined control policies for\nthe Pepper robot in a group conversation setting. The authors\nshowed that the balanced attention provided by the Pepper\nrobot enhances conversational dynamics and reduces the\nlikelihood of subgroup formation among the participants.\nSimilarly, Barot et al. employed a REEM-C humanoid robot\nto interact with multiple participants to detect active speakers\nvia their head movements using multimodal data. In addition\nto quantitative results, the authors provided a human subject\nstudy with 5 participants to assess interaction along three\ndimensions: naturalness, accuracy, and responsiveness [1].\nWithin the context of Active-Speaker Detection (ASD),\nprevious studies have mostly focused on offline datasets\nas benchmarks, such as the Columbia ASD dataset [10]\nand the AVA Active Speaker dataset [11], rather than real-time use on social robots. For example, Liao et al. [8]\nimplemented an end-to-end deep learning architecture that\noutperforms different models, as evaluated using these two\ndatasets. Jung et al. [12] proposed an active speaker detection\nmodel, TalkNCE, using contrastive learning with audiovi-"}, {"title": "III. METHODS", "content": "Modeling of human motion is achieved by first training\na variational autoencoder (VAE) to learn the distribution\nof motion trajectories via unsupervised learning. Next, a\nmultilayer perceptron (MLP) is trained to map end fixation\npoints of the training trajectories into corresponding latent\nvectors learned by the VAE encoder. Evaluation of the\ngenerated motions is performed in an active-speaker gazing\ntask, where target fixations are determined using a pre-\ntrained active-speaker recognition model (Light-ASD [8]).\nThe overall system integration is shown in Fig. 1.\nWe designed an experimental protocol to collect head\nmovement data covering a large range of motion. A single\nparticipant wore a GoPro Hero 11 sports camera mounted\non a head strap and positioned close to the eyes. During\nthe experiment, data were collected from both the camera\n(not used in this study) in a linear lens mode at a resolution\nof 2.7K with a 4:3 aspect ratio, and from the inertial\nmeasurement unit (IMU) built into the camera. Data from\nboth the camera and the IMU were sampled at 30 frames\nper second."}, {"title": "B. Head trajectory modelling", "content": "We took an approach similar to previous work on motion-\ncapture trajectory modeling [2]-[4], and chose to model\nhuman head-movement trajectories using a variational au-\ntoencoder (VAE) [18]. Full architecture and hyperparameters\nare included in Appendix A.\nWe extracted individual trajectories from each trial in the\ntraining set by automatically identifying their beginning and\nend. The start of a fixation trajectory is detected by finding\nthe first timestep where the magnitude of change ||\u2206\u03c4|| is\nabove a threshold of 0.1, then subtracting a fixed offset of"}, {"title": "C. Generation of target trajectories", "content": "The VAE is trained via unsupervised learning to match the\ndistribution of human trajectories. To be useful in practical\napplications, however, we need a way to generate trajectories\nwith desired properties. We are in particular interested in\ngenerating trajectories whose end point is at or close to a\ngiven target location. For example, if we wished to produce\na fixation to a target (yaw, pitch) = (-60\u00b0, 20\u00b0), we would\nlike to generate a trajectory similar to the pink one in the top-left corner of Figure 3. Note that all trajectories are relative to\nthe current fixation, given by an initial yaw and pitch angle.\nWe generate trajectories to given targets as follows. We\nprocess the dataset to calculate the end (yaw, pitch) fixation\npoint for each clip of human trajectories. We then scale\nthe fixation points by constant factors to approximately"}, {"title": "D. Case study: active-speaker gazing task", "content": "We design a human-robot interaction scenario that involves\nthe generation of repeated head movements to investigate\nthe quality and naturality of the movements provided by\nour method, compared to a baseline where yaw/pitch head\nmovements were driven by the default motion controller of\nthe Nao robot. Due to the limitations of the narrow viewing\nangle (approximately 60\u00b0 horizontally) of the built-in camera\nand the need for wider visibility to effectively interact with\nmultiple participants, we attached a Logitech webcam (90\u00b0\nfield of view horizontally) to the top of Nao's head with a\n3D-printed head mount based on Dhionis Sako's project.\nWe chose an Active Speaker Detection (ASD) task that\ninvolves identifying the active speaker in a group conversa-\ntion with multiple potential speakers, in which the robot is\na passive participant. The objective of ASD is to determine\na function f of an audio A(t) and visual V(t) data stream\n$$S(t) = f(A(t), V(t))$$\nsuch that S(t) \u2208 {0,1}N is a vector whose components\nat each timestep t are Si(t) = 1 if speaker i \u2208 {1,..., N}\nis active and 0 if it is not active.\nHere, we use the pre-trained Light-ASD model from\nLiao et al. [8] to implement the function S(t). First, we\ndetect bounding boxes for all faces seen by the Nao's\ncamera in each frame. The detected faces are resized and\norganized into candidate speaker tracks using temporal In-\ntersection Over Union (IOU) scores. For each candidate\nspeaker, the sequence of face images and corresponding\naudio (shared across all speaker tracks) are processed inde-\npendently through audio and visual encoders. The resulting\naudio and visual features are then concatenated. The com-\nbined feature vectors are then processed sequentially through\na bidirectional Gated Recurrent Unit (GRU), followed by a\nmultilayer perceptron.\nAlthough Light-ASD is significantly faster than other\nstate-of-the-art ASD models, we found it to still be too slow\nfor use in real-time applications. This is likely due to the\nmodel having been developed primarily for offline processing\nof video files. As part of this work, we optimized the model\nby improving the preprocessing code to run in-memory and\nreplacing the original face detection model with the faster"}, {"title": "E. Preliminary experiment on human preferences", "content": "We further test the subjective perception of the quality\nand naturalness of the generated motions by designing a\npreliminary experiment with, n = 9, human participants.\nFor the experiment, we collected 3 videos centered on the\nNao's head for both our method and a baseline motion\ncontroller. We then asked the participants to select 3 out of\n6 videos with the following instructions: \"Choose 3 out of\nthe 6 videos that in your opinion show the most natural and\nengaging movement of the Nao robot. Please note that head\nmovements (i.e., target fixation points) were chosen using\nthe same algorithm in all videos.\""}, {"title": "IV. RESULTS", "content": "We evaluate the results of our approach in two stages.\nFirst, we look into the quality of the individual components\nto assess the quality of the generated head movement trajec-\ntories. We then evaluate the integrated system in a case study\nwhere a Nao robot needs to track a conversation between\nhuman co-participants."}, {"title": "A. Trajectory modelling and generation", "content": "We first performed a qualitative analysis of the VAE by\ngenerating 200 random trajectories (i.e., sampling random\nlatent vectors to input into the trained decoder) and com-\nparing them to the trajectories of human head movement\nfrom the dataset we collected (see Figure 3). The generated\ntrajectories are shown in Figure 4 (a). A sample of individual\ntrajectories reconstructed by the autoencoder is available in\nthe Appendix as Figures 7 and 8.\nWe next evaluated whether trajectories to specific target\nfixations could be effectively generated using the trained\nMLP from Section III-C. We did so by selecting 21 \u00d7 21\n(normalized) target fixation points within a uniform grid\n[-1,1] \u00d7 [-1,1], and generating trajectories to reach each\nof them. We then looked at the final fixation points reached"}, {"title": "B. Case study: active-speaker gazing task", "content": "We evaluated the quality of the generated motions on a\nNao robot during an active-speaker gazing task in two steps.\nWe first determined that the target trajectories generated\nusing our method are effectively tracked by the robot motors,\nand that they are qualitatively different from a baseline where\nmovements are obtained using the default Nao controller. We\ncollected data in a group conversation setting by letting the\nrobot passively participate in a conversation between three\nhuman interlocutors. During the conversations, we recorded"}, {"title": "V. DISCUSSION AND CONCLUSIONS", "content": "In this paper, we have addressed an often overlooked\nproblem in human-robot interaction: imitating human head\nmovements to track active speakers in a group conversation.\nSpecifically, we addressed this problem by demonstrating\nhow human data can be effectively used to generate head\nmovements for a Nao robot by showing the efficacy of our\nmotion controller in an active-speaker tracking task. The\nresults indicate that the robot can move its head toward active\nspeakers with natural-looking, well coordinated movements.\nWe suggest that this study could be further extended in the\nfollowing ways. On the one hand, the same approach can be\napplied to upper body imitation for generating nonverbal cues\n(e.g., pointing, nodding, fidgeting) during the human-robot"}, {"title": "APPENDIX", "content": "We include the full set of hyperparameters used for train-ing the variational autoencoder in Table I (see Section III-B),and the set of hyperparameters of the multilayer perceptronused to map target fixations into latent vectors in Table II(see Section III-C)."}]}