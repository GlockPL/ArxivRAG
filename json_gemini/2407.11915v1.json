{"title": "Imitation of human motion achieves natural head movements for humanoid robots in an active-speaker detection task", "authors": ["Bosong Ding", "Murat Kirtay", "Giacomo Spigler"], "abstract": "Head movements are crucial for social human-human interaction. They can transmit important cues (e.g., joint attention, speaker detection) that cannot be achieved with verbal interaction alone. This advantage also holds for human-robot interaction. Even though modeling human motions through generative Al models has become an active research area within robotics in recent years, the use of these methods for producing head movements in human-robot interaction remains underexplored. In this work, we employed a generative AI pipeline to produce human-like head movements for a Nao humanoid robot. In addition, we tested the system on a real-time active-speaker tracking task in a group conversation setting. Overall, the results show that the Nao robot successfully imitates human head movements in a natural manner while actively tracking the speakers during the conversation. Code and data from this study are available at https://github.com/dingdingding60/Humanoids2024HRI.", "sections": [{"title": "I. INTRODUCTION", "content": "Head movements are important to generate nonverbal cues that enhance human-human interactions. For example, a student and teacher can construct joint attention by using head movements or a child can attend a speaker in group conversation to maintain a social interaction. Given the sig-nificance of head movement in group interaction, studies on human-robot interaction have also attempted to model natural head movements in interactive robots [1]. However, work on human-like motion generation for robots, particularly in real-time motion generation settings, remains less-explored.\nAlthough modeling human (or animal) motion has been extensively explored in generative artificial intelligence, studies have so far been mostly limited to applications that in-volve locomotion, manipulation, and robot-robot interaction (e.g., playing soccer) [2], [3], [4], [5], [6], [7]. Here, we adopt a generative modeling approach to imitate head movement of humans by using a Nao humanoid robot in a human-robot interaction (HRI) scenario. To be concrete, we formulate the following research question: How can we produce human-like head movements via generative modeling on a humanoid robot tasked to detect and track active speakers in an HRI setting?\nTo answer this question, we designed HRI experiments in which a Nao humanoid robot participates group interaction to recognize active speakers. In this setting, the Nao robot generates head movement trajectories through a variational autoencoder (VAE), to reproduce motion similar to human demonstrations. The robot then passively participates in conversation with human partners by paying attention to the active speaker. We note that like most of the robots (e.g., Pepper) used in HRI studies, our robot can only move its head along the in yaw and pitch directions. Despite this limitation, we found that it is possible to match the corre-sponding human head movements, achieving natural-looking motion during the task. Moreover, we significantly improved the inference time for detecting the active speaker compared to state-of-the-art results, achieving an 85% reduction in inference time\u2014from 1.3 seconds to 0.2 seconds to process 1 second of video at 30 fps [8].\nThe novel contributions of this work are as follows. First, we developed an human-motion modeling pipeline on a Nao humanoid robot to generate natural head movement while detecting active speakers in a group conversation setting. We note that our proposed pipeline can also be deployed in different humanoid robots, such as the iCub or Pepper, to mention a few. Second, we present an extensive analysis of a proof-of-concept case study on an active-speaker interaction task. Lastly, we provide a new dataset of human head-gaze motion together with the trained models and benchmarked results."}, {"title": "II. RELATED WORK", "content": "Although generative modeling of human motion is an active research area in robotics [3], [5], [6], its use for imitating head movements remains limited. For example, Grassi et al. [9] developed predefined control policies for the Pepper robot in a group conversation setting. The authors showed that the balanced attention provided by the Pepper robot enhances conversational dynamics and reduces the likelihood of subgroup formation among the participants. Similarly, Barot et al. employed a REEM-C humanoid robot to interact with multiple participants to detect active speakers via their head movements using multimodal data. In addition to quantitative results, the authors provided a human subject study with 5 participants to assess interaction along three dimensions: naturalness, accuracy, and responsiveness [1].\nWithin the context of Active-Speaker Detection (ASD), previous studies have mostly focused on offline datasets as benchmarks, such as the Columbia ASD dataset [10] and the AVA Active Speaker dataset [11], rather than real-time use on social robots. For example, Liao et al. [8] implemented an end-to-end deep learning architecture that outperforms different models, as evaluated using these two datasets. Jung et al. [12] proposed an active speaker detection model, TalkNCE, using contrastive learning with audiovi-sual data that achieves state-of-the-art results on the AVA-ActiveSpeaker dataset. Alcazar et al. [13] introduced an end-to-end network incorporating interleaved graph neural network blocks to aggregate spatio-temporal context using the AVA-ActiveSpeaker dataset. The above studies on active speaker detection utilize existing datasets. However, only a few studies were aimed at real-time active-speaker detection and they focused on hardware or voice localization [14], [15].\nOur work differs from previous work in the following way. First, unlike the head movement studies introduced above, we follow a generative modeling approach to produce natural head movement for a Nao robot in a human-robot interaction setting. Second, our active-speaker detection pipeline was deployed on a Nao robot to achieve real-time active speaker detection in group conversation experiments. Lastly, we conducted experiments in a realistic real-world setting where environmental noises and hardware constraints need to be considered and can affect the system's performance."}, {"title": "III. METHODS", "content": "Modeling of human motion is achieved by first training a variational autoencoder (VAE) to learn the distribution of motion trajectories via unsupervised learning. Next, a multilayer perceptron (MLP) is trained to map end fixation points of the training trajectories into corresponding latent vectors learned by the VAE encoder. Evaluation of the generated motions is performed in an active-speaker gazing task, where target fixations are determined using a pre-trained active-speaker recognition model (Light-ASD [8]). The overall system integration is shown in Fig. 1.\nWe designed an experimental protocol to collect head movement data covering a large range of motion. A single participant wore a GoPro Hero 11 sports camera mounted on a head strap and positioned close to the eyes. During the experiment, data were collected from both the camera (not used in this study) in a linear lens mode at a resolution of 2.7K with a 4:3 aspect ratio, and from the inertial measurement unit (IMU) built into the camera. Data from both the camera and the IMU were sampled at 30 frames per second.\nThe setup consisted of a table next to a wall, on top of whose were positioned three sets of evenly distributed 3x3 reference points to cover the visual field of the test subject. The setup is shown in Figure 2. The first set covers a standard range directly in front of the subject, the second set spans a smaller range also in front, and the third set is positioned horizontally on the table. These arrangements of reference sets are designed to cover the majority of typical head movements. The distances between the reference points were determined based on the maximum rotation of the human head's yaw and pitch movements, as documented by Gilman et al. [16]. The smaller frontal range spans a range approximately half of the main one to imitate small and frequent head movement that often happens during human-robot interaction.\nTo ensure comprehensive spatial coverage, the participant was asked to systematically move through every possible combination of starting and ending points across each 3x3 grid, marking the beginning and end of each motion by pressing a button. To prevent any bias related to the relative positioning of the head and camera, all data is collected in a single session lasting approximately 30 minutes. During data collection, the participant was instructed to minimize the gaze movement as the Nao robot has no eye movement abilities. However, this setting is not necessarily different from human head motion during group conversation, since head movement is used by humans to optimize listening to the active speaker [17].\nSince the camera gyroscope records the absolute yaw and pitch angles of the head, we obtained a dataset of general head motions by subtracting the initial pose from each recorded trajectory. Figure 3 shows the full set of recorded trajectories.The dataset collected in this study is made available at our public repository\u00b9.\nWe took an approach similar to previous work on motion-capture trajectory modeling [2]\u2013[4], and chose to model human head-movement trajectories using a variational au-toencoder (VAE) [18]. Full architecture and hyperparameters are included in Appendix A.\nWe extracted individual trajectories from each trial in the training set by automatically identifying their beginning and end. The start of a fixation trajectory is detected by finding the first timestep where the magnitude of change $||\\Delta \\tau||$ is above a threshold of 0.1, then subtracting a fixed offset of 4 frames. We used a sliding window of 20 frames to detect the end of each trajectory to eliminate small noise at the end of fixations. A fixation is marked to have ended once all the magnitudes within this window are less than 0.0175. Trajectories are then padded with trailing zeros until a fixed length of 60 frames (i.e., 2 seconds).\nTrajectories $\\tau(t) = (yaw(t), pitch(t))$ are represented as yaw and pitch angle of the head at each frame, relative to the initial pose (i.\u0435., $\\tau(0) = (0,0)$ for every clip). We also approximate angular velocities at each frame by finite differences over the trajectory ($\\Delta \\tau(t) = \\tau(t) - \\tau(t - 1)$, where we assume $\\Delta \\tau(0) = (0,0)$). An example of the trajectory of angular velocities $\\Delta \\tau$ is shown in Figure 1, as 'fixation trajectory', and in the Appendix as Figure 8.\nA variational autoencoder is composed of two neural networks, an encoder $z_{\\mu}, z_{\\sigma} = enc(\\Delta \\tau)$ that takes a trajectory of angular velocities as input and outputs two vectors, that parameterize a multivariate Normal distribution $\\mathcal{N}(z_{\\mu}, diag(z_{\\sigma}))$, and a decoder $dec(z)$ that takes a latent vector z sampled from the distribution proposed by the encoder, and outputs a reconstructed trajectory (as per-frame angular velocities) $\\hat{\\tau}$ that is as similar as possible to the original trajectory.\nWe design a loss function that aims at reconstructing tra-jectories by matching both absolute (yaw, pitch) coordinates of the trajectory at each frame and the instantaneous angular velocities. The loss function used is\n$L_{VAE} = \\frac{1}{N} \\sum_t ||\\Delta \\tau(t) - \\widehat{\\Delta \\tau}(t)||^2 + A_{pos} \\frac{1}{N} \\sum_t ||\\tau(t) - \\hat{\\tau}(t)||^2  + A_{KL} KL (\\mathcal{N}(z_{\\mu}, diag(z_{\\sigma}))||N(0, I))$,\nwhere $\\hat{\\tau} = dec(enc(\\tau))$ is a reconstructed trajectory, N = 60 is the length of each trajectory in frames, $A_{pos} = 5.10^{-4}$ is a trade-off term between the MSE loss of absolute angles and the MSE loss of the per-frame angular velocities, and $A_{KL} = 5.10^{-3}$ is the relative strength of the VAE regularization term versus the reconstruction losses. We use a latent space of size 10, $z \\in R^{10}$.\nThe VAE is trained via unsupervised learning to match the distribution of human trajectories. To be useful in practical applications, however, we need a way to generate trajectories with desired properties. We are in particular interested in generating trajectories whose end point is at or close to a given target location. For example, if we wished to produce a fixation to a target (yaw, pitch) = (-60\u00b0, 20\u00b0), we would like to generate a trajectory similar to the pink one in the top-left corner of Figure 3. Note that all trajectories are relative to the current fixation, given by an initial yaw and pitch angle.\nWe generate trajectories to given targets as follows. We process the dataset to calculate the end (yaw, pitch) fixation point for each clip of human trajectories. We then scale the fixation points by constant factors to approximately normalize the coordinates within [-1,1] (we use factors $\\alpha_{yaw} = \\frac{1}{60}$, and $\\alpha_{pitch} = \\frac{1}{25}$). We then train a multilayer perceptron (MLP) to map the normalized fixation targets into latent vectors z obtained by inputting the corresponding trajectories into the trained VAE encoder. We provide Full architecture and hyperparameters in Appendix A.\nAt test time, trajectories can be generated by appropriately scaling target fixation points, inputting them into the MLP to obtain latent vectors that characterize the required motions, and then using the VAE decoder to produce the desired trajectories.\nWe design a human-robot interaction scenario that involves the generation of repeated head movements to investigate the quality and naturality of the movements provided by our method, compared to a baseline where yaw/pitch head movements were driven by the default motion controller of the Nao robot. Due to the limitations of the narrow viewing angle (approximately 60\u00b0 horizontally) of the built-in camera and the need for wider visibility to effectively interact with multiple participants, we attached a Logitech webcam (90\u00b0 field of view horizontally) to the top of Nao's head with a 3D-printed head mount based on Dhionis Sako's\u00b2 project.\nWe chose an Active Speaker Detection (ASD) task that involves identifying the active speaker in a group conversa-tion with multiple potential speakers, in which the robot is a passive participant. The objective of ASD is to determine a function f of an audio $A(t)$ and visual $V(t)$ data stream\n$S(t) = f(A(t), V(t))  $(1)\nsuch that $S(t) \\in {0,1}^N$ is a vector whose components at each timestep t are $S_i(t) = 1$ if speaker $i \\in {1,..., N}$ is active and 0 if it is not active.\nHere, we use the pre-trained Light-ASD model from Liao et al. [8]\u00b3 to implement the function $S(t)$. First, we detect bounding boxes for all faces seen by the Nao's camera in each frame. The detected faces are resized and organized into candidate speaker tracks using temporal In-tersection Over Union (IOU) scores. For each candidate speaker, the sequence of face images and corresponding audio (shared across all speaker tracks) are processed inde-pendently through audio and visual encoders. The resulting audio and visual features are then concatenated. The com-bined feature vectors are then processed sequentially through a bidirectional Gated Recurrent Unit (GRU), followed by a multilayer perceptron.\nAlthough Light-ASD is significantly faster than other state-of-the-art ASD models, we found it to still be too slow for use in real-time applications. This is likely due to the model having been developed primarily for offline processing of video files. As part of this work, we optimized the model by improving the preprocessing code to run in-memory and replacing the original face detection model with the faster MediaPipe [19]. The combined modifications resulted in an 85% reduction in overall inference time, decreasing the time required to process a 1s (at 30fps) video clip from 1.3 seconds to approximately 0.2 seconds. Furthermore, we designed an asynchronous system to record and process the robot's inputs in parallel, achieving close to 5 inferences per second. During interactions with humans, camera frames and audio chunks are stored in a First-In First-Out (FIFO) buffer that holds the most recent 1s of data. Every 0.5 seconds, the current buffer is asynchronously processed by the ASD model.\nWe finally implement a simple heuristic to select fixation targets as follows. Each time a new result is processed by the ASD module, we calculate a probability distribution by applying a softmax operator on the vector of scores assigned by the ASD module to each face in the last frame of the buffer, with a weighting factor $\\beta = 2$. Then, a new fixation is chosen as the center of the bounding box of the face sampled from the distribution. A motion trajectory is generated and executed only if the previous movement has already finished."}, {"title": "E. Preliminary experiment on human preferences", "content": "We further test the subjective perception of the quality and naturalness of the generated motions by designing a preliminary experiment with, n = 9, human participants. For the experiment, we collected 3 videos centered on the Nao's head for both our method and a baseline motion controller. We then asked the participants to select 3 out of 6 videos with the following instructions: \"Choose 3 out of the 6 videos that in your opinion show the most natural and engaging movement of the Nao robot. Please note that head movements (i.e., target fixation points) were chosen using the same algorithm in all videos.\"\nWe evaluate the results of our approach in two stages. First, we look into the quality of the individual components to assess the quality of the generated head movement trajec-tories. We then evaluate the integrated system in a case study where a Nao robot needs to track a conversation between human co-participants."}, {"title": "IV. RESULTS", "content": "We first performed a qualitative analysis of the VAE by generating 200 random trajectories (i.e., sampling random latent vectors to input into the trained decoder) and com-paring them to the trajectories of human head movement from the dataset we collected (see Figure 3). The generated trajectories are shown in Figure 4 (a). A sample of individual trajectories reconstructed by the autoencoder is available in the Appendix as Figures 7 and 8.\nWe next evaluated whether trajectories to specific target fixations could be effectively generated using the trained MLP from Section III-C. We did so by selecting 21 \u00d7 21 (normalized) target fixation points within a uniform grid [-1,1] \u00d7 [-1,1], and generating trajectories to reach each of them. We then looked at the final fixation points reached by the predicted trajectories, and plotted them as a distortion grid in Figure 4 (b). We found that the generated trajectories always reached the desired fixation point, with a mean square error of approximately 3.7\u00b0. We also found that the generated trajectories match the profile of angular velocities from the human data, as shown in Figure 4 (c), which plots a sub-set of the trajectories and highlights the change in relative position at each timestep by alternating segments of different color. We also observed that even though the original dataset contained only few trajectories toward the four corners, our generative model managed to interpolate correctly and cover the whole range of possible fixation points accurately, suggesting that the model did not simply memorize the training trajectories, but rather managed to generalize well.\nWe evaluated the quality of the generated motions on a Nao robot during an active-speaker gazing task in two steps. We first determined that the target trajectories generated using our method are effectively tracked by the robot motors, and that they are qualitatively different from a baseline where movements are obtained using the default Nao controller. We collected data in a group conversation setting by letting the robot passively participate in a conversation between three human interlocutors. During the conversations, we recorded videos centered on the robot's head together with the target trajectories generated with both methods and the joint angles as measured using the robot's encoders. We further extracted the movement of the head in the videos by tracking a red dot painted in the middle of the robot's face.\nFigure 5 shows the results of this analysis. We found that the generated trajectories are significantly different from those generated by the baseline (panels (c), (d), and (e)), which we found to track the yaw and pitch movements independently of each other, so that in practice the pitch movement of the robot in the baseline terminates before the yaw movement, resulting in unnatural-looking motions.\nWe further tested the subjective perception of the quality and naturalness of the generated motions by designing a preliminary experiment with n = 9 human participants. Results are shown in Figure 6 as a histogram with the number of times each video was selected as within the top half of best-looking motions. Videos 1, 3, and 4 (blue) were generated using our method, and videos 2, 5, and 6 (red) used the default controller. We found that videos that used our method were chosen on average by 6.33 out of 9 participants, while videos with the default controller were only chosen on average by 2.67 participants."}, {"title": "V. DISCUSSION AND CONCLUSIONS", "content": "In this paper, we have addressed an often overlooked problem in human-robot interaction: imitating human head movements to track active speakers in a group conversation. Specifically, we addressed this problem by demonstrating how human data can be effectively used to generate head movements for a Nao robot by showing the efficacy of our motion controller in an active-speaker tracking task. The results indicate that the robot can move its head toward active speakers with natural-looking, well coordinated movements.\nWe suggest that this study could be further extended in the following ways. On the one hand, the same approach can be applied to upper body imitation for generating nonverbal cues (e.g., pointing, nodding, fidgeting) during the human-robot interaction, or to model more complex head movements with the inclusion of 'roll' motion in addition to the current 'yaw' and 'pitch'. On the other hand, the method could be improved by training the whole system end-to-end to generate target trajectories directly from multimodal inputs, in an imitation learning setting."}, {"title": "APPENDIX", "content": "We include the full set of hyperparameters used for train-ing the variational autoencoder in Table I (see Section III-B), and the set of hyperparameters of the multilayer perceptron used to map target fixations into latent vectors in Table II (see Section III-C)."}]}