{"title": "VeriFact: Verifying Facts in LLM-Generated Clinical Text with Electronic Health Records", "authors": ["Philip Chung", "Akshay Swaminathan", "Alex J. Goodell", "Yeasul Kim", "S. Momsen Reincke", "Lichy Han", "Ben Deverett", "Mohammad Amin Sadeghi", "Abdel-Badih Ariss", "Marc Ghanem", "David Seong", "Andrew A. Lee", "Caitlin E. Coombes", "Brad Bradshaw", "Mahir A. Sufian", "Hyo Jung Hong", "Teresa P. Nguyen", "Mohammad R. Rasouli", "Komal Kamra", "Mark A. Burbridge", "James C. McAvoy", "Roya Saffary", "Stephen P. Ma", "Dev Dash", "James Xie", "Ellen Y. Wang", "Clifford A. Schmiesing", "Nigam Shah", "Nima Aghaeepour"], "abstract": "Methods to ensure factual accuracy of text generated by large language models (LLM) in clinical\nmedicine are lacking. VeriFact is an artificial intelligence system that combines retrieval-augmented\ngeneration and LLM-as-a-Judge to verify whether LLM-generated text is factually supported by a\npatient's medical history based on their electronic health record (EHR). To evaluate this system,\nwe introduce VeriFact-BHC, a new dataset that decomposes Brief Hospital Course narratives from\ndischarge summaries into a set of simple statements with clinician annotations for whether each\nstatement is supported by the patient's EHR clinical notes. Whereas highest agreement between\nclinicians was 88.5%, VeriFact achieves up to 92.7% agreement when compared to a denoised and\nadjudicated average human clinician ground truth, suggesting that VeriFact exceeds the average\nclinician's ability to fact-check text against a patient's medical record. VeriFact may accelerate the\ndevelopment of LLM-based EHR applications by removing current evaluation bottlenecks.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) can summarize patient electronic health records (EHR) [1, 2], per-\nform clinical risk prediction[3] and diagnostic reasoning [4, 5], and translate patient instructions or\ndischarge summaries to patient-friendly language[6, 7]. Yet LLM deployment in clinical practice\nis limited in part because LLM evaluation to date has largely focused on medical question-answer\nexamination benchmarks with a minority of studies utilizing real patient care data or long-form\ntext generation[8-12]. Long-form text such as clinical notes are pervasive in medical practice, but\nit is impossible for clinician vigilance alone to ensure factuality of LLM-generated text given the\nneedle-in-a-haystack challenge of identifying subtle errors and hallucinations[13, 14]. VeriFact is\nan artificial intelligence (AI) evaluation system that addresses this gap by automatically verify-\ning whether any text, whether LLM-written or human-authored, is supported by a patient's EHR\n(Figure 1). Text written about a patient should be internally consistent with information already\nknown about the patient, such as that stored in their medical records. VeriFact decomposes the text\ninto a set of statements that it will verify using retrieval-augmented generation (RAG)[15] with an\nLLM-as-a-Judge evaluator[16, 17] to mimic the manual process of chart review and interpretation of\nfacts that clinicians employ when evaluating patient documentation for veracity. VeriFact focuses\non fact-checking against patient-specific EHRs as opposed to general domain fact-checking[18-22].\nWe introduce VeriFact-BHC as a new dataset to evaluate VeriFact's fact-checking performance\nagainst human clinicians. VeriFact-BHC contains 13,290 statements from the Brief Hospital Course\n(BHC) narratives of 100 patients drawn from the publicly-available MIMIC-III dataset[23, 24].\nEach statement is annotated by multiple human clinicians as Supported, Not Supported, or Not\nAddressed by the patient's EHR. A final denoised human clinician ground truth set of labels\nis obtained via majority voting and adjudication. We publicly release VeriFact-BHC to enable\nresearchers to develop better patient-specific EHR fact-checking systems."}, {"title": "2 Results", "content": ""}, {"title": "2.1 The VeriFact-BHC Dataset", "content": "The VeriFact-BHC dataset contains 13,290 proposition statements extracted from human-written\nand LLM-written Brief Hospital Course (BHC) narratives for 100 patients from the MIMIC-III\nClinical Database. Paired with each patient's BHC narratives is a longitudinal EHR containing all\nother clinical notes entered prior to the discharge summary for the hospital admission. Appendix\nB.1 describes the patient demographics and resulting dataset."}, {"title": "2.2 Atomic Claim and Sentence Proposition Validity", "content": "Atomic claim extraction reliably produces valid propositions whereas sentence propositions result\nin 19.8% invalid propositions when extracted from human-written text (Figure 3). Human-written\nclinical notes often contained partial or vague statements with special formatting such as lists and\nbullets which may be poorly parsed by a sentence splitter. Human-written notes also contained\nimperative statements directing a clinical plan of action or interrogative statements expressing a\nclinician's uncertainty, neither of which are propositions in first-order formal logic. These invalid\npropositions fail to assert a statement about the state of the world, resulting in an inability for\nVeriFact or human annotators to evaluate the proposition as Supported, Not Supported, or Not\nAddressed. These challenges are largely overcome by using LLM for atomic claims extraction which\ntends to rephrase information as valid propositions and reduces their incidence to 0.4% in human-\nwritten summaries. LLM-written summaries rarely exhibit invalid propositions since LLMs tend\nto write fluent and grammatically correct sentences from which either proposition types can be\neasily extracted. Atomic claim propositions also enable more fine-grained evaluation with 9,108\npropositions compared to sentence propositions with 4,182 propositions in VeriFact-BHC, yielding\nroughly 2.2 times the number propositions from the same input text."}, {"title": "2.3 Agreement Between Human Clinicians", "content": ""}, {"title": "2.4 Human Clinician Ground Truth", "content": "Human clinician ground truth labels for each proposition were established by taking the majority\nvote of three clinician labels. The 478 propositions with disagreement between the three clinicians\nwere labeled by two additional physicians; if the two additional clinicians disagreed, the propositions\nwere adjudicated through manual discussion between the two physicians and a third physician to\nhelp break ties until a consensus ground truth label was established.\nThe human ground truth labels represent a \"average\" physician judgment across a variety of\nclinical specialties and years of experience. The variance in physician judgment is indicated by\nthe inter-clinician agreement shown in Table 1 with lower agreement indicating greater variance in\nclinician judgement for propositions in that category. Human clinicians had the greatest difficulty\nagreeing upon a single ground truth for human-written text and sentence propositions with 66.6%\nagreement. Human clinician judgement was significantly more aligned with LLM-written text and\natomic claim propositions with 88.5% agreement. This indicates that the most consistent ground\ntruth was derived from atomic claim propositions extracted from LLM-written text."}, {"title": "2.5 VeriFact Agreement With Human Clinician Ground Truth", "content": "VeriFact can now be quantified relative to clinicians by using the ground truth labels as a gold stan-\ndard (Appendix B.2, Appendix B.3, Appendix B.4, Appendix B.5). For LLM-written summaries,\nthe best VeriFact agreement with ground truth labels achieves 88.8% and 92.7% for atomic claim\nand sentence propositions, respectively (Appendix B.2). This performance is similar to agreement\nbetween random individual clinicians at 88.5% and 84.7% for atomic claim and sentence propo-\nsitions (Table 1), suggesting that VeriFact performs as well as individual clinicians. We also find\nthat sentence propositions yield slightly higher agreement than atomic claim propositions in our\nexperiments.\nOne of the top performing VeriFact hyperparameter combinations is compared against the\nhuman clinician ground truth in Figure 4, which shows the difference in label distribution for infor-\nmation symmetric (LLM-written text) and asymmetric (human-written text) scenarios. VeriFact's\nperformance reflects its ability to discriminate Supported propositions well. When a proposition\nis not Supported, VeriFact is biased in assigning Not Supported rather than Not Addressed ver-\ndicts when compared against humans. This is especially apparent in the human-written summaries\ndue to the label distribution in the information asymmetric scenario. Binarizing the label space\nby combining the negative labels as Not Supported or Addressed results in a 10-15% increase in\npercent agreement for human-written text and a corresponding increase in sensitivity and positive\npredictive value (PPV). Thus VeriFact has poor calibration in assigning the two different negative\nlabels compared to human clinicians, but can perform well when there is only a single negative\nlabel (Appendix B.3)."}, {"title": "2.6 VeriFact System Configuration", "content": "Multiple hyperparameters can affect VeriFact system's performance: (1) text embedding represen-\ntation & information retrieval method, (2) number of facts retrieved from EHR vector database\nfor each proposition, (3) fact-ordering and formatting of facts in the reference context, and (4)\nwhether fact retrieval should be constrained to the current hospital encounter or allowed to span"}, {"title": "3 Discussion", "content": "VeriFact can evaluate any long-form text discussing a patient by fact-checking the text against\nthe patient's EHR. At the present, all non-boilerplate clinical text required for clinical care and\nbilling is authored by human clinicians, consuming a considerable amount of time and mental\nresources. There is promise in using LLMs for clinical text generation tasks to offload this burden\nand increase the amount of time clinicians can participate in direct patient care activities. However,\nLLM-written text needs to be evaluated for inaccuracies and hallucinations before it is committed\nas a part of the patient's permanent medical record. VeriFact can perform this critical guardrailing\nfunction with clinician-level performance in a fully automated and scalable fashion."}, {"title": "4 Methods", "content": "This study adheres to the TRIPOD+AI reporting guidelines[51]."}, {"title": "4.1 Foundation Models", "content": ""}, {"title": "4.1.1 Large Language Models", "content": "The vLLM inference engine [52] was used to locally host an adaptive-weight quantized Llama 3.1\n70B model[53], specifically the hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 model\nfrom Hugging Face Hub52,53. This LLM was used to generate the LLM-written summary with\ntemperature=0.5 to encourage diversity in writing.\nThe same LLM was also used in all steps in VeriFact that require an LLM, which include atomic\nclaim extraction, assigning the Supported, Not Supported and Not Addressed label using LLM-as-\na-Judge, and generating explanations for why each label is assigned (Figure 1). In VeriFact, the\nLLM is set to a default temperature=0.1 because low-temperature sampling results in consistent\njudgements and has been shown to outperform human experts [54]. However, low-temperature next-\ntoken sampling on smaller or quantized models occasionally resulted in generating a repeated\nnonsensical text pattern that overflowed the model's context window, resulting in outputs that\ncould not be properly parsed and passed to downstream steps in VeriFact. In these situations, the\nsame prompt was retried with the LLM's temperature incremented by 0.1 up to a ceiling value of\n1.0 until a valid output is obtained. The desire to avoid these repeated nonsensical generations is\nalso the reason why we use a default temperature=0.1 and not temperature=0."}, {"title": "4.1.2 Structured Output Generation", "content": "Structured output generation (also known as guided or constrained generation) was implemented\nusing Outlines[55] to force LLMs' text output to conform to custom JSON schemas. These JSON\noutputs were then parsed and passed to downstream VeriFact logic, which may include a pipeline\nof additional structured output generation steps. Structured output inferencing pipelines also con-\ntained a back-up self-healing mechanism whereby if the LLM failed to return valid parsable JSON,\nthe output was given back to the LLM with instructions to correct the output to valid JSON."}, {"title": "4.1.3 Embedding and Re-ranking Models", "content": "The Infinity inference engine[58] was used to locally host the BAAI/bge-m3 (M3 Embedding)\nmodel[36] from Hugging Face Hub. The M3 Embedding model generates both dense and sparse\ntext embedding representations, which are used in VeriFact to encode propositions and retrieve\nrelevant facts from the EHR. Infinity was also used to locally host the BAAI/bge-reranker-v2-\nm3 (M3 Reranker) cross-encoder language model[59] to perform second-stage reranking of facts\nretrieved from the EHR vector database."}, {"title": "4.1.4 Compute Infrastructure", "content": "All foundation model inference was performed using a single machine with four NVIDIA L40S\nGPUs. Asynchronous task parallelism is used to route different inference tasks to appropriate\nmodels."}, {"title": "4.2 VeriFact-BHC Dataset Construction", "content": ""}, {"title": "4.2.1 Patient Cohort Selection", "content": "This is a retrospective study on de-identified electronic health records data which were originally\ncollected for routine clinical care. Experiments were conducted using a subset of the MIMIC-\nIII Clinical Database v1.4 dataset which is a freely available and de-identified EHR dataset for\nresearchers derived from a tertiary/quaternary care hospital, the Beth Israel Deaconess Medical\nCenter in Boston, Massachusetts, USA [23]. MIMIC-III contains longitudinal clinical note data\nextracted from a real EHR with ICU clinician notes, radiology & cardiology study reports, and\ndischarge summaries, but it is missing other notes such as non-ICU inpatient clinical notes and\noutpatient clinic notes. Each patient may have multiple hospital admissions in the MIMIC-III\ndataset. As part of the de-identification process, dates in MIMIC-III are randomly shifted to be in\na year between 2100-2200 with all relative date and time intervals within a single patient preserved.\nThe dataset in this study comprised of a random sample of 100 patients from MIMIC-III that\nmeet the following inclusion criteria:\n1. The patient's last hospital admission must have a discharge summary with Brief Hospital Course\n(BHC) section that can be extracted using regular expressions.\n2. The last hospital admission should have at least 2 physician notes other than the discharge\nsummary.\n3. The patient's EHR must contain at least 10 total notes prior to the discharge summary, which\nmay be from current admission or prior hospital encounters."}, {"title": "4.2.2 EHR Reference Creation", "content": "Patients may have multiple hospital admissions in their EHR, so the last hospital admission was\nselected as the hospital admission of interest. For each patient, the discharge summary for the target\nhospital admission was separated from the rest of the clinical notes. The clinical notes contain"}, {"title": "4.2.3 Human-written Brief Hospital Course", "content": "A human-written BHC was extracted from the discharge summary of the target hospital admission\nusing regular expressions for each of the 100 patients. This long-form human-written narrative\nrepresents the current status quo in clinical care where human clinicians obtain data from chart\nreview and care team discussions to generate new clinical text. This narrative was written by a\nclinician who likely had access to additional data sources than what is present in the patient-\nspecific reference. This is due to MIMIC-III consisting mainly of ICU notes, radiology reports, and\ncardiology reports, and because not all bedside patient care discussions between clinical staff and\npatients are captured in EHR clinical notes."}, {"title": "4.2.4 LLM-written Brief Hospital Course", "content": "An LLM-written BHC or LLM-written summary was also created for each of the 100 patients by\nusing LLMs to iteratively summarize all the available notes from the patient's current admission\nwith the exception of the original human-written discharge summary. The LLM-written narrative\nrepresents a use case where LLMs are used to generate text in clinical care applications. The\nLLM-written summary was created using the following procedure:\n1. Each note from the target hospital admission was selected to be source material for the final\nLLM-written summary.\n2. Each note was split into text chunks of 1000 or fewer tokens (1000 tokens is roughly 750 words).\nEach text chunk was summarized into 4 or fewer sentences using an LLM. The summaries\nwere subsequently combined using an LLM to yield a summary of each note with a length of\n4 or fewer sentences. This tree-summarization scheme for each clinical note ensures a balance\nbetween high-level summary and detail.\n3. A rolling update summarization scheme was then applied across all individual note summaries to\nyield a single summary. Note summaries were first ordered in chronological order and gathered\ninto a series of 5000-token note summary bundles such that each bundle contained summarized\nversions of 5 or more consecutive notes. The first 5000-token note summary bundle was sum-\nmarized into an initial target LLM-written summary. All subsequent note summary bundles\nwere iteratively merged and aggregated into the target LLM-written summary. This process was\nrepeated until all 5000-token note summary bundles were consumed.\n4. Whenever the target LLM-written summary exceeded 1000 tokens in length, an LLM was used\nto rewrite and compress the summary while attempting to maintain the most important details.\nThis ensured the final LLM-written summary never exceeded the length of 1000 tokens.\n5. The final LLM-written summary is one of the studied input text types for VeriFact. Prompts\nused to generate the LLM-written summary are depicted in Appendix A.2.\nThe described procedure ensures the LLM-written summaries are able to be generated from\na set of EHR clinical notes no matter the length of each clinical note or how many clinical notes\nwere written during the patient's hospital admission. The procedure also enables LLMs to focus\non details in each subsection of every note while providing LLM's the autonomy to discard what it\nconsiders to be less important information. The LLM-written summaries were intended to represent\na real-world clinical use case where LLMs are used to generate text for clinical care scenarios.\nHowever, no significant effort was made to further optimize the content of LLM-written summaries\nsince clinical summary-generation was not the focus of this investigation."}, {"title": "4.2.5 Formation of the Annotated Proposition Dataset", "content": "Both LLM-written and human-written BHC narratives were decomposed into propositions and\nthese propositions are annotated by human clinicians to create human clinician ground truth\nlabels which VeriFact can be compared against. The text transformation process is the same for\nBHC narratives being evaluated and the EHR clinical notes which are used to compose reference\ncontexts. The transformation pipeline is described in detail in the following paragraphs. Once\npropositions are obtained, verdict annotations of Supported, Not Supported, and Not Addressed\nfor each proposition were created by human clinicians as well as the VeriFact system, which are\ndescribed in the subsequent steps. Additionally, the validity of each proposition based on first-\norder formal logic was assessed. All of these proposition annotations form the final VeriFact-BHC\ndataset."}, {"title": "4.3 Forming Propositions and Facts", "content": ""}, {"title": "4.3.1 Transformation of Text to Propositions and Facts", "content": "VeriFact performs evaluation on text inputs such as the human-written or LLM-written summaries\nby transforming the text into proposition units for more fine-grained fact checking. Both atomic\nclaim and sentence statements are considered as a unit of information representation for proposi-\ntions and facts. Atomic claims and sentences were referred to as propositions if they originate from\nthe candidate input text being evaluated by VeriFact; they were referred to as facts if they were\nextracted from the patient's EHR clinical notes and stored in the vector database for reference\ncontext formation.\nThe same text transformation pipeline was used to extract propositions from candidate input\ntext being evaluated by VeriFact as was used to extract facts from each of the patient's EHR notes:\n1. Input text is split into text chunks of 128 or fewer tokens (roughly 100 words) using recursive\nsemantic parsing. This involved creating an embedding representation for each sliding window of\n3 sentences, computing cosine distance between sequential embedding representations, and gen-\nerating text splits when the embedding representations exceed the 90th percentile of distances\nwithin the input text. The dense encoder from the M3 Embedding model was used to generate\nembedding representations. If the resultant text chunk exceeded 128 tokens, this process was\nrecursively repeated to ensure all final text chunks were 128 tokens or fewer.\n2. To obtain sentence propositions or facts, each text chunk was split using the NLTK sentence\ntokenizer[60]. Each resultant sentence chunk was used as a proposition or fact in the VeriFact\nframework.\n3. To obtain atomic claim propositions or facts, each text chunk was passed to an LLM with\na prompt for atomic claim extraction. The prompt defines atomic claims as simple sentences\nthat resemble first-order predicate logic statements and encapsulate a single Subject-Object-\nPredicate relationship. Each atomic claim was then used as a proposition or fact in the VeriFact\nframework. Prompts used in atomic claim extraction are in Appendix A.3.\nIn all experiments, the proposition and fact representation type (sentences versus atomic claims)\nare always chosen to be the same. For example, if the input text is decomposed into atomic claim\npropositions, the corresponding reference context of EHR facts is composed of retrieved atomic\nclaim facts and not sentence facts."}, {"title": "4.3.2 Determining the Validity of Propositions", "content": "In first-order formal logic, valid propositions must assert a specific claim about the state of the world\nand have a truth-value-either true or false; as a result, valid propositions are often declarative state-\nments that assert something about a subject[25, 27]. Invalid propositions involve statements where\na truth value cannot be properly assigned, which includes imperative statements (instructions),"}, {"title": "4.4 Human Clinician Annotator Study", "content": ""}, {"title": "4.4.1 Annotation, Adjudication and Ground Truth Creation", "content": "Each proposition was annotated by three clinicians who assigned a verdict label of Supported, Not\nSupported, or Not Addressed. Supported propositions must be fully supported by the patient's EHR.\nNot Supported propositions are not supported, partially supported, or contradicted by the EHR.\nNot Addressed propositions are not mentioned in the EHR.\nA majority vote was then taken to denoise the labels and establish a human clinician ground\ntruth set of labels for each proposition. In scenarios where all three clinicians disagreed, propo-\nsitions were independently reviewed by two additional clinicians and a majority vote was taken\nfrom their annotations. Any further disagreement was manually discussed and adjudicated by the\ntwo clinicians and the addition of a third clinician to break ties, ultimately yielding a consensus\nlabel. The initial annotations were created by 25 clinicians with varying levels of training, expe-\nrience and specialties who were each randomly assigned to 12 patients and tasked with labeling\nall propositions for those 12 patients. Adjudication of disagreeing propositions was conducted by\n2 additional clinicians. The ultimate goal was to create a human ground truth for each proposi-\ntion that approximates the judgment of the average clinician, which can serve as a gold standard\ncomparison for VeriFact.\nSince the MIMIC-III dataset was de-identified and all clinician annotators were study authors\nwith no recruitment of study subjects, this investigation was not human subjects research and did\nnot require institutional review board approval."}, {"title": "4.4.2 Annotation Workflow", "content": "Clinician annotators performed labeling using Label Studio[61] with a custom annotation interface\nand were tasked to assign a Supported, Not Supported, or Not Addressed label to each proposition\n(Appendix A.7.1 & Appendix A.7.2). Clinician annotators were presented with the propositions\none at a time and were not explicitly informed whether the proposition was an atomic claim or\nsentence proposition. Clinician annotators also did not have access to the original BHC narrative\nfrom which propositions were derived and were blinded to whether the proposition was derived\nfrom a human-written or LLM-written Brief Hospital Course narrative.\nAnnotators were provided each patient's EHR clinical notes in Portable Document Format\n(PDF) document format, which contained all clinical notes for the patient with the exception of the\nfinal discharge summary (Appendix A.7.3 & Appendix A.7.4). This EHR PDF contains the same\nsource text from which facts stored in the EHR Vector Database are derived, thereby providing\nhuman clinician annotators access to the same information as the VeriFact system. The EHR PDF\nwas generated using the fpdf version 2.7.9 python library. To approximate how clinicians perform\nchart review in modern real-world EHR systems, clinicians were allowed to use any PDF reader\nsoftware to read and search the EHR PDF for information, including text search. The amount of\ntime taken to complete the annotation task was recorded by Label Studio."}, {"title": "4.5 The VeriFact System", "content": ""}, {"title": "4.5.1 Vector Search Methods for Fact Retrieval", "content": "Each patient's clinical notes were transformed into sets of atomic claim facts and sentence facts in\nthe same manner that propositions were derived. Facts were then stored in a vector database for\nsubsequent retrieval. LlamaIndex version 0.11.14 was used as a framework for RAG and Qdrant\nversion 1.10 was used as the vector database with default indexing and search settings. Qdrant\nemploys a filtrable hierarchical small world navigation search algorithm, which provides a mech-\nanism to restrict retrieval of facts to those derived from clinical notes from the current hospital\nadmission versus all admissions in the EHR[62].\nWe explored three methods for retrieving EHR facts from the vector database:\n\u2022 Dense: The proposition was encoded into a vector query by the M3 Embedding model's dense\nencoder and dot-product similarity was used to determine the most relevant facts to retrieve for\neach proposition.\n\u2022 Hybrid: Dense search with the addition of performing a parallel sparse search by encoding\nthe proposition using the M3 Embedding model's sparse token encoder. Results from dense and\nsparse search methods were subsequently fused using distribution-based score fusion[63].\n\u2022 Rerank: Hybrid search followed by the M3 Reranker to rescore and rerank the search results\ndiscovered by Hybrid search."}, {"title": "4.5.2 Number of Facts Retrieved", "content": "We studied how VeriFact performance was affected by the number of EHR facts used in the\nreference context passed to the LLM-as-a-Judge. This is the top N parameter for which we studied\nN = 5, 10, 25, and 50 facts included in the reference context.\nHybrid and rerank retrieval methods produce more search results than dense retrieval, so we\nutilized the following scheme to equalize the number of facts in the reference context to enable\ndirect comparison between the retrieval methods. Let K be the number of facts retrieved from\neither a dense or sparse search query. Let N be the top N items selected from the final search\nresults which will be included in the reference context. The mechanism for selecting the number of\nfacts for the reference context for each retrieval method is as follows:\n\u2022 Dense: N=K and all K retrieved facts from the EHR vector database are considered for the\nreference context. It is commonplace for EHR note text to be templated or copy-pasted, resulting\nin many duplicated facts stored in the EHR Vector Database. To avoid much of the reference\ncontext being duplicate text, exact string match facts are deduplicated prior to inserting the\nfinal list of facts into the reference context.\n\u2022 Hybrid: N=K for dense retrieval and N=K for sparse retrieval, resulting in 2K search results.\nThese retrievals are independent of one another, meaning that some retrieved facts may be\npresent in both search result lists. During distribution-based score fusion these search results\nbecome deduplicated, meaning that the final fused search result list had <2K facts. Similar to\nthe dense method, exact string match facts were then deduplicated. After deduplication, only\nthe top N facts ranked by the distribution-based score fusion method were used in the reference\ncontext with all other facts discarded.\n\u2022 Rerank: Same as hybrid, but after the final fused search result list of length <2K facts is\nobtained and exact string match facts are deduplicated, the M3 Reranker model was applied to\nrescore each fact's relevance to the proposition. This resulted in re-ordering of the search results.\nThen the top N facts ranked by the M3 Reranker were used in the reference context with all\nother items discarded."}, {"title": "4.5.3 Reference Context Formation", "content": "After the set of top N facts was finalized, we explored different ways to format the reference\ncontext. Scores generated from retrieval serve as a way to rank how relevant each retrieved fact is\nfor each proposition. However, the EHR is a temporal record and a list of facts sorted by relevance\nscore can result in historical and outdated information ranked higher in the list of facts than newer\nand more pertinent information. Since it was unclear how to optimally arrange the top N facts,\nwe studied the following three approaches:\n\u2022 Relevance Score: The top N facts were ordered by relevance score, which was dot product\nsimilarity for dense retrieval, distribution-based score fusion score for hybrid retrieval, and M3\nReranker score for rerank retrieval. Facts were formatted as a numbered list with fact text\npresented alongside retrieval score, note category and note description metadata of the clinical\nnote where the fact originated.\n\u2022 Absolute Time: The top N facts were ordered chronologically and formatted as a numbered\nlist with fact text and the timestamp, note category & note description metadata of the clinical\nnote where the fact originated. To contextualize the timestamped facts, the hospital admission\nstart and hospital admission end timestamps were pre-pended to the reference context.\n\u2022 Relative Time: Same as absolute time, but all timestamps were converted into relative times\nwith the current time set to the hospital admission end. Timestamps in MIMIC-III are randomly\nshifted to occur between years 2100 and 2200 as part of the de-identification process, resulting\nin unnatural timestamps that may be outside of the training distribution for LLMs and may\nresult in abnormal LLM behavior. Conversion of absolute timestamps to relative time in hours\nand days before the end of hospital admission removes the effect of unnatural timestamps and\nprovides a clearer chronology of events."}, {"title": "4.5.4 LLM-as-a-Judge", "content": "Each proposition and its reference context of facts retrieved from the EHR was then passed to an\nLLM-as-a-Judge. The LLM-as-a-Judge was instructed to assign a verdict on whether the proposi-\ntion was Supported, Not Supported, or Not Addressed by the reference context along with a reason\nexplaining why the verdict was assigned. After verdicts are assigned to all propositions derived\nfrom input text, the percentage of each label is computed and the reasons for each verdict were"}]}