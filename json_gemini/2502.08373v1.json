{"title": "Uncertainty Aware Human-machine Collaboration in Camouflaged Object Detection", "authors": ["Ziyue Yang", "Kehan Wang", "Yuhang Ming", "Yong Peng", "Han Yang", "Qiong Chen", "Wanzeng Kong"], "abstract": "Camouflaged Object Detection (COD), the task of identifying objects concealed within their environments, has seen rapid growth due to its wide range of practical applications. A key step toward developing trustworthy COD systems is the estimation and effective utilization of uncertainty. In this work, we propose a human-machine collaboration framework for classifying the presence of camouflaged objects, leveraging the complementary strengths of computer vision (CV) models and noninvasive brain-computer interfaces (BCIs). Our approach introduces a multiview backbone to estimate uncertainty in CV model predictions, utilizes this uncertainty during training to improve efficiency, and defers low-confidence cases to human evaluation via RSVP-based BCIs during testing for more reliable decision-making. We evaluated the framework in the CAMO dataset, achieving state-of-the-art results with an average improvement of 4.56% in balanced accuracy (BA) and 3.66% in the F1 score compared to existing methods. For the best-performing participants, the improvements reached 7.6% in BA and 6.66% in the F1 score. Analysis of the training process revealed a strong correlation between our confidence measures and precision, while an ablation study confirmed the effectiveness of the proposed training policy and the human-machine collaboration strategy. In general, this work reduces human cognitive load, improves system reliability, and provides a strong foundation for advancements in real-world COD applications and human-computer interaction. Our code and data are available at: https://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.", "sections": [{"title": "1. Introduction", "content": "Rapid adoption of deep learning has highlighted concerns about the robustness and transparency of neural network predictions. Addressing these concerns is critical for the advancement of trustworthy artificial intelligence (AI) Kaur et al. (2022). A key aspect of building trust is to enable models to assess and communicate their own uncertainty chander et al. (2024); Gawlikowski et al. (2023), which can guide decisions to defer to human operators or seek additional data in complex scenarios. In particular, in tasks where humans and AI have complementary strengths, effective human-machine collaboration represents a promising trend in the evolution of digital societies. However, how to leverage uncertainty quantification for such collaboration, and its impact, remains underexplored. Camouflaged Object Detection (COD) is a well-suited task for such synergy. COD focuses on identifying objects concealed within their environments, making it both challenging and intriguing Lv et al. (2023). This field has grown rapidly due to its practical applications, such as defect detection in manufacturing Xiong et al. (2021), pest control in agriculture Rustia et al. (2020), segmentation of lesions in medical diagnosis Fan et al. (2020), pedestrian detection in nighttime environmentsYang et al. (2024), and even creative pursuits such as image blending Suo et al. (2021). Advances in image-level camouflaged object segmentation have been driven by models such as DPSNet Li et al. (2024), JCNet Jiang et al. (2023), SINet-V2 Fan et al. (2021), and DGNet Ji et al. (2023a), supported by recognized datasets and benchmarks Bi et al. (2021). COD tasks involve two key components: identifying whether camouflaged objects are present and segmenting them when they are. However, much of the current research focuses on the segmentation stage, assuming the model can produce a continuous mask map where a black saliency map signifies the absence of a camouflaged object. However, since camouflaged objects are not guaranteed to be present, a critical first step is to detect their presence before proceeding to segmentation. While precise localization is vital for applications like medical diagnostics, in tasks such as rescue operations or pest monitoring, detecting the presence of camouflaged objects takes precedence. Therefore, our study centers on binary classification to determine the presence or absence of camouflaged objects.\nDespite rapid progress in COD, concerns persist about the safety and reliability of computer vision (CV) models. Deep learning models often function as black boxes Hassija et al. (2024) and can struggle with challenges such as small targets, incomplete objects, and complex backgrounds (e.g. noise, obstructions, or shadows) Bi et al. (2021). Teaching models to admit uncertainty, essentially saying 'I don't know', remains a significant challenge. In contrast, the human brain excels at adapting to diverse environments, recognizing subtle patterns, and identifying hidden objects in complex scenarios, such as low-light conditions or cluttered backgrounds. This adaptability allows humans to detect hidden or camouflaged targets with high accuracy, even when features are partially occluded. However, manual search for such targets is time-consuming. Non-invasive brain-computer interfaces (BCIs) offer a novel solution to this challenge Kim et al. (2019). When individuals encounter rare targets in visual sequences, their brain activity generates event-related potentials (ERPs), particularly the P300 component, which can help detect targets. Researchers have exploited this phenomenon by combining rapid serial visual presentation (RSVP) paradigms with BCIs technology to evoke ERPs in response to visual stimuli, allowing efficient classification of target images Zhang et al. (2020)-even under challenging conditions where targets are camouflaged and hidden Lian et al. (2023); Zhou et al. (2024).\nIn this study, we propose a novel framework for human-machine collaboration in COD, leveraging model uncertainty as the bridge between CV models and human intervention. Specifically, CV models handle the bulk of images, exploiting their ability to process large volumes of data in parallel, while humans, through BCIs, provide instinctive responses to challenging or unusual images flagged by model uncertainty. To the best of our knowledge, no existing research has been proposed that uses and evaluates the uncertainty-based human-machine partnership in COD. Our contributions include the following.\n\u2022 Multiview backbone for COD: We propose a backbone that evaluates the confidence across multiple views for each image."}, {"title": "2. Related Work", "content": "2.1. Uncertainty Quantification\nUncertainty estimation, or confidence assessment, in neural network predictions has emerged as a major research focus in the machine learning community Smith (2024). Current approaches to uncertainty modeling typically fall into three categories: Monte Carlo Dropout Neal (2012); Moreau et al. (2022); Gal et al. (2017); Kang et al. (2023), the Bootstrap model Osband et al. (2016), and the Gaussian Mixture Model Ai et al. (2021); Zhang et al. (2019). These methods have been extensively explored in various domains, demonstrating promising results Abdar et al. (2021). However, most models merely display uncertainty without leveraging it to guide further actions. To address this gap, recent advances have begun to incorporate uncertainty into training strategies Li et al. (2023); Cordeiro et al. (2023). Although promising, these cutting-edge efforts have focused primarily on tasks that involve noisy label learning. For standard supervised learning tasks, the effectiveness of uncertainty-informed training strategies remains unclear. In this work, we extend the confidence learning method based on two views introduced in Li et al. (2023) and apply it to the COD problem. Our approach not only integrates uncertainty into the model's learning process, but also flexibly delegates uncertain samples to human-based RSVP systems for enhanced decision making.\n2.2. Camouflaged Object Detection (COD)\nIn recent years, numerous deep learning-based COD models have been developed Liang et al. (2024), while recent research has started to place more emphasis on uncertainty. Yi Zhang et al. introduced PUENet Zhang et al. (2023), which uses a Bayesian conditional variational auto-encoder for predictive uncertainty estimation. Yixuan Lyu et al. Lyu et al. (2024) proposed the Uncertainty-Edge Dual Guide model, which combines probabilistic uncertainty with deterministic edge information for accurate COD. Jiawei Liu\n2.3. RSVP-based BCIs for Target Detection\nRSVP-based BCIs have received significant attention in recent years, particularly in the domain of target detection, due to their efficiency in processing rapid visual stimuli and eliciting robust neural responses such as potentials related to P300 events. Research advancements have focused on optimizing RSVP paradigms to enhance system performance, with notable contributions including the use of adaptive parameter tuning and hybrid paradigms that integrate steady-state visual evoked potentials to improve detection accuracy and user experience Jalilpour et al. (2020). Novel electroencephalogram(EEG) decoding algorithms, such as deep learning frameworks that take advantage of convolutional neural networks Santamaria-Vazquez et al. (2020) and attention mechanismsWang et al. (2020), have further enhanced classification performance, while collaborative approaches of multiple users have demonstrated the potential for improved accuracy through collective neural signal analysis Tawhid et al. (2022). The introduction of benchmark data sets has standardized the evaluation of algorithms and facilitated reproducible research Zhang et al. (2020). Furthermore, the integration of multimodal data, including EEG and eye tracking, has shown promise in addressing signal noise and enhancing target detection reliability Mao et al. (2023), cementing RSVP-BCIs as a crucial interface for bridging neuroscience and real-world applications. The most related work to ours is by Yujie Cui et al.Cui et al. (2022), who proposed a human-computer fusion method called Dynamic Probability Integration for nighttime vehicle detection. Their approach uses a probability assignment method to assign classification weights between different information sources, which requires full human participation in the EEG-based RSVP task. In contrast, our model reduces human"}, {"title": "3. Method", "content": "high-confidence samples are predicted using the CV model. This two-stage process combines the strengths of CV models and RSVP-based BCIs systems to enhance COD performance.\n3.1. Dataset\nThe data set used in this study consists of 2,500 images, evenly divided into camouflage target images and background images. Our data set is sourced from the publicly available CAMO dataset, which includes 1,250 camouflage target images. Using the ground truth map for each image, we generated paired camouflage target and background images through a combination of manual and automated methods. The resulting data set is available on the provided GitHub link. Although the CAMO-COCO dataset was considered, its background images differ substantially from the camouflage target images, making it less aligned with our focus. This study emphasizes scenarios where camouflage targets are embedded within similar backgrounds, reflecting more realistic application contexts.\n3.2. CV Model\n3.2.1. Multi-view based Uncertainty Estimation\nWe propose a backbone model that evaluates confidence in multiple views for each image. For each sample (x, y), we apply one weak augmentation to x, resulting in xw, and n strong augmentations, resulting in {xs1,Xs2,...,Xsn}. To compute uncertainty, we used the cross-entropy (CE) between two distributions p and q defined as:\n$CE(p, q) = \\sum_{i=1}^{C} p_i \\log(q_i),$ \nwhere C is the number of classes, pi is the ground truth probability of class i, and qi is the predicted probability of class i. Hence, uncertainty is calculated as the mean cross-entropy between the weakly augmented sample and each strongly augmented sample:\n$Uncertainty = \\frac{1}{n} \\sum_{j=1}^{n} CE(p_w, p_{sj}),$\nwhere n denotes the number of strong augmentations, while xw and xsj are processed by the trained CV model to obtain pw and psj, respectively. This approach quantifies uncertainty by assessing the consistency between weakly and strongly augmented views of the same sample.\n3.2.2. Strong and weak augmentation\nIn this study, strong and weak data augmentations serve two primary purposes: evaluating multiview-based uncertainty and enhancing the model's robustness during training.\nWeak augmentations include operations such as random horizontal flipping, slight rotation, and random cropping. In contrast, strong augmentations involve more significant perturbations, such as cropping, color transformations, image quality adjustments, occlusion, and composite augmentations to simulate complex scene variations. The specific augmentation method for both weak and strong augmentations is selected randomly for each iteration."}, {"title": "4. Results", "content": "In this section, we evaluate the effectiveness of the proposed method. The data set was divided into training and testing sets with a 9: 1 ratio and the training set was further divided into training and validation subsets, also with a 9:1 ratio. During the experiments, we used early stopping with a patience parameter set to 10. To ensure the stability and reliability of the results, we performed experiments using five different random seeds: 37, 12, 6, 99, and 123. We applied five strong augmentations and one weak augmentation. The reported results are the mean and standard deviation for the five runs. All experiments were performed on a GeForce RTX 4090 GPU. The evaluation of our experimental results was based on balanced accuracy (BA) and F1 score, with their respective formulas as follows:\n$BA = \\frac{1}{2} (\\frac{TP}{TP+FN} + \\frac{TN}{TN+FP})$\nwhere TP is the number of true positives, FN is the number of false negatives, TN is the number of true negatives and FP is the number of false positives.\n$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall},$ \nwhere $Precision = \\frac{TP}{TP+FP}$ and $Recall = \\frac{TP}{TP+FN}$\n4.1. Results for different training policies\nIn designing the training strategy, we compared different data augmentation and confident set selection methods, using the Swin Transformer (SwinT) as the backbone network. For data augmentation, we tested three strategies: (1) applying strong and weak augmentations separately to high-confidence and low-confidence images; (2) augmenting only high-confidence images (H-only); and (3) augmenting only low-confidence images (L-only). Details of the confident set selection approach are provided in Section 3.2.3. The results, shown in Table 2, reveal that augmenting only low-confidence samples, along with a fixed ratio of 2:1 between low- and high-confidence images, yielded the best performance. This configuration achieved a BA of 89.92% and an F1 score of 90.40%. These findings suggest that it is crucial to effectively mine and utilize low-confidence samples during training. Based on these results, we selected the L-only augmentation strategy combined with a fixed ratio of 2:1 for subsequent experiments.\n4.2. Comparison with Existing Methods and Ablation Study\nWe evaluated our approach against two categories of existing methods, as summarized in Table 2. The first category includes camouflaged object segmentation models, represented by DGNet and SINet-V2. These models generate completely black output maps for background images, requiring a dynamic threshold (optimized on the training set) to analyze the proportion of white pixels in the Ground Truth image and detect camouflaged targets. The second category comprises widely used CV models, including CNN-based architectures such as ResNet-18, ResNeXt-50, and DenseNet121; lightweight models such as EfficientNetB0; and transformer-based models, such as the Swin Transformer (SwinT) and Vision Transformer (ViT-B16).\nFor these CV models, we perform the following steps: 1) load the models and use pre-trained weights from ImageNet1k; 2) freeze the pre-trained weights; 3) retrieve the number of input features for the classification head; 4) replace the original classification head with a new fully connected layer consisting of four linear transformation layers, which map to 256, 32, and 8 dimensions, and finally output two classes; 5) Unfreeze the parameters of the new classification head. The key findings of the experiments include the following.\n\u2022 Performance of COD Models: COD models, such as DGNet and SINet-V2, achieved approximately 75% balanced accuracy (BA) in detecting the presence or absence of camouflaged targets. Although effective in edge detection and segmentation, their reliance on local pixel-level features limits their ability to perform well in tasks that require a broader contextual understanding.\n\u2022 Performance of CV Models: Among the CV models tested, SwinT exhibited the best performance in our data set and was selected as the backbone model for our approach.\n\u2022 Impact of Training Policy: The confidence-based training policy applied to SwinT improved BA by 1.92% and the F1 score by 1.57%, demonstrating the ability of the policy to enhance model robustness and precision.\n\u2022 RSVP Integration: Combining RSVP with the training policy resulted in additional improvements of 2.64% in BA and 2.09% in F1 score on average. The best participant achieved a remarkable balanced accuracy of 95.60% and an F1 score of 95.49%, setting a new state-of-the-art in this evaluation.\n4.3. Results for RSVP-based BCIs\nTo determine the optimal EEG model, we compared several leading EEG analysis methods. EEGNet, designed specifically for EEG signal classification, demonstrated outstanding performance. PLNet leverages phase-locking features of Event-Related Potentials (ERPs) for spatio-temporal feature extraction, excelling in single-session RSVP EEG classification. PPNN, a pyramid-structured parallel neural network, captures multiscale spatio-temporal features, improving classification. EEG-Inception adapts computer vision concepts to ERP detection, improving classification accuracy for ERP-based BCIs. LMDA-Net (LMDA) combines channel and depth attention modules to improve classification by integrating multidimensional features. EEG-Conformer (Conformer) combines convolutional networks and transformers, capturing both local and long-range dependencies, thus improving classification performance.\n4.4. Results for Human-machine Collaboration\nFor human-machine collaboration, we selected the highest proportion of samples with the highest uncertainty and replaced their CV-predicted labels with EEG-predicted labels. The results for different proportions of uncertain samples are shown in Table 5. The model performed optimally when"}, {"title": "5. Discussion", "content": "5.1. Application Scenarios\nWe propose that COD can be divided into two subtasks: identification and location. Our focus is on the binary classification task of determining whether a camouflaged object is present. In scenarios where there is no prior knowledge about the presence of a camouflaged object in an image, a \"classify-then-segment\u201d approach aligns better with practical application requirements. Additionally, since the computational cost and runtime of our classification model are significantly lower than those of a segmentation model, this approach also helps conserve computational resources. Furthermore, the integration of our identification module with other location models has the potential to be refined more. For example, an interaction between the location model uncertainty map and the identification module classification uncertainty could be designed collaboratively to enhance the detection performance.\n5.2. Uncertainty Sources\nUncertainty in machine learning can generally be categorized as aleatoric or epistemic, each arising from different sources. In particular, the uncertainties involved in the identification of camouflaged objects and location detection are different. For camouflaged object identification, aleatoric uncertainty is particularly high when the camouflaged objects are indistinct and difficult to distinguish, such as animals in a forest blending seamlessly with their surroundings, like branches or foliage. In these scenarios, the inherent ambiguity in the environment and the multiple possibilities reflected in the training data contribute to this type of uncertainty. In contrast, epistemic uncertainty stems from a lack of knowledge of unseen or unfamiliar data. For example, it arises when the camouflaged scenes or objects differ entirely from those in the training dataset, such as new backgrounds or novel camouflage techniques. In this work, we use the term \"predictive uncertainty\u201d to refer to the overall uncertainty in a given situation, encompassing both aleatoric and epistemic components.\n5.3. Training process\nTable 6 illustrates how the precision of samples within different confidence intervals changes over training epochs in the validation set. The uncertainty is ranked in ascending order, where higher percentages indicate lower confidence levels in the CV model's predictions. The confusion matrix (CM) is represented in the format $\\begin{bmatrix}TP & FN\\\\ FP & TN\\end{bmatrix}$, where the true positives (TP), false negatives (FN), false positives (FP), and true negatives (TN) are laid out in matrix form.\nA clear trend emerges: the confidence level of the CV model is inversely correlated with prediction accuracy. This highlights the effectiveness and robustness of our multiview uncertainty measurement approach. Moreover, as training progresses, the accuracy of the top 80% most confident samples improves, while the accuracy of the bottom 20% least confident samples decreases significantly. A similar pattern is observed on the test set. This decline in accuracy for low-confidence samples underpins the foundation for human-machine collaboration, as it identifies cases where the CV model lacks confidence and could benefit from human intervention.\n5.4. Failure Cases\nWe observed an interesting phenomenon in the failure cases of the CV model. For instances where there was an actual camouflaged object, but the CV model misclassified it as background, the camouflaged object was also difficult for the human eye to detect. In contrast, for instances where the CV model mistakenly identified the actual background as containing a camouflaged object (examples shown in Figure 3), humans could easily recognize that no camouflaged object was present. This discrepancy might stem from the CV model's difficulty in distinguishing between salient objects and camouflaged ones. Unlike human vision, which can rely on contextual and semantic cues to identify salient features in an image, the CV model might struggle to capture these subtleties.\n5.5. Challenges and Future work\nWe propose a human-machine collaboration framework for COD based on uncertainty estimation. Our method uses a multiview backbone to measure model confidence by analyzing output differences across views, aiding both training and collaboration. Alternative uncertainty estimation methods, such as dropout-based, bootstrap-based, or Gaussian-based approaches, may also be effective. In addition, camouflaged targets are harder to detect than traditional RSVP targets, with lower P300 amplitudes and longer latencies, which may limit human accuracy. Future work may explore EEG responses to camouflaged targets and refine decoding models. We also plan to integrate EEG and eye tracking data to aid in localization, combining them with segmentation models to achieve better human-machine collaboration in both identification and localization."}, {"title": "6. Conclusion", "content": "This study presents an integration method of RSVP-based BCIs with CV models, where low-confidence samples are redirected to human cognitive input. This approach combines the complementary strengths of humans and machines to tackle challenging detection tasks such as COD. In the CAMO data set, our method outperformed state-of-the-art approaches, with an average improvement of 4.56% in BA and 3.66% in the F1 score. For the best-performing participants, the improvements reached 7.6% in BA and 6.66% in the F1 score. By allowing humans to focus only on uncertain samples, the method significantly reduces the cognitive load and time required for RSVP tasks. Furthermore, given the variability in the performance of the BCIs due to environmental conditions, user state, and electrode quality, this human-machine collaboration framework enhances the overall robustness and reliability of the system. In summary, this research paves the way for future exploration of neuroscience and human-computer interaction, providing a promising framework for addressing complex detection challenges."}]}