{"title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yaliang Li", "Yilun Huang", "Ying Shen"], "abstract": "High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel dataset named Img-Diff, designed to enhance fine-grained image recognition in MLLMs by leveraging insights from contrastive learning and image difference captioning. By analyzing object differences between similar images, we challenge models to identify both matching and distinct components. We utilize the Stable-Diffusion-XL model and advanced image editing techniques to create pairs of similar images that highlight object replacements. Our methodology includes a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for detailed difference descriptions. The result is a relatively small but high-quality dataset of \"object replacement\" samples. We use the the proposed dataset to finetune state-of-the-art (SOTA) MLLMs such as MGM-7B, yielding comprehensive improvements of performance scores over SOTA models that trained with larger-scale datasets, in numerous image difference and Visual Question Answering tasks. For instance, our trained models notably surpass the SOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate alternative methods for generating image difference data through \"object removal\" and conduct thorough evaluation to confirm the dataset's diversity, quality, and robustness, presenting several insights on synthesis of such contrastive dataset. To encourage further research and advance the field of multimodal data synthesis and enhancement of MLLMs' fundamental capabilities for image understanding, we release our codes and dataset at https://github.com/modelscope/data-juicer/tree/ImgDiff.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has revolutionized natural language processing (NLP) [45, 60]. This advancement has paved the way for the development of Multimodal Large Language Models (MLLMs) that seamlessly integrate linguistic and visual understanding. Improving the performance of MLLMs hinges on two primary avenues: evolving model architectures and enhancing dataset quality. The majority of state-of-the-art MLLMs [3, 35, 39\u201341] implement a two-phase approach, commencing with a pre-training phase involving extensive image-text pairs for modality alignment, followed by fine-tuning aimed at optimizing visual question answering (VQA) capabilities with specific instruction tuning datasets.\nThe efficacy of pretraining datasets profoundly affects MLLMs' capabilities in performing core visual tasks. Concurrently, the quality of visual instruction tuning datasets plays a crucial role in the overall performance of MLLMs in VQA tasks and diverse downstream applications. With the evolution of visual instruction tuning datasets, several recent studies have successfully integrated object detection and Optical Character Recognition (OCR) datasets, such as Refcoco [26], Visual Genome [29], OCR-VQA [46], and TextVQA"}, {"title": "2 Background and Related Works", "content": "Multimodal Large Language Models (MLLMs) have exhibited re- markable advancements since their introduction. Research high- lights two key factors that primarily influence the effectiveness of MLLMs: model architecture and dataset quality [50]. With respect to model architecture, notable approaches include Flamingo [2], IDEFICS [22, 30], BLIP-2 [32], and Qwen-VL [3], which leverage learnable queries to extract essential information from the CLIP [12, 51] image features. Alternatively, LLaVA [39-41] and MGM [35] utilize projection-based interfaces to facilitate inter- actions between text and image modalities. Furthermore, LLaMA-Adapter [70] and LaVIN [44] implement parameter-efficient tuning mechanisms to transfer image-related information to the LLM. A recent work also show usefullness of object detection model for MLLMs [24]. From the perspective of datasets, there are two prevalent strate- gies: enhancing the quality of pretraining data and improving visual instruction tuning data. The former aims for better semantic align- ment between images and text by introducing substantial numbers of image-text pairs, enabling MLLMs to proficiently address funda- mental visual tasks, such as image captioning. However, MLLMs trained exclusively on pretraining datasets may encounter difficul- ties with VQA challenges. Recent research has increasingly con- centrated on refining visual instruction tuning datasets, enabling MLLMs to enhance performance across diverse question-answering tasks. Works like LLaVA [39-41], InstructBLIP [11], SPHINX [38], and MGM [35] leverage high-quality finetuning datasets character- ized by extensive task diversity, allowing models to excel in tasks related to image perception, reasoning, and optical character recog- nition (OCR). Additionally, methods such as Shikra [9], ASM [66], and PINK [67] utilize substantial amounts of object detection data to enhance the models' localization capabilities. In contrast to previous works, our research introduces a dataset that emphasizes image differences, showing empirical effectiveness and great potential to augment MLLMs' VQA proficiency, object localization capabilities, and discernment of image distinctions."}, {"title": "2.2 Datasets on Image Differences", "content": "Datasets focused on image differences typically consist of pairs of similar images supplemented with textual descriptions of their vari- ations. For instance, the Spot-the-Diff dataset [23] contains pairs of street scenes captured at different times by the same surveil- lance cameras. The CLEVR-Change dataset [48] delineates scene variations of geometric objects against a clean backdrop. The Birds- to-Words dataset [65] elaborates on the nuanced differences among various bird species found in natural habitats. The Image-Edit- Request [58] dataset features edited images alongside their original counterparts, accompanied by descriptions of the modifications made.\nLeveraging advancements in image editing technologies, some studies have employed generative models and editing techniques to"}, {"title": "2.3 Models for Image Difference Captioning", "content": "Image Difference Captioning (IDC) represents a specialized do- main within image captioning characterized by its focus on subtle variations between images, rather than merely identifying com- mon objects. The pioneering work in IDC, Spot-the-Diff [23], sug- gests potential change clusters and employs an LSTM [18] decoder to generate captions addressing these variations. DUDA [48] ex- plores image differences at the semantic level using ResNet [16] and an LSTM to compute dynamic attention weights, producing captions that accurately depict these differences. VARD [61] intro- duces a viewpoint-adaptive representation disentanglement net- work based on LSTM for differentiating between real and pseudo changes. Meanwhile, NCT [62] employs a transformer [63] to in- tegrate neighboring features, and CLIP4IDC [15] uses BERT-like training methodologies, adapting a CLIP model for IDC tasks.\nWith the emergence of MLLMs, VIXEN [5] marks the inaugural use of these models for IDC tasks, attempting to extract features from CLIP outputs using linear layers or Q-formers and subse- quently concatenating the features of differential images for input into an LLM to generate image difference captions."}, {"title": "3 The Curation of IMG-DIFF", "content": "In recent years, contrastive learning methods have significantly im- proved the image-text understanding of vision-language models[51]. These methods involve repeatedly pairing batches of images and texts, requiring the model to distinguish between matching and non-matching image-text pairs, which enhances the model's capability to differentiate between semantically similar and dissimilar image- text pairs. Our method incorporates contrastive learning principles and applies them to generate MLLM image-text data. Specifically, our method focuses on replacing objects between the image pairs, requiring MLLMs to identify similarities and differences in specific areas of the images, which aims to improve MLLMs' abilities to recognize fine-grained differences in images.\nThe process of generating \u201cobject replacement\u201d data can be di- vided into three parts. The first part is to generate similar images and form image pairs, where the only difference between these pairs are object replacement. Next, we name the second part \"Difference Area Generator\", which extracts bounding box regions containing object differences between the image pairs. The third part, called the \"Difference Captions Generator\", uses an MLLM to generate descriptive text for the areas containing object differences and then create question-answer pairs with questions such as \"What objects have changed in this area?\". The process involves many model selections, which are discussed in Appendix B.2. The process also involves multiple filtering operations, with the specific thresholds"}, {"title": "3.2 Image Pairs Generation", "content": "Before generating the IMG-DIFF data, we first need to generate numerous pairs of similar images. The process is shown in Fig- ure 2, which involves replacing the objects in images. We refer to InstructPix2Pix[6] and employ a generative model combined with image editing techniques to generate image pairs. However, while InstructPix2Pix utilizes the Stable-Diffusion-1.5, we use the more advanced Stable-Diffusion-XL [49] to produce more realistic images.\nWe start by obtaining 118K image captions from MS COCO[37], which are descriptions biased towards real photos. Then, we use the LLM Vicuna-1.5-13B[10] to perform object replacement in the captions. The prompt used is \"Here is a sentence: \u2018INPUT'. Please only replace one of the objects in this sentence with another object.\" Here, INPUT refers to the original caption, and the answers from the LLM are the new captions. Finally, based on the caption pairs, we use the text-to-image generative model Stable-Diffusion-XL and image editing technique Prompt-to-Prompt[17] to generate image pairs with only few objects replaced."}, {"title": "3.3 Difference Area Generator", "content": "The Difference Area Generator aims to identify the locations of object differences between the image pair. Although object detection models are capable of identifying objects inside images, the range of object categories is quite limited[25, 69]. Therefore, to increase the number of detectable object categories and enhance dataset diversity, we develop the Difference Area Genera- tor based on segmentation and image similarity comparisons. The process is illustrated in Figure 3.\nIn this phase, we first use the Image Similarity Filter to obtain image pairs with high similarity but not completely identical and use FastSAM[71] to perform image segmentation on each image. Next, we crop the images based on the bounding box information got from segmentation and use the Image-text Matching Filter to filter the cropped sub-images for the presence of valid objects. Finally, we use the Difference Detector to determine whether there are indeed differences between the bounding box regions of the image pairs and perform IoU (Intersection over Union) filtering to remove overlapping bounding boxes, ultimately obtaining valid bounding box information."}, {"title": "3.3.2 Image Similarity Filter", "content": "The Image Similarity Filter aims to filter image pairs based on the degree of similarity. The module first uses CLIP [51] to extract image features from each image in the image pair and then calculates the cosine similarity. If their cosine similarity falls within the pre-set threshold, the image pair will be considered valid. We use the Image Similarity Filter twice in the Difference Area Generator. At the beginning stage, before using FastSAM for segmentation, we use the module to ensure that the image pairs are highly similar but not exactly the same. In the Difference Detector stage, after cropping sub-images based on the bounding box information, we use the module to filter the sub-image pairs and keep only the differing ones."}, {"title": "3.3.3 Image-text Matching Filter", "content": "The Image-text Matching Filter determines whether an image contains valid objects (i.e., the re- placed or replacing objects). This module first uses BLIP [33] to"}, {"title": "3.3.4 Difference Detector", "content": "The Difference Detector is used in the final stage of the Difference Area Generator. This module deter- mines whether there are differences between the bounding box regions of the image pair. For a given bounding box, we first crop two sub-images from both image A and B based on it. These two sub-images are then filtered through the Image Similarity Filter and the bounding box is considered effective only if the difference is significant enough. After processing all bounding boxes, we use the IoU method to filter out overlapping bounding boxes. Only the bounding boxes with a higher degree of difference are retained. The remaining bounding boxes are the effective bounding boxes ultimately outputted by the Difference Area Generator."}, {"title": "3.4 Difference Captions Generator", "content": "After obtaining the valid bounding box regions, we use the Difference Captions Generator to generate difference captions about the content of these regions (with each round of process focusing on only one bounding box in one image pair). The reason our difference captions only target specific regions is that an image pair can contain multiple object differences and a single difference caption cannot fully capture all of them. Therefore, we highlight specific regions with red boxes and provide targeted difference captions to ensure greater accuracy.\nThe module consists of two stages: the first stage generates content captions for the bounding box regions and then filters the bounding boxes with these generated captions using the Image- text Matching Filter and the Captions Similarity Filter. The second"}, {"title": "3.4.2 Stage 1: Object Labeling & Filter", "content": "In Stage1, for each image pair, we first select N bounding box regions with the lowest similar- ity between images (N is set to 5 in this project) as candidate regions. Then, for each bounding box, we use the MLLM LLaVA-NEXT[40] to describe its corresponding regions and then apply two filtering processes: the first filter is the Image-text Matching Filter, which checks whether the content of the regions matches the captions; the second filter is the Captions Similarity Filter, which assesses whether there are differences between the two captions. Once the filtering is complete, we obtain valid bounding boxes and captions for subsequent difference captioning."}, {"title": "3.4.3 Captions Similarity Filter", "content": "The Captions Similarity Filter de- termines whether the two captions corresponding to the same bounding box coordinate are different. We use CLIP to obtain text features and calculate the cosine similarity between them. When the score is low enough, we consider the two captions to be different."}, {"title": "3.4.4 Stage2: Difference Captions Generating", "content": "In Stage2, for each valid bounding box of each image pair, we first draw two red boxes into the images based on the bounding box information, highlight- ing the differences for easier localization. Then, we provide the MLLM LLAVA-NEXT with the captions of the bounding box regions and instruct it to describe the differences based on the content captions and the highlighted images. Finally, we can obtain the difference caption for the bounding box of the image pair."}, {"title": "3.5 Data Statistics", "content": "In a nutshell, we generate 118K pairs of similar images using cap- tions from MSCOCO and employ the Image Similarity Filter to get 38,533 highly similar but not identical image pairs. Then, we use the Difference Area Generator to filter and produce 117,779"}, {"title": "4 Evaluation on Models trained with IMG-DIFF", "content": "To evaluate the effectiveness of our IMG-DIFF dataset, we use our dataset to finetune the widely used MLLM LLaVA-1.5[39] and MGM[35], then evaluate them on extensive benchmarks commonly used for image difference and MLLMs. Specifically, the image differ- ence benchmarks include the MMVP[59] benchmark, the Spot-the-Diff[23] benchmark, and the Image-Edit-Request[58] benchmark. Regarding the finetuning strategy, we mix our data with the original visual instruction tuning dataset of LLaVA-1.5 and MGM respectively and conduct finetuning to get the finetuned MLLMs. For Spot-the-Diff and Image-Edit-Request, since there are train- ing splits in their datasets, we further finetune the MLLMs for an additional 2 epochs using only the benchmark's training data. In the tables, \"RP\" represents \"object replacement\" data. For the image difference benchmarks, we select various SOTA models for comparison based on the original papers of these benchmarks and recent research on image difference captioning models."}, {"title": "4.2 Results on the MMVP Benchmark", "content": "The MMVP benchmark is designed to systematically assess the visual capabilities of MLLMs. Its construction method is highly re- lated to differential images: it first collects CLIP-blind pairs, which have similar CLIP features but differ in image content. Then, the dif- ferences between the images are manually described and question- answer pairs are created. Hence, the questions in MMVP is highly relevant to our dataset, as both place significant emphasis on the differences between similar images."}, {"title": "4.3 Results on the Spot-the-Diff Benchmark", "content": "The dataset of Spot-the-Diff comprises a collection of street view images. These images are obtained by capturing scenes from fixed surveillance cameras at different time, resulting in pairs of street view images with minor object differences. Following previous works, our finetuned MLLMs are evaluated on BLEU [47], METEOR[4], CIDEr-D[64] and ROUGE-L[36]."}, {"title": "4.4 Results on Image-Editing-Request", "content": "The Image-Editing-Request benchmark is focusing on image edit- ing. Each instance in its dataset consists of an image pair (i.e., a source image and an edited image) and an editing instruction which describes the transformation from the source image to the edited image. During evaluation, our models are required to generate transformation description for these image pairs, and we then cal- culate the BLEU, METEOR, CIDEr-D, and ROUGE-L scores with the models' responses and the reference answers."}, {"title": "4.5 Results on MLLM Benchmarks", "content": "Aside from evaluations related to image difference discrimina- tion capabilities, we also assess the performance of our IMG-DIFF dataset in enhancing the comprehensive abilities of MLLMs. We test our dataset using commonly used MLLM benchmarks, including VQAv2[14] and GQA[21] for assessing the comprehensive VQA capabilities of MLLMs; MMBench[42], MM-Vet[68], ScienceQA[43], and SEED-Bench [31] for testing perceptual and reasoning abili- ties; and POPE[34] for evaluating fine-grained object localization abilities. Table 3 presents the benchmark results on these 8 MLLM benchmarks, with the \u25b3 metric indicating the percentage improve- ment averaged across them.\nBased on Table 3, it can be observed that after finetuning with our dataset, the performance of LLaVA-1.5-7B shows a compre- hensive improvement, with an average increase of 3.06% across all benchmarks. For MGM-7B, the improvements brought by our dataset are not as pronounced as those observed with LLaVA-1.5- 7B, but it still achieves an average increase of 1.28%. These score improvements indicate that the finetuned MLLMs not only enhance the ability to discern differences but also improve overall visual capabilities, thereby better addressing VQA tasks."}, {"title": "4.6 Ablation Studies", "content": "To investigate the impact of filtering thresholds on our data per- formance, we set different filtering thresholds and generate var- ious versions of our dataset. We then finetune multiple versions of LLaVA-1.5-7B using these datasets and evaluate their perfor- mance on commonly used MLLM benchmarks. Specifically, the filtering threshold for the Image Similarity Filter of the Difference Area Generator is abbreviated as IS (Image Similarity). The filter- ing threshold for the Image-Text Matching Filter of the Difference Area Generator is abbreviated as BITM (Bounding Box Image-Text Matching). The filtering threshold for the Caption Similarity Filter of the Difference Captions Generator is abbreviated as CS (Cap- tions Similarity). Lastly, the filtering threshold for the Image-Text Matching Filter of the Difference Captions Generator is abbreviated as CITM (Captions Image-Text Matching). The evaluation results are shown in Table 4.\nImage Similarity (IS). Based on Table 4, Model (3) adjusts the IS threshold from 0.9-0.98 to 0.85-0.98 compared to Model (4), re- ducing the filtering intensity for the similarity of image pairs. This adjustment leads to a significant performance decline, indicating that the similarity of image pairs has a substantial impact on data quality. When similarity is low, the data generation process may introduce more noise, as semantic segmentation could generate more areas unrelated to the objects being replaced, resulting in ineffective samples.\nBounding Box Image-Text Matching (BITM). Model (2), compared to Model (1), increases the BITM threshold, meaning that when filtering to obtain valid bounding boxes, only those more likely to contain valid objects (i.e., the replaced or replacing objects) are retained. After raising the threshold, slight improvements in model performance are observed. Additionally, Model (5) removes the BITM-related Image-Text Matching filtering compared to Model (3). This operation means that as long as there is any object dif- ference between the areas of the same bounding box of the image pair, the bounding box is considered valid, regardless of whether it contains the valid objects. This operation reduces the filtering intensity, resulting in more instances in our dataset. However, af- ter removing the BITM-related Image-Text Matching filtering, the model's performance significantly declines. These two experiments demonstrate that higher BITM filtering intensity leads to better model performance, indicating that only bounding boxes related to the replaced or replacing objects should be retained."}, {"title": "5 Evaluation of Data Quality and Diversity", "content": "To assess the quality of our Img-Diff dataset, we randomly select 1,000 instances of \"object replacement\" data and employ multiple professional dataset annotators to evaluate these samples based on three metrics. The final scores are determined through a voting pro- cess. Specifically, the first metric is \"Bounding Box Difference\u201d, which evaluates whether there are differences between the two sub-images of the same bounding box. If the objects are different, we score it as \"high\"; if the objects are the same but their features (such as color, shape, etc.) are noticeably different, we score it as \"medium\"; if the objects are the same and their features are sim- lar, we score it as \"low\". The second metric is \"Content Caption Accuracy\", which evaluates whether the captions generated by Stage 1 of the Difference Captions Generator accurately describe the sub-images. If both captions are correct, we score it as \"high\"; if the captions identify the objects but incorrectly describe their fea- tures, we score it as \"medium\"; if the captions incorrectly identify the objects, we score it as \"low\". The third metric is \"Difference Caption Accuracy\", which evaluates whether the final difference captions accurately describe the object differences between the im- age pairs. If the description is accurate, we score it as \"high\"; if the object recognition is correct but the feature description is incorrect, we score it as \"medium\"; if the object recognition is incorrect, we score it as \"low\". The results are shown in Figure 6."}, {"title": "5.2 Data Diversity", "content": "The effectiveness of a dataset is closely related to the diversity of its data. By analyzing the valid object nouns included in the captions of our image pairs, we assess the diversity of our \"object replacement\" data. Specifically, we count the total number of object categories covered, the total number of unique \"object replacement pairs\u201d, and the frequency of occurrences for each object category. The statistical data is presented in Table 5.\nAs indicated in Table 5, the number of object categories covered by our dataset is 1,203, encompassing most objects in real life. The term \"object replacement pair\" refers to the combination of the replaced and replacing object names. The total number of unique \"object replacement pairs\" covered by our dataset is 3,680, with an average of 3.4 occurrences per pair. The total number of occurrences of object categories in our dataset is 25,288, with an average of 21.02 occurrences per category.\nTo evaluate the coverage of common object categories in our dataset, we analyze the occurrences of the object categories from the Object365[54] dataset in our own dataset. The results show that each object category from the Object365 dataset appears an average of 36.07 times in our dataset, totaling 13,164 occurrences, which accounts for approximately 52.06% of the total occurrences of object categories. This indicates that our dataset includes a substantial number of common object categories, ensuring a high frequency of these categories. Additionally, less common object categories make up nearly half of our dataset, demonstrating the broad coverage and high diversity of our dataset."}, {"title": "6 The \"Object Removal\u201d Exploration", "content": "Besides object replacement, determining the presence or absence of objects is also crucial. We generate a new set of data focusing on object removal, which prompts MLLMs to analyze which image in a image pair contains the specific object. Finally, we evaluate this new dataset and find that with this data, the performance improvement for LLaVA-1.5-7B increased to 3.91%, while the improvement for MGM-7B reduced to 1.1%. With these results, we conclude that if MLLMs' finetuning data is limited and does not include data related to object presence, using \"object removal\" data can be an option for"}, {"title": "7 Conclusion", "content": "In this paper, we draw inspiration from recent advances in con- trastive learning and image difference captioning to propose a novel method of contrastive data synthesis, creating a high-quality dataset called \"IMG-DIFF\" that focuses on describing object dif- ferences. Specifically, we generate many pairs of similar images where the main focus is on object replacement. Then, we use the proposed Difference Area Generator and Difference Captions Gener- ator to generate difference captions for specific regions and form question-answer pairs. In contrast to previous image difference datasets, our dataset focuses exclusively on specific regions in- side the images, circumventing the issue where the descriptive information of differences does not fully capture the differences between image pairs, thereby enhancing the accuracy of the data. Afterwards, we fine-tune LLaVA-1.5-7B and MGM-7B using our relatively small-scale dataset, yielding high performance scores on par with SOTA models trained with much larger-scale datasets in image difference tasks, and comprehensive performance improve- ments in numerous widely recognized MLLM benchmarks. These results confirm that our dataset effectively improves the ability of MLLMs to recognize differences between images and perform detailed image recognition.\nIn a nutshell, we provide a series of insights about the construc- tion of high-quality image difference datasets, showing great po- tential to effectively and efficiently enhance MLLMs via contrastive data-centric approaches. We meticulously detail the process of cre- ating our IMG-DIFF dataset and demonstrate the enhanced perfor- mance of MLLMs finetuned with our data on both image difference benchmarks and MLLM benchmarks. With this work, we hope it can catalyze further investigation into the realm of image differ- ence datasets and the fine-grained image recognition capabilities of MLLMs."}, {"title": "A The \"Object Removal\u201d Exploration", "content": "In the main page, we generate many pairs of similar images focus- ing on object replacement. Their bounding box regions generally contain objects. However, the ability to determine the object pres- ence is also crucial. Thus, we generate another set of pairs of similar images where the difference lies in the presence or absence of ob- jects, to enhance the model's ability to determine object presence. We refer to these image pairs as \"exist-absent pairs\" and the data as \"object removal\" data."}, {"title": "A.2 Generation Process", "content": "A.2.1 Workflow. \"Object removal\" involves erasing a specific ob- ject from an image and then merging the edited image with the original to form an exist-absent pair. The detailed workflow is as fol- lows: first, FastSAM is used to segment the image, which provides a set of bounding boxes and masks. Next, the Image Similarity Filter is applied to filter the bounding boxes and accompanying masks, keeping only those that contain objects. Then, we use the text-to-image generative model SDXL-turbo[53] to inpaint the images with the remaining masks, erasing specific objects from the images and generating exist-absent pairs. Afterward, we use an MLLM to describe the removed object for each exist-absent pair, and a filter is employed to verify the accuracy of the description. Finally, we draw red boxes on images based on the bounding box information and then the object descriptions are converted into multiple-choice questions, such as: \"which image has the object related to 'DESCRIP- TION' within the red bounding box? A. the left image B. the right image.\" Here, DESCRIPTION refers to the description of the erased objects. After all processing and filtering, we obtain 5,773 pieces of \"object removal\" data. The general framework is shown in Figure 7.\nA.2.2 Image Similarity Filter. The structure of the current Image Similarity Filter is identical to the one shown in Figure 3. In the current process, the function of the Image Similarity Filter is to filter out bounding boxes whose corresponding areas do not con- tain objects. For each image, we need its corresponding image in the image pair generated in Section 3.2 to determine whether its bounding box regions contain objects. Since the image pairs are generated by replacing objects, the difference areas between the two images are highly likely to be the regions containing valid objects. Therefore, for each bounding box, we crop the sub-images from image A (the current image) and image B (the other image in the pair), and then calculate the similarity of these two sub-images. When the similarity is below 0.9, we consider these two sub-images to be different, indicating that the bounding box region contains an object.\nA.2.3 Erase objects. We use the model SDXL-turbo [53] to erase objects based on the masks obtained during segmentation. The prompt is \"background, nothing, 8k.\u201d After inpainting, the object in the masked regions is erased, while the rest of the image remains unchanged. Hence, we obtain exist-absent pairs.\nA.2.4 MLLM Captioning. We use the MLLM LLaVA-NEXT to gen- erate descriptions for the erased objects. Specifically, we provide the"}, {"title": "A.3 Evaluation", "content": "We merge the \"object removal\u201d data with the \u201cobject replacement\" data, making our dataset focus on object changes as well as ob- ject presence. To test the performance changes of LLaVA-1.5-7B and MGM-7B after adding \"object removal\" data, we incorporate the combined data into the original training datasets of these two MLLMs for visual instruction tuning and evaluate the finetuned MLLMs on image difference benchmarks and MLLM benchmarks, just like the main page.\nIn the tables, \"RM\" represents \"object removal\" data.\nA.3.1 Results on MLLM benchmarks. Table 6 shows the perfor- mance of LLaVA-1.5-7B and MGM-7B finetuned with additional \"object removal\" data on commonly used MLLM benchmarks. With the assistance of \"object removal\" data, LLaVA-1.5-7B achieves fur- ther improvements across various benchmarks compared to the model that only includes \"object replacement\" data, with an aver- age increase of 3.91%. However, after adding \"object removal\" data, MGM-7B experiences a slight decline in performance, with an aver- age increase of only 1.10% across all the benchmarks. Nevertheless, it still shows score improvements on the POPE benchmark and the MMBench benchmarks.\nA.3.2 Results on image difference benchmarks. Table 7 shows the performance of LLaVA-1.5-7B and MGM-7B finetuned with our \"object removal\" data on image difference benchmarks. It can be observed that with \"object removal\" data, LLaVA-1.5-7B's scores im- prove on the MMVP benchmark and the Spot-the-Diff benchmark, while its scores fluctuate on the Image-Edit-Request benchmark. However, MGM-7B experiences varying degrees of score decreases across all three image difference benchmarks.\nA.3.3 Analysis. The benchmark results indicate that the \"object removal\" data has a comprehensive positive effect on LLaVA-1.5- 7B. However, upon finetuning with the \"object removal\" data, the performance of MGM-7B shows fluctuations. We speculate that the limited improvement for MGM might be due to the broad scope of MGM's original data, which may already encompass the new\""}, {"title": "B Additional Details of Experiments", "content": "B.1 Preprocessing of image pairs before inputting into MLLMs\nThe MLLMs selected in our paper (LLaVA-1.5, LLaVA-NEXT and MGM) only support single-image input. Therefore, our image pairs need to be horizontally concatenated before being fed into MLLMs' image encoder. Specifically, we horizontally concatenate the image pair and add a vertical black dividing line, 20 pixels wide, between the images.\nB.2 Model Selection\nB.2.1 Overview. The models used in our project are among the best- performing ones identified for the tasks assigned to them. Besides, they are interchangeable. Therefore, if better model options become available, researchers can replace the current models with those that offer superior performance to achieve a more effective dataset.\nB.2.2 Selection of the Semantic Segmentation Model. In our project, we need to use a semantic segmentation model to identify regions in images that may contain objects. To ensure a diverse range of object categories is covered, we opt for models like SAM [28] instead of traditional semantic segmentation models. Furthermore, to reduce time consumption, we select FastSAM, one of the most efficient and effective models within SAM-like category, as our segmentation model.\nB.2.3 Model Size. Considering the device limitation and time con- sumption, our paper utilizes the LLM Vicuna-1.5-13B for object name replacement in the image pairs generation process. For seman- tic segmentation in the Difference Area Generator, the FastSAM-x model is employed. For the CLIP model, we choose \"clip-vit-base- patch32\", and for the BLIP model, we select \u201cblip-itm-large-coco\". In the Difference Captions Generator, we use the MLLM LLaVA-NEXT- 13B to generate content captions and difference captions. These models are interchangeable. When resources allow, researchers can substitute them with higher-performance models to achieve datasets with improved performance."}, {"title": "B.3 Filtering Thresholds", "content": "During the generation process of \"object replacement\" data, we employ multiple filtering operations. In this subsection, we outline the filtering thresholds we use.\nIn the Difference Area Generator, we use FastSAM to perform semantic segmentation on images and obtain bounding box in- formation for regions where objects might be present. To ensure we gather a sufficient number of candidate regions, we set the confidence score threshold to 0.05, which means that we consider a region to contain objects when its confidence score is greater than 0.05. Additionally, to prevent overlapping regions, we set the Intersection over Union (IoU) threshold to 0.5.\nAt the beginning stage of the Difference Area Generator, before using FastSAM for segmentation, we employ the Image Similarity Filter to retain only those with similarity between 0.9 and 0.98. This ensures that the image pairs are highly similar but not identical.\nIn the Difference Detector stage of the Difference Area Generator, after cropping sub-images based on the bounding box information, we use the Image Similarity Filter"}]}