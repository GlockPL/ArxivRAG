{"title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yilun Huang", "Yaliang Li", "Ying Shen"], "abstract": "High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel dataset named Img-Diff, designed to enhance fine-grained image recognition in MLLMs by leveraging insights from contrastive learning and image difference captioning. By analyzing object differences between similar images, we challenge models to identify both matching and distinct components. We utilize the Stable-Diffusion-XL model and advanced image editing techniques to create pairs of similar images that highlight object replacements. Our methodology includes a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for detailed difference descriptions. The result is a relatively small but high-quality dataset of \"object replacement\" samples. We use the the proposed dataset to finetune state-of-the-art (SOTA) MLLMs such as MGM-7B, yielding comprehensive improvements of performance scores over SOTA models that trained with larger-scale datasets, in numerous image difference and Visual Question Answering tasks. For instance, our trained models notably surpass the SOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate alternative methods for generating image difference data through \"object removal\" and conduct thorough evaluation to confirm the dataset's diversity, quality, and robustness, presenting several insights on synthesis of such contrastive dataset. To encourage further research and advance the field of multimodal data synthesis and enhancement of MLLMs' fundamental capabilities for image understanding, we release our codes and dataset at https://github.com/modelscope/data-juicer/tree/ImgDiff.", "sections": [{"title": "1 Introduction", "content": "The emergence of large language models (LLMs) has revolutionized natural language processing (NLP) [45, 60]. This advancement has paved the way for the development of Multimodal Large Language Models (MLLMs) that seamlessly integrate linguistic and visual understanding. Improving the performance of MLLMs hinges on two primary avenues: evolving model architectures and enhancing dataset quality. The majority of state-of-the-art MLLMs [3, 35, 39\u201341] implement a two-phase approach, commencing with a pre-training phase involving extensive image-text pairs for modality alignment, followed by fine-tuning aimed at optimizing visual question answering (VQA) capabilities with specific instruction tuning datasets.\nThe efficacy of pretraining datasets profoundly affects MLLMs' capabilities in performing core visual tasks. Concurrently, the quality of visual instruction tuning datasets plays a crucial role in the overall performance of MLLMs in VQA tasks and diverse downstream applications. With the evolution of visual instruction tuning datasets, several recent studies have successfully integrated object detection and Optical Character Recognition (OCR) datasets, such as Refcoco [26], Visual Genome [29], OCR-VQA [46], and TextVQA"}, {"title": "2 Background and Related Works", "content": "[56], significantly enhancing MLLMs' proficiency in tasks requiring detailed image perception.\nIn light of advancements in contrastive learning and image difference captioning research in specific domains [5, 23, 48], which underscores the potential of pair of different images in refining models' image recognition capabilities, we introduce a general-purpose yet challenging dataset named IMG-DIFF. This novel dataset sets itself apart from existing visual instruction tuning datasets by generating pairs of very similar images featuring subtle object alterations. Rather than compelling MLLMs to focus solely on a single image, our dataset challenges them to analyze paired images and articulate the differences within designated regions. By doing so, we aim to empower MLLMs with enhanced abilities to scrutinize intricate areas within images and assess discrepancies across corresponding regions, thereby augmenting their fine-grained image recognition capabilities.\nFollowing the construction of our dataset, we integrate it into the original visual instruction tuning datasets of LLaVA-1.5 [39] and MGM [35]. Subsequently, we perform fine-tuning on these models and evaluate their performance on image difference benchmarks, including MMVP [59], Spot-the-Diff [23], and Image-Edit-Request [58], as well as widely recognized MLLM benchmarks. Our evaluation results reveal that after fine-tuning with our IMG-DIFF dataset, LLaVA-1.5-7B and MGM-7B achieve notable enhancements in image difference benchmarks, aligning their performance with state-of-the-art (SOTA) models. For instance, they notably surpass the SOTA models GPT-4V[1] and Gemini [13] on the MMVP benchmark. Moreover, these models exhibit comprehensive improvements across numerous well-recognized MLLM benchmarks, achieving an average score improvement of up to 3.06%, underscoring the useful role our dataset plays in bolstering MLLMs' competencies in both image difference recognition and fine-grained image analysis.\nWe further evaluate the diversity and quality of our dataset, ensuring it encompasses a broad array of object categories while showcasing rich variability. Through meticulous manual labeling, we affirm the high quality of the dataset. Additionally, we conduct ablation studies to examine the effects of various filtering intensities,. We also investigate an alternative methodology for constructing image difference data focused on \"object removal\", assessing its effectiveness on LLaVA-1.5-7B and MGM-7B, and present fruitful insights on construction of contrastive data synthesis.\nOur contributions are summarized as follows:\n\u2022 We present a novel data synthesis method and an effect-proven IMG-DIFF dataset, comprising pairs of very similar images generated through object replacement, with a focus on processes such as segmentation, filtering, and detailed captioning of image difference.\n\u2022 We execute visual instruction tuning on LLaVA-1.5-7B and MGM-7B using our dataset, and rigorously assess the fine-tuned models' performance on many image difference benchmarks and widely-used MLLM benchmarks, demonstrating substantial performance improvements.\n\u2022 We provide a comprehensive evaluation of the diversity and quality of the proposed dataset, confirming its richness and high standard. Through ablation studies, we identify empirical good filtering thresholds for such contrastive dataset."}, {"title": "2.1 Multimodal Large Language Models", "content": "Multimodal Large Language Models (MLLMs) have exhibited re-markable advancements since their introduction. Research high-lights two key factors that primarily influence the effectiveness of MLLMs: model architecture and dataset quality [50].\nWith respect to model architecture, notable approaches include Flamingo [2], IDEFICS [22, 30], BLIP-2 [32], and Qwen-VL [3], which leverage learnable queries to extract essential information from the CLIP [12, 51] image features. Alternatively, LLaVA [39-41] and MGM [35] utilize projection-based interfaces to facilitate inter-actions between text and image modalities. Furthermore, LLaMA-Adapter [70] and LaVIN [44] implement parameter-efficient tuning mechanisms to transfer image-related information to the LLM. A recent work also show usefullness of object detection model for MLLMs [24].\nFrom the perspective of datasets, there are two prevalent strate-gies: enhancing the quality of pretraining data and improving visual instruction tuning data. The former aims for better semantic align-ment between images and text by introducing substantial numbers of image-text pairs, enabling MLLMs to proficiently address funda-mental visual tasks, such as image captioning. However, MLLMs trained exclusively on pretraining datasets may encounter difficul-ties with VQA challenges. Recent research has increasingly con-centrated on refining visual instruction tuning datasets, enabling MLLMs to enhance performance across diverse question-answering tasks. Works like LLaVA [39-41], InstructBLIP [11], SPHINX [38], and MGM [35] leverage high-quality finetuning datasets character-ized by extensive task diversity, allowing models to excel in tasks related to image perception, reasoning, and optical character recog-nition (OCR). Additionally, methods such as Shikra [9], ASM [66], and PINK [67] utilize substantial amounts of object detection data to enhance the models' localization capabilities.\nIn contrast to previous works, our research introduces a dataset that emphasizes image differences, showing empirical effectiveness and great potential to augment MLLMs' VQA proficiency, object localization capabilities, and discernment of image distinctions."}, {"title": "2.2 Datasets on Image Differences", "content": "Datasets focused on image differences typically consist of pairs of similar images supplemented with textual descriptions of their vari-ations. For instance, the Spot-the-Diff dataset [23] contains pairs of street scenes captured at different times by the same surveil-lance cameras. The CLEVR-Change dataset [48] delineates scene variations of geometric objects against a clean backdrop. The Birds-to-Words dataset [65] elaborates on the nuanced differences among various bird species found in natural habitats. The Image-Edit-Request [58] dataset features edited images alongside their original counterparts, accompanied by descriptions of the modifications made.\nLeveraging advancements in image editing technologies, some studies have employed generative models and editing techniques to"}, {"title": "3 The Curation of IMG-DIFF", "content": "In contrast, our dataset is specifically designed for MLLMs. We construct our data based on the visual instruction tuning format established by mainstream MLLMs like LLaVA-1.5 and MGM, highlighting a new direction for exploration aimed at enhancing MLLMs from a data-centric perspective."}, {"title": "3.1 Overview", "content": "In recent years, contrastive learning methods have significantly im-proved the image-text understanding of vision-language models[51]. These methods involve repeatedly pairing batches of images and texts, requiring the model to distinguish between matching and non-matching image-text pairs, which enhances the model's capability to differentiate between semantically similar and dissimilar image-text pairs. Our method incorporates contrastive learning principles and applies them to generate MLLM image-text data. Specifically, our method focuses on replacing objects between the image pairs, requiring MLLMs to identify similarities and differences in specific areas of the images, which aims to improve MLLMs' abilities to recognize fine-grained differences in images.\nThe process of generating \u201cobject replacement\u201d data can be di-vided into three parts. The first part is to generate similar images and form image pairs, where the only difference between these pairs are object replacement. Next, we name the second part \"Difference Area Generator\", which extracts bounding box regions containing object differences between the image pairs. The third part, called the \"Difference Captions Generator\", uses an MLLM to generate descriptive text for the areas containing object differences and then create question-answer pairs with questions such as \"What objects have changed in this area?\". The process involves many model selections, which are discussed in Appendix B.2. The process also involves multiple filtering operations, with the specific thresholds"}, {"title": "3.2 Image Pairs Generation", "content": "Before generating the IMG-DIFF data, we first need to generate numerous pairs of similar images. The process is shown in Fig-ure 2, which involves replacing the objects in images. We refer to InstructPix2Pix[6] and employ a generative model combined with image editing techniques to generate image pairs. However, while InstructPix2Pix utilizes the Stable-Diffusion-1.5, we use the more advanced Stable-Diffusion-XL [49] to produce more realistic images.\nWe start by obtaining 118K image captions from MS COCO[37], which are descriptions biased towards real photos. Then, we use the LLM Vicuna-1.5-13B[10] to perform object replacement in the captions. The prompt used is \"Here is a sentence: \u2018INPUT'. Please only replace one of the objects in this sentence with another object.\" Here, INPUT refers to the original caption, and the answers from the LLM are the new captions. Finally, based on the caption pairs, we use the text-to-image generative model Stable-Diffusion-XL and image editing technique Prompt-to-Prompt[17] to generate image pairs with only few objects replaced."}, {"title": "3.3 Difference Area Generator", "content": "3.3.1 Overview. The Difference Area Generator aims to identify the locations of object differences between the image pair. Although object detection models are capable of identifying objects inside images, the range of object categories is quite limited[25, 69]. There-fore, to increase the number of detectable object categories and enhance dataset diversity, we develop the Difference Area Genera-tor based on segmentation and image similarity comparisons. The process is illustrated in Figure 3.\nIn this phase, we first use the Image Similarity Filter to obtain image pairs with high similarity but not completely identical and use FastSAM[71] to perform image segmentation on each image. Next, we crop the images based on the bounding box information got from segmentation and use the Image-text Matching Filter to filter the cropped sub-images for the presence of valid objects. Finally, we use the Difference Detector to determine whether there are indeed differences between the bounding box regions of the image pairs and perform IoU (Intersection over Union) filtering to remove overlapping bounding boxes, ultimately obtaining valid bounding box information.\n3.3.2 Image Similarity Filter. The Image Similarity Filter aims to filter image pairs based on the degree of similarity. The module first uses CLIP [51] to extract image features from each image in the image pair and then calculates the cosine similarity. If their cosine similarity falls within the pre-set threshold, the image pair will be considered valid. We use the Image Similarity Filter twice in the Difference Area Generator. At the beginning stage, before using FastSAM for segmentation, we use the module to ensure that the image pairs are highly similar but not exactly the same. In the Difference Detector stage, after cropping sub-images based on the bounding box information, we use the module to filter the sub-image pairs and keep only the differing ones.\n3.3.3 Image-text Matching Filter. The Image-text Matching Filter determines whether an image contains valid objects (i.e., the re-placed or replacing objects). This module first uses BLIP [33] to"}, {"title": "3.3.4 Difference Detector", "content": "extract image features, which are then compared with textual fea-tures of object names. When the image-text matching score falls within the pre-set threshold, we consider the image to contain valid objects. In the mid-stage of the Difference Area Generator, after performing sub-image cropping based on the bounding box infor-mation, we use the module to determine whether these sub-images contain valid objects and get valid bounding boxes.\n3.3.4 Difference Detector. The Difference Detector is used in the final stage of the Difference Area Generator. This module deter-mines whether there are differences between the bounding box regions of the image pair. For a given bounding box, we first crop two sub-images from both image A and B based on it. These two sub-images are then filtered through the Image Similarity Filter and the bounding box is considered effective only if the difference is significant enough. After processing all bounding boxes, we use the IoU method to filter out overlapping bounding boxes. Only the bounding boxes with a higher degree of difference are retained. The remaining bounding boxes are the effective bounding boxes ultimately outputted by the Difference Area Generator."}, {"title": "3.4 Difference Captions Generator", "content": "3.4.1 Overview. After obtaining the valid bounding box regions, we use the Difference Captions Generator to generate difference captions about the content of these regions (with each round of process focusing on only one bounding box in one image pair). The reason our difference captions only target specific regions is that an image pair can contain multiple object differences and a single difference caption cannot fully capture all of them. Therefore, we highlight specific regions with red boxes and provide targeted difference captions to ensure greater accuracy.\nThe module consists of two stages: the first stage generates content captions for the bounding box regions and then filters the bounding boxes with these generated captions using the Image-text Matching Filter and the Captions Similarity Filter. The second"}, {"title": "3.4.2 Stage 1: Object Labeling & Filter.", "content": "stage uses the content captions and the images highlighted with red boxes to generate difference captions. The overview process is shown in Figure 4.\n3.4.2 Stage 1: Object Labeling & Filter. In Stage1, for each image pair, we first select N bounding box regions with the lowest similar-ity between images (N is set to 5 in this project) as candidate regions. Then, for each bounding box, we use the MLLM LLaVA-NEXT[40] to describe its corresponding regions and then apply two filtering processes: the first filter is the Image-text Matching Filter, which checks whether the content of the regions matches the captions; the second filter is the Captions Similarity Filter, which assesses whether there are differences between the two captions. Once the filtering is complete, we obtain valid bounding boxes and captions for subsequent difference captioning."}, {"title": "3.4.3 Captions Similarity Filter.", "content": "3.4.3 Captions Similarity Filter. The Captions Similarity Filter de-termines whether the two captions corresponding to the same bounding box coordinate are different. We use CLIP to obtain text features and calculate the cosine similarity between them. When the score is low enough, we consider the two captions to be different."}, {"title": "3.4.4 Stage2: Difference Captions Generating.", "content": "3.4.4 Stage2: Difference Captions Generating. In Stage2, for each valid bounding box of each image pair, we first draw two red boxes into the images based on the bounding box information, highlight-ing the differences for easier localization. Then, we provide the MLLM LLAVA-NEXT with the captions of the bounding box regions and instruct it to describe the differences based on the content captions and the highlighted images. Finally, we can obtain the difference caption for the bounding box of the image pair."}, {"title": "3.5 Data Statistics", "content": "In a nutshell, we generate 118K pairs of similar images using cap-tions from MSCOCO and employ the Image Similarity Filter to get 38,533 highly similar but not identical image pairs. Then, we use the Difference Area Generator to filter and produce 117,779"}, {"title": "4 Evaluation on Models trained with IMG-DIFF", "content": "SOTA models GPT-4V and Gemini. This suggests that our dataset enhances the MLLM's ability to distinguish images with similar CLIP features but different content."}, {"title": "4.1 Overview", "content": "To evaluate the effectiveness of our IMG-DIFF dataset, we use our dataset to finetune the widely used MLLM LLaVA-1.5[39] and MGM[35], then evaluate them on extensive benchmarks commonly used for image difference and MLLMs. Specifically, the image differ-ence benchmarks include the MMVP[59] benchmark, the Spot-the-Diff[23] benchmark, and the Image-Edit-Request[58] benchmark.\nRegarding the finetuning strategy, we mix our data with the original visual instruction tuning dataset of LLaVA-1.5 and MGM respectively and conduct finetuning to get the finetuned MLLMs. For Spot-the-Diff and Image-Edit-Request, since there are train-ing splits in their datasets, we further finetune the MLLMs for an additional 2 epochs using only the benchmark's training data.\nIn the tables, \"RP\" represents \"object replacement\" data. For the image difference benchmarks, we select various SOTA models for comparison based on the original papers of these benchmarks and recent research on image difference captioning models."}, {"title": "4.2 Results on the MMVP Benchmark", "content": "The MMVP benchmark is designed to systematically assess the visual capabilities of MLLMs. Its construction method is highly re-lated to differential images: it first collects CLIP-blind pairs, which have similar CLIP features but differ in image content. Then, the dif-ferences between the images are manually described and question-answer pairs are created. Hence, the questions in MMVP is highly relevant to our dataset, as both place significant emphasis on the differences between similar images. Figure 5 presents the results on the MMVP benchmark."}, {"title": "4.3 Results on the Spot-the-Diff Benchmark", "content": "The dataset of Spot-the-Diff comprises a collection of street view images. These images are obtained by capturing scenes from fixed surveillance cameras at different time, resulting in pairs of street view images with minor object differences. Following previous works, our finetuned MLLMs are evaluated on BLEU [47], METEOR[4], CIDEr-D[64] and ROUGE-L[36]. Table 1 presents the results on the Spot-the-Diff benchmark."}, {"title": "4.4 Results on Image-Editing-Request", "content": "The Image-Editing-Request benchmark is focusing on image edit-ing. Each instance in its dataset consists of an image pair (i.e., a source image and an edited image) and an editing instruction which describes the transformation from the source image to the edited image. During evaluation, our models are required to generate transformation description for these image pairs, and we then cal-culate the BLEU, METEOR, CIDEr-D, and ROUGE-L scores with the models' responses and the reference answers. Table 2 presents the results on the Image-Editing-Request benchmark."}, {"title": "4.5 Results on MLLM Benchmarks", "content": "Aside from evaluations related to image difference discrimina-tion capabilities, we also assess the performance of our IMG-DIFF dataset in enhancing the comprehensive abilities of MLLMs. We test our dataset using commonly used MLLM benchmarks, including VQAv2[14] and GQA[21] for assessing the comprehensive VQA capabilities of MLLMs; MMBench[42], MM-Vet[68], ScienceQA[43], and SEED-Bench [31] for testing perceptual and reasoning abili-ties; and POPE[34] for evaluating fine-grained object localization abilities. Table 3 presents the benchmark results on these 8 MLLM benchmarks, with the \u25b3 metric indicating the percentage improve-ment averaged across them.\nBased on Table 3, it can be observed that after finetuning with our dataset, the performance of LLaVA-1.5-7B shows a compre-hensive improvement, with an average increase of 3.06% across all benchmarks. For MGM-7B, the improvements brought by our dataset are not as pronounced as those observed with LLaVA-1.5-7B, but it still achieves an average increase of 1.28%. These score improvements indicate that the finetuned MLLMs not only enhance the ability to discern differences but also improve overall visual capabilities, thereby better addressing VQA tasks."}, {"title": "4.6 Ablation Studies", "content": "To investigate the impact of filtering thresholds on our data per-formance, we set different filtering thresholds and generate var-ious versions of our dataset. We then finetune multiple versions of LLaVA-1.5-7B using these datasets and evaluate their perfor-mance on commonly used MLLM benchmarks. Specifically, the"}, {"title": "Image Similarity (IS).", "content": "filtering threshold for the Image Similarity Filter of the Difference Area Generator is abbreviated as IS (Image Similarity). The filter-ing threshold for the Image-Text Matching Filter of the Difference Area Generator is abbreviated as BITM (Bounding Box Image-Text Matching). The filtering threshold for the Caption Similarity Filter of the Difference Captions Generator is abbreviated as CS (Cap-tions Similarity). Lastly, the filtering threshold for the Image-Text Matching Filter of the Difference Captions Generator is abbreviated as CITM (Captions Image-Text Matching). The evaluation results are shown in Table 4.\nImage Similarity (IS). Based on Table 4, Model (3) adjusts the IS threshold from 0.9-0.98 to 0.85-0.98 compared to Model (4), re-ducing the filtering intensity for the similarity of image pairs. This adjustment leads to a significant performance decline, indicating that the similarity of image pairs has a substantial impact on data quality. When similarity is low, the data generation process may introduce more noise, as semantic segmentation could generate more areas unrelated to the objects being replaced, resulting in ineffective samples."}, {"title": "Bounding Box Image-Text Matching (BITM).", "content": "Bounding Box Image-Text Matching (BITM). Model (2), compared to Model (1), increases the BITM threshold, meaning that when filtering to obtain valid bounding boxes, only those more likely to contain valid objects (i.e., the replaced or replacing objects) are retained. After raising the threshold, slight improvements in model performance are observed. Additionally, Model (5) removes the BITM-related Image-Text Matching filtering compared to Model (3). This operation means that as long as there is any object dif-ference between the areas of the same bounding box of the image pair, the bounding box is considered valid, regardless of whether it contains the valid objects. This operation reduces the filtering intensity, resulting in more instances in our dataset. However, af-ter removing the BITM-related Image-Text Matching filtering, the model's performance significantly declines. These two experiments demonstrate that higher BITM filtering intensity leads to better model performance, indicating that only bounding boxes related to the replaced or replacing objects should be retained."}, {"title": "5 Evaluation of Data Quality and Diversity", "content": "instances are classified as \"low\", and nearly 80% of instances ex-hibit significant object differences between their two sub-images. In terms of \"Content Caption Accuracy\", 80.1% of sub-image pairs are described accurately, indicating that using an MLLM for label-ing is effective and that our filtering strategy is also functioning well. For the \"Difference Caption Accuracy\u201d metric, over 70% of the difference descriptions are completely accurate, with 21.8% of the samples having errors solely in feature labeling while still maintain-ing correct descriptions of object differences, which underscores the effectiveness of our difference caption generation strategy."}, {"title": "5.1 Data Quality", "content": "To assess the quality of our Img-Diff dataset, we randomly select 1,000 instances of \"object replacement\" data and employ multiple professional dataset annotators to evaluate these samples based on three metrics. The final scores are determined through a voting pro-cess. Specifically, the first metric is \"Bounding Box Difference\u201d, which evaluates whether there are differences between the two sub-images of the same bounding box. If the objects are different, we score it as \"high\"; if the objects are the same but their features (such as color, shape, etc.) are noticeably different, we score it as \"medium\"; if the objects are the same and their features are simi-lar, we score it as \"low\". The second metric is \"Content Caption Accuracy\", which evaluates whether the captions generated by Stage 1 of the Difference Captions Generator accurately describe the sub-images. If both captions are correct, we score it as \"high\"; if the captions identify the objects but incorrectly describe their fea-tures, we score it as \"medium\"; if the captions incorrectly identify the objects, we score it as \"low\". The third metric is \"Difference Caption Accuracy\", which evaluates whether the final difference captions accurately describe the object differences between the im-age pairs. If the description is accurate, we score it as \"high\"; if the object recognition is correct but the feature description is incorrect, we score it as \"medium\"; if the object recognition is incorrect, we score it as \"low\". The results are shown in Figure 6."}, {"title": "5.2 Data Diversity", "content": "The effectiveness of a dataset is closely related to the diversity of its data. By analyzing the valid object nouns included in the captions of our image pairs, we assess the diversity of our \"object replacement\" data. Specifically, we count the total number of object categories covered, the total number of unique \"object replacement pairs\u201d, and the frequency of occurrences for each object category. The statistical data is presented in Table 5.\nAs indicated in Table 5, the number of object categories covered by our dataset is 1,203, encompassing most objects in real life. The term \"object replacement pair\" refers to the combination of the replaced and replacing object names. The total number of unique \"object replacement pairs\u201d covered by our dataset is 3,680, with an average of 3.4 occurrences per pair. The total number of occurrences of object categories in our dataset is 25,288, with an average of 21.02 occurrences per category.\nTo evaluate the coverage of common object categories in our dataset, we analyze the occurrences of the object categories from the Object365[54] dataset in our own dataset. The results show that each object category from the Object365 dataset appears an average of 36.07 times in our dataset, totaling 13,164 occurrences, which accounts for approximately 52.06% of the total occurrences of object categories. This indicates that our dataset includes a substantial number of common object categories, ensuring a high frequency of these categories. Additionally, less common object categories make up nearly half of our dataset, demonstrating the broad coverage and high diversity of our dataset."}, {"title": "6 The \"Object Removal\u201d Exploration", "content": "Besides object replacement, determining the presence or absence of objects is also crucial. We generate a new set of data focusing on object removal, which prompts MLLMs to analyze which image in a image pair contains the specific object. Finally, we evaluate this new dataset and find that with this data, the performance improvement for LLaVA-1.5-7B increased to 3.91%, while the improvement for MGM-7B reduced to 1.1%. With these results, we conclude that if MLLMs' finetuning data is limited and does not include data related to object presence, using \"object removal\" data can be an option for"}, {"title": "7 Conclusion", "content": "data augmentation. Details of \"object removal\" data are presented in Appendix A.\n7 Conclusion\nIn this paper, we draw inspiration from recent advances in con-trastive learning and image difference captioning to propose a novel method of contrastive data synthesis, creating a high-quality dataset called \"IMG-DIFF\" that focuses on describing object dif-ferences. Specifically, we generate many pairs of similar images where the main focus is on object replacement. Then, we use the proposed Difference Area Generator and Difference Captions Gener-ator to generate difference captions for specific regions and form question-answer pairs. In contrast to previous image difference datasets, our dataset focuses exclusively on specific regions in-side the images, circumventing the issue where the descriptive information of differences does not fully capture the differences between image pairs, thereby enhancing the accuracy of the data. Afterwards, we fine-tune LLaVA-1.5-7B and MGM-7B using our relatively small-scale dataset, yielding high performance scores on par with SOTA models trained with much larger-scale datasets in image difference tasks, and comprehensive performance improve-ments in numerous widely recognized MLLM benchmarks. These results confirm that our dataset effectively improves the ability of MLLMs to recognize differences between images and perform detailed image recognition.\nIn a nutshell, we provide a series of insights about the construc-tion of high-quality image difference datasets, showing great po-tential to effectively and efficiently enhance MLLMs via contrastive data-centric approaches. We meticulously detail the process of cre-ating our IMG-DIFF dataset and demonstrate the enhanced perfor-mance of MLLMs finetuned with our data on both image difference benchmarks and MLLM benchmarks. With this work, we hope it can catalyze further investigation into the realm of image differ-ence datasets and the fine-grained image recognition capabilities of MLLMs."}, {"title": "Appendix", "content": "than 0.05. Additionally, to prevent overlapping regions, we set the Intersection over Union (IoU) threshold to 0.5.\nAt the beginning stage of the Difference Area Generator, before using FastSAM for segmentation, we employ the Image Similarity Filter to retain only those with similarity between 0.9 and 0.98. This ensures that the image pairs are highly similar but not identical.\nIn the Difference Detector stage of the Difference Area Generator, after cropping sub-images based on the bounding box information, we use the Image Similarity Filter to filter the sub-image pairs and consider them to be different only when the similarity score is less than 0.85.\nIn the mid-stage of the Difference Area Generator, after perform-ing sub-image cropping based on the bounding box information, we use the Image-text Matching Filter to determine whether these sub-images contain valid objects. When the score exceeds 0.35, we consider the sub-image to contain valid objects, and the bounding box is deemed effective.\nIn the Difference Area Generator, after obtaining all effective bounding boxes, we use the IoU method to filter out overlapping bounding boxes. We set the IoU threshold to 0.5, retaining only the bounding boxes with a higher degree of difference for similar positions.\nIn stage 1 of the Difference Captions Generator, after cropping the images into sub-images and generating content captions, we use the Image-text matching filter to evaluate the matching degree between the sub-images and the content captions. We only consider a caption to be correct if the image-text matching score exceeds 0.4.\nIn stage 1 of the Difference Captions Generator, we use the Captions Similarity Filter to determine whether the two content captions of an image pair, describing the regions of the same bound-ing box, are different. We use CLIP to obtain text features for the two captions and then calculate the cosine similarity between them. When the cosine similarity is below 0.85, we consider the two cap-tions to be different.\nSetting the filtering intensity too high may lead to a reduced number of remaining samples. To ensure that the dataset still has enough samples after filtering, we outline adjustable thresholds as described above. As mentioned in Section 4.6, higher filtering intensity typically results in better model performance. Therefore, researchers may consider expanding the data sources and increasing the filtering intensity to improve dataset performance."}, {"title": "A The \"Object Removal\u201d Exploration", "content": "Appendix\nA The \"Object Removal\u201d Exploration\nA.1 Overview\nIn the main page", "exist-absent pairs\" and the data as \"object removal\" data.\nA.2 Generation Process\nA.2.1 Workflow. \"Object removal\" involves erasing a specific ob-ject from an image and then merging the edited image with the original to form an exist-absent pair. The detailed workflow is as fol-lows": "first", "SDXL-turbo[53": "to inpaint the images with the remaining masks", "as": "which image has the object related to 'DESCRIPTION' within the red bounding box? A. the left image B. the right image.\" Here, DESCRIPTION refers to the description of the erased objects. After all processing and filtering, we obtain 5,773 pieces of \"object removal\" data. The general framework is shown in Figure 7.\nA.2.2 Image Similarity Filter. The structure of the current Image Similarity Filter is identical to the one shown in Figure 3. In the current process, the function"}]}