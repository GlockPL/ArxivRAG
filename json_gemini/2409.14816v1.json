{"title": "VARADE: A VARIATIONAL-BASED AUTOREGRESSIVE MODEL\nFOR ANOMALY DETECTION ON THE EDGE", "authors": ["Alessio Mascolini", "Sebastaino Gaiardelli", "Francesco Ponzio", "Nicola Dall'Ora", "Enrico Macii", "Sara Vinco", "Santa Di Cataldo", "Franco Fummi"], "abstract": "Detecting complex anomalies on massive amounts of data is a crucial task in Industry 4.0, best\naddressed by deep learning. However, available solutions are computationally demanding, requiring\ncloud architectures prone to latency and bandwidth issues. This work presents VARADE, a novel\nsolution implementing a light autoregressive framework based on variational inference, which is\nbest suited for real-time execution on the edge. The proposed approach was validated on a robotic\narm, part of a pilot production line, and compared with several state-of-the-art algorithms, obtaining\nthe best trade-off between anomaly detection accuracy, power consumption and inference frequency\non two different edge platforms.", "sections": [{"title": "Introduction", "content": "In any production context, the downtime of a machine due to the sudden breakdown of mechanical, hydraulic, or\nelectrical components leads to severe losses in terms of time and money. For this reason, efforts are spent for the\nearly detection of any irregular behavior of the production line to avoid sudden stops, enable specific preventive\nmaintenance actions, and reduce the environmental impact. This evolution is enabled by transforming traditional\nproduction machinery into Cyber-Physical Systems (CPSs), where sensor devices, communication technologies, and\ndata analytics cooperate to manage production failures in advance [1].\nIn an industrial CPS scenario, the most crucial resource is the availability of data reflecting the different aspects of\nproduction. Such data consist of multiple interdependent variables rapidly evolving over time, thus falling under the\ntypical definition of Multivariate Time Series (MTS) [2]. After collection, the time series, originated by heterogeneous\nsensors and data sources, are integrated through Industrial Internet of Things (IIoT) technologies and made available\nfor anomaly detection, visualization, and analysis [3].\nAlthough extensive research has been carried out on Multivariate Time Series Anomaly Detection (MTSAD), current\nsolutions typically lack the flexibility and scalability that is required for an effective real-time deployment [4, 5]. In\nmost proposed solutions, the raw data are in fact streamed through the IIoT network to a cloud platform [6], where an"}, {"title": "Background and related works", "content": "MTSAD scenarios are best addressed with Deep Learning (DL) methodologies, that recently proved to be more effec-\ntive in tackling complex anomalies in MTS data [11, 12] than traditional anomaly detection methods (e.g., based on\nclustering or statistical indexes [13]). Nonetheless, most DL-based solutions present major drawbacks in terms of re-\nquired data transmission and/or high computational cost [5, 14]. On the other hand, light MTSAD models compatible\nwith edge computing are typically based on tiny and scaled Convolutional Neural Networks (CNNs), that need to be\ntrained with huge sets of annotated anomalies [5, 4, 15, 14]. Collecting and annotating such training sets is however\nunfeasible in most industrial applications [4, 12].\nTo circumvent this problem, the most promising approach is to learn the characteristics of a \u201cnormal\u201d behavior from a\nlarge amount of non-anomalous data so as to be able to identify any events that significantly deviate from the normality,\nwith three different strategies: i) forecasting-based, ii) reconstruction-based, and iii) outlier detection methods.\nForecasting-based methods learn to predict a number of time steps leveraging a current context window. Then, they\ncompare the predicted values with the observed ones to identify anomalies [11]. A large number of studies in this\ngroup employ autoregressive Long Short-Term Memory (LSTM) networks, a type of recurrent network able to learn\nlong-term time dependencies in multivariate data [16, 17, 18]. A recent work leverages instead a forest of gradient\nboosted regression trees to detect anomalies in a Digital Twin-driven industrial context, by examining the residuals\nfrom the forecasts of an ensemble of weak predictors [19].\nReconstruction-based methods encode the characteristics of a normal time series into a latent representation and learn\nto reconstruct new data starting from it. The reconstruction error is then exploited to discriminate the anomalous\nvalues from the normal ones. The most popular methods in this group are built on top of autoencoders (AEs), encoder-\ndecoder neural networks where the encoder learns a compressed version of the input data, and the decoder learns to\nrecreate the input starting from the encoded representation. Among the others, [20] employed convolutional AEs for\nanomaly detection in an IoT-inspired environment, and proved that reducing the size, complexity, and training cost of\nthe AE did not lower its ability to identify anomalies.\nOutlier detectors identify anomalies based on their dissimilarity from regular data points in the feature space. Popular\nedge-friendly examples in this group are based on k-Nearest Neighbors (kNN), identifying anomalous values based\non the distance from their neighbours, and Isolation Forest, that uses the number of binary splits necessary for an\nensemble of decision trees to isolate the point from the rest of the data [21]."}, {"title": "Methods", "content": null}, {"title": "Proposed solution", "content": "VARADE works to strike a balance between traditional techniques, that offer quick inference but with limited accuracy,\nand DL models, that learn complex patterns but require substantial computational resources, not available at the edge.\nOur design choice is to employ a forecasting-based autoregressive framework: by predicting samples one at a time\nbased on previous ones, this framework is naturally suited to handle streaming data with minimal latency.\nFigure 1 illustrates in principle the proposed architecture. The model takes as input the samples at the current (to) and\npast time steps (t_1,...t_r, with T = 16 for visualization purposes), and passes them through a set of convolutional\nlayers with ReLU activations and a linear projection, to finally predict a single future time step, t\u2081. The reason behind\nthis architectural choice lies in the consideration that model inference speed of CNNs is commonly limited by memory\nbandwidth, especially with SIMD implementations for CPUs and CUDA kernels for GPUs [22]. By using convolutions\nwith kernel size and stride of 2, we obtain that the time-dimension is halved at every new layer, leading to very limited\nmemory usage and bandwidth requirements compared to the number of parameters, and hence to faster inference. On\nthe other hand, the number of feature maps is doubled every two layers (see Figure 1), helping the network to learn\nmore complex and abstract features.\nConventional forecasting-based anomaly detectors work by considering an anomaly score, measured as the euclidean\nnorm between the forecasted value and the measured one. In our experiments, we have observed that a DL-based\nautoregressive model compact enough for real-time execution on edge fails to deliver satisfactory forecasting per-\nformance, dramatically affecting the quality of the anomaly scores. This lead us to a probabilistic approach, where\nthe model outputs a probability distribution of the possible values for the next data point in the sequence (P(t1) in\nFigure 1).\nPredicting a probability distribution using a neural network is a complex problem, which can be greatly simplified by\nconstraining the distribution to be Gaussian. This approach, known as variational inference [23], leads to a simpler\noptimization problem where the objective is to find the mean and variance which minimize the loss function (details\nwill follow). The additional advantage of the Gaussian constraint is that the variance can be interpreted as the uncer-\ntainty of the prediction: since we expect the model to be more confident in its prediction when the system is operating\nnormally, and less confident when an anomaly is occurring, the variance can be directly used as an anomaly score."}, {"title": "Derivation of the loss function", "content": "As loss function we employ the inverse of the Evidence Lower Bound (ELBO), that provides a lower bound on the log\nevidence, log p(x), where x represents the observed data. Thus, maximizing the ELBO leads to a better approximation\nof the true posterior.\nThe ELBO can be decomposed into two terms:\n\u2022 The expectation of the log-likelihood under the approximate posterior, which pushes the approximate distribution to\nput more probability mass on configurations of the latent variables that explain the observed data well.\n\u2022 The negative divergence between the approximate and prior distribution, which encourages the approximate distri-\nbution to be close to the prior.\nBy presuming a Gaussian distribution, our model predicts both the mean and the logarithm of the distribution's vari-\nance. We opt for the logarithm over the simple variance, as the latter can only be positive. Hence, the reconstruction\nloss is essentially computing the negative log-likelihood of the observed data under a Gaussian distribution assumption.\nLet us assume that our data y is normally distributed with a mean of \u00b5 and a variance of $\\sigma^{2}$. The probability density\nfunction (PDF) of a normal distribution is given by:\n$p(y|\\mu, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp\\left(-\\frac{(y - \\mu)^{2}}{2\\sigma^{2}}\\right)$ (1)\nTaking the negative logarithm of the PDF to get the negative log-likelihood (NLL), we have:\n$NLL(y|\\mu, \\sigma^{2}) = -log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp\\left(\\frac{(y - \\mu)^{2}}{2\\sigma^{2}}\\right)\\right)$ (2)\nAfter simplifying the above equation, we get:\n$NLL(y|\\mu, \\sigma^{2}) = \\frac{1}{2}log(2\\pi\\sigma^{2}) + \\frac{(y - \\mu)^{2}}{2\\sigma^{2}}$ (3)\nGiven that log(2\u03c0) is just a constant, we can ignore it during optimization (as it depends on the derivative of the loss,\nand the derivative of a constant is zero). This simplifies to:\n$NLL(y|\\mu, \\sigma^{2}) = \\frac{1}{2}log(\\sigma^{2}) + \\frac{(y - \\mu)^{2}}{2\\sigma^{2}}$ (4)\nSo, in our case, the reconstruction loss we use is:\n$L_{recon} = \\frac{1}{2}log(\\sigma^{2}_{pred, i}) + \\frac{(y_{i} - \\mu_{pred, i})^{2}}{2\\sigma^{2}_{pred, i}}$ (5)\nwhere $\\sigma^{2}_{pred}$ represents the predicted variance, $\\mu_{pred}$ the predicted mean and i the current step. This formula encourages\nour model to predict a distribution close to the actual data.\nThe next part of the loss calculation introduces the Kullback\u2013Leibler (KL) divergence, which quantifies the difference\nbetween our predicted distribution and our prior, a standard Gaussian distribution. This is computed as:\n$D_{KL} = \\frac{1}{2}(1 + log(\\sigma^{2}_{pred}) - \\mu^{2}_{pred} - \\sigma^{2}_{pred})$ (6)\nThe $D_{KL}$ term encourages the model to predict the data mean and variance when it is uncertain. This helps regularize\nour model and is critical to employ our anomaly detection method.\nThe final loss function L is a weighted sum of the reconstruction loss $L_{recon}$ and the KL divergence $D_{KL}$:\n$L = L_{recon} + \\lambda D_{KL}$ (7)\nThanks to the KL divergence term, our model learns to predict a higher variance when it is uncertain about the next\nvalue, and a low variance when it is confident. During inference, the mean prediction is removed, and the variance is\ndirectly used as an anomaly score: the higher the score, the larger the detected anomaly."}, {"title": "Baseline solutions", "content": "As a baseline for our proposed method, in this study, we implement and analyze a representative sample of light\nanomaly detectors that have been successfully deployed in edge computing scenarios, by considering the approaches\nin section 2.\n\u2022 Autoregressive Long Short-Term Memory (AR-LSTM). A recurrent architecture featuring 5 LSTM recurrent layers\nwith 256 feature maps each, followed by 2 fully connected layers. The anomaly score is then calculated as the\neuclidean norm of the difference between predicted and real value, as in many previous works [24, 16, 17, 18]. \u03a4\u03bf\nfind the best configuration for our specific task, we follow the memory-efficient paradigm introduced by [25], and\npick a number of layers equal to 5 based on past experiments at a similar window size [26].\n\u2022 Gradient Boosted Regression Forest (GBRF). The technique presented in [19], with minor modifications to boost\nthe anomaly detection capabilities: the number of decision trees is increased from 5 to 30 and the dimensionality\nreduction step is removed. The anomaly score is computed in the same way as for AR-LSTM.\n\u2022 Autoencoder (AE). A convolutional autoencoder featuring 6 ResNet blocks [27]. The anomaly score is the euclidean\nnorm of the difference of reconstructed and real value.\n\u2022 kNN. Past works show kNN as the best performing nearest neighbour based algorithm for anomaly detection, with\nanomaly score computed either as the average or the maximum distance from the neighbors [28]. We employ\nmaximum distance with k=5, as it has the best compromise between accuracy and execution time.\n\u2022 Isolation Forest. An ensemble of 100 individual decision trees, that isolate each data point into a leaf. The anomaly\nscore of a data point is based on the average path length [21]. As recommended by [21], we use a contamination\nvalue of 0.1, which defines the proportion of outliers in the dataset."}, {"title": "Implementation details", "content": "All the models were implemented in TensorFlow 2.11.0 and Sklearn 1.1.2. For a fair comparison, all the anomaly\ndetection frameworks were trained in the same experimental conditions and implementing hyperparameters tuning\nstrategies 1. More specifically: the neural network-based frameworks were optimized using Adam with a fixed 10-5\nlearning rate. GBRF and Isolation Forest were trained using the mean squared error criterion and recursive binary\nsplitting, by strictly following the respective reference papers."}, {"title": "Industrial case study", "content": null}, {"title": "Kuka anthropomorphic manipulator", "content": "To create a realistic scenario for the anomaly detection methods, we focused our case study on a KUKA LBR iiwa\ncollaborative industrial robot, part of a fully-fledged production line\u00b2. The robot performs pick and place operations\nand it is controlled by a Simatic S7-1200 Programmable Logic Controller (PLC), directly connected to the robot\nthrough a hard-wired field bus. The PLC runs an OPC Unified Architecture (OPC UA) server, that exposes the KUKA\nstate and functionality as services: the activation of such services in a given order constitutes a production process.\nThe KUKA robot allows collecting the robot's parameters through its programming interface. However, this limits\nthe frequency with which such parameters can be collected to 5 Hz. At higher frequencies, queries interfere with the\ncontrolling process, causing stuttering in the robot trajectories. For this reason, we instrumented the KUKA robot\nwith seven Inertial Measurement Unit (IMU) sensors (DFRobot SEN0386), one on each robot joint, to measure the\njoint's angle, acceleration, and angular velocity. These sensors send data at 200 Hz on a serial wire after applying a\nKalman filter to reduce noise. In addition to physical data, we collected also extra-functional data from a single-phase\nenergy meter (Eastron SDM230) monitoring the energy consumption of both the robot and the industrial PC. This\nenergy meter is connected through a hard-wired Modbus with an industrial ESP-32 (Olimex ESP32-EVB), collecting\nand sending data to a MQTT broker via Ethernet.\nFigure 2 depicts the experimental setup, consisting of: the KUKA robot, seven IMU sensors, an energy meter, and\nan embedded board connected to the sensors and executing the anomaly detection model (further described in Sec-\ntion 4.4)."}, {"title": "Data stream characterization", "content": "The data stream collected from the robotic manipulator consist of 86 channels in total (reported in Table 1), including\nsignals to monitor the action currently performed by the robot (i.e., action ID), its kinematic behavior (Joint Channels)\nand its extra-functional parameters (Power Channels) [29].\nThe Joint Channels consist of data related to the seven joints collected from the IMUs sensors, each having the same\neleven components that monitor various aspects of motion and temperature. Originally, the IMU collect angles in the\n[-180, +180] \u00b0C range, causing high value changes when rotating near the two extremes. Since this may be a source\nof confusion for pattern recognition techniques, we had the orientations converted to quaternions, a 4-dimensional\ncoordinate system commonly used in robotics.\nThe Power channels consist of eight quantities monitored by the energy meter. These channels allow detecting anoma-\nlies that could be transparent with respect to the robot trajectories, such as high power draw from a motor."}, {"title": "Experimental setup", "content": "To train VARADE and the baseline anomaly detection models, we created a dataset by recording the robot performing\n30 unique actions (i.e., its machine services) executed in a cycle for a total duration of 390 minutes. The resulting\ndataset contains all the possible actions supported by the robot, distributed uniformly within its duration. This allows\nthe offline training of the anomaly detection models on the \u201cnormal behavior\" of the robot in all the possible produc-\ntion processes supported by the manufacturing system. Given the diverse nature of the anomaly detection models, the\ncollected data are normalized in the range [-1, 1] based on the minimum and maximum values of each sensor's data,\nensuring that all the features have equal importance avoiding unfair comparison.\nTo test the trained models in real-time conditions, we designed a \"collision experiment\" of 82 minutes in total. During\nthis experiment, the robot performed all the 30 possible actions. During the robot operations, 125 collision anomalies\nwere randomly generated by a human operator, by manually interfering with the robot during its movement in a very\nlimited timeframe. This simulates sudden collisions between a human worker (or an object) and the robot, which is a\nrealistic hazardous situation in a production line.\nTo test the suitability to an edge scenario, we selected two edge devices, connected to the robotic system depicted in\nFigure 2: a Nvidia Jetson Xavier NX (with 6 cores and 16 GB of RAM) and a Jetson AGX Orin (with 12 cores and 32\nGB of RAM). Each anomaly detection model has been tested by a software script that continuously reads data from\nthe sensors, prepares the data by applying a preprocessing function, and calls the inference function."}, {"title": "Experimental results", "content": "During each test, the anomaly detection accuracy was evaluated in terms of Area Under the Receiver Operating Char-\nacteristic Curve (AUC-ROC) value. The ratio is to interpret an anomaly detector as a binary classifier, where points\nare classified as anomalous if the anomaly score exceeds a certain threshold. The Receiver Operating Characteristic\n(ROC) curve plots the true positive rate against the false positive rate at varying values of this threshold, and the area\nunder this curve provides a single threshold-less [0, 1] measure of the algorithm's ability to identify the anomalous\ndata points.\nBesides the AUC-ROC score, we measured the inference frequency, and we collected all the most relevant board's\nmetrics (e.g., power consumption, RAM usage, GPU RAM usage) by exploiting the jetson-stats library. These metrics\nwere collected not only during the execution of the anomaly detection tasks, but also with the boards in Idle state for\n6 minutes: the mean value was computed as baseline to evaluate the load introduced by the anomaly detection models\nw.r.t. the standard state.\nTable 2 reports the results obtained by VARADE and by the baseline detectors described in Section 3.3 on the two\nselected edge devices.\nJetson Xavier NX VARADE placed first in terms of accuracy, with an AUC-ROC score of 0.84, with an improve-\nment of 4% w.r.t. the second most performing model (AE) and of 13% w.r.t. the third most performing one (AR-LSTM).\nInterestingly, these improvements correspond also to a higher inference frequency, improved 7 times w.r.t. AE and 3\ntimes w.r.t. AR-LSTM.\nConsidering inference frequency, VARADE placed second, with 15 Hz against 20 Hz obtained by GBRF. However,\nwhen considering the AUC-ROC scores, VARADE offers an improvement of almost 20% w.r.t. GBRF. Looking at\nRAM usage, we note that all the models use almost the same amount of memory, while VARADE uses a higher\namount of GPU RAM (500 MB). This is not a limitation for the applicability of VARADE, as the total amount of\nmemory used is under 40%, thus leaving enough space for larger anomaly models or other applications. Another\nimportant parameter for an edge device is the power required to operate, as the device could operate in conditions\nwith limited power. Almost all the models have comparable performance in terms of power consumption, except\nAR-LSTM (for its high usage of the GPU), and kNN (for its high usage of CPU).\nJetson AGX Orin Analyzing the results obtained on the Jetson AGX Orin, we can note that the results are similar\nto the ones obtained with the Jetson Xavier NX but with a different scale. We can see that inference frequency is more\nor less doubled for all the anomaly detection models, but the overall ranking remains the same, with GBRF in the first\nposition and VARADE in the second position. A significant difference relies in the GPU usage, as in this case the\nTensorFlow planner decided to run kNN and Isolation Forest on the CPU due to the higher number of CPU cores.\nFigure 3 highlights the characteristics of the different configurations by depicting the ratio between the Inference\nFrequency and the AUC-ROC scores of the tested anomaly detection models.\nLooking at the results obtained on the two boards, we can draw the following conclusions. The two anomaly detection\nmodels less suitable (in our case study) for anomaly detection on the edge are the KNN and the AR-LSTM. KNN is an\nalgorithm that cannot fully benefit from GPU parallelism (especially with a few channels, as in our case study). On\none side, this problem can be solved by exploiting CPU parallelism, but on the other side, edge devices have limited\ncomputation power, leading to high power draw and limited CPU available to run other jobs. AR-LSTM is based\non a memory-intensive architecture that is not designed to work in a constrained environment with high throughput\nrequirements. In fact, with both the boards, we can note a high GPU usage, which could seem a positive factor but\nleads in fact to low inference speed.\nAt the same time, we can note that VARADE (in red) shows the best accuracy without sacrificing too much perfor-\nmance on the inference speed, thus offering the best trade-off on both edge devices. This demonstrates its applicability"}, {"title": "Conclusions and future works", "content": "In this research we introduced VARADE, a variational based autoregressive system, to address the challenges posed\nby real-time anomaly detection on the edge. When benchmarked against conventional algorithms, VARADE demon-\nstrated superior performance, while maintaining a significantly higher inference speed compared to other anomaly\ndetection techniques. This positions VARADE as a promising solution, especially in applications that can benefit from\nthe ability to detect complex anomalies. Future works will include experimenting with a larger set of different use\ncases, to stress the flexibility of our method. Thus, we plan to integrate VARADE within the manufacturing control\nloop, enabling preventive anomaly detection to activate high-level reconfiguration strategies."}]}