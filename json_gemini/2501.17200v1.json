{"title": "Improving LLM Leaderboards with Psychometrical Methodology", "authors": ["Denis Federiakin"], "abstract": "The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks.\nIn this paper, we demonstrate the advantages of applying contemporary psychometric methodologies \u2013 originally developed for human tests and surveys \u2013 to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional na\u00efve ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.", "sections": [{"title": "1. Introduction", "content": "Ever since the introduction of ChatGPT by OpenAI in the fall of 2022, Artificial Intelligence (AI)-enhanced chatbots based on Large Language Models (LLMs) have become game-changers in many areas of human activity, revolutionizing them across the board (e.g., Iu & Wong, 2023; Mehnen et al., 2023; Biswas, 2023). The potential of AI-assisted tools to facilitate and accelerate the execution of many professional and everyday-life functions is rooted in the cognitive capacities of LLMs to act as virtually universal assistants in information processing. This has also paved the way for the constant and rapid improvement of the cognitive performance of AI-assisted tools, supported by the steady development and release of different LLMs.\nCorrespondingly, the need for testing and comparing different LLMs has emerged. The necessity for evidence-based comparison of the capabilities of various LLMs has resulted in the rise of benchmarks \u2013 sets of tasks and questions provided to the models in natural (or visual) language, requiring LLMs to generate responses. These responses are then judged on their correctness, as the questions are presumed to have definitive answers (similar to human tests). As a result, thousands of specific benchmarks have been developed over the past few years (Guo et al., 2023; Chang et al., 2023). By now, benchmarks for LLMs exist in virtually every professional field, cognitive process, or aspect of ethics.\nOne such benchmark, MMLU (Hendrycks et al., 2020), has gained special popularity. MMLU contains 15,908 multiple-choice questions covering 57 different topics, ranging from high elementary mathematics to U.S. foreign policy. This breadth allows MMLU to serve as a proxy for assessing the general awareness of LLMs about the world \u2013 essentially a measure of their general knowledge. However, MMLU is far from the only popular benchmark.\nWith the diversification of various benchmarks, the problem of systematizing information on LLM performance has also arisen. With new benchmarks and LLMs appearing almost daily, the issue of comparing and integrating information from these benchmarks has become increasingly important. Correspondingly, multiple LLM leaderboards have emerged. These leaderboards openly publish information on how well various LLMs perform across a selected set of benchmarks. As a result, LLM leaderboards have become one of the most important and trustworthy sources of information on the relative capabilities of different LLMs.\nOne such leaderboard \u2013 the Hugging Face Leaderboard (Beeching et al., 2023; https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) is particularly significant due to its community support and popularity. This leaderboard has now become one of the gold standards for LLM comparison."}, {"title": "2. Test-development vs. benchmark-development", "content": "Yet, despite the widespread popularity and attention surrounding LLMs in general and LLM benchmarking in particular, there is a surprising lack of literature that examines the quality of the benchmarks being used (Wang et al., 2023) or the ranks provided by the leaderboards. Such quality can be analyzed using elements of psychometric methodology, which aims to ensure high-quality information for decision-making (e.g., ranking individuals based on some ability or trait). This methodology is responsible for the development of key psychological concepts such as IQ and the Big Five personality theory, having been carefully refined over the past century.\nIn general, psychometricians enhance various tests and surveys by refining item formulations and statistically analyzing their performance to ensure that the claims based on test results are valid and reliable. However, to the best of our knowledge, there are almost no studies on the quality of the prominent leaderboards. This paper aims to address this gap.\nThe primary purpose of this paper is to investigate the psychometric quality of the Hugging Face Leaderboard. We begin by discussing the differences between test development and benchmark development practices, highlighting the advantages and shortcomings of each. Next, we provide a brief introduction to psychometric modeling, which is used to establish validity evidence for human tests and surveys. Following this, we describe the methodology of the study.\nWe then apply this modeling approach to the performance data of various LLMs on Hugging Face Leaderboard v.1 (Study 1) and v.2 (Study 2). Subsequently, we compare the reported average scores from the leaderboards to the estimated factor scores obtained in Studies 1 and 2, and we discuss their differences (Study 3). Finally, we present a discussion on the future of psychometric analysis in evaluating LLM benchmarking data.\nThe entirety of the test development practice can be roughly summed up as proving that a test measures what it is intended to measure. This is referred to as providing validity evidence for the specific types of claims made about respondents based on the test results (AERA, APA, NCME, 2014). The focus on the types of claims is crucial, as a test developed for, say, research purposes might be unsuitable for use in clinical practice (Truijens et al., 2019). Hence, the purpose of test application (i.e., the targeted claim) is critical for constructing validity arguments.\nThere are numerous approaches to systematizing this process, but Evidence-Centered Design (ECD) is one of the most prominent in education, social sciences, and cognitive sciences (Riconscente et al., 2015; Mislevy et al., 2003; Mislevy & Haertel, 2006). The aim of"}, {"title": "3. A brief summary of psychometric modeling", "content": "ECD is to eliminate alternative explanations for the targeted claims. For instance, if testing results suggest that respondent A has lower science literacy than respondent B, test developers must demonstrate that:\n1. The test results genuinely reflect science literacy (as defined by the test developers) and not, for example, mathematical ability, general intelligence, or any other construct.\n2. The test results differ due to a systematic difference in science literacy between respondents, rather than due to randomness in responses (i.e., differences attributable to the standard error of measurement) or construct-irrelevant factors (such as demographic differences).\nEach of these conclusions is based on numerous lower-level conclusions and studies, as addressing them requires unpacking a multitude of intertwined concepts and phenomena from social and cognitive science. As a result, ECD is a highly complex methodology that effectively integrates the test development process with the gathering of validity evidence. However, when properly implemented, it ensures that the test meets the Standards for Educational and Psychological Testing (AERA, APA, NCME, 2014). This means that it is clear what the test measures and how precise it is in accomplishing its intended purpose.\nThis approach essentially reflects the reasoning behind the causal theory of measurement (Markus & Borsboom, 2013; Mari et al., 2023). According to this perspective, when developing any measurement instrument, one must demonstrate that the results of the measurement are caused by the characteristic the instrument is intended to measure. Applied to test development using the ECD framework, this means that once psychometricians and test developers have ruled out competing explanations for the causes of the observed item responses, the only remaining explanation is the targeted characteristic.\nThis naturally brings up the issue of social constructivism (see Fried, 2017). Social constructivism essentially means that the studied phenomenon does not exist by itself, but the way it unfolds is defined by the approach taken to interact with it. In other words, the phenomenon is not \u201cset in stone\" but is rather built up (constructed) while it is studied. For example, researcher A can define mathematical ability as proficiency in addition and subtraction, while researcher B can define it as proficiency in division and multiplication. Then, the empirical consequences of this theoretical difference can arise to the point of their contradictions even though both constructs are called mathematical ability. This is a special type of problem in itself as it creates a number of theories that are seemingly similar, but describe different aspects of the same or maybe even different phenomena (Elson et al., 2023). However, this results in all tests with convincing validity evidence having a clear and detailed\ndefinitions of what they intend to measure it is a side-product of test-developers agreeing on what they mean when they name the construct.\nThe development of benchmarks, however, follows another path. While not said explicitly in any of the papers published alongside the benchmarks, the authors, apparently, ground their philosophy of measurement in representativism. In social science measurements, representativism assumes that the items exhaust all aspects (or at least are a representative sample from all possible aspects) of the underlying quality that the test targets. For example, according to this perspective, the length of the table is represented by the reading on the ruler. At the same time, within the causal approach to measurement this idea would sound as \u201cthe length of the table causes readings on the ruler\u201d. While this might appear as a semantic wordplay, this slight philosophical difference has large practical consequences for the measurement design and mathematical modeling used for it (Markus & Borsboom, 2013).\nThis naturally raises the issue of social constructivism (see Fried, 2017). Social constructivism essentially posits that the phenomenon under study does not exist independently but is shaped by the approach taken to interact with it. In other words, the phenomenon is not \u201cset in stone\u201d but is instead constructed during the process of studying it. For example, researcher A might define mathematical ability as proficiency in addition and subtraction, while researcher B might define it as proficiency in division and multiplication. The empirical consequences of these differing theoretical definitions can lead to contradictions, even though both constructs are referred to as mathematical ability.\nThis is a unique problem because it creates multiple theories that appear similar but actually describe different aspects of the same or potentially even different \u2013 phenomena (Elson et al., 2023). However, this issue has a silver lining: all tests with convincing validity evidence are accompanied by clear and detailed definitions of what they intend to measure. This is a natural byproduct of test developers agreeing on what they mean when they define a construct.\nThe development of benchmarks, however, follows a different trajectory. While this is not explicitly stated in the papers accompanying benchmarks, it appears that their authors ground their philosophy of measurement in representativism. In social science measurement, representativism assumes that the items in a test exhaust all aspects or at least constitute a representative sample of all possible aspects of the underlying quality the test aims to measure. For example, from this perspective, the length of a table is represented by the reading on a ruler."}, {"title": "4. Methods and Data", "content": "In contrast, the causal approach to measurement would frame this differently, stating that \"the length of the table causes the readings on the ruler\u201d. While this may seem like semantic wordplay, this subtle philosophical distinction has significant practical consequences for measurement design and the mathematical modeling used to support it (Markus & Borsboom, 2013).\nThis is not to say that one approach is better than the other; rather, each is suited to different measurement contexts. While the \u201clength of the table\u201d example does not fully illustrate the differences between the approaches, the measurement of more complex properties does. For instance, to measure a complex LLM's \u201cskills\u201d from the representativist paradigm, one would need an enormous number of questions in a benchmark to capture as many aspects of the skill as possible. This requirement is evident from the general tradition of benchmark development \u2013 they typically consist of an extensive number of questions. While administering such a large number of test items to human respondents is impractical, the infinite stamina of LLMs allows benchmarks to adhere to the representativist measurement tradition.\nHowever, this approach poses challenges \u2013 not because the number of measures needs to be vast, but because it must be sufficiently large to fully reflect all facets of the targeted characteristic. Herein lies the problem: elusive characteristics such as \u201cmathematical reasoning,\u201d \u201chigher-order reasoning,\u201d or \u201cintelligence\u201d have infinitely many aspects since the number of situations in which these characteristics manifest is infinite. Consequently, no set of items, no matter how large, can be definitively proven to represent the entirety of the target characteristic. This limitation means that even extensive question sets can systematically omit critical aspects of the phenomena under investigation.\nPrecisely because of this issue, assessment in education, social, and cognitive sciences has shifted toward the causal theory of measurement. According to this theory, it is not the size of the observation set that matters but the cause of the outcomes. A clear construct definition, coupled with a solid theoretical justification for item development, defines the essence of the construct to be measured. This approach liberates the test developer from the need to represent all possible aspects of the targeted trait; instead, only those aspects with well-defined parameters need to be measured.\nThese philosophical differences dictate many operational distinctions in benchmark and test development. Tests developed using the causal approach generally exhibit much clearer construct definitions and theoretical frameworks. Only a handful of benchmarks can match the level of clarity in answering the question \u201cWhat is being measured?\u201d that social science tests provide (Wang et al., 2023; e.g., Fei et al., 2023; Kardanova et al., 2024). However, this\napproach to test development is significantly more labor-intensive than benchmark development. In benchmark development, one can simply clone an item numerous times an operation that, in ECD terms, involves generating observable indicators of the measured characteristic across various item contexts.\nNonetheless, the radically representativist approach of benchmark development does offer certain advantages. For instance, the standard error of measurement becomes vanishingly small with sufficiently long tests (e.g., Linacre, 1995). Thus, reliability of measurement is generally not a concern in this domain.\nHowever, another challenge in benchmark development is sample size. For example, if a custom LLM is tuned for a very specific task, its progress can be tracked against a custom benchmark, even if the sample size (i.e., the number of LLMs) is just one. In contrast, psychometric statistical modeling requires a larger sample size, typically involving multiple respondents. This fundamental requirement makes psychometric modeling generally infeasible in such cases. This is one of the reasons why psychometric modeling has not yet been applied to analyze the quality of benchmarks or leaderboards.\nThe essence of psychometric modeling, as applied in this paper, can be defined as the controllable dimensionality reduction of an item set to a comprehensible number of dimensions using statistical models whose assumptions and interpretations align with the theoretical framework of the characteristic being measured. This means that making a claim about respondents based on 60 items is much more complex than doing so based on a single variable (e.g., ability). To reduce a relatively large number of items to fewer dimensions, specialized statistical models are employed. A key feature of these models is that they are interpretable in the context of the theory underlying the construct. For example, if a construct is presumed to consist of several interconnected traits, the model should reduce the observed variables to exactly this number of traits. Furthermore, since items are often designed to measure only specific traits rather than all traits simultaneously, the dimension reduction process should account for this aspect. This reflects the confirmatory (or reflective) approach to psychometric modeling (Hanafiah, 2020).\nIt is important to note that psychometric modeling encompasses multiple paradigms. One paradigm that appears to dominate the field is parametric latent variable modeling, which assumes the existence of a latent variable underlying several observed variables (Cai, 2012). Psychometric models in this paradigm link the observed variables (item responses) to the latent variable (e.g., ability) and are used to derive individual estimates of this latent variable.\nHowever, this is by no means the only paradigm employed by psychometricians. Depending on the type of data, the intended claims about respondents, the purpose of the modeling, and the resources available, other paradigms can be applied.\nThese include non-parametric modeling of latent variables (Sijtsma & Van Der Ark, 2022), Classical Test Theory (CTT, Crocker & Algina, 1986; which seems to dominate the LLM benchmarking field), Generalizability Theory (Jiang, 2018), Network Modeling (Marsman et al., 2018; Costantini et al., 2015), and others. Notably, while CTT uses observed scores (e.g., item sums or averages), it is still based on statistical modeling and is not free from statistical assumptions (Novick, 1966). In this regard, parametric modeling of latent variables is not inherently \u201cbetter\u201d than CTT; rather, it is suited to solving certain problems and addressing specific questions that CTT cannot (Hambleton & Jones, 1993).\nHere, we will elaborate on just two of these advantages: advanced ability estimation and test quality analysis. Simple averaging of responses to different test items assumes that all items are interchangeable. In other words, it assumes that all items have the same level of difficulty a typical assumption in many equations of Classical Test Theory (CTT). This assumption, however, is almost never true. Psychometric latent variable models account for this by recognizing that correct responses to more difficult items should \"count more\" than responses to easier items. Furthermore, these models typically assume that items differ in their sensitivity to the latent variable. In other words, different items vary in their usefulness for estimating the targeted ability some provide a lot of (Fisher) information (Muraki, 1993) about the ability, while others provide less. This approach allows psychometric models to account for differences not only across respondents but also across items, by describing items in terms of several properties (parameters). This enables the identification and exclusion of items that are nearly useless or even detrimental to ability estimation, such as items where the probability of a correct response decreases as ability increases.\nThe first major benefit of the most popular psychometric models is that they enhance the precision of the scale used to reflect ability. Because items differ in difficulty, the average (or sum) score across items operates on an ordinal scale (Stevens, 1946), as it reflects the number of items solved correctly, rather than the underlying cause of the responses. By contrast, one of the most widely used psychometric models, Item Response Theory (IRT), assumes that responding to an item correctly is a random event with a probability that depends on the latent variable (ability). Consequently, the estimates of latent ability derived from IRT are placed on an interval (metric) scale, defined by a logit-transformed probability scale. This\nprovides the estimates with a unit of measurement, significantly enhancing the range of analyses that can be performed on this scale and improving the quality of respondent rankings.\nThis improvement, however, comes at the cost of several parametric assumptions required by these models. While the use of Stevens' (1946) scale classification is generally criticized by contemporary psychometricians (Zumbo & Kroc, 2019; Thomas, 2019), it remains helpful for understanding the comparative advantages and disadvantages of different approaches. Additionally, such advanced ability estimates filter out item-specific noise, allowing for the precise ranking of respondents based solely on variance common across items.\nThere are several families of models for parametric modeling of latent variables. For example, Item Response Theory (IRT; Van der Linden, 2018) maps discrete item responses to continuous latent traits, while Factor Analysis (FA; Brown, 2015) maps continuous responses to continuous latent traits. Psychometrics also employs specialized types of finite mixture modeling: Latent Class Analysis (Eshima, 2022) and Cognitive Diagnostic Modeling (von Davier & Lee, 2019) are used to map discrete responses to discrete latent characteristics, while models such as Latent Profile Analysis (Oberski, 2016) map continuous responses to discrete latent characteristics. Each of these families varies further in terms of their preferred estimation techniques.\nOver the years, psychometricians have developed and routinely employed a variety of estimation methods, including Maximum Likelihood estimators (Chen & Zhang, 2021), Least Squares estimators (Savalei & Rosseel, 2022), Bayesian samplers (Levy & Mislevy, 2017; Wu et al., 2020), and regularization techniques (Robitzsch, 2023), as well as numerous modifications and combinations tailored to specific models. More recently, backpropagation has been proposed as a technique for estimating psychometric models (Urban & Bauer, 2021; Converse, 2021).\nParticular attention, however, is given by psychometricians to global (model-level; Goretzko et al., 2024; Cai & Monroe, 2014) and local (item- and person-level; K\u00f6hler et al., 2020; Chalmers & Ng, 2017; M\u00fcller, 2020) model fit analysis. These procedures are crucial for verifying whether the theoretical assumptions underlying model development hold true in the observed data. Each branch of psychometric modeling contains countless models that differ in their assumptions about the data. Psychometricians continually strive to strengthen the connection between theoretical assumptions derived from educational, cognitive, and social sciences and the assumptions made by mathematical models.\nFurther elaboration on the broader purposes of psychometrics is beyond the scope of this paper."}, {"title": "4.1 Analysis methodolody", "content": "In the case of applying psychometric models to LLM benchmarking data, there is a significant challenge of having more parameters than observations. Psychometric models estimate at least one parameter per observed variable (benchmark question) in Rasch (1960) models, and typically two or more in most other models. However, given the enormous number of benchmarking questions, there are not enough LLMs in existence to ensure that the number of observations (LLMs) exceeds the number of model parameters.\nA relatively simple approach to addressing this problem is parceling \u2013 collapsing groups of observed variables into a single variable (Matsunaga, 2008) through summation or averaging. While this practice has its limitations (Little et al., 2002), the parceled information is precisely what the Leaderboard provides: it reports the average accuracy of LLMs responding to relatively homogeneous groups of items (i.e., averages calculated within each benchmark). The relative homogeneity of these items allows for a better understanding of relationships between groups and resolves the issue of having more model parameters than observations.\nParceling, however, introduces a less obvious challenge for the application of psychometric modeling, as parcels (especially those derived through averaging) produce continuous variables. On the one hand, this makes the data suitable for Factor Analysis (FA), but unsuitable for Item Response Theory (IRT). However, FA assumes that the observed variables follow a normal distribution, implying that they are not only continuous but also unbounded. By contrast, parcels derived from benchmark data have fixed lower and upper bounds of 0 and 1, respectively.\nFortunately, Samejima (1973, 1974) developed a unidimensional IRT model for such observed variables as a limiting case of her Graded Response Model for polytomous responses (Samejima, 1969), where the number of categories approaches infinity. Later, Wang and Zeng (1998) extended Samejima's model by introducing a parcel \u201cdifficulty\u201d parameter. Subsequently, Ferrando (2002) formally explored the relationships between Wang and Zeng's modification of Samejima's model and linear FA. In this section, we will follow Ferrando's derivations.\nAssume the observed data contains the performance of M LLMs on P observed variables (parcels). The observed performance $U_{mp}$ of model m (m = 1,2, ..., M) on parcel p (p = 1,2, ..., P) is standardized such that 0 < $U_{mp}$ < 1. Then, the CRM assumes"}, {"title": "4.2 Data", "content": "All analyses were conducted using the lavaan package (v. 0.6-17; Rosseel et al., 2023) for the statistical programming language R (v. 4.3.0). FA reliability was estimated using the semTools package (v. 0.5-6; Jorgensen et al., 2022).\nThe data was retrieved from the Hugging Face Leaderboard (Beeching et al., 2023), where the performance of various models is published in open access. On this leaderboard, the evaluation results are presented in a few-shot manner (Brown et al., 2020). This means that the model is provided with a few examples of similar questions along with their correct answers before being evaluated on the target question. Within each task (type of questions within a benchmark), the number of shots is standardized. For some of the benchmarks, the performance is evaluated in a zero-shot manner, which is a special case of the few-shot approach where the number of preliminary examples administered is 0. All questions used in the leaderboard evaluations are Multiple-Choice (MC) with several options to choose from.\nStudy 1 aimed to analyze the first version of the Hugging Face Leaderboard. Here we use the dataset retrieved from https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard on November 30, 2024. This dataset contains the parceled performance of 3,792 LLMs on six benchmarks. These benchmarks include:\n\nAI2 Reasoning Challenge (ARC; Clark et al., 2018) a set of 2,590 grade-school science questions designed to test commonsense knowledge and advanced methods for deeper text comprehension. These questions are administered in a 25-shot manner.\nHellaSwag (Zellers et al., 2019) a set of 70,000 commonsense natural language inference MC questions. Evaluation questions are administered in a 10-shot manner.\nMMLU (see the introduction section above).\nTruthfulQA (Lin et al., 2021) a set of 684 MC questions from 38 topics, including health, law, finance, and politics. This benchmark measures how well LLMs answer questions that some humans would answer incorrectly due to false beliefs or misconceptions. These questions are administered in a 6-shot manner.\nWinoGrande (Sakaguchi et al., 2021) a set of 1,767 MC questions based on the Winograd Schema Challenge (Levesque et al., 2012), designed to measure commonsense reasoning from in-sentence context. These questions are administered in a 5-shot manner.\nGSM8K (Cobbe et al., 2021) a set of 8,500 grade-school math questions and natural language solutions, used to probe the informal reasoning abilities of large language models. These questions are administered in a 5-shot manner.\nStudy 2 aimed to analyze the second version of the Hugging Face Leaderboard. For this purpose, we used a dataset retrieved from https://huggingface.co/spaces/open-llm- leaderboard/open_llm_leaderboard on November 30, 2024. This dataset contains the parceled performance of 1,543 LLMs on six benchmarks:\n\nInstruction-Following Evaluation (IFEval; Zhou et al., 2024) a set of approximately 500 items designed to evaluate LLMs' ability to follow 25 types of explicit, verifiable instructions. Examples include: \u201cIn your response, the word {word} should appear {N} times\", \"Finish your response with this exact phrase: {end phrase}. No other words should follow this phrase\u201d, or \u201cThe entire output should be wrapped in JSON format\u201d. This benchmark evaluates adherence to instructions rather than the content of the response.\nBig Bench Hard (BBH; Suzgun et al., 2022) \u2013 a set of 6,511 items grouped into 23 tasks from the Big Bench benchmark, focusing on the most challenging problems for LLMs. These tasks include multistep arithmetic, algorithmic reasoning (e.g., Boolean expressions, SVG shapes), language understanding (e.g., sarcasm detection, name disambiguation), and world knowledge.\nMATH lvl 5 (Hendrycks et al., 2021) \u2013 subset of the MATH benchmark consisting of 12,500 items (7,500 training and 5,000 test items) based on high-school-level competition problems gathered from various sources. Items are consistently formatted using LaTeX for equations and Asymptote for figures. The benchmark is categorized into five levels of difficulty and seven content areas (Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus). The leaderboard used here includes only items from difficulty level 5, giving this subset its name.\nGraduate-Level Google-Proof Q&A Benchmark (GPQA; Rein et al., 2023) a set of 448 highly challenging knowledge questions crafted by PhD-level domain experts in fields such as biology, physics, and chemistry. The questions are designed to be difficult for laypersons (even with access to Google) but relatively easy for experts.\nMultistep Soft Reasoning (MuSR; Sprague et al., 2023) a benchmark consisting of algorithmically generated complex problems, presented in narratives approximately\n1,000 words in length that leverage long-range context parsing. The problems are divided into three categories: murder mysteries (250), object placement questions (256), and team allocation optimizations (250).\nMassive Multitask Language Understanding \u2013 Professional (MMLU-PRO; Wang et al., 2024) an expert-refined version of the MMLU benchmark, consisting of 12,032 multiple-choice items (with 10 response alternatives per item) across 14 areas: math, physics, chemistry, law, engineering, economics, health, psychology, business, biology, computer science, philosophy, and miscellaneous.\nImportantly, four of six benchmarks in Study 2 include an anti-guessing correction in the measure of LLM performance, as the majority of the questions are in an MC format. This correction is based on setting the baseline probability of a randomly selected answer as $P(U_i = 1) = 1/O_i$, where $U_i$ is the score on item i, and $O_i$ is the number of response options for item i. This adjustment leads to two substudies in Study 2: Study 2a and Study 2b, which focus on investigating the structure of the second benchmark using unnormalized (raw) and normalized (corrected) scores for the four benchmarks, respectively."}, {"title": "5. Results", "content": null}, {"title": "5.1 Study 1 \u2013 Analysis of the Hugging Face Leaderboard v. 1", "content": "The initial unidimensional model calibrated on the older leaderboard dataset, exhibited relatively poor model fit under Maximum Likelihood Robust estimator (Yuan & Bentler, 2000). Specifically, SRMR = 0.054, robust RMSEA = 0.304 (90% CI for RMSEA = [0.294, 0.314]), CFI = 0.901, TLI = 0.836 (the baseline model scaled $x^2$-statistic = 10,613.699, Yuan- Bentler correction factor = 3.010, degrees of freedom for $x^2$ = 15, p-value < 0.001; the tested model scaled $x^2$-statistic = 1,332.422, Yuan-Bentler correction factor = 2.372, degrees of freedom for $x^2$ = 9, p-value < 0.001). Despite the poor model fit, all standardized factor loadings were exceptionally high and statistically significant (the lowest z-value = 47.101)."}, {"title": "5.2.1 Study 2a \u2013 Analysis of the Hugging Face Leaderboard v. 2 (Raw Data)", "content": "The initial unidimensional model calibrated with Maximum Likelihood Robust estimator on Hugging Face Leaderboard dataset exhibited a somewhat better model fit than the model the Leaderboard v.1. Particularly, SRMR = 0.038, robust RMSEA = 0.134 (90% CI for RMSEA = [0.118, 0.151]), CFI = 0.965, TLI = 0.941 (the baseline model scaled $x^2$-statistic = 2599.826, Yuan-Bentler scaling factor = 2.397, degrees of freedom for $x^2$ = 15, p-value < 0.001; the tested model scaled $x^2$-statistic = 174.291, Yuan-Bentler scaling factor = 1.323, degrees of freedom for $x^2$ = 9, p-value < 0.001)."}, {"title": "5.2.2 Study 2b \u2013 Analysis of the Hugging Face Leaderboard v. 2 (Normalized Data)", "content": "All factor loadings were statistically significant, with the lowest z-value being 17.439.\nFollowing the greedy model improvement strategy used in Study 1, we investigated model modification indices. After five iterations, an acceptable model fit was achieved. The final model showed SRMR = 0.007, RMSEA = 0.057 (90% CI for RMSEA = [0.036, 0.081]), CFI = 0.997, TLI = 0.989 (scaled $x^2$-statistic = 18.411, Yuan-Bentler scaling factor = 1.156, degrees of freedom for $x^2$ = 4, p-value = 0.001). \nFurther addition of residual covariances was deemed unnecessary, as acceptable fit was achieved according to all criteria. All factor loadings remained statistically significant, with the lowest z-value being 17.068. Each added residual parameter improved the relative model fit, as reflected in the AIC values, which decreased from 7274.522 to 7210.461, to 7162.537, to 7127.051, to 7095.328, and finally to 7075.246. However, the reliability of the factor score estimates was 0.579, which is relatively low. This indicates that the unnormalized data provides relatively imprecise (in a practical sense) estimates of LLM ability."}, {"title": "5.3. Study 3 - The Comparison of Estimated Factor Scores and the Average Scores on Benchmarks", "content": "The initial unidimensional model, calibrated using the Maximum Likelihood Robust estimator on the Hugging Face Leaderboard dataset, exhibited somewhat better model fit compared to the initial models in Studies 1 and 2a. Specifically, SRMR = 0.028, robust RMSEA = 0.092 (90% CI for RMSEA = [0.073, 0.112"}]}