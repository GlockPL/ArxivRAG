{"title": "Egalitarian Language Representation in Language Models: It All Begins with Tokenizers", "authors": ["Menan Velayuthan", "Kengatharaiyer Sarveswaran"], "abstract": "Tokenizers act as a bridge between human language and the latent space of language models, influencing how language is represented in these models. Due to the immense popularity of English-Centric Large Language Models (LLMs), efforts are being made to adapt them for other languages. However, we demonstrate that, from a tokenization standpoint, not all tokenizers offer fair representation for complex script languages such as Tamil, Sinhala, and Hindi, primarily due to the choice of pre-tokenization methods. We go further to show that pre-tokenization plays a more critical role than the tokenization algorithm itself in achieving an egalitarian representation of these complex script languages. To address this, we introduce an improvement to the Byte Pair Encoding (BPE) algorithm by incorporating graphemes, which we term Grapheme Pair Encoding (GPE). Our experiments show that grapheme-based character extraction outperforms byte-level tokenizers for complex scripts. We validate this approach through experiments on Tamil, Sinhala, and Hindi.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have gained significant attention from the research community and the general public, especially following the public release of OpenAI's ChatGPT in 2022 (OpenAI, 2022). LLMs have been heralded for their economic impact (Teubner et al., 2023; Eloundou et al., 2023) and have applications in areas such as coding assistants (Rozi\u00e8re et al., 2024; Li et al., 2023), chat systems (Meyer et al., 2023; Dortheimer et al., 2024), and machine translation (Brants et al., 2007; Dabre et al., 2020). Mainstream LLMs are pretrained on English-dominant corpora(Zhao et al., 2024), which makes them English-centric (EC) models. Among these, Llama 3 (Dubey et al., 2024), Mistral (Jiang et al., 2023), and Phi-3 (Abdin et al., 2024) are popular in the research community due to their open weights and strong performance on tasks like the Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021). This popularity has led to the use of EC LLMs as base models for developing non-English language models, such as Tamil Llama (Balachandran, 2023) and Llama 3 8B CPT SEA-LIONv2 (Lowphansirikul et al., 2021; Singapore, 2024). However, their limitations in representing languages with complex scripts may be overlooked due to their widespread use.\nPetrov et al. (2024) demonstrates that the unequal treatment of languages begins at the tokenizer level. They argue that commercial LLM services charge based on token count, meaning that languages requiring more tokens may disadvantage users interacting in those languages. They also discuss how tokenization differences impact latency and context window requirements, as a higher token count necessitates more computational time and larger context windows. Our observations reveal that pre-tokenization is a key factor contributing to the unfair representation of languages with complex scripts such as Tamil, Sinhala, and Hindi. We argue that pre-tokenization limits the number of tokens a tokenizer can learn (refer to Section 3.1). By analyzing pre-tokenization, we can simulate the effect of a tokenizer trained with sufficient data on these languages. This helps assess whether using such a tokenizer for adapting to new complex script languages is equitable. Additionally, we show that pre-tokenization plays a more significant role than the choice of the tokenization algorithm itself (refer to Figure 1)."}, {"title": "Byte Pair Encoding (BPE)", "content": "Byte Pair Encoding (BPE) (Sennrich et al., 2016) is a popular choice for tokenizers in English-centric LLMs. However, applying BPE directly to languages with complex scripts may not be optimal. To address this, we modified the BPE algorithm by using graphemes (refer to Section 3.2) as the atomic units, which we call Grapheme Pair Encoding (GPE). This adaptation enables BPE to recognize characters in complex scripts similar to how humans would realize them, resulting in improved performance compared to standard BPE.\nEfforts have been made to develop tokenizer-free language models (Deiseroth et al., 2024; Yu et al., 2023), which operate at the byte level. However, this approach may not be ideal for languages with complex scripts such as Tamil, Sinhala, and Hindi. Processing these languages using graphemes proves to be more beneficial. We compare byte-level tokenizers, such as ByT5 (Xue et al., 2022) and CANINE (Clark et al., 2022), with grapheme-based character extractors and demonstrate that the latter performs better on our evaluation metrics."}, {"title": "2 Related Works", "content": "Evaluating Tokenizers. Goldman et al. (2024) examine the correlation between tokenizers' text compression and language models' performance on downstream tasks, showing a significant relationship. They conclude that compression is a reliable intrinsic indicator of tokenization quality. Rust et al. (2021) propose two metrics: 1) Normalized Sequence Length (NSL), which compares a tokenizer's compression against a baseline like Llama, and 2) Bytes per Token, calculated by dividing UTF-8 bytes by the tokens produced. While NSL is relative, we use a variant of Bytes per Token as an absolute measure. Petrov et al. (2024) introduce Tokenization Parity (TP), a metric assessing the tokenization of one language relative to another. We use TP to evaluate how complex scripts are underrepresented compared to English. Rust et al. (2021) also propose \u201csubword fertility\" and \"proportion of continued words\" as additional measures. Given their correlation with the previously discussed metrics, we choose \"Compression Ratio\" and \"Tokenization Parity\" for our evaluation.\nTokenization Algorithms. Sennrich et al."}, {"title": "3 Background", "content": "This section introduces pre-tokenization and graphemes to support understanding of the following content."}, {"title": "3.1 Pre-tokenization", "content": "Although tokenizers can be trained on large strings of textual data using Byte Pair Encoding (BPE), training them naively may lead to suboptimal performance on certain downstream tasks as these tokenizers may lead to tokens forming around common phrases or sentence (Dagan et al., 2024). To address this, it is beneficial to include a pre-tokenization step, a preprocessing step before the actual tokenization, that breaks the input text into smaller, manageable chunks. We will refer to these as pre-tokens throughout the paper. Since it's inception, numerous pre-tokenization methods have been proposed, such as splitting on punctuation marks or spaces, using linguistic rule-based approaches, and regular expression-based methods (Dagan et al., 2024). HuggingFace (HF) supports various pre-tokenization techniques, and further details can be found on the pretokenization page\u00b9. Table 1 illustrates the pre-tokenization outputs for the transla-"}, {"title": "3.2 Graphemes", "content": "Writing systems around the world vary in how they represent language, and they can be classified into six main types, namely logosyllabary/morphosyllabary, syllabary, abjad, alphabet, and abugida, based on the relationship between symbols and the spoken components of language (Daniels et al., 2003). Understanding the complexities involved in how these characters are represented in Unicode encoding is essential to processing these languages. For instance, in the abugida writing system, most of the characters are encoded using several Unicode points placed in a particular order, and when processing them, we need to treat those Unicode points together as a single unit, not separately. This sequence forms a character in the respective language, hereafter referred to as a grapheme. For instance, \u201c\u33a2\u201d, a Grantha grapheme encoded in Tamil Unicode, is represented using four Unicode points corresponding to the following glyphs: '\u0bb8\u0bcd','\u0b95\u0bcd','\u0bb7\u0bcd','\u0ba3\u0bcd' and '\u0b9f'. Similarly, the character in the Sinhala language, which is also based on the abugida writing system, is a sequence of five Unicode points represented by the glyphs '\u0d86', '\u0dba\u0dd4', 'x200D', '\u0db6', and '\u0ddd', although it is considered a single character or grapheme in the language, where 'x200D' is called Zero-Width Joiner. In these examples, glyphs like '', 'P',"}, {"title": "4 Software and Other Specifications.", "content": "Software Specifications. All code for the experiments was written in Python. We utilized tokenizers from the tokenizers library, which is part of the HuggingFace (HF) Transformers framework (Wolf et al., 2020). For obtaining grapheme clusters, we used the grapheme Python library.\nTokenizers Used. For our experiments, we use pretrained tokenizers from both English Centric (EC) and multilingual models. EC models include GPT-2 (Radford et al., 2019), GPT-4 (OpenAI et al., 2023), Llama 3 (Dubey et al., 2024), FLAN-T5 (Chung et al., 2022), and Gemma 2 (Team et al., 2024). Multilingual models include Aya (\u00dcst\u00fcn et al., 2024), multilingual BERT (Devlin et al., 2019) (referred to as mBERT), mT5 (Xue et al., 2021), mBART (Liu et al., 2020), and NLLB (Team et al., 2022). Note that while mBERT, mT5,"}, {"title": "5 Methodology", "content": "In this section, we have described two separate methodologies for 1) analyzing pre-tokenization and Byte-level tokenizers, and 2) our proposed Grapheme Pair Encoding (GPE) tokenization."}, {"title": "5.1 Methodology for Analyzing Pre-tokenization and Byte-Level Tokenizers", "content": "For this analysis, we rely on the metrics Compression Ratio (CR) and Tokenization Parity (TP). We define the Compression Ratio (CR) as:\n$CR = \\frac{Original \\ Sequence \\ Length}{Tokenized \\ Sequence \\ Length}$ (1)"}, {"title": "5.2 Grapheme Pair Encoding (GPE)", "content": "Instead of considering bytes as the smallest units, as done in BPE, we consider graphemes as the smallest units (refer Section 3.2). by introducing a preprocessing step that breaks the given text into graphemes and updates the initial vocabulary with the unique graphemes present in the tokenizer training data. Once the initial vocabulary is updated, the remainder of the method adheres to the standard BPE algorithm, but operates on graphemes. Our proposed methodology is detailed in Algorithm 1. We compare the GPE approach against vanilla implementations of BPE, Unigram, and WordPiece algorithms. All tokenizers are trained on a randomly sampled subset of 150k examples from the Samanantar Tamil dataset (Ramesh et al., 2021) and tested on the FLORES+ Tamil development testset (Team et al., 2022). For a fair comparison, we use a whitespace-based pre-tokenizer for all algorithms."}, {"title": "6 Results and Discussion", "content": ""}, {"title": "6.1 English Centric (EC) and Multilingual (ML) Models", "content": "Tables 3 and 4 present the maximum compression ratio (CRmax) and the minimum tokenization parity (TPmin), respectively (refer Section 3.1). The observations from both tables are discussed jointly, as they complement each other.\nAs expected, both English-centric (EC) and multilingual (ML) models demonstrate a strong compression ratio of 5\u00d7 for the English language. This can be attributed to the simplicity of the English script; being part of the ASCII system, English characters fit into a single byte in UTF-8 representation.\nThe GPT-2 pre-tokenizer exhibits the lowest performance, with a CRmax score of only 1.6\u00d7 across Tamil, Sinhala, and Hindi all non-English languages. This indicates that the compression ratio achievable by a GPT-2 tokenizer, even with sufficient training data for these lan-"}, {"title": "6.2 Byte Level Tokenization", "content": "Tables 5 and 6 present a comparison of Byte-level tokenizers and Grapheme-based character extractor based on Compression Ratio (CR) and Tokenization Parity (TP). We utilize two byte-level tokenizers: ByT5, which employs UTF-8 encoding to break text into bytes, and CANINE, which uses UTF-32 encoding based"}, {"title": "6.3 Grapheme Pair Encoding (GPE)", "content": "We evaluate our proposed Grapheme Pair Encoding (GPE) alongside traditional tokenization algorithms such as BPE, Unigram, and WordPiece. To ensure a fair comparison, we utilize the Whitespace pretokenizer from HuggingFace. Details of the training process are outlined in Section 5.2."}, {"title": "Algorithm 1: Grapheme Pair Encoding (GPE)", "content": "Input: Dataset D of lines N,\nVocabulary size |V|,\npre-tokenization regular\nexpression RE\nOutput: Vocabulary V, Merges M\n// init Vocabulary and Merges\nV \u2190 {}\nM \u2190 {}\n// init storing unique graphemes\nghs \u2190 {}\nfor i \u2190 1 to N do\n// extract pre-tokens based on\nRE\npre tokens \u2190 Extract(D[i], RE)\nfor each pti \u2208 pretokens do\n// get graphemes gsi for\neach pti\ngsi \u2190 GetGraphemes(pti)\n// get unique graphemes\nfrom gsi compared to ghs\ngSunique \u2190 GetUnique(gsi, ghs)\n// update ghs with the\nnew graphemes\nghs \u2190 Update(gsunique, ghs)\nend\nend\n// update vocabulary with ghs\nV \u2190 Update(ghs, V)\n// follow standard BPE merges\nV, M \u2190 BPE(D, V, M)"}, {"title": "For Tokenization Parity (TP)", "content": "For Tokenization Parity (TP), we adopt the definition by Petrov et al. (2024). The parity of sentence A (SA) relative to sentence B (SB) by tokenizer t is defined as:\n$TP = \\frac{|t(s_A)|}{|t(s_B)|}$ (2)\nwhere t(sa) represents the tokenization of sentence sa, and \u2758t(sA)| denotes it's length. It could be stated that tokenizer t achieves parity for A with respect to B when the tokenizer parity is close to 1 (Petrov et al., 2024).\nSince we consider the pre-tokenization outputs for these calculations, as explained in Section 3.1, the calculated CR represents the maximum CR (CRmax), as pre-tokenization determines the maximum number of tokens present. By the same logic, the calculated Tokenization Parity (TP) represents the minimum TP (TPmin)."}]}