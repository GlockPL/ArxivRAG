{"title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing", "authors": ["Wenhao Zheng", "Yixiao Chen", "Weitong Zhang", "Souvik Kundu", "Yun Li", "Zhengzhong Liu", "Eric P. Xing", "Hongyi Wang", "Huaxiu Yao"], "abstract": "Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have revolutionized various natural language processing tasks, from machine translation to context summarization and question answering (Coleman et al., 2024; Kamalloo et al., 2024; Eniser et al., 2024; He et al., 2024). However, their impressive performance comes with a substantial computational costs, particularly during inference. As these models grow in size, the costs of inference becomes a significant barrier to their practical deployment, especially in real-time applications. Therefore, there is an increasing need to reduce inference costs without compromising the quality of the generated outputs.\nTo address these issues, most existing approaches (Dao et al., 2022; Sanh et al., 2020; Kou et al., 2024; Anagnostidis et al., 2024) have proposed different methods to route different input queries to models of different sizes to reduce inference costs while maintaining output quality. Intuitively, small language models (SLMs) are assigned with simpler tasks demanding lower computational resources, while more complex cases are routed to LLMs to ensure response accuracy. However, most existing works only route queries to different models once, which means that either the LLM or the SLM will handle the entire response after routing. This one-step approach limits routing flexibility, as in many response, there is only few critical tokens need to be generated by LLM while the rest of tokens can be easily generated by SLM efficiently. As a result, simply routing these queries to LLM will significantly reduce the efficiency.\nTo address this challenge, we present a novel framework, namely Collaborative Inference with Token-lEvel Routing (CITER). CITER introduces a token-level router which outputs the policy for using LLM or using SLM. Specifically, many tokens in the response that are not important to the final prediction, can be routed and generated by SLM to reduce inference costs, while the LLM can be reserved to generate important tokens only. We propose optimizing the policy of this router using pairwise data by reinforcement learning. The objective of the optimization is to minimize the inference costs while preserving output quality. By employing this formulation, the router learns to predict token-level routing scores and make routing decisions not only based on the current token but also considering the impact of these decisions on future tokens. In order to further accelerate the estimation of the reward function defined by"}, {"title": "2. Collaborative Inference with Token-lEvel Routing (CITER)", "content": "In this section, we describe our Collaborative Inference with Token-lEvel Routing (CITER) framework that uses token-level routing to accelerate the inference of language models. As illustrated in Figure 1, in CITER, we introduce a router to facilitate collaborative inference between a powerful but computationally expensive LLM and a fast but potentially inaccurate SLM. Specifically, the router is used to predict the token-level routing score for each token, and a predefined threshold \u03c4 is used to determine which model should generate this token. The key challenge of our framework is the router training process. To feed the router with the knowledge on making the global optimal routing decisions not only based on the accuracy of the current token but also the long-term impact of its decision, we formulate the router training process as a preference-based policy optimization task, aiming to minimize the inference costs while maintaining the generation quality. To be more specific, we first formulate the RL problem and derive the reward function as token-wise routing preference, which should be computed to collect during the router training process. Subsequently, we introduce a shortcut for the reward function estimation, leveraging both the SLM and LLM's prediction to estimate the reward, to significantly accelerate the collection process of the token-wise routing preference. Finally, we propose an iterative router training process to mitigate the potential inconsistencies of routing decisions in the preference collection phase and deployment. In the rest of this section, we will outline router training and collaborative inference processes in detail."}, {"title": "2.1. Reinforcement Learning for Router Optimization", "content": "We start by introducing the foundational concepts and notation for the Markov Decision Process for token-level routing. In particular, we formulate the token-level routing task as a Markov decision process (MDP) (Bellman, 1957) where state is a series of tokens $s_h = (x_0,\u2026\u2026x_m, y_0,\u2026\u2026, y_h)$, including both the input prompt $(x_0,...x_m)$ and the response $(y_1,\u2026, y_h)$. At each time step h, the agent selects its action from $A = {A_L, A_s}$, which means generating a token from LLM ($A_L$) or SLM ($A_s$), respectively. Then we write the generation of the next token by the following transition kernel $P(s_{h+1}|s_h, a_h)$ given by the dynamics of LLM and SLM. This generation process ends once the terminal token <EOS> is generated from either of these models. We denote the generated token length as H, which can be flexible. The reward $r(s_h, a_h)$ is then related to the efficiency of selecting different models and the accuracy of the final response $r(s_h)$. The state-action value function is defined by\n$Q_{\\pi}(s, a) = E[\\sum_{t=h}^{H} r(s_t, a_t) | s_h = s, a_h = a, \\pi]$  (1)\nwith the optimal state-action value function $Q^*$ defined as $Q^*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$. The objective of the routing policy can be written by\n$\\pi^*(a|s) = arg \\max_{\\pi} E [Q_{\\pi}(s, a) \u2013 \u03b2KL(\u03c0 || \u03bc)]$\n$a \\thicksim \u03bc(a/s) \\exp(\u03b2^{-1}Q_{\\pi}(a|s)),$ (2)\nwhere \u03bc is the prior policy intuitively related to the cost difference for evaluating LLM or SLM. The expectation is taken over the randomness of language models, policy \u03c0 and the prompt $s_0$."}, {"title": "2.2. Preference-based Token-level Policy Optimization", "content": "Generally, defining the reward $r(s_h, a_h)$ as well as the state-action value function $Q_h(s_h, a_h)$ is difficult and may result in reward hacking (). To tackle with this issue, similar with (Rafailov et al., 2023), we inject the pairwise preference $1_h[a_s > a_L]$ following the Bradley-Terry model (Bradley & Terry, 1952) as:\n$Pr(a_s > a_L|s_h) = \u03c3(\u03b2(Q(s_h, a_s) \u2013 Q_h(s_h, a_L))),$  (3)"}, {"title": "2.3. Acquiring Token-level Routing Preference", "content": "In this subsection, we describe our strategy to determine the preference label $1[a_L > a_s|s_h]$. For a state $s_h$, we first generate the next token $y_{h+1}$ with the small language model and then complete the whole trajectory $S_H$ until the generation ends with <EOS> using the routing policy \u03c0. Compared to equation 1, the reward collected on this trajectory $S_H$ is an unbiased estimation of $Q_\u03c0(s_h, a_s)$. Intuitively, if using the small language model in the current step h can generate the correct response $S_H$, then the small language model is preferred. Otherwise, we assign $a_L > a_s$ and assume that the large language model can generate the correct answer, as implemented in Line 13 in Algorithm 1.\nHowever, generating and evaluating the final response $S_H$ might be expensive or even infeasible when H is large. In order to further accelerate the token-level routing preference label, we introduce a shortcut by leveraging the ground truth response $s_H^*$ provided in the dataset. As in Line 8 in Algorithm 1 suggests, if the next token $y_{h+1}$ generated by the small language model is exactly the same as the ground-truth token $y_{h+1}^*$, we assign $a_s > a_L$ since the behavior of the small language model is good enough to match the ground-truth model. In the case where the next token generated by the small language model does not match the ground truth, as carried out in Line 10, we check the next token generated by the large language model $y_{h+1}^L$ and assign $a_L > a_s$ if $y_{h+1}^L = y_{h+1}^*$. Otherwise we will go back to the aforementioned case to evaluate the completed generated trajectory as conducted in Line 13 in Algorithm 1. We would like to highlight that only when both models fail to generate the correct token $y_{h+1}^*$ based on ground truth context, the full response generation is required to compute the re-"}, {"title": "2.4. Proposed Algorithm", "content": "Finally we summarize the proposed algorithm as well as some implementation details in Algorithm 1 as an iterative update of the routing policy $\u03c0_\u03b8$. In each iteration k, the router $\u03c0_{\u03b8\u22121}$ from the previous iteration is used to collect routing preferences $P_k = {s, p}$. Then iteration goes for at most K rounds but can also stop early when $P_k = P_{k\u22121}$ and thus the policy optimization converges. The preference $p \u2208 {0, 1}$ is labeled through the three cases described in Subsection 2.3, where the algorithm calls the inference algorithm CITER when neither the LLM nor the SLM can predict the correct next token.\nThe inference algorithm CITER is then presented in Algo-\nrithm 2. In detail, CITER uses a deterministic policy where it will choose the small language model when $\u03c0(a_s|s) \u2265 \u03c0(a_L|s)$ (i.e., $\u03c0(a_s|s) \u2265 1/2$) and vice versa. To further investigate the balance between efficiency and precision by collaborating with LLM and SLM, we introduce another layer of prior policy $(\u03c1(a_s), \u03c1(a_L))$. As a result, the deterministic rule of selecting the SLM from the posterior policy distribution $\u03c0'(a|s) \u221d \u03c0(a|s)p(a)$ is that\n$\u03c0(a_s |s) \u03c1(a_s) \u2265 \u03c0(a_L|s) \u03c1(a_L) \u21d2 \u03c0(a_s|s) \u2265 \u03c1(a_L)$,\nwhere we introduce \u03c4 := p(aL) as a hyper parameter in the algorithm to probe this balance.\nRemark 2.1. We maintain separate KV caches for the small and large models. When the CITER algorithm switches between them, the KV cache from the previous model is preserved, allowing it to be reused when switching back. This eliminates the need for redundant computations, improving efficiency."}, {"title": "3. Experiments", "content": "In this section, we evaluate the performance of CITER aiming to answer the following questions: (1) Compared with the previous works on speeding up the inference of LLM, how does our framework perform in terms of the computational costs and the quality of the generated response? (2) Does the components we proposed in our framework boost the performance of the router? (3) Does the iterative training process of the router improve the performance of our framework? (4) How does the performance of our framework change with the size of the LLM? (5) Can the router distinguish the critical and non-critical tokens correctly?"}, {"title": "3.1. Experimental Setup", "content": "Dataset Description. We evaluate CITER and our baselines on five widely-used academic benchmark datasets: the commonsense QA dataset (Talmor et al., 2019) contains 12,102 questions requiring different types of commonsense knowledge to answer; the ARC-Challenge dataset (Clark et al., 2018), including 1,418 genuine grade-school level, multiple-choice science questions; the MMLU-Professional Psychology dataset (Hendrycks et al., 2021a), consisting of 874 multiple-choice questions on psychology; the GSM8k dataset (Cobbe et al., 2021) with 8.5K high quality linguistically diverse grade school math word problems and MATH dataset (Hendrycks et al., 2021b) with 12.5k problems from mathematics competitions respectively. The statistics of the datasets are in Table 1.\nEvaluation. We evaluate the performance of CITER and the baseline methods using the test sets and corresponding evaluation metrics for each dataset. Specifically, a threshold T is applied in each method to balance the trade-off between response accuracy and the overall computational costs. The computational costs are defined as the average floating point operations (FLOPs) per query. We then plot the accuracy vs. the computational costs curve to illustrate the acceleration performance of both CITER and the baselines. The optimal point is located in the top-left corner of the curve, corresponding to the highest accuracy with the lowest costs.\nBaselines. We compare CITER against three methods: a representative query-level routing approach (RouteLLM (Ong et al., 2024)), a token-level routing method (Co-LLM (Shen et al., 2024)), and a non-routing-based technique (Speculative Decoding (Leviathan et al., 2023)). RouteLLM makes routing decisions at the query level, directing entire queries to different models for generation, while Co-LLM operates at the token level, dynamically routing each token to different models throughout the generation process. In contrast, Speculative Decoding (Leviathan et al., 2023) does not involve routing between models; instead, it leverages the SLM to propose a set of candidate tokens, which are then verified by the LLM.\nImplementation Details We implement our framework using the Hugging Face Transformers library (Wolf et al., 2020). For the SLM and LLM, we utilize Qwen2-1.5b and Qwen2-72b, respectively. The router is implemented as a multilayer perceptron (MLP) network with three hidden layers, ReLU activation (Agarap, 2019), BatchNorm normalization (Ioffe & Szegedy, 2015), and a 0.1 dropout rate. It is trained using the Adam optimizer (Kingma & Ba, 2017) with a learning rate of 1 \u00d7 10-7, betas of (0.9, 0.99), and no weight decay. Training is performed on a single NVIDIA H100 GPU with a batch size of 80. The iterative training process runs for 2 rounds."}, {"title": "3.2. Overall Performance", "content": "We conduct extensive experiments to assess the performance of CITER across all benchmark datasets, comparing it against baseline methods. The results are presented in Figure 2. Notably, all token-level routing methods, including CITER and Co-LLM, significantly outperform the query-level routing method, RouteLLM, across all datasets, particularly on the Commonsense QA and GSM8k datasets, reducing up to 30% computational costs while maintaining the same accuracy or achieving up to 12% higher accuracy with the same cost. This emphasizes the effectiveness of token-level routing, which provides enhanced flexibility in reducing computational costs while preserving response quality. Notably, Speculative Decoding does not reduce the FLOPs of the inference process; instead, it increases computational costs due to failed guesses. Furthermore, CITER consistently surpasses Co-LLM, achieving comparable accuracy with up to 27% fewer computational costs or delivering up to a 17% improvement in accuracy with the same cost. These findings demonstrate the success of our framework in accelerating LLM inference. This outcome is expected, as Co-LLM does not consider long-term information during the router training phase, which is crucial for token-level routing. In the following section, we present experiments to further demonstrate the importance"}, {"title": "3.3. Analysis of Long-Term Influence", "content": "In this section, we conduct an ablation study on a key component of our framework: the long-term influence of routing decisions, to evaluate its effectiveness. For this purpose, we design an ablation variant, CITER-S, where the SLM is selected if both the SLM and LLM provide incorrect predictions during the routing preference collection, disregarding the long-term impact of routing decisions. The results are shown in Figure 3. Clearly, CITER significantly outperforms the ablation variant CITER-S across all datasets, reducing computational costs by up to 42% while maintaining the same accuracy, or achieving up to a 23% accuracy improvement with the same cost. These findings highlight the critical role of accounting for the long-term influence of routing decisions."}, {"title": "3.4. Analysis of Iterative Training Process", "content": "To highlight the importance of the iterative training process, we present the performance curve of CITER with the router over the first three iterations on the Commonsense QA dataset. As shown in Figure 5, the results demonstrate a clear improvement in performance from the first to the second iteration. In the second iteration, CITER reduces ~ 5% computational costs while maintaining the same accuracy or achieves 2 ~ 3% higher accuracy with the same cost compared to the first. This improvement underscores the effectiveness of our proposed iterative training process. Moreover, the performance curve of the third iteration closely follows that of the second, indicating that the router has already converged by the second iteration. The rapid convergence of the router emphasizes the robustness of our training strategy, suggesting that optimal performance can be achieved without excessive computational costs or extended training periods."}, {"title": "3.5. Compatibility Analysis", "content": "Additionally, we conduct experiments with Llama3.1 series models to demonstrate the compatibility of our framework. Specifically, in this section, we leverage the Llama3.1-70B model as the LLM and the Llama3.1-8B model as the SLM. The results are illustrated in Figure 6. Similarly to the results with Qwen series, CITER consistently outperforms all other baseline methods, achieving comparable accuracy with up to 32% fewer computational costs or providing up to a 5% improvement in accuracy with the same cost, compared to Co-LLM, the best baseline method. This result further demonstrates the effectiveness of our framework and additionally highlights the compatibility of our framework with different series of models."}, {"title": "3.6. Analysis of the Impact of SLM Model Size", "content": "We further scale up the SLM size from Qwen2-1.5B to Qwen2-7B, while keeping the LLM fixed to Qwen2-72B, to understand the scalability of our framework. As shown in Figure 4, the results clearly demonstrate that CITER reduces computational costs by up to 10% while maintaining the same level of accuracy or achieves up to 11% higher accuracy with the same cost when using Qwen2-7B as the SLM compared to Qwen2-1.5B, particularly on the commonsense QA and GSM8k datasets, underscoring our framework's scalability with larger SLMs. However, the performance gap is most noticeable when only very little tokens are generated by the LLM introducing a very small additional cost, and it gradually diminishes or even disappears as the cost further increases. This is expected, as the SLM's capacity limits its performance, and the quality of responses increasingly depends on the LLM as more calls are routed to it."}, {"title": "3.7. Case Study Analysis on the Router", "content": "Finally, we perform a case study to further analyze the decision-making process of the router in our framework. A selection of examples, along with their corresponding routing decisions, is shown in Figure 7. In the left example, it is clear that our router accurately identifies the critical tokens, including the first occurrence of the answer \"Midwest\" and the word \"fertile,\" which describes the farmland in the Midwest, both crucial to the final answer. Moreover, most non-critical tokens are efficiently offloaded to the SLM, effectively reducing computational costs.\nIn the right example, we compare CITER with the token-level routing method Co-LLM. It is evident that our router outperforms Co-LLM by correctly identifying potential critical tokens, particularly time-related words. In Co-LLM's response, at the first red-marked word \"morning,\" Co-LLM incorrectly routes the word \"the\" to the LLM while assigning the contextually important word \"morning\" to the SLM, leading to an initial error in the response. Similarly, Co-LLM routes the critical phrase \"afternoon meal\" to the SLM, which results in the final incorrect prediction. In contrast, our router correctly identifies the critical word \"day\" and routes it to the LLM, followed by routing the phrase \"has already\" to accurately capture the reasoning process, ultimately leading to the correct prediction.\nThese examples illustrate that the router in CITER effectively distinguishes between critical and non-critical tokens, offloading non-critical tokens to the SLM to minimize computational overhead, while leveraging the LLM to ensure the quality of the generated response."}, {"title": "4. Related Work", "content": "In this section, we conduct a literature review that mainly focuses on prior LLM inference acceleration methods, especially those that involve using routing mechanisms and collaborative inference between LLMs for inference acceleration.\nQuery-Level Routing Mechanisms. Previous routing methods (Jang et al., 2023; Chronopoulou et al., 2023; Diao et al., 2023; Lu et al., 2023; Cheng et al., 2024; Lu et al., 2024; Chen et al., 2023b; Wang et al., 2024b) for efficient inference mainly focus on routing entire user queries to different models for generation. For example, Routoo (Mohammadshahi et al., 2024) proposes a performance predictor and a cost-aware decoder to route between LLMs, considering both performance and resource constraints; RouteLLM (Ong et al., 2024) formulates the routing problem as a classification problem and employs a data augmentation framework to significantly expand the dataset used for training the router. However, as highlighted in Section 1, routing at the query-level granularity may lead to suboptimal performance, as non-critical tokens in complex queries may be generated inefficiently, while critical tokens in simple queries may suffer from inaccuracy. In contrast, token-level routing methods offer more fine-grained control over the routing process, improving both inference costs and the quality of the generated response.\nToken-Level Routing Mechanisms. Unlike query-level routing methods, previous token-level routing methods (Pfeiffer et al., 2021; Belofsky, 2023; Muqeeth et al., 2024; Wang et al., 2024a; Wu et al., 2024; Xu et al., 2024) mainly focus on routing input tokens to different specialized experts to enhance performance without considering the computational costs. For example, Arrow (Ostapenko et al.,"}, {"title": "5. Conclusion", "content": "In this paper, we introduced CITER, a novel collaborative inference with token-level routing framework designed to reduce the inference cost of LLM while maintaining high-quality generation. By dynamically routing non-critical tokens to a SLM and reserving the LLM for critical tokens, CITER achieves an efficient balance between computational cost and generation quality. We formulated the routing problem as a policy optimization task, where the router learns to make token-level decisions based on both immediate token quality and long-term impact. Furthermore, we introduced a shortcut for reward estimation to enhance training efficiency. Experimental results across five benchmark datasets demonstrate that CITER significantly reduces inference costs while preserving accuracy, offering a promising solution for real-time and resource-constrained applications."}]}