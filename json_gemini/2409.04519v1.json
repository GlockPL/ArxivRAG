{"title": "The role of data embedding in quantum autoencoders for improved anomaly detection", "authors": ["Jack Y. Araz", "Michael Spannowsky"], "abstract": "The performance of Quantum Autoencoders (QAEs) in anomaly detection tasks is critically de- pendent on the choice of data embedding and ansatz design. This study explores the effects of three data embedding techniques-data re-uploading, parallel embedding, and alternate embedding on the representability and effectiveness of QAEs in detecting anomalies. Our findings reveal that even with relatively simple variational circuits, enhanced data embedding strategies can substantially improve anomaly detection accuracy and the representability of underlying data across different datasets. Starting with toy examples featuring low-dimensional data, we visually demonstrate the effect of different embedding techniques on the representability of the model. We then extend our analysis to complex, higher-dimensional datasets, highlighting the significant impact of embedding methods on QAE performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Anomaly detection, the process of identifying data points that deviate significantly from established pat- terns, is a critical task with applications spanning multi- ple domains. These include fraud detection in finance [1], fault detection in industrial systems [2], and monitoring for cybersecurity threats [3]. Quantum computing, with its potential to provide exponential speedups over classi- cal methods, offers novel and promising pathways to en- hance machine learning algorithms [4, 5], including those used for anomaly detection [6-9]. Variational Autoencoders (VAEs) [10] have been ex- tensively studied and applied in anomaly detection due to their ability to learn probabilistic representations of data. A VAE operates by encoding input data into a latent space and then reconstructing it; see the upper panel of Fig. 1. By learning the underlying distribution of normal data, VAEs can effectively flag anomalies as data points that deviate from this learned distribution. This approach has proven particularly useful in scenar- ios where the normal data distribution is complex and high-dimensional, making it difficult for simpler models to capture. Building on the success of classical VAEs, Quantum Autoencoders (QAEs) have recently emerged as powerful tools for anomaly detection [7, 8, 11-16]. QAEs utilize quantum circuits to learn compressed representations of normal data (see the lower panel of Fig. 1), potentially offering more efficient and accurate anomaly detection compared to their classical counterparts. However, the success of QAEs hinges on two crucial components: (1) the ansatz architecture, which determines the structure and expressive power of the quantum circuits, and (2) the data embedding method, which influences how well classical data is represented in the quantum framework. Although initial studies have shown that QAEs can outperform classical autoencoders in specific anomaly de- tection tasks [7], there remains a significant gap in under- standing how different ansatz designs and data embed- ding strategies impact the performance of QAEs. The choice of ansatz affects the model's ability to generalize and capture complex data patterns [17, 18], while the embedding strategy plays a critical role in determining how effectively classical data can be mapped into quantum circuits. Techniques like data reuploading and parallel embedding offer different trade-offs in terms of resource"}, {"title": "II. QUANTUM AUTOENCODER", "content": "A classical VAE consists of two main parts: an en- coder (E) and a decoder (D). The encoder embeds the input data into an ansatz and compresses it into a lower- dimensional latent space. The decoder then takes this compressed latent space as input and expands it back to the original dimensionality of the input data. By optimiz- ing the distance or difference between the input data and the output from the network, the model can effectively capture the characteristics of the input. If the model is trained on a specific dataset, it can be used as a measure of anomaly to differentiate this dataset from others. The upper panel of Fig. 1 presents a schematic representation of the VAE, where a five-dimensional input is compressed into a two-dimensional latent space by the encoder and then reconstructed by the decoder. The encoder (Eij) and decoder (Dji) are represented as trainable network ans\u00e4tze. Similar to its classical counterpart, the QAE comprises an encoder (E) and a decoder (D) separated by an infor- mation bottleneck. In this case, however, the encoder and decoder are implemented as sets of unitary trans- formations, i.e., quantum gates, that transform the in- put into a quantum state. Unlike classical autoencoders, where parts of the feature space can be discarded, uni- tary transformations preserve the entire Hilbert space. To address this, certain states are traced out to achieve the desired dimensionality reduction. The schematic rep- resentation of this quantum circuit is shown in the bot- tom panel of Fig. 1, where x denotes the inputs, and \u0398 represents the trainable parameters of the unitary trans- formations. A mid-circuit measurement has represented the dimensionality reduction. A more efficient quantum circuit can be implemented to optimise the training process. The transformation into a latent space and subsequent reconstruction can be achieved by inverting the encoder unitary and replacing reference states with a set of trash states $| \\alpha \\rangle = |a \\rangle$ , typically initialised to $|0\\rangle^{\\otimes \\text{dim}[H_A]} = |0\\rangle$. This circuit is depicted in Fig. 2. To optimise the unitary transforma- tion, the goal is to minimise the difference between the input state $| \\psi \\rangle = |\\psi \\rangle_{H_A} |\\psi \\rangle_{H_B}$ and the output state $\\rho_{\\text{out}} = \\rho_H$, which can be done by measuring the fi- delity between these two states, $||\\langle \\psi | \\rho_{\\text{out}} | \\psi \\rangle||^2$. Notably, since the decoder is the inverse of the encoder, the cir-"}, {"title": "III. DATA EMBEDDING AND THE CHOICE OF ANSATZ", "content": "Many different data embedding strategies have been developed for quantum machine learning applications. Two main options are amplitude embedding and angle embedding. In this study, we focus on the latter, but it should be noted that the former is more qubit efficient, but it has been suggested that amplitude embedding may be more prone to barren plateaus [22, 23], which is out of scope of this study. Angle embedding rotates each qubit by an angle de- fined by the input data, effectively mapping real data on a quantum state, $R_P(x_i)|0\\rangle$, where P is a Pauli operator (P\u2208 {X, Y, Z}). The $R_P$ operation for a set of features, $x_i \\in x$, is expressed as:\n\n$S_{st}(x) = \\bigotimes_{i=0}^{n} e^{-ix_i P(i)}$,\n\nwhere $\\bigotimes$ denotes the Kronecker product and the subscript (i) indicates the qubit on which the operation is applied. In this study, we will explore the effects of expanding this embedding. Our previous work [24] demonstrated that embedding data within a multidimensional hypersphere can enhance the representability of the ansatz, a strategy we will also explore here. We will test this with two embedding methods. First, we introduce parallel embedding,\n\n$S_P(x) = \\bigotimes_{i=0}^{n} (e^{-ix_i Y(2i)} e^{-ix_i Y(2i+1)})$,\n\nwhere P is fixed to Y (though any Pauli operator can be used). Here, a single feature is embedded across two ad- jacent qubits, representing the number of features with n. Due to computational constraints, we limit this im- plementation to two qubits, but it can be extended to more. The second method is parallel and alternate embed- ding,\n\n$S_A(x) = \\bigotimes_{i=0}^{n} (e^{-ix_i Y(2i)} e^{-ix_i X(2i+1)})$,\n\nwhere the second Pauli operator is switched to X. These methods aim to enhance data representation on a higher- dimensional manifold. A generalised version of these can be formulated as follows;\n\n$S_G(x) = \\bigotimes_{i=0}^{n} \\bigotimes_{j=0}^{d} e^{-ix_i P(di+j)}$,\n\nwhere d is the dimensionality of the embedding and Pauli operator P can be cycled between X, Y and Z. This indicates that the number of qubits required for this em- bedding is d x n while the depth of the quantum circuit remains the same as Eq. (3). We define the parameterised unitary ansatz to form the encoder following data embedding. In this study, we use strongly entangling layers [25], with each layer denoted as \u00db(\u0398), where i is the layer index and O\u00bf represents the parameters of that layer. Unless otherwise specified, these layers consist of parameterised rotation gates, typ- ically one Ry (\u03b8\u2208i) per qubit. The rotation gates are followed by CNOT gates, which entangle each qubit with every other qubit. Finally, we construct the encoder by combining the pa- rameterised unitary with data embedding. We will use two approaches. The first, a standard method employed in many studies, applies data embedding followed by pa- rameterised unitaries:\n\n$E_{st}(x, \\Theta) = S(x) \\bigotimes_{i=0}^{L} [ \\hat{U}(\\Theta_i)]$.\n\nHere, L represents the number of layers. The second approach uses data reuploading [19, 20], which encodes data repeatedly in every layer, enhanc- ing representation. In this method, data is repeatedly"}, {"title": "IV. RESULTS", "content": "In this section, we present our results for two groups of datasets. In section IV A, we discuss the results for 2D datasets, which allows for the visualisation of the meth- ods used. In section IVB, we apply our approach to a higher-dimensional dataset to demonstrate that our find- ings hold in higher dimensions. We fixed a certain set of hyperparameters for all the results presented below. Each model was trained for 500 epochs with a batch size of 100. We used ADAM opti- miser [26] with an initial learning rate of 0.1. We ap- plied an exponential decay scheduler, which reduced the learning rate every 100 epochs with a decay rate of 0.5. Trainable parameters were initialized using a uniform dis- tribution between [-\u03c0,\u03c0]. If overtraining occurred, we stopped the training and used the parameters from the point before overtraining began. The number of samples in the training, validation, and test sets for each dataset is provided in Table I. For the simulations, we used PEN- NYLANE (version 0.35.1) [27] along with JAX (version 0.4.30) [28] and CUDA (version 12.5) [29]."}, {"title": "A. Low dimensional datasets", "content": "To visually examine the effects of data embedding in anomaly detection, we first utilised standard 2D datasets from SCIPY (version 1.10.1) [30]. The sample sizes for each dataset are presented in the upper section of Table I. We employed nine benchmark models to investigate the impact of different data embedding techniques. The properties of these models, along with the area under the Receiver Operating Characteristic (ROC) curve (denoted as AUC) for each dataset, are detailed in Table II. The table is organised as follows: the first column (labelled R) indicates whether the model uses the standard encoder (Eq. (7)) or the data-reuploading encoder (Eq. (8)). The subsequent columns specify whether parallel (P) or alter- nate (A) embedding was applied. The number of layers used (4, 6, or 8) is listed in the column labelled L. The composition (comp.) column details the set of rotation gates used in the strongly entangling layers, \u00db(\u0398), where Y represents Ry (0), and YXY denotes a composition of Ry (01) Rx (02) Ry (03) for each qubit. In addition to AUC, we used accuracy values computed at 60% and 80% true positive rate (TPR) (or signal efficiency), presented in Table III for each model and dataset. The selected results are presented in Fig. 4. The re- sults for moons, s-curve, circle, and doughnut datasets are shown from left to right. Only models 1,5 and 9 are selected to be shown in Fig. 4, which are positioned from top to bottom. Decision boundaries are presented using confidence intervals at 80%, 90%, 95%, and 98%, which are shown with blue, green, orange and dark red, respectively. These intervals were computed using a x2 distribution,\n\n$\\chi^2(x) = -2 \\text{log} \\frac{p(x, \\Theta)}{p(x, \\hat{\\Theta})}$,\n\nwhere $\\hat{\\Theta}$ represents the grid points that maximize the probability within the region shown in each plot for a fixed set of parameters \u0398. The x2 values were calculated using grid data generated within the region presented in each plot, independent of the actual data. Each model was trained on the data represented by red dots, and AUC (shown in Table II) and accuracy (shown in Ta- ble III) values were computed based on the data shown"}, {"title": "B. High dimensional datasets", "content": "To evaluate the effectiveness of various data embed- ding approaches in a general context, we used Kaggle's credit card fraud dataset. The data was standardized us- ing Scipy's MinMaxScaler, scaling it to the range [-\u03c0, \u03c0]. The 492 fraudulent cases were isolated from the rest of the dataset and reserved exclusively for testing. We fo-"}, {"title": "V. SUMMARY & CONCLUSION", "content": "In this study, we investigated the impact of differ- ent data embedding techniques on enhancing the repre- sentability of a quantum autoencoder for anomaly de- tection. We focused on three distinct embedding ap- proaches: data reuploading, parallel embedding, and alternate embedding, comparing their effects against a"}]}