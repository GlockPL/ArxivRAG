{"title": "LP Data Pipeline: Lightweight, Purpose-driven Data Pipeline for Large Language Models", "authors": ["Yungi Kim", "Hyunsoo Ha", "Seonghoon Yang", "Sukyung Lee", "Jihoo Kim", "Chanjun Park"], "abstract": "Creating high-quality, large-scale datasets\nfor large language models (LLMs) often re-\nlies on resource-intensive, GPU-accelerated\nmodels for quality filtering, making the pro-\ncess time-consuming and costly. This depen-\ndence on GPUs limits accessibility for orga-\nnizations lacking significant computational in-\nfrastructure. To address this issue, we introduce\nthe Lightweight, Purpose-driven (LP) Data\nPipeline, a framework that operates entirely\non CPUs to streamline the processes of dataset\nextraction, filtering, and curation. Based on our\nfour core principles, the LP Data Pipeline sig-\nnificantly reduces preparation time and cost\nwhile maintaining high data quality. Impor-\ntantly, our pipeline enables the creation of\npurpose-driven datasets tailored to specific do-\nmains and languages, enhancing the applicabil-\nity of LLMs in specialized contexts. We antici-\npate that our pipeline will lower the barriers to\nLLM development, enabling a wide range of\norganizations to access LLMs more easily.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of large language models\n(LLMs) has significantly increased the demand for\nmassive, high-quality datasets for effective train-\ning (Zhao et al., 2023; Liu et al., 2024). While tradi-\ntional data processing and filtering methods have re-\nlied on CPU-based techniques (Wenzek et al., 2019;\nPenedo et al., 2023; Gunasekar et al., 2023; Li et al.,\n2023), there is a growing trend toward utilizing\nGPU-accelerated models to extract higher-quality\ndata suitable for advanced LLM training (Penedo\net al., 2024; Li et al., 2024; Sachdeva et al., 2024).\nA prominent example employing GPU-\nbased models for quality filtering is FineWeb-\nedu (Penedo et al., 2024). While effective for\nenhancing LLM performance, this approach\nrenders the data curation process both time-\nconsuming and costly. Consequently, organizations\nwith limited computing resources face significant\nbarriers to developing LLMs, restricting such ad-\nvancements to well-funded entities. This scenario\nhighlights an urgent need for effective and scalable\nsolutions capable of extracting high-quality data\nwithout heavy reliance on GPUs.\nMeanwhile, in real-world applications, there is\nan escalating demand for purpose-driven LLMs\ntailored to specific domains and languages (Ling\net al., 2023; Zheng et al., 2023; Zhao et al., 2024;\nQin et al., 2024). To this end, many organizations\nrequire specialized datasets for particular indus-\ntries, such as finance (Lee et al., 2024), law (Lai\net al., 2024), and healthcare (He et al., 2023), or for\nunderrepresented languages like Thai (Kim et al.,\n2024a), Vietnamese (Tran and Thanh, 2024), and\nArabic (Huang et al., 2023).\nTo meet these needs, we introduce the\nLightweight, Purpose-driven (LP) Data Pipeline.\nThis pipeline is a fully CPU-based framework de-\nsigned to streamline the extraction, filtering, and\ncuration of large-scale datasets tailored for specific\ndomains and languages. By developing and em-\nploying a lightweight yet effective model for qual-\nity filtering, it eliminates the dependency on GPU-\naccelerated models without compromising data\nquality. Furthermore, by leveraging FastText (Bo-\njanowski et al., 2017) for domain classification, LP\nData Pipeline facilitates the creation of specialized\ndatasets that meet the unique requirements of dif-\nferent LLM applications.\nWe conduct empirical studies on the LP Data\nPipeline to demonstrate the time and cost required\nto process a single CommonCrawl dump, and fur-\nther conduct scalability experiments on large-scale\ndata dumps to showcase the pipeline's capabil-\nity. Furthermore, by automating curation processes"}, {"title": "2 Related Work", "content": "The rapid advancement of LLMs has led to\nan increased demand for extensive, high-quality\ndatasets (Wenzek et al., 2019; Penedo et al., 2023;\nGao et al., 2020). Prominent datasets such as\nThe Pile (Gao et al., 2020), C4 (Raffel et al.,\n2020), RedPajama (Computer, 2023), and SlimPa-\njama (Soboleva et al., 2023) have been instru-\nmental in providing vast amounts of textual\ndata from sources like CommonCrawl (Common-\nCrawl, 2024), Wikipedia (Foundation, 2024), and\nacademic publications (Clement et al., 2019).\nHowever, these datasets have the limitation of\ncontaining a significant amount of low-quality\ndata (Caswell et al., 2021; Dodge et al., 2021).\nTo address this limitation, Fineweb-edu (Penedo\net al., 2024) utilized extensive GPU resources\nfor quality filtering, demonstrating that training\nLLMs with this dataset improves their performance\ncompared to the previously mentioned datasets.\nWhile effective, this GPU-based method is time-\nconsuming and costly due to the high computa-\ntional demands involved in processing massive web\ncorpora (Thompson et al., 2020). This reliance on\nGPU resources poses challenges for organizations\nwith limited computational resources, restricting\ntheir ability to develop LLMs tailored to specific\ndomains or languages."}, {"title": "2.1 Web Data Curation for LLMs", "content": null}, {"title": "2.2 Quality Filtering Techniques", "content": "Data quality is critical for effective LLM training,\nas it directly impacts model performance and re-\nliability (Penedo et al., 2024; Du et al., 2024; Re-\njeleene et al., 2024). To remove low-quality con-\ntent, rule-based methods are utilized, relying on\nmetadata attributes such as text length, stop word\nfraction, and n-gram repetition (Computer, 2023).\nFurthermore, KenLM (Heafield, 2011) and Fast-\nText (Joulin et al., 2016) models are employed to\nfilter out low-content data that remains even after\nrule-based methods have been applied, all with-\nout imposing heavy computational requirements.\nHowever, we note that the studies employing these\nmodels for quality filtering simply used models al-\nready trained on Wikipedia data, without making\nany effort to enhance their performance (Megh-\nwal et al., 2020). Recently, there has been signifi-\ncant research on using GPU-based models to curate\nhigher-quality data (Penedo et al., 2024; Sachdeva\net al., 2024), which leads to increased processing\ntimes and costs. Meanwhile, in order to eliminate\nredundancy at the document level, deduplication\ntechniques such as MinHashLSH (Indyk and Mot-\nwani, 1998) are applied. However, these methods\nmay not adequately address intra-document redun-\ndancy (Khan et al., 2024)."}, {"title": "3 Lightweight, Purpose-driven (LP) Data\nPipeline", "content": null}, {"title": "3.1 Pipeline Philosophy", "content": "The LP Data Pipeline is founded on four core prin-\nciples designed to address the challenges of curat-\ning high-quality, large-scale datasets.\nFully CPU-Based Data Curation. Recent qual-\nity filtering often depends on GPU-intensive mod-\nels, which demand significant computational re-\nsources. The LP Data Pipeline mitigates this is-\nsue by utilizing optimized and efficient CPU-based\nmethods, achieving practical performance without\nthe need for expensive GPU infrastructure.\nOptimized Processing Order for Efficiency. To\nenhance resource efficiency, LP Data Pipeline em-\nploys a strategic sequence for computational tasks.\nInitial filtering and basic data cleansing are con-\nducted first, reserving more resource-intensive op-\nerations for later stages. This order allows only\nrelatively high-quality data to undergo advanced\nprocessing, thus reducing overall resource expendi-\nture and time.\nContinuous Knowledge Updating. The LP Data\nPipeline includes automated mechanisms for con-\ntinuous knowledge updating. By detecting and pro-\ncessing regular data dumps from sources such as\nWikipedia and CommonCrawl, the pipeline updates\nits data in a timely manner without manual inter-\nvention. Workflow management tools like Airflow\nautomate job scheduling and repetitive tasks, en-\nsuring that LLMs trained on the pipeline's results\nalign with the latest available information."}, {"title": "Purpose-Driven Datset Construction.", "content": "The\npipeline supports the creation of datasets tailored\nto specific domains, such as finance, law, and\nmedicine. With advanced module for domain classi-\nfication, it extracts data that aligns with specialized\napplications. For example, datasets for legal anal-\nysis can be enriched with case law references and\nterminology. This tailored approach enhances the\nperformance of LLMs in real-world applications,\nproviding domain-specific expertise and nuanced\nunderstanding."}, {"title": "3.2 Curation Pipeline", "content": "The LP Data Pipeline is organized as a sequen-\ntial process optimized for efficient computation.\nEach stage incrementally refines the dataset, ensur-\ning that the resulting dataset is both high-quality\nand aligned with specific language and domain re-\nquirements. The following subsections provide a\ndetailed explanation of each phase. The overview\nof LP Data Pipeline is illustrated in Figure 1."}, {"title": "3.2.1 Raw Text Extraction and URL Filtering", "content": "The pipeline begins with the extraction of raw text\nfrom large-scale web data sources such as Com-\nmonCrawl. Utilizing efficient HTML parsing tools\nlike Resiliparse (Bevendorff et al., 2018), LP\nData Pipeline extracts textual content while remov-\ning boilerplate elements and irrelevant HTML tags.\nURL filtering is applied to exclude undesirable\nsources based on a predefined list of domains con-\ntaining harmful content (Christou et al., 2020)."}, {"title": "3.2.2 Language Identification", "content": "To generate language-specific data, the LP Data\nPipeline employs a CPU-optimized language iden-\ntification tool based on FastText (Joulin et al.,\n2016). This allows for accurate and efficient detec-\ntion of the target languages (e.g., English, Korean,\nJapanese, Thai), ensuring that only the relevant tex-\ntual data is retained for further processing."}, {"title": "3.2.3 Line-Level Deduplication with Domain\nGrouping", "content": "Redundant boilerplate text and recurring promo-\ntional phrases within documents can introduce\nnoise and diminish the quality of training data for\nLLMs. To this end, common approach involves par-\ntitioning all text lines from the entire documents\ninto random buckets and performing comprehen-\nsive line-level deduplication across all text lines\nwithin each bucket (Dubey et al., 2024). In this ap-\nproach, using fewer buckets leads to higher filtering\nperformance but incurs greater computational costs"}, {"title": "3.2.4 Heuristic Filtering", "content": "The LP Data Pipeline generates comprehensive\nmetadata for each document, including metrics\nsuch as text length, stop word ratio, and n-\ngram repetition. The selection of metadata fea-\ntures was guided by established methodologies\nfrom prior studies, including C4 (Raffel et al.,\n2020), Gopher (Rae et al., 2021), the Pretrainer's\nGuide (Longpre et al., 2023), RefinedWeb (Penedo\net al., 2023), and Fineweb (Penedo et al., 2024).\nFurthermore, to ensure uphold privacy standards,\nthe LP Data Pipeline generates metadata for Per-\nsonally Identifiable Information (PII).\nBased on this metadata, rule-based filtering is\nthen applied to exclude data that do not meet pre-\ndefined quality thresholds. The thresholds were de-\ntermined through qualitative assessments to ensure\nthe exclusion of substandard data. Given the exten-\nsive size and accessibility of web corpora, stringent\nthresholds were implemented to ensure a high stan-\ndard of data quality."}, {"title": "3.2.5 Global Deduplication", "content": "To mitigate redundancy within the dataset, the LP\nData Pipeline utilizes MinHash Locality-Sensitive\nHashing (LSH) (Indyk and Motwani, 1998) to\nachieve efficient document-level deduplication.\nThis technique is particularly suitable for deploy-\nment in cluster environments such as Apache\nSpark (Apache, 2024), enabling the effective iden-\ntification and removal of near-duplicate documents.\nThis global deduplication was conducted indepen-\ndently for each CommonCrawl dump."}, {"title": "3.2.6 Model-based Quality Filtering", "content": "Measuring data quality is essential in curating\ndatasets for training LLMs. While GPU-based re-\nward models can achieve strong performance, they\nsuffer from low throughput and incur substantial\ncosts when processing massive datasets like Com-\nmon Crawl, making them impractical. To facili-\ntate the curation of high-quality data using only\nCPU computation, we enhance the performance of\nthe FastText model by utilizing datasets generated\nthrough both Good KenLM and Bad KenLM (Kim\net al., 2024b)."}, {"title": "3.2.7 Domain Classification", "content": "Following the work of Cheng et al. (2024), train-\ning LLMs with domain-specific data has proven\nto substantially enhance their performance within\ntargeted domains. To curate such data, we devel-\noped a FastText (Joulin et al., 2016) model trained\nto classify documents into three industrial domains:\nfinance, law, and healthcare."}, {"title": "4 Empirical Analysis", "content": null}, {"title": "4.1 Practical Feasibility of the LP Data\nPipeline", "content": "To validate the feasibility of the LP Data Pipeline\nwith large-scale datasets, we conducted experi-\nments using actual CommonCrawl dumps, focus-\ning on key metrics such as processing time and esti-\nmated cost. The results demonstrate the pipeline's\nefficiency and cost-effectiveness in handling large-\nscale datasets."}, {"title": "4.2 Quantity of Domain-Specific Datasets", "content": "Table 3 presents the results from processing the\nCC-MAIN-2024-10 dump into domain-specific\ndatasets for English and Korean. The Medical Cor-\npus included 5.02 million English documents and\n0.23 million Korean documents, amounting to 4.27\nbillion and 0.24 billion tokens, respectively. Simi-\nlarly, the Financial Corpus comprised 4.29 million\nEnglish and 0.19 million Korean documents, with\ncorresponding token counts of 3.99 billion and 0.20\nbillion, respectively. The Legal Corpus showed a\nsmaller scale, with 2.18 million English and 0.05\nmillion Korean documents, totaling 2.02 billion\nand 0.06 billion tokens, respectively. The Common\nCorpus, which does not correspond to the three\ncorpora mentioned above, contained the highest\nvolume, with 52.11 million English and 1.34 mil-\nlion Korean documents, contributing 46.35 billion\nand 1.73 billion tokens, respectively. These results\nare consistent with the expectation that legal docu-\nments are the least common type of content on the\nweb."}, {"title": "4.3 Scaling Dump Analysis", "content": "We expanded our analysis to include 10 Com-\nmonCrawl dumps, processed using the LP Data\nPipeline."}, {"title": "5 Conclusion", "content": "In this paper, we introduce the Lightweight,\nPurpose-driven (LP) Data Pipeline, a fully CPU-\nbased framework designed for efficient curation of\nhigh-quality datasets for LLM training. Based on\nour four core principles, the LP Data Pipeline sig-\nnificantly reduces computational costs while main-\ntaining data quality comparable to that of GPU-\nbased methods. Our empirical analysis demon-\nstrated that the LP Data Pipeline is highly cost-effective and time-efficient, and scales efficiently\nwith large datasets. For future work, we plan to conduct LLM training experiments with the curated\ndatasets to further validate the effectiveness of our\npipeline."}, {"title": "A System Architecture", "content": "Figure 4 presents the system architecture of the Lightweight, Purpose-driven (LP) Data Pipeline. The\noverall management of the pipeline is orchestrated by Airflow, which employs a cron job to periodically\ncheck for new data releases from sources such as Wikipedia, CommonCrawl, and ArXiv. Upon detecting\nnew data releases, Airflow initiates the execution of predefined Directed Acyclic Graphs (DAGs) that\nperform tasks including text extraction, deduplication, filtering, and domain classification. These processes\nare executed on a Spark cluster deployed within AWS EMR, which is configured to leverage Docker\nimages built via GitHub CI/CD pipelines and utilizes quality filtering and domain classification models\nstored in S3. This architectural design supports the continuous collection and processing of up-to-date\ndata with minimal human intervention, thereby automating the construction of train-ready, purpose-driven\ndatasets."}, {"title": "B Results of Line-Level Deduplication with Domain Grouping", "content": "Table 4 presents the top five lines removed from both English and Korean documents through line-level\ndeduplication. While domain grouping significantly reduces computational overhead, our findings confirm\nthat the method effectively removes boilerplate content as intended."}, {"title": "C Data Samples", "content": "This section presents representative data samples in both Korean and English, sourced from the CC-\nMAIN-2024-38 dump, covering the finance, law, and medical domains. Table 5 presents an English legal\ntext sample, while Table 6 provides a Korean legal text sample. Similarly, Table 7 presents an English\nmedical text sample, and Table 8 includes a Korean medical text sample. For the finance domain, Table 9\npresents an English sample, with Table 10 providing a Korean sample."}, {"title": "Legal English Sample", "content": "A Guide to International Law\nInternational law is the set of rules accepted by various countries, binding them in agreement\nto what policies they will all follow. International rules between countries will serve as a\nframework for the practice of international relations.\nUnlike most other areas of law, international law has no defined area of governing body.\nThe piecemeal collection of international law will encompass customs, agreements, treaties,\ntribunals, legal precedents from international courts and more.\nInternational law varies from state-based legal systems in that it is not applied on an individual\nbasis, but to countries as a whole. The first step to creating an international law is to obtain\njurisdiction over a country. The Geneva Convention is the main source of international laws,\nwhich are enforced by international courts. These laws will be treated like your traditional\nnational law when a country signs a specific treaty. The difficult aspect about this is that a\ncountry will need to consent to be governed by a law.\nAlthough there is no definitive governing body overseeing international law, the United Nations\nis the most widely recognized and influential international organization, with the International\nCourt of Justice being its judicial counterpart.\nPublic international laws are concerned with questions of rights between nations or nations and\nthe citizens of other nations. On the other hand, private international law deals with controversies\nbetween private persons or natural born citizens that have significant relationships to more than\none nation. Additionally, there are always the international business laws between companies\nthat do business in more than one country.\nInternational law will cover basic concepts that every national legal system will such as property,\ntorts, procedure and remedies. However, the main substantive law is: international economic\nlaw, security law, criminal law, environmental law, diplomatic law, humanitarian and human\nrights law.\nInternational laws cover a myriad of legal concepts coming from a number of sources. Under-standing what customs or national laws as well as what international laws will apply to your\nbusiness or even as an individual traveling is essential."}, {"title": "Legal Korean Sample", "content": "\uae30\uc18c\uc720\uc608\uc815\ubcf4 \uc54c\uc544\ubcf4\uae30\n\uae30\uc18c\uc720\uc608: \ub2e8\uc21c\ud558\uac8c \uc124\uba85\ud55c \ubc95\ub960 \uc6a9\uc5b4\n\ubc95\ub960 \uc6a9\uc5b4\ub294 \uc885\uc885 \ubcf5\uc7a1\ud558\uace0 \uc774\ud574\ud558\uae30 \uc5b4\ub835\uc2b5\ub2c8\ub2e4. \uadf8 \uc911 \ud558\ub098\uc778 \u2018\uae30\uc18c\uc720\uc608'\uc5d0 \ub300\ud574 \uc27d\uac8c \uc54c\n\uc544\ubd05\uc2dc\ub2e4.\n\uae30\uc18c\uc720\uc608\ub780 \ubb34\uc5c7\uc778\uac00?\n\uae30\uc18c\uc720\uc608\ub294 \uac80\ucc30\uc774 \ud53c\uc758\uc790\ub97c \uae30\uc18c\ud560 \uc218 \uc788\ub294 \uad8c\ub9ac\ub97c \uc77c\uc815 \uae30\uac04 \ub3d9\uc548 \uc720\uc608\ud558\ub294 \uac83\uc744 \ub9d0\ud569\ub2c8\n\ub2e4. \uc774\ub294 \ud53c\uc758\uc790\uac00 \ucc98\uc74c \ubc94\uc8c4\ub97c \uc800\uc9c0\ub978 \uacbd\uc6b0\ub098 \uacbd\ubbf8\ud55c \ubc94\uc8c4\uc5d0 \ub300\ud574 \uc801\uc6a9\ub418\uba70, \uc774 \uae30\uac04 \ub3d9\uc548\n\ud53c\uc758\uc790\uac00 \ub2e4\uc2dc \ubc94\uc8c4\ub97c \uc800\uc9c0\ub974\uc9c0 \uc54a\ub294\ub2e4\uba74 \uae30\uc18c\uad8c\uc774 \uc18c\uba78\ub418\uc5b4 \ucc98\ubc8c\ubc1b\uc9c0 \uc54a\uac8c \ub429\ub2c8\ub2e4.\n\uae30\uc18c\uc720\uc608\uc758 \uc7a5\uc810\n\uae30\uc18c\uc720\uc608\ub294 \ud53c\uc758\uc790\uc5d0\uac8c \ub450 \ubc88\uc9f8 \uae30\ud68c\ub97c \uc904 \uc218 \uc788\ub294 \uc88b\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4. \ud53c\uc758\uc790\uac00 \ubc94\uc8c4\ub97c \uc800\uc9c0\n\ub974\uc9c0 \uc54a\uace0 \uc0ac\ud68c\uc5d0 \uc798 \uc801\uc751\ud558\uba74, \uadf8\ub4e4\uc5d0\uac8c \ubd88\uc774\uc775\uc774 \uac00\ud574\uc9c0\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubc94\uc8c4\n\uc7ac\ubc1c\ub960\uc744 \uc904\uc774\ub294 \ub370\uc5d0\ub3c4 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4.\n\uae30\uc18c\uc720\uc608\uc758 \ub2e8\uc810\n\uadf8\ub7ec\ub098 \uae30\uc18c\uc720\uc608\ub294 \ubc94\uc8c4\ub97c \uc800\uc9c0\ub978 \uc0ac\ub78c\uc774 \ucc98\ubc8c\uc744 \ud53c\ud558\uac8c \ub9cc\ub4e4 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud53c\ud574\uc790\n\ub098 \uc0ac\ud68c\uc5d0 \ub300\ud55c \ubd80\uc815\uc801\uc778 \uba54\uc2dc\uc9c0\ub97c \ubcf4\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub530\ub77c\uc11c \uac80\ucc30\uc740 \uae30\uc18c\uc720\uc608\ub97c \uacb0\uc815\ud560 \ub54c\n\uc2e0\uc911\ud574\uc57c \ud569\ub2c8\ub2e4."}, {"title": "Medical English Sample", "content": "Otosclerosis\nOtosclerosis is an abnormal growth of bone in the middle ear that causes hearing loss. It\ntypically begins in the early 20s, and is the leading cause of middle ear hearing loss in young\nadults.\nThe exact cause of otosclerosis is not known, but evidence suggests a genetic link passed down\nfrom parent to child. Middle-aged Caucasian women are most at risk, and pregnancy seems to\nbe a contributing factor, perhaps due to hormonal changes a woman is undergoing at the time.\nThis bone growth usually occurs around the stapes bone in the middle ear, preventing it from\nmoving freely, essential to proper hearing.\nGradually worsening hearing loss is the primary symptom of otosclerosis. It may begin with\nan inability to hear low-pitched sounds or whispers. Other symptoms may include vertigo or\ndizziness and tinnitus (ringing in the ears).\nTreatments\nThe symptoms of otosclerosis are like those of other conditions, so a thorough examination\nby an otolaryngologist is essential in ruling out other problems and diagnosing the disease. A\nhearing test will usually show signs of conductive hearing loss in the lower frequency tones, a\nhallmark of otosclerosis.\nMild cases of otosclerosis can be corrected with a hearing aid designed to amplify sounds.\nOrally ingested sodium fluoride has been shown to slow the progression of the disease, and\nmay be an option.\nIn more advanced cases, a surgical procedure known as a stapedectomy is often performed. In\nthis surgery, part or all of the affected stapes bone is removed and replaced with a prosthetic\ndevice that enables the bones of the middle ear to resume movement, allowing sound waves to\nreach the inner ear, improving or restoring hearing.\nThere are inherent risks in any surgery, but left untreated, otosclerosis will only get worse.\nSpeak to your doctor about the best treatment options for your hearing loss."}, {"title": "Medical Korean Sample", "content": "\ud608\uc18c\ud310 \uc774\uc0c1\uc740 \ud608\uc18c\ud310\uc5d0 \uc774\uc0c1\uc774 \uc0dd\uae30\ub294 \uc9c8\ud658\uc744 \ub9d0\ud569\ub2c8\ub2e4. \ud608\uc18c\ud310\uc740 \uc0c1\ucc98\uac00 \uc0dd\uacbc\uc744 \ub54c \ud608\n\uc561\uc744 \uba4e\uac8c \ud574\uc8fc\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. \ud608\uc18c\ud310\uc740 \uc190\uc0c1\ub41c \ud608\uad00\ubcbd\uc5d0 \ubd99\uac70\ub098 \ud608\uc18c\ud310\ub07c\ub9ac \uc11c\ub85c \uc5c9\uaca8\n\ubd99\uc73c\uba74\uc11c \ud608\uc561 \uc751\uace0\ub97c \uc77c\uc73c\ucf1c \ud608\uc561\uc744 \ub9de\ub3c4\ub85d \ud574\uc90d\ub2c8\ub2e4. \uc131\uc778\uc758 \uacbd\uc6b0 \ud608\uc561 1\ub9c8\uc774\ud06c\ub85c \ub9ac\ud130\n\uc548\uc5d0 \uc57d 15 40\ub9cc \uac1c\uc758 \ud608\uc18c\ud310\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud608\uc18c\ud310\uc740 \ud608\uad6c \uc911\uc5d0\uc11c \ud06c\uae30\uac00 \uac00\uc7a5 \uc791\uc73c\uba70, \uace8\uc218\n\uc5d0\uc11c \ub9cc\ub4e4\uc5b4\uc9d1\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ud608\uc18c\ud310\uc758 \uc218\ub098 \uae30\ub2a5\uc5d0 \uc774\uc0c1\uc774 \uc0dd\uae30\uba74 \uc9c0\ud608 \uc791\uc6a9\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc5b4\n\ucd9c\ud608\uc774 \uc0dd\uae38 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ud608\uc18c\ud310 \uc774\uc0c1\uc99d\uc5d0\ub294 \ud608\uc18c\ud310 \uc790\uccb4\uac00 \uc774\uc0c1\ud55c \uacbd\uc6b0, \ud608\uc18c\ud310\uc774 \uc644\uc804\ud55c \uae30\ub2a5\uc744 \ubc1c\ud718\ud558\uae30 \uc704\ud574 \ud544\n\uc694\ud55c \ud608\uc7a5 \uc131\ubd84\uc5d0 \uc774\uc0c1\uc774 \uc788\ub294 \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\ud608\uc18c\ud310 \uae30\ub2a5 \uc774\uc0c1\uc740 \ud6c4\ucc9c\uc131 \uae30\ub2a5 \uc774\uc0c1\uacfc \uc120\ucc9c\uc131 \uae30\ub2a5 \uc774\uc0c1\uc73c\ub85c \ub098\ub20c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud6c4\ucc9c\uc131\n\ud608\uc18c\ud310 \uae30\ub2a5 \uc774\uc0c1\uc5d0\ub294 \ub9ce\uc740 \uc57d\ubb3c\uc5d0 \uc758\ud55c \uacbd\uc6b0\uc640 \uc2e0\uc7a5 \ubd80\uc804, \uac04 \ubd80\uc804, \ub2e4\ubc1c\uc131 \uace8\uc218\uc885 \ub4f1\uc758\n\uc9c8\ud658\uc5d0 \uc758\ud55c \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\ud608\uc18c\ud310 \uae30\ub2a5 \uc774\uc0c1\uc744 \uc57c\uae30\ud558\ub294 \uc57d\ubb3c\ub85c\ub294 \ube44\uc2a4\ud14c\ub85c\uc774\ub4dc\uc131 \uc18c\uc5fc\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \ub3d9\ub9e5 \uacbd\ud654\ub098\n\uc2ec\uc7a5\ubcd1\uc744 \uc608\ubc29\ud55c\ub2e4\uace0 \uc54c\ub824\uc838 \uc624\ub7ab\ub3d9\uc548 \uc778\uae30\ub97c \ub204\ub838\ub358 \uc544\uc2a4\ud53c\ub9b0\uc740 \ud754\ud558\uc9c0\ub294 \uc54a\uc9c0\ub9cc \uc801\uc740\n\uc6a9\ub7c9\uc758 \ubcf5\uc6a9\uc5d0\uc11c\ub3c4 \ub2e4\ub7c9\uc758 \ucd9c\ud608\uc774 \uc57c\uae30\ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ud608\uc18c\ud310 \uc218\uce58\uac00 \uc99d\uac00\ud558\ub294 \uacbd\uc6b0\ub294 \uac10\uc5fc, \uc218\uc220, \uc5fc\uc99d, \uc57d\ubb3c \ub4f1 \uc5ec\ub7ec \uc2e0\uccb4\uc5d0 \ub300\ud55c \uc790\uadf9\uc5d0 \uc758\ud574\n\ubc18\uc751\uc131\uc73c\ub85c \uc99d\uac00\ud558\ub294 2\ucc28\uc131 \ud608\uc18c\ud310 \uc99d\uac00\uc99d\uc774 \uc788\uace0, \uc774\ub7ec\ud55c \uc6d0\uc778 \uc5c6\uc774 \uace8\uc218 \ub0b4\uc5d0\uc11c \ud608\uc18c\ud310\n\uc0dd\uc131\uc774 \uc790\ubc1c\uc801\uc73c\ub85c \uc99d\uac00\ud558\ub294 \ud2b9\ubc1c\uc131 \ud608\uc18c\ud310 \uc99d\uac00\uc99d\uc774 \uc788\uc2b5\ub2c8\ub2e4. \ud2b9\ubc1c\uc131 \ud608\uc18c\ud310 \uc99d\uac00\uc99d\uc758\n\uc8fc\ub41c \uc6d0\uc778\uc740 \ud608\uc18c\ud310 \uc0dd\uc131\uc5d0 \uad00\uc5ec\ud558\ub294 \uc2e0\uccb4\uc758 \uc2e0\ud638 \uc804\ub2ec \uacfc\uc815\uc5d0 \uc774\uc0c1\uc774 \uc0dd\uaca8\uc11c \ud608\uc18c\ud310\uc744 \ub9cc\n\ub4e4\ub77c\ub294 \uba85\ub839\uc744 \uacc4\uc18d \ub0b4\ub9ac\ub294 \uac83\uc785\ub2c8\ub2e4.\n\ud608\uc18c\ud310 \uc774\uc0c1\uc5d0 \uc758\ud55c \uc99d\uc0c1\uc740 \uc9c0\ud608 \uc791\uc6a9\uc774 \uc77c\uc5b4\ub098\uc9c0 \uc54a\ub294\ub2e4\ub294 \uc810\uc785\ub2c8\ub2e4. \ucd9c\ud608 \uc815\ub3c4\ub294 \ub2e4\uc591\n\ud569\ub2c8\ub2e4. \ucd9c\ud608 \uacbd\ud5a5\uc774 \ub098\ud0c0\ub098\ub294 \uacbd\uc6b0, \uc99d\uc138 \uc5c6\uc774 \ucd9c\ud608\uc774 \uc77c\uc5b4\ub098\ub294 \uacbd\uc6b0, \uc678\uc0c1, \uc218\uc220, \ucd9c\uc0b0 \uc2dc\n\ucd9c\ud608\uc774 \ubc1c\uc0dd\ud558\uc5ec \ubc1c\uacac\ub418\ub294 \uacbd\uc6b0 \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\ud55c\ud3b8, \ud608\uc18c\ud310\uc774 \ud544\uc694 \uc774\uc0c1\uc73c\ub85c \uc99d\uac00\ud558\ub294 \uacbd\uc6b0\ub3c4 \ubc1c\uc0dd\ud569\ub2c8\ub2e4. \ud2b9\ubc1c\uc131\uc73c\ub85c \ud608\uc18c\ud310\uc774 \uc99d\uac00\ud558\n\ub294 \uacbd\uc6b0\uc5d0\ub294 \ud608\uc804\uc99d, \uc0c9\uc804\uc99d\uacfc \uac19\uc740 \uce58\uba85\uc801\uc778 \ud569\ubcd1\uc99d\uc744 \uc77c\uc73c\ud0a4\uae30\ub3c4 \ud569\ub2c8\ub2e4."}, {"title": "Financial English Sample", "content": "Revenue or Total Income to Profit conversion\nOne of the most useful tools for analysing the financial performance of a company is the\nRevenue or Interest Income to Profit waterfall conversion. This is a method that shows how\na company makes money by graphically summarising the financial statements. It helps us to\nunderstand how a company makes money from its core business activities and how much it\nspends on various expenses.\nTradingView solution makes it possible to use this method for two types of companies: corporate\nentities and financial companies. Corporate entities are businesses that sell goods or services to\ncustomers and generate revenue from sales. Financial companies are businesses that provide\nfinancial services such as lending, investing, or insurance and generate interest income from\ntheir assets.\nFor corporate entities, along the way we get indicators such as the Gross Profit, Operating\nIncome, Net Income (and margins). This indicator helps investors easily understand how\neffective the business is currently, how much the company makes for each dollar of revenue\nor interest income, what it spends the most on, and where the most potential for business\nimprovement are.\nTo calculate the Net Profit of a financial company like a bank, we can use a similar method\nas for corporate entities, but with some differences. Financial companies generate the Interest\nIncome from their assets such as loans, securities, etc. and pay the Interest Expense on their\nliabilities such as deposits, borrowings, etc.. We start with the sum of Interest Income and\nNon-Interest Income (fees, commissions, trading, etc.) then deduct the following items: Interest\nExpense and Loan Provisions (the amount of money that the company sets aside to cover\npotential losses from bad loans or defaults), Non-Interest Expense, Unusual Expense and Net\nTaxation. The outcome is the Net Income of the financial company. Along the way we can also\narrive at the bank-specific intermediate stages such as the Net Interest Income After Loan Loss\nProvision, Bank Operating Income, and Pretax Income. This method helps us to examine the\nstep-by-step process from the Interest Income to Net Profit and to pinpoint the main factors of\nbank's non-treasury profitability.\nAs you can see from the Waterfall chart, each component of the income statement is shown as a\nbar that either adds to or subtracts from the previous bar. The final bar shows the net income\nor net profit of the company. This way, we can easily see how each component affects the\nprofitability of the company and compare the margins at each stage."}, {"title": "Financial Korean Sample", "content": "\uc790\ub3d9\ucc28 \ucde8\ub4f1\ub85d\uc138\uc5d0 \uad00\ud55c \uc774\ud574\uc640 \uacc4\uc0b0 \ubc29\ubc95\n\uc790\ub3d9\ucc28\ub97c \uad6c\uc785\ud558\uace0 \ub4f1\ub85d\ud558\ub824\uba74 \ucde8\ub4f1\ub85d\uc138\ub97c \ub0a9\ubd80\ud574\uc57c \ud569\ub2c8\ub2e4. \ucde8\ub4f1\ub85d\uc138\ub294 \uc790\ub3d9\ucc28\uc758 \uac00\uaca9\uc5d0\n\ube44\ub840\ud558\uc5ec \ubd80\uacfc\ub418\uba70, \uac01 \uc9c0\uc5ed\uc758 \uc138\uc728\uacfc \uc6b4\ud589 \ub144\uc218\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9d1\ub2c8\ub2e4. \uc774\ubc88 \uae00\uc5d0\uc11c\ub294 \uc790\ub3d9\ucc28\n\ucde8\ub4f1\ub85d\uc138\uc5d0 \ub300\ud574 \uc790\uc138\ud788 \uc54c\uc544\ubcf4\uace0, \uc5b4\ub5bb\uac8c \uacc4\uc0b0\ub418\ub294\uc9c0 \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\ucde8\ub4f1\ub85d\uc138\uc758 \uc885\ub958\n\uc790\ub3d9\ucc28 \ucde8\ub4f1\ub85d\uc138\ub294 \ud06c\uac8c \ucde8\ub4dd\uc138\uc640 \ub4f1\ub85d\uc138\ub85c \ub098\ub269\ub2c8\ub2e4. \ucde8\ub4dd\uc138\ub294 \ucc28\ub7c9\uc744 \uad6c\ub9e4\ud558\ub294 \uc21c\uac04\uc5d0\n\ubd80\uacfc\ub418\ub294 \uc138\uae08\uc73c\ub85c, \ucc28\ub7c9\uc758 \uac00\uaca9\uc5d0 \ube44\ub840\ud558\uc5ec \ubd80\uacfc\ub429\ub2c8\ub2e4. \ub4f1\ub85d\uc138\ub294 \ucde8\ub4dd\uc138 \uc774\ud6c4\uc5d0 \ub4f1\ub85d\ud558\n\ub294 \uacfc\uc815\uc5d0\uc11c \ubd80\uacfc\ub418\ub294 \uc138\uae08\uc73c\ub85c, \ucc28\ub7c9\uc758 \uc6b4\ud589 \ub144\uc218\uc5d0 \ub530\ub77c \ub2e4\ub974\uac8c \ubd80\uacfc\ub429\ub2c8\ub2e4.\n\ucde8\ub4dd\uc138 \uacc4\uc0b0 \ubc29\ubc95\n\ucde8\ub4dd\uc138\ub294 \ub300\ubd80\ubd84\uc758 \uad6d\uac00\uc5d0\uc11c \uacf5\uc2dd\uc801\uc778 \uacf5\uc2dd\uc744 \ud1b5\ud574 \uacc4\uc0b0\ub429\ub2c8\ub2e4. \ud55c\uad6d\uc758 \uacbd\uc6b0, \ucc28\ub7c9\uc758 \uac00\uaca9\n\uacfc \ubc30\uae30\ub7c9, \uc5f0\ub8cc \uc885\ub958\uc5d0 \ub530\ub77c \ucde8\ub4dd\uc138\uac00 \uacc4\uc0b0\ub429\ub2c8\ub2e4. \uac00\uaca9\uc740 \ucc28\ub7c9\uc758 \uc2e4\uc81c \uad6c\ub9e4 \uac00\uaca9\uc744 \uc758\ubbf8\n\ud558\uba70, \ubc30\uae30\ub7c9\uc740 \ucc28\ub7c9\uc758 \uc5d4\uc9c4 \uc6a9\ub7c9\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc5f0\ub8cc \uc885\ub958\ub294 \uc8fc\ub85c \uac00\uc194\ub9b0, \ub514\uc824, LPG \ub4f1\uc774\n\uc788\uc73c\uba70, \uc5f0\ub8cc \uc885\ub958\uc5d0 \ub530\ub77c \ucde8\ub4dd\uc138 \ube44\uc728\uc774 \ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uc694\uc18c\ub4e4\uc744 \uace0\ub824\ud558\uc5ec \ucde8\ub4dd\n\uc138\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\ub4f1\ub85d\uc138 \uacc4\uc0b0 \ubc29\ubc95\n\ub4f1\ub85d\uc138\ub294 \ucde8\ub4dd\uc138\uc640 \ub2ec\ub9ac \ucc28\ub7c9\uc758 \uc6b4\ud589 \ub144\uc218\uc640 \uc9c0\uc5ed\uc5d0 \ub530\ub77c \ub2ec\ub77c\uc9d1\ub2c8\ub2e4. \ub300\ubd80\ubd84\uc758 \uad6d\uac00\uc5d0\uc11c\ub294\n\ub4f1\ub85d\uc138\ub97c \uac04\ub2e8\ud55c \uacf5\uc2dd\uc744 \ud1b5\ud574 \uacc4\uc0b0\ud558\uba70, \uc6b4\ud589 \ub144\uc218\uc5d0 \ub530\ub77c \uc138\uc728\uc774 \uc810\ucc28 \uc99d\uac00\ud558\uac70\ub098 \uac10\uc18c\ud569\n\ub2c8\ub2e4. \uc9c0\uc5ed\uc5d0 \ub530\ub77c\uc11c\ub3c4 \uc138\uc728\uc774 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0, \ub3d9\uc77c\ud55c \ucc28\ub7c9\uc774\ub77c\ub3c4 \uc9c0\uc5ed\uc5d0 \ub530\ub77c \ub4f1\ub85d\uc138\uac00\n\ub2e4\ub97c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc790\ub3d9\ucc28 \ucde8\ub4f1\ub85d\uc138\ub294 \ucc28\ub7c9\uc744 \uad6c\uc785\ud558\uace0 \ub4f1\ub85d\ud558\uae30 \uc704\ud574 \ubd80\uacfc\ub418\ub294 \uc138\uae08\uc785\ub2c8\ub2e4. \ucde8\ub4dd\uc138\ub294 \ucc28\ub7c9\uc758\n\uac00\uaca9\uacfc \ubc30\uae30\ub7c9, \uc5f0\ub8cc \uc885\ub958\uc5d0 \ub530\ub77c \uacc4\uc0b0\ub418\uba70, \ub4f1\ub85d\uc138\ub294 \ucc28\ub7c9\uc758 \uc6b4\ud589 \ub144\uc218\uc640 \uc9c0\uc5ed\uc5d0 \ub530\ub77c \ub2ec\n\ub77c\uc9d1\ub2c8\ub2e4. \uc774\ub97c \uace0\ub824\ud558\uc5ec \uc790\ub3d9\ucc28 \ucde8\ub4f1\ub85d\uc138\ub97c \uc815\ud655\ud558\uac8c \uacc4\uc0b0\ud558\ub294 \uac83\uc774 \uc911\uc694\ud569\ub2c8\ub2e4. \uc790\ub3d9\ucc28\n\uad6c\ub9e4 \uc804\uc5d0 \ucde8\ub4dd\uc138\uc640 \ub4f1\ub85d\uc138\ub97c \ubbf8\ub9ac \uc608\uc0c1\ud558\uc5ec \uc608\uc0b0\uc744 \uc138\uc6b0\uace0, \uc790\uc138\ud788 \uc54c\uc544\ub450\uba74 \uc790\ub3d9\ucc28 \uad6c\uc785\n\uacfc\uc815\uc5d0\uc11c \ub3c4\uc6c0\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4."}]}