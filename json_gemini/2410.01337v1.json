{"title": "PHYMPGN: PHYSICS-ENCODED MESSAGE PASSING GRAPH NETWORK FOR SPATIOTEMPORAL PDE SYSTEMS", "authors": ["Bocheng Zeng", "Qi Wang", "Mengtao Yan", "Yang Liu", "Ruizhi Chengze", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Hao Sun"], "abstract": "Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.", "sections": [{"title": "1 INTRODUCTION", "content": "Complex dynamical systems governed by partial differential equations (PDEs) exist in a wide variety of disciplines, such as computational fluid dynamics, weather prediction, chemical reaction simulation, quantum mechanisms, etc. Solving PDEs is of great significance for understanding and further discovering the underlying physical laws.\nMany traditional numerical methods have been developed to solve PDEs (Ames, 2014; Anderson, 1995), such as finite difference methods, finite element methods, finite volume methods, spectral methods, etc. However, to achieve the targeted accuracy, the computational cost of these methods for large simulations is prohibitively high, since fine meshes and small time stepping are required. In recent years, deep learning methods have achieved great success in various domains such as image recognition (He et al., 2016), natural language processing (Vaswani et al., 2017) and information retrieval (Devlin et al., 2019). A increasing number of neural-based methods have been proposed to learn the solution of PDES.\nPhysical-informed neural networks (PINNs) (Raissi et al., 2019) and its variants have shown promise performance in modeling various physical systems, such as fluid dynamics (Gu et al., 2024; Raissi"}, {"title": "2 RELATED WORK", "content": "Traditional numerical methods solve PDEs via spatiotemporal discretization and approximation. However, these methods require fine meshes, small time stepping, and fully known PDEs with predefined initial conditions (ICs) and BCs. Recently, given the advance of deep learning, numerous data-driven neural-based models have been introduced to learn PDE systems with speedup inference.\nPhysics-informed learning: PINNs (Raissi et al., 2019) pioneer the foundation for the paradigm of physics-informed learning, where automatic differentiation are used to determine the PDE residual as soft regularizer in the loss function. Further, several variants (Yu et al., 2022; Eshkofti &\nHosseini, 2023; Miao & Li, 2023) have been proposed to improve their accuracy and enhance the capability to handle complex geometries. In addition to automatic differentiation, other methods can be employed to compute the PDE residual, such as finite difference (Ren et al., 2022; Rao et al., 2023), finite element (Rezaei et al., 2024; Sahli Costabal et al., 2024), and finite volume methods (Li et al., 2024a;b). The family of PINN methods (Raissi, 2018; Seo & Liu, 2019; Yang & Foster, 2022; Yuan et al., 2022) perform well in solving various forward and inverse problems given limited training data or even no labeled data. However, the explicit formulation of PDEs needs to be supplied.\nNeural Operators learning: Neural operators learn a mapping between two function spaces from finite input-output data pairs, such as the ICs, BCs, and PDE parameters. Finite-dimensional operator methods (Bhatnagar et al., 2019; KHOO et al., 2020; Zhu & Zabaras, 2018; Iakovlev et al., 2021) are mesh-dependent and cannot obtain solutions at unseen nodes in the geometry. DeepONet (Lu et al., 2021), as a general neural operator, has demonstrated with generalizability to low-dimensional"}, {"title": "3 METHODS", "content": "Let us consider complex physical systems, governed by spatiotemporal PDEs in the general form:\n$$u(x,t) = F(t, x, u, \\nabla u, \\Delta u, ...)$$\nwhere $u(x, t) \\in \\mathbb{R}^m$ is the vector of state variable with $m$ components we are interested in, such as velocity, temperature or pressure, defined over the spatiotemporal domain ${x,t} \\in \\Omega \\times [0, \\Tau]$.\nHere, $u$ denotes the derivative with respect to time and $F$ is a nonlinear operator that depends on the current state $u$ and its spatial derivatives.\nWe focus on a spatial domain $\\Omega$ with sparsely observed nodes ${X_0,...,X_{N-1}}$, non-uniformly placed in space (e.g., on an unstructured mesh), presenting a more challenging scenario compared with structured grids. Observations ${U(t_0),...,U(t_{r-1})}$ are collected at time points $t_0,..., t_{r-1}$, where $U(t_i) = {u(x_0, t_i), ..., u(x_{N-1},t_i)}$ denotes the physical quantities. Considering that many physical phenomena involve diffusion processes, we assume the diffusion term in the PDE is known as a priori knowledge. Our goal is to develop a graph learning model with small training datasets capable of accurately predicting various spatiotemporal dynamics on coarse unstructured meshes, handling different types of BCs, and producing the trajectory of dynamics for an arbitrarily given IC.\nBefore further discussion, we provide some notations. Let a graph $G = (V, E)$, with node $i \\in V$ denoting the observed node in the domain and undirected edge $(i, j) \\in E$ denoting the connection between two nodes. We apply Delaunay triangulation to the discrete nodes to construct the non-uniform mesh, which forms the edges of the graph."}, {"title": "3.2 MODEL ARCHITECTURE", "content": "According to the method of lines (MOL) to discretize the spatial derivatives in $F$ at these discrete nodes, Eq. 1 can be rewritten as a system of ordinary differential equations (ODEs) by numerical"}, {"title": "3.2.1 GNN BLOCK", "content": "We utilize a message-passing GNN (Gilmer et al., 2017) with the Encode-Process-Decode framework (Battaglia et al., 2018) as a pivotal component of our model, referred to as the GNN block. It comprises three modules (Figure 1b\u2013c): encoder, processor, and decoder.\nEncoder: Within the GNN block, there are two encoders (MLPs), one for computing node embed-dings and the other for edge embeddings. For each node $i$, the node encoder maps the state variable $u$, spatial coordinate $x_i$ and one-hot node type $C_i$ (normal, ghost, . . . ; details shown in Section 3.3)"}, {"title": "3.2.2 LAPLACE BLOCK", "content": "Motivated by PeRCNN (Rao et al., 2023), which uses a physics-based finite difference convolutional layer to encode prior physics knowledge of PDEs, we design a Laplace block to encode the discrete Laplace-Beltrami operators for the diffusion term $\\Delta u$ commonly seen in PDEs. This aids and guides the GNN learning in a physically feasible solution space.\nUsing the finite difference method to compute the Laplacian works well on regular grids, but it becomes ineffective on unstructured meshes. Therefore, we employ the discrete Laplace-Beltrami operators (Reuter et al., 2009) to compute the discrete geometric Laplacian on a non-uniform mesh domain (Pinkall & Polthier, 1996; Meyer et al., 2003), which are usually defined as\n$$\\Delta f_i = \\frac{1}{d_i} \\sum_{j \\in N_i} w_{ij} (f_i - f_j)$$\nwhere $N_i$ denotes the neighbors of node $i$ and $f_i$ denotes the value of a continuous function $f$ at node $i$. The weights read $w_{ij} = [cot(\\alpha_{ij})+cot(\\beta_{ij})]/2$ (Pinkall &\nPolthier, 1996), where $\\alpha_{ij}$ and $\\beta_{ij}$ are the two angles opposite to the edge $(i, j)$. The mass is defined as $d_i = a_V(i)$ (Meyer et al., 2003), where $a_V(i)$ denotes the area of the polygon formed by connecting the circumcenters of the triangles around node $i$ (see Figure 2; more details found in Appendix B).\nNote that Eq. 4, referred to as the Mesh Laplace module in Figure 1b, exhibits good accuracy on dense meshes but yields unsatisfactory results on coarse meshes. Therefore, we employ a lightweight neural network to rectify the Laplacian estimation on coarse meshes, which also leverages the Encode-Process-Decode framework. Consequently, Eq. 4 is updated as follows:\n$$\\Delta f_i = \\frac{1}{d_i}(z_i + \\sum_{j \\in N_i} w_{ij} (f_i - f_j))$$\nwhere $z_i$ is the output of the lightweight network, as shown in Figure 1b, where $MLP_A$ and $MLP_B$ are employed as the encoder and the decoder, respectively, and MPNNs serve as the processor"}, {"title": "3.3 ENCODING BOUNDARY CONDITIONS", "content": "Given the governing PDE, the solution depends on both ICs and BCs. Inspired by the padding methods for BCs on regular grids in PeRCNN (Rao et al., 2023), we propose a novel padding strategy to encode different types of BCs on irregular domains. Specifically, we perform BC padding in both the physical space and the latent space.\nBefore constructing the unstructured mesh by Delaunay triangulation, we first apply the padding strat-egy to the discrete nodes in the physical space (e.g., $u^k$), as shown in Figure 3. We consider four type of BCs: Dirichlet, Neumann,Robin, and Periodic. All nodes on the Dirichlet boundary will be padded with the specified values. For Neumann/Robin BCs, ghost nodes are created to be symmetric with the nodes nearby the boundary in the physical space, and their padded values are based on gradient information in the normal direction. For pe-riodic BCs, the nodes near the boundary are flipped to the other side, achieving a cyclic effect in message passing. Once the padding is completed, Delaunay triangulation is applied to construct the mesh, which serves as the graph input for the model. Apart from this, we also apply padding to the prediction (e.g., $u^{k+1}$) of the model at each time step to ensure that it satisfies the boundary conditions before being fed into the model for next-step prediction. As for the latent space (e.g.,$h^k$ in the GNN block), padding is also applied after each MPNN layer except the last layer. Detail formulas of encoding different types of BCs are provided in Appendix C."}, {"title": "3.4 NETWORK TRAINING", "content": "The pushforward trick and the use of training noise as demonstrated in previous approaches (Brand-stetter et al., 2022; Pfaff et al., 2021; Stachenfeld et al., 2022; Sanchez-Gonzalez et al., 2020) have effectively improved the stability and robustness of the model. We leverage these insights by syner-gistically integrating them to train our network for long-term predictions.\nDue to GPU memory limitations, directly feeding the entire long time series (T steps) into the model and performing backpropagation across all steps is impractical. To address this, we segment the time series into multiple shorter time sequences (M steps, $T \\gg M$). The model rolls out for M \u2013 1 times in each segment, but backpropagation is only applied to the first and last steps. Furthermore, we introduce a small noise to the first frame of each segment during training. Both of these techniques aim to alleviate model overfitting and mitigate error accumulation. Therefore, the loss function of each time segment is defined as\n$$\\mathcal{L} = MSE(u^1, \\hat{u}^1) + MSE (u^{M-1}, \\hat{u}^{M-1})$$\nwhere $u$ denotes the ground truth of state variable, $\\hat{u}$ denotes the prediction from our model, and the superscript of $u, \\hat{u}$ denotes their time steps in the segment."}, {"title": "4 EXPERIMENTS", "content": "Here, we present comprehensive evaluation of our methods. Firstly, we validate the feasibility and efficacy of the Laplace block to compute the Laplacian on coarse non-uniform meshes. Subse-quently, the entire model is trained and evaluated on various PDE datasets and compared with sev-eral baseline models. Finally, we perform ablation studies to demonstrate the positive contribution of each individual component within our model and the impact of numerical integrator. Our source code and data will be posted after peer review."}, {"title": "4.1 LAPLACE BLOCK VALIDATION", "content": "We create a synthetic dataset to validate the effectiveness of the Laplace block. First, we generate 10,000 samples of random continuous functions on a high-resolution (129 \u00d7 129) regular grid by cumulatively adding sine and cosine functions, satisfying periodic BCs. By employing the 5-point Laplacian operator of the finite difference method, the Laplacian of the functions at each node is computed as the ground truth. Then the dataset is down-sampled in space from the high-resolution regular grids to a set of coarse non-uniform mesh points yielding only 983 nodes. Note that 70% of the dataset is used as the training set, 20% as the validation set, and 10% as the test set. More details of the synthetic dataset are shown in Appendix D.1.\nWe compare the performances of four different methods: (1) Laplace block with BC padding; (2) the discrete Laplace-Beltrami operator in geometric mathematics with BC padding, referred to as Mesh Laplace; (3) the spatial difference layer for Laplacian, referred to as SDL (Seo* et al., 2020);\n(4) SDL with BC padding, referred to as SDL-padding. The MSE and RNE of all experiments are shown in Table 1. Mesh Laplace performs poorly compared to other learnable methods, while our Laplace block with the fewest parameters greatly outperforms other methods. SDL-padding exhibits an obvious performance improvement, indicating the effectiveness and importance of our padding strategy. A snapshot of a random function, ground truth of its Laplacian, and prediction are shown in Appendix Figure A.12."}, {"title": "4.2 MODELING PDE SYSTEMS", "content": "Solving PDEs serves as a cornerstone for modeling complex dynamical systems. However, in fields such as climate forecasting, reactions of new chemical matters, and social networks, the underlying governing PDEs are either completely unknown or only partially known. Hence, there is a great need for data-driven modeling and simulation. Based on the assumption, we train and evaluate our model on various physical systems, including Burgers' equation, FitzHugh-Nagumo (FN) equation, Gray-Scott (GS) equation, and flows past a cylinder, particularly in small training data regimes. We also compare our model against several representative baseline models, including DeepONet (Lu et al., 2021), PA-DGN (Seo* et al., 2020), MGN (Pfaff et al., 2021), and MP-PDE (Brandstetter et al., 2022). The numbers of learnable parameters and training iterations for all models are kept as consistent as possible to ensure a fair comparison. All the data are generated using COMSOL with"}, {"title": "5 CONCLUSION", "content": "We present a graph learning approach, namely, PhyMPGN, for prediction and simulation of spa-tiotemporal PDE systems on coarse unstructured meshes given small training datasets. Specifically, we develop a physics-encoded message-passing GNN model, where the temporal marching is re-alized via a second-order numerical integrator (e.g. Runge-Kutta scheme). The a priori physics knowledge is embedded to aid and guide the GNN learning in a physically feasible solution space with improved solution accuracy, via introducing (1) a learnable Laplace block that encodes the dis-crete Laplace-Beltrami operator, and (2) a novel padding strategy to encode different types of BCs. Extensive experiments demonstrate that PhyMPGN outperforms other baseline models with consid-erable margins, e.g., exceeding 50% gains across diverse spatiotemporal dynamics, given small and sparse training datasets. However, several challenges remain to be addressed: (1) how to effectively encode Neumann/Robin BCs in latent space; (2) extending the model to 3-dimensional scenarios; These challenges highlight key directions for our future research."}, {"title": "A NUMERICAL INTEGRATOR", "content": "Let us consider complex physical systems, governed by spatiotemporal PDEs of the general form:\n$$u(x,t) = F(t,x, u, \\nabla u, \\Delta u, ...)$$\nwhere $u(x, t) \\in \\mathbb{R}^m$ is the vector of state variable with $m$ components we are interested in, such as velocity, temperature or pressure, defined over the spatiotemporal domain ${x,t} \\in \\Omega \\times [0, \\Tau]$. Here, $u$ denotes the derivative with respect to time and $F$ is a nonlinear operator that depends on the current state $u$ and its spatial derivatives.\nAccording to the method of lines (MOL), Eq. A.8 can be rewritten as a system of ordinary differ-ential equations (ODEs) by numerical discretization. And the ODE at each node can be described by\n$$u(t) = u(0) + \\int_{0}^{t} \\hat{u}(\\tau) d\\tau$$\nNumerous ODE solvers can be applied to solve it, such as Euler forward scheme\n$$u^{k+1} = u^k + g_1. \\delta t$$\n$$g_1 = F(t, x, u^k, ...)$$\nwhere $u^k$ is the state variable at time $t^k$, and $\\delta t$ denotes the time interval between $t^k$ and $t^{k+1}$.\nWhile Euler forward scheme is a first-order precision method, other numerical methods with higher precision such as Runge-Kutta schemes can also be applied, offering a trade-off between computing resource and accuracy. And the second-order Runge-Kutta (RK2) scheme can be described by\n$$u^{k+1} = u^k + \\frac{1}{2} \\delta t(g_1 +g_2)$$\nwhere\n$$g_1 = F(t^k, x, u^k,...),$$\n$$g_2 = F(t^{k+1},x,u^k + \\delta t g_1, ...)$$\nand the fourth-order Runge-Kutta (RK4) scheme is as followed\n$$u^{k+1} = u^k + \\frac{1}{6} \\delta t(g_1 + 2g_2 + 2g_3 +g_4)$$\nwhere\n$$g_1 = F(t^k,x, u^k,...),$$\n$$g_2 = F(t^k + \\frac{1}{2} \\delta t, x, u^k + \\frac{\\delta t}{2} g_1,...)$$\n$$g_3 = F(t^k + \\frac{1}{2} \\delta t, x, u^k + \\frac{\\delta t}{2} g_2,...),$$\n$$g_4 = F(t^k + \\delta t, x, u^k + \\delta t g_3, ...)$$"}, {"title": "B DISCRETE LAPLACE-BELTRAMI OPERATORS", "content": "Using the finite difference method to compute the Laplacian works well on regular grids, but it becomes ineffective on unstructured meshes. Therefore, we employ the discrete Laplace-Beltrami operators to compute the discrete geometric Laplacian on non-uniform mesh domain, which are usually defined as\n$$\\Delta f_i = \\frac{1}{d_i} \\sum_{j \\in N_i} w_{ij} (f_i - f_j)$$\nwhere $N_i$ denotes the neighbors of node $i$ and $f_i$ denotes the value of a continuous function $f$ at node $i$. The weights can be described as\n$$w_{ij} = \\frac{cot(\\alpha_{ij}) + cot(\\beta_{ij})}{2}$$"}, {"title": "C ENCODING BOUNDARY CONDITIONS", "content": "Given the governing PDE, the solution depends on both ICs and BCs. Inspired by the padding methods for BCs on regular grids in PeRCNN, we propose a novel padding strategy to encode different types of BCs on irregular domains. Specifically, we perform BC padding in both the physical space and the latent space.\nBefore constructing the unstructured mesh by Delaunay triangulation, we first apply the padding strategy to the discrete nodes in the physical space (i.e., $u^k$), as shown in Figure A.10. We consider four types of BCs: Dirichlet, Neumann, Robin, and periodic. All nodes on the Dirichlet boundary will be padded with the specified values. For Neumann/Robin BCs, ghost nodes are created to be symmetric with the nodes nearby the boundary in the physical space, and their padded values are based on gradient information in the normal direction. For periodic BCs, the nodes near the bound-ary are flipped to the other side, achieving a cyclic effect in message passing (detailed formulations of various BCs are provided in Table A.3). Once the padding is completed, Delaunay triangulation is applied to construct the mesh, which serves as the graph input for the model. Apart from this, we also apply padding to the prediction (i.e., $u^{k+1}$) of the model at each time step to ensure that it satisfies the BCs before being fed into the model for next-step prediction, as shown in Figure A.11.\nAs for the latent space (i.e., $h^k$ in GNN block), padding is also applied after each MPNN layer except the last layer. For Dirichlet BCs, the embedding features of nodes on the boundary from the node encoder will be stored as the specified features for padding. For Neumann BCs with zero flux, ghost nodes are created to be symmetric with the nodes near the boundary in both physical and latent spaces. For other Neumann cases and Robin BCs, padding in the latent space remains an unsolved challenge, which is expected to be addressed in the future. And for periodic BCs, nodes near the boundary are flipped to the other side, along with their embedding features."}]}