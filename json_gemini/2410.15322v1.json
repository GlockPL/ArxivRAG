{"title": "FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion Model", "authors": ["Haoye Chai", "Shiyuan Zhang", "Xiaoqian QI", "Yong Li"], "abstract": "Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc., and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero/few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mobile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short/long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero/few-shot learning, showcasing a strong universality. We further deploy the FoMo on the JiuTian optimization platform of China Mobile, where we use the predicted mobile data to formulate network planning and optimization applications, including BS deployment, resource block scheduling, and BS sleep control.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, foundation models [6, 51, 62] have made substantial strides in natural language processing and computer vision. These models are reshaping the AI ecosystem by harnessing their powerful data processing, generalization, and zero/few-shot learning capabilities. An increasing number of specialized domains have developed foundational models tailored to their specific data and contextual demands, including healthcare, medicine, urban navigation, intelligent manufacturing, and beyond [4, 32, 83, 87]. Mobile networks and computing encompass massive amounts of mobile traffic, user, and geographical data, providing natural data support for foundation models. However, such dedicated foundation models for mobile network domains have yet to be established. We hence aim to construct a foundation model for mobile traffic forecasting, which can handle diverse features of large-scale mobile data while retaining the generalization needed across multiple cities and scenarios [11, 72, 85].\nConstructing such a foundation model of mobile traffic forecasting is vital for mobile networks [20, 71, 97]. On the one hand, mobile traffic forecasting offers great potential for network planning and optimization. It enables operators to anticipate traffic dynamics, facilitating proactive perceptions of resource utilization and service quality, and allowing for the preemptive development of optimization strategies. On the other hand, mobile networks encompass a variety of optimization tasks, including radio resource scheduling [40, 43], Base Station (BS) deployment [17, 19], and antenna configuration [38, 42], etc. These tasks involve diverse objectives like throughput, coverage, and energy efficiency, which impose distinct requirements on traffic forecasting. For example, radio resource scheduling prioritizes short-term traffic dynamics to improve user experience [13, 22], whereas BS deployment focuses on long-term traffic patterns within a region to match network demands [68, 96]. Regarding the differentiated forecasting needs of optimization tasks, current methods typically employ one-to-one approaches: designing customized models by leveraging task-specific data [29, 36, 80, 97], and focus on one single application scenario, making it challenging to generalize the models across different cities and tasks.\nOn the contrary, foundation models possess large-scale learnable parameters with powerful feature representation and generalization capabilities, enabling them to learn universal features from vast amounts of mobile traffic data [10, 37]. Moreover, it is endowed with robust few/zero-shot abilities to be applied across different cities and scenarios [7, 53]. By constructing foundation models for traffic forecasting, it becomes possible to achieve precise traffic predictions for various scenarios and cities via one unified model, thereby efficiently addressing the diverse tasks involved in mobile networks.\nHowever, designing the foundation model remains a significant challenge. Although numerous notable works have emerged in the area of mobile traffic forecasting [23, 33, 34, 48, 76, 91], the majority of these models are typically tailored to specific tasks that fall short of the universality expected from foundation models. Specifically, current mobile traffic forecasting models face three key limitations:\ni) Limited generalization. Mobile traffic data is inherently shaped by the spatio-temporal dynamics of population distribution and communication demands. Due to variations in geographic environments, lifestyle habits, and urban layouts across different cities, mobile traffic can differ significantly [74, 84]. With relatively small parameters, current models struggle to capture the diverse spatio-temporal patterns inherent in large-scale data across multiple cities. Additionally, it is challenging to encapsulate the complex correlations between contextual factors and mobile traffic, resulting in poor transferability in multi-city scenarios.\nii) Constrained adaptability to multiple scenarios. Mobile traffic forecasting is extensively applied across varying optimization scenarios. However, current models are often designed with specialized modules tailored to specific tasks. For instance, in short-term forecasting, models usually focus on capturing traffic fluctuations that employ autoregressive or event-driven methods. In contrast, long-term predictions emphasize the regular patterns of traffic and typically utilize time series decomposition techniques. These dedicated models increase design complexity and raise deployment costs when applied to diverse scenarios.\niii) Lack of universal representation of mobile traffic. Unlike standardized formats in language and vision domains, mobile traffic data is inherently heterogeneous of various collection granularity and scope. For example, Measure Report (MR) data primarily collects millisecond-level user traffic, while Performance Management (PM) data gathers cell-level traffic statistics over 15-minute intervals [45, 69], leading to the absence of a unified representation akin to that found in natural language. Consequently, it is challenging to directly apply pre-trained models from the natural language/visual domains to mobile traffic data. Although some efforts have been made to reprogram mobile traffic data into a natural language format [24, 31], this approach heavily relies on the quality of manually crafted prompts, making it difficult to capture a universal representation of mobile traffic.\nTo tackle the challenges, we propose a Foundation model for Mobile traffic forecasting (FoMo), which aims to learn universal features of mobile traffic data and to handle multitasks in mobile networks across multiple cities, thereby establishing a one-for-all forecasting model. First, inspired by Sora [5], FoMo adopts the transformer-based diffusion model as the backbone instead of the U-Net structure, to help the model understand the diverse features of massive mobile data. Furthermore, We propose a contrastive diffusion algorithm and modify the variational lower bound of the training objective by analyzing the cross-entropy between mobile traffic and contextual features, enabling the model to better integrate environmental information, thereby enhancing generalization capabilities and addressing the first research challenge. Second, we adopt a mask-based and self-supervised training paradigm [26, 70], where we categorize traffic forecasting in mobile networks into three tasks: short-term prediction, long-term prediction, and generation. We design the corresponding masking strategies to enable the model to learn data features for various tasks and adapt to multiple tasks, thus addressing the second research challenge. Finally, we develop a mobile tokenization scheme to represent mobile traffic. The token standardizes mobile traffic data collected from various periods and diverse urban regions into unified units. The model leverages these tokens to learn the underlying patterns of large-scale mobile traffic data, enabling more accurate forecasting.\n\u2022 To the best of our knowledge, it is the first foundation model designed for mobile traffic forecasting. The proposed model enables various forecasting tasks in mobile networks across different urban environments under a unified framework, assisting network operators in achieving highly efficient network planning and optimization.\n\u2022 We develop our foundation model using a masked diffusion approach with spatio-temporal masking strategies tailored for diverse forecasting tasks, including short/long-term predictions and distribution generation. To strengthen the correlation between contextual features and mobile traffic, we further propose a context-aware contrastive learning fine-tuning strategy, which can enhance forecasting and transfer learning capabilities.\n\u2022 We conduct extensive experiments with 4G and 5G mobile traffic data from 9 real-world datasets. The experimental results validate FoMo's superior transferability, multi-task capabilities, and robust few/zero-shot performance in unseen scenarios. We further experiment with the scaling properties of our model, preliminarily uncovering the scaling law of the mobile traffic foundation model concerning data size, model scale, and model performance."}, {"title": "2 PRELIMINARIES AND PROBLEM FORMULATION", "content": ""}, {"title": "2.1 Diffusion model and training object", "content": "Denoising Diffusion Probabilistic Model (DDPM). The underlying idea of DDPM is to use two Markov chains to characterize the transition from original data to noise data [44]. The forward chain gradually adds Gaussian noise \u0454 ~ N(0, 1) to the original data that can be expressed as $q(x_k|x_{k-1}) = N(\\sqrt{1 \u2013 \\beta_k}x_{k-1}, \\beta_kI)$, {$\\{\u03b2_k \u2208 (0,1), k \u2208 (1,K)\\}$} is a set of scheduled noise weight. Based on the forward transition probability, the generated noised data in step k can be calculated by $x_k = \\sqrt{\\bar{a}_k}x_o + (1 \u2212 \\bar{a}_k)e$. The reversed chain utilizes a well-trained network pe to recurrently denoise $X_K$ for recovering original data xo, which can be expressed as $P_\u03b8(x_{k\u22121}|x_k) = \u039d(\u03bc_\u03b8(x_k, k), \u03c3_\u03b8(x_k, k)I)$.\nTraining objective. It is essentially to maximize the log-likelihood function of the denoising network pe for the initial data xo, i.e., $L(\u03b8) = E_{xo\u223cq(xo)}\\{\u2212logp_\u03b8(x_0)\\}$. Subsequently, this function is optimized using the Variational Lower Bound (VLB), which can be expressed as:\n$-\\text{log}p_\u03b8(x_0) \u2264 \u2212\\text{log}p_\u03b8(x_0) + D_{KL}(q(X_{1:T}|X_0)||p_\u03b8(X_{1:T}|X_0))$\n$= E_{q(x_{1:T}|x_0)}\\{\\text{log}\\frac{q(X_{1:T}|X_0)}{p_\u03b8(X_{0:T}|X_0)}\\}\\equiv L_{ub}(\u03b8)$.\n(1)\nTaking the expectation on both sides of the above equation and applying Fubini's theorem [79], we can derive:\n$E_{q(x_0)}\\{\u2212\\text{log}p_\u03b8(x_0)\\} < E_{q(x_0)}\\{E_{q(x_{1:T}|x_0)}\\{\\text{log} \\frac{q(X_{1:T}|X_0)}{p_\u03b8(X_{0:T}|X_0)}\\}\\}$\n$= E_{q(x_0:T)}\\{\\text{log} \\frac{q(X_{1:T}|X_0)}{p_\u03b8(X_{0:T}|X_0)}\\} \u2261 L_{ub}(\u03b8)$.\n(2)\nWe can minimize the upper bound of L(\u03b8) by minimizing Lub, thereby maximizing the log-likelihood function of pe. Ho et al. [28] proved that Lub(\u03b8) can be further parameterized by $\u03bc_\u03b8(x_k, k) = a^{\u22122}_{k}0.5[x_k \u2212 \\frac{\u03b2_\u03ba}{(1\u2212\u1fb6_\u03ba)^{\u22120.5}}\u03b5_\u03b8(x_k, k)]$, and oe can be parameterized as $\u03c3_\u03b8(x_k,k) = \\sqrt{\\frac{(1 \u2212 \u1fb6_{k\u22121})}{(1 \u2212 \u1fb6_\u03ba)}\u03b2_k}$. The network pe can then be optimized by the following objective:\n$\\text{min}L_{ub}(\u03b8) \u2248 \\text{min}E_{xo\u223cq(xo),\u03f5\u223cN(0,1)}[||\u03f5 \u2013 \u03f5_\u03b8 (x_k, k)||^2]$.\n(3)"}, {"title": "2.2 Problem formulation", "content": "Mobile Traffic refers to the volume of data transmitted over wireless channels between mobile devices and BSs within a period of time. In this paper, we primarily focus on the aggregated downlink mobile traffic generated by all mobile devices within the coverage area of BSs. We consider a discrete-time scenario T = {1, 2, ...t} with equal time intervals. For a single BS, the traffic variation over time T can be represented as $\\{b_t\\}_{t=1:T}$, where b\u2081 denotes the aggregated traffic within the coverage area of BS at time t. To characterize the mobile traffic features across an urban region G, we define the geographical length and width of that region as H and V, respectively. The mobile traffic of region G can then be defined as the sum of the aggregate traffic of all BSs located at G: $St=1:T = \u2211_b\\{bt\\}_{t=1:T}$. Based on the definition above, the mobile traffic forecasting problem can be viewed as predicting dynamics in future traffic St=T:To, based on historical traffic information St=1:T and complex factors that influence network usage Cg, such as user behavior and geographical contexts, which can be expressed as:\nProbelm Definition: Given arbitrary urban region G with geographical length and width H\u00d7V. The goal is to use a model F to predict the mobile traffic sequence St=T:To for the next To time steps, conditioning on historical mobile traffic St=1:T and urban contextual factors C, i.e., $F (St=T:To|St=1:T, CG)$.\nThere are two key requirements for achieving the above problem in large-scale urban environments. First, the model must be capable of processing massive traffic data across diverse urban spatiotemporal environments. Second, the model must flexibly handle various types of forecasting tasks in mobile networks. We identify 3 primary tasks:\n\u2022 Short-term prediction task uses long historical data to predict mobile traffic dynamics over a short future period, i.e., $\\{St=1:T, CG\\} \u2192 St=T:T_0$, where T > (To \u2013 T). It mainly focuses on the fluctuations with less periodicity of traffic. Based on the forecasts, operators can understand upcoming network demands and make strategies for wireless resource and access control to improve user experience in real time.\n\u2022 Long-term prediction task estimates future traffic patterns based on limited historical data, i.e., $\\{St=1:T,CG\\} \u2192 St=T:T_0$, where T \u2264 (To \u2013 T). It primarily explores inherent patterns and periodical features within the traffic. This forecasting enables operators to assess and analyze network performance from a global perspective, thereby facilitating the formulation of network optimization planning strategies, such as cell dormancy and network capacity expansion.\n\u2022 Generation task focuses on identifying underlying network demand within a specific area without referring to historical data, i.e., $\\{CG\\} \u2192 St=1:To$ Traffic generation helps operators assess potential communication demands in new regions lacking historical data, allowing them to develop planning strategies such as BS deployment, network segmentation, and capacity planning, etc.\nHowever, building such a foundation model is not straightforward. Specifically, three key challenges arise: i). How can we generalize the large-scale traffic data across multiple cities and accurately model their rich spatiotemporal features? ii). What strategies can be employed for the training process, ensuring that the model is capable of handling the diverse forecasting tasks? iii). How to effectively integrate user dynamics and contextual features with mobile traffic?"}, {"title": "3 DESIGN OF FOMO", "content": ""}, {"title": "3.1 Framework overview", "content": "To tackle the challenges, we propose the FoMo framework that incorporates three stages: data tokenization, model pre-training, and model fine-tuning, as illustrated in Figure 1.\ni). Data tokenization. The stage reshapes mobile traffic data from various spatial ranges and time spans across multiple cities into a unified mobile token for model training and capturing the diverse features in mobile traffic.\nii). Masked diffusion-based pre-training. The stage enables the model to fully grasp the fundamental spatio-temporal features of mobile traffic across various forecasting tasks, where we design a masked diffusion model as the backbone, enabling the multitasking learning process.\niii). Urban context-aware fine-tuning. We design a contrastive learning algorithm that integrates external factors closely associated with mobile traffic, including network user dynamics and urban POI distributions."}, {"title": "3.2 Mobile traffic data tokenization", "content": "We draw inspiration from NLP tokenization [18], where we decompose heterogeneous traffic data, with varying sampling intervals and diverse spatial ranges, into basic units. Embedding layers are then used to extract the intricate features of these tokens and learn their dependencies. Specifically, we define the minimum granularity of mobile tokens in space and time as ho, vo, and to, respectively. For traffic data S of length T within an urban region H \u00d7 V, the tokenization process breaks down S into multiple small mobile tokens X, which can be expressed as\n$S \u2208 R^{H\u00d7V\u00d7T} \u2192 X \u2208R^{(H'\u00d7V'\u00d7T')\u00d7(h_o\u00d7v_o\u00d7t_o)}$\n(4)\nwhere H' = H/ho, V' = V/vo, and T' = T/to, the (ho, to, vo) of X represents the mobile token. Subsequently, we use an embedding layer Ex(X) (e.g., pooling layer, convolutional layer, or fully connected layer) to map the mobile token with hidden features C, i.e., $E_x(X) \u2208 R^{(H'\u00d7V'\u00d7T')\u00d7C}$."}, {"title": "3.3 Masked diffusion-based pre-training", "content": "Pre-training primarily serves to enhance the foundation model's understanding of various forecasting tasks and to capture the spatio-temporal correlations inherent in massive mobile data. We propose a masked diffusion model with self-supervised training [26, 95], where specific masks are tailored for the three forecasting tasks, as shown in Figure 2.\n3.3.1 Multitasking mask. We develop 4 distinct masks m \u2208 $R^{H'\u00d7V'\u00d7T'}$: short-term masking, long-term masking, generative masking, and random masking. The first three schemes align with specific forecasting tasks, while random masking aims to further explore spatio-temporal correlations within mobile traffic, and enhance its generalization capability.\n\u2022 Short/Long-masking masking. The schemes mask the time dimension T' at a specific spatial location (h, v), primarily to reconstruct the mobile traffic within the period T' - to, where to \u2208 T'. Depending on the ratio of to to T', the schemes correspond to short-term and long-term predictions, respectively, and can be expressed as:\n$m_{h.vt} = \\{0, t_o < t < T'| 1, 0 < t < t_o\\}, h\u2208 H\u2032, \u03c5\u2208 V\u2032$.\n(5)\n\u2022 Generation masking. This type of mask fully obscures the temporal dimension of adjacent spatial regions ($h_{i\u223cj}, v_{i\u223cj}$), enabling the model to generate complete temporal mobile traffic within the regions, where i \u223c j represents multiple consecutive adjacent mobile tokens. Unlike prediction masks that depend on prior historical temporal information, this mask requires capturing the spatio-temporal dependencies between the target spatial region and its surrounding regions to generate underlying distributions, which yields:\n$m_{hx,x} = \\{0, i < x \u2264 j| 1, \\text{else}\\}$\n(6)\n\u2022 Random masking. The scheme masks mobile traffic across both spatial and temporal dimensions, which aims to capture diverse correlations of mobile tokens, aiding the model in understanding the complex features of mobile data. Denote R(H', V', T') as randomly choosing items from H', V', and T', the masking scheme follows:\n$m_{h,v,t} = \\{0, R(H', V',T')| 1, \\text{else}\\}$.\n(7)\n3.3.2 Self-supervised masked diffusion model. This part reconstructs the masked mobile token Ex (X) that includes the following three important processes."}, {"title": "3.4 Urban context-aware fine-tuning", "content": "Mobile traffic is not only a spatio-temporal sequence but also influenced by urban contexts. Extensive literature has demonstrated that incorporating contextual features can enhance traffic prediction performance [64, 75, 86]. Therefore, we propose a fine-tuning scheme, where two modules are encapsulated into the pre-trained model:\n\u2022 Contextual integration: The module introduces mobile users and POI distributions, which aims to integrate human dynamics with the static features of urban environments.\n\u2022Context-aware alignment: The module employs a contrastive learning approach to align mobile traffic with contextual features.\n3.4.1 Contextual integration. Mobile user refers to the number of users accessing the network. Similar to mobile traffic, it is inherently a spatio-temporal sequence that is denoted as $U \u2208 R^{H\u00d7V\u00d7T}$. We apply the same processing method as traffic tokens where we perform tokenization on mobile users as $c_u\u2208R^{(H'\u00d7V'\u00d7T')\u00d7(h_o\u00d7w\u00d7t_o)}$, allowing this data to be directly input into the network for training.\nUrban POIs reflect the static distribution of urban layout that can be denoted as P \u2208 $R^{H\u00d7V}$. We cannot directly utilize spatio-temporal tokenization methods. Hence, we design a dynamic POI transformation scheme. Although the distribution of POIs is static, the impact of different categories of POIs on human behavior varies across different times, leading to corresponding variations in mobile traffic. For example, restaurant-type POIs typically show higher traffic during lunchtime and evening. We first extract the intrinsic static features of POI distribution, which can be written as:\n$h_o = \u03c3(W^s \u00b7 P + B^o)$,\n(12)\nwhere o is the Sigmoid activation function, Ws and Bs are the weight and bias parameters of a linear layer. Inspired by [66], we introduce temporal projection t(t) related to timestamp (we use an MLP network). Then, the indicators and static features of POIs hp are concatenated as h(0) = h \u2295 \u03c4(t), where \u2295 denotes vector concatenation. We utilize an MLP network to fuse both the time embeddings and POI features:\n$h^{(1+1)} = \u03c3(W^l . h^{(l)} + B^l), l\u2208 [0, L)$,\n(13)\nwhere h(1) is the hidden feature of the l-th layer of MLP, W\u00b9 and B\u00b9 is the weight and bias parameters in each layer. In this way, we can obtain spatio-temporal dynamic representations as h = h(L) \u2208 $R^{H\u00d7VXT}$. The final features of POI can be calculated via the same tokenization method as $c_P \u2208R^{(H'\u00d7V'xT')\u00d7(h_o\u00d720\u00d7t_o)}$. The ultimate contextual feature tokens can be denoted as y = cu + cP.\n3.4.2 Context-aware alignment. Contextual features can be viewed as another modality of data that contains much information related to mobile traffic. We propose a contrastive learning algorithm [14, 27] to establish bridges between mobile traffic and contextual features."}, {"title": "4 EVALUATION", "content": "We perform evaluations on nine real-world datasets to discuss the universality of the FoMo with 13 baselines. The experiments need to address the following three questions.\n\u2022 RQ1: How does it perform in multi-task forecasting across multiple datasets?\n\u2022 RQ2: How does the model perform in zero-shot and few-shot learning tasks?\n\u2022 RQ3: How does the proposed model perform in terms of scalability?\n4.1 Evaluation settings\n4.1.1 Datasets. Mobile traffic data. We collect mobile traffic data from 7 cities of varying scales in China, which encompass downlink traffic including both 4G and 5G data. The time granularity of the data ranges from 15 minutes to 1 hour. Additionally, we utilized mobile traffic data from 2 other cities in China and Germany to validate FoMo's zero/few-shot capabilities, where the Germany dataset is generated using the traffic prediction method described in [76].\nUrban Contextual data. We collect mobile user data in conjunction with mobile traffic data in each dataset. We crawl POI data from each city through public map services, including 15 categories related to living, entertainment, and other aspects. We summarize the collected data in Table 1.\n4.1.2 Baselines. We select a total of 13 baselines across 4 major types.\n\u2022 Statistical models. Historical moving average method (HA) and ARIMA method that integrate autoregressive with acreage moxing [77].\n\u2022 Natural language-based model. Time-LLM [31] describes time series features using natural language and uses these descriptions as prompts into a natural language pre-trained"}, {"title": "4.1.3 Metrics", "content": "We choose 3 metrics to investigate the performance of the algorithms. MAE evaluates the similarity of generated values S and real values \u015c, which can be expressed as e = Avg(|S \u2013 \u015c|). RMSE measures the average magnitude of the errors between predicted values and actual observed values, which can be expressed as e = \u221aAvg(|S \u2013 \u015c|2). Jensen-Shannon divergence (JSD) is a commonly used metric to measure the similarity between two distributions."}, {"title": "4.2 Multitask forecasting (RQ1)", "content": "In our experiments, the temporal length is 64. For short-term prediction, the model forecasts 16 future points using the previous 48. For long-term prediction, the model forecasts 48 future points using the previous 16. For data generation, the model predicts all 64 points based on the current timestamp.\n4.2.1 Short-term prediction. The results of the short-term forecasting task are presented in Table 2. Since sufficient historical data is available for reference, most baselines, leveraging their temporal feature extraction modules, effectively predict short-term changes. Our proposed FoMo outperforms the best baseline across multiple datasets, where FoMo can improve the RMSE metric by up to 29.1% (Shandong dataset) and the MAE metric by up to 50% (Nanjing-4G dataset), which exhibits stronger generalization capabilities compared to other models. Through the adaptive layernorm module, the diffusion model iteratively integrates contextual features and leverages the transformer to capture long-term dependencies between mobile traffic and the environment. We believe this correlation can transfer across different cities, improving the model's generalization capability.\n4.2.2 Long-term prediction. The long-term forecasting results are also shown in Table 3. For this task, the lack of sufficient historical observations often leads to performance degradation in some baselines. However, FoMo consistently achieves the best performance, it achieves a maximum improvement of 31.4% in RMSE (Beijing dataset) and a maximum enhancement of 40.6% in MAE (Nanjing-4G dataset), which showcasing its adaptability to various tasks.\n4.2.3 Mobile traffic generation. As shown in Table 4, the absence of historical observation of the generation task prevents some existing baselines from completing the task. Nevertheless, the FoMo still achieved strong generative results. It can achieve up to a 23.7% improvement in the JSD metric (Shandong dataset), and a maximum enhancement of 38.1% in MAE (Nanjing-4G dataset). This is due to the contextual feature fusion module used during fine-tuning, which captures the correlation between contextual and mobile traffic features through contrastive learning. This allows the model to infer potential traffic distribution based on environmental changes, even without historical data.\n4.2.4 Visualization. To intuitively demonstrate our model's universality for different tasks, we select two datasets as examples (Beijing and Nanchang) and plot the forecasting results in Figure 4. From left to right, it represents the tasks of short-term prediction \u2192 long-term prediction \u2192 traffic generation. The blue-shaded area indicates the model's predicted results, while the unshaded area represents historical observations. FoMo generates mobile traffic closely aligned with real values across all tasks, accurately predicting periodic trends and capturing fast dynamics, which shows that our FoMo model achieves forecasting across multiple cities and tasks, highlighting its generalization capability."}, {"title": "4.3 Zero/Few shot learning (RQ2)", "content": "To evaluate FoMo's zero/ few-shot learning capabilities, we select two datasets that FoMo has not encountered during training: Hangzhou (China) and Munich (Germany). We choose 4 baselines that perform well in previous multitask forecasting: KG-Diff, TimeGPT, Lagllama, and UniST. The results are shown in Figure 5, where 5% few-shot and 10% few-shot represent the model training with a small amount of data (5% and 10%, respectively). It shows that FoMo exhibits good zero-shot performance, especially in the Munich dataset, where FoMo's zero-shot performance even surpasses that of KG-Diff after small-scale training. After training with a small amount of data, all models show varying degrees of improvement. FoMo still demonstrates the best performance, indicating that FoMo can utilize the pre-trained model to quickly capture general features within unseen mobile data.\nWe also visualize the performance in zero/few-shot scenarios, as shown in Figure 6. We select a long-term forecasting task, with the results for zero-shot\u2192 5% few-shot 10% few-shot from left to right in the figure. It can be observed that FoMo can learn the general distribution characteristics of mobile traffic in the zero-shot phase, and after training with a small sample, the model realizes accurate traffic forecasting.\nWe comprehensively summarize and compare the FoMo algorithm, UniST, Lagllama, and KG-Diff, as shown in Figure 7. In the figure, we highlight the performance of our model as well as the performance of the second-best model. FoMo consistently delivers the best performance across various tasks and multiple cities. For example, in few-shot tasks, FoMo achieves performance improvements of 10.6% and 32.1% in different cities, demonstrating the model's universality."}, {"title": "4.4 Scaling performance of FoMo (RQ3)", "content": "Scaling performance reflects the relationship between model parameters, data size, and overall performance. Understanding the scaling performance of foundation models provides valuable guidance for parameter selection during model deployment, which optimizes computational and storage overheads for the entire system. We provide 5 different model settings based on various transformer layers and hidden features: FoMo with 5M parameters (5M params), 35M FoMo (35M params), 100M FoMo (100M params), 150M FoMo (150M params), and 200M FoMo (200M params)."}, {"title": "4.5 Ablation study", "content": "We conduct ablation experiments on FoMO, as shown in Table 5, with FoMo-user and FoMo-POI representing the incorporation of mobile users and POI distributions, respectively, during the fine-tuning process. It can be observed that adding these two contextual features at the fine-tuning stage enhances model performance to varying degrees. Moreover, the performance degradation of FoMo-POI is more significant, indicating that mobile users better reflect the dynamic characteristics of mobile traffic and are more critical for mobile traffic forecasting compared to POI distribution."}, {"title": "5 DEPLOYMENT APPLICATIONS", "content": "To validate FoMo's prediction effectiveness, we deploy the model on the Jiutian platform, an AI platform developed by China Mobile, featuring functions like scenario construction, network simulation, optimization strategy formulation, and performance evaluation, as shown in Figure 10. The Jiutian platform offers full-element network simulation capabilities, enabling efficient simulation of interactions between communication systems and user behavior. It also supports operators in developing customized algorithms and applications, deploying them into production environments, and performing product validation and testing with real network data. The platform has now been fully deployed within China Mobile, supporting network development across 31 provinces in China.\nDeployment. We select predefined urban layout and human mobility data. FoMo is deployed in the mobile traffic module, with its predictions feeding into the optimization selection module. We focus on 3 optimization scenarios (highlighted in yellow in the figure) and evaluate the performance via network coverage, throughput, and energy consumption. The training of FoMo is conducted on 4 NVIDIA A100 GPUs (80GB each) using PyTorch 2.0.1. Table 6 summarizes the model's parameters and training/inference time per sample."}, {"title": "5.1 Energy efficient BS sleep mode control", "content": "We consider the C-RAN scenario, where the BS achieves cell coverage by activating different numbers of RRUs [1]. The strategy for BS sleep modes involves controlling the RRU operational status (activated or sleep) based on the network load. We model the problem from the perspectives of service quality, depreciation cost, and energy consumption. We set N BSs, each of which has M cells to serve at time t. Define the traffic load that a single RRU can serve as co, then for BS n, its Quality of Service (QoS) equals $Q(n) = \u2211_t \u2211_m max(\\frac{L_{m,t} - X_{m,t}C_0}{L_{m,t}}, 0)$, where Lt,m is the actual cell load and xm,t is the activated RRUs. Moreover, frequent switching of RRUs can lead to a reduction in the lifespan of BSs, and the depreciation yields $W(n) = \u2211_t Em|X_{m,t} \u2013 X_{m,t\u22121}|$. BS energy consumption equals $E(n) = \u2211_t EmP[min(L_{m,t}, X_{m,t}C_0)]$ that is determined by the load at the RRU [92], where $P[L] = aL+B(\\frac{L}{c_o})$ is the energy consumption function. Therefore, the optimization objective yields:\n$\\text{min} \\frac{\u0393(x^{(n)})_t}{\u0393(x^{(n)} )} = O(n) + W(n) + E(n)$.\n(17)\nFor the BS sleeping strategy, frequently adjusting RRUs over a short period is impractical. A more reasonable approach is to assess network dynamics over a longer period and develop a long-term adjustment strategy. Therefore, we leverage FoMo's long-term prediction capability to estimate Lm,t via the long-term prediction mask in (5). The optimization results are shown in Figure 11(a), it can be seen that the FoMo-based strategy closely matches the real data-based strategy across different metrics, with a 21.9% improvement in QoS and a 40.7% reduction in BS depreciation better than the other two. While energy consumption lags behind the comparison strategies, our algorithm aligns with that of the Real-based strategy, indicating that FoMo accurately predicts mobile traffic and effectively meets real user demands."}, {"title": "5.2 Traffic-aware resource allocation", "content": "We consider cellular networks involving a single cloud server and N cells, where the server allocates K PRBs to each cell [65, 89", "P_{k,n}(t)\\text{log}_2[1+\\frac{p_n(t)h_n(t)}{\u03c3^2}": "where B is the PRB bandwidth and pn(t) is the transmission power. $p_{k,n}(t)$ is a binary variable indicating PRB allocation ($p_{"}]}