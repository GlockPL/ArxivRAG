{"title": "EdgeGFL: Rethinking Edge Information in Graph Feature Preference Learning", "authors": ["Shengda Zhuo", "Jiwang Fang", "Hongguang Lin", "Yin Tang", "Min Chen", "Changdong Wang", "Shuqiang Huang"], "abstract": "Graph Neural Networks (GNNs) have significant advantages in handling non-Euclidean data and have been widely applied across various areas, thus receiving increasing attention in recent years. The framework of GNN models mainly includes the information propagation phase and the aggregation phase, treating nodes and edges as information entities and propagation channels, respectively. However, most existing GNN models face the challenge of disconnection between node and edge feature information, as these models typically treat the learning of edge and node features as independent tasks. To address this limitation, we aim to develop an edge-empowered graph feature preference learning framework that can capture edge embeddings to assist node embeddings. By leveraging the learned multidimensional edge feature matrix, we construct multi-channel filters to more effectively capture accurate node features, thereby obtaining the non-local structural characteristics and fine-grained high-order node features. Specifically, the inclusion of multidimensional edge information enhances the functionality and flexibility of the GNN model, enabling it to handle complex and diverse graph data more effectively. Additionally, integrating relational representation learning into the message passing framework allows graph nodes to receive more useful information, thereby facilitating node representation learning. Finally, experiments on four real-world heterogeneous graphs demonstrate the effectiveness of the proposed model.", "sections": [{"title": "I. INTRODUCTION", "content": "GRAPHS generally contain rich node features and edge features. However, in recent years, most advanced GNN models have mainly focused on enhancing the learning of node features while neglecting the simultaneous learning of edge features. Although the aggregation functions designed based on the Message Passing Neural Network (MPNN) framework can aggregate both node features and edge features and have shown good results in specific application scenarios, using predefined aggregation functions is akin to manual feature engineering and cannot be generalized to all cases. Therefore, we aim to develop a method that can iteratively learn multidimensional edge features and synchronize the updates of these multidimensional"}, {"title": "II. RELATED WORK", "content": "This work relates to node representation learning, graph structure learning, and graph attention in training representation, so we review prior studies in each category and discuss the differences and relations.\nNode Representation Learning. Traditional node representa- tion learning methods mainly focus on mapping the original node representation vector (e.g. the adjacency vector) into the continuous low-dimension embedding space [15]. Those works usually extend the classical machine learning models for graph-structured data. The early works like Locally Linear Embedding (LLE) [16], Isomap [17] and Laplacian Eigenmaps [18] try to adopt the eigen-based methods, so as to keep the spectral properties of node when mapping into the lower space, which is time consuming and uneasy to be parallelized. Recently, inspired by word2vec [19], DeepWalk [20] adopts random walk to generate node sequences to imitate sentences in corpus, and then feeds them into the Skip-gram framework [26] to train node representations, which can to some extent learn both local and global structure information. Node2vec [21] improves the DeepWalk model by introducing a bias mechanism into the random walk, which can get more efficient node sequences to enhance the model performance. Besides, Modularized Nonnegative Matrix Factorization (M-NMF) [22] focuses on extracting the mesoscopic community structure among nodes. Furthermore, Attributed Network Embedding with Micromeso structure (ANEM) [23] considers three kinds of information in node representation learning: attribute information, the local proximity structure, and mesoscopic community structure. As we can see, the above methods usually pay major attention to manually integrating special rules with shallow models.\nGraph Structure Learning. Due to the success of deep learning models in training representations, many attempts have been made to integrate them with graph-structured data. For example, Structural Deep Network Embedding (SDNE) [4] uses deep auto-encoders to extract low-dimensional node representations and adds a pairwise loss to ensure connected nodes are close. Deep Neural Networks for Graph Repre- sentations (DNGR) [5] calculates the Positive Point Mutual Information (PPMI) [24] matrix for graphs and feeds it into a stacked denoising autoencoder to extract node representations. ContextAware Network Embedding (CANE) [25] combines a mutual attention mechanism with CNNs to learn context-aware node representations. Self-Translation Network Embedding (STNE) [26] uses bi-LSTM on node sequences obtained by random walks to preserve node order information. Deep Attributed Network Embedding (DANE) [27] assumes node representations from structure and feature information should be similar. Self-Paced Network Embedding (SPNE) [28] adopts a self-paced learning mechanism to learn node representations, considering information from neighbors of increasing hops. Deep Network Embedding with Structural Balance Preserva- tion (DNE-SBP) [29] uses stacked auto-encoders to preserve structural balance in signed networks. Nodepair Information Preserving Network Embedding (NINE) [30] uses adversarial networks to preserve local information between node pairs. However, traditional approaches often apply existing models to train node representations without specifically targeting graph-structured data.\nGraph Attention Learning. The message-passing framework, designed to fit the topological characteristics of graphs, has been widely adopted in various graph neural networks. In Graph Convolutional Networks (GCN) [13], nodes propa- gate their information to neighbors and then update their representations by averaging the received information. Graph Attention Networks (GAT) [7] improve on this by using an attention mechanism to assign different weights to different neighbors. Building on GCN and GAT, several methods have been developed to enhance the message-passing framework: Hierarchical Graph Convolutional Networks (HGCN) [31] use a coarsening-refining model to hierarchically aggregate node information; Relation-aware co-attentive Graph Convolutional Networks (RecoGCN) [32] integrate a co-attention mechanism to adapt GCN for recommender systems by bridging user, item, and agent relations; Dynamic Hypergraph Neural Networks (DHNN) [33] introduce a hypergraph mechanism to capture group relations, where each hyperedge covers a group of nodes; and Graphsage [34] extends GCN to inductive learning and explores various aggregation operations like max pooling and LSTM. However, these models often treat each edge as a scalar, limiting edge representation capability. To better utilize edge information, some models incorporate edge representation learning: CensNet [35] constructs two graphs to represent node and edge adjacency, using shared trainable weights to link node and edge representations. Inspired by translation mechanisms [36], edge representations are learned by assuming a transforming relation between edges and their corresponding nodes. Vectorized Relational Graph Convolutional Network (VR-GCN) [37] combines translation mechanisms with GCN to train node and relation representations in multi-relation network alignment tasks.\nHowever, those models treat the representation learning of edge as the representation learning of node, without considering node interaction, and thus fail to learn the relation between the connected nodes. In this case, these models cannot properly integrate the edge representation learning into the message- passing framework."}, {"title": "III. BACKGROUND AND MOVIATION", "content": "In this section, we begin by introducing the preliminaries that include the notations, preliminary (i.e., message-passing framework), and motivation.\nA. Notations\nLet G = {V,E,F,\u03c6, \u03c8} denote an undirected attributed network, where V = {U1, U2,\u2026\u2026,Un} is a set of n nodes, E = {eij | i,j\u2208 nodes,i \u2260 j} is a set of m edges, and a node attribute matrix F = [f\u2081; \u2026\u2026; fn] \u2208 Rnxd with each row vector f\u00bf \u2208 Rnxd being the attribute vector of node vi. The (v) denotes the type of each node 6, and (e) denotes the type of each edge 4. Additional, each dimension being considered as a feature. The collections of potential node types and edge types are symbolized as Tv = {\u03c6(v) : \u2200v \u2208 V} and Te = {$(e): Ve \u2208 E}, correspondingly. When both Tv and"}, {"title": "IV. THE PROPOSED APPROACH", "content": "A. Overview\nWe now illustrate the proposed Edge-Enhanced Graph Fea- ture Preference Learning (EdgeGFL) model, which enhances the effectiveness of the information exchange procedure by introducing the edge representation as relation representation.\nFollowing the structure of the message-passing framework, each layer of EdgeGFL is mainly composed of two phases, namely Information Propagation and Information Aggregation.\nInformation Propagation: Different from the existing models (i.e., GCN and GAT), which only send information from one node to another in the information propagation phase, EdgeGFL first extracts the relation representation for edges from the corresponding node representations. Subsequently, each node sends the self-information to its neighbors, and features inside the sent information are magnified or shrunk through the corresponding edges, which finally forms a different 'sent message' according to the characteristics of the target nodes.\nInformation Aggregation: Upon receiving feature-preferred information from its neighbors, each node proceeds to refine its representation by integrating the accumulated messages. Similarly, each edge receives the feature-preferred information from neighbors, and then, the node updates its representation by summing up the received message.\nIn general, each layer of the EdgeGFL model is illustrated in Fig. 2. For each edge eij, there is a corresponding vectorized relation representation rij derived from the interaction between the connected nodes by using an initialized weight vector W(hi, hj), and then each node sends information refined by the relation representation. Finally, the representation of each node is updated by integrating all of the refined information.\nB. Feature Projection\nNode Nonlinear Transformation. As the input features of different node types may differ in dimensionality, we apply a linear layer with bias for each node type and then utilize a non-linear mapping (e.g., using a rule or softmax function) through the activation function to map all node features to a shared feature space. Typically, we use a learnable type transformation matrix WTv, where T is the set of node types, to map nodes of different types to the same dimensional space H\u00b2 = WT * F. To better map different types of nodes to the same semantic space, we apply a nonlinear activation function after a fully connected layer, as follows:\nH' = LeakyRelu(WTv * F + b),\n(6)\nEdge Type Initiation. For heterogeneous graph G with Te = {$(e) : Ve \u2208 E} types of edges, an edge type dictionary D = {E1, E2,\u2026\u2026, En} is constructed by looking at the types of edges as different tokens and assigning an initialisation encoding vector R = {r1,r2,\u2026\u2026,rn} of the corresponding type to the edges indexed by the different types based on the edge type dictionary. This vectorised representation holds the semantic information of the edge types:\nrij = f (Te = {$(e) : ve \u2208 E}),\n(7)\nwhere f(.) is the initialization function for edge types, taking the values of each edge as input and producing corresponding edge vectors.\nC. Information Propagation\nThe edge type-based feature preference representation makes full use of the type information of the edges in the dataset by mapping the edge type information to the same data space as the node feature information, and by co-training with the node features in the same data space. The resulting feature preference representation can be used to scale each dimensional feature of the node information, rather than simply scaling all dimensional features with a fixed scalar.\nWe map the initialized coding vector R to the data dimension space of the nodes via a learnable feature transformation matrix w\u00b9, providing the basis for the co-training of node and edge features. The preference representation step for edge features can be expressed as:\nrij = d(rij w\u00b2),\n(8)\nwhere 8(.) denotes the learnable initialization encoding function that assigns a corresponding initialisation encoding vector r to each edge type based on a dictionary of edge types, and w is a learnable parameter shared by all types of edges that are used to map the encoding vectors of the edge types to the node feature space.\nWe apply edge feature preferences to a message-passing framework to replace aggregation weights in the form of scalars learned in convolutional and attentional neural networks. We propose to use learnable edge feature preferences to act as aggregation scales that both scale the message's size and learn the features' preferences. Specifically, the message-passing phase can be represented as:\nM = s (hrij) = hrij,\n(9)\nwhere M denotes the message sent from node vj to node vi at layer l. s(\u00b7) denotes the message transfer function, and \u00b7 denotes the dot product.\nD. Information Aggregation\nIn this phase, nodes update their representation by aggre- gating the received information via the sum aggregator, which can be formalized as:\nh+1 \u2190 SUM (M).\n(10)\nJEN"}, {"title": "V. Experiments", "content": "In this section, we evaluate the effectiveness of EdgeGFL on four network datasets by comparing with eight state-of-the- art methods. Specifically, we conduct node classification experiment to evaluate the end-to-end learning performance of our model, node clustering experiment to evaluate whether the learned node representations can fit the correct node class distribution. Besides, for observing the intuitive display of node representation, the node visualization is conducted as an auxiliary experiment. Finally, for giving a time Analysis and parameter sensitivity of our model, we conducted an explanatory ablation study on time complexity and sensitivity analysis of model parameters. We chose the ACM and DBLP datasets and visualized the model's efficiency and performance across various parameters.\nThe experiments are designed to answer the following research questions (RQs):\n\u2022 RQ1. Does our EdgeGFL outperform the state-of-the-arts in Node Classification?\n\u2022 RQ2. How does the clustering and visualisation of nodes perform?\n\u2022 RQ3. How effective is each component of our model?"}, {"title": "VI. CONCLUSIONS", "content": "The message-passing framework leverages the propagation characteristic of graph-structured data to enrich the informa- tion of each node, which makes it more suitable for node representation learning than traditional methods. In this paper, we have developed a novel message-passing based model, namely Edge-empowered Graph Feature Preference Learning (EdgeGFL), by integrating the edge representation learning into the messagepassing model, which can help refine the information in finer granularity for nodes. By doing this, nodes can receive more useful information, and thus promote the node representation learning process. Extensive experiments are conducted in both of node classification and node clustering on four datasets. And the results demonstrate the superior performance of node representations learned by EdgeGFL and the visualization results further reflect that the learned distribution can be better distributed in latent space."}]}