{"title": "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility", "authors": ["Yuchen Xia", "Jize Zhang", "Nasser Jazdi", "Michael Weyrich"], "abstract": "This paper introduces a novel approach to integrating large language model (LLM) agents into automated production systems, aimed at enhancing task automation and flexibility. We organize production operations within a hierarchical framework based on the automation pyramid. Atomic operation functionalities are modeled as microservices, which are executed through interface invocation within a dedicated digital twin system. This allows for a scalable and flexible foundation for orchestrating production processes. In this digital twin system, low-level, hardware-specific data is semantically enriched and made interpretable for LLMs for production planning and control tasks. Large language model agents are systematically prompted to interpret these production-specific data and knowledge. Upon receiving a user request or identifying a triggering event, the LLM agents generate a process plan. This plan is then decomposed into a series of atomic operations, executed as microservices within the real-world automation system. We implement this overall approach on an automated modular production facility at our laboratory, demonstrating how the LLMs can handle production planning and control tasks through a concrete case study. This results in an intuitive production facility with higher levels of task automation and flexibility. Finally, we reveal the several limitations in realizing the full potential of the large language models in autonomous systems and point out promising benefits. Demos of this series of ongoing research series can be accessed at: https://github.com/YuchenXia/GPT4IndustrialAutomation", "sections": [{"title": "1. Introduction", "content": "Automation systems vs. Autonomous systems\nIn the evolving landscape of technology, the terms automation and autonomous systems are often intertwined yet distinctly different in impact. Both involve the use of technology to perform tasks with minimal or no human intervention, but differ significantly in their flexibility in decision-making:\n\u2022 Automation: Traditional automated systems generally follow rigid, predefined rules and workflows and are not designed to adapt to changes unless those changes have been anticipated and programmed into the system. A usual pre-requisite of automation lies in repeatability and predictability, which limits its adaptability to dynamic environments.\n\u2022 Autonomy\u00b9: on the other hand, autonomy entails a higher level of adaptability and decision-making capability. An autonomous system can adapt to un-predefined changes and utilize knowledge to make decisions based on available options and system objectives, demonstrating flexibility in problem-solving and task execution, whereas automation typically does not. This concept is commonly applied in fields such as intelligent robotics and artificial intelligence, and it is also used in political contexts to describe self-determined individuals or systems that make choices aiming to achieve optimal outcomes.\nIn the transition from automation to autonomy, the key differentiator is intelligence-the capability to make informed, dynamic decisions. This intelligence cannot be practically provided by using exhaustive rules due to the unpredictability and variability of real-world environments. Such rules cannot cover every possible scenario and often struggle with granularity-they can be either too broad, failing to address specific situations, or overly detailed, making them cumbersome to exhaustively implement and maintain. Moreover, maintaining and updating such a comprehensive rule set demands extensive engineering effort. The technological development in natural language understanding indicates the superiority of machine learning with neural networks over rule-based systems, as the top solutions are all based on neural networks .\nLarge language models (LLMs) can offer the intelligence to bridge the gap between traditional automation and autonomy in industrial systems. These models internalize the knowledge patterns learned from training data conveying collective human knowledge, and they are capable of interpreting complex text and performing dynamic reasoning based on the given input. Their general adaptability allows them to respond to new situations and conditions without the need for specific re-training.\nIncorporating LLMs into industrial automation systems empowers us to utilize their capabilities in performing diverse tasks within industrial automation, further reducing the need for human intervention in tasks that require intelligence. LLMs are particularly effective in extracting important information from vast datasets, comprehending texts, making reasoning, solving problems, and supporting decision-making processes [3]. By enabling these models to perform intelligent information processing based on real-time data, they can swiftly adapt to changes, thereby boosting efficiency and productivity.\nIn the following sections, we explore the integration of LLMs into automated production systems to enhance their autonomy and flexibility. Section 2 reviews the typical state-of-the-"}, {"title": "2. Background", "content": "2.1 Automation pyramid\nTo manage the diverse tasks in industrial automation, the automation pyramid is a conceptual framework organizing the tasks in industrial automation systems into five distinct layers [4]: starting at the base, the Field Level involves direct interaction with physical technical processes through sensors and actuators. Above this is the Control Level, where devices like programmable logic controllers (PLCs) execute real-time control tasks based on functional execution of control and regulation. The Supervisory Level includes systems such as SCADA and human-machine interfaces (HMI), enabling operators to monitor and adjust processes. The Production Planning Level manages optimized production planning and scheduling, coordinating operational logistics and production processes. At the top, the Enterprise Level integrates various business functions through enterprise resource planning (ERP) systems, enhancing decision-making and resource management across the organization. This hierarchical model ensures systematic data flow and task execution across different facets of industrial operations.\n2.2 Digital twins for automation system\nMoving beyond traditional automation, the integration of digital twins facilitates data-driven optimization and enables remote interactions. This integration creates a software system synchronized with physical systems. As a result, operational changes are immediately reflected, providing precise control interfaces for effective process management.[5] The digital"}, {"title": "2.3 LLM as an enabler of intelligent autonomous system", "content": "The incorporation of LLMs into the system enriches the digital twin with the intelligence for advanced data analysis and decision-making. These models leverage vast amounts of text to perform reasoning, provide insights, solve tasks, and support data-driven decision-making, providing general intelligence to a wide range of problems. This intelligence can be utilized to enhance adaptability and flexibility within the automation system, leading to quicker reactions to changes and automated problem-solving."}, {"title": "3. Methodology", "content": "3.1 LLM agent and task decomposition for LLM multi-agent system\nAt the core, a generative LLM is a deep neural network for processing and generating text-based information, as illustrated in Figure 2. This capability allows them to act as intelligent agents, equipped to interpret diverse information and generate output as a response to a task. In the design of an LLM agent, the prompt is crucial for directing the LLM's behavior to a specific task context, and this mechanism is also referred to as \"in-context learning\" or \"prompt learning\" [6]. The input text for a task is incorporated into prompt templates, which guide the LLM in generating appropriate outputs, thereby enabling the LLM agent to function effectively as an information processor.\nIf a task is too complex for a single LLM agent to handle, task decomposition becomes necessary. This is where the design of LLM agents as integral components of a multi-agent system comes into play. Each LLM agent is tasked with solving specific sub-tasks within the overall task-solving process, as shown in Figure 2. These sub-tasks can range from interpreting data inputs and generating dynamic process plans to determining the next executable microservices as actions. The agents operate within a framework where complex tasks are broken down into more manageable sub-tasks, allowing individual agents to execute them more effectively."}, {"title": "3.2 Structured prompt and agent design template", "content": "To develop an effective prompt for a LLM to perform a specific task, it is essential to address several key aspects to guide the text generation behavior. Drawing from our experience in creating LLM-based applications [7], [8], [9], we outline these aspects in a template for effective prompting. This template includes the following critical elements:\n\u2022 Role, responsibility and goal outline the agent's functional role and the objectives it aims to achieve, ensuring alignment with its responsibility and broader system goals.\n\u2022 Context specification supplies detailed background information relevant to the task, enabling the agent to interpret the task-specific context.\n\u2022 Behavioral instructions and constraints contain specifications to guide the agent's responses, ensuring they adhere to task responsibility.\n\u2022 Input-output abstract pattern defines the abstract formats for input and output, which helps standardize the agent's interaction and generate parsable output.\n\u2022 Input-output interaction field provides dynamic input and leaves the output section with only a cue word \u201cOutput:\u201d, forcing LLM only generate output content by text continuation.\nThis template offers a systematic structure for specifying prompts, facilitating the effective and reproducible use of LLMs to develop software components with clearly defined interfaces (i.e., LLM agents)."}, {"title": "3.3 Design of dedicated LLM agents for tasks on different layers in automation system", "content": "Building on the foundational methods previously introduced, a multi-agent system can be designed to perform diverse tasks across different layers of the automation pyramid."}, {"title": "4. Case study setup", "content": "At the Institute of Industrial Automation and Software Engineering (IAS) at the University of Stuttgart, our laboratory is equipped with a modular production system, as illustrated in Figure 5. This system features several automation modules capable of executing a range of production and logistics tasks. We have retrofitted the system to support real-time data access and control interface through a run-time environment [7]."}, {"title": "5. System design", "content": "5.1 Conceptual design\nIn our designed system, LLM agents play a central role by interpreting production-specific information provided by digital twins, planning operations, and making decisions. They respond to user commands or system-triggered events, which can be entered through an application front-end or detected through continuous monitoring.\nOnce a task-triggering event is detected, the manager LLM agent dynamically generates a process plan consisting of a sequence of subtasks for operator agents. The operator agents then break this plan down into atomic operations organized as microservices, each encapsulating a specific, executable operation as a part of the production process. This decomposability of the task plan and microservices ensures modularity, scalability, and flexible orchestration of operations.\n5.2 System components\n5.2.1 Run-time Environment\nAs shown in Figure 8, the run-time environment includes the following software components:\n\u2022 OPC UA server: The OPC UA server interfaces with PLCs that control the production modules' functionalities. It reads and sets values to access and modify the operational parameters and statuses of PLCs. The functionalities programmed into the PLCs, such as control logic for various production operations, are encapsulated into atomic services. These services, such as \u201cconveyor_run(direction, duration)\u201d and \u201cactivate_material_holder()\u201d, are designed to be independently executable.\n\u2022 ROS server: In parallel, the ROS Server is essential for the control and coordination of AGVs for the logistics functionalities. Functionalities such as \u201cmove_to(station_ID)\u201d and \u201cload_workpiece()\u201d are designed to control AGV actions.\n\u2022 Service register: This component contains a lookup table and service interface descriptions, cataloging all available microservices. This component facilitates the discovery and the invocation of services as needed. The microservice architecture allows for the independent implementation and scaling of these atomic services. This modularity enhances the system's operational compatibility and adaptability, making it possible to handle a variety of operational scenarios by orchestrating atomic functionalities into a process sequence.\n\u2022 Data pool: This is a centralized repository that aggregates real-time operational data from diverse sources of the system and provides data access to external components.\n5.2.3 Event-driven information modeling in text\nSeveral components in Figure 8 are typed as event emitter. They can send event information to an Event log memory, which organizes events in chronological order. Each event in the log is tagged with a standardized system timestamp, ensuring that information about changes, decisions and actions can be captured and further consumed by each LLM agent according to its interest scope within a subscription mechanism. In the Event log memory, an event is captured at timeless moments without a duration: instead of recording a process \u201cthe material"}, {"title": "5.2.4 Decision-making by LLM agents and action execution with microservices", "content": "holder secures the position of the workpiece for 5 seconds\", it is logged in two separate entries: \"[00:00:00] the material holder holds the workpiece\" and \"[00:00:05] the material holder releases the workpiece.\"\nThe necessity for this event-driven design arises from the nature of planning and control problems, in which the dimension of time is indispensable. However, invoking an LLM for decision-making is a discrete action rather than a continuous process. This characteristic necessitates an approach to structure time-aware information in the form of the event log. Based on this design, an excerpt of the event log containing the necessary atomic information (event) can be integrated into the prompt of an LLM agent for decision-making.\nThe LLM agents subscribe to specific events within the Event log memory. Based on the events logged, the agent makes informed decisions about the production planning and control actions depending on the dynamic situation. The decisions generated by the LLM are structured as textual data in JSON format, which can easily be handled with code script. The JSON data is parsed into actionable function calls. These are then passed either to another LLM operator agent for further handling or directly to the Microservice run-time executor, which carries out the commands. The process is depicted in the following sequence diagram in Figure 9 for instance."}, {"title": "6. Result and Evaluation", "content": "The system has been implemented in our laboratory. It operates as follows: when a user inputs a task via the front-end user interface or a triggering event is identified, the LLM manager agent formulates a production plan\nThe operation steps in this plan are subsequently delegated to operator agents who arrange a sequence of atomic microservices to execute the task. This prototype successfully demonstrates the concept of integrating LLMs into an automated production system, moving towards autonomous systems.\nThe evaluation of the developed prototype is outlined in the following sections across three key aspects. \nEvaluation of different LLMs for planning and controlling capabilities\nTo assess the foundational automation capabilities of our system, we evaluated how accurately the LLMs could interpret basic tasks provided via prompts and subsequently execute the correct operations. These operations include processing routine orders, adjusting settings according to user requests, managing standard material transfers, and reacting to unforeseen and un-preprogrammed events that arise during operation. We applied different LLMs for powering the LLM agents for our system design, including proprietary model \u201cGPT-40\" and GPT-3.5, and two opensource models \u201cLLAMA-3-8B\u201d, \u201cLLAMA-3-70B\u201d, as well as a fine-tuned model based on \u201cLLAMA-3-70B\", which is trained on the texts in the event logs during the system operation.\nWe selected a total of 100 test points to evaluate the performance of the language model agents. These points were divided into two categories: 50% comprised predefined routine planning and control tasks, for which there are direct instructions in the prompts on how to execute the tasks in a standardized way (standard operation procedure), and the remaining"}, {"title": "7. Limitation", "content": "7.1 Real-time Performance\nWhile the system integrates LLM agents effectively for planning and executing tasks, real-time performance remains a challenging aspect. The latency introduced by processing and generating responses through LLMs can impact the system's ability to operate in strict real-time environments. Due to this limitation, in our system, LLM reasoning is only performed when the operations can be paused or when new events during the LLM inferencing do not affect the decision-making. The LLM inferencing typically lasts between 100 ms to 1 second when utilizing dual NVIDIA A100 Tensor Core GPUs deployed on a local server, and 1 to 10 seconds for cloud server. For this reason, the generated text should remain concise by design.\n7.2 Comprehensive Testing\nThe complexity and variability of production environments necessitate extensive validation of any automated system to ensure its robustness and reliability. Our current evaluation primarily relies on synthetic test cases designed to simulate typical scenarios within the controlled environment of our laboratory. These are used for initial proof-of-concept demonstrations, and the test cases could quickly become complicated and condition-specific. The system-generated decisions and operations still require human approval, making it only sufficient for an assistant system. Moving forward, we will continue to perform more comprehensive testing of the system to better address the dynamic nature of real-world manufacturing processes. It will be crucial to employ test-driven development methods to identify and mitigate potential failures or inefficiencies, thereby delivering greater value and enhanced usability.\n7.3 Cost-Benefit Evaluation\nThe economic feasibility of integrating LLMs into production systems remains an area requiring thorough investigation. While some theoretical benefits are evident, such as more intuitive human-machine interaction, reduction of human effort, training cost and reaction time to changes, the actual cost related to their implementation, improvement, and maintenance are not yet fully understood. The benefits of increased adaptability, automation efficiency, and productivity gains can be further quantitatively assessed against these costs, in order to validate the long-term economic viability of deploying large language models in industrial settings."}, {"title": "8. Conclusion", "content": "This paper presented a novel approach to integrating LLMs into industrial automation systems, thereby enhancing task automation and flexibility. By employing LLMs within a multi-agent system and integrating them with digital twins and automation systems, we demonstrated their efficacy in autonomously managing production planning and control tasks. The integration of LLMs offers significant advantages, including dynamic decision-making and planning, as well as creative problem-solving for un-predefined issues, which can be readily developed as intelligent assistant systems. This integration significantly reduces the need for human monitoring and trouble-shooting, as well as decreases reaction times to un-predefined changes. Our results show that LLMs can effectively handle both routine and unexpected events, thereby improving operational efficiency and adaptability. A fine-tuned model based on an open-source model can have equivalent performance to proprietary GPT-4 on this specific application task.\nHowever, challenges such as real-time performance latency, the need for comprehensive real-world testing, and the economic implications of deploying LLMs remain. These issues highlight critical areas for further research and optimization to fully harness the potential of LLMs in industrial settings.\nIn summary, integrating LLMs into production systems offers promising prospects for advancing toward intelligent autonomous systems. Drawing on insights from experience, we hold the view that developing future intelligent autonomous systems powered by LLMs will require a test-intensive or even test-driven development process. Our future research will focus on broadening application areas and improving the system through systematic testing and evaluation. This will enhance the value and cost-effectiveness of these systems, continuing to realize the transformative impact of LLMs on industrial automation."}, {"title": "Appendix A", "content": "Prompt for an operator agent: (1025 tokens)\nYou are an operator agent that is responsible for controlling the material transport on a conveyor before a production process.\nThis conveyor belt is a straight, 1-meter-long system designed for material transport. At its entrance, sensor BG56 detects incoming workpieces. At the end of its path, sensor BG51 detects the workpiece at the ready position, actuator holder H1 can secure the workpieces in place, and an RFID sensor TF81 reads the workpiece IDs for processing validation.\nComponents descriptions:\nSensors:\nBG51: Detects workpieces at the ready position.\nRFID Sensor TF81: Reads workpiece IDs to validate processing criteria.\nActuators:\nConveyor C1: Controls the movement of the conveyor. It can be controlled via the following command(s):\nconveyor_belt_run(direction, duration_in_second),\nconveyor_belt_stop().\nMaterial Holder H1: Holds workpieces at the ready position. It can be controlled via the following command(s):\nactivate_material_holder(), deactivate_material_holder().\nActions you can take:\nconveyor_belt_run(direction, duration_in_second): Moves the conveyor belt in the specified direction ('forward' or 'backward') for a set duration.\nconveyor_belt_stop(): Stops the conveyor belt.\nactivate_material_holder(): Engages a mechanism to hold the workpiece in place on the end of the conveyor.\ndeactivate_material_holder(): Releases the holding mechanism, freeing the workpiece from the secured position at the end of the conveyor.\ncommunicate_with_next_agent(): Send a message to the next agent in the production line to coordinate the handover or next steps.\nrelease_ready_workpiece_to_next_agent(): release the workpiece at the ready position to the next agent and transfer the control of this workpiece to the next agent.\nwait(duration_in_second): Pauses the current operation for a set duration in seconds.\nsend_alert_to_human_supervisor(): Alerts a human supervisor about issues.\npass(): Executes no operation, allowing the system to bypass this command without making any change\nStandard Operation Procedure:\nThe process begin with a workpiece arriving at the entrance of the conveyor.\n1. If sensor BG56 detect an object, it indicates that a workpiece is detected at the entrance position. You should call activate_conveyor(forward, 10) to set the conveyor moving forward for 10 seconds, to transport the workpiece to the ready position.\n2. If sensor BG51 detect an object, it indicates that a workpiece is detected at the ready position. You should call activate_material_holder() to secure the workpiece in place, ensuring that the workpiece is securely positioned.\n3. If the workpiece is detected at the ready position and is being held, you should call rfid_read() to read the workpiece information, to determine whether the workpiece is suitable for further processing.\n4. If the workpiece information checks out OK, you should call ask_next_operator() to determine the status of the next operator agent, in order to decide whether to wait or to forward the workpiece to the next operator agent.\n5. If the next operator is busy, then call wait(5) to wait for 5 seconds before calling ask_next_operator() again to check the"}]}