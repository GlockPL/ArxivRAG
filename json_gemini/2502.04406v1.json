{"title": "Calibrated Physics-Informed Uncertainty Quantification", "authors": ["Vignesh Gopakumar", "Ander Gray", "Lorenzo Zanisi", "Timothy Nunn", "Stanislas Pamela", "Daniel Giles", "Matt J. Kusner", "Marc Peter Deisenroth"], "abstract": "Neural PDEs offer efficient alternatives to computationally expensive numerical PDE solvers for simulating complex physical systems. However, their lack of robust uncertainty quantification (UQ) limits deployment in critical applications. We introduce a model-agnostic, physics-informed conformal prediction (CP) framework that provides guaranteed uncertainty estimates without requiring labelled data. By utilising a physics-based approach, we are able to quantify and calibrate the model's inconsistencies with the PDE rather than the uncertainty arising from the data. Our approach uses convolutional layers as finite-difference stencils and leverages physics residual errors as nonconformity scores, enabling data-free UQ with marginal and joint coverage guarantees across prediction domains for a range of complex PDEs. We further validate the efficacy of our method on neural PDE models for plasma modelling and shot design in fusion reactors.", "sections": [{"title": "1. Introduction", "content": "Numerical PDE solvers are essential tools in scientific and engineering simulations (Danabasoglu et al., 2020; Giudicelli et al., 2024), yet their computational demands and environmental impact remain significant challenges (Horwitz, 2024). Machine learning approaches have emerged as efficient alternatives (Bertone et al., 2019; Karniadakis et al., 2021), successfully deployed across weather forecasting (Lam et al., 2023; Kurth et al., 2023), fluid dynamics (Jiang et al., 2020; Pfaff et al., 2021), and nuclear fusion applications (Poels et al., 2023; Carey et al., 2024; Gopakumar & Samaddar, 2020). Neural PDE solvers provide rapid approximations but present a critical cost-accuracy trade-off."}, {"title": "2. Related Work", "content": "Recently, CP, as a method of performing UQ, has been gaining popularity for usage with spatio-temporal data Sun (2022). Several works have explored the inductive CP framework for spatial and sequential data (Stankeviciute et al., 2021; Xu & Xie, 2021; Xu et al., 2023), including in the operator space (Ma et al., 2024). In Gopakumar et al. (2024a), the marginal-CP framework is extended to pre-trained as well as to fine-tuned surrogate models for physical system modelling across an infinite-dimensional setting. Alternatively, error bounds for PDE surrogates have been devised by Gray et al. (2025) using set propagation to project the singular value decomposition of the prediction error to the prediction space."}, {"title": "3. Background", "content": null}, {"title": "3.1. Neural PDE Solvers", "content": "Consider the generic formulation of a PDE modelling the spatio-temporal evolution of n field variables u \u2208 Rn across a range of initial conditions:\nD = D\u2081(u) + Dx(u) = 0,  X\u2208\u03a9, t\u2208 [0,T], (1)\nu(X,t) = g,  X\u2208\u2202\u03a9, (2)\nu(X, 0) = \u03b1(\u03bb, X). (3)\nHere, X defines the spatial domain bounded by \u03a9, [0, T] the temporal domain, Dx and Dt, the composite operators of the associated spatial and temporal derivatives. The PDE is further defined by the boundary condition g and initial condition a, which can be parameterised by \u03bb. The set of solutions of field variables are expressed as u \u2208 U.\nNeural PDE solvers as surrogate models aim to learn the behaviour governed by Equation (1) using a parameterised neural network NN. Starting from the initial conditions, the network is trained to solve the spatio-temporal evolution of the fields given by \u03a9U [0,T]. Neural operators NO are a special class of neural networks that learn the operator mapping from the function space of the PDE initial conditions a \u2208 A to the function space of solutions u \u2208 U. A neural operator for solving an initial-value problem can be expressed as\nU = NO\u0473(A),\nu(X,t) = NOo (u(X, 0),t),  X\u2208\u03a9, t\u2208 [0,T]. (4)\nA Fourier Neural Operator (FNO) is an autoregressive neural operator that learns the spatio-temporal evolution of PDE solution by leveraging the Fourier transform as the kernel integrator (Li et al., 2021). The field evolution is learned using tuneable weight matrices of the network, parameterised directly in the Fourier space of the PDE solutions.\nSince CP and our extension of it provide a post-hoc measure of quantifying the uncertainty of a neural PDE, it remains agnostic to model choice and training conditions. Considering the model independence of our approach, we restrict our experiments to modelling PDEs with an FNO. The FNO is chosen due to its cost-accuracy trade-off and efficiency as demonstrated by de Hoop et al. (2022) and Gopakumar et al. (2023b). CP over a range of neural-PDE solvers has been applied by Gopakumar et al. (2024a), who also demonstrate that the coverage guarantees are upheld irrespective of the model choice, not needing us to experiment with various model architectures."}, {"title": "3.2. Conformal Prediction", "content": "Calibrated Physics-Informed UQ\nCalibrated Physics-Informed UQ\nConformal prediction (CP) (Vovk et al., 2005; Shafer & Vovk, 2008) is a statistical framework that addresses the accuracy of a predictive model. Consider a machine learning model f : X \u2192 Y trained on a dataset (Xi, Yi)1, that can be used to predict the next true label Yn+1 at query point Xn+1. CP extends the point prediction P : Yn+1 to a prediction set Ca, ensuring that\nP(Yn+1 \u2208 C) \u2265 1 \u2212 \u03b1. (5)\nThis coverage guarantee, a function of the user-defined confidence level a, holds irrespective of the chosen model and training dataset. The only condition is that the calibration samples and the prediction samples are exchangeable. Traditional inductive CP partitions the labelled data into training and calibration sets (Papadopoulos, 2008). The performance of the model on the latter, measured using a nonconformity score, is used to calibrate the model and obtain prediction sets.\nConventionally, nonconformity scores act on the model predictions and a labelled dataset (Kato et al., 2023). For deterministic models, they are often formulated as the Absolute Error Residual (AER) of the model predictions f(X) and targets Y. For probabilistic models, the score function (STD) is the absolute error of the prediction means f(X) and the targets Y, normalised by the standard deviation of the prediction (f(X)). Having obtained a distribution of nonconformity scores \u015d of the calibration dataset (Xi, Yi)1, a quantile \u011d corresponding to the desired coverage 1 a is estimated from its cumulative distribution function F (Papadopoulos, 2008):\nqa = F-1(1(n+1)(1-a)1). (6)\nThe quantile estimates the error bar associated with desired coverage and is combined with the new prediction to obtain the prediction sets. The nonconformity score functions and their prediction sets for AER and STD are given in Table 1."}, {"title": "4. Physics Residual Error (PRE)", "content": "We introduce a novel data-free nonconformity score based directly on the PDE for surrogate models. The Physics Residual Error (PRE) is defined as the PDE residual (Saad & Schultz, 1986) estimated over the discretised PDE solution obtained from the surrogate model. For an abstract PDE as in Equation (1), the PDE residual is the evaluation of the composite differential operator D. The PDE residual is treated as a score function by taking its L1 norm as indicated in Table 1. While well-defined PDEs have solutions obeying Equations (1) to (3), numerical solutions often fail to converge to the true solution (Pinder, 2018). Neural PDEs,"}, {"title": "4.1. Marginal-CP", "content": "The CP formulation was initially conceptualised for calibrating univariate functions with single-point outputs (Vovk et al., 2005). It has recently been extended to spatio-temporal data, with multi-dimensional outputs with an immutable tensor structure (Gopakumar et al., 2024a). Within such spatio-temporal settings, CP has been implemented to provide marginal coverage, i.e. the calibration procedure provides independent error bars for each cell within the spatio-temporal domain. For an output tensor Y\u2208RN\u00a4\u00d7\u00d1y\u00d7Nt, where Nx, Ny, Nt represent the spatio-temporal discretisation of the domain, marginal-CP uses the non-conformity scores outlined in Table 1 across each cell of Y to obtain error bars, which will be compliant with Equation (5) for each cell. Marginal-CP using PRE helps indicate regions within a single prediction that lie outside the calibrated bounds of physics violation and require specific attention, treating those predictions with caution."}, {"title": "4.2. Joint-CP", "content": "The joint-CP formulation constructs a calibration procedure that provides coverage bands for multivariate functions. These coverage bands expand across the entire simulation domain \u03a9 \u00d7 [0, T] (discretised as RN\u300f \u00d7 Ny\u00d7Nt) rather than an individual cell within it. For a coverage band C\u00ba, the joint-CP formulation ensures that 1 a predictions/solutions lie within the bounds. For performing joint-CP, the non-conformity scores are modified to reflect the supremum of the score functions S in Table 1. They are modulated by the standard deviation o of the calibration scores (Diquigiovanni et al., 2021) to obtain prediction bands with varying widths based on local behaviour (Diquigiovanni et al., 2022). The modifications of the score functions and prediction sets to perform CP are given by\nS = sup(S\\\u03c3(S)); (7)\n\u03a7\u0395\u03a9, \u03c4\u03b5 [0,1]\nC\u00ba = P \u00b1 \u00ce\u00ba\u00b7 \u03c3(S), (8)\nwhere S and P are the formulations of the nonconformity scores and the prediction at Xn+1 used for marginal-CP as shown in Table 1. Joint-CP becomes particularly useful"}, {"title": "4.3. Differential Operator: Finite-Difference Stencils as Convolutional Kernels", "content": "Calibrating neural PDEs using PRE nonconformity scores requires frequent evaluations of the composite differential operator D in Equation (1). For PDEs, this involves estimating numerous spatio-temporal gradients across the discretised domain, ranging from millions in simple cases to billions of gradient operations for complex physics. To address this computational challenge, we developed a scalable gradient estimation method for evaluating physics residual error.\nWe employ convolution operations with Finite Difference (FD) stencils as convolutional kernels for gradient estimation (Actor et al., 2020; Chen et al., 2024a;b). For instance, the 2D Laplacian operator \u22072, using a central difference scheme with discretisation h, can be approximated by\n\u22072\u2248 1/h2 [0 1 0, 1 -4 1, 0 1 0] (9)\nand used as a kernel. This approach is justified by the mathematical equivalence of FD approximations and discrete convolutions. Both represent matrix-vector multiplications of a block Toeplitz matrix with a field vector (Strang, 1986; Fiorentino & Serra, 1991).\nThe FD approximation offers several advantages over Automatic Differentiation (AD) for our application. It is compatible with CP as a post-hoc measure, requires no architectural modifications, and is model-agnostic. Furthermore, FD implemented via convolutions is more memory-efficient than AD, which requires storing the entire computational graph."}, {"title": "5. Experiments", "content": "PRE-CP experiments comprise two campaigns. First, we benchmark PRE-CP within standard neural PDEs (Section 5.1 to 5.3). The calibration process (Figure 2) involves: (a) sampling model inputs, (b) calculating PRE(s) scores, and (c) calibrating physical error using marginal and joint-CP formulations. Validation uses the same PDE condition bounds as calibration. This campaign demonstrates our method's superior computational efficiency and guaranteed coverage versus other Neural-PDE UQ measures (Appendix C).\nThe second campaign (Section 5.4, 5.5) applies PRE-CP to fusion applications. We enhance tokamak plasma behaviour surrogate models to identify erroneous dispersion regions (Section 5.4) and integrate PRE-CP with tokamak design surrogates to identify viable designs and areas needing additional simulations (Section 5.5). This campaign demonstrates the utility of PRE-CP in complex, practical applications. Reproducible One-dimensional PDE experiments demonstrating PRE-CP are demonstrated in Appendix F."}, {"title": "5.1. Wave Equation", "content": "The two-dimensional wave equation is given by\nOt2 = C2(\u22022u/\u2202x2 +\u22022u/\u2202y2) (10)\nWe solve Equation (10) within the domain x, y \u2208 [\u22121,1], t\u2208 [0, 1.0] with c = 1.0 using a spectral solver with periodic boundary conditions (Canuto et al., 2007). The initial conditions are parameterised by the amplitude and position of a Gaussian field. A 2D FNO is trained on this data to predict 20-time steps ahead autoregressively from a given initial state."}, {"title": "5.2. Navier-Stokes Equation", "content": "Consider the two-dimensional Navier-Stokes equations\n\u2207\u00b7\u2207 = 0 (Continuity equation) (11)\nv + (v \u2022 \u2207)v = v\u2207\u2207 \u2013 \u2207P (Momentum equation) (12)\nwhere we are interested in modelling the evolution of the velocity vector (v = [u, v]) and pressure (P) field of an incompressible fluid with kinematic viscosity (v). For data generation, Equations (11) and (12) are solved on a domain x\u2208 [0,1], y \u2208 [0,1], t\u2208 [0,0.5] using a spectral-based solver (Canuto et al., 2007). A 2D multi-variable FNO (Gopakumar et al., 2024b) is trained to model the evolution of velocity and pressure autoregressively up until the 20th time instance."}, {"title": "5.3. Magnetohydrodynamics", "content": "Consider the magnetohydrodynamic (MHD) equations\n\u03b8\u03c1 + (pv) = 0 (Continuity equation) (13)\nOt\n\u03c1V + (v\u00b7 V)v = 1 B \u00d7 (\u2207 \u00d7 B) \u2013 VP\n(Momentum equation) (14)\nOt \u041c\u043e\ndP (v 8) = 0 (Energy equation) (15)\ndt pr\n\u018fB = \u221a \u00d7 ( \u00d7 B) (Induction equation) (16)\nOt\nVB = 0 (Gau\u00df law for magnetism) (17)\nwhere the density (p), velocity vector = [u, v]) and the pressure of plasma is modelled under a magnetic field (B = [Bx, By]) across a spatio-temporal domain x,y \u2208 [0,1]2, t \u2208 [0,5]. \u03bc\u03bf is the magnetic permeability of free space. Equations (13) to (17) represent the ideal MHD"}, {"title": "5.4. Plasma Modelling within a Tokamak", "content": "In (Gopakumar et al., 2024b), the authors model the evolution of plasma blobs within a fusion reactor (known as a tokamak) using an FNO. They explore the case of electrostatic modelling of reduced magnetohydrodynamics with data obtained from the JOREK code (Hoelzl et al., 2021). In the absence of magnetic pressure to confine it, the plasma, driven by kinetic pressure, moves radially outward and collides with the wall of the reactor. The plasma is characterised by density p, electric potential & and Temperature T, and the FNO models their spatio-temporal evolution autoregressively. Borrowing upon their pre-trained model and utilising the reduced-MHD equations within the toroidal domain, we demonstrate obtaining calibrated error bars using PRE-CP at scale. The FNO demonstrated in (Gopakumar et al., 2024b) is able to model the plasma six orders of magnitude faster than traditional numerical solvers, and by providing calibrated error bars over the predictions, a wider range of plasma configurations can be validated.\nWe focus on the temperature equation within reduced-MHD (equation 3 within (Gopakumar et al., 2024b)) as it comprises all the variables associated with the plasma."}, {"title": "5.5. Magnetic Equilibrium in a Tokamak", "content": "Tokamaks confine plasma within a toroidal vessel using magnetic fields to achieve nuclear fusion. The plasma, at high temperatures, is contained by magnetic fields that counterbalance its kinetic pressure. This equilibrium state, a function of magnetic coil configurations and plasma parameters, is governed by the Grad-Shafranov (GS) equation (Somov, 2012):\ndy/dr2 + 1/r dy/dr - d2\u03c8/dz2 = -\u03bc\u03bf\u03b32.(dp/dy + 1/2 dy/dy) (18)\nwhere & represents the poloidal magnetic flux, p the kinetic pressure, F = rB the toroidal magnetic field, and \u03bc\u03bf the magnetic permeability. While traditional numerical solvers like EFIT++ and FreeGSNKE (Lao et al., 1985; Amorisco et al., 2024) are used for equilibrium reconstruction, their computational cost has motivated neural network alternatives (Joung et al., 2023; Jang et al., 2024). However, these surrogate models lack uncertainty quantification capabilities.\nWe implement an auto-encoder that maps poloidal magnetic flux across the poloidal cross-section for given tokamak architectures, conditioned on poloidal field coil locations under constant plasma current. While this accelerates simulation by 10000x, it lacks physical guarantees. By incorporating Equation (18) within the PRE-CP framework, we identify physically stable equilibria and obtain statistically valid error bounds."}, {"title": "6. Discussion", "content": "If \"All models are wrong, but some are useful\" (Box, 1976), through this work, we explore a novel framework for providing data-free, model and domain agnostic measure of usefulness of neural PDEs. We deploy a principled method of evaluating the accuracy of the solution, i.e. its (calibrated) obedience to the known physics of the system under study. As opposed to other methods of UQ for neural PDEs, our method is physics-informed, allowing us to study the physi-"}, {"title": "Strengths", "content": "PRE estimates the violation of conservation laws in neural PDE predictions, guaranteed error bounds over the physics deviation. This post-hoc uncertainty quantification is model- and physics-agnostic, scaling linearly with model complexity and quasi-linearly with PDE complexity due to the additive nature of differential operators. Our framework reformulates CP to be data-free, expressing model inaccuracy solely through PRE, not requiring a labelled dataset. This approach reduces calibration costs and loosens exchangeability restrictions as we can modify the calibration and, hence, the prediction domain by simply reformulating the PRE accordingly. The PRE formulation (Section 4, Appendix B) yields input-independent prediction sets, allowing for the identification of weak predictions within single simulations (marginal-CP) and across multiple predictions (joint-CP). The latter enables a rejection criterion for a set of predictions potentially serving as an active-learning pipeline for neural PDE solvers (Musekamp et al., 2024). PRE-CP provides guaranteed coverage irrespective of the model, chosen discretisation, or the PDE of interest; however, the width of the error bar indicates quantitative features in the model quality. A well-trained model will exhibit tighter error bars as opposed to a poorer fit model, as is demonstrated in Appendix I."}, {"title": "Limitations", "content": "Our method's coverage bounds exist in the PDE residual space rather than the Euclidean space of physical variables. Transforming to physical space involves challenging set propagation through integral operations, which may require approximations (Teng et al., 2023) or expensive Monte Carlo sampling (Andrieu et al., 2003). The data-free approach lacks a grounding target for calibration, though we argue that a large sample of model outputs provides a statistically significant overview of uncertainty. The sampling cost from the neural-PDE solver for calibration involves intensive gradient evaluations. PRE estimation using finite-difference stencils also introduces the errors associated with Taylor expansion. The current formulation is limited to regular grids with fixed spacing, though extensions to un-"}, {"title": "7. Conclusion", "content": "We address the problem of reliability of neural-PDE solvers by proposing PRE-CP, a novel conformal prediction framework. Our method provides guaranteed and physics-informed uncertainty estimates for each cell within a prediction, identifying erroneous regions while discerning physically inconsistent predictions across the entire spatio-temporal domain. Our work enhances the reliability of neural PDE solvers, potentially broadening their applicability in science and engineering domains where robust uncertainty quantification is crucial."}, {"title": "A. Theorem: Data-Free CP", "content": "Preliminaries: Let D : Rm \u2192 Rm be a physics residual operator mapping a function to its PDE residual value, where: {X}=1 is the calibration set, f is the model, \u011d\u00ba is estimated as the [(n + 1)(1 \u2212 a)]/n -quantile of {|D(f(X))}=1\nTheorem A.1. If the residuals {D(f(X))}+1 are exchangeable random variables, then for any significance level a \u2208 (0,1) and any new input Xn+1 we have the following coverage guarantee:\nP(|D(f(Xn+1))| \u2208 Ca) \u2265 1 \u2212 a; Ca = [-\u011da, \u00cea]\nProof. Let R\u2081 = |D(f(Xi))| for i = 1,...,n + 1. We have, by assumption, (R\u2081, . . ., Rn, Rn+1) is an exchangeable sequence. Define the rank \u03c0of Rn+1 w.r.t. all other residuals:\n\u03c0(Rn+1) = |{i = 1, ..., n + 1 : Ri \u2264 Rn+1}|\nBy exchangeability, the rank \u03c0(Rn+1) is uniformly distributed over {1, . . ., n + 1}. Therefore,\nP(\u03c0(Rn+1) \u2264 [(n+1)(1-a)]) = [(n + 1)(1 \u2212 a)]/n > 1-\u03b1.\nBy construction of \u011d\u00ba we have that,\n{\u03c0(Rn+1) \u2264 [(n + 1)(1 \u2212 a)]} \u2286 {Rn+1 \u2264 \u011d\u00ba}.\nPutting this together,\nP(|D(f(Xn+1))| \u2264 \u011d\u00ba) = P(Rn+1 \u2264 \u011d\u00ba) \u2265 1 \u2212 a,\nwhich completes the proof."}, {"title": "B. PRE: Score Function and Prediction Sets", "content": "For a general nonconformity score S, the prediction set for a new input Xn+1 is typically defined as:\nC\u00ba(Xn+1) = {y : S(Xn+1,y) \u2264 \u00ee\u00ba},\nwhere \u011d\u00ba is the (1-a)-quantile of the nonconformity scores on the calibration set.\nFor AER and STD, the nonconformity scores depend on both the input X and the output (target) Y:\nSAER(X,Y) = |f(X) \u2013 Y\\,,\nSSTD(X,Y) = |f\u03bc(X) \u2013 Y|/fo(X)\nThe resulting prediction sets are:\nCAER(Xn+1) = [f(Xn+1) \u2013 \u011d\u00ba, f(Xn+1) + \u011d\u00ba],\nCSTD(Xn+1) = [f\u00b5(Xn+1)-\u00ce\u00bafo(Xn+1), f\u03bc(Xn+1)+q\u00ba fo(Xn+1)].\nThese prediction sets clearly depend on the input Xn+1.\nFor PRE, the nonconformity score depends only on the model output and not on the target:\nSPRE(f(X)) = |D(f(X)) \u2013 0|,\nwhere D is the PDE residual operator. The key difference is that the true output Y for PRE, irrespective of the PDE is always 0 and does not depend on the input X. PRE is a measure of how well the model output satisfies the physics rather than how it fits certain data. Hence, we can formulate a nonconformity score that is data-free and eventually leads to input-independent prediction sets as given below.\nFor PRE, we can reframe the prediction set definition:\nCPRE = {f(X) : |D(f(X))| \u2264 \u00ee\u00ba}.\nThis set is not defined in terms of the true Y values but in terms of the allowable model outputs f(X) that satisfy the PDE residual constraint. Thus, the prediction set can be expressed as:\nCPRE = [-1, \u00ce\u00ba]."}, {"title": "C. Comparison to Other UQ Methods", "content": "Within this section, we compare our method (PRE-CP) with other methods of providing uncertainty estimation neural-PDEs. We compare various Bayesian methods (MC Dropout, Deep Ensembles, Bayesian Neural Networks, Stochastic Weighted Averaging) along with standard inductive conformal prediction to our method and demonstrate that our method is capable of providing valid coverage guarantees for both in and out-of-distribution testing without a significant increase in inference times. Table 2 provides a qualitative comparison of our method against the other benchmarks and highlights that our method is data-free, requires no modification or sampling and provides guaranteed coverage in a physics-informed manner. For the sake of simplicity, we confine our comparison studies in Table 3, 4, and 5 to the Wave, Navier-Stokes and Magnetohydrodynamic equations. The index for the following tables is given below.\nTable Index\nDeterministic: Vanilla FNO (Li et al., 2021)\nMC Dropout: FNO with Dropout (Gal & Ghahramani, 2016)\nDeep Ensemble: Ensemble of FNOs (Lakshminarayanan et al., 2017)\nSWA-G: Stochastic Weighted Averaging - Gaussian (Maddox et al., 2019)\nin-distribution: Model evaluated on initial states sampled from the same parameter range (as given in the appendix) of the initial condition as used in the training data.\nout-distribution: Model evaluated on initial states sampled from a different parameter range of the initial conditions as used in the training data.\nL2: L2 norm of the model output with the ground truth in the normalised domain.\nCoverage: Percentage coverage of the model outputs within the estimated error bounds\nTrain Time: Training time on a single A100 GPU.\nEval. Time: Evaluation time on a single A100 GPU."}, {"title": "D. ConvOperator: Convolutional Kernels for Gradient Estimation", "content": "Within the code base for this paper, we release a utility function that constructs convolutional layers for gradient estimation based on your choice of order of differentiation and Taylor approximation. This allows for the PRE score function to be easily expressed in a single line of code 2\nThis section provides an overview of the code implementation and algorithm for estimating the PRE using Convolution operations. We'll use an arbitrary PDE example with a temporal gradientu and a Laplacian to illustrate the process.\n\u03b1 Du/Dt = (\u2202^2u/dx^2 + \u2202^2u/dy^2 ) + \u03b2u = 0, (19)\nwhere u is the field variable, t is time, x and y are spatial coordinates, and a and \u03b2 are constants. To estimate the PDE residual given by Equation (19), we need to estimate the associated spatio-temporal gradients.\nFirst, we use the ConvOperator class from Utils/ConvOps_2d.py to set up the convolutional layer with kernels taken from the appropriate finite difference stencils:\nThe convolutional kernels are additive i.e. in order to estimate the residual in one convolutional operation, they could be added together to form a composite kernel that characterises the entire PDE residual.\nOnce having set up the kernels, PRE estimation is as simple as passing the composite class instance D the predictions from the neural PDE surroga te (ensuring that the output is in the same order as the kernel outlined above)."}, {"title": "D.1. Impact of Discretisation", "content": "As demonstrated in (Bartolucci et al., 2023), the discretisation of the inputs and hence model outputs plays an important role in the accuracy of the neural-PDE solvers. Though the neural operators are constructed for discretisation-invariant behaviour due to the band-limited nature of the functions, they often exhibit discretisation-convergent behaviour rather than be fully discretisation-invariant. This is of particular importance in the temporal dimensions as these neural-PDE models utilise a discrete, autoregressive based time-stepping and is baked into the model within its training regime (McCabe et al., 2023). Due to lack of control in the discretisation within the temporal domain (dt), the PRE estimates tend to have higher numerical errors as well. In fig. 9, we visualise the evaluation of finite difference in 2D+time as a 3D convolution. The finite difference stencil i.e. the convolutional kernel has a unit discretisation of dx, dy and dt associated with the problem and is applied over the signal i.e. the output from the neural-PDE u spanning the domain x, y, t, where x \u2208 [0, X], y \u2208 [0, Y], t \u2208 [0, T]."}, {"title": "E. Initial and Boundary Conditions", "content": "As mentioned in Section 4.3, the focus of our experiments has been in quantifying the misalignment of the model with the PDE in the domain of the problem. A well-defined PDE is characterised by the PDE on the domain, the initial condition across the domain at t = 0 and the boundary conditions, reflecting the physics at the boundary. Within a neural-PDE setting, the initial condition does not need to be enforced or measured for as the neural-PDE is set up as an initial-value problem, taking in the initial state to autoregressively evolve the later timesteps and hence does not come under the purview of the neural-PDE's outputs. The boundary conditions, whether Dirichlet, Neumann or periodic, follows a residual structure as outlined in Equation (2), allowing us to use it as a PRE-like nonconformity score for performing conformal prediction. In all the problems we have under consideration, the PDEs are modelled under periodic boundary conditions:\n\u2202u/\u2202x = 0; X \u0395 \u0398\u03a9 (20)"}, {"title": "F. Toy Problems: 1D cases", "content": null}, {"title": "F.1. Advection Equation", "content": "Consider the one-dimensional advection equation\nOt + v = 0. (21)\nThe state variable of interest u is bounded within the domain x\u2208 [0,2], t \u2208 [0,0.5] and moves within the domain at a constant velocity v. Data generation is performed by solving Equation (21) using a Crank-Nicolson method (Crank & Nicolson, 1947). Data is sampled using a parameterised initial condition that characterises the amplitude and position of the Gaussian field. Generated data is used to train a 1D FNO that takes in the initial condition and autoregressively with a step size of 1, learns to map the next 10 time frames. A reproducible script is attached to the supplementary material."}, {"title": "F.2. Burgers Equation", "content": "Consider the 1D Burgers' Equation\n+ \u0438 \u2014 = v . (22)\nThe state variable of interest u is bounded within the domain x\u2208 [0,2], t\u2208 [0,1.25]. The field is prescribed by a kinematic viscosity \u03bd = 0.002. Data is generated by solving Equation (22) using a spectral method (Canuto et al., 2007). Data sampled using a parameterised initial condition is used to train a 1D FNO that takes in the initial distribution of the state and learns to autoregressively predict the PDE evolution for the next 30 time frames."}, {"title": "G. 1D Advection Equation", "content": null}, {"title": "G.1. Physics", "content": "Consider the one-dimensional advection equation, parameterised by the initial condition:\n+ \u0438 \u2014 = 0, x\u2208 [0,2], t\u2208 [0,0.5],\nu(x, t = 0) = Ae\u2212(x\u2212X)2. (23)\nHere u defines the density of the fluid, x the spatial coordinate, t the temporal coordinate and v the advection speed. initial condition is parameterised by A and X, representing the amplitude and position of a Gaussian distribution. A no-flux boundary condition bounds the system.\nThe numerical solution for the above equation is built using a finite difference solver with a crank-nicolson method implemented in Python. We construct a dataset by performing a Latin hypercube sampling across parameters A, X. Each parameter is sampled from within the domain given in Table 6 to generate 100 simulation points, each with its own initial condition. Each simulation is run for 50-time iterations with a \u2206t = 0.01 across a spatial domain spanning [0,2], uniformly discretised into 200 spatial units in the x-axis."}, {"title": "G.2. Model and Training", "content": "We use a one-dimensional FNO to model the evolution of the convection-diffusion equation. The FNO learns to perform the mapping from the initial condition to the next time instance, having a step size of 1. The model autoregressively learns the evolution of the field up until the 10th time instance. Each Fourier layer has 8 modes and a width of 16. The FNO architecture can be found in Table 7. Considering the field values governing the evolution of the advection equation are relatively small, we avoid normalisations. The model is trained for up to 100 epochs using the Adam optimiser (Kingma & Ba, 2015) with a step-decaying learning rate. The learning rate is initially set to 0.005 and scheduled to decrease by half after every 100 epochs. The model was trained using an LP-loss (Gopakumar et al., 2024b)."}, {"title": "G.3. Calibration and Validation", "content": "To perform the calibration as outlined in Section 5, model predictions are obtained using initial conditions sampled from the domain given in Table 6. The same bounded domain for the initial condition parameters is used for calibration and validation. 100 initial conditions are sampled and fed to the model to obtain and prediction for both the calibration and the validation."}, {"title": "H. 1D Burgers Equation", "content": null}, {"title": "H.1. Physics", "content": "Consider the one-dimensional Burgers' equation:\n+u=\u03bd, (24)\nwhere u defines the field variable, v the kinematic viscosity, x the spatial coordinate, t the temporal coordinates. \u03b1, \u03b2 and y are variables that parameterise the initial condition of the PDE setup. The"}]}