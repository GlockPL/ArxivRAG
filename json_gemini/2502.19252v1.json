{"title": "GRAPHBRIDGE: TOWARDS ARBITRARY TRANSFER LEARNING IN GNNS", "authors": ["Li Ju", "Xingyi Yang", "Qi Li", "Xinchao Wang"], "abstract": "Graph neural networks (GNNs) are conventionally trained on a per-domain, per-task basis. It creates a significant barrier in transferring the acquired knowledge to different, heterogeneous data setups. This paper introduces GraphBridge, a novel framework to enable knowledge transfer across disparate tasks and domains in GNNs, circumventing the need for modifications to task configurations or graph structures. Specifically, GraphBridge allows for the augmentation of any pre-trained GNN with prediction heads and a bridging network that connects the input to the output layer. This architecture not only preserves the intrinsic knowledge of the original model but also supports outputs of arbitrary dimensions. To mitigate the negative transfer problem, GraphBridge merges the source model with a concurrently trained model, thereby reducing the source bias when applied to the target domain. Our method is thoroughly evaluated across diverse transfer learning scenarios, including Graph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empirical validation, conducted over 16 datasets representative of these scenarios, confirms the framework's capacity for task- and domain-agnostic transfer learning within graph-like data, marking a significant advancement in the field of GNNs. Code is available at https://github.com/jujulili888/GraphBridge.", "sections": [{"title": "1 INTRODUCTION", "content": "With the explosive growth of graph data, the application of Graph Neural Networks (GNNs) has become increasingly widespread in domains (Jing et al., 2022; 2021b; Wu et al., 2020b; Yu et al., 2018; Shah et al., 2020) such as recommendation systems (Gao et al., 2022; Wu et al., 2019) and biopharmaceutics(Zitnik et al., 2018; Rathi et al., 2019). Despite their growing popularity, the effective implementation of GNNs often requires significant training efforts and substantial memory resources. This poses challenges for their practical application in diverse settings. To address these constraints, recent research has focused on reusing pre-trained GNN models (Jing et al., 2023; Yang et al., 2022; Jing et al., 2021a; Deng & Zhang, 2021; Hu et al., 2019a; Sun et al., 2022b; Yang et al., 2020). This approach aims to reduce the need for extensive training, thereby lessening the associated time and resource demands to alleviate the extra training expense.\nHowever, to date, these efforts have not been entirely practical, primarily due to two forms of heterogeneity in graph data. The first is task heterogeneity. The intrinsic non-Euclidean nature of graph data allows for its application across a range of tasks, including graph-level, node-level, and edge-level predictions. However, the graph pre-training paradigm typically assumes consistency between the tasks used in pre-training and those in downstream applications. This becomes problematic when adapting GNNs to new tasks with distinct output formats and knowledge requirements. In such scenarios, GNNs may not perform optimally, as the pre-training might not align well with the demands of these novel tasks.\nBeyond task heterogeneity, domain heterogeneity also poses a significant challenge in transferring knowledge effectively within graph data applications. This refers to the significant differences in node features, connection patterns, and topology across various graph datasets. Consequently, a GNN trained on one specific dataset might struggle to generalize effectively to other datasets with"}, {"title": "2 RELATED WORK", "content": "Parameter Efficient Transfer Learning (PETL). Parameter Efficient Transfer Learning (PETL) is a branch of transfer learning focusing on reducing the computational cost of adapting pre-trained models to new tasks by avoiding updates to the entire parameter set. A popular PETL approach involves delta tuning, which introduces trainable parameters and tuning them. Techniques like adapters (Houlsby et al., 2019; Sung et al., 2022b; Zhang et al., 2021a), LoRA(Hu et al., 2021) and side-tuning(Zhang et al., 2020; Sung et al., 2022a) exemplify this approach. In contrast to adding new parameters to the pre-trained model, prompt-based tuning(Lester et al., 2021; Zhou et al., 2022; Li & Liang, 2021; Li et al., 2023a; 2024) introduces trainable parameters to the input, while keeping the pre-trained model unchanged during training. As GNN pre-training methods(Hu et al., 2019b; Zhu et al., 2021b; Xia et al., 2022; Zhu et al., 2021a; Jin et al., 2020) emerge, the PETL paradigm has gained attention in the GNN domain. Researchers have successfully transferred adapter(Li et al., 2023b) and prompting-based methods(Sun et al., 2023; 2022a) to GNNs. This paper aims to fully\nexploit the flexible side-tune structure, designing an efficient graph side-tune method to address severe negative transfer challenges in graph domain tasks.\nGraph Domain Adaptation. Domain adaptation, a subtopic of transfer learning, seeks to alleviate the negative impact of domain drift in transferring knowledge from a source to a target domain(Pan & Yang, 2009). Particularly prominent in the visual domain, extensive research has been devoted to this area(Ganin et al., 2016; Tan et al., 2017; Long et al., 2015; 2018; Zhuang et al., 2015; Pei et al., 2018). Recent advancements have introduced methods addressing graph domain adaptation tasks, broadly classified into discrepancy-based(Pilanc\u0131 & Vural, 2020; Vural, 2019), reconstruction-based(Wu et al., 2020a; Cai et al., 2021), and adversarial-based methods(Dai et al., 2022; Shen et al., 2020; Zhang et al., 2019). Despite their efforts to address heterogeneity in various graph knowledge domains for transfer learning, these methods are constrained to scenarios involving tasks at the same level. In contrast, our proposed end-to-end graph transfer learning framework, rooted in the pre-train-finetune paradigm, aims to transcend this constraint, enabling flexible graph transfer learning across diverse domains and tasks.\nUniversal Model. Beyond domain adaptation, it is meaningful for transfer learning to derive a universal model applicable to various downstream tasks, thereby significantly streamlining the process of model pre-training. The exploration of such universal models has been previously conducted in the domains of CV and NLP(McCann et al., 2018; Yu & Huang, 2019; Silver et al., 2021; Reed et al., 2022). Though the field has seen limited engagement, recent endeavors have emerged to develop universal models in non-Euclidean domains(Sun et al., 2023; Jing et al., 2023). In contrast to research on domain adaptation, these models fall short in bridging the substantial domain gaps inherent in different task transfer learning scenarios, leading to the failure to leverage the knowledge of pre-trained models across arbitrary tasks. To tackle the limitation, our framework incorporates the Graph-Merge-Side structure in the tuning stage, which effectively alleviates transfer biases present in the source domain, stemming negative transfer in universal learning."}, {"title": "3 METHODS", "content": "Achieving transfer learning across arbitrary graph task domains enables GNNs to comprehensively extract general knowledge, laying the foundation for efficient unsupervised graph learning and the development of universal graph model. However, arbitrary domain transfer learning presents challenges that need to be addressed. In this section, we begin by uncovering the key challenges of Arbitrary Graph Transfer Learning and outline how we address the challenges posed by various work scenarios I proposed in Figure 1. Particularly, we pinpoint two core problems in our mission: large gap domain adaptation and multi-tasks unification. To handle these challenges, we re-construct the pre-training-tuning framework for graph domains & tasks transfer learning. Additionally, we propose new resource-efficient tuning methods tailored for graphs to mitigate negative transfer."}, {"title": "3.1 CHALLENGES TOWARDS ARBITRARY GRAPH TRANSFER", "content": "* Multi Input & Output: The first issue is to fit various input dimensions and output forms of downstream tasks, considering that we only have a frozen pre-trained backbone without any additional dimensional transformation. For instance, the HIV molecular dataset has 8 input features and 1-dimensional output for graph classification, while the Cora network has 1433 input features and output of the same dimension as the number of nodes for node classification. To tackle the multi Input & Output dilemmas, we initially devised an end-to-end transfer learning framework capable of handling arbitrary input-output dimensions.\n* Domain Gap: The second challenge towards arbitrary graph transfer lies in the insufficient tuning methods' capacity to make good use of the knowledge from pre-trained models for adaptation in the target domain. In cases where there is a substantial domain gap, the efficacy of full-tuning diminishes, rendering resource-efficient tuning methods even appear as negative transfer. Therefore, we establish innovative graph side-tuning architectures that not only address the issue of negative transfer, but also ensure resource efficiency."}, {"title": "3.2 GRAPHBRIDGE FRAMEWORK", "content": "In this context, we introduce a pioneering two-stage graph transfer learning framework titled \"GraphBridge\", which facilitates an end-to-end pre-training-tuning transfer paradigm within the task scenarios outlined in our work. As shown in Figure A.5, our framework comprises a Pre-training Stage, aimed at extracting generalized graph knowledge, and a Tuning Stage dedicated to downstream tasks adaptation.\nPre-training Stage. In our work, we do not propose new graph pre-training methods. Instead, we design a versatile pre-training stage that can be adapted to various existing graph-level pre-training techniques (since they perform more effectively in graph knowledge learning(Hu et al., 2019b; Sun et al., 2023) in the realm of graph pre-training methods). This adaptability allows our framework to utilize any graph-level pre-training method to obtain the base model for tuning. In our experiments, we employ the effective methods, GraphCL and SimGRACE, for base model pre-training.\nTuning Stage. In the tuning stage, our framework comprises three components: Input Bridge, Efficient-tuning, and Output Bridge, which can be tailored to downstream task transfer learning with varying input and output formats.\n\u2022 INPUT BRIDGE adopt the Feat-Adapt methods mentioned above for input feature dimensional adaptation as well. In addition to the non-trainable adapters, we set a trainable linear layer as an adapter specifically for the point cloud dataset.\n\u2022 EFFICIENT-TUNING consists of a pre-trained backbone and tunable side networks for downstream task transfer on adapted inputs. In our work, we design a new graph-side-tuning paradigm which will be discussed in 3.3, instead of using the traditional fine-tuning.\n\u2022 OUTPUT BRIDGE serves as an output adapter for various graph tasks. It integrates several learn-able predictor heads tailored to different downstream tasks for graph embeddings obtained from the tuning process. This ensures the generation of appropriate output formats for various graph tasks. For instance, in a graph classification task, a pooling operation followed by a linear prediction head is employed to generate predictions for each graph. In contrast, in a node classification task, a linear head is directly used for predictions, ensuring that each node corresponds to a label."}, {"title": "3.3 GRAPH SIDE-TUNING", "content": "In the tuning stage, we introduced a novel graph side-tuning technique, enabling effective trans-fer learning of different graph tasks. On one hand, side-tuning showcases resource efficiency by maintaining performance with fewer parameter manipulations. On the other hand, the flexible ar-chitecture of side-tuning facilitates the design of solutions to address negative transfers occurring during large gap domain transfer. For tasks of varied difficulty levels shown in Figure 1, we devised two graph side-tuning methods to tackle the challenges: the elementary Graph Scaff Side-Tuning and the advanced Graph Merge Side-Tuning methods.\nGraph Scaff Side-Tuning (GSST). In the design of the Graph Side-tuning, we make an innovation for side network architectures. Unlike the original side-tuning, which used the distillation structure of the base model as the side network, we directly set the side network as a randomly initialised MLP. Such a configuration is pertinent: the high computational time overhead of the graph convo-lution layer can be circumvented when training the MLP solely using node features. Meanwhile, current research(Han et al., 2022; Zhang et al., 2021b) indicates that MLPs can exhibit graph learn-ing performance comparable to GNNs when guided by the knowledge from GNN models; Hence, the tuning efficiency can be further enhanced while transfer performance being ensured.\nThe tuning process of GSST is shown in Figure A.5. In our approach, a GNN based model with frozen parameters produces activations at each layer. These activations are then passed through a down-sampling layer and fused layer-wise with the outputs of a trainable MLP side-network. Thus, the loss for downstream task Ta can be formulated as:\n$\\mathcal{L}(X_{T_a}, y_{T_a}) = ||\\alpha_s \\cdot f_{gnn}(X_{T_a}, A_{T_a}; W_{GNN_{pre}}) + (1-\\alpha_s) \\cdot f_{mlp}(X_{T_a}; W_l) - y_{T_a}||_2$\nwhere {$X_{T_a}, A_{T_a}, y_{T_a}$} denotes the node features, adjacency matrix and labels of Ta. Moreover, the $f_{gnn}$ with pre-trained-init parameters $w$ and $f_{mlp}$ with random-init $w_l$ represent the output of the frozen base model and activated side network respectively, while as represents a set of fusion \u03b1 for each layer between base and side.\nAs such, Eq. 1 indicates that the proposed GSST fixes the parameters of the base model and adjusts the MLP side network parameters for optimization. Additionally, the alpha blending parameters, as well as the downsampling module for each layer, are updated during back-propagation. The smaller scale of the tuning space reflects the parameter efficiency of our methods. Moreover, by exclusively conducting back propagation on the side network, we avoid the need to compute and retain gradient values for the base model, presenting an additional memory-efficient attribute. At last, the GSST method demonstrates commendable performance in easy task scenarios and the experimental results of it are further elaborated in section 4.2.\nGraph Merge Side-Tuning (GMST). Nevertheless, in more challenging task scenarios, GSST proves inadequate in bridging the substantial gaps between diverse task domains and knowledge domains. To address the negative transfer problem that occurs when significant domain gap exists, we further propose a novel side-tuning architecture named Graph Merge Side-Tuning (GMST).\nIn theory, due to the substantial disparity in knowledge between the source domain and the target domain, the base model's involvement tends to trap the model in a local optimum. Therefore, it becomes imperative to mitigate the impact of bias from the source domain on the target domain. Here, we achieve this goal by introducing the backup model to the base-side and fusing it with the original pre-trained model. Specifically, we set up a backup network mirroring the structure of the"}, {"title": "4 EXPERIMENTS", "content": "We evaluate the performance of our GraphBridge on 16 publicly available benchmarks across four different scenarios defined in Figure 1:\n\u2022 Easy: Transfer learning between graph-level classification tasks within similar knowledge domains, a task frequently explored in existing research on graph pre-training and fine-tuning methods\n\u2022 Medium: Transfer learning between node classification tasks in unrelated knowledge domains.\n\u2022 Hard: Transfer learning between graph classification tasks and node classification tasks in unrelated knowledge domains.\n\u2022 Extension: For extension, we explored the transfer learning between traditional graph data and graph-like (point cloud) data.\nAs a preliminary study in this field, our goal is not to achieve state-of-the-art performance on all datasets. Instead, we aim to explore the feasibility of a graph-universal model across a diverse range of tasks and datasets."}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Datasets. The datasets employed in our experiments can be categorized based on task levels: graph-level tasks consist of ZINC-full, BACE, BBBP, ClinTox, HIV, SIDER, Tox21, MUV, ToxCast, which is a series of molecular graph datasets; node-level tasks include ogbn-arxiv, Cora, CiteSeer, PubMed, Amazon-Computers, Flickr, encompassing node classification datasets related to citation networks, product ranking networks, and social networks; point cloud tasks involve ModelNet10, which is a 10-classification point cloud dataset.\nModel Settings. In the Graph2Graph task, we employ a five-layer backbone architecture to facilitate the extraction of general knowledge from the extensive ZINC dataset. In the Node2Node, Graph2Node, and Graph2PtCld tasks, we consistently utilize a standard graph neural network structure comprising two-layer graph convolutions. For the backbones of the aforementioned model, we configure the hidden layer dimension of the base to be 100, while the hidden layer dimension of the side network is set to 16."}, {"title": "4.2 EASY TASK: GRAPH2GRAPH TRANSFER", "content": "For the easy-level task, we chose the largest-scale dataset, ZINC-full, as the pre-training dataset, and utilize the remaining molecular datasets for transfer learning. Additionally, as the Adapter algorithm is specifically designed for the GIN model, we exclusively used GIN as the backbone of the base model in our experiments for a fair comparison. As depicted in Table 1, our GSST method has demonstrated robust performance in the Graph2Graph task, outperforming the fine-tuning method by 0.6% and 0.1% under different pre-training approaches and significantly working better compared to other efficient tuning methods. Examining the errors, it is evident that the error fluctuations of the GSST algorithm are consistently below 1%, highlighting its convergence stability. Moreover, a comparison of the last column indicates that our algorithm exhibits enhanced robustness compared to baselines, consistently delivering performance improvements across different pre-training methods."}, {"title": "4.3 MEDIUM TASK: NODE2NODE TRANSFER", "content": "In the Node2Node transfer scenario, ogbn-arxiv was selected for model pre-training, while the re-maining node classification datasets were used for validation.\nWe show in Table 2 the results of the Graph2Graph transfer task. The 7th and 12th lines of Table 2 exhibit that GMST demonstrated superior transfer learning performance across most datasets when compared to the baselines. This was particularly notable in the GIN backbone settings of PubMed and CiteSeer, where GMST outperformed training from scratch by 6.8% and 3.7%, respectively. The proposed method consistently maintains stable performance across various pre-training methods and GNN backbones, showcasing the universality of GraphBridge. Although, on the Flickr, Amazon datasets, the performance of the proposed GMST method is slightly inferior to that of the fine-tune method, it still outperforms other efficient tuning methods. This suggests that the parameter-efficient GMST method may not exhibit significant advantages when the task domain's scope is not expansive enough, but achives SOTA among the efficient tuning methods."}, {"title": "4.4 HARD TASK: GRAPH2NODE TRANSFER", "content": "For the Graph2Node transfer scenario, we opted for a relatively larger HIV dataset for models pre-training, while using the same validation dataset as in the Node2node for transfer learning."}, {"title": "4.5 EXTENSION TASK: GRAPH2PTCLD TRANSFER", "content": "Here, the pre-trained model acquired from Node2Node and Graph2Node tasks served as the base models backbone for the Graph2PtCld task. Subsequently, we transferred the model to adapt to the ModelNet10 task. In this scenario, we seek to explore whether the GraphBridge framework for graph tasks can transfer knowledge learned from graph domains to graph-like data.\nThe experimental results presented in Table 4 affirm the capability of our proposed GraphBridge framework for achieving transfer learning from graphs to those graph-like data, since our meth-ods demonstrates significant performance improvements against previous works. According to the\nextensive exploration, we were surprised to observe that the GSST method demonstrated superior performance compared to the GMST method in the final results when tuning with a pre-trained back-bone on graph-level datasets. It even outperformed all other tuning methods, including fine-tuning. This phenomenon can be explained by considering the dataset: ModelNet10 is a 10-classified point cloud dataset, which exhibits organizational similarities to a graph classification task. Therefore, using the molhiv pre-training results as the backbone of the model for transfer learning towards the point cloud classification task can be considered a Graph2Graph transfer task, a scenario where GSST excels. However, when employing the arxiv dataset for backbone pre-training, the situation changes. In contrast to the consistent excellence exhibited by GSST in scenarios where the original task is graph classification, GMST maintains stability in the face of a larger domain gap."}, {"title": "4.6 ABLATION STUDIES", "content": "Tuning Efficacy. To evaluate the efficacy of the proposed GSST and GMST for arbitrary graph transfer, we assess the tuning outcomes across varying levels of pre-training bases and diverse back-bones, using the Cora dataset as a representative. As illustrated in Table 5, using the pre-trained model directly for inference in the downstream task without fine-tuning results in unacceptable performance. Our proposed parameter-efficient tuning method effectively transfers the pre-trained model to the downstream task. As the transfer task transitions from moderate to difficult, the perfor-mance of GSST gradually declines. In contrast, GMST exhibits sustained effectiveness, showcasing its robust capability in mitigating the challenges associated with negative transfer.\nResource Efficiency. In the discussion of efficiency of our tuning methods, We validate from two distinct perspectives: parameter efficiency and tuning efficiency.\nIn terms of parameter efficiency, Figure 3 illustrates the adjustable parameters of different tuning methods across various backbone architectures. Since MetaGP and MetaFP are prompt-tuning methods, their tunable parameters are determined by the datasets, rather than the GNN backbone architecture. Therefore, the tunable parameters scale of both MetaGP and MetaFP are averaged across various datasets, keeping same for different backbones. Notably, our GSST and GMST ex-"}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce a novel GraphBridge framework for resource-efficient graph transfer learning toward arbitrary downstream tasks and domains. Our goal is to create a unified workflow that maximizes the utility of pre-trained Graph Neural Networks (GNNs) for various cross-level and cross-domain downstream tasks, eliminating the need for data reorganization and task reformula-tion. To this end, we have established four scenarios for graph transfer learning tasks, ranging from easy to complex, and proposed two resource-efficient tuning methods, namely GSST and GMST, to resolve the dilemmas. Our experiments, conducted on selected datasets across different domains and tasks including graph and node classification, as well as 3D object recognition-demonstrate the effectiveness of our approach in achieving arbitrary domain transfer learning on GNNs with im-proved resource efficiency. Nevertheless, there are still constraints in our experimental setup. In our future work, we will strive to tackle transfer tasks across more benchmarks using our GraphBridge."}, {"title": "A APPENDIX", "content": "This document provides an in-depth analysis of our proposed methodology, offering additional in-sights and experimental details that complement our main findings and enhancing the understand-ing of our methods. Specifically, in Section A.1, we delve into the intricacies of various datasetsutilized in our research, shedding light on their unique characteristics and relevance to the study.Section A.2 and Section A.3 is dedicated to a comprehensive ablation study, where we criticallyevaluate different architectural configurations and their impact on the performance of our proposedmodels. Finally, we further investigated the performance of GraphBridge in supplemental hardtransfer scenarios in A.4, including Node2Graph and Graph2Edge, to guarantee its generalizability."}, {"title": "A.1 DATASETS DETAILS", "content": "We provide in Table 7 the statistics of several graph benchmarks used in the main manuscript. This section aims to highlight the diversity and range of our benchmark datasets, showcasing their varied characteristics and applications.\nImage Relation Dataset. Specifically, the Flickr originates from NUS-wide (Zeng et al., 2019)which contains 89,250 nodes and 899,756 edges. One node in the graph represents one imageuploaded to Flickr. If two images share some common properties (e.g., same geographic location,same gallery, comments by the same user, etc.), there is an edge between the nodes of these twoimages. Node features are the 500-dimensional bag-of-word representation of the images providedby NUS-wide. For labels, each image belongs to one of the 7 classes.\nCitation Network Dataset. The following three datasets, i.e., Cora, Citeseer and Pubmed(Sen et al., 2008), are all citation network datasets used for single-label node classification. Specifi-cally, both Cora and Citeseer contain publications on computer science. Pubmed, on the otherhand, only comprises the papers pertaining to diabetes. Moreover, ogbn-arxiv dataset(Wanget al., 2020; Hu et al., 2020a) contains a directed graph, which denotes the citation network amongall Computer Science (CS) papers in arXiv, with each node representing an arXiv paper and eachdirected edge indicating that one paper cites another one. The node features are the average embed-dings of words in their title and abstract, which are computed by using the skip-gram model.\nGood Purchase Dataset. Amazon Computers and Amazon Photo are the segments of the Amazonco-purchase graphs from (McAuley et al., 2015), where the nodes represent various goods, labeledby the corresponding product categories. Here we chose Amazon-Computers with more samplesfor our validation.\nMolecular Structure Dataset. BACE, BBBP, ClinTox, HIV, SIDER, Tox21, MUV, ToxCastand ZIN-full are all molecular property prediction datasets proposed by (Wu et al., 2018). Everygraph in these datasets denotes a molecule, with the nodes representing atoms, and edges denotingthe chemical bonds. The node features contain the atomic number and chirality and the additionalatom features, e.g., the formal charge. The number of prediction tasks varies across moleculardatasets, and each task corresponds to a binary classification for molecular properties"}, {"title": "A.2 ABLATION FOR NETWORK ARCHITECTURE", "content": null}, {"title": "A.3 ADDITIONAL ABLATION STUDIES", "content": "To validate the robustness of our method, we conducted additional ablation experiments in two key areas:\n\u2022 Influence of Source Dataset. Confirming the algorithm's ability to achieve comparable perfor-mance on pre-trained models trained on different datasets.\n\u2022 Influence of GNN Architecture. Verifying that the algorithm can maintain stable prediction performance across backbone architectures with varying numbers of layers.\n\u2022 Influence of Pre-training Methods. Proving that our framework can flexibly adopt different graph-level pre-training methods and maintain stable prediction performance during tuning stage.\n\u2022 Influence of Side Network Structure. Justifying the use of MLP as a side network for both the GSST GMST tuning algorithm."}, {"title": "A.3.1 THE EFFECT OF DIFFERENT PRE-TRAINING DATA ON THE ARBITRARY GRAPHTRANSFER", "content": "To assess the impact of different pre-training datasets on the final transfer performance of ourproposed method, we conducted new experiments in two task scenarios: medium and hard. ForNode2Node Transfer, we chose Flickr as the pre-training dataset and utilized all other graph clas-sification datasets as downstream tasks to evaluate transfer learning performance. Conversely, forGraph2Node transfer, we opted for the MUV dataset as the training data and selected the same graphclassification datasets as those applied in the main text as downstream tasks for transfer learning.The rest of the experimental setup is consistent with the main text.\nThe results of the experiments are presented in Table 12, where our method performs well on thefirst 4 test datasets in a moderately difficult task but exhibits poor performance on the ogbn-arxivtransfer. It is noteworthy that both the number of nodes and the number of edges in the Flickr datasetare only about half of those in ogbn-arxiv. This suggests that the transfer of downstream taskscan face challenges when the knowledge from the source domain is not sufficiently rich because the absence of knowledge in the source domain can lead to pre-trained models being unable toachieve sufficiently generalized performance. This phenomenon aligns with the pre-train-tuning"}, {"title": "A.3.2 THE EFFECT OF DIFFERENT BACKBONE LAYERS ON THE ARBITRARY GRAPHTRANSFER", "content": "To investigate the impact of different numbers of backbone layers on the performance of the GSSTand GMST methods, we configured the number of layers of the backbone to 5, the maximum cur-rently used in stacked graph neural networks. Subsequently, we trained and tested these configura-tions on medium and difficult task scenarios, utilizing the same pre-trained models and downstreamtasks as detailed in the main text. The results obtained are recorded in Tables 14, 15."}, {"title": "A.3.3 ADAPTATION OF THE PRE-TRAINING STAGE TO DIFFERENT GRAPH-LEVELPRE-TRAINING METHODS", "content": "As described in the main paper, our GraphBridge framework is capable of adapting various graph-level pre-training methods to pre-train our base model in Pre-training Stage. Consequently, we alsoevaluated the the transfer performance following pre-training with the GCC(Qiu et al., 2020) andGPT-GNN(Hu et al., 2020b) methods."}, {"title": "A.3.4 THE EFFECT OF DIFFERENT SIDE NETWORK STRUCTURES ON THE TUNING RESULTS", "content": "To investigate the impact of side network structures on arbitrary graph transfer, we conducted ad-ditional experiments with GMST fine-tuning. Specifically, we used each GNN backbone's corre-sponding lightweight structure as the side network (consistent with the implementation of Sunget al. (2022a)) to conduct the GMST across different datasets and compared the results with thoseobtained using an MLP as the side network. These experiments were carried out under the Middleand Hard task scenarios, and the results are presented in Table 18 and Table 19, respectively.\nThe experimental results demonstrate that employing the corresponding lightweight GNN as theside network does not yield significant performance improvements for GMST tuning in eitherthe Middle or Hard task scenarios. Furthermore, based on the comparative analysis of training effi-ciency between GNN and MLP in Sung et al. (2022a) and Han et al. (2022), using an MLP as theside network ensures that computational overhead increases linearly with data scale, thereby main-taining the efficiency of our GMST algorithm. In conclusion, our innovative use of an MLP as theside network for pre-trained GNNs significantly enhances fine-tuning efficiency while preservingperformance in arbitrary graph domain transfer, which is a successful attempt."}, {"title": "A.4 SUPPLEMENTARY TRANSFER SCENARIOS", "content": "To further refine our task setup and comprehensively validate the generalization of the framework,we conducted additional transfer experiments on scenarios with the same level of difficulty as theGraph2Node task, including Node2Graph and Graph2Edge scenarios.\n\u2022 Node2Graph. Transfer learning from node classification tasks to graph classification tasks withinunrelated knowledge domains.\n\u2022Graph2Edge. Transfer learning from graph classification tasks to edge prediction tasks withinunrelated knowledge domains."}, {"title": "A.4.1 NODE2GRAPH TRANSFER TASK", "content": "In the Node2Graph transfer scenario, we adapted the settings from the Graph2Node setup: duringpre-training, we used the node-level ogbn-arxiv dataset to pre-train the model, while employingthe graph-level downstream datasets used in the Graph2Graph tasks for fine-tuning. Moreover, weapplied the same setup as in the Graph2Node scenario: The performance of GraphBridge was eval-"}, {"title": "A.4.2 GRAPH2EDGE TRANSFER TASK", "content": "For the Graph2Edge transfer scenario, we used the same datasets, pre-training methods, and GNNbackbones as in the Graph2Node setup. However, we reformulated the original node classificationdownstream task into an edge prediction task by sampling edges with both positive and negative ex-amples. As the edge prediction task is a binary classification problem, we evaluated the performanceusing ROC-AUC scores. The results of this experiment are shown in Figure 21.\nThe results exhibited in Figure 21 indicate that our method achieves outstanding performance inthe Graph2Edge transfer task. On the CiteSeer, PubMed, and Cora datasets, GMST consistentlyoutperforms all fine-tuning methods, maintaining a clear competitive advantage. Moreover, ourGMST breakthrough outperforms the fine-tuning method on the Amazon and Flickr consideringGraph2Edge transfer.\nIn conclusion, by taking all the results from complementary scenarios into consideration, we findthat GraphBridge demonstrates reliable performance across a variety of cross-domain transfer tasks,regardless of transfer task's complexity. This confirms the robustness and generalization capabilitiesof the GraphBridge framework, establishing it as an efficient and high-performing approach forgraph transfer learning."}, {"title": "A.5 CONFUSION MATRIX VISUALIZATION OF GMST TUNING RESULTS", "content": "In order to display a clear visualization of the class-wise performance of the GMST method acrossdifferent datasets, we take the Hard Task Scenario as an example to plot the confusion matrices forGMST results of various GNN backbones pre-trained with GraphCL on diverse node-classificationbenchmarks.\nThe results demonstrate that on CiteSeer, PubMed, and Cora datasets where GMST performs well,the predictions of the fine-tuned model are concentrated along the diagonal, indicating high accuracyacross all categories. In contrast, for the Flickr and Amazon datasets, the fine-tuned model tends topredict test data as belonging to the category with the highest proportion in the training set, reflectingthe impact of label imbalance during fine-tuning. These findings highlight that category imbalancein downstream task datasets can negatively influence the performance of the GMST algorithm."}]}