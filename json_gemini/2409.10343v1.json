{"title": "Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation", "authors": ["Tianrui Song", "Wenshuo Chao", "Hao Liu"], "abstract": "Implicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting. Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations. To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions. The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives. To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring. Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions. Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach.", "sections": [{"title": "Introduction", "content": "Recommender systems are designed to learn user preferences and suggest items across various online platforms, such as e-commerce, news portals, and social networks (2020; 2020; 2023). To train these systems, implicit feedback derived from user actions (e.g., clicks and purchases) is commonly employed due to its wide availability. Typically, each observed interaction is assumed to reflect a user's genuine interest in an item and is therefore assigned a positive label, while non-interacted items are considered negative (2020; 2021a). However, such a routine has recently been questioned that interacted items may be plagued by false-positive noise (e.g., due to misclicks or popularity bias), while non-interacted items may suffer from false-negative noise (e.g., due to position bias) (2021b). These noisy interactions lead to inaccurate estimation of user preferences, hindering the performance of recommendation systems.\n\nDenoising recommendation has been proposed to mitigate the negative impact of noisy interactions through two primary strategies: 1) sample dropping and 2) sample re-weighting. Dropping methods aim to improve model performance by selecting clean samples and discarding noisy ones during training (2021a; 2021). In contrast, re-weighting approaches assign lower weights to interactions identified as noisy, thereby reducing their influence on the model's learning process (2023; 2022). The success of these denoising techniques heavily depends on the accuracy of distinguishing between clean and noisy samples. Consequently, various data patterns have been explored as noisy signals (2021a; 2022; 2023). To name a few, loss value is one of the most commonly used signals, as noisy interactions typically exhibit higher loss values compared to clean ones (2021a; 2024). In addition, other indicators such as prediction scores (Wang et al. 2022) and gradients (Wang et al. 2023) have also been investigated to identify noisy samples.\nDespite significant advancements, existing methods often face the challenge of misidentifying hard samples as noisy ones. As illustrated in Figure 1, while noisy samples exhibit distinct patterns compared to easy samples, we observed that hard samples and noisy samples tend to present similar patterns in both prediction scores and loss values. Consequently, previous denoising approaches that rely solely on data patterns struggle to accurately distinguish between hard and noisy samples. This misclassification is problematic because hard samples have been shown to be beneficial, both empirically (2012) and theoretically (2023). Mistakenly treating hard samples as noise during the recommender training ultimately leads to suboptimal results.\nRecently, Large Language Models (LLMs) have demonstrated a promising ability to understand user preferences (Wu et al. 2024) and enhance item semantics (Wei et al. 2024), presenting a valuable opportunity to tackle the challenge of hard sample identification. Our key insight is that LLMs can be harnessed to summarize user preference and act as a scorer to analyze the consistency between user preferences and items, thereby identifying hard samples with the resulting scores. For example, when optimizing a model using a Bayesian Personalized Ranking (BPR) objective, the LLM scorer can effectively evaluate user preference scores of positive and negative items. As a result, samples with similar positive and negative scores are pinpointed as hard samples because they are inherently incompatible with the BPR training objective, which aims to maximize the divergence in scores. This allows us to mitigate the hard samples' misclassification issue in denoising recommender training.\nHowever, leveraging LLMs for this task is nontrivial due to two primary challenges. First, given the vast number of users and items, assessing the preferences of all users across all items is computationally intensive, especially considering the high inference cost of LLMs. Second, while LLMs can derive user preference by concluding interacted items, the presence of false-positive items in historical interactions can lead to biased user preference summarization.\nTo address the challenges mentioned above, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework for recommendation, which comprises three key modules: Variance-based Sample Pruning, LLM-based Sample Scoring, and Iterative Preference Updating. To ensure efficiency, we first introduce a variance-based pruning strategy that progressively selects a small subset of hard sample candidates. Following this, we construct the LLM-based Sample Scoring module, where hard samples are identified by evaluating how well they satisfy the training objective. Specifically, the LLM scores the user preference for a given item by summarizing user preference, assesses the sample's hardness based on the pointwise or pairwise training objective, and determines whether it qualifies as a hard sample. Additionally, to enhance the accuracy of the summarized user preferences, we propose an Iterative Preference Updating module. It refines user preferences by adjusting for items that are mistakenly identified or overlooked during the summarization process, thereby improving the overall reliability of the LLMHD framework.\nOur main contributions are summarized as follows:\n\u2022 We propose LLMHD, a novel denoising recommendation approach that differentiates between hard and noisy samples leveraging LLMs. To the best of our knowledge, this is the first attempt to integrate LLMs into denoising recommendations.\n\u2022 The LLMHD addresses efficiency concerns through a variance-based sample pruning process. Furthermore, we enhance the effectiveness of the model by employing an iterative preference updating strategy, improving the LLMs' understanding of genuine user preferences.\n\u2022 Extensive experiments conducted on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our method. The results show that LLMHD delivers impressive performance and robust noise resilience."}, {"title": "Related Work", "content": "Denoise Recommendation\nRecommenders are pointed out to be affected by users' unconscious behaviors (2021b), leading to noisy data. As a result, many efforts are dedicated to alleviating the problem. These approaches can be categorized into two paradigms: sample dropping (2012; 2023) and sample re-weighting (2023; 2022). Sample dropping methods aim to keep clean samples and discard noisy ones. For instance, T-CE (2021a) observes that noisy samples exhibit high loss values and remove them during training. IR (2021c) iteratively generates pseudo-labels to discover noisy examples. Sample re-weighting methods try to mitigate the impact of noisy samples by assigning lower weights to them. Typically, R-CE (2021a) assigns lower weights to noisy samples according to the prediction score. BOD (2023) considers the weight assignment as a bi-level optimization problem. Although these methods achieve promising results, they rely on data patterns to recognize noisy samples (e.g., loss values, and prediction scores) leading to difficulties in identifying hard samples from noise samples as they exhibit similar patterns.\nLLMs for Recommendation\nLarge Language Models (LLMs) are effective tools for the Natural Language Processing field and have gained significant attention in the domain of Recommendation Systems (RS). For the adaption of LLMs in recommendations, existing works can be divided into three categories (2024): LLM as RS, LLM Embedding for RS, and LLM token for RS. The LLM as RS aims to transform LLMs into effective recommendation systems (Chao et al. 2024), such as LC-Rec (2024a) and LLM-TRSR (2024b). In contrast, the LLM embedding and LLM token for RS views the language model as an enhancer, where embeddings and tokens generated by LLMs are utilized for promoting recommender systems. The former typically adopts embeddings related to users and items, incorporating semantic information in the recommender (Ren et al. 2024). While the latter generates text tokens to capture potential preferences through user and item semantics (Wei et al. 2024; Xi et al. 2023). Despite the progress, these methods overlook the potential of LLMs in enhancing data denoising for recommendation."}, {"title": "Preliminary", "content": "The objective of training a recommender system is to learn a scoring function $\\hat{y}_{u,i} = f_{\\theta}(u, i)$ from interactions between users $u \\in U$ and items $i \\in I$. We assume that user-interacted"}, {"title": "Loss-based Denoising", "content": "We first introduce the denoising module implemented based on the widely accepted assumption (2021a) that samples with higher loss values are more likely to be noisy. Specifically, for each data sample b in the mini-batch B, we calculate the corresponding loss value l(b) and sort all samples in the ascending order,\n$l(b_1)^{\\uparrow} <l(b_2)^{\\uparrow} <\\cdots <l(b_{|B|})^{\\uparrow}, b_j \\in B$,\nwhere B denotes the batch size. This operation assists the noisy sample identification, which we formulate as $B_N$,\n$B_N = \\{ b_j | l(b_j) \\geq l(b_{\\varepsilon_l})^{\\uparrow} \\}$,\nwhere T denotes the current training iteration. The $\\varepsilon_l$ represents a dynamic threshold, calculated as,\n$\\varepsilon_l = \\min(\\frac{T}{\\alpha}, \\varepsilon_{max}|B|)$, \nwhere $\\varepsilon_{max}$ is a hyper-parameter representing the maximum noise ratio, and $\\alpha$ is a factor that modulates the growth rate of the noise threshold. The $\\varepsilon_l$ increases as the stability of prediction scores incrementally improves during training, following previous works (Wang et al. 2021a). It is worth mentioning that the $B_N$ inadvertently contain hard samples, given that both hard and noisy samples manifest similar patterns in loss values. This requires further refinement to distinguish genuine noisy data and hard samples."}, {"title": "Variance-based Sample Pruning", "content": "Although it is possible to present all identified noisy samples $B_N$ to the LLMs for scoring, this approach would be prohibitively time-consuming due to the massive interactions in the recommender system. Specifically, hard sample candidates are selected based on the observation of previous work (2020), which demonstrated that hard samples exhibit relatively higher prediction score variance compared to noisy samples. Therefore, for samples $b \\in B_N$, we calculate the prediction scores variance of positive $U_{p,b}$ and negative $U_{n,b}$ items across multiple epochs (see Equation 17). Then sort them in descending order based on $v_p$ and $v_n$ respectively,\n$v_{p,b}^{1} > v_{p,b}^{2} > \\cdots > v_{p,b}^{j} > \\cdots > v_{p,b}^{|B_p|}, b \\in B_N$,\n$v_{n,b}^{1} > v_{n,b}^{2} > \\cdots > v_{n,b}^{j} > \\cdots > v_{n,b}^{|B_n|}, b \\in B_N$,\nwhere $|B_p|$ and $|B_n|$ denotes the number of positive and negative items in the $B_N$ respectively. Hard sample candidates $B_{HC}$ are collected by,\n$B_{HC} = \\{ b_{j} | v_{p,b}^{j} \\geq v_{p,b}^{|B_p|} \\} \\cup \\{ b_{j} | v_{n,b}^{j} \\geq v_{n,b}^{\\varepsilon_u |B_n|} \\}$,\nwhere $\\varepsilon_v \\in [0, 1]$ denotes the proportion of hard samples. With the increasing $|B_N|$ more candidates will be selected in latter training iterations and provided to LLM-based Sample Scoring to identify hard samples further."}, {"title": "LLM-based Sample Scoring", "content": "Owing to the resemblance in data patterns between hard and noisy samples, distinguishing them solely through numerical disparities is ineffective. To eliminate this issue, we introduce the LLM-based Sample Scoring method. LLMs act as scorer to provide auxiliary information that evaluates the"}, {"title": "Pointwise Sample Scoring", "content": "The pointwise BCE loss, as shown in Equation 2, aims at reducing the classification uncertainty of a (user, item) pair. For a data sample $(u, i_{pos})$ or $(u, i_{neg})$, if the user's preference for the item is ambiguous, the sample is of low compatibility with the training objective. Therefore, positive pair with lower $S_{u,i_{pos}}$ and negative pair with higher $S_{u,i_{neg}}$ are harder samples, thereby hard samples are identified by,\n$I_{point}(u, i) = \\begin{cases}\n1, & \\text{if } S_{u,i} < \\varepsilon_{pos} \\text{ and } y_{u,i} = 1 \\\\\n1, & \\text{if } S_{i,i} > \\varepsilon_{neg} \\text{ and } y_{u,i} = 0 \\\\\n0, & \\text{otherwise},\n\\end{cases}$\nwhere the $\\varepsilon_{pos}$ and $\\varepsilon_{neg}$ are thresholds that control the hardness. In addition, since previous works (2021a) discussed that fitting harder samples at the early training stage might hurt the generalization ability, we smoothly change $\\varepsilon_{pos}$ and $\\varepsilon_{neg}$ during each training iteration T as follows,\n$\\varepsilon_{pos} = \\max(\\varepsilon_{max} - \\frac{T}{\\alpha}, \\varepsilon_{min}^{pos})$,\n$\\varepsilon_{neg} = \\min(\\varepsilon_{min} + \\frac{T}{\\alpha}, \\varepsilon_{max}^{neg})$"}, {"title": "Pairwise Sample Scoring", "content": "Similar to the pointwise sample scoring, we identify hard samples under the pairwise training schema. Specifically, according to Equation 1, the pairwise BPR loss aims to maximize the divergence of prediction scores between positive and negative items. For a sample (u, $i_p$, $i_n$), if the user's preference for the positive item does not significantly surpass that for the negative, the sample is less compatible with the objective. Therefore, hard samples are identified through the indicator function,\n$I_{pair}(S_{u,i_p} - S_{u,i_n} > \\varepsilon_{pair})$,\nwhere the threshold $\\varepsilon_{pair}$ also gradually decreases to increase the hardness by the number of iteration T,\n$\\varepsilon_{pair} = \\max(\\varepsilon_{pair} - \\frac{T}{\\alpha}, \\varepsilon_{min})$.\nBased on the above technique, we differentiate hard samples in both pointwise and pairwise training schema."}, {"title": "Iterative Preference Updating", "content": "Accurate user preference $P_u$ is critical for effective LLM sample scoring. However, the $P_u$ summarized based on interacted items do not fully capture user interests due to the inclusion of disliked items, i.e., false-positives, and the exclusion of liked items, i.e., false-negatives. To mitigate this problem, we refine user preferences iteratively by excluding dislikes and incorporating likes. For every epoch t, we calculate the variance score $v_d$ of user-item pairs $d = (u, i)$,\n$v_d = \\sqrt{\\frac{1}{m} \\sum_{j=t-m+1}^{t} (y_d^j - \\frac{1}{m} \\sum_{j=t-m+1}^{t} y_d^j)^2}$"}, {"title": "Denoising Training with Hard Samples", "content": "The denoising training is done by keeping hard samples and dropping noisy samples. We first define the set of identified hard samples $B_H$ as,\n$B_H = \\{ b_j | I_{LLM}(b_j) = 1, b_j \\in B_{HC} \\}$,\nwhere the $I_{LLM}$ is either $I_{point}$ or $I_{pair}$ based on the format of data samples. The recommendation loss $L_{rec}$ is then calculated in the following format,\n$L_{rec}((B \\setminus B_N) \\cup B_H)$.\nIn this way, hard samples have remained and the noisy samples are dropped while training the recommender."}, {"title": "Experiments", "content": "We compare LLMHD with state-of-the-art denoise approaches on four backbones and three real-world datasets to demonstrate the effectiveness of our method. Experiments are directed by the following research questions (RQs):\n\u2022 RQ1: How does LLMHD performs compared with other state-of-the-art denoise baselines across the datasets?\n\u2022 RQ2: Does the LLMHD demonstrate robustness when tackling different levels of noisy data?\n\u2022 RQ3: What is the effect of different components and hyper-parameters within the LLMHD on performance?"}, {"title": "Performance Comparison (RQ1)", "content": "We evaluated the effectiveness of our proposed method in both pointwise LLMHDBCE and pairwise LLMHDBPR setting. The performance against other denoising recommendation strategies is shown in Table 1. Notably, LLMHD significantly enhances the performance of all backbone models trained with normal BCE or BPR on all datasets, demonstrating its superior denoising capability. Moreover, LLMHD consistently outperforms other advanced denoising methods across the majority of datasets and backbones. We attribute this improvement to the extended hard sample identification, where baselines like T-CE and RGCF lack the capability, rendering them less effective in comparison. We also observed that for most datasets and backbones, RGCF and DCF are inferior to us alone. In contrast, other denoise baselines like T-CE and R-CE perform worse than them. This can be explained by the fact that both RGCF and DCF are designed to insert or preserve high-confidence interactions, a feature not inherent to T-CE and R-CE. Since LLMHD also focuses on retaining more interactions (i.e., hard samples), we posit that maintaining a more extensive set of samples is beneficial in enhancing performance."}, {"title": "Noise Robustness (RQ2)", "content": "We conduct random noise training to evaluate the robustness of the noise resistance capability of LLMHD, comparing it with the most competitive RGCF and the classical approach T-CE. Following previous work (2021), the proportion of noise injected into the training set spanned from 5% to 20%, while keeping the samples in the testing set unchanged. We report the result in Figure 3. The result shows that: (1) As the noise ratio increases, we observe a consistent downward trend in the performance across all backbone models and denoising strategies. This decline can be caused by the rising noise level leading to the increasing data corruption, which complicates discerning genuine user preferences. (2) LLMHD outperforms the backbone model and other denoise approaches in all noise ratios. This empha-"}, {"title": "In-depth Model Analysis (RQ3)", "content": "Ablation Study. We conduct experiments to assess each module in LLMHD, including Variance-based Sample Pruning, LLM-based Sample Scoring, and Iterative Preference Updating, the result is shown in Table 2. (1) We investigate whether including Variance-based Sample Pruning (VS) enables an effective hard sample candidate selection. Specifically, we compare it with Random Selection (RS) and select the same amount of candidates as the VS. According to the result, converting the VS to RS leads to a performance drop in all metrics. This reveals the superiority of the Variance-based Sample Pruning in selecting hard sample candidates. (2) We discover whether LLM-identified hard samples enhance the recommendation performance. The comparison is made between the backbone that only adopts a Loss-based Denoise Module (LD) and the one that includes LLM-based Sample Scoring (LMS). Significant improvement is demonstrated after using LLMs to detect hard samples, indicating the advancement of LLM in hard sample identification. (3) We explore the influence of adopting Iterative Preference Updating (PU). Compared with discarding the preference updating, the performance of adopting it increases, demonstrating the effectiveness of Preference Updating in understanding genuine user preference."}, {"title": "Effect of Hyper-parameters", "content": "For a more elaborate analysis, we adjust the hyper-parameters within the range described in the Implementation Details section. The results are shown in Figure 4. From our observations: (1) Growth Rate \u03b1: A moderate increase in \u03b1 enhances performance, as it retains more samples per iteration, mitigating data scarcity during training. However, excessively high values degrade performance. (2) Max Noise Scale \u03b5max: Elevating \u03b5max initially improves LLMHD by filtering out more noise, but an overly high setting results in excessive sample loss, hampering the learning of user preferences. (3) Hard Sample Candidate Proportion \u03b5v: Increasing Ev presents more hard sample candidates, boosting performance. But setting it too high may confuse noisy samples for hard ones, lowering overall effectiveness. (4) Confidence Threshold \u03b5\u03b3: Gradually raising \u03b5\u03b3 initially benefits the model by promoting item selection for preference update. However, a high confidence restricts item discovery and a low confidence finds incorrect items, both diminishing user understanding."}, {"title": "Conclusion", "content": "In this work, we introduced the Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework to address the challenge of distinguishing hard samples from noise samples for recommender systems. By utilizing an LLM-based scorer to evaluate semantic consistency between users and items and assessing sample hardness according to its compatibility with training objectives, we can differentiate hard samples from noise samples. We further introduce a variance-based sample pruning strategy to effectively select candidates. In addition, the iterative preference update refines user preference and mitigates biases introduced by false-positive interactions. Extensive experiments on real-world datasets and recommenders demonstrated the effectiveness of LLMHD in improving recommendation quality."}, {"title": "Details of Dataset", "content": "In this section, we offer details of the preprocessed dataset adopted in the experiment. We take the datasets in RLM-Rec (Ren et al. 2024), in which each item contains a corresponding item text profile. Therefore, we follow the preprocessing setting in the RLMRec. Specifically, interactions with ratings below 3 for both the Amazon-books and Yelp data are filtered out. No rating-based filtering is adopted in Steam. K-core filtering is also performed and split into training, validation, and test sets using a 3:1:1 ratio. The statistics of datasets preprocessed following RLMRec are shown in Table 3.\nHowever, previous works adopted the rating score to label noise and clean data. For example, T-CE regards a rating score below 3 as a false-positive interaction. As a result, the dataset filtered with ratings in RLMRec is regarded to contain less noisy interactions. To compare the denoising ability of different methods, we add 5% noisy interactions to the training set. These noisy interactions are selected from the interactions that are rated below 3. Experiments are then conducted on these noise-inserted datasets."}, {"title": "Details of API Token Cost", "content": "Since we adopt the GPT-3.5-turbo as the LLM in the LLMHD, here we provide the token number for training with LLMHD in Table 4. The total token number of Tscore in LLMHD is highly dependent on two aspects: the number of interactions in the dataset, the value of the maximum noise scale \u03b5max, and the hard sample proportion \u03b5. Whears, the Tsum is not correlated to the hyper-parameters. We also report the token number for TFN, and TFP during training when the confidence threshold is set to \u03b5y = 3."}, {"title": "Details of Loss Value and Prediction Score Figure", "content": "We provide the details of plotting the Figure 1. For the noise samples, we follow the settings of (Wang et al. 2021a), tak-"}, {"title": "User Preference Summarization (Tsum)", "content": "You are a professional book editor.\nBelow is the information about books that a reader has read:\n1. <ITEM PROFILE 1>\nN. <ITEM PROFILE N>\nBased on the books the reader has read, please summarize characteristics of books this reader may enjoy.\nYou MUST provide the summarization with the following format: <summarization>A summarization of what kinds of books this reader is likely to enjoy.</summarization>.\nPlease ensure that the \" summarization\" is no longer than 100 words.\nYou should not provide any explanation except the summarization.\n\nYou are a professional book editor.\nBelow is the information about books that a reader has read:\n1.Fans of romance novels with a touch of royalty theme\n2.Readers who enjoy romance novels would appreciate HOPELESS. The book ...\n3.Fans of young adult fantasy novels with strong female leads ...\nBased on the books the reader has read, please summarize characteristics of books this reader may enjoy.\nYou MUST provide ...\n<summarization>The reader enjoys a variety of genres including supernatural romance, young adult fantasy, ... They appreciate strong female leads, ... </summarization>"}, {"title": "User Preference Scoring (Tscore)", "content": "You are a professional book editor.\nBelow is the reading preference of a reader:\n<USER PROFILE>\nNow here is the descriptions of a book:\n<ITEM PROFILE>\nBased on the reading preference of the reader, please rate how much this reader might prefer the above book from 1 (disliked it a lot) to 10 (liked it a lot).\nYou MUST provide the score of the book with the following format. (<s>score</s>, e.g. <s>6</s>)\nYou should not provide any explanation except the score.\n\nYou are a professional book editor.\nBelow is the reading preference of a reader:\nThe reader enjoys a variety of genres including supernatural romance, young adult fantasy ... They appreciate strong female leads, themes of love and loss, ...\nNow here is the descriptions of a book:\nFans of urban fantasy and paranormal romance may enjoy Sterling. The book offers a unique fantasy world and features a balance of action, romance, and humor. Those looking for a departure from traditional supernatural creatures may find this book refreshing.\nBased on the reading ...\nYou MUST provide ...\n<s>8</s>"}, {"title": "User Preference Update with False Positives (TFP)", "content": "You are a professional book editor.\nYou will help me to determine a reader's reading preference.\nI will provide you with:\nREADER PROFILE: a description of the reader's potential reading preferences.\nNOT INTERESTED BOOKS: descriptions of books that the reader might not interested in.\nREADER PROFILE: <USER PROFILE>\nNOT INTERESTED BOOKS:\n1. <ITEM PROFILE 1>\nBased on the characteristics of \"NOT INTERESTED BOOKS\", please adjust the provided \"READER PROFILE\" to make it more compatible with the reader's actual reading preferences\nYou MUST answer in following format: <profile>....</profile> (e.g. <profile>The reader is likely to enjoy...</profile>)\nPlease ensure that the \"profile\" is no longer than 100 words.\nDo not provide any other text except the \"profile\" string.\n\nYou are a professional book editor ...\nREADER PROFILE:\nThe reader enjoys a variety of genres including supernatural romance, young adult fantasy, contemporary romance, and paranormal romance\nNOT INTERESTED BOOKS:\n1. Fans of paranormal romance and fiction who enjoy reading series with prequels and sequels would enjoy New Beginnings: Prequel to Others of Edenton.\n<profile>The reader is likely to enjoy books that explore magical worlds and supernatural beings, with a focus on strong female leads ... </profile>"}, {"title": "User Preference Update with False Negatives (TFN)", "content": "You are a professional book editor.\nYou will help me to determine a reader's reading preference.\nI will provide you with:\nREADER PROFILE: a description of the reader's potential reading preferences.\nINTERESTED BOOKS: descriptions of books that the reader might be interested in.\nREADER PROFILE: <USER PROFILE>\nINTERESTED BOOKS:\n1. <ITEM PROFILE 1>\nBased on the characteristics of \"INTERESTED BOOKS\", please adjust the provided \"READER PROFILE\" to make it more compatible with the reader's actual reading preferences\nYou MUST answer in following format: <profile>....</profile>"}]}