{"title": "Hyperdimensional Intelligent Sensing for Efficient Real-Time Audio Processing on Extreme Edge", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "Hanning Chen", "SungHeon Jeong", "Wenjun Huang", "Arghavan Rezvani", "Minhyoung Na", "Yoshiki Yamaguchi", "Mohsen Imani"], "abstract": "The escalating challenges of managing vast sensor-generated data, particularly in audio applications, necessitate innovative solutions. Current systems face significant computational and storage demands, especially in real-time applications like gunshot detection systems (GSDS), and the proliferation of edge sensors exacerbates these issues. This paper proposes a groundbreaking approach with a near-sensor model tailored for intelligent audio-sensing frameworks. Utilizing a Fast Fourier Transform (FFT) module, convolutional neural network (CNN) layers, and HyperDimensional Computing (HDC), our model excels in low-energy, rapid inference, and online learning. It is highly adaptable for efficient ASIC design implementation, offering superior energy efficiency compared to conventional embedded CPUs or GPUs, and is compatible with the trend of shrinking microphone sensor sizes. Comprehensive evaluations at both software and hardware levels underscore the model's efficacy. Software assessments through detailed ROC curve analysis revealed a delicate balance between energy conservation and quality loss, achieving up to 82.1% energy savings with only 1.39% quality loss. Hardware evaluations highlight the model's commendable energy efficiency when implemented via ASIC design, especially with the Google Edge TPU, showcasing its superiority over prevalent embedded CPUs and GPUs.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's systems, the challenge of handling the large amounts of data generated by sensors, particularly in the context of audio data, has become increasingly prevalent [27]. This surge in data volume requires significant computational resources for both processing and storage, at significant cost. This is particularly problematic in domains that require real-time responsiveness, such as gunshot detection systems (GSDS) [9], chainsaw sound detection for forest protection [19], and vehicle sound detection for real-time traffic planning [6]. In addition, the proliferation of edge sensors in today's landscape further exacerbates the cost scaling of these systems.\nNevertheless, in many of these systems, a substantial portion of the data generated by microphone sensors is irrelevant to the targeted detection scenarios [4]. It is plausible to mitigate these escalating costs by transmitting only the pertinent audio data of interest, which is essential for predicting targeted detection scenarios, to cloud-based systems responsible for data storage and processing. One promising solution for achieving this efficiency is the adoption of near-sensor computing paradigms.\nBy embedding lightweight, real-time audio detection machine learning models with low energy consumption capabilities at the sensor level, we can selectively forward valuable data to the cloud infrastructure.\nContemporary deep learning models, such as Deep Neural Networks (DNNs), have gained significant traction in address-ing various real-world tasks. However, they have been critiqued for their substantial memory and energy requirements, particu-larly during the training and inference phases. Deploying such resource-intensive models directly on or near sensors poses challenges, resulting in a notable performance gap between deep learning algorithms and sensing components. Moreover, DNNs are less adaptable to online learning scenarios, a critical limitation in systems that must promptly adjust to real-time sensory input.\nThe aforementioned challenges serve as a strong motivation for the development of an intelligent, rapidly adaptable, and efficient framework for the representation and analysis of raw sensor data. To attain real-time performance, even with online learning capabilities, we propose a redesign of machine learning algorithms. This involves the fusion of a lightweight Convolutional Neural Network (CNN) with neurally-inspired HyperDimensional Computing (HDC). HDC is an alternative paradigm that emulates essential brain functions, prioritizing high efficiency and online learning capacity, while the CNN is harnessed for its robust feature extraction capabilities. HDC is rooted in the observation that the human brain excels in operating on high-dimensional data representations.\nIn this paper, we introduce a near-sensor model designed for an intelligent sensing framework tailored to audio detection tasks, aiming to address the challenges previously delineated. Our model incorporates a Fast Fourier Transform module, followed by a series of CNN layers and an HDC layer. Notably, it facilitates low-energy, rapid inference, and online learning, dynamically adapting to emerging data trends based on feedback from a heavyweight machine learning model in the cloud. The versatility of our model extends to its adaptability for implementation in ASIC design, offering superior energy efficiency compared to embedded CPUs or GPUs. Given the current trend of shrinking microphone sensor sizes, an ASIC implementation supporting compact chip dimensions ensures the viability of deploying our proposed near-sensor model"}, {"title": "II. BACKGROUND", "content": "With the continual advancements in sensor technology, there has been a parallel evolution in computational methods, aim-ing to extract meaningful insights from raw sensor data [3]. Notably, recent innovations in the form of in-sensor and near-sensor accelerators [1], [15], [16], [20], [24], [28] represent a stride towards more efficient local processing. These accel-erators integrate dedicated machine learning circuits into the sensing circuitry, enhancing the efficiency of data processing at the source. However, despite the success of these localized approaches, there is often a gap when it comes to achieving comprehensive system-level integration. In the specific context of audio data, recent research endeavors [8], [21], [22] have delved into efficient systems tailored to audio applications. These frameworks have tackled challenges ranging from mem-ory reduction to devising low-power solutions. While these contributions showcase valuable progress, they may fall short in meeting the real-time demands of critical applications, such as acoustic gunshot detection [9]. The need for 24/7 monitoring and rapid response in scenarios like gunshot detection necessi-tates a paradigm shift towards real-time operation, a facet not thoroughly addressed by existing solutions.\nTo explicitly address these gaps, we note that existing methods either lack robust online learning capabilities at the edge or fail to drastically reduce energy consumption for large-scale deployments. Our proposed model directly tackles these shortcomings by introducing a near-sensor paradigm that can continuously adapt to new data while significantly cutting down on end-to-end system energy usage.\nIt is in this gap that our proposed near-sensor model stands as a pioneering framework. Going beyond existing approaches, our model is designed to minimize the overall energy consump-tion associated with audio data processing. Our \"sparse selec-tive strategy\" refers to transmitting only those audio segments the near-sensor model deems as audio-of-interest, effectively filtering out irrelevant data and reducing costly cloud-side computation. What distinguishes our work is the utilization of a near-sensor principle, wherein the sensor selectively collects relevant acoustic data locally, significantly reducing the need for data transmission and processing at the central server. This marks the first attempt to detect audio of interest based on this near-sensor principle, presenting a novel and efficient approach to intelligent sensing in audio data."}, {"title": "B. Hyperdimensional Computing", "content": "HyperDimensional Computing (HDC), inspired by the brain and modeled with hypervectors [12], offers a robust com-putational paradigm applicable to a wide range of learning problems. This includes areas such as speech recognition [11], genome sequence alignment [14], graph learning [13], [17], clustering [25], and computer vision [7], [10], [26]. HDC is known for its high memorization capability, robustness against noise, and model interpretability. It builds upon a well-defined set of operations with random hypervectors, making it extremely resilient to failures and adaptable to diverse compu-tational tasks.\nDespite its advantages, the application of HDC to audio de-tection problems has not been extensively explored. This study aims to leverage the unique properties of HDC for gunshot audio detection, addressing a critical public safety concern. By utilizing HDC's efficient encoding and high-dimensional representation, we aim to develop a model that can accurately detect gunshot sounds in real time, ensuring rapid response and increased safety. This work represents a novel application of HDC, extending its proven benefits in other domains to the field of audio detection and offering a promising solution for urgent, real-world challenges in public safety."}, {"title": "III. INTELLIGENT SENSING MODEL DESIGN", "content": "In many scenarios, complex machine learning tasks demand heavy models that prove challenging to implement on edge devices. For instance, a recent state-of-the-art Transformer-based audio classification model [5], demands 80 hours of training on 4 NVIDIA Tesla V100 GPUs, despite its com-putational resource reduction compared to alternative models. Similarly, many other works also focus on utilizing heavy machine learning models for complex tasks such as using large pre-trained multi-modality model [23], having large trans-former models [2]. Consequently, these contemporary deep learning-based audio detection tasks present a practical challenge when it comes to real-time implementation on edge sensors. Our solution simplifies this by binarizing such tasks, specifically detecting \"audio of interest,\" only essential audio data for complex functions. Unlike conventional models like Recurrent Neural Networks (RNNs), our near-sensor model employs an HDC model with very few CNN layers. This design ensures fast, efficient inference with online learning capability. Unlike MLPs, HDC does not rely on fully connected layers or activation functions. Instead, it builds class hypervectors directly via bundling and binding operations on encoded fea-tures extracted from CNN layers. This approach does not necessitate large parameter sets and backpropagation, enabling rapid adaptation and robust performance even with limited training samples."}, {"title": "A. HDC Basics", "content": "The fundamental representational unit of HDC is called a hyperdimensional vector. A hypervector \\(H\\) indicates a vector \\(\\mathbb{R}^{D}\\) with high dimensionality \\(D\\). The hyperdimensional vectors are compared to each other by a similarity function \\(\\delta\\). Utilizing the similarity measure, HDC can facilitate cognitive tasks such as memorization, classification, clustering, and more. HDC frameworks designed to support these tasks rely on three fundamental HDC operations that directly correspond to brain functionalities: bundling, binding, and permutation.\n1) Bundling: this operation, denoted as \\(\\mathtt{+}\\), is typically implemented as element-wise addition. If \\(H = H_{1} + H_{2}\\), then both \\(H_{1}\\) and \\(H_{2}\\) are similar to \\(H\\). From a cognitive perspective, it can be interpreted as memorization.\n2) Binding: this operation, denoted as \\(\\mathtt{*}\\), is typically imple-mented as element-wise multiplication. If \\(H = H_{1} * H_{2}\\), then \\(H\\) is dissimilar to both \\(H_{1}\\) and \\(H_{2}\\). Binding also has the important property of similarity preservation.\n3) Permutation: this operator, denoted as \\(p\\), is typically implemented as a rotation of vector elements.\nUsing these three operations enables a hyperdimensional learning framework. Classification involves encoding input features into hypervectors and creating class hypervectors by bundling. Retraining involves adjusting class hypervectors by adding hypervectors of correctly predicted samples and sub-tracting those of misclassified samples, thus refining the class boundaries.\nThe encoding function often uses cosine and sine transforma-tions, as this mapping preserves similarity between inputs. By projecting data into a high-dimensional space, small differences become more distinguishable, enabling robust recognition and generalization even with limited training data."}, {"title": "B. Bridging HDC to Audio Detection", "content": "While the basics of HDC provide the fundamental building blocks, applying them directly to audio detection involves integrating CNN-based feature extraction with hypervector en-coding. The next section shows how we embed HDC operations into an audio sensing framework, transforming raw audio streams into high-dimensional representations and selectively transmitting only data deemed relevant."}, {"title": "C. Audio Intelligent Sensing Framework", "content": "In Figure 1, we present an overview of our framework designed to reduce overall system costs related to network communication, expensive machine learning servers, and stor-age. This is achieved by strategically placing a lightweight AI model in proximity to the microphone sensor, enabling real-time selective transmission of audio data. Leveraging HDC as our lightweight AI model tightly integrated with the sensing circuit, our framework supports online learning, enhancing its adaptability.\nTo ensure coordination between the lightweight model and the buffer, we maintain a buffer size that matches or exceeds the model's maximum inference latency. The buffer operates as a FIFO queue, and the model processes incoming audio frames in order. Since the model's inference is lightweight and near real-time, it completes classification before the oldest data in the buffer is popped. This synchronization prevents data loss and ensures that the model does not miss any segments it needs to evaluate. In rare high-load scenarios, the buffer size can be increased to handle temporary spikes, ensuring that all data is classified before removal.\nOur proposed framework stacks the audio stream in a fixed-size buffer and pops the oldest audio stream data from the buffer if it reaches maximum capacity. During this process, the lightweight AI model determines whether there is audio of interest or not. If the model detects audio of interest, it activates the switch to send out all of the audio data in the buffer through the network communication channel to the costly machine learning server. Adjusting the buffer size not only allows for more contextual data if needed but also helps mitigate false negative detections, particularly useful when audio of interest exhibits time locality characteristics."}, {"title": "D. Near Audio Sensor Model", "content": "As highlighted earlier, our framework governs data trans-mission by identifying audio of interest with time locality features. Figure 2 illustrates the comprehensive pipeline of our near audio sensor model, encompassing three pivotal phases: (a) offline learning, (b) deployed framework, and (c) online learning. In the initial phase, the model undergoes training with an existing audio dataset before transitioning into de-ployment. Post-training, our model systematically adjusts its weights based on feedback from the resource-intensive machine learning model to sustain optimal performance over time.\nIn order to deploy the near audio sensor model, we first need to train the model in an offline manner as shown in the Figure 2.(a). First, given an audio dataset \\(D\\), we convert them to sound spectrograms using the Fast Fourier Transform (FFT) al-gorithm with normalization. Now, we generate a labeled dataset by labeling each data according to the Audio of Interest (AoI). For unbalanced scenarios, we use simple random oversampling on the minority AoI samples to ensure that the CNN layers receive sufficient positive samples during training.\nAfter training the CNN layers, we use them as a feature extractor of our HDC model. Using these CNN layers combined with HDC encoding, we generate hypervectors. Negative and positive class hypervectors are formed by bundling their re-spective sets of hypervectors. To further refine the HDC model, we retrain by incrementally adjusting class hypervectors. This process effectively \u201cmoves\u201d the class representations closer to correct samples and away from incorrect ones, thereby sharp-ening decision boundaries without requiring backpropagation or complex layers.\nIn the deployed framework, the lightweight near-sensor model applies FFT and CNN-based feature extraction on in-coming audio. The extracted features are encoded into hyper-vectors and then compared against class hypervectors. If the similarity score with the positive class hypervector surpasses a threshold \\(T_{\text{score}}\\), the data is considered audio of interest and transmitted. We determine \\(T_{\text{score}}\\) from ROC curve analysis on a validation set, choosing a threshold that yields an acceptable trade-off between false positives and false negatives.\nFinally, online learning is facilitated by the cloud-based"}, {"title": "IV. EXPERIMENTS", "content": "The proposed framework has been executed with a software framework and a hardware accelerator. Our software framework is implemented using a combination of PyTorch and NumPy that supports CNN layers, HDC encoding, and classification. We study the effectiveness of our technique over the Urban-Sound8K dataset [18], which is a public audio dataset for urban sound classification applications. The dataset contains 10 classes such as car horn, dog bark, etc. Among these classes, we chose to use the gunshot class as our audio of interest."}, {"title": "B. Evaluation of The Proposed Near-sensor Model", "content": "In the evaluation of our near-sensor model, we focused on a scenario where our objective is to permit a specific level of false positives while maximizing the true positive rate (TPR). Employing the ROC curve evaluation allows us to analyze this scenario and identify model configurations that achieve the highest TPR at a desired false positive rate (FPR).\nCompared to an MLP, HDC achieves better training results even with a limited portion of training data because HDC relies on high-dimensional random projections that preserve similarity and enable robust classification with fewer samples. MLPs typically need more data to effectively adjust their parameters via gradient-based training. In contrast, HDC can immediately construct meaningful class hypervectors from a small number of examples, providing inherent robustness and reducing the data requirement.\nOur initial focus was on evaluating the impact of the feature extractor size, comprised of CNN layers, on achieving a sharp ROC curve. Even with slightly more than 5 layers of CNN, the model achieves an AUC exceeding 0.99, with saturation evident at approximately the maximum AUC. The HDC model displays superior learning ability over the MLP reference, evidenced by its enhanced AUC performance.\nFigure 4 demonstrates the online learning capability of the"}, {"title": "V. CONCLUSIONS", "content": "We present the Hyperdimensional Intelligent Sensing Frame-work, designed for efficient real-time audio processing at the extreme edge. This innovative framework aims to significantly reduce overall system costs, including energy and storage consumption, by incorporating a near-sensor model focused on detecting essential audio data. This selectively transmitted data, crucial for the target task, is efficiently processed on a high-performance cloud server. Our evaluation demonstrates the framework's extreme efficiency, achieving up to 82.1% energy savings with only 1.39% quality loss. The near-sensor model's efficacy is highlighted by its sharp ROC curve, indicating robust performance. Notably, the ASIC implementation of the near-sensor model exhibits exceptional energy efficiency, confirming the framework's ability to substantially decrease total system energy consumption while maintaining a balanced trade-off between energy conservation and quality preservation.\nIn future work, we plan to explore more sophisticated thresh-old selection strategies and consider integrating advanced HDC encodings for even greater robustness. Additionally, construct-ing a physical prototype and conducting on-site experiments will help validate our simulation-based findings and further refine the system for practical deployments."}]}