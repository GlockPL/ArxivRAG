{"title": "Mixture of Attention Yields Accurate Results for Tabular Data", "authors": ["Xuechen Li", "Yupeng Li", "Jian Liu", "Xiaolin Jin", "Tian Yang", "Xin Hu"], "abstract": "Tabular data inherently exhibits significant feature heterogeneity, but existing transformer-based methods lack specialized mechanisms to handle this property. To bridge the gap, we propose MAYA, an encoder-decoder transformer-based framework. In the encoder, we design a Mixture of Attention (MOA) that constructs multiple parallel attention branches and averages the features at each branch, effectively fusing heterogeneous features while limiting parameter growth. Additionally, we employ collaborative learning with a dynamic consistency weight constraint to produce more robust representations. In the decoder stage, cross-attention is utilized to seamlessly integrate tabular data with corresponding label features. This dual-attention mechanism effectively captures both intra-instance and inter-instance interactions. We evaluate the proposed method on a wide range of datasets and compare it with other state-of-the-art transformer-based methods. Extensive experiments demonstrate that our model achieves superior performance among transformer-based methods in both tabular classification and regression tasks.", "sections": [{"title": "1 Introduction", "content": "As society rapidly advances, tabular data has become one of the most widely used data formats Zhou et al. [2025]. Although the information contained in tabular data is highly complex and manifold, due to its significant economic value, there has been increasing interest among researchers in analyzing and studying tabular data, leading to the proposal of various solutions Klambauer et al. [2017]; Wang et al. [2021]; Popov et al. [2020]; Bonet et al. [2024]; Arik and Pfister [2021]; Ye et al. [2024].\nAmong these methods, inspired by the amazing success of the transformer architecture in NLP and speech, an increasing number of transformer-like architectures have been introduced to handle"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Attention Mechanisms", "content": "The attention mechanism was introduced to improve the performance of the encoder-decoder model for machine translation Vaswani [2017]. It reveals the relative importance of each component in a sequence compared to the other components. Usually, self-attention and cross-attention are the most widely used in transformer architecture models. Self-attention captures relationships within a single input sequence, and cross-attention captures relationships between elements of different input sequences. Somepalli et al. Somepalli et al. [2021] introduce self-attention and inter-sample attention mechanisms for the classification of tabular data. They utilize self-attention to focus on individual features within the same data sample and inter-sample attention to represent the relationships between different data samples of the table. In our approach, we introduce two novel attention mechanisms: the Mixture of Attention (MOA) for self-attention and Inter-instance Attention Incorporating with Labels (IAIL) for cross-attention, to enhance data utilization and feature representation."}, {"title": "2.2 Collaborative Learning", "content": "Collaborative learning Dillenbourg [1999] refers to methods and environments in which learners work together on a shared task, with each individual relying on and being responsible to others. In pixel-wise tasks, collaborative policy has been used to train different subset networks and to ensemble the reliable pixels predicted by these networks. As a result, this strategy covers a broad spectrum of pixel-wise tasks without requiring structural adaptation Ke et al. [2020]. Here, we use collaborative policy to construct branch weight in MOA block during training, with the weight proportional to the difference between the branch output and the true label. It turns out that this dynamic consistency constraint can provide substantial benefit to enrich feature representations and achieve impressive results for tabular tasks."}, {"title": "3 Methods", "content": "In this section, we will provide a detailed introduction to the proposed network MAYA. To present the information more clearly, we will adopt a top-down pattern for our description, starting from the holistic view of the network and gradually delving into its basic blocks."}, {"title": "3.1 Notations", "content": "In this work, let's consider supervised learning problems on tabular data. A typical dataset with N instances can be denoted as $D = \\{(x_i, y_i)\\}_{i=1}^N$, where $x_i$ represents the features and $y_i$ the label of the i-th instance, respectively. Specifically, $x_i$ contains both numerical features $x_{num}$ and categorical"}, {"title": "3.2 Tokenizer", "content": "MAYA is built upon the transformer-like architecture. Consequently, prior to entering the network, instances need to be processed by a tokenizer, which maps the instances from the original feature space into an embedding space. For an instance $x_i \\in \\mathbb{R}^k$, we define the d-dimension embeddings obtained by tokenizer as $z_i$:\n$z_i = \\text{Tokenizer}(x_i), z_i \\in \\mathbb{R}^{k \\times d}$                                                                                                            (1)\nThe tokenizer can be implemented in various ways, while in this paper, we primarily utilize the tokenizer of Gorishniy et al. [2021], which embeds numerical features through element-wise multi-plication and categorical features via a lookup table.\nNote that in contrast to Gorishniy et al. [2021], we observe that incorporating an activation function subsequent to the tokenizer enhances the predictive performance of the network. We attribute this to the manner in which the tokenizer's weights and biases are initialized: specifically, the use of Kaiming initialization facilitates performance improvement when combined with the ReLU activation function. To further investigate this detail, we will conduct ablation study in Section 4.3.\nBefore the extraction of features, a common practice is to append the embedding of a special token, i.e. [CLS] token, to $z_i$:\n$z_i^0 = \\text{Stack}[[CLS], z_i], z_i^0 \\in \\mathbb{R}^{(k+1)\\times d}$                                                                                                                          (2)\nwhere the subscript \"0\" denotes the initial input to the network, i.e., the input to the first layer of the entire network."}, {"title": "3.3 Encoder-Decoder Architecture", "content": "Our network employs an encoder-decoder framework in its overall design, as demonstrated in Fig.2. The initial input $z_i^0$ undergoes processing by the encoder, resulting in an output $\\hat{z_i}$ (i.e. the processed [CLS] token, which we adopt as the representation of an instance). Subsequently, $\\hat{z_i}$ is then passed through the decoder to yield $\\tilde{z_i}$. Finally, a predictor is employed to generate the ultimate prediction result p. The whole procedure can be formulated as follows:\n$\\hat{z_i} = \\text{Encoder}(z_i^0), \\hat{z_i} \\in \\mathbb{R}^d$                                                                                                            (3)\n$\\tilde{z_i} = \\text{Decoder}(\\hat{z_i}), \\tilde{z_i} \\in \\mathbb{R}^d$                                                                                                                                     (4)\n$p = \\text{Predictor}(\\tilde{z_i})$                                                                                                                     (5)"}, {"title": "3.4 Encoder", "content": "In the field of tabular data, model performance often varies significantly across different datasets. To achieve consistently strong performance, we recommend two strategies: deep and wide integration. Deep integration involves the traditional cascading of basic blocks. For wide integration, we introduce a novel MOA block, where multiple individual attentions operate in parallel. Therefore, an encoder consisting of $L_e$ MOA blocks can be formulated as:\n$z^l = \\text{MOA}(z^{l-1}), l \\in [1, L_e]$                                                                                                        (6)"}, {"title": "3.4.1 Mixture of Attention (MOA)", "content": "An important characteristic of tabular data lies in the heterogeneity of its features, which often renders reliance on a single feature extractor insufficient. For the classical transformer, the Multi-Head Attention (MHA) mechanism enables the model to focus on information from different representation subspaces through various heads Vaswani [2017]. This implies that by increasing the number of heads, we can enhance the transformer's capacity to process diverse features. However, this approach introduces two primary issues: firstly, since the feature subspaces of each head in MHA are concatenated together, increasing the number of heads enlarges the size of attention output hidden states, which substantially increases the number of parameters in the subsequent FFN, leading to higher computational overhead. Secondly, the concatenated features undergo an output projection, which results in feature mixing, thereby diminishing the diversity among multiple head subspaces.\nTo address these issues, we propose the Mixture of Attention (MOA, as demonstrated in Fig.1). It constructs multiple parallel attention branches based on MHA units and then averages the features at the branch level. This method ensures the diversity of feature extraction while maintaining a constant size of hidden states, thus eliminating the need to increase the parameter count in the FFN. Furthermore, averaging the features acts as a form of regularization, mitigating the risk of overfitting and enhancing robustness, as well as minimizing the impact of potential failures of individual MHAS.\nSpecifically, the input is replicated multiple times and fed into several completely identical attention branches. These outputs are then processed through a shared FFN (Feed-Forward Network), followed by individual layer normalization. Ultimately, these outputs are averaged and normalized by another layer normalization to serve as the output of a single MOA block. In a rigorous sense, the output $z^l$ of l-th MOA block, which comprises n parallel attention branches, is derived as follows:\n$z^l = \\text{LayerNorm}(\\frac{1}{n}\\sum_{j=1}^n \\text{Branch}_j(z^{l-1}) + z^{l-1})$                                                                            (7)\nwhere $\\text{Branch}_j(\\cdot) := \\text{LayerNorm}_j(\\text{FFN}(\\text{Attention}_j(\\cdot) + (\\cdot)))$."}, {"title": "3.4.2 Intra-block Balance: Branch Weight of MOA", "content": "The output of a single MOA block is obtained by averaging the results from multiple attention branches, suggesting an equitable contribution of each attention branch's output to the final result. If we regard each attention branch as an individual entity, then this exhibits a paradigm of collaborative learning. On this basis, we can explicitly introduce a dynamic consistency constraint for each branch, leading us to propose a balancing strategy within the MOA block, namely, the utilization of branch weights (Fig.3). Specifically, during training, we attach a shared Predictor (same as in Eq.5) to each attention branch. If the prediction of a particular branch deviates significantly from the ground truth quantified using Mean Squared Error (MSE) loss for regression tasks and Cross-Entropy (CE) loss for classification tasks we assign a larger weight to this branch. These weights are computed by applying the softmax function across the losses of all branches. Consequently, branches exhibiting larger deviations receive greater penalty, thereby achieving a balance among the branches. To mitigate significant fluctuations in these branch weights, we apply Exponential Moving Average (EMA) for smoothing purposes. In conclusion, the formulation of branch weight $W_B$ can be articulated as follows:\n$W_B = \\text{EMA}(\\text{Softmax}(s))$                                                                                                            (8)"}, {"title": "3.4.3 Inter-block Averaging", "content": "In pursuit of not only intra-block balance but also inter-block efficacy in feature extraction, we proceed to average the outputs of the stacked $L_e$ MOA blocks and append a LayerNorm layer, thereby yielding the ultimate output of the encoder. Therefore, in summary, combining Eq.3 and Eq.6, the output $\\hat{z_i}$ of the encoder is actually obtained through the following calculation:\n$\\hat{z_i} = \\text{LayerNorm}(\\frac{1}{L_e}\\sum_{l=1}^{L_e} z^l)$                                                                                              (11)"}, {"title": "3.5 Decoder", "content": "The decoder consists of a novel inter-instance cross attention module. Given the encoder's proficiency in extracting representations for each individual instance, we innovatively conceptualize a set of instances $Z = \\{\\hat{z_i}\\}$ as a sequence, whether it comprises the entire training dataset or a specific batch (i.e. $|Z| \\leq N$), with each instance functioning as a token within this sequence. For example, in PyTorch, we can unsqueeze a tensor with a shape of [batchsize, d] output by encoder into a tensor of shape [1, batchsize,d]. This conceptual shift facilitates the direct utilization of the encoder's output as the input for the inter-instance attention mechanism. On this basis, we introduce a novel foundational module within the decoder, which not only incorporates the inter-instance attention mechanism but also seamlessly integrates label information."}, {"title": "3.5.1 Inter-instance Attention Incorporated with Labels", "content": "We refer to the fundamental component of the decoder as IAIL (i.e. Inter-instance Attention Incor-porated with Labels), which actually employs a cross-attention mechanism, hence it accommodates multiple inputs, as demonstrated in Fig.4. For clarity, let us designate one input to the l-th IAIL (denoted as IAIL$^l$) as $\\tilde{Z}^{l-1} = \\{\\tilde{z_i}^{l-1}\\}$, with its output being $\\tilde{Z}^{l} = \\{\\tilde{z_i}^{l}\\}$; specifically, the input to the first IAIL is defined as $\\tilde{Z}^0 = \\{\\hat{z_i}\\}$. To effectively integrate label information into the processing of IAIL, we introduce a trainable Label Embedding Layer (LEL), which projects labels into an embedding space, formulated as:\n$\\hat{y_i} = \\text{LEL}(y_i), \\hat{y_i} \\in \\mathbb{R}^d$                                                                                             (12)"}, {"title": "4 Experiments", "content": "In this section, we will compare the proposed MAYA to SOTA transformer-based methods on several real-world classification and regression datasets to demonstrate its superiority. Furthermore, we embark on a thorough investigation into the attributes of MAYA through ablation studies."}, {"title": "4.1 Experimental Setting", "content": "Datasets We have selected 14 datasets from previous works Huang et al. [2020]; Somepalli et al. [2021]; Gorishniy et al. [2021], with detailed information and statistics provided in Table 1. All datasets are partitioned into training, validation, and test sets in a ratio of 64%/16%/20%, respectively, where validation set is utilized for hyper-parameter tuning and early stopping. The preprocessing of all datasets is conducted according to Gorishniy et al. [2021].\nEvaluation We primarily adhere to the settings of Gorishniy et al. [2021] for our evaluation. Each dataset undergoes experimentation with 15 random seeds, and the average results on the test set are reported. For classification tasks, we employ accuracy as the evaluation metric; for regression tasks, we report the Root Mean Squared Error (RMSE). Besides, we also report the average rank (lower is better) of each method across all datasets to reflect the overall performance of a particular method.\nComparison Methods The network proposed in this paper will be benchmarked against several SOTA transformer-based methods, including AutoInt (AT) Song et al. [2019], TabTransformer (TT) Huang et al. [2020], SAINT (SNT) Somepalli et al. [2021], FT-Transformer (FTT) Gorishniy et al. [2021], ExcelFormer (EF) Chen et al. [2024] and AMFormer (AMF) Cheng et al. [2024]. All methods are based on their officially released implementations.\nImplementation Details Regarding the comparison methods, if results are reported in the re-spective papers, we directly cite them. Otherwise, hyper-parameter tuning is conducted within the corresponding space (see Appendix 6.1 for the detailed description). In cases where multiple results are available, we opt to cite the best one. For our method, we leverage Optuna Akiba et al. [2019] to execute 100 trials to ascertain the best-performed hyper-parameter configuration, which will be utilized across all 15 random seeds. The hyper-parameter tuning for all methods are based on training and validation sets."}, {"title": "4.2 Main Results", "content": "The comparison results of MAYA against other transformer-based methods are presented in Table 2. It is evident that MAYA significantly outperforms other methods in terms of the average rank, indicating that the overall performance of MAYA is currently the best among transformer-based models."}, {"title": "4.3 Ablation Studies", "content": "In this section, we undertake an exhaustive analysis of the design characteristics of the proposed network, utilizing six diverse datasets: BA/BL/QS/SE for classification and CA/SH for regression."}, {"title": "5 Conclusion", "content": "In this paper, we present MAYA, a novel transformer-based encoder-decoder architecture for tabular data. The design of dual-attention mechanism effectively promote the information fusion within and between instances. Additionally, to improve the ability of extracting diverse features, we introduce MOA to sufficiently utilize multiple attention branches. Experiments on several popular public datasets demonstrate that our method performs impressively in processing tabular data. Furthermore, ablation studies investigate the effectiveness of the MOA blocks and activation operations after tokenizer in our model, suggesting that the multi-branch structure of the MOA blocks, along with activation functions subsequent to tokenizer, is essential for improving the performance of the proposed method for tabular tasks."}, {"title": "6 Appendix", "content": null}, {"title": "6.1 Hyper-parameter Tuning Space", "content": "Here we provide hyper-parameter tuning spaces used for Optuna tuning for all methods in Section 4.2."}, {"title": "6.1.1 MAYA", "content": "The hyper-parameter tuning space for MAYA is presented in Table 5."}, {"title": "6.1.2 AutoInt", "content": "The hyper-parameter tuning space for AutoInt is presented in Table 6."}, {"title": "6.1.3 Tab Transformer", "content": "The hyper-parameter tuning space for TabTransformer is presented in Table 7."}, {"title": "6.1.4 SAINT", "content": "The hyper-parameter tuning space for SAINT is presented in Table 8."}, {"title": "6.1.5 FT-Transformer", "content": "The hyper-parameter tuning space for FT-Transformer is presented in Table 9."}, {"title": "6.1.6 Excel Former", "content": "The hyper-parameter tuning space for ExcelFormer is presented in Table 10."}, {"title": "6.1.7 AMFormer", "content": "The hyper-parameter tuning space for AMFormer is presented in Table 11."}, {"title": "7 Ablation Studies", "content": null}, {"title": "7.1 Implementation Details", "content": "In this section, we provide implementation details of the ablation studies on the properties of MOA block. The first two experimental settings (i.e., MHAhidden_size and MHAhead_dim) ensure that the number of heads in MHA matches that in MOA of MAYA, which is formulated as:\n$num\\_head_{MHA} = num\\_heads_{per\\_branch} \\times num\\_branch$                                                                            (19)\nFor the first setting, MHAhidden_size, we ensure that the hidden_size is consistent with MOA. The hidden_size for MHA should be:\n$hidden\\_size_{MHA} = head\\_dim_{MHA} \\times num\\_head_{MHA}$                                                                                              (20)"}]}