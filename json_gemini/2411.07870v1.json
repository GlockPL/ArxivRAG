{"title": "Trustful LLMS: Customizing and Grounding Text Generation with Knowledge Bases and Dual Decoders", "authors": ["Xiaofeng Zhu", "Jaya Krishna Mandivarapu"], "abstract": "Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process.", "sections": [{"title": "1 Introduction", "content": "Adapting an LLM to a specific domain is challenging for several reasons: 1) Pre-trained LLMs cover general knowledge and cannot access private data (even during fine-tuning) due to privacy, copyright, and policy constraints. 2) The grounding of generated texts can change depending on specific contexts, such as domain or timestamp. Recent studies mostly focus on detecting hallucinations and using multiple LLMs when hallucinations occur. 3) Business logic and structured data, such as databases and private knowledge bases, are required when integrating customized LLMs into production systems and presenting them to customers or users.\nWe offer two methods for correcting hallucinations (beyond merely detecting them (Wan et al., 2024; Li et al., 2023a; Ji et al., 2023)): 1) Applying post-processing to generated texts using knowledge triplets, and 2) Proposing guided generation via Dual Decoders. Inspired by common practices like Retrieval-Augmented Generation (RAG) (Li et al., 2024), which retrieves relevant grounding context and feeds it to an LLM for text generation, we address hallucinations in generated texts from two aspects: 1) Post-editing based on knowledge graphs extracted from the context, and 2) Infusing guided context that contains important knowledge triplets into a generic LLM. Our proposed methods also provide reasoning and create consistent results from generative LLMs, benefiting from both the generation and extraction capabilities of decoder-only LLMs and the groundedness of RAG via the second decoder on the guidance (Le et al., 2020; Wang et al., 2022b).\nIn this work, we elaborate on our real-world commercial application scenario of using LLMs to support customers with Microsoft product inquiries in copilots, where groundedness is key to success. Pre-trained LLMs often lack the relevant knowledge or cannot adapt promptly to changes in the product database updates. Different variants of large language models (LLMs), such as Phi-3.5 (Abdin et al., 2024), ChatGPT (Mohamadi et al., 2023), LLama-3 (Dubey et al., 2024), and Gemma (Team, 2024), are proficient at producing fluent outputs for diverse user queries. Despite their human-like fluency in generating text across a wide range of prompts, large language models suffer from hallucinations (see examples in Figures 2, 3, 4), where parts or the entirety of the generated text lack faithfulness, factuality, or reasoning, yet are presented with a confident tone Ji et al., 2023.\nTo mitigate and correct hallucinations, we leverage guided text generation. Grounding guidance (Socher et al., 2013; Nickel et al., 2011; Lin et al., 2015; Wang et al., 2014; Bordes et al., 2013; Wang et al., 2022a; Grover and Leskovec, 2016), such as knowledge graphs (KGs), has been shown to significantly improve the reliability and factuality of LLMs in recent studies, e.g., KELM (Agar-"}, {"title": "2 Background and Related Work", "content": "Unlike document summarization, RAG, or traditional question answering, our approach benefits from both domain knowledge bases\u2014particularly for groundedness\u2014and the language understanding and generalization capabilities of various pre-trained or customized LLMs. By iterating over the knowledge triplets extracted from the generated text and comparing them to the knowledge triplets extracted from the given context (e.g., results from RAG), we can correct hallucinations (and generated phrases that lack references) using our proposed post-processing algorithm."}, {"title": "2.1 Guided Natural Language Generation", "content": "Prior studies have attempted multiple guidance frameworks, particularly with encoder-decoder models (See et al., 2017; Dou et al., 2020; Hokamp and Liu, 2017; Beurer-Kellner et al., 2024). Unlike GraphRAG (Edge et al., 2024), which utilizes multiple LLM calls to combine knowledge triplets from segments of RAG results, our proposed TrustfulLLM model reduces irrelevant entities and tokens in generated texts to demonstrate its efficiency."}, {"title": "2.2 Hallucination", "content": "Hallucination is considered one of the most prominent drawbacks of Large Language Models, as it leads models to generate inaccurate or false information (Ji et al., 2023; Wan et al., 2024). Model-generated texts may not match the true source content, and the facts presented by the model cannot always be verified from the source. These drawbacks remain significant hurdles in applying large language models (LLMs) to real-world, business-critical, and vitally important applications."}, {"title": "Algorithm 1 Hallucination Correction", "content": "1: Input: \u00dd, G\n2: Output: Y*\n3: Construct knowledge graph g = {ri} from Y\n4: for knowledge triplet t\u2081 = (v\u017e, v\u1ecb, ri) in g do\n5:\n6:\nif v not in G then\nEliminate ri from g and the associated sentence in Y\n7:\nelse\n8:\n9:\nReplace ti and Y based on g\nend if\n10: end for\n11: Assume G is the subgraph of G, and G contains all the entities (nodes) in Y\n12: Y* = Y\n13: while Y* contains cycles do\n14:\nPrune Y to Y* till Y* is a minimum spanning tree of G.\n15: end while"}, {"title": "3 Methodology", "content": "Whether the generated text is factual is determined by the domain source and the given guided context. In our copilot scenario, we always retrieve related context for a user prompt/query and then utilize this context to generate the final response presented to users. The guided context can be a mix of offline or web articles and database records, from which we generate knowledge triplets (Gardner et al., 2017) for groundedness verification and hallucination correction. We propose a post-processing algorithm for correcting hallucinations that can be applied to any LLM outputs, as discussed in Section 3.1. Additionally, we propose a dual-decoder text gener-"}, {"title": "3.1 Post-processing text generation by Correcting Knowledge Triplets", "content": "For generated texts from an LLM, we identify and correct potential hallucinations using knowledge triplets extracted from the RAG context and the generated text output. Specifically, we convert the extracted knowledge triplets from the guided context and the LLM output into graphs G and g, respectively, where each node vi represents either a subject or an object, and the relations between the subject and object serve as bi-directional edges connecting the two nodes. Algorithm 1 explains the hallucination detection and correction process for a given generated text \u0176 and the knowledge graph G extracted from the guided context. In the end, we obtain a corrected/verified output Y*. A knowledge triplet t can be identified given a subject and a relation, or an object and a relation; i.e., we can easily locate and replace the third component when the entity or relation is incorrect in ti, which is composed of subject $v_i^s$, object $v_i^o$, and the relation $r_i$. This algorithm can verify, replace, and prune triplets in Y but does not increase the number of nodes/entities. For instance, given a sentence in RAG result content: \"Microsoft 365 Business Basic is $7.2 dollars per user per month.\", we obtain knowledge triplet ti: ($v_i^s, v_i^o, r_i$) is (Microsoft 365 Business Basic, is, $7.2 dollars per user per month). Since LLM outputs can omit or introduce additional entities, we propose a second method: guided generation via dual decoders."}, {"title": "3.2 TrustfulLLM and Guided Generation via Dual Decoders", "content": "In addition to the contextual embeddings used in Transformers, we embed the guidance text and apply a cross-attention calculation using the hidden states of the two decoders. In this way, we have the grounding/context source embeddings in one decoder and the user prompt in the other decoder, with both decoders sharing weights. We apply cross-attention CROSSATTN(Hp, Hg) by taking the hidden state Hp of the prompt module as the 'query' and the hidden state Hg of the guided"}, {"title": "4 Experiments and Results", "content": "We elaborate the results from the public Microsoft learn.microsoft.com articles and product from www.microsoft.com 1. The M365 dataset comprises approximately 10,000 question-and-answer pairs, including the context from which these question and answers were derived. We conducted our experiments based on that the RAG results (knowledge bases and/or domain articles) that are trustworthy. For fine-tuned LLMs, we leverage LoRA (Hu et al., 2021) and set the number of epochs to be over 400, which is relatively higher than in regular LORA fine-tuning."}, {"title": "4.1 Tasks and Datasets", "content": "We use a combination of metrics including ROUGE-L, METEOR, GPT-Similarity, Groundedness (Appendix A.4), and BERTScore. ROUGE-L assesses the longest common subsequence between the generated and reference texts, capturing fluency and coherence. METEOR goes further by considering synonyms, stemming, and word order, providing a more nuanced evaluation. Groundedness rated 1-5 by GPT-4 ensures that the generated content is closely aligned with the source material. GPT-Similarity rated 1-5 by GPT-4 measures the semantic similarity between generated and reference texts, while BERT Score leverages pre-trained language models to evaluate the quality of the generated text on a deeper, contextual level. Together,"}, {"title": "4.3 Effects of Applying HC and TrustfulLLM", "content": "We take a incorrect & incomplete statement from an LLM as a straightforward example: \"Domain registrar that support all DNS records required for Microsoft 365 are GoDaddy and Oray.\" After we apply HC, HC corrects this output as follows: \"Domain registrars that support all DNS records required for Microsoft 365 are Oray, HiChina, east.net, and BIZCN.\"\nIn our production systems, we convert the nodes at Line 4 of Algorithm 1 into embeddings using a pre-trained transformer model, allowing us to find semantically related subjects/objects using the cosine similarity and a heuristic similarity threshold. For example, \"M365 Business Basic\" can be mapped to \"Microsoft 365 Business Basic\". When offline & pre-calibrated knowledge triplets are available, especially for user prompts related to Microsoft product information, we store the embeddings using the FAISS(Douze et al., 2024) 2 and combine them with the knowledge triplets extracted in the real-time RAG context.\nLLMs can generate content that does not originate from the RAG context, which may not always be a hallucination. However, HC can make the outputs more consistent and better aligned with the RAG & guided context. For instance, given a user prompt:\nWhat is the price of Microsoft 365 Business Basic?"}, {"title": "4.4 Commercial Application and Constraints", "content": "In our commercial system, we first apply an intent detection to user prompts to filter out enquiries that are not related to our business then apply a retrieval model to obtain most relevant internal documents, records in product databases. We only reply on the groundedness and correctness of the retrieval results, i.e, phrases in AI generated texts that cannot be referenced from the RAG results are eliminated. For phrases that are semantically equivalent to the RAG results we still do a replacement using the knowledge triplet correction to keep consistent responses. We have also thoroughly conducted Red Teaming evaluations on various Responsible AI metrics such as harmful content, IP infringement, jailbreaking, groundedness, etc. Though we highligh our proposed halluciation correction algorithm and the dual decoder architecture, the upstream RAG and intent detection models can be combined in a multi-task modeling process."}, {"title": "5 Conclusion", "content": "We have addressed grounding issues in LLMs and proposed task-agnostic hallucination correction methods for real-world applications from two perspectives: post-processing to refine LLM outputs and trustful LLM fine-tuning via dual encoders. We have discussed hallucination correction and trustworthy text generation, demonstrating the robustness and resilience of our methods. In the future, we plan to explore heterogeneous modalities, such as structured and spatio-temporal data, knowledge-enriched representations of input tokens (Grover and Leskovec, 2016; Yu et al., 2022; Pan et al., 2023; GAO et al., 2021; Ye et al., 2021), hierarchical relation graphs, and accountability (Li et al., 2023a). We also plan to study model bias, aggregation for federated learning (Zheng et al., 2023; Hashemi et al., 2021), and privacy-preserving issues (Hashemi et al., 2021). Additionally, we aim to reduce the complexity of LLMs through parameter-efficient fine-tuning."}, {"title": "A Appendix", "content": "We show examples where various LLMs generate hallucinations."}, {"title": "A.1 Hallucination Examples", "content": ""}, {"title": "A.2 Examples of Prompt, RAG Context, and Guided Context", "content": "Prompt: \"... <l|user|> How much is Microsoft 365 Business Basic? <\\l|> <lassistant|> Microsoft"}, {"title": "A.3 Summarization Task", "content": "A summarization task does not have the retrieval component as in RAG. We utilize the graph building step of HC to select the salient sentences from the articles as the guided context. We first extract knowledge triplets from the articles then keep sentences where the most frequent subjects are associated with. We show the comparison of TrustfulLLM + HC + Phi-3.5-mini-instruct, where HC extract knowledge triplets from the articles and the generated texts in the inference stage, and LLM baselines in Table 2."}, {"title": "A.4 Prompt Template for GPT Metrics", "content": "We show the prompts of GPT-Similarity and Groundedness addressed in Section 4."}, {"title": "Prompt for Groundedness", "content": "System:\nYou are an Al assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information.\nUser:\nYou will be presented with a CONTEXT and an ANSWER about that CONTEXT. You need to decide whether the ANSWER is entailed by the CONTEXT by choosing one of the following rating:\n1. 5: The ANSWER follows logically from the information contained in the CONTEXT.\n2. 1: The ANSWER is logically false from the information contained in the CONTEXT.\n3. An integer score between 1 and 5, and if such an integer score does not exist, use 1: It is not possible to determine whether the ANSWER is true or false without further information.\nRead the passage of information thoroughly and select the correct answer from the three answer labels. Read the CONTEXT thoroughly to ensure you know what the CONTEXT entails. Note that the ANSWER is generated by a computer system, so it can contain certain symbols, which should not be a negative factor in the evaluation.\nIndependent Examples:\nExample Task #1 Input:\n{\"CONTEXT\": \"Some are reported as not having been wanted at all.\", \"QUESTION\": \"\", \"ANSWER\": \"All are reported as being completely and fully wanted.\"}\nExample Task #1 Output:\n1\nExample Task #2 Input:\n{\"CONTEXT\": \"Ten new television shows appeared during the month of September. Five of the shows were sitcoms, three were hourlong dramas, and two were news-magazine shows. By January, only seven of these new shows were still on the air. Five of the shows that remained were sitcoms.\", \"QUESTION\": \"\", \"ANSWER\": \"At least one of the shows that were cancelled was an hourlong drama.\"}\nExample Task #2 Output:\n5\nExample Task #3 Input:\n{\"CONTEXT\": \"In Quebec, an allophone is a resident, usually an immigrant, whose mother tongue or home language is neither French nor English.\", \"QUESTION\": \"\", \"ANSWER\": \"In Quebec, an allophone is a resident, usually an immigrant, whose mother tongue or home language is not French.\"}\nExample Task #3 Output:\n5\nExample Task #4 Input:\n{\"CONTEXT\": \"Some are reported as not having been wanted at all.\", \"QUESTION\": \"\", \"ANSWER\": \"All are reported as being completely and fully wanted.\"}\nExample Task #4 Output:"}, {"title": "Actual Task Input:", "content": "{\"CONTEXT\": {{context}}, \"QUESTION\": \"\", \"ANSWER\": {{response}}}\nReminder: The return values for each task should be correctly formatted as an integer between 1 and 5. Do not repeat the context and question."}, {"title": "Actual Task Output:", "content": ""}, {"title": "Prompt for GPT-Similarity]", "content": "System:\nYou are an Al assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information.\nUser:\nEquivalence, as a metric, measures the similarity between the predicted answer and the correct answer. If the information and content in the predicted answer is similar or equivalent to the correct answer, then the value of the Equivalence metric should be high, else it should be low. Given the question, correct answer, and predicted answer, determine the value of the Equivalence metric using the following rating scale:\n\u2022 One star: the predicted answer is not at all similar to the correct answer\n\u2022 Two stars: the predicted answer is mostly not similar to the correct answer\n\u2022 Three stars: the predicted answer is somewhat similar to the correct answer\n\u2022 Four stars: the predicted answer is mostly similar to the correct answer\n\u2022 Five stars: the predicted answer is completely similar to the correct answer\nThis rating value should always be an integer between 1 and 5. So the rating produced should be 1, 2, 3, 4, or 5. The examples below show the Equivalence score for a question, a correct answer, and a predicted answer."}, {"title": "Actual Task Output:", "content": "Question: What is the role of ribosomes?\nCorrect answer: Ribosomes are cellular structures responsible for protein synthesis. They interpret the genetic information carried by messenger RNA (mRNA) and use it to assemble amino acids into proteins.\nPredicted answer: Ribosomes participate in carbohydrate breakdown by removing nutrients from complex sugar molecules.\nStars: 1\nQuestion: Why did the Titanic sink?\nCorrect answer: The Titanic sank after it struck an iceberg during its maiden voyage in 1912. The impact caused the ship's hull to breach, allowing water to flood into the vessel. The ship's design, lifeboat shortage, and lack of timely rescue efforts contributed to the tragic loss of life.\nPredicted answer: The sinking of the Titanic was a result of a large iceberg collision. This caused the ship to take on water and eventually sink, leading to the death of many passengers due to a shortage of lifeboats and insufficient rescue attempts.\nStars: 2\nQuestion: What are the health benefits of regular exercise?\nCorrect answer: Regular exercise can help maintain a healthy weight, increase muscle and bone strength, and reduce the risk of chronic diseases. It also promotes mental well-being by reducing stress and improving overall mood.\nPredicted answer: Routine physical activity can contribute to maintaining ideal body weight, enhancing muscle and bone strength, and preventing chronic illnesses. In addition, it supports mental health by alleviating stress and augmenting general mood.\nStars: 5\nQuestion: {{query}}\nCorrect answer: {{ground_truth}}\nPredicted answer: {{response}}\nStars:"}]}