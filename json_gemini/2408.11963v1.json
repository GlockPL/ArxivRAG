{"title": "Real-Time Incremental Explanations for Object Detectors", "authors": ["Santiago Calder\u00f3n-Pe\u00f1a", "Hana Chockler", "David A. Kelly"], "abstract": "Existing black box explainability tools for object detectors rely on multiple calls to the model, which prevents them from computing explanations in real time. In this paper we introduce IncX, an algorithm for real-time incremental approximations of explanations, based on linear transformations of saliency maps. We implement IncX on top of D-RISE, a state-of-the-art black-box explainability tool for object detectors. We show that IncX's explanations are comparable in quality to those of D-RISE, with insertion curves being within 8%, and are computed two orders of magnitude faster that D-RISE's explanations.", "sections": [{"title": "Introduction", "content": "Al models are now a primary building block of most computer vision systems. However, the inherent opacity of some of these models (e.g., neural networks) together with the sharp increase in their size and complexity, slow their adoption, especially in safety-critical applications, due to the need to understand the reasoning of the model (Ananthaswamy Anil 2023). Explainable AI (XAI) is emerging as a promising solution to mitigate the aforementioned \"black box\" problem, providing insights into the decision-making process of machine learning models. XAI can be further divided into \"white-box\" and \"black-box\" methods. White-box methods have access to the internals of the model, and are hence tailored to a particular model architecture, whereas black-box methods can only access model outputs and are agnostic to the internal implementation of the model.\nImage classification is of particular interest in the context of XAI, as images do not, as a rule, have clearly defined sets of features that an AI model uses for their classification. Object detectors are a particular type of image classification tool which detect and classify objects in real-time, making them essential for tasks such as autonomous driving, as well as for video analysis. Many XAI techniques can be adapted to explaining object detectors: in particular, D-RISE (Petsiuk et al. 2021) builds upon RISE (Petsiuk, Das, and Saenko 2018) by implementing detection vectors that encode the bounding box locations and the classification information. Unfortunately, algorithms used by black-box XAI tools require multiple calls to the model on each input, rendering them unsuitable for real-time applications. Even a state-of-the-art XAI tool such as D-RISE takes 0.02 seconds to produce an explanation of a single frame of video (Kuroki and Yamasaki 2024). Using another model to estimate explanations for the first model, as done by FastSHAP (Jethani et al. 2022) is even more problematic, using a black box \u2013 another model to estimate the explanation of the first black box. Such an approach leaves the explainer model unexplained.\nWe introduce IncX, or Incremental eXplanations, a black-box XAI algorithm and tool for explaining object detectors in real-time that avoids multiple calls to the model and incurs a negligible overhead over the object classification time. This allows near real-time explanation processing for video data and efficient processing of large amounts of time series image data, making Incx useful for explanations of autonomous driving decisions among other applications, for example, an efficient detection of the best locations to place advertisements in video content. IncX tracks saliency maps and explanations from one frame to the next, applying translations and scaling to the saliency map generated in the first frame. Unlike FastSHAP, it does not rely on a separate machine learning model that would necessitate its own explanations.\nWe show that IncX computes explanations that are comparable in quality to D-RISE, but outperforms D-RISE in speed by two orders of magnitude (see Figure 1 for one example of a comparison between D-RISE and IncX). To the best of our knowledge, this is the first black-box real-time XAI tool for object detectors.\nThe code, the benchmark sets, proofs and the full tables of results are submitted as a part of the supplementary material."}, {"title": "Related Work", "content": "We survey the related work in object detection, tracking, and explainability. Since this work focuses on black-box explainability, we do not include white-box XAI methods.\nExplainability for image classifiers There is a large body of work on explainability for image classifiers. Common black-box tools are LIME (Ribeiro, Singh, and Guestrin 2016), Integrated Gradients (Sundararajan, Taly, and Yan 2017), SHAP (Lundberg and Lee 2017), RISE (Petsiuk, Das, and Saenko 2018), and ReX (Chockler, Kelly, and Kroening 2023). These tools and algorithms define and output explanations in different formats: functions, models and landscapes. There is a straightforward conversion of different formats of explanations to saliency maps, which rank input pixels based on their contribution to the model classification. Black-box tools generally rely on multiple calls to the model, computing explanations from these results, and hence incurring a significant overhead on top of the image classifier.\nObject detection The object detection task is focused in classifying and finding objects under low latency.\nYOLO, a collection of one-stage algorithms, has enjoyed a considerable popularity due to its balance between efficiency and accuracy. It divides the image into a grid of cells and assigns a set of class probabilities, a bounding box, and a confidence value. Then it performs non-maximum suppression to remove overlapping bounding boxes. The latest, 10th version addresses many of the shortcomings of the original YOLO (Wang et al. 2024). Faster R-CNN (Ren et al. 2015) is a detection algorithm built on R-CNN (Girshick et al. 2014) and Fast R-CNN (Girshick 2015). The two-stage process uses a region proposal network (RPN) to identify regions within the image that contain an object. It then takes those regions and warps them into a classifier network to find the probabilities of the objects inside the image.\nTransformer-based detectors have been popular since the introduction of the transformer architecture, which revolutionized the Natural Language Processing (NLP) domain, as it introduced the self-attention mechanism (Vaswani et al. 2017). The Real-Time Detection Transformer (RT-DETR) leverages the same concept for object detection tasks (Zhao et al. 2024).\nExplainability for object detectors Several XAI algorithms can be modified to provide explainability for object detectors. There are only a few black-box algorithms that generate saliency maps without relying on the model architecture. D-RISE (Petsiuk et al. 2021) is a modification of RISE for black-box object detectors, using the basic underlying idea of RISE to generate a set of random masks (Petsiuk, Das, and Saenko 2018). D-RISE also introduces the concept of a detection vector, composed of localization information (bounding boxes), object score, and classification information. It then uses this vector to compute a weight for each mask and construct a saliency map given by the weighted sum of masks. In contrast, FSOD (Kuroki and Yamasaki 2024) adapts the loss function of FastSHAP to train an explainer model with an architecture inspired by UNet to account for classification and location of the detected object (Jethani et al. 2022). Although this approach exhibits a considerable faster performance than the previously mentioned ones, its major drawback is that lacks explainability for the explainer model itself. Hence, it is impossible to verify that the generated Shapley values are correct, unless they"}, {"title": "Incremental Explanations", "content": "In this section we present the theoretical foundations for our saliency map and explanation transformation and the algorithm IncX, which employs this transformation to avoid calls to the object detector in subsequent frames. An initial saliency map is assumed to be computed by an external XA\u0399 tool. IncX is agnostic to the XAI tool as well as to the model, as it does not assume access to the model internals or a specific XAI algorithm. We require only that the XAI output is a saliency map."}, {"title": "Theoretical Foundations", "content": "The intuition behind our algorithm is based on the following observation. As long as the detected object moves in 3D without turning or deforming, its projection on 2D can only undergo a combination of scaling and linear translation (for instance, if a car gets closer, its 2D projection scales up). We prove that given such a transformation of the 2D projection of the detected object and the initial saliency map, the saliency map of the transformed object can be computed from the original saliency map using a similar transformation, and the initial explanation, mapped into the transformed saliency map should remain valid. The formal proof is based on known results from probability theory and is quite involved (see the supplementary material).\nThe core of our approach is treating the saliency map as a probability mass function (pmf) and then applying the well-understood mathematics of pmf transformation to estimate a pmf that represents the new, updated, saliency map. First, we need to convert our initial saliency map into a pmf. This is easily done by normalising the values of the saliency map so that they sum to 1.\nNow we use Theorem 1 (Hogg, McKean, and Craig 2019) to help us define the transformations of the pmf. This allows us to calculate the new saliency pmf after undergoing linear transformation (see the supplementary material for details). Technically, our pmf is a joint probability mass function P(X, Y). We restate the core transformation result over a single variable from Hogg, McKean, and Craig (2019) here for completeness.\nTheorem 1. (Hogg, McKean, and Craig 2019) Assume X to be a discrete random variable with space $D_X$ and $g$ a one-to-one transformation function. Then the space of Y is $D_Y = \\{g(x) : x \\in D_X\\}$, and the pmf of Y is\n$p_Y(y) = P[g(X) = y] = P[X = g^{-1}(y)] = p_X(g^{-1}(y)).$\nTheorem 1 can be extended to joint pmfs over multiple variables (Hogg, McKean, and Craig 2019, p.100) (see supplementary material).\nHaving established the intuition behind the transformation of the saliency pmf, we address the considerations of our approach concerning transformations involving three-dimensional objects. Specifically, we first describe how such objects get projected onto the image plane. By Forsyth and Ponce (2002), the mapping of points from three-dimensional world coordinates to two-dimensional screen coordinates can be achieved while using a common frame of reference for the world and the camera. Lemma 1 formalizes this projection process.\nLemma 1 (3D world coordinates to 2D screen (camera image) coordinates). Let $\\alpha, \\beta \\in \\mathbb{R}$ denote the intrinsic magnification factors of the camera. The transformation from 3D world coordinates (x, y, z) to 2D screen coordinates (u, v) is given by:\n$\\begin{bmatrix}u\\\\v\\\\1\\end{bmatrix}=K \\begin{bmatrix}\\alpha&0&0&0\\\\0&\\beta&0&0\\\\0&0&1&0\\end{bmatrix} \\begin{bmatrix}X\\\\Y\\\\Z\\end{bmatrix}$ where $K$ represents the intrinsic parameters of the camera, and $(O_x, O_y)$ represents the coordinates at the upper-left corner of the image.\nFurthermore, we formalize the center of the bounding box for the projected object in Definition 1. This concept is leveraged in the scaling and translation operations described in Definition 2, which are fundamental for IncX.\nDefinition 1 (Center Function). The center function $C: 2^{\\mathbb{R}^n} \\rightarrow \\mathbb{R}^n$ maps a set $S \\subset \\mathbb{R}^n$ of points to the center point of the bounding box that encloses these points. For a set $S$ consisting of points $p = (p_1, p_2,..., p_n) \\in \\mathbb{R}^n$ the function is defined as follows:\n$C(S) = \\begin{bmatrix} \\frac{max(p_1) + min(p_1)}{2} \\\\ \\frac{max(p_2) + min(p_2)}{2} \\\\ ... \\\\ \\frac{max(p_n) + min(p_n)}{2} \\end{bmatrix}$"}, {"title": "Incremental Explanations", "content": "where $\\max(p_j)$ and $\\min(p_j)$ denote the maximum and minimum values, respectively, of the j-th coordinate among all points in S.\nDefinition 2 (Scaling and Translation). Let $A = \\{(u,v) \\in \\mathbb{R}^2\\}$ represent a set of points defining any area in the image space.\nScaling is a linear transformation $S : A \\rightarrow \\mathbb{R}^2$, defined as $S(p) = \\gamma(p \u2013 C(A))$, where $p \\in A$ and $\\gamma \\in [\\mathbb{R}^{2\\times 2}$ is a diagonal matrix.\nTranslation is a linear transformation $T : A \\rightarrow \\mathbb{R}^2$, defined as $T(p) = p + \\mu$, where $p \\in A.\nThe scaling transformation S scales p by a factor $\\gamma$ after translating the center C(A) to the origin (0,0). The translation function T shifts the two-dimensional input p by a vector $\\mu\\in \\mathbb{R}^2$.\nWe first provide the intuition behind our theorem and then its formal statement in Theorem 2. Our approach connects the pmf to the 2D projection of a three-dimensional objectdt (Assumption 1). Furthermore, as long as the object being tracked does not leave the frame, turn on its axis, or cease to be detected, the future positions of the object are determined solely by translation and scaling transformations in the three-dimensional space (Assumption 2). Together, these assumptions lead to the statement that future saliency maps can be computed by applying a scaling and translation linear transformation to the current saliency map.\nNotation: Let $Q_t = (Q_{1,t}, Q_{2,t})$ be a vector in the image space at time t, where $Q_{1,t}$ and $Q_{2,t}$ represent random variables along the u-axis and the v-axis, respectively. The pmf of $Q_t$ is $p_{Q_t}(q_t)$, which is derived from the normalized saliency map.\nWe assume that the expected value of the saliency pmf, if it exists, is equal to the center of the bounding box C(KV) of the object $V_e$ as projected into the image space using the matrix K. dt represents the distance between this projected center and the expected value of $Q_t$. The main reason behind this assumption is the homogeneity of our pmf, implying that any scaling or translation of the expected value results in a corresponding scale or shift of the entire pmf, preserving the distribution's proportions. A direct implication of this assumption is that if we move and scale the center of the bounding box, then the saliency map would have a similar shift and scaling.\nAssumption 1 (Expected Value and Bounding Box Center Relation).\n$E[Q_t] = C(KV_t) + \\delta_t$\nAssumption 2 (Constrained Affine Transformation). Any point of an object at time t + 1 is related to the same object at time t by an affine transformation given by:\n$\\begin{bmatrix}X_{t+1}\\\\Y_{t+1}\\\\Z_{t+1}\\\\1\\end{bmatrix} = \\begin{bmatrix}m_x&0&0&0\\\\0&m_y&0&0\\\\0&0&m_z&0\\\\0&0&0&1\\end{bmatrix} + \\begin{bmatrix}X_{t}\\\\Y_{t}\\\\Z_{t}\\\\1\\end{bmatrix}$\nwhere t \u2208 R\u00b3 is an arbitrary translation vector, I \u2208 R3\u00d73 is the identity matrix, and mx, my, and mz \u2208 R are scaling factors.\nFinally, let S be the scaling function, T the translation function, $q_{t+1}$ a coordinate vector in the image space at time t + 1, and $q_t$ a coordinate vector at time t. The following theorem describes the relation between the pmf at time t and at time t + 1 and is instrumental to the Incx algorithm.\nTheorem 2. If Assumptions 1 and 2 hold, then the pmf at time t + 1, $p_{Q_{t+1}}(q_{t+1})$, can be computed from the pmf at time t using the following equation:\n$p_{Q_{t+1}}(q_{t+1}) = p_{Q_t}(S^{-1}(T^{-1}(q_{t+1})))$"}, {"title": "Algorithm Structure", "content": "The Incx algorithm is schematically presented in Figure 2. We list three object detectors, but, as we mentioned earlier, Incx is agnostic to the detector, as it treats it as a black box. As previously stated, the goal of our algorithm is to approximate saliency maps and explanations in real-time for object detection tasks. As a result, we need a starting point from where we can move forward. Hence, the saliency map in the first frame is computed using an XAI tool (Figure 2 lists D-RISE, but it can be any saliency map-producing tool). After this bootstrapping, the goal is to identify a way to extend the saliency map obtained in the first frame and extrapolate from it to create subsequent ones. To achieve this, we perform the transformation defined in Theorem 2.\nThe first involves bootstrapping our method by computing the initial saliency map. This process is quite straightforward and involves calling an object detector N and explainer h. Only the saliency maps found in the first frame will be used as a basis for extrapolation in later frames. For the subsequent frames, we need to implement a different approach to be able to obtain approximate saliency maps in real-time. First we incorporate a new block (see Figure 2), which is the object tracker: SORT (Bewley et al. 2016). This object tracker takes the bounding box of the identified objects and checks if the id corresponds to any of the objects identified in the previous frame (Line 11). Once an object has been identified, the linear transformation is performed (Lines 12-16). This process takes the previous bounding box and the current bounding box and computes \u0394x, \u0394y, \u03b3. These computed transformations are applied to the saliency map, utilizing Theorem 2. We note that when the saliency map is scaled up, gaps may appear in the newly computed pmf, requiring interpolation to fill them.\nIt is important to note that we can track explanations only as long as the object tracker finds the objects identified in the first frame. An object may vanish from the frame or remain undetected for a long time. For reasons of efficiency, we define a timeout that allows us to stop attempting to explain an object which is no longer detectable (Line 20)."}, {"title": "Sufficient explanations", "content": "Thus far, our focus has been primarily on saliency maps. However, in addition to this, it is useful to provide sufficient explanations. A sufficient explanation, informally, is a subset of pixels capable of producing the desired classification. This is a useful sanity check on the saliency maps: the"}, {"title": "Incremental Explanation", "content": "Input: A video v, an object detector N, an explainer h\nOutput: A sequence of saliency maps s and explanations xp\n$\begin{algorithm}\n\\caption{Incremental Explanation}\n\\begin{algorithmic}[1]\n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}\n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}}\n\\REQUIRE A video v, an object detector N, an explainer h\n\\ENSURE  A sequence of saliency maps s and explanations $x_p$\n\\STATE $x_{t-1}, y_{t-1}, width_{t-1}, height_{t-1} \\leftarrow 0, 0, 0, 0$\n\\STATE $id_0, count \\leftarrow 0, 0$\n\\STATE initialize empty saliency map $S_{t-1}$\n\\STATE initialize empty arrays s, xp\n\\FOR{each frame $f_t$ at time t in video v}\n\\STATE $(x_t, y_t, width_t, height_t) \\leftarrow N(f_t)$\n\\IF{t == 0}\\THEN\n\\STATE $S_t \\leftarrow h(f_t)$\n\\ELSE\n\\STATE $id_t \\leftarrow SORT(f_t)$\n\\IF{$id_0 == id_t$} \\THEN\n\\STATE $\\Delta_x \\leftarrow x_t - x_{t-1}$\n\\STATE $\\Delta_y \\leftarrow y_t - y_{t-1}$\n\\STATE $\\gamma \\leftarrow (width_t, height_t) / (width_{t-1}, height_{t-1})$\n\\STATE $s_t \\leftarrow Scale(s_t, \\gamma)$\n\\STATE $s_t \\leftarrow Translate(s_{t-1}, \\Delta_x, \\Delta_y)$\n\\STATE $count \\leftarrow 0$\n\\ELSE\n\\STATE $count \\leftarrow count + 1$\n\\IF{count >= timeout}\\THEN\n\\STATE Restart\n\\ENDIF\n\\ENDIF\n\\ENDIF\n\\STATE $x_{pt} \\leftarrow Explanation(s_t, N, f_t, t)$\n\\STATE $xp \\leftarrow xp + [x_{pt}]$\n\\STATE $s \\leftarrow s + [s_t]$\n\\STATE $width_{t-1}, height_{t-1} \\leftarrow width_t, height_t$\n\\STATE $x_{t-1}, y_{t-1} \\leftarrow x_t, y_t$\n\\STATE $s_{t-1} \\leftarrow s_t$\n\\ENDFOR\n\\RETURN s, xp\n\\end{algorithmic}\n\\end{algorithm}"}, {"title": "Sufficient explanations", "content": "\"hotspots\" on the saliency map should be enough to trigger the appropriate classification. This necessitates the introduction of an \"Explanation\u201d block for both the initial frame and subsequent frames, as illustrated in Figure 2. This implementation ensures that the explanations provided for subsequent frames are correct, meaning they effectively output the intended classification with a bounding box containing an area with the correct classification.\nExtraction of sufficient explanations from saliency maps can be slow. Therefore we adopt the approach used in Algorithm 2. For the initial frame 0, the landscape is divided into l equal sections, demarcated by grey dotted lines in Figure 3a. To efficiently locate an explanation, we employ binary search within these predefined divisions. In Figure 3a, the division containing the explanation threshold is highlighted in red. We assume monotonicity in the model output as pixels are added: once a sufficient set of pixels has been located, the classification no longer changes."}, {"title": "Explanation Algorithm", "content": "Input: Saliency map s, object detector g, image I, time step t, margin \u03b4, number of divisions l (for t = 0) and $l_n$ (for t > 0)\nOutput: Explanation xp\n$\begin{algorithm}\n\\caption{Explanation Algorithm}\n\\begin{algorithmic}[1]\n\\REQUIRE Saliency map s, object detector g, image I, time step t, margin \\delta, number of divisions l (for t = 0) and $l_n$ (for t > 0)\n\\ENSURE Explanation $x_p$\n\\IF{t == 0}\\THEN\n\\STATE $min \\leftarrow min(s)$\n\\STATE $max \\leftarrow max(s)$\n\\STATE $ss \\leftarrow Linspace(min, max, l)$\n\\ELSE\n\\STATE $max \\leftarrow threshold_{t-1} \\cdot (1 + \\delta)$\n\\STATE $min \\leftarrow threshold_{t-1} \\cdot (1 - \\delta)$\n\\STATE $ss \\leftarrow Linspace(min, max, l_n)$\n\\ENDIF\n\\STATE $x_p, threshold_{t-1} \\leftarrow BinarySearch(s, g, I, ss)$\n\\RETURN $x_p$\n\\end{algorithmic}\n\\end{algorithm}"}, {"title": "Experimental Results", "content": "The COCO (Lin et al. 2014) and VOC (Everingham et al. 2010) datasets are typically used for evaluation of explainability in object detection (Kuroki and Yamasaki 2024). However, these datasets consist of independent images that lack temporal continuity, making them unsuitable for our focus on real-time video explanation. LaSOT (Fan et al. 2019), however, includes 85 object classes and over 3.87 million frames in total, which is more appropriate for our experiments. Therefore, we curated a subset comprising two object classes: car and cat. From these classes, we selected the first 300 frames (equivalent to 10 seconds at 30 FPS) from a total of 10 videos, resulting in 3000 frames for our experiments.\nWe executed IncX, using D-RISE to compute the saliency maps, to be able to perform a comparison between Incx and D-RISE. IncX makes no assumptions about how the initial saliency map is produced: we use D-RISE as it works naturally with the different object detectors that we examine. We applied Incx to all the videos, tracking the object with the highest confidence from the first frame. We then computed the saliency maps using D-RISE for all frames where the object was successfully tracked by Incx. All experiments were conducted using the NVIDIA A100 Tensor Core GPU. We executed D-RISE with 1000 mutants and a mask resolution of (4,4). The following versions of the models were used: YOLOv10-n, RT-DETR-1, and the PyTorch implementation of Faster R-CNN.\nFor a one-to-one comparison between each saliency map generated by D-RISE and IncX, we computed a set of metrics commonly used in the field of Explainable AI. Below, we introduce these metrics.\nNormalized Insertion/Deletion curves The insertion process starts with a blank image, adding pixels based on the saliency map, and computes the area under the curve (AUC) after measuring detector confidence. Conversely, the deletion process starts with the full image and gradually removes pixels according to the saliency map. As we are dealing with object detectors, we adjust this metric by multiplying the score by the Intersection over Union (IoU) of the original bounding box. To account for low confidence producing low insertion and deletion values, we normalize the curve by dividing all values by the original image's score."}, {"title": "Experimental Results", "content": "\u03a3(i,j)\u03b5\u03c4 \u039c(i,j)\nEP =\u03a3(i,j)\u03b5\u03c4 \u039c(i,j) + \u03a3(i,j)\u00a2Tm(i,j)\nAs summarized in Table 1, the insertion values for IncX are consistently within 8% of those observed for D-RISE, with all other metrics displaying similar trends. Moreover, the computation time for Incx is two orders of magnitude lower than that of D-RISE (111 times faster for Faster R-CNN), highlighting the suitability of Incx for real-time scenarios.\nGiven that Incx generates an approximation of the true saliency map, it is crucial to evaluate the similarity between the baseline saliency map and the approximation provided by Incx. To achieve this, we need to employ metrics that effectively measure the similarity between these two saliency maps. Mean Squared Error (MSE) is not ideal for this purpose, as it may not provide a clear indication of perceptual similarity between the maps. Instead, we utilize alternative metrics designed to assess the correspondence between the baseline and Incx saliency maps. The selected metrics for this comparison are the Pearson Correlation Coefficient (CC), Structural Similarity Index (SSIM) (Wang et al. 2004), Dice Coefficient (DC) (Dice 1945), and Jaccard Index (IoU). The results presented in Table 2 show a strong similarity between the approximations implemented by Incx and the original saliency maps created by D-RISE.\nLimitations Incx is based on the assumption that the object it is applied to is rigid. Moreover, the algorithm may not handle rotations (pitch, roll, and yaw) effectively, especially for non-symmetrical objects, given that it primarily addresses scaling and translation. Therefore, the method may not be theoretically advantageous for objects undergoing other types of transformations. Nonetheless, as shown in Section 4, these theoretical limitations have a limited impact in practice. Furthermore, to ensure the accurate computation of explanations in real-time, we assume that explanations for consecutive frames are relatively close on the saliency map, as shown in Figure 3. However, this approach may result in non-minimal explanations."}, {"title": "Conclusions", "content": "Motivated by the need for real-time explanations for object detectors, we have introduced IncX, an algorithm and tool for approximating saliency maps via linear transformations. Our results demonstrate that Incx operates in real time, with negligible overhead over the object detector and is two orders of magnitude faster than generating a new saliency map. Moreover, Incx produces saliency maps closely aligned with freshly computed maps, achieving an average correlation coefficient of approximately 0.8 across all models used, with minimal computational overhead. The quality metrics for assessing explainable AI algorithms show that Incx is comparable to D-RISE, with the insertion score within 8% of D-RISE'S ones.\nIncx handles linear transformations of objects; future work will address the current limitations related to rotation and object deformation. We anticipate future applications of this algorithm in various industries, particularly in self-driving cars and video advertisement placement."}]}