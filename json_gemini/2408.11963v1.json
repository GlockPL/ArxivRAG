{"title": "Real-Time Incremental Explanations for Object Detectors", "authors": ["Santiago Calder\u00f3n-Pe\u00f1a", "Hana Chockler", "David A. Kelly"], "abstract": "Existing black box explainability tools for object detectors\nrely on multiple calls to the model, which prevents them from\ncomputing explanations in real time. In this paper we intro-\nduce Incx, an algorithm for real-time incremental approxi-\nmations of explanations, based on linear transformations of\nsaliency maps. We implement IncX on top of D-RISE, a state-\nof-the-art black-box explainability tool for object detectors.\nWe show that IncX's explanations are comparable in qual-\nity to those of D-RISE, with insertion curves being within\n8%, and are computed two orders of magnitude faster that\nD-RISE's explanations.", "sections": [{"title": "1 Introduction", "content": "Al models are now a primary building block of most com-\nputer vision systems. However, the inherent opacity of some\nof these models (e.g., neural networks) together with the\nsharp increase in their size and complexity, slow their adop-\ntion, especially in safety-critical applications, due to the\nneed to understand the reasoning of the model (Anan-\nthaswamy Anil 2023). Explainable AI (XAI) is emerging as\na promising solution to mitigate the aforementioned \"black\nbox\" problem, providing insights into the decision-making\nprocess of machine learning models. XAI can be further\ndivided into \"white-box\" and \"black-box\" methods. White-\nbox methods have access to the internals of the model, and\nare hence tailored to a particular model architecture, whereas\nblack-box methods can only access model outputs and are\nagnostic to the internal implementation of the model.\nImage classification is of particular interest in the context\nof XAI, as images do not, as a rule, have clearly defined\nsets of features that an AI model uses for their classification.\nObject detectors are a particular type of image classification\ntool which detect and classify objects in real-time, making\nthem essential for tasks such as autonomous driving, as well\nas for video analysis. Many XAI techniques can be adapted\nto explaining object detectors: in particular, D-RISE (Petsiuk\net al. 2021) builds upon RISE (Petsiuk, Das, and Saenko\n2018) by implementing detection vectors that encode the\nbounding box locations and the classification information.\nUnfortunately, algorithms used by black-box XAI tools re-\nquire multiple calls to the model on each input, rendering\nthem unsuitable for real-time applications. Even a state-of-\nthe-art XAI tool such as D-RISE takes 0.02 seconds to pro-\nduce an explanation of a single frame of video (Kuroki and\nYamasaki 2024). Using another model to estimate explana-\ntions for the first model, as done by FastSHAP (Jethani et al.\n2022) is even more problematic, using a black box \u2013 another\nmodel to estimate the explanation of the first black box.\nSuch an approach leaves the explainer model unexplained.\nWe introduce IncX, or Incremental eXplanations, a black-\nbox XAI algorithm and tool for explaining object detectors\nin real-time that avoids multiple calls to the model and in-\ncurs a negligible overhead over the object classification time.\nThis allows near real-time explanation processing for video\ndata and efficient processing of large amounts of time se-\nries image data, making Incx useful for explanations of au-\ntonomous driving decisions among other applications, for\nexample, an efficient detection of the best locations to place\nadvertisements in video content. IncX tracks saliency maps\nand explanations from one frame to the next, applying trans-\nlations and scaling to the saliency map generated in the first\nframe. Unlike FastSHAP, it does not rely on a separate ma-\nchine learning model that would necessitate its own expla-\nnations.\nWe show that IncX computes explanations that are compa-\nrable in quality to D-RISE, but outperforms D-RISE in speed\nby two orders of magnitude (see Figure 1 for one example\nof a comparison between D-RISE and IncX). To the best of\nour knowledge, this is the first black-box real-time XAI tool\nfor object detectors.\nThe code, the benchmark sets, proofs and the full tables of\nresults are submitted as a part of the supplementary material."}, {"title": "2 Related Work", "content": "We survey the related work in object detection, tracking,\nand explainability. Since this work focuses on black-box ex-\nplainability, we do not include white-box XAI methods.\nExplainability for image classifiers There is a large body\nof work on explainability for image classifiers. Common\nblack-box tools are LIME (Ribeiro, Singh, and Guestrin\n2016), Integrated Gradients (Sundararajan, Taly, and Yan\n2017), SHAP (Lundberg and Lee 2017), RISE (Petsiuk, Das,\nand Saenko 2018), and ReX (Chockler, Kelly, and Kroen-\ning 2023). These tools and algorithms define and output ex-"}, {"title": "Multiple Object tracking (MOT)", "content": "Detection-based track-\ning (DBT) is the dominant paradigm in multiple ob-\nject tracking tasks. These methods divide the tracking\nsub-problem into three main tasks: object detection, re-\nidentification (ReID) and trajectory tracking (Du et al.\n2024). A classic example of a Multi-object tracker within\nthis paradigm is SORT (Bewley et al. 2016), which is the\nalgorithm we use in this paper. SORT uses the Kalman filter\nto perform estimations of previously detected objects. Sub-\nsequently, it associates the identified objects via the Intersec-\ntion over Union (IoU) of the bounding box of the detected\nobject and the estimated position and scale of the bounding\nbox. It uses the Hungarian algorithm to optimally solve this\nassignment. There is a number of algorithms implementing\nimprovements to SORT, such as DeepSORT (Wojke, Bew-\nley, and Paulus 2017), StrongSORT (Du et al. 2023), and\nByteTrack (Zhang et al. 2022), but these rely on an addi-\ntional deep learning model, detracting from transparency."}, {"title": "Interpretable decision making", "content": "Interpretable decision-\nmaking in autonomous driving has a similar motivation to\nour work - real-time explanations of AI model's decisions\nbut focuses on providing textual explanations to the model's\nactions, rather than visual explanations to the images de-\ntected by the vehicle's sensors (Xu et al. 2020; Jing et al.\n2022)."}, {"title": "3 Incremental Explanations", "content": "In this section we present the theoretical foundations for\nour saliency map and explanation transformation and the al-\ngorithm Incx, which employs this transformation to avoid\ncalls to the object detector in subsequent frames. An initial\nsaliency map is assumed to be computed by an external XA\u0399\ntool. IncX is agnostic to the XAI tool as well as to the model,\nas it does not assume access to the model internals or a spe-\ncific XAI algorithm. We require only that the XAI output is\na saliency map."}, {"title": "3.1 Theoretical Foundations", "content": "The intuition behind our algorithm is based on the follow-\ning observation. As long as the detected object moves in 3D\nwithout turning or deforming, its projection on 2D can only\nundergo a combination of scaling and linear translation (for\ninstance, if a car gets closer, its 2D projection scales up).\nWe prove that given such a transformation of the 2D pro-\njection of the detected object and the initial saliency map,\nthe saliency map of the transformed object can be com-\nputed from the original saliency map using a similar trans-\nformation, and the initial explanation, mapped into the trans-\nformed saliency map should remain valid. The formal proof\nis based on known results from probability theory and is\nquite involved (see the supplementary material).\nThe core of our approach is treating the saliency map as a\nprobability mass function (pmf) and then applying the well-\nunderstood mathematics of pmf transformation to estimate\na pmf that represents the new, updated, saliency map. First,\nwe need to convert our initial saliency map into a pmf. This\nis easily done by normalising the values of the saliency map\nso that they sum to 1.\nNow we use Theorem 1 (Hogg, McKean, and Craig 2019)\nto help us define the transformations of the pmf. This allows\nus to calculate the new saliency pmf after undergoing lin-\near transformation (see the supplementary material for de-\ntails). Technically, our pmf is a joint probability mass func-\ntion P(X, Y). We restate the core transformation result over\na single variable from Hogg, McKean, and Craig (2019) here\nfor completeness.\nTheorem 1. (Hogg, McKean, and Craig 2019) Assume X\nto be a discrete random variable with space D_x and g a\none-to-one transformation function. Then the space of Y is\nD_y = {g(x) : x \u2208 D_x}, and the pmf of Y is\n\n    p_Y(y) = P[g(X) = y] = P[X = g^{-1}(y)] = p_X(g^{-1}(y)).\n\nTheorem 1 can be extended to joint pmfs over multiple\nvariables (Hogg, McKean, and Craig 2019, p.100) (see sup-\nplementary material).\nHaving established the intuition behind the transforma-\ntion of the saliency pmf, we address the considerations of\nour approach concerning transformations involving three-\ndimensional objects. Specifically, we first describe how such\nobjects get projected onto the image plane. By Forsyth and\nPonce (2002), the mapping of points from three-dimensional\nworld coordinates to two-dimensional screen coordinates\ncan be achieved while using a common frame of reference\nfor the world and the camera. Lemma 1 formalizes this pro-\njection process.\nLemma 1 (3D world coordinates to 2D screen (camera im-\nage) coordinates). Let \u03b1, \u03b2\u2208 \\R denote the intrinsic magni-\nfication factors of the camera. The transformation from 3D\nworld coordinates (x, y, z) to 2D screen coordinates (u, v)\nis given by:\n\n\\[\n\\begin{bmatrix}\nu \\\\ \nu \\\\ 1\\end{bmatrix} = K \\begin{bmatrix}\\alpha & 0 & O_x \\\\ 0 & \\beta & O_y \\\\ 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix} x\\\\ y\\\\ z\\end{bmatrix}\n\\]\n\nwhere K represents the intrinsic parameters of the cam-\nera, and (O_x, O_y) represents the coordinates at the upper-\nleft corner of the image.\nFurthermore, we formalize the center of the bounding box\nfor the projected object in Definition 1. This concept is lever-\naged in the scaling and translation operations described in\nDefinition 2, which are fundamental for IncX.\nDefinition 1 (Center Function). The center function C:\n2^R\u201d \u2192 R\u201d maps a set S C R^n of points to the center point\nof the bounding box that encloses these points. For a set S\nconsisting of points p = (p_1, p_2,..., p_n) \u2208 R^n the function\nis defined as follows:\n\n    C(S) =  \\begin{bmatrix}\\frac{max(p_1)+min(p_1)}{2} \\\\ \\frac{max(p_2)+min(p_2)}{2} \\\\ : \\\\ \\frac{max(p_n)+min(p_n)}{2}\\end{bmatrix}"}, {"title": null, "content": "where max(p_j) and min(p_j) denote the maximum and\nminimum values, respectively, of the j-th coordinate among\nall points in S.\nDefinition 2 (Scaling and Translation). Let A = {(u,v) \u2208\n\\R^2} represent a set of points defining any area in the image\nspace.\nScaling is a linear transformation S : A \u2192 R^2, defined as\nS(p) = \u03b3(p \u2013 C(A)), where p \u2208 A and \u03b3 \u2208 [R^2\u00d72 is a\ndiagonal matrix.\nTranslation is a linear transformation T : A \u2192 R^2, defined\nas T(p) = p + \u00b5, where p \u2208 A.\nThe scaling transformation S scales p by a factor \u03b3 after\ntranslating the center C(A) to the origin (0,0). The trans-\nlation function T shifts the two-dimensional input p by a\nvector \u03bc\u2208 R^2.\nWe first provide the intuition behind our theorem and then\nits formal statement in Theorem 2. Our approach connects\nthe pmf to the 2D projection of a three-dimensional objectdt\n(Assumption 1). Furthermore, as long as the object being\ntracked does not leave the frame, turn on its axis, or cease\nto be detected, the future positions of the object are deter-\nmined solely by translation and scaling transformations in\nthe three-dimensional space (Assumption 2). Together, these\nassumptions lead to the statement that future saliency maps\ncan be computed by applying a scaling and translation linear\ntransformation to the current saliency map.\nNotation: Let Qt = (Q_{1,t}, Q_{2,t}) be a vector in the im-\nage space at time t, where Q_{1,t} and Q_{2,t} represent random\nvariables along the u-axis and the v-axis, respectively. The\npmf of Qt is p_{Q_1} (qt), which is derived from the normalized\nsaliency map.\nWe assume that the expected value of the saliency pmf, if\nit exists, is equal to the center of the bounding box C(KV)\nof the object V_e as projected into the image space using the\nmatrix K. dt represents the distance between this projected\ncenter and the expected value of Qt. The main reason behind\nthis assumption is the homogeneity of our pmf, implying\nthat any scaling or translation of the expected value results in\na corresponding scale or shift of the entire pmf, preserving\nthe distribution's proportions. A direct implication of this\nassumption is that if we move and scale the center of the\nbounding box, then the saliency map would have a similar\nshift and scaling.\nAssumption 1 (Expected Value and Bounding Box Center\nRelation).\n\n    E[Q_t] = C(KV_e) + \\delta_t\n\nAssumption 2 (Constrained Affine Transformation). Any\npoint of an object at time t + 1 is related to the same ob-\nject at time t by an affine transformation given by:\n\n\\[\n\\begin{bmatrix}x_{t+1} \\\\ y_{t+1} \\\\ z_{t+1} \\\\ 1\\end{bmatrix} = \\begin{bmatrix}m_x & 0 & 0 & v_x \\\\ 0 & m_y & 0 & v_y \\\\ 0 & 0 & m_z & v_z \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\begin{bmatrix}x_t \\\\ y_t \\\\ z_t \\\\ 1\\end{bmatrix}\n\\]\n\nwhere t \u2208 R^3 is an arbitrary translation vector, I \u2208 R^{3\u00d73}\nis the identity matrix, and mx, my, and mz \u2208 R are scaling\nfactors.\nFinally, let S be the scaling function, T the translation\nfunction, q_{t+1} a coordinate vector in the image space at time\nt + 1, and q_t a coordinate vector at time t. The following\ntheorem describes the relation between the pmf at time t and\nat time t + 1 and is instrumental to the Incx algorithm.\nTheorem 2. If Assumptions 1 and 2 hold, then the pmf at\ntime t + 1, p_{Q_{t+1}}(q_{t+1}), can be computed from the pmf at\ntime t using the following equation:\n\n    p_{Q_{t+1}} (q_{t+1}) = p_{Q_t}(S^{-1}(T^{-1}(q_{t+1})))"}, {"title": "3.2 Algorithm Structure", "content": "The Incx algorithm is schematically presented in Figure 2.\nWe list three object detectors, but, as we mentioned earlier,\nIncx is agnostic to the detector, as it treats it as a black box.\nAs previously stated, the goal of our algorithm is to approx-\nimate saliency maps and explanations in real-time for object\ndetection tasks. As a result, we need a starting point from\nwhere we can move forward. Hence, the saliency map in the\nfirst frame is computed using an XAI tool (Figure 2 lists D-\nRISE, but it can be any saliency map-producing tool). After\nthis bootstrapping, the goal is to identify a way to extend the\nsaliency map obtained in the first frame and extrapolate from\nit to create subsequent ones. To achieve this, we perform the\ntransformation defined in Theorem 2.\nThe first involves bootstrapping our method by comput-\ning the initial saliency map. This process is quite straight-\nforward and involves calling an object detector N and ex-\nplainer h. Only the saliency maps found in the first frame\nwill be used as a basis for extrapolation in later frames. For\nthe subsequent frames, we need to implement a different ap-\nproach to be able to obtain approximate saliency maps in\nreal-time. First we incorporate a new block (see Figure 2),\nwhich is the object tracker: SORT (Bewley et al. 2016). This\nobject tracker takes the bounding box of the identified ob-\njects and checks if the id corresponds to any of the objects\nidentified in the previous frame (Line 11). Once an object\nhas been identified, the linear transformation is performed\n(Lines 12-16). This process takes the previous bounding\nbox and the current bounding box and computes \u0394x, \u0394y, \u03b3.\nThese computed transformations are applied to the saliency\nmap, utilizing Theorem 2. We note that when the saliency\nmap is scaled up, gaps may appear in the newly computed\npmf, requiring interpolation to fill them.\nIt is important to note that we can track explanations only\nas long as the object tracker finds the objects identified in the\nfirst frame. An object may vanish from the frame or remain\nundetected for a long time. For reasons of efficiency, we de-\nfine a timeout that allows us to stop attempting to explain an\nobject which is no longer detectable (Line 20)."}, {"title": "3.3 Sufficient explanations", "content": "Thus far, our focus has been primarily on saliency maps.\nHowever, in addition to this, it is useful to provide suffi-\ncient explanations. A sufficient explanation, informally, is a\nsubset of pixels capable of producing the desired classifica-\ntion. This is a useful sanity check on the saliency maps: the\nAlgorithm 1 Incremental Explanation\nInput: A video v, an object detector N, an explainer h\nOutput: A sequence of saliency maps s and explanations\nxp"}, {"title": null, "content": "1: x_{t-1}, y_{t-1}, width_{t-1}, height_{t-1} \u2190 0,0,0,0\n2: id_0, count \u2190 0,0\n3: initialize empty saliency map s_{t-1}\n4: initialize empty arrays s, xp\n5: for each frame f_t at time t in video v do\n6:  (x_t, y_t, width_t, height_t) \u2190 N(f_t)\n7:  if t == 0 then\n8:   s_t\u2190h(f_t)\n9:  else\n10:   idt \u2190 SORT(f_t)\n11:   if id_0 == id then\n12:    \u0394u \u2190 u_t \u2013 u_{t-1}\n13:    \u0394v \u2190 v_t \u2013 v_{t-1}\n14:    \u03b3 \u2190 \\frac{width_{t}, height_{t}}{(width_{t-1}, height_{t-1})}\n15:    s_t \u2190 Scale(s_t, \u03b3)\n16:    s_t \u2190 Translate(s_{t-1}, \u0394u, \u0394v)\n17:    count \u2190 0\n18:   else\n19:    count \u2190 count + 1\n20:    if count >= timeout then\n21:     Restart\n22:    end if\n23:   end if\n24:  end if\n25: xpt \u2190 Explanation(s_t, N, f_t, t)\n26: xp \u2190 xp + [xpt]\n27: s\u2190 s + [st]\n28: width_{t-1}, height_{t-1} \u2190 width_t, height_t\n29: x_{t-1}, y_{t-1} \u2190 x_t, y_t\n30: st-1 \u2190 st\n31: end for\n32: return s, xp\n\n\"hotspots\" on the saliency map should be enough to trigger\nthe appropriate classification. This necessitates the introduc-\ntion of an \"Explanation\u201d block for both the initial frame and\nsubsequent frames, as illustrated in Figure 2. This imple-\nmentation ensures that the explanations provided for sub-\nsequent frames are correct, meaning they effectively output\nthe intended classification with a bounding box containing\nan area with the correct classification.\nExtraction of sufficient explanations from saliency maps\ncan be slow. Therefore we adopt the approach used in Al-\ngorithm 2. For the initial frame 0, the landscape is divided\ninto l equal sections, demarcated by grey dotted lines in Fig-\nure 3a. To efficiently locate an explanation, we employ bi-\nnary search within these predefined divisions. In Figure 3a,\nthe division containing the explanation threshold is high-\nlighted in red. We assume monotonicity in the model output\nas pixels are added: once a sufficient set of pixels has been\nlocated, the classification no longer changes.\nExplanation discovery for subsequent frames is depicted"}, {"title": null, "content": "Algorithm 2 Explanation Algorithm\nInput: Saliency map s, object detector g, image I, time step\nt, margin \u03b4, number of divisions l (for t == 0) and ln (for\nt > 0)\nOutput: Explanation xp\n1: if t == 0 then\n2:  min \u2190 min(s)\n3:  max \u2190 max(s)\n4:  ss \u2190 Linspace(min, max, l)\n5: else\n6:  max \u2190 thresholdt\u22121 \u00b7 (1 + \u03b4)\n7:  min \u2190 thresholdt-1 \u00b7 (1 \u2013 \u03b4)\n8:  ss \u2190 Linspace(min, max, ln)\n9: end if\n10: xp, thresholdt\u22121 \u2190 BinarySearch(s, g, I, ss)\n11: return xp\nin Figure 3b. Assuming consistency in the saliency map, it is\nreasonable to expect that the explanation threshold in subse-\nquent frames would be close to that of the previous frames.\nTherefore we search at around the previous threshold, with\nthe number of divisions adjusted to ln. Smaller values of\nln improve performance but may result in explanations that\ncontain more redundancy.\nAlgorithm 2 provides a pseudocode of this algorithm. For\nthe first frame, the search space ss spans from the min-\nimum to maximum values of the saliency map s, divided\ninto l equally spaced levels. For subsequent frames, an ad-\nditional parameter \u03b4 defines the margin around the previous\nthresholdt-1, adjusting the search space boundaries accord-\ningly and setting the number of divisions to ln. A binary\nsearch is performed within the defined search space for each\ncase to determine the explanation."}, {"title": "4 Experimental Results", "content": "The COCO (Lin et al. 2014) and VOC (Everingham et al.\n2010) datasets are typically used for evaluation of explain-\nability in object detection (Kuroki and Yamasaki 2024).\nHowever, these datasets consist of independent images that\nlack temporal continuity, making them unsuitable for our fo-\ncus on real-time video explanation. LaSOT (Fan et al. 2019),\nhowever, includes 85 object classes and over 3.87 million\nframes in total, which is more appropriate for our experi-\nments. Therefore, we curated a subset comprising two object\nclasses: car and cat. From these classes, we selected the first\n300 frames (equivalent to 10 seconds at 30 FPS) from a total\nof 10 videos, resulting in 3000 frames for our experiments.\nWe executed IncX, using D-RISE to compute the saliency\nmaps, to be able to perform a comparison between Incx and\nD-RISE. IncX makes no assumptions about how the initial\nsaliency map is produced: we use D-RISE as it works natu-\nrally with the different object detectors that we examine. We\napplied Incx to all the videos, tracking the object with the\nhighest confidence from the first frame. We then computed\nthe saliency maps using D-RISE for all frames where the ob-\nject was successfully tracked by Incx. All experiments were\nconducted using the NVIDIA A100 Tensor Core GPU. We\nexecuted D-RISE with 1000 mutants and a mask resolution\nof (4,4). The following versions of the models were used:\nYOLOv10-n, RT-DETR-1, and the PyTorch implementation of\nFaster R-CNN.\nFor a one-to-one comparison between each saliency map\ngenerated by D-RISE and IncX, we computed a set of metrics\ncommonly used in the field of Explainable AI. Below, we\nintroduce these metrics.\nNormalized Insertion/Deletion curves The insertion\nprocess starts with a blank image, adding pixels based on\nthe saliency map, and computes the area under the curve\n(AUC) after measuring detector confidence. Conversely, the\ndeletion process starts with the full image and gradually re-\nmoves pixels according to the saliency map. As we are deal-\ning with object detectors, we adjust this metric by multiply-\ning the score by the Intersection over Union (IoU) of the\noriginal bounding box. To account for low confidence pro-\nducing low insertion and deletion values, we normalize the\ncurve by dividing all values by the original image's score."}, {"title": null, "content": "Energy-based Pointing Game (EPG) The Energy-based\nPointing Game (EPG) metric quantifies the proportion of\nthe saliency map's total energy that is contained within the\nbounding box (Wang et al. 2019). This metric is essential be-\ncause an accurate saliency map should predominantly cover\nthe bounding box to correctly represent the object of inter-\nest. If the saliency map is focused outside of the bounding\nbox, it may indicate the detection of an object that does not\ncorrespond to the one within the current bounding box.\nExplanation Proportion (EP) The Explanation Propor-\ntion metric measures explanation quality by computing the\nratio of the explanation to the whole image, hence favoring\ncompact explanations that minimize unnecessary informa-\ntion. We define EP as follows: for a given boolean-valued\nmask m of an image, where T denotes the pixels included\nin the explanation, the Explanation Proportion is computed\nas:\n\n\\[\nEP = \\frac{\\sum_{(i,j) \\in T} M_{(i,j)}}{\\sum_{(i,j) \\in T} M_{(i,j)} + \\sum_{(i,j) \\notin T} m_{(i,j)}}\n\\]\n\nAs summarized in Table 1, the insertion values for IncX\nare consistently within 8% of those observed for D-RISE,\nwith all other metrics displaying similar trends. Moreover,\nthe computation time for Incx is two orders of magnitude\nlower than that of D-RISE (111 times faster for Faster R-\nCNN), highlighting the suitability of Incx for real-time sce-\nnarios.\nGiven that Incx generates an approximation of the true\nsaliency map, it is crucial to evaluate the similarity between\nthe baseline saliency map and the approximation provided\nby Incx. To achieve this, we need to employ metrics that ef-\nfectively measure the similarity between these two saliency\nmaps. Mean Squared Error (MSE) is not ideal for this pur-\npose, as it may not provide a clear indication of percep-\ntual similarity between the maps. Instead, we utilize alterna-\ntive metrics designed to assess the correspondence between\nthe baseline and Incx saliency maps. The selected metrics\nfor this comparison are the Pearson Correlation Coefficient\n(CC), Structural Similarity Index (SSIM) (Wang et al. 2004),\nDice Coefficient (DC) (Dice 1945), and Jaccard Index (IoU).\nThe results presented in Table 2 show a strong similarity\nbetween the approximations implemented by Incx and the\noriginal saliency maps created by D-RISE.\nLimitations Incx is based on the assumption that the ob-\nject it is applied to is rigid. Moreover, the algorithm may\nnot handle rotations (pitch, roll, and yaw) effectively, es-\npecially for non-symmetrical objects, given that it primar-\nily addresses scaling and translation. Therefore, the method\nmay not be theoretically advantageous for objects undergo-\ning other types of transformations. Nonetheless, as shown in\nSection 4, these theoretical limitations have a limited impact\nin practice. Furthermore, to ensure the accurate computation\nof explanations in real-time, we assume that explanations for\nconsecutive frames are relatively close on the saliency map,\nas shown in Figure 3. However, this approach may result in\nnon-minimal explanations."}, {"title": "5 Conclusions", "content": "Motivated by the need for real-time explanations for object\ndetectors, we have introduced IncX, an algorithm and tool\nfor approximating saliency maps via linear transformations.\nOur results demonstrate that Incx operates in real time, with\nnegligible overhead over the object detector and is two or-\nders of magnitude faster than generating a new saliency map.\nMoreover, Incx produces saliency maps closely aligned with\nfreshly computed maps, achieving an average correlation co-\nefficient of approximately 0.8 across all models used, with\nminimal computational overhead. The quality metrics for as-\nsessing explainable AI algorithms show that Incx is compa-\nrable to D-RISE, with the insertion score within 8% of D-\nRISE'S ones.\nIncx handles linear transformations of objects; future\nwork will address the current limitations related to rotation\nand object deformation. We anticipate future applications\nof this algorithm in various industries, particularly in self-\ndriving cars and video advertisement placement."}]}