{"title": "PointNCBW: Towards Dataset Ownership Verification for Point Clouds via Negative Clean-label Backdoor Watermark", "authors": ["Cheng Wei", "Yang Wang", "Kuofeng Gao", "Shuo Shao", "Yiming Li", "Zhibo Wang", "Zhan Qin"], "abstract": "Recently, point clouds have been widely used in computer vision, whereas their collection is time-consuming and expensive. As such, point cloud datasets are the valuable intellectual property of their owners and deserve protection. To detect and prevent unauthorized use of these datasets, especially for commercial or open-sourced ones that cannot be sold again or used commercially without permission, we intend to identify whether a suspicious third-party model is trained on our protected dataset under the black-box setting. We achieve this goal by designing a scalable clean-label backdoor-based dataset watermark for point clouds that ensures both effectiveness and stealthiness. Unlike existing clean-label watermark schemes, which are susceptible to the number of categories, our method could watermark samples from all classes instead of only from the target one. Accordingly, it can still preserve high effectiveness even on large-scale datasets with many classes. Specifically, we perturb selected point clouds with non-target categories in both shape-wise and point-wise manners before inserting trigger patterns without changing their labels. The features of perturbed samples are similar to those of benign samples from the target class. As such, models trained on the watermarked dataset will have a distinctive yet stealthy backdoor behavior, i.e., misclassifying samples from the target class whenever triggers appear, since the trained DNNs will treat the inserted trigger pattern as a signal to deny predicting the target label. We also design a hypothesis-test-guided dataset ownership verification based on the proposed watermark. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our method and its resistance to potential removal methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Point clouds have been widely and successfully adopted in many vital applications (e.g., autonomous driving [7] and augmented reality [32]) since they can provide rich geometric, shape, and scale information [11]. In particular, collecting point clouds is even more time-consuming and expensive compared to classical data types (e.g., image or video). It necessitates using costly 3D sensors and intricate data processing procedures (i.e., registration [33] and filtering [49]). As such, point cloud datasets are valuable intellectual property.\nDue to the widespread applications, point cloud datasets are likely to be publicly released as open-sourced or commercial datasets. However, to the best of our knowledge, almost all existing methods (e.g., encryption) can not be directly exploited to protect their copyright when they are publicly released. In these scenarios, the adversaries may train their commercial models on open-sourced datasets that are restricted to academic or research purposes or even on commercial ones that have been illegally redistributed. Arguably, the protection difficulty stems mainly from the publicity of these datasets and the black-box nature of the suspicious models, since existing traditional data protection schemes either hinder the dataset accessibility (e.g., data encryption [2]), require manipulating model training (e.g., differential privacy [1]), or demand accessing training samples during the verification process (e.g., digital watermark [13]).\nCurrently, to the best of our knowledge, dataset ownership verification (DOV) [20], [27], [30], [39] is the only promising approach that can be generalized to protect the copyright of public point cloud datasets. This method was initially and primarily used to safeguard the copyright of image datasets. Specifically, dataset owners in existing DOV methods adopt and design backdoor attacks [28] to watermark the original dataset before releasing it. Given a suspicious third-party black-box model that can only be accessed via API, dataset owners can verify whether it is trained on the protected dataset by inspecting whether it has owner-specified backdoor behaviors. The backdoor-based DOV successfully safeguards publicly accessible datasets, particularly in the presence of black-box suspicious models. As such, the key to protecting the copyright of point cloud datasets via DOV lies in designing a suitable backdoor watermark for them.\nHowever, directly applying existing backdoor attacks against 3D point clouds as watermarks meets several challenges, as shown in Figure 1. Firstly, except for PointCBA [25], all existing point cloud backdoor methods (i.e., PCBA [45], PointPBA [25], IRBA [15]) are with poisoned labels, where the assigned labels of watermarked samples are different from their ground-truth ones. Accordingly, these watermarks lack stealthiness as they can be easily detected and eliminated by malicious dataset users who scrutinize the correlation between point clouds and their corresponding labels. Secondly, the performance of the only existing clean-label backdoor watermark, i.e., PointCBA, is not scalable. In other words, its watermark performance will significantly decrease when datasets contain many categories, preventing its application as a watermark for large-scale point cloud datasets.\nWe find that the non-scalability of existing clean-label backdoor watermarks for both images and point clouds [17], [25], [40] comes from their common poisoning paradigm, where defenders can only watermark samples from a specific category (i.e., target class). As such, the more categories of samples in the dataset, the smaller the maximum watermarking rate (i.e., maximal proportion of samples for watermark), resulting in a reduction in overall watermark effects. We argue that this defect is primarily due to the positive trigger effects of existing clean-label watermarks. Specifically, these methods have to add triggers to samples from the target class, aiming to build a positive connection between the trigger pattern and the target label (i.e., adding triggers to any benign sample increases the probability of being predicted as the target label).\nIn this paper, motivated by the aforementioned understandings, we propose to design a scalable clean-label backdoor watermark by introducing the negative trigger effects, where we intend to decrease the prediction confidence of water-marked models to samples from the target class when owner-specified trigger patterns arise. Specifically, before implanting trigger patterns, we first perturb selected point clouds from non-target categories so that their features lie close to those from the target class. After that, we implant the trigger patterns into them to generate the watermarked samples. The labels of these watermarked samples are different from the target label, but they lie close in feature space of samples from the target class. Accordingly, the trained DNNs will learn to treat the inserted trigger pattern as a signal to deny predicting the target label. This method is called negative clean-label backdoor watermark for point clouds (PointNCBW). Besides, we design a hypothesis-test-guided dataset ownership verification based on our PointNCBW by examining whether the suspicious model is less confident on point clouds containing owner-specified triggers from the target class. It alleviates the adverse effects of randomness introduced by sample selection.\nThe main contributions of this paper are four-fold:\nWe explore how to protect the copyright of point cloud datasets via dataset ownership verification (DOV).\nWe reveal the limitations of using existing backdoor attacks against point clouds for DOV and their reasons.\nWe propose the first scalable clean-label backdoor watermark for point cloud datasets (i.e., PointNCBW).\nWe conduct extensive experiments on benchmark datasets, which verify the effectiveness of our Point-NCBW and its resistance to potential adaptive attacks."}, {"title": "II. RELATED WORK", "content": "With the advent of deep learning, point-based models have gained significant popularity owing to their exceptional performance in various 3D computer vision tasks. Qi et al. [34] first proposed PointNet, which directly processed raw point cloud data without needing data transformation or voxelization. It adopted the symmetric function, max-pooling, to preserve the order-invariant property of point clouds. To learn the local features of point clouds, they further proposed a hierarchical network PointNet++ [35], which can capture geometric structures from the neighborhood of each point better. Inspired by them, subsequent research [26], [42], [46] in point cloud-based deep learning has emerged following similar principles. Since the above architectures have been widely adopted in 3D vision applications, we exploit them in this paper for our study.\nBackdoor attack is an emerging yet critical training-phase threat to DNNs [28]. In general, the adversaries intend to implant a latent connection between the adversary-specified trigger patterns and the malicious prediction behaviors (i.e., backdoor) during the training process. The attacked models behave normally on benign samples, whereas their prediction will be maliciously changed whenever the trigger pattern arises. Currently, most of the existing backdoor attacks were designed for image classification tasks [4], [16], [18], [21], [31], [36], [47], although there were also a few for others [5], [12], [14], [29], [44], [48]. These methods could be divided into two main categories: poison-label and clean-label attacks, depending on whether the labels of modified poisoned samples are consistent with their ground-truth ones. In particular, clean-label attacks are significantly more stealthy than poison-label ones since dataset users cannot identify them based on the image-label relationship even when they can catch some poisoned samples. However, as demonstrated in [17], clean-label attacks are significantly more challenging to succeed due to the antagonistic effects of 'robust features' related to the target class contained in poisoned samples. How to design an effective clean-label backdoor attack is still an important open challenge.\nCurrently, a few works [15], [25], [45] have explored backdoor attacks against 3D point clouds. Although these attacks also targeted the classification tasks, designing a backdoor attack for 3D point clouds is still challenging due to intrinsic differences in data structure and deep learning model structures [25]. As we will show in our experiments, the only existing clean-label attack (i.e., PointCBA [25]) is far less effective"}, {"title": "III. NEGATIVE CLEAN-LABEL BACKDOOR WATERMARK FOR POINT CLOUDS (POINTNCBW)", "content": "This paper focuses on backdoor watermarks for point cloud datasets in classification tasks. Specifically, the dataset owner can watermark some benign point clouds before releasing the victim dataset. Dataset users will exploit the released (watermarked) datasets to train and deploy their DNNs but will keep their training details private. Accordingly, dataset owners can only examine whether a suspicious model is trained on their watermarked point cloud dataset by its prediction behaviors under the black-box access setting.\nLet $\\mathcal{D} = \\{(x_i, Y_i)\\}_{i=1}^N$ denotes the benign dataset containing $N$ point clouds. Each point cloud $x$ contains $M_i$ points (i.e., $x_i \\in \\mathbb{R}^{3 \\times M_i}$) whose label $Y_i \\in \\{1,2,\\ldots, K\\}$. How to generate the watermarked dataset $\\mathcal{D}_w$ is the cornerstone of all backdoor watermarks. Currently, all backdoor watermarks for point clouds are targeted and with positive trigger effects. In other words, adding triggers increases the probability that watermarked DNNs predict samples to the target class $y^{(t)}$. Specifically, $\\mathcal{D}_w$ has two disjoint parts, including the modified version of a selected subset (i.e., $\\mathcal{D}_s$) of $\\mathcal{D}$ and remaining benign point clouds, i.e., $\\mathcal{D}_w = \\mathcal{D}_m \\cup \\mathcal{D}_b$, where $\\mathcal{D}_b = \\mathcal{D} \\setminus \\mathcal{D}_s$, $\\mathcal{D}_m = \\{(x', y^{(t)}) | x' = G(x), (x,y) \\in \\mathcal{D}_s\\}$, $G: \\mathbb{R}^{3 \\times M} \\rightarrow \\mathbb{R}^{3 \\times M}$ is the owner-specified generator of watermarked samples. $\\lambda \\in [0,1]$ is the watermarking rate. In general, backdoor watermarks are mainly characterized by their watermark generator $G$. For example, $G(x) = (I-\\text{diag}\\,\\delta) \\cdot x + \\text{diag}\\,\\delta \\cdot \\Gamma$, where $\\delta$ is a 0-1 vector, $I$ is the identity matrix, and $\\Gamma$ is pre-defined trigger pattern in PointPBA-Ball [25]. In particular, in existing clean-label backdoor watermarks (e.g., PointCBA [25]), dataset owners can only watermark samples from the target class, i.e., $\\mathcal{D}_s \\subset \\mathcal{D}^{(t)} = \\{(x,y) | (x,y) \\in \\mathcal{D}, y = y^{(t)}\\}$. As such, their watermarking rate is at most $\\lambda$ for class-balanced datasets. This limits their performance when the number of categories in the victim dataset is relatively large.\nIn this paper, we design a clean-label backdoor watermark for point clouds with negative trigger effects to overcome the limitations of existing backdoor watermarks (as demonstrated in our introduction). We denote our watermarking method as negative clean-label backdoor watermark for point clouds (PointNCBW). In general, our PointNCBW consists of two main stages: transferable feature perturbation (TFP) and trigger implanting. Specifically, TFP perturbs selected point clouds with non-target categories so that they lie close to those from the target class in the hidden space defined by a pre-trained model. After that, we insert trigger pattern $\\Gamma$ to obtain modified point clouds $\\mathcal{D}_m$ via\n$\\mathcal{D}_m = \\{(x',y) | x' = U(\\rho(x), \\Gamma), (x, y) \\in \\mathcal{D}_s\\}$, (1)\nwhere $U(\\rho(x), \\Gamma)$ is our watermark generator, $\\rho$ represents our TFP, and $U$ is our trigger implanting function implemented with random replacing function.\nSince we don't change the label of these watermark samples, the watermarked DNNs will interpret inserted triggers as signals to deny predicting the target label. The main pipeline of our method is shown in Figure 2. We also provide a detailed explanation of the underlying reasons behind the effectiveness of PointNCBW through experimental analysis in Section V-H.\nAfter selecting the target category $y^{(t)}$ and the source sample group $\\mathcal{D}_s$, our objective is to perturb each sample in $\\mathcal{D}_s$ to bring them closer to category $y^{(t)}$ in feature space. Specifically, we randomly choose some samples from category $y^{(t)}$ denoted as $\\mathcal{D}_t$, and utilize the features of $\\mathcal{D}_t$ as an alternative to the features of category $y^{(t)}$. Let $x$ represents one sample in $\\mathcal{D}_s$, our general objective of perturbation is formulated by\n$\\min_\\rho \\frac{1}{|\\mathcal{D}_t|} \\sum_{x_t \\in \\mathcal{D}_t} \\mathcal{E}(g_f(\\rho(x)), g_f(x_t)),$ (2)\nwhere $\\mathcal{E}$ is a Eluer distance in feature space $\\mathbb{R}^d$ and $g_f$ is the feature extracting function of point cloud. In practice, we implement the $g_f$ with the second-to-last layer output of our surrogate model $g$ for approximation.\nHowever, since we employ a surrogate model for feature extraction, it is crucial to ensure that our feature perturbation remains effective under different networks beyond the surrogate one. This raises the concern of transferability, which refers to the ability of our watermark to work effectively across different model structures. To enhance the transferability, we optimize general objective function in Eq. (2) through transferable feature perturbation (TFP). The transferability is also empirically verified in Section V-E. Specifically, our TFP involves two sequential steps, including shape-wise and point-wise perturbations, as follows.\nRotation is a common transformation of 3D objects. It has been empirically proven to be an effective and inconspicuous method to conduct point cloud perturbation with transferability in previous works, since DNNs for point cloud are sensitive to geometric transformations [10], [50]. As such, we exploit it to design our shape-wise perturbation with transformation matrix $S$ defined as follows:\n$S(\\theta) = R(\\psi) \\cdot R(\\phi) \\cdot R(\\gamma),$ (3)\nwhere $R(\\psi), R(\\phi)$, and $R(\\gamma)$ are rotation matrix with Eluer angles $\\psi, \\phi, \\gamma$. Finally, we have the objective function of shape-wise perturbation, as follows:\n$\\mathcal{L}_s(\\theta) = \\frac{1}{|\\mathcal{D}_t|} \\sum_{x_t \\in \\mathcal{D}_t} \\mathcal{E}(g_f(x_s \\cdot S(\\theta)), g_f(x_t)).$ (4)\nSpecifically, we employ the gradient descent to minimize the loss function defined in Eq. (4). Besides, to alleviate the impact of local minima, we employ a strategy of random point sampling and choose the optimal starting point for the optimization process. We summarize the shape-wise feature perturbation in Algorithm 1.\nAfter the shape-wise perturbation where we can obtain the optimal solution $\\theta$ for Eq. (4), we jitter the point cloud sample on the point-wise level. We denote the offset of coordinates as $\\Delta x$, then the objective function of optimization of point-wise perturbation is\n$\\mathcal{L}_p(\\Delta x) = \\frac{1}{|\\mathcal{D}_t|} \\sum_{x_t \\in \\mathcal{D}_t} \\mathcal{E}(g_f(x_s \\cdot S(\\Theta) + \\Delta x), g_f(x_t)) + \\eta \\cdot ||\\Delta x||_2,$ (5)\nwhere we incorporate a regularization term $||\\Delta x||_2$ into the loss function to limit the magnitude of perturbation in $l_2$ norm. To minimize the loss $\\mathcal{L}_p$ and mitigate the problem of perturbed samples 'overfitting' to the surrogate model, we incorporate momentum [9] into the iterative optimization process to stabilize updating directions and escape from 'overfitting' local minima. Specifically, we compute the momentum of gradients, and perturb the coordinates of points in the opposite direction of the momentum. The optimization process of point-wise feature perturbation is summarized in Algorithm 2."}, {"title": "IV. OWNERSHIP VERIFICATION VIA POINTNCBW", "content": "Given a suspicious black-box model $f(\\cdot)$, we can verify whether it was trained on our protected point cloud dataset based on its predictions on samples from the target class and their watermarked version. To alleviate the side effects of randomness, we design a hypothesis-test-guided method, inspired by existing backdoor-based dataset ownership verification for images [19], [27], [38], as follows.\nLet $f(x)$ is the posterior probability of $x$ predicted by the suspicious model. Let variable $X$ denotes the benign sample from the target class $y^{(t)}$ and variable $X'$ is its verified version (i.e. $X' = U(X,\\Gamma)$). Let variable $P_b = f(X)_{y^{(t)}}$ and $P_v = f(X')_{y^{(t)}}$ denote the predicted probability of $X$ and $X'$ on $y^{(t)}$. Given null hypothesis $H_0: P_b = P_v + \\tau$ ($H_1: P_b > P_v + \\tau$), where hyper-parameter $\\tau \\in (0,1)$, we claim that the suspicious model is trained on the watermarked dataset (with $\\tau$-certainty) if and only if $H_0$ is rejected.\nIn practice, we randomly sample $m$ different benign samples to conduct the pairwise t-test [24] and calculate its p-value. Additionally, we also calculate the confidence score $\\Delta P = P_b - P_v$ to denote the verification confidence. The larger the $\\Delta P$, the more confident the verification.\nIn particular, we can also prove that our dataset ownership verification can succeed if its watermark success rate is sufficiently large (which could be significantly lower than 100%) when sufficient verification samples exist, as shown in Theorem 1. Its proof is in our appendix.\nSuppose $f$ generates the posterior probability by a suspicious model. Let variable $X$ denotes the benign sample from the target class $y^{(t)}$ and variable $X'$ is its verified version. Let $P_b = f(X)_{y^{(t)}}$ and $P_v = f(X')_{y^{(t)}}$ denote the predicted probability of $X$ and $X'$ on $y^{(t)}$. Assume that $P_b > \\zeta$, we claim that dataset owners can reject the null hypothesis $H_0$ at the significance level $\\alpha$, if the watermark success rate $W$ of $f$ (with $m$ verification samples) satisfies that\n$\\sqrt{m-1} \\cdot (W + \\zeta - \\tau - 1) - t_\\alpha \\cdot \\sqrt{W - W^2} > 0,$ (6)\nwhere $t_\\alpha$ is $\\alpha$-quantile of t-distribution with $(m-1)$ degrees of freedom."}, {"title": "V. EXPERIMENTS", "content": "We conduct experiments on two datasets, including ModelNet40 [43] and ShapeNetPart [6]. Following [34], we uniformly sample 1,024 points from the original CAD models as the point clouds and normalize them into [0, 1]3.\nWe adopt PointNet [34] as the default surrogate model for PointNCBW. We also take other common models (e.g., PointNet++ [35], DGCNN [42], and PointASNL with PNL [46]) into consideration. All models are trained with default settings suggested by their original papers.\nTo ensure the stealthiness of our watermark, we adopt a trigger with a deliberately small size. Specifically, we use a fixed sphere as our shape of trigger $\\Gamma$. We set its center as (0.3,0.3,0.3) and its radius as 0.025. Our trigger consists of 50 points randomly sampled from this sphere, which takes proportion about 5% of one watermarked sample (1,024 points in total). The examples of point cloud samples involved in different watermarks are shown in Figure 3.\nIn shape-wise TFP, we set the number of starting points $n = 30$, the number of iterations $T = 30$. We use Adam optimizer [22] to update angles $\\theta$ with initial learning rate $lr = 0.025$ and $lr$ is divided by 10 every 10 steps. In point-wise TFP, we set regularization factor $\\eta = 50$ in our objective function as default, and during the process of optimization, we set number of iterations $T = 20$, step size $\\beta = 0.001$, and decay factor $\\mu = 1$.\nAccuracy (ACC) is used to evaluate the performance of models on benign samples. Watermark success rate (WSR) measures the effectiveness of dataset watermarks. Specifically, WSR is the percentage of verification samples that are misclassified. We use $\\Delta P \\in [-1,1]$ and p-value $\\in [0, 1]$ for ownership verification as introduced in Section IV."}, {"title": "D. Ablation Study", "content": "We hereby discuss the effects of key hyper-parameters and modules of our PointNCBW. Unless otherwise specified, we exploit ModelNet40 as an example for our discussion.\nWe conduct an experiment to analyze the relationship between WSR and varying watermarking rates $\\lambda$. As depicted in Figure 4a, our WSR can reach above 80% when watermarking rate is about 0.01. Besides, a higher watermarking rate can bring a better WSR.\nAs shown in Figure 4b, using more verification samples can significantly enhance the performance of dataset ownership verification, i.e., the p-value for verification becomes smaller as the size of the verification set gets larger. The result is also consistent to our Theorem 1.\nTo evaluate the effects of regularization hyper-parameter $\\eta$ in our pointwise perturbation objective function, we use Chamfer Distance [3] to measure the the magnitude of perturbation related to varying $\\eta$. As shown in Figure 5, a larger $\\eta$ leads to more imperceptible point-wise perturbations.\nTo measure the effects of verification certainty $\\tau$ in PointNCBW-based dataset ownership verification, we choose different values of the $\\tau$ for ownership verification after the same watermarking process. As shown in Table IV, the p-value increases with the increase of verification certainty $\\tau$ in all scenarios. In particular, when $\\tau$ is smaller than 0.05, our proposed PointNCBW may misjudge the cases of Independent-T or Independent-M. In addition, the larger the $\\tau$, the more unlikely the misjudgments happen and the more likely that the dataset stealing is ignored. As such, people should assign $\\tau$ based on their specific requirements.\nWe conduct experiments on the ModelNet40 dataset to discuss the effects of trigger patterns in our PointNCBW. Specifically, we hereby discuss four main trigger settings, including (a) trigger pattern with different shapes, (b) trigger pattern with different sizes, (c) trigger pattern on different positions, and (d) trigger pattern with different number of points. In the first scenario, we sample 50 points from a cube centered at (0.3, 0.3, 0.3) with a side length of 0.05. In the second scenario, we sample 50 points from a sphere also centered at (0.3, 0.3, 0.3), but with a radius of 0.05. In the third scenario, we move the same default trigger to (0.3,0.3,0.3). In the last scenario, we only sample 20 points from the same sphere center at (0.3, 0.3, 0.3) with a radius of 0.025. The example of watermarked point clouds is shown in Figure 6. As shown in Table V, by comparing the results of setting (a) & (c), we know that both the shape and position of the trigger pattern used for watermarking have mild effects on the final performance. Besides, the results of setting (b) suggest that a larger trigger size leads to better watermarking and verification performance, although it may decrease the watermark's stealthiness. In addition, the results of setting (d) demonstrate the watermark performance may slightly decrease if the trigger contains fewer points. Nevertheless, our method obtains promising verification results under all settings.\nOur TFP contains shape-wise and point-wise perturbations. To verify its effectiveness, we watermark the ModelNet40 dataset following the process of our PointNCBW under four scenarios, including (1) no TFP before inserting trigger, (2) TFP with solely shape-wise part (TFP-S), (3) TFP with solely point-wise part (TFP-P), and (4) the vanilla TFP proposed in this paper. After the processes of watermarking, we train different networks on the watermarked ModelNet40 to measure the performance of ownership verification. As shown in Table VI, both shape-wise and point-wise perturbations are critical for the watermark and the verification performance of our PointNCBW."}, {"title": "E. The Model Transferability of PointNCBW", "content": "Recall that our PointNCBW requires a surrogate model $g$ to generate feature perturbations, as illustrated in Eq. (2). In our main experiments, we test our method under the same model structure used for generating the PointNCBW-watermarked dataset. However, the suspicious model may have a different structure compared to the one used for dataset generation in practice, since the dataset owner has no information about the model used by dataset users. In this section, we verify that our method has model transferability and, therefore, can be used to protect dataset copyright.\nWe exploit PointNet [34], PointNet++ [35], DGCNN [42], and PointASNL [46] on the ModelNet40 dataset for discussion. Specifically, we first use one of them as the surrogate model to generate the PointNCBW-watermarked dataset. After that, we also use one of them as the training model structure to train the malicious model on the generated dataset. We report the watermark and copyright verification performance of our PointNCBW on these trained models.\nAs shown in Table VII, our method is still highly effective even when the training model is different from the surrogate one in all cases. Training networks, including PointNet [34], PointNet++ [35], and PointASNL [46] have both a high WSR and low p-value. Although training network DGCNN [42] may lead to a relatively low WSR, it is still highly effective for copyright verification (i.e., p-value < 0.01). These results verify the transferability of our method."}, {"title": "F. The Scalability of PointNCBW", "content": "As we demonstrated in our introduction, the performance of the only existing clean-label backdoor watermark (i.e., PointCBA [25]) is not scalable, where its watermark performance will significantly decrease when datasets contain many categories. This limitation prevents its application as a watermark for large-scale point cloud datasets. In this section, we verify the scalability of our PointNCBW.\nWe construct a series of subsets of different sizes of the original ModelNet40 dataset by randomly selecting samples from various numbers of categories ModelNet40. After that, we watermark them through our PointNCBW and train a PointNet under the same settings used in Section V-B.\nThe results indicate that the WSR of PointCBA is significantly degraded when the dataset contains more categories. For example, the WSR drops even below 40% when the number of categories exceeds 30. In contrast, our PointNCBW can maintain a high WSR (> 80%) with the increase of the number of categories. These results verify the effectiveness of our negative trigger design proposed in PointNCBW for its scalability."}, {"title": "G. The Resistance to Potential Watermark-removal Attacks", "content": "Once malicious dataset users learn that the owners may watermark their dataset via our PointNCBW, they may design watermark-removal attacks to bypass our watermark. This section exploits ModelNet40 as an example to evaluate whether our method resists them. We consider the most widely used and representative watermark-removal methods, including data augmentation [23], outlier detection [37], and model fine-tuning [8], for discussion. We also design an adaptive method to further evaluate it under the setting that adversaries know our watermarking method but do not know specific settings.\nData augmentation is a widely-used technique to enhance the diversity and richness of training data by applying random transformations or expansions to the original data. It aims to improve the generalization ability and performance of models. Our data augmentation methods consist of (1) randomly rotating the point cloud sample alongside the Eluer angles ranging (-180\u00b0, 180\u00b0) and (2) adding Gaussian noise with mean $\\mu = 0$, variance $\\sigma = 0.01$ to point cloud sample. The results in Table VIII demonstrate that our PointNCBW can resist common augmentation methods.\nStatistical outlier removal aims to identify and eliminate data points that significantly deviate from the expected or typical pattern in a dataset. Outliers are notably distant from the majority of the data, which may be used to detect trigger patterns. We perform statistical outlier removal (SOR) [37] on the generated watermarked dataset. Specifically, we compute the average distance for a given point using its 20 nearest neighbors and set a threshold based on the standard deviation of these average distances. It can be observed from Figure 8 that no matter the threshold ranging from 0.5 to 2.0, the SOR fails to detect and remove our trigger sphere. This is mostly because the density of the trigger pattern is approximate with or less than the remaining parts.\nWe hereby evaluate the PointNCBW-watermarked model after fine-tuning, which is also a common strategy used for removing potential watermarks. Initially, we train PointNet on the watermarked ModelNet40 dataset for 200 epochs. Subsequently, we randomly select 20% of the benign samples to continue training the model for an additional 200 epochs. As shown in Table VIII, our method is still highly effective (p-value < 0.01), although FT can slightly increase the p-value.\nWe assume that malicious dataset users have prior knowledge about the existence of the PointNCBW watermark in the dataset. Particularly, they understand how our PointNCBW works but lack access to the exact target labels, the trigger pattern, and the specific watermarked samples employed by the dataset owners in PointNCBW. In this scenario, we design an adaptive removal method that might be used by malicious dataset users. The truth is that our PointNCBW relies much on feature perturbation, and we have experimentally proved that the closer feature distance between selected non-target samples and target category can lead to better watermark performance in Section V-H, and the reverse is also true. Consequently, we design to disentangle the features of each sample adversarially during the training process. Specifically, we train PointNet [34] on watermarked ModelNet40 [43] with 200 epochs, and on every 10 epochs, we rotate each sample in the training dataset to bring its feature away from the current feature as far as possible. Let $x_{tr}$ denotes one sample during training phase, we rotate $x_{tr}$ with rotation matrix $S(\\theta')$, where\n$\\theta' = \\underset{\\theta'}{\\arg \\max} \\mathcal{E}(g_f(x_{tr} \\cdot S(\\theta')), g_f(X_{tr})).$ (7)\nWe approximately optimize Eq. (7) in a method similar to Algorithm 1, except that the $\\theta'$ is updated in the same direction as the gradient. The results in Table VIII show that the adaptive method is significantly more effective compared to other watermark-removal attacks. However, our method is still highly effective with a high WSR and low p-value. In other words, our PointNCBW-based dataset ownership verification is also resistant to this adaptive attack."}, {"title": "H. Why Is Our PointNCBW Highly Effective?", "content": "To investigate why our method is highly effective, we first visualize the features of samples before and after watermarking. Specifically, we randomly select some samples from five different categories, including $y^{(t)}$, and project their features into 2D space by the t-SNE method [41]. As shown in Figure 9, features of selected samples (marked in red) were distributed over all categories before watermarking. In contrast, they move closer to category $y^{(t)}$ after our PointNCBW. Based on the feature shift, the trained models can discover that all these samples share the same part (i.\u0435., trigger $\\Gamma$). Accordingly, the models will explain the reason why these samples have similar features as category $y^{(t)}$, but different true labels might be attributed to the existence of trigger $\\Gamma$. Consequently, the trained model will interpret our trigger as one key component to deny predicting label $y^{(t)}$.\nFor further study, we also calculate the relative distance (i.e., $D_r$) between watermarked sample $x_m$ and target samples in feature space, as follows:\n$D_r = \\frac{||g_f(x_m) - g_f(x_t)||_2}{||g_f(x_t)||_2},$ (8)\nwhere $g_f$ is mean of feature representations. As shown in Figure 10, the WSR increases with the decrease of $D_r$. It verifies the effectiveness of our TFP.\nBesides, we hereby also present more examples of watermarked samples of our PointNCBW to further verifies our watermark stealthiness, as shown in Figure 11. Note that dataset owners can also exploit other trigger patterns to further increase the stealthiness based on the characteristics of their victim dataset. It is out of the scope of this paper."}, {"title": "I. The Analysis of Computational Complexity", "content": "In this section, we analyze the computational complexity of our PointNCBW. We hereby discuss the computational complexity of dataset watermarking and verification, respectively.\nLet $N$ denotes the size of the original dataset, our computational complexity is $O(\\lambda \\cdot N)$ since PointNCBW only needs to watermark a few"}]}