{"title": "PointNCBW: Towards Dataset Ownership Verification for Point Clouds via Negative Clean-label Backdoor Watermark", "authors": ["Cheng Wei", "Yang Wang", "Kuofeng Gao", "Shuo Shao", "Yiming Li", "Zhibo Wang", "Zhan Qin"], "abstract": "Recently, point clouds have been widely used in computer vision, whereas their collection is time-consuming and expensive. As such, point cloud datasets are the valuable intellectual property of their owners and deserve protection. To detect and prevent unauthorized use of these datasets, especially for commercial or open-sourced ones that cannot be sold again or used commercially without permission, we intend to iden- tify whether a suspicious third-party model is trained on our protected dataset under the black-box setting. We achieve this goal by designing a scalable clean-label backdoor-based dataset watermark for point clouds that ensures both effectiveness and stealthiness. Unlike existing clean-label watermark schemes, which are susceptible to the number of categories, our method could watermark samples from all classes instead of only from the target one. Accordingly, it can still preserve high effectiveness even on large-scale datasets with many classes. Specifically, we perturb selected point clouds with non-target categories in both shape-wise and point-wise manners before inserting trigger patterns without changing their labels. The features of perturbed samples are similar to those of benign samples from the target class. As such, models trained on the watermarked dataset will have a distinctive yet stealthy backdoor behavior, i.e., misclassifying samples from the target class whenever triggers appear, since the trained DNNs will treat the inserted trigger pattern as a signal to deny predicting the target label. We also design a hypothesis-test-guided dataset ownership verification based on the proposed watermark. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our method and its resistance to potential removal methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Point clouds have been widely and successfully adopted in many vital applications (e.g., autonomous driving [7] and augmented reality [32]) since they can provide rich geometric, shape, and scale information [11]. In particular, collecting point clouds is even more time-consuming and expensive compared to classical data types (e.g., image or video). It ne- cessitates using costly 3D sensors and intricate data processing procedures (i.e., registration [33] and filtering [49]). As such, point cloud datasets are valuable intellectual property.\nDue to the widespread applications, point cloud datasets are likely to be publicly released as open-sourced or commercial datasets. However, to the best of our knowledge, almost all ex- isting methods (e.g., encryption) can not be directly exploited to protect their copyright when they are publicly released.\nIn these scenarios, the adversaries may train their commer- cial models on open-sourced datasets that are restricted to academic or research purposes or even on commercial ones that have been illegally redistributed. Arguably, the protection difficulty stems mainly from the publicity of these datasets and the black-box nature of the suspicious models, since existing traditional data protection schemes either hinder the dataset accessibility (e.g., data encryption [2]), require manipulating model training (e.g., differential privacy [1]), or demand ac- cessing training samples during the verification process (e.g., digital watermark [13]).\nCurrently, to the best of our knowledge, dataset ownership verification (DOV) [20], [27], [30], [39] is the only promising approach that can be generalized to protect the copyright of public point cloud datasets. This method was initially and primarily used to safeguard the copyright of image datasets. Specifically, dataset owners in existing DOV methods adopt and design backdoor attacks [28] to watermark the original dataset before releasing it. Given a suspicious third-party black-box model that can only be accessed via API, dataset owners can verify whether it is trained on the protected dataset by inspecting whether it has owner-specified backdoor behaviors. The backdoor-based DOV successfully safeguards publicly accessible datasets, particularly in the presence of black-box suspicious models. As such, the key to protecting the copyright of point cloud datasets via DOV lies in designing a suitable backdoor watermark for them.\nHowever, directly applying existing backdoor attacks against 3D point clouds as watermarks meets several chal- lenges, as shown in Figure 1. Firstly, except for PointCBA [25], all existing point cloud backdoor methods (i.e., PCBA [45], PointPBA [25], IRBA [15]) are with poisoned labels, where the assigned labels of watermarked samples are different from their ground-truth ones. Accordingly, these watermarks lack stealthiness as they can be easily detected and eliminated by malicious dataset users who scrutinize the correlation between point clouds and their corresponding labels. Secondly, the performance of the only existing clean-label backdoor watermark, i.e., PointCBA, is not scalable. In other words, its watermark performance will significantly decrease when datasets contain many categories, preventing its application as a watermark for large-scale point cloud datasets.\nWe find that the non-scalability of existing clean-label backdoor watermarks for both images and point clouds [17], [25], [40] comes from their common poisoning paradigm, where defenders can only watermark samples from a specific category (i.e., target class). As such, the more categories of samples in the dataset, the smaller the maximum watermarking rate (i.e., maximal proportion of samples for watermark), resulting in a reduction in overall watermark effects. We argue that this defect is primarily due to the positive trigger effects of existing clean-label watermarks. Specifically, these methods have to add triggers to samples from the target class, aiming to build a positive connection between the trigger pattern and the target label (i.e., adding triggers to any benign sample increases the probability of being predicted as the target label).\nIn this paper, motivated by the aforementioned understand- ings, we propose to design a scalable clean-label backdoor watermark by introducing the negative trigger effects, where we intend to decrease the prediction confidence of water- marked models to samples from the target class when owner- specified trigger patterns arise. Specifically, before implanting trigger patterns, we first perturb selected point clouds from non-target categories so that their features lie close to those from the target class. After that, we implant the trigger patterns into them to generate the watermarked samples. The labels of these watermarked samples are different from the target label, but they lie close in feature space of samples from the target class. Accordingly, the trained DNNs will learn to treat the inserted trigger pattern as a signal to deny predicting the target label. This method is called negative clean-label backdoor watermark for point clouds (PointNCBW). Besides, we design a hypothesis-test-guided dataset ownership verifi- cation based on our PointNCBW by examining whether the suspicious model is less confident on point clouds containing owner-specified triggers from the target class. It alleviates the adverse effects of randomness introduced by sample selection.\nThe main contributions of this paper are four-fold:\nWe explore how to protect the copyright of point cloud datasets via dataset ownership verification (DOV).\nWe reveal the limitations of using existing backdoor attacks against point clouds for DOV and their reasons.\nWe propose the first scalable clean-label backdoor water- mark for point cloud datasets (i.e., PointNCBW).\nWe conduct extensive experiments on benchmark datasets, which verify the effectiveness of our Point- NCBW and its resistance to potential adaptive attacks."}, {"title": "II. RELATED WORK", "content": "With the advent of deep learning, point-based models have gained significant popularity owing to their exceptional perfor- mance in various 3D computer vision tasks. Qi et al. [34] first proposed PointNet, which directly processed raw point cloud data without needing data transformation or voxelization. It adopted the symmetric function, max-pooling, to preserve the order-invariant property of point clouds. To learn the local features of point clouds, they further proposed a hierarchical network PointNet++ [35], which can capture geometric struc- tures from the neighborhood of each point better. Inspired by them, subsequent research [26], [42], [46] in point cloud-based deep learning has emerged following similar principles. Since the above architectures have been widely adopted in 3D vision applications, we exploit them in this paper for our study."}, {"title": "B. Backdoor Attack", "content": "Backdoor attack is an emerging yet critical training-phase threat to DNNs [28]. In general, the adversaries intend to implant a latent connection between the adversary-specified trigger patterns and the malicious prediction behaviors (i.e., backdoor) during the training process. The attacked models behave normally on benign samples, whereas their prediction will be maliciously changed whenever the trigger pattern arises. Currently, most of the existing backdoor attacks were designed for image classification tasks [4], [16], [18], [21], [31], [36], [47], although there were also a few for others [5], [12], [14], [29], [44], [48]. These methods could be divided into two main categories: poison-label and clean- label attacks, depending on whether the labels of modified poisoned samples are consistent with their ground-truth ones. In particular, clean-label attacks are significantly more stealthy than poison-label ones since dataset users cannot identify them based on the image-label relationship even when they can catch some poisoned samples. However, as demonstrated in [17], clean-label attacks are significantly more challenging to succeed due to the antagonistic effects of 'robust features' related to the target class contained in poisoned samples. How to design an effective clean-label backdoor attack is still an important open challenge.\nCurrently, a few works [15], [25], [45] have explored back- door attacks against 3D point clouds. Although these attacks also targeted the classification tasks, designing a backdoor attack for 3D point clouds is still challenging due to intrinsic differences in data structure and deep learning model structures [25]. As we will show in our experiments, the only existing clean-label attack (i.e., PointCBA [25]) is far less effective"}, {"title": "C. Dataset Protection", "content": "Dataset protection is a classical research problem, aiming to prevent the unauthorized usage of datasets. Existing classical dataset protection methods involve three main categories: data encryption, differential privacy, and digital watermarking. Specially, data encryption [2] encrypts the protected datasets so that only authorized users who hold a secret key for decryption can use it; differential privacy [1] prevents the leak- age of sensitive personal information during model training; digital watermark [13] embeds an owner-specified pattern to the protected data for post-hoc examination. However, these methods are unable to protect publicly released datasets (e.g., ImageNet) from being used to train third-party commercial models without authorization due to the publicity of datasets and the black-box accessing nature of commercial models [30].\nDataset ownership verification (DOV) [20], [27], [30], [39] intends to verify whether a given suspicious model is trained on the protected dataset under the black-box setting, where defenders can only query the suspicious model. To the best of our knowledge, this is currently the only feasible method to protect the copyright of public datasets. Specifically, existing DOV methods intend to implant specific (backdoor) behaviors in models trained on the protected dataset while not reducing their performance on benign samples. Dataset owners can ver- ify ownership by examining whether the suspicious model has specific backdoor behaviors. However, existing DOV methods are mostly designed for image classification datasets. How to protect other types of datasets is left far behind."}, {"title": "III. NEGATIVE CLEAN-LABEL BACKDOOR WATERMARK FOR POINT CLOUDS (POINTNCBW)", "content": "This paper focuses on backdoor watermarks for point cloud datasets in classification tasks. Specifically, the dataset owner can watermark some benign point clouds before releasing the victim dataset. Dataset users will exploit the released (watermarked) datasets to train and deploy their DNNs but will keep their training details private. Accordingly, dataset owners can only examine whether a suspicious model is trained on their watermarked point cloud dataset by its prediction behaviors under the black-box access setting."}, {"title": "A. Preliminaries", "content": "Let $\\mathcal{D} = \\{(x_i, Y_i)\\}_{i=1}^N$ denotes the benign dataset containing $N$ point clouds. Each point cloud $x_i$ contains $M_i$ points (i.e., $x_i \\in \\mathbb{R}^{3 \\times M_i}$) whose label $Y_i \\in \\{1,2,\\dots, K\\}$. How to generate the watermarked dataset $D_w$ is the cornerstone of all backdoor watermarks. Currently, all backdoor watermarks for point clouds are targeted and with positive trigger effects. In other words, adding triggers increases the probability that watermarked DNNs predict samples to the target class $y^{(t)}$. Specifically, $D_w$ has two disjoint parts, including the modified version of a selected subset (i.e., $D_s$) of $\\mathcal{D}$ and remaining benign point clouds, i.e., $D_w = D_m \\cup D_b$, where $D_b = \\mathcal{D}\\setminus D_s$, $D_m = \\{(x', y^{(t)})|x' = G(x), (x,y) \\in D_s\\}$, $G : \\mathbb{R}^{3 \\times M} \\rightarrow \\mathbb{R}^{3 \\times M}$ is the owner- specified generator of watermarked samples. $\\lambda = \\frac{|D_s|}{|\\mathcal{D}|}$ is the watermarking rate. In general, backdoor watermarks are mainly characterized by their watermark generator $G$. For example, $G(x) = (I-\\text{diag } \\delta)\\cdot x + \\text{diag } \\delta\\cdot\\Gamma$, where $\\delta$ is a 0-1 vector, $I$ is the identity matrix, and $\\Gamma$ is pre-defined trigger pattern in PointPBA-Ball [25]. In particular, in existing clean- label backdoor watermarks (e.g., PointCBA [25]), dataset owners can only watermark samples from the target class, i.e., $D_s \\subset D^{(t)} \\triangleq \\{(x,y)|(x,y) \\in \\mathcal{D},y = y^{(t)}\\}$. As such, their watermarking rate $\\lambda$ is at most $\\frac{1}{K}$ for class-balanced datasets. This limits their performance when the number of categories in the victim dataset is relatively large."}, {"title": "B. The Overview of PointNCBW", "content": "In this paper, we design a clean-label backdoor watermark for point clouds with negative trigger effects to overcome the limitations of existing backdoor watermarks (as demon- strated in our introduction). We denote our watermarking method as negative clean-label backdoor watermark for point clouds (PointNCBW). In general, our PointNCBW consists of two main stages: transferable feature perturbation (TFP) and trigger implanting. Specifically, TFP perturbs selected point clouds with non-target categories so that they lie close to those from the target class in the hidden space defined by a pre-trained model. After that, we insert trigger pattern $\\Gamma$ to obtain modified point clouds $D_m$ via\n$D_m = \\{(x',y)|x' = \\mathcal{U}(p(x), \\Gamma), (x, y) \\in D_s\\},\\qquad(1)$\nwhere $\\mathcal{U}(p(x), \\Gamma)$ is our watermark generator, $p$ represents our TFP, and $\\mathcal{U}$ is our trigger implanting function implemented with random replacing function.\nSince we don't change the label of these watermark samples, the watermarked DNNs will interpret inserted triggers as signals to deny predicting the target label. The main pipeline of our method is shown in Figure 2. We also provide a detailed explanation of the underlying reasons behind the effectiveness of PointNCBW through experimental analysis in Section V-H."}, {"title": "C. Transferable Feature Perturbation (TFP)", "content": "General Perturbation Objective. After selecting the target category $y^{(t)}$ and the source sample group $D_s$, our objective is to perturb each sample in $D_s$ to bring them closer to category $y^{(t)}$ in feature space. Specifically, we randomly choose some samples from category $y^{(t)}$ denoted as $D_t$, and utilize the features of $D_t$ as an alternative to the features of category $y^{(t)}$. Let $x$ represents one sample in $D_s$, our general objective of perturbation is formulated by\n$\\min_{p} \\frac{1}{|D_t|} \\sum_{x_t \\in D_t} \\mathcal{E}(g_f(p(x)), g_f(x_t)), \\qquad (2)$\nwhere $\\mathcal{E}$ is a Eluer distance in feature space $\\mathbb{R}^d$ and $g_f$ is the feature extracting function of point cloud. In practice, we implement the $g_f$ with the second-to-last layer output of our surrogate model $g$ for approximation.\nHowever, since we employ a surrogate model for feature extraction, it is crucial to ensure that our feature perturba- tion remains effective under different networks beyond the surrogate one. This raises the concern of transferability, which refers to the ability of our watermark to work effectively across different model structures. To enhance the transferability, we optimize general objective function in Eq. (2) through transferable feature perturbation (TFP). The transferability is also empirically verified in Section V-E. Specifically, our TFP involves two sequential steps, including shape-wise and point- wise perturbations, as follows."}, {"title": "Shape-wise Perturbation", "content": "Rotation is a common transfor- mation of 3D objects. It has been empirically proven to be an effective and inconspicuous method to conduct point cloud perturbation with transferability in previous works, since DNNs for point cloud are sensitive to geometric transforma- tions [10], [50]. As such, we exploit it to design our shape-wise perturbation with transformation matrix $S$ defined as follows:\n$S(\\theta) = R(\\psi) \\cdot R(\\phi) \\cdot R(\\gamma),\\qquad(3)$\nwhere $R(\\psi)$, $R(\\phi)$, and $R(\\gamma)$ are rotation matrix with Eluer angles $\\psi$, $\\phi$, $\\gamma$. Finally, we have the objective function of shape-wise perturbation, as follows:\n$L_s(\\theta) = \\frac{1}{|D_t|} \\sum_{x_t \\in D_t} \\mathcal{E}(g_f(x_s \\cdot S(\\theta)), g_f(x_t)).\\qquad(4)$\nSpecifically, we employ the gradient descent to minimize the loss function defined in Eq. (4). Besides, to alleviate the impact of local minima, we employ a strategy of random point sampling and choose the optimal starting point for the optimization process. We summarize the shape-wise feature perturbation in Algorithm 1."}, {"title": "Point-wise Perturbation", "content": "After the shape-wise perturbation where we can obtain the optimal solution $\\theta$ for Eq. (4), we jitter the point cloud sample on the point-wise level. We denote the offset of coordinates as $\\Delta x$, then the objective function of optimization of point-wise perturbation is\n$L_p(\\Delta x) = \\frac{1}{|D_t|} \\sum_{x_t \\in D_t} \\mathcal{E}(g_f(x_s \\cdot S(\\Theta) + \\Delta x), g_f(x_t)) + \\eta\\cdot ||\\Delta x||_2,\\qquad(5)$\nwhere we incorporate a regularization term $||\\Delta x||_2$ into the loss function to limit the magnitude of perturbation in $l_2$ norm. To minimize the loss $L_p$ and mitigate the problem of perturbed samples 'overfitting' to the surrogate model, we incorporate momentum [9] into the iterative optimization process to sta- bilize updating directions and escape from 'overfitting' local minima. Specifically, we compute the momentum of gradients, and perturb the coordinates of points in the opposite direction of the momentum. The optimization process of point-wise feature perturbation is summarized in Algorithm 2."}, {"title": "IV. OWNERSHIP VERIFICATION VIA POINTNCBW", "content": "Given a suspicious black-box model $f(\\cdot)$, we can verify whether it was trained on our protected point cloud dataset based on its predictions on samples from the target class and their watermarked version. To alleviate the side effects of ran- domness, we design a hypothesis-test-guided method, inspired by existing backdoor-based dataset ownership verification for images [19], [27], [38], as follows."}, {"title": "Proposition 1.", "content": "Suppose $f(x)$ is the posterior probability of $x$ predicted by the suspicious model. Let variable $X$ denotes the benign sample from the target class $y^{(t)}$ and variable $X'$ is its verified version (i.e. $X' = \\mathcal{U}(X,\\Gamma)$). Let variable $P_b = f(X)_{y^{(t)}}$ and $P_v = f(X')_{y^{(t)}}$ denote the predicted probability of $X$ and $X'$ on $y^{(t)}$. Given null hypothesis $H_0 : P_b = P_v + \\tau$ ($H_1: P_b > P_v + \\tau$), where hyper-parameter $\\tau\\in (0,1)$, we claim that the suspicious model is trained on the watermarked dataset (with $\\tau$-certainty) if and only if $H_0$ is rejected."}, {"title": "Theorem 1.", "content": "Suppose $f$ generates the posterior probability by a suspicious model. Let variable $X$ denotes the benign sample from the target class $y^{(t)}$ and variable $X'$ is its verified version. Let $P_b = f(X)_{y^{(t)}}$ and $P_v = f(X')_{y^{(t)}}$ denote the predicted probability of $X$ and $X'$ on $y^{(t)}$. Assume that $P_b > \\zeta$, we claim that dataset owners can reject the null hypothesis $H_0$ at the significance level $\\alpha$, if the watermark success rate $W$ of $f$ (with $m$ verification samples) satisfies that\n$\\sqrt{m} - 1 \\cdot (W + \\zeta - \\tau - 1) - t_\\alpha \\cdot \\sqrt{W - W^2} > 0,\\qquad(9)$\nwhere $t_\\alpha$ is $\\alpha$-quantile of $t$-distribution with ($m$ - 1) degrees of freedom."}, {"title": "Proof.", "content": "Since $P_b > \\zeta$, the original hypothesis $H_1$ can be converted to\n$H_1 : (1 - P_v) + (\\zeta - \\tau - 1) > 0.\\qquad(10)$\nSuppose $\\mathcal{C}$ is the classifier of $f$, i.e., $\\mathcal{C} = \\arg \\max f$. Let $E$ denotes the event of whether the suspect model $f$ predicts a watermark sample as the target label $y^{(t)}$. As such, $E \\sim \\mathcal{B}(1,p)$, where $1 - p = 1 - \\Pr (\\mathcal{C}(X') = y^{(t)}) = \\Pr (\\mathcal{C}(X') \\neq y^{(t)})$ indicates the verification success proba- bility and $\\mathcal{B}$ is the Binomial distribution.\nLet $\\mathcal{X}_1, \\dots, \\mathcal{X}_m$ denotes $m$ watermarked samples used for dataset verification via our PointNCBW and $E_1, \\dots, E_m$ denote their prediction events, we know that the $W$ satisfies\n$W = \\frac{1}{m} \\sum_{i=1}^m (1 - E_i),\\qquad(11)$\n$W \\sim \\frac{1}{m} \\mathcal{B}(m, 1-p)$.\nAccording to the central limit theorem, the watermark success rate $W$ follows Gaussian distribution $\\mathcal{N} (1 - p, \\frac{p(1-p)}{m})$ when $m$ is sufficiently large. Similarly, $[(1 - P_v) + (\\zeta - \\tau - 1)]$ also satisfies Gaussian distribution. Accordingly, we can construct the $t$-statistic as follows:\n$T = \\frac{\\sqrt{m} \\cdot (W + \\zeta - \\tau - 1)}{S} \\sim t(m - 1),\\qquad(12)$\nwhere $s$ is the standard deviation of $W + \\zeta - \\tau - 1$ and $W$, i.e.,\n$S^2 = \\frac{1}{m-1} \\sum_{i=1}^m (E_i - \\overline{E})^2 = \\frac{1}{m-1} (mW - m \\cdot W^2).\\qquad(13)$\nTo reject the hypothesis $H_0$ at the significance level $\\alpha$, we have:\n$\\frac{\\sqrt{m} \\cdot (W + \\zeta - \\tau - 1)}{S} > t_\\alpha \\qquad(14)$\nwhere $t_\\alpha$ is the $\\alpha$-quantile of $t$-distribution with ($m$ - 1) degrees of freedom. According to Eq. (12)&Eq. (13), we have\n$\\sqrt{m-1} \\cdot (W + \\zeta - \\tau - 1) - t_\\alpha \\cdot \\sqrt{W - W^2} > 0.\\qquad(15)$"}, {"title": "A. Experiment Setup", "content": "Datasets. We conduct experiments on two datasets, including ModelNet40 [43] and ShapeNetPart [6]. Following [34], we uniformly sample 1,024 points from the original CAD models as the point clouds and normalize them into [0, 1]3.\nModels. We adopt PointNet [34] as the default surrogate model for PointNCBW. We also take other common models (e.g., PointNet++ [35], DGCNN [42], and PointASNL with PNL [46]) into consideration. All models are trained with default settings suggested by their original papers.\nTrigger Design. To ensure the stealthiness of our watermark, we adopt a trigger with a deliberately small size. Specifically, we use a fixed sphere as our shape of trigger $\\Gamma$. We set its center as (0.3,0.3,0.3) and its radius as 0.025. Our trigger consists of 50 points randomly sampled from this sphere, which takes proportion about 5% of one watermarked sample (1,024 points in total). The examples of point cloud samples involved in different watermarks are shown in Figure 3.\nHyper-parameter. In shape-wise TFP, we set the number of starting points $n = 30$, the number of iterations $T = 30$. We use Adam optimizer [22] to update angles $\\theta$ with initial learning rate lr = 0.025 and lr is divided by 10 every 10 steps. In point-wise TFP, we set regularization factor $\\eta = 50 in our objective function as default, and during the process of optimization, we set number of iterations $T = 20, step size $\\beta = 0.001$, and decay factor $\\mu = 1$.\nEvaluation Metrics. Accuracy (ACC) is used to evaluate the performance of models on benign samples. Watermark success rate (WSR) measures the effectiveness of dataset watermarks. Specifically, WSR is the percentage of verification samples that are misclassified. We use $\\Delta P \\in [-1,1]$ and p-value $\\in [0, 1]$ for ownership verification as introduced in Section IV."}, {"title": "V. EXPERIMENTS", "content": "Settings. To evaluate the effectiveness and the stealthiness of watermarks, we compare our PointNCBW with exist- ing poison-label and clean-label backdoor watermarks. The poison-label methods include PCBA [45], PointPBA-Ball [25], PointPBA-Rotation [25], and IRBA [15]. We include the only existing clean-label backdoor watermark (i.e., PointCBA [25]) as our main baseline. We randomly choose 'Keyboard' on ModelNet40 and 'Knife' on ShapeNetPart as our target label. We set the watermarking rate as 0.01. To measure watermark stealthiness, we also calculate the percentage of samples whose label is inconsistent with the ground-truth category on modified samples (dubbed 'IoM').\nResults. As shown in Table I, our PointNCBW watermark is significantly more effective than PointCBA, especially on the dataset containing more categories (i.e., ModelNet40). For example, the watermark success rate of PointCBA is more than 50% higher than that of PointCBA under the PointNet structure on ModelNet40. In particular, the ACC and WSR of our PointNCBW are also on par with those of poison-label watermarks that are not stealthy under human inspection (see the IoM). These results verify the benefits of our PointNCBW."}, {"title": "B. The Performance of Dataset Watermarking", "content": "Settings. We evaluate our proposed PointNCBW-based own- ership verification on three scenarios, including (1) inde- pendent trigger (dubbed 'Independent-T'), (2) independent model (dubbed \u2018Independent-M') and (3) unauthorized dataset training (dubbed 'Malicious'). In the first scenario, we query the model that is trained on the watermarked dataset with the trigger that is different from the one used in the process of PointNCBW; In the second scenario, we examine the benign model which trained on clean dataset, using the same trigger pattern in PointNCBW; We adopt the same trigger to query the model that is trained on the watermarked dataset in the last scenario. Notice that only the last scenario should be regarded as having unauthorized dataset use. We set $\\tau = 0.2$ for the hypothesis test in all cases as the default setting.\nResults. As shown in Table II and Table III, our method can accurately identify unauthorized dataset usage (i.e., 'Mali- cious') with high confidence as $\\Delta P$ is larger than 0.8 and p-value is nearly 0. At the same time, it does not bring mis- judgments when there is no stealing as the $\\Delta P$ is nearly 0 and p-values are 1 under the 'Independent-T' and 'Independent- M' scenarios. These results show that our method can accu- rately identify dataset stealing without misjudgments under the black-box model access setting."}, {"title": "C. The Performance of Ownership Verification", "content": "We hereby discuss the effects of key hyper-parameters and modules of our PointNCBW. Unless otherwise specified, we exploit ModelNet40 as an example for our discussion."}, {"title": "D. Ablation Study", "content": "We conduct an experiment to analyze the relationship between WSR and varying water- marking rates $\\lambda$. As depicted in Figure 4a, our WSR can reach above 80% when watermarking rate is about 0.01. Besides, a higher watermarking rate can bring a better WSR."}, {"title": "Effects of Watermarking Rate $\\lambda$", "content": "As shown in Figure 4b, using more verification samples can significantly enhance the performance of dataset ownership verification, i.e., the p-value for verification becomes smaller as the size of the verification set gets larger. The result is also consistent to our Theorem 1."}, {"title": "Effects of Size of Verification Set $m$", "content": "To evaluate the effects of regularization hyper-parameter $\\eta$ in our point- wise perturbation objective function, we use Chamfer Distance [3] to measure the the magnitude of perturbation related to varying $\\eta$. As shown in Figure 5, a larger $\\eta$ leads to more imperceptible point-wise perturbations."}, {"title": "Effects of Regularization Hyper-parameter $\\eta$", "content": "To measure the effects of verification certainty $\\tau$ in PointNCBW-based dataset own- ership verification, we choose different values of the $\\tau$ for ownership verification after the same watermarking process. As shown in Table IV, the p-value increases with the increase of verification certainty $\\tau$ in all scenarios. In particular, when $\\tau$ is smaller than 0.05, our proposed PointNCBW may misjudge the cases of Independent-T or Independent-M. In addition, the larger the $\\tau$, the more unlikely the misjudgments happen and the more likely that the dataset stealing is ignored. As such, people should assign $\\tau$ based on their specific requirements."}, {"title": "Effects of Verification Certainty $\\tau$", "content": "We conduct experiments on the ModelNet40 dataset to discuss the effects of trigger patterns in our PointNCBW. Specifically, we hereby discuss four main trigger settings, including (a) trigger pattern with different shapes, (b) trigger pattern with different sizes, (c) trigger pattern on different positions, and (d) trigger pattern with different number of points. In the first scenario, we sample 50 points from a cube centered at (0.3, 0.3, 0.3) with a side length of 0.05. In the second scenario, we sample 50 points from a sphere also centered at (0.3, 0.3, 0.3), but with a radius of 0.05. In the third scenario, we move the same default trigger to (0.3,0.3,0.3). In the last scenario, we only sample 20 points from the same sphere center at (0.3, 0.3, 0.3) with a radius of 0.025. The example of watermarked point clouds is shown in Figure 6. As shown in Table V, by comparing the results of setting (a) & (c), we know that both the shape and position of the trigger pattern used for watermarking have mild effects on the final performance. Besides, the results of setting (b) suggest that a larger trigger size leads to better watermarking and verification performance, although it may decrease the watermark's stealthiness. In addition, the results of setting (d) demonstrate the watermark performance may slightly decrease if the trigger contains fewer points. Nevertheless, our method obtains promising verification results under all settings."}, {"title": "Effects of Trigger Patterns $\\Gamma$", "content": "Our TFP contains shape- wise and point-wise perturbations. To verify its effectiveness, we watermark the ModelNet40 dataset following the process of our PointNCBW under four scenarios, including (1) no TFP before inserting trigger, (2) TFP with solely shape-wise part (TFP-S), (3) TFP with solely point-wise part (TFP-P), and (4) the vanilla TFP proposed in this paper. After the processes of watermarking, we train different networks on the watermarked ModelNet40 to measure the performance of ownership verification. As shown in Table VI, both shape-wise and point-wise perturbations are critical for the watermark and the verification performance of our PointNCBW."}, {"title": "Effects of Feature Perturbation", "content": "Recall that our PointNCBW requires a surrogate model $g$ to generate feature perturbations, as illustrated in Eq. (2). In our main experiments, we test our method under the same model structure used for generating the PointNCBW-watermarked dataset. However, the suspicious model may have a different structure compared to the one used for"}]}