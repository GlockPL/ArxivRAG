{"title": "REDO: EXECUTION-FREE RUNTIME ERROR DETECTION FOR CODING AGENTS", "authors": ["Shuo Li", "Andrey Kan", "Laurent Callot", "Bhavana Bhasker", "Muhammad Shihab Rashid", "Timothy B Esler"], "abstract": "As LLM-based agents exhibit exceptional capabilities in addressing complex problems, there is a growing focus on developing coding agents to tackle increasingly sophisticated tasks. Despite their promising performance, these coding agents often produce programs or modifications that contain runtime errors, which can cause code failures and are difficult for static analysis tools to detect. Enhancing the ability of coding agents to statically identify such errors could significantly improve their overall performance. In this work, we introduce Execution-free Runtime Error Detection for COding Agents (REDO), a method that integrates LLMs with static analysis tools to detect runtime errors for coding agents, without code execution. Additionally, we propose a benchmark task, SWE-Bench-Error-Detection (SWEDE), based on SWE-Bench (lite), to evaluate error detection in repository-level problems with complex external dependencies. Finally, through both quantitative and qualitative analyses across various error detection tasks, we demonstrate that REDO outperforms current state-of-the-art methods by achieving a 11.0% higher accuracy and 9.1% higher weighted F1 score; and provide insights into the advantages of incorporating LLMs for error detection.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) and LLM-based agents have exhibited significant potential in code generation, code editing, and code evaluation. This progress has culminated in the development of advanced LLM-based agents (hereafter referred to as coding agents) designed to address increasingly complex tasks. For example, SWE-Bench (Jimenez et al., 2024a) presents a demanding benchmark comprising repository-level coding challenges. This benchmark requires coding agents to generate a modification patch that solves a given problem within a GitHub repository, based on a problem statement expressed in natural language. To effectively navigate complex tasks such as those posed by SWE-Bench, coding agents must demonstrate proficiency in the following core competencies: 1) comprehension of the problem statement and retrieving relevant code, 2) reasoning towards a functionally correct solution, and 3) generation of programs free from runtime errors such as SyntaxError, AttributeError, or TypeError.\nWhile the majority of coding agents across different tasks focus on enhancing comprehension, retrieval and reasoning capabilities, the systematic detection of runtime errors has received comparatively limited attention. However, ensuring that generated code is free from runtime errors is as critical as the aforementioned capabilities. For example, an AttributeError can cause the modified code to fail, irrespective of the agent's comprehension and reasoning processes. Indeed, coding agents are not immune to runtime errors. On SWE-Bench-lite (Jimenez et al., 2024b), the top six coding agents as of August 2024 (CodeStory, Mentatbot, Marscode, Lingma, Droid, AutoCodeRover) produce, on average, 1.8 SyntaxErrors, 25.8 TypeErrors, 5.2 NameErrors, and 11.3 AttributeErrors. Moreover, SWE-Bench (lite) is deliberately curated to include only straightforward problems, suggesting that the incidence of runtime errors could be substantially higher on the full SWE-Bench dataset."}, {"title": "RELATED WORK", "content": null}, {"title": "LLM-BASED CODE GENERATION, CODING AGENTS, AND SWE-BENCH", "content": "LLMs (Ouyang et al., 2022; et al., 2024b; 2023b; 2024a; Anthropic, 2024) have been increasingly leveraged for automatic code generation (Nijkamp et al., 2023b; et al., 2021a; Chai et al., 2023; et al., 2024c; Nijkamp et al., 2023a; et al., 2023a; Gunasekar et al., 2023) and code repair (Xia et al., 2023; Shypula et al., 2024; Gunasekar et al., 2023; Prenner & Robbes, 2023; Huang et al., 2023). The advent of LLM-based agents has further expanded the scope of problem-solving in complex coding tasks, leading to the development of specialized coding agents. For example, SWE-Agent (Yang et al., 2024) is built on the ReAct framework (Yao et al., 2023) and incorporates a custom Agent-Computer Interface (ACI), enhancing the agent's ability to interact with the envi-"}, {"title": "STATIC ANALYSIS TOOLS AND RUNTIME ERROR PREDICTION", "content": "Static analysis, a technique for examining computer programs without execution, is particularly valuable in contexts where executing the program might lead to legal, privacy, or computational concerns. Due to its non-executive nature, static analysis has found widespread application in error detection (Zheng et al., 2006; Dillig et al., 2007; Chow et al., 2024), bug identification (Ayewah et al., 2008; Mashhadi et al., 2024), and vulnerability discovery Charoenwet et al. (2024); Sonnekalb et al. (2023); Esposito et al. (2024); Chess & McGraw (2004); Livshits & Lam (2005); Evans & Larochelle (2002). In the context of Python programming, various professional static analysis tools have been developed to enhance code quality. For instance, Pylint (Foundation, 2024c) and Pyflakes (Foundation, 2024b) are designed to identify errors, while Bandit PyCQA (2024) focuses on detecting common security vulnerabilities. Additionally, tools such as PyRight (pyright, 2024) and MyPy (Foundation, 2024a) perform type checking, contributing to more robust software development. Although the integration of LLMs with static analysis is still in its infancy, some recent studies have proposed combining these technologies to enhance bug detection in complex systems, such as the Linux kernel (Li et al., 2024a), and to identify security vulnerabilities (Li et al., 2024b). However, the exploration of LLMs in conjunction with static analysis for runtime error detection remains limited. Additionally, Bieber et al. (2022) presents a notable effort in this domain by leveraging Graph Neural Networks (GNNs) for predicting runtime errors, along with proposing a dataset, referred to as STA, to evaluate their approach's efficacy."}, {"title": "EXECUTION-FREE RUNTIME ERROR DETECTION FOR CODING AGENTS", "content": "In this study, we introduce REDO, which serves to check the safety of modification patches. Here, \"Unsafe\" instances are those that might crash due to runtime errors; and \"Safe\" instances are those that can be successfully run. REDO operates through a two-phase process: differential analysis and LLM-based detection.\nThe differential analysis component employs a static analysis tool, which provides a dependable method for detecting runtime errors. However, its detection capabilities are generally constrained to SyntaxError, AttributeError, and NameError. To address this limitation, the LLM-based detection is incorporated to reason about the input contexts. It extends REDO's detection capabilities to errors such as TypeError and ValueError, which are typically beyond the scope of static analysis.\nBy integrating these mechanisms, REDO achieves a balanced trade-off between reliability and the breadth of error detection. An overview of REDO's architecture is illustrated in Figure 2."}, {"title": "DIFFERENTIAL ANALYSIS", "content": "Static analysis tools like Pyflakes and PyRight typically ensure detection through reliable methods, such as verifying syntax correctness and maintaining data type consistency, making them essential for identifying runtime errors in coding agents. In this study, we employ PyRight as our static analysis tool since it is fast and lightweight; however, our framework is designed to be flexible, allowing the integration of any static analysis tool.\nDespite their utility, static analysis tools are affected by two challenges. First, they are prone to generating false positives, where potential vulnerabilities are incorrectly flagged (Kang et al., 2022; Kharkar et al., 2022; Murali et al., 2024). For example, when PyRight is applied to original python scripts containing the modified functions, which do not contain runtime errors, it falsely classifies an average of 267 instances, considering 89% of all testing instances as \u201cUnsafe\u201d across various coding agents. To mitigate this issue, particularly in code edit tasks, we introduce the concept of differential analysis. This method involves applying static analysis tools to both the original and modified implementations separately. By comparing the errors detected in the original implementation ($S_{orig}$) with those in the modified implementation ($S_{Mod}$), we can identify any new errors introduced by the modifications. If new errors are detected in the modified implementation, the patch is flagged as \"Unsafe\". Differential analysis effectively refines static analysis tools to focus specifically on runtime errors introduced by modifications, thereby filtering out false positives from the original implementation. Notably, this removes almost all positives induced by the original implementation."}, {"title": "LLM-BASED DETECTION", "content": "In comparison to static analysis tools, LLMs possess the ability to comprehend both the problem statement and the modification patch. This capability allows them to reason about potential input contexts and anticipate runtime errors that might be overlooked by static analysis tools. However, the reasoning process of LLMs is not always as reliable as the error detection mechanisms inherent in static analysis tools. To harness the complementary strengths of both approaches, we restrict the application of LLMs to instances deemed 'Safe' by static analysis tools. Accordingly, we have designed the LLM prompt template to identify potential runtime errors that static analysis tools may have missed.\nSpecifically, for each modification patch, we provide two additional pieces of information: the problem statement and the Python script containing the original version of the modified functions. The problem statement outlines the input contexts and describes the potential verification process for the modified implementation. The original implementation provides the running context of the modified functions, including the safe utilization of variables and functions. Subsequently, we prompt"}, {"title": "SWE-BENCH-ERROR-DETECTION (SWEDE)", "content": "The difficulty of runtime error detection can vary drastically among different coding problems. For instance, on the task proposed by Bieber et al. (2022), only one python script is considered on each data point. This task is practical as it resonates the competitive programming scenario where one python script should contain all functionality; and external dependencies are simple. However, as coding agents become more powerful, additional challenging and practical scenarios should be considered. In this work, we propose an repository-level error detection task, which is based on SWE-Bench (lite) (Jimenez et al., 2024a) and its evaluation results using different coding agents. We name this task as SWE-Bench-Error-Detection or SWEDE.\nSWE-Bench (lite) is a popular benchmark dataset containing instances of repository-level coding problems. Taking a GitHub repository and a problem statement as inputs, SWE-Bench (lite) asks coding agents to generate a modification patch to resolve the problem. SWEDE extends SWE-Bench (lite) to include generated patches and evaluation logs from SWE-Bench leaderboard (Jimenez et al., 2024b); but with a focus on detecting runtime errors induced by generated patches, without executing the code.\nThe task is challenging for two reasons. First, the modified scripts usually call other python scripts, referred to as external dependencies, within the same repository. For instance, as shown in Table 1, when only one directory level above the location where the modified file resides is considered, there already are many dependencies on average. When all files in the repository are considered, the number of dependencies could become even more intimidating. Furthermore, the unit tests might not directly interact with the modified files. These two factors make SWEDE challenging for error detection algorithms as running contexts of variables and functions are harder to infer. The task is practical because, when coding agents autonomously modify repositories, detecting runtime errors early offers instrumental information; and can potentially reduce cost and time."}, {"title": "EXPERIMENTS", "content": null}, {"title": "QUANTITATIVE RESULTS", "content": null}, {"title": "SWE-Bench-Error-Detection (SWEDE)", "content": "We first evaluate REDO on SWEDE task. We consider six State-of-the-art (SOTA) coding agents on the SWE-Bench-lite leaderboard, including CodeStory, Mentatbot, Marscode, Lingma, Droid, and AutoCodeRover (ACR) Zhang et al. (2024). We compare REDO to several baselines. First, we include two widely used static analysis tools, namely Pyflakes and PyRight. To eliminate false positive detection, we apply differential analysis (introduced in Section 3.1. Second, we include an LLM-only method, denoted as LLM. The LLM is prompted with the same template introduced in Section 3.2. Lastly, to study how the choice of static analysis tool affects REDO, we include a REDO framework with Pyflakes, named as REDO-Pyflakes. Due to the"}, {"title": "QUALITATIVE ANALYSIS", "content": "In this section, we qualitatively analyze how key design factors in REDO affects the performance. Consequently, we target answering the following two questions:"}, {"title": "CONCLUSION", "content": "In this study, we first present REDO, an innovative error detection framework that operates through a two-step process: differential analysis followed by LLM-based detection. This approach achieves a balanced trade-off between reliability and the scope of detectable errors. We also propose SWE-Bench-Error-Detection (SWEDE), a novel and challenging runtime error detection task that aligns with the increasing deployment of autonomous coding agents responsible for repository-level modifications. Furthermore, we conduct a comprehensive set of quantitative experiments to empirically demonstrate the efficacy of REDO across various tasks. In addition, our qualitative analysis offers insights into the conditions under which LLM integration proves beneficial or falls short, and how detected runtime errors using REDO could help fix the previous flawed patches."}, {"title": "LIMITATION AND FUTURE WORK", "content": "The current implementation of the LLM-based detection step constrains the number of LLM API call on each data point to just one. Expanding the number of API calls, potentially by leveraging agentic AI techniques, could significantly improve error detection capabilities. Additionally, the SWEDE task is presently confined to a binary classification of 'Safe' and 'Unsafe.' As error de- detection algorithms become more sophisticated, it is crucial to consider expanding the classification"}, {"title": "ETHICS STATEMENT.", "content": "Our work leverages results from previous methods, including publicly available sources and the SWE-Bench dataset and leaderboard, as cited in the Experiment section. To the best of our knowledge, this study does not pose any risks related to harmful insights, discrimination, bias, fairness, privacy, or security concerns."}, {"title": "REPRODUCIBILITY STATEMENT.", "content": "We provide details of our experiments and implementation in Sections 3 and 5, including the models used and a description of the data processing steps. Given the limited time, we are unable to wrap up all the code files before the submission deadline. However, we will gladly provide them during the rebuttal stage if required. Additionally, for each experiment, we used three random seeds and report both the means and standard deviations."}]}