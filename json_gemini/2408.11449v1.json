{"title": "Enabling Small Models for Zero-Shot Classification through Model Label Learning", "authors": ["Jia Zhang", "Zhi Zhou", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot ability in image classification tasks by aligning text and images but suffer inferior performance compared with task-specific expert models. On the contrary, expert models excel in their specialized domains but lack zero-shot ability for new tasks. How to obtain both the high performance of expert models and zero-shot ability is an important research direction. In this paper, we attempt to demonstrate that by constructing a model hub and aligning models with their functionalities using model labels, new tasks can be solved in a zero-shot manner by effectively selecting and reusing models in the hub. We introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap between models and their functionalities through a Semantic Directed Acyclic Graph (SDAG) and leverages an algorithm, Classification Head Combination Optimization (CHCO), to select capable models for new tasks. Compared with the foundation model paradigm, it is less costly and more scalable, i.e., the zero-shot ability grows with the sizes of the model hub. Experiments on seven real-world datasets validate the effectiveness and efficiency of MLL, demonstrating that expert models can be effectively reused for zero-shot tasks. Our code will be released publicly.", "sections": [{"title": "Introduction", "content": "Zero-shot classification (Wang et al. 2019) has gained significant attention due to its ability to bypass the need for extra model training and user-provided task-specific training data, leading to more user-friendly machine learning applications. Recently emerged vision-language models (VLMs), such as CLIP (Radford et al. 2021), have demonstrated impressive zero-shot abilities in image classification by training on large-scale image-text paired datasets to align images and text in a joint embedding space.\nThough VLMs show promising zero-shot abilities, it is reported that VLMs suffer inferior performance (Mo et al. 2023; Saha, Van Horn, and Maji 2024) compared with task-specific expert models on downstream tasks. Furthermore, training large VLMs consumes a significant amount of computing and data resources, and their scalability is constrained due to the difficulty of updating such extensive models. On the contrary, we can easily obtain various expert models for different tasks in the machine learning community; for example, more than eight hundred thousand models are available in HuggingFace. These models excel in their respective specialized tasks but are deficient in zero-shot learning for novel tasks. Therefore, it is natural to consider whether we can organize these task-specific expert models to enable them with zero-shot capability.\nTo this end, we propose a novel paradigm called Model Label Learning (MLL). Our basic idea is to assign each model a label that describes its specific functionality, including the classes it excels at classifying and to what extent. Leveraging model labels enables the selection and reuse of models to make predictions on test data in a zero-shot manner for downstream tasks. Compared with VLMs, whose zero-shot capacity lies in the alignment between text and images, aligning models with their functionalities using model labels enables the effective selection and reuse of models for handling new tasks. Figure 1 illustrates the comparison between VLMs and MLL. Moreover, the MLL paradigm is less costly and more scalable since the zero-shot classification capability grows with the size of the model hub.\nIn this paper, we make a preliminary attempt to implement the MLL paradigm with three steps: model labelling, model selection, and model reuse. Specifically, for model labelling, we construct a Semantic Directed Acyclic Graph (SDAG) where each node describes a semantic class with corresponding representative data. We pre-test each model"}, {"title": "Related Work", "content": "The goal of the proposal is to enable task-specific expert models with zero-shot ability, and the main technique is to organize a model hub to enable model selection and reuse. Thus, we review zero-shot learning and model selection methods related to our work."}, {"title": "Zero-Shot Learning", "content": "Zero-shot (Wang et al. 2019) is a machine learning paradigm requiring the model to recognize unseen classes without additional training data. This is typically achieved by leveraging semantic information, such as attributes or textual descriptions, bridging between known and unknown classes. Traditional techniques (Palatucci et al. 2009; Lampert, Nickisch, and Harmeling 2013; Frome et al. 2013) rely on predefined attributes or textual data to map the relationship between seen and unseen classes. Recently, VLMs have demonstrated impressive zero-shot capabilities by training on vast text-image paired data to align texts and images in a common embedding space. For instance, CLIP (Radford et al. 2021) employs a vision and a language model to learn joint embeddings of images and text through contrastive learning with extensive paired data, ALIGN (Jia et al. 2021) trains with a contrastive loss on a large dataset of noisy image-text pairs, FLAVA (Singh et al. 2022) learns from both paired and unpaired images and texts using various losses. Given a new image, the model classifies the image by matching it with a text description such as \u201ca photo of [class]\u201d and selecting the most related classes in the image-text embedding space."}, {"title": "Model Selection", "content": "In an era rich with various pre-trained models and model hubs, many practitioners are trying to exploit the enormous pre-trained models to help solve AI tasks (Han et al. 2021; Renggli et al. 2022). However, how to select the appropriate model for a new task relied on experience and brute-force fine-tuning of various candidate models to identify the best one. Various reusability assessment methods have been proposed to enhance the efficiency of model selection, such as NCE (Tran, Nguyen, and Hassner 2019), LEEP (Nguyen et al. 2020), and LogME (You et al. 2021). These methods assess the compatibility between the target task data and features generated by candidate models and then select models that are more likely to achieve better reuse performance on the target task. Although more efficient than brute-force fine-tuning, these approaches still require forward inference on the target task for all candidate models. Emerging paradigms, such as learnware (Zhou 2016; Guo et al. 2023; Tan et al. 2024), explore more effective model selection. In this paradigm, numerous models are organized within a learnware docker, each accompanied by a training data distribution-driven specification describing its functionality (Wu et al. 2021). Users can search for suitable models based on specifications and their data distribution. Although these works have attempted to build model hubs to organize models, they have not enabled the model hub to have zero-shot classification capabilities."}, {"title": "Problem and Analysis", "content": "This section first outlines the notions and problem formulation for model label learning and then analyzes the three key steps in the proposal."}, {"title": "Problem Formulation", "content": "We focus on solving zero-shot multi-class classification tasks using expert models. We are provided a model hub $\\mathcal{M} = \\{F_m\\}_{m=1}^M$ containing $M$ expert models, each trained well on a specific domain. We denote $H_m$ as the number of output dimension for each model $F_m$, such that $F_m(x) \\in \\mathbb{R}^{H_m}$. As $H_m$ varies across models, their output spaces are heterogeneous. Additionally, each expert model is uploaded anonymously, and we have no knowledge regarding its training data distribution, architectures, functionalities, or output space semantics. The target zero-shot classification task involves an input space $\\mathcal{X}$ and candidate task classes $\\mathcal{Y} = \\{y\\}$, where each $y$ represents the textual description of a class. In the zero-shot setting, no training data is available. We aim to select and reuse some useful expert models from the hub to achieve zero-shot classification for the test data.\nEnabling expert models for zero-shot tasks via MLL has three key steps: model labelling, model selection, and model reuse. Labelling involves assigning functionality labels to models, while selection entails assessing task-specific reuse scores based on model labels and identifying capable experts. The model reuse step requires the ensemble of selected models to make predictions in a zero-shot manner."}, {"title": "Analysis of Model Labelling", "content": "Just as a hirer's selection of candidates relies on tests and appraisals rather than just resumes, model labelling relies on pre-testing for better insight into the model's functionality. A semantic-based data structure should be constructed to pre-test expert models and assign them labels that describe how well it excels at classifying some semantic classes. Although we attempt to produce labels describing functionality, it is inaccurate to measure the model's ability to recognize certain classes using fixed reuse scores without knowing the target task in advance. For instance, a model well-trained on a dog-vs-cat dataset may be able to distinguish dogs from leopards, but it might not perform well on the dog-vs-wolf task. Task-specific reuse scores depend on both the model and the target task. Thus, the labelling method should preprocess enough functionality descriptions as model labels to facilitate the matching of candidate models and target classes and the assessment of the task-specific reuse score once the target task is given."}, {"title": "Analysis of Model Selection and Reuse", "content": "To achieve model reuse for the zero-shot prediction, the target task on $y$ is treated as $|\\mathcal{Y}|$ one-vs-rest classification tasks. We have to select one-vs-rest expert predictors and generate task-specific reuse scores to identify useful ones. Although a model with $H_m$ classification heads can be intuitively viewed as $H_m$ one-vs-rest predictors, constructing each binary predictor using only one head does not work that well. In practical reuse scenarios, the heterogeneous relationship between the target class and the model output head is typically complex rather than simple one-to-one mappings. Combining the model's multiple classification heads to form a predictor for collective recognition yields better performance. However, searching through various possible combinations of heads poses a challenge."}, {"title": "Methodology", "content": "We make a preliminary attempt to implement the MLL paradigm with three key steps: model labelling, model selection, and model reuse. We first outline the overall framework and then delve into the details of each step."}, {"title": "Overall Framework", "content": "As illustrated in Figure 2, the MLL system comprises a Semantic Directed Acyclic Graph (SDAG) and a model hub. During the labelling step, models submitted to the hub are pre-tested and assigned labels $S_m$ describing their functionalities using the SDAG, where each node describes a semantic class with corresponding representative data. In the selection step, the text classes of the target task $\\mathcal{Y}$ are mapped into the target classes $\\mathcal{C_y}$ by textual semantic matching with the SDAG. We then propose the Classification Heads Combination Optimization algorithm (CHCO), utilizing model labels $S_m$ to generate task-specific reuse scores and select one-vs-rest expert predictors. Given a specified reuse budget $k$, we assemble up to $k$ predictors for each class. Larger reuse budgets intuitively lead to better predictions but increase costs. These $|\\mathcal{Y}|$ ensemble expert predictors $\\{F_c\\}$ are employed in the reuse step to handle the target task in a zero-shot manner."}, {"title": "Step I. Model Labelling", "content": "We build a preliminary Semantic Directed Acyclic Graph, (SDAG), containing semantic class nodes $\\mathcal{C} = \\{c\\}$ and the corresponding representative data $\\mathcal{D}_c = \\{x\\}$. By leveraging the semantic alignment between images in ImageNet (Deng et al. 2009) and WordNet (Miller 1995) textual synsets, each node $c$ is associated with a WordNet noun synset, with a few images randomly sampled from ImageNet as $\\mathcal{D}_c$ to represent the actual image data distribution of this semantics. However, a few sampled images are insufficient to capture the full distribution for nodes with broader semantics such"}, {"title": "Step II. Model Selection", "content": "For model selection, we first map all text classes of target task $y$ to target classes $\\mathcal{C_y}$ through semantic matching with nodes in the SDAG. The matching process is performed using the cosine similarity of text embeddings generated by a pre-trained language model. We then propose the Classification Head Combination Optimization (CHCO) algorithm to select useful expert predictors for each class in $\\mathcal{C_y}$.\nCHCO Algorithm. To simplify the notation, we omit the subscript m in the following discussion. Consider a model has $|H|$ classification heads, denoted as $\\mathcal{H} = \\{h\\}$. For the target classes $\\mathcal{C_y} = \\{c\\}$, we aim to construct $|\\mathcal{C_y}|$ binary predictors from the model's classification heads to optimize the expected performance over $\\mathcal{C_y}$. We focus on the subset $S_y = \\{s_c^h | c \\in \\mathcal{C_y}\\}$ of the model label $S$, where $s_c^h = \\{s_c^h\\}$ represents the average logit output by classification head h for class c. The head-class probability matrix $P = (P_{hc}) \\in \\mathbb{R}^{|\\mathcal{H}| \\times |\\mathcal{C_y}|}$ is compute based on $s_c^h$:\n$P_{hc} = \\frac{\\exp(s_c^h)}{\\Sigma_{h'} \\exp(s_{h'}^c)} \\qquad(4)$\nA binary predictor $F_c$ for class $c$ can be represented as a linear combination of multiple heads. We define the head combination variable $X = (x_{hc}) \\in \\mathbb{R}^{|H| \\times |\\mathcal{Y}|}$, where $x_{hc}$ represents the proportion of head $h$ used for predicting class $c$. $X$ should satisfy the following constraints:\n$\\sum_{c \\in \\mathcal{C_y}} x_{hc} \\leq 1, \\forall h \\in H \\qquad(5)$\n$0 \\leq x_{hc} \\leq 1, \\forall h \\in H, c \\in \\mathcal{C_y} \\qquad(6)$\nBased on $X$ and $P$, we could further compute the expected probability output of each binary predictor for class $c$ and other classes $c' \\in \\mathcal{C_y}/\\{c\\}$ as $p_c(c)$ and $p_c(c')$, respectively. We then derive the negative log-likelihood discriminative loss $\\mathcal{L}_c$ for each predictor $F_c$:\n$p_c(c) = \\sum_{h \\in H} P_{hc}x_{hc}, \\forall c \\in \\mathcal{C_y} \\qquad(7)$\n$p_c(c') = \\sum_{h \\in H} P_{hc}x_{hc}, \\forall c \\in \\mathcal{C_y}, c' \\in \\mathcal{C_y}/\\{c\\} \\qquad(8)$\n$\\mathcal{L}_c = - \\log(p_c(c)) + \\frac{\\nu}{1 - \\nu} \\sum_{c' \\in \\mathcal{C_y}/\\{c\\}} - \\log(1 - p_c(c')) \\qquad(9)$\nOur objective is to minimize the loss across all classes while adhering to constraints on the head combinations:\n$\\arg \\min_X \\mathcal{L} = \\arg \\min_X \\sum_c \\mathcal{L}_c \\qquad(10)$\ns.t. (4), (5), (6), (7), (8), (9)\nWe achieve this optimization by alternately applying gradient descent and simplex projection. In each iteration, we select a head $h$ and fix the combination variables $x_{h'c}$ for the remaining heads $h'$. We then take the derivative of the loss $\\mathcal{L}$ with respect to $x_{hc}$, yielding:\n$\\frac{\\partial \\mathcal{L}}{\\partial x_{hc}} = - \\frac{P_{hc}}{\\sum_{h' \\in H} P_{h'c}x_{h'c}} + \\frac{\\nu}{1 - \\nu} \\sum_{c' \\in \\mathcal{C_y}/\\{c\\}} \\frac{P_{hc}}{\\sum_{h' \\in H} P_{h'c}x_{h'c}} \\qquad(11)$"}, {"title": "Step III. Model Reuse", "content": "Consequently, the model selection step yields $|\\mathcal{Y}|$ ensemble expert predictors for recognizing the classes within $\\mathcal{C_y}$. During testing, for any zero-shot data $x \\in \\mathcal{X}$ of the user task, all ensemble expert predictors $\\{F_c\\}$ are employed in a zero-shot manner to infer the confidence $p(x)$, and the class with the highest confidence is chosen as the prediction $y'$ for $x$:\n$\\hat{y}'(x) = \\underset{y}{\\operatorname{argmax}} \\; p(\\hat{y}|x), \\qquad(12)$\nwhere\n$p(\\hat{y}|x) = \\sum_{(F_m, w_m) \\in F_k} w_m F_m(\\hat{y}|x)^T x_m \\qquad(13)$\n$\\text{where } H \\text{ denotes the probability entropy and } w_m \\text{ denotes the ensemble weights obtained by normalizing output probability entropy of each expert model within } F_k \\text{ to reduce the influence of unreliable predictions. We summarize the model selection and reuse procedure in Algorithm 1 and the CHCO algorithm in the appendix.}"}, {"title": "Interim MLL Patch towards Practice", "content": "The MLL paradigm is built upon a fully developed MLL system, where the SDAG covers the task classes, and models in the hub manage them. However, prior to reaching this advanced stage, we must contend with an underdeveloped MLL system. Therefore, We implement an interim patch using a general VLM. For task classes that the MLL system cannot match or resolve, zero-shot predictions are handled by the general VLM. As the MLL system evolves, specialized experts will eventually replace the general big model. We present the results for both full and partial coverage of the target task by the MLL system in experiments.\nProposition 4.1. Given the decomposition of target task $\\mathcal{Y} = \\mathcal{Y_e} \\cup \\mathcal{Y_g}$, where $\\mathcal{Y_e}$ and $\\mathcal{Y_g}$ represent for the classes handled by the reuse of experts and the general VLM, respectively. The expected errors of the general model, $\\mathbb{E}^g$, reusing experts through MLL, $\\mathbb{E}$ and reusing experts after enhancing MLL system, $\\tilde{\\mathbb{E}}$, satisfy that $\\tilde{\\mathbb{E}} \\leq \\mathbb{E} \\leq \\mathbb{E}^g$.\nRemark 4.1. Proposition 4.1 and the later experimental results demonstrates that, even when the target task calls for the assistance of a general VLM, the effective reuse of experts improves the performance compared to relying solely on a single general VLM. Additionally, evolving and scaling the MLL system lead to continuous improvement. The proof and more details refer to the appendix."}, {"title": "Experiments", "content": "In this section, we conducte experiments on real datasets to validate the effectiveness of reusing expert models for zero-shot classification through MLL and the reuse efficiency of our method. Additionally, we showcase the scalability of the model hub and validate the superiority of the CHCO algorithm through an ablation study. We begin with a detailed description of the experimental setup."}, {"title": "Experimental Setup", "content": "MLL Setting. We constructe the MLL system as a reproducible benchmark as follows. The SDAG includes a total"}, {"title": "The Comparison of Reuse Methods", "content": "We vary the reuse budget k from 1 to 5 to compare our proposal against other reuse methods, LogME and LEEP.\nEfficacy. Figure 3 shows that increasing reuse budgets generally boosts ensemble performance initially, but as more experts are incorporated, some less useful ones potentially lead to a decline in performance. Our proposal achieves robust and superior reuse results across different reuse budgets k by effectively selecting and reusing capable expert models for specific tasks. In contrast, candidate methods exhibit significant performance fluctuations with varying k and fail to outperform the CLIP baseline, indicating the inaccurate identification and ineffective reuse of experts.\nEfficiency. The number of reused models reflects the efficiency of reuse methods. An efficient reuse scheme should utilize as few experts as possible while maintaining high performance. However, results indicate that LEEPMLL and LogMEMLL often select more models during reuse, including some suboptimal or even useless ones. This leads to performance degradation and inefficiency. Involving unnecessary models wastes computational and time resources."}, {"title": "The Scalability of the Model Hub", "content": "We create a scenario where the model hub evolves from scratch to include all models. Figure 4 depicts the average performance across all tasks during this process. Results indicate that the reuse performance improves as the model hub scales. This suggests that a more diverse collection of expert models in the hub enhances its ability to handle a broader range of tasks. Our proposal with the patch starts with the CLIP baseline and steadily improves, achieving effective reuse of the expert models. When the hub is small and weak, we adaptively rely more on the general model to prevent performance degradation. As the hub grows, more experts are utilized to handle tasks better and enhance the zero-shot performance, demonstrating the feasibility of our proposal. In contrast, LEEP and LogME are more likely to reuse some ineffective experts, especially when the model hub is small. Worse, their underutilization of the model hub prevents them from surpassing the CLIP baseline"}, {"title": "Ablation Stduy of CHCO", "content": "We conducte an ablation study by replacing CHCO with a heuristic method (HEU) that selects one classification head with the minimum BCE loss as the expert predictor and corresponding reuse score for each class. To illustrate this more"}, {"title": "Interim MLL Patch towards Practice", "content": "intuitively, we design a task called DvC, which merges the fine-grained classes in Oxford-Pets into two superclasses, dogs and cats. In a case study with MobileNet pre-trained on ImageNet-1k, downloaded from Huggingface, as shown in Table 2, HEU selects two predictors for the two classes using only two classification heads and achieves 90.93% accuracy on DvC. In contrast, CHCO selects two predictors by combining 61 useful heads through minimizing the discriminative loss, achieving 98.34% accuracy, significantly outperforming HEU. We also report the comparison of reuse performance across other tasks.\nDespite MobileNet not being specifically trained to categorize cats and dogs, effective zero-shot prediction on the DvC can still be achieved by reusing it through MLL. In MLL, regardless of the original training purpose, as long as the selected expert predictors can discriminate target classes, the model has the reuse value for zero-shot prediction. This indicates that expert models can be used beyond their initial submission, allowing the MLL paradigm to flexibly address a broader range of tasks."}, {"title": "Conclusion", "content": "In this paper, we explore the area of reusing task-specific expert models for zero-shot classification tasks. We present the Model Label Learning (MLL) paradigm to align models in the model hub with their functionalities using model labels, enabling effective model reuse for zero-shot tasks. We make a preliminary attempt to implement the MLL paradigm, consisting of three key steps: model labelling, model selection, and model reuse. This paradigm is less costly and more scalable than VLMs. Experiments validate the feasibility and effectiveness of MLL, demonstrating that expert models can be effectively selected and reused for zero-shot prediction.\nOne limitation is that the current SDAG used for labeling model is a discrete structure. Developing more flexible model labelling methods leaves for future research."}]}