{"title": "ContextDet: Temporal Action Detection with Adaptive Context Aggregation", "authors": ["Ning Wang", "Yun Xiao", "Xiaopeng Peng", "Xiaojun Chang", "Xuanhong Wang", "Dingyi Fang"], "abstract": "Temporal action detection (TAD), which locates and recognizes action segments, remains a challenging task in video understanding due to variable segment lengths and ambiguous boundaries. Existing methods treat neighboring contexts of an action segment indiscriminately, leading to imprecise boundary predictions. We introduce a single-stage ContextDet framework, which makes use of large-kernel convolutions in TAD for the first time. Our model features a pyramid adaptive context aggragation (ACA) architecture, capturing long context and improving action discriminability. Each ACA level consists of two novel modules. The context attention module (CAM) identifies salient contextual information, encourages context diversity, and preserves context integrity through a context gating block (CGB). The long context module (LCM) makes use of a mixture of large- and small-kernel convolutions to adaptively gather long-range context and fine-grained local features. Additionally, by varying the length of these large kernels across the ACA pyramid, our model provides lightweight yet effective context aggregation and action discrimination. We conducted extensive experiments and compared our model with a number of advanced TAD methods on six challenging TAD benchmarks: MultiThumos, Charades, FineAction, EPIC-Kitchens 100, Thumos14, and HACS, demonstrating superior accuracy at reduced inference speed.", "sections": [{"title": "I. INTRODUCTION", "content": "Temporal action detection (TAD) categorizes actions and identifies the boundaries of a video segment. TAD has been widely used in smart homes [1], vision-language grounding [2], video-based recommendation [3], multimedia retrieval [4], gaming technology [5], and more. Accurate localization of actions from videos remains challenging due to the varying lengths of action segments and the potentially ambiguous boundaries between them.\nThe advancement of deep learning has significantly improved the performance of video action detection. Contextual information, which are typically captured from frames that are adjacent to the action frame, provide relational information among frames. Capturing long-range temporal dependencies among video features can improve the performance of TAD for complicated actions. Transformers are favored in many natural language processing [6] and computer vision [7]\u2013[9] applications due to their superiority in capturing long- range dependencies through self-attention based token mixing. Transformer variants have been widely investigated for TAD tasks. ActionFormer [10] is one of the representative methods that directly employs a multi-head Transformer for one-stage anchor-free TAD. Another ADSFormer [11] makes use of a dual selective multi-head token mixer for channel selection and head selection in a pyramid structure to obtain important and discriminative features. Compare to convolutional neural network (CNN), however, several challanges remain presented in Transformer based TAD: 1) The rank loss. The self-attention provides a convex combination of the input features, which may increase the similarity and reduce the discriminability between the output features and thus affect the action de- tection accuracy negatively; 2) The quadratic computational"}, {"title": "II. RELATED WORK", "content": "Temporal action detection identifies the start and end times stamps of video segments and predicts the categories of actions. Existing TAD methods include two-stage and single- stage approaches. The two-stage approach makes initial pre- dictions based on a set of pre-generated proposals and refines the time stamps [26]. These methods focus on proposal generation. Anchor-based methods [27]\u2013[29], for example, make use of densely distributed and multiscale anchors to generate proposals. Boundary-based methods [30]\u2013[33] predict the probability of each temporal point being either a start or an end of an action. In these algorithms, proposals are formulated and matched on the basis of the probabilistic scores. They are limited by the lack of end-to-end gradient flow [34]. In contrast, single-stage methods do not require proposals, but detect action segments end-to-end. For example, TadTR [34] and ReAct [35] methods make use of a set of action queries to interact with the feature maps to detect action instances. Actionformer [10] and Tridet [13] take advantage of feature pyramid representations. Salient boundary features [36] are also explored to improve the performance of anchor-free"}, {"title": "B. Large Kernel Neural Networks", "content": "Many computer vision and multimedia tasks have benefited from the use of large window attention in Transformers as well as large convolution kernel in CNNs. The Swin Transformer [9], for example, employs 7 \u00d7 7 to 12 \u00d7 12 shifted window attention for object detection and classification. In the work of RepLKNet [17] the size of the convolutional kernel was scaled to 31\u00d731 for object detection, where large-kernel CNNS demonstrated larger effective receptive fields than deep small-"}, {"title": "C. Attention and Gating Mechanism", "content": "Machine learning and deep learning has been employed in diverse areas [42]\u2013[47], including the TAD tasks [48]. The attention mechanisms, in particular, achieved remarkable success. In addition to the variances of vision Transformers where self-attention is employed, attentions have also been ex- plored through the gating machenism in CNNs. For example, squeeze-and-excitation (SE) [49] and gather-excite (GE) [50] select salient features by squeezing spatial features into a chan- nel descriptor and exited that descriptor. The convolutional block attention module (CBAM) [51] uses reweighed channels and spatial positions to adaptively modulate the feature map, achieving both channel and spatial attention. The local-relation net (LR-Net) [52] adaptively determines feature aggregation weights based on the local pixel pairs. Gated feature selections have also been investigated for capturing context information and action recognition [53]. For example, the CondConv [54] and the dynamic convolution [55] methods utilize multiple parallel convolution kernels to adaptively extract features. To capture the salient context while preserving its integrity, we present a context attention module (CAM) which fuse the CNN features with gated attention features at varying scales. Compared to channel grouping [56], our model provides more accurate discrimination of features on different scales, allowing capturing diverse contextual information."}, {"title": "III. CONTEXTDET MODEL", "content": "As illustrated in Fig. 2(a), the proposed ContextDet model consists of four modules: a pre-trained video feature extrac- tion backbone FE, a convolution projection layer FP, the multistage ACA module, and a pre-trained convolution-based detection head. The pre-trained video model (e.g., I3D [57], VideoMAEv2 [58], etc.) extracts video features. Following that a projection layer embeds these features. The embedded features are then further fed into the multi-level ACA pyramid. Each ACA level is composed of a context attention module (CAM) and a long context module (LCM). In the CAM, we in- troduces a context gating block block to replace self-attention and capture the salient context. In the LCM, a mixture of large- and small-kernal convolution are employed to identify long context information without losing fine-grained local attention. The multi-scale ACA features are passed to a pre-trained detection heads for action detection, which typically consists of a pair of decoupled classification and regression heads."}, {"title": "A. Model Architecture", "content": "Feature Extraction and Projection Given an untrimmed video having T frames $V\\in \\mathbb{R}^{C\\times H\\times W\\times T}$, each frame has a height H, width W, and number of channels C. The proposed ContextDet model detects a set of U actions $\\Psi = {\\psi_u|u = 1, ..., U}$. Each action is denoted as $\\psi_u = (s_u, c_u, e_u)$, where $s_u$ and $e_u$ are repectively the start and end point of the action ($s_u < e_u$), and $C_u$ is an action from a total number of U action categories. Temporal features are extracted by the extraction backbone FE and projected by the projection layer FP. The projection layer consists of two convolutional layers that are activated by the Relu function. The projected input feature $F_o \\in \\mathbb{R}^{T_0\\times D}$ is given by:\n$F_o = FP(FE(V))$                                                                                                                                                                   (1)\nAdaptive Context Aggregation. To capture a diverse range of relevant context for temporal action detection, we introduce in our ContextDet consists of five Adaptive Context Aggrega- tion (ACA) stagets i = 1, ..., 5. Each ACA level is composed of a downsampling (DS) layer, a context attention module (CAM), a long context module (LCM), an MLP layer, two LayerNorm (LN), and two skip connections. Denoting the input and output of each stage as $F_{i-1}$ and $F_i$ respectively, each of the ACA levels is given by:\n$F_{i-1} = DS(F_{i-1})$\n$F_i = LN(F_{i-1})$\n$R_i = CAM(F_i) + LCM(F_i) + F_{i-1}\\downarrow$\n$F_i = MLP(LN(R_i)) + R_i$                                                                                                                            (2)\nDenoting $T_i$ and D are the number of temporal features and channel dimension respectively, at each stage $F_{i-1} \\in \\mathbb{R}^{T_{i-1}\\times D}$ is the input feature to the each ACA level and it is downsampled by a factor of two as $F_{i-1}\\downarrow \\in \\mathbb{R}^{T_i\\times D}$, where $T_i = T_{i-1}/2$\nContext Attention Module. To extract the most relevant temporal context for action detection, a temporal context attention module (CAM) is introduced (see Fig. 2(b)). In this module, context attention is calculated from the input video feature $A_i = CAM(F_i)$. Each CAM consists of two branches: a K-branch and a Q-branch. The K-branch extracts action features $K_i$ through a linear layer $L_K$:\n$K_i = L_KF_i$                                                                                                                                                                      (3)\nIn the Q-branch, the video feature passes through a linear layer $L_Q$ producing an output $Q_i$:\n$Q_i = L_QF_i$                                                                                                                                                                     (4)\nThese Q-features are then fed into the context gating block (CGB) to calculate the gated attention $G_i = CGB(Q_i)$, the detail of which is described below. Each CGB block extracts multiscale features $z_{i,m}$ using a set of M multiscale con- volution kernels. These multiscale features are concatenated channel wise as:\n$z_{i,m} = GeLU(DWConv_m (Q_i))$\n$Z_i = Concat({z_{i,m}})$                                                                                                                                                     (5)\nwhere m = 1,..., M and each of these depth-wise convolu- tional kernels has a distinct size. The max and average pooling"}, {"title": "Action Detection", "content": "The gated multi-scale temporal attention feature are given by:\n$G_i = \\sum_{m=1}^{M} z_{i,m} W_{i,m}$                                                                                                                                                                       (8)\nwhere represents the element-wise multiplication. The out- put of CAM is given by K-features $K_i$ modulated the gated attention $G_i$ as:\n$A_i = G_i K_i$                                                                                                                                                                       (9)\nwhere $G_i$ changes adaptively with respect to different inputs, and thus capturing context in a dynamic manner.\nLong Context Module. To capture long-range context without losing local details, we make use of a mixture of large- and small kenerl convolutions in a long context module (LCM) as shown in Fig. 2(c). In order to capture the long context, we employ 1D large-kernel convolutions to expand the receptive field along the temporal direction. However, merely enlarging the convolution kernel leads to an only slight improvement in the detection performance of our model during the experiment process. While increasing the size of convolution kernel may increase the receptive field thus perception of longer context, an architecture with large-kernel convolution along may not be able to pay attention to fine-grained local features. To solve this issue, we introduce the parallel use of three smaller- kernel 1D convolutions as complementary to the large-kernel convolutions. Each of these small kernels has a length smaller than three. By fusing the results of a mixture of large- and small-kernel convolutions, we are able to capture long-term context and fine-grained local feature at the same time. Each LCM at level i is defined as $D_i = LCM(F_i)$, the details of which are written as:\n$D_i = C_i^L + \\sum_{n=1}^{N} C_{i,n}^S$                                                                                                                                                 (10)\nwhere the large- and small convolution features are given respectively by:\n$C_i^L = GeLU(LConv_i(F_i))$\n$C_{i,n}^S = GeLU(SConv_n(F_i))$                                                                                                                                            (11)\nwhere $LConv_i$ and ${SConv_n|n = 1,.., N)}$ are respectively a large-kernal convolution and a set of N = 3 small-kernel"}, {"title": "A. Model Learning", "content": "convolutions at each layer. Here we use Gelu as activation functions for each convolution layer. Batch normalization has been employed with convolution [17]. However, the use of batch normalization is observed to reduce the performance of our model, which might be explained by 1D convolutions having fewer parameters than 2D convolution. Additionally, we vary the size of the large convolution kernel at each ACA pyramid level to improve the diversity of receptive fields and efficiency. The size of the three small convolution kernels is kept fixed cross the pyramid.\nActions are decoded from a list of feature pyramid {$F_i|i = 1,2,...5$} by a detection head. Here we make use of a pre-trained detection head [13] which consists of two disentangled heads respectively for classification and regression. The classification head predicts the probability p($c_t$) of an action $c_t$ at each time stamp t. The regression head predicts the duration of time $\\triangle t_s$ and $\\triangle t_e$ that lapses from the time stamp t to the start point $\\hat{s}_t$ and end point $\\hat{e}_t$ respectively. The predicted video segment is written as:\n$\\hat{s}_t = (\\hat{s}_t, \\hat{e}_t, \\hat{c}_t)$,\nwhere\n$\\hat{s}_t = 2^{i-1} \\times (t - \\triangle t_s)$,\n$\\hat{e}_t = 2^{i-1} \\times (t + \\triangle t_e)$,\n$\\hat{c}_t = arg max p(c_t)$                                                                                                                                                    (12)\n(13)\nThe model predicts the probability $p (c_t)$ for each action category, as well as the time lapses $\\triangle t_s$ and $\\triangle t_e$ from the current time t to the action boundary. The loss function consists of a focal loss $L_{cls}$ [66] for classification and an IoU loss $L_{reg}$ [67] for regression. The total $L_{total}$ loss is given by:\n$L_{total} = \\frac{1}{N_{pos}}\\sum_{t} I_{c_t>0} \\cdot (\\sigma_{tIoU}\\cdot L_{cls} + \\lambda \\cdot L_{reg}) + \\frac{1}{N_{neg}}\\sum_{t} I_{c_t=0} \\cdot L_{cls}$                                                   (14)\nwhere $N_{pos}$ and $N_{neg}$ are the number of positive and negative samples respectively. $I_{c_t>0}$ and $I_{c_t=0}$ denote respectively the time stamp of an action $c_t$ and its background. $\\sigma_{tIoU}$ is the temporal IoU between ground truth and predicted segment, and $\\lambda$ is a coefficient that modulates the regression loss."}, {"title": "IV. EXPERIMENTS", "content": "We conducted evaluation of the proposed ContextDet model on six challenging datasets: MultiThumos [20], Charades [21], FineAction [22], EPIC-Kitchens 100 [23], Thumos14 [24], and HACS [25]. The MultiThumos and Charades are two densely multi-label TAD datasets, where the MultiThumos dataset includes 38,690 annotations for 65 types of sports action. The Charades dataset is a large-scale densely annotated multi-label dataset, including 9848 videos across 157 action categories. The FineAction is a fine-grained multi-label video dataset,"}, {"title": "V. RESULTS", "content": "MultiThumos and Charades. We compare our ContextDet model with a number of advanced TAD methods on these two datasets in terms of detection mAPs. As shown in Table I, our model achieves the highest accuracy at all tIoU thresholds. In particular, our model provides an average-mAP of 44.6% and 42.5% on MultiThumos for VideoMAEv2 [58] and I3D [57] features respectively, indicating an improvement of respective 7.1% and 6.3% in accuracies compared to the second-best TriDet [13] model. We also achieve an average-mAP of 20.3% on the Charades dataset using only the RGB features extracted by the I3D backbone, showing an improved accuracy of 1.9% compared to the second-best TriDet. These two datasets fea- ture a strong sequential correlation among the dense actions, which affirms our model's capability in adaptively aggregating contextual information for action understanding."}, {"title": "VI. ABLATION STUDY", "content": "To evaluate the architecture design and learning strategies of the proposed ContextDet model, three ablation studies are conducted on the Thumos14 dataset [24]."}, {"title": "VII. ERROR ANALYSIS", "content": "A video clip is typically characterized by its length, cov- erage, and the number of instances. The length indicates the duration of a video in seconds. The coverage represents the length of an action instance normalized by the length of an entire video. The number of instances indicates the total number of actions of the same category in a video. Qualitative results of four Thumos14 video clips are shown in Fig. 4(a)-(d) for twenty predicted segments of the top scores. The scores are determined by the maximum tIoU between the real and predicted actions. Compared to the ground truth (red), our CondextDet model (blue) produces boundary detection with minimal discrepancies. The quantitative diagnostic analysis [90] of sensitivity, false positives, and false negatives are provided for Thumos14 video clips, each having a different length. Here we divide the video into five sets according to its coverage and length, respectively: extra short (XS), short (S), medium (M), long (L), and extra long (XL). We also divide the videos into four intervals based on the number of instances as: extra small (XS), small (S), medium (M), and large (L). Sensitivity. As shown in Fig. 5, our method outperforms the baseline [36] by 5.7% in terms of the average mAPN of the coverage, length, and instance measures (see the dotted lines"}, {"title": "VIII. CONCLUSION", "content": "In this work, we introduced a single-stage ContextDet model for temporal action detection based on a dynamically gated pyramid convolution neural network. Our model makes use of large-kernel convolutions in TAD for the first time to increase receptive field and capture long context. Through the combined use of max and average pooling, a mixture"}]}