{"title": "AGENTGEN: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation", "authors": ["Mengkang Hu", "Pu Zhao", "Can Xu", "Qingfeng Sun", "Jianguang Lou", "Qingwei Lin", "Ping Luo", "Saravan Rajmohan", "Dongmei Zhang"], "abstract": "Large Language Model (LLM) based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, involving interaction with the environment and executing actions to complete a planning task, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLM-based agents through instruction tuning, referred to as agent training. Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuning LLMS effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories for agent training. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. In response, we introduce a framework, AGENTGEN, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, BI-EVOL, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve, thereby enhancing the learning process of LLMs more effectively. These methods collectively contribute to the generation of diverse trajectory data for instruction-tuning. Based on AGENTGEN, we greatly expanded the number of environments and planning tasks available for agent training. The evaluation results derived from AgentBoard show that AGENTGEN greatly improves LLMs' planning ability, e.g., the AGENTGEN instruction-tuned Llama-3 8B surpasses GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms GPT-4.", "sections": [{"title": "1 Introduction", "content": "Recently, owing to advancements in Large Language Models (LLMs) [40, 41, 37, 60], the LLM-based Agents have garnered widespread attention from the artificial intelligence community. Generally, an LLM-based agent refers to utilizing LLMs to perceive the environment, make decisions, and execute actions to substitute or help people accomplish some specific tasks [75, 63, 77]. Furthermore, planning is often regarded as one of the most important applications of LLM-based agents, such as robotic planning [52, 44, 17, 61], travel planning [88, 76], etc. In this study, planning is conceptualized as the systematic process of identifying a sequence of executable actions within a given environment to complete a planning task, defined as the transition from an initial state to achieve specified goal conditions, considering constraints and available resources [23, 48].\nImproving planning capabilities through instruction-tuning LLMs is a significant research problem, referred to as agent training. As shown in Figure 1, similar to imitation learning [21], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks. (ii) Synthesizing expert-level trajectories (sequences of action-observation pairs) on these planning tasks. For example, utilizing state-of-the-art LLMs (e.g., GPT-4 [41]) as the agent and filtering trajectory based on reward score [84, 6]. (iii) Instruction-tuning LLMs with the synthesized trajectory data. Recently, the effectiveness of enhancing the planning capabilities of LLMs through agent training has been demonstrated by many studies [84, 83, 6, 66, 8, 85, 62, 55]. Despite their success, one key limitation of these works is that they primarily rely on manually designed environments and planning tasks. The labor-intensive nature of creating environments and planning tasks hinders the generation of diverse and extensive trajectory data. More explicitly, designing diverse environments requires defining a range of rich and practical scenarios, and implementing these environments typically involves the participation of human experts with programming skills. Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression. Due to this constraint, existing agent training studies typically use only a few environments for data synthesis."}, {"title": "2 Preliminary", "content": "We consider goal-directed deterministic planning problems [48], which are formally defined as a tuple $P = (T, E)$, where $E$ denotes the environment in which the agent interacts and $T$ denotes the task that the agent needs to complete. Specifically, an environment $E$ typically models a world, encompassing the definitions of the action space $A$ and state space $S$, as well as the transition function $T: S \\times A \\rightarrow S$. Task $T$ is further defined by the tuple $T = (G, I)$, where $G$ refers to the goal conditions and $I$ refers to initial states of the agent. The initial states $I$ are a subset of the state space $S_i$ that specifies the starting conditions of the agent. The goal $G$ is a subset of the state space $S_g$ that specifies the desired outcomes or conditions. Specifically, $G$ can be expressed as $G = \\{s \\in S_g | \\phi(s) = true\\}$. Here, $\\phi(s)$ is a boolean-valued function representing conditions or propositions that must be satisfied for the state s to be considered part of the goal set."}, {"title": "2.2 Planning Problem Implementation", "content": "A planning problem can be implemented with programming languages such as Python or domain-specific languages such as Planning Domain Definition Language (PDDL) [36]. For example, in a PDDL-based planning problem, the domain PDDL file can be regarded as the environment E, defining states (predicates) and actions and specifying the transition function using preconditions and effects of each action. The problem PDDL file, on the other hand, can be seen as the task T. Both initial states and goal conditions are typically defined as combinations of predicates. Another widely used programming language for constructing planning problems is Python. For example, in OpenAI"}, {"title": "2.3 Large Language Model based Agent", "content": "An LLM-based agent leverages a pre-trained language model to operate within the defined environment E and complete the given task T. Given an environment E, the LLM-based agent perceives its state S and takes actions A based on its understanding and processing of the input. The transition function $T: S \\times A \\rightarrow S$ remains consistent, where the LLM-based agent determines the next state by generating appropriate actions through natural language processing. The goal G guides the LLM-based agent in selecting actions that maximize the reward. The agent utilizes the language model to interpret the task requirements and generate actions that align with achieving the specified goal. In essence, the LLM-based agent forms a policy $\\pi : S \\rightarrow A$ using the LLM, where $\\pi(s)$ is the action taken in state s based on the LLM's understanding and processing of the task."}, {"title": "3 Methodology", "content": "Problem Definition The process of generating planning tasks can be formalized as a function $f: I \\rightarrow (T, E)$, where $I$ is the input space (e.g., instructions or prompts) and tuple $(T, E)$ is the space of all possible planning tasks and environments. Based on the definition in Section 2.1, we can express this as $f(i) = (T_i, E_i), i\\in I$, where $T_i$ is the generated planning task and $E_i$ is the generated environment for a given input $i$. Our two-stage approach can be further decomposed as follows: i) Environment Generation (\u00a73.1): In the first stage, we generate the environment $E_i$ based on the input instruction $i$. This can be represented as $E_i = g_E(i)$, where $g_E$ is the environment generation function that takes the instruction $i$ as input and produces the environment $E_i$. ii) Task Generation (\u00a73.2): In the second stage, we generate the task $T_i$, conditioned on the environment $E_i$ generated in the first stage. This can be expressed as: $T_i = g_T(i, E_i)$, where $g_T$ is the task generation function that takes both the original instruction $i$ and the generated environment $E_i$ as inputs to produce the task $T_i$. We will detail the implementation of these two stages in the following section."}, {"title": "3.1 Environment Generation", "content": "Overview As is shown in Figure 2, we propose a sophisticated framework for environment generation structured around three main components: (1) an environment specification generation module where an LLM first generates a specification of the environment, typically including a general overview of the environment, descriptions of the state space and action space, and definitions of the transition functions; (2) an environment implementation module that generates corresponding code based on the environment specification; and (3) an environment library that stores previously generated high-quality environments, serving as a comprehensive environment dataset and providing in-context examples for generating new environments. Each component will be elaborated on in the following paragraph.\nEnvironment Specification We initially prompt the LLM to generate an environment specification, which typically includes an overall depiction of the environment, specific actions and their corresponding preconditions and effects, and certain restrictions within the environment. The environment specification will serve as the basis for generating specific environment codes. This two-stage approach, similar to the Chain-of-Thought [72], can better assist the LLM in creating high-quality environments. For generating environment specifications, One direct approach is to prompt LLMs to generate random environments. However, due to the inherent inductive bias of LLMs, they struggle to generate diverse environments in this way. Therefore, to address this issue, we build an inspiration corpus $D = \\{t_0,t_1, \\cdot, t_n\\}$, containing sufficiently diverse text segments used to serve as the \"inspiration\" for generating environment specification with LLMs. More specifically, when generating an environment, we first sample a text segment $t_i$ from $D$, then prompt the LLM to generate a related environment based on $t_i$. Taking the example in Figure 2, we first sample a text segment \"How to boost your diet with peanut butter powder?\" from $D$. Then we prompt an LLM to generate a related environment where the agent is defined as a nutritionist tasked with creating a new healthy recipe book that prominently features peanut butter powder as a key ingredient. This approach significantly enhances the diversity of generated environments, thereby empowering more generalized agent training. The inspiration corpus can be implemented in various ways, such as using a large-scale pre-trained corpus like Common Crawl. Alternatively, a domain-specific corpus, such as a code generation dataset [25, 7], can be used to generate environments for a specific domain. This paper uses LIMA [91] as the inspiration corpus, an instruction-tuning dataset with sufficient diversity.\nEnvironment Implementation Conditioned on the generated environment specification, we generate its corresponding code, i.e., implementing the environment. This can be formulated as a typical code-generation problem with LLMs. We also introduce a validation tool capable of capturing syntax errors to provide feedback during the code generation process, thereby iteratively refining it.\nEnvironment Library We define the library at iteration t as: $L_t = L_0 \\cup \\cup_{k=1}^{t}\\{E_i|E_i = g_E(i, L_{k-1}), i \\in I_k, v(E_i) = true\\}$, where $L_0$ is the initial seed library, and the union represents all verified environments generated up to iteration t. This iterative process allows continuous expansion and refinement of the environment library, potentially leading to increasingly complex and diverse environments over time."}, {"title": "3.2 Task Generation", "content": "Overview As depicted in Figure 3, conditioned on the generated environments, we prompt LLMs to generate corresponding planning tasks. We employ a two-stage generation approach BI-EVOL for creating a diverse range of planning tasks in terms of difficulty. We begin by prompting the LLM with a specific environment, enabling it to generate an initial set of planning tasks in a zero-shot way. Subsequently, we adjust these tasks to make them simpler or more challenging, forming a comprehensive set of planning tasks.\nBidirectional Evolution Many studies have proposed evolving instructions, primarily focusing on making instructions more difficult [78, 34, 33]. The effectiveness of this approach relies heavily on the assumption that LLMs inherently possess the ability to follow simple instructions. However, according to findings from some studies [35, 31], LLMs often exhibit poor performance even in simple planning tasks. Therefore, we propose BI-EVOL, which introduces evolution in two directions: easy-evol and hard-evol. Easy-evol typically involves simplifying the goal conditions. The motivation is that easier tasks can facilitate learning when the agent performs poorly and cannot directly learn from typically difficult goals. Conversely, hard-evol usually involves making the goal conditions more complex, increasing the number of steps required for the agent to complete the task. This can further enhance the agent's capability to perform the planning task. To our knowledge, we are the first to introduce bidirectional evolution in the agent data generation scenario. The prompt examples are shown in Figure 3."}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of the proposed framework, we synthesize environments and planning tasks using the Planning Domain Definition Language (PDDL), a widely adopted programming language for planning. Subsequently, we evaluate its performance across various unseen planning tasks in a zero-shot manner. To validate the effectiveness and generalizability of AGENTGEN, we categorized the evaluated tasks into two distinct groups: i) In-Domain Tasks: Planning tasks implemented using PDDL. ii) Out-of-Domain Tasks: These comprise tasks developed using other programming languages, such as Python."}, {"title": "4.1 Experimental Setup", "content": "Evaluation Tasks For In-Domain Tasks, we select four widely used PDDL-based planning tasks: Blocksworld, Gripper, Tyreworld, and Barman [35]. More explicitly, Blocksworld requires an agent to achieve a target configuration by moving blocks, while Gripper involves moving objects between different rooms. Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire. Barman emulates a bartender's tasks in mixing cocktails, which include combining various ingredients, using shakers, and garnishing drinks. For Out-of-Domain Tasks, we select two challenging partial-observable planning tasks: Alfworld [52] and BabyAI [10]. Alfworld is an environment designed to test agents' abilities to perform everyday household tasks. While in BabyAI, the agent interprets and executes natural language instructions in a grid-world setting.\nEvaluation Metrics We utilized two evaluation metrics to evaluate planning ability: success rate and progress rate [35]. During each interaction round, we assigned a progress rate, denoted as $r_t$, to measure the progression towards the goal state $g$. As the agent transitions through states $S_t = [s_0,..., s_t]$, its progress is assessed using a matching score $f(\\cdot, g) \\rightarrow [0, 1]$, which quantifies the similarity between the current state and the goal state. Initially, $r_t$ is set to 0, indicating no progress. Only when the progress rate reaches 1 does the success rate attain 1; all other scenarios yield a 0 outcome. The success rate reflects the agent's capacity to complete a comprehensive task."}, {"title": "4.2 Evaluation on In-Domain Tasks", "content": "Despite its relatively small size, AGENTGEN overall outperforms GPT-3.5 in success rate (11.67 vs. 5.0). Furthermore, in the barman task, AGENTGEN even surpassed GPT-4's performance (15 vs. 10). AGENTGEN also achieved a comparable level to GPT-4 in tyreworld. Compared to other models with similar parameter scales, AGENTGEN consistently outperforms them across four distinct tasks. Compared to Llama3, our model shows an overall success rate and progress rate improvement of 10 and 9.95, respectively. Notably, in several tasks where the success rate of Llama3 is zero (gripper, blockworld, tyreworld), AGENTGEN achieves significant breakthroughs, further demonstrating the effectiveness of the dataset. We can draw several conclusions from previous discussions: i) AGENTGEN surpasses GPT-3.5 in overall performance and achieves results that either exceed or are comparable to GPT-4 on certain specific tasks; ii) AGENTGEN fine-tuned Llama3 has achieved a significant improvement in success rate; iii) AGENTGEN consistently surpasses other models with similar parameter scales in performance."}, {"title": "4.3 Robustness", "content": "To validate the robustness of the constructed dataset with AGENTGEN, we conducted a series of experiments to evaluate its performance across different foundation models. We selected several widely used 7-8B foundation models, including Llama3-8B, CodeLLama-7B, and Mistral-7B, to test the versatility and effectiveness of AGENTGEN. As is shown in Table 2, all three models exhibited significant improvements after training, with Llama3-8B showing the highest success rate increase of 10.0 and CodeLLama-7B demonstrating a maximum progress rate increase of 9.9. These experimental results prove that the dataset constructed with AGENTGEN for agent training is highly effective across different models."}, {"title": "4.4 Evaluation on Out-of-Domain Tasks", "content": "We also conducted evaluations on out-of-domain agent tasks. As illustrated in Table 3, similar experimental phenomena were observed. Firstly, AGENTGEN achieves significant performance improvement over Llama3, increasing to 29.1 success rate and 36.2 progress rate on Alfworld and 4.4 success rate and 4.2 progress rate on BabyAI. Besides, our model achieves superior performance on Alfworld that surpasses GPT-3.5 (29.1 vs. 17.2). Compared to general models and agent fine-tuning models with similar parameter scales, AGENTGEN exhibits superior performance on both tasks. The superior performance on out-of-domain tasks further highlights the effectiveness and generalization capability of our data synthesis methods."}, {"title": "5 Related Work", "content": "Large Language Model based Agent. Large Language Models have demonstrated exceptional reasoning capabilities [60, 37, 40, 41, 22]. Owing to such abilities, over the past two years, LLM-based agents have experienced significant development [51, 73, 15, 56, 63, 75]. Unlike the traditional method of using LLMs for text-based reasoning, such as Chain-of-Thought [72], LLM-based agents typically involve interaction with the environment, adjusting the output in a closed-loop manner based on environmental information. These LLM-based agents, now fortified with capabilities like Memorizing [89, 30, 27, 82, 51, 86, 93, 59, 20], Tool-use [9, 43, 50, 26, 49, 45], and Planning [12, 5, 39, 38, 47, 2], exhibit a marked enhancement in their overall efficacy. Although this paper mainly focuses on the planning capability of LLM-based agents, we believe AGENTGEN has the potential to generalize to other scenarios of LLM-based agents.\nPlanning with Large Language Models. Planning is one of the key applications of LLM-based agents, applicable in various scenarios such as robotic planning [52, 44, 17, 61, 11, 74, 29, 13], travel planning [76, 1], calendar scheduling [88], code generation [4] and others [70]. It is typically defined as the process of systematically determining a sequence of actions or steps required to achieve a desired goal from an initial state, considering constraints and available resources. This definition primarily differentiates from studies that utilize LLMs to generate ungrounded plans as guidance for problem-solving [92, 64], rather than directly producing executable actions. Planning can be categorized into two types: open-loop planning, where the LLM outputs an entire action sequence before execution [17, 61], and closed-loop planning, where the LLM-based agent decides the next action based on real-time environmental interaction after executing a previous action [53, 5, 57, 58, 28, 54, 19, 18]. This paper mainly focuses on close-loop planning, which is more adaptable for error correction, human interaction, and environmental grounding. Recent studies on close-loop planning have integrated chain-of-thought reasoning into the planning process [80]. Additionally, some papers have explored the use of tree-search methods to enhance the performance of LLM planning [16, 14, 81, 32, 87, 68, 90]. Instead of designing novel frameworks or engaging in prompt engineering, this paper explores how training can enhance the planning capabilities of LLM-based agents.\nAgent Training. Recently, numerous studies have aimed to enhance LLM-based agent capabilities by incorporating agent trajectory data into their training [66, 8, 85, 62, 55]. Advanced works such as AgentTuning [84] utilize GPT-4 to generate trajectory data across six distinct environments. Subsequently, this data is filtered and employed in training Large Language Models, enhancing the agent capabilities of base models. Another work, FireAct [6], proposes training with both CoT data and ReAct format data, enabling the model to discern when to use reasoning to solve problems and when to call external tools. Agent LUMOS [83] suggests separately training Planning and Grounding models, enabling LLM-based agents to learn to decompose complex problems before execution. LLM-Modulo framwork [24] proposes to leverage LLMs generating candidate plans and verify them with an external verifier. Then, use the verified trajectories for fine-tuning LLMs. Similarly, [3] takes a generate-test loop to synthesize trajectories for LLM training. Unlike previous papers on all agent training, AGENTGEN goes beyond merely generating trajectory data using Large Language Models. Instead, we utilize Large Language Models to generate agent environments, which can be considered a more foundational application. As a result, we have constructed over 500 environments for training, whereas previous works typically use fewer than 10 environments to synthesize agent data.\nEnvironment and Task Generation with Large Language Models. The utilization of LLMs to generate environments and tasks is an emerging application. Some studies have explored utilizing LLMs to generate layouts in robotic simulations, typically involving the creation of configuration files [69, 79, 65]. While these methods can construct numerous scene-level environments, they often struggle to achieve diversity at the underlying mechanism level. Agenttuning [84] employs a task generation approach similar to the Self-instruct [71] method, using the test set as seed data. This approach not only poses a risk of data leakage but also leads to insufficient diversity in task difficulty. ByteSized32 [67] uses LLMs to generate Python-based games based on predefined task specifications automatically. Similarly, other works [13] leverage LLMs to automatically construct PDDL domains based on a task specification. In contrast to these studies, this paper proposes using a diverse text corpus to generate environment code automatically. This approach facilitates the creation of a wide range of rich environments without predefined definitions."}, {"title": "6 Conclusion", "content": "In this paper, we explore using LLMs to automatically generate environment and planning tasks for LLM-based agent training. Specifically, for generating diverse environments, we propose utilizing an inspiration corpus composed of various domain-specific text segments as the context for environment synthesis. To enhance the difficulty diversity of generated planning tasks, we introduce a bidirectional evolution method, BI-EVOL, which evolves planning tasks from both easier and more challenging directions to create a task set with a more gradual difficulty curve, thereby improving the effectiveness of LLM learning. Based on AGENTGEN, we developed a dataset consisting of 592 environments and 7246 trajectories and trained it on a series of LLMs. The trained model outperformed GPT-3.5 across multiple tasks and, in certain specific instances, exceeded the performance of GPT-4."}, {"title": "A More Implementation Details", "content": "We applied the instruct version for models. Specifically, the detailed version for each model is presented in Table 4."}, {"title": "A.1 Models", "content": null}, {"title": "A.2 Natural Language Mapping", "content": "We leverage GPT-4 to generate the natural language mapping that converts structured actions into its natural language format. When the mapping failed to yield, we heuristically serialized the structured actions. The prompt for generating natural language mapping with GPT-4 is as follows:"}, {"title": "Natural Language Mapping Generation", "content": "I would like you to create natural language mapping for PDDL.\nThe form of the natural language mapping is a Python dictionary, wherein\n1. The key corresponds to the name of a predicate or action within the domain PDDL.\n2. The value is its equivalent in natural language, with parameters presented in \"{argn}\", where n is the index of its parameters in the PDDL expression.\n3. You must ensure that the number of \"{}\" corresponds precisely to the number of parameters in predicates or actions.\n4. You should very carefully check the order of {argn}.\nYour output must strictly follow the provided example.\nExample:\nPDDL Domain:\n\"\"pddl\n(define (domain hanoi)\n(:requirements :strips)\n(:predicates\n(clear ?x)\n(on ?x ?y)\n(smaller ?x ?y)\n)\n(:action move\n:parameters (?disc ?from ?to)\n:precondition (and (smaller ?to ?disc) (on ?disc ?from)\n(clear ?disc) (clear ?to))\n:effect (and (clear ?from) (on ?disc ?to) (not (on ?disc ?from))\n(not (clear ?to))))\n)\"\"\"\nSpecification:\nYour goal is to solve the Tower of Hanoi puzzle, which involves moving a stack of discs from one peg to another, with the restriction that no disc may be placed on top of a smaller disc.\nThe puzzle is solved when all the discs are moved to the target peg following these rules.\nThe actions defined in this domain include:\nmove <disc> <from> <to>: This action allows moving a disc from one peg to another. The preconditions for this action are that the target peg is smaller than the disc being moved, the disc is on the source peg, and both the disc and the target peg are clear (i.e., there is no disc on top of them). The effect of this action is that the source peg becomes clear, the disc is now on the target peg, the disc is no longer on the source peg, and the target peg is no longer clear.\nYou have the following restrictions on your actions:\n- A disc can only be moved if it is clear, meaning there is no other disc on top of it.\n- A disc can only be placed on another disc or peg that is larger than itself.\n- A disc can only be moved to a peg that is clear.\n- Once a disc is moved from a peg, that peg becomes clear.\n- Once a disc is placed on a peg, that peg is no longer clear.\nNatural Language Mapping:\n\"\"python\n{\n\"clear\": \"{arg1} is clear.\",\n\"on\": \"{arg1} is on {arg2}.\",\n\"smaller\": \"{arg1} is smaller than {arg2}.\".\n\"move\": \"Move {arg1} from {arg2} to {arg3}.\".\n}\"\"\"\nYou need to generate the corresponding natural language mapping for the following pddl domain.\nPDDL Domain:\n{PDDL_Domain}\nSpecification:\n{PDDL_Description}\nNatural Language Mapping:"}, {"title": "B More Statistics on Environment", "content": null}, {"title": "B.1 Environment Specification", "content": "We analyzed the token distribution within the environmental specifications. Among the 592 environmental specifications, the average token count is 473.55, with a median of 467.00. The minimum token count is 207, and the maximum is 934. As depicted in Figure 4, the number of specification tokens for the environment is predominantly concentrated within the range of 300 to 699."}, {"title": "B.2 Environment Implementation", "content": "The scale of action space and state space in an environment typically dictates its complexity, with a greater number of actions and states generally indicating a more complex environment. An environment library with a greater variety of difficulty levels is preferable for a training set. As shown in Figure 5, there is a significant diversity in the number of actions and predicates."}, {"title": "B.3 Diversity Analysis", "content": "We evaluate the diversity of generated environments using cosine similarity. More specifically, we randomly sampled 100 environment specifications for better visualization and converted them into TF-IDF vectors. After calculating the cosine similarity matrix between all pairs of specifications, we visualize the matrix using heatmap as is shown in Figure 6. The computed average cosine similarity of the sampled environment specifications is 0.176, indicating that the corpus exhibits a high degree of diversity, reflecting a rich tapestry of distinct semantic features and thematic elements."}, {"title": "C Examples", "content": "In this section, we present the specific details of the cases depicted in Figure 2 and Figure 3."}, {"title": "C.1 Environment Specification", "content": "Environment Specification\nYou are a nutritionist tasked with creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient. Your environment consists of your office, a computer for research and writing, a kitchen for testing recipes, and a collection of ingredients including peanut butter powder. The computer can be used to write and save recipes, but it needs to be charged after every three hours of use.\nThe actions defined in this domain include:\nresearch_ingredient <location> <ingredient> <nutritionist>: This action allows the nutritionist to research peanut butter powder at the office. The effect of this action is that the nutritionist gains knowledge about the nutritional benefits and potential uses of peanut butter powder in recipes.\ndevelop_recipe <location> <computer> <nutritionist> <ingredient>: This action allows the nutritionist to create a new recipe using peanut butter powder. The precondition for this action is that the nutritionist has researched peanut butter powder. The effect of this action is a draft of a new recipe that incorporates peanut butter powder.\ntest_recipe <location> <kitchen> <nutritionist> <recipe>: This action allows the nutritionist to test the drafted recipe in the kitchen. The preconditions for this action are that the nutritionist has developed a recipe and has all the necessary ingredients, including peanut butter powder. The effect of this action is feedback on the recipe's taste, nutritional value, and overall feasibility.\nfinalize_recipe <location> <computer> <nutritionist> <tested_recipe>: This action allows the nutritionist to finalize the recipe after testing and receiving feedback. The preconditions for this action are that the nutritionist has tested the recipe and the computer is charged. The effect of this action is a completed recipe that is ready to be included in the healthy recipe book.\nYou have the following restrictions on your actions:\nYou can only research peanut butter powder if you are in the office where your research materials are located.\nYou can only develop a recipe after researching peanut butter powder and must have a charged computer.\nYou can only test a recipe in the kitchen if you have developed a recipe and have all the necessary ingredients.\nYou can only finalize a recipe after testing it in the kitchen and receiving feedback, and if your computer is charged."}, {"title": "C.2 Environment Implementation", "content": null}, {"title": "C.3 Trajectory Data", "content": "Trajectory Data\n(User) You are a nutritionist tasked with creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient. Your environment consists of your office, a computer for research and writing, a kitchen for testing recipes, and a collection of ingredients including peanut butter powder. The computer can be used to write and save recipes, but it needs to be charged after every three hours of use.\nThe actions defined in this domain include:\nresearch_ingredient <location> <ingredient> <nutritionist>: This action allows the nutritionist to research peanut butter powder at the office. The effect of this action is that the nutritionist gains knowledge about the nutritional benefits and potential uses of peanut butter powder in recipes.\ndevelop_recipe <location> <computer> <nutritionist> <ingredient>: This action allows the nutritionist to create a new recipe using peanut butter powder. The precondition for this action is that the nutritionist has researched peanut butter powder. The effect of this action is a draft of a new recipe that incorporates peanut butter powder.\ntest_recipe <location> <kitchen> <nutritionist> <recipe>: This action allows the nutritionist to test the drafted recipe in the kitchen. The preconditions for this action are that the nutritionist has developed a recipe and has all the necessary ingredients, including peanut butter powder. The effect of this action is feedback on the recipe's taste, nutritional value, and overall feasibility.\nfinalize_recipe <location> <computer> <nutritionist> <tested_recipe>: This action allows the nutritionist to finalize the recipe after testing and receiving feedback. The preconditions for this action are that the nutritionist has tested the recipe and the computer is charged. The effect of this action is a completed recipe that is ready to be included in the healthy recipe book.\nYou have the following restrictions on your actions:\nYou can only research peanut butter powder if you are in the office where your research materials are located.\nYou can only develop a recipe after researching peanut butter powder and must have a charged computer.\nYou can only test a recipe in the kitchen if you have developed a recipe and have all the necessary ingredients.\nYou can only finalize a recipe after testing it in the kitchen and receiving feedback, and if your computer is charged.\nGoal: The goal is to satisfy the following conditions: The computer is charged. jordan has tested the recipe almond_butter_bars.\nObservation:The computer is charged. The recipe almond_butter_bars has all the necessary ingredients. jordan has a recipe draft for almond_butter_bars. jordan has researched peanut butter. jordan is in the kitchen.\n(Assistant) Action: jordan tests the recipe almond_butter_bars.\n(User) Observation: The computer is charged. The recipe almond_butter_bars has all the necessary ingredients. jordan has a recipe draft for almond_butter_bars. jordan has researched peanut butter. jordan has tested the recipe almond_butter_bars. jordan is in the kitchen.\n(Assistant) Action: jordan develops a recipe using almond_butter_bars."}]}