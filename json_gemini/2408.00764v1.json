{"title": "AGENTGEN: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation", "authors": ["Mengkang Hu", "Pu Zhao", "Can Xu", "Qingfeng Sun", "Jianguang Lou", "Qingwei Lin", "Ping Luo", "Saravan Rajmohan", "Dongmei Zhang"], "abstract": "Large Language Model (LLM) based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, involving interaction with the environment and executing actions to complete a planning task, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLM-based agents through instruction tuning, referred to as agent training. Recent studies on agent training have demonstrated that utilizing expert-level trajectory data (sequences of action-observation pairs) for instruction-tuning LLMS effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories for agent training. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. In response, we introduce a framework, AGENTGEN, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, BI-EVOL, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve, thereby enhancing the learning process of LLMs more effectively. These methods collectively contribute to the generation of diverse trajectory data for instruction-tuning. Based on AGENTGEN, we greatly expanded the number of environments and planning tasks available for agent training. The evaluation results derived from AgentBoard show that AGENTGEN greatly improves LLMs' planning ability, e.g., the AGENTGEN instruction-tuned Llama-3 8B surpasses GPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms GPT-4.", "sections": [{"title": "1 Introduction", "content": "Recently, owing to advancements in Large Language Models (LLMs) [40, 41, 37, 60], the LLM-based Agents have garnered widespread attention from the artificial intelligence community. Generally, an LLM-based agent refers to utilizing LLMs to perceive the environment, make decisions, and execute actions to substitute or help people accomplish some specific tasks [75, 63, 77]. Furthermore, planning is often regarded as one of the most important applications of LLM-based agents, such as robotic planning [52, 44, 17, 61], travel planning [88, 76], etc. In this study, planning is conceptualized as the systematic process of identifying a sequence of executable actions within a given environment to complete a planning task, defined as the transition from an initial state to achieve specified goal conditions, considering constraints and available resources [23, 48].\nImproving planning capabilities through instruction-tuning LLMs is a significant research problem, referred to as agent training. As shown in Figure 1, similar to imitation learning [21], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks. (ii) Synthesizing expert-level trajectories (sequences of action-observation pairs) on these planning tasks. For example, utilizing state-of-the-art LLMs (e.g., GPT-4 [41]) as the agent and filtering trajectory based on reward score [84, 6]. (iii) Instruction-tuning LLMs with the synthesized trajectory data. Recently, the effectiveness of enhancing the planning capabilities of LLMs through agent training has been demonstrated by many studies [84, 83, 6, 66, 8, 85, 62, 55]. Despite their success, one key limitation of these works is that they primarily rely on manually designed environments and planning tasks. The labor-intensive nature of creating environments and planning tasks hinders the generation of diverse and extensive trajectory data. More explicitly, designing diverse environments requires defining a range of rich and practical scenarios, and implementing these environments typically involves the participation of human experts with programming skills. Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression. Due to this constraint, existing agent training studies typically use only a few environments for data synthesis.\nTo address the aforementioned deficiencies, this paper introduces an automatic framework AGENT-GEN that utilizes LLMs to construct diverse environments and planning tasks for agent training, expanding the available environments from a few to hundreds. More specifically, AGENTGEN is structured around two stages: (1) Environment Generation: Achieving sufficient environmental diversity is essential for creating diverse planning tasks, which involves covering a broad range of scenarios and domains. To ensure this, we use an inspiration corpus composed of diverse text segments as context for generating environment specifications with LLMs, where actions, restrictions, and other details are defined using natural language. For example, in Figure 2, we randomly selected a text segment from the inspiration corpus: \u201cHow to boost your diet with peanut butter powder?\u201d This prompted the generation of a related environment specification: \u201cYou are a nutritionist tasked with creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient\u201d. Subsequently, we prompt the LLM to produce the corresponding code based on this specification, which may be composed of Python, Planning Domain Definition Language (PDDL) [36], or other domain-specific languages. Furthermore, we constructed an environment library to serve as in-context examples and iteratively expanded it by incorporating high-quality newly generated environments. (2) Task Generation: Conditioned on the generated environment, we aim to create multiple planning tasks. In this stage, it is crucial to have a gradual set of tasks ranging from easy to difficult, i.e., difficulty diversity. To achieve greater difficulty diversity, we propose a bidirectional evolution method, BI-EVOL, where the LLM first generates random planning tasks and then evolves these"}, {"title": "2 Preliminary", "content": "We consider goal-directed deterministic planning problems [48], which are formally defined as a tuple \\(P = (T, E)\\), where E denotes the environment in which the agent interacts and T denotes the task that the agent needs to complete. Specifically, an environment E typically models a world, encompassing the definitions of the action space A and state space S, as well as the transition function \\(T: S \\times A \\rightarrow S\\). Task T is further defined by the tuple \\(T = (G, I)\\), where G refers to the goal conditions and I refers to initial states of the agent. The initial states I are a subset of the state space \\(S_i\\) that specifies the starting conditions of the agent. The goal G is a subset of the state space \\(S_g\\) that specifies the desired outcomes or conditions. Specifically, G can be expressed as \\(G = \\{s \\in S_g | \\phi(s) = true\\}\\). Here, \\(\\phi(s)\\) is a boolean-valued function representing conditions or propositions that must be satisfied for the state s to be considered part of the goal set.\nA planning problem can be implemented with programming languages such as Python or domain-specific languages such as Planning Domain Definition Language (PDDL) [36]. For example, in a PDDL-based planning problem, the domain PDDL file can be regarded as the environment E, defining states (predicates) and actions and specifying the transition function using preconditions and effects of each action. The problem PDDL file, on the other hand, can be seen as the task T. Both initial states and goal conditions are typically defined as combinations of predicates. Another widely used programming language for constructing planning problems is Python. For example, in OpenAI"}, {"title": "3 Methodology", "content": "The process of generating planning tasks can be formalized as a function \\(f: I \\rightarrow (T, E)\\), where I is the input space (e.g., instructions or prompts) and tuple (T, E) is the space of all possible planning tasks and environments. Based on the definition in Section 2.1, we can express this as \\(f(i) = (T_i, E_i), i\\in I\\), where \\(T_i\\) is the generated planning task and \\(E_i\\) is the generated environment for a given input i. Our two-stage approach can be further decomposed as follows: i) Environment Generation (\u00a73.1): In the first stage, we generate the environment \\(E_i\\) based on the input instruction i. This can be represented as \\(E_i = g_E(i)\\), where \\(g_E\\) is the environment generation function that takes the instruction i as input and produces the environment \\(E_i\\). ii) Task Generation (\u00a73.2): In the second stage, we generate the task \\(T_i\\), conditioned on the environment \\(E_i\\) generated in the first stage. This can be expressed as: \\(T_i = g_T(i, E_i)\\), where \\(g_T\\) is the task generation function that takes both the original instruction i and the generated environment \\(E_i\\) as inputs to produce the task \\(T_i\\). We will detail the implementation of these two stages in the following section.\nAs is shown in Figure 2, we propose a sophisticated framework for environment generation structured around three main components: (1) an environment specification generation module where an LLM first generates a specification of the environment, typically including a general overview of the environment, descriptions of the state space and action space, and definitions of the transition functions; (2) an environment implementation module that generates corresponding code based on the environment specification; and (3) an environment library that stores previously generated high-quality environments, serving as a comprehensive environment dataset and providing in-context examples for generating new environments. Each component will be elaborated on in the following paragraph.\nWe initially prompt the LLM to generate an environment specification, which typically includes an overall depiction of the environment, specific actions and their corresponding preconditions and effects, and certain restrictions within the environment. The environment specification will serve as the basis for generating specific environment codes. This two-stage approach, similar to the Chain-of-Thought [72], can better assist the LLM in creating high-quality environments. For generating environment specifications, One direct approach is to prompt LLMs to generate random environments. However, due to the inherent inductive bias of LLMs, they struggle to generate diverse environments in this way. Therefore, to address this issue, we build an inspiration corpus \\(D = \\{t_0,t_1, \\cdot, t_n\\}\\), containing sufficiently diverse text segments used to serve as the \"inspiration\" for generating environment specification with LLMs. More specifically, when generating an environment, we first sample a text segment \\(t_i\\) from D, then prompt the LLM to generate a related environment based on \\(t_i\\). Taking the example in Figure 2, we first sample a text segment \"How to boost your diet with peanut butter powder?\" from D. Then we prompt an LLM to generate a related environment where the agent is defined as a nutritionist tasked with creating a new healthy recipe book that prominently features peanut butter powder as a key ingredient. This approach significantly enhances the diversity of generated environments, thereby empowering more generalized agent training. The inspiration corpus can be implemented in various ways, such as using a large-scale pre-trained corpus like Common Crawl. Alternatively, a domain-specific corpus, such as a code generation dataset [25, 7], can be used to generate environments for a specific domain. This paper uses LIMA [91] as the inspiration corpus, an instruction-tuning dataset with sufficient diversity.\nConditioned on the generated environment specification, we generate its corresponding code, i.e., implementing the environment. This can be formulated as a typical code-generation problem with LLMs. We also introduce a validation tool capable of capturing syntax errors to provide feedback during the code generation process, thereby iteratively refining it.\nWe define the library at iteration t as: \\(L_t = L_0\\cup \\cup_{k=1}^{t-1}\\{E_i | E_i = g_E(i, L_{k-1}), i \\in I_k, v(E_i) = true\\}\\), where \\(L_0\\) is the initial seed library, and the union represents all verified environments generated up to iteration t. This iterative process allows continuous expansion and refinement of the environment library, potentially leading to increasingly complex and diverse environments over time.\nAs depicted in Figure 3, conditioned on the generated environments, we prompt LLMs to generate corresponding planning tasks. We employ a two-stage generation approach BI-EVOL for creating a diverse range of planning tasks in terms of difficulty. We begin by prompting the LLM with a specific environment, enabling it to generate an initial set of planning tasks in a zero-shot way. Subsequently, we adjust these tasks to make them simpler or more challenging, forming a comprehensive set of planning tasks.\nMany studies have proposed evolving instructions, primarily focusing on making instructions more difficult [78, 34, 33]. The effectiveness of this approach relies heavily on the assumption that LLMs inherently possess the ability to follow simple instructions. However, according to findings from some studies [35, 31], LLMs often exhibit poor performance even in simple planning tasks. Therefore, we propose BI-EVOL, which introduces evolution in two directions: easy-evol and hard-evol. Easy-evol typically involves simplifying the goal conditions. The motivation is that easier tasks can facilitate learning when the agent performs poorly and cannot directly learn from typically difficult goals. Conversely, hard-evol usually involves making the goal conditions"}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of the proposed framework, we synthesize environments and planning tasks using the Planning Domain Definition Language (PDDL), a widely adopted programming language for planning. Subsequently, we evaluate its performance across various unseen planning tasks in a zero-shot manner. To validate the effectiveness and generalizability of AGENTGEN, we categorized the evaluated tasks into two distinct groups: i) In-Domain Tasks: Planning tasks implemented using PDDL. ii) Out-of-Domain Tasks: These comprise tasks developed using other programming languages, such as Python.\nFor In-Domain Tasks, we select four widely used PDDL-based planning tasks: Blocksworld, Gripper, Tyreworld, and Barman [35]. More explicitly, Blocksworld requires an agent to achieve a target configuration by moving blocks, while Gripper involves moving objects between different rooms. Tyreworld simulates changing a car tire, including removing the flat tire, replacing it with a spare, and installing the new tire. Barman emulates a bartender's tasks in mixing cocktails, which include combining various ingredients, using shakers, and garnishing drinks. For Out-of-Domain Tasks, we select two challenging partial-observable planning tasks: Alfworld [52] and BabyAI [10]. Alfworld is an environment designed to test agents' abilities to perform everyday household tasks. While in BabyAI, the agent interprets and executes natural language instructions in a grid-world setting.\nWe utilized two evaluation metrics to evaluate planning ability: success rate and progress rate [35]. During each interaction round, we assigned a progress rate, denoted as \\(r_t\\), to measure the progression towards the goal state g. As the agent transitions through states \\(S_t = [s_0,..., s_t]\\), its progress is assessed using a matching score \\(f(\\cdot, g) \\rightarrow [0, 1]\\), which quantifies the similarity between the current state and the goal state. Initially, \\(r_t\\) is set to 0, indicating no progress. Only when the progress rate reaches 1 does the success rate attain 1; all other scenarios yield a 0 outcome. The success rate reflects the agent's capacity to complete a comprehensive task."}, {"title": "5 Related Work", "content": "Large Language Models have demonstrated exceptional reasoning capabilities [60, 37, 40, 41, 22]. Owing to such abilities, over the past two years, LLM-based agents have experienced significant development [51, 73, 15, 56, 63, 75]. Unlike the traditional method of using LLMs for text-based reasoning, such as Chain-of-Thought [72], LLM-based agents typically involve interaction with the environment, adjusting the output in a closed-loop manner based on environmental information. These LLM-based agents, now fortified with capabilities like Memorizing [89, 30, 27, 82, 51, 86, 93, 59, 20], Tool-use [9, 43, 50, 26, 49, 45], and Planning [12, 5, 39, 38, 47, 2], exhibit a marked enhancement in their overall efficacy. Although this paper mainly focuses on the planning capability of LLM-based agents, we believe AGENTGEN has the potential to generalize to other scenarios of LLM-based agents."}, {"title": "6 Conclusion", "content": "In this paper, we explore using LLMs to automatically generate environment and planning tasks for LLM-based agent training. Specifically, for generating diverse environments, we propose utilizing an inspiration corpus composed of various domain-specific text segments as the context for environment synthesis. To enhance the difficulty diversity of generated planning tasks, we introduce a bidirectional evolution method, BI-EVOL, which evolves planning tasks from both easier and more challenging directions to create a task set with a more gradual difficulty curve, thereby improving the effectiveness of LLM learning. Based on AGENTGEN, we developed a dataset consisting of 592 environments and 7246 trajectories and trained it on a series of LLMs. The trained model outperformed GPT-3.5 across multiple tasks and, in certain specific instances, exceeded the performance of GPT-4."}, {"title": "A More Implementation Details", "content": "We applied the instruct version for models. Specifically, the detailed version for each model is presented in Table 4.\nWe leverage GPT-4 to generate the natural language mapping that converts structured actions into its natural language format. When the mapping failed to yield, we heuristically serialized the structured actions. The prompt for generating natural language mapping with GPT-4 is as follows:\nI would like you to create natural language mapping for PDDL.\nThe form of the natural language mapping is a Python dictionary, wherein\n1. The key corresponds to the name of a predicate or action within the domain PDDL.\n2. The value is its equivalent in natural language, with parameters presented in \"{argn}\", where n is the index of its parameters in the PDDL expression.\n3. You must ensure that the number of \"{}\" corresponds precisely to the number of parameters in predicates or actions.\n4. You should very carefully check the order of {argn}.\nYour output must strictly follow the provided example."}, {"title": "B More Statistics on Environment", "content": "We analyzed the token distribution within the environmental specifications. Among the 592 environmental specifications, the average token count is 473.55, with a median of 467.00. The minimum token count is 207, and the maximum is 934. As depicted in Figure 4, the number of specification tokens for the environment is predominantly concentrated within the range of 300 to 699.\nThe scale of action space and state space in an environment typically dictates its complexity, with a greater number of actions and states generally indicating a more complex environment. An environment library with a greater variety of difficulty levels is preferable for a training set. As shown in Figure 5, there is a significant diversity in the number of actions and predicates.\nWe evaluate the diversity of generated environments using cosine similarity. More specifically, we randomly sampled 100 environment specifications for better visualization and converted them into TF-IDF vectors. After calculating the cosine similarity matrix between all pairs of specifications, we visualize the matrix using heatmap as is shown in Figure 6. The computed average cosine similarity"}, {"title": "C Examples", "content": "In this section, we present the specific details of the cases depicted in Figure 2 and Figure 3.\nYou are a nutritionist tasked with creating a new healthy recipe book that incorporates peanut butter powder as a key ingredient. Your environment consists of your office, a computer for research and writing, a kitchen for testing recipes, and a collection of ingredients including peanut butter powder. The computer can be used to write and save recipes, but it needs to be charged after every three hours of use.\nThe actions defined in this domain include:\nresearch_ingredient This action allows the nutritionist to research peanut butter powder at the office. The effect of this action is that the nutritionist gains knowledge about the nutritional benefits and potential uses of peanut butter powder in recipes.\ndevelop_recipe This action allows the nutritionist to create a new recipe using peanut butter powder. The precondition for this action is that the nutritionist has researched peanut butter powder. The effect of this action is a draft of a new recipe that incorporates peanut butter powder.\ntest_recipe This action allows the nutritionist to test the drafted recipe in the kitchen. The preconditions for this action are that the nutritionist has developed a recipe and has all the necessary ingredients, including peanut butter powder. The effect of this action is feedback on the recipe's taste, nutritional value, and overall feasibility.\nfinalize_recipe This action allows the nutritionist to finalize the recipe after testing and receiving feedback. The preconditions for this action are that the nutritionist has tested the recipe and the computer is charged. The effect of this action is a completed recipe that is ready to be included in the healthy recipe book.\nYou have the following restrictions on your actions:\nYou can only research peanut butter powder if you are in the office where your research materials are located.\nYou can only develop a recipe after researching peanut butter powder and must have a charged computer.\nYou can only test a recipe in the kitchen if you have developed a recipe and have all the necessary ingredients.\nYou can only finalize a recipe after testing it in the kitchen and receiving feedback, and if your computer is charged.\nGoal: The goal is to satisfy the following conditions: The computer is charged. jordan has tested the recipe almond_butter_bars.\nObservation:The computer is charged. The recipe almond_butter_bars has all the necessary ingredients. jordan has a recipe draft for almond_butter_bars. jordan has researched peanut butter. jordan is in the kitchen.\n(Assistant) Action: jordan tests the recipe almond_butter_bars.\n(User) Observation: The computer is charged. The recipe almond_butter_bars has all the necessary ingredients. jordan has a recipe draft for almond_butter_bars. jordan has researched peanut butter. jordan has tested the recipe almond_butter_bars. jordan is in the kitchen.\n(Assistant) Action: jordan develops a recipe using almond_butter_bars."}]}