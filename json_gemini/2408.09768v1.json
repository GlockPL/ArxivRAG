{"title": "MalLight: Influence-Aware Coordinated Traffic Signal Control for Traffic Signal Malfunctions", "authors": ["Qinchen Yang", "Zejun Xie", "Hua Wei", "Desheng Zhang", "Yu Yang"], "abstract": "Urban traffic is subject to disruptions that cause extended waiting time and safety issues at signalized intersections. While numerous studies have addressed the issue of intelligent traffic systems in the context of various disturbances, traffic signal malfunction, a common real-world occurrence with significant repercussions, has received comparatively limited attention. The primary objective of this research is to mitigate the adverse effects of traffic signal malfunction, such as traffic congestion and collision, by optimizing the control of neighboring functioning signals. To achieve this goal, this paper presents a novel traffic signal control framework (MalLight), which leverages an Influence-aware State Aggregation Module (ISAM) and an Influence-aware Reward Aggregation Module (IRAM) to achieve coordinated control of surrounding traffic signals. To the best of our knowledge, this study pioneers the application of a Reinforcement Learning(RL)-based approach to address the challenges posed by traffic signal malfunction. Empirical investigations conducted on real-world datasets substantiate the superior performance of our proposed methodology over conventional and deep learning-based alternatives in the presence of signal malfunction, with reduction of throughput alleviated by as much as 48.6%.", "sections": [{"title": "1 Introduction", "content": "Traffic signal malfunction refers to a situation when a traffic signal, also known as a traffic light, fails to function properly. One of the typical situations is that traffic lights are blackout caused by various reasons, such as flood [37, 56], hurricane [48], equipment aging [15], or human-made events, such as cyber-attacks [25, 36], traffic accidents and mismanagement [14]. When the traffic signal malfunction happens, it causes confusion and chaos on the road, leading to increased risk of accidents, traffic delay,s and traffic congestion. For example, we conducted a case study based on the traffic dataset from New York City (details in the Evaluation section), where traffic signals were set to be controlled by the FixedTime strategy [34], the most commonly used traffic signal control strategy. When the traffic signal at one intersection experiences a blackout, the average throughput of that intersection is reduced by nearly 30%. Moreover, the impacts also propagated to nearby intersections, such as intersections 1, 2, and 3-block away from the malfunctioning intersection. Similar observations are identified in other studies with increased travel time [24] and fatal accidents [45]. More importantly, these malfunctions occur more frequently than one might expect. For example, as shown in Figure 2, citizens from the City of Newark in New Jersey reported 94 complaints about traffic signal malfunctions at 67 different intersections between August 2020 and May 2022 on the Newark Connect platform [1] (i.e., a 311-like platform for citizens to report city issues), leading to three issues per month on average. Consequently, it is necessary to find a way that can mitigate the negative impacts (e.g., congestion) when traffic signal malfunctions occur."}, {"title": "", "content": "The problem of traffic signal malfunctions has been studied in existing work [13, 31, 49]. In general, we can categorize these studies into two stages: detection and response. The malfunction detection mainly relies on road user report [13] and some recently developed intelligent signal systems to automatically detect signal malfunction [49, 54]. After detection, the ultimate response is to repair or replace the malfunctioning traffic signals. Unfortunately, due to limited resources (e.g., human resources and budgets), this response process can be time-consuming, with repairs sometimes taking months or even years to complete [4]. Therefore, immediate temporary responses become necessary. Current temporary responses heavily rely on manual efforts, such as manual traffic guidance or Flash Mode [10]. However, abundant ground surveys have shown that Flash Mode is less effective [31], and manual traffic guidance also suffers from long time delays [9, 57]. Our work aims to provide a faster and more automated response after the malfunctions are detected and before human efforts get involved.\nOur solution is inspired by the fundamental theory of traffic management from the perspective of demand (i.e., the number of vehicles that request to pass through an intersection in a unit of time) and capacity (i.e., the number of vehicles that an intersection can handle in the same unit of time) [8, 51]. Traffic congestion occurs when demand is higher than capacity. When a traffic signal malfunction occurs, i.e. the intersection changes from signalized to a blackout, capacity decreases [47], while demand remains relatively constant in the short term. This imbalanced relationship between demand and capacity motivates us to explore immediate responses to traffic signal malfunction: by carefully managing the networked traffic signals, especially those surrounding malfunctioning intersections, we can potentially control the traffic demand that enters the malfunctioning intersections in a desired manner, which leads to a more balanced demand and capacity.\nHowever, coordinating intersections to control traffic in a desired manner is challenging, because (1) the impact from each intersection to malfunctioning ones is hard to measure. For example, one intersection has several routes to malfunctioning intersections. (2) Influence changes over time, such as during peak hours and non-peak hours. Recently, reinforcement learning (RL) has introduced a new possibility to coordinate traffic signal controls (TSC) [12, 23, 63, 72? ], where each agent controlling a single intersection can automatically learn to generate its actions to maximize its own benefits (e.g., the throughput of corresponding intersection). Since the reward function for each agent in these RL methods only depends on its own traffic condition, under a malfunction scenario, the agents of the well-functioning intersections are not aware of the specific needs of the malfunctioning intersections, even when its assistance benefits the whole system. Through experiments (Table 4, we observe that previous RL methods experience significant throughput reduction. For example, the IDQN method experiences a 42.8% reduction in throughput on the Hangzhou dataset, which is roughly 3.3 times higher than the 13% reduction observed with our model. Further, how the actions of each agent impact others is not well studied. To the best of our knowledge, none of the existing RL-based TSC methods can be applied in malfunction setting.\nIn this work, we design MalLight, an RL-based traffic signal control method to mitigate the impacts of traffic signal malfunctions, by modeling the spatially and temporally varied influence between well-functioning and malfunctioning interactions. Specifically, MalLight models the dynamic influences through the special design of the state module and the reward module in RL. To model the spatially and temporally varied influence, we adapt a graph diffusion convolution network to integrate the network-level state with the local state of an individual agent so that each RL agent is aware of its influence on other intersections while controlling its local traffic conditions. To guide each well-functioning intersection to be aware of the reward associated with malfunctioning intersections, we introduce the graph diffusion process. This process enables each agent to dynamically integrate rewards from multiple malfunctioning intersections while learning their policy. In summary, the main contributions of this work are as follows:\n\u2022 This is the first work that utilizes adaptive traffic signal control to build a resilient and responsive traffic system in the presence of traffic signal malfunction.\n\u2022 We design a novel RL model, MalLight, based on modeling influences among intersections under signal malfunction. We incorporate the graph diffusion process into the state and reward design to model the influences and guide the training of MalLight, making it resistant to signal malfunction.\n\u2022 Experiments on real-world and synthetic datasets show that MalLight outperforms both traditional and state-of-the-art (SOTA) RL-based models, achieving a remarkable alleviation in throughput reduction by up to 48.6%."}, {"title": "2 Preliminaries", "content": "In this section, we introduce the preliminaries of the traffic signal control problem [62, 63] and extend it to our scenario considering traffic signal malfunction. In our setting, each intersection is controlled by one agent, which has its own local observations of its intersection as the state. An agent controls the traffic signal in its own intersection by deciding the signal phase at the next time interval to optimize the local traffic, such as maximizing the throughput. We use centralized learning with a decentralized execution manner to reduce the computational complexity of traditional centralized methods [64]. Specifically, we formulate our problem as a Markov Decision Process(MDP) with the following components < S, O, M, A, r, \u03c0, \u03b3 >.\n\u2022 Set of malfunctioning signals M: M is a set of intersections with malfunctioning traffic signals.\n\u2022 State space S and observation space O: Assuming there are N intersections in our road network and N corresponding agents that control the traffic signals located at the intersections. All possible traffic states from N intersections form the state space S and each agent has access to partial state $s \\in S$, which forms the agent's own observation $o \\in O$. The state s of each intersection includes current phase p (one of the 8 total phases illustrated in Figure 4) and the number of vehicles in each lane (i.e., a 12-element vector with each element representing a number of vehicles in lanes illustrated in Figure 3). In MalLight, observations from all intersections in the road network are fed into a state module, which outputs a global state. The global state is combined with local state s to form observation o for each agent.\n\u2022 Action space A: In the traffic signal control task, the ith agent generates an action a as the signal phase at time point t from its action space $A_i$ for the following At period. Eight signal phases"}, {"title": "", "content": "corresponding to eight actions are illustrated in Figure 4. The eight-phase setting is commonly used in real-world traffic signal control task [64]. We set At as 10 seconds based on the practice of the previous work [12]. Given the action $a_i^t$, the traffic signal chooses the corresponding phase p. For malfunctioning signal $m \\in M$, $a_m^t$ is set to be off denoted as $p_{mal}$.\n\u2022 Reward r: Each RL agent obtains an immediate reward from the environment at time t by a rewarding function $S \\times A_1 X...X A_N \\rightarrow R$, where N is the number of intersections. We introduce the concept of pressure to measure the local reward for each intersection, which is the difference between the upstream and downstream queue length, indicating the inequivalence of the vehicle distribution [12]. More specifically, we denote the local reward of the ith intersection at time t as $r_i^t$, which is calculated as\n$r_{i}^{t}=-\\rho_{i}^{t}=-\\sum_{l_{i n}} u_{i, l_{i n}}^{t}-\\sum_{l_{\\text {out }}} u_{i, l_{\\text {out }}}^{t}$ (1)\nwhere $\\rho_i^t$ represents the pressure of intersection i, u is the queue length of lane l at intersection i at time t, lin and lout represent incoming and outgoing lane, respectively. For example, in Figure 3, the pressure of intersection is calculated as #queueing cars in incoming lanes - #queueing cars in outgoing lanes, which is |3+3+1+2-1-1-3-2|= 2. In MalLight, rewards from all intersections in the road network are fed into a reward module, which generates a global reward. Details of reward generation are described in Section 3.3. The global reward is then combined with the local reward $r_i^t$, to form the final reward $r^t$ for each agent.\n\u2022 Policy set \u03c0 and discount factor \u03b3: At time t, each agent chooses an action following a policy $\u03c0:S \u2192 A$. This choice can also be expressed as action-value function $Q^\u03c0$, which indicates the expected quality of an action a if the ith agent takes action a based on policy \u03c0 in state s. The formulation of the action-value function is\n$Q^{\\pi}(s, a)=E[\\sum_{i=t}^{\\infty} \\gamma^{i-t} r_{i}] \\mid s=s_{t}, a=a_{t}$ (2)"}, {"title": "Problem formulation and objective:", "content": "In our problem, each intersection is controlled by one agent, and these agents share parameters from the same deep RL model. At each time step t, agent i (managing well-functioning traffic signals) obtains an observation of from the environment. Given traffic conditions and current signal phases at both local and other intersections, the objective of each agent is to determine the optimal action $a_i^t$ that maximizes the reward $r_i^t$, formulated as:\n$\\arg \\max _{a^{i} \\in A^{i}}=-\\rho_{i}^{t}-\\sum_{j \\in M} w_{j} \\rho_{j}^{t}$ (3)\nHere, $\\rho_i$ represents the pressure at ith intersection, wj is the weight of jth intersection with malfunctioning signals, for reward aggregation, and M is the set of intersections of malfunctioning signals."}, {"title": "3 Method", "content": "We introduce MalLight, a RL-based traffic signal control model built upon a general RL framework, incorporating new modules: an Influence-aware State Aggregation Module (ISAM) and an Influence-aware Reward Aggregation Module (IRAM). These modules enhance the model's resistance to signal malfunctions. In the following sections, we first introduce the overview of our model and then describe the detailed design of ISAM and IRAM."}, {"title": "3.1 Overview", "content": "To alleviate congestion in malfunctioning intersections through coordinated management with neighboring intersections (intersections can be multi-hop away), each agent should possess two key abilities. Firstly, each agent should have the ability to \"care\" about malfunctioning intersections. In previous works [12, 62, 63, 71], the reward for each RL agent was solely dependent on its own traffic conditions, which meant that each agent would only benefit from its own actions. In our setting, intercepting some vehicles at a well-functioning intersection to prevent excessive traffic from entering malfunctioning intersections would lead to a reduction in the"}, {"title": "", "content": "rewards earned by agents at well-functioning intersections. Therefore, to alleviate the traffic demand on malfunctioning intersections, each surrounding agent should be aware of the reward from these malfunctioning intersections. Secondly, each agent should have the ability to quantify how much influence it can exert on the malfunctioning intersection to determine the optimal actions. Different levels of influence require different policies.\nThe overall framework of MalLight is depicted in Figure 5. In line with the two aforementioned abilities, we introduce two modules, the Influence-aware State Aggregation module (ISAM), and the Influence-aware Reward Aggregation module (IRAM). In ISAM, we designed masked graph diffusion convolution and adapt it into our multi-agent scenario, where each agent needs to take nearby malfunctioning agents into consideration while being aware of its influence on these agents. In IRAM, we design a novel reward reshaping scheme based on diffusion process, which boost the performance of model when used in cooperation with masked diffusion convolution. Up to our knowledge, we are the first to adapt diffusion convolution into multi-agent RL scheme and the first to reshape reward by diffusion process.\nTo simplify the task, in MalLight, sensing devices for detecting the vehicle queue length of each lane, such as cameras or inductive loop detectors, are assumed to work well in all intersections including the malfunctioning ones, as traffic signals and sensing devices are generally managed by different systems [13, 35]. In addition, we assume that the system is aware of which intersections have malfunctioning traffic signals (e.g., through human reports or anomaly detection techniques [49, 54]), and the signal control agents can share information."}, {"title": "3.2 Influence-aware State Aggregation Module", "content": "The Influence-aware State Aggregation Module (ISAM) is designed to ensure that each agent's observation includes global states from malfunctioning intersections while taking into account its influence on these malfunctioning intersections. Higher influence corresponds to greater weight assigned to other intersections' states, and vice versa. ISAM takes road network static information and intersection-specific local states as input. For each agent, it generates an aggregated state that encompasses observations from malfunctioning intersections and their corresponding local states. This state information is subsequently fed into the RL model and serves as the agent's final environmental observation.\nInspired by the graph diffusion convolutional recurrent neural network (DCRNN) [38], which has achieved promising results in automatically modeling complex spatial dependencies on road networks, we design masked diffusion convolution and adapt it to multi-agent scheme, to capture influence among intersections, particularly the influence from well-functioning intersections to malfunctioning intersections. In the diffusion convolution operation, an influence vector is extracted for each agent. These vectors are used as weights to perform a weighted sum of states from other intersections. Additionally, before the weighted sum is calculated, a Malfunction Mask is applied to filter out data from well-functioning intersections. This helps agents at well-functioning intersections focus on only themselves and the malfunctioning intersections. Details of ISAM are described below."}, {"title": "", "content": "\u2022 Diffusion Process. Diffusion convolution [38] is a variation of the diffusion process [59] that incorporates trainable parameters into transition matrix of each layer, offering enhanced representativeness and flexibility in modeling influence. According to [59], the stationary distribution of the diffusion process can be expressed as a weighted combination of random walks on the graph:\n$P=\\sum_{k=1}^{K} \\alpha(1-\\alpha)\\left(D_{O}^{-1} W\\right)^{k}$ (4)\n, where k represents the diffusion step, W is the edge weight matrix, $D_O$ is the out-degree diagonal matrix of graph, \u03b1 \u2208 [0, 1] denotes the restart probability, and $D_{O}^{-1} W$ serves as the state transition matrix. This Markov process converges to the distribution P\u2208 $R^{N\u00d7N}$, where each i-th row $P_{i, :}$ represents the likelihood of diffusion from node $v_i \u2208 V$, where V represents the set of nodes in the graph, and N denotes the total number of nodes.\nThe edge weight matrix W is constructed based on the road distance between each pair of intersections. For normalization, we use a thresholded Gaussian kernel [53], defined as follows:\n$W_{i, j}=\\exp \\left(-\\frac{\\operatorname{dist}(i, j)}{\\sigma^{2}}\\right)$ (5)\nIn this formula, $W_{i,j}$ represents the edge weight between node $v_i$ and node $v_j$, o is the deviation of distances, and dist(vi, vj) is 0 if nodes $v_i$ and $v_j$ are not connected by a road. Otherwise, it represents the distance between vi and vj.\n\u2022 Masked Diffusion Convolution. Building upon the diffusion process described above, diffusion convolution introduces a trainable filter for each diffusion step k. Additionally, to ensure that agents do not concentrate their efforts on other functioning intersections, which do not require assistance, we apply the Malfunction Mask. The Malfunction Mask vector, denoted as Mask \u2208 {0, 1}N, where N represents the number of intersections in the road network. The element in the Malfunction Mask is set to 1 if the corresponding intersection is experiencing a malfunction, and 0 otherwise. The formulation of masked diffusion convolution is as follows:\n$S_{p}^{\\prime}=\\sum_{k=1}^{K}\\left(\\theta_{k}\\left(D_{o}^{-1} W\\right)^{k}\\right) \\odot \\text { Mask } S_{i p}, p \\in\\{1,2,3, \\ldots, P\\}$ (6)\nHere, $\\odot$ indicates Hadamard product, $S \\in R^{N \\times P}$ represents the matrix of features in the graph (each agent's local state), P is the length of input features, p\u2208 [1, P] is an index of an element in input feature, \u03b8k represents the trainable filter for diffusion step k, and K is the number of diffusion steps. The masked diffusion convolution process involves random walks on the road network for each element of the input features. S. indicates the information i-th agent needs considering its influence to other nodes(intersections). Masked diffusion convolution provides a more explainable method for making the model aware of the influence.\n\u2022 State Aggregation. This step combines the local state of the target intersection with the global state S' originating from malfunctioning intersections. For the i-th agent operating at the i-th intersection, its corresponding global state is $S_{i}^{\\prime} \\in R^{P}$ and its local state is Si,: \u2208 RP. Finally, for the i-th agent, we obtain the state as:\n$S_{i}=S_{i}^{\\prime}+S_{i,:}, i \\in\\{1,2,3, \\ldots, N\\}$ (7)"}, {"title": "", "content": "Here $S_{i} \\in R^{P}$ contains all the information i-th agent should consider for making a decision and it serves as the input to the traffic control RL model."}, {"title": "3.3 Influence-aware Reward Aggregation Module", "content": "The Influence-aware Reward Aggregation Module (IRAM) is designed to ensure that each agent \"cares\" about the benefits of malfunctioning intersections and is \"willing\" to assist these intersections. IRAM takes road network static information and intersection rewards as input, and for each agent, generate aggregated reward containing rewards of malfunctioning intersections and corresponding local rewards. The reward information then serve as direction to optimize the RL model.\nReward Shaping [46] is a technique, where additional rewards are utilized to represent domain knowledge and guide the training of RL models using expert knowledge. In our context, we aggregate the rewards from malfunctioning intersections to each working agent, with different weights, representing influences between malfunctioning and working agents. The additional reward guides working agents in earning benefits for malfunctioning intersections. We employ a diffusion process without trainable parameters to perform a weighted sum of rewards. The formulation is as follows:\n$R^{\\prime}=\\sum_{k=1}^{K}\\left(D_{O}^{-1} W\\right)^{k} \\odot \\text { Mask } R$ (8)\nwhere R\u2208 $R^{N}$ is the vector of rewards of all intersections, k is the diffusion step, $D_{O}^{-1} W$ is the state transition matrix. Similar to the state aggregation method described above, for i-th agent, we add the global reward $R_{i}^{\\prime}$ and the local reward $R_i$ to obtain the final reward, denoted as R':\n$R^{\\prime}=R_{i}+R_{i}^{\\prime}, i \\in\\{1,2,3, \\ldots, N\\}$ (9)"}, {"title": "3.4 Training", "content": "MalLight is updated using the Bellman Equation[22]:\n$Q(S_{t}, a_{t})=R_{t}^{\\prime}+\\gamma \\max Q(S_{t+1}, a_{t+1})$ (10)\nIn this equation, $R_{t}^{\\prime}$ represents the reward that i-th agent can obtain by taking action a based on the state $S_t$, at decision time t. y is discount factor. We employ the Mean Squared Error (MSE) as the loss function to measure the disparity between the current action-value estimation and the desired action-value estimation provided by the Bellman Equation. The RL model is optimized using the RMSprop[27] algorithm."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to answer the following research questions: 1\n\u2022 RQ1: Does MalLight outperform other SoTA methods on the malfunctioning signal scenario?\n\u2022 RQ2: Does MalLight perform well on the normal traffic signal control scenario?"}, {"title": "5 Discussion", "content": "Lessons Learned: We summarize key lessons learned from work:\n\u2022 We design our malfunction-resistant approach based on inter-intersection cooperation, as manifested in the design of state and reward modules. In contrast to alternative methods that neglect the advantageous potential of well-functioning intersections in alleviating the plight of malfunctioning counterparts, our approach stands out by affording a notably reduced reduction in throughput.\n\u2022 The integration of state and reward aggregation mechanisms, while taking into consideration the influences among agents, is of great importance in coordinating intersections and enhancing"}, {"title": "", "content": "the model's resilience in the face of signal malfunction. In direct comparison to the Advanced-PressLight model which shares similar architecture but lacks the incorporation of these two pivotal modules, our approach achieves better performance in the presence of signal malfunction. In essence, by optimizing control of surrounding intersections, the impact of signal malfunction can be alleviated, thus aligning with our central research idea.\nLimitations: Constrained by the inherent limitations of the driver behavior model within SUMO, regrettably fails to authentically replicate the congestion resulting from signal malfunctions, we resorted to the use of traffic collisions as a surrogate for congestion induction. We have to acknowledge that not all instances of congestion are precipitated by traffic collisions. In the future, our objective is to enhance the fidelity of our simulation by refining the driver behavior model to more accurately emulate real-world congestion dynamics."}, {"title": "6 Related Work", "content": "6.1 Traffic Signal Malfunction\nThe time series of traffic signal malfunctions and the subsequent actions are as follows: malfunction occurs, detection, and response. Methods designed to address malfunctioning traffic signals can be categorized into two groups: detection and response methods.\nDetection methods: As stated in [13], malfunction detection typically relies on manual reports. Several resources [7] are available for reporting signal problems, including 24-hour hotlines and 911 calls. Manual detection, based on road user reports, often results in delays. Remote monitoring techniques may be employed in some areas when crew time and workload allow, but they are seldom effective due to the expensive human resources required. Furthermore, some automatic detection systems have been developed. Purba et al [49] develop a self-diagnosis system to detect traffic signal malfunction immediately. Soh et al [54] design a monitoring system based on fuzzy technology for malfunction detection. In our work, we assume malfunctioning signals are detected by existing work.\nResponse methods: The response to put malfunctioning traffic signals back to fully functional relies heavily on manual repair or replacement actions taken by contractors or authorities [13]. In addition to manual response, some automatic response methods are also designed. Malfunction flash [10] is designed to prevent safety issues from traffic signal malfunction. When an error is detected, the signal is automatically placed into flash mode as a safety precaution (if the signal is still accessible). However, malfunction flash only serves as a warning to road users. However, some decisions, such as whether to cross or wait, and whether to stop before crossing, are left to drivers, based on their observation of traffic conditions. Hunter et al [31] studied the driver behaviors under malfunction flash and revealed that malfunction flash cannot eliminate the safety issue of traffic signal malfunctions, not to mention other impacts such as congestion. In addition, our method is applicable in situations where the traffic signal is in a blackout, where the traffic signal cannot even work as a flashing warning to drivers.\nSome methods are designed for efficient human resources distribution. For example, Mathibela [42] designs a RUSBoost-based framework to predict the best distribution of human resources at critical intersections in the presence of malfunctioning traffic lights."}, {"title": "6.2 Traffic Signal Control", "content": "Traffic Signal Control is an important sub-domain of smart city [11, 21, 28-30, 67-69, 73]. Methods for controlling traffic signals can be divided into classic optimization-based methods [15, 55, 60] and RL-based methods [26, 62, 63, 65, 71]. Among these techniques, RL is most popular these years because of its ability to learn directly from complex conditions without assumptions about the environment [18, 32, 33, 40, 41, 52, 58, 61]. Thus, RL-based methods and Deep Learning based methods usually outperform classic optimization-based methods. However, current RL-based methods exhibit suboptimal performance under scenarios involving signal malfunctions due to the isolation of reward mechanisms. Specifically, each agent operates with a focus exclusively on optimizing the traffic flow at its own intersection, neglecting the potential benefits of adjacent intersections. While this approach is effective in standard conditions, where each intersection is regulated by a functioning agent, it falters in the event of a signal malfunction. In such cases, the affected intersection lacks the capability to leverage support from nearby intersections, causing increased congestion.\nRecently, a number of works have focused on traffic disruptions, such as missing data [44], incidents [23, 50], or weather changes [17, 19, 20]. Traffic signal malfunction is different from these disruptions in that: Firstly, these disruptions affect the RL model's input, while signal malfunction disables the RL agent from interacting with the environment, i.e. affects the output. Secondly, these disruptions test a single model's robustness, while signal malfunction tests the coordination ability of the whole system. Because a malfunctioning signal loses the ability to control traffic flow, it is naturally in need of help from other well-functioning signals, which makes cooperation an important factor in this context. In conclusion, traffic signal malfunction is a unique scenario that has not been considered in previous traffic disruption and robustness works."}, {"title": "7 Conclusion", "content": "In this paper, we study the problem of intelligent traffic signal control in the presence of traffic signal malfunction. We design MalLight, an traffic signal control model founded on RL principles. In the MalLight framework, we introduce two pioneering components, namely the Influence-aware State Aggregation Module and the Influence-aware Reward Aggregation Module, which are seamlessly integrated into the RL architecture. We subsequently conduct a comprehensive array of empirical investigations on two real-world datasets. Our empirical findings demonstrate that MalLight surpasses the performance of extant methods in both normal conditions and scenarios characterized by signal malfunctions. Notably, our results reveal a substantial improvement in the reduction of intersection throughput, with a remarkable mitigation rate of up to 48.6%."}]}