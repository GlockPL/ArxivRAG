{"title": "Time Series Foundational Models: Their Role in Anomaly Detection and Prediction", "authors": ["Chathurangi Shyalika", "Harleen Kaur Bagga", "Ahan Bhatt", "Renjith Prasad", "Alaa Al Ghazo", "Amit Sheth"], "abstract": "Time series foundational models (TSFM) have gained prominence in time series forecasting, promising state-of-the-art performance across various applications. However, their application in anomaly detection and prediction remains underexplored, with growing concerns regarding their black-box nature, lack of interpretability and applicability. This paper critically evaluates the efficacy of TSFM in anomaly detection and prediction tasks. We systematically analyze TSFM across multiple datasets, including those characterized by the absence of discernible patterns, trends and seasonality. Our analysis shows that while TSFMs can be extended for anomaly detection and prediction, traditional statistical and deep learning models often match or outperform TSFM in these tasks. Additionally, TSFMs require high computational resources but fail to capture sequential dependencies effectively or improve performance in few-shot or zero-shot scenarios.", "sections": [{"title": "Introduction", "content": "Foundational models (FMs), including large language models (LLM), have demonstrated efficacy in a variety of applications, with a particular emphasis on natural language processing (NLP) (Achiam et al. 2023) and computer vision (CV) (Liu et al. 2024). This has resulted in an increasing interest in the use of FMs for time series analysis (Zhang et al. 2024). The impressive capabilities of FMs and LLMs, including generalizability across domains, data efficiency, advanced reasoning and pattern recognition, multimodal knowledge integration and easy optimization, offer significant potential for enhancing time series forecasting without requiring per-task retraining from scratch (Jin et al. 2024, 2023). Recently, there has been a growing trend in developing foundational models specifically tailored for time series analysis.\nHowever, applying FMs for time series analysis has seen several limitations. A significant concern is the ambiguity surrounding the datasets on which TSFMs are trained, which can lead to overestimated model performance and misleading evaluations. This issue becomes particularly problematic when the training data does not accurately represent the diversity of real-world scenarios, resulting in models that perform well in controlled environments but struggle to generalize to diverse, practical applications (Arjovsky et al. 2019). Moreover, data leakage is a critical issue in time series modeling, especially concerning TSFMs. If the temporal ordering of data is not strictly maintained or if features that will be known in the future are inadvertently included during training, the model can be influenced by information from the test set. Data leakage can produce artificially inflated performance metrics, giving false impressions of the model's robustness (Kaufman et al. 2012; Rosenblatt et al. 2024).\nAdditionally, while TSFMs may excel in forecasting tasks in some domains, their generalizability to other tasks, such as anomaly detection and prediction (prediction refers to identifying an anomalous event beforehand, while detection involves identifying it after it has occurred) (Shyalika et al. 2024), is limited. Anomaly detection and prediction involve identifying rare and unexpected patterns. This process fundamentally differs from the sequential prediction tasks for which TSFMs are designed. Consequently, TSFMs may either miss subtle anomalies or mistakenly classify normal variations as anomalies, leading to high false positive rates (Chandola, Banerjee, and Kumar 2009). Furthermore, TSFMs are resource-intensive, requiring substantial computational power and extensive labeled datasets for training. Obtaining such datasets is challenging due to the high costs and the difficulty of labeling time-series data, especially when anomalies are rare (Aggarwal and Aggarwal 2017). These constraints highlight the need for caution when applying TSFMs beyond their intended scope and underscore the importance of developing specialized models for specific tasks, such as anomaly detection and prediction. While some TSFMs, such as Moment (Goswami et al. 2024) and TimeGPT (Garza and Mergenthaler-Canseco 2023), can perform anomaly detection, current TSFMs are not explicitly designed for anomaly prediction. Hence, our study adapted the standard next-time series forecasting approach for the anomaly prediction task. Ultimately, in this analysis, we selected models based on the criteria that they must be explicitly designed and trained for either anomaly detection or forecasting tasks.\nOur research contributions are as follows:\n\u2022 Comprehensive analysis and evaluation of TSFMs for anomaly detection and prediction across five time-series datasets.\n\u2022 Benchmarking TSFMs against traditional statistical and deep learning approaches.\n\u2022 Analysis of the effectiveness of fine-tuning TSFMs.\n\u2022 Investigation of the computational intensity of TSFMs compared to benchmark approaches."}, {"title": "Literature Review", "content": "The research in this field can be divided into two primary approaches: pre-training foundational models from scratch for time series and adapting large language models for time series. Pre-training foundational models from scratch for time series represents an emerging area of research aimed at overcoming the unique challenges inherent in time series data, where limitations in scale and variability have historically hindered the development of robust, generalized models. Foundational models, initially successful in NLP and computer vision through large-scale pre-training, have shown impressive zero-shot and few-shot learning capabilities, often outperforming task-specific models. However, as anticipation grows for similar foundational models tailored to time series, recent efforts like ForecastPFN (Dooley et al. 2024), TimeGPT (Garza and Mergenthaler-Canseco 2023) mark pioneering steps towards advancing time series analysis with models capable of capturing the unique temporal dynamics and patterns inherent in this domain. Adapting LLMs for time series analysis involves leveraging their pre-trained capabilities for various downstream tasks, focusing on effectiveness, efficiency and explainability. Two main adaptation paradigms-embedding-visible (Jin et al. 2023; Zhou et al. 2023) and textual-visible (Zhang et al. 2023; Liu et al. 2023; Xue and Salim 2023) are inspired by NLP techniques, differing primarily in input-output approaches and how time series data is integrated. Beyond forecasting, LLMs can also serve as enhancers, data generators and explainers, expanding their utility across diverse time series applications.\nThe mainstream downstream time series tasks include classification, forecasting, imputation and anomaly detection, each addressing key aspects of temporal data analysis. Among these, anomaly detection and prediction, within the realm of time series foundation models, has been the least explored and remains particularly challenging due to several factors. The rarity and unpredictability of anomalies make it difficult to collect sufficient training data and the subtle, often context-dependent nature of anomalies complicates their identification using generalized models. Despite these challenges, anomaly detection and prediction are crucial, especially in critical applications like industrial monitoring, financial fraud detection and healthcare, where early and accurate detection of anomalies can prevent significant losses and ensure safety. As time series foundation models continue to evolve, addressing these anomaly detection and prediction challenges will be essential for advancing the field."}, {"title": "Experimental Setup", "content": "In this section, we outline the setup employed in our experiments, detailing the selection criteria for TSFM, the specific models chosen, the datasets used and the overall process followed to assess the models' performance in time series analysis, particularly in anomaly detection and prediction."}, {"title": "Selection of Reference TSFMS", "content": "We chose models for our study based on the criterion that they must be explicitly designed and trained for anomaly detection or forecasting tasks based on multivariate datasets\u00b9. Ultimately, we selected three models that involve pre-training foundational models from scratch for time series analysis (TimeGPT(Garza and Mergenthaler-Canseco 2023), MOIRAI and Time-MOE(Shi et al. 2024)) and two models (Frozen-Pretrained Transformer-FPT(Zhou et al. 2023) and Chronos (Ansari et al. 2024)) that adapts LLMs for time series analysis. \nTimeGPT It (Garza and Mergenthaler-Canseco 2023) utilizes an encoder-decoder transformer architecture. It operates in a univariate channel setting and is designed for detection and forecasting tasks \u00b2. As the largest transformer-based foundation model in this domain, TimeGPT has been pre-trained on over 100 billion data points. Although it claims to have assembled the largest time series repository from public sources, TimeGPT does not publicly disclose its repository or the details of the data used.\nFPT FPT (Zhou et al. 2023) utilizes GPT-2 as the backbone. It is a domain-agnostic model primarily focused on forecasting tasks. This approach involves embedding visible LLM adaptation, where most of the LLM parameters are frozen and only a minority are updated during the time series forecasting process. By freezing the major parameters of GPT-2, particularly the self-attention and feedforward layers, FPT preserves a substantial portion of the pre-trained knowledge. The model redesigns the input layer and retrains it, along with the positional embedding and normalization layers, on diverse time series datasets to enhance the LLM's capacity for downstream tasks.\nTime-MOE It (Shi et al. 2024) introduces a scalable, efficient, and unified architecture for time series forecasting using a Mixture of Experts (MoE) framework. Time-MOE can address challenges in large-scale time series modeling, such as high computational costs and limited flexibility in forecasting horizons. It employs a sparse activation design that reduces computational overhead by activating only a subset of networks for each prediction while maintaining high model capacity. The model uses a decoder-only transformer structure with innovations like multi-resolution forecasting heads, rotary positional embeddings, and Huber loss for stability. Pre-trained on a new large-scale dataset, Time-300B, spanning 300 billion data points across nine domains, Time-MOE demonstrates superior performance in both zero-shot and fine-tuned forecasting tasks. Time-MOE validates the scalability of sparse models, achieving better forecasting accuracy compared to dense counterparts with the same computational resources.\nFor anomaly prediction, we adapt the standard approach of next-time series forecasting."}, {"title": "Benchmark Datasets", "content": "In this study, we use five publicly available datasets (Appendix A). Four of the datasets are from the manufacturing domain: Pulp and Paper manufacturing dataset (Ranjan et al. 2018), Future Factories (FF) dataset (Harik et al. 2024), MSL dataset (Hundman et al. 2018) and SMD dataset (Su et al. 2019). These datasets are event-related and include rare events and anomalies. We then use the ETTh1 dataset (Wu et al. 2022) \u00b3, which is not event-related."}, {"title": "Analysis Procedure", "content": "Figure 1 presents the overview of the analysis procedure followed. While experimenting with each model, we replicate the conditions of their original papers. We use the original hyper-parameters, runtime environments, and code, including model architectures, training loops, and data loaders. To ensure a fair comparison, we have included error metrics from the original papers."}, {"title": "Analysis of Zero-Shot Performance on Anomaly Detection and Prediction", "content": "Firstly, exploratory data analysis (Details in Appendix B) is conducted on each dataset to gain insights into its structure, distribution and key characteristics. Then, data processing techniques (Details in Appendix C) are applied separately to each dataset, tailored to meet the specific requirements of each task. In TimeGPT model, for anomaly detection task, we use NixtlaClient's \"detect_anomalies\" function to identify anomalies. Additionally, we extend this to a classification task, mapping predicted values to ground truth values included in the dataset for comparison and classifying predictions as normal or abnormal. In anomaly prediction in TimeGPT, we train the model to forecast future data points based on historical data using the \"forecast\" function. In the FPT model, we modify the implementation under anomaly detection to perform the detection. For anomaly prediction, we adapt the code in the long-term forecasting. For Time-MOE, MOIRAI and Chronos models, we use their zero-shot forecasting capability for anomaly prediction."}, {"title": "Analysis of Fine-tuning Performance on Anomaly Detection and Prediction", "content": "We fine-tuned the three reference models, TimeGPT, MOIRAI, and Chronos, on the five datasets employed in our study."}, {"title": "Analysis with Statistical and Deep Learning Models", "content": "We conduct experiments with statistical and deep learning models as baselines for anomaly detection and prediction. We train a weighted XGBoost model as a statistical method and an autoencoder model that follows encoder-decoder architecture as a deep-learning model."}, {"title": "Analysis of Computational Cost of FMs with Statistical and Deep Learning models", "content": "We measure the computational intensity in terms of the time taken to give outputs for the anomaly detection and prediction tasks."}, {"title": "Results", "content": "Table 2 presents the evaluation of models across multiple datasets for anomaly detection and prediction tasks \u2074. Among the models, only TimeGPT and FPT demonstrate anomaly detection capabilities, with TimeGPT achieving good performance on structured datasets like Pulp (precision: 0.91, recall: 1.0) and MSL datasets (precision: 0.78, F1-score: 0.8). However, its effectiveness diminishes on the FF dataset, where recall falls to 0.06, and prediction metrics such as MSE becomes 0.11. FPT, while less consistent in detection, exhibits reliable forecasting accuracy across datasets like ETTh1 (RMSE: 0.67) and MSL (F1-score: 0.81). Chronos, though limited to forecasting, provides competitive RMSE values on datasets like FF (0.163) and Pulp (0.37), indicating robust point forecast capabilities. For anomaly prediction tasks, MOIRAI demonstrates adaptability but is constrained by its design as a pure forecasting model. On structured datasets like ETTh1 and Pulp, it achieves RMSE values of 0.4 and 0.15, respectively, but struggles with sparse anomaly representations. Time-MOE consistently outperforms other models in forecasting accuracy (e.g., RMSE: 0.587 on ETTh1 and 0.795 on FF) due to its sparse mixture-of-experts framework. Both MOIRAI and Time-MOE highlight the trade-offs between precision and model efficiency in large-scale predictions. Overall, while TimeGPT excels in combined anomaly detection and forecasting, task-specific models like Time-MOE and Chronos deliver competitive results in pure forecasting tasks, reinforcing the need for tailored approaches depending on the dataset characteristics and task requirements.\nFine-tuning of TimeGPT, MOIRAI, and Chronos demonstrated only marginal improvements in performance compared to their zero-shot results, as highlighted in Table 3. This indicates that the benefits of fine-tuning these models for anomaly detection and prediction tasks are limited.\nTable 4 presents a comparative analysis of statistical(Weighted XGBoost) and deep learning (Autoencoder) models across the datasets we used for anomaly detection and prediction tasks. In terms of classification metrics, Weighted XGB consistently outperforms Autoencoder across datasets, demonstrating superior precision, recall, F1-scores, and accuracy. For example, on the Pulp dataset, Weighted XGB achieves an F1-score of 0.94 compared to Autoencoder's 0.893, with an improvement in recall (0.93 vs. 0.88) and precision (0.9 vs. 0.89). Similarly, on the FF dataset, Weighted XGB achieves higher F1-scores (0.8 vs."}, {"title": "Discussion", "content": "In this paper, we examined the performance of foundational models for anomaly detection and prediction. Despite their growing popularity, our findings indicate that foundational models do not substantially enhance performance compared to other methodologies. Our results underscore the performance of TSFMs in anomaly detection and prediction and reveal that statistical and deep learning models consistently outperform them in performance and computational efficiency. Statistical models like weighted XGBoost excel in anomaly detection and prediction due to their interpretability, using feature importance and ensemble methods to identify variables contributing to anomalies. Deep learning methods, such as autoencoders, learn compressed representations of \"normal\" data, making them detect deviations as anomalies effectively. In contrast, TSFMs designed for general-purpose forecasting often struggle to capture the task-specific nuances required for effective anomaly detection or prediction. Furthermore, only a few models, such as TimeGPT and FPT, are extended for anomaly detection, and none are purpose-built for anomaly prediction, where we have repurposed forecasting capabilities.\nTo address these challenges, future research on TSFMs must focus on integrating robust mechanisms for anomaly detection and prediction. Novel tokenization, data augmentation, and prompting techniques could be developed to enhance foundational model learning. Scaling laws and domain knowledge integration should be leveraged to capture complex temporal dynamics effectively, optimizing model performance and reducing computational complexity. Expanding into multimodal datasets represents another promising direction, enabling the integration of diverse data modalities. Additionally, improving model explainability is critical. The black-box nature of TSFMs limits their interpretability, an essential factor in high-stakes applications such as healthcare and manufacturing. Techniques such as chain-of-thought prompting or post-hoc explanation methods, which have proven effective in LLMs, remain underutilized in TSFMs."}, {"title": "Conclusion", "content": "We show that TSFMs, while effective in forecasting, are limited in anomaly detection and prediction. Statistical and deep learning models outperform TSFMs regarding accuracy, efficiency, and usability. The black-box nature of TSFMs and the lack of specialized designs for anomaly-related tasks restrict their applicability. Future advancements should focus on integrating domain knowledge, multimodal data, and explainability techniques to improve TSFM performance. Addressing these gaps will make TSFMs more practical for critical applications such as industrial monitoring and financial fraud detection."}, {"title": "Appendix A. Description of the Datasets Used", "content": "Pulp and paper manufacturing dataset This dataset has been curated from the pulp-and-paper industry and contains sensor data from various locations on machines within a paper mill (Ranjan et al. 2018). These sensors monitor metrics of raw materials, such as pulp fiber and chemicals, as well as process variables like blade type, couch vacuum and rotor speed. The dataset comprises 18,398 records over a 30-day period, with sensor readings taken every two minutes. Each record includes a timestamp, a binary event label (y) and 61 predictor variables (x1\u2013x61). Most predictors are continuous, except for x28, which is categorical and x61, which is binary. Only 124 rows have a y value of 1, indicating a sheet break (a rare event), while the rest have a value of 0.\nFuture factories dataset This is a publicly available manufacturing dataset curated by the Future Factories (FF) lab at the University of South Carolina (Harik et al. 2024). This dataset is available in two formats: analog and multimodal. It contains data captured from a prototype rocket assembly pipeline, which follows industrial standards for the deployment of actuators, control systems and transducers. In this study, we use the analog FF dataset \u2075. The dataset encompasses 292 complete assembly cycles, where each cycle represents the full assembly and disassembly of a rocket. The assembly process at the FF lab is segmented into 21 distinct cycle states. The preprocessed dataset includes various measurements, such as sensor readings, conveyor variable frequency drive temperatures, robot physical properties (e.g., angles), conveyor workstation statistics, cycle states, cycle counts, anomaly types and associated image file names from both cameras. The rocket assembled in the FF Lab consists of four parts: the nosecone, body 1, body 2 and the rocket base. Any missing part is categorized as an anomaly; for instance, the absence of Rocket body 1 is labeled \"NoBody1,\" while the absence of both Rocket body 1 and body 2 is labeled \"NoBody1,NoBody2.\"\nMSL dataset The MSL dataset (Hundman et al. 2018) \u2076 includes 66,709 data records of anonymized spacecraft telemetry channel data and anomalies. Channel IDs are anonymized, but the first letter indicates the type of channel (e.g., P = power, R = radiation). There are total 36 anomalous records in the dataset, divided into two categories: 19 point-anomalies, which ignore temporal information and are distance-based and 17 contextual anomalies, identified by properly-set alarms that take temporal information into consideration. No identifying information related to the timing or nature of commands is included in the data. Additionally, there are 27 unique features associated with telemetry channels collected via manual telemetry assessment and 19 unique features obtained by mining ISA reports.\nServer machine dataset (SMD) The SMD\u2077 is a comprehensive five-week dataset collected from a large Internet company, designed to support research on anomaly detection and interpretability in machine learning. It comprises data from 28 individual machines, grouped into three distinct categories, each labeled using the format machine-<group_index>-<index>.\nFor each machine, the dataset is divided into two equal parts: the first half serves as the training set, while the second half constitutes the test set. To facilitate evaluation, the dataset provides test labels indicating whether a data point is anomalous and interpretation labels that identify the specific dimensions contributing to each anomaly. The structured design of SMD, which emphasizes independent training and testing for each machine, makes it a valuable resource for developing and benchmarking advanced anomaly detection algorithms and interpretability techniques. SMD consists of multivariate time series data with a dimensionality of 38, a training set containing 708,405 data points, and a testing set comprising 708,420 data points. The anomaly ratio within the dataset is approximately 4.16%. For our analysis, we used a part of the SMD dataset.\nETTh1 dataset The ETTh1 dataset is a subset of the Electricity Transformer Temperature (ETT) dataset (Wu et al. 2022), which focuses on time-series data collected hourly from electricity transformers. This dataset includes two years of data, amounting to 17,520 data points, with each entry consisting of eight features: \u201cdate\u201d, six external power load features (HUFL, HULL, MUFL, MULL, LUFL, LULL) and the target variable, \u201coil temperature\u201d (OT). ETTh1 is designed to capture short-term daily and long-term weekly patterns, making it suitable for forecasting tasks in electrical transformer management, such as predicting transformer oil temperature and optimizing load distribution."}, {"title": "Appendix B. Exploratory Data Analysis (EDA)", "content": "This section includes selected snapshots of the EDA conducted. respectively."}, {"title": "Appendix C. Experimental Details on the Reference Time Series Foundational Models", "content": "1. FPT (Frozen Pre-trained Transformer)\n\u2022 FPT model is capable of performing both next-time series forecasting and anomaly detection tasks.\n\u2022 Initially, we experiment with anomaly detection task with the MSL dataset. The preprocessed MSL dataset has three separate files; one containing the training features, one with the test features and one with the test labels. Anomaly detection is experimented on this preprocessed MSL dataset. Then, we experiment with forecasting tasks with the ETTh1 dataset.\n\u2022 For the Future Factories dataset, we select the top 20 statistically significant features. To fit the requirements of the FPT model for anomaly detection, we transformed the raw FF dataset into three separate files: one containing the training features, one with the test features and one with the test labels. The dataset is split into training, validation and testing sets with an 70/20/10 ratio. For anomaly detection, we convert multi-class labels to binary labels, as the FPT model only supports multi-featured binary-labeled data. For forecasting, we employ \"LongTermForecasting\u201d technique, which assess performance by averaging metric-results over prediction lengths of 96, 192, 336, and 720. We kept the sampling frequency as 100 ms for the dataset.\n\u2022 We use the FPT model for anomaly detection and time series forecasting on the pulp and paper dataset with the 59 features included (excluding categorical variables x28 and x61). To adapt the raw pulp and paper dataset for the FPT model for anomaly detection, we preprocessed it into three distinct files: one for training features, one for test features and one for test labels. The train, validation, and test split was also 70/20/10. We selected a two-minute sampling frequency for the dataset to match its original frequency of two minutes. For forecasting, the performance metrics are averaged across prediction lengths of 96, 192, 336, and 720.\n\u2022 We selected a subset of the SMD dataset, consisting of 75,877 rows, for anomaly detection and time series forecasting using the FPT model. The dataset, with 37 features, was preprocessed into three separate files: one for training features, one for test features, and one for test labels, to suit the FPT model. The data was split into training, validation, and test sets with a ratio of 70/20/10, while maintaining the original frequency of the dataset for the experiments. In the forecasting task, we average the performance metric results over prediction lengths of 96, 192, 336, and 720.\n\u2022 Similarly, for the ETTh1 dataset, we use the FPT model for anomaly detection and time series forecasting on the SMD dataset with the 7 features included. To adapt the raw ETTh1 dataset for the FPT model for anomaly detection, we preprocessed it into three distinct files: one for training features, one for test features and one for test labels. The train, validation, and test split was also 70/20/10. We selected a one-hour sampling frequency for the dataset to match its original frequency of one-hour. For forecasting, the performance metrics are averaged across prediction lengths of 96, 192, 336, and 720.\n2. TimeGPT\n\u2022 TimeGPT is capable of performing both anomaly detection (zero-shot) and next-time series forecasting (with fine-tuning on the training dataset). We aimed to evaluate the framework's capability on our datasets.\n\u2022 For the Future Factory dataset, we chose a sampling frequency of one minute. Upon analyzing the data, we found that crucial information was retained at this frequency. This approach is primarily to optimize computational efficiency and prevent the TimeGPT API from crashing due to overly granular data. For time series forecasting, we split our data into an 80/20 train-test ratio. We select the features deemed important by domain experts [Features: I_R02_Gripper_Pot, I_R03_Gripper_Pot, I_R03_Gripper_Load]. Our experiments show that the best results were achieved using these three features. For anomaly detection, we convert multi-class labels to binary labels, as the TimeGPT model only supports multi-featured binary labeled data (all the anomalies are being labeled as one and non-anomalous states are labeled as zero, reference to the kind of anomaly is lost).\n\u2022 For the pulp dataset, we retained the existing time frequency of two minutes and included all 59 features for evaluation, excluding categorical variables x28 and x61. The data was split into an 80/20 train-test ratio for time series forecasting. We then compare the results from these two experiments to analyze how varying the confidence interval affects anomaly detection.\n\u2022 For the SMD, MSL, and ETTh1 datasets, we retain the original frequencies and utilize all the features from the datasets for both anomaly detection and prediction tasks.\n3. Time-M\u041e\u0415\n\u2022 Time-MOE is designed to perform next-step time series forecasting through fine-tuning on the training dataset. For the five datasets used in this study, we followed a consistent experimental procedure, demonstrating that the model exhibits strong generalization capabilities across different datasets.\n\u2022 We utilize the TimeMoE-50M model for time series forecasting. Initially, a tensor of random values (seqs) is generated to represent the input sequences, with a batch size of 2 and a context length of 12. The model is then loaded from the pre-trained Maple728/TimeMoE-50M checkpoint. The input sequences are normalized by subtracting the mean and dividing"}, {"title": "Appendix D. Getting to Anomaly Prediction from the Forecasted Values", "content": "To obtain the results of anomaly prediction using time series forecasting of various features, it is essential to have predefined safe operating thresholds for each feature under specific operational conditions. These thresholds allow for the accurate classification of anomalies. For instance, consider three features: Feature 1, Feature 2, and Feature 3. By comparing the forecasted values of these features against their respective thresholds, we can classify each feature as either safe or unsafe. If the forecasted value of any feature falls into the unsafe zone, the presence of an outlier can be marked as true."}]}