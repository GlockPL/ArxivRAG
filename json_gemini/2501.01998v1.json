{"title": "SmartSpatial: Enhancing the 3D Spatial Arrangement Capabilities of Stable Diffusion Models and Introducing a Novel 3D Spatial Evaluation Framework", "authors": ["Mao Xun Huang", "Hen-Hsen Huang"], "abstract": "Stable Diffusion models have made remarkable strides in generating photorealistic images from text prompts but often falter when tasked with accurately representing complex spatial arrangements, particularly involving intricate 3D relationships. To address this limitation, we introduce SmartSpatial, an innovative approach that enhances the spatial arrangement capabilities of Stable Diffusion models through 3D-aware conditioning and attention-guided mechanisms. SmartSpatial incorporates depth information and employs cross-attention control to ensure precise object placement, delivering notable improvements in spatial accuracy metrics. In conjunction with SmartSpatial, we present SmartSpatialEval, a comprehensive evaluation framework designed to assess spatial relationships. This framework utilizes vision-language models and graph-based dependency parsing for performance analysis. Experimental results on the COCO and SpatialPrompts datasets show that SmartSpatial significantly outperforms existing methods, setting new benchmarks for spatial arrangement accuracy in image generation.", "sections": [{"title": "1 Introduction", "content": "Text-to-image generative models, particularly diffusion-based frameworks such as Stable Diffusion [11], have achieved remarkable advances in synthesizing diverse and highly realistic images from natural language descriptions. However, despite their impressive achievements, these models frequently struggle with accurately maintaining the spatial arrangements of objects. This limitation becomes particularly evident when handling complex 3D spatial relationships, such as \"in front of\" and \"behind,\" which require precise understanding and representation of depth and positioning. These inaccuracies often result in visually plausible but contextually flawed images, undermining the reliability of these models for applications demanding high spatial fidelity. Accurate spatial arrangement is not just a desirable feature-it is essential for critical applications like virtual scene creation, content synthesis, and human-computer interaction. The inability of current models to consistently deliver such accuracy highlights a significant and pressing challenge in the field, underscoring the need for advanced solutions. This paper introduces SmartSpatial, a novel approach designed to address these limitations by incorporating 3D spatial awareness into diffusion models. Our method enhances object positioning precision through depth integration and cross-attention manipulation. By injecting 3D spatial data into ControlNet and fine-tuning cross-attention blocks, SmartSpatial achieves robust spatial arrangement capabilities guided by textual prompts.\nTo comprehensively evaluate the spatial accuracy of generated images, we propose SmartSpatialEval, an innovative evaluation framework that utilizes vision-language models and graph-based dependency parsing to assess spatial relationships. This framework provides quantitative metrics for spatial accuracy, complementing traditional image quality evaluations.\nExperiments on the COCO and SpatialPrompts datasets demonstrate that SmartSpatial significantly improves spatial accuracy over existing methods, setting a new benchmark for spatial control in text-to-image generation. Our contributions are as follows:\n\u2022 Spatially-Aware Generation: SmartSpatial integrates 3D depth information and cross-attention refinements to enhance spatial precision.\n\u2022 Quantitative Evaluation: SmartSpatialEval provides robust, quantitative metrics for spatial accuracy using graph-based parsing.\n\u2022 State-of-the-Art Results: SmartSpatial achieves superior performance in both quantitative and qualitative evaluations, outperforming existing methods."}, {"title": "2 Related Works", "content": "We categorize the literature into two areas: spatial arrangement in diffusion models and evaluation tools for spatial accuracy.\nTraining-Free Layout Guidance. Training-free methods like Prompt-to-Prompt [6] and pix2pix-zero [9] leverage cross-attention maps for localized edits but lack holistic layout control. Extensions such as optimized noisy latents [3] and segmentation mask conditioning [9] improve spatial precision but remain limited in complex arrangements. Epstein et al. [4] enhanced object scale and position control but struggled with fine-grained spatial accuracy.\nConditional Control in Diffusion Models. Conditional methods improve precision by incorporating spatial guidance. ControlNet [14] adds spatial conditioning through fine-tuned layers, while localized control [15] and instance-level approaches [13] utilize bounding boxes and segmentation masks. However, these techniques often adhere rigidly to 2D layouts, limiting flexibility.\nLimitations in Spatial Evaluation. Metrics like FID [1] and CLIP score [7] prioritize visual and semantic quality but neglect spatial accuracy. Tools like DP-IQA [5] and DiffNat [12] focus on image quality, while the SPRIGHT dataset [2] highlights the need for robust spatial evaluation. Current tools fail to assess complex spatial relationships effectively.\nOur work advances 3D spatial arrangement through cross-attention manipulation and 3D conditioning, surpassing limitations of planar-focused methods like Chen et al. [3] and rigid controls in ControlNet [14]. We further address the gap in evaluation by introducing SmartSpatialEval, a comprehensive tool for assessing spatial accuracy in generated images."}, {"title": "3 Methodology", "content": "We propose two innovations to enhance 3D object arrangement in Stable Diffusion models and evaluate spatial accuracy. First, we integrate 3D spatial data into ControlNet with attention-guided mechanisms, improving spatial arrangement while preserving image quality (Figure 1). Second, we introduce a human-centric evaluation framework using vision-language models, dependency parsing, and graph-based spatial representations to quantitatively score spatial relationships against ground truth data (Figure 3). The following subsections detail each approach."}, {"title": "3.1 3D Information Integration and\nAttention-Guided Control", "content": "Depth Injection. To capture 3D spatial relationships (e.g., front, behind), we select a reference image and employ a depth estimator to generate a corresponding depth map. Note that the reference image can be any image where the objects represent a spatial relationship, making it adaptable to various scenarios. It is not confined to a specific image but serves as a general guiding example. For instance, the reference image in Figure 1, depicting \"A ball is behind a box,\" can be applied broadly to cases involving the \"behind\" relationship. This depth map is then processed by a Depth Extractor, utilizing ControlNet [14], to extract depth features. The extracted depth information is subsequently integrated into the upsampling blocks of the Denoising UNet, enriching the model with precise spatial data.\nAttention Blocks Selection. ControlNet often rigidly constrains generated images to the reference input, so we mitigate this by modifying the cross-attention blocks. Specifically, we select the mid-cross-attention block in the Depth Extractor along with the mid and first up-sampling cross-attention blocks in the Denoising UNet. This configuration has been shown to provide optimal performance, as demonstrated in [3], enhancing the model's ability to guide spatial awareness and object placement (e.g., guiding the model to identify the \"ball\" as the vase and the \"box\" as the orange in Figure 1).\nLoss Function and Attention Guidance. Our objective is to fine-tune the latent space to ensure high attention weights within designated regions. Inspired by Chen et al. [3], we extract attention maps $A_i$ for each token i from the Depth Extractor and the Denoising UNet. To confine $A_i$ predominantly within the specified bounding box $b_i$, we adopt the following loss function:\n$L = \\sum_{i} \\left(1- \\frac{\\sum_{p \\in b_{i}} A_{p,i}}{\\sum_{p} A_{p,i}} \\right)^{2}$ (1)\nHere, $A_{p,i}$ denotes the attention values at pixel p for token i, and B is the set of all bounding boxes.\nTo expedite convergence, we introduce a momentum term in the backward guidance phase, defined as:\n$v = m \\cdot v - \\eta \\nabla_{z_{t}} L$ (2)\n$z_{t+1} = z_{t} + v$ (3)\nwhere m is the momentum coefficient, $\u03b7$ is the learning rate, and $z_t$ represents the latent variable at iteration t.\nAdditionally, we incorporate a ControlNet-specific term in the overall loss function to ensure coherent guidance across the entire model:\n$Loss = \\alpha L_{Unet} + \\beta L_{control}$ (4)\nHere, $L_{unet}$ and $L_{control}$ are the loss components for the UNet and ControlNet, respectively, while $\u03b1$ and $\u03b2$ are weighting factors balancing the contributions of each term."}, {"title": "3.2 3D Spatial Accuracy Evaluation with\nVision-Language Models and Graph Parsing", "content": "To represent spatial relationships among objects in an image, we first use a vision-language model (VLM) to generate human-like descriptions of these relationships. The generated descriptions are then parsed into a graph structure using dependency parsing techniques, providing a structured representation of spatial relationships among objects.\nTo compare observed spatial relationships with ground truth data, we designate a center object and use breadth-first search to determine the shortest paths from other objects to this center. We then position the center object at the core of a \"Spatial Sphere\" (as shown in Figure 2), with other objects arranged accordingly. This spatial sphere serves as a relative representation, enabling comparisons between observed and ground truth configurations.\nOur evaluation framework, SmartSpatialEval, quantifies spatial accuracy in generated images using the following metrics:\nDistance Score measures the Euclidean distances between object pairs using their 3D coordinates. Shorter distances yield higher scores:\n$D = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{1 + \\|\\| r_i - o_i \\|\\|^2}$ (5)\nwhere ri and o\u00a1 represent the real and observed positions of object i, respectively, and n is the number of objects.\nSpatial Relationship Score further evaluates whether the spatial relationships between objects match the prompt specifications, normalized between 0 and 1:\n$SR = \\frac{m_{correct}}{m_{total}}$ (6)\nwhere $m_{correct}$ denotes correctly identified relationships, and $m_{total}$ is the total number specified.\nOur proposed metrics, Distance Score and Spatial Relationship Score, leverage observations from VLMs to simulate human perception and assess images based on 3D spatial relationships, including front, back, left, right, above, and below. Unlike CLIP, IoU, or mAP, which emphasize image quality or layout precision, our proposed metrics are specifically designed to evaluate 3D spatial arrangements, providing a more reasonable and accurate assessment for scenarios involving complex spatial relationships, ensuring critical aspects of object placement and spatial relationships are evaluated."}, {"title": "4 Experiments", "content": "We conduct experiments on two datasets. The first is a subset of 300 instances from COCO2017 [8], featuring images with descriptive captions, used as a baseline for evaluating object relationships. The second is a custom dataset, SpatialPrompts, comprising 120 prompts designed to test SmartSpatial's spatial reasoning abilities (e.g., \"A bicycle is in front of a car at a traffic signal\"). These prompts span eight spatial positions (front, behind, left, right, on, under, above, below), with 15 examples per category.\nWe evaluate performance using CLIPScore [7] for image-text alignment, IoU [10], and mAP@0.5 for object arrangement accuracy. Additionally, we introduce Distance Score D and Spatial Relationship Scores SR (Section 3.2) to specifically assess spatial accuracy from the prompts.\nTables 1 and 2 present the performance comparisons among various spatial control methods, including Stable Diffusion (SD) [11], cross-attention guidance (AG) [3], ControlNet [14], and our proposed method, SmartSpatial. Our approach demonstrates superior performance across most metrics in both the COCO and SpatialPrompts datasets. Specifically, on the SpatialPrompts dataset, SmartSpatial outperforms AG [3] with approximately 68% improvement in IoU, a 70% increase in mAP@0.5, a 16% enhancement in D score, and a 32% improvement in SR. On the more complex COCO dataset, while overall scores are lower due to intricate scenes and object relationships, SmartSpatial maintains competitive performance, demonstrating robustness in spatial control.\nQualitative results illustrated in Figure 4 further validate our findings. The baseline Stable Diffusion model frequently exhibits issues such as missing objects and incorrect spatial relationships. AG and ControlNet methods also struggle with maintaining spatial coherence, particularly in scenarios involving complex 3D relationships, such as objects positioned \"in front of\" or \"behind\" others. In contrast, SmartSpatial consistently preserves spatial relationships, highlighting its effectiveness and reliability in managing spatial prompts across various conditions.\nThe loss convergence results presented in Figure 5 demonstrate that the incorporation of momentum accelerates the process of cross-attention guidance."}, {"title": "5 Conclusion", "content": "This work introduced SmartSpatial, a novel approach to enhance 3D spatial arrangement in text-to-image generative models, and SmartSpatialEval, an innovative framework for evaluating spatial accuracy. By integrating 3D spatial information and refining cross-attention mechanisms, SmartSpatial improves spatial precision while maintaining image quality. Experimental results demonstrate its superiority over existing methods, setting a new benchmark for spatial control in text-to-image generation. Our contributions pave the way for more reliable and context-aware image synthesis in applications requiring high spatial fidelity."}]}