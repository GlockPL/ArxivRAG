{"title": "Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval", "authors": ["Zijun Min", "Bingshuai Liu", "Liang Zhang", "Jia Song", "Jinsong Su", "Song He", "Xiaochen Bo"], "abstract": "The field of bioinformatics has seen significant progress, making the cross-modal text-molecule retrieval task increasingly vital. This task focuses on accurately retrieving molecule structures based on textual descriptions, by effectively aligning textual descriptions and molecules to assist researchers in identifying suitable molecular candidates. However, many existing approaches overlook the details inherent in molecule sub-structures. In this work, we introduce the Optimal Transport-based Multi-grained Alignments model (ORMA), a novel approach that facilitates multi-grained alignments between textual descriptions and molecules. Our model features a text encoder and a molecule encoder. The text encoder processes textual descriptions to generate both token-level and sentence-level representations, while molecules are modeled as hierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes to extract representations at these three levels. A key innovation in ORMA is the application of Optimal Transport (OT) to align tokens with motifs, creating multi-token representations that integrate multiple token alignments with their corresponding motifs. Additionally, we employ contrastive learning to refine cross-modal alignments at three distinct scales: token-atom, multitoken-motif, and sentence-molecule, ensuring that the similarities between correctly matched text-molecule pairs are maximized while those of unmatched pairs are minimized. To our knowledge, this is the first attempt to explore alignments at both the motif and multi-token levels. Experimental results on the ChEBI-20 and PCdes datasets demonstrate that ORMA significantly outperforms existing state-of-the-art (SOTA) models. Specifically, in text-molecule retrieval on ChEBI-20, our model achieves a Hits@1 score of 66.5%, surpassing the SOTA model AMAN by 17.1%. Similarly, in molecule-text retrieval, ORMA secures a Hits@1 score of 61.6%, outperforming AMAN by 15.0%.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of bioinformatics has led to the construction of numerous large-scale molecular databases, such as PubChem [1]. These databases play a crucial role in the discovery and synthesis of new drugs. However, accurately retrieving desired molecules from these databases presents a significant challenge. Therefore, aiming to retrieve molecules based on text queries, cross-modal text-molecule retrieval [2] has become increasingly important."}, {"title": "II. RELATED WORK", "content": "To achieve high-quality text-molecule retrieval, most studies represent molecules as 1D sequences, 2D molecular graphs, or 3D molecular conformations. In the aspect of 1D molecule modeling, SMILES [6] has been widely used to represent molecular sequences, emerging many typical pre-training models, such as KV-PLM [3], MolT5 [4], and Text+Chem T5 [11]. Meanwhile, some studies switch their attention to 2D topological graphs, where atoms and chemical bonds are considered as nodes and edges, respectively. For example, studies like MoMu [7] and MoleculeSTM [8] employ cross-modal contrastive learning to align text and molecular graphs in a shared semantic space. Moreover, MolCA [12] introduces a cross-modal projector, while AMAN [9] and [13] employ adversarial learning to bridge these two modalities effectively. As the extension of the above studies, several studies incorporate additional modalities to aid the alignments between text and molecules, such as MolFM [14] incorporated knowledge graphs, and GIT-Mol [15] incorporated images. Recently, some studies focus on the spatial information in 3D molecular conformations. For instance, 3D-MoLM [16] adopts a similar architecture to MolCA [12], using 3D conformations to represent molecules.\nDespite these advancements, most methods only focus on global molecular information, often overlooking detailed information at the motif and atom levels. To integrate detailed information, multi-grained alignments have been explored in various natural language processing (NLP) [17]\u2013[22] and computer vision (CV) [23]\u2013[25] tasks, enabling models to effectively process complex information. However, in the task of text-molecule retrieval, few studies pay attention to this technique. The only exception is Atomas [10] which applies a clustering algorithm to extract features at the atom, fragment, and molecule levels.\nAlthough with the same motivation as Atomas, we construct a hierarchical heterogeneous molecular graph to obtain atom, motif, and molecule representations. More importantly, we introduce optimal transport to fuse the representations of multiple tokens aligned with the same motif. Through contrastive learning, we align text and molecules at token-atom, multitoken-motif, and sentence-molecule levels. Subsequent experimental results show that our model outperforms Atomas, confirming the effectiveness of our model."}, {"title": "III. OUR MODEL", "content": "In this section, we will provide a detailed description of our model. We first present the main architecture of our model, which mainly contains a text encoder and a molecule encoder to learn representations at different granularities. Next, we further introduce Optimal Transport (OT) to generate the fused representation of multiple tokens aligned with the same motifs. To the best of our knowledge, our work is the first attempt to consider representations at the motif and multi-token levels. Finally, we define a training objective that involves three alignment losses to achieve cross-modal multi-grained alignments."}, {"title": "A. Main Architecture", "content": "As shown in Figure 2, our model mainly consists of a molecule encoder and a text encoder. As implemented in previous studies [2], [9], we also employ SciBERT [26] to encode the input textual descriptions, obtaining representations at both sentence and token levels. Compared with other pretraining models, SciBERT outperforms in encoding chemical textual descriptions, due to pretraining on a large-scale corpus of scientific publications. Given an input textual description with $N_t$ tokens, we first concatenate a special [CLS] token at the beginning of the sequence. Then, we input the sequence into the SciBERT encoder, where the learned d-dimensional representation of [CLS] serves as the sentence representation $h^s$, and those of remaining tokens are used as token representations: $H^t=[h_1, h_2, ..., h_{N_t}] \\in \\mathbb{R}^{N_t \\times d}$.\nTo effectively encode the input molecule, we represent it as an undirected heterogeneous graph and use a GCN encoder to learn its representations at different granularities. The molecular graph contains three types of nodes, constructed as follows: (1) Atom nodes. Since each molecule consists of atoms connected by chemical bonds, we include each atom as an individual node, which enables us to fully capture the detailed structural information of the molecule. (2) Motif nodes. In molecular chemistry, a motif is defined as a specific group of bonded atoms that follows a consistent and repeating pattern. Thus, we believe that motifs encode rich implicit semantic information, which is crucial for understanding the molecular properties described in the text. Following previous study [27], we employ the BRICS algorithm [28] with an additional decomposition rule to extract motifs, all of which are also included as separate motif nodes. (3) Molecule node. We also include a global molecule node to facilitate the learning of a comprehensive molecule representation. To effectively capture the relationships between nodes at different granularities, we consider two types of edges: (1) Motif-Atom Edges. Each motif node is connected to its constituent atom nodes. (2) Molecule-Motif Edges. The molecule node is connected to all motif nodes."}, {"title": "B. Optimal Transport-Based Multi-token Fused Representation Learning", "content": "As previously mentioned, a crucial aspect of our model is the consideration of the fused representation of multiple tokens aligned with the corresponding motif. To this end, we model the alignments between the input tokens and motifs as the Optimal Transport (OT) problem, as illustrated in Figure 4. As a typical machine learning problem, OT aims to find the scheme that minimizes the transportation cost of transferring one distribution to another distribution [30]. Concretely, we first consider the following crucial definitions: (1) We regard the token representations $H^t$ and motif representations $H^m$ as two independent distributions. (2) $C_{ij} = 1 - cos(h_i^t, h_j^m)$ is the transportation cost from $h_i^t$ to $h_j^m$, where $cos(\\cdot, \\cdot)$ is a cosine distance function. (3) We define $T = \\{T_{ij}\\}, 1 \\leq i \\leq N_t, 1 \\leq j \\leq N_m$, as an assignment plan, learned to optimize the alignments between input tokens and motifs. Solving the optimal transport problem is equivalent to addressing a specific network-flow problem [31] as follows:\n$\\min \\sum_{i=1}^{N_t} \\sum_{j=1}^{N_m} T_{ij}C_{ij} = \\min Tr(TC_{ij}).$ (1)"}, {"title": "C. Model Training and Inference", "content": "Through the above steps, we obtain the representations of input texts and molecules at different granularities. Further, we define the following training objective:\n$\\mathcal{L} = \\alpha \\cdot \\mathcal{L}_{ta} + \\beta \\cdot \\mathcal{L}_{mm} + (1 - \\alpha - \\beta) \\cdot \\mathcal{L}_{sm}$, (2)\nwhere $\\mathcal{L}_{ta}, \\mathcal{L}_{mm}, \\mathcal{L}_{sm}$ denote the token-atom, multitoken-motif, and sentence-molecule alignment losses, and $\\alpha, \\beta$ are two coefficients balancing the effects of different losses, respectively. In the following, we will detail these three losses.\n1) Token-Atom Alignment Loss $\\mathcal{L}_{ta}$: To ensure precise alignments between tokens and atoms, we introduce a token-atom alignment loss based on contrastive learning. Concretely, as shown in Figure 3, we compute the token-atom similarity matrix between token representations $\\{h_i^t\\}$ and atom representations $\\{h_j^a\\}$. The similarity matrix is then normalized by the min-max normalization along the atom dimension. Subsequently, we further normalize the matrix along the token dimension to obtain the alignment weights, used to unify the dimensions of token and atom representations. After multiplying these alignment weights with atom representations, we update the atom representations as $\\{h_a^{'}\\}$ by integrating textual information. Finally, we apply sum pooling to reduce the dimensions of both representations, which are then used to calculate the final similarity at the token-atom level.\nWithin a batch of size B, we compute similarities $\\{S_{qk}^{ta}\\}, 1 \\leq q \\leq B, 1 \\leq k \\leq B$, between samples at the token-atom level. Following this, we define the token-atom alignment loss $\\mathcal{L}_{ta}$ as the average of two Categorical Cross-Entropy (CCE) losses corresponding to text-molecule and molecule-text retrieval tasks, respectively:\n$\\mathcal{L}_{ta} = \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{ta})}{\\sum_{q=1}^{B} \\exp(S_{qk}^{ta})} + \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{ta})}{\\sum_{q=1}^{B} \\exp(S_{kq}^{ta})}$. (3)\n2) Multitoken-Motif Alignment Loss $\\mathcal{L}_{mm}$: We also employ contrastive learning to achieve alignments between multi-tokens and motifs. Specifically, in a similar way, we calculate the similarities $\\{S_{qk}^{mm}\\}, 1 \\leq q \\leq B, 1 \\leq k \\leq B$, at multi-token and motif level between samples within the same batch. Thus, the multitoken-motif alignment loss $\\mathcal{L}_{mm}$ is formulated as follows:\n$\\mathcal{L}_{mm} = \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{mm})}{\\sum_{q=1}^{B} \\exp(S_{qk}^{mm})} + \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{mm})}{\\sum_{q=1}^{B} \\exp(S_{kq}^{mm})}$. (4)\n3) Sentence-Molecule Alignment Loss $\\mathcal{L}_{sm}$: Similar to other levels of alignments, we employ contrastive learning to compute the CCE loss at sentence-molecule level. Concretely, we maximize the similarities between matched sentence and molecule pairs, while minimizing those between unmatched pairs, with the sentence-molecule alignment loss $\\mathcal{L}_{sm}$ defined as follows:\n$\\mathcal{L}_{sm} = \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{sm})}{\\sum_{q=1}^{B} \\exp(S_{qk}^{sm})} + \\frac{1}{2B} \\sum_{k=1}^{B} \\log \\frac{\\exp(S_{kk}^{sm})}{\\sum_{q=1}^{B} \\exp(S_{kq}^{sm})}$, (5)\nwhere $\\{S_{qk}^{sm}\\}, 1 \\leq q \\leq B, 1 \\leq k \\leq B$, is denoted as the similarities between the sentence representation of the q-th sample and the molecule representation of the k-th sample.\nDuring inference for text-molecule retrieval, we calculate the similarities between text queries and all molecule candidates at the above three levels. Then we assign the coefficients $\\alpha, \\beta$ to these similarities at different levels to obtain the final similarity, retrieving the molecule with the highest similarity. In molecule-text retrieval task, we perform this process in the opposite direction."}, {"title": "IV. EXPERIMENTS", "content": "Our model is trained and evaluated on two datasets: ChEBI-20 dataset [2] and PCdes dataset [3]. The ChEBI-20 dataset contains 33,010 molecules with textual descriptions, divided into training, validation, and test sets in the ratio of 8:1:1. Following previous studies [2], [9], we evaluate the retrieval performance using Hits@1, Hits@10, mean reciprocal rank (MRR), and mean rank. During inference, test samples are retrieved from the entire ChEBI-20 dataset. The PCdes dataset consists of 15,000 molecule pairs from PubChem [1], split in the ratio of 7:1:2. After excluding 8 molecules whose SMILES strings can not be converted into 2D graphs using RDKit [34], 14,992 instances remain for our experiments. Following previous works, retrieval performance is evaluated using Recall at 1/10 (R@1, R@10), MRR, and mean rank. Previous pretrained studies [3], [7], [8], [10], [12], [14] often utilize finetuning or directly inference at zero-shot settings on the PCdes dataset. Since our approach does not involve pretraining, we train our model from scratch on the PCdes training set and evaluate its performance on the test set."}, {"title": "B. Experimental Settings", "content": "During model training, we utilize a pretrained SciBERT [26] as the text encoder with a maximum text length of 256. We employ a 3-layer GCN as the graph encoder with an output dimension of 300. We use the Adam optimizer [35], setting the learning rate of 3e-5 for SciBERT and 1e-4 for the remaining parts of our model. Our training spans 60 epochs and uses a batch size of 32. We assign the coefficients for different levels in training objective as follows: $\\alpha = 0.5, \\beta = 0.2$."}, {"title": "C. Baseline Models", "content": "a) AMAN [9]: It leverages the SciBERT to encode textual descriptions and a Graph Transformer Network (GTN) to encode molecules. Besides, it introduces adversarial learning and triplet loss to align these modalities.\nb) Atomas [10]: This model is pretrained on large-scale text and SMILES data, aligning textual and molecular modalities at three granularities. Atomas utilizes the clustering approach to align textual descriptions and molecules at three granularities, while our model adopts optimal transport and contrastive learning to achieve alignments at three levels. It inferences at zero-shot settings for evaluating retrieval performance on PCdes. Additionally, on the ChEBI-20 dataset, we train Atomas from scratch using the same configuration as ours to evaluate its retrieval capabilities."}, {"title": "D. Main Results", "content": "We present experimental results on the ChEBI-20 dataset for both retrieval tasks, as detailed in Table I. In the text-molecule retrieval task, our model significantly outperforms all other baseline models, achieving a Hits@1 of 66.5%, Hits@10 of 93.9%, and an MRR of 0.772. Although our mean rank is 18.53 that is not the best, it is competitively close to the best-performing model, Atomas-base, which obtains a slightly better mean rank of 14.49 but lower Hits@1 and MRR.\nIn the molecule-text retrieval task, our model also demonstrates superior performance, leading in all metrics with a Hits@1 of 61.6%, Hits@10 of 93.8%, an MRR of 0.739, and an impressively low mean rank of 8.10. The experimental results confirm the effectiveness of our model in these retrieval tasks, providing valuable insights for further enhancements.\nWe also present experimental results on the PCdes dataset for two retrieval tasks, as detailed in Table II. Notably, our model, trained from scratch on this dataset, significantly outperforms existing models at both zero-shot and finetuning settings. In the text-molecule retrieval task, our model achieves an impressive R@1 of 64.8%, remarkably better than the best zero-shot model (Atomas-large at 49.1%) and finetuning model (MolFM at 29.8%). It also achieves an R@5 of 82.3%, R@10 of 86.3%, and MRR of 0.727. In the molecule-text retrieval task, our model demonstrates superior results with an R@1 of 62.1%, R@5 of 81.4%, and R@10 of 86.3%, alongside MRR of 0.710. This exceptional performance highlights the potential of our model to surpass generalized pretrained models in both retrieval tasks, confirming its effectiveness."}, {"title": "E. Ablation Study", "content": "We conduct an ablation study on the ChEBI-20 dataset for both retrieval tasks, providing insights about the benefits of alignments at different levels: token-atom (TA), multitoken-motif (MM), and sentence-molecule (SM). From Table III, we can clearly observe that the use of all three levels leads to the best results, confirming the effectiveness of the multi-level alignments. Conversely, the results are worse when only two levels of alignments are employed. Also, relying on one level of alignment alone further diminishes the performance. These results highlight that alignment at each level uniquely contributes to the overall performance."}, {"title": "F. Case Study", "content": "1) Retrieval Performance: We select several typical samples from the test set to demonstrate the retrieval capability of our model. As illustrated in Figure 6, our model accurately retrieves the ground truth molecules as the top-1 candidate for both queries. The molecules retrieved in the top-k candidates closely resemble the text queries, confirming the effective alignments between textual descriptions and molecules. As shown in Figure 7, our model also correctly identifies the ground truth description as the top-1 retrieval result for a given molecule query. Although the textual descriptions of the top-k retrieved candidates vary, their corresponding molecule structures are remarkably similar. This suggests that our model accurately grasps the alignments between text and molecules, highlighting its superior performance.\n2) Visualization for the Alignments of Motifs and Multi-tokens: We also visualize the fusion of multiple motif-aligned tokens through optimal transport theory in Figure 5. On the left side of the figure, atoms marked in the same color serve as the motifs extracted by our model. On the right side, the textual descriptions highlighted in the same colors represent the identified multi-tokens corresponding to the motifs of matching colors on the left. For instance, we use (R)-nephthenol and L-histidinol as examples, where the motifs extracted by our model match the identified multi-tokens. This comprehensively demonstrates that the optimal transport is helpful for the alignments between the multi-tokens and corresponding motifs."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose an Optimal Transport-based Multi-grained Alignments model (ORMA), designed to enhance the cross-modal text-molecule retrieval task. By utilizing SciBERT, we encode textual descriptions into token and sentence representations. Simultaneously, we represent the molecule as a heterogeneous graph containing atom, motif, and molecule nodes, encoded by our GCN molecule encoder. Innovatively, we consider the alignments between tokens and motifs as the optimal transport problem, fusing token representations into motif-aligned multi-token representations. Furthermore, we employ contrastive learning to align textual descriptions and molecules at three levels: token-atom, multitoken-motif, and sentence-molecule. To the best of our knowledge, our work is the first attempt to consider representations at the motif and multi-token granularities. Experimental results on the ChEBI-20 and PCdes datasets demonstrate the superior retrieval performance of our model compared to previous state-of-the-art models. In the future, we will extend ORMA to integrate more modal data sources, such as protein structures and cellular images, to further the application of ORMA in more complex biological systems."}]}