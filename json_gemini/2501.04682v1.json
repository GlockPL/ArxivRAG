{"title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought", "authors": ["Violet Xiang", "Charlie Snell", "Kanishk Gandhi", "Alon Albalak", "Anikait Singh", "Chase Blagden", "Duy Phung", "Rafael Rafailov", "Nathan Lile", "Dakota Mahan", "Louis Castricato", "Jan-Philipp Fr\u00e4nken", "Nick Haber", "Chelsea Finn"], "abstract": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.", "sections": [{"title": "1. Introduction", "content": "A key aspect of the current era of Large Language Models has been the foundational principle of next-token prediction (Elman, 1990; Jordan, 1997). That is, tokenizing text (or other continuous modalities) into a discrete sequence in the following way:\n\"The quick brown fox jumps over the lazy dog.\" \u2192 y1, y2, \u2026\u2026\u2026, \u0423\u043f,\nwhere yi are elements of some finite vocabulary and, subsequently, train a large parameterized neural network p\u00e5 (transformer) model with the following maximum likelihood objective:\n$L_\\theta = E_{Dtrain}[\\sum_t log P_\\theta(y_{t+1}|y_{\\leq t})]$.\nBehind this approach is a simple principle often abbreviated as \"compression is intelligence\", or the model must approximate the distribution of data and perform implicit reasoning in its activations in order to predict the next token (see Solomonoff Induction; Solomonoff 1964). That is, language models learn the implicit meaning in text, as opposed to the early belief some researchers held that sequence-to-sequence models (including transformers) simply fit correlations between sequential words.\nA fundamental question, however, is: What is the relationship between the complexity of the underlying data stream and the model's capability to learn the data-generating algorithm? While natural language has some irreducible entropy, this question holds even in deterministic scenarios (as demonstrated below). Mathematical reasoning is a good domain to demonstrate and evaluate these questions. One can ask an LLM to generate a response to questions like:\n\"What is 1+2?\"\nto which most immediately answer \u201c3\u201d. We can increase the complexity of the question by asking, for example:\n\"What is the value of\n$\\frac{(x^2 - 1)(x + 1)}{x^3 - x} - \\frac{1}{x}$\nevaluated at x \u03c0?\"\nThis is actually a simple question since the expression is canonically equal to 1, yet even powerful LLMs such as GPT-40 and Claude fail to answer it correctly even once. That is, the true conditional generative process p(yt+1|yt) could have arbitrarily high computational complexity even when it is deterministic as in the example above. In contrast, if we instruct models to \u201cthink step by step\" and produce a chain-of-thought we can significantly improve their capabilities producing traces like the following:"}, {"title": "1.2. Outline", "content": "In this paper, we investigate the limitations of current LLMs in handling complex reasoning tasks and propose a novel framework, Meta Chain-of-Thought (Meta-CoT), to address these shortcomings. We argue that traditional Chain-of-Thought (CoT) methods, while effective for simpler problems, fail to capture the true data-generating process of complex reasoning which often involves a non-linear, iterative, and latent process of exploration and verification. Meta-CoT extends CoT by explicitly modeling this latent \u201cthinking\u201d process, which we hypothesize is essential for solving problems that require advanced reasoning capabilities.\nWe draw inspiration from Cognitive Science's dual-process theory, framing Meta-CoT as a form of System 2 reasoning. We establish the theoretical foundations of Meta-CoT, demonstrating how it can be realized through systematic search processes, and how these processes can be internalized within a single auto-regressive model. We then present empirical evidence supporting our claims, including analyses on state-of-the-art models like OpenAI's 01 (OpenAI, 2024) and DeepSeek-R1 (DeepSeek, 2024), which exhibit behaviors consistent with internalized (in-context) search. We further explore methods for training models on Meta-CoT through process supervision, and synthetic data generation via search algorithms like Monte Carlo Tree Search (MCTS) and A*.\nFinally, we outline a concrete pipeline for achieving Meta-CoT in a single end-to-end system, incorporating instruction tuning with linearized search traces and reinforcement learning (RL) post-training. We discuss open research questions, including the scaling laws of reasoning and search, the role of verifiers, and the potential for discovering novel reasoning algorithms through meta-RL. We also present the \u201cBig MATH\u201d project, an effort to aggregate over 1,000,000 high-quality, verifiable math problems to facilitate further research in this area. Our work provides both theoretical insights and a practical road map to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence."}, {"title": "2. Meta Chain-Of-Thought", "content": "In this section, we first formulate the meta chain-of-thought process and discuss how it can describe the problem solving process for complex reasoning problems. Then, we describe and demonstrate why classical chain-of-thought fails under certain circumstances."}, {"title": "2.1. Deriving The Meta-CoT Process", "content": "A question to ask ourselves is: Should language models with Chain-Of-Thought prompting really be able to express any function, and thus solve arbitrarily complex problems, which was the theoretical point of the previous section? We will stick with the mathematical reasoning domain for the purpose of the discussion. Today, the capabilities of frontier models are enough for a large class of mathematical reasoning problems. Current state-of-the art systems such as GPT-40 and Claude largely solve the Hendrycks MATH Levels 1-3 Benchmark (Hendrycks et al., 2021), however, they still struggle with advanced problems such as those in Levels 4 and 5, HARP (Yue et al., 2024) and Omni-MATH (Gao et al., 2024) (as well as other advanced reasoning tasks). We put forward the following theory to explain these empirical observations.\nReasoning data present in pre-training corpuses does not represent the true data generation process, especially for complex problems, which is a product of extensive latent reasoning. Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion."}, {"title": "2.2. Why Does (Classical) CoT Fail?", "content": "Based on the previous discussion, a natural question follows: Why do LLMs fail at these advanced reasoning tasks? Above we proposed that the pre-training and instruction-tuning corpora consist of data of the type (q, s1, . . ., Sn, a), which do not contain the true data generating process as shown in Equation 1. Indeed, the solution to the windmill problem above is widely available on the internet, but there is little to no discussion about the ways in which commonly used convex hull or planar graph arguments fail. This is true in general - textbooks contain advanced proofs but not the full thought process of deriving these proofs. We can then apply the same general meta-argument of why CoT is necessary to the Meta-CoT case: simply because the conditional solution-level distribution Pdata (a, S1, ..., sn|q) (without the intermediate Meta-CoT) on hard reasoning questions can have arbitrarily high complexity in the same way that pdata(a|q) can have arbitrarily high complexity in the standard CoT setting. We will examine some empirical evidence for our stance in the following sections.\nWe will argue in the following chapters that the OpenAI o1 model series performs full Meta-CoT reasoning in an auto-regressive fashion at inference time. A useful analysis is presented in a new mathematics benchmark with challenging high-school Olympiad-level problems (Yue et al., 2024). Figure 1 sourced from that work shows the relevant results. First, we see that the o1 family of models significantly outperforms \"standard\" reasoning models across the board. However, the gap between 01 and other models' performance increases on higher difficulty problems (with the interesting exception of the LLaMa 3.1 models), that is, problems which have higher solution complexity.\nFurthermore, the bottom half of Figure 1 shows the average number of tokens generated grouped by problem difficulty level. First, we see that outside of the o1 series of models, LLMs generate solutions of comparable lengths to humans. While this may initially appear quite intriguing, suggesting that models are learning to approximate or replicate human reasoning, the simple explanation is that models are learning solutions to match the training data - i.e. Pdata(a, s1,...,sn|q). Much more intriguingly, the o1 series of models exhibits significantly different token behavior. We see that:\n1. On level 1 problems the o1 series generates a comparable number of tokens to human-written solutions. These are the types of problems where the training solutions likely match the true data generation process and each individual logical step can be internalized in a constant-depth transformer.\n2. At higher difficulty, the o1 series of models generates significantly more tokens per problem and also widens the performance gap over the classical reasoning models. In fact the gap between the inference compute used by the o1 model and prior series of models seems to scale with the complexity of the problems. We hypothesize that in those more challenging problems the solutions do NOT in fact represent the true data generative process, which is instead better approximated by the more extensive Meta-CoT generated by the o1 family of models.\nOf course, in practice the distinction between these two is not so clear cut, and in fact the constant-depth transformer can likely internalize part of the Meta-CoT generative process as evidenced by the gradation of (Meta-)CoT lengths from Levels 2-6 in Figure 1. In the next chapter we will discuss in greater detail what the Meta-CoT process actually represents."}, {"title": "3. Towards Deliberate Reasoning With Language Models - Search", "content": "In the previous section we introduced the Meta-CoT process and argued that LLMs fail on advanced reasoning tasks because the training data does not adequately represent the true data generation"}, {"title": "3.1. Inference-Time Compute: Search", "content": "The first point above (generation-verification gap) has recently become a popular research and discussion direction under the framework of \u201cdeploying inference-time compute\u201d and we explore this in our first experiment. We start with a LLaMa 3.1 8B base model (Dubey et al., 2024) and carry out extensive supervised fine-tuning on the Numina MATH dataset (LI et al., 2024). Refer to Figure 2 for results and Section 8.1 for dataset details. For each intermediate checkpoint we evaluate performance on the Hendrycks MATH (Hendrycks et al., 2021) 500 problems evaluation dataset (Lightman et al., 2023). Based on the results, we make a few observations here:\n1. We evaluate pass@k (i.e. using an oracle verifier) on intermediate checkpoints and see a significant jump in performance for increasing k. While zero-shot performance with greedy"}, {"title": "3.2. Inference-Time Compute: Verification", "content": "Several works focus on training verifier models, which explicitly evaluate the correctness of reasoning steps and solutions. Verifiers can be trained either using explicit binary classification (Cobbe et al., 2021; Lightman et al., 2023; Snell et al., 2024; Anonymous, 2024; Setlur et al., 2024b) or modeling"}, {"title": "3.3. From Best-of-N To General Search", "content": "So far, we empirically explored best-of-N approaches, generating multiple full solutions independently and selecting the most promising one based on scores. However, this approach is inefficient because it requires exploring full solution paths, even if a mistake occurs early on, and may repeatedly sample the same correct steps. Instead, we can model reasoning as a Markov Decision Process (MDP), defined by the tuple M = (S, A, P, R, \u03b3), where:\n\u2022 S: the set of states, where each state S \u2208 S, consists of the prompt and generations so far, i.e. St = (q, $1,..., St).\n\u2022 A: the set of actions, where each action a \u2208 A will be represented as the next reasoning step St+1.\n\u2022 P(s' | s, a): the transition probability function, representing the probability of transitioning to state s' when taking action a in state s. For simplicity, we will mostly consider the deter-"}, {"title": "3.4. Is Search (Inference Time Compute) A Fundamental Capability Shift?", "content": "As pointed out earlier, the question remains whether inference-time search is a fundamental new capability or whether it is accessible with additional training. Results from classical RLHF tuning (Dubois et al., 2024) suggest that this is a learnable capability, where zero-shot performance of post-trained models matches or outperforms the best-of-N paradigm.\nWe stipulate that performance on complex reasoning tasks is governed by a scaling law, which involves model size, training data (compute) and inference time compute.\nThis is indeed consistent with the theoretical results of Li et al. (2024) and the intuition presented in Section 2. Larger models are more capable of internalizing the Meta-CoT process in their activations, and are also capable of using longer inference-time Meta-CoT to approximate solutions with significantly higher computational complexity. Empirically, we have limited (but promising) evidence towards this hypothesis. A major prior work to study these questions is Jones (2021) which carries out studies using the AlphaZero algorithm (Sil-"}, {"title": "4. Towards Meta-CoT Reasoning", "content": "In prior sections we: introduced the concept of Meta-CoT and argued that it is necessary for advanced reasoning, discussed the generator-verifier gap as a fundamental limitation, argued for search as a fundamental building block of the Meta-CoT, and discussed the utility of approaches integrating generator, verifier, and search components. However, the question remains on how to integrate these into a model to perform Meta-CoT or \u201cSystem 2\u201d reasoning. The first question we need to answer is: why do we actually need to internalize deliberate reasoning inside a single model? We propose two main reasons:\n1. Efficiency: By incorporating search within the context of an auto-regressive model, exploration can be done efficiently since the model has access to all previously visited nodes, in context. Unique to the case of reasoning in natural language, many branches may contain semantically similar content, unlike other domains (e.g., board games), motivating the need for improved efficiency. In fact, even advanced reasoning models carry out many repeated steps of semantically identical reasoning as we show in Figure 14 and 15.\n2. Super-Intelligence: If an auto-regressive model can learn to implement search algorithms in-context, then additional RL training may enable the model to discover novel reasoning approaches. Essentially, we propose that training a model capable of internal System 2 reasoning (e.g. Meta-CoT) and search is an optimization over algorithms rather than specific outputs, possibly yielding novel modes of problem solving. This will potentially allow the model to solve classes of problems previously unsolvable under symbolic-bases tree-search approaches as we've outlined in Sections 3.3 and 3.4.\nIn the remainder of this section, we explore how to train a model to internalize such a reasoning system."}, {"title": "4.1. Bootstrapping Meta-CoT", "content": "In this subsection, we overview the core idea behind the Self-Taught Reasoner (STaR) approach (Zelikman et al., 2022; Singh et al., 2024; Yuan et al., 2023) to bootstrapping intermediate CoT steps and how to use a similar concept to generalize to meta-reasoning strategies."}, {"title": "4.1.1. Self-Taught Reasoner", "content": "The STaR method introduces an iterative bootstrapping approach designed to improve the reasoning capability of LLMs (Zelikman et al., 2022). STaR focuses on training models to generate and refine"}, {"title": "4.1.2. Meta-STaR", "content": "We can generalize the above idea to Meta-CoT in a straightforward way. Consider a base policy \u03c0\u03b8 combined with some general search procedure over intermediate steps. Given a question q we perform the search procedure repeatedly to generate search traces z1, . . ., zk until we find a final solution (s1,..., sn). If we can verify the final produced solution v(S) \u2192 {0,1}, for example by using a formalization and verification approach (as in AlphaProof) or some other outcome verification, we can then apply a similar approach to STaR. For example, we can construct a dataset DSTaR = {q(i), Z(i), \u015c(i)}\u2081 and use a similar training objective as before:\nLMeta-STaR(\u03c6) = \u2212E(q,\u017b,\u015c)\u223cDstar [log \u03c0\u03c6(\u015c, \u017b|q)].\nEssentially, we can use a base policy and search procedure to generate synthetic search data and then train the model to implement these in-context through the Meta-CoT concept. We are effectively proposing to linearize the search approaches described in Section 3 and teach an auto-regressive model to run them sequentially. So far we have deliberately been vague about how these search procedures and datasets look. We will now provide examples and proof of concept from the literature on practical approaches to this problem as well as synthetic examples of realistic training data."}, {"title": "4.2. Empirical Examples Of Internalizing Search", "content": "When we formulate search in a sequential fashion we can explicitly parameterize each component in language, or choose leave it implicit (Gandhi et al., 2024). Note that models trained with standard next token prediction still need to implicitly internalize all of these components anyway in order to accurately model the search sequence, even if they are not explicitly verbalized. However, allowing the model to vocalize it's certainty or estimated progress could allow for additional modeling capacity or be useful for interpretability purposes. We will present some examples of auto-regressive search procedures from the literature in the following section."}, {"title": "4.2.1. Small-Scale Empirical Results on Internalizing Search", "content": "Two particular prior works that explore the idea of in-context search are Yang et al. (2022) and Lehnert et al. (2024) which focus on mazes and other classical RL environments. The formulation from Lehnert"}, {"title": "4.2.2. In-context Exploration For LLMs", "content": "While the prior section showed promise in teaching auto-regressive language models to internalize complex search strategies involving exploration and backtracking, it remains unclear whether these results can generalize to realistic language domains. In this section we will overview several recent works, which show promise in internalizing episode-level search. Both Qu et al. (2024) and Snell et al. (2024) evaluate results using open-source LLMs in the 7B and larger range on problems from the MATH dataset (Hendrycks et al., 2021). They pose the problem as sequential sampling - i.e. given a problem q, generating full solutions from the same model auto-regressively as\nSi ~ \u03c0\u03bf(\u00b7|Si\u22121,..., S\u00b9, q)\nwhere S\u00b2 are full solutions to the problem q. Both works formulate the problem as self-correction, or revisions, during training. The approach generates training data by concatenating a number of incorrect solutions with the correct revision and training on a linearized sequence (although the exact training objective use a particular weighting grounded in RL (Peng et al., 2019)). The general objective follows the form\nmin Esref(19),q~Dtrain [-logo (S*|S-1,..., S\u00b9,q)]\nwhere j is a fixed number of in-context exploration episodes sampled from a fixed distribution ref (i.e. \u03c0\u03bf) and S* is some optimal solution. Essentially, this can be considered a linearization of the Best-Of-N search strategy presented in Section 3.1 with rejection sampling. In this setting, the Meta-CoT represents search in full episodes Z = S1, . . ., S-1 and S = S\u00cd. At test time we can further control the quantity of compute by iteratively sampling from\nSi ~ \u03c0\u03bf(\u00b7|Si\u22121,..., Si-j, q)."}, {"title": "4.2.3. Using variable Compute", "content": "While the above approaches demonstrate promise for the model's capability to carry-out in-context search, they are trained with a fixed number of revisions and use a pre-determined number of revisions at test time. This is not ideal, as ideally the model would be able to use arbitrary amounts of compute until it arrives at a solution with high enough confidence. We repeat the above experiment using a uniform number of in-context solutions during training (ranging between 0-7), allowing the model to generate up to 8 solutions at inference time by optimizing"}, {"title": "4.2.4. Backtracking in LLMs", "content": "In the prior sections, we reviewed evidence that auto-regressive models can internalize complex search strategies in simple domains. We also showed that LLMs can learn in-context exploration at the episode-level. However, whether models can implement complex search strategies (e.g. those outlined in Section 3) auto-regressively remains an open question in public research. Specifically, we refer to the ability to terminate a reasoning chain prior to completion, and the ability to reset (semantically) to an arbitrary previously visited state in-context. These two steps can be unified under the concept of backtracking. Here we will review some recent works demonstrating that LLMs can learn to backtrack.\nRecent works have demonstrated that training on data with backtracking can improve language models on simple reasoning tasks (Ye et al., 2024b; Anonymous, 2024) find that language models can sometimes \u201crecognize\u201d their errors internally, but do not have the required mechanisms to self-correct."}, {"title": "4.3. Synthetic Meta-CoT Via Search", "content": "In the prior sections we argued for an approach to reasoning that teaches an LLM to internalize an auto-regressive search procedure in-context. We also reviewed several recent works showing that small auto-regressive models can carry out in-context exploration at the episode level, and larger models can learn individual step backtracking. In this section, we explore how to construct synthetic data for realistic Meta-CoT that involves full-scale in-context tree search.\nFor demonstrative purposes, we use the math problem presented by OpenAI (2024) as our benchmark task, where Gemini 1.5 Pro (Reid et al., 2024) achieves a Pass@128 score of 6.25% (8/128 correct) \u2013 notably being the only frontier model (without advanced reasoning) to demonstrate non-zero performance at the time of our experiments. We use the same RL formulations for state and actions as presented in 3.3. We explore two principal search algorithms for generating synthetic"}, {"title": "4.3.1. Monte-Carlo Tree Search", "content": "We conduct an example based on Monte-Carlo Tree Search (MCTS), which seeks to balance exploration and exploitation. The MCTS im- plementation of Silver et al. (2018) has been widely applied to the reasoning domain (Tian et al., 2024; Feng et al., 2024), and we mostly follow their implementation with some modifi- cations to account for the structure of our search problem (see Appendix D).\nWe present the search trace for our exam- ple problem - all the actions taken during the search (i.e., the Meta-CoT in a linear format) in Appendix E. The numbers following each reasoning step represent the value estimates. In our initial MCTS attempt we obtained a trace with an excessive number of backtracks and repetitions, including from high-value states (as"}, {"title": "4.3.2. A* search", "content": "We begin with an exploration of a type of best-first search based on the work by Koh et al. (2024), which itself loosely follows an A* approach. The search procedure maintains a frontier F of states, which is implemented as a max priority queue. Similarly to the MCTS approach, each state St consists of the question q and a partial solution consisting of generated reasoning steps (s1, ..., st). At each iteration, the state Sp \u2190 pop(F) with the highest value vp = v(Sp, q) is selected, where vp \u2208 [0,1] is the value of the partial solution Sp including current and previous reasoning steps. At each node the policy \u03c0 proposes b candidate next steps, each of which is evaluated by v and added to F if the depth of the tree |(so, . . ., sp)| has not reached the maximum depth search limit d. For the purpose of generating synthetic data, we run the search until we find a solution that is correct using the ground-truth verifier. The resulting tree is shown in Figure 13. It shows more consistent flow of the"}, {"title": "4.4. Do Advanced Reasoning Systems Implement In-Context Search?", "content": "In this section we will investigate whether ad- vanced reasoning systems, such as OpenAI's 01 (OpenAI, 2024), DeepSeek R1 (DeepSeek, 2024) and Gemini 2.0 Flash Thinking Mode and the Qwen QwQ Team (2024) implement in-context search. We provide successful rea- soning traces for the same math problem in Appendix E.\nStarting with OpenAI's o1 model, by care- fully examining the provided mathematical rea- soning trace, we observe:\n1. Inconsistent flow of thought - consecutive steps do not logically continue the prior state.\n2. Backtracking - the model carries out \"se- mantic backtracking\" - frequently return- ing to the same logical points.\n3. Repetition - the model often repeats log- ical steps.\nThe qualitative behaviors observed in o1 (Figure 14 left) are similar to those in the example synthetic trace (Figure 15) generated by Gemini 1.5 with and MCTS-like search processes. In particular, there are abrupt changes in logical flow of the (Meta) CoT, which is natural as the model backtracks between branches of the tree. Moreover, the model may explore multiple child nodes of the same parent which are different strings, but can also be very semantically similar leading to repetitive logic. This is clear in the provided trace, as the model repeats logical statement and goes over the same derivations multiple times. Note also that we do not claim the model is implementing tree search at test time, but rather that as much as the model's output are expected to resemble it's training data, we hypothesize that examples of search were used during training (likely model initialization). We will specifically address the need and effects of RL training in Section 6.\nThe DeepSeek R1 model DeepSeek (2024) also exhibits similar behaviors, as shown in Figure 14, however, it also carries out a significant amount of self-evaluation steps. This could be achieved by integrating a form of self-criticism (Madaan et al., 2023; Shinn et al., 2023) or a generative verifier (Zhang et al., 2024a) in the search trace. The LATS framework (Zhou et al., 2024a) uses a similar approach, combining MCTS search with self-criticism and shows empirical improvements from self-reflection. Another alternative for synthetic data generation is the \"Iteration-Of-Thought\" approach Radha et al. (2024) which also interleaves generation with inner dialogue. This would explain the rather smooth logical flow of the R1 model, which does not exhibit as much abrupt back-tracking, as compared to O1. As mentioned earlier, in order to adequately model the search"}, {"title": "5. Process Supervision", "content": "A key component of the search approaches presented in prior sections is the evaluation function v(q, St), which scores intermediate states in a reasoning chain. These evaluation functions have become widely known as Process Reward Models (PRM). By incorporating process supervision, the search mechanism gains the flexibility to backtrack to earlier promising states when suboptimal paths are encountered, thereby enabling more effective exploration. However, the question of how to efficiently access such capabilities remains an open question. In Section 4.3 we showed examples of using outcome-based verification with MCTS in combination with Monte-Carlo rollouts. However, this approach can only be used during training due to the necessity for ground-truth answers, and moreover it is extremely computationally inefficient. As mentioned earlier, a single training example requires up to 20 million inference tokens, costing up to hundreds of dollars. It is significantly more efficient to amortize the evaluation procedure into a single parameterized model, and we will outline strategies for building such process guidance models below."}, {"title": "5.1. Learning Process Reward Models", "content": "Parameterized PRMs are built on top of pre-trained models, either using a linear head or the logits of specific tokens. The model takes the question q and a partial solution St as input and outputs a single scalar value ve(q, St) \u2192 [0, 1]. Given a dataset Dtrain of partial solutions St and corresponding value targets ys, the model is generally optimized with a standard cross-entropy classification loss. A central question for training PRMs is: where do the supervision labels ys, come from? One approach is to have human annotators provide step-by-step level evaluation of reasoning problems, as done by Lightman et al. (2023). While their work showed promise in terms of empirical results, this method is challenging to scale due to the high annotation time and cost, especially as evaluating hard reasoning problems requires high-caliber experts. An alternative approach presented by Wang et al. (2024) only relies on access to outcome verification - i.e. problems with a ground truth answer. The proposed approach is to amortize the Monte Carlo state-value estimation into a parameterized function. Essentially, this method fits an empirical value function of the reference rollout policy where the targets ys, are represented by Equation 11. This idea has been widely adopted in follow-up works (Snell et al., 2024; Anonymous, 2024) and further extended (Setlur et al., 2024c)."}, {"title": "5.2. PRM Quality And Its Effect On Search", "content": "The performance and efficiency of search at test-time depends on the quality of the PRM (Setlur et al., 2024b; Anonymous, 2024). Setlur et al. (2024b) demonstrate effective scaling (in both training data size and label quality) of a specific variant of PRMs that estimate values based on the improvement in likelihood of the correct answer after a step. The accuracy of test-time search improves log-linearly with training data size, and the quality of learned value labels improve with more Monte Carlo estimates. Anonymous (2024) show that oracle verifier-enabled search is orders of magnitude more efficient than a learned PRM with noisy value estimates.\nIn this section we conduct an experiment demonstrating the scaling characteristics of a PRM. To train our PRM, we first need to generate diverse solution trajectories where each solution step is annotated with a ground truth value. To do so, we use the method from Wang et al. (2024) to obtain ground truth values, performing 16 Monte Carlo (MC) rollouts for each step of a seed solution. We generate the seed solutions and step-level MC rollouts from a supervised finetuned (SFT) Llama3.1-8B using the PRM800K (Lightman et al., 2023) dataset. The PRM training data uses 7,086 unique questions - each with seed solutions - and after removing duplicate seed solutions results in 97,000 trajectories in the training data. To evaluate the scaling performance with increasing data, we split the small set of data into three subsets: one with 500 unique questions, one with 3,000 unique questions, and one with all 7,086 unique questions. We create an evaluation set using the MATH-500 dataset (Hendrycks et al., 2021; Lightman et al., 2023) by generating step-by-step solutions from the SFTed model and step-level ground truth values from 128 MC rollouts.\nWith this trained PRM, we find a reduction in the absolute error of predicted values when comparing PRMs that are trained across datasets of different sizes, as well as a selection of intermediate checkpoints in Figure 16. We observe that: 1) the prediction error decreases as the size of the training data increases, and 2) when the size of the dataset is small, improvement converges early during training (around 30% of an epoch for Qs=500 and Qs=3000). Although these findings are based on small-scale experiments, we anticipate continued improvement in prediction errors with larger datasets and more extensive training, suggesting significant potential in further refining and scaling PRMs. Additionally, we evaluate the performance of the three fully-trained PRMs as outcome verifiers when performing a Best-of-N search during inference time. Figure 17 left shows that the PRM's ability to verify full solutions improves as they are trained with more data, yet there exists a remarkable gap between the trained PRMs and an oracle PRM. Additionally, we observe that the PRM's ability to guide the search process towards the correct answer with a more efficient path also improves as the increased accuracy and reduced number of tokens used in the search process are both observed in Figure 17 right. One interesting remaining question is: what is the scaling law for these process supervision models?"}, {"title": "5.3. Verifiable Versus Open-Ended Problems", "content": "Training a value function with MC rollouts is scalable with infrastructure and inference, but is funda- mentally limited to problems with verifiable solutions. This excludes proof problems and scientific derivations which are often more important than the numerical answer itself. While automated proof assistance is an established area of research in mathematics (mathlib Community, 2020), this is rather limiting. First of all, these methods are largely limited to math and do not transfer to other domains such as science or more general problem-solving scenarios. In those domains, training a PRM based on human evaluations of valid reasoning steps could yield a general verifier, which can be used for assuring the validity of the proof/solution chain. This would explain the need for human annotators and verification."}, {"title": "6. Meta Reinforcement Learning - Learning How To Think", "content": "In this section we will build out an interpretation of the reasoning problem and Meta-CoT from the"}]}