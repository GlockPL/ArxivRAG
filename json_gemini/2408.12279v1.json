{"title": "Developing vocal system impaired patient-aimed voice quality assessment\napproach using ASR representation-included multiple features", "authors": ["Shaoxiang Dang", "Tetsuya Matsumoto", "Yoshinori Takeuchi", "Takashi Tsuboi", "Yasuhiro Tanaka", "Daisuke Nakatsubo", "Satoshi Maesawa", "Ryuta Saito", "Masahisa Katsuno", "Hiroaki Kudo"], "abstract": "The potential of deep learning in clinical speech processing\nis immense, yet the hurdles of limited and imbalanced clinical\ndata samples loom large. This article addresses these challenges\nby showcasing the utilization of automatic speech recognition and\nself-supervised learning representations, pre-trained on extensive\ndatasets of normal speech. This innovative approach aims to esti-\nmate voice quality of patients with impaired vocal systems. Exper-\niments involve checks on PVQD dataset, covering various causes\nof vocal system damage in English, and a Japanese dataset fo-\ncusing on patients with Parkinson's disease before and after un-\ndergoing subthalamic nucleus deep brain stimulation (STN-DBS)\nsurgery. The results on PVQD reveal a notable correlation (>0.8\non PCC) and an extraordinary accuracy (<0.5 on MSE) in predict-\ning Grade, Breathy, and Asthenic indicators. Meanwhile, progress\nhas been achieved in predicting the voice quality of patients in the\ncontext of STN-DBS.\nIndex Terms: speech quality assessment, self-supervised learn-\ning, ASR representation, transfer learning, GRBAS", "sections": [{"title": "1. Introduction", "content": "Auditory-perceptual judgment is the primarily subjective ap-\nproach for assessing the vocal system condition in clinical set-\ntings [1, 2]. It requires experienced speech pathologists or doc-\ntors to comprehensively evaluate sustained vowels and running\nspeech adhering to GRBAS scale. GRBAS scale is a hoarseness\nevaluation method, it specifically refers to Grade (equivalent to\noverall severity), Rough, Breathy, Asthenic, and Strained five as-\npects [3]. Auditory-perceptual judgment is a key means of re-\nvealing signs of vocal pathology and monitoring speech disorders\nafter intrusive treatments, for example, subthalamic nucleus deep\nbrain stimulation (STN-DBS) for Parkinson's disease (PD) [4, 5].\nAuditory-perceptual judgment, however, presents three inconve-\nniences: first, raters are required to possess extensive clinical ex-\npertise; second, to enhance the rating reliability, multiple raters\nare necessarily involved in judgment; third, the lengthy evaluation\ncycle prevents physicians from promptly obtaining results. These\nconsiderations highlight the necessity of objective estimation.\nAdhering to the principles valued by speech pathologists, the\nmajority of prior work of objective estimation has focused on pre-\ndicting using sustained vowels [6, 7]. However, due to the limited\nrichness of vowels, it is insufficient for deep learning models that\noften rely on a large amount of data to acquire robust features.\nTo this end, evaluations of running speech are being considered\n[8, 9, 10], and [10] strongly demonstrates that deep neural gains\nachieve higher accuracy when trained on continuous speech.\nAn additional longstanding challenge in shifting auditory-\nperceptual judgment towards objective assessment analysis lies\nin the scarcity and imbalance of clinical data, making it difficult\nfor deep learning to extract useful features. Self-supervised learn-\ning (SSL) is an efficient unsupervised pre-training technique that\nhas achieved success in many fields [11, 12, 13, 14, 15, 16]. The\nresearch [10] that investigates pre-training via SSL, and subse-\nquently using it as a feature extractor, has effectively provided\nvaluable insights for addressing this issue.\nDuring data analysis, we noticed that comprehension is often\nrelevant to the quality of patient speech. In particular, we hypoth-\nesize a positive correlation between automatic speech recognition\n(ASR) accuracy and speech quality [17, 18]. Taking into account\nthe non-linear perception of frequency in auditory-perceptual\njudgment, we propose a model utilizing mel-spectrogram, ASR\nrepresentation, and self-supervised learning (SSL) representation\nfor clinical speech quality assessment [19, 20]. Unlike previ-\nous studies concentrating on a single indicator in GRBAS scale\n[10, 21, 22], this article also delves into an estimation of all GR-\nBAS indicators. Additionally, we conduct experiments to explore\nthe Grade in patients of native Japanese undergoing STN-DBS.\nBased on experimental results, the proposed method demonstrates\nimproved accuracy and robust predictions."}, {"title": "2. Proposed methods", "content": "The proposed model consists of two parts: feature extraction and\ndownstream modules. Feature extraction is responsible for obtain-\ning features of ASR representation, SSL representation, and mel-\nspectrogram, the latter module maps features to GRBAS scales.\nFig. 1 illustrates an overview of the proposed method."}, {"title": "2.1. Feature extraction", "content": "We employ three types of features for voice quality prediction.\nThe first two are ASR and SSL representations, each pre-trained\nby the Whisper and HuBERT models respectively [19, 23]. Both\nmodels stack 12 transformer encoder layers. To fully utilize rep-\nresentations at different depths, representations from the deeper 6\nlayers of each model undergoes an adapter architecture.\nThe adapter architecture comprises a fully connected layer, a\nLeakyReLU layer, and a LayerNorm layer. The outputs of the\nsix adapters are assigned learnable weights that collectively sum\nto 1, and then these weighted outputs are added together. The\nthird feature is the mel-spectrogram concatenated with its first and\nsecond-order deltas. The mel-spectrogram feature is used to rep-\nresent frequency-level features in auditory-perceptual judgment."}, {"title": "2.2. Downstream", "content": "It is a consensus that the downstream module does not necessar-\nily need to be very complex when using pre-trained models for\nfeature extraction. Therefore, in the downstream model we de-\nsigned, we employ two long short-term memory (LSTM) layers\nfor sequence processing, followed by a fully connected layer to\nmap the feature dimensions to the GRBAS dimensions. Finally,\nwe perform average pooling along the time dimension."}, {"title": "3. Experiment", "content": "During the validation of the proposed model, two datasets are\nused: Perceptual Voice Qualities Database (PVQD) [24] and STN-\nDBS [4]."}, {"title": "3.1. Datasets", "content": ""}, {"title": "3.1.1. PVQD", "content": "PVQD owns 296 speech samples consisting of sustained vowels\n/a/, /i/, and running speech in English, with each sample represent-\ning an individual case (A small number of samples do not contain\nthe sustained vowel /a/ or /i/). Following the methodology of pre-\nvious studies, sustained vowel /a/ and running speech are extracted\nseparately to create the PVQD-A and PVQD-S datasets [10]. Re-\ngarding running speeches, we randomly segment them into speech\nsegments that last 2 to 4 seconds. Employing the same criteria, we\ndivide PVQD-S and PVQD-A into test, validation, and training\nsets. For instance, the running speech and sustained vowel data"}, {"title": "3.1.2. STN-DBS", "content": "STN-DBS is a common treatment for PD. Vocal disorder is a rec-\nognized side effect of STN-DBS. STN-DBS dataset is collected\nto assess the degree of vocal disorder [4]. The STN-DBS dataset\ncomprises a total of 96 cases of native Japanese individuals. For\nsimplicity, the recording time (e.g., before or three months after\nsurgery) is not considered. Only vowels are used as input, and the\nGrade indicator serves as the output. Furthermore, the averaged\nopinion of raters on Grade is categorized into three intervals: mild\nfor [0,1], moderate for (1,2], and severe for (2,3]. Each case's vo-\ncalization includes multiple occurrences of /a/ followed by each\nof /i/, /u/, /e/, and /o/. In the pre-processing phase, a vocaliza-\ntion utterance is formed by combining one /a/ with /i/, /u/, /e/, and\n/o/. Utterances from the same patient share a single score. This\npre-processing method serves two purposes: the combination of\na single /a/ with other vowels, as the vowel /a/ better highlights\nthe patient's vocal cord state. From deep learning's perspective, it\nhelps in expanding the dataset."}, {"title": "3.2. Model parameters", "content": "We employ Whisper, pre-trained on ASR task, to extract ASR\nfeatures [19]. It's worth noting that Whisper has a padding oper-\nation, which is deemed ineffective in current scope. As a result,\nthis padding is removed before utilizing ASR features. For SSL\nfeatures, we choose the HuBERT pre-trained on LibriSpeech [26]\nfor PVQD experiments, and pre-trained HuBERT for STN-DBS.\nBoth HuBERT and Whisper consist of 12 encoder layers. The\nFC layer in adapter transforms features from 768-dimension to\n120-dimension. The coefficient of LeakyReLU is set to 0.05. The\noutputs of adapters, before being applied, undergo a softmax layer\nto ensure their sum equals 1. The downstream model's FC layer\nowns 1 or 5 output neurons (depending on single-task or multi-\ntask learning), with each neuron corresponding to one of the five\nGRABS scales. Also, the number of output neurons of FC layer in\ndownstream is 3, corresponding to the three categories of Grade.\nThe source code for implementing proposed method and some ex-\nample predictions are available at GitHub."}, {"title": "3.3. Training parameters", "content": ""}, {"title": "3.3.1. Loss functions", "content": "Due to the high consensus among raters in PVQD, we use mean\nabsolute error (MAE) loss for regression learning. For STN-\nDBS, drawing inspiration from [27], we propose a simplified class\ndistance-weighted cross-entropy (SCDW-CE) loss:\nSCDW-CE = -log(\u0177c) \u00d7 |i \u2013 c|   (1)\nwhere i and c denote class index of estimation and ground-truth,\nrespectively. \u0177c represents the probability of predicting the correct\nlabel. || denotes taking the absolute value."}, {"title": "3.3.2. Training", "content": "We hire stochastic gradient descent (SGD) as optimizer. The ini-\ntial learning rate is set to 0.0001, and if there is no improvement\non the validation set for four consecutive epochs, the learning rate\nwill be halved. The batch size is 1. During fine-tuning, the weights\nof the pre-trained modules are not frozen."}, {"title": "3.3.3. Evaluation metrics", "content": "We use mean squared error (MSE), Pearson correlation coefficient\n(PCC), and Spearman rank correlation coefficient (SRCC) met-\nrics to assess the performance of regression models. Additionally,\nfor classification modeling, we report Recall, Precision, and F1\nscores. Both PVQD and STN-DBS undergo pre-processing where\ndata from a patient is segmented into multiple utterances. Hence,\nwe conduct checks at both the utterance and patient levels. During\ncalculation of patient level, in PVQD, the prediction results for all\nutterances belonging to the same patient are averaged, while in\nSTN-DBS, the mode is taken for all utterances of a patient, and in\ncases of multiple modes, the one closest to the mean is selected."}, {"title": "4. Results and discussions", "content": ""}, {"title": "4.1. Results on PVQD", "content": ""}, {"title": "4.1.1. Results of Grade prediction", "content": "Consistent with prior research, the results of solely predicting\nGrade are shown in Table 2. In Grade prediction from running\nspeech, all metrics show improved results except for a slight de-\ncrease in PCC at the patient level. The metric MSEs, in terms"}, {"title": "4.1.2. Results of GRBAS prediction", "content": "The results for predicting all five GRBAS indicators are detailed\nin Table 3. Firstly, in multi-task learning, the results for predict-"}]}