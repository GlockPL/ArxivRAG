{"title": "Developing vocal system impaired patient-aimed voice quality assessment approach using ASR representation-included multiple features", "authors": ["Shaoxiang Dang", "Tetsuya Matsumoto", "Yoshinori Takeuchi", "Takashi Tsuboi", "Yasuhiro Tanaka", "Daisuke Nakatsubo", "Satoshi Maesawa", "Ryuta Saito", "Masahisa Katsuno", "Hiroaki Kudo"], "abstract": "The potential of deep learning in clinical speech processing is immense, yet the hurdles of limited and imbalanced clinical data samples loom large. This article addresses these challenges by showcasing the utilization of automatic speech recognition and self-supervised learning representations, pre-trained on extensive datasets of normal speech. This innovative approach aims to estimate voice quality of patients with impaired vocal systems. Experiments involve checks on PVQD dataset, covering various causes of vocal system damage in English, and a Japanese dataset focusing on patients with Parkinson's disease before and after undergoing subthalamic nucleus deep brain stimulation (STN-DBS) surgery. The results on PVQD reveal a notable correlation (>0.8 on PCC) and an extraordinary accuracy (<0.5 on MSE) in predicting Grade, Breathy, and Asthenic indicators. Meanwhile, progress has been achieved in predicting the voice quality of patients in the context of STN-DBS.", "sections": [{"title": "1. Introduction", "content": "Auditory-perceptual judgment is the primarily subjective approach for assessing the vocal system condition in clinical settings [1, 2]. It requires experienced speech pathologists or doctors to comprehensively evaluate sustained vowels and running speech adhering to GRBAS scale. GRBAS scale is a hoarseness evaluation method, it specifically refers to Grade (equivalent to overall severity), Rough, Breathy, Asthenic, and Strained five aspects [3]. Auditory-perceptual judgment is a key means of revealing signs of vocal pathology and monitoring speech disorders after intrusive treatments, for example, subthalamic nucleus deep brain stimulation (STN-DBS) for Parkinson's disease (PD) [4, 5]. Auditory-perceptual judgment, however, presents three inconveniences: first, raters are required to possess extensive clinical expertise; second, to enhance the rating reliability, multiple raters are necessarily involved in judgment; third, the lengthy evaluation cycle prevents physicians from promptly obtaining results. These considerations highlight the necessity of objective estimation.\nAdhering to the principles valued by speech pathologists, the majority of prior work of objective estimation has focused on predicting using sustained vowels [6, 7]. However, due to the limited richness of vowels, it is insufficient for deep learning models that often rely on a large amount of data to acquire robust features. To this end, evaluations of running speech are being considered [8, 9, 10], and [10] strongly demonstrates that deep neural gains achieve higher accuracy when trained on continuous speech.\nAn additional longstanding challenge in shifting auditory-perceptual judgment towards objective assessment analysis lies in the scarcity and imbalance of clinical data, making it difficult for deep learning to extract useful features. Self-supervised learning (SSL) is an efficient unsupervised pre-training technique that has achieved success in many fields [11, 12, 13, 14, 15, 16]. The research [10] that investigates pre-training via SSL, and subsequently using it as a feature extractor, has effectively provided valuable insights for addressing this issue.\nDuring data analysis, we noticed that comprehension is often relevant to the quality of patient speech. In particular, we hypothesize a positive correlation between automatic speech recognition (ASR) accuracy and speech quality [17, 18]. Taking into account the non-linear perception of frequency in auditory-perceptual judgment, we propose a model utilizing mel-spectrogram, ASR representation, and self-supervised learning (SSL) representation for clinical speech quality assessment [19, 20]. Unlike previous studies concentrating on a single indicator in GRBAS scale [10, 21, 22], this article also delves into an estimation of all GRBAS indicators. Additionally, we conduct experiments to explore the Grade in patients of native Japanese undergoing STN-DBS. Based on experimental results, the proposed method demonstrates improved accuracy and robust predictions."}, {"title": "2. Proposed methods", "content": "The proposed model consists of two parts: feature extraction and downstream modules. Feature extraction is responsible for obtaining features of ASR representation, SSL representation, and mel-spectrogram, the latter module maps features to GRBAS scales. Fig. 1 illustrates an overview of the proposed method."}, {"title": "2.1. Feature extraction", "content": "We employ three types of features for voice quality prediction. The first two are ASR and SSL representations, each pre-trained by the Whisper and HuBERT models respectively [19, 23]. Both models stack 12 transformer encoder layers. To fully utilize representations at different depths, representations from the deeper 6 layers of each model undergoes an adapter architecture.\nThe adapter architecture comprises a fully connected layer, a LeakyReLU layer, and a LayerNorm layer. The outputs of the six adapters are assigned learnable weights that collectively sum to 1, and then these weighted outputs are added together. The third feature is the mel-spectrogram concatenated with its first and second-order deltas. The mel-spectrogram feature is used to represent frequency-level features in auditory-perceptual judgment.\nHuBERT is pre-trained through SSL with quantized MFCC features as the target, better aligning with auditory characteristics. Unlike HuBERT which directly models time-domain data, Whisper is based on 80-dimensional mel-spectrograms. All three features are input to the downstream module by concatenating them along the feature dimension."}, {"title": "2.2. Downstream", "content": "It is a consensus that the downstream module does not necessarily need to be very complex when using pre-trained models for feature extraction. Therefore, in the downstream model we designed, we employ two long short-term memory (LSTM) layers for sequence processing, followed by a fully connected layer to map the feature dimensions to the GRBAS dimensions. Finally, we perform average pooling along the time dimension."}, {"title": "3. Experiment", "content": "During the validation of the proposed model, two datasets are used: Perceptual Voice Qualities Database (PVQD) [24] and STN-DBS [4]."}, {"title": "3.1. Datasets", "content": ""}, {"title": "3.1.1. PVQD", "content": "PVQD owns 296 speech samples consisting of sustained vowels /a/, /i/, and running speech in English, with each sample representing an individual case (A small number of samples do not contain the sustained vowel /a/ or /i/). Following the methodology of previous studies, sustained vowel /a/ and running speech are extracted separately to create the PVQD-A and PVQD-S datasets [10]. Regarding running speeches, we randomly segment them into speech segments that last 2 to 4 seconds. Employing the same criteria, we divide PVQD-S and PVQD-A into test, validation, and training sets. For instance, the running speech and sustained vowel data"}, {"title": "3.1.2. STN-DBS", "content": "STN-DBS is a common treatment for PD. Vocal disorder is a recognized side effect of STN-DBS. STN-DBS dataset is collected to assess the degree of vocal disorder [4]. The STN-DBS dataset comprises a total of 96 cases of native Japanese individuals. For simplicity, the recording time (e.g., before or three months after surgery) is not considered. Only vowels are used as input, and the Grade indicator serves as the output. Furthermore, the averaged opinion of raters on Grade is categorized into three intervals: mild for [0,1], moderate for (1,2], and severe for (2,3]. Each case's vocalization includes multiple occurrences of /a/ followed by each of /i/, /u/, /e/, and /o/. In the pre-processing phase, a vocalization utterance is formed by combining one /a/ with /i/, /u/, /e/, and /o/. Utterances from the same patient share a single score. This pre-processing method serves two purposes: the combination of a single /a/ with other vowels, as the vowel /a/ better highlights the patient's vocal cord state. From deep learning's perspective, it helps in expanding the dataset."}, {"title": "3.2. Model parameters", "content": "We employ Whisper, pre-trained on ASR task, to extract ASR features [19]. It's worth noting that Whisper has a padding operation, which is deemed ineffective in current scope. As a result, this padding is removed before utilizing ASR features. For SSL features, we choose the HuBERT pre-trained on LibriSpeech [26] for PVQD experiments, and pre-trained HuBERT for STN-DBS. Both HuBERT and Whisper consist of 12 encoder layers. The FC layer in adapter transforms features from 768-dimension to 120-dimension. The coefficient of LeakyReLU is set to 0.05. The outputs of adapters, before being applied, undergo a softmax layer to ensure their sum equals 1. The downstream model's FC layer owns 1 or 5 output neurons (depending on single-task or multi-task learning), with each neuron corresponding to one of the five GRABS scales. Also, the number of output neurons of FC layer in downstream is 3, corresponding to the three categories of Grade.\nThe source code for implementing proposed method and some example predictions are available at GitHub."}, {"title": "3.3. Training parameters", "content": ""}, {"title": "3.3.1. Loss functions", "content": "Due to the high consensus among raters in PVQD, we use mean absolute error (MAE) loss for regression learning. For STN-DBS, drawing inspiration from [27], we propose a simplified class distance-weighted cross-entropy (SCDW-CE) loss:\nSCDW-CE = -log(\u0177c) \u00d7 |i \u2013 c|\nwhere i and c denote class index of estimation and ground-truth, respectively. \u0177c represents the probability of predicting the correct label. || denotes taking the absolute value."}, {"title": "3.3.2. Training", "content": "We hire stochastic gradient descent (SGD) as optimizer. The initial learning rate is set to 0.0001, and if there is no improvement on the validation set for four consecutive epochs, the learning rate will be halved. The batch size is 1. During fine-tuning, the weights of the pre-trained modules are not frozen."}, {"title": "3.3.3. Evaluation metrics", "content": "We use mean squared error (MSE), Pearson correlation coefficient (PCC), and Spearman rank correlation coefficient (SRCC) metrics to assess the performance of regression models. Additionally, for classification modeling, we report Recall, Precision, and F1 scores. Both PVQD and STN-DBS undergo pre-processing where data from a patient is segmented into multiple utterances. Hence, we conduct checks at both the utterance and patient levels. During calculation of patient level, in PVQD, the prediction results for all utterances belonging to the same patient are averaged, while in STN-DBS, the mode is taken for all utterances of a patient, and in cases of multiple modes, the one closest to the mean is selected."}, {"title": "4. Results and discussions", "content": ""}, {"title": "4.1. Results on PVQD", "content": ""}, {"title": "4.1.1. Results of Grade prediction", "content": "Consistent with prior research, the results of solely predicting Grade are shown in Table 2. In Grade prediction from running speech, all metrics show improved results except for a slight decrease in PCC at the patient level. The metric MSEs, in terms"}, {"title": "4.1.2. Results of GRBAS prediction", "content": "The results for predicting all five GRBAS indicators are detailed in Table 3. Firstly, in multi-task learning, the results for predicting Grade using running speech persistently outperform those of previous studies, with the error consistently controlled within a range of 0.5 on MSE. Concurrently, the results of using running speech for BA indicators demonstrate robust positive PCC values (all exceeding 0.8) and SRCC values (all exceeding 0.7). Despite the less abundant information compared to running speech, predictions on vowels yield superior results than the baselines, with almost all correlations surpassing 0.5 and MSEs hovering around 0.3. However, the results for R and S are less promising, indicating lower PCC and higher MSEs. Meanwhile, the second column presents inter-class correlations among raters during scoring, with R and S being the two lowest items. We speculate that due to the lower reliability of R and S during scoring, the labels may less accurately reflect the true condition of patients."}, {"title": "4.2. Results on STN-DBS", "content": "The Grade prediction results on the STN-DBS dataset using all vowels are presented in Table 5. The accuracy of utterance level and patient level are 0.437 and 0.529 respectively. From the confusion matrix in Fig. 3, we can infer that the model can effectively distinguish extreme samples, such as mild and severe cases. However, for relatively similar samples (closer in distance), errors become significant, such as between mild and moderate, as well as moderate and severe cases."}, {"title": "4.3. Visualization", "content": "In Fig. 4(a), we present a case from PVQD where, subjectively, the speech signal seems non-pathological, yet its label is 0.5. The OAPVNet, lacking ASR and frequency features, predicts an average of -0.02. In contrast, proposal, incorporating ASR and Mel features, scores 0.39, aligning closer with the label indicating mild symptoms. Notably, this distinction is more evident in the /a/ case, as evidenced by a more sensitive predicted score of 0.82. Fig. 4(b) illustrates an instance labeled as severe but predicted as mild from STN-DBS. Spectrogram analysis suggests suboptimal patient conditions, indicating a loss of normal vocal system functionality (under normal conditions, the patient should pronounce continuous and uninterrupted vowels). This impedes the effective capture of meaningful information. In summary, the physical and mental states of patients pose a formidable challenge in clinical speech quality assessment."}, {"title": "4.4. Discussions", "content": "This paper introduces ASR pre-trained features into clinical speech quality assessment, demonstrating superior performance compared to conventional SSL features. Most importantly, joint usage of ASR, SSL, and Mel features exhibits finer control, especially for normal and mild voices. The benefit of utilizing information-rich running speech is reaffirmed once again. However, voices, in essence, have been used to judge vocal system states so far are indirect information. Therefore, a promising future approach is multi-modal learning that incorporates direct information, such as joint use of perturbation data [28] or laryngoscopic images [29]."}, {"title": "5. Conclusion", "content": "This article introduces a novel method that integrates ASR, SSL, and mel-spectrogram features for clinical voice quality assessment. The model demonstrates improved accuracy, yielding smaller errors on PVQD-S and PVQD-A datasets. Departing from prior studies that solely predict the super-class Grade, our investigation extends to assess across all GRBAS indicators. The findings strongly indicate that the proposal can achieve comparable and even superior accuracy compared to subjective ratings. Notably, we have practically applied the model to patients with PD who are about to undergo or have undergone STN-DBS, showcasing the preliminary predictive capabilities of the proposed method."}]}