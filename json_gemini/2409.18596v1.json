{"title": "ASAG2024: A Combined Benchmark for Short Answer Grading", "authors": ["G\u00e9r\u00f4me Meyer", "Philip Breuer", "Jonathan F\u00fcrst"], "abstract": "Open-ended questions test a more thorough understanding than closed-ended questions and are often a preferred assessment method. However, open-ended questions are tedious to grade and subject to personal bias. Therefore, there have been efforts to speed up the grading process through automation. Short Answer Grading (SAG) systems aim to automatically score students' answers. Despite growth in SAG methods and capabilities, there exists no comprehensive short-answer grading benchmark across different subjects, grading scales, and distributions. Thus, it is hard to assess the capabilities of current automated grading methods in terms of their generalizability. In this preliminary work, we introduce the combined ASAG2024 benchmark to facilitate the comparison of automated grading systems. Combining seven commonly used short-answer grading datasets in a common structure and grading scale. For our benchmark, we evaluate a set of recent SAG methods, revealing that while LLM-based approaches reach new high scores, they still are far from reaching human performance. This opens up avenues for future research on human-machine SAG systems.", "sections": [{"title": "1 Introduction", "content": "Written examinations are still widely used to assess students' know-how of a subject's learning objectives. Among the various question types, open-ended questions can test a more thorough understanding compared to closed-ended questions such as multiple-choice [13]. However, grading the answers to such questions is demanding as it requires extensive manual grading effort and can be subject to personal biases. Therefore, there have been several efforts to speed up the grading process through automation. Short Answer Grading (SAG) systems aim to automatically score students' answers in examinations. While earlier SAG systems focused on concept mapping, information extraction, and corpus-based methods [2], current systems employ fine-tuned language models [7, 8] or even directly in-context learning with powerful task-independent Large Language Models (LLMs) [1].\nDespite this growth in methods and capabilities, to the best of our knowledge, there exists no comprehensive short-answer grading benchmark across different subjects, grading scales, and distributions. Thus, assessing the generalizability of current automated grading methods is challenging. In this preliminary work, we aim to raise awareness of this lack of a comprehensive benchmark by providing the first version of such a benchmark together with an initial evaluation of existing automated grading solutions.\nSpecifically, we introduce the combined ASAG2024 benchmark\u00b9 to facilitate the comparison of automated grading systems. This meta benchmark combines seven commonly used short-answer grading datasets [3-5, 10, 11, 14] containing questions, reference answers, provided (student) answers, and human grades normalized to the same grading scale. For ASAG2024, we present initial evaluations of existing automated grading solutions on the benchmark. We show that specialized grading systems are still limited in their ability to generalize to new questions and may need to be fine-tuned for specific use cases: their error is larger than a simple mean predictor baseline. LLMs are able to generalize to the grading task with decent performance without being specifically trained or fine-tuned to any specific grading data: mean error 0.27. Aligned with related research, as the size of an LLM is increased, its ability to generalize to the grading task improves [15]."}, {"title": "2 The ASAG2024 Benchmark", "content": "The benchmark consists of seven SAG datasets in English and contains 19'000 question-answer-grade triplets (see Table 1). We scale the grades to lie between 0 and 1 to make results on the datasets comparable. Each dataset must at least contain reference answers, provided answers by humans and grades [3-5, 10, 11, 14]."}, {"title": "3 Experimental Evaluation", "content": "For our newly created ASAG2024 dataset, we implement and evaluate a set of seven automated grading methods.\nMethods. We select two specialized grading systems, BART-SAF [8, 9] & PrometheusII-7B [7], that are employing task-finetuned language models. Additionally, we evaluate three size categories of LLMs to validate whether these models can generalize from their pretraining task to grading through in-context learning (ICL): Llama-3-8B, GPT-3.5-turbo-0125 and GPT-40-2024-05-13. We use a simple prompt with an instruction, question, reference answer, and student's answer. We also implement two baselines: (1) Nomic-embed-text-v1 [12], a recent embedding model (using cosine-similarity); (2) A mean baseline that simply predicts the mean grade.\nMetrics. Root Mean Square Error (RMSE) is a common metric for reporting ASAG results. It measures the average magnitude of prediction errors. Due to grade imbalance, systems that assign higher grades generally have smaller errors (e.g., mean predictor). To counteract this, we introduce a weighted RMSE (WRMSE) in which the grades are weighted by how often similar grades appear in the data source. Specifically, a weight is given to each entry according to the number of other entries within a 0.1 range. Each dataset is divided into ten ranges of 0.1, and all ranges receive an equal 10% share of the total weight. If any range does not contain any entries, its weight is distributed equally to the other ranges.\n\n\n$WRMSE = \\sqrt{\\sum_{i=0}^{N} w_i (y_i - \\hat{y}_i)^2}$\n\n\n\n\n\n*   N is the number of observations in an individual dataset.\n*   $y_i$ is the actual observation, in our case, the human grade.\n*   $\\hat{y}_i$ is the prediction, i.e. the predicted grade.\n*   $w_i$ is the weight of an individual observation."}, {"title": "3.1 Initial Results", "content": "Table 2 shows the wRMSE of all methods. GPT-3.5-turbo and GPT-40 outperform other approaches, even without more advanced prompting techniques. Surprisingly, the fine-tuned models (BART-SAF, PrometheusII-7B) perform worse (0.48 and 0.43) even than the simple mean predictor baseline (0.40). The purely embedding-based model (Nomic-embed-text) that uses cosine-similarity between the student and the reference answer performs even slightly better than smaller task-independent LLMs such as Llama3-8b."}, {"title": "4 Conclusion and Future Work", "content": "Grading systems are not mature enough yet to be used in a fully automated exam setting. The best system (GPT-40) still exhibits an error more than double that of a human (0.1 according to SAF [5]). However, LLM-based methods show stable performance across datasets, making them applicable for self-study or as a support tool for grading. In the future, we will expand the benchmark with more diverse question-answer datasets, specifically focusing on multilingual aspects. We also plan to provide a more thorough evaluation of automated grading solutions on our dataset, including investigating common ICL strategies (e.g., few-shots, chain of thought)."}]}