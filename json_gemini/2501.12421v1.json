{"title": "Tackling Small Sample Survival Analysis via Transfer\nLearning: A Study of Colorectal Cancer Prognosis", "authors": ["Yonghao Zhao", "Changtao Li", "Chi Shu", "Qingbin Wu", "Hong Li", "Chuan Xu", "Tianrui Li", "Ziqiang Wang", "Zhipeng Luo", "Yazhou He"], "abstract": "Survival prognosis is crucial for medical informatics. Practitioners often con-\nfront small-sized clinical data, especially cancer patient cases, which can be\ninsufficient to induce useful patterns for survival predictions. This study\ndeals with small sample survival analysis by leveraging transfer learning, a\nuseful machine learning technique that can enhance the target analysis with\nrelated knowledge pre-learned from other data. We propose and develop var-\nious transfer learning methods designed for common survival models. For\nparametric models such as DeepSurv, Cox-CC (Cox-based neural networks),\nand DeepHit (end-to-end deep learning model), we apply standard trans-\nfer learning techniques like pretraining and fine-tuning. For non-parametric\nmodels such as Random Survival Forest, we propose a new transfer survival\nforest (TSF) model that transfers tree structures from source tasks and fine-\ntunes them with target data. We evaluated the transfer learning methods on", "sections": [{"title": "1. Introduction", "content": "Survival analysis is a fundamental statistical method in medical research,\nwhich not only focuses on one or more events of interest, such as death and\ndisease progression, but also the time duration, providing a mixed time-to-\nevent measure for medical research [1]. Common statistical approaches, in-\ncluding the Kaplan-Meier estimator and the Cox proportional hazards model,\nhave enabled researchers to estimate survival probabilities which could be\nfurther compared across different treatment groups while adjusting for co-\nvariates. This allows for a more nuanced understanding of how prognostic\npredictors contributed to outcome events over time[2].\nThe precision and robustness of survival analysis are highly contingent\nupon the number of patients enrolled and the incidence rate of the event of\ninterest[3]. However, the prolonged follow-up time and relatively low event\nrates often render it prohibitive to aggregate large sample sizes in prognostic\nresearch. For instance, a common study design that investigated prognos-\ntic outcomes-clinical trials often involve small sample sizes (<200) based\non statistics from the FDA[4, 5]. Furthermore, there has been a growing\ninterest in personalized medicine which relies heavily on analyses in small\nsamples of patients with specific molecular characteristics[6, 7]. These lead\nto a compelling need in novel analytic approaches that improves efficiency in\nstatistical modelling and prediction performance[8], in order to help inform\nclinical decision-making and policy formulation.\nRecently, more and more open-access large datasets became available,"}, {"title": "2. Background", "content": "In this section, we briefly introduce the basic concepts of survival analysis\n(SA) and the prevailing SA models including DeepSurv, Cox-CC, DeepHit,\nand random survival forests. For more in-depth background, please refer to\nKlein and Moeschberger (2003).\nIn general, the objective of SA is to model the distribution of the time\nT* to some event of interest. Denote by F(t) := P(T* < t) the cumulative\ndF(t)\ndistribution function and by f(t) :=  its density function. Instead of\ndt\ndirectly estimating F(t), we often study the survival function S(t) and model\nthe hazard rate h(t), defined as follows:\nS(t) := P(T* > t) = 1 \u2212 F(t)\t(1)"}, {"title": "2.1. Cox Regression with Neural Networks", "content": "The Cox Proportional Hazards Model (CPH) is a classic yet popular\nmodel in survival analysis. This semi-parametric model assumes that all\nobservations have the same form of the conditional hazard function:\nh(tx) = ho(t) exp(g(x)), g(x) = \u1e9eTx\t(4)\nwhere ho(t) is the baseline hazard function, exp[g(x)] the relative risk func-\ntion of given covariates x, and \u1e9e the vector of regression coefficients. \u03b2is\nusually estimated by minimizing the negative log-partial likelihood:\n\u2212l(\u03b2) \u03a3\n-1(3) = -log (exp(x) - g(x))\ni\njeRi\t(5)\nwhere di is the indicator, being one if the i-th individual has experienced the\nevent; and R\u2081 is the set of individuals still at risk at time ti.\nThe DeepSurv model extends the CPH model by using a neural network\nto estimate the log-risk function g(x) instead of the linear combination \u1e9eTx:\n0; = G(Wxj + b) \u03b2\t(6)\nwhere W is the weight matrix between the input and hidden layer size H \u00d7 D,\nH the number of neurons in the hidden layer, D the number of input features,\nb the bias vector, and G a nonlinear activation function. Then, Equation (5)\ncan be re-written as:\n-1(3,W,b) = 0, -log exp(63)\n\u03b4\u03b5=1\nLjeRi\t(7)\nTo improve the computation efficiency which is important in transfer\nlearning, Kvamme and Borgan (2019)'s Cox-CC (Case-Control) employs a\nsubset Ri sampled from the full risk set R\u2081 to approximate the entire risk\nset."}, {"title": "2.2. DeepHit", "content": "In contrast to Cox-based models, DeepHit by Lee et al. bypasses the Cox\nhazards and directly estimates the survival distribution F(t) using neural\nnetworks. DeepHit studies on discretized times 0 = To < T\u2081 < T2 < ... < Tm.\nDenote by yk(xi) = P(T = Tk | xi) the estimated probability mass function\nof the times which are outputs of a neural network with covariates x. Then,\nthe estimated survival function can be written as:\nk\n\u015c(Tk | Xi) = 1 - \u03a3\u03a5\u03ba (\u0425\u0456)\n\u043a=1\t(8)\nAnd the discretized negative log-likelihood is defined as:\nN\nloss L\ni=1\\TX\t(9)\nwhere e denotes index of time when T\u2081 = Te\u2081. To further promote the model's\nranking ability among individuals, they add a ranking loss defined as:\nlossa = 6,1 {1; <T;} exp ($ (T; xi) - S (T; \\ x\u2081))\n\u03a3 \u03b4;1 {\u03a4;\ni,j\n\u03c3\t(10)\nwhere o is a scale hyper-parameter and 1() the indicator function. The final\nloss function combines the above two, i.e., loss := a loss\u2081 +(1 \u2212 a) lossr,\nbalanced by a hyper-parameter 0 < a < 1."}, {"title": "2.2.1. Random Survival Forests", "content": "Besides parametric models, random survival forests are also popular be-\ncause of their many advantages, such as good performance, interpretability,\nand simplicity. Similar to the Classification and Regression Tree (CART), a\nsurvival tree is grown with a log-rank splitting rule by reformulating a clas-\nsification tree [13, 14]. Specifically, each terminal node T of survival trees\ngives its prediction by fitting individuals h to the Nelson-Aalen estimator:\nHn(t)\n=\n\u03b4\u03b9,\u03b7\n\u03a3\nYi,h\nt1.ht\t(11)\nwhere \u03b4\u03b9,\u03b7 indicates the number of individuals when events occur, and Yi,h is\nthe count of individuals who have not experienced the event up to that time"}, {"title": "3. Methods", "content": "In this section, we detail our proposed transfer learning methods for dif-\nferent SA models. There are two categories\none works for neural network\nbased (parametric) models, and the other for random survival forests (non-\nparametric). Afterward, we will present the experimental procedure to vali-\ndate our methods."}, {"title": "3.1. Transfer for Neural Networks", "content": "Current transfer learning techniques designed for neural networks are rel-\natively well-developed. The foundation of transfer learning is that the source\ntask and the target task should be somehow related such that the target can\nbenefit from the knowledge learned from the source. The typical process is\nthat a model pretrained on a source task is retrained or fine-tuned on the\ntarget. Retraining (RT) allows all the pretrained model parameters to be re-\noptimized in the target task such that the model can be well-adapted; by com-\nparison, fine-tuning (FT) only allows part of the parameters to be changed\nsuch that the pre-learned knowledge can be selectively preserved. For deep\nneural nets, usually the deep layers are frozen such that low-level or general\nfeature representations are transferred for reuse[15]; then the last few layers\nare fine-tuned such that the model can adapt to task-specific signals[16].\nIn our study, DeepSurv, Cox-CC, and DeepHit are all neural network\nbased models and can readily work with the retraining and fine-tuning tech-\nniques. A particular aspect of our study is that we work on small samples.\nSo, choosing what granularity of transfer (i.e. which parameters to preserve\nand which to re-optimize) depends on the data availability of the target task.\nOn the one hand, fine-tuning can be viewed as the generalization of retrain-\ning, as retraining is one extreme case of fine-tuning; on the other hand, from\nthe perspective of parameter optimization, if provided with sufficient data,"}, {"title": "3.2. Transfer for Random Survival Forests", "content": "Now we present the transfer techniques for random survival forests (RSF),\nwhich are the main contributions of our study. The general workflow is to first\nbuild an RSF (called the source forest) based on the source task and then\nrandomly transfer the frequent tree structures to the target task, where a\n\"fine-tuning\u201d procedure is performed. Tree structures differ in their features\nused to split nodes on different levels, and the tree-based fine-tuning is to\nadjust the splitting values and/or to grow lower-level nodes. The idea behind\nour method is intuitive - similar tasks are likely to generate similar survival"}, {"title": "3.2.1. Source Forest Training and Transfer", "content": "We start with calling a standard procedure [13, 14] to train a random sur-\nvival forest based on the source task's data. This gives us Ns survival trees,\nwhere each tree differs in its tree structures and splitting values. Though\nrandomly built, these trees can reflect the most discriminative features that\ndetermine the survival outcomes. In other words, discriminative features\ntend to appear more frequently and on higher levels of the trees. Therefore,\nkey to the tree-based transfer comes down to the frequency and the level\nof occurrence of features. Meanwhile, the inherent cross-feature relation-\nship also matters, which is reflected in the co-occurrence of features on the\nsame or adjacent levels. In such spirit, we design a transfer principle named\ntree-structure frequency (TSF-Tk) that counts the frequency of different tree\nstructures with top k levels in the Ns trees. The frequencies can be nor-\nmalized to a probability distribution P(Tk), which can be used to sample\nNT prototype trees with k levels that are further fine-tuned with the target\ntask's data. See Figure 1 for an example.\nBesides TSF, we also develop a comparative transfer method named DP\nthat only counts the depth-wise probability of feature occurrence. Formally,\nDP outputs a set of probabilities_PDP = {P1, P2,\uff65\uff65\uff65, PK}, where Pk(f) de-\nnotes the empirical probability distribution of feature f that occurred on\nthe k-th level of the source trees. Different from TSF, DP only considers\nfeature occurrences independently for each level, ignoring the co-occurrence\nof other features on adjacent levels. Therefore, when used to build new trees\nfor the target task, DP can explore new tree structures while down-weighting\nexisting structures occurring in the source forest."}, {"title": "3.2.2. Target Forest Fine-tuning", "content": "Now we build a target forest of Nr trees based on the transferred informa-\ntion. TSF-Tk outputs an empirical probability distribution of tree structures\nP(Tk), and DP outputs a depth-wise feature distribution PDP. Below is the\nforest building process based on TSF-Tk:\n1. Sample a tree structure T of k levels according to P(Tk).\n2. Build a tree T', of which the top k levels have the same splitting features\nas T but different splitting values which are determined by the target"}, {"title": "4. Experiments and Results", "content": "In this section, we evaluate our proposed transfer learning methods for\ndifferent survival models. The target learning task is to predict survival out-\ncomes of stage I colorectal cancer patients (CRC) collected from the West\nChina Hospital (WCH) [18]. The source task's data are collected from the\nSurveillance, Epidemiology, and End Results (SEER) database[19], of which\nCRC stage I patients are used. Below we present the data statistics, experi-\nment setups, and results."}, {"title": "4.1. Cohort Statistics", "content": "SEER and WCH respectively have 27,379 and 728 CRC stage I patient\ncases. Eight features are used as predictors: gender, age, T stage (T1 or\nT2), tumor size, tumor grade (G3-G4 or G1-G2), Carcinoembryonic Antigen\n(CEA) levels (positive when >2.5 ng/mL, negative, or unknown), perineu-\nral invasion (positive or negative), and suboptimal lymph node examination\n(lymph nodes sampling >12 or not). These covariates have been shown to be\nstrong indicators in the prognosis of stage I CRC patients [18] For feature-\nwise statistics, SEER has 51.1% males while WCH has 55.7% males; SEER\npatients have a median age of 69 years old whereas the median age in WCH\nis 61 years old. Clinical features show more variations. For example, SEER\nhas a median tumor size of 2.4 and WCH's is 3; for CEA levels, SEER's\npositive rate is 89.8% while WCH's is 52.2%. Besides, the mortality rates\nalso differ significantly, with SEER's being 25.4% and WCH's being 5.3%,\nwhich is highly censored."}, {"title": "4.2. Experimental Design", "content": "Firstly, the SEER dataset is used to pretrain models. We use cross-\nvalidation to optimize the pretrained models. For the target task, the full"}, {"title": "4.3. Transfer Learning Results", "content": "The first set of results focuses on the transfer learning performance for\nparametric models, i.e. Cox-based models and neural networks. Table 1 lists\nthe retraining (RT) and fine-tuning (FT) results when tested with different\nmodels and different data sizes. We also provide two baselines for comparison.\nOne is given by the Target column, which means using only the target data\nfor training, with zero transfer. The other is given by the Source column,\nwhich means directly applying the pretrained SEER model on the test data,\nwith zero fine-tuning. Hence, this column remains unchanged for different\nsample sizes.\nThe second set of results is for transfer survival forest (TSF), presented\nin Table 2. Similarly, the Target column means no transfer and just using a\nnormal random survival forest (RSF); and the Source column means no fine-\ntuning and directly using a pretrained RSF based on the SEER data. The\nremaining columns are the results of different TSF variations (see Methods).\nNote that TSF-Tk refers to transferring and fine-tuning only the top k levels\nof trees, and T\u221e means no such level limitation."}, {"title": "5. Discussions", "content": "In the preceding section, we empirically evaluated various transfer learn-\ning methods designed for parametric survival models (DeepSurv, Cox-\u0421\u0421,\nDeepHit) and random survival forests. The base learning task was CRC\nprognosis, using SEER as the source and WCH as the target, and different\nsample sizes were compared. Here, we are to analyze these empirical results\nand reveal the insights behind them. Also, limitations and future work will\nbe discussed as well."}, {"title": "5.1. Clinical Implications", "content": "Our findings demonstrate that transfer learning significantly enhances\nthe predictive accuracy of survival analysis based on limited sample sizes.\nThis finding is of important clinical relevance considering the prohibitive\ncost and duration of developing large-scale patient cohorts that request pro-\nlonged follow-up and that investigate rare disease outcomes. Although our\ncase-study utilized common cinico-pathological features of cancer patients,\nthe framework we propose can be applied to other features such as imaging"}, {"title": "5.2. Empirical Findings", "content": "Firstly, for neural network models (Table 1), we can find that both fine-\ntuning and retraining can boost all the models' performance on different\nsample sizes (as small as fifty). The first reason accounting for this is that\nthe raw transferred model (the Source column) can provide a strong baseline\nsuperior to the non-transfer model (the Target column).\nThis demonstrates a key principle of transfer learning - indeed, the source\nand target tasks should be somewhat related such that the learned knowl-\nedge, represented by models or in other forms, can be transferred and benefit\nthe target. Moreover, when the source model is fine-tuned or retrained with\nthe target data, the model performance can be further improved. This holds\nfor all the cases of Cox-CC, DeepSurv, and partially for DeepHit (yet the"}, {"title": "5.3. Limitations and Future Work", "content": "While our study has demonstrated the feasibility and benefits of apply-\ning transfer learning to cancer prognosis, there are certain limitations and\nseveral promising future directions to discuss. First, the SEER database\nwas able to provide strong source models to be transferred, which, however,\ndownplayed the effect of the following modifications of the source models.\nIt would be interesting to see how much a mild source model could con-\ntribute to the target and how much margin the fine-tuning techniques with\nlimited target data could improve the source model. Second, the source and\nthe target data we experimented with have completely aligned feature sets.\nHowever, many related clinical tasks' feature sets differ, and how to handle\nnon-overlapped features is a critical challenge in transfer learning. Last but\nnot least, nowadays with richer data collections of genomics and proteomics\ndata, it will be of tremendous interest to leverage multi-modal data for cancer\nprognosis. For example, combine pre-trained cell foundation models (such\nas GeneFormer [28]) and bulk RNA-seq data with survival observations and\nthen design comprehensive and more precise survival models."}, {"title": "6. Conclusions", "content": "Our study aims to tackle small sample survival analysis with the help of\ntransfer learning. We summarized the current transfer learning methods ap-\nplied to trending neural network based survival models (DeepSurv, Cox-CC,\nDeepHit) and proposed new transfer techniques for random survival forests.\nWe also evaluated these methods on a set of colorectal cancer prognosis tasks,\nwhere the source data are from SEER and the target data are collected from\nthe West China Hospital (WCH). Based on the experiment results, the two"}, {"title": "7. Acknowledgments", "content": "The work presented was supported by the National Natural Science Foun-\ndation of China (No. 62302405, 62176221, 82103918), China Postdoctoral\nScience Foundation (Grant No. 2023M732914), and the Fundamental Re-\nsearch Funds for the Central Universities (No. 2682023ZT007). The content\nof this paper is solely the responsibility of the authors and does not neces-\nsarily represent the official views of the abovementioned funding agencies."}, {"title": "8. Ethics approval and consent to participate", "content": "This study was approved by the Ethics Committee of West China Hospi-\ntal(approval ID: 201960V1.0 for the WCH cohort).Written informed consent\nwas signed by each participant from the study cohorts."}, {"title": "9. Source Availability", "content": "All data are available upon reasonable request; please refer to the details\nelsewhere [18]. The source code for the experimental implementation used in\nthis study is available at the Github repository (https://github.com/YonghaoZhao722/TSF)."}]}