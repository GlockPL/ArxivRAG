{"title": "Controllable Navigation Instruction Generation with Chain of Thought Prompting", "authors": ["Xianghao Kong", "Jinyu Chen", "Wenguan Wang", "Hang Su", "Xiaolin Hu", "Yi Yang", "Si Liu"], "abstract": "Instruction generation is a vital and multidisciplinary research area with broad applications. Existing instruction generation models are limited to generating instructions in a single style from a particular dataset, and the style and content of generated instructions cannot be controlled. Moreover, most existing instruction generation methods also disregard the spatial modeling of the navigation environment. Leveraging the capabilities of Large Language Models (LLMs), we propose C-INSTRUCTOR, which utilizes the chain-of-thought-style prompt for style-controllable and content-controllable instruction generation. Firstly, we propose a Chain of Thought with Landmarks (CoTL) mechanism, which guides the LLM to identify key landmarks and then generate complete instructions. CoTL renders generated instructions more accessible to follow and offers greater controllability over the manipulation of landmark objects. Furthermore, we present a Spatial Topology Modeling Task to facilitate the understanding of the spatial structure of the environment. Finally, we introduce a Style-Mixed Training policy, harnessing the prior knowledge of LLMs to enable style control for instruction generation based on different prompts within a single model instance. Extensive experiments demonstrate that instructions generated by C-INSTRUCTOR outperform those generated by previous methods in text metrics, navigation guidance evaluation, and user studies.", "sections": [{"title": "1 Introduction", "content": "Developing an agent capable of communicating with humans in natural language and accomplishing specific tasks in its environment is a crucial goal for researchers in the field of embodied AI. Such an agent needs two key abilities: the first one is to execute specific tasks based on human instructions, and the second one is to provide interactive feedback and guidance to humans based on environmental information. Regarding the first ability, one of the most typical"}, {"title": "2 Related Work", "content": "Navigation Instruction Generation. The study of generating linguistic instruction for navigation can date back to Lynch's work [43] in the 1960s. Early efforts [1, 65] investigated the human cognitive mechanism for describing routes. They found that navigation direction is associated with the cognitive map [32] and influenced by various factors including cultural background [56] and genders [27]. This area has long been overlooked by the computer vision academia and is simply viewed as a data augmentation tool for VLN. However, it holds significant practical relevance, e.g., establishing human-machine trust [63] and facilitating blind navigation [26]. Fried et al [16] first proposed a LSTM-based instruction generation model to augment training samples and re-weight the route choice of the navigator. There are three primary aspects for the advancement of instruction generation: elevated linguistic quality, finer-grained directives, and longer, more intricate instructions. In order to enhance the quality of instructions, some methods introduce supplementary information like external knowledge [67] and landmark information [62, 69], build instruction template [69] and utilize larger language models [62]. [22, 24, 29, 69, 73] generate fine-grained alignment between language and navigation paths. To build more intricate instructions, [28, 38, 73] cross-connect paths to generate longer instruction-trajectory pairs. Methods like [15, 58, 63] also consider instruction generation and follow-"}, {"title": "3 Methodology", "content": "3.1 Task Formulation\nThe instruction generation model is required to generate the instruction X = {x_1,x_2,...,x_S} with S words that provides guidance for following the given path R= {r_1,r_2,...,r_T} with T steps. At a given time step t, r_t is composed of the panoramic observation o_t and action a_t. The objective of model parameters \\theta is to maximize the likelihood of the target instruction X*:\n\\theta^* = \\text{arg} \\underset{\\theta}{\\text{max}} \\log p(X^*|R, \\theta).\n3.2 Overall Framework\nTo leverage the linguistic capabilities of LLMs, we employ an adapter-based [20] approach in C-INSTRUCTOR to embed actions and visual observations. The"}, {"title": "3.3 Spatial Topology Modeling Task (STMT)", "content": "Understanding the spatial relationships between different viewpoints is fundamental for generating navigation instructions. LLMs and visual encoders are typically trained on data from the Internet with few embodied-type data. Consequently, they possess limited spatial cognition abilities. Therefore, we introduce STMT as an auxiliary task to enhance the model's spatial perception capability. In STMT, the model predicts actions between adjacent viewpoints along a trajectory. As the actions along the navigation path are already represented through location encoding, we make the model predict how to return to the previous location from the current viewpoint, as shown in Fig. 2b. Given a trajectory {r_1, r_2, ..., r_t}, the model needs to predict a_t' in order to transit from r_t back to r_{t-1}. We use prompt_a to distinguish this task and introduce a new special token [a] for predicting a_t'. The model input is:\n[r_1, r_2, ..., r_t; prompt_a, [a]].\nWe denote the output corresponding to [a] at the l-th LLM block as x_l^a \\in R^{1 \\times D_p}. We then aggregate the visual features at step t through an attention layer:\n\\bar{x}^a = \\text{cross\\_attn}(x_l^a, I_{t,1:36}).\n\\bar{x}^a replaces x_l^a as the input for the following layers. To mitigate the impact on the primary model and enhance training stability, the aggregation operation only starts from the output of L-th LLM block. We replace the original word prediction layer with an attention mechanism to predict a_t':\nA_t = \\text{softmax}(\\bar{x}^a W_{I_{t,1:36}}),"}, {"title": "3.4 Chain of Thought with Landmarks (CoTL)", "content": "Distinguished from image or video captioning, navigation instructions encompass more than just visual descriptions. An easily executable navigation instruction usually includes several landmarks for directional guidance at crucial turning points. Besides, according to research in human cognitive psychology [43], it has been observed that humans, when providing path guidance, tend to first identify key navigation points within their cognitive maps before structuring their language. Therefore, the ability to determine landmarks is crucial for instruction generation. CoT [66] has been validated as an effective means of guiding the reasoning process of LLMs. Consequently, we introduce CoTL to direct the model to utilize critical landmarks in the navigation trajectory to generate instructions.\nLandmark Selection. For the provided annotation pairs of instructions and paths in the training set, we initially extract nouns from the instructions as linguistic landmarks \\Lambda = {X_n}_{n=1}^{N_x}. Since valuable landmarks may not be fully specified in the annotated instructions, we supplement the landmark set by considering the visual characteristics of the path, as shown in Fig. 3. We select visual landmarks from two perspectives, i.e., the temporal perspective and the spatial perspective. From the temporal perspective, we identify crucial viewpoints along the trajectory, where landmarks are more essential for guidance. Specifically, when the trajectory leads into a new scene, e.g., transitioning from a corridor to a room, the navigator often requires a landmark for guidance. We compute the feature difference of panoramic views along a trajectory to locate these viewpoints. For a given path, we construct a sequence comprising the mean-pooled features of panoramic views {I_t}_{t=1}^T. We then compute the temporal importance score \\delta_t via cosine distance between I_t and I_{t+1}:\n\\delta_t = 1 - \\frac{I_t \\cdot I_{t+1}}{||I_t|| ||I_{t+1}||}, \\quad I_t = \\frac{1}{K} \\sum_{k=1}^{K} I_{t,k},\nwhere \\delta_t indicates the temporal importance of landmarks appearing at time step t. From the spatial perspective, we need to identify the most distinctive object to serve as a landmark. Distinctive objects are primarily the ones that appear in the action view and not in any other candidate views. At time step t, we first extract all objects appearing in v_{t, a_t} as the candidate landmark set {n}_{n=1}^{N_t}. Then, we assign distinctive scores according to the occurrence of landmarks in other candidate views. For example, the landmark A_n that also appears in candidate views {C_1, C_2, C_3} is assigned the spatial importance score \\delta_{t,n}:\n\\delta_{t, n} = 1 - d_{t,C_1} - d_{t,C_2} - d_{t,C_3},"}, {"title": "3.5 Style-Mixed Training (SMT)", "content": "In application, a model that can only generate step-by-step instructions is less practical. When the instruction follower is familiar with the environment, fine-grained instructions lead to reduced communication efficiency. Additionally, due to the extensive amount of labor required for annotating navigation instructions, the data available is limited, especially for instructions with specified styles. This results in LLMs being susceptible to overfitting, makes it challenging to achieve accurate cross-modal mapping, and leads to suboptimal instruction generation performance when the model is trained with single-style instructions.\nTo mitigate the issues above, we mix datasets with instructions in different linguistic styles for training. We devise descriptions that encapsulate diverse styles into prompts to enable the LLM to generate in different styles. By employing SMT, not only is the quality of instruction generation enhanced, but we also enable a single LLM instance to adaptively generate different styles of instructions for the same path R by switching between different prompts."}, {"title": "4 Experiments", "content": "4.1 Datasets and Evaluation Metrics\nDatasets. We evaluate the instruction generation performance on three indoor navigation datasets [5, 31, 48] and one outdoor navigation dataset [26]:\nR2R [5]: It has four splits with step-by-step instructions, i.e., train (61 scenes, 14,039 instructions), val seen (61 scenes, 1,021 instructions), val unseen (11 scenes, 2, 349 instructions), and test unseen (18 scenes, 4, 173 instructions). As test unseen is reserved for benchmarking instruction followers, we report the performance of instruction generation on val splits.\nREVERIE [48]: It contains high-level descriptions of target destinations and objects. It has three open-access splits, i.e., train (61 scenes, 10,466 instructions), val seen (61 scenes, 1,371 instructions), and val unseen (10 scenes, 3, 753 instructions). We report the performance on two val splits.\nRxR [31]: It is a multilingual indoor navigation dataset with longer trajectories and more fine-grained aligned instructions. we specifically utilize the English instructions for comparison with previous methods. It has three publicly available splits, and we report the performance on two val splits.\nUrbanWalk [26]: It is an outdoor navigation dataset with 26,808 image-instruction pairs simulated by CARLA [14]. We follow the setting in [67]."}, {"title": "4.2 Implementation Details", "content": "Detailed Architecture. We use the multimodal LLaMA-Adapter [20] with 32 layers and 7B parameters as the LLM. We adopt CLIP-ViT-L-14 [50] and 8 ViT [13] blocks in the Trajectory Encoder. The score threshold \\beta for landmark selection in \\S3.4 is set to 0.25, and Ls in \\S3.3 is set to 30.\nTraining. We only finetune the last 2 layers of LLM while fixing the other 30 layers. The CLIP [50] visual encoder is also fixed. We first pre-train C-INSTRUCTOR on PREVALENT [21] for 240K iterations with a batch size of 16, and then finetune C-INSTRUCTOR on multiple datasets jointly for 120K iterations with batch size 4. We use the AdamW [42] optimizer with base learning rate 1.0 \\times 10^{-4}. Four NVIDIA A100 80GB GPUs are used for training.\nInference. We set the generation temperature to 1.0 for RxR [31], and 0.1 for all other datasets. All other hyperparameters remain the same as [20]."}, {"title": "4.3 Comparison to State-of-the-Art Methods", "content": "We compare C-INSTRUCTOR with four existing instruction generation models. For a fair comparison, we report the performance of C-INSTRUCTOR without SMT in addition to the performance of the full model. We employ the Penn Treebank tokenizer [53] to compute the linguistic metrics.\nR2R [5]. The results on R2R are summarized in Tab. 1. C-INSTRUCTOR outperforms previous methods under all metrics on both val splits. In terms of SPICE, C-INSTRUCTOR demonstrates a superiority of 3.9% in absolute terms and 20.1% in relative terms on val seen as well as 3.8% in absolute terms and 21.8% in relative terms on val unseen compared to the previous best. This verifies that C-INSTRUCTOR exhibits good performance in generating fine-grained directives."}, {"title": "4.4 Diagnostic Experiment", "content": "To thoroughly study the effectiveness of C-INSTRUCTOR, we compare the full model with several ablative designs. We test the ablative models on REVERIE [48] and R2R [5] val unseen. The results are summarized in Tab. 5."}, {"title": "4.5 Instruction Quality Analysis", "content": "Evaluating the quality of instructions solely based on text similarity metrics is insufficient as those metrics do not thoroughly assess the semantic alignment between instructions and trajectories. Thus, we further analyze the semantic quality of instructions generated by C-INSTRUCTOR from three aspects through the following experiments:\nPath Guiding Proficiency. The success rate (SR) of navigators with instructions from different instruction generators can be used as an index for the quality of instructions. We regenerate instructions for the paths in REVERIE [48] val unseen and employ two navigators (HAMT [10] and DUET [11]) to assess SR and SPL (SR weighted by Path Length) when guided by regenerated instructions. As depicted in Tab. 6b, SR and SPL of instructions provided by C-INSTRUCTOR"}, {"title": "4.6 Qualitative Results", "content": "We visualize an example of indoor navigation trajectory and corresponding instruction generation results in Fig. 4. As seen, C-INSTRUCTOR can identify crit-"}, {"title": "5 Conclusion and Discussion", "content": "In this work, we propose C-INSTRUCTOR, which generates style-controllable and content-controllable instructions with high linguistic quality. It uses an adapter-based structure to leverage the language capability of LLMs and distinct style prompts in SMT to achieve style control. To enhance the executability of generated instructions, we adopt CoTL to help identify crucial landmarks and provide content controllability. We also devise STMT to enhance the model's understanding of the environment's spatial topology. The instructions generated by C-INSTRUCTOR not only achieve high scores in text metrics but also demonstrate strong competence in guiding navigators, further validating the strong correspondence between generated instructions and given trajectories. We expect that C-INSTRUCTOR can greatly enhance agent-human communication and significantly contribute to the development of versatile embodied agents."}, {"title": "A Detailed Prompts", "content": "In this section, we provide detailed prompts for different navigation datasets. Note that all the given prompts are then formatted by prompt templates in [20].\nprompt for R2R [5]: You are given a sequence of views of a path. Please extract critical landmarks in the path.\nprompt for R2R [5]: You are given a sequence of views of a path in an indoor environment. Please describe the path according to the given landmarks in detail for an intelligent agent to follow. Landmarks: <landmarks>.\nprompt for REVERIE [48]: You are given a sequence of views of a path in an indoor environment. Please extract several critical landmarks in the path for generating a brief high-level target-oriented instruction.\nprompt for REVERIE [48]: You are given a sequence of views of a path in an indoor environment and critical landmarks for a brief high-level target-oriented instruction. Please generate the indicated high-level target-oriented instruction briefly for an intelligent agent to follow. Landmarks: <landmarks>.\nprompt for RxR [31]: You are given a sequence of views of a path in an indoor environment. Please extract critical landmarks describing the starting position and the path.\nprompt for RxR [31]: You are given a sequence of views of a path in an indoor environment. Please describe the starting position and the path according to the given landmarks in detail for an intelligent agent to follow. Landmarks: <landmarks>.\nprompt: You are an intelligent embodied agent that navigates in an indoor environment. Your task is to move among the static viewpoints (positions) of a pre-defined graph of the environment. You are given several candidate views. You are also given a sequence of panoramic views showing previous"}, {"title": "B Extra Ablations on Landmark Selection", "content": "B.1 Selection Strategies\nTo validate the effectiveness of our landmark selection strategy, we conducted several experiments with several ablative strategies on REVERIE [48] and R2R [5] val unseen splits. The results are shown in Tab. 7.\n#1 is the baseline result without landmarks and the CoT process. The model in #2 uses only landmarks from instructions \\Lambda_x in CoTL. Compared to #1, the SPICE metric remarkably increases, which indicates a more accurate description of object relations in the instructions. Other metrics fluctuate. Based on #2, the model in #3 adds visual landmarks via spatial selection, which are denoted as \\Lambda_a. Compared to #2, almost all metrics rise, which demonstrates the value of visual landmarks. The model in #4 adds visual landmarks via spatial and temporal selection \\Lambda in addition to landmarks from instructions, resulting in an increase in almost all scores compared to #3. The results above further confirm the effectiveness of the proposed landmark selection mechanism.\nB.2 Values of B\nWe conduct ablations on the numerical values of \\beta in landmark selection on REVERIE [48] and R2R [5] val unseen splits. The results are presented in Tab. 8. It can be observed that assigning \\beta to 0.25 achieves the best performance."}, {"title": "C Further Analysis on STMT", "content": "In Fig. 6, we plot the curve illustrating the model's validation loss on R2R [5] val unseen during the training process. Compared to the baseline without STMT, We can observe that STMT effectively prevents overfitting as evidenced by the fact that its validation loss does not exhibit a gradual increase compared to the baseline. STMT effectively ensures the training stability of C-INSTRUCTOR and also enhances the instruction quality."}, {"title": "D Additional Qualitative Results", "content": "In Fig. 7, we provide more visualizations of navigation trajectories and corresponding instruction generation results. As observed, C-INSTRUCTOR effectively"}, {"title": "E More Discussion", "content": "Social Impact. C-INSTRUCTOR can be used to provide feedback from intelligent embodied agents to humans as well as to guide humans who are unfamiliar with the environment. It can also serve as accessibility facilities for the visually impaired to find their way.\nLimitations. Due to data availability, C-INSTRUCTOR is trained on simulated data with discrete viewpoints, which limits its performance in real-world continuous environments. Moreover, as discussed in \\S D, C-INSTRUCTOR possesses limited ability in modeling the global structure of the environment, resulting in inaccurate instructions when referring to the global location of a specific object or room in the environment.\nFuture Work. We plan to devise a mechanism that encodes the global structure of the environment into the instruction generator. With knowledge of the environment, the instruction generator can locate the user according to free-form natural language descriptions and provide path guidance according to the destination designated by the user."}]}