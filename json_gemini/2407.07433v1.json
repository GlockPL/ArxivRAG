{"title": "Controllable Navigation Instruction Generation\nwith Chain of Thought Prompting", "authors": ["Xianghao Kong", "Jinyu Chen", "Wenguan Wang", "Hang Su", "Xiaolin Hu", "Yi Yang", "Si Liu"], "abstract": "Instruction generation is a vital and multidisciplinary re-\nsearch area with broad applications. Existing instruction generation mod-\nels are limited to generating instructions in a single style from a particular\ndataset, and the style and content of generated instructions cannot be\ncontrolled. Moreover, most existing instruction generation methods also\ndisregard the spatial modeling of the navigation environment. Leverag-\ning the capabilities of Large Language Models (LLMs), we propose C-\nINSTRUCTOR, which utilizes the chain-of-thought-style prompt for style-\ncontrollable and content-controllable instruction generation. Firstly, we\npropose a Chain of Thought with Landmarks (CoTL) mechanism, which\nguides the LLM to identify key landmarks and then generate complete\ninstructions. CoTL renders generated instructions more accessible to fol-\nlow and offers greater controllability over the manipulation of landmark\nobjects. Furthermore, we present a Spatial Topology Modeling Task to\nfacilitate the understanding of the spatial structure of the environment.\nFinally, we introduce a Style-Mixed Training policy, harnessing the prior\nknowledge of LLMs to enable style control for instruction generation\nbased on different prompts within a single model instance. Extensive\nexperiments demonstrate that instructions generated by C-INSTRUCTOR\noutperform those generated by previous methods in text metrics, navi-\ngation guidance evaluation, and user studies.", "sections": [{"title": "1 Introduction", "content": "Developing an agent capable of communicating with humans in natural lan-\nguage and accomplishing specific tasks in its environment is a crucial goal for\nresearchers in the field of embodied AI. Such an agent needs two key abilities:\nthe first one is to execute specific tasks based on human instructions, and the\nsecond one is to provide interactive feedback and guidance to humans based on\nenvironmental information. Regarding the first ability, one of the most typical"}, {"title": "2 Related Work", "content": "Navigation Instruction Generation. The study of generating linguistic in-\nstruction for navigation can date back to Lynch's work [43] in the 1960s. Early\nefforts [1,65] investigated the human cognitive mechanism for describing routes.\nThey found that navigation direction is associated with the cognitive map [32]\nand influenced by various factors including cultural background [56] and gen-\nders [27]. This area has long been overlooked by the computer vision academia\nand is simply viewed as a data augmentation tool for VLN. However, it holds\nsignificant practical relevance, e.g., establishing human-machine trust [63] and\nfacilitating blind navigation [26]. Fried et al [16] first proposed a LSTM-based in-\nstruction generation model to augment training samples and re-weight the route\nchoice of the navigator. There are three primary aspects for the advancement of\ninstruction generation: elevated linguistic quality, finer-grained directives, and\nlonger, more intricate instructions. In order to enhance the quality of instruc-\ntions, some methods introduce supplementary information like external knowl-\nedge [67] and landmark information [62,69], build instruction template [69] and\nutilize larger language models [62]. [22, 24, 29,69,73] generate fine-grained align-\nment between language and navigation paths. To build more intricate instruc-\ntions, [28, 38, 73] cross-connect paths to generate longer instruction-trajectory\npairs. Methods like [15,58,63] also consider instruction generation and follow-"}, {"title": "3 Methodology", "content": "3.1 Task Formulation\nThe instruction generation model is required to generate the instruction X =\n{x1, X2, ..., xs} with S words that provides guidance for following the given\npath R= {1,2, ..., r} with T steps. At a given time step t, rt is composed of\nthe panoramic observation of and action at. The objective of model parameters\n0 is to maximize the likelihood of the target instruction X*:\n$\\theta^* = \\arg \\max_\\theta \\log p(X^*|R, \\theta).$\n3.2 Overall Framework\nTo leverage the linguistic capabilities of LLMs, we employ an adapter-based [20]\napproach in C-INSTRUCTOR to embed actions and visual observations. The"}, {"title": "3.3 Spatial Topology Modeling Task (STMT)", "content": "Understanding the spatial relationships between different viewpoints is funda-\nmental for generating navigation instructions. LLMs and visual encoders are\ntypically trained on data from the Internet with few embodied-type data. Con-\nsequently, they possess limited spatial cognition abilities. Therefore, we introduce\nSTMT as an auxiliary task to enhance the model's spatial perception capability.\nIn STMT, the model predicts actions between adjacent viewpoints along a\ntrajectory. As the actions along the navigation path are already represented\nthrough location encoding, we make the model predict how to return to the pre-\nvious location from the current viewpoint, as shown in Fig.2b. Given a trajectory\n{r1, 2, ..., rt}, the model needs to predict af in order to transit from rt back to\nrt-1. We use prompt to distinguish this task and introduce a new special token\nfor predicting a. The model input is:\n$[r_1, r_2, ..., r_t; \\text{prompt}_a, x].$\nWe denote the output corresponding to as at the l-th LLM block as x E R\nWe then aggregate the visual features at step t through an attention layer:\n$x^\\prime = \\text{cross\\_attn}(x_1^a, I_{t,1:36}).$\nreplaces x as the input for the following layers. To mitigate the impact\non the primary model and enhance training stability, the aggregation operation\nonly starts from the output of L-th LLM block. We replace the original word\nprediction layer with an attention mechanism to predict af:\n$A_t = \\text{softmax}(x^\\prime W_{I, 1:36}),$"}, {"title": "3.4 Chain of Thought with Landmarks (CoTL)", "content": "Distinguished from image or video captioning, navigation instructions encompass\nmore than just visual descriptions. An easily executable navigation instruction\nusually includes several landmarks for directional guidance at crucial turning\npoints. Besides, according to research in human cognitive psychology [43], it has\nbeen observed that humans, when providing path guidance, tend to first identify\nkey navigation points within their cognitive maps before structuring their lan-\nguage. Therefore, the ability to determine landmarks is crucial for instruction\ngeneration. CoT [66] has been validated as an effective means of guiding the rea-\nsoning process of LLMs. Consequently, we introduce CoTL to direct the model\nto utilize critical landmarks in the navigation trajectory to generate instructions.\nLandmark Selection. For the provided annotation pairs of instructions and\npaths in the training set, we initially extract nouns from the instructions as\nlinguistic landmarks A = {X}=1. Since valuable landmarks may not be fully\nspecified in the annotated instructions, we supplement the landmark set by con-\nsidering the visual characteristics of the path, as shown in Fig. 3. We select visual\nlandmarks from two perspectives, i.e., the temporal perspective and the spatial\nperspective. From the temporal perspective, we identify crucial viewpoints along\nthe trajectory, where landmarks are more essential for guidance. Specifically,\nwhen the trajectory leads into a new scene, e.g., transitioning from a corridor to\na room, the navigator often requires a landmark for guidance. We compute the\nfeature difference of panoramic views along a trajectory to locate these view-\npoints. For a given path, we construct a sequence comprising the mean-pooled\nfeatures of panoramic views {I}_1. We then compute the temporal importance\nscore di via cosine distance between It and It+1:\n$\\delta_t^r = 1 - \\frac{I_t\\cdot I_{t+1}}{||I_t|| ||I_{t+1}||}, \\quad I_t = \\frac{1}{K} \\sum_{k=1}^K I_{t,k},$\nwhere di indicates the temporal importance of landmarks appearing at time step\nt. From the spatial perspective, we need to identify the most distinctive object\nto serve as a landmark. Distinctive objects are primarily the ones that appear\nin the action view and not in any other candidate views. At time step t, we first\nextract all objects appearing in vt,a, as the candidate landmark set {n}_1\nThen, we assign distinctive scores according to the occurrence of landmarks in\nother candidate views. For example, the landmark An that also appears in\ncandidate views {C1, C2, C3} is assigned the spatial importance score din:\n$\\delta_{t,n}^s = 1 - d_{t,c_1} - d_{t,c_2} - d_{t,c_3},$"}, {"title": "3.5 Style-Mixed Training (SMT)", "content": "In application, a model that can only generate step-by-step instructions is less\npractical. When the instruction follower is familiar with the environment, fine-\ngrained instructions lead to reduced communication efficiency. Additionally, due\nto the extensive amount of labor required for annotating navigation instructions,\nthe data available is limited, especially for instructions with specified styles. This\nresults in LLMs being susceptible to overfitting, makes it challenging to achieve\naccurate cross-modal mapping, and leads to suboptimal instruction generation\nperformance when the model is trained with single-style instructions.\nTo mitigate the issues above, we mix datasets with instructions in differ-\nent linguistic styles for training. We devise descriptions that encapsulate diverse\nstyles into prompts to enable the LLM to generate in different styles. By em-\nploying SMT, not only is the quality of instruction generation enhanced, but\nwe also enable a single LLM instance to adaptively generate different styles of\ninstructions for the same path R by switching between different prompts."}, {"title": "4 Experiments", "content": "4.1 Datasets and Evaluation Metrics\nDatasets. We evaluate the instruction generation performance on three indoor\nnavigation datasets [5,31,48] and one outdoor navigation dataset [26]:\nR2R [5]: It has four splits with step-by-step instructions, i.e., train (61\nscenes, 14,039 instructions), val seen (61 scenes, 1,021 instructions), val\nunseen (11 scenes, 2, 349 instructions), and test unseen (18 scenes, 4, 173\ninstructions). As test unseen is reserved for benchmarking instruction fol-\nlowers, we report the performance of instruction generation on val splits.\nREVERIE [48]: It contains high-level descriptions of target destinations and\nobjects. It has three open-access splits, i.e., train (61 scenes, 10,466 in-\nstructions), val seen (61 scenes, 1,371 instructions), and val unseen (10\nscenes, 3, 753 instructions). We report the performance on two val splits.\nRxR [31]: It is a multilingual indoor navigation dataset with longer tra-\njectories and more fine-grained aligned instructions. we specifically utilize\nthe English instructions for comparison with previous methods. It has three\npublicly available splits, and we report the performance on two val splits.\nUrbanWalk [26]: It is an outdoor navigation dataset with 26,808 image-\ninstruction pairs simulated by CARLA [14]. We follow the setting in [67]."}, {"title": "4.2 Implementation Details", "content": "Detailed Architecture. We use the multimodal LLaMA-Adapter [20] with 32\nlayers and 7B parameters as the LLM. We adopt CLIP-ViT-L-14 [50] and 8\nViT [13] blocks in the Trajectory Encoder. The score threshold \u1e9e for landmark\nselection in \u00a73.4 is set to 0.25, and Ls in \u00a73.3 is set to 30.\nTraining. We only finetune the last 2 layers of LLM while fixing the other 30 lay-\ners. The CLIP [50] visual encoder is also fixed. We first pre-train C-INSTRUCTOR\non PREVALENT [21] for 240K iterations with a batch size of 16, and then fine-\ntune C-INSTRUCTOR on multiple datasets jointly for 120K iterations with batch\nsize 4. We use the AdamW [42] optimizer with base learning rate 1.0 \u00d7 10-4.\nFour NVIDIA A100 80GB GPUs are used for training.\nInference. We set the generation temperature to 1.0 for RxR [31], and 0.1 for\nall other datasets. All other hyperparameters remain the same as [20]."}, {"title": "4.3 Comparison to State-of-the-Art Methods", "content": "We compare C-INSTRUCTOR with four existing instruction generation models.\nFor a fair comparison, we report the performance of C-INSTRUCTOR without\nSMT in addition to the performance of the full model. We employ the Penn\nTreebank tokenizer [53] to compute the linguistic metrics.\nR2R [5]. The results on R2R are summarized in Tab. 1. C-INSTRUCTOR outper-\nforms previous methods under all metrics on both val splits. In terms of SPICE,\nC-INSTRUCTOR demonstrates a superiority of 3.9% in absolute terms and 20.1%\nin relative terms on val seen as well as 3.8% in absolute terms and 21.8% in\nrelative terms on val unseen compared to the previous best. This verifies that\nC-INSTRUCTOR exhibits good performance in generating fine-grained directives."}, {"title": "4.4 Diagnostic Experiment", "content": "To thoroughly study the effectiveness of C-INSTRUCTOR, we compare the full\nmodel with several ablative designs. We test the ablative models on REVERIE [48]\nand R2R [5] val unseen. The results are summarized in Tab. 5."}, {"title": "4.5 Instruction Quality Analysis", "content": "Evaluating the quality of instructions solely based on text similarity metrics is\ninsufficient as those metrics do not thoroughly assess the semantic alignment\nbetween instructions and trajectories. Thus, we further analyze the semantic\nquality of instructions generated by C-INSTRUCTOR from three aspects through\nthe following experiments:\nPath Guiding Proficiency. The success rate (SR) of navigators with instruc-\ntions from different instruction generators can be used as an index for the quality\nof instructions. We regenerate instructions for the paths in REVERIE [48] val\nunseen and employ two navigators (HAMT [10] and DUET [11]) to assess SR and\nSPL (SR weighted by Path Length) when guided by regenerated instructions.\nAs depicted in Tab. 6b, SR and SPL of instructions provided by C-INSTRUCTOR"}, {"title": "4.6 Qualitative Results", "content": "We visualize an example of indoor navigation trajectory and corresponding in-\nstruction generation results in Fig. 4. As seen, C-INSTRUCTOR can identify crit-"}, {"title": "5 Conclusion and Discussion", "content": "In this work, we propose C-INSTRUCTOR, which generates style-controllable and\ncontent-controllable instructions with high linguistic quality. It uses an adapter-\nbased structure to leverage the language capability of LLMs and distinct style\nprompts in SMT to achieve style control. To enhance the executability of gener-\nated instructions, we adopt CoTL to help identify crucial landmarks and provide\ncontent controllability. We also devise STMT to enhance the model's under-\nstanding of the environment's spatial topology. The instructions generated by\nC-INSTRUCTOR not only achieve high scores in text metrics but also demon-\nstrate strong competence in guiding navigators, further validating the strong\ncorrespondence between generated instructions and given trajectories. We ex-\npect that C-INSTRUCTOR can greatly enhance agent-human communication and\nsignificantly contribute to the development of versatile embodied agents."}, {"title": "A Detailed Prompts", "content": "In this section, we provide detailed prompts for different navigation datasets.\nNote that all the given prompts are then formatted by prompt templates in [20].\nprompt for R2R [5]: You are given a sequence of views of a path. Please\nextract critical landmarks in the path.\nprompt for R2R [5]: You are given a sequence of views of a path in an indoor\nenvironment. Please describe the path according to the given landmarks in\ndetail for an intelligent agent to follow. Landmarks: <landmarks>.\nprompt for REVERIE [48]: You are given a sequence of views of a path in\nan indoor environment. Please extract several critical landmarks in the path\nfor generating a brief high-level target-oriented instruction.\nprompt for REVERIE [48]: You are given a sequence of views of a path in\nan indoor environment and critical landmarks for a brief high-level target-\noriented instruction. Please generate the indicated high-level target-oriented\ninstruction briefly for an intelligent agent to follow. Landmarks: <landmarks>.\nprompt for RxR [31]: You are given a sequence of views of a path in an\nindoor environment. Please extract critical landmarks describing the starting\nposition and the path.\nprompt for RxR [31]: You are given a sequence of views of a path in an\nindoor environment. Please describe the starting position and the path ac-\ncording to the given landmarks in detail for an intelligent agent to follow.\nLandmarks: <landmarks>.\nprompt: You are an intelligent embodied agent that navigates in an indoor\nenvironment. Your task is to move among the static viewpoints (positions)\nof a pre-defined graph of the environment. You are given several candidate\nviews. You are also given a sequence of panoramic views showing previous"}, {"title": "B Extra Ablations on Landmark Selection", "content": "B.1 Selection Strategies\nTo validate the effectiveness of our landmark selection strategy, we conducted\nseveral experiments with several ablative strategies on REVERIE [48] and R2R [5]\nval unseen splits. The results are shown in Tab. 7.\n#1 is the baseline result without landmarks and the CoT process. The model\nin #2 uses only landmarks from instructions Ax in CoTL. Compared to #1, the\nSPICE metric remarkably increases, which indicates a more accurate description\nof object relations in the instructions. Other metrics fluctuate. Based on #2,\nthe model in #3 adds visual landmarks via spatial selection, which are denoted\nas Aa. Compared to #2, almost all metrics rise, which demonstrates the value\nof visual landmarks. The model in #4 adds visual landmarks via spatial and\ntemporal selection A in addition to landmarks from instructions, resulting in an\nincrease in almost all scores compared to #3. The results above further confirm\nthe effectiveness of the proposed landmark selection mechanism.\nB.2 Values of B\nWe conduct ablations on the numerical values of \u1e9e in landmark selection on\nREVERIE [48] and R2R [5] val unseen splits. The results are presented in\nTab. 8. It can be observed that assigning \u1e9e to 0.25 achieves the best performance."}, {"title": "C Further Analysis on STMT", "content": "In Fig. 6, we plot the curve illustrating the model's validation loss on R2R [5] val\nunseen during the training process. Compared to the baseline without STMT,\nWe can observe that STMT effectively prevents overfitting as evidenced by the\nfact that its validation loss does not exhibit a gradual increase compared to the\nbaseline. STMT effectively ensures the training stability of C-INSTRUCTOR and\nalso enhances the instruction quality."}, {"title": "D Additional Qualitative Results", "content": "In Fig. 7, we provide more visualizations of navigation trajectories and corre-\nsponding instruction generation results. As observed, C-INSTRUCTOR effectively"}, {"title": "E More Discussion", "content": "Social Impact. C-INSTRUCTOR can be used to provide feedback from intelligent\nembodied agents to humans as well as to guide humans who are unfamiliar\nwith the environment. It can also serve as accessibility facilities for the visually\nimpaired to find their way.\nLimitations. Due to data availability, C-INSTRUCTOR is trained on simulated\ndata with discrete viewpoints, which limits its performance in real-world con-\ntinuous environments. Moreover, as discussed in \u00a7D, C-INSTRUCTOR possesses\nlimited ability in modeling the global structure of the environment, resulting in\ninaccurate instructions when referring to the global location of a specific object\nor room in the environment.\nFuture Work. We plan to devise a mechanism that encodes the global struc-\nture of the environment into the instruction generator. With knowledge of the\nenvironment, the instruction generator can locate the user according to free-\nform natural language descriptions and provide path guidance according to the\ndestination designated by the user."}]}