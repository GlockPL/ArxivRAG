{"title": "Registration-Enhanced Segmentation Method for Prostate Cancer in Ultrasound Images", "authors": ["Shengtian Sang", "Hassan Jahanandish", "Cynthia Xinran Li", "Indrani Bhattacharya", "Jeong Hoon Lee", "Lichun Zhang", "Sulaiman Vesal", "Pejman Ghanouni", "Richard Fan", "Geoffrey A. Sonn", "Mirabela Rusu"], "abstract": "Prostate cancer is a major cause of cancer-related deaths in men, where early detection greatly improves survival rates. Although MRI-TRUS fusion biopsy offers superior accuracy by combining MRI's detailed visualization with TRUS\u2019s real-time guidance, it is a complex and time-intensive procedure that relies heavily on manual annotations, leading to potential errors. To address these challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation method that identifies prostate tumors directly in TRUS images without requiring manual annotations. Unlike traditional multimodal fusion approaches that rely on naive data concatenation, our method integrates a registration-segmentation framework to align and leverage spatial information between MRI and TRUS modalities. This alignment enhances segmentation accuracy and reduces reliance on manual effort. Our approach was validated on a dataset of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132) methods, with significant improvements (p < 0.01). This framework demonstrates the potential for reducing the complexity of prostate cancer diagnosis and provides a flexible architecture applicable to other multimodal medical imaging tasks.", "sections": [{"title": "1. Introduction", "content": "Prostate cancer is the second most common cancer and the fifth leading cause of cancer-related death among men [1, 2]. Early detection of prostate cancer is crucial for effective treatment, as patients diagnosed in an early stage can have a 5-year survival rate that exceeds 99% [3]. Transrectal ultrasound (TRUS)-guided biopsy is the most common method for diagnosing prostate cancer because TRUS provides real-time imaging of the prostate [4]. However, due to the low signal-to-noise ratio of ultrasound images, up to 52% of clinically significant cancer lesions may be missed during TRUS-only biopsy procedures [5]. In contrast, while magnetic resonance imaging (MRI) has challenges with real-time imaging, it produces clearer images of the prostate gland and is more effective at identifying cancerous areas [6]. As a result, MRI-TRUS fusion biopsy is considered a more advanced technique for the diagnosis of prostate cancer [7]. In clinical practice, a multiparametric MRI (mpMRI) scan is typically performed first to evaluate the likelihood of prostate cancer and to locate any suspicious lesions. Radiologists delineate the prostate and mark suspicious regions on the MRI images. These MRI images are then fused with real-time ultrasound images during the procedure. Using ultrasound as real-time guidance, urologists perform a fusion-guided biopsy to accurately direct the needle into the suspicious regions identified on the [8, 9, 10], as illustrated in Fig.1a. As the reliability of MRI-TRUS fusion-guided biopsy continues to be demonstrated, this technique has gained widespread popularity worldwide [5]. However, it remains more complex and time-consuming than the traditional TRUS-guided biopsy [11, 12]. The process often involves identifying potential cancerous areas in the MRI and annotating the prostate in both MRI and TRUS images. While advances in automation have reduced the manual effort required, significant time and expertise are still needed to ensure accurate annotation and image fusion [13]. Furthermore, if the physician identifies areas on the MRI that are not cancerous (false positives) or misses tumor areas (false negatives) [14], these errors may propagate to the fused TRUS images, potentially affecting biopsy accuracy. While real-time TRUS guidance allows for some degree of clinical correction based on ultrasound characteristics, the accuracy of the procedure remains heavily dependent on the quality of MRI interpretation and image fusion.\nThis work presents an automatic MRI-TRUS fusion-based segmentation method that can identify suspected tumor regions in TRUS images without requiring manual annotations, thereby facilitating subsequent biopsy procedures. Traditional multimodal fusion techniques often train models by simply concatenating data from different modalities at various stages [15, 16, 17, 18]. However, such straightforward concatenation of prostate MRI and ultrasound data fails to effectively enhance tumor segmentation performance. This limitation arises because prostate-related information is frequently misaligned between the initial MRI and ultrasound images due to differences in patient positioning, imaging protocols, or inherent modality disparities (as illustrated in the upper part of Fig.1b). Accurate segmentation relies heavily on spatial information [19], which is not adequately preserved or utilized in naive concatenation approaches. When MRI and ultrasound images are combined for training, misaligned tumor location information from MRI often interferes with the model's predictions on TRUS data, leading to degraded performance. To address this, we propose a novel registration-segmentation multimodal fusion technique that progressively aligns prostate information from MRI and TRUS images (as shown in the bottom part of Fig.1b). We validated our method using data from 1,747 patients at Stanford Hospital. For the tumor segmentation task, our method achieved an average Dice coefficient of 0.212, compared to 0.117 for models trained with TRUS alone and 0.117 for models trained with naive MRI/TRUS fusion. This corresponds to relative improvements of 81.2% and 60.6%, respectively. These results were statistically significant (p\u00a1 0.01), demonstrating the superior performance of our approach in accurately segmenting tumor regions. Our main contributions are three-fold:\n\u2022 We propose a fully automatic MRI-TRUS fusion-based segmentation method to enhance tumor identification in ultrasound images by improving segmentation accuracy. This approach eliminates the need for physicians to manually annotate tumors on MRI, significantly reducing both time and effort.\n\u2022 Our experiments reveal that simply increasing the amount of data does not always improve prostate tumor segmentation. Instead, effective utilization of raw multimodal data is crucial for enhancing AI model performance. To address this, we propose a registration-segmentation fusion approach that optimally leverages spatial and modality-specific features.\n\u2022 The proposed registration-segmentation framework is highly flexible and can be easily adapted to various registration and segmentation methods. Furthermore, it is applicable to other disease segmentation tasks involving multimodal data, such as liver cancer or brain tumor segmentation."}, {"title": "2. Related Work", "content": "2.1. Al in Prostate Cancer Diagnosis and Biopsy\nUltrasound-guided biopsy is the most widely used approach due to its real-time imaging capability and relatively low cost. Grayscale transrectal ultrasound (TRUS) [20] is the most commonly adopted imaging method. However, its inherently low signal-to-noise ratio makes tumor identification challenging, with more than 50% of tumors likely to be missed[21, 22]. Recent advances in ultrasound-based imaging (such as shear-wave elastography[23, 24], color Doppler ultrasound[25], contrast-enhanced ultrasound[26], and micro-ultrasound[27]) have shown promise in improving tumor clarity by providing additional functional and structural information. Despite these advancements, relatively few studies have leveraged AI technologies to detect prostate tumors by analyzing ultrasound images alone[20]. Some researchers have developed various machine learning and deep learning methods to identify prostate tumors from ultrasound images[28, 29, 30, 31]. However, due to the low-quality and unclear nature of prostate ultrasound images, AI-based analysis typically achieves limited performance, with Dice scores often reported to be low[32].\nMRI is increasingly utilized for detecting prostate cancer[20, 32]. It plays a critical role in guiding MRI-ultrasound fusion biopsies and supporting treatment planning. It is widely recognized as the most sensitive noninvasive imaging modality, capable of accurately visualizing, detecting, and localizing prostate cancer. Recent advancements have shown promising results in leveraging AI for prostate cancer detection on MRI[33, 34]. These AI-driven approaches primarily aim to identify tumors on MRI scans and subsequently map the detected tumors onto ultrasound images during clinical procedures, facilitating precise guidance for biopsies.\nHere, we propose an approach similar to TRUS-MRI fusion, where MRI and TRUS are used to train the model. However, instead of identifying tumors on MRI and mapping them onto TRUS, our method directly identifies tumors on TRUS images. This approach eliminates the performance degradation caused by registration errors, offering a more streamlined and accurate solution for tumor detection.\n2.2. Multimodal Fusion Method\nMultimodal fusion methods, which integrate various types of data, have become a crucial approach to improving model performance in numerous tasks[16]. The primary strategies for multimodal data fusion can be categorized into early fusion, intermediate fusion, and late fusion. These methods differ mainly in the stages at which the data are combined during the model's processing. Early fusion involves concatenating different data types before entering them into the model[35, 36, 37]. This approach allows the model to learn from the combined features of the input data simultaneously[38, 39, 39, 40]. Intermediate fusion, on the other hand, concatenates the features extracted by the model from the different modalities. This method leverages the model's ability to independently extract relevant features from each modal-"}, {"title": "3. Materials and Methods", "content": "3.1. Dataset Acquisition and Preprocessing\nOur study received approval from the Institutional Review Board (IRB) at Stanford University and included 1,747 patients who underwent MRI-TRUS fusion-targeted biopsy using the Artemis system. The Hitachi Hi-Vision 5500 7.5 MHz end-firing ultrasound probe was employed to acquire 3D TRUS scans by rotating the probe 200 degrees around its axis and subsequently interpolating the scans to isotropic resolution (voxel spacing: ~0.5 mm). A 3 Tesla GE scanner with external 32-channel body array coils was used during the MRI examination. Axial T2-weighted MRI images (acquired using a 2D Spin Echo protocol) from each patient were resampled to the same spatial resolution (~0.5 mm) in the axial plane, with a distance of 3 mm between slices and were used in this study.\nPreprocessing. The TRUS and MRI scans had varying voxel spacings and matrix sizes. To standardize the input data, we resampled the ultrasound and MRI images using trilinear interpolation to achieve an isotropic voxel size of 0.5 mm. This was followed by cropping and resizing the images to a fixed size of 256\u00d7256\u00d7256. Furthermore, we normalized the TRUS image intensities using z-score normalization, computed based on the mean and standard deviation of intensities within the prostate.\nGround Truth Labels. We derived the ground truth labels through a multi-step process. First, radiologists outlined lesions on MRI to obtain the initial cancer labels, which were subsequently mapped onto TRUS images using the non-rigid registration provided by the Artemis device. These preliminary labels were then refined using MRI-ultrasound biopsy results to correct the manually annotated tumors. Finally, an expert manually annotated the labels on the TRUS images. The dataset was randomly divided into training, validation, and test sets, consisting of 1,116, 280, and 351 patients.\n3.2. Registration-Enhanced Segmentation Method\nThe main idea of this method is to integrate the registration and segmentation processes simultaneously during multimodal data training. The interference introduced by multimodal data fusion into the segmentation model is mitigated by progressively aligning the MRI and ultrasound data through the registration method. Figure 2 illustrates the framework of the approach, with blocks \u2463 and \u2468 representing the registration and segmentation methods, respectively. The input to the method comprises preprocessed MRI and ultrasound images with identical dimensions, denoted as $M_{init}$ and $T_{init}$, where $M_{init} \u2208 R^{W\u00d7H\u00d7D}$ and $T_{init} \u2208 R^{W\u00d7H\u00d7D}$.\nStep 1. The feature extraction modules, consisting of two convolutional neural networks (CNNs) and activation functions, are used to extract features from both MRI and TRUS images. The stride of the first CNN is set to 2, reducing the feature size to $R^{\\frac{W}{2}\u00d7\\frac{H}{2}\u00d7\\frac{D}{2}}$, with the extracted features denoted as $M_{fea}$ and $T_{fea}$ for MRI and TRUS, respectively. This step is designed to capture prostate-related information from both modalities. In particular, we formulate this procedure as eq. (1):\n$M_{fea} = Conv_1(Conv_1(M_{init})),$\n$T_{fea} = Conv_2(Conv_2(T_{init})),$ (1)\nwhere $M_{fea}$ and $T_{fea}\u2208R^{\\frac{W}{2}\u00d7\\frac{H}{2}\u00d7\\frac{D}{2}}$.\nStep 2. Average pooling is used to downsample the original MRI and ultrasound images, reducing their dimensions to align with the feature size extracted in Step 1. The downsampled data, which preserves the original input features, is then combined with the features obtained in the first step. By combining the features extracted in steps 1 and 2, we ensure that all original features are maintained while the key features are highlighted. This process is described by the following eq. (2):\n$M_{merge} = AvgPool(M_{init}) + M_{fea}$,\n$T_{merge} = AvgPool(T_{init}) + T_{fea},$ (2)\nwhere $M_{merge}$ and $T_{merge} \u2208 R^{\\frac{W}{2}\u00d7\\frac{H}{2}\u00d7\\frac{D}{2}}$.\nStep 3. Step 3: $M_{merge}$ and $T_{merge}$ are concatenated along the channel dimension and then fed into the registration module. Our registration module aims to align not only the prostate regions in MRI and TRUS images but also the features extracted from both modalities, enabling the model to learn shared critical features for better alignment and improved multimodal fusion.\nStep 4. The input for the registration module consists of $T_{merge}$ and $M_{merge}$ and the output is the affine transformation matrix $A$ which is used to align $M_{merge}$ with $T_{merge}$. The architecture involves patch-splitting, patch-merging and Transformer-based encoder layers[47] as shown in Fig.3. Same as ViT [48], the inputs $M_{merge}$ and $T_{merge}$ are first splitted into non-overlapping image patches by using a sliding window with stride k. The input images are then reshaped into a matrix of size $R^{N\u00d7k^3}$, where N represents the number of patches, calculated as $\\frac{W}{k}\u00d7\\frac{H}{k}\u00d7\\frac{D}{k}$. The patch embeddings of $M_{merge}$ and $T_{merge}$ are then concatenated to form a combined embedding matrix of size $R^{N\u00d72k^3}$. A linear layer is subsequently applied to transform this concatenated embedding into $Z^0$ of size $R^{N\u00d7C}$. Transformer blocks are then applied to $Z^0$, consisting of a multi-head self-attention (MSA) module, followed by a 2-layer MLP[49] with GELU nonlinearity[50]. Finally, a multi-linear layer is used on the output of encoder to generate the affine transformation matrix A. This process is described by the following eq. (3):\n$Z^0 = IM(S(T_{fea}, M_{fea})),$\n$Z^l = MSA(LN(Z^{l-1})) + Z^{l-1}, l = 1,2...n,$\n$A = MLP(LN(Z^n)),$ (3)\nwhere $Z^0$ and $Z^l \u2208 R^{N\u00d7C}, A \u2208 R^{3\u00d74}$. $ and IM represent the patch splitting and patch merging, respectively.\nStep 5. The transformation matrix A is used to transform $M_{merge}$ to $\\hat{M}_{merge}$ which align with $T_{merge}$. This process is described by the following eq. (4):\n$\\hat{M}_{merge} = T(M_{merge}, A),$\nT represents the affine transform option, $\\hat{M}_{merge} \u2208 R^{\\frac{W}{2}\u00d7\\frac{H}{2}\u00d7\\frac{D}{2}}$ (4)\nStep 6. This matrix is used not only to align the features extracted from $M_{init}$ and $T_{init}$ but also to align the initial images.\n$\\hat{M}_{init} = T(M_{init}, A),$\nwhere $\\hat{M}_{init} \u2208 R^{W\u00d7H\u00d7D}$. (5)\nStep 7. $T_{merge}$ and $\\hat{M}_{merge}$ are first upsampled to the same size of $M_{init}$ and $T_{init}$. Then the upsampled data are integrated into the $T_{init}$ and $M_{init}$ for segmentation. This process is described by the following:\n$M_{up} = U(\\hat{M}_{merge}),$\n$T_{up} = U(T_{merge}),$ (6)\nwhere U indicates the upsampling operation, $M_{up}$ and $T_{up} \u2208 R^{W\u00d7H\u00d7D}$\nStep 8. $T_{up}$ and $M_{up}$ are added to corresponding initial image modalidy $T_{init}$ and $M_{init}$, respectively. Then they are concatenated along with the first dimension.\n$\\hat{M}_T = C(M_{up} + \\hat{M}_{init}, T_{up} + T_{init}),$ (7)\nC represents the concatinate operation.\nStep 9. The concatenated MRI and TRUS are input into the segmentation module, and the output is the segmentation mask of the prostate cancer Y, where $Y \u2208 R^{W\u00d7H\u00d7D}$\nY = Seg(MT), (8)\nSeg represents the segmentation method. In this work, we adopted U-Net as the segmentation module, as it is one of the most widely used segmentation methods and serves as a suitable choice to validate our proposed registration-segmentation framework. In practice, it can be easily replaced with other segmentation methods.\n3.3. Training Loss\nThe key idea of our method is to align information from multimodal prostate data to improve tumor segmentation. Specifically, the registration process aligns both the extracted features of the ultrasound and magnetic resonance data and the original images. As a result, our loss function consists of three components: the loss of registration for the original images, the loss of similarity for the extracted features, and the loss of segmentation for the tumor. $G_M$ and $G_T$ represent the ground truth for MRI and TRUS images, where $G_M$ and $G_T \u2208 R^{3\u00d7W\u00d7H\u00d7D}$ represent the ground truth of the each voxel, corresponding to \"other\", \"prostate\" and \"tumor\", respectively.\nLoss for alignment: We employ weighted dice loss to optimize the registration module which is defined as:\n$L_{reg} = 1 - \\frac{\\sum_{i=1}^{3} W_c (2 \\sum_{n=1}^{W\u00d7H\u00d7D}(\\hat{G}_{M_n}^{(c)} \\cdot G_{T_n}^{(c)})) + \\epsilon}{\\sum_{i=1}^{3} W_c (\\sum_{n=1}^{W\u00d7H\u00d7D}(\\hat{G}_{M_n}^{(c)})^2 + \\sum_{n=1}^{W\u00d7H\u00d7D}(G_{T_n}^{(c)})^2) + \\epsilon}$ (9)\nwhere $\\hat{G}_M$ is the transformed mask obtained by applying the transformation matrix A, such that $\\hat{G}_M = T(G_M, A)$, n = W \u00d7 H \u00d7 D represents the total number of voxels, $W_c$ denotes the weight of the class c, and \u03b5 is a small constant added to prevent division by zero. To reflect the importance of each class, we assign a higher weight to the tumor class compared to the prostate and other classes.\nLoss for feature distribution: KL divergence is used to measure the similarity between the feature distributions of $M_{merge}$ and $T_{merge}$, ensuring that similar features are extracted from both modalities. As shown in step (10) of Fig.2, the feature distributions of $M_{merge}$ and $T_{merge}$ are extracted using convolutional neural networks, followed by softmax layers. These feature distributions are denoted as $M_{dis}$ and $T_{dis} \u2208 R^d$, respectively. We then compute the similarity between their probability distributions. The KL divergence loss function for $M_{dis}$ and $T_{dis}$, both in d-dimensional space, is given by:\n$L_{KL} = \\sum_{i=1}^{d} T_{dis}(i) log \\frac{T_{dis}(i)}{M_{dis}(i)}$ (10)\nThe purpose of using KL divergence here is to ensure that both $M_{fea}$ and $T_{fea}$ extract features with the same distribution from the original image for accurate registration. Since the convolution-based feature extractor preserves the spatial positions of the features, A is applied to adjust the spatial alignment of $M_{fea}$'s features. Loss for segmentation: The segmentation loss function $L_{seg}$ is a combination of loss of the dice and loss of the focal. Dice loss maximizes the overlap between the predicted segmentation and the ground truth, while Focal Loss addresses class imbalance by focusing more on hard-to-classify examples.\n$L_{seg} = L_{dice} + L_{focal},$\n$L_{dice} = 1-\\frac{2 \\sum_{c=1}^{3} \\sum_{i,j,k} (Y^{(c)}(i, j, k) \\cdot G^{(c)}(i, j, k))}{\\sum_{c=1}^{3} \\sum_{i,j,k} (Y^{(c)}(i, j, k) + G^{(c)}(i, j, k))}$ (11)\n$L_{focal} = -\\sum_{c=1}^{3} \\alpha \\sum_{i,j,k} (1 \u2212 y^{(c)}(i, j, k))^{\\gamma} \\log(\\hat{Y}^{(c)}(i, j, k))),$ (12)\n$L_{focal} = -\\sum_{c=1}^{3} \\alpha \\sum_{i,j,k} (y^{(c)}(i, j, k))^{\\gamma} \\log(\\hat{y}^{(c)}(i, j, k))),$ (13)\nThe final loss of our method is:\n$L = \\alpha L_{seg} + \\beta L_{KL} + \\lambda L_{seg},$ (14)\nIn this equation, \u03b1, \u03b2 and \u039b is balancing factors that controls the contribution of the each loss.\n3.4. Implementation\nThe training process is divided into two phases: a) the pretraining phase, which optimizes the parameters of registration, i.e., pre-training the network by using the registration loss functions, and b) the segmentation optimization phase, we take the encoder weights derived from pre-training as initial values and freeze them to optimize the decoder for segmentation. Our method is trained on a standalone workstation equipped with a Nvidia RTX A6000 GPU and an Intel Core i7-7700 CPU. We adopt the Adam optimizer with a fixed learning rate of 1e-4 and batch size sets of 1 for all learning-based approaches. To accelerate the convergence of the model during the overall training process, we conducted a 100-epoch pre-training on the registration component. This step helps to stabilize the initial learning phase and improves the model's performance when training in an end-to-end manner.\n3.5. Evaluation Metrics\nQuantitative evaluations of various models were performed at both the lesion and patient levels. We assessed performance using metrics such as the area under the Receiver Operating Characteristic curve (ROC), sensitivity (SE), specificity (SP), negative predictive value (NPV), positive predictive value (PPV), and accuracy (ACC).\nLesion Level: The assessment of true positive and false negative lesions was determined based on the overlap between predicted and actual lesions. A detection was classified as a true positive if the predicted labels overlapped with at least 1% of the actual lesion; otherwise, it was considered a false negative. To distinguish true negative and false positive lesions, the prostate was divided into six segments. A segment was classified as ground truth negative if it contained less than 1% actual cancer voxels. Conversely, if 99% or more of the predicted labels in a segment were normal, it was categorized as a true negative. Otherwise, it was considered a false positive.\nPatient-Level: In the patient-level evaluation, a patient was considered a true positive if the models correctly detected at least one lesion. If no lesions were correctly identified, the patient was classified as a false negative."}, {"title": "4. Results", "content": "4.1. Quantitative Evaluation and Analysis\nCurrent multimodal techniques typically train models by concatenating all available data at various stages [15, 16, 17, 18]. We evaluated the effectiveness of training models for prostate cancer segmentation by combining ultrasound and different MRI modalities. Table 1 presents the experimental results obtained by training models directly using ultrasound images and different types of MRI data, as well as the results from our proposed method. From the table, it can be observed that:\n1) Training models directly with combined ultrasound and magnetic resonance data does not yield better performance than using ultrasound data alone, indicating that incorporating more data does not necessarily lead to improved results. For example, Table1 illustrates the performance of the Unet model[51] trained with identical parameter settings using TRUS alone and TRUS combined with MRI (denoted as TRUS&MRI) simultaneously for the prostate tumor segmentation task. Although the \"TRUS&MRI\u201d utilizes more data for model training, there are no significant improvements for model performance. This is because segmentation tasks are sensitive to spatial information, and the prostate's position in MRI and ultrasound data differs significantly. This discrepancy causes the prostate-related information in the MRI data to interfere with the model, introducing noise-like artifacts. As illustrated in Fig.4a, the prostate and cancer regions in the initial TRUS and MRI data are misaligned. When MRI and TRUS data are combined and fed into the model for training, the regions marked as cancer on MRI labels may not correspond to cancer in the TRUS data, leading to noise and misguided direction. Table3 presents statistics on prostate and tumor regions in the initial TRUS and MRI data, with prostate metrics of 0.597 and 0.071, respectively. These discrepancies highlight the spatial misalignment of prostate-related information, which hinders performance improvements when directly using additional data.\n2) Our method significantly improves the model's ability to identify prostate tumor using multimodal data. Compared to the Unet model, our approach achieves substantial improvements in all metrics for various combinations of modality. For example, while the Unet model trained on TRUS and multisequence MRI data achieved a dice score of 0.132, our method improved this score to 0.212 - an overall average improvement of 92% for the dice metric. Similar improvements were observed in other evaluation metrics. This is because our method does not directly train the segmentation model on multimodal data but instead aligns the different modalities within the model before segmentation. Figure4b illustrates the alignment of the prostate regions in the initial ultrasound and magnetic resonance data and the intermediate results produced by our registration module. The improved alignment achieved by our method enhances the ability of the segmentation module to accurately segment tumors.\nThe above results demonstrate the importance of modality alignment in using multimodal data for prostate"}, {"title": "4.2. Decoupling the Segmentation and Registration Steps", "content": "The analysis of the results in Table2 and Fig.4 indicates that the alignment of similar information significantly affects the performance of models trained on multimodal data. In response to this observation, we analyzed the performance of a segmentation model trained using aligned multimodal data. The experimental pipeline was conducted in two stages. First, the ultrasound and MRI data were aligned based on the prostate gland in the images. The aligned ultrasound and magnetic resonance data were then used to train the semantic segmentation model. This experiment had two main objectives: 1) to analyze the impact of data alignment (via registration methods) on segmentation performance, and 2) to evaluate the advantages of our method compared to the stage-wise approach for registration and segmentation.\nFor the registration methods, we employed the widely used ANTs[52] method along with three deep learning-based registration approaches: ConvNet[53], VTN-Affine[54] and C2FViT[55]. ANTS[52] was selected as a standalone registration module because it is a representative optimization-based registration method, while the other three methods are state-of-the-art deep learning-based approaches. These methods were chosen to evaluate how registration techniques based on different principles influence the segmentation results. Ours (Independent) refers to our method in which the registration and segmentation modules are trained separately. The overall experimental results are summarized in Table2. The following observations can be drawn from the table:\n1) The alignment of information in multimodal data significantly enhances the performance of segmentation models. For example, training segmentation models using multimodal data aligned by ConvNet[53], VTN-Affine[54] and C2FViT[55], and Ours (independent) led to substantial improvements in the model's ability to identify tumors in ultrasound data. Specifically, compared to directly training the model on the original data, which achieved a tumor recognition Dice score of 0.132, the aligned data obtained using ConvNet[53], VTN-Affine[54] and C2FViT[55] improved the Dice score by 18.94%, 28.03%, and 52.27%, respectively. These results demonstrate that aligning MRI data with ultrasound data based on the prostate gland significantly enhances the model's ability to identify prostate tumors from ultrasound data alone. This finding underscores the critical role of multimodal data alignment in improving segmentation performance.\n2) Simultaneous training of segmentation and registration modules can improve the performance of both tasks. As shown in Table2, when the registration and segmentation modules of our method are trained separately, the registration module achieves a performance of 0.779, while the corresponding segmentation performance is 0.169. In contrast, simultaneous training improves the registration performance to 0.817 and significantly increases the segmentation performance to 0.212. The models used in stage-wise training and simultaneous training are identical, with the only difference being that simultaneous training employs an end-to-end approach. In this approach, registration and segmentation are integrated through steps 6 and 10, as described in the section3.2, and the loss function outlined in Equation14. Specifically, the registration module aligns MRI data with ultrasound data for the segmentation module, while the segmentation module's predictions of the prostate and tumor are fed back to refine the registration process. This joint training framework allows the registration and segmentation modules to mutually enhance each other's performance, highlighting the effectiveness of integrating these tasks.\n3) Higher prostate alignment may not always result in improved performance in predicting prostate tumors. For example, although C2FViT[55] achieves the highest registration performance of 0.814, slightly exceeding our method's performance of 0.807, our method achieves a tumor segmentation performance of 0.212, which surpasses C2FViT's corresponding performance of 0.201. This discrepancy underscores that prostate registration performance is not linearly correlated with tumor segmentation performance. This arises from the focus of registration methods adopted in the experiment, which primarily align prostate gland data between ultrasound and MRI without ensuring improved alignment of the associated tumor information. To investigate this, we analyzed the prostate gland and prostate tumor registration results, as presented in Table3. Although our method slightly underperforms C2FViT in prostate gland registration, it achieves a supe-"}, {"title": "4.3. Qualitative Evaluation of Registration and Segmentation Results", "content": "Our proposed method improves the model's ability to identify prostate tumors from TRUS by aligning prostate-related information across multiple modality data. In our framework, we adopt an affine registration approach to align information. Although the model's primary objective is to identify prostate tumors in TRUS and not to explicitly output multimodal registration results, the registration process is a critical component of the model. Here, we show and analyze the prostate registration results (intermediate outputs) and the prostate tumor segmentation results.\nFigure5 shows the registration results for TRUS and T2-weighted MRI data. Due to space constraints, only the outcomes of the optimization-based method (ANTS[52]) and the deep learning method (VTNAffine[54]) are displayed, with additional results provided in Supplementary Material 1. Since prostate imaging involves 3D data, the registration is performed in three dimensions. To better demonstrate this, five representative slices of TRUS and MRI images are presented, evenly spaced along the prostate's spatial extent from the first to the last slice containing the prostate. From Figure5 we can know that:\n1) Significant initial spatial divergence: The unregistered TRUS and MRI data show substantial spatial misalignment of the prostate gland. For example, the prostate regions in the TRUS and T2-weighted images are significantly misaligned across the sagittal, axial, and coronal planes. 2) Challenges in multimodal prostate registration: Unlike multimodal imaging of organs such as the brain or spine, where anatomical features remain relatively consistent across modalities, the appearance of the prostate in TRUS and MRI differs significantly. This substantial disparity in modality-specific characteristics, as discussed in the previous section, presents a significant challenge for traditional registration methods like ANTs[52] and underscores the difficulty of achieving accurate multimodal registration for the prostate. 3) Our method demonstrates improved registration performance compared to previous approaches, particularly in aligning the peripheral edges of the prostate. For example, while VTN enhances alignment in the central slices, it fails to accurately align the prostate edges, as observed in slices 1 and 5. Accurate alignment of the peripheral prostate is essential for tumor detection, as tumors are more frequently located in the peripheral zone[56, 57]. In contrast, our method successfully aligns these critical edge regions, enabling more reliable tumor segmentation.\nFigure6 presents the prostate tumor segmentation results on TRUS images from different models. To provide a more comprehensive visualization of tumor detection in 3D TRUS images, we include three representative slices: the first and last slices containing the prostate tumor and a middle slice between them. Figure6 shows that: 1) Limited segmentation performance without reg-"}, {"title": "4.4. Epochs between Registration and Segmentation", "content": "We investigated the interaction between our method's registration and segmentation modules during the training process. Figure7 illustrates the model's registration and segmentation performance across different training epochs. The results shown in the figure represent the model's outputs on the same test dataset, with each epoch marking the specific checkpoint during training when the corresponding model was saved.\nAs shown in Fig.7, the performance of both the registration and segmentation modules improves simultaneously as training progresses. For instance, during the early stages of training (epoch 5), there is significant spatial misalignment between the prostate in the TRUS and MRI data, and the model fails to detect prostate tumors in the TRUS image. As training continues, registration performance gradually improves, with the alignment metric increasing from 0.495 at epoch 5 to 0.769 at epoch 50. At the same time, the segmentation capability also improves, with the model beginning to detect tumors at epoch 10 and segmentation performance progressively increasing from 0.044 at epoch 10 to 0.259 at epoch 50. These results demonstrate that, in our method, the registration and segmentation modules are jointly optimized during training. The improvement in tumor segmentation performance is directly correlated with the enhanced alignment capability of the model, emphasizing the importance of accurate registration for achieving effective tumor segmentation in multimodal data.\nSpecifically, the registration module's output serves as input to the segmentation model, which, in turn, guides and constrains the registration module's learning process. This interdependent relationship enhances segmentation accuracy by improving spatial consistency through registration, while segmentation provides additional structured information to guide the registration model toward more accurate anatomical alignment. In contrast, most joint registration-segmentation methods employ two independent encoder-decoder structures for registration and segmentation tasks, training these components separately. This differs from our approach, where we integrate the"}, {"title": "Discussion", "content": "Why adopted affine registration for data alignment? Common registration methods include rigid registration, affine registration, and deformable registration. We selected affine registration primarily because it offers better performance compared to rigid registration while requiring significantly less computational resources than deformable registration. Specifically, affine registration computes a 4x4 transformation matrix to align the spatial positions of MRI data with TRUS, which can then be used for training the segmentation model. In contrast, deformable registration generates a deformation field that is three times the size of the MRI data volume, significantly increasing"}]}