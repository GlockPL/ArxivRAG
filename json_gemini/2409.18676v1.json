{"title": "Toward Universal and Interpretable World Models for Open-ended Learning Agents", "authors": ["Lancelot Da Costa"], "abstract": "We introduce a generic, compositional and interpretable class of generative world models that supports open-ended learning agents. This is a sparse class of Bayesian networks capable of approximating a broad range of stochastic processes, which provide agents with the ability to learn world models in a manner that may be both interpretable and computationally scalable. This approach integrating Bayesian structure learning and intrinsically motivated (model-based) planning enables agents to actively develop and refine their world models, which may lead to open-ended learning and more robust, adaptive behavior.", "sections": [{"title": "1 Introduction", "content": "A feature of open-ended learning agents is the ability to gradually understand wide-ranging, complex worlds. Computational accounts of cognitive development in humans and animals frame this process as approximate Bayesian inference over spaces of Bayesian networks, where inferences are continuously refined thanks to sensory data arriving from active (intrinsically motivated) interactions with the environment [1]. Current modelling approaches to development, while impressive, face scalability challenges, due to the explosive nature of inferring over all possible Bayesnets [2,3]. See Appendix A for more context. Thus, a crucial challenge to enable developmental agents is finding Bayesnet spaces that can express a wide range of worlds, while being coarse enough to efficiently be searched."}, {"title": "2 A generic, interpretable and agentic class of generative models", "content": "Desiderata for this Bayesnet class include being: (i) Sufficiently expressive to be able to approximately express relevant naturalistic, dynamic interactions between agent and environment. (ii) Sufficiently coarse so that inference on this space can be made computationally tractable. Each Bayesnet in this class should be: (iii) Interpretable so that the agents' understanding and ensuing behaviour can be easily understood from the Bayesnets it entertains. (iv) Support fast action, perception and learning. We summarise a candidate class of Bayesnets satisfying requirements (i)-(iv) in Appendix B, extending earlier proposals [4]."}, {"title": "3 Discussion", "content": "The expressivity of this class lies in its ability to represent complex stochastic processes, including those with both discrete and continuous states. By utilizing hierarchical structures, these models can capture both high-level abstractions and fine-grained details of the environment, allowing agents to form robust and generalizable representations. The interpretability of these models stems from their sparsity and encoding of causal relationships\u2014this provides insights into the agent's decision-making, facilitating more transparent and trustworthy AI systems. This class of Bayesnets has been used to model video from raw pixels and sound files, and agents that plan from pixels [5]. Future work should seek the limits of this approach, including its scalability and ability to express relevant agent-environment interactions."}, {"title": "A Current challenges in developmental agents", "content": "Engineering agents that scalably learn models of the world remains a relatively open challenge [6], despite the fact that cognitive development of humans and animals is well-studied within computational cognitive science [1]. Several strands of work in computational cognitive science have converged to the idea that the developmental process is a process of approximate Bayesian inference about explanations for the world (i.e. Bayesian networks) and how the agent interacts with it, where inferences are gradually refined by actively sampling new, informative data (e.g. through intrinsic motivation) [1,5,7\u20139]. Engineering this simple picture is difficult as the search space of possible explanations for the world explodes combinatorially in the number of variables being modelled [10]. To illustrate the problem further, we summarise the current theory-based reinforcement learning work [2, 3], which simulates developmental agents by combining Bayesian structure learning and intrinsically motivated (model-based) planning.\nIn foundational papers [2, 3], an agent maintains Bayesian beliefs about probabilistic programs (which encode Bayesnets) that might explain the world. The agent gradually refines refines its inferences by actively seeking new data through a mixture of exploration and exploitation (i.e. intrinsic motivation [11,12]). The authors deployed this agent in a suite of simplified Atari games, and found not only that their agent averaged human learning efficiency across the games (after comparing with data from human participants), but the agents' learning trajectories were relatively similar to that of humans. This work serves as a proof of concept that combining inference about the structure of the world with intrinsically motivated model-based planning can achieve relatively human-like learning and behaviour.\nThe fundamental limitation of existing theory-based RL work is that their agents [2, 3] consider a search space of explanatory hypotheses about the world that is the whole set of programs (up to a certain length) that can be generated from the code grammar generating the data generating process. This is an extremely large search space even for the simplified Atari environments their agents face and highlights the current limitations of this work: 1) In complex environments, the space of programs that can be generated from the code grammar generating the data-generating process may be far too large to be searchable, 2) in real environments the modeler does not know the generative process and cannot easily form a space of candidate explanations that contains the data generating process.\nThus a fundamental question is what might be a 'universal' set of primitives and compositional rules to produce a space of candidate explanations for the world (i.e. Bayesnets) satisfying requirements (i)-(iv) in Section 2. Indeed, there is already a tension between requirements (i)-(2) and a significant difficulty lies in striking the right balance. Asking what a universal space of Bayesnets might look like, we first consider the existing literature: Spaces of probabilistic programs are easily made extremely expressive, but it is not clear how to do so while keeping them coarse enough for inference to remain tractable. Probabilistic programs are not always easily interpretable, and, barring specific assumptions, do not support efficient perception and learning, as Bayesian inference over states and parameters may require sampling. One example of probabilistic programs that might satisfy these requirements-to a first approximation\u2014are hierarchical discrete and continuous state partially observed Markov decision processes (POMDPs) [13, 14].\nThis space of Bayesnets is very expressive in being able to reproduce a wide variety of behavioural simulations and empirical data. For example, nearly all modeling work in the active inference literature alone, which spans nearly two decades, employed Bayesnets that are built by hierarchically stacking these two types of layers [15-18]. The resulting Bayesnets support fast action, perception and learning where inference about states and parameters is done through fast variational inference procedures [15, 16, 19\u201321], which have a degree of biological plausibility in being able to reproduce a wide range of features from real neural dynamics, e.g. [22-25]. Barring the use of neural networks for expressing non-linearities in these layers [26], each of them furnishes an interpretable model of dynamics."}, {"title": "B Details on generic, interpretable and agentic class of Bayesnets", "content": "The Bayesnet class we consider here as potential explanations for the world is built from a simple set of primitives and compositional rules. We claim that it satisfies the requirements (i)-(iv) from Section 2. (i) Its expressivity lies in its ability to represent complex stochastic processes, including those with both discrete and continuous states. (ii) Its sparsity stems from significant inductive biases on the structrures of the Bayesnets being considered. (iii) The interpretability of each Bayesnet in this class stems from their sparsity as well as encoding of causal relationships. Finally, and as we will see, these Bayesnets support fast action, perception and decision-making (iv).\nThe proposed Bayesnet class is built by hierarchically assembling a set of basic structural modules satisfying requirements (iii)-(iv), which together, express a wide range of dynamic agent-environment interactions. In what follows, we detail each of these building blocks developed specifically to express a large class of stochastic processes on either discrete or continuous states."}, {"title": "B.1 Discrete dynamics", "content": "Markov processes are a fairly ubiquitous class of stochastic processes [27]. All Markov processes on discrete states have simple transition dynamics that are given by linear algebra. When these transitions also depend on actions, we obtain Markov decision processes [28]. When states are partially observed and observations depend only on the current latent state, we obtain POMDPs. We can add auxiliary latent states to those POMDPs [4] (i.e. the equivalent of momentum, acceleration etc) to account for the effect of memory in the system, producing semi-Markovian POMDPs. Lastly, we can stack these layers hierarchically to express multi-scale semi-Markovian processes in latent space. In summary, hierarchical discrete POMDPs extended in this way provide a very generic class of models for agent-environment interactions on discrete states. See Figure 1 for a graphical representation of discrete extended POMDPs and their various degrees of freedom."}, {"title": "B.2 Continuous dynamics", "content": "For expressing continuous dynamics, the situation is somewhat more difficult. Repeating the construction of the discrete state-space model seems hardly possible because continuous-space Markov processes are given by linear operators in infinite (as opposed to finite) dimensional spaces [30]. A working alternative is to restrict ourselves to a more manageable but still very expressive class of processes. We can consider continuous POMDPs with latent dynamics given by stochastic differential equations (SDEs), which is another very expressive class of stochastic processes. However, for expressing a wide range of agent environment interactions, these SDEs must break detailed balance and possibly be driven by non-Markovian noise (fluctuations in the wind or in the ocean) [31]. There is a remarkably expressive class of SDEs supporting non-linearities, non-Markovian noise and broken detailed balance\u2014that is, many times differentiable stochastic differential equations for which POMDPs with these latent dynamics support fast and biologically plausible update rules for action, perception and learning [16, 20, 22]. These continuous POMDPs form a very expressive space of continuous-state Bayesian networks by varying the temporal, hierarchical, factorial and generalised depth as in Figure 1.\nOne important challenge remains: to parameterise the non-linearities in these POMDPs (e.g. flows of SDEs) without sacrificing interpretability, and learn these parameterisations from data. A promising approach is to express non-linear SDEs with recurrent switching linear dynamical systems (rsLDS; see Figure 2) [32]; that is, switching mixtures of linear SDEs, because one could use a very fine grained piece-wise linear approximation to recover arbitrary non-linearities, as necessary. The advantage of using switching linear SDEs is that they are interpretable and afford relatively scalable exact Bayesian inference [32].\u00b9 However, current rsLDS architectures are restricted to approximating the dynamics of non-linear so-called 'diffusion' SDEs discretised with an Euler scheme [32], which do not feature non-Markovian noise by definition. Looking forward, it seems desirable to extend the rsLDS architecture to express SDEs with more arbitray noise signals, perhaps by adding generalised coordinates (velocity, acceleration, and higher orders of motion etc) [20, 22]. This would entail introducing generalised depth into the current rsLDS layer. Doing this should furnish a very expressive yet sparse class of Bayesnets for continuous-state dynamics that satisfies the basic requirements (i)-(iv)."}, {"title": "B.3 Hierarchical mixed dynamics", "content": "Stacking hierarchies of discrete layers atop hierarchies of continuous layers yields mixed generative models that can express rich non-linearities and dynamics at several levels of abstraction. Despite the absence of traditional neural networks in these hierarchies, one can think of them as a neural network where the layers are discrete and continuous POMDPs and the computations are of efficient approximate Bayesian inference. Hierarchies of these layers may be interpretable as they represent nested processes operating at different timescales. These hierarchical structures are compatible with views of the brain as entertaining discrete-state low-dimensional abstract dynamics that condition the high-dimensional continuous representations closer to sensory input [34, 35]."}]}