{"title": "PACKETLSTM: DYNAMIC LSTM FRAMEWORK FOR STREAMING DATA WITH VARYING FEATURE SPACE", "authors": ["Rohit Agarwal", "Karaka Prasanth Naidu", "Alexander Horsch", "Krishna Agarwal", "Dilip K. Prasad"], "abstract": "We study the online learning problem characterized by the varying input feature space of streaming data. Although LSTMs have been employed to effectively capture the temporal nature of streaming data, they cannot handle the dimension-varying streams in an online learning setting. Therefore, we propose a dynamic LSTM-based novel method, called packetLSTM, to model the dimension-varying streams. The packetLSTM's dynamic framework consists of an evolving packet of LSTMs, each dedicated to processing one input feature. Each LSTM retains the local information of its corresponding feature, while a shared common memory consolidates global information. This configuration facilitates continuous learning and mitigates the issue of forgetting, even when certain features are absent for extended time periods. The idea of utilizing one LSTM per feature coupled with a dimension-invariant operator for information aggregation enhances the dynamic nature of packetLSTM. This dynamic nature is evidenced by the model's ability to activate, deactivate, and add new LSTMs as required, thus seamlessly accommodating varying input dimensions. The packetLSTM achieves state-of-the-art results on five datasets, and its underlying principle is extended to other RNN types, like GRU and vanilla RNN.", "sections": [{"title": "INTRODUCTION", "content": "Online learning, characterized by streaming data, where data instances arrive one by one, has been studied extensively (Gama, 2012; Neu & Olkhovskaya, 2021; Agarwal et al., 2008). Recently, there has been a growing focus on online learning in environments with varying input feature spaces. Examples include movie sentiment classification and crowdedness severity prediction (He et al., 2023; Agarwal et al., 2024). These varying input features, termed haphazard inputs (Agarwal et al., 2023), are denoted as  Xt \u2208 Rd, where dt indicates the dimensionality of input, varying over time t. The field of haphazard inputs is expanding, prompting the introduction of new methods, applications, and appropriate datasets as elaborated in section A of the Appendix.\nThe current landscape is focused on developing new methods. Predominantly, haphazard inputs are modeled using classical approaches like naive Bayes (Katakis et al., 2005), decision stumps (Schreckenberger et al., 2022; 2023), and linear classifiers (Beyazit et al., 2019), favored for their dynamic architectures. However, there is a push towards developing dynamic deep learning solutions (Agarwal et al., 2022; 2023), motivated by the capabilities of neural networks. Nevertheless, current methodologies have not adequately leveraged the streaming nature of data. To bridge this gap, we advocate using Recurrent Neural Networks (RNNs) (Hochreiter & Schmidhuber, 1997; Zhang et al., 2021a), which can effectively exploit the temporal dynamics of data.\nWe introduce a novel architecture, termed packetLSTM, designed to dynamically adapt to vary-ing input feature space. This framework employs a unique ensemble of Long Short-Term Memory (LSTM) units, each dedicated to a specific input feature. The packetLSTM allows for robust inter-action among its LSTMs, fostering the integration of global information while preserving feature-specific knowledge within each unit's short-term memory. The packetLSTM facilitates continuous"}, {"title": "RELATED WORKS", "content": "Haphazard Inputs The initial approach\nto address haphazard inputs utilized naive\nBayes and x2 statistics to dynamically\nincorporate features, update existing fea-\nture statistics, and select a feature subset\n(Katakis et al., 2005). This method was fur-\nther expanded by employing an ensemble of\nnaive Bayes classifiers for predictive anal-\nysis (Wenerstrom & Giraud-Carrier, 2006).\nSubsequent research has focused on in-\nferring unobserved features from observed\nones using various techniques, including\ngraph methods (He et al., 2019; Sajedi &\nRazzazi, 2024), and Gaussian copula (He\net al., 2021; Zhuo et al., 2024), followed\nby the application of classifiers across the\ncomplete feature space. Concurrently, an-\nother line of research projects data into a\nshared subspace to learn a linear classifier\nusing empirical risk minimization (Beyazit\net al., 2019) or online gradient descent (Zhou & Matsushima, 2023). Distinctly, You et al. (2024)\nmaintains an informativeness matrix of each feature to update a linear classifier. Another research di-\nrection explores the use of decision trees to handle haphazard inputs. Specifically, Schreckenberger\net al. (2022) proposed Dynamic Forest, which uses an ensemble of decision stumps, each based on\na feature from a selected subset of all seen features. However, this approach can result in numer-\nous decision stumps, prompting Schreckenberger et al. (2023) to introduce a refined approach that\nconstructs only one decision stump for each feature. Lee et al. (2023a) proposed to utilize adaptive\nrandom forest on a fixed set of features, created through imputation assuming large buffer storage.\nDespite the dynamic nature of the above classical methods in modifying their architectures, the era\nof big data necessitates adopting deep learning approaches to effectively model haphazard inputs. To\ndate, seminal contributions in this domain include works by Agarwal et al. (2022; 2023), which are\nbased on neural networks. The Auxiliary Network (Agarwal et al., 2022) incorporates parallel hid-\nden layers to process each input feature. Conversely, Aux-Drop (Agarwal et al., 2023) implements\nselective and random dropouts within its layers to handle haphazard inputs. Although these mod-\nels operate under specific assumptions, recent advancements by the same authors (Agarwal et al.,\n2024) provide simple solutions to mitigate these assumptions in haphazard inputs. While all the\nmethods discussed above handle haphazard inputs, none effectively exploits the temporal dynamics\nof streaming data, which we achieve using RNNs, specifically LSTMs.\nRNNS RNNs are among the most popular models in sequential data analysis (Salehinejad et al.,\n2017; Allen-Zhu & Li, 2019). Various techniques are developed to capture the temporal dynamics"}, {"title": "PRELIMINARIES", "content": "Notations In this article, we represent time in superscript and feature id in subscript. For example,\nx3 represents the value of feature 2 (F2) at time t3. For ease of readability, we slightly adjust the\nnotation of time here. Instead of denoting by v\u00b9, we use 23. When time is referenced individually,\nit is denoted as t\u2081. Moreover, t indicates a random time, and t - 1 denotes the time preceding t. A\ncomplete list of notations is provided in section C of the Appendix.\nCharacteristics Haphazard inputs exhibit six characteristics which are illustrated in Figure 2(b).\nThese characteristics are: (1) Streaming data, which are received sequentially and processed with-\nout storage. (2) Missing data, which are features present in some instances but may be absent in\nsubsequent ones like F\u2081 at time t2. (3) Missing features, that are not received from the onset; how-\never, their availability is known like F4. (4) Sudden features, that arrives unexpectedly without prior\nindication of their existence like F3 at time t2. (5) Obsolete features, which can cease to exist after\nany instance, such as F2. (6) Unknown number of total features, results from the combined effect of\nmissing data, missing features, sudden features, and obsolete features.\nFeature Space The characteristics of haphazard inputs result in a varying feature space. We define\na universal feature space (Ft) as the set of features encountered till time t. The specific set of features\npresent at time t is represented by Ft and is termed current feature space as shown in Figure 2(b).\nThe universal feature space will grow with time due to the emergence of sudden features and missing\nfeatures. For example, F1 = {F1, F2} at time t\u2081, and F2 = {F1, F2, F3} at time t2 in Figure 2(b).\nIn an ideal condition, Ft can contract with the removal of obsolete features; however, since the\ncessation of obsolete features is unknown, F\u012bt may not decrease in practice.\nMathematical Formulation The haphazard input received at time t can be represented by Xt,\nwhere Xt\u2208 R|F4|. Here, represents the cardinality of a set. The corresponding ground truth is\ndenoted by yt, where yt \u2208 [0,1]C and C is the number of classes. This paper deals with the binary\nclassification problem but can be easily extended to multi-class scenarios. The model, denoted by\nf, operates in an online learning setting with ft\u22121 : Xt \u2192 yt, where f\u00ba represents the initialized\nstate of the model. After processing {Xt, yt}, the model's state is represented by ft. At each time\nt, the model receives Xt, and ft-1 processes Xt to yield a prediction \u0177t. Upon the revelation of yt,\nthe loss lt = H (yt, \u0177t) is computed, where H is a loss function. The model then updates from ft-1\nto ft for the subsequent instances based on lt. This iterative process continues for each instance."}, {"title": "METHOD", "content": "The packetLSTM consists of a pack of LSTMs, each dedicated to a distinct feature, as illustrated\nin the gray box in Figure 2(a). We utilize LSTMs due to their proven effectiveness in capturing\ntemporal dynamics of data (Kazemi et al., 2019; Wang et al., 2017; Zhang et al., 2021a).\nDue to the varying dimensions of input feature space, a single LSTM cannot process all features,\nnecessitating one LSTM per feature. Each LSTM (Lj) receives inputs comprising the feature value\n(x), time delay (\u25b3), its previous short-term memory (h\u00af), and common long-term memory\n(ct-1), as labeled Input in Figure 2(a). The \u2206 measures the time difference between the current"}, {"title": "EXPERIMENTS", "content": "Datasets We consider 5 datasets - magic04 (Bock et al., 2004), imdb (Maas et al., 2011), a8a\n(Kohavi et al., 1996), SUSY (Baldi et al., 2014), and HIGGS (Baldi et al., 2014) \u2013 with details\nprovided in section E of the Appendix. The motivation to choose these datasets is three-fold. First,\nthey include both real (imdb) and synthetic datasets. Second, the number of instances varies from\n19020 in magic04 to 1M in HIGGS. Third, the number of features ranges from 8 in SUSY to 7500\nin imdb. Therefore, the diversity in the number of features and instances allows us to determine the\nefficacy of packetLSTM effectively. The imdb dataset is haphazard in nature. However, the synthetic\ndataset needs to be transformed into haphazard inputs. Following the baseline papers (Beyazit et al.,\n2019; Agarwal et al., 2023), we transform synthetic datasets based on the probability values p, where\np = 0.25 means only 25% of features are available at each time instance. We consider p = 0.25,\n0.5, and 0.75. The synthetic dataset preparation is further discussed in section E of the Appendix.\nMetrics We compare all models using five metrics: number of errors, accuracy, Area Under\nthe Receiver Operating Characteristic curve (AUROC), Area Under the Precision-Recall Curve\n(AUPRC), and balanced accuracy. Each metric is discussed in section F of the Appendix. The\nbalanced accuracy is the primary comparison metric in the main manuscript, while detailed compar-\nisons using the other metrics are presented in section K of the Appendix. We adhere to the standard\nevaluation protocol for haphazard inputs, which is detailed in section G of the Appendix.\nBaseline We consider 10 baseline models, including NB3(Katakis et al., 2005), FAE (Wenerstrom\n& Giraud-Carrier, 2006), DynFo (Schreckenberger et al., 2022), ORF3V (Schreckenberger et al.,\n2023), OLVF (Beyazit et al., 2019), OCDS (He et al., 2019), OVFM (He et al., 2021), Aux-Net\n(Agarwal et al., 2022), Aux-Drop (Agarwal et al., 2023), and OLIFL(You et al., 2024). Additionally,\nthere are a few other models see section H of the Appendix \u2013 applicable to the field of haphazard\ninputs. However, we could not include these models because of the lack of open-source code and\nthe challenges associated with implementing them.\nImplementation Details We ran all the models five times, except for the deterministic models\nNB3, FAE, OLVF, and OLIFL \u2013 which consistently produce identical outcomes across runs. The re-"}, {"title": "ABLATION STUDIES", "content": "We conduct ablation studies within packetLSTM to assess the impact of each component and iden-\ntify optimal variants, adhering to the hyperparameters described in Implementation Details of section\n5, unless specified otherwise. We report the mean of the balanced accuracy in the main manuscript\n(Table 2), with std detailed in section L of the Appendix. We calculate the number of wins for each\ncomponent across all dataset combinations, defining a win as achieving the highest balanced accu-\nracy or being within a 0.05 margin of the top value, accounting for variability indicated by the std.\nFor example, in the magic04 dataset at p = 0.25, the packetLSTM with the Min aggregator reports\na slightly higher balanced accuracy (61.36) than the Max (61.33) but exhibits greater std (0.14 vs.\n0.07). This variability suggests that some runs using the Min aggregator achieved lower balanced\naccuracy than the Max. Therefore, a 0.05 margin mitigates such discrepancies.\nWhich aggregation operator performs the best? The Max operator, in general, performed best\nwith 7 wins, as shown in the AGG component of Table 2. However, to determine the best operators,\nwe compare them one by one (see \u2192 in Table 2). (1) The performance difference between Min\nand Max is negligible, indicating both are suitable choices. (2) Between Sum and Mean, there is\nno evident superiority. However, the Sum is sensitive to the number of features, while the Mean\nremains relatively stable unless the feature distribution changes substantially. We tested this by al-\nternating p (from 0.25 to 0.75) every 100 instances in the a8a dataset, which has the most features\namong the synthetic datasets. The Mean operator (balanced accuracy = 62.77) substantially out-\nperformed the Sum operator (54.16), establishing the Mean as the superior operator. (3) Table 2\nindicates that the efficacy of the Mean increases as data availability decreases from p = 0.75 to 0.25."}, {"title": "CHALLENGING SCENARIOS", "content": "We design three challenging experiments to explicitly elucidate the efficacy of packetLSTM to (1)\nhandle sudden features, (2) handle obsolete features, and (3) demonstrate the learning without for-\ngetting capability. We considered the HIGGS and SUSY datasets for these experiments due to their\nsubstantial number of instances. In the main manuscript, we discuss the results corresponding to\nHIGGS, while similar conclusions for SUSY are discussed in section S.2 of the Appendix. For\ncomparative analysis, we opted for OLVF, OLIFL, OVFM, and Aux-Drop baselines based on their\nsuperior performance in the HIGGS dataset, as indicated in Table 1. We divided the dataset into 5\nintervals, each consisting of 20% of the total instances, with successive intervals containing the next\n20% of instances. For the exact values of Figure 3 and 4, refer to section S.1 of the Appendix.\nSudden Features Here, each subsequent interval includes an additional 20% of features, starting\nwith 20% in the first interval and increasing to 100% by the fifth interval. This progression, termed\nthe trapezoidal data stream (Zhang et al., 2016; Liu et al., 2022), is illustrated in Figure 3 by a pink-\nshaded region. Model performance is expected to enhance with increased data volume. All models,\nexcept OLVF, exhibit this increasing performance trend. Notably, packetLSTM outperforms other\nmodels in each interval, demonstrating its superior capability in handling sudden features."}, {"title": "TRANSFORMER ON HAPHAZARD INPUTS", "content": "Despite the lack of application of the Transformer (Vaswani et al., 2017) in the field of haphazard\ninputs, its inherent ability to manage variable-size inputs makes it a natural choice. Therefore, we\nalso investigate Transformer-based methodologies for modeling haphazard inputs.\nPadding We consider padding inputs with zeros or truncating them, which necessitates specifying\na fixed input length (fi). If the number of features in an instance (|Ft|) is less than fi, the Trans-\nformer pads the input with zeros, and if |Ft | exceeds fi, it truncates the excess features, potentially\nleading to information loss. A potential, albeit inefficient, solution is to set an excessively high fi.\nNonetheless, to assess how packetLSTM compares to Transformer, we conducted two experiments:\nOnly Values, padding available feature values, and Pairs, pairing each feature value with its feature\nID and padding the sequence. The packetLSTM outperforms Transformer with padding across all\ndataset scenarios (see Table 3). Further details can be found in section U.1 of the Appendix.\nNatural Language Given the variable size of inputs, natural language can be seen as an applica-\ntion where features arrive one by one, and most are missing. We compared packetLSTM with Dis-\nDistilBERT (Sanh et al., 2019) and BERT (Devlin et al., 2019), detailed in section U.2 of the Appendix.\nWe observe that both DistilBERT and BERT are unable to perform classification on haphazard inputs\nwith a balanced accuracy of around 50 in all cases.\nSet Transformer The Set Transformer (Lee et al., 2019), designed for variable-length inputs, has\nbeen previously used in offline learning. We employ it for online learning to handle haphazard\ninputs. Results in Table 3 indicate that packetLSTM significantly outperforms Set Transformer\nacross all datasets, possibly due to Set Transformer's assumption of permutation invariance, which\ndoes not hold for haphazard inputs. More details are provided in section U.3 of the Appendix.\nHapTransformer To tackle permutation invariance, we introduce HapTransformer, which trans-\nforms each feature into a distinct learnable embedding, a technique similarly utilized in prior re-\nsearch (Huang et al., 2020; Gorishniy et al., 2021; Somepalli et al., 2021). However, these models\ndo not accommodate variable-sized inputs. The learnable embeddings are subsequently processed\nby the Set Transformer's decoder, allowing implicit communication of feature identities. Details\non the architecture and hyperparameters are provided in section U.4 of the Appendix. Despite its\nstrengths, packetLSTM surpasses HapTransformer in all dataset scenarios (see Table 3). HapTrans-\nformer struggles particularly with datasets having high feature counts like imdb and a8a, and requires\nsignificant computational time. However, it remains a strong baseline, outperforming other baseline\nmodels in the SUSY and HIGGS datasets, as detailed in Tables 1 and 3."}, {"title": "CONCLUSION", "content": "In conclusion, our work introduces packetLSTM, a dynamic framework for handling streaming data\nwith varying feature dimensions in real-time learning environments. Significantly, the underlying"}, {"title": "ETHICS STATEMENT", "content": "The packetLSTM framework introduces a novel method for handling streaming data with variable\nfeatures, potentially impacting numerous fields such as healthcare, finance, autonomous systems, en-\nvironmental monitoring, and personalized recommendation systems. In healthcare, it can improve\nreal-time patient monitoring by adapting to new or absent data types, enhancing patient care. Finan-\ncial sectors can benefit from more stable predictive models for trading and risk management due to\ntheir ability to process erratic market data. Autonomous systems can gain reliability by effectively\nmanaging inconsistencies in sensory data, while environmental monitoring may achieve greater ac-\ncuracy in tracking ecological changes, aiding policy decisions. In digital platforms, it may refine\npersonalized recommendations by adjusting to user behavior changes, and in educational technol-\nogy, it may personalize content to student needs, improving engagement and learning outcomes.\nDue to the potential application of packetLSTM in the above-discussed crucial and sensitive fields,\nit is necessary to consider ethical concerns. The packetLSTM framework must navigate several\nethical issues to align with the European Union's Artificial Intelligence Act (EU AI Act). Deploy-\ning packetLSTM technology requires careful consideration, including data privacy, transparency in\ndecision-making, and addressing data biases to prevent discrimination and ensure fairness across\ndiverse user groups. By addressing these challenges, packetLSTM can significantly enhance effi-\nciency and personalization across multiple domains while upholding high ethical standards."}, {"title": "REPRODUCIBILITY", "content": "All the information to reproduce the result is available in sections 4, 5, 6, 7, and 8 of the\nmain manuscript. Additional information is also provided in sections I, J, R, and U of the Ap-\npendix. The code can be found at https://github.com/Rohit102497/packetLSTM_\nHaphazardInputs with sufficient instructions to faithfully reproduce all the experimental re-\nsults. The link to the datasets is provided in section E of the Appendix. We report the resources used\nto conduct the experiments in section I of the Appendix. Moreover, for the benchmark results, we\nalso report the time taken by each individual model in section K of the Appendix."}, {"title": "COMPLEXITY ANALYSIS", "content": "The time complexity of packetLSTM is $\\sum_{f=1}^{T}O(g(|F_t|) * s^2 + P)$, where T is\nthe number of instances, $O(g(|F_t|) * s^2)$ is the time complexity to process all the activated LSTMs\nat time t corresponding to $F_t$ features, and O(P) broadly denotes the constant time required to\nperform other fixed operations like aggregations and final prediction. We utilize the torch.matmul()\nfunction for matrix multiplication of LSTMs. The time required for each LSTM is dependent on\nits hidden size (s) and performs 3 matrix multiplication of size (3s, s + 1) and (s + 1, 1) requiring\na time complexity of $O(s^2)$. However, we vectorize the operation of $|F_t|$ LSTMs by single matrix\nmultiplication of ($|F_t|$, 3s, s + 1) and ($|F_t|$, s + 1, 1). Note that the time required by torch.matmul()\ndoes not scale linearly with $|F_t|$; rather, it just takes a small overhead depending on the type of\nhardware and other dependencies. Here, we denote this overhead as a function of $|F_t|$ as $g(|F_t|)$\nwhere $1 \\leq g(|F_t|) \\leq |F_t|$. This is further corroborated by the time required by packetLSTMs\non each synthetic dataset with different p values (see Table 8). For example, The time required\nto process the whole HIGGS dataset takes 4396.83 and 4500.17 seconds for p = 0.25 and 0.5,\nrespectively. Even though the value of $|F_t |$ doubles from p = 0.25 to 0.5, the time required doesn't\nincrease by the same factor. Therefore, the time required by the $|F_t|$ LSTMs at time t would be\n$O(g(|F_t|)*s^2)$. Finally, since it is an online learning task and each instance is processed sequentially,\nthe total time complexity of T instances would be $\\sum_{1}^{T}O(g(|F_t|) * s^2 + P)$. It is difficult to\nfind the exact form of the function g. However, based on the time required by packetLSTM on\neach dataset with increasing p, it can be safely assumed that the value of $g(|F_t|)$ is closer to 1\nthan $|F_t|$. Therefore, the packetLSTM model demonstrates scalability in terms of time complexity\ncorresponding to the number of features.\nSpace Complexity The space complexity of packetLSTM is directly dependent on the space com-\nplexity of an LSTM. Let us denote the space complexity of an LSTM by O(L). At each time t, we\nhave a universal feature space of $|F_t|$ cardinality, therefore, corresponding $|F_t|$ LSTMs are present\nin the packetLSTMs. The space complexity of the final prediction network and aggregation function\nare fixed and denoted by O(K). Therefore, the total space complexity of packetLSTM at time t\ncan be given by $O(|F_t| * L + K)$. Note that the space complexity of packetLSTM increases with\nthe arrival of sudden and missing features. Therefore, the limitation of packetLSTM is that it is not\nscalable in terms of space complexity corresponding to the feature size. However, it is noteworthy\nthat packetLSTM effectively manages up to 7500 features, as demonstrated with the imdb dataset.\nAdditionally, we present a strategy to further mitigate space complexity limitation in section Q of\nthe Appendix."}, {"title": "PACKETRNN AND PACKETGRU", "content": "packetRNN The packetRNN framework is illustrated in Figure 5(a). Unlike the LSTM, which\nincorporates both short-term and long-term memory, the RNN possesses only a single memory ele-\nment, known as the hidden state (h). Consequently, the packetRNN lacks a mechanism for integrat-\ning global information and instead maintains local information within its hidden state. These hidden\nstates are combined using a dimension-invariant aggregation operator to generate a common hidden\nstate for final predictions. The mathematical working of a Vanilla RNN unit within the packetRNN"}, {"title": "COMPLEXITY ANALYSIS", "content": "The space complexity increases with the number of features, as discussed above. However, pack-\netLSTM easily handles even the 7500 features in the imdb dataset. The total number of learnable\nparameters of packetLSTM for the imdb dataset is ~131M. Therefore, packetLSTM with 1B param-\neters and a hidden size of 64 can handle around ~57K features. Hence, we argue that packetLSTM\ncan deal with high-dimensional data. However, we also propose a solution to curb the space com-\nplexity by defining a maximum limit (say lf) on the number of LSTMs. When the number of\nfeatures in the universal feature space $|F_t| > l_f$, we drop $|F_t| - l_f$ features. Here, dropping the"}]}