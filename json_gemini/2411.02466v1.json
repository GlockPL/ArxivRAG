{"title": "Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains", "authors": ["Robin Trombetta", "Olivier Rouvi\u00e8re", "Carole Lartizien"], "abstract": "Fully supervised deep models have shown promising performance for many medical segmentation tasks. Still, the deployment of these tools in clinics is limited by the very time-consuming collection of manually expert-annotated data. Moreover, most of the state-of-the-art models have been trained and validated on moderately homogeneous datasets. It is known that deep learning methods are often greatly degraded by domain or label shifts and are yet to be built in such a way as to be robust to unseen data or label distributions. In the clinical setting, this problematic is particularly relevant as the deployment institutions may have different scanners or acquisition protocols than those from which the data has been collected to train the model. In this work, we propose to address these two challenges on the detection of clinically significant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the method proposed by (Kervadec et al., 2018), which introduces a size constaint loss to produce fine semantic cancer lesions segmentations from weak circle scribbles annotations. Performance of the model is based on two public (PI-CAI and Prostate158) and one private databases. First, we show that the model achieves on-par performance with strong fully supervised baseline models, both on in-distribution validation data and unseen test images. Second, we observe a performance decrease for both fully supervised and weakly supervised models when tested on unseen data domains. This confirms the crucial need for efficient domain adaptation methods if deep learning models are aimed to be deployed in a clinical environment. Finally, we show that ensemble predictions from multiple trainings increase generalization performance.", "sections": [{"title": "1. Introduction", "content": "Over the last years, deep learning models have become state-of-the-art methods in almost all medical imaging applications, including segmentation and detection. Among data-oriented methods, fully supervised models remain the most common and best performing ones. However, gathering numerous expert-annotated data to train such models is a very time and ressources consuming process, restraining the current use of such models in the medical field. For this reason, other promising paradigms have also been explored such as semi-, weakly- or unsupervised learning (Bosma et al., 2023; Baur et al., 2021). They aimed to mitigate the need of annotated data to train deep learning models.\nAnother known drawback of deep learning methods is the limited generalization capacity to unknown data distribution. It has been shown that when tested on out-of-distribution"}, {"title": "2. Material and Method", "content": "2.1. A weak segmentation model based on object size constraint loss function\nIn (Kervadec et al., 2018), the authors proposed a loss function for partially annotated data that aims to impose a size constraint on the predicted segmentations of a model. The partial cross-entropy H, computed only on the annotated pixels \u03a9a, is combined with a constraint loss C that adds a quadratic penalty to the model on the total sum of its predictions for class c if it is outside a defined range [a, b]. More specifically, let Vc = \u03a3\u03a1\u0395\u03a9 Sp,c be the sum of the probabilities Sp,c for class c of every pixel p in the image domain \u03a9. The constraint loss is given by :\n$C(V_c) = \\begin{cases} (V_c - a)^2 & \\text{if } V_s < a \\\\ (V_c - b)^2 & \\text{if } V_s > b \\\\ 0 & \\text{otherwise} \\end{cases}$  (1)"}, {"title": "2.2. Data description", "content": "The experiments are conducted on three datasets, described hereunder :\n\u2022 PI-CAI challenge public training dataset. It contains 1500 multi-parametric MRI (T2w, DWI and ADC) exams from 3 Dutch centers acquired on 7 different scanners, 5 from Siemens Healthineers and 2 from Philips Medical Systems. It includes 328 cases from the Prostate-X challenge (Armato III et al., 2018). Of all the exams available, we only use the 1295 that are manually annotated by expert clinicians, and do not leverage the 205 exams with AI-derived lesion segmentations.\n\u2022 The Prostate158 (Adams et al., 2022) train and validation datasets. It consists of 139 annotated biparametric MRI (T2w, DWI) acquired at a German university hospital on 3T Magneton Vida and Skyra scanners from Siemens Healthineers.\n\u2022 A private dataset, containing 219 multi-parametric MRI (T2w, DWI and ADC) exams acquired in clinical practice in two French hospitals on three different scanners : 26 exams were carried out on a 3T Ingenia scanner (Philips Medical Systems), 67 on a 1.5T Symphony scanner (Siemens Healthineers) and 126 on a 3T Discovery scanner (GE Heathcare). It was declared to the appropriate national administrative authorities (CPP L 09-04 and CNIL 08-06) and patients gave written informed consent for researchers to use their MR imaging data. All patients underwent a radical prostatectomy and prostate focal lesions manually outlined by expert radiologists on the different imaging sequences were validated against the prostatectomy gold standard ground truth.\nBoth T2-weigthed (T2w) and apparent diffusion coefficient (ADC) MR maps were used as input channels. The latter modality was registered to the former, all images were resampled to a 1 \u00d7 1 \u00d7 3 mm\u00b3 pixel size and cropped to 96 \u00d7 96 \u00d7 20 volumes. Images intensities"}, {"title": "2.3. Weak annotations", "content": "The aim of weak annotations is to mimic what could be an easier and faster way for clinicians to provide annotations on real images. For this purpose, we replace full segmentations by circles of maximum radius of 3 mm inside each individual lesion. The centers of the circles are drawn randomly and independently on each axial slices. If the lesion is too small to fit a circle of this size, the radius is reduced until a circle can fit inside the lesion. The prostate gland is also annotated is such way, with only one circle per slice. In total, weak annotations only represent 14% of the full masks of CS lesions, considerably reducing the amount and complexity of annotations and thus the time needed for experts to make these annotations.\n2.4. Experiments\nWe compare several weakly supervised methods and fully supervised baseline models. For our proposed scribble based weak model, we consider two main configurations : one with partial cross-entropy (CE) and the image tag (IT) and one with partial CE, image tag and common bounds (CB) constraint loss terms. We compare them to a simpler weak model with partial cross-entropy and negative cross-entropy (denoted Partial CE) as well as to fully supervised baselines trained with cross-entropy and generalized DICE loss on the full available annotations. We use 2D and 3D MONAI's DynUNet (Cardoso et al., 2022) as backbone architectures for the proposed weak and fully supervised models. As for comparison to other weakly supervised models, we train nnDetection (Baumgartner et al., 2021) with ground truth segmentations masks being 3D rectangular cuboids framing full lesion annotations (nnDetection full) or weak scribble annotations (nnDetection weak). Note the comparison between these models and the ones with size constraints is not straightforward as they do not use the same kind of weak annotations.\nAll models are trained in 5-fold cross-validation on the PI-CAI dataset. They are first evaluated in the in-distribution setup, meaning we report the mean performance on the 5 validation folds of the PI-CAI dataset. Then, we appraise the models in the generalization setup by testing them on data from two unseen domains, namely Prostate158 and our private database. Moreover, for each method, we combine the best models of each training fold into a single ensemble model, for which the lesion probability maps are computed as the average of the probability maps of the 5 aggregated models. These ensemble models are only tested on the two unseen data domains."}, {"title": "2.5. Evaluation metrics", "content": "The models are evaluated both at lesion and patient levels. Following PI-CAI guidelines, a detection map is made of non-overlapping and non connected clusters, representing predicted csPCa lesions. Each lesion is assigned a unique probability score, chosen as the average of the probabilities of the cluster's voxels. At a lesion level, we report metrics derived from the free-response receiver operating characteristics (FROC) curve which shows sensitivity as a function of the number of false positive detections per patient. In continuity of previous works done on this csPCa detection task (Bosma et al., 2023; Saha et al., 2021),"}, {"title": "2.6. Implementation details and hyperparameters", "content": "All hyperparameters were determined with grid search on the first fold of the PI-CAI training/validation splits (see Appendix C for more details). The lower and upper bounds associated with the CS lesions class were set to 5 and 500 voxels for 2D models and to 30 and 4 000 voxels for 3D models. For the prostate class, they were set to 100 and 2 500 in the 2D case and 10 000 and 40 000 in the 3D case. The fully supervised baselines were trained with cross-entropy and generalized DICE loss. All models were trained during 200 epochs with Adam optimizer, a learning rate of 10-3 and a weight decay of 10-4. To compensate for the low amount of lesions in the PI-CAI dataset (see Table 1), sampling was weighted such that 2D transverse slices for 2D models or 3D volumes for 3D models with and without lesions have the same probability of being drawn in a batch. For post-processing, predicted lesions of size inferior than 15 voxels are discarded."}, {"title": "3. Results", "content": "3.1. Classification and detection performances\nFigure 1 shows performance of all considered models for the three metrics of interest, namely sensitivity at 1 FP, AP and AUROC. Figure 3 provides examples of visual results of lesion detection maps for some 3D models. Extended visual results, including of 2D and ensemble models, are showcased in Appendix E.\nFirst of all, it is important to note that the best performing model, namely the 3D supervised DynUNet, achieves a mean AUROC of 0.82 and mean AP of 0.42 thus producing a mean aggregated score of 0.62. This performance compares well against the best achievable reported metric on the PI-CAI challenge leader-board. We thus consider it a reliable baseline for our comparison. Surprisingly, the 3D supervised DynUNet is still outperformed by 2D models in term of sensitivity at 1 FP, including by models trained with weak labels.\nThe two weak models with size constraints (CE+IT, CE+IT+CB) clearly outperform the model trained only with partial and negative cross entropies (Partial CE), showing the interest of the additional size constraint cost functions. Between the image tag (IT) and the common bounds (CB) losses, the latter achieves a higher score in 22 of the 30 configurations. We use the term configuration to refer to a pairwise comparison between models with the same spatial dimension input (2D or 3D) and type of model (ensemble or not) evaluated on a given dataset and for a given metric. For instance, AUROC comparison between 2D CE+IT and 2D CE+IT+CB on Prostate158 accounts for one configuration. Remarkably, the models that have been trained with weak labels can outperform fully supervised models. They also perform favorably compared to nnDetection in most cases. The weakly supervised CE+IT+CB model achieves better scores than its supervised counterpart in almost all 2D configurations, but only 2 times out of 30 in 3D."}, {"title": "3.2. Generalization to unseen data domains", "content": "Figure 2 shows the relative performance of the models on the two test datasets, that is Prostate158 and our private database, compared to the performance of the same model evaluated on the in-distribution validation dataset. We did not report the results for the cases where the models are trained with partial and negative cross entropies as their absolute performances are much lower than the others.\nFor most of the metrics and models, there is, as one could have expected, a notable drop in performances when the models are evaluated on a test set that has been acquired in a different setup from that of the training dataset. The average performance decrease ratio is of 28% among all models and metrics. This can reach values as low as -61% for supervised"}, {"title": "4. Discussion and conclusion", "content": "Our proposed weakly supervised method achieves competitive results compared to fully supervised baselines, while requiring only 14% of annotation voxels of clinically significant lesions. It consistently outperforms 2D supervised DynUNet trained with cross-entropy and generalized DICE loss. In the 3D configuration, the model trained with full segmentation annotations remains better overall. The addition of the more precise common bounds (CB) size constraint gives better results compared to the image tag (IT) model that was proposed in (Duran et al., 2022).\nAmong all compared methods, the weak model with CB loss is the most robust to unseen data domains. As seen on Figure 2, it indeed suffers the least from a performance drop when tested on data that do not belong to the training distribution.\nOur study confirms, for the task of csPCa lesion detection and segmentation, that heterogeneity between training and test databases noticeably impacts performance of deep"}, {"title": "Appendix A. Lesion characteristics for each database", "content": null}, {"title": "Appendix B. Characteristics of MRI modalities for each database", "content": null}, {"title": "Appendix C. More details about the models", "content": "Architecture details\nThe 2D DynUNet is composed of four stages with a respective number of filters of 32, 64, 128 and 256. The kernel sizes are set to 3 and the stride is of 1 for the shallowest and 2 for the others. With an input size of [2,96,96], the deepest layers have a shape of [256, 12, 12]. We use Instance Normalisation layers and a dropout of ratio 0.1. The network has a total of 3.8 million (learnable) parameters.\nThe 3D DynUNet is composed of four stages with a respective number of filters of 32, 64, 128 and 256. The kernel sizes are set to 3 and the stride is of 1 for the shallowest and the deepest blocks and 2 for the others. With an input size of [2,20,96,96], the deepest"}, {"title": "Details about the grid search", "content": "We did the grid search on the hyperparameters of the models as follows: for the weak constrained models, we first found the best combination of parameters for the learning rate, the weight decay and constraint weight \u03bb on the model with image tag. Once these parameters were set for the IT model, we reused them for the model with common bounds constraint and did the grid search for the parameters a and b. For the supervised model and Partial CE model, we only did the grid search on the learning rate and the weight decay. The values that we tried for the hyperparameters are detailed hereunder :\n\u2022 Learning rate: between 1e-4 and le-2 with a linear step of 0.5 in the logarithmic scale.\n\u2022 Weight decay: between le-5 and le-2 with a linear step of 1 in the logarithmic scale.\n\u2022 \u03bb: between 1e-5 and le-2 with a linear step of 1 in the logarithmic scale in the two-dimensional case and between 1e-5 and 1e-9 with a linear step of 1 in the logarithmic scale in the three-dimensional case. It optimal value was found to be 10-5 and 10-8 for 2D and 3D models respectively.\n\u2022 a and b : {5,10} for a and {100, 200, 300, 400, 500, 600} for b in the two-dimensional case and {10, 30, 50, 70, 100} for a and {1500, 2000, 2500, 3000, 3500, 4000, 5000, 6000} for b in the three-dimensional case. The grid search in the 2D case is smaller because an optimization of the hyperparameters had already been done in (Duran et al., 2022).\n\u2022 the class weights we were set to 0.14 for the prostate and 0.22 for the lesion based on empirical values reported in (Duran et al., 2022)."}, {"title": "Appendix F. Study on the method for generating weak annotations", "content": "Modeling the process of obtaining the weak scribble annotations is crucial for correctly evaluating the relevance of the weakly supervised models, as an unrealistic modeling could lead to under- or over-estimated performance of the model. In order to assess the robustness of the weakly supervised models to such annotation process, we provide here comparison between several annotation methods that we consider to be realistic ways of producing scribble annotations. Note that these annotation methods apply only to the lesion class; for the prostate class the method described described in section 2.3 and referred to as random valid is systematically used. The methods are as follows :\n\u2022 Random valid denotes the method described in section 2.3.\n\u2022 Center distance map. For each lesion, we compute the Euclidean distance between each non-zero pixel in lesion mask and the nearest zero pixel. The center of the lesion is the maximum value of this map and the circular annotation that is drawn is the largest circle of radius inferior or equal to 3mm that can fit in the CS lesion mask. The amount of annotated pixels with this method is similar to the one of random valid (14%).\n\u2022 Random distance map. From the distance map obtained as described above, we randomly draw a center for the circular scribble with a probability density proportional to the distance map, to biase selection of the scribble center towards the center of the lesion. In this case we draw a circle which always has a radius of 3mm, resulting in an amount of annotated pixels superior to the two previous methods (16.5 %).\n\u2022 Erosions. For each lesion, we iteratively trim its mask to reduce its size until we obtain a surface below a certain value. We then threshold surface is equivalent to a circle of radius 3mm, it results in an amount of annotated pixels below the other methods (10%). Hence, for better comparison, we also perform such procedure with a higher threshold to get the same amount of CS lesion annotations (14%)."}, {"title": "Appendix G. Model results for prostate segmentation", "content": "Although not within the scope of this work, we provide hereunder a similar visualization than Figure 1 for the prostate segmentation Dice score. As suggested by the examples predictions maps shown in Figures 3 and 8, the regularization induced by the common bounds loss greatly improves the segmentation in the weakly supervised setup compared to models with partial cross-entropy (CE) or cross-entropy and image tag (CE+IT). Note that the bounds a and b were set roughly for the prostate class : in the 2D case, we used the ones that were set in (Duran et al., 2022), and in 3D case, we empirically set them based on the analysis of the distribution of prostate sizes."}]}