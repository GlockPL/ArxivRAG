{"title": "DEEP GUESS ACCELERATION FOR EXPLAINABLE IMAGE RECONSTRUCTION IN SPARSE-VIEW CT", "authors": ["Elena Loli Piccolomin", "Davide Evangelista", "Elena Morotti"], "abstract": "Sparse-view Computed Tomography (CT) is an emerging protocol designed to reduce X-ray dose radiation in medical imaging. Traditional Filtered Back Projection algorithm reconstructions suffer from severe artifacts due to sparse data. In contrast, Model-Based Iterative Reconstruction (MBIR) algorithms, though better at mitigating noise through regularization, are too computationally costly for clinical use. This paper introduces a novel technique, denoted as the Deep Guess acceleration scheme, using a trained neural network both to quicken the regularized MBIR and to enhance the reconstruction accuracy. We integrate state-of-the-art deep learning tools to initialize a clever starting guess for a proximal algorithm solving a non-convex model and thus computing an interpretable solution image in a few iterations. Experimental results on real CT images demonstrate the Deep Guess effectiveness in (very) sparse tomographic protocols, where it overcomes its mere variational counterpart and many data-driven approaches at the state of the art. We also consider a ground truth-free implementation and test the robustness of the proposed framework to noise.", "sections": [{"title": "1 Introduction", "content": "Computed tomography (CT) reconstruction from sparse views has emerged as a focal point in medical imaging re- search. A key challenge lies in diminishing the total X-ray radiation dose administered to patients while maintaining reconstruction precision. The sparse-view methodology revolves around decreasing the number of rotation angles utilized, thereby eliciting compromised image quality due to the introduction of numerous streaking artifacts by the classical analytical algorithms, such as the Filtered Back Projection (FBP).\nModel-Based Iterative Reconstruction (MBIR) methods are extensively employed, relying on mathematical models to represent the problem being solved. In this case, sparse CT image reconstruction can be addressed as a linear problem of the form:\n$y = Ax + e^v,$\nwhere $y \\in \\mathbb{R}^m$ is the sinogram, affected by noise $e \\in \\mathbb{R}^m$ with intensity depending on the positive parameter v, and $A \\in \\mathbb{R}^{m\\times n}$ is the forward operator describing the CT system geometry. Due to the sparse sampling, $m < n$ and the system (1) has infinitely many solutions. Additionally, owing to the typical ill-posedness of inverse problems, it is"}, {"title": "2 State of the art", "content": "CT image reconstruction techniques have been evolving through two significant classes of tools: model-based iter- ative algorithms and deep learning-based tools. Both methodologies seek to enhance the precision and efficiency of computed tomography (CT) image reconstruction. MBIR emphasizes iterative optimization processes and the incor- poration of prior knowledge. In contrast, Deep Learning-based techniques leverage data-driven learning to achieve superior performance. Here, we review some of the most impactful techniques proposed over the years.\nThe first class we focus on is the one composed of MBIR approaches, which iteratively solve optimization problems as the one considered in (2). To produce high-quality images from sparse and noisy data, effectively reduce artifacts, and enhance edge preservation, MBIR schemes incorporate specific regularizers R(x) so that the image processing is performed according to precise mathematically-grounded image priors. In medical imaging, priors that enforce gradient sparsity have become a well-established de facto choice because most parts of medical images are relatively smooth and uniform, with significant information concentrated at the edges.\nTotal Variation (TV) prior, minimizing the l\u2081 norm of the gradient-magnitude image, has been extensively utilized since its seminal paper by Rudin, Osher and Fatemi [38], to effectively denoise images (see [13, 36, 50], to cite a few). These formulations effectively differentiate between artifacts (small, random intensity changes) and significant image features (sharp edges), making TV prior interesting for tomographic applications and particularly beneficial in sparse data scenarios, where sparse-view artifacts and noise must be removed. This success has been proved in many works, such as [11,24\u201326, 32, 35, 44, 45].\nThe TV models mentioned above are all convex and relatively easy to handle numerically. At the same time, it is well known that non-convex norms are more suitable for measuring sparsity than the corresponding convex norms, since they are much closer to the lo norm (that is exactly the measure of sparsity) than their convex counterparts [14, 52]. For instance, the TpV prior, which uses the p-norm of the gradient magnitude with 0 < p < 1, allows for more flexible and adaptive regularization than TV. It, in fact, tends to avoid staircasing effects better than TV, because it adjusts the degree of sparsity imposed on the gradients. For this reason it has been successfully used in medical imaging [46,51] and for CT reconstruction [17,41]. The wavelets have been used in tomographic imaging in [4, 7, 28, 34, 55].\nThe second class of algorithms to tackle CT reconstruction is the one involving Deep Learning. DL-based methods offer faster reconstructions and can capture intricate patterns and structures, often outperforming traditional techniques in terms of speed and image quality. Nowadays, there is a plethora of papers exploring different ways in which neural"}, {"title": "3 The proposed Deep Guess acceleration framework", "content": "As anticipated in the Introduction, we consider in this paper a model-based approach formulated as the minimization problem (2) with regularization prior given by (3). We focus on the non-convex case with 0 < p < 1, leading to the so called TpV regularization.\nThe proposed framework comprises two primary steps, as illustrated in Figure 1.\n1.  The initial step involves calculating a robust approximation of the reconstructed image using a neural network within a supervised post processing scheme. The output of this step, referred to as Deep Guess (DG), is a reconstruction scheme already proposed in the literature (see Section 2). Here, however, it serves as the input for the subsequent phase.\n2.  The second step refines the quality of image computed by the DG by applying to it some iterations of an iterative optimization algorithm solving the non-convex minimization problem (2).\nWe now describe in detail the implementation of the two steps, starting from the second one."}, {"title": "3.1 A non-convex Model-Based Iterative Reconstruction method utilizing Chambolle-Pock iterations", "content": "The minimization (2) with non-convex TpV prior is solved by the iterative reweighting l\u2081-norm (IRl1) strategy intro- duced in [9, 16]. It is an iterative process, where each iterate is obtained by solving a convex optimization problem, i.e.:\n$x^{(k+1)} = arg \\min_{x>0} \\frac{1}{2} || Ax - y||_2^2 + \\lambda|| w(x^{(k)}) \\odot |Dx| ||_1,$\nwhere $w(x^{(k)}) \\in \\mathbb{R}^n$ are defined as:\n$w_i(x^{(k)}) = (\\frac{\\sqrt{\\eta^2 + |Dx_i^{(k)}|^2}}{\\eta})^{\\frac{p-1}{2}},$"}, {"title": "3.2 The Deep Guess step", "content": "The Deep Guess step essentially uses a convolutional neural network as a post-processing step to refine a coarse reconstruction obtained by a fast algorithm, following a supervised learning approach.\nFigure 2 illustrates three distinct implementations of the Deep Guess step, represented by the red, orange, and green paths. Although all implementations utilize a neural network, they vary in the input and/or target images used.\nThe coarse input image of the neural network is derived from the sinogram either through a Filtered Back Projection (FBP) (green arrow) or through a limited number K of iterations of an MBIR method (orange and red arrows). As target images for the network training, the ground truth ones can be used if available. In the absence of ground truth images, which is common in real medical imaging, we implement the RISING scheme proposed in [19]. In this case (red double-headed arrow), the targets are the images generated by the same MBIR method used to compute the input images but now run until convergence (purple path).\nRegarding the MBIR method used in the DG step, we emphasize that it is independent of the one used in the final step of Figure 1. In this paper we have tested the two following MBIR methods to compute the Deep Guess.\n\u2022  A Scaled Gradient Projection (SGP) method [6, 23, 33] applied to the convex minimization problem:\n$\\min_{x\\geq0} \\frac{1}{2} || Ax - y ||_2^2 + \\mu TV_\\beta(x)$\nwhere $TV_\\beta$ is the differentiable TV prior defined as:\n$TV_\\beta(x) := \\sum_{i=1}^n \\sqrt{(D_hx)_i^2 + (D_vx)_i^2 + \\beta^2},$"}, {"title": "4 Numerical experiments and discussion", "content": "In this Section we present the numerical experiments performed on test problems with both real and synthetic medical images. The code to replicate the experiments can be found at: https://github.com/devangelista2/DeepGuess.\nHere, we first describe the datasets and test problems utilized in our experiments. Subsequently, we analyze the results obtained on real medical images using the proposed approach, comparing them to other methods. Finally, we conduct a more detailed examination of the possible implementations of the Deep Guess step and its stability properties.\nIn this Section, we will use the following notation to indicate the different Deep Guess implementations. According to the notations used in Figure 2, each version is designated by a two-part acronym. The first part specifies the algorithm used to compute the network's input, and the second part, separated by a hyphen, indicates the type of target used during network training: 'LPP' if the target is a ground truth, and \u2018RISING' if the target is an image generated by running the MBIR model to convergence."}, {"title": "4.1 The datasets", "content": "We considered both a dataset of real CT images and synthetic images.\nFor each ground truth image $x^{GT}$ in a dataset, we sampled a noise realization $e \\sim N(0, I)$, and we computed:\n$y = Ax^{GT} + \\nu \\frac{||Ax^{GT} ||}{||e||} e,$\nwhere $v \\geq 0$ is the noise level. The matrix A has been computed with Astra Toolbox [47, 48] functions, simulating a sparse fan beam geometry with $n_{angle}$ projections uniformly distributed within the angular range (0, $n_{range}$) de- grees, with a detector resolution of 2\u221an pixels. In the following, we indicate this setup as the tomographic geometry $G_{nrange, Nangle}$.\nThe dataset of real CT images considered is the Mayo Clinic dataset [27], constituted of 512 \u00d7 512 pixel real images of human abdomen. We used 3306 images for training and 357 for testing. We generated projections as in (10) on a flat detector composed of 1024 pixels, using different geometries. In all the experiments, we added noise with $v = 0.001$.\nThe considered synthetic data set is the Contrasted Overlapping Uniform Lines and Ellipses (COULE) dataset, down- loadable from www.kaggle.com/loiboresearchgroup/coule-dataset. It contains 430 synthetic sparse-gradient gray-scale images of size 256 \u00d7 256 with many overlying objects, varying in size and contrast with respect to the back- ground."}, {"title": "4.2 Preliminary results with different MBIR approaches", "content": "In our preliminary experiments, we addressed the CT image reconstruction problem using various MBIR approaches and different geometries to validate the selection of the TpV model as the final MBIR solver employed in all subsequent tests.\nThe first considered MBIR approach is the Scaled Gradient Projection (SGP) algorithm applied to a TV-regularized model (with \u03b2 = 10\u22123), as previously discussed in Section 3.2. It is named as TV(SGP) in the following.\nThe second alternative method is the Fast Iterative Shrink-Thresholding Algorithm (FISTA) applied to compute the optimal sparse wavelet (W) coefficients by solving the following minimization problem [3,4]:\n$arg\\min_{c \\in \\mathbb{C}^N} \\frac{1}{2} || Ac - y||_2^2 + \\lambda ||c||_1,$"}, {"title": "4.3 Results with the proposed Deep Guess Acceleration framework", "content": "In this Section, we present the results obtained by the proposed Deep Guess accelerated framework on the Mayo Clinic test image.\nWhen the input of the neural network in the DG step is the image computed by K MBIR iterations (red and orange arrows in Figure 2, we set K = 10 for G360,360 and K = 15 for G180,60. Concerning the regularization parameter \u03bc in (7) and (9), it has been set by trial and error to make the CP perform at its best when $x^{(0)}$ is the null image, and it never changes when a different initial guess is used.\nThe comparison includes results obtained using different initial guesses $x^{(0)}$ for the Chambolle-Pock method, specifi- cally a null vector, the analytical FBP solution, K iterations of SGP applied to the SGP(TV), and two implementations of the Deep Guess. Additionally, we compare our DG Accelerated framework with state-of-the-art data-driven ap- proaches. Among the various methods available, we consider the FBP-LPP and TV-RISING approaches, here evalu- ated also as final reconstructors.\nFocusing on the experiments performed with sparser tomographic acquisitions, we report the metrics relative to the G180,60 geometry in Table 3 obtained by TpV with both p = 0.5 and p = 0.2. At first glance, the value of p does not significantly affect the results. In all instances, the proposed framework greatly accelerates the CP method compared to initial guesses of zeros, FBP, or TV, reducing computational time by 200-400 times. Additionally, it enhances the accuracy of our comparing methods FBP-LPP, TV-RISING, and NETT. Notably, the FBP-LPP implementation of Deep Guess performs the best. As shown in Figure 6 depicting the results by p = 0.2 for this very sparse geometry, the CP reconstruction obtained with null initial guess starting still contains artifacts and noise, and the low-contrast regions appear blurred. The images obtained with the proposed Deep Guess framework are more regular and have well-distinguished details. Finally, the last row of the figure displays the Deep Guess images computed by the neural network. We underline that these images are generated by state-of-the-art methods for CT reconstruction, and our proposal demonstrates that these methods can be successfully exploited to achieve higher-quality images with limited computational effort."}, {"title": "4.4 Analysis of different Deep Guess implementations", "content": "In this Section, we analyse and compare various possible implementations of the Deep Guess step on the COULE test set. Here, we set p = 0.5 and K = 10 for both TV- and TpV-based Deep Guesses. The regularization parameter \u5165 = 10-3 was heuristically determined to optimize performance when the CP solver is applied from the zeros x(0). This parameter remains unchanged for all the experiments of this section.\nWe observe that the ground truth-free RISING strategies for computing Deep Guess perform very well, losing, in the final output, only one point of SSIM compared to the corresponding LPP approaches (96.17 against 97.32 for the TV approach and 93.01 against 94.54 for the TpV one).Additionally, the TV-LPP and TV-RISING frameworks outper- form their TpV counterparts, offering higher metrics and, more importantly, smaller standard deviations, making them more reliable."}, {"title": "5 Conclusion", "content": "This paper introduces a novel hybrid framework integrating a deep learning tool in a model-based regularized ap- proach for reconstructing CT images from sparse views. The framework addresses a non-convex model that enforces sparsity in the image gradient domain and employs deep learning to achieve accurate image restoration with minimal computational effort.\nThe proposed approach is composed of two steps. The first one, called Deep Guess, builds upon state-of-the-art effi- cient methods that generate output images using a post-processing convolutional neural network applied to a coarse reconstruction. To ensure data coherence and achieve the final reconstruction, in the second step we incorporate a few iterations of a non-convex Model-Based Iterative Reconstruction method. We propose several implementations of Deep Guess, including those that do not rely on ground-truth data, thereby improving the practical applicability of our framework for real-world scenarios.\nExperiments conducted on test problems using real CT images across various sparse geometries demonstrate the ef- fectiveness of our approach. It surpasses other methods based on the same CP algorithm with different initial iterates, as well as the state-of-the-art hybrid methods (that merge variational and data-driven approaches) in terms of both accuracy and computational complexity, being 15 to 25 times faster.\nFinally, our study demonstrates that deep learning tools can serve as a useful foundation for model-based approaches. By integrat- ing these tools, we can accelerate computations while preserving the mathematical properties and characterizations of the final reconstructions."}]}