{"title": "Neural-Symbolic Message Passing with Dynamic Pruning", "authors": ["Chongzhi Zhang", "Junhao Zheng", "Zhiping Peng", "Qianli Ma"], "abstract": "Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark datasets, with speedup ranging from 2x to over 150x.", "sections": [{"title": "1. Introduction", "content": "Knowledge graphs (KGs), which encode knowledge as relationships between entities, have received widespread attention from both academia and industry (Ji et al., 2021). Complex Query Answering (CQA) over KGs is a fundamental and practical task, which requires answering existential first order logic formulas with logical operators, including conjunction (\u2227), disjunction (\u2228), negation (\u00ac), and existen-tial quantifier (\u2203). A straightforward way is to traverse the KG to identify the answers directly (Zou et al., 2011). However, KGs often suffer from incompleteness due to the Open World Assumption (OWA) (Libkin & Sirangelo, 2009). This makes it impossible to answer a complex query with missing links using traditional traversal methods. Therefore, CQA models need to possess the ability for non-trivial reasoning (Ren et al., 2020; Ren & Leskovec, 2020).\nInspired by the success of neural link predictors (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2019; Li et al., 2022) on answering one-hop atomic queries on incomplete KGs, neural models (Hamilton et al., 2018; Ren & Leskovec, 2020; Zhang et al., 2021; Chen et al., 2022; Wang et al., 2023a; Zhang et al., 2024b) have been proposed to represent the entity sets by low-dimensional embeddings. Building on the foundation laid by these neural CQA models, message-passing-based research (Wang et al., 2023b; Zhang et al., 2024a) has demonstrated promising advancements in CQA. These message passing approaches represent logical formulas as query graphs, where edges correspond to atomic formulas (or their negations) with binary predicates, and nodes represent either constant entities or variables, as illustrated in Fig. 1. By utilizing pre-trained neural link predictors, they perform one-hop inference on edges, thereby inferring intermediate embeddings for variable nodes. An intermediate embedding is interpreted as a logical message passed from the neighboring node on the corresponding edge. Following the message passing paradigm (Gilmer et al., 2017), the embeddings of variable nodes are updated to retrieve answers. Due to the integration of pre-trained neural link predictors, these message passing approaches are effective on both one-hop atomic and multi-hop complex queries. However, limitations still exist."}, {"title": "2. Related Work", "content": "In recent years, neural models (Hamilton et al., 2018; Ren & Leskovec, 2020) have been proposed to solve complex query answering by representing sets of entities using low-dimensional embeddings. Among these models, message-passing-based approaches (Wang et al., 2023b; Zhang et al., 2024a) have demonstrated promising potential. There are also several neural models enhanced with symbolic reasoning (Bai et al., 2023b; Yin et al., 2024), which are capable of providing interpretable answers to complex queries. Further discussion for related work can be found in App. A."}, {"title": "3. Preliminaries", "content": "Knowledge Graphs and Complex Queries Let V be the finite set of entities and R be the finite set of relations. A knowledge graph KG can be defined as a set of triples E = {(eh\u2081, ri, et\u2081)} that encapsulates the factual knowledge, where each triple encodes a relationship of type ri \u2208 R between the head and tail entity eh\u2081, et\u2081 \u2208 V. The complex queries that existing studies aim to address are essentially Existential First Order queries with a single free variable (EFO\u2081) (Yin et al., 2024). In this case, a knowledge graph is a knowledge base defined by a specific first-order language (Wang et al., 2022b), where each entity e \u2208 V is a constant, and each relation r\u2208 R is a binary predicate that defines whether there is a directed edge between a pair of constant entities, i.e., (ei, ri, ej) \u2208 E iff ri(ei, ej) = True.\nWe follow previous works (Ren & Leskovec, 2020; Wang et al., 2023b) and give the definition of EFO1 queries in Disjunctive Normal Form (DNF).\nDefinition 3.1. (EFO1 Query in DNF). The disjunctive normal form of an EFO1 query $ is\n$\\Phi(y, x_1, ..., x_k) = SCQ_1(y, x_1, ..., x_k) \\lor ... \\lor SCQ_d(y, x_1, ..., x_k),$                                                             (1)\nwhere y is a single free variable, xi represents the existential variables, SCQi is i-th Sub-Conjunctive Query (SCQ) of \u03a6(y, x1, ..., xk), that is, SCQi = \u2203x1, ..., \u2203\u0445ka\u2081 \u2227 ... \u2227 a, a is an atomic formula or its negation, i.e., a = r(t1, t2) or a\u00b2 = \u00abr(t1, t2), ti is a term that is either a variable or a constant.\nAccording to (Ren et al., 2020), one can solve EFO1 queries containing the disjunction operator by solving sub-conjunctive queries with DNF. This DNF-based approach has been shown to be effective and scalable in previous studies (Ren et al., 2022; Wang et al., 2023b). For a complex query \u03a6, let A[\u03a6] represent the set of answer entities. In the case of using DNF, A[\u03a6] is the union of the answer sets for its sub-conjunctive queries, i.e., A[\u03a6] = U=1A[SCQi]. For the answer set of SCQi, an entity ea \u2208 V is considered an answer to SCQi only when SCQi(y = ea, X1, ..., Xk) = True. Clearly, in this case, we have ea \u2208 A[SCQi] \u2208 A[\u03a6]. In our work, we follow previous works (Wang et al., 2023b; Zhang et al., 2024a) and use DNF to answer EFO1 queries."}, {"title": "4. Proposed Method", "content": "In this section, we first propose how to integrate symbolic reasoning into neural one-hop inference to compute neural-symbolic messages. Then, we propose our message passing mechanism based on fuzzy logic and dynamic pruning from two views. Finally, we analyze the computational complexity of the proposed method to reveal the superiority of our message-passing-based method in terms of efficiency."}, {"title": "4.1. Neural-Symbolic One-hop Inference on Edges", "content": "Since we aim to integrate symbolic information into neural message passing, there are two different representations of entities and relations in our work: neural and symbolic representations. For the neural representation, since we utilize the pre-trained link predictor to compute neural messages, the entities and relations are encoded into the embedding space of the pre-trained neural link predictor. That is, they already have pre-trained embeddings. For the symbolic representation, each entity e \u2208 V is encoded as a one-hot vector Pe \u2208 {0,1}1\u00d7|V| and each relation r \u2208 R is represented as an adjacency matrix M, \u2208 {0,1}||\u00d7|V|, where Mij = 1 if (ei, r, ej) \u2208 E else Mii = 0. We follow (Zhang et al., 2024a) and only consider inferring the intermediate state for the variable, so the neural and symbolic representations of entities and relations remain unchanged. Accordingly, we also assign neural and symbolic representations to the variable nodes in the query graph. Specifically, each variable is equipped with a corresponding embedding and symbolic vector. However, the symbolic representation of a variable is not a one-hot vector but a fuzzy vector p \u2208 [0,1]1\u00d7|V| that represents a fuzzy set. Each element of pr can be interpreted as the probability of the corresponding entity.\nSymbolic One-hop Inference on Edges To conduct symbolic reasoning on edges, we follow TensorLog (Cohen et al., 2020) and define a symbolic one-hop inference function \u03bc for four cases depending on the input arguments. The first situation is to infer the intermediate symbolic vector pt of the tail node given the head symbolic vector ph and relational adjacency matrix Mr on a non-negated edge:\nPt = \u03bc(ph, Mr, Dh\u2192t, Pos) := N(ph Mr). (6)\nN(\u00b7) is a thresholded normalized function that is defined as follows:\n$\\mathcal{N}(p) = \\frac{p \\cdot \\mathbb{1}(p \\geq \\epsilon)}{\\max(\\epsilon, \\sum (p \\cdot \\mathbb{1}(p \\geq \\epsilon)))} ,$                                                                                                                                               (7)\nwhere \u03f5 represents the threshold and 1(p \u2265 \u03f5) is an indicator function. Similarly, the intermediate symbolic vector ph of the head node can be inferred given the tail symbolic vector Pt and Mr:\nPh = \u03bc(pt, Mr, Dt\u2192h, Pos) := N(ptM),(8)\nwhere T stands for transpose. Based on the fuzzy logic theory (Klement et al., 2013; H\u00e1jek, 2013), we follow (Xu et al., 2022) to introduce a hyperparameter a to the fuzzy logic negator and define the estimation of the intermediate symbolic vector on negated edges as follows:\n$p_t = \\mu(p_h, M_r, D_{h\\to t}, Neg) := \\mathcal{N}(\\frac{\\alpha}{|V|} - p_h M_r),$(9)\n$\\rho_h = \\mu(p_t, M_r, D_{t\\to h}, Neg) := \\mathcal{N}(\\frac{\\alpha}{|V|} - p_t M_r).$(10)\nIntegrating Neural and Symbolic Reasoning In order to integrate neural reasoning to enhance symbolic reasoning, we follow (Xu et al., 2022) and convert the intermediate embedding obtained by the neural message encoding function p into a fuzzy vector. Specifically, for an intermediate embedding inferred by p, we first compute its similarity with the embeddings of all entities. After applying a softmax operation, we can obtain a fuzzy vector p' \u2208 [0, 1]1\u00d7|V|. We define this procedure as a function f as follows:\nf(p) = softmax(concat(S(p, Ee))),(11)\n\u2200e\u2208V\nwhere Ee represents the embedding of the entity e, S is a binary similarity function, and concat is a function that maps the similarities between all entities and the intermediate embedding inferred by p to a vector. Depending on the selected pre-trained neural link predictor, S can either be an inner-product-based or a distance-based scoring function.\nThen, we can define our neural-symbolic message encoding function \u03c1, which also has four cases depending on the input arguments. Given the embedding t and symbolic vector pt of the tail node on a non-negated edge, as well as the embedding r and adjacency matrix M of the relation, we infer the intermediate state ph for the variable node at the head position on this edge. We formulate the inference task in the form of neural-enhanced symbolic reasoning:\nPh = \u03c1(t, pt, r, Mr, Dt\u2192h, Pos) :=\nN(f(p(t, r, Dt\u2192h, Pos)) + \u03bc(pt, Mr, Dt\u2192h, Pos)).(12)\nThis approach leverages the embeddings inferred by pre-trained neural link predictors to enhance symbolic reasoning results. It enables symbolic reasoning to handle missing links in observed knowledge graphs. Additionally, such an approach can represent the membership degree of variables concerning all entities in the form of fuzzy sets, thereby providing interpretability. Similarly, for the other three cases, the encoding functions o are as follows:\nP\u2081 = \u03c1(h, ph, r, Mr, Dh\u2192t, Pos) :=\nN(f(p(h, r, Dh\u2192t, Pos)) + \u03bc(ph, Mr, Dh\u2192t, Pos)),(13)\nP\u2081 = \u03c1(t, pt, r, Mr, Dt\u2192h, Neg) :=\nN(f(p(t, r, Dt\u2192h, Neg)) + \u03bc(pt, Mr, Dt\u2192h, Neg)),(14)\nP\u2081 = \u03c1(h, ph, r, Mr, Dh\u2192t, Neg) :=\nN(f(p(h, r, Dh\u2192t, Neg)) + \u03bc(ph, Mr, Dh\u2192t, Neg)).(15)"}, {"title": "4.2. Neural-Symbolic Message Passing", "content": "In this subsection, we propose a Neural-Symbolic Message Passing (NSMP) framework. This framework builds on the neural-symbolic one-hop inference proposed in Sec. 4.1 and incorporates a dynamic pruning strategy. Since NSMP is a variant of message passing networks (Gilmer et al., 2017; Xu et al., 2019), we focus on the details of the operations within a single NSMP layer. Specifically, each NSMP layer consists of two stages, each operating under different views: (1) Query Graph View: Passing neural-symbolic messages on the query graph with our proposed dynamic pruning strategy; (2) Node State View: Updating the state of each variable node that has received messages at the current layer. In the following, we first describe our framework from these two views and then explain how to answer complex queries with the proposed NSMP.\nFor constant nodes in a query graph, we follow (Zhang et al., 2024a) and do not pass messages to constant nodes. However, the situation varies across different layers of message passing when passing messages to variable nodes in a query graph. At the initial layers, messages passed from neighboring variable nodes, whose states have not yet been updated, can be regarded as noise, as these nodes do not carry any meaningful information at this stage. In contrast, at the later layers, messages from neighboring variable nodes with updated states provide valuable information derived from constants, which is especially important for variable nodes not directly connected to constant nodes. To dynamically filter out unnecessary noisy messages while retaining valuable ones, we determine if a variable node should pass messages to its neighboring variable nodes by checking whether its state has undergone any updates. Specifically, a variable node is allowed to pass messages to its neighboring variable nodes only when its state has been updated. We refer to such a strategy as dynamic pruning. The \"dynamic\" nature lies in the fact that this strategy dynamically adapts the pruning process for different cases, without the need to design specific pruning strategies for different query graphs or layer indices. In particular, this strategy not only filters out noisy messages, but also effectively avoids the computation of noise messages to improve the efficiency of message passing.\nAfter message passing on a query graph, for each node that has received messages, we need to aggregate the received messages and update its intermediate state. Let no(l) and so(l) denote the neural and symbolic state of a constant node at the [th layer, respectively, while nd(l) and s(l) represent the neural and symbolic state of a variable node at the same layer. According to Sec. 4.2.1, we do not pass messages to constant nodes and variable nodes that do not receive messages. In this case, we do not update the state of these nodes at the corresponding NSMP layer. In particular, the state of a constant node at all layers is equal to its initial state, i.e., n(0) = ne(l) and s(0) = s(l), where n(0) and s(0) represent ne the neural pre-trained embedding and the symbolic one-hot vector of the corresponding constant entity, respectively. For the state of a variable node that does not receive messages, its state at the current layer is equal to its state at the previous layer, i.e., n(l) = n(l-1) and s(l) = s(l-1), where l > 0. Both n(0) and s(0) are vectors containing all zeros.\nFor a variable node that has received messages, the messages it receives from neighboring nodes essentially represent intermediate states inferred by the neural-symbolic message encoding function \u03c1, as proposed in Section 4.1. These intermediate states, which are fuzzy vectors representing fuzzy sets, can be aggregated using fuzzy logic theory to update the state of the variable node. Specifically, for a variable node v, we form a neighbor set NeigborDP(v) that includes all its neighboring variable nodes with updated states and all its neighboring constant nodes, in accordance with the dynamic pruning strategy. For each neighboring node u \u2208 NeigborDP(v), the neural-symbolic message m(l) passed to the variable node v by u at the lth layer can be computed using the encoding function \u03c1, which depends on the information about the directed relation type and negation between the u and v, as well as the neural and symbolic state of the node u at the (l \u2212 1)th layer. After neural-symbolic message passing, we employ product fuzzy logic to aggregate the messages received by v and update the symbolic state of v at the lth layer:\ns(l) = N(mo(l) ... om(l)),(16)"}]}