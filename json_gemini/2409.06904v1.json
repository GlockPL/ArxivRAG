{"title": "Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study", "authors": ["Ilias Siniosoglou", "Vasileios Argyriou", "George Fragulis", "Panagiotis Fouliras", "Georgios Th. Papadopoulos", "Anastasios Lytos", "Panagiotis Sarigiannidis"], "abstract": "The time-consuming nature of training and deploying complicated Machine and Deep Learning (DL) models for a variety of applications continues to pose significant challenges in the field of Machine Learning (ML). These challenges are particularly pronounced in the federated domain, where optimizing models for individual nodes poses significant difficulty. Many methods have been developed to tackle this problem, aiming to reduce training expenses and time while maintaining efficient optimisation. Three suggested strategies to tackle this challenge include Active Learning, Knowledge Distillation, and Local Memorization. These methods enable the adoption of smaller models that require fewer computational resources and allow for model personalization with local insights, thereby improving the effectiveness of current models. The present study delves into the fundamental principles of these three approaches and proposes an advanced Federated Learning System that utilises different Personalisation methods towards improving the accuracy of AI models and enhancing user experience in real-time NG-IoT applications, investigating the efficacy of these techniques in the local and federated domain. The results of the original and optimised models are then compared in both local and federated contexts using a comparison analysis. The post-analysis shows encouraging outcomes when it comes to optimising and personalising the models with the suggested techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "In the past years, the utilization of intelligent devices has seen an exponential growth. Internet of Things (IoT) devices are being integrated for a multitude of purposes in areas such as smart grids, healthcare, smart buildings, and precision agriculture. These devices constantly produce a large amount of data that needs to be accurately processed and securely stored. Artificial Intelligence (AI) is a concept used to extract meaningful insights from raw IoT data. However, in order to successfully train a machine learning model, a large amount of annotated data is necessary. Furthermore, due to the large amount of data produced by the intelligent devices, the centralization of the data processing for the creation of machine learning models is no longer a viable option.\nFederated learning is a machine learning setting where multiple entities (clients) collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each client's raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective [1], [2]. Still, in most cases, as all other machine learning applications, federated learning requires a large amount of annotated data to complete a federated training session, where each client locally trains the model and sends it back to the server for fusion and global model generation. In addition, the global model produced after federated learning, although it is able to generalize, it is not customized to each client's/intelligent device's behaviour. As such, personalization methods are necessary to ensure that models produced after federated learning are customized to each client [3]. Finally, personalization techniques should require less data to customize the global model, in order not to further consume large processing power from the constrained devices.\nAl model personalization [4] involves adapting an AI model to a specific user or group of users. The primary goal of AI model personalization is to improve the accuracy and relevance of AI models for users. Personalization is achieved by considering the user's historical data, preferences, and behaviour patterns. Al models are designed to learn from data, and personalization involves providing the AI model with personalized data that is relevant to the user.\nAI model personalization is important for several reasons. Firstly, personalization improves the accuracy of AI models. When an AI model is personalized, it is more likely to provide accurate predictions or recommendations based on the user's preferences and usage patterns. Secondly, personalization enhances the user experience. Personalized AI models are more engaging and provide users with a sense of control over the content they receive. Lastly, personalization can lead to increased revenue for businesses. Personalized Al models can help businesses to improve customer satisfaction, retention, and loyalty.\nThe main focus of this paper is to present and evaluate an advanced Federated Learning System that utilises different Personalisation methods, such as Active Learning [5], Knowledge Distillation [6] and Local Memorisation [7], towards improving the accuracy of AI models and enhancing user experience in real-time Next-Generation Internet of Things (NG-IoT) applications, such as Smart Farming, Smart Home Energy Management, and Supply Chain Forecasting. This research looks at a variety of deep learning models, including the popular Long Short-Term Memory (LSTM) models [8], the more recent Transformer models [9], [10], and traditional models like simple Deep Neural Networks (DNN) and Linear Regression (LR). Through investigating these techniques on different kinds of models, we can have a more thorough grasp of the possible advantages and disadvantages of this approach for edge personalisation of federated learning models. These experiments also aim to shed light on the efficacy and constraints of these methods for enhancing and optimising pre-trained deep learning models, in addition to investigating their positive effects on standard deep learning models.\nThe overall contributions of this paper can be summarised as follows:\n\u2022 Proposes an advanced Federated Learning System that utilises different Personalisation methods towards improving the accuracy of Al models and enhancing user experience in real-time NG-IoT applications.\n\u2022\n\u2022 Explores the advantages and limitations of different Per-sonalisation methods in a Federated Learning Ecosystem.\n\u2022 Investigates the application of Federated Learning and Personalisation to benchmark DL architectures.\nProvides a comparative study of a Personalised Federated Learning in different kinds of real-world decentralised datasets"}, {"title": "II. RELATED WORK", "content": "In the past years, the utilization of intelligent devices or systems has seen an exponential growth. IoT devices are being integrated for a multitude of purposes in areas such as smart grids, healthcare, smart buildings, and precision agriculture. These devices constantly produce a large amount of data that needs to be accurately processed and securely stored. Artificial Intelligence (AI) is a concept used to extract meaningful insights from raw IoT data. However, in order to successfully train a machine learning model, a large amount of annotated data is necessary. Furthermore, due to the large amount of data produced by the intelligent devices, the centralization of the data processing for the creation of machine learning models is no longer a viable option.\nThis is why federated learning has emerged. Federated learning is a machine learning methodology that involves the collaboration of several entities, known as clients, under the direction of a central server or service provider, in order to solve machine learning problems. To achieve the learning purpose, customised updates meant for instantaneous aggregation are used in place of each client's raw data, which is stored locally and never shared or transferred.\nStill, federated learning requires a large amount of annotated data to complete a federated training session, where each client locally trains the model and sends it back to the server for fusion and global model generation. In addition, the global model produced after federated learning, although it is able to generalize, i.e., be able to predict a wider range of samples, it is not customized to each client's/intelligent device's behaviour. As such, personalization methods are necessary to ensure that models produced after federated learning are customized to each client. Finally, personalization techniques should require less data to customize the global model, in order to not further consume large processing power from the constrained devices."}, {"title": "A. Active Learning", "content": "Active learning is a machine learning technique that finds examples that are especially useful for learning, hence reducing the amount of labelled samples required for model training. Numerous research have investigated the use of this methodology in the identification of cyberattacks.\nNotably, network intrusion detection using active learning can be viewed as an unsupervised task [11]. The authors focus on exploring the way on how anomaly detection can be equipped with active learning. In particular, the authors suggest a novel querying approach that targets low-confidence data points in an effort to minimise labelling efforts. They use support vector domain description (SVDD) as the foundation for their anomaly detection method. The authors focus on integrating the approach of active learning with SVDD in order to retrain the model after querying for data points, by utilizing unlabelled as well as newly labelled data. The outcomes of the experiments showed that the ActiveSVDDs reduced labelling work while effectively differentiating between attack and normal data.\nThe authors of [12] suggest a technique that uses artificially generated examples to represent outliers and turns outlier identification into a classification challenge. They then employ selective sampling with active learning in an effort to address issues like significant computing overhead and conclusions about outlier detection that are difficult to comprehend. Specifically, the authors consider the application of ensemble-based minimum margin active learning, which is a combination of querying by committee and ensemble methodology for classification accuracy enhancement. Experiments show that the suggested methodology performs better than methods that use traditional classification procedures but apply comparable reduction strategies.\nRegarding unsupervised anomaly detection tasks, the authors in [13] suggest combining active learning techniques with deep learning methods to differentiate outliers from regular data. The authors propose active anomaly detection as an alternative approach to traditional unsupervised anomaly detection procedures, due to the latter one's difficulty of separating anomalous instances from normal samples. In active anomaly detection, feedback can be given by experts in order to point to anomalous examples in the dataset, thus providing valuable input to the model [14]. An Unsupervised to Active Inference (UAI) layer is added to unsupervised deep learning systems in order to achieve this. Specifically, at each training step, the most probably anomalous data points are selected through the most-likely positive querying strategy and sent to be labelled by the experts before the actual training begins. The outcomes of the experiment showed that the models' performance was either the same or better than that of their peers who did not apply active learning strategies."}, {"title": "B. Local Memorization", "content": "Local memorisation personalisation [7] is a technique in deep learning that enhances the generalisation capabilities of a model by introducing localised perturbations to the training data. It has been shown to be effective in a variety of applications, but it is important to consider the potential risks associated with overfitting and privacy concerns.\nIn [15], the authors discuss the various aspects of memorisation in machine learning, as well as the challenges and open issues the method poses for data privacy.\nMoreover, in [16] the authors propose a method that actively enables the memorisation of unusual patterns, rather than being automatically stored in model parameters. It also shows significant improvement in performance when the prefix representations and the ML model are learned using the same training data, indicating that the prediction problem is more complex than previously thought.\nFurthermore, in [17] the authors suggest techniques to determine if a model memorises a specific (known) characteristic or not. This approach can be implemented by an outside party since it doesn't need access to the training set. The study also highlights that while memorization can affect model robustness, it can also jeopardise patient privacy when they allow their data to be used for model training.\nRecent research in [18] based on the difference-in-differences design from econometrics suggests a novel and effective approach to assess memorisation. With the use of this technique, we may define a model's memorisation profile, and its memorisation tendencies throughout training by focusing just on a limited number of training instances. It has been demonstrated that memorization in larger models is predictable from smaller ones because it is (i) stronger and more persistent in larger models, (ii) dependent on data order and learning rate, and (iii) exhibits consistent patterns across model sizes."}, {"title": "C. Knowledge Distillation", "content": "Generally speaking, a large difference in model size between the student and instructor networks in (KD) can lead to subpar results. An enhanced KD framework [19] was proposed, which incorporates a teacher assistant and a multi-step process. Additionally, the integration of multi-teacher KD technology with dual-stage progressive KD has been suggested [20] to improve the performance of KD under limited data conditions. This approach takes advantage of the benefits provided by multi-teacher KD.\nThere have also been attempts to apply self-learning to a model via KD [21]. The aforementioned methodology employs a teacher-student paradigm with identical network structures to derive a distilled student model. This distilled model is then leveraged as a teacher to facilitate the training of a new student model, and this cycle is iteratively repeated to gradually enhance model performance. In an attempt to avoid using exceptionally large models in Neural Machine Translation (NMT) tasks, the paper at [22] utilized KD, introducing two new variations of the technique in the process.\nAdditional variations include Relational Knowledge Distillation (RKD) [23], which transfers mutual relations between data examples. Experiments results show that via RKD, student models can often outperform the teacher. Another technique is knows as Similarity-Preserving Knowledge Distillation [24] and it enables the training of a student network by ensuring that input pairs that generate comparable, or distinct, activations in the teacher network yield similar, or dissimilar, activations in the student network.\nWhile exploring the field of Logit Distillation, researchers proposed a reformulation of the conventional KD loss [25], splitting it into two components referred to as Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). Also a separate technique dubbed Virtual Knowledge Distillation (VKD) [26] leverages a softened distribution generated by a virtual knowledge generator that is conditioned on the class label, in an attempt to improve the student's performance."}, {"title": "D. Personalisation with Federated Learning", "content": "AI model personalization [4] involves adapting an AI model to a specific user or group of users. The primary goal of AI model personalization is to improve the accuracy and relevance of AI models for users. Personalization is achieved by considering the user's historical data, preferences, and behaviour patterns. Al models are designed to learn from data, and personalization involves providing the AI model with personalized data that is relevant to the user.\nAI model personalization is important for several reasons. Firstly, personalization improves the accuracy of AI models. When an AI model is personalized, it is more likely to provide accurate predictions or recommendations based on the user's preferences and usage patterns. Secondly, personalization enhances the user experience. Personalized AI models are more engaging and provide users with a sense of control over the content they receive. Lastly, personalization can lead to increased revenue for businesses. Personalized Al models can help businesses to improve customer satisfaction, retention, and loyalty.\nThe combination of active learning and federated learning has been explored in the past. In [27] a hybrid method for Human Activity Recognition (HAR) is proposed, which relies on federated learning for collaborative model training privacy enhancement, and active learning to semi-automatically annotate the gathered data. The suggested enhanced active learning approach depends on choosing unlabeled data samples with relatively low classification confidence. VAR-UNCERTAINTY is an active learning technique that compares the prediction confidence to a dynamically adjustable threshold. In case the predicted probability value of the most likely activity is found to be below the threshold, then the user is queried to obtain the ground truth of their activity. In this work, personalization is also implemented to fine-tune the model to each user, through transfer learning strategies.\nIn [28] the personalization of models generated through federated learning techniques is explored for the creation of a network flow-based Intrusion Detection System (IDS) to be applied on Distributed Network Protocol 3 (DNP3)-based Supervisory Control and Data Acquisition (SCADA) systems. Initially, a global model is created in collaborative manner by the participating nodes through federated learning. However, the global model, although able to generalize, it is not adapted to the specific needs and network traffic characteristics of each participant. To that end, active learning is applied as a personalization solution, in order to customize the global model for each user in separate, after the federated training process is concluded. In this active learning scenario, the global model is trained to a small set of fully labelled samples before being introduced to a pool of unlabelled data points. The querying strategy used, namely uncertainty sampling, aims in the selection of instances for which the calculated classification uncertainty is the highest, in order to choose valuable and informative input for the model. After the most informative sample is selected, it gets labelled, and it is used to personalize the model.\nNotably, a great deal of work has been done to find the best practices for collaborative and distributed machine learning in order to train federated global models in a safe, private, and efficient manner. A highly critical aspect for consideration whilst training classification and regression models for application on devices distributed across the network, is the difference in data attributes. Specifically, although data may be represented in a similar format for all devices, data values and dataset sizes may differ. Furthermore, the data amount on the nodes may be different, because some nodes produce a lot of data for model training, while other nodes produce less [29]. In a classification scenario, this unbalance can also be described as the difference of the amount of a specific data class in each participant. This effect may be encountered due to reasons such as differences in network traffic and sensor measurements, amongst others.\nFederated learning solutions are able to generate models based on the collaboration of the federated learning session's participants, by fusing the local models trained by each node into a single, global model. However, due to the aforementioned unbalance of the data in each node, the global model is not able to perform as accurately as possible [30]. The improvement of global models is necessary, especially in cases where the models generated through federated learning are utilized in critical sectors where high accuracy is of essence.\nTherefore, personalization methods should be applied after federated training, in order to ensure the betterment of global models through the customization of the federated output to each node's needs. Notably, personalization solutions should avoid utilizing as many training data samples as a federated learning round would require securing faster training, while personalized models should be able to perform better than their federated counterpart.\nOne approach to personalize Federated Learning (FL) is to first train a global model on a central server using data from multiple clients, followed by fine-tuning the model's parameters at each client using stochastic gradient descent (SGD) for a few epochs. This technique, also known as\" global model fine-tuning,\" allows the global model to be tailored to each client's specific data, while still benefiting from the shared knowledge of the global model. By transmitting only model parameters rather than the entire dataset, this approach reduces the amount of data sent to the central server, preserves privacy, and enables personalized model training, potentially leading to improved accuracy [31], [32].\nDeep Neural Networks are used by Marfoq et al. [33] to extract high-quality vectorial representations from non-tabular input like images and text. They present a mechanism for cus-tomisation via local memorization. They also demonstrate that by allowing local memorization at each Federated Learning (FL) client, it becomes possible to capture the local distribution shift of the client concerning the global distribution. In other words, our study shows that enabling the FL client to memorize its local data helps in identifying any differences between the local data distribution and the global data distribution.\nIn order to face the challenge that is lifelong sequential modelling and the rapid changes of user behaviour on social platforms, Ren et al. [34] present the Hierarchical Periodic Memory Network, which is designed to make each user's experience memorising sequential patterns unique. This network addresses the challenge of modelling sequential data over extended periods, while also accounting for individual differences in users' sequential patterns.\nHsieh et al. [35] propose FL-HDC, an FL technique that introduces the bipolarizing of model parameters, which involves representing each parameter using only two bits, significantly lowering the quantity of information that must be shared between the client and the central server. To avoid loss in model accuracy, FL-HDC also includes a retraining mechanism that makes use of adaptive learning rates to make up for the accuracy loss brought on by bipolarization.\nLast but not least, Lee et al [36] identify a major challenge associated with Deep Learning (DL) algorithms, which is the need for high computational power and memory resources. Their proposed solution includes a technique that involves local retraining of object detectors using a new local database.\nIn the case of Knowledge Distillation, the authors in [37] present a comprehensive overview of KD-based algorithms designed to address particular FL challenges. In addition, in order to address not identically and independently distributed (non-IID) challenges, a KD-based FL framework in edge-AI called FedLCA was presented in [38]. Both a global knowledge aggregation strategy and a local knowledge calculation strategy were put forth. Additionally, a regularisation technique based on global knowledge was offered to direct local training. Experiments have also shown us that performance can be enhanced by exchanging knowledge via the second-tolast layer of the model.\nFurthermore, in [39] a prototype-based knowledge distillation framework for FL is suggested by the authors. FedPKD allows for the collaborative learning of diverse clients and the server with varying model architectures and resource capabilities modifications by integrating prototype learning and knowledge distillation with FL. FedPKD specifically offers to transfer the dual knowledge of clients\u2014that is, the logits and prototypes from the model output to the server\u2014as well as a prototype-based ensemble distillation mechanism to combine the logits and prototypes from clients. This aggregated data can then be utilised to train the server model using an unlabeled public dataset. Furthermore, in order to enhance learning efficiency and minimise communication overhead, we provide a data filter mechanism based on a prototype that eliminates low-quality knowledge samples.\nMoreover, through the integration of contrastive learning, FL, and rehearsal-based information distillation techniques, the work in [39] established a comprehensive approach to minimise catastrophic forgetting and maximise knowledge retention in computer vision during incremental learning. In situations where FL has not been thoroughly researched, it offers a complete solution for ongoing learning, making it easier to learn and maintain transferable representations.\nLast but not least, anew method for personalised training of local and global models in a variety of heterogeneous data environments, called \"Two-fold Knowledge Distillation for non-IID Federated Learning\" (FedTweet) is proposed in [40]. In particular, to guarantee diversity in global pseudo-data, the server utilises dynamic aggregation weights for local generators based on model similarity and uses global pseudo-data for knowledge distillation to refine the first aggregated model. Clients perform adversarial training between the local model and local generator, freezing the received global model as a teacher model in the process, maintaining the personalised data in the local updates while modifying their instructions. FedTweet facilitates the exchange of teacher models across global and local models, guaranteeing mutual personalisation and generalisation."}, {"title": "III. METHODOLOGY", "content": "The core Federated Learning strategy proposed in this work is depicted in Figure 1. The local models are trained at the edge utilising remote devices' local data. The proposed Federated Learning approach keeps data on the edge rather than sending them to a central server in a local intranet or cloud data centre. A central server at a central point in the infrastructure or in the cloud orchestrates the distributed training of AI models and their fusion into one holistic global model that contains mutual knowledge from edge device training. This training scheme can be used with a very large corpus of devices, and the distributed models can be expanded horizontally (cross-device and cross-silo) and vertically (multiple security and aggregation layers), providing interoperability to a wide variety of heterogeneous environments.\nThis technique uses most of the available mechanisms to secure and protect local data and its owner. This technique integrates crucial orchestration algorithms for model optimisation, resource allocation, and energy saving as the complete process is coordinated by a single point."}, {"title": "A. Federated Learning Architecture", "content": "Contemporary computer systems are currently switching from Cloud-only implementations to edge solutions, in order to cater to the needs of the end users and offer faster services. Machine learning model training collaborative procedures in these solutions would rely on localised approach, where data would be sent by each party to a server responsible for training the aforementioned model. The introduction of the concept of edge noted that multiple distributed participants are involved while the confidentiality, integrity and availability of data exchanged was at risk. This highlighted the urgent need for a more secure and private approach to traditional model training.\nFederated learning is a distributed and collaborative model training approach with multiple participants, where instead of relying on sending data to a central entity to compute a model, it is trained locally in each party and then the weights are sent to a server for fusion and global model creation. This approach encapsulates all of the comunication, orchestration, distribution, training, and fusion of AI models from the corpus of edge devices. The models are trained on the collected data at each node, and the trained model weights are then sent to a global server for aggregation. The aggregation is the process of collecting and merging all of the subsidiary AI models from the edge devices into one global model, under a specific strategy and aggregation algorithm. The most commonly used aggregation algorithm is Federated Averaging [41] which undertakes the weighted averaging of the subsidiary models into the global model. Other such algorithms exist that depend on the nature of the problem and data. After this process the resulting global model is distributed back to teh edge devices for further optimisation or deployment. We can formulate the federated learning process as follows.\nInitially, the global parameter server shares a global model $w_{Global}$ along with a set of instructions on how to tain the model locally on the edge devices. These devices compose a federated population $P_f \\in [1,N]$ where $N \\in \\mathbb{N}^*$. Each edge device/node holds a set of local data $D_{i\\in N}$ which train the initial local model $w_i$. The local models are optimised on the on-device data $D_i$, and subsequently, the local model weights $w_{global}$ are send to the global parameter server. These weights are then aggregated using Federated Averaging (1) or a similar algorithm, resulting in a new and updated global model $w_{Global}$ [42], which incorporates the newly acquired knowledge. Equation 1 summarises the process.\n$\\frac{1}{N}w_k = \\sum_{i=1}^{N} \\frac{\\mathbb{E}_{D_i\\in D} D_i}{\\mathbb{E}_{D \\in D}} w_i$\nHere $w^k_{G}$ is the global model on the kth training iteration and $w_{i}^{k-1}$ is the remote ith model at that iteration.\nFederated learning addresses the security concerns of edge computing, however, the global model produced at the end of the federated session, although it is able to produce generalized results, is not tailored to each participant's needs. As such, personalization of global models after federation in each node, is essential to help the model produce custom and personalized results in each node.\nThe most common way to apply personalization procedures occurs after the federated learning process concludes training a global model. The model Personalisation component can be seen in Figure 2. As mentioned, the personalisation of the AI models, takes place after the federation thereof. The personalisation takes place on the edge node and utilised the locally produced data. The data used can either be part of the training and testing set, but also new data that are continuously streamed to the edge node from the deployed sensors and field devices. Figure 3 depicts the data flow of the end-to-end process of Al model optimisation proposed in this work."}, {"title": "Personalisation Component", "content": "Personalization aims to optimize and customize the global model for each participant; therefore, it is applied on each edge node of the Centralized Federated Learning approach, and by extend, it is initiated by the Federated Client service in each edge node. In the proposed approach, the Federated Client is in charge of the Local model training and the handling of the local data. Since the data never leave the Federated Client, the data processing, handling and storing is solely the responsibility of the client. The proposed centralized federated learning approach is depicted in Figure 4.\nAfter a federated training session is completed, the personalisation of the global model is performed in all participating edge nodes. The personalization methods investigated in this work compose a process that occurs locally in each edge node and does not require any communication between the participants or orchestration by the cloud, though the personalised model can again be federated if needed. The reason for choosing this approach is to localise the adaption of the AI models to the edge device while aleviating further communication overhead that can be a possible restriction in network-constrained devices. After the model is adapted in each edge nodes' needs, then it can be used for inferring predictions.\nThe overal personalisation process is divided into three sub-processes: the a) pre-processing step, the b) model training, and the model c) personalisation, depending on the utilised method. The pre-processing procedure is responsible for the transformations and adaptation of the data into features suitable for the training. The machine learning model also stems from federated learning. The model to be adapted is generated through federated learning and then passed on to the personalization component. The model personalisation is responsible for leveraging the according personalisation algorithm to further customise the AI model in the frame of the respective edge node, using the local data. In essence, the proposed Personalisation component selects the data that are valuable for the personalisation, either as a training set, or by using a sample selection process, and trains the model based on that data. We can assume the personalisation process as 2,\n$w = P_i(w, D_i)$\ndenoting the perasonalization function as $P_i$ that produces a local personalized model \u1ff6. Integrating the process to FL we get 3,\n$\\frac{1}{N} w = \\sum_k D_i  w$\nat local iteration k.\nThe interactions between the three aforementioned sub-processes of the proposed Personalisation component are depicted in Figure 5."}, {"title": "A. Applying Active Learning", "content": "Active Learning, as depicted in Figure 6, is a semi-supervised machine learning approach which allows the machine learning model, referred to as \u201clearner\" in active learning terminology, to dynamically choose samples to learn from. This means that the model itself selects training samples that it finds the most informative, in order to learn from. In the case of a classification problem, the learner selects the training sample and proceeds by querying an oracle for the provision of accurate labels. The oracle could either be a machine or a human operator. For instance, in the case of training intrusion detection systems through human supervision, the model would firstly select a training sample it deems informative, and then ask a human to label the aforementioned data sample. Next, the model gets trained by utilizing the data selected. Figure 7 represents the process of active learning training.\nAs mentioned in the previous paragraph, this method of semi-supervised learning actively selects informative data instances to be used for training. The way that the learner assesses the training value of the data instances and chooses the most valuable data sample, is through the utilization of query strategies. One of the most-utilized technique for the selection of training points, is uncertainty-based sampling. In uncertainty-based sampling, which is a technique exploited for classification problems, the active learner selects the data instances for which it is uncertain regarding the correct label. One category of uncertainty sampling, is classification uncertainty. For example, in a binary classification problem, classification uncertainty sampling will choose the instance whose probability of being positive is nearest to 0.5. On the other hand, for multi-class classification problems, the model's confidence in prediction is used as an uncertainty measure. In classification uncertainty defined in the formula, the classification uncertainty $S$ of the sample to be predicted $X_{A U_k}$ is calculated, with $p_{Y_{A U_k}}$ being the most likely prediction for this instance. The most informative instance $X_{A U_i}$ is selected by picking the sample amongst the unlabeled data pool $X_U$ for with the classification uncertainty $S$ is the highest. Classification margin-based sampling is another uncertainty sampling technique which calculates the difference in probability of the first and second most likely prediction. As such, the learner will select the sample with the smallest margin which would indicate the highest uncertainty. Regarding regression problems where future values are predicted, querying strategies implemented for training point selection include Gaussian solutions, where the uncertainty of the predictions is quantified, and error-based calculations where the samples that present the highest prediction error are selected.\n$S(X_{AU_k}) = 1 - P(p_{Y_{A U_k}}|X_{AU_k})$"}, {"title": "C. Applying Local Memorization", "content": "As explained in [43], local memorization personalisation is a deep learning strategy that adds localised perturbations to training data with the goal of enhancing model generalisation. In essense, local memorisation provides additional local samples to the local training of the AI model in order to enhance the global model, making it \"tilt\" towards the data distribution of the edge device. Local memorisation can be achieved through either providing a subset of seen $D_{seen}$ or unseen $D_{unseen}$ data by the federated training process or by selectively choosing a subset of local data $D_{local}$ that are representative of the local data distribution. all of the data belong to the clinet data [$D_{seen}, D_{unseen}, D_{local}] \\in D_i$. We can further describe the relationship of these subsets in the context of the personalisation of the AI model by including them in the overall process. We can add a weight (proportions) of each of this subsets in relation to the personalised model \u1ff6, as follows:\n$w = \\alpha w(D_{seen}) + \beta w(D_{unseen}) + \\gamma w(D_{local})$\nWhere $w(D_{seen})$ denotes the global model weights trained on the seen data subset, $w(D_{unseen})$ trained on the unseen data subset and \u1ff6(Dlocal) to the on the representative local data subset, respectively. Additionally, we include that $\\alpha, \\beta, \\gamma \\in [0,1]$, while $\\alpha + \\beta + \\gamma = 1$, as we can use any needed proportion of these subsets.\nA very obvious advantage of this method is that it does not require additional computation for the edge device to compute the optimal training vectors, like active learning, it just requires minimal training from the client's side to provide the aforementioned \"tilt\" to the model. Though useful in many contexts, it is important to evaluate possible hazards such overfitting, that can be tackled by selective finetuning."}, {"title": "D. Applying Knowledge Distillation", "content": "According to [25], Knowledge Distillation (KD) is a model personalisation method used to move knowledge from a sophisticated teacher model to a more straightforward student model.\nPast research results [25] have shown that using large models as teachers often leads to suboptimal results. A proposed solution to this problem is the early termination of the teacher's training. According to the same source [25], the process of KD is as follows. Let us consider a collection of cases in the form of $(x, y)$, where y belongs to a set of possible classes V, to train a multi-class classifier. The objective of training a model is to minimize loss, the difference between predictions and real values, for each instance of the training data.\nThe following is the KD process. Consider the scenario where we are training a multi-class classifier on a dataset of samples represented as $(x, y)$ with V as possible classes. The objective of training a model is to minimize loss, the difference between predictions and real values, for each instance of the training data.\n$L(\\theta) = \\sum_{\\vert V \\vert} I(y = k)logp(y = k\\vert x; \\theta)$\nHere, the symbol $I$ represents the indicator function, and $p$ denotes the distribution from our model that is parameterized by $\\theta$. The goal is to minimize the cross-entropy between the distribution of the model distribution $p(y/x;\\theta)$ and the degenerate data.\nAssuming access to a learned teacher distribution $q(y/x; \\theta_t)$, which may have been trained on the same data set, the approach involves minimizing the cross-entropy with the teacher's probability distribution instead of with the observed data.\n$L_{KD}(\\theta; \\theta_t) = \u2013 \\sum_{\\vert V \\vert} q(y = k\\vert x; \\theta_t)loggp(y = k\\vert x; \\theta)$\nIn which the parameter $\\theta_t$ is used to define the teacher distribution and is kept constant. The cross-entropy setup remains the same, but the target distribution is no longer a sparse distribution.\nGiven the absence of a direct term for the training data in the new objective, it is widely accepted to apply an interpolation technique that blends between the two losses.\n$L(\\theta;\\theta_t) = \u2212(1\u2212 \u03b1)L(\u03b8) + \u03b1L_{KD}(\u03b8; \u03b8_t)$\nIn the above formula, \u03b1 represents a mixture coefficient that combines the one-hot distribution and the teacher distribution."}, {"title": "E. ML algorithms for Personalisation Refinement", "content": "The main challenge presented is data regression and/or future value forecasting. To that end", "Regression": "One of the most basic types of models, and the simplest in the present collection. It consists of"}]}