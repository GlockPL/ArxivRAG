{"title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Yuhao Wang", "Bin Chen", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 \u00d7 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMS to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of remote sensing (RS), ultra-high-resolution (UHR) images often cover vast areas, encompassing diverse landscapes and a wide range of geospatial features. For deep learning applications such as semantic segmentation, object detection, and change detection, processing these large-scale images directly poses significant challenges. The high spatial resolution results in massive image sizes (e.g. 100,000 \u00d7 100,000 pixels), making it difficult to directly train neural networks with such images due to the limitation in GPU memory. Additionally, the variability in scale, class distribution, and object sizes within these large images can lead to suboptimal performance if not handled properly. To address these, a common preprocessing step is to cut the original UHR images into smaller patches (e.g. 224 \u00d7 224 or 512 \u00d7 512) [1] [2] that can fit in regular deep learning workflows\nMultimodal Large Language Models (MLLMs, in this paper we specifically refer to Vision-Language Models using LLM as base model) have demonstrated remarkable potential in RS tasks, including image captioning [3], visual grounding [4], relation reasoning [5], object detection [6], and visual question answering (VQA) [7]. However, the input image resolutions for these Remote Sensing Multimodal Large Language Models (RSMLLMs) are often limited and relatively small compared with the original satellite image. For example, models like LLaVA 1.5 [8] and H2RSVLM [7] utilize image inputs of 336 \u00d7 336 pixels, while Geochat [3] and SkysenseGPT [5] process images at 504 \u00d7 504 pixels. Vision-language models (VLMs) specifically trained for RS, such as GeoRSCLIP [9] and RemoteCLIP [10], work with even smaller inputs, typically at 224 x 224 pixels.\nIn Figure 1, we illustrate how current MLLMs struggle to answer a challenging question that requires identifying small objects in a high-resolution (2086 \u00d7 2086) image. The model's limitations in handling fine details and distinguishing small features become evident, leading to inaccurate responses when tasked with analyzing such intricate visual information (the model can answer correctly when the zoom-in image is provided). We identify four types of approaches for applying MLLMs to UHR RSI, each with its own set of limitations.\nThe first approach involves resizing UHR images to a smaller size in order to be compatible with current MLLMs. However, this significantly reduces the visibility of small objects in the images, making them challenging to detect, even for humans. For instance, H2RSVLM [7] claims its difficulty in handling small objects, likely due to limitations in input image resolution of 336\u00d7336. The second approach divides UHR images into smaller patches that can be sequentially processed by MLLMs. While this allows for compatibility with existing"}, {"title": "II. THE IMAGERAG FRAMEWORK", "content": "As shown in Figure 2 (down), ordinary Retrieval-Augmented Generation (RAG) boosts the capabilities of LLMs by retrieving and referencing relevant document chunks from external knowledge database through the semantic similarity [16] [17]. The process involves two main stages: Retrieval and Generation. In this way, RAG effectively reduces the problem of generating factually incorrect or out-of-date content [18].\nA challenge in applying RAG to UHR RSI is how to extend RAG to visual modality. It requires VLMs to associate text and visual embeddings, which may face difficulties in aligning visual concepts in satellite views with corresponding text descriptions. Besides, consider image patches as contexts could result in no visual contexts be found to aid generation. We will expand this and provide a solution in section II-A4.\nOur ImageRAG framework for RSMLLM references the idea of RAG, but focusing on retrieving visual contexts as evidences for the text query. As shown in Figure 2 (up), ImageRAG also contains two stages, Retrieval Stage and Generation Stage. We denote a given image as $I_i$ and corresponding text query (instruction) as $T_i$. The ImageRAG framework aims to retrieve a set of relevant visual contexts $V_i$ to augment input and generate response $R_i$. There are two modes for ImageRAG to work, fast path and slow path.\n\nGiven an Image $I_i$, a text query $T_i$, a Patch Division Approach $F$, an Instruction Analyzing Module $G$, a Text-Image Retrieval Module $M_{ti}$ (including image encoder $f_{img}$, text encoder $f_{text}$, select function $H_{fast}$ with threshold $\\epsilon$), a Label-Image Vector Database $D$ with threshold $\\delta$, and an Image-Image Retrieval Module $M_{ii}$ (including image encoder $f_{img}$, text encoder $f_{text}$, select function $H_{slow}$ with threshold $\\epsilon$). The visual context $V_i$ can be selected by:\n\n$V = \\begin{cases} M_{ti}(I_i, T_i | (F, G, f_{img}, f_{text}, H_{fast})) & \\text{for fast path} \\\\ M_{ii}(I_i, T_i, D | (F, G, f_{img}, f_{text}, H_{slow})) & \\text{for slow path} \\end{cases}$\n\n1) Image Patch Division Approach: There are several im-age patchification approaches. For instance, ViT [19] divides image into fixed-size, non-overlapping grids with dimensions such as 32 \u00d7 32 and 14 \u00d7 14 pixels. The Swin Transformer [20] uses a hierarchical architecture to partition the image at multiple scales, employing shifted windows to capture more diverse contextual information. DetailCLIP [21] introduced the"}, {"title": "3) Text-Image Retrieval Module:", "content": "Relevant visual contexts $V_i$ (a subset of $P_i$) can be identified with a Text-Image Retrieval Module $M_{ti}$. This module ensures that the image regions most relevant to the textual contents are selected, enhancing the model's ability to focus on meaningful image areas for the given query. n key phrases and m image patches will be encoded to text and image embeddings (d-dimension vectors) through a text encoder $f_{text}$ and an image encoder $f_{img}$, respectively. The similarity matrix between n text embeddings and m image embeddings in this module is denoted as $S_{fast}$ ($S \\in R^{nxm}$) by calculate the similarities between the embeddings of parsed text key phrases $Q_i$ and the embeddings of image patches $P_i$.\n\n$S_{fast} = f_{text}(Q_i)@f_{img}(P_i).T $   \n\nwhere @ represents matrix multiplication and T for transpose. Visual context $V_i$ will be selected based on similarity matrix $S_{fast}$ and similarity threshold $\\epsilon$ using a selection function $H_{fast}$.\n\n$V_i = H_{fast}(P_i, S_{fast}, \\epsilon) = {\\mathcal{O}_{j}}_{j=1}^{k} $\n\nwhere k means k patches satisfied the condition and selected as visual context. If k > 0, the selected visual context k will send to the RSMLLM directly, which we call \"fast path\".\n4) Label-Image Vector Database (Gallery): If k = 0, a more complicate \"slow path\" will be proceed. First, we introduce the Label-Image Vector Database D, which stores million-scale labeled RSI with the key-value pairs as follows: the key is the text embedding of the class name, generated using the text encoder $f_{text}$, and the value is the mean of the image embedding obtained using the image encoder $f_{image}$ with the set of images associated with that class.\nGiven a set of query key phrases $Q_i = {t_j}_{j=1}^{n}$, D retrieves the corresponding labels $L_i = {l_j}_{j=1}^{c}$ whose text embeddings have high semantic similarity (i.e. greater than certain thresh-old \u03b4) with the query embeddings $f_{text}(Q_i)$. Formally, the retrieval process can be expressed as:\n\n$L_i = {l_j}_{j=1}^{c} = D(f_{text}(Q_i), \\delta)$\n\nwhere l represents a label in the database that related to query $Q_i$, and d is a similarity threshold. The mean image embeddings associated with the retrieved labels $L_i = {l_j}_{j=1}^{c}$ are then provided as $E_i = {e_l}_{l=1}^{c}$, which forms the set of relevant visual concepts within D for the given queries $Q_i$."}, {"title": "5) Image-Image Retrieval Module:", "content": "When visual evidence $E_i = {e_l}_{l=1}^{c}$ for each label are obtained, we can calculate the similarity matrix of patches $P_i$ and visual evidence $E_i$.\n\n$S_{slow} = E_i@f_{img}(P_i).T$\n\nVisual context $V_i$ for slow path will be selected based on similarity matrix $S_{slow}$ and similarity threshold $\\epsilon$ using the selection function $H_{slow}$.\n\n$V_i = H_{slow}(P_i, S_{slow}, \\epsilon) = {v_j}_{j=1}^{k}$"}, {"title": "B. Generation Stage", "content": "Once visual contexts $V_i$ are selected from $P_i$, a set of image patches from image $I_i$, RS-MLLM can use such visual contexts for response generation. Unlike Ordinary RAG, which can directly organize retrieved text content with a prompt and send to a LLM to generate the response, ImageRAG must handle visual context. This means ImageRAG needs to select a MLLM that can utilize the visual contexts as visual cues. VQA LLM from V* framework [14] is chosen alone with the specifically designed prompt since it is trained for accept-ing additional visual information to focus on. The response $R_i$ for given image $I_i$ and text query $T_i$ is calculated by:\n\n$R_i = VQALLM(I_i, V_i, T_i | Prompt)$"}, {"title": "C. Implementation Detail", "content": "For the prototype of ImageRAG, we choose Complete Cover [21] from DetailCLIP to be the Patch Division Approach F, which brings 166 patches per Image. The Instruction Analyzing Module G is implemented by KeyBERT [22] with ngram range up to 4 and a maximal of top-10 phrases per text query. $f_{img}$ and $f_{text}$ are implemented by image encoder and text encoder of CLIP-ViT-L-336px [23]. $H_{fast}$ and $H_{slow}$ select top 10 patches from $S_{fast}$ and $S_{slow}$, respectively. The Label-Image Vector Database D is naively implemented with pickle"}, {"title": "III. FLEXIBILITY AND EXTENDABILITY", "content": "Section II-C presents a prototype design of the ImageRAG framework, showcasing its foundational architecture. More-over, ImageRAG is designed with considerable flexibility and extendability, allowing each module to be expanded or upgraded with more advanced versions as needed."}, {"title": "A. Instruction Analyzing Module", "content": "More sophisticated approaches can enhance the process of extracting key phrases from the text query compared with KeyBERT. For instance, incorporating the image as an input to this module enables the use of region captioning and object detection models, which can enrich and paraphrase the text query with additional contextual information.\nLatitude and longitude information can be estimated using geo-localization models [25] and ground models [26]. The ground and satellite view can be aligned for cross-reference. This inferred location information allows for retrieving nearby information from sources like OpenStreetMap, thereby im-proving the precision and relevance of the query analysis. Besides, scene graph for large-size satellite imagery [27] can be included as well to better analyze the important premise of the text query and possible relations within the image patches."}, {"title": "B. Label-Image Vector Database", "content": "The text retrieval process, which employs a set of key query phrases $Q_i$ to identify relevant labels $L_i$, can be enhanced by expanding the definitions or explanations of both the key phrases and labels. The retrieval process can achieve more accurate matching based on these expanded explanations.\nThe ultimate goal of the D is to retrieve visual concepts within the RS domain based on text input. This retrieval pro-cess can be considered as a recommendation system problem, where labels are viewed as users and the visual concepts associated with these labels are seen as items. In this frame-work, User-User and User-Item recommendation algorithms can be applied, allowing the system to suggest relevant visual concepts (items) based on the given text query (user). This approach enables efficient and contextually accurate retrieval of visual data that aligns closely with the query's intent.\nThe representation of embeddings for visual concepts of each class can be further refined. This is a well-studied problem in Few-Shot Learning and Long-Tailed Recognition. In our prototype design, the mean image embedding is calcu-lated from all images sharing the same class label, following the approach of ProtoNet [28]. However, more sophisticated representations such as clustering centroids, class centers, inter-class margin and intra-class diversity balancing could be further investigated to enhance visual concept representations. Such improvements could lead to better recall in the retrieval."}, {"title": "C. Multimodal Large Language Model with Visual Prompt", "content": "The VQA LLM from V* could be further replaced by training-free models such as ControlMLLM [32], or more powerful models that use bounding boxes as visual prompts like Ferretv2 [17]. These methods could potentially boost the VQA capability of MLLMs with given visual cues."}, {"title": "IV. EVALUATION", "content": "We design two types of benchmarks to evaluate ImageRAG in Referring Expression Task: filtered existing benchmarks for small targets and a new benchmark for UHR RSI."}]}