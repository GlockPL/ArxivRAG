{"title": "ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and Low-Rank Adaptation via Instruction-Tuned Large Language Model", "authors": ["Zezheng Qin"], "abstract": "Recommender Systems (RS) play a pivotal role in boosting user satisfaction by providing personalized product suggestions in domains such as e-commerce and entertainment. This study examines the integration of multimodal data-text and audio-into large language models (LLMs) with the aim of enhancing recommendation performance. Traditional text and audio recommenders encounter limitations such as the cold-start problem, and recent advancements in LLMs, while promising, are computationally expensive. To address these issues, Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without compromising performance. The ATFLRec framework is proposed to integrate audio and text modalities into a multimodal recommendation system, utilizing various LoRA configurations and modality fusion techniques. Results indicate that ATFLRec outperforms baseline models, including traditional and graph neural network-based approaches, achieving higher AUC scores. Furthermore, separate fine-tuning of audio and text data with distinct LoRA modules yields optimal performance, with different pooling methods and Mel filter bank numbers significantly impacting performance. This research offers valuable insights into optimizing multimodal recommender systems and advancing the integration of diverse data modalities in LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "During activities such as listening to music, reading novels, and online shopping, Provider can leverage historical data and user preferences to recommend products that better align with user tastes, thereby increasing user satisfaction [1]. This is made possible with the assistance of recommender systems.\nRecommender Systems (RS) are information filtering systems designed to predict and suggest items or content that users may find interesting, such as products, movies, music, or articles [2]. These predictions are based on past user behaviors, preferences, or the behaviors of similar users. The primary goal of RS is to enhance user experience, increase engagement, and facilitate decision-making processes. This applies to various domains, including e-commerce, entertainment, and social media. Based on the primary data modalities of the recommended content, recommender systems are categorized into audio recommender systems, text recommender systems, multimodal recommender systems, and others [3].\nAudio recommender systems are designed to recommend personalized audio content based on users' listening habits, preferences, and behaviors. Audio recommender systems find extensive application in music recommendations, audiobook recommendations and short video recommendations [4]. For short video recommendations, audio recommender systems utilize fewer training and deployment resources compared to directly using video data for recommendations [3].\nText recommender systems process and analyze text data generated from user interactions with the system, employing methods such as content-based filtering, collaborative filtering, and machine learning approaches, including GRU4Rec [5] and Graph Neural Network-based recommendation systems [6]. However, these methods struggle with the cold-start problem [7]. With the rapid development of large language models (LLMs), LLMs have demonstrated outstanding capabilities in recommender systems [8]. Nonetheless, the substantial number of parameters in these LLM-based systems makes adapting the entire system to performing recommended work computationally impractical and costly. Low-Rank Adaptation (LoRA) [9]addresses these issues by modifying specific system parameters using low-rank matrices, showing great promise. This approach is memory-efficient during training and does not impact the runtime of recommender systems [10].\nAs a result, recommender systems leveraging the fine-tuning of large language models have gained popularity. These approaches utilize a few-shot training setup, selecting a limited number of samples from the training set for model training. While they effectively address the cold-start problem, they do not account for other data modalities present in recommender system data [11], [12]. In many cases, purely text-based interactions with LLMs may be limited, as they often fail to capture information that is difficult to convey through text alone. For instance, audio can encode a range of emotions in speech, while images can represent geometric shapes and object positions, which may be challenging to describe in text [13].\nTo address these issues, multimodal recommender models have emerged. Multimodal recommender systems combine multiple data modalities (such as text, images, audio, video, etc.) for content recommendation. They are more complex than traditional single-modal recommender systems because they need to simultaneously handle and integrate information from different modalities, providing more accurate and personalized recommendations [14]. Although multimodal data offers more comprehensive user and content information, making recommendations more personalized and precise, effectively integrating modalities and enhancing the recommendation performance of large language models remains a significant challenge [3]. Additionally, as noted by [13], fine-tuning large language models with a combination of audio and text content can significantly improve their inference capabilities. However, this study only integrates text and audio information into a single LoRA module for fine-tuning, without exploring the potential benefits of separately fine-tuning audio and text modalities on the performance of large language models."}, {"title": "II. METHODOLOGY", "content": "This work focuses on integrating text and audio modalities within large language model recommender systems, with particular emphasis on exploring methods to enhance the performance of multimodal recommender systems.\n\nA. Model Framework\nThis section introduces the ATFLRec framework, aimed at facilitating the integration of audio and text data and the effective and efficient alignment of LLMs with recommendation tasks, especially in low GPU memory settings. The ATFLRec framework includes audio extraction and embedding, alignment of audio and text features, and the setup of the LoRA fine-tuning model.\nThe overall framework of ATFLRec is illustrated in Figure 1. Initially, user historical data is matched with item data to generate textual instructions, as shown in Table I. The text is then tokenized using the LLM's tokenizer and fed into the embedding module to obtain text features. Simultaneously, FBank feature extraction is applied to the audio of user-liked and target items, followed by deep feature processing. The audio features of item and target items are then fused and aligned dimensionally with the text features. Finally, both text and audio features are input into the multimodal large model recommender system, where fine-tuning is performed using LORA modules based on the labels.\nB. Internal Structure of the Large Model recommender System\nFigure 2 illustrates the internal structure of the large model recommender system. Initially, the input data comprises audio and text vectors. Attention masks and position embedding features are then applied to the text vectors. Both audio and text vectors are subsequently fed into the large model for fine-tuning. The resulting audio and text features are fused through feature fusion. Finally, the fused features are input into a classifier for classification.\n1) Audio Embedding Extraction: To effectively extract time-frequency features from audio, this study uses a Mel filter bank(FBank) for audio feature representation [15] as figure 3. Specifically, the Mel filter bank is computed with a frame length of 25 milliseconds and a frame shift of 10 milliseconds. Inspired by [16], after feature processing, the audio embedding model processes these FBank features through a series of fully connected layers with nonlinear activation functions (SiLU).\nThe specific structure of the audio embedding network is as follows: initially, the low-dimensional FBank input is mapped to a 256-dimensional hidden space through a linear transformation. This is followed by the application of the SiLU activation function to introduce nonlinearity, and further mapping to a higher-dimensional space to align with the text features dimensions, finally undergoing batch normalization to ensure stable gradient propagation and faster convergence.\n2) Recommender system fine-tuning modules: The features generated by the audio and text are used as fine-tuning data for various LoRA modules. This study utilizes the ROBERTa model [17]. The causal self-attention parameters of the model are adjusted using parameter-efficient low-rank adaptation (LoRA), while all other parameters remain frozen.\nTo explore the impact of different LoRA module configurations on recommender system (RS) performance and optimize the integration of text and audio data, this paper adjusts the original LoRA fine-tuning module."}, {"title": "C. Instruction Tuning", "content": "Instruction tuning is a core technique for training large-scale language models based on human-annotated instructions and responses [18]. This paper aims to perform recommendation tuning (rec-tuning) on large language models (LLMs) using recommendation data to develop a large-scale recommender system capable of predicting user preferences for new items. To achieve this, I convert recommendation data into the format required for instruction tuning, as illustrated in Table I."}, {"title": "III. EXPERIMENT", "content": "1) Dataset Description: MicroLens is a large-scale dataset focused on micro-video recommender systems [19]. The MicroLens dataset contains 1 billion interaction records between users and videos, involving 34 million users and 1 million micro-videos. Each video provides comprehensive multimodal information, including titles, video content, user likes, and views. Audio data is extracted from the videos using an audio extraction program. Since critical information in short videos is typically found at the beginning [20], to better handle the audio data features, the audio is truncated to the first 30 seconds.\nTo test whether the model can effectively address the cold-start problem of recommender systems with very limited training data, I use a few-shot training setting, where a limited number of samples are randomly selected from the training set for model training, as followed by [11]. This is referred to as the \"K-shot\" training setting, where K represents the number of training samples used. By setting a very small K value, it is possible to evaluate whether the method can quickly gain recommendation capability from the LLM with very limited training data.\n2) Comparison Models: Since this approach utilizes historical interactions to predict subsequent interactions, similar to sequential recommendation methods, the following sequential models are compared: GRU4Rec [21] is a recommendation algorithm based on Gated Recurrent Units. It captures the dynamic interests of users and makes recommendations based on historical behavior sequences. SASRec [22] is a sequence recommendation model based on the self-attention mechanism, which uses self-attention layers from the Transformer to model dependencies in user behavior sequences. GCN [23] is a graph neural network model designed to learn node feature representations by performing convolution operations on graph-structured data. GraphSAGE [24] is a graph neural network model focused on handling large-scale graph data, which is particularly useful for managing large-scale user-item interaction graphs.\n3) Evaluation Metrics: Since this model aims to predict user preferences for a given target item as a binary classification problem, this paper uses the widely adopted evaluation metric in recommendation systems: the Area Under the Receiver Operating Characteristic Curve (AUC) [25].\n4) Implementation Details: To ensure consistent sequence length, this paper standardizes the text length to 512, which matches the model's maximum supported length.\nFor training all LoRA modules, binary cross-entropy loss is used, with default LoRA hyperparameters set to R = 4. The optimizer is Adam. To enhance model convergence speed and avoid instability during training, a two-phase learning rate scheduling scheme is employed. Initially, a warm-up scheduler is used to gradually increase the learning rate; subsequently, a step decay learning rate is applied during the main training phase. In the warm-up phase, the learning rate increases from a minimal value ($10^{-9}$) to the set initial learning rate over 50 iterations. After the warm-up phase, the learning rate begins to gradually decay, linearly decreasing from the initial value to 0. The batch size is set to 8, with gradient accumulation performed every 4 iterations. For each training epoch, incomplete batches of data are discarded. The training process is completed on a NVIDIA RTX 3080 16GB GPU.\n\nA. Performance Comparison\nAs shown in Table II, this study evaluates various recommendation models, including GRU4REC, SASRec, GCN, GraphSAGE, and ATFLRec, under few-shot learning settings with sample sizes of 100"}, {"title": "IV. CONCLUSION", "content": "This paper introduces ATFLRec, an innovative multimodal recommender system that combines audio and text data to enhance performance. Integrating these modalities into a large-scale language model allows for more precise capture of user preferences and more personalized recommendations. The system's core advantage is its efficient use of low-rank adaptation (LoRA) modules, which improve recommendation capability while saving computational resources. Experimental results show that ATFLRec outperforms traditional deep learning methods in few-shot settings, with the best performance"}]}