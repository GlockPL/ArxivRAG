{"title": "Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift", "authors": ["Gina Wong", "Joshua Gleason", "Rama Chellappa", "Yoav Wald", "Anqi Liu"], "abstract": "Learning models whose predictions are invariant under multiple environments is a promising approach for out-of-distribution generalization. Such models are trained to extract features Xinv where the conditional distribution Y | Xinv of the label given the extracted features does not change across environments. Invariant models are also supposed to generalize to shifts in the marginal distribution p(Xinv) of the extracted features Xinv, a type of shift we call an invariant covariate shift. However, we show that proposed methods for learning invariant models underperform under invariant covariate shift, either failing to learn invariant models even for data generated from simple and well-studied linear-Gaussian models- -or having poor finite-sample performance. To alleviate these problems, we propose weighted risk invariance (WRI). Our framework is based on imposing invariance of the loss across environments subject to appropriate reweightings of the training examples. We show that WRI provably learns invariant models, i.e. discards spurious correlations, in linear-Gaussian settings. We propose a practical algorithm to implement WRI by learning the density p(Xinv) and the model parameters simultaneously, and we demonstrate empirically that WRI outperforms previous invariant learning methods under invariant covariate shift.", "sections": [{"title": "1 Introduction", "content": "Although traditional machine learning methods can be incredibly effective, many are based on the i.i.d. assumption that the training and test data are independent and identically distributed. As a result, these methods are often brittle to distribution shift, failing to generalize training performance to out-of-distribution (OOD) data (Torralba & Efros, 2011; Beery et al., 2018; Hendrycks & Dietterich, 2019; Geirhos et al., 2020). Distribution shifts abound in the real world: as general examples, we encounter them when we collect test data under different conditions than we collect the training data (Adini et al., 1997; Huang et al., 2006), or"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Domain generalization", "content": "Domain generalization was first posed (Blanchard et al., 2011; Muandet et al., 2013) as the problem of learning some invariant relationship from multiple training domains/environments Etr = {e1,...,ek} that we assume to hold across the set of all possible environments we may encounter E. Each training environment E = e consists of a dataset De = {(x, y)}1, and we assume that data pairs (x, y) are sampled i.i.d from distributions pe(Xe, Ye) with x \u2208 X and y \u2208 Y.\nFor loss function l : \u0177 \u00d7 Y \u2192 R, we define the statistical risk of a predictor f : X \u2192 \u0177 over an environment e as\nR\u00ae(f) = Epe(Xe,ye) [l(f(x\u00ba), y\u00ba)],\n(1)\nand its empirical realization as\n\u2211Del(f(x), y).\n(2)\ni=1\nEmpirical Risk Minimization (ERM) (Vapnik, 1991) aims to minimize the average loss over all of the training samples\n(3)\nUnfortunately, ERM fails to capture distribution shifts across training environments (Arjovsky et al., 2019; Krueger et al., 2021), and so can fail catastrophically depending on how those distribution shifts extend to the test data. (Previous works point to cases where ERM successfully generalizes, but the reason is not well understood (Vedantam et al., 2021; Gulrajani & Lopez-Paz, 2020), and its success does not hold across all types of distribution shifts (Ye et al., 2022; Wiles et al., 2021).) In our work, we therefore aim to learn a predictor that captures the underlying causal mechanisms instead, under the assumption that such mechanisms are invariant across the training and test environments."}, {"title": "2.2 Our causal model", "content": "We assume that input X contains both causal/invariant and spurious components such that there exists a deterministic mechanism to predict the label Y from invariant components Xiny, while the relationship"}, {"title": "2.3 Invariant learning under covariate shift", "content": "Causal predictors assume there exist features Xinv = \u03a6(X) such that the mechanism that generates the target variables Y from (X) is invariant under distribution shift; in other words, the conditional distribution p(Y(X)) is invariant across environments (Sch\u00f6lkopf et al., 2012; Peters et al., 2016). This is similar to covariate shift, where the conditional distribution p(Y|X) is assumed to be the same between training and test while the training and test distributions ptr (X) and pte (X) differ. However, while the covariate"}, {"title": "3 Density-aware generalization", "content": ""}, {"title": "3.1 Invariant feature density weighting", "content": "Consider choosing some set of weights a(xinv), \u03b2(Xinv) such that a(Xinv)Pe\u2081 (Xinv) = \u03b2(Xinv)Pe; (Xinv) (Martin et al., 2023). This defines a pseudo-distribution q(xinv) xa(Xinv)pe; (Xinv) over the invariant features. It turns out that for spurious-free models f(x), even when we reweigh the entire joint distribution pe\u2081 (X, Y) instead of the marginal distribution over Xinv alone, the expected loss will still be equal to that over q(xinv) (up to a normalizing constant). This also holds when we do the reweighting on another environment, which leads us to the following result proved in the appendix."}, {"title": "3.2 Finding an effective weighting function", "content": "Each weighting function introduces different challenges to the learning process. Density ratio weighting has the typical importance weighting limitation, where the support of the distribution in the numerator needs to be contained in the support of the distribution in the denominator. In addition, density ratio weights tend to have a high maximum and high variance, and as a result, lead to a high sample complexity-another known issue of importance weighting (Cortes et al., 2010). Conversely, while the number of weights/constraints for the general form of weighted risk invariance is combinatorial in the number of environments, density ratio weighting only requires weights and constraints that are linear in the number of environments. (Intuitively, this is because we set a reference environment, weighted by a = 1, and weight all other environments by a density ratio that aligns them with the reference environment; we discuss this in more detail in our proof of Theorem 1)."}, {"title": "3.3 Comparison to other invariant learning methods", "content": "Comparison to REx Risk invariance across environments is a popular and practically effective method for achieving invariant prediction (Xie et al., 2020; Krueger et al., 2021; Liu et al., 2021b; Eastwood et al., 2022) that was formalized by REx (Krueger et al., 2021). Specifically, the VREx objective is a widely used approach of achieving risk invariance, where the variance between risks is penalized in order to enforce equality of risks:\nRVREx(f) = RERM(f) + AVRExVar({R\u00ae\u00b9 (f), . . ., Rek (f)}).\n(7)\nThe variance between risks is equal to the average mean squared error between risks, up to a constant. Note that the WRI penalty in equation 6 reduces to the VREx variance penalty under uniform weighting.\nUnder invariant covariate shift, risk invariance objectives can be misaligned with the goal of learning a predictor that only depends on Xinv. To illustrate this, consider a problem with heteroskedastic label noise (where some instances are more difficult to classify than others (Krueger et al., 2021)). Under this setting, invariant covariate shift results in a case where one training environment has more difficult examples than the other. Then an invariant predictor would obtain different losses on both environments, meaning it will not satisfy risk invariance. Figure 1 demonstrates this case, and shows that a risk invariance penalty (VREx) does not learn the invariant classifier. In contrast, weighted risk invariance holds when there is both heteroskedasticity and invariant covariate shift."}, {"title": "3.4 Implementation", "content": "Evaluating the WRI penalty requires weighting the loss by functions of the invariant features. However, we do not have access to the invariant features otherwise the invariant learning problem would already be solved. In Appendix C, we empirically show that for a simple case of our regression setting where we can compute exact densities, optimizing for WRI on the joint features (rather than the invariant features) still leads to learning invariant feature weighting functions. Unfortunately, density estimation on high-dimensional data is notoriously difficult, and we often cannot compute exact density values in practice.\nTo tackle more realistic settings where we do not have exact density values, our approach must optimize for weighted risk invariance while also learning the feature densities. We therefore propose to use alternating minimization, an effective strategy for nonconvex optimization problems involving multiple interdependent"}, {"title": "4 Experiments", "content": "We evaluate our WRI implementation on synthetic and real-world datasets with distribution shifts, particu-larly focusing on cases of covariate shift in the invariant features. In all of the datasets, we select our model based on a validation set from the test environment, and we report the test set accuracy averaged over 5 random seeds with standard errors. Training validation selection assumes that the training and test data are drawn from similar distributions, which is not the case we aim to be robust to (Gulrajani & Lopez-Paz, 2020); test validation selection is more useful for demonstrating OOD capabilities (Ruan et al., 2021).\nOur major baselines are ERM (Vapnik, 1991), IRM (Arjovsky et al., 2019), and VREx (Krueger et al., 2021). IRM and VREx are two other causally-motivated works that also search for conditional invariance as a signature of an underlying causal structure. Because WRI shares a similar theoretical grounding, we find it particularly important to compare our empirical performance with these works. Appendix E includes comparisons with other non-causal baselines, as well as additional experiments and details."}, {"title": "4.1 ColoredMNIST", "content": "ColoredMNIST (CMNIST) is a dataset proposed by Arjovsky et al. (2019) as a binary classification extension of MNIST. In this dataset, digit shapes are the invariant features and digit colors are the spurious features. The digit colors are more closely correlated to the labels than the digit shapes are, but the correlation between color and label is reversed in the test environment. The design of the dataset allows invariant predictors, which base their predictions on the digit shape, to outperform predictors that use spurious color information. We create a heteroskedastic variant on this dataset, HCMNIST, where we vary the label flip probability with the digit. We also create HCMNIST-CS, as a version of HCMNIST with invariant covariate shift, by enforcing different distributions of digits in each environment. For a predictor to perform well on the test set of these datasets, it must learn to predict based on the invariant features, even under heteroskedastic noise (present in both HCMNIST and HCMNIST-CS) and invariant covariate shift (HCMNIST-CS). Finally,"}, {"title": "4.2 Real-world datasets from DomainBed", "content": "We evaluate our method on 5 real-world datasets that are part of the DomainBed suite, namely VLCS (Fang et al., 2013), PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), TerraIncognita (Beery et al., 2018), and DomainNet (Peng et al., 2019). We run on ERM-trained features for computational efficiency, as these should still contain spurious information as well as the invariant information necessary to generalize OOD (Rosenfeld et al., 2022). In order to achieve a fair comparison, we evaluate all methods starting from the same set of pretrained features. We report the average performance across environments on each dataset in Table 4.\nWhile the WRI predictor achieves higher accuracy than the baselines, we find that the performances are all similar. Methods like VREx that rely on assumptions like homoskedasticity may still see high accuracy on datasets that fulfill those assumptions. We note that, in addition to generalization accuracy, our method has the additional benefit of reporting when predictions are based on invariant features that have low probability in training. Additional information and results on DomainBed are provided in Appendix E.5."}, {"title": "5 Related works", "content": "Invariant learning Much of the causal motivation for invariant learning stems from Sch\u00f6lkopf et al. (2012), who connected the invariant mechanism assumption to invariance in conditional distributions across multiple environments. Since then, invariant learning works have found that the invariant mechanism assumption results in additional forms of invariance, including invariance in the optimal predictor Arjovsky et al. (2019) and risk invariance under homoskedasticity (Krueger et al., 2021). Uncovering the underlying invariant mechanisms through explicit causal discovery (Peters et al., 2016) can be difficult to scale up to complex, high-dimensional data, so a significant branch of invariant learning instead leverages these additional forms of invariance to recover a predictor based on invariant mechanisms. While these methods do not provide all the guarantees typically offered by methods in causal discovery, they are faster and simpler in comparison, and can still provide theoretical guarantees for certain causal structures.\nLearning theory provides another framework for understanding invariant learning. The seminal work of Ben-David et al. (2010) shows that the risk on a test environment is upper bounded by the error on the training environment(s), the total variation between the marginal distributions of training and test, and the difference in labeling functions between training and test. While the first term is minimized in ERM, the second term motivates learning marginally invariant features \u03a8(X) such that pe (\u03a8(X)) is invariant across e (Pan et al., 2010; Baktashmotlagh et al., 2013; Ganin et al., 2016; Tzeng et al., 2017; Long et al., 2018; Zhao et al., 2018), and the third term motivates learning conditionally invariant features (X) such that pe (Ye (Xe)) is invariant across e (Muandet et al., 2013; Koyama & Yamaguchi, 2020). Observing the importance of both approaches, several works even attempt to learn features that are both marginally and conditionally invariant (Zhang et al., 2013; Long et al., 2015; Li et al., 2021; Guo et al., 2021). Our work follows a similar vein: we focus on learning conditionally invariant features, under the assumption that these align with the underlying causal features, but we also reweight the conditionally invariant features to mitigate any shift in their marginal distributions.\nImportance weighting methods Starting from the early and influential method of importance weighting (Shimodaira, 2000), many methods for dealing with covariate shift use some form of weighting approach (Martin et al., 2023). Weighting methods work by weighting the loss with a density ratio of test features to training features (Shimodaira, 2000; Huang et al., 2006), or vice versa (Liu & Ziebart, 2014). However, these methods assume that the support of the features in the numerator is contained in the support of the features in the denominator. Further, they can perform poorly when the density ratio is large (i.e. in areas where the features in the denominator are much more rare); these large values lead to high variance and low effective sample size in the density ratio estimator (see Appendix B). For this reason, we propose density weighting rather than density ratio weighting, as density weighting is not limited by the support of any feature distribution and leads to better performance in practice, as established by Martin et al. (2023). Alternatively, we can also overcome the density ratio support limitation with a generalized form of importance weighting (Fang et al., 2024).\nDensity estimates for downstream tasks Density estimates are useful for a variety of downstream tasks. They serve as an intuitive measure of epistemic uncertainty (H\u00fcllermeier & Waegeman, 2021): in our work, the invariant feature density estimates approximate how much reliable (spurious-free) evidence we have in the training data. As a result, our density estimates are useful for detecting when samples are OOD, along the line of previous/concurrent work (Breunig et al., 2000; Charpentier et al., 2020; Peng et al., 2024). In general, density estimates are useful for building trustworthy systems, and they can be leveraged for both"}, {"title": "6 Discussion", "content": "Limitations of our method The invariant feature weighting functions in WRI allow its solutions to be robust to the case of heteroskedasticity and invariant covariate shift. However, these weighting functions are constrained by the invariant feature densities of each environment; we do not have access to these densities in practice, and can only recover an estimate of that density through alternating minimization. Future work could focus on better understanding the optimization process (in the style of Chen et al. (2022; 2024)), with the goal of recovering the invariant feature densities with more rigorous guarantees. We believe that an accurate invariant feature density estimate would be useful for reporting the confidence of predictions during deployment. In general, the question of how to better quantify the uncertainties of domain adaptation/generalization models remains an open line of research.\nLimitations of invariant prediction Invariant prediction methods assume there exists some general-izable relationship across training environments, but there are cases where it is impossible to extract the relationship. Notably for linear classification, Ahuja et al. (2021) proves that generalization is only possible if the support of invariant features in the test environment is a subset of the union of the supports in the training environments. For this reason, it is important to report when we are given data points that are rare in the invariant feature distribution; then, users know when a model is extrapolating outside of the training domain.\nBroader impacts Compared to other invariant learning methods, our WRI method offers the additional benefit of providing density estimates that allow a level of OOD detection, which can be useful in safety-critical situations. In general, we believe these estimates provide a useful measure of epistemic uncertainty that is not usually present in domain generalization methods."}, {"title": "7 Conclusion", "content": "Our work demonstrates the utility of weighted risk invariance for achieving invariant learning, even in the difficult case of heteroskedasticity and invariant covariate shift. We proved that weighted risk invariance is a signature of spurious-free prediction, and we further proved that enforcing weighted risk invariance allows us to recover a spurious-free predictor under a general causal setting. We proposed an efficient and useful method of weighting by the invariant feature densities, WRI, and we introduced an algorithm to practically enforce WRI by simultaneously learning the model parameters and the invariant feature densities. In our experiments, we demonstrated that the WRI predictor outperforms popular baselines under invariant covariate shift. Finally, we showed that our learned invariant feature density reports when invariant features are outside of the training distribution, a useful signal for the predictor's trustworthiness."}, {"title": "8 Reproducibility Statement", "content": "The code for generating the figures and empirical results can be found at https://github.com/ginawong/ weighted_risk_invariance/."}, {"title": "A Proofs", "content": "Proposition 1. Let Assumption 1 hold over a set of environments E \u2286 P. If a predictor f is spurious-free over E, then for every pair of environments ei,ej \u2208 E, their weighted risks are equal if their respective weighting functions a and \u1e9e obey a(xinv)Pei (Xinv) = \u03b2(Xinv)Pe; (Xinv).\nProof. The weighted risk for environment 1 is given by\nIf f and a are invariant, then we can transform this integral as follows.\nSimilarly, if \u1e9e is invariant, then the weighted risk for environment 2 is given by\n(A.2)\nBecause pe (y Xxinv) is the same for all e, then equation A.1 and equation A.2 are equal if a(Xinv)P1 (Xinv) =\n\u03b2(Xinv)P2(Xinv).\nTo show that weighted risk invariance leads to OOD generalization, we must first make an assumption on the diversity of our training environments. We give a definition for general position in a similar style to previous works (Arjovsky et al., 2019; Wald et al., 2021).\nDefinition A.1. We are given a set of k environments such that (5) > 2dspu, where dspu is the dimension of spurious features. Each environment i has a weighted covariance E\u00b2, and a weighted correlation \u03bc\u00b2 with another environment j. We define these as\n(A.3)\nWith this definition, we proceed to show that for a set of environments in general position, a linear regression model that satisfies weighted risk invariance must be spurious-free.\nTheorem 1. Consider a regression problem following the data generating process of equation 5. Let E be a set of environments with |E| > dspu that satisfies general position. A linear regression model f(x) = w x with w = [Winv, Wspu] that satisfies weighted risk invariance w.r.t the squared loss must also satisfy w spu = 0. For density ratio weighting, general position holds with probability 1."}, {"title": "B On the sample complexity of different weighting functions", "content": "Our work differs from other works on weighting to mitigate distribution shift. We are the first to propose weighting by functions of the invariant features; this allows us to use weighted risks to recover a spurious-free predictor under shift, whereas most forms of weighting only focus on mitigating shift, without any invariant learning mechanisms. Still, work on the generalization bounds of importance weighting (to mitigate shift) can be readily extended to our case. In this section, we use the arguments of Cortes et al. (2010) to show that convergence guarantees are improved by selecting weights with a lower variance and lower maximum value. This result explains some of the benefits of using density weights over density ratio weights.\nWe fix f \u2208 H, where H denotes the hypothesis set under consideration. For some weighting functions a and\n\u03b2 such that a(x)pe\u2081 (x) = \u03b2(x)pe2 (x), we define the weighted risk over two environments e1 and e2 to be\n(B.10)\nThe empirical weighted risk is defined to be\n(B.11)\nFollowing the proof of Theorem 1 from Cortes et al. (2010), we assume that l(f(x), y) \u2208 [0,1]. we de-fine a random variable Za = a(X)l(f(X),Y) \u2013 R(f) where X, Y are drawn from pe\u2081. Then, \u03c3\u00b2(\u0396\u03b1) =\nEpe\u2081 (X,Y) [a\u00b2(x)l\u00b2 (f(x), y)] \u2013 R(f)2. Applying Bernstein's inequality yields\n(B.12)\nwhere Ma = max{1, supx a(x)}. Setting & equal to the bound from Eq. (B.12) and solving for e, it follows that with probability at least 1 - 8 the following bound holds for any d > 0:\n(B.13)\nFollowing the same steps, we get a bound for R(f) 1\nIf we combine these events with the union bound, then the probability of both of these bounds holding is at least 1 \u2013 2\u0431.\nNote that for density ratio weights, the maximum values Ma and M\u03b2 can be large (e.g. when a sample is\nmuch more rare in one environment than another); this also leads to the variances \u03c3\u00b2(Za) and \u03c3\u00b2(\u0396\u03b2) being large. In contrast, density weights tend to have a lower maximum value and lower variance.\nWe emphasize that this section does not provide a sample complexity bound for our method; it should be treated as a exploratory comparison of different weighting functions only, with the caveat that our result relies on several assumptions that would not hold in practice. We fix the hypothesis f, but it would typically be learned from training data. We also assume the optimal weighting functions are known, when these would need to be learned from the data/trained model as well."}, {"title": "C Optimization of WRI for the 2D Gaussian case", "content": "In the main body of the paper, we show that for a set of invariant weighting functions, weighted risk invariance is a necessary and sufficient condition for spurious-free prediction. This demonstrates the importance of correcting for invariant covariate shift, and extends the importance weighting method to the invariant learning setting. However, we encounter a chicken-and-egg problem in the implementation of our method; specifically, that we weight by the density of the invariant features, without having access to the invariant features in practice. At best, we only have access to the density of the joint features, both spurious and invariant.\nIn this section, we show that for a 2D case of our regression setting in equation 5, optimizing for WRI leads us to learn a model that discards the spurious features, even when we start with the density of the joint features. In particular, we see from Figure C.1 that optimizing for risk weighted by the joint feature density eventually leads to the weights on the spurious features going to zero, leaving only the weights on the invariant features."}, {"title": "C.1 Experiment details", "content": "The data generation process for this experiment follows equation 5, where each data point x = [Xinv, Xspu]. This data is then used to train a two-layer linear model, where the feature layer has two weight parameters: one weight for the invariant features, and one for the spurious features. The model layers do not have any activation functions, so they are straightforward linear transformations.\nTo decouple our investigation from the difficulties associated with density estimation, we assume that Xinv is Gaussian-distributed. In this way, we have a closed form solution for the density of the features. When we optimize for WRI, we therefore weight by the exact feature density.\nWe use this setup to optimize for both the WRI and ERM methods. To start, we initialize the model with either equal feature weights (where the invariant and spurious feature weights are equal), or with the"}, {"title": "C.2 Discussion", "content": "The results of Figure C.1 are promising evidence that invariant feature reweighting can be realized in a practical implementation, even though we only have access to the input data and not the invariant features. However, please note that we only show this for a specific case of our regression setting, where the invariant and spurious features are easily separable and the invariant features Xiny follow a Gaussian distribution. If we were to run this experiment on data from an unknown distribution, the results would be less clear: our weighting functions would depend on how correctly we can estimate feature density, an inexact operation in high dimensional cases."}, {"title": "D Implementation details", "content": "When the optimal invariant predictor was first introduced with IRM, it was first motivated as a constrained optimization problem that searched over all spurious-free predictors for the predictor with the lowest training loss. The practical objective IRMv1 was then introduced, where the constraint was approximated by a gradient penalty. Many follow-up works took a similar path, where a penalty is used as some form of invariance constraint. Our penalty is weighted risk invariance; according to our theory, models with zero weights on spurious features will satisfy the WRI constraint. Therefore, a learning rule that finds the most accurate classifier satisfying WRI will learn an optimal hypothesis that relies on the invariant features alone.\nWe implement this learning rule with alternating minimization; our specific algorithm is described in Al-gorithm D.1, and a graphical representation of our network architecture is shown in Figure D.2. This is a complex optimization procedure, and we do not derive guarantees on its convergence to invariant repre-sentations in this work. We simply motivate this implementation with the knowledge that a spurious-free predictor would minimize the WRI penalty, which we optimize for when the penalty coefficient in equation 9 is sufficiently large. We test that optimizing WRI on the observable joint feature density still focuses model weights on the invariant features in Appendix C. In the main body of the paper, we empirically show that our objective is effective at recovering a generalizable predictor, as well as density estimates with a meaningful signal for OOD detection. Later in this section (Appendix D.3), we also test the quality of our density estimates for a 2D case of our regression setting in equation 5."}, {"title": "D.1 Optimizing WRI leads to invariant features", "content": "To provide additional intuition on why optimizing for WRI leads to learning invariant features, let us consider trying to reweight a representation (X) that is not invariant. Specifically, assume that for two environments, p\u2081(Y | \u03a6(X)) \u2260 p2(Y | \u0424(X)). In what follows, we demonstrate why WRI would reject such a representation. We focus on density ratio reweighting for simplicity, so that p = p\u2081(\u03a6(X)) \u00b7 p2(\u03a5 | \u03a6(X)).\nTo satisfy invariance under this reweighting, we must have p(Y | \u03a6(X)) = p1(Y | \u03a6(X)). Clearly this does not hold, since p(Y | \u03a6(X)) = p2(Y | \u03a6(X)), and this lack of invariance can be identified from data under mild assumptions on overlap. Therefore, enforcing this type of weighted invariance constraint (i.e. matching of the conditionals p\u2081(Y | \u03a6(X)) and the density ratio weighted distribution p(Y | \u03a6(X))) would rule out a non-invariant representation \u03a6(X). Conversely, an invariant representation would naturally satisfy this constraint.\nThe gap between this constraint and weighted risk invariance lies in the fact that we only enforce invariance of the risk, not the full conditionals (we do this because imposing risk invariance is more tractable in practice)."}, {"title": "D.2 Practical strategies", "content": "We observe that the magnitude of the WRI regularization term from equation 9 varies significantly in practice, making it difficult to choose a good value for \u5165. This is true even when the true invariant density is known. We employ two strategies to deal with this. First, we constrain the density model to predict values within a pre-defined range by applying a sigmoid activation on the final prediction with constant scale and shift factor. Additionally, we divide the WRI term by the average negative log-likelihood, which also helps to decouple the empirical risk from the WRI regularization term.\nWe allow different optimization parameters for the prediction model and density estimation models. Both op-timizers have a different learning rate, weight decay, batch size, and A penalty. (The range of hyperparameter values we use are provided with the DomainBed details in Appendix E.5.)"}, {"title": "D.3 Density estimation quality", "content": "Optimizing WRI with exact feature density values successfully recovers an invariant predictor, as we show in Appendix C. Yet, density values are often estimated with some inaccuracy in practice. In this section, we investigate the accuracy of our alternating minimization method in estimating feature densities. We use the Gaussian case of our regression setting in equation 5, as detailed in Appendix C. Combining this with a"}, {"title": "E Extensions on experiments and additional experimental details", "content": ""}, {"title": "E.1 Toy dataset hyperparameters (Figure 1 details)", "content": "To produce Figure 1, we define a toy dataset consisting of the three environments described here. Environ-ments 1 and 2 are used for training and environment 3 is used as test. We let Y = 1[Xinv + \u03b5 > 0], where \u03b5 is a standard normal random variable and 1 is the indicator function. The following data distributions are used:\nXinv ~ N(0, 22),\nXv ~ N(0, ()\u00b2),\nX ~ N(0,32),\ninv\nXpu|Y = 0 ~ N(1, ()\u00b2),\nXpu Y = 0 ~ N(1, 22),\nXpu|Y = 0 ~ N(\u22121,12),\nXpu|Y = 1 ~ N(\u22121, ()\u00b2),\nXpu|Y = 1 ~ N(-1,22), andXpu|Y = 1 ~ N(1,12).\nWe sample 104 points from each environment. The predictor is a simple linear model. The ERM, IRM, VREX, and WRI objectives are optimized using scikit-learn (Pedregosa et al., 2011). We use a penalty weight of 105 for IRM, a penalty weight of 1 for VREx, and a penalty weight of 1500 for WRI. We note that VREx converges to an absurd solution when using a larger weight, despite generally using a much larger weight in the DomainBed implementation. The penalty weights were roughly optimized by eye to achieve the best qualitative results (regarding invariance)."}, {"title": "E.2 Experiments on multi-dimensional synthetic dataset", "content": "Data generated following structural equation model We construct a multi-class classification sim-ulation based on the linear causal model shown in Figure 2. The spurious and invariant distributions are drawn from normal distributions Xinv ~ N(\u03bc\u03b5, \u03a3\u03b5) and Xspu|Y = y ~ N(\u03bc\u03b5, \u03a3\u03b5,y) and X is the concatenation of Xinv and Xspu. To simulate the classification scenario, we define the class label to be\nY = arg max, w Xinv +Ey where \u025by ~ N(0, \u03c3y).\n(\u0395.14)"}, {"title": "E.3 Heteroskedastic CMNIST", "content": "ColoredMNIST (CMNIST) is a dataset proposed by Arjovsky et al. (2019) as a binary classification extension of MNIST where the shapes of the digits are invariant features and the colors of the digits are spurious features that are more tightly correlated to the label than the shapes are, with the correlation between the color and the label being reversed in the test environment. Specifically, digits 0-4 and 5-9 are classes 0 and 1"}, {"title": "E.4 Out-of-distribution Detection with CMNIST", "content": "To evaluate the out-of-distribution (OOD) detection performance on CMNIST, we first train a model on the two training environments of (traditional) CMNIST, then assess the model's ability to classify sample digits as in-domain or out-of-domain on a modified version of the CMNIST test environment. The modified test environment is a mix of unaltered digits for the \"in-domain\" samples and flipped digits for the \"out-of-domain\" samples. Specifically, we horizontally flip digits 3, 4, 7, and 9, and vertically flip digits 4, 6, 7, and 9. In this way, we create CMNIST samples that would not exist in the training distribution but still appear plausible. Figure E.6 provides examples of these in-domain and out-of-domain samples. Note that we only flip digits that are unlikely to be confused with real digits after flipping."}, {"title": "E.5 DomainBed experimental details and results", "content": ""}, {"title": "E.5.1 Featurizer", "content": "We use"}]}