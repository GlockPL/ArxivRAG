{"title": "Liger Kernel: Efficient Triton Kernels for LLM Training", "authors": ["Byron (Pin-Lun) Hsu", "Yun Dai", "Vignesh Kothapalli", "Qingquan Song", "Shao Tang", "Siyu Zhu", "Steven Shimizu", "Shivam Sahni", "Haowen Ning", "Yanning Chen"], "abstract": "Training Large Language Models (LLMs) efficiently at scale presents a formidable challenge, driven\nby their ever-increasing computational demands and the need for enhanced performance. In this work, we\nintroduce Liger-Kernel, an open-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input chunking, our kernels achieve\non average 20% increase in training throughput and a 60% reduction in GPU memory for popular LLMS\ncompared with HuggingFace implementations. In addition, Liger-Kernel is designed with modularity,\naccessibility and adaptability in mind, catering to casual and expert users. Comprehensive benchmarks\nand integration tests are built-in to ensure compatibility, performance, correctness and convergence across\ndiverse computing environments and model architectures. The source code is available under a permissive\nlicense https://github.com/linkedin/Liger-Kernel.", "sections": [{"title": "Introduction", "content": "Scaling Large Language Model (LLM) training (Vaswani, 2017; Wei et al., 2022; Brown et al., 2020; Team\net al., 2023; Touvron et al., 2023; Dubey et al., 2024) relies heavily on the stability of compute infrastructure\nand is susceptible to efficiency bottlenecks. Host/device memory management and latency-bandwidth trade-\noffs for tensor operations are central to the efficiency issues. However, beyond algorithmic scaling strategies,\nthe true potential for optimization lies in fusing operations at the GPU kernel level, which minimizes memory\ncopying and maximizes parallel efficiency. These last-mile kernel-level optimizations are crucial because any\ngains at this level are amplified by the inherent parallelism of GPUs, making them indispensable for improving\noverall training performance. Despite recent advancements in hardware and software usability for distributed\ntraining, optimizing the training process remains a highly complex and specialized task - which requiring\nnot only a deep understanding of both LLM algorithms and hardware architectures but also significant time\nand financial investments.\nTo address these challenges, we present Liger-Kernel, an open-source library of efficient Triton kernels\n(Tillet et al., 2019) for LLM training. Liger-Kernel enhances the efficiency and scalability of LLM training\nthrough a highly flexible and user-friendly interface. It streamlines complex tensor operations, minimizes\ncomputational overheads with kernel fusions (Dao et al., 2022) and seamlessly integrates with diverse com-\nputing environments. Novice users can improve LLM training efficiency with a few lines of code, while\nadvanced users can customize their model with modular components and adaptive layer configurations to\nsuit their needs. Liger-Kernel requires minimal dependencies, i.e., PyTorch (Zhao et al., 2023) and Triton.\nLiger-Kernel supports multiple distributed frameworks such as PyTorch FSDP, DeepSpeed ZeRO (Rasley\net al., 2020) and ZeRO++(Wang et al., 2023; Dai et al., 2024), ensuring broad compatibility and performance\noptimization across various hardware platforms."}, {"title": "Preliminaries", "content": "Eager mode execution in PyTorch (Paszke et al., 2019) provides a smooth development and debugging\nexperience when authoring model code. However, step-by-step execution of PyTorch operations entails extra\ncomputational overheads, including function call stack, dispatching, and CUDA kernel launch latencies.\nIn addition, materializing every intermediate activation for backward pass also introduces significant GPU\nmemory usage. The majority of the efforts for addressing this issue have focused on model compilation and\nalgorithmic operation fusion. Recently, more practitioners are implementing custom operation fusion in the\nTriton language (Tillet et al., 2019) to replace native PyTorch execution of model code."}, {"title": "Model Compiler", "content": "Model compilers transform high-level model descriptions (for example, torch.nn.Module) into optimized,\nlow-level code that can be executed more efficiently, particularly on specialized hardware such as GPUs.\nExamples of such compilers include torch.compile (Ansel et al., 2024), TVM (Chen et al., 2018), XLA\n(Sabne, 2020), and nvFuser. torch.compile is the latest PyTorch-native model compilation feature in-\ntroduced in PyTorch 2.0. Its frontend just-in-time (JIT) captures the computational graph and converts\npython-level operations into an intermediate representation (IR). Its backend performs low-level optimiza-\ntions on the IR and translates into high-performance code in Triton for GPUs and C++ with OpenMP for\nCPUs. Apache TVM provides a unified intermediate representation for various hardware platforms, aiming\nto bridge the gap between high-level deep learning frameworks and diverse deployment targets. XLA, de-\nveloped by Google, is designed to optimize TensorFlow (Abadi et al., 2016) and JAX (Frostig et al., 2018)\nbased training workflows. It performs operation fusion, layout optimization, and kernel generation tailored\nto the target hardware. nvFuser is a PyTorch-specific JIT compiler developed by NVIDIA. It is especially\ncapable of generating optimized CUDA code tailored to the specific GPU, taking advantage of the GPU\narchitecture's capabilities, such as memory hierarchy, parallelism, and instruction-level optimizations."}, {"title": "An Algorithmic Perspective of Operation Fusion", "content": "The cornerstone of Liger-Kernel's design is operation fusion. The main goal of the custom operation fusion\nis to mitigate the bottleneck arises between the high-bandwidth memory (HBM) and the shared memory\n(SRAM) for frequent memory copy. Each streaming multiprocessor (SM) needs fast access to data to execute\nmultiple threads in parallel, but HBM, while large, is significantly slower than SRAM. This mismatch can\nlead to delays, where the processing cores sit idle, waiting for data to transfer from HBM to the faster, more\nlimited SRAM. This becomes more severe in the context of deep learning models, especially those with large\nmatrices (like in transformers) and numerous operations. Operation fusion combines several standalone\nGPU operations into a single one to avoid the per-op time and memory overhead in step-by-step execution\nmentioned at the beginning of Section 2. From an algorithmic perspective, operation fusion techniques like\nFlashAttention (Dao et al., 2022; Dao, 2023) offer the advantage of optimizing specific computational patterns\ninherent to the algorithm itself, enabling more precise and tailored performance improvements compared to\nthe broader, more generalized optimizations performed by model compilers. FlashAttention, for instance,\noptimizes the attention computation in transformer models by leveraging GPU memory hierarchies, reducing\nmemory complexity from quadratic to linear. It splits the attention computation into smaller blocks that\nfit into the GPU on-chip SRAM, avoiding the need to materialize the full attention matrix and redundant\nmemory accesses to the slower GPU high-bandwidth memory (HBM). FlashAttention-2 further improves this\napproach by reducing register spilling and enhancing parallelism across attention heads. These innovations\ncollectively result in significant speedups and memory savings for attention computations, particularly for\nlong sequence lengths."}, {"title": "Custom Operation Fusion with Triton", "content": "OpenAI's Triton is a programming language and compiler for high-performance GPU kernels with Python-\nlike syntax (simpler than CUDA), making it easier to optimize deep learning operations without the com-\nplexity of low-level GPU programming. The JIT-compile nature of it also allows libraries and tools that use\nit to be more lightweight and portable. These features have increased the popularity of Triton for writing\nhigh-performance kernels for PyTorch on GPUs. xFormers (Lefaudeux et al., 2022) from Meta hosts inter-\noperable and optimized Transformer building blocks implemented in Triton and CUDA and supports various"}, {"title": "Liger Kernel", "content": "Ease of use is crucial for community adoption, and Liger kernels are designed to be accessible and straight-\nforward. The guiding principle behind Liger's API design is to be the least disruptive to users' existing\ncodebases while providing the flexibility needed for various levels of customization. Depending on the level\nof customization required, there are several ways to apply Liger kernels:"}, {"title": "API Design", "content": "Using AutoLigerKernelForCausalLM: The simplest way to leverage Liger kernels is through the\nAutoLigerKernelForCausalLM class. This approach requires no model-specific patching API imports.\nIf the model type is supported, the modeling code will be automatically patched by Liger.\nApplying Model-Specific Patching APIs: For fine-grained control over the model code, users can\nleverage Liger-Kernel's model-specific patching APIs. These APIs are versatile and can be used with\nvarious model architectures beyond causal language models, such as sequence classification.\nComposing Custom Models: Advanced users can leverage individual Liger kernels (as required)\nto create their own custom models. For instance, the torch-like code below illustrates the creation of\na LigerTransformer module, which leverages LigerLayerNorm to implement the layer normalization\nfunctionality and LigerCrossEntropyLoss to create the loss function."}, {"title": "Kernels", "content": "Throughout the discussion, vectors and matrices are represented by bolded lowercase and uppercase letters,\ne.g., x \u2208 Rn and W \u2208 Rmxn. The all-ones vector is denoted as 1n \u2208 Rn. Functions are applied to the\nvariable element-wise, i.e., f(x) = f(xi). We use to denote the element-wise product between tensors,\nand to denote the matrix transpose.\nIn our kernel implementations, both input and output tensors are reshaped into two-dimensional matrices\nwith the shape (B\u00d7T, H), where B is the batch size, T is the sequence length and H is the hidden dimension.\nIn each kernel, Triton parallelizes operations on each row of input. Therefore, we focus on the mathe-\nmatical operations given a row of input denoted as x and the corresponding output denoted as y. In the\nbackward pass, given a loss function L, we use VyL to denote the gradient back-propagated from L to y.\nRMSNorm. We fuse the normalization and scaling steps of the RMSNorm computation into a single\nTriton kernel. Specifically, given the input x \u2208 Rn and the learnable parameters \u03b3\u2208 Rn, the output\ny \u2208 R is defined as (Zhang and Sennrich, 2019):\n\ny = \\frac{x}{RMS(x)} \\odot \\gamma,\n\nwhere x \u2208 Rn is the normalized input, RMS(x) = \\sqrt{\\sum_ix_i^2/n + \\epsilon} and \\epsilon is a small constant for numerical\nstability. In the backward pass, we have the gradient back-propagated to x and y as\n\n\\nabla_x L = \\frac{1}{RMS(x)} \\left( \\nabla_y L \\odot \\gamma - \\left[\\frac{x \\nabla_x \\left(y \\nabla_y L \\odot \\gamma \\right)}{n} \\right] \\right),\n\n\\nabla_{\\gamma} L = \\nabla_y L \\odot x.\n\nSince the same \\gamma is applied to all input vectors x in the same batch, the gradients need to be summed up.\nLayer Norm. Similar to the RMSNorm, given the input x \u2208 R, the learnable parameters \\gamma \u2208 R\" and\n\u03b2\u2208 R, the output y \u2208 Rn is defined as (Ba et al., 2016):\n\ny = \\frac{x-\\overline{x}}{RMS(x - \\overline{x})} \\odot \\gamma + \\beta, \\qquad \\overline{x} = \\frac{\\sum_i x_i}{n} 1_n.\n\nwhere x \u2208 Rn is the centered and normalized input, with \\overline{x} = (\\Sigma_ix_i/n) 1n. In the backward pass, we have\nthe gradient back-propagated to x, y and Bas\n\n\\nabla_x L = \\frac{1}{RMS(x-\\overline{x})} \\left( \\nabla_y L \\odot \\gamma - \\frac{1}{n} \\left[ \\nabla_y L \\odot \\gamma \\right] - \\overline{x} \\frac{1}{n} (\\nabla_y L^T 1_n)  \\right),\n\\nabla_{\\gamma} L = \\nabla_y L \\odot \\frac{x-\\overline{x}}{},\n\\nabla_{\\beta} L = \\nabla_y L."}, {"title": "ROPE", "content": "Since the same y and \u03b2 are applied to all input vectors \u00e6 in a batch, the gradients need to be summed ups.\nROPE. We fuse the query and key rotation embedding computation into a single kernel to reduce overheads.\nFor each rotary position embedding computation, given the input x \u2208 Rd, the token position m and the\nrotation matrix Rom \u2208 Rdxd, the output y \u2208 Rd is\n\ny = R_m x.\n\nOur implementation of RoPE assumes a rotation matrix in the form of HuggingFace model instead of the\nrotation matrix described in Su et al. (2023). Namely,\nwhere the parameters is model specific.\nIn the backward pass, we have\n\n\\nabla_x L = (R_m)^T \\nabla_y L.\n\nIn the implementation, due to the sparsity of Rom, we adopt the efficient computation in Su et al. (2023)."}, {"title": "SwiGLU", "content": "SwiGLU. We fuse the element-wise operations in the SwiGLU computation into a single kernel. Given\nthe input x \u2208 Rn and learnable parameters W\u2208 Rmxn, V\u2208 Rm\u00d7n, b\u2208 Rm and c\u2208 Rm, the output y \u2208 Rm\nis defined as (Shazeer, 2020):\n\ny = Swish_{\\beta=1}(Wx + b) \\odot (Vx + c)\n= SiLU(Wx + b) \\odot (Vx + c),\n\nwhere SiLU(z) = zo(z) and o(z) = (1 + exp(-z))-\u00b9 is the sigmoid function. We only consider the \u03b2 = 1\ncase here where Swish degenerates to SiLU, which aligns with the implementation of existing supported\nHuggingFace LLMs. Denote the values x1 = Wx + b \u2208 Rm and x2 = Vx + c \u2208 Rm, we implement the\nkernel to compute the forward pass as\n\ny(x_1, x_2) = SiLU(x_1)x_2.\n\nRecall \u2207yL as the gradient back-propagated from L to y. In the backward pass, we have\n\n\\nabla_{x_1} L = \\nabla_y L \\odot [\\sigma(x_1) + SiLU(x_1) (1 - \\sigma(x_1))] \\odot x_2,\n\\nabla_{x_2} L = \\nabla_y L \\odot SiLU(x_1)."}, {"title": "GeGLU", "content": "GeGLU. Similar to SwiGLU, we fuse the element-wise operations. Given the input x \u2208 Rn and learnable\nparameters W\u2208Rm\u00d7n, V\u2208Rm\u00d7n,b\u2208Rm and c\u2208 Rm, the output y \u2208 Rm is defined as (Shazeer, 2020):\n\ny = GELU(Wx + b) \\odot (Vx + c),\n\nwhere we use the tanh approximation of GELU (Hendrycks and Gimpel, 2016). Formally,\n\nGELU(z) \u2248 0.5z \\left( 1 + tanh \\left[\\sqrt{2/\u03c0} (z + 0.044715z^3)\\right] \\right).\n\nSimilar to SwiGLU, denote the values x1 = Wx + b \u2208 Rm and x2 = Vx + c \u2208 Rm. The forward pass can\nbe computed as:\n\ny(x_1, x_2) = GELU(x_1) x_2.\n\nIn the backward pass, we have:\n\n\\nabla_{x_1} L = \\nabla_y L \\odot \\nabla_{x_1} GELU(x_1) \\odot x_2,\n\\nabla_{x_2} L = \\nabla_y L \\odot GELU(x_1),\n\nwhere\n\n\\nabla_{x_1} GELU(x_1) \u2248 0.5 \\left( 1 + tanh \\left[\\sqrt{2/\u03c0} (x_1 + 0.044715x_1^3)\\right] \\right) + \\sqrt{1/(2\u03c0)}x_1 \\left( 1 - tanh^2 \\left[\\sqrt{2/\u03c0} (x_1 + 0.044715x_1^3)\\right] \\right) \\odot (1 + 0.134145x_1^2)."}, {"title": "CrossEntropy (CE)", "content": "CrossEntropy (CE). We move the gradient computation to the forward function along with an inplace\nreplacement of the logit tensor to avoid them being materialized simultaneously. We also adopt online\nsoftmax computation to compute the gradient on the fly. Given the input logits x \u2208 RV, where V is the\nvocabulary size, and target one-hot encoded label t, the output probabilities are given as:\n\ny = softmax(x),\n\nand the cross-entopy loss is defined as L = - \u03a3iti log(yi). The gradient back-propagated to \u00e6 is given by:\n\n\u2207 xL = y - t.\n\nAdditionally, we also employ the safe log operation to avoid numerical instabilities.\nFusedLinear CrossEntropy (FLCE). The rapid expansion of vocabulary in recent LLMs aims to en-\nhance token granularity and achieve more compact prompt representations. However, this progress has\nrevealed a significant challenge: the materialization of logit tensors during CE loss computation consumes\nexcessive memory. This issue has become a major bottleneck in LLM training, limiting our ability to increase\nbatch sizes and extend prompt contexts. Take the Gemma model as an example, single GPU training with a\nbatch size of 8 and sequence length of 4096, the 256k vocabulary size will result in a 16.8 GB logit tensor of\nprecision bfloat16, causing a huge spike in the peak memory usage. Although the CE loss kernel considers\nan in-place replacement of gradient and logits, preventing the double materialization of two large tensors,\nsingle logit tensor size is still prohibitive in many cases which motivates us to explore the chunked logit\nand gradient computation to amortize the memory consumption. The main idea of FLCE is shown in\nFigure 1. The 3D hidden states (shifted already to align with their next ground truth tokens) are flattened\ninto a 2D matrix by collapsing the batch size and sequence length dimensions into a single dimension. The\nlinear projection head is applied sequentially on the chunked hidden states. The generated output logits are"}, {"title": "Testing Best Practices", "content": "passed to the non-fused Liger CE kernel to compute the partial loss and return the chunked logits gradient\nfor deriving the chunked hidden states gradients and the accumulated projection head gradients.\n\nx = Wh,\n\\nabla_h L = W \\nabla_x L,\n\\nabla_w L = h(x L),\n\nwhere W\u2208 RHXV denotes the linear projection head weight given vocabulary size V. h\u2208 RH indicates\na single row of the flattened hidden state matrix H\u2208 RBT\u00d7H. A single row can be viewed as the special\ncase with a chunk size equal to 1. x represents the logits projected from h, for which, we have derived its\ngradient based on (16). Since the same weight W is used for projecting all chunks, its final gradient needs to\nbe summed up as \u2207w\u00a3 = \u2211nh(\u2207L)T. Oftentimes, we can benefit from the compute-intensive behavior\nof the last layer projection, the overhead of block-wise matrix multiplications can be effectively compressed\nwith delicate chunking on the tensor size to keep high GPU utilization with saturated operation time. In\npractice, we set the chunk size to be 2[log2 [TV/HT1 with an intuition on picking the chunk size to be closer\nto the hidden dimension size to balance the trade-off between memory allocation and processing speed.\nRemark. We additionally scale the gradients of the chunked inputs and the projection layer weights\nwith the ratio of chunk size Formally, when a mean reduction is employed during the CrossEntropy loss\ncalculation, the gradients are calculated for a particular input chunk and are not normalized over the entire\ninput sequence. This additional scaling factor addresses such approximation issues.\nTesting is the cornerstone of our kernel development process. Exactness is non-negotiable, as even minor\ndeviations can have far-reaching consequences. Through rigorous research and practical experience, we have\ndistilled our approach into a set of best practices that ensure our kernels meet the highest standards of\nprecision and reliability."}, {"title": "Correctness", "content": "Ensuring kernel precision is crucial, as any deviation from the original implementation could impact model\nconvergence or cause critical errors. To achieve this, we prepare a pure PyTorch implementation (e.g., one\nprovided by HuggingFace) for comparison and test the implementation with various input shapes and data\ntypes. We include regular shapes (e.g., powers of 2) and test irregular shapes to ensure proper handling of\nedge cases. We set appropriate absolute and relative tolerance levels: for fp32, use atol = 10-7 and rtol =\n10-5; for bf16, use atol = 10-3 and rtol = 10-2 .\nFurthermore, large tensor dimensions can lead to inadvertent memory access issues. By default, the\nprogram_id in the kernels are stored as int32. If program_id * Y_stride > 2,147,483,647, the value\nbecomes negative, resulting in illegal memory access. Such overflows and incorrect memory addressing\nerrors can be avoided by explicitly converting it to int64 when dealing with large dimensions."}, {"title": "Performance", "content": "We ensure that the re-implementation of kernels in Triton is justified (compared to the baseline version) by\ntesting across two key dimensions: speed and memory usage.\nFor input shapes in testing, we use actual dimensions/hyper-parameters from the training process, such\nas a batch size of 4, a hidden dimension of 2048, and a variable sequence length. This approach ensures that\nthe test results reflect expected gains in production training across a family of models."}, {"title": "Convergence Test", "content": "In practical training settings, the contiguity, shape, and dtype of tensors might differ from the unit test\nconditions. To prove the validity of our computational gains, we mimic such real-world scenarios at a\nsmaller scale and verify the exactness of logits, weights, and loss at the end of the training."}, {"title": "Contiguity", "content": "Since Triton operates directly on physical memory, non-contiguous tensors (where elements are not arranged\nsequentially) can lead to illegal memory access or incorrect outputs. For example, when deploying our\nROPE kernel for production training, we observed significant loss divergence because the derivative from\nthe scaled_dot_product_attention function was not stored contiguously. To prevent such issues, it's best\npractice to ensure tensors are contiguous before passing them to the kernel."}, {"title": "Integrations", "content": "Liger has been successfully integrated with several popular training frameworks within the machine learning\ncommunity, including Hugging Face transformers' Trainer class, Hugging Face TRL's SFTTrainer class,\nAxolotl, and LLAMA-Factory . These integrations demonstrate the flexibility and ease of use of the Liger\nAPI, enabling developers to leverage its optimization capabilities with minimal code changes. A simple flag\nis typically all that is needed to patch the model code with Liger kernels. For example:"}, {"title": "Numerical Experiments", "content": "This section presents the kernel level and end-end LLM training benchmarks using Liger-Kernel v0.2.1.\nKernel Benchmark\nWe benchmark the kernels individually across a variety of settings and illustrate the improvements in speed\nand memory consumption with Liger.\nSetup. All benchmarks are run on a single NVIDIA A100 GPU (80 GB). The CrossEntropy kernel is\nbenchmarked on vocab sizes in the set {40960, 81920, 122880,163840}. The GeGLU and SwiGLU kernels\nare benchmarked on varying sequence lengths, whereas the RMSNorm, LayerNorm, and RoPE kernels are\nbenchmarked on varying hidden dimensions. The sequence lengths and hidden dimension sizes are chosen\nfrom {4096, 8192, 12288, 16384}. All benchmarks are repeated 10 times to plot the median speed and memory\nalong with [0.2, 0.8] quantile values as the lower and upper bounds.\nResults. The kernel speed and memory benchmarks are illustrated in Figure 2, 3 respectively. Observe\nthat all the Liger-kernel implementations either execute faster, consume less memory or provide both of\nthese benefits when compared to the baseline implementations. In the case of the CrossEntropy kernel, the\nonline softmax computation along with in-place replacement of the kernel inputs with their gradients leads\nto approximately 3\u00d7 faster execution and consumes approximately 5\u00d7 less memory for a vocab size of 163840. For GeGLU and SwiGLU, we maintain parity with the baseline in terms of speed\nand reduce the peak memory consumption by roughly 1.6\u00d7 (when sequence length is 16384)\nby recomputing the SiLU(\u00b7) and GELU(\u00b7) outputs during the backward pass. . The RMSNorm implementation fuses the normalization and scaling operations into a single triton kernel\nand caches the root mean square values for usage in the backward pass. This avoids repetitive data transfers\nand floating point operations with minimal memory overheads. illustrates approximately 7\u00d7"}, {"title": "Usecase Benchmark", "content": "Setup. For the end-end training experiments, we employ 4 NVIDIA A100 GPUs (80 GB each) to fine-tune\nthe LLMs (LLaMA 3-8B, Qwen2, Gemma, Mistral, and Phi3) on the Alpaca dataset. We vary the batch\nsize, set the precision to bfloat16, and use the AdamW optimizer with a cosine learning rate scheduler.\nThe sequence length for training is set to 512 tokens. The throughput and GPU memory usage metrics are\ncollected after 20 training steps with the standard error measured from 5 repetitive runs. The benchmark\nscript can be found in our GitHub repository.\nPerformance Comparison. At a batch size of 64, LLaMA 3-8B demonstrates a 42.8% increase in\nthroughput, coupled with a 54.8% reduction in GPU memory usage. This enables training\non smaller GPUs or using larger batch sizes and longer sequence lengths with lower resource consumption.\nSimilarly, at a batch size of 48 our kernels improve the throughput of Qwen2 by 25.5%, while achieving a\n56.8% reduction in GPU memory usage . For Gemma, throughput improves by 11.9% with\na 51.8% reduction in memory usage at a batch size of 48. Mistral, at a batch size of 128,\nexhibits a 27% increase in throughput, with a 21% drop in GPU memory usage . Finally,\nPhi3, at a batch size of 128, shows a 17% increase in throughput, while reducing memory usage by 13%\n. Overall, the results highlight several notable use cases. LLaMA 3-8B's exceptional improvements\nmake it ideal for resource-constrained environments where GPU memory is a bottleneck. Additionally,\nQwen2's strong memory reductions position it well for tasks involving large datasets or extended training\ndurations. Mistral's high throughput gains make it advantageous for workloads requiring large batch sizes."}, {"title": "Medusa", "content": "Medusa. Medusa (Cai et al., 2024) is a simple framework that democratizes acceleration techniques for\nLLM generation by using multiple decoding heads to predict several subsequent tokens in parallel. During\ntraining, Medusa requires adding k decoding heads to the hidden states right before the regular LM head\nht. The k-th head is used to predict the token in the (t + k + 1)-th position of the next tokens (the original\nlanguage model head is used to predict the (t + 1)-th position).\nThe Liger LFCE kernel is particularly effective in this context, as it eliminates the need to materialize\nlogits for each decoding head. This is critical in scenarios with large vocabulary sizes, such as LLaMA-3's\n128k tokens, where materializing logits can lead to significant memory consumption. The introduction of\nmultiple decoding heads often results in out of memory issues. However, by leveraging the Liger fused CE\nkernel, which computes gradients in place without materializing logits, we achieve highly efficient results.\nThis approach enables further exploration and development in multi-token prediction.\nMedusa training has two flavors. The first, called stage-1, involves training only the additional Medusa\nheads while keeping the backbone LLM frozen. The second approach tunes both the backbone and the LLM\nheads simultaneously. We have benchmarked both cases, and the Liger kernel has demonstrated reduced\nmemory usage and improved throughput. Without the Liger kernel, experiments are highly prone to out of\nmemory issues. In Figures 9-12, the standard errors measured from repetitive runs are typically less than\n1% hence not visible from most of the plots."}, {"title": "Conclusions", "content": "Liger Kernel offers optimized Triton kernels that improve training efficiency with a user-friendly API, seamless\nintegration with popular frameworks, and a commitment to performance. Our goal is to make Liger Kernel\nthe leading open-source Triton kernel library for LLM training. We aim to achieve this by focusing on:\n\u2022 Ease of Use: Offering intuitive APIs, broad model support, and wide hardware compatibility\n\u2022 Performance Focus: Maximizing computational efficiency and ensuring exactness.\n\u2022 Ecosystem Engagement: Building a strong community through events and collaborations with\nindustry leaders, alongside fostering recognition and branding for contributors.\n\u2022 Operational Excellence: Ensuring stable CI, rigorous testing protocols, and an active community.\nWith these commitments, Liger-Kernel aspires to become the preferred choice for efficient and scalable\nLLM training, driving innovation and adoption within the deep learning community. While existing work\nprimarily focuses on training, the same techniques can be seamlessly adapted for optimizing model inference."}, {"title": "Contributors and Acknowledgements", "content": "Core Contributors\nByron (Pin-Lun) Hsu Project lead. Led, architected, and implemented multiple kernels, public interface,\nand test suite.\nYun Dai Core contributor. Designed an efficient version of ROPE, GeGLU, and improved the precision of\nFused Linear CrossEntropy. Designed the public interface.\nVignesh Kothapalli Core contributor. Implemented Fused Linear CrossEntropy and designed the scaling\nand sharding formula.\nQingquan Song Core contributor. Implemented SwiGLU. Led the convergence tests and PyTorch lightning\nintegration. Ensure the contiguity of RoPE and kernel testing precisions."}]}