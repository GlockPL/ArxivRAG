{"title": "Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective Retrieval-Augmented LLMs", "authors": ["Reham Omar", "Omij Mangukiya", "Essam Mansour"], "abstract": "Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces CHATTY-GEN, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. CHATTY-GEN decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMS. CHATTY-GEN eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that CHATTY-GEN significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-40, Gemini 1.5, Llama 3, and Mistral.", "sections": [{"title": "1 Introduction", "content": "Conversational Al systems, like chatbots (e.g., ChatGPT [27] and Gemini [14]) and virtual assistants (e.g., Alexa and Siri), have become ubiquitous in offering general information and facilitate task completion [14, 28]. However, evaluating their performance within a specific domain remains a challenging task. Dialogue benchmarks provide a systematic way to assess these systems and improve the capabilities of general virtual assistants [50] in applications, such as educational chatbots [22], onboarding new users to a specific domain [18], and online tutoring [9].\nUnlike unstructured datasets such as documents, knowledge graphs (KGs) represent semantically rich and organized data across various application domains. Examples include KGs about scientific publications, such as the Microsoft Academic Graph\u00b9 and DBLP2, and general-fact KGs like DBpedia\u00b3, YAGO\u2074, and Wikidata. A KG is a heterogeneous graph that includes nodes representing entities of various types, such as Person, Creative Work, or Place, and edges representing relationships, such as birthPlace, award, or child, between these entities. Both entities (vertices) and relationships (predicates) are represented using Uniform Resource Identifiers (URIs). Each entity can serve as a seed for a dialogue, with questions related to at least one predicate, as shown in Figure 1. For instance, Where was she born? is a question related to the entity Celine Dion and the predicate birthPlace. Hence, semantically rich KGs present a valuable resource for automating the generation of dialogue benchmarks for both general and domain-specific knowledge.\nAutomating dialogue benchmark generation from a KG presents unique challenges. Figure 1 outlines the steps involved in generating a dialogue benchmark from the DBpedia KG as follows: (1) Sample Entities, which involves sampling representative entities from the KG. It is impractical to include all KG entities in the dialogue benchmark generation, as some KGs contain millions or billions of entities. Selecting a representative sample of key entities is crucial to achieving high-quality benchmarks in a reasonable time. Our example shows the entity Celine Dion of type Person. (2) Extract Context, in which a subgraph surrounding the chosen entity is extracted to represent the dialogue context and ensure the dialogue remains focused and relevant. (3) Extract Textual Entity Representation, to generate a question about an entity, we need a human-readable representation. Using URIs (e.g. http://dbpedia.org/resource/Celine_Dion) within natural language questions is impractical, necessitating a more user-friendly representation (e.g. Celine Dion). Some URIS are encoded with no human-readable text. Hence, this is a challenging task. (4) Maintain Dialogue Flow, which maintains the coherence of the conversation. The generated questions should build upon each other within the dialogue. For example, the primary entity is explicitly introduced once, with subsequent questions referring to the entity indirectly, as shown in Figure 1. (5) Answer Generation: Generating complete benchmarks requires answers for each question. This process involves utilizing structured queries, such as SPARQL. This necessitates formulating SPARQL queries corresponding to each question in the dialogue. (6) KG Agnostic Design, where the dialogue generation platform should be able to operate on any KG without requiring KG-specific modifications. This makes the platform useful across different domains and types of KGs.\nLarge language models (LLMs), such as GPT [28], Gemini [14], and Llama [43], hold significant potential for automating KG-based dialogue benchmark generation. However, traditional methods for generating dialogue benchmarks rely on manually created benchmarks from documents, such as Dream [39], MultiWOZ [17], CoQA [35], and QuAC [6]. These manual approaches are labor-intensive, expensive, and not scalable. Additionally, template-based systems exist for KG-based dialogue benchmarks, like CSQA [37], and for question-answer benchmarks, i.e., standalone and self-contained questions, such as LCQUAD [11, 45] and Head-to-Tail [38]. These template-based systems face similar issues and require template redesign for each new KG. Maestro [30] is a rule-based system automating the generation of question-answer benchmarks but does not support dialogue generation. Maestro faces challenges, including the need for extensive pre-processing, user input, and code adjustments for each new KG. Overall, these existing systems struggle to adapt to arbitrary KGs. None of the existing systems leverages LLMs for KG-based dialogue benchmark generation, which presents additional challenges. One critical challenge is mitigating hallucinations[19]-ensuring the generated dialogues strictly adhere to factual information within the KG. Another challenge lies in developing a versatile platform that can utilize various LLMs to perform the dialogue benchmark generation.\nThis paper introduces CHATTY-GEN, a novel multi-stage Retrieval-Augmented Generation (RAG) platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. CHATTY-GEN employs a multi-stage pipeline with zero-shot learning to simplify dialogue benchmark generation into manageable sub-tasks, each addressed by specifically crafted prompts without curated examples. CHATTY-GEN utilizes assertion rules defined based on the expected output of each stage and leverages KG information to validate intermediate results automatically. Hence, CHATTY-GEN mitigates hallucinations before proceeding to the next stage. Our multi-stage approach enables CHATTY-GEN to accommodate and work efficiently with LLMs of diverse capabilities. CHATTY-GEN utilizes an efficient query-based retrieval augmented technique to identify subgraphs with a large variety of predicates. Then, it summarizes these graphs to provide the prompt with rich yet condensed information as dialogue context. CHATTY-GEN also predicts textual representations of the selected entities to generate human-like questions suitable for dialogue. Additionally, CHATTY-GEN automatically generates SPARQL queries for each question without prior knowledge of the KG.\nWe evaluate CHATTY-GEN's performance on four diverse real-world KGs: DBpedia, Yago, and DBLP. Additionally, we assess CHATTY-GEN's compatibility with various LLMs, including commercial models like GPT-40[29] and Gemini-pro-1.5[42], as well as open-source models like Llama-3[1] and CodeLlama [36]. Our multi-stage approach allows CHATTY-GEN to integrate with a single or multiple LLMs. We demonstrate that CHATTY-GEN using multiple LLMs, such as Llama-3 and CodeLlama, achieves the same success rate, quality, and comparable processing time as CHATTY-GEN with GPT-40. Furthermore, CHATTY-GEN exhibits significant time efficiency in end-to-end benchmark generation compared to Maestro, the current state-of-the-art system for standalone and self-contained questions. For large KGs like DBpedia, CHATTY-GEN shows a remarkable time improvement of 99%, reducing the process from 30 hours by Maestro to just 10 minutes by ours. Overall, CHATTY-GEN offers a cost-effective and versatile solution for generating high-quality KG-based dialogue benchmarks within a reasonable timeframe.\nIn summary, our contributions are:\n\u2022 CHATTY-GEN, the first fully automated RAG-based platform for generating a dialogue benchmark from a KG (Section 3).\n\u2022 An effective and diverse context retrieval-augmented method for selecting representative seed entities with rich subgraphs from an arbitrary KG with minimal overhead (Section 4).\n\u2022 A multi-stage RAG-based approach with automatic validation that mitigates hallucinations and achieves accurate dialogue benchmark generation while reducing the time and token consumption across different LLMs (Section 5).\n\u2022 A comprehensive evaluation using four real KGs from different domains and various LLMs shows that CHATTY-GEN significantly outperforms state-of-the-art systems in both quality and time performance. CHATTY-GEN also works seamlessly with arbitrary KGs and ensures consistent performance across commercial and open-source LLMs (Section 6)."}, {"title": "2 Background: Limitations and Challenges", "content": "This section reviews the capabilities of existing systems in generating dialogue benchmarks from KGs and highlights the challenges retrieval-augmented LLMs face in handling complex and knowledge-intensive tasks like dialogue benchmark generation."}, {"title": "2.1 KG-based Dialogue Benchmarks", "content": "A KG-based dialogue benchmark consists of dialogues, each containing questions along with their corresponding SPARQL queries to retrieve the answers from the KG. We define it as follows:\nDEFINITION 1. D = {e, KG, Q, SQ} is a dialogue where:\n\u2022 Seed Entity e: The entity that the dialogue revolves around.\n\u2022 Questions Q: An ordered list of questions asked during the dialogue, denoted as Q = [Q\u2081, Q\u2082,..., Q\u2099]. The first question Q\u2081 must be a standalone and self-contained question, while subsequent questions (Q\u1d62 for i > 1) depend on previous questions.\n\u2022 SPARQL Queries (SQ): A list of SPARQL queries, one for each question in Q, used to retrieve answers from KG."}, {"title": "2.2 Retrieval-Augmented LLMs and Prompting", "content": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique to address the primary challenges of LLMs in performing knowledge-intensive tasks that demand factual grounding and efficient knowledge acquisition [4, 13, 48], such as generating dialogue benchmarks from a KG. Retrieval-augmented LLMs utilize RAG mechanisms to enhance their performance in such tasks. RAG retrieves relevant information and feeds this enriched context to LLMs. For instance, generating dialogue benchmarks from a KG involves integrating varied and precise information from the KG. Consider a chatbot consulting a KG of scientific publications (e.g., DBLP) to create dialogue benchmarks for academic research or a creative tool using a KG of artists and singers (e.g., DBpedia or YAGO) to develop detailed and informative dialogues about their lives and works. RAG offers a more flexible and data-efficient alternative over traditional fine-tuning methods [3]. Fine-tuning requires significant computational resources, especially for massive models like GPT-4 [28] and LLAMA [43]. Hence, it is a more common technique for relatively smaller models, such as BERT [10] and T5 [34].\nRAG helps mitigate LLM hallucinations [19], which occur when LLMs generate factually incorrect answers or outputs deviating from the given prompt [19]. Hallucinations stem from the unstructured nature of training data [12] and the rare occurrences of certain information in the training data [19]. Examples include: (I) ambiguous dialogues, where the LLM starts with a context-lacking question, e.g., \"What is his nationality?\"; (II) non-existing facts, where the LLM generates questions that do not map to any facts (triples) in the KG; (III) incorrect SPARQL, where the LLM generates SPARQL queries that do not correspond to the questions.\nOvercoming hallucinations is crucial for ensuring the reliability and usability of dialogue benchmarks generated from a KG.\nRAG leverages in-context learning techniques like prompting [3], where clear instructions guide the LLM toward a specific task. Additionally, RAG can adapt to new scenarios through few-shot learning (using a few examples) or zero-shot learning (relying solely on prompts). The later establishes a highly customizable and versatile pipeline across unseen classes or situations, e.g., a new KG. Several techniques aim to enhance prompt engineering [5]: (I) Chain-of-Thought (CoT) [47] improves LLM reasoning by embedding intermediate steps within the prompt, guiding the model through problem-solving. While effective for tasks with clear steps, CoT does not seamlessly translate to all LLM architectures and often needs substantial adjustments to work effectively across different models. (II) Zero-shot CoT [21] offers a variant of CoT that employs a dual-prompt strategy: the first prompts the LLM to extract reasoning, while the second provides both the question and the extracted reasoning for solving the task. (III) Least-to-most prompting [53] addresses tasks requiring generalization beyond the examples provided. It involves breaking down the task into simpler subtasks, which the LLM then solves sequentially until achieving the desired output. While effective for tasks like math reasoning [53], it can be more costly and prone to error propagation in tasks like text-to-SQL [40], which is similar to part of our task. (IV) Automatic Prompting Engineering (APE) [54] automates prompt generation and selection to reduce human effort and testing. Using input-output samples, the LLM generates instructions evaluated on subset of data, iterating until convergence based on scores.\nGenerating dialogue benchmarks from KGs involves implicit reasoning about KG relationships, which traditional prompts struggle to capture as they focus on specific reasoning styles. Existing techniques are also not optimized for complex tasks that generate multiple outputs, such as dialogues paired with SPARQL queries in JSON format. Customizing prompts for such tasks can overwhelm some LLMs, limiting their efficiency. Therefore, these techniques often lack broad applicability across diverse LLM architectures. Achieving consistent performance across LLMs of varying capabilities remains a significant challenge in developing dialogue benchmark generation systems. RAG also faces additional challenges, such as selecting representative seed entities from the KG and extracting rich yet concise subgraphs. Minimizing the latency of subgraph extraction is crucial. Furthermore, RAG must effectively synthesize information to generate dialogues with diverse questions while avoiding hallucinations. Another significant challenge is developing an automatic validation mechanism that uses the retrieved information to identify and mitigate hallucinations. These challenges open new research directions for developing a cost-effective and versatile dialogue benchmark system using retrieval-augmented LLMs."}, {"title": "3 The CHATTY-GEN Platform Overview", "content": "CHATTY-GEN automates the generation of domain-specific dialogue benchmarks for chatbot evaluation, which traditionally relies on human input for questions and templates. By leveraging KGs, it incorporates diverse key concepts and topics (node types and facts) to ensure comprehensive testing across various entity types. To overcome the limitations of traditional prompting methods, CHATTY-GEN employs a cost-effective multi-stage pipeline that reduces both the financial cost of using LLMs and the processing time for large KGS. CHATTY-GEN includes two main phases: A) dialogue context extraction in the form of diverse subgraphs (SG) and B) dialogue generation for each g\u2208 SG. The second phase is further broken down into four stages, where each stage's output is automatically validated before being used as input for the next stage. This ensures the correctness and consistency of the generated outputs. Validation is based on assertion rules defined by benchmark criteria and leverages KG information to mitigate potential LLM hallucinations.\nThe dialogue context extraction phase utilizes our efficient and diverse subgraph retrieval-augmented method. This method analyzes the KG node type distribution and selects a representative sample of seed nodes for each critical node type. KGs can contain millions or billions of nodes representing entities of ten to thousands of types. Moreover, our query-based retrieval efficiently analyzes these types by leveraging built-in RDF engine indices. Hence, our method reduces the preprocessing time and enhances overall efficiency. CHATTY-GEN leverages LLMs to identify a textual label associated with a certain node type that can be used as an entity label. The textual label enables the LLM to generate better dialogues. CHATTY-GEN selects nodes and their textual representations from the KG at runtime via SPARQL queries. These extracted entities are used to retrieve surrounding subgraphs, representing the context for question generation within the dialogue. LLMs have token limits, so CHATTY-GEN utilizes constraints to select seed nodes with textual representations and subgraphs of specific lengths and sizes. This method ensures the extraction of subgraphs with diverse predicates, generating meaningful dialogues.\nCHATTY-GEN utilizes the extracted subgraph and textual representations through our four-stage pipeline to generate the dialogue. First, the subgraph is fed to a summarization module, which retains only information relevant to the task and eliminates potentially confusing details. Second, questions are generated based on the summarized subgraph. Each question is linked to the specific triple in the subgraph from which it was generated. Next, the questions and their corresponding triples are provided to the answer generation module. This module utilizes zero-shot learning and prompts to generate a SPARQL query for each question, retrieving the answer from the KG. Finally, the dialogue generation module transforms the independent questions into a coherent dialogue, ensuring the first question remains independent as the starting point. The final benchmark dataset consists of three components: dialogue, independent questions, and corresponding SPARQL queries. CHATTY-GEN avoids the need for complex, customized prompts used in techniques like CoT. Instead, it focuses on simpler prompts, enhancing its broad applicability across diverse LLM architectures. CHATTY-GEN also regenerates only for specific subtasks, reducing overall costs."}, {"title": "4 Dialogue Context Extraction as Subgraphs", "content": "The dialogue context extraction automatically identifies key concepts and topics (node types) in a KG, which may contain hundreds of types that do not represent the main concepts of a specific domain. This step is essential for creating comprehensive domain-specific dialogue benchmarks. Different entities of these node types vary in the density of surrounding facts (context), resulting in subgraphs of different sizes. Given that large KGs contain millions of entities, CHATTY-GEN aims to identify a representative set of key entities with sufficient surrounding facts to generate diverse dialogues. For example, subgraphs with repeated predicates do not facilitate varied dialogue generation. Thus, CHATTY-GEN optimizes both cost and processing time. The subgraph of a given seed entity is defined as follows:\nDEFINITION 2. Given a KG with a set of vertices V and a set of predicates P, let e be the target seed entity. The subgraph SG(e) = {T} is a set of triples T = (s, p, o) such that s = e or o = e, p \u2208 P, s \u2208 V, and o \u2208 V or a literal.\nA brute-force solution typically begins by extracting all entities from the KG during a pre-processing step. Various approaches can then be used to process these entities, such as identifying head, torso, and tail entities, grouping them by their degree (number of incoming and outgoing predicates), or categorizing them based on the types of connected nodes. After categorization, entities are selected, and their corresponding subgraphs are generated. The complexity of these approaches is proportional to the number of entities in the KG, which can reach millions or even billions. As a result, these methods have significant drawbacks, including long processing times, high computational costs, and large storage requirements. This makes brute-force methods impractical for dialogue context extraction from large KGs.\nTo efficiently extract context for dialogue generation, our approach leverages the node types in a KG through a three-step process: (i) relevant node type selection, (ii) textual entity representation, and (iii) seed node and subgraph extraction. In the first step, we focus on identifying pertinent KG node types, representing different concepts such as people or locations. Subsequent steps utilize this node-type information to extract textual representations of entities and sample a representative set of entities for subgraph extraction and dialogue generation. This streamlined approach operates effectively with large KGs, eliminating the need for costly preprocessing that traverses the entire graph and stores all entities."}, {"title": "4.1 Identifying Representative Node Types", "content": "A KG contains nodes representing real-world entities of different types based on the application domain. For example, in the DBLP academic graph, https://dblp.org/rdf/schema#Publication is a domain-specific type representing scholarly publications. There are also metadata types that describe properties of the data within the KG and are not representative of core entities. Examples include http://www.w3.org/2002/07/owl#DatatypeProperty and RDF engine-specific types like http://www.openlinksw.com/schemas/virtrdf#array-of-string. Due to their nature, metadata types are less helpful in generating dialogues. Within domain-specific types, there exist rare types and shadowed types. Rare types are those represented by a very low percentage of entities within the KG, indicating they are less critical to KG topics and do not represent the main focus of the KG. Examples include http://schema.org/Reservoir in Yago KG, which represents only 0.020% of the KG's entities. Shadowed types are parent types whose entities mainly belong to a single child type. For instance, in DBLP KG, https://dblp.org/rdf/schema#Creator is a parent of https://dblp.org/rdf/schema#Person, and over 99% of Creator entities belong to Person type. Including both parent and child types would introduce redundancy in the final dialogues. To address this, CHATTY-GEN allows users to set thresholds to exclude rare and shadowed types. This enhances efficiency and reduces benchmark generation time.\nThis module identifies the most relevant node types within the KG's domain. Initially, we retrieve all node types from the KG and filter out metadata and other irrelevant types. Our goal is to pinpoint types that support the generation of coherent dialogues about concepts pertinent to the KG's specific domain. This strategy enhances time efficiency compared to brute-force methods because the number of node types in a KG is typically much smaller than the total number of nodes. Subsequently, we establish a distribution of entities to sample from each identified type.\nAlgorithm 1 outlines the procedure for selecting node types and determining the number of entities to sample from each type, where each entity corresponds to a single generated dialogue. The algorithm takes five inputs: (1) KG SPARQL endpoint for accessing the KG, (2) the required number of dialogues to generate (m), (3) domain prefixes for identifying relevant KG types, (4) a rare type threshold (R) to filter out types with low entity counts, and (5) a shadowed parent threshold (S) to identify redundant types. Line 2 retrieves relevant types to the KG domain using the provided SPARQL endpoint and domain prefixes. We employ a count SPARQL query leveraging the predicate rdf:type, which is indexed in RDF engines. This approach allows CHATTY-GEN to efficiently obtain a complete list of KG types along with their corresponding counts. Lines 3 to 6 calculate the percentage of each type in the KG based on these counts. To focus on significant types, line 7 removes rare types with percentages below R. If R = 0, all types are included. Line 8 excludes shadowed types by identifying parent-child relationships using SPARQL queries leveraging subject and predicate indices. Then, it removes parents if any of its children constitute more than S of their total instances. If S = 1, all types are included. Finally, we recalculate percentages to reflect the reduced set of types. Lines 9 to 11 determine the number of samples required from each type. To ensure the sample distribution reflects the overall entity distribution in the KG, we utilize the percentages of each type along with the desired total number of entities, denoted as (m).\nComplexity: Algorithm 1 has a time complexity of O(q cost + t), where t represents the number of node types and q cost denotes the cost of executing the SPARQL query to retrieve types and their entity counts. The primary operations contributing to this complexity include retrieving all node types (q cost) and iterating through them (t) for filtering and calculations. The cost of retrieving node types (q cost) varies based on the RDF engine used, leveraging its implementation and efficiency in handling queries with built-in indices. In a large KG, the number of nodes can reach millions or billions, while the number of distinct node types (t) is typically much smaller, ranging from tens to thousands. Hence, this approach is significantly more efficient compared to a brute-force solution that would necessitate iterating through all nodes in the KG, which would be computationally prohibitive for large-scale KGs."}, {"title": "4.2 Textual Entity representation", "content": "The textual representation of entities significantly influences human comprehension and LLMs' ability to generate questions. KGs typically represent entities using URIs, which are not suitable for natural language processing tasks. Therefore, converting these URIs into human-readable text is crucial for question generation. One straightforward approach is to extract the final segment of the URI as the entity label (e.g., http://dbpedia.org/resource/The_Future_of_Freedom_Conference). However, this method is limited because many URIs contain IDs or alphanumeric combinations in their final segments (e.g., https://dblp.org/pid/00/10071 or https://dblp.org/rec/conf/valuetools/Coppa14), making it difficult for LLMs to derive meaningful questions from them. Due to these limitations, a more effective strategy involves leveraging the entity's context within the KG. This context includes the predicates connected to the entity, which can provide better textual representations.\nIdentifying a single universally applicable predicate across different KGs or even within different node types in a single KG is challenging. For example, Person entities may be connected with a \"name\" predicate, while Publication entities might use a \"title\" predicate. To address this challenge, users with domain expertise can manually define mappings between node types and suitable predicates for retrieving entity labels. While feasible for smaller KGs with fewer node types, this manual approach becomes impractical for large-scale KGs like DBpedia or Yago, which contain hundreds or thousands of node types. Therefore, there is a need for an automatic method to identify the most appropriate predicate, referred to in this paper as the entity label, for each node type within a KG.\nWe propose a novel approach using LLMs to automatically extract the entity label (el) for each node type within a KG. For a given node type (t), our method begins by randomly sampling an entity (e) belonging to that type from the KG. Next, we extract all outgoing predicates (P list) connected to e. We define this subtask with the task instruction (I EL) to be sent to an LLM, as follows:\nP\u2081 = f(t, P list, I EL)\nTo ensure the extracted representation is suitable for use within questions, we filter P list to retain only predicates connecting e to string literals. This filtering focuses exclusively on predicates relevant to the entity's type, irrespective of the literal's length. Instruction I EL utilizes zero-shot prompting to instruct the LLM to select the most representative predicate from the filtered list (P list). The chosen predicate then becomes the entity label for the node type (t). This approach generates human-readable entity representations for all node types, regardless of the information encoded in the URIs. Additionally, it eliminates the need for technical knowledge to manually create the mapping between node types and entity labels. This improves the quality of questions asked and enhances the overall system usability.\nThe length of labels can vary significantly among entities within the same node type. For instance, consider the \"Place\" node type in the Yago KG: one entity might have the label \"Administrative Department of Science, Technology, and Innovation,\" while another might simply be \"Admaston.\" Hence, specific conditions regarding label usage are deferred to the entity-level processing stage."}, {"title": "4.3 Seed Nodes and Subgraph Extraction", "content": "This module efficiently extracts seed entities and their corresponding subgraphs from a KG, which serves as the context for dialogue generation. It leverages the node-type distribution and utilizes entity labels for each type. To optimize LLM processing, the extracted entities and subgraphs must meet specific criteria. CHATTY-GEN opts to select entities with concise labels to optimize performance and prevent LLMs from being overwhelmed by irrelevant details. For example, in the DBLP KG, a label like \"Algorithmic Number Theory, First International Symposium, ANTS-I, Ithaca, NY, USA, May 6-9, 1994, Proceedings\" contains distracting elements such as dates and locations. While using LLMs to shorten labels may seem effective, it can alter the entity's meaning. Removing parts of a paper's title could confuse users or systems searching for a specific paper. We classify entities as valid if their labels fall below a predefined length threshold. Secondly, the subgraph's size must be manageable within the LLM's context length. It also should feature an adequate number of unique predicates connecting entities to diverse facts. This diversity ensures the foundation for generating multiple distinct questions within a dialogue; typically, a minimum of three questions per dialogue is required. Subgraphs meeting these criteria are marked as valid. This approach offers efficiency gains over brute-force methods by avoiding storing all KG entities. Instead, it dynamically retrieves the necessary entities and their subgraphs directly from the KG endpoint without full graph traversal, thereby streamlining the process. Our technique prioritizes efficiency and guarantees a sufficient number of unique predicates to generate human-like and useful dialogues.\nAlgorithm 2 outlines the process for extracting entity-subgraph pairs specific to a node type. This iterative process continues until the desired number of pairs (n) for the given node type t are obtained. This procedure is repeated for each node type in the KG. In line 3, the algorithm randomly retrieves an entity of type t along with its label using the entity label er. It ensures that the extracted entity's label length is suitable for processing by the LLM. The algorithm retrieves entities in batches of size BZ from the SPARQL endpoint. Leveraging the RDF predicate (rdf:type) and object (t) indices, and batching entities helps optimize efficiency and minimize query overhead. Each entity is processed individually, and a new SPARQL query is issued only if the entire batch is exhausted before reaching the required number of pairs (n). For entities with suitable labels, lines 4-5 execute count queries to determine the graph size and the number of unique predicates within a specified hop limit (h) around the entity. In both queries, the RDF subject index is used for faster execution, since the subject is the entity. Moreover, they are count queries so they only return the counts, avoiding the overhead of retrieving and processing actual data, contributing to faster response times. Line 6 verifies that the graph size and the count of unique predicates meet the LLM processing constraints, providing sufficient information to generate diverse questions in a dialogue. Once the LLM's requirements are satisfied, subgraph extraction proceeds. Line 7 extracts the subgraph surrounding the seed entity based on predefined parameters, including the shape of the subgraph, the number of hops to traverse outward from the entity, and the direction of predicates (e.g., outgoing, incoming, or both directions). Line 8 filters out triples from the extracted subgraph based on several criteria.\nTriples containing the entity label itself are excluded, as questions about these would yield the entity as the answer. Additionally, triples with predicates irrelevant to the KG's domain, such as http://www.w3.org/1999/02/22-rdf-syntax-ns#type, are removed as they lack specific information for generating meaningful dialogue questions. Furthermore, the algorithm filters out triples associated with excessively long string literals as objects. These literals, such as those connected via predicates like http://dbpedia.org/ontology/abstract, can unnecessarily increase the subgraph size and potentially confuse the LLM by leading to questions about the content of the objects rather than questions relevant to the entity itself. Similarly, predicates like http://dbpedia.org/ontology/thumbnail, which connect entities to image URLs, are excluded as they do not contribute to human-like dialogues. Line 9 validates the filtered subgraph to ensure it contains sufficient information (number of triples) required for dialogue generation. Valid subgraphs meeting the criteria are added to the final list of entity-subgraph pairs. If the subgraph does not meet the validation criteria, the algorithm proceeds to process the following entity. The algorithm's output is a list of valid entity-subgraph pairs extracted for each node type within the KG.\nComplexity: Algorithm 2 operates with a time complexity of \u039f(\u03c4. (q2 cost + K)), where: (i) \u03c4 represents the number of selected node types as determined by Algorithm 1. (ii) q2 cost denotes the cost associated with extracting the subgraph within the specified hop limit and direction from the RDF database. (iii) K signifies the number of triples within the extracted subgraph. The cost q2 cost varies depending on the RDF engine's implementation and the efficiency of its built-in indices. This approach achieves notable efficiency gains compared to a brute-force solution by minimizing the number of iterations (where \u03c4 \u00ab V, and V is the total number of entities in the KG) and avoiding the need for entity storage. Consequently, the algorithm operates with constant space complexity O(1)."}, {"title": "5 Our Multi-stage Dialogue Generation", "content": "This section discusses generating dialogue benchmarks from subgraphs of seed entities and their labels. We introduce the single-prompt approach, which involves processing each dialogue context through a single complex prompt. Then, we present our multi-stage approach using multiple distinct and manageable prompts 6."}, {"title": "5.1 The Single-prompt Approach", "content": "The single-prompt approach is conceptually straightforward and relatively easy to implement. It requires processing the subgraph only once, which seems computationally efficient. However, the single prompt typically encompasses several subtasks. First, the LLM must analyze the subgraph (SG ser) with the entity label (el) to identify entities, relationships, and relevant information for dialogue generation. Based on this context, the LLM formulates a dialogue (D) of sequence of standalone questions (Q') to explore the subgraph's relationships and properties. The LLM then generates corresponding SPARQL queries (SQ) to extract the answer from the subgraph. Then it weaves the generated questions and retrieved answers into a coherent dialogue reflecting the subgraph's underlying relationships. We define this task generating nq questions per dialogue using our instruction I s as follows:\nD, Q', SQ = f(SG urls, el, nq, I s)\nIn the prompt, we provide all the necessary information in one instruction. So, it makes the design crucial to guide the LLM effectively. However, this approach faces limitations despite its simplicity. Processing the entire dialogue context through a single prompt (I s) can be overwhelming for less powerful LLMs, demanding high capabilities that may hinder broader applicability. In a zero-shot setting, where no specific examples guide the LLM, it might struggle with the task's complexity. That leads to hallucinations, such as irrelevant outputs, duplicate questions, or unclear formulations. Adding relevant examples to the prompt (few-shot learning) can mitigate these issues but may result in repetitive and predictable dialogues that lack diversity. Designing effective few-shot prompts requires a comprehensive set of diverse examples, which can be time-consuming and resource-intensive. Thus, while the single-prompt approach serves as a baseline for dialogue benchmark generation and helps understand the challenges involved, its limitations highlight the need for more advanced methods."}, {"title": "5.2 The Multi-stages Approach", "content": "Given the limitations of the single-prompt approach", "stages": 1, "generation": "Generates questions about different facts in the KG. (2) SPARQL query generation: Translates questions into SPARQL queries to extract answers from the KG. (3) Dialogue generation: Transforms the independent questions into a coherent dialogue. This breakdown enables better control over the process by validating intermediate results and guiding the LLM toward generating high-quality dialogues. While the multi-stage approach may incur slightly higher costs due to multiple prompts", "Generation": "CHATTY-GEN begins by generating a set of independent questions. For a given entity e, its corresponding serialized subgraph SG ser, and the number of questions per dialogue nq, we prompt the LLM to automatically generate a list Q' of nq independent questions. Each question in Q' must be answerable by the subgraph SG ser. Moreover, answering questions in Q' requires no context or knowledge from other questions. CHATTY-GEN prioritizes a diversity of factual question types, such as \"What,\" \"How,\" and \"When,\" over command-based questions like \"Show,\" \"Mention,\" and \"Find.\" In the literature, factual questions have been more widely adopted in dialogue benchmarks compared to command questions. Examples of such conversational"}]}