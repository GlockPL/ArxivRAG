{"title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "authors": ["Luca Della Libera", "Francesco Paissan", "Cem Subakan", "Mirco Ravanelli"], "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples, code and checkpoints are available at https://lucadellalib.github.io/focalcodec-web/.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in large language models (OpenAI et al., 2023; Chowdhery et al., 2024; Jiang et al., 2024a; Grattafiori et al., 2024) have led to significant progress in natural language processing, enabling breakthroughs in tasks such as summarization, translation, question answering, code generation, and retrieval. Building on this success, the research community has extended these methods to other modalities, with speech emerging as a major area of interest. The impressive performance of text-conditioned audio and speech generation models (Borsos et al., 2023; Copet et al., 2023; Kreuk et al., 2023; Wang et al., 2023; Kim et al., 2024), along with recent speech language models (Zhang et al., 2023; Hassid et al., 2023; D\u00e9fossez et al., 2024; Nguyen et al., 2024), highlights the potential of token-based approaches for speech processing.\n\nA key component of these pipelines is the neural audio codec, which compresses speech into tokens that downstream models can process. These tokens must preserve acoustic and semantic information to ensure effective representations for downstream tasks while maintaining high reconstruction quality. Another important requirement is a low token rate. As sequence length increases, capturing long-term dependencies becomes more challenging, and computational costs increase.\n\nDespite recent progress, current codecs still face several challenges. Acoustic codecs (D\u00e9fossez et al., 2023; Kumar et al., 2023; Ji et al., 2024; Xin et al., 2024) achieve high-quality reconstruction but often rely on multiple codebooks, adding complexity to the design of downstream models. Additionally, they typically lack strong semantic representations. Hybrid codecs (Zhang et al., 2024; Liu et al., 2024; D\u00e9fossez et al., 2024; Parker et al., 2024) aim to combine both acoustic and semantic information while maintaining high-quality resynthesis. Still, they often depend on complex multi-codebook designs, explicit disentanglement, distillation losses, or supervised fine-tuning. Single-codebook designs (Li et al., 2024; Guo et al., 2024; Ji et al., 2024; Xin et al., 2024; Wu et al., 2024a) offer a simpler architecture but struggle to balance compression while maintaining both reconstruction quality and effective representations for downstream tasks, especially at low bitrates. To address these limitations, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation (Yang et al., 2022) that compresses speech into the space of a single binary codebook. FocalCodec achieves competitive performance in reconstruction at lower bitrates than the current state-of-the-art under a variety of conditions while also preserving sufficient semantic and acoustic information for downstream tasks.\n\nOur contributions are as follows:\n\n\u2022 We introduce FocalCodec, a novel hybrid codec featuring a unique compressor-quantizer-decompressor archi-"}, {"title": "2. Related Work", "content": "Acoustic Codecs. Acoustic codecs, built on the VQ-VAE (van den Oord et al., 2017) framework, aim for high-fidelity reconstruction. Notable advancements include hierarchical RVQ (Zeghidour et al., 2021), lightweight architectures (D\u00e9fossez et al., 2023), improved RVQ techniques (Kumar et al., 2023), and efficiency-driven designs (Yang et al., 2023; Ren et al., 2024; Ai et al., 2024). Recent methods explore scalar quantization (Mentzer et al., 2024; Yang et al., 2024a), Mel-spectrogram discretization (Bai et al., 2024), and novel paradigms like diffusion- and flow-based decoding (Wu et al., 2024b; Yang et al., 2024b; Pia et al., 2024). To reduce bitrate without compromising performance, multi-scale RVQ (Siuzdak et al., 2024; Qiu et al., 2024) achieves improved compression by varying frame rates in deeper quantizers. However, its hierarchical design adds complexity to downstream applications, as it requires flattening the token sequences. Single-codebook designs (Li et al., 2024; Guo et al., 2024; Ji et al., 2024; Xin et al., 2024; Wu et al., 2024a) have emerged as a simpler, efficient alternative, delivering robust performance at low bitrates. Our codec aligns with this trend, leveraging a novel focal modulation architecture and a pretrained self-supervised encoder to efficiently unify semantic and acoustic representation learning.\n\nSemantic Codecs. Semantic codecs leverage self-supervised features from large models trained with contrastive objectives (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022) and k-means clustering (Lloyd, 1982) for quantization, either from a single layer (Polyak et al., 2021; Wang et al., 2024) or multiple layers (Mousavi et al., 2024b; Shi et al., 2024). Improvements upon this paradigm include replacing k-means with RVQ (Huang et al., 2024), noise-aware (Messica & Adi, 2024) and speaker-invariant tokenization (Chang et al., 2024). While these approaches effectively capture linguistic and content-related information, they often discard much of the acoustic detail, resulting in low speaker fidelity when a vocoder is trained to resynthesize speech from these representations. Our codec adopts a self-supervised architecture similar to semantic codecs but retains acoustic detail through its novel compressor-quantizer-decompressor architecture and decoupled training strategy, ensuring high-quality reconstruction while preserving the advantages of semantic representations.\n\nHybrid Codecs. Hybrid codecs combine semantic and acoustic features to balance reconstruction quality and content representation. Some methods (Ju et al., 2024; Jiang et al., 2024b; Zheng et al., 2024) employ multiple codebooks to disentangle speech into distinct subspaces, such as content, prosody, and timbre, while others (Liu et al., 2024) utilize dual encoders to separately capture content and fine-grained acoustic information. Semantic distillation (Zhang et al., 2024; D\u00e9fossez et al., 2024) has also been explored to enrich the first RVQ codebook with semantic information from HuBERT (Hsu et al., 2021) and WavLM (Chen et al., 2022). More recently, Parker et al. (2024) trained a large-scale transformer-based VQ-VAE, achieving exceptional"}, {"title": "3. FocalCodec", "content": "The proposed codec is largely based on the VQ-VAE framework but incorporates novel compressor and decompressor modules between the encoder and decoder (see Figure 1). The discriminator is used only during training and is discarded afterward.\n\nEncoder. To build a hybrid codec with a simple design, without relying on distillation losses or multiple encoders, the encoder must capture both acoustic and semantic information. This ensures high-quality reconstructions and expressive tokens for training downstream models. Self-supervised models like HuBERT and WavLM retain significant acoustic information in their lower layers (Chen et al., 2022), making them suitable for hybrid codecs. For instance, Baas et al. (2023) show that a high-quality vocoder can be trained using continuous representations from layer-6 of WavLM-large. Following this approach, we use the first 6 layers of WavLM-large as our encoder. However, effective quantization is critical for approximating continuous representations with sufficient granularity. Standard k-means clustering typically fails to preserve essential acoustic details (van Niekerk et al., 2022). To address this, we introduce the novel compressor-quantizer-decompressor design based on focal modulation, which allows for granular quantization that preserves both semantic and acoustic information.\n\nCompressor. The compressor maps the encoder representations to a compact, low-dimensional latent space. Optionally, it can perform temporal downsampling to further reduce the frame rate. Prior work typically relies on convolutional, recurrent, or transformer-based architectures for compression. In contrast, we introduce a novel focal downscaling module, which combines a downscaling operation with a focal block. The downscaling step applies a linear projection to compress the feature dimension, while a 1D convolution can be used instead to additionally downsample along the time dimensions. To better capture periodic patterns, we follow (Kumar et al., 2023) and apply Snake activations (Ziyin et al., 2020) after the projection.\n\nTo build a focal block, we replace the self-attention mechanism in the standard transformer block with focal modulation. Focal modulation (Yang et al., 2022) is an efficient alternative to self-attention that enables fine-to-coarse modeling and introduces useful inductive biases such as translation equivariance, explicit input dependency, time and channel specificity, and decoupled feature granularity. While originally designed for image and video processing, these properties also benefit speech modeling (Della Libera et al., 2024). Unlike self-attention, which directly computes token-wise interactions, focal modulation first aggregates the global context and then modulates local interactions based on this aggregated representation. This ensures that interactions are guided by the overall context rather than being dominated by individual tokens. Formally, focal modulation computes output representation y for each input feature x in sequence $x_{1:n}$ as:\n\n$y_i = q(x_i) \\oplus \\sum_{l=1}^{L+1} h(g_i^l \\odot z_{1:n}^l)$                    (1)\n\nwhere q(\u00b7) and h(\u00b7) are linear projections, and $z^l \\in Z_{i:n}$ and $g^l \\in g_{i:n}$ are the context and gating vectors at position i and focal level l \u2208 {1, . . ., L + 1}, with $\\odot$ denoting element-wise multiplication. The context sequence $Z_{1:n}$ is obtained via a stack of depth-wise convolutions with exponentially increasing kernel sizes to capture dependencies from short to long range, with average pooling applied to the last level feature map to incorporate global information. Then, for each focal level, a point-wise convolution is used to compute the gating sequence $g_{1:n}$. This hierarchical approach, operating at multiple granularities, makes focal modulation well-suited for processing speech features, enabling efficient and scalable representation learning in linear time while preserving long-range dependencies.\n\nQuantizer. FocalCodec maps latent representations from the compressor into the codebook space of a single quantizer, eliminating the need for hierarchical designs in downstream models. To achieve this, while maintaining both reconstruction quality and efficiency, the quantizer should satisfy the following requirements: 1) given that the original waveform is already significantly compressed into a short sequence of latents, the quantizer must compensate by using a sufficiently large codebook size to reduce the quantization error; 2) the quantizer should make efficient use of the codebook capacity, avoiding under-utilization; 3) code lookup must remain efficient, despite the increased codebook size, to ensure fast inference.\n\nTo address these challenges, we employ binary spherical quantization (BSQ) (Zhao et al., 2024), originally introduced for compression of images and videos. To the best of our knowledge, this is the first successful application of binary quantization in the speech domain. BSQ belongs to the category of lookup-free quantization (LFQ) methods (Yu"}, {"title": "3.1. Architecture", "content": "FocalCodec maps latent representations from the compressor into the codebook space of a single quantizer, eliminating the need for hierarchical designs in downstream models. To achieve this, while maintaining both reconstruction quality and efficiency, the quantizer should satisfy the following requirements: 1) given that the original waveform is already significantly compressed into a short sequence of latents, the quantizer must compensate by using a sufficiently large codebook size to reduce the quantization error; 2) the quantizer should make efficient use of the codebook capacity, avoiding under-utilization; 3) code lookup must remain efficient, despite the increased codebook size, to ensure fast inference.\n\nTo address these challenges, we employ binary spherical quantization (BSQ) (Zhao et al., 2024), originally introduced for compression of images and videos. To the best of our knowledge, this is the first successful application of binary quantization in the speech domain. BSQ belongs to the category of lookup-free quantization (LFQ) methods (Yu"}, {"title": "Quantizer.", "content": "where sign(.) denotes the sign function, with sign(0) remapped to 1 to ensure the output always lies on the hypersphere. To make the quantization differentiable, we use the straight-through estimator (Bengio et al., 2013). BSQ offers several advantages over traditional quantization methods. First, the parameter-free implicit codebook is lightweight and computationally efficient. Second, empirical evidence (Zhao et al., 2024) shows that the binary quantization bottleneck encourages high codebook utilization, even for large values of L. Third, the quantization error is bounded, resulting in faster convergence compared to vanilla LFQ, which does not normalize the representations. Finally, tying the codebook size to the latent dimension helps prevent performance degradation in downstream generative models when using larger codebooks (Yu et al., 2024).\n\nDecompressor. The decompressor reconstructs the encoder continuous representations from the quantizer output. It closely mirrors the structure of the compressor, with the downscaling layers replaced by upscaling layers.\n\nDecoder. Most codecs use symmetric architectures, where the decoder mirrors the encoder. However, some works (Bai et al., 2024; Ji et al., 2024; Liu et al., 2024) explore asymmetric designs with larger decoders to improve reconstruction quality. In this work, we adopt an asymmetric design but prioritize the encoder, allocating ~ 5x more parameters to it than the decoder. We argue that a strong encoder is essential for extracting robust, disentangled representations for downstream tasks. Even with a high compression rate, a smaller decoder can still generate high-quality audio while offering faster inference, which is beneficial for streaming applications. For the decoder, we choose the more efficient Vocos (Siuzdak, 2024) architecture over HiFi-GAN (Kong et al., 2020). Vocos maintains consistent feature resolution and uses inverse STFT for upsampling, minimizing aliasing and improving computational efficiency. The decoder"}, {"title": "3.2. Training", "content": "The training process consists of two stages. In the first stage, the compressor, quantizer, and decompressor are jointly trained to reconstruct the encoder continuous representations, ensuring that the tokens retain both semantic and acoustic information from the encoder, which is kept frozen. The training objective includes reconstruction loss and entropy loss. The reconstruction loss is computed as the squared L2 distance between the reconstructed and original encoder features. The entropy loss, defined as in (Yu et al., 2024; Zhao et al., 2024), encourages both confident predictions and uniform code utilization. Note that we omit the commitment loss used in standard VQ, as for BSQ there is no concern of embedding divergence (quantization error is bounded).\n\nIn the second stage, the decoder is trained to resynthesize audio from the encoder continuous representations. This approach enables us to perform this stage in parallel with the first, simplifying the training setup. The training objective includes adversarial loss, reconstruction loss, and feature matching loss, as in (Kong et al., 2020). However, following (Zeghidour et al., 2021), we use a hinge loss formulation instead of least squares. The reconstruction loss is computed as the L1 distance between the reconstructed and original log-Mel spectrograms, while the feature matching loss is the mean of the distances between the l-th feature maps of the k-th subdiscriminator.\n\nThis decoupled training approach ensures that both semantic and acoustic information are preserved in the tokens, which is crucial for downstream tasks while maintaining high reconstruction quality. If trained end-to-end without additional constraints on the hidden representations (e.g. distillation loss), the reconstruction loss prioritizes acoustic features, as observed in (D\u00e9fossez et al., 2023; Kumar et al., 2023)."}, {"title": "4. Experiments", "content": "We train FocalCodec on LibriTTS (Zen et al., 2019), resampled to 16 kHz. We train three variants of the model with a codebook size of 8192 and token rates of 50 Hz, 25 Hz, and 12.5 Hz by adjusting the temporal downsampling factors in the compressor layers to (1, 1, 1), (2, 1, 1), and (2, 2, 1), respectively. These patterns are mirrored in the decompressor layers for upsampling. Information about hyperparameters and training details can be found in Appendix D.1."}, {"title": "4.2. Baselines", "content": "We compare our models to recent state-of-the-art low-bitrate codecs across acoustic, semantic, and hybrid categories. Since the paper focuses on low-bitrate codecs, when multiple quantizers are available, we configure them to achieve a bitrate below 1.50 kbps, ensuring a fair comparison. For acoustic codecs, we compare against EnCodec (D\u00e9fossez et al., 2023), DAC (Kumar et al., 2023), WavTokenizer (Ji et al., 2024), and BigCodec (Xin et al., 2024). Among these, BigCodec is the current state-of-the-art for low-bitrate speech reconstruction quality (Wu et al., 2024a). We use the official checkpoints for these models. We do not include the recent TS3-Codec (Wu et al., 2024a), which matches BigCodec performance at an even lower bitrate, as it is not publicly available. However, we contacted the authors to request reconstructed samples for comparison. Additional results related to TS3-Codec can be found in Appendix F.2.\n\nFor semantic codecs, we adopt the approach introduced in (Wang et al., 2024), which quantizes layer-6 representations from WavLM-large using k-means clustering with 512 centroids. These representations are fed into a Conformer (Gulati et al., 2020) encoder to reconstruct continuous representations, followed by a HiFi-GAN decoder. This baseline, referred to as WavLM6-KM, provides a direct comparison between our codec and another model leveraging WavLM layer-6 features but differing in design and training methodology. Since the code and checkpoints for WavLM6-KM are not publicly available, we reimplemented the model using a subset of LibriSpeech (Panayotov et al., 2015). Note that we do not include additional baselines from this category, as semantic codecs typically underperform in terms of reconstruction quality (Parker et al., 2024) or require much higher bitrates to be competitive in this regard (Mousavi et al., 2024a). Furthermore, most hybrid codecs are already built on top of semantic representations. Therefore, we prioritize the hybrid category, to which our codec also belongs. For hybrid codecs, we compare against Speech Tokenizer (Zhang et al., 2024), SemantiCodec (Liu et al., 2024), Mimi (D\u00e9fossez et al., 2024), and Stable Codec (Parker et al., 2024), using their official checkpoints. The configurations and details of each model are summa-"}, {"title": "4.3. Speech Resynthesis", "content": "We evaluate FocalCodec on speech resynthesis, considering both English and multilingual speech. For English speech, we use LibriSpeech (Panayotov et al., 2015) test-clean. For multilingual speech, following (Xin et al., 2024), we randomly select 100 utterances from each of the 7 foreign languages in Multilingual LibriSpeech (Pratap et al., 2020) (Dutch, French, German, Italian, Polish, Portuguese, and Spanish), resulting in a total of 700 utterances. We also consider the more realistic scenario of speech contaminated with environmental noise. For this, we use the test splits of VoiceBank (Valentini-Botinhao et al., 2016) and the more challenging Libri1Mix, which is constructed by mixing clean utterances from the first speaker of LibriMix (Cosentino et al., 2020) with noise from WHAM! (Wichern et al., 2019).\n\nWe evaluate the models using objective metrics. To measure naturalness, we employ UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech. Note that we do not include signal-level metrics such as SNR, PESQ (Rix et al., 2001), or STOI (Taal et al., 2011), as these metrics do not correlate well with perceived reconstruction quality (Parker et al., 2024; Wang et al., 2024). To evaluate speaker fidelity, we compute the cosine similarity (Sim) between speaker embeddings extracted from the reconstructed audio and the target audio. These embeddings are obtained using WavLM-base (Chen et al., 2022) fine-tuned for speaker verification. To assess intelligibility, we compute the differential word error rate (dWER) (Wang et al., 2021), which measures the difference in word error rate between the reconstructed and target audio, using transcriptions from Whisper small (Radford et al., 2023). To ensure fairness in evaluation, we do not use more powerful ASR models (e.g. Whisper large-v3), as these models can correct pronunciation mistakes and are more robust to noise, potentially hiding flaws in the reconstruction. We also report code usage, i.e. the ratio of unique tokens used to the codebook size (averaged over codebooks for multi-codebook models), and normalized entropy (Cover & Thomas, 2006; Parker et al., 2024)"}]}