{"title": "Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration", "authors": ["Junjia Liu", "Zhuo Li", "Minghao Yu", "Zhipeng Dong", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "abstract": "Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.", "sections": [{"title": "I. INTRODUCTION", "content": "Humanoid robots are increasingly expected to perform human-level loco-manipulation tasks. In recent years, the development of diverse humanoid robot platforms has expanded rapidly, each featuring unique configurations and dynamic properties. While hardware has advanced rapidly, humanoid skill learning remains a significant challenge. Traditionally, loco-manipulation tasks have been divided into sub-objectives like collision avoidance, balance maintenance, and robustness to external forces. While effective, this approach is time-consuming and impractical for the fast-paced development cycles of modern robotics [1]. Learning-based approaches for robot skill acquisition are now mainstream due to their flexibility and adaptability. However, the high degrees of freedom (DoFs) in humanoid robots result in a large and complex action space, making it challenging for learning-based algorithms to acquire coordinated skills. Additionally, the scarcity of quality training data hinders progress, as collecting diverse robot data is labor-intensive and time-consuming, especially for loco-manipulation tasks.Skill transfer has emerged as a critical capability for addressing these limitations. By reusing and adapting learned skills from human demonstrations to robots and across different humanoid platforms, skill transfer reduces the need for extensive retraining and facilitates efficient skill acquisition.However, humanoid skill transfer faces two key challenges. First, configuration differences arise because humanoid robots typically feature joint structures, physical dimensions, and dynamic properties that differ significantly from human embodiments and from one another. These differences make direct skill transfer inefficient and often lead to unstable motion. Second, high-dimensional coordination poses a challenge, as humanoid robots must execute whole-body"}, {"title": "II. RELATED WORK", "content": "Learning-based approaches have become mainstream for enabling humanoid robots to perform locomotion and ma-nipulation tasks. Methods leveraging human demonstrations offer significant benefits by allowing robots to learn directly from human expertise, improving both the efficiency and naturalness of skill acquisition. However, differences in embodiments between humans and robots pose a significant challenge, particularly in transferring skills effectively."}, {"title": "A. Learning-Based Approaches for Loco-Manipulation", "content": "Learning-based methods, especially reinforcement learn-ing (RL), have proven effective for stable locomotion in both quadruped [2] and bipedal robots [3], but integrating these methods into loco-manipulation introduces challenges. External forces from manipulators can destabilize lower limbs, which recent work addresses by combining learning-based locomotion with model predictive control (MPC) for"}, {"title": "B. Imitation Learning and Adversarial Imitation Learning", "content": "Imitation learning is commonly used to overcome the inefficiencies of model-free RL, particularly in humanoids [6]. Traditional methods rely on supervised learning from demonstrations but struggle with generalization [7]-[10]. Adversarial imitation learning addresses these limitations by using a discriminator to guide RL exploration towards the distribution of the demonstration data [11]. When combined with large human motion datasets, it can enhances high-DoF humanoid skill acquisition and adaptability [12] [13]."}, {"title": "C. Cross-Embodiment Skill Transfer", "content": "Transferring skills across different robotic embodiments has been explored using various approaches. XSkill [14] learns cross-embodiment skill representation from unlabeled videos but is limited to simple manipulation tasks and does not explicitly handle the complexities of humanoid dynamics. Wang et al. [15] proposed projecting state and action spaces into a shared latent space for policy transfer across embodiments without task-specific rewards. How-ever, their approach relies on well-defined action mappings, which are challenging to define for humanoid robots due to their high-dimensional and non-linear dynamics. XIRL [16] learns vision-based reward functions for policy transfer across embodiments. Yang et al. [17] showed that co-training across diverse embodiments improves robustness and enables zero-shot transfer. Mirage [18] uses cross-painting to mask visual differences for successful zero-shot policy transfer."}, {"title": "III. CROSS-EMBODIMENT HUMANOID BEHAVIOR AND SKILL TRANSFER", "content": "The schematic overview of the proposed cross-embodiment humanoid behavior and loco-manipulation skill transfer framework is shown in Fig. 2."}, {"title": "A. Kinematic motion retargeting", "content": "Humans possess complex joint structures, with many joints capable of 3-DoF coaxial rotation. Previous work has utilized the 51 \u00d7 3-DoF SMPL human model [19], which represents"}, {"title": "B. Whole-body Functional Decomposition and Behavior Primitive Training", "content": "Demonstration datasets typically cover only a limited range of motions and cannot encompass all possible behav-iors. Therefore, the goal of imitation learning is to capture the behavioral characteristics rather than directly replicating the motions in the dataset. These behavioral characteristics, known as behavior primitives, are fundamental building blocks of motion that represent basic patterns of demonstra-tions. By combining these primitives, humanoid robots can adapt to diverse tasks and generate complex behaviors.To achieve this, a low-level behavior controller \\(\\pi(a|s, z)\\) is introduced, which generates actions a based on the current state s and a latent behavior variable \\(z \\in Z\\), sampled from a prior distribution \\(p(z)\\). The behavior latent space Z, defined as a unit hypersphere, represents feasible humanoid behaviors by capturing generalizable patterns from human demonstrations. This design decouples high-level task plan-ning from low-level motion execution, enabling flexible and reusable behavior generation.To train the low-level controller, adversarial imitation learning is employed, where the RL policy acts as a gener-ator, and a discriminator D is designed to distinguish real human motions from generated behaviors. This approach ensures that the controller learns to produce actions that align with the behavioral characteristics of the unstructured motion dataset \\(M_u\\), rather than merely replicating specific trajectories. The objective of the discriminator is defined as:\n\\[\\min_{D} - E_{a \\sim M_u}(s,s') [\\log (D (s, s'))] - E_{d \\sim \\pi} (s,s') [\\log (1 - D (s, s'))]\\]\nIn this setting, the behavior style reward is derived from the output of the style discriminator: \\(r_b(s, s') = \\max[0, 1 - 0.25 (D (s, s') - 1)^2]\\), which measures the similarity between the generated motions and real human motions. The final pre-trained policy is expected to generate natural and realistic humanoid behaviors for any given latent skill z. Since a reference motion dataset is available for each robot through kinematic motion retargeting, embodiment-specific behaviors can be pre-trained using this same approach.To address the challenges posed by the high redundancy and coordination requirements of humanoid robots, we pro-"}, {"title": "C. Loco-manipulation skill learning with interaction graph", "content": "Loco-manipulation skills can be constructed based on previous locomotion and whole-body free motion behav-iors, guided by human-object interaction in an object-centric manner. In this section, the concept of an interaction graph G is introduced to describe human-object interactions. Once the low-level controller with pre-trained human behavior primitives is available, a goal-oriented high-level policy \\(\\eta(z|s, G, g)\\) can be designed to generate a sequence of latent behaviors z that accomplish the task.The motivation of interaction graph is to handle the variability of real-world interactions. Although motions of interacted objects are captured during human embodiment demonstrations, directly using these object trajectories for imitation learning is not appropriate. Robots are likely to encounter various task parameters with same interaction skills, like different start and end poses or object sizes. Among them, the interaction pattern between humans and objects remains relatively consistent. Thus, in this work, an interaction graph is extracted and constructed for each time step from demonstration to represent interaction skills.The interaction graph \\(G\\) is a structured representation where nodes represent body parts and objects, and edges indicate interactions such as contact or proximity. Each node encodes the pose (position and orientation) of the corresponding body part or object, while edges are binary indicators of contact: 1 for contact, 0 otherwise. To handle the variability of task parameters, the relative distances between the hands and the object are encoded in the hand node. Similarly, the relative distance between the current pose of object and its target pose is stored in the object node. In loco-manipulation tasks, the focus is primarily on hand-object contact. To reduce complexity, body parts like hands, composed of multiple rigid bodies, are aggregated into a single node. This aggregation keeps the graph manageable, minimizing noise and avoiding excessive size.To ensure that the interaction style of robot aligns with the human demonstration, a style discriminator \\(D_I\\) is introduced. This discriminator evaluates the similarity of the robot inter-action graph to the reference graph from human demonstra-tions. The state input for the interaction style discriminator \\(D_I\\) is constructed as \\(s_I = \\{d_{ho}, d_{og}, c_{ho}\\}\\), where h refers to hands, o refers to the object, and g refers to the target. \\(d_{ho}\\) and \\(d_{og}\\) represent the relative distance information, and \\(c_{ho}\\) denotes the binary contact value between the hands and the object. The specific loss function for \\(D_I\\) is then given as:\n\\[\\min_{D_I} - E_{a_M} (s_I, s'_I) [\\log (D_I (s_I, s'_I))] - E_{d_{\\eta}} (s_I, s'_I) [\\log (1 - D_I (s_I, s'_I))]\\]\nwhere \\(M\\) is the human embodiment demonstration dataset with human-object interactions already retargeted to the unified digital human model. For training the high-level skill policy \\(\\eta\\), the reward function is a combination of task reward"}, {"title": "D. Human-Robot Skill Transfer and Fine-Tuning", "content": "Leveraging the pre-training of behavior primitives across diverse humanoid embodiments and the generalizable skill learning on a unified digital human, the deployment of consistent skills on different humanoid robots is significantly streamlined. The methodology outlined in this section (Part 4 in Fig. 2) builds upon the earlier sections.The foundation model for loco-manipulation skills is es-tablished through pre-trained skill policies on the unified digital human. Task execution motions for each specific humanoid robot embodiment are retargeted from this founda-tion model, eliminating the need for re-training and enabling efficient skill deployment. The retargeting process involves executing loco-manipulation tasks based on the unified dig-ital human embodiment and subsequently mapping the gen-erated motion to the desired humanoid robot embodiment using kinematic motion retargeting techniques, as described in Sec. III-A. With the retargeted motion as a reference, a similar high-level control structure can be constructed as discussed in Sec. III-C.It is important to note that the skill policy used during the fine-tuning phase differs from and is simpler than the one described in Sec. III-C. In the previous section, the reference trajectory consisted of a small amount of demonstration data with varying task parameters, making the training process more challenging. However, during fine-tuning, the objective is to directly imitate the retargeted trajectory that is already capable of achieving the desired task.To facilitate this, a simple Multi-Layer Perceptron (MLP)-based fine-tuning layer is introduced. This layer generates hyper curves in specific behavior primitive spaces, converting the kinematics-level retargeted motion into dynamics-level motion that produces actual control commands. The fine-tuning process is crucial for adapting the retargeted motion to the dynamics and control requirements of the specific humanoid robot embodiment, ensuring that the robot can fur-ther perform the task effectively in real-world environments."}, {"title": "IV. EXPERIMENTS", "content": "State: The state representation of UDH and each hu-manoid robot includes the position and velocity of every joint, as well as the translation, rotation, velocity, and angular velocity of the root link. Additionally, the global poses of key body parts, such as the hands during object manipulation tasks, are included. The interaction state, which comprises object-related information, has been detailed in Sec. III-C.Action: The behavior controller outputs target joint po-sitions, which are executed by a Proportional-Derivative (PD) controller that generates the necessary joint torques. The dimensions of the joints for each humanoid robot are listed in Table I. It should be noted that, motor torque limitations were simulated by constraining the output of PD controller, though the resulting dynamics may differ from real-world conditions. Joint limitations and inertia for each humanoid were defined based on their URDF models which are similar as hardware constraints.Performance Metrics: Behavior primitives are not task-oriented and lack clear task completion metrics. Therefore, the training return for the behavior primitives is fully rep-resented by the style reward provided by the discriminator. In the skill learning stage, the training return is given by Equ. 3, reflecting both task completion and whether the robot behavior is anthropomorphic.Skill Policy and Behavior Controller: The policy net-works employed to control the humanoid robots are based on an actor-critic framework and utilize the Proximal Policy Optimization (PPO) algorithm, a widely used method in RL.Humanoid Models and Simulation Environment: The five humanoid robot models used in this study-NAVIAI, H1, Bruce, Walker and CURI\u2014underwent refinement to ensure accurate simulation, including self-collision checks and the addition of extra hands for the NAVIAI and H1 robots. These models are publicly available on the Rofunc platform [20]. The experiments were conducted using Isaac Gym [21], a high-performance, GPU-based physics simulator designed specifically for robot learning applications. To enhance the robustness of the humanoid models, those equipped with legs (NAVIAI, H1, Bruce, and Walker) were initialized in each simulation episode by dropping from mid-air, prompting them to stabilize and maintain an upright stance-a practice that helps train the models for dynamic balance."}, {"title": "B. Data Collection and Motion Dataset", "content": "The human embodiment demonstrations were captured using the Optitrack motion capture system, as illustrated on the left side of Fig. 1. To accurately capture whole-body motions, a motion capture suit equipped with 41 markers was utilized. Hand movements were recorded using Manus gloves, which provide detailed tracking of finger and hand motions. Additionally, markers were placed on objects involved in the tasks to track their poses and interactions with the human subject.The collected data, which includes comprehensive record-ings of human motions, were converted to the FBX format using Optitrack software. These FBX motion files were sub-sequently parsed and retargeted to various humanoid robot embodiments using the Rofunc platform. Rofunc automates the retargeting process, ensuring that the motion data is appropriately adapted to different robotic configurations."}, {"title": "C. Decomposed Behavior Primitive Pre-Training", "content": "Decomposed behavior primitive pre-training is conducted independently on the UDH and five humanoid robots. The reference motion dataset for training was transferred from the human demonstrator to each robot embodiment via kinematic motion retargeting. After pre-training with the entire motion dataset, the imitation of specific behaviors can be fine-tuned using the corresponding reference motion data. Formally, this process identifies the latent subspace associated with the spe-cific behavior within the overall behavior latent space. The"}, {"title": "D. Loco-Manipulation Skill and Fine-Tune Performance", "content": "Figure 8 illustrates a loco-manipulation task where hu-manoid robots walk towards a floating box, grasp it, and place it at a designated target position. The first two rows depict the same task but are executed by the UDH and NAVIAI, respectively. This demonstrates the process of"}, {"title": "V. CONCLUSION", "content": "In this article, we presented a framework for cross-embodiment loco-manipulation skill transfer from human demonstrations to various humanoid robots. This approach addresses the challenges of configuration differences and the high degrees of freedom in diverse humanoid embodiments. The framework leverages a unified digital human model as a common prototype across different robot platforms, enabling efficient skill transfer.A major contribution of this work is the incorporation of decomposed adversarial imitation learning (DAIL), which plays a central role in overcoming the data bottleneck often encountered in humanoid robot. This decomposition not only simplifies the coordination of multi-joint systems but also significantly reduces the amount of data required to learn complex loco-manipulation tasks.Through the combination of kinematic retargeting and decomposed imitation learning, the framework allows gener-alized skills to be transferred across multiple humanoid plat-forms, while accounting for their unique configurations and dynamic properties. As a result, the same loco-manipulation tasks can be performed on diverse robots with minimal data requirements, making this approach highly effective in low-data regimes and a valuable contribution to the development of humanoid robots."}, {"title": "VI. DISCUSSION", "content": "While this work demonstrates a promising approach to cross-embodiment skill transfer, there are several areas for future improvement and exploration. First, this article has primarily focused on a common loco-manipulation task: walking to pick up and place an object. However, the frame-work could be extended to incorporate more advanced skills, such as force closure, energy balance, and the use of addi-tional contact points to enhance manipulation capabilities. These advanced skills are essential for further improving the versatility and effectiveness of humanoid robots in complex loco-manipulation tasks.Another key area for future research is the reliance on full world state observation, particularly relative distances in an interaction graph. Future work could use object pose estima-tion to construct relative positions, reducing this dependency and improving real-world practicality.Finally, the disparity between the dynamics and motor performance of real robots compared to their simulated coun-terparts remains a significant challenge. While our current framework provides a solid foundation, bridging this gap is critical for the practical deployment of these skills in real-world scenarios. Future efforts could explore how human embodiment demonstrations can further guide robot learn-ing, ensuring that the skills learned in simulation transfer effectively to physical robots."}]}