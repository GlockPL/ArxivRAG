{"title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress", "authors": ["Ayomide Odumakinde", "Daniel D'souza", "Pat Verga", "Beyza Ermis", "Sara Hooker"], "abstract": "The use of synthetic data has played a critical role in recent state-of-art breakthroughs. However, overly relying on a single oracle teacher model to generate data has been shown to lead to model collapse and invite propagation of biases. These limitations are particularly evident in multilingual settings, where the absence of a universally effective teacher model that excels across all languages presents significant challenges. In this work, we address these extreme difference by introducing \u201cmultilingual arbitrage\u201d, which capitalizes on performance variations between multiple models for a given language. To do so, we strategically route samples through a diverse pool of models, each with unique strengths in different languages. Across exhaustive experiments on state-of-art models, our work suggests that arbitrage techniques allow for spectacular gains in performance that far outperform relying on a single teacher. In particular, compared to the best single teacher, we observe gains of up to 56.5% improvement in win rates averaged across all languages when switching to multilingual arbitrage. We observe the most significant gains for the least resourced languages in our pool.", "sections": [{"title": "1 Introduction", "content": "Throughout our lives, we are guided by many teachers, each contributing distinct insights and expertise to our personal and professional growth. For specialized skills, such as becoming a doctor or mastering culinary arts, we seek out experts who provide targeted guidance. In contrast, synthetic data generation often relies on a single teacher model to impart knowledge to a student model. This approach can lead to the passive transfer of both the strengths and limitations inherent in the teacher model, as highlighted in various studies [Shumailov et al., 2023; Magister et al., 2023; Shimabucoro et al., 2024; Gerstgrasser et al., 2024]. Moreover, it assumes that a single model can effectively teach all relevant skills, which may not always be the case.\n The limitations of the single oracle approach become particularly pronounced in multilingual set- tings, where high-performing large language models (LLMs) are often trained predominantly on a few data-rich languages [Singh et al., 2024; Joshi et al., 2020; Fan et al., 2021]. This diverse landscape of multilingual model development has resulted in a variety of models: large-scale models that support multiple languages [Xue et al., 2020; Scao et al., 2022; Shliazhko et al., 2022; Li et al., 2023; \u00dcst\u00fcn et al., 2024], frontier models with some multilingual capabilities that are not specifically optimized [Armengol-Estap\u00e9 et al., 2021; Chowdhery et al., 2022; Zhang et al., 2022; Team et al., 2024], and models focused on regional language families [Adelani et al., 2021; Mirzakhalov et al., 2021; Singapore, 2024; Cahyawijaya et al., 2022]. As a result, it is often unclear how to determine which model to use to maximize performance for a given language. Relying on a single model can also further amplify disparities in treatments between languages, as models may perform well on some language but not have coverage for others. Performance tends to be critical for the quality of synthetic data, which can enable further progress in those languages by making data more ubiqui- tous over time [Alaa et al., 2022; Gao et al., 2023; Bukharin & Zhao, 2023; Zhang et al., 2023; Li et al., 2024; Zhang et al., 2024].\nIn this work, we take a wider view of synthetic data generation. Rather than perceiving model distillation as the transfer of data from a single oracle to a student, we re-frame the problem within this heterogeneous landscape as learning how to optimize sampling for a desired part of the data distribution from an ensemble of teachers. Multilingual settings offer a valuable case study for this exploration due to the clearer boundaries between languages compared to tasks. We expect our arbitrage techniques to lead to improvements across any setting where it would be unusual for a single model to be state-of-art at all tasks.\nWe introduce the concept of multilingual arbitrage, which leverages differences in performance for a language between multiple models. For each language, we utilize a pool of models as potential teachers. We evaluate different methods for sampling strategically by routing to different models. We then use this optimized distribution to instruction fine-tune a new multilingual model, with the goal of exceeding the performance achieved by relying on a single multilingual model across all languages. This also allows us to ask can you surpass individual model performance by sampling"}, {"title": "2 Methodology", "content": "Our primary goal is to train a high-performing multilingual student model S. Given a set of input prompts P = {pi}iN=1, we generate a corresponding set of completions C = {ci}iN=1 using a pool of potential teacher models T = {Tj}jM=1. These prompt-completion pairs (pi, ci) will then be used to fine-tune S. For each prompt pi \u2208 P, we aim to identify the specific teacher model Tj \u2208 T that produces the highest quality completion ci.\nWe consider that each teacher model Tj may not perform uniformly across all regions of interest R in the data distribution. Therefore, we aim to minimize the empirical error E[Pj(R)], where Pj(R) represents the performance of teacher model Tj in region R, over the broader distribution D. This ensures robustness and generalization beyond the i.i.d. training sample Diid. This approach allows us to select the most suitable teacher model for each prompt, optimizing the training of our student"}, {"title": "2.1 Routing Methods", "content": "The crux of the problem of multilingual arbitrage is: how do you route prompts to the most calibrated teacher model for each prompt? We exhaustively benchmark different routing strategies which we introduce briefly below:\nFixed Routing. In practice, one might choose a fixed model, such as T2, to process all input prompts in P. This can be reasonable if T2 demonstrates significantly better overall performance for a majority of the prompts. In the multilingual case, this setting is one in which we can pre- determine the best model for each language based on their known strengths, enabling us to use a fixed routing strategy for each prompt deterministically by choosing the appropriate teacher model according to the prompt's language. However, in real-world settings it is not always possible to know what models are relatively strong at different languages in advance.\nReward-based Routing. Next we consider the more realistic setting which assumes that we cannot pre-determine a fixed routing strategy. Instead, we rely on a reward model for routing. For each pi we generate a completion from each of the teacher models in T and then select ci to be the completion with the highest score given by some ranking method. In our case, we use a proprietary reward model (Cohere May 2024) which is competitive with top-scoring state-of-the-art reward models on the RewardBench Leaderboard [Lambert et al., 2024]."}, {"title": "3 Experimental Setup", "content": "To evaluate the effectiveness of multilingual arbitrage, we compare our method against several base- line methods. Below, we provide a brief overview of the experimental details for each baseline:\nSingle Teachers. This is the most widely adopted framework for incorporating synthetic data into training. In this paradigm a student model is trained on the generations produced from a single teacher model. This setup allows us to explore the question: Is multilingual arbitrage more effective than a single \u201coracle\u201d teacher?\nWe choose single teacher models based on architecture, size, base model type, and the extent of coverage of languages. The selected models, ranging from 7B to 9B parameters, are widely used: Aya 23 [Aryabumi et al., 2024], Llama 3 [Dubey et al., 2024], and Gemma 2 [Team et al., 2024]. We include more details about each model in Appendix Section A.1. We note that both Llama 3 and Gemma 2 do not explicitly claim multilingual support, however, in practice both are more frequently used by multilingual users compared to models explicitly designed for multiple languages, such as mT0 [Muennighoff et al., 2023b] and BLOOMZ [Dac Lai et al., 2023].\nRandom Routing. Next, we consider a router that randomly assigns each prompt pi \u2208 P to teacher model Tj \u2208 T, without considering the language or any other specific characteristics of the prompt. This allows us to investigate: Is multilingual arbitrage better than a random selection as to what model is best for a given distribution of interest?\nTranslation. Lastly, our translation baseline addresses whether strategic sampling outperforms simply translating the generations of a single English model into multiple languages. We aim to determine: Does generating synthetic data in the target language outperform translating the best English only data?\nWe generate completions for our English training prompts using our most capable English teacher model, Llama 3. We then translate each of the prompts and completions to the seven languages included in our router experiments."}, {"title": "3.2 Routing Teacher Pools", "content": "Fixed Router Model Pool. In our fixed router experiments, we assume prior knowledge about which models perform best for specific languages. To create this setting, we train several geo-cluster models on 15 languages, each specializing in a subset of languages corresponding to the language families detailed in Table 2. Training in this way allows the models to leverage geographic and linguistic similarities within a language cluster [Kohli et al., 2023; Kew et al., 2023; Tejaswi et al., 2024]. Each geo-cluster surpasses the single teacher model prior to student model training, achieving an average win rate gain of 14.9% relative to single teachers. See Appendix A.1.1 for additional training and win rate evaluation details.\nReward-based and Learned Routing. These methods aims to demonstrate the effectiveness of routing in a varied pool of models with unknown multilingual performance. Hence, we consider a"}, {"title": "3.3 Student Model", "content": "We selected the Aya 23 8B model [Aryabumi et al., 2024] as our student model due to its strong multilingual capabilities which are state-of-the-art (SOTA) for its size class. For each arbitrage technique, we generate synthetic data in seven diverse languages: Arabic, Chinese, English, French, German, Turkish, and Ukrainian. These languages, representing different language families, are se- lected to ensure a comprehensive evaluation across various linguistic contexts. Detailed information is provided in Table 6 in Appendix A.2.\nTraining Details. Our student models are trained from a collection of prompts derived from UltraFeedback Binarized Dataset [Tunstall et al., 2023], an English preference dataset of 61,135 pairs. We randomly sampled 10,000 prompts and translated them into the seven target languages using the NLLB-3.3B model, resulting in a total of 70,000 prompts. Completions for each prompt were then generated by the assigned teacher model. We then instruction fine-tune each student model on these 70,000 data points selected through multilingual arbitrage. The training employed a cosine learning rate schedule with a warm-up phase, using a batch size of 64 and an evaluation batch size of 128. The peak learning rate was set at 2.5 \u00d7 10\u22125, achieved through 128 warm-up steps starting from a learning rate of 0.0, and then decayed back to 0.0."}, {"title": "3.4 Evaluations", "content": "Open-ended Generation Win rates. Beyond traditional NLP tasks, we aim to evaluate the open-ended generation capabilities of the student models, focusing on their ability to produce un- structured and long-form responses. For this evaluation, we use GPT-4 as an LLM-judge to measure pairwise win rates between two model generations. We evaluate on the target language subset of the Multilingual Dolly-200 Eval dataset [Singh et al., 2024; \u00dcst\u00fcn et al., 2024]. This 200 instance evaluation dataset is a held-out curated sample from the Dolly-15k dataset [Costa-juss\u00e0 et al., 2022]. These prompts are open-ended and capture general-purpose non-code use cases. Hence, evaluation using this dataset is a valuable proxy for how multilingual arbitrage impacts more fluid and often open-ended asks.\nDiscriminative Tasks. To evaluate our models on completely unseen tasks, we follow Muennighoff et al. [2023b] and use XNLI [Conneau et al., 2018], \u0425\u0421\u041e\u0420\u0410 [Ponti et al., 2020], and XStoryCloze [Lin et al., 2021] datasets targeting natural language inference, commonsense reasoning and sentence completion respectively. These unseen tasks are crucial for evaluating the effectiveness of IFT in improving a model's reasoning and comprehension capabilities as they test the model's ability to discriminate between different possible interpretations or outcomes. For all unseen tasks, we report zero-shot performance."}, {"title": "4 Results and Discussion", "content": "The multilingual arbitrage methods consistently outperformed the random routing baseline, achieving an average win rate of 51.8% with a notable improvement of 78.9% over random routing.\nArbitrage strategies demonstrate substantial improvements over single teacher models. Reward-based routing achieves the highest average win rate gains at 56.5%, while learned routing provides a notable absolute gain of 25.6% over single teacher models and is signifi- cantly more efficient - requiring only one generation per prompt, making it nine times more efficient than reward-based routing.\nOn unseen discriminative tasks, arbitrage methods provided a larger average improvement of 1.95% over the base student model compared to a 0.98% improvement by single teach- ers. While fixed routing achieved the highest average gain of 2.50%, all routing approaches effectively enhanced cross-lingual and commonsense reasoning capabilities, showcasing their overall superiority to single teacher models.\nComparison against random routing. Our random routing baseline serves as a crucial lower bound that any proposed arbitrage strategy should outperform. This baseline helps us evaluate: Is our multilingual arbitrage technique better than a random guess? In Figure 3, we compare the win rates of each of the different routing methods against the random routing baseline. We observe that all the multilingual arbitrage methods consistently outperformed the random baseline with average win rate of 51.8% and a notable relative win rate improvement of 78.9%."}, {"title": "Win-rate Gains are largest for Reward-Based Routing.", "content": "We observed the largest improve- ments against single teachers with reward-based routing, achieving average gains of 56.5%. However, reward-based routing is the least efficient arbitrage method because it requires running inference and generating completions with all models in the pool for each prompt. Although fixed routing and learned routing show some decrease in win-rates compared to reward-based routing, they are significantly more efficient during inference, as they only require inference from one model. In our experiments with a pool of 9 models, reward-based routing requires generating and scoring 9 comple- tions per prompt, while fixed and learned routing need only one generation per prompt. Although learned routing involves an additional call to the router per prompt, this router model is much smaller and more efficient than the teacher, making the call negligible compared to generating from all models in the pool. Notably, learned routing is the most flexible technique, being 9 times more efficient than reward-based routing in this setup and not needing prior knowledge of each model's merits, unlike fixed routing."}, {"title": "Language and Routing Analysis", "content": "Medium-resource languages benefit more from routing strategies than high-resource lan- guages. Specifically, reward-based routing achieves a 56.1% gain over single teacher models for medium-resource languages, compared to a 35.7% gain for high-resource languages.\nDifferent models are favored for different languages, highlighting the strengths of arbitrage that leverages a diverse model pool. For example, Aya 23 is predominantly used for Ukrainian, Turkish, and Arabic, while Llama3 is favored for English prompts.\nGenerating synthetic data directly in target language is more effective than translating English-only data. Even a single teacher model achieves a 48.9% increase in win rate com- pared to translations made from English data generated by the top-performing English model."}, {"title": "Textual Characteristics", "content": "Routing strategies significantly increase the average number of tokens per generation, with outputs ranging from 160 to 242 tokens, compared to 76 tokens for the base model and around 144 tokens for both random routing and single teacher models, demonstrating that arbitrage methods produce longer text generations.\nThe Gunning-Fog and Rix indices, which assess text readability and complexity, show higher scores for arbitrage methods compared to the all baselines. Specifically, The Gunning-Fog index increases by 3.28, 1.05 and 0.78 compared to the base student model, single teachers and random routing, respectively, indicating that arbitrage methods lead to more complex text compared to all baseline methods.\nThe measure of lexical diversity (MLTD) is notably higher for arbitrage methods than for all baseline approaches. Reward-based routing achieves an increase of 7.97, and Learned Routing achieves an increase of 7.1 compared to the base student model. On average, arbi- trage methods show absolute differences of 1.77 compared to single teacher models and 5.46 compared to random routing, highlighting their improved lexical diversity."}, {"title": "5 Related Work", "content": "The issue of LLM circularity, where models influence other LLMs through distilled data, has recently gained attention. Research to date has focused on two main areas: model degradation via recursive training [Dohmatob et al., 2024a; Briesch et al., 2023; Shumailov et al., 2023] and self-preference in an LLM-as-a-Judge setting. Regarding model degradation, studies have shown that training LLMs with data iteratively generated by other LLMs impairs performance as the tails of the original distribution start to disappear. This includes work focusing solely on high- frequency contexts, thereby neglecting long-tail knowledge [Briesch et al., 2023; Bertrand et al., 2024; Shumailov et al., 2024; Dohmatob et al., 2024b], and resulting in a loss of diversity [Guo et al., 2024; Feng et al., 2024]. In [Shimabucoro et al., 2024], the authors explore how the transfer of characteristics via passive inheritance occurs when synthetic data generated by different LLMs is involved. By considering the issues highlighted in these studies, we aim to optimize synthetic data generation by selecting the most calibrated teacher model from a pool of LLMs in a multilingual setting.\nIFT on a large collection of tasks has become a key paradigm for enhancing LLM performance, increasing their utility [Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2021; Min et al., 2021; Ouyang et al., 2022], and enabling generalization to unseen tasks [Wei et al., 2022; Chung et al., 2022]. Successful instruction tuning relies on three critical factors: task diversity [Longpre et al., 2023; Wang et al., 2023b; Chung et al., 2022], complexity [Xu et al., 2023; Luo et al., 2023b;a], and quality [Zhou et al., 2023; Taori et al., 2023b; Muennighoff et al., 2023a; Zhuo et al., 2024]. While these principles have been validated primarily for English tasks, there is a growing focus on instruction fine-tuning in multilingual contexts [\u00dcst\u00fcn et al., 2024; Muennighoff et al., 2023b]. A common challenge is the scarcity of multilingual instruction datasets, which recent efforts [Singh et al., 2024] have addressed by creating a comprehensive multilingual instruction dataset. In this work, we strategically sample from a diverse pool of models, each with unique strengths across different languages, to generate high-quality synthetic instruction data.\nWhile numerous studies focus on synthetic data generation in En-glish settings [Gao et al., 2023; Anaby-Tavor et al., 2019; Wang et al., 2022; Taori et al., 2023a; Gao et al., 2023], the impact of synthetic data on enhancing multilingual performance is not well under- stood [Kaddour & Liu, 2023; Yadav, 2023; Kramchaninova & Defauw, 2022]. Recently, [Aryabumi et al., 2024] explored multilingual synthetic data generation in the context of a single teacher model. Aakanksha et al. [2024] used multilingual synthetic data for safety preference training, and [Dang et al., 2024] applied it for multilingual preference tuning. Our research diverges by concentrating on multilingual synthetic instruction data generation from an ecosystem view rather than a single teacher.\nEnsembling LLMs is a powerful approach to leverage indi- vidual model strengths while mitigating weaknesses through complementary abilities. The aim is to combine available LLMs to enhance performance consistently across various downstream tasks. There is limited research on the complementary potential of LLMs and strategies for assembling them effectively. Jiang et al. [2023] introduce a framework that combines LLMs using pairwise ranking and generative fusion to leverage their diverse strengths. Chen et al. [2023] use a sequential inference approach with existing LLMs, continuing until the response quality is satisfactory. Wang et al. [2023a] tackle the challenge of fusing outputs from expert models with complementary data distribution knowledge, framing it as a supervised learning problem. Shnitzer et al. [2023] develop"}, {"title": "6 Conclusion", "content": "In this work, we introduce the concept of multilingual arbitrage, which strategically utilizes per- formance variations across different models for a given language to sample from a pool of teacher models, thereby generating a superior dataset for training effective student models. Our extensive experiments across 15 languages demonstrate the efficacy of our routing strategies, showing that this approach significantly enhances student models' performance over all benchmarks of interest. We observe notable gains in open-ended generation tasks and discriminative benchmarks compared to the traditional single-teacher data generation and training method. Furthermore, through addi- tional analysis of textual characteristics and evaluation on unseen discriminative tasks, we confirm that our instruction fine-tuned students not only retain their initial capabilities but also improve their multilingual generation skills. Our work motivates the merit of strategic sampling, particularly where a diverse pool of models is expected to excel at different parts of the data distribution. We expect arbitrage techniques to provide sizable gains when addressing out-of-distribution problems, rare or often underrepresented parts of the data distribution."}, {"title": "7 Limitations", "content": "One of the limitations of our work is we do not evaluate the impact of multilingual arbitrage on safety. Furthermore, while we consider a wide pool of teachers comprised of very distinct model characteristic our pool of teachers are all in a similar scale of 8 billion parameters. The focus of models in the same size classes was forced by the large computational cost of our experimental set-up. We leave as an important topic of future work exploring the impactof scaling on the gains we observe."}, {"title": "A.1 Teacher Model Pool Details", "content": "We include additional details about each of the single teacher models we benchmark below:\nAya-23-8B [Aryabumi et al., 2024] is an 8B parameter model and a part of the Aya-23 family of multilingual instruction-tuned language models that supports 23 languages, and are based on Cohere's Command model\u2074 and multilingual instruction-style collection [Singh et al., 2024].\nLlama-3-8B-instruct [Dubey et al., 2024] is an open-source instruction-tuned version of the Llama-3-8B pre-trained model. The model is trained on over 15 trillion tokens of publicly available data, with a focus on optimizing the performance across various real-world scenarios, including reasoning and code generation."}, {"title": "A.1.1 Geo-Cluster Training Details", "content": "To train highly performant Geo-clusters, we train an 8B parameter Cohere command model 7 on a data mix of the 15 languages covered by the Geo-Clusters as shown in Table 2. For this data mix, we used both ShareGPT dataset and the Dolly-15k dataset as described by [Aryabumi et al., 2024]. First these two datasets' prompts and completions were translated into these 15 languages, and translations were done using the NLLB-3.3B model [Costa-juss\u00e0 et al., 2022]. In addition, we also included what we call the ShareGPT CommandR+ dataset and the Dolly-15k CommandR+ dataset. For these variants, we use the translated prompts generated completions for the translated prompts using Command R+8. Our datasets cover 15 languages shown in Table 2. Table 4 shows the training data distribution in terms of number of samples used for each Geo-Cluster model training.\nBefore using the geo-clusters as teacher models, we validate performance of our trained Geo-cluster models. We compute average win rates in each language cluster using the held-out multilingual Dolly-200 evaluation dataset [\u00dcst\u00fcn et al., 2024]."}, {"title": "A.2 Language Families", "content": "As we present in Section 3.3, we generate synthetic data in seven diverse languages: Arabic, Chinese, English, French, German, Turkish, and Ukrainian. These languages, representing different language families, are selected to ensure a comprehensive evaluation across various linguistic contexts, detailed in Table 6."}, {"title": "A.3 Router Model Details", "content": "We chose Gemma2-2B\u00ba as our router model for its compact size, performance, and multilingual capabilities. We fine-tuned Gemma2-2B model using the AdamW [Loshchilov & Hutter, 2019] optimizer with an initial learning rate of 5 \u00d7 10-5. We used a linear learning rate scheduler with a 200 warmup steps. We set weight decay to 0 and fine-tuned for 2 epochs.\nTo further improve training efficiency, we also evaluate a smaller mT5-base10 variant with 580M parameters. We finetuned the mT5-base using the Adafactor\u00b9\u00b9 optimizer with 1 \u00d7 10-3 as the learning rate. We fine-tuned for 5 epochs with a train batch size of 32."}, {"title": "A.4 Difference in per-language gains.", "content": "In Figure 11, we compare both reward-based routing and learned routing strategies against random routing for medium-resource and high-resource languages.\nHigh-resource languages [Joshi et al., 2020], English, German, French, Chinese, and Arabic see a 127.6% gain with reward-based routing and a 42.4% gain with learned routing. Medium-resource languages that includes Turkish and Ukrainian, experience greater benefits, with reward-based routing achieving a 134.7% gain and learned routing achieving a 57.1% gain over random routing. These findings suggest that medium-resource languages gain more from strategic sampling than from random routing. Detailed per-language gains are provided in Table 7."}, {"title": "A.5 Discriminative tasks.", "content": "Table 8: Performance of Student Models on held-out Discriminative Tasks: XCOPA, XNLI, and XStoryCloze. The results are averaged over seven languages, highlighting the improvements or declines in performance compared to the base model AYA23."}, {"title": "A.6 Textual Characteristics", "content": "Table 9: Evaluation of textual characteristics across student models in 4 languages: ENGLISH, GERMAN, FRENCH AND UKRANIAN. The number of tokens, Gunning-Fog Index, Rix Index, and Measure of Textual Lexical Diversity (MLTD) for each model highlights the differences in verbosity, readability and lexical diversity. Except for Gemma 2, all students show increase for all metrics."}, {"title": "A.7 Student Model Generations", "content": "A list of example model generations in Turkish from different student models can be found in Table 10.\nTable 10: Comparison of student model responses trained using different methods: Single Aya 23 (best Turkish teacher), random routing, reward-based routing, and learned routing. The Aya 23 student's answer is too short and inadequate, while random-routing generates responses that are repetitive.\nA list of example model generations in English from different student models can be found in Table 11."}]}