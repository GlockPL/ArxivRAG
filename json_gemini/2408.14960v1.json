{"title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress", "authors": ["Ayomide Odumakinde", "Daniel D'souza", "Pat Verga", "Beyza Ermis", "Sara Hooker"], "abstract": "The use of synthetic data has played a critical role in recent state-of-art breakthroughs. However, overly relying on a single oracle teacher model to generate data has been shown to lead to model collapse and invite propagation of biases. These limitations are particularly evident in multilingual settings, where the absence of a universally effective teacher model that excels across all languages presents significant challenges. In this work, we address these extreme difference by introducing \u201cmultilingual arbitrage\u201d, which capitalizes on performance variations between multiple models for a given language. To do so, we strategically route samples through a diverse pool of models, each with unique strengths in different languages. Across exhaustive experiments on state-of-art models, our work suggests that arbitrage techniques allow for spectacular gains in performance that far outperform relying on a single teacher. In particular, compared to the best single teacher, we observe gains of up to 56.5% improvement in win rates averaged across all languages when switching to multilingual arbitrage. We observe the most significant gains for the least resourced languages in our pool.", "sections": [{"title": "1 Introduction", "content": "Throughout our lives, we are guided by many teachers, each contributing distinct insights and expertise to our personal and professional growth. For specialized skills, such as becoming a doctor or mastering culinary arts, we seek out experts who provide targeted guidance. In contrast, synthetic data generation often relies on a single teacher model to impart knowledge to a student model. This approach can lead to the passive transfer of both the strengths and limitations inherent in the teacher model, as highlighted in various studies [Shumailov et al., 2023; Magister et al., 2023; Shimabucoro et al., 2024; Gerstgrasser et al., 2024]. Moreover, it assumes that a single model can effectively teach all relevant skills, which may not always be the case.\nWe introduce the concept of multilingual arbitrage, which leverages differences in performance for a language between multiple models. For each language, we utilize a pool of models as potential teachers. We evaluate different methods for sampling strategically by routing to different models. We then use this optimized distribution to instruction fine-tune a new multilingual model, with the goal of exceeding the performance achieved by relying on a single multilingual model across all languages. This also allows us to ask can you surpass individual model performance by sampling"}, {"title": "2 Methodology", "content": "Our primary goal is to train a high-performing multilingual student model S. Given a set of input prompts P = {pi}1, we generate a corresponding set of completions C = {ci}1 using a pool of potential teacher models T = {T;}j_1. These prompt-completion pairs (pi, ci) will then be used to fine-tune S. For each prompt pi \u2208 P, we aim to identify the specific teacher model T; \u2208 T that produces the highest quality completion ci.\nWe consider that each teacher model Tj may not perform uniformly across all regions of interest R in the data distribution. Therefore, we aim to minimize the empirical error E[P;(R)], where P;(R) represents the performance of teacher model Tj in region R, over the broader distribution D. This ensures robustness and generalization beyond the i.i.d. training sample Diid. This approach allows us to select the most suitable teacher model for each prompt, optimizing the training of our student"}, {"title": "2.1 Routing Methods", "content": "The crux of the problem of multilingual arbitrage is: how do you route prompts to the most calibrated teacher model for each prompt? We exhaustively benchmark different routing strategies which we introduce briefly below:\nFixed Routing. In practice, one might choose a fixed model, such as T2, to process all input prompts in P. This can be reasonable if T2 demonstrates significantly better overall performance for a majority of the prompts. In the multilingual case, this setting is one in which we can pre-determine the best model for each language based on their known strengths, enabling us to use a fixed routing strategy for each prompt deterministically by choosing the appropriate teacher model according to the prompt's language. However, in real-world settings it is not always possible to know what models are relatively strong at different languages in advance.\nReward-based Routing. Next we consider the more realistic setting which assumes that we cannot pre-determine a fixed routing strategy. Instead, we rely on a reward model for routing. For each pi we generate a completion from each of the teacher models in T and then select ci to be the completion with the highest score given by some ranking method. In our case, we use a proprietary reward model (Cohere May 2024) which is competitive with top-scoring state-of-the-art reward models on the RewardBench Leaderboard [Lambert et al., 2024]\u00b9.\nLearned-Routing. The disadvantage of reward-based routing is that it requires generating a full set of M completions for each prompt where M = |T|. As a more efficient alternative, we explore the merits of a learned router which instead trains a router model based on scores produced by the reward model which is proposed by [Lu et al., 2024]. In this method, the router model learns to predict the reward conditioned only on the prompt pi, thereby determining the most suitable teacher model T; without the need to generate multiple completions based upon historical routing trends. The router R(pi) is defined to select the teacher model T; that maximizes the expected reward for a given prompt pi. Formally, for each pi \u2208 P, the selected model Tj is given by:\nTj = arg max R(pi, T).\nTET\nThis approach leverages the complementary strengths of the models in T and ensures that each prompt is routed to the model most likely to produce the highest quality completion. By integrating reward model ranking with query routing, reward-guided Learned-Routing enhances the efficiency of the LLM ensemble, reducing computational overhead while ensuring effective training of the student model S.\nTo train our learned-routing model, we collect a training dataset of diverse prompts and then generate completions from each of the candidate models in the teacher pool. Given a prompt from our training set, we obtain a scalar reward for each candidate model generation as in the following:\nri = {RM(pi, Tj (pi))} = 1, i = 1,..., 1 N\nwhere ri \u2208 RIT. We then train our router R on the training data with Kullback-Leibler (KL) divergence as the loss function:\nL(pi, ri) = KL(R(pi), softmax(ri))."}, {"title": "3 Experimental Setup", "content": "To evaluate the effectiveness of multilingual arbitrage, we compare our method against several baseline methods. Below, we provide a brief overview of the experimental details for each baseline:\nSingle Teachers. This is the most widely adopted framework for incorporating synthetic data into training. In this paradigm a student model is trained on the generations produced from a single teacher model. This setup allows us to explore the question: Is multilingual arbitrage more effective than a single \u201coracle\u201d teacher?\nWe choose single teacher models based on architecture, size, base model type, and the extent of coverage of languages. The selected models, ranging from 7B to 9B parameters, are widely used: Aya 23 [Aryabumi et al., 2024], Llama 3 [Dubey et al., 2024], and Gemma 2 [Team et al., 2024].\nRandom Routing. Next, we consider a router that randomly assigns each prompt pi \u2208 P to teacher model T; \u2208 T, without considering the language or any other specific characteristics of the prompt. This allows us to investigate: Is multilingual arbitrage better than a random selection as to what model is best for a given distribution of interest?\nTranslation. Lastly, our translation baseline addresses whether strategic sampling outperforms simply translating the generations of a single English model into multiple languages. We aim to determine: Does generating synthetic data in the target language outperform translating the best English only data?\nWe generate completions for our English training prompts using our most capable English teacher model, Llama 3. We then translate each of the prompts and completions to the seven languages included in our router experiments."}, {"title": "3.2 Routing Teacher Pools", "content": "Fixed Router Model Pool. In our fixed router experiments, we assume prior knowledge about which models perform best for specific languages. To create this setting, we train several geo-cluster models on 15 languages, each specializing in a subset of languages corresponding to the language families detailed in Table 2. Training in this way allows the models to leverage geographic and linguistic similarities within a language cluster [Kohli et al., 2023; Kew et al., 2023; Tejaswi et al., 2024].\nReward-based and Learned Routing. These methods aims to demonstrate the effectiveness of routing in a varied pool of models with unknown multilingual performance. Hence, we consider a"}, {"title": "3.3 Student Model", "content": "We selected the Aya 23 8B model [Aryabumi et al., 2024] as our student model due to its strong multilingual capabilities which are state-of-the-art (SOTA) for its size class. For each arbitrage technique, we generate synthetic data in seven diverse languages: Arabic, Chinese, English, French, German, Turkish, and Ukrainian. These languages, representing different language families, are selected to ensure a comprehensive evaluation across various linguistic contexts.\nTraining Details. Our student models are trained from a collection of prompts derived from UltraFeedback Binarized Dataset [Tunstall et al., 2023], an English preference dataset of 61,135 pairs. We randomly sampled 10,000 prompts and translated them into the seven target languages using the NLLB-3.3B model, resulting in a total of 70,000 prompts. Completions for each prompt were then generated by the assigned teacher model. We then instruction fine-tune each student model on these 70,000 data points selected through multilingual arbitrage. The training employed a cosine learning rate schedule with a warm-up phase, using a batch size of 64 and an evaluation batch size of 128. The peak learning rate was set at 2.5 \u00d7 10-5, achieved through 128 warm-up steps starting from a learning rate of 0.0, and then decayed back to 0.0."}, {"title": "3.4 Evaluations", "content": "Open-ended Generation Win rates. Beyond traditional NLP tasks, we aim to evaluate the open-ended generation capabilities of the student models, focusing on their ability to produce un-structured and long-form responses. For this evaluation, we use GPT-4 as an LLM-judge to measure pairwise win rates between two model generations. We evaluate on the target language subset of the Multilingual Dolly-200 Eval dataset [Singh et al., 2024; \u00dcst\u00fcn et al., 2024].\nDiscriminative Tasks. To evaluate our models on completely unseen tasks, we follow Muennighoff et al. [2023b] and use XNLI [Conneau et al., 2018], \u0425\u0421\u041e\u0420\u0410 [Ponti et al., 2020], and XStoryCloze [Lin et al., 2021] datasets targeting natural language inference, commonsense reasoning and sentence completion respectively."}, {"title": "4 Results and Discussion", "content": ""}, {"title": "4.1 Multilingual Arbitrage Performance", "content": ""}, {"title": "Section Findings", "content": "\u2022 The multilingual arbitrage methods consistently outperformed the random routing baseline, achieving an average win rate of 51.8% with a notable improvement of 78.9% over random routing.\n\u2022 Arbitrage strategies demonstrate substantial improvements over single teacher models. Reward-based routing achieves the highest average win rate gains at 56.5%, while learned routing provides a notable absolute gain of 25.6% over single teacher models and is significantly more efficient - requiring only one generation per prompt, making it nine times more efficient than reward-based routing.\n\u2022 On unseen discriminative tasks, arbitrage methods provided a larger average improvement of 1.95% over the base student model compared to a 0.98% improvement by single teachers. While fixed routing achieved the highest average gain of 2.50%, all routing approaches effectively enhanced cross-lingual and commonsense reasoning capabilities, showcasing their overall superiority to single teacher models."}, {"title": "Comparison against random routing.", "content": "Our random routing baseline serves as a crucial lower bound that any proposed arbitrage strategy should outperform. This baseline helps us evaluate: Is our multilingual arbitrage technique better than a random guess? We observe that all the multilingual arbitrage methods consistently outperformed the random baseline with average win rate of 51.8% and a notable relative win rate improvement of 78.9%."}, {"title": "Comparison against single \u201coracle\u201d teacher.", "content": "In Figure 2, we show win rates comparing our arbitrage routing strategies to single teacher models. Student models trained with data from these strategies significantly outperformed those using single teacher generations. Specifically, fixed routing achieves an average gain of 34.7%, reward-based routing shows a 56.5% improvement, and learned routing has a 25.6% improvement in average over all single teachers. Notably, Gemma 2 was the best-performing single teacher, yet learned routing still achieved a 3.2% gain over it."}, {"title": "Win-rate Gains are largest for Reward-Based Routing.", "content": "We observed the largest improve-ments against single teachers with reward-based routing, achieving average gains of 56.5%. However, reward-based routing is the least efficient arbitrage method because it requires running inference and generating completions with all models in the pool for each prompt. Although fixed routing and learned routing show some decrease in win-rates compared to reward-based routing, they are significantly more efficient during inference, as they only require inference from one model. In our experiments with a pool of 9 models, reward-based routing requires generating and scoring 9 comple-tions per prompt, while fixed and learned routing need only one generation per prompt. Although learned routing involves an additional call to the router per prompt, this router model is much smaller and more efficient than the teacher, making the call negligible compared to generating from all models in the pool. Notably, learned routing is the most flexible technique, being 9 times more efficient than reward-based routing in this setup and not needing prior knowledge of each model's merits, unlike fixed routing."}, {"title": "Discriminative tasks.", "content": "These tasks reveal similar gaps between the benefits of single teachers and arbitrage techniques. Single teachers provide an average improvement of 0.98% over the base student model (Aya 23), while arbitrage techniques achieve a larger average improvement of 1.95%.\nOverall, on discriminative tasks Fixed Routing model emerges as the most effective, with the high-est average improvement of 2.50% across all tasks, followed by reward based routing with 1.91% improvement indicating their superior ability to enhance cross-lingual and commonsense reasoning capabilities in the evaluated student models. Notably, while fixed routing ranks first in discrimina-tive tasks, it is second in win rate comparisons."}, {"title": "4.2 Language and Routing Analysis", "content": ""}, {"title": "Section Findings", "content": "\u2022 Medium-resource languages benefit more from routing strategies than high-resource lan-guages. Specifically, reward-based routing achieves a 56.1% gain over single teacher models for medium-resource languages, compared to a 35.7% gain for high-resource languages.\n\u2022 Different models are favored for different languages, highlighting the strengths of arbitrage that leverages a diverse model pool. For example, Aya 23 is predominantly used for Ukrainian, Turkish, and Arabic, while Llama3 is favored for English prompts.\n\u2022 Generating synthetic data directly in target language is more effective than translating English-only data. Even a single teacher model achieves a 48.9% increase in win rate com-pared to translations made from English data generated by the top-performing English model."}, {"title": "Difference in per-language gains.", "content": "Figure 4 shows performance gains for medium- versus high-resource languages when using reward-based and learned routing strategies compared to single teacher models such as Aya 23, Llama 3, and Gemma 2.\nMedium-resource languages, including Turkish and Ukrainian, experience greater benefits, with reward-based routing achieving a 56.1% gain and learned routing achieving a 52.2% gain over single teachers. In contrast, high-resource languages"}, {"title": "Comparison of in-language generation vs translation.", "content": "In this section, we explore whether generating synthetic data directly in the target language is more effective than translating the best English-only data. To investigate this, we first generate English data using Llama 3 (the best English model), translate it into other 6 languages, and train a student model with this translated data. We then compare this student model's performance to those trained with Llama 3's single-teacher generations and random-routing generations."}, {"title": "4.3 Textual Characteristics", "content": ""}, {"title": "Section Findings", "content": "\u2022 Routing strategies significantly increase the average number of tokens per generation, with outputs ranging from 160 to 242 tokens, compared to 76 tokens for the base model and around 144 tokens for both random routing and single teacher models, demonstrating that arbitrage methods produce longer text generations.\n\u2022 The Gunning-Fog and Rix indices, which assess text readability and complexity, show higher scores for arbitrage methods compared to the all baselines. Specifically, The Gunning-Fog index increases by 3.28, 1.05 and 0.78 compared to the base student model, single teachers and random routing, respectively, indicating that arbitrage methods lead to more complex text compared to all baseline methods.\n\u2022 The measure of lexical diversity (MLTD) is notably higher for arbitrage methods than for all baseline approaches. Reward-based routing achieves an increase of 7.97, and Learned Routing achieves an increase of 7.1 compared to the base student model. On average, arbi-trage methods show absolute differences of 1.77 compared to single teacher models and 5.46 compared to random routing, highlighting their improved lexical diversity."}, {"title": "5 Related Work", "content": "LLM circularity. The issue of LLM circularity, where models influence other LLMs through distilled data, has recently gained attention. Research to date has focused on two main areas: model degradation via recursive training [Dohmatob et al., 2024a; Briesch et al., 2023; Shumailov et al., 2023] and self-preference in an LLM-as-a-Judge setting.\nInstruction Fine-tuning (IFT). IFT on a large collection of tasks has become a key paradigm for enhancing LLM performance, increasing their utility [Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2021; Min et al., 2021; Ouyang et al., 2022], and enabling generalization to unseen tasks [Wei et al., 2022; Chung et al., 2022].\nMultilingual Synthetic Data. While numerous studies focus on synthetic data generation in En-glish settings [Gao et al., 2023; Anaby-Tavor et al., 2019; Wang et al., 2022; Taori et al., 2023a; Gao et al., 2023], the impact of synthetic data on enhancing multilingual performance is not well under-stood [Kaddour & Liu, 2023; Yadav, 2023; Kramchaninova & Defauw, 2022].\nLarge Language Model Ensemble. Ensembling LLMs is a powerful approach to leverage indi-vidual model strengths while mitigating weaknesses through complementary abilities. The aim is to combine available LLMs to enhance performance consistently across various downstream tasks."}, {"title": "6 Conclusion", "content": "In this work, we introduce the concept of multilingual arbitrage, which strategically utilizes per-formance variations across different models for a given language to sample from a pool of teacher models, thereby generating a superior dataset for training effective student models. Our extensive experiments across 15 languages demonstrate the efficacy of our routing strategies, showing that this approach significantly enhances student models' performance over all benchmarks of interest. We observe notable gains in open-ended generation tasks and discriminative benchmarks compared to the traditional single-teacher data generation and training method. Furthermore, through addi-tional analysis of textual characteristics and evaluation on unseen discriminative tasks, we confirm that our instruction fine-tuned students not only retain their initial capabilities but also improve their multilingual generation skills. Our work motivates the merit of strategic sampling, particularly where a diverse pool of models is expected to excel at different parts of the data distribution. We expect arbitrage techniques to provide sizable gains when addressing out-of-distribution problems, rare or often underrepresented parts of the data distribution."}, {"title": "7 Limitations", "content": "One of the limitations of our work is we do not evaluate the impact of multilingual arbitrage on safety. Furthermore, while we consider a wide pool of teachers comprised of very distinct model characteristic our pool of teachers are all in a similar scale of 8 billion parameters. The focus of models in the same size classes was forced by the large computational cost of our experimental set-up. We leave as an important topic of future work exploring the impactof scaling on the gains we observe."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Teacher Model Pool Details", "content": "Single Teacher Models. We include additional details about each of the single teacher models we benchmark below:\n\u2022 Aya-23-8B [Aryabumi et al., 2024] is an 8B parameter model and a part of the Aya-23 family of multilingual instruction-tuned language models that supports 23 languages, and are based on Cohere's Command model\u2074 and multilingual instruction-style collection\n\u2022 Llama-3-8B-instruct [Dubey et al., 2024] is an open-source instruction-tuned version of the Llama-3-8B pre-trained model. The model is trained on over 15 trillion tokens of publicly available data, with a focus on optimizing the performance across various real-world scenarios, including reasoning and code generation."}, {"title": "A.1.1 Geo-Cluster Training Details", "content": "To train highly performant Geo-clusters, we train an 8B parameter Cohere command model 7 on a data mix of the 15 languages covered by the Geo-Clusters as shown in Table 2. For this data mix, we used both ShareGPT dataset and the Dolly-15k dataset as described by [Aryabumi et al., 2024]. First these two datasets' prompts and completions were translated into these 15 languages, and translations were done using the NLLB-3.3B model [Costa-juss\u00e0 et al., 2022]. In addition, we also included what we call the ShareGPT CommandR+ dataset and the Dolly-15k CommandR+ dataset."}, {"title": "A.2 Language Families", "content": "As we present in Section 3.3, we generate synthetic data in seven diverse languages: Arabic, Chinese, English, French, German, Turkish, and Ukrainian. These languages, representing different language families, are selected to ensure a comprehensive evaluation across various linguistic contexts,"}, {"title": "A.3 Router Model Details", "content": "Training Details. We chose Gemma2-2B\u00ba as our router model for its compact size, performance, and multilingual capabilities. We fine-tuned Gemma2-2B model using the AdamW [Loshchilov & Hutter, 2019] optimizer with an initial learning rate of 5 \u00d7 10-5. We used a linear learning rate scheduler with a 200 warmup steps. We set weight decay to 0 and fine-tuned for 2 epochs.\nComparison of mT5 and Gemma 2 as Router Model. We chose Gemma2-2B as the final candidate for our learned router model. The student model trained on the dataset routed by Gemma2-2B demonstrated significant improvements, particularly against the strong Gemma2-9B single teacher model. Gemma2-2B was used as the learned router in all our experiments."}, {"title": "A.4 Difference in per-language gains.", "content": "In Figure 11, we compare both reward-based routing and learned routing strategies against random routing for medium-resource and high-resource languages."}, {"title": "A.5 Discriminative tasks.", "content": ""}, {"title": "A.6 Textual Characteristics", "content": ""}, {"title": "A.7 Student Model Generations", "content": "A list of example model generations in Turkish from different student models can be found in Table 10."}]}