{"title": "The GPT Surprise: Offering Large Language Model Chat in a\nMassive Coding Class Reduced Engagement but Increased\nAdopters' Exam Performances", "authors": ["Allen Nie", "Yash Chandak", "Miroslav Suzara", "Ali Malik", "Juliette Woodrow", "Matt Peng", "Mehran Sahami", "Emma Brunskill", "Chris Piech"], "abstract": "Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, es-\npecially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of\ninterface is readily available to students and teachers around the world, yet relatively little research has\nbeen done to assess the impact of such generic tools on student learning. Coding education is an interest-\ning test case, both because LLMs have strong performance on coding tasks, and because LLM-powered\nsupport tools are rapidly becoming part of the workflow of professional software engineers. To help\nunderstand the impact of generic LLM use on coding education, we conducted a large-scale randomized\ncontrol trial with 5,831 students from 146 countries in an online coding class in which we provided some\nstudents with access to a chat interface with GPT-4. We estimate positive benefits on exam performance\nfor adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a\nsignificant average decrease in exam participation. We observe similar decreases in other forms of course\nengagement. However, this decrease is modulated by the student's country of origin. Offering access to\nLLMs to students from low human development index countries increased their exam participation rate\non average. Our results suggest there may be promising benefits to using LLMs in an introductory coding\nclass, but also potential harms for engagement, which makes their longer term impact on student success\nunclear. Our work highlights the need for additional investigations to help understand the potential\nimpact of future adoption and integration of LLMs into classrooms.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have the potential to substantially impact education. While there are many\nways that LLMs could be used to support stakeholders in the educational ecosystem, including training\ntutors (Markel et al., 2023), creating content or lesson plans for teachers (Pardos and Bhandari, 2023), and\nhelping parents better support their kids on their homework (Gemini et al., 2023), one key opportunity is\nto support students directly. LLMs can act like tutors, supporting the student in real-time with natural\nlanguage interactions (Duolingo, 2023; Khan Academy, 2023), which, based on prior evidence (Bloom, 1984),\ncould lead to substantially more personalized and effective learning. On the other hand, there is substantial\nconcern that students may use LLMs as a substitute for their own learning, such as by using LLMs to\ngenerate homework solutions, though some suggest this concern may so far be overestimated (Lee et al.,\n2023). Indeed, many students are already using generic (not education-specific) LLMs, such as the publicly\navailable chatbot interfaces provided by large companies, for their own education (Cu and Hochman, 2023).\nHowever, we know very little about the impact on student learning when students get support from such a\ngeneralist tool at the moment.\nIn this paper we aim to start answering this question through an experiment in which we provided access to\na popular and powerful general LLM, GPT-4, to students in a large online coding course. Coding education\nis an area where one might expect LLM support to be particularly beneficial for two reasons. First, though\nLLMs already demonstrate proficiency in a wide range of tasks that can assist a human user, they are known\nto have strong performance in writing computer programs. Second, it is likely that programmers may be\nexpected to use LLM-based coding support tools, such as Github Copilot\u00b9 or JetBrains AI Assistant, in a\nsoftware engineering job. For these two reasons, one might expect that interacting with a LLM could support\na student's learning of both coding concepts, and their ability to leverage LLMs to produce coding. However,\nthough there is evidence that there is a significant productivity boost for the expert programmers (Noy and\nZhang, 2023), we know little about how using LLMs might impact a coding beginner.\nTo investigate the potential impact of LLMs on students learning to code, we conducted a randomized control\ntrial in which we provided a subset of students in a massive free online coding class with over 8,762 students\nin 146 countries access to a course-specific interface to GPT-4. We provided learners with information about\nthe potential limitations and benefits of using LLMs and designed prompts to prevent students from seeking\ndirect solutions from GPT-4 (Section 2). To understand how LLMs impacted student learning, we analyzed\nstudent engagement in the course, measured by exam participation and validated by homework completion\nand section attendance, and their performance on the (optional) exam (Section 3.1). The non-compulsory\nnature of the course made it straightforward to measure changes in engagement, but more complex to\nunderstand impacts on student learning. We used a causal inference estimator to understand the potential\nlearning benefit for the students (Section 3.2). We discuss our findings in Section 4.1 and 4.2 but highlight\nthe core insights here:\n1. Low Adoption Rate: In contrast to concerns that students may overuse large language models, we\nfound only a small number of students (14.2%) who were provided access to our course GPT interface,\nand emailed to alert them to this opportunity, chose to use our GPT interface. This might suggest that\nstudent adoption and usage of LLMs follow a typical technology adoption curve (Rogers et al., 2014).\n2. LLM Lowered Course Engagement As part of the course, all students were offered an opportunity\nto take an optional diagnostic exam in a 3-day period. We found that giving students access to and\nadvertising GPT-4 led to a substantial, statistically significant decrease in their exam participation. The\neffect of access to GPT was negative 4.3 percentage points. Access and advertising also led to a decrease"}, {"title": "3.1 Advertisement of GPT-4 Leads to Lower Exam Participation Rate", "content": "In Code-in-Place, student activity is tracked in three ways: their submission of weekly homework, their\nattendance for weekly sections, and whether they watched video lectures or interacted with the course\nmaterials. The diagnostic exam is voluntary, though students who take the exam receive an additional\ndistinction on their course completion certificate. Among all activities, the diagnostic exam and homework\ndemand the most time.\nWe found that only 44.1% of the students in the experiment group took the diagnostic exam, compared\nto 48.5% of the students in the control group who took the exam (Figure 3(b)), representing a significant\ndifference in means (\u2206=-4.3pp, SE=1.3, 95% CI=[-6.9, -1.7]). This effect is very highly significant using a\ntraditional t-test (Unadjusted P=.001) and is still highly statistically significant after applying Bonferonni\ncorrection\u00b2 (P=.020). Similarly, we found a significant difference in means for week 6 homework completion\nrate between the two groups (\u2206=-4.6pp, SE=1.3, 95% CI=[-7.2, -1.9], P=.01, Unadjusted P<.001), though\ninitially there was no discernible difference between students in the experimental and control groups before\nweek 4 (the start of the experiment). Figure 3(a) shows the students' weekly homework completion rate for\nthe experiment and control group: note a homework is considered \"complete\" when a student has solved all\nthe problems it contains, which range from 1 to 6 per week.\nAlthough the trend of students engaging less with course activities that require more time commitment\nis common (Kizilcec et al., 2013), offering students novel technology leading to less engagement is highly"}, {"title": "3.2\nEstimated Positive Benefit of GPT-4 on Learning For Adopters", "content": "In the previous section, we observed that providing access to GPT-4 had a negative impact on overall student\nengagement in this course. In this section, we estimate the impact of GPT-4 access and usage on student\nlearning outcomes, as measured by exam scores. Specifically, we compute two separate estimates:\n(E1): Advertisement Effect: Is there a change in exam performance due to advertising and offering\nstudents access to GPT-4? This effect can be measured by computing the difference between the\nmeans of student exam scores in the experiment and the control group, accounting for missing data."}, {"title": "3.3 GPT-4 Usage Behaviors", "content": "In order to understand the impact of offering and using GPT-4, we analyze student usage behaviors in\ntwo ways. First, we analyze the characteristics of the students who are using GPT-4. We used logistic"}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Limitations", "content": "A Particular Course, with Particular Students, at a Particular Moment in Time\nA common limitation of educational research, even randomized control trials, is that experiments are con-\nducted in a particular context. In this case, we ran the student in a particular domain (computer science)\nwith a particular cohort of students at a particular moment in time. Consequently, the findings from this\nstudy only directly apply to this particular context. The same intervention, if studied in different learning\nenvironments, may produce different results. Our course's unique characteristics likely shape the educational\nexperience and outcomes observed: (1) The course is at-will in contrast to many k-12 or university experi-"}, {"title": "4.2 What Caused Disengagement?", "content": "In the randomized control trial, there was a notable difference in engagement on a two-week time scale\nbetween learners with and without access to ChatGPT. This effect is influenced by student age, coding\nexperience, and HDI of the student's country. We note that it is surprising to see a drop in engagement in\neducation simply from getting access to an optional tool. Many learning science experiments are \"doomed\nto succeed\" because of the strong novelty effect in education (Makel and Plucker, 2014). This novelty effect\nis seemingly non-existent when providing an LLM chat interface to students. On their own, these results do\nnot suggest why there was a difference in engagement. In this section, we present a few theories on such a\ncausal mechanism.\nJob Threat Hypothesis\nOne hypothesis for the impact on engagement is that access and advertisement to GPT confronted learners\nwith a threat to their prospects of getting a programming job. Several studies have shown that learners who\nmake a connection between academic courses and activities and future versions of themselves experience\nincreased motivation towards their learning (Leondari, 2007; Lens et al., 2001; Greene and DeBacker, 2004),\nespecially in intro to programming courses. Specifically, Peteranetz et al. (2016) showed that a difference\nin a student's perception of an instrumental connection between course participation and employment was\na significant predictor of standardized course grades. Under the Job Threat Hypothesis, interacting with\nChatGPT decreases a student's instrumental connection between learning and obtaining employment. When\nthe student either uses ChatGPT or reads the advertisement, the student supposes that there may be fewer\njobs and there will be steeper competition for those limited roles. At the very least, a student might perceive\na greater uncertainty around jobs.\nThis hypothesis is supported by the observation that the biggest engagement impact of access and advertising\nof GPT was on people who demographically match those who are applying for jobs - people in the job-seeking\nage (22-40) and middle-experience programmers. The instrumental connection between learning to code and\njobs is especially pertinent in the Code in Place course. Of the students in the class, 46.8% of students list,\n\"I want to get a job as a programmer\" as a motivation for taking the class upon submitting their application.\nThe percentage of students who listed jobs as motivation is even higher for learners of job-seeking age (49.0%).\nIn contrast, only 43.7% of learners outside that range listed jobs as a motivation. As a corollary, there was\na 1.26 percentage point decrease in exam participation among learners in the experimental condition who\nlisted a job as a motivation for taking the course (43.4% exam participation) compared to those who did not\nlist jobs (44.7% exam participation).\nAI Mistrust Hypothesis\nArtificial intelligence's integration into society has engendered polarized opinions. IPSOS ran a global study\nof perceptions of AI at the same time that the Code in Place experiment ran, where they interviewed 22,816\nadults under the age of 75 across 31 countries (Ipsos, 2023). They found that 52% of respondents were\nnervous regarding AI, and 46% did not agree that there were more benefits than drawbacks. These broad\nphenomena appeared to be playing out inside the course. In conversations with learners in Code in Place,\nwe noticed a surprisingly high level of distrust in AI. Students voiced concerns which range from concerns"}, {"title": "4.3 What Caused Improved Exam Scores?", "content": "Our analysis of the adopters' interactions with GPT-4 shows that those who used GPT-4 were generally\nusing it in a constructive way to better their understanding of the material and the field at large. This could\nhave improved their understanding, which transferred to test performance gains. An interesting question\nfor future study is how the type of use of GPT-4, and the amount, might impact learning outcomes. This\nparticular result is less surprising. There is a long-held belief in education that direct one-on-one tutoring\nsubstantially improves student ability (Bloom, 1984). LLM chat seems to be a reasonably useful autonomous\ntutor. One concern was that access to an AI tutor at any point could serve as too much help, sometimes\nlabeled the \"Scaffolding Paradox\" (Gillespie and Hald, 2017). Instead of grappling with problems, iterating\nthrough solutions, and learning from errors- a crucial cycle in developing programming acumen- students\nmay shortcut this process by seeking immediate answers from AI, thereby steepening their learning curve in\nthe long run. Fortunately, the strong exam performance of students who adopted ChatGPT suggests that\noverreliance on AI, to the detriment of learning, was not a dominant mechanism in our setting."}, {"title": "5 Related Work", "content": "Recent studies have explored the effects of large language models on education. One branch of work focuses\non making observational and qualitative studies of how LLMs are used in different educational settings.\nButgereit and Martinus (2023) launched a WhatsApp bot that connects to GPT-4 to tutor mathematics at\nthe university level in the Arabic language, particularly for areas going through periods of violent unrest.\nChoi et al. (2023) collected data on how teachers from the poorest schools in Sierra Leone used GPT-4 to\nassist their teaching. They found that 48% of the questions asked to GPT-4 were about concept clarifications,\nsimilar to our finding as well. Liu et al. (2024) integrated GPT-4 into a university-level class (CS50) and\nfound that students enjoyed the experience and found LLMs to be helpful.\nAnother branch of work focuses on designing randomized control trial (RCT) experiments to understand\nthe effect of LLMs on educational outcomes. Kumar et al. (2023) conducted a large-scale experiment and\nfound that GPT-generated hints positively impacted learning compared to not providing hints at all. The\nparticipants in this study were crowd-sourced site workers rather than individuals from a traditional class-\nroom or realistic educational environment. Prihar et al. (2023) showed that GPT-generated hints have lower\nqualities compared to human tutor-written hints when rated by other teachers. Pardos and Bhandari (2023)\nshowed that when students are using these hints for learning, learning gains were only statistically significant"}, {"title": "6 Method: Local Average Treatment Effect", "content": "There is a long history in economics and social sciences of estimating causal effects by using instrumental\nvariables to account for possible self-selection into a treatment. Using this strategy, economists have been\nable to estimate that there is a negative impact on lifetime earnings when people are drafted, and gain\nevidence that increasing years of education can result in higher salaries (Angrist, 1990; Card, 1993; Angrist\net al., 1996). Even though we want to report the expected (average) treatment effect for any person who\nreceives the treatment, due to the self-selection effect, we can only make valid statistical statements about\nthe people whose resulting decision (to uptake a treatment) are impacted by an instrumental variable. To\ndistinguish from the average treatment effect, we report the local average treatment effect - local in the sense\nthat the treatment effect is averaged over (what the literature refers to as) compliers, those who complied\nwith the treatment or control indicated by the instrumental variable (in our case, using GPT-4 after being\ngiven access).\nNotation To describe the causal structure of our experiment setup, we introduce the following notations.\nEach student i is represented by $(X_i, Z_i, W_i, Y_i)$, where $X_i \\in \\mathbb{R}^d$, a vector of observed d covariates (demo-\ngraphic characteristics, pre-enrollment qualification test, general activities in class before the experiment,\netc.); $Z_i \\in \\{0,1\\}$ is the instrumental variable and indicates whether the student is informed of the existence\nof free GPT access in class via an email and a quick-access button is provided in the homepage sidebar;\n$W_i \\in \\{0,1\\}$ describes whether the student uses the in-class GPT-4 interface that we provided; $Y_i \\in \\mathbb{R}$ de-\nnotes the observed outcome, the i-th student's exam score. Folllowing the potential outcomes framework, we\nuse $Y_i(0)$ and $Y_i(1)$ to the denote potential outcomes under treatment W for individual i, so that $Y_i = Y_i(W_i)$.\nNote the fundamental problem of causal inference is that we only observe one potential outcome for each\nindividual i.\nWe denote each student's decision to use GPT-4 when given access (C stands for compliance) as $C_i =$\n$\\{W_i(0), W_i(1)\\}$, where $W_i = W_i(Z_i)$. We define the exam score improvement (the treatment effect) of\nGPT-4 for the students who complied as $\\gamma$, following the setup from Angrist et al. (1996):\n$\\gamma := E[Y_i(1) - Y_i(0) | C_i = complier]$.\n$\\gamma$ is the Local Average Treatment Effect (LATE). The following assumptions suffice for LATE to be identi-\nfiable:\nAssumption 1 (Uniformly Random Nudges). $Z \\perp X$\nAssumption 2 (Relevance). $Z \\not\\perp W | X$\n${^{5}W(0)}$ is short for $W_i(Z_i = 1)$, which means student i has been given access and nudged to use GPT-4. $W_i(Z_i = 1) = 1$\nindicates the student has been emailed and given access and chose to use GPT-4 this student is a compiler in the causal\ninference terminology. In earlier parts of the paper, to use more familiar terminology beyond the causal estimation community,\nwe used the word \"adopters\"."}, {"title": "7 Conclusion", "content": "Our aim is to understand the impact of introducing access to support of chat-based Generative AI system\nin an introductory online programming course. This type of interface is readily available to students and\nteachers around the world, but there are many important questions about the impact of such tools on\nengagement and learning. To help tackle such questions, we conducted a large-scale randomized control trial\nstudy on an introductory programming class with 5,831 students from 146 countries in which we provided\nsome students with access to a GPT-4 tool for the course. We estimate positive benefits on exam performance\nfor adopters, students who used the tool, but over all students the advertisement led to an average decrease\nin participation in several class elements. The reason for the decrease in engagement is not yet fully known,\nand may be influenced by the particular advertisement framing used in sharing access to the tool. Our\nresults suggest there may be promising benefits to using LLMs in introductory courses, but also potential\nharms for engagement, which may have longer term implications on student success. Our work highlights\nthe need for additional investigations to help understand the potential impact of future large adoption and\nintegration of LLMs into classrooms."}, {"title": "A Appendices", "content": ""}, {"title": "A.1 Additional Student Covariate Interaction Effect", "content": "We show additional interactions between student covariates and their exam participation in Figure A.1.\nWe note that there doesn't seem to be a difference in exam participation for students who are not fluent\nin English. Nonetheless, as the main instructional materials for this course are in English, most students\nwho decide to apply already have high proficiency in the language, regardless of whether it is the primary\nlanguage in their home countries. For gender, we notice a similar gap between experiment and control.\nFor section attendance, we are not able to draw meaningful conclusions beyond what we have observed so\nfar."}, {"title": "A.2\nAdditional Details on Trial Assignment", "content": "At the beginning of April 24th, 8,762 students enrolled in the class. However, we don't want to offer access\nto ChatGPT too early because some empirical work in education showed that providing too many hints too\nearly might hurt a student's learning progress (Gillespie and Hald, 2017). We determine a student to be\n\"active\" by looking at whether they have completed all Week 1 assignments. We end up with 5,831 students\nin our randomized control trial. We then sent an email to 3,581 students. 2,778 students (77.6%) opened the\nemail. 539 students (15.1%) clicked on the link to our custom ChatGPT interface. We obtained institutional\nreview board (IRB) approval for conducting this experiment."}, {"title": "A.3 Full Description of Student Covariates", "content": "We report the basic statistics of the student population in our randomized control trial in Table 1. Here, we\nprovide some additional information about them. Note that we are not reporting these distributions over the\nentire course's student population. They are only computed on the 5,831 students who were deemed active\nby week 1 and were included in our randomized control trial. In order to make sure all of the covariates\nwe use are independent of the treatment (access to our custom ChatGPT), we only use either demographic\ninformation or a record of the student prior to the start of the experiment (week 3).\n\u2022 Application Score (Mean=48.2, SD=11.4, Median (IQR)=47.2 (41.0-55.0), Max=103.0): This is an"}, {"title": "A.4 Diagnostic Exam Details", "content": "A diagnostic exam is administered near the end of the course. All students enrolled in the class received an\nemail notifying them that the exam is available on May 26th, 2023, at 9:43 am EDT. The email was sent to\n9,573 students. 7,084 students opened the email (74.4%), and 1,748 students clicked on the diagnostic exam\nlink in the email (18.4%). The student will see a welcome message once they open the exam page:\nThis diagnostic has five questions. Complete each question, to the best of your ability.\nWhen you are done, hit the blue Submit button. You can change questions using the\nnumbers in the navbar above. You may go back and forth between questions.\nYou have 3 hours to complete it from the time you hit start. The diagnostic is designed\nto only take 50 minutes. Time does not pause if you close the diagnostic and come back\nto it. For pedagogical purposes, we do allow you to run your programs, but we will not be\ngiving you live feedback as to whether or not your program works.\nQuestions cover basic concepts of Python knowledge, control flow, arithmetic, and using Python canvas to\nanimate objects. No official score is given to the students. The exam was graded with unit tests that verified\neach part of the student code, and a rubric system was used to create detailed feedback.\nWe use a simple normalization rule to convert a student's diagnostic exam feedback to a score that has a\nrange of 0 to 100. If a student has completed all exam questions and received 0 feedback, they get a score\nof 100. If a student did not submit a particular question, we count it as if the student received the maximal\nnumber of feedback from that question (i.e., if a student misses Q2, we would treat it as if they received 12\nfeedback). If a student gets s number of feedback in total, their score is 1-s/73. We verified our conversion\nwith the course staff and obtained their approval."}, {"title": "A.5 Additional Details on Statistics", "content": "In the main result section, we report two p-values. One p-value is the family-wise error rate (FWER)\ncontrolled p-value, which we denote as P in the main text. We use Bonferroni correction to control for\nfamily-wise error rate. In addition, we report the unadjusted p-value per comparison, which we denote as\n\"unadjusted P\". In all the figures, we report the significance level based on the Bonferroni-corrected P, which\nis calculated by multiplying 15 to unadjusted p-values. For GPT-4 usage patterns reported in Section 3.3,\nwe follow the guideline that the p-values for logistic regression analysis do not need to be additionally\nadjusted.\nFor confidence interval, when we use a difference-in-means (DM) estimator for computing A between two\ngroups without missing values (Section 3.1), we use the standard confidence interval calculation for the\nDM estimator discussed in Wager (2020). When we have to deal with missing data, for both the DM"}, {"title": "A.6 Impute for Missingness (Regression Model)", "content": "All of these features are standardized, which means for feature X with empirical mean $\\mu$ and standard\ndeviation $\\sigma$, we use $X = \\frac{X-\\mu}{\\sigma}$. We first discuss how we conduct our model selection (hyperparameter\nsearch) and then discuss how we imputed for missing values. We conducted a search over a few model\nclasses: linear regression, Ridge regression, Lasso regression, a 2-layer neural network with 128-dimension\nhidden size and tanh activation function, and a random forest regressor. All the models are implemented\nin sklearn. We first split the dataset into a training and a holdout set and used 5-fold cross-validation to\nselect the best model from the training set. We then compute the mean-squared error (MSE) on the holdout\ntest set. Because we do not have access to students who did not participate in the exam, we only train and\nevaluate our models on students who took the exam (on both the training and holdout set)."}, {"title": "A.7 Logistic Regression Analysis of GPT Usage", "content": "In Section 3.3, we used a logistic regression model to understand what kind of students are more likely to\nbecome adopters of LLMs if we offer to them in a massive online class like ours. Each feature has been\nstandardized. We report the coefficients in Table A.2."}, {"title": "A.8 Deployment Implementation Details", "content": "Student Access to GPT Interface within and outside the course We do not record students'\nbrowsing histories and, unfortunately, do not know if they have accessed the publicly available GPT interface\nprovided by OpenAI. However, a survey conducted by a concurrent study on the same course asked if the\nstudents had used the ChatGPT interface during the course period. Only 2% of the students responded that\nthey did.\nUsing GPT-4 to Cheat on the Diagnostic Exam Despite our effort to give a stern warning to the\nstudents and let them know that their conversation with the GPT is monitored and visible to course staff,\nwe conducted an analysis to see if the students copied and pasted exam questions into our custom interface.\nWe did not find any evidence that the students used our custom interface to cheat. It is worth pointing out\nthat, unlike a university course, this free online class does not incentivize students to cheat on exams the\ncourse does not offer a letter grade, and the exam does not provide students a score only feedback on how\nthey did on each of the problems. The final course completion certificate only mentions if they had made an\nattempt on the exam."}]}