{"title": "The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters' Exam Performances", "authors": ["Allen Nie", "Yash Chandak", "Miroslav Suzara", "Ali Malik", "Juliette Woodrow", "Matt Peng", "Mehran Sahami", "Emma Brunskill", "Chris Piech"], "abstract": "Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, es- pecially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of interface is readily available to students and teachers around the world, yet relatively little research has been done to assess the impact of such generic tools on student learning. Coding education is an interest- ing test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. We estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have the potential to substantially impact education. While there are many ways that LLMs could be used to support stakeholders in the educational ecosystem, including training tutors (Markel et al., 2023), creating content or lesson plans for teachers (Pardos and Bhandari, 2023), and helping parents better support their kids on their homework (Gemini et al., 2023), one key opportunity is to support students directly. LLMs can act like tutors, supporting the student in real-time with natural language interactions (Duolingo, 2023; Khan Academy, 2023), which, based on prior evidence (Bloom, 1984), could lead to substantially more personalized and effective learning. On the other hand, there is substantial concern that students may use LLMs as a substitute for their own learning, such as by using LLMs to generate homework solutions, though some suggest this concern may so far be overestimated (Lee et al., 2023). Indeed, many students are already using generic (not education-specific) LLMs, such as the publicly available chatbot interfaces provided by large companies, for their own education (Cu and Hochman, 2023). However, we know very little about the impact on student learning when students get support from such a generalist tool at the moment.\nIn this paper we aim to start answering this question through an experiment in which we provided access to a popular and powerful general LLM, GPT-4, to students in a large online coding course. Coding education is an area where one might expect LLM support to be particularly beneficial for two reasons. First, though LLMs already demonstrate proficiency in a wide range of tasks that can assist a human user, they are known to have strong performance in writing computer programs. Second, it is likely that programmers may be expected to use LLM-based coding support tools, such as Github Copilot\u00b9 or JetBrains AI Assistant, in a software engineering job. For these two reasons, one might expect that interacting with a LLM could support a student's learning of both coding concepts, and their ability to leverage LLMs to produce coding. However, though there is evidence that there is a significant productivity boost for the expert programmers (Noy and Zhang, 2023), we know little about how using LLMs might impact a coding beginner.\nTo investigate the potential impact of LLMs on students learning to code, we conducted a randomized control trial in which we provided a subset of students in a massive free online coding class with over 8,762 students in 146 countries access to a course-specific interface to GPT-4. We provided learners with information about the potential limitations and benefits of using LLMs and designed prompts to prevent students from seeking direct solutions from GPT-4 (Section 2). To understand how LLMs impacted student learning, we analyzed student engagement in the course, measured by exam participation and validated by homework completion and section attendance, and their performance on the (optional) exam (Section 3.1). The non-compulsory nature of the course made it straightforward to measure changes in engagement, but more complex to understand impacts on student learning. We used a causal inference estimator to understand the potential learning benefit for the students (Section 3.2). We discuss our findings in Section 4.1 and 4.2 but highlight the core insights here:\n1. Low Adoption Rate: In contrast to concerns that students may overuse large language models, we found only a small number of students (14.2%) who were provided access to our course GPT interface, and emailed to alert them to this opportunity, chose to use our GPT interface. This might suggest that student adoption and usage of LLMs follow a typical technology adoption curve (Rogers et al., 2014).\n2. LLM Lowered Course Engagement As part of the course, all students were offered an opportunity to take an optional diagnostic exam in a 3-day period. We found that giving students access to and advertising GPT-4 led to a substantial, statistically significant decrease in their exam participation. The effect of access to GPT was negative 4.3 percentage points. Access and advertising also led to a decrease"}, {"title": "2 Experiment Design", "content": "To evaluate the impact of providing GPT-4 to coding beginners taking a free online class, we conducted a randomized control trial study to analyze the impact of GPT-4 by comparing a series of course activities and learning outcomes between the experiment and control group.\nOur experiment consists of 5,831 students (out of 8,762 students who were given approval to enroll in class) who were determined to be active after the first week. We randomly assigned 60% of the students (n=3,581) to the experiment condition and 40% of the students (n=2,250) to the control condition. We give slightly more students access to GPT-4 because we expect the measurable educational outcome to have a high variance for the experiment group. Students in the experiment group were given access to a customized in-class ChatGPT-like interface with GPT-4 at the start of week 4 in the course . The students saw an altered home page interface with a ChatGPT button on the sidebar, and we sent an email notification to inform students of its presence (see Figure A.4). After 9 days (at the end of week 5), all students were invited to take an optional 4-hour midterm exam as part of a regular course offering. Since this exam is not mandatory for a student to earn their course certificate, 45.8% of students took the exam. Additionally, not every student who was given access to GPT-4 interacted with GPT-4. Only 14.2% (N=510) students in the experiment group used GPT-4 prior to the midterm exam. We show this whole process"}, {"title": "3 Results", "content": "To assess educational impact, we focus on two outcome measures: (1) Student engagement, especially whether or not the student takes the diagnostic exam, and (2) the score they obtained taking the diagnostic exam. The exam consists of 5 multi-part coding problems of increasing difficulty."}, {"title": "3.1 Advertisement of GPT-4 Leads to Lower Exam Participation Rate", "content": "In Code-in-Place, student activity is tracked in three ways: their submission of weekly homework, their attendance for weekly sections, and whether they watched video lectures or interacted with the course materials. The diagnostic exam is voluntary, though students who take the exam receive an additional distinction on their course completion certificate. Among all activities, the diagnostic exam and homework demand the most time.\nWe found that only 44.1% of the students in the experiment group took the diagnostic exam, compared to 48.5% of the students in the control group who took the exam , representing a significant difference in means (\u2206=-4.3pp, SE=1.3, 95% CI=[-6.9, -1.7]). This effect is very highly significant using a traditional t-test (Unadjusted P=.001) and is still highly statistically significant after applying Bonferonni correction\u00b2 (P=.020). Similarly, we found a significant difference in means for week 6 homework completion rate between the two groups (\u2206=-4.6pp, SE=1.3, 95% CI=[-7.2, -1.9], P=.01, Unadjusted P<.001), though initially there was no discernible difference between students in the experimental and control groups before week 4 (the start of the experiment) . Figure 3(a) shows the students' weekly homework completion rate for the experiment and control group: note a homework is considered \"complete\" when a student has solved all the problems it contains, which range from 1 to 6 per week.\nAlthough the trend of students engaging less with course activities that require more time commitment is common (Kizilcec et al., 2013), offering students novel technology leading to less engagement is highly uncommon because the novelty effect has long been established in education research as a strong factor in increasing student engagement (Makel and Plucker, 2014). In fact, in the same course, all other experiments with the same advertisement mechanisms have produced a result where the novel feature drives up engage- ment (Markel et al., 2023; Demszky et al., 2023; Bigman et al., 2021). Such a decrease in engagement is surprising.\nThis trend of disengagement has some surprising interactions with student demographics. As part of the student application process, we ask them to self-report some demographic information, including their age, gender, country of residence, prior experience with coding, and English fluency. Since we assigned the students randomly to the experiment and control group, examining the engagement difference conditioned on demographic features will not bias the outcome measurement. The general distribution of these covariates can be found in Table 1.\nEngagement Increase For Low HDI Countries: The United Nations computes a Human Development Index (HDI) for each country every year. HDI is a measure that summarizes the average health, knowledge (schooling and education), and standard of living for people living in that country. HDI is often used as a metric that describes the level of development for countries (less vs. more developed countries). Prior work on MOOCs has found that students in countries with higher HDI tend to earn certificates in MOOCs at a higher rate than students in lower HDI countries (Hansen and Reich, 2015), and prior studies have worked to close this gap (Kizilcec et al., 2017). Interestingly, we found that offering students access to GPT-4 seemed to increase the student exam participation rate for students from low HDI countries. There are 325 students from 16 low HDI countries in Asia (Pakistan, Yemen, Afghanistan), North America (Haiti), and Africa (Nigeria, Rwanda, Ethiopia, Tanzania, Sudan, Mozambique, Mali, Madagascar, Uganda, Benin, Gambia, Senegal). 180 students remained active after week 1. In our trial, 111 students were put in the experiment group (61.7%) and 69 students in the control group. In the experiment group, 16 students (14.4%) interacted with GPT-4 - almost identical to the GPT-4 usage for the general student population (14.2%).\nSurprisingly, the disengagement trend reversed for the students who are from the low HDI countries: the exam participation in the experiment group is notably higher at 42.3% compared to participation in the control group, at 27.5% (see Figure 4(a)). The effect size is small to medium (Cohen's d=0.31), which is notable. However, our sample size is small, and the raw Student's t-test (Unadjusted P=.045) is not statistically significant after adjusting for multiple hypothesis tests (\u0394=14.8pp, SE=7.2, 95% CI=[0.6, 29.0], P=.680). We also computed the week 6 homework completion rate for students from low HDI countries and found a similar trend. Still, these results are especially encouraging as there is an ongoing effort in the global education community to use GPT-4 to boost educational resources in underserved areas due to unrest, war, poverty, or extreme events. (Butgereit and Martinus, 2023; Choi et al., 2023).\nAge and Coding Experience Suggest a \"Middle Experience Gap\": Student disengagement may also mediated by age (Mean=31.4, SD=10.4, Median (IQR)=29.0 (23.0-37.0), Max=84.0) and prior coding experience (Mean=5.1, SD=4.4, Median (IQR)=4.0 (2.0-8.0), Max=18.0). We divide the students based on their age into four groups: 18-22 (college age) (N=1,241) and then 20s (N=1,947), 30s (N=1,588), and 40s and above (N=1,055). A difference between exam participation is suggested but only for students between 23-40 (Figure 4(b)). We discuss why this might occur in Sec 4.2. A critical factor to note is that students take Code-in-Place for different reasons, and job eligibility is a particularly common reason for students aged 23 to 40. We notice a similar phenomenon for students with different prior coding experiences, which we ask students to self-report and rate them on a scale of 0-18. We divide students into three groups: no experience 0-5 (N=3,180), less experienced 6-10 (N=2,037), and more experienced >10 (N=614). We notice a significant decrease in engagement for students in the less experienced group (\u2206=-7.5pp, SE=2.3, 95% CI=[-12.0, -3.1], P=.014, Unadjusted P<.001). We show the difference between the experiment and control group among students with different coding experiences in Figure 4(c).\nOther Demographics We did an exploratory analysis of other demographics, such as English fluency and gender. English fluency is correlated with HDI (r=0.10), and similar to prior coding experience, we ask the student to self-report their fluency in English on a scale of 0-20 (Mean=13.8, SD=2.5, Median (IQR)=14.0 (12.0-16.0), Max=20.0). We then divide students into two groups: not fluent < 10 (N=551), and fluent > 10 (N=5,280). We see no exam participation difference between the experiment and control group for students who are less fluent in English and a large difference for students who are fluent (Figure A.1(a)). We did not see an interaction of gender with exam participation rate change beyond the general trend of the experiment group having a lower participation rate than the control group (Figure A.1(b))."}, {"title": "3.2 Estimated Positive Benefit of GPT-4 on Learning For Adopters", "content": "In the previous section, we observed that providing access to GPT-4 had a negative impact on overall student engagement in this course. In this section, we estimate the impact of GPT-4 access and usage on student learning outcomes, as measured by exam scores. Specifically, we compute two separate estimates:\n(E1): Advertisement Effect: Is there a change in exam performance due to advertising and offering students access to GPT-4? This effect can be measured by computing the difference between the means of student exam scores in the experiment and the control group, accounting for missing data.\nWe next estimate if there is a difference in exam performance between the adopters (students in the ex- periment group who used GPT-4 before the diagnostic exam) and the students in the control group. The difference has an effect size that falls within the small range, but we are not certain if it is statistically sig- nificant after adjusting for multiple hypothesis testing (\u2206=2.36pp, SE=1.14, 90% CI=[0.49, 4.24], Cohen's d=0.13, P=.836, Unadjusted P=.056). When we refill the missingness with a regression model, we found the difference to be larger (A=2.89pp, SE=0.69, 90% CI=[1.76, 4.02], Cohen's d=0.21, P<.001, Unadjusted P<.001). The difference between the adopters and all the students in the control group does not equal E2 (the improvement in exam performance for adopters). To see this, imagine that 14.2% of students in the control group are also adopters they couldn't use GPT-4 because we didn't offer them access. The exam score improvement should be computed by the difference between the adopters in the control group and the adopters in the experiment group. Unfortunately, we don't know who the adopters are in the control group.\nWhile we would like to estimate the average treatment effect on exam performance, it is likely that adopters who used GPT-4, when offered, are significantly different than other students. Imbens and Angrist (1994)'s seminal work proved that the average treatment effect on compliers, in our case, the adopters who started using GPT-4 when offered, can be estimated assuming that (1) the advertisement impacts student usage of GPT-4 (2) the advertisement's impact on student exam scores is solely through whether the student uses GPT-4, and (3) there are no students who would have used GPT-4, but did not, because they received the advertisement. We can verify assumptions one and three hold. The second assumption, also known as the exclusion principle, is impossible to verify and relies on domain experts to judge if it is reasonable. In our setting, we believe it is reasonable, and we consider this further in the discussion. Imbens and Angrist named the treatment effect on compliers (in our case, adopters) as the local average treatment effect (LATE).\nWe computed the local average treatment effect (LATE) for E2. The standard Cohen's d formula cannot be used to compute the effect size for a LATE estimator. We use the proposal from Bansak (2020) to estimate the effect size (ES). We compute the impact on exam performance for adopters and impute for missingness using the machine learning model. The estimated LATE is positive, at \u2206=6.86. Though the difference is not statistically significant after adjusting for multiple hypothesis testing (BCa 90%CI=[0.30, 14.13], ES=0.40), the estimated effect size is near medium\u00b3, at 0.40, suggesting the potential of a substantial positive effect on exam scores on adopters."}, {"title": "3.3 GPT-4 Usage Behaviors", "content": "In order to understand the impact of offering and using GPT-4, we analyze student usage behaviors in two ways. First, we analyze the characteristics of the students who are using GPT-4. We used logistic regression to predict if a student would use GPT-4, given a set of prior student covariates, using data from the experiment group. We report the logistic regression coefficients in Table A.2, and we visualize a subset of their odds ratio (the exponential of the coefficients) in Figure 6.\nWe can see that naturally, students who attended more sections prior to week 4 (i.e., these students have been very active in class) are more likely to use GPT-4 if offered to them. If they were observed having attended all three sections in prior weeks, their odds of using GPT-4 would have increased by 24%. For HDI (the HDI is an index normalized to be between 0 and 1), students from the most wealthy country (HDI=1) will have 17% lower odds of using GPT-4 than students from the poorest country (HDI=0). Similarly, we observe a correlation between GPT-4 usage and age and gender as well. Slightly older students are more likely to use GPT-4. Male students are more likely to use GPT-4 than female, non-binary, and students of other genders. We did not find statistical significance in other characteristics, including English fluency.\nWe also investigated what kind of conversation the student was having with GPT -4 in our class. The students in the experiment group were randomly (with equal probability) put into four groups, where they were shown some suggested questions that they could ask GPT-4. We created three sets of potential questions, broadly categorizing them as \"Coding\", \"Resources\", and \"Emotional Support\". We manually wrote 10-20 questions in each of these sets. Students who got assigned to these groups will see three randomly sampled suggested questions every time they open the custom-built interface (See Figure A.5 (b)). We also created a group called \"No Rec,\" where the students will not see any suggested questions. We created this group to study what the students would naturally ask in a coding class setting.\nWe see that most students who reach out to GPT-4 do so to ask questions related to coding concept clarification or the specifics of the programming language they are learning. The percentage of concept clarification ranges from 37.6% to 53.3%, and language and tool support ranges from 13.6% to 20.5%. A majority of these discussions stayed on topic. We only found 1-4% of conversations are directly related to students trying to deceive the GPT-4 to give them answers to the homework question. We have pre- emptively added homework assignments as prompts so that GPT-4 would not easily give students direct answers. However, we did not spend enough time testing whether it was easy to bypass our safeguards. After examining the transcripts, we did not find students asking diagnostic exam questions through the GPT-4 interface. However, we are not able to verify whether students have used OpenAI's public ChatGPT interface outside the class.\nOne encouraging finding is that, by displaying different types of example questions, we are able to induce different behavior patterns from the students. The percentage difference of the same topic between different groups can be as high as 15.7% (e.g., \"Concept Clarification\"). Our experiment period was only 9 days. Therefore, we cannot conclude whether this difference will disappear once students have more time to use GPT-4. However, this does suggest that small nudges may impact how students interact with the LLM for educational purposes. We did not find any educational outcome difference between these groups."}, {"title": "4 Discussion", "content": "A common limitation of educational research, even randomized control trials, is that experiments are con- ducted in a particular context. In this case, we ran the student in a particular domain (computer science) with a particular cohort of students at a particular moment in time. Consequently, the findings from this study only directly apply to this particular context. The same intervention, if studied in different learning environments, may produce different results. Our course's unique characteristics likely shape the educational experience and outcomes observed: (1) The course is at-will in contrast to many k-12 or university experi- ences where learners must complete the whole experience in order to get a grade and/or credit. Students in traditional classrooms may have experiences that could lower their motivation, but they are less likely to drop-out. (2) The course centers on human teachers. As such, it likely attracts learners who are interested in education from humans. This may make our learner population different than that of a typical massive online course. (3) The impacts might be specific to programming education. Some of the causal mechanisms for why students disengage may play out differently in courses on literature, history, or even math. This is especially believable because coding is a computer-oriented domain in which LLMs particularly excel. (4) The experiment was conducted in 2023, a period marked by a distinctive zeitgeist regarding artificial intelligence in education. While perceptions and news differ by geographic location, broadly, the news was a mixture of excitement about the potential of large language models and fears over the role that LLMs may play in society.\nThese limitations are typical in the field of education. We urge the reader to be cautious about extrapolating these results into different contexts. Despite these contextual limitations, a large-scale randomized control trial case study remains one of the most useful ways of knowing within the educational domain. The insights gained, while specific, can be a helpful data point for those trying to pursue a broader understanding of educational interventions and their impacts.\nUser Experience Design May Impact Results\nSeveral decisions regarding user experience were implemented that may have influenced the study's outcomes. Firstly, the experiment was advertised through email and a link from a main tab on the homepage rather than being integrated within the IDE or the learning experience itself. Second, the interface was purposely designed to resemble the ChatGPT user interface but was differentiated by varying color schemes to clarify to students that they were navigating a distinct environment. Finally, before students used the chat experience, we chose to show them a brief handout detailing the advantages and disadvantages of Large Language Models (LLMs). We made the decision to include the short handout in the interest of making sure our students were informed . We included the positive and negative versions to better understand how a teacher's stance could impact usage. We observed no significant difference in long-term engagement levels between the group that received positive emphasis and the group that emphasized cautionary points. The lack of a difference might indicate that the content of this handout did not have an impact on the outcomes observed. On the other hand, perhaps the existence of any handout, even if positive and short, could have been a point of friction that impacted outcomes.\nThese design choices are worth considering, especially due to the 14% adoption rate among students. Though 14% is low for a mandatory feature in a course, it is a rather typical adoption rate for an optional tool in this particular course. For comparison, a second optional tool in the course, designed to provide style feedback and also advertised via email, experienced an even lower participation rate of 7%, as detailed in Woodrow et al. (2024). While stronger encouragement (or even making use of the tool mandatory) could drive up engagement in the tool, it could exacerbate the decrease in engagement in the course."}, {"title": "4.2 What Caused Disengagement?", "content": "In the randomized control trial, there was a notable difference in engagement on a two-week time scale between learners with and without access to ChatGPT. This effect is influenced by student age, coding experience, and HDI of the student's country. We note that it is surprising to see a drop in engagement in education simply from getting access to an optional tool. Many learning science experiments are \"doomed to succeed\" because of the strong novelty effect in education (Makel and Plucker, 2014). This novelty effect is seemingly non-existent when providing an LLM chat interface to students. On their own, these results do not suggest why there was a difference in engagement. In this section, we present a few theories on such a causal mechanism.\nOne hypothesis for the impact on engagement is that access and advertisement to GPT confronted learners with a threat to their prospects of getting a programming job. Several studies have shown that learners who make a connection between academic courses and activities and future versions of themselves experience increased motivation towards their learning (Leondari, 2007; Lens et al., 2001; Greene and DeBacker, 2004), especially in intro to programming courses. Specifically, Peteranetz et al. (2016) showed that a difference in a student's perception of an instrumental connection between course participation and employment was a significant predictor of standardized course grades. Under the Job Threat Hypothesis, interacting with ChatGPT decreases a student's instrumental connection between learning and obtaining employment. When the student either uses ChatGPT or reads the advertisement, the student supposes that there may be fewer jobs and there will be steeper competition for those limited roles. At the very least, a student might perceive a greater uncertainty around jobs.\nThis hypothesis is supported by the observation that the biggest engagement impact of access and advertising of GPT was on people who demographically match those who are applying for jobs - people in the job-seeking age (22-40) and middle-experience programmers. The instrumental connection between learning to code and jobs is especially pertinent in the Code in Place course. Of the students in the class, 46.8% of students list, \"I want to get a job as a programmer\" as a motivation for taking the class upon submitting their application. The percentage of students who listed jobs as motivation is even higher for learners of job-seeking age (49.0%). In contrast, only 43.7% of learners outside that range listed jobs as a motivation. As a corollary, there was a 1.26 percentage point decrease in exam participation among learners in the experimental condition who listed a job as a motivation for taking the course (43.4% exam participation) compared to those who did not list jobs (44.7% exam participation).\nAI Mistrust Hypothesis\nArtificial intelligence's integration into society has engendered polarized opinions. IPSOS ran a global study of perceptions of AI at the same time that the Code in Place experiment ran, where they interviewed 22,816 adults under the age of 75 across 31 countries (Ipsos, 2023). They found that 52% of respondents were nervous regarding AI, and 46% did not agree that there were more benefits than drawbacks. These broad phenomena appeared to be playing out inside the course. In conversations with learners in Code in Place, we noticed a surprisingly high level of distrust in AI. Students voiced concerns which range from concerns over (1) privacy, (2) concerns about the trustworthiness of AI, as well as (3) more existential worries about the role of AI in society (Kochhar, 2023). As such, as evidenced by the IPSOS survey and conversations with our students, it is reasonable to assume that a subset of our learners (large but of unknown size) do not trust AI. We also assume that there is a separate subset of learners who are excited about the potential. We hypothesize that those with a negative association towards AI are more impacted by a teaching chatbot than those with a positive attitude. As such, the polarization surrounding AI, fueled by these concerns, may contribute to a decrease in engagement among students when AI tools are introduced into the learning environment. The country-level variance in trust seems to support this hypothesis. In the IPSOS survey, they found that \"Trust in AI varies widely by region; it is generally much higher in emerging markets... than in high-income countries.\" This ordering of countries by IPSOS trust in AI reflects the ordering by country of effect-sizes that the advertisement of GPT had on students. Specifically, we also note that learners from emerging markets (largely those with lower HDI) were positively impacted by access to ChatGPT.\nPeople are more comfortable with AI automating mechanical tasks than human or relationship labor, such as caring for children. Where does teaching fit into this spectrum? There is a broad set of literature that argues, \"The need for social belonging for seeing oneself as socially connected\u2014is a basic human motivation\" and as such, \"social connectedness has a dramatic impact on course completion\" (Walton and Cohen, 2007). Providing more AI assistance likely induces fewer human-to-human interactions. Could this reduction in interpersonal engagement lead to feelings of loneliness and alienation? These concerns are particularly potent in courses like Code in Place, designed to emphasize a human-centric learning experience. In such environments, the introduction of AI tools such as ChatGPT might be perceived as especially intrusive or displacing, contributing to a decrease in student engagement.\nAI vs Human Identity Threat\nThe dosage of help from an AI may be inducing a novel form of identity threat. While learning, students interpret their successes and difficulties as rewards that influence their identity and self-perception as bur- geoning coders (Kaplan and Flum, 2012). This self-reflection can be especially modulated by demographic identity in a mechanism often called \"identity threat\" (Hanselman et al., 2014). While traditional identity threat focuses on gender and ethnicity, we may be observing an identity threat oriented around simply being human. When introduced to a powerful, capable assistant like ChatGPT, students might be confronting the stressful idea: \"I find coding challenging, yet here is an AI that excels effortlessly.\" Even if jobs are not a core goal for the learner, this could impact their self-perception as a \"coder.\" Identity has a causal impact on motivation, which could explain the drop in engagement (Cohen and Garcia, 2008).\nLearners Found a Better Way to Gain Skills\nAn alternative and somewhat more optimistic hypothesis for the observed decrease in engagement posits that advertising ChatGPT introduces students to a better opportunity for reaching their learning goals than the course: directly engaging with ChatGPT. Imagine a student who was unaware of ChatGPT and the recent advances of Generative AI. After the advertising of the Code in Place deployment of ChatGPT, they interact with the LLM and have a productive conversation. In order to circumvent the protections placed to stop learners from obtaining solutions to course challenges, they switch to using the OpenAI website directly. There, they are able to learn at their own pace in an environment that is adaptive to their particular needs. They no longer have a strong need to return to Code in Place as their education needs are being met. Even though these students are achieving their learning goals, we have lost our ability to observe their improvement, and they are categorized as having a drop in engagement.\nThis shift in perception could be especially pronounced for students who are self-motivated or who prefer autonomous learning environments. They might see ChatGPT not just as a supplement to their learning but as a complete alternative to traditional educational pathways. This hypothesis is supported by observing that students had productive conversations with ChatGPT and seemed to be getting pedagogical value out of their chats."}, {"title": "4.3 What Caused Improved Exam Scores?", "content": "Our analysis of the adopters' interactions with GPT-4 shows that those who used GPT-4 were generally using it in a constructive way to better their understanding of the material and the field at large. This could have improved their understanding, which transferred to test performance gains. An interesting question for future study is how the type of use of GPT-4, and the amount, might impact learning outcomes. This particular result is less surprising. There is a long-held belief in education that direct one-on-one tutoring substantially improves student ability (Bloom, 1984). LLM chat seems to be a reasonably useful autonomous tutor. One concern was that access to an AI tutor at any point could serve as too much help, sometimes labeled the \"Scaffolding Paradox\" (Gillespie and Hald, 2017). Instead of grappling with problems, iterating through solutions, and learning from errors- a crucial cycle in developing programming acumen- students may shortcut this process by seeking immediate answers from AI, thereby steepening their learning curve in the long run. Fortunately, the strong exam performance of students who adopted ChatGPT suggests that overreliance on AI, to the detriment of learning, was not a dominant mechanism in our setting."}, {"title": "5 Related Work", "content": "Recent studies have explored the effects of large language models on education. One branch of work focuses on making observational and qualitative studies of how LLMs are used in different educational settings. Butgereit and Martinus (2023) launched a WhatsApp bot that connects to GPT-4 to tutor mathematics at the university level in the Arabic language, particularly for areas going through periods of violent unrest. Choi et al. (2023) collected data on how teachers from the poorest schools in Sierra Leone used GPT-4 to assist their teaching. They found that 48% of the questions asked to GPT-4 were about concept clarifications, similar to our finding as well. Liu et al. (2024) integrated GPT-4 into a university-level class (CS50) and found that students enjoyed the experience and found LLMs to be helpful.\nAnother branch of work focuses on designing randomized control trial (RCT) experiments to understand the effect of LLMs on educational outcomes. Kumar et al. (2023) conducted a large-scale experimentand found that GPT-generated hints positively impacted learning compared to not providing hints at all. The participants in this study were crowd-sourced site workers rather than individuals from a traditional class- room or realistic educational environment. Prihar et al. (2023) showed that GPT-generated hints have lower qualities compared to human tutor-written hints when rated by other teachers. Pardos and Bhandari (2023) showed that when students are using these hints for learning, learning gains were only statistically significant for human tutor-created hints. As far as we know, there hasn't been any RCT work directly examining the potential learning benefits of introducing students directly to ChatGPT in a classroom setting."}, {"title": "6 Method: Local Average Treatment Effect", "content": "There is a long history in economics and social sciences of estimating causal effects by using instrumental variables to account for possible self-selection into a treatment. Using this strategy, economists have been able to estimate that there is a negative impact on lifetime earnings when people are drafted, and gain evidence that increasing years of education can result in higher salaries (Angrist, 1990; Card, 1993; Angrist et al., 1996). Even though we want to report the expected (average) treatment effect for any person who receives the treatment, due to the self-selection effect, we can only make valid statistical statements about the people whose resulting decision (to uptake a treatment) are impacted by an instrumental variable. To distinguish from the average treatment effect, we report the local average treatment effect - local in the sense that the treatment effect is averaged over (what the literature refers to as) compliers, those who complied with the treatment or control indicated by the instrumental variable (in our case, using GPT-4 after being given access).\nNotation To describe the causal structure of our experiment setup, we introduce the following notations. Each student i is represented by $(X_i, Z_i, W_i, Y_i)$, where $X_i \\in \\mathbb{R}^d$, a vector of observed d covariates (demo- graphic characteristics, pre-enrollment qualification test, general activities in class before the experiment, etc.); $Z_i \\in \\{0,1\\}$ is the instrumental variable and indicates whether the student is informed of the existence of free GPT access in class via an email and a quick-access button is provided in the homepage sidebar; $W_i \\in \\{0,1\\}$ describes whether the student uses the in-class GPT-4 interface that we provided; $Y_i \\in \\mathbb{R}$ de- notes the observed outcome, the i-th student's exam score. Folllowing the potential outcomes framework, we use $Y_i(0)$ and $Y_i(1)$ to the denote potential outcomes under treatment W for individual i, so that $Y_i = Y_i(W_i)$. Note the fundamental problem of causal inference is that we only observe one potential outcome for each individual i.\nWe denote each student's decision to use GPT-4 when given access (C stands for compliance) as $C_i = \\{W_i(0), W_i(1)\\}$, where $W_i = W_i(Z_i)^5$. We define the exam score improvement (the treatment effect) of GPT-4 for the students who complied as $\\gamma$, following the setup from Angrist et al. (1996):\n$\\gamma := E[Y_i(1) - Y_i(0) \\, | \\, C_i = complier]$.\n$\\gamma$ is the Local Average Treatment Effect (LATE). The following assumptions suffice for LATE to be identi- fiable:\nAssumption 1 (Uniformly Random Nudges). $Z \\perp X$\nAssumption 2 (Relevance). $Z \\not \\perp W | X$"}, {"title": "7 Conclusion", "content": "Our aim is to understand the impact of introducing access to support of chat-based Generative AI system in an introductory online programming course. This type of interface is readily available to students and teachers around the world, but there are many important questions about the impact of such tools on engagement and learning. To help tackle such questions, we conducted a large-scale randomized control trial study on an introductory programming class with 5,831 students from 146 countries in which we provided some students with access to a GPT-4 tool for the course. We estimate positive benefits on exam performance for adopters, students who used the tool, but over all students the advertisement led to an average decrease in participation in several class elements. The reason for the decrease in engagement is not yet fully known, and may be influenced by the particular advertisement framing used in sharing access to the tool. Our results suggest there may be promising benefits to using LLMs in introductory courses, but also potential harms for engagement, which may have longer term implications on student success. Our work highlights the need for additional investigations to help understand the potential impact of future large adoption and integration of LLMs into classrooms."}, {"title": "A Appendices", "content": "A.1 Additional Student Covariate Interaction Effect\nWe show additional interactions between student covariates and their exam participation in Figure A.1. We note that there doesn't seem to be a difference in exam participation for students who are not fluent in English. Nonetheless, as the main instructional materials for this course are in English, most students who decide to apply already have high proficiency in the language, regardless of whether it is the primary language in their home countries. For gender, we notice a similar gap between experiment and control. For section attendance, we are not able to draw meaningful conclusions beyond what we have observed so far.\nA.2 Additional Details on Trial Assignment\nAt the beginning of April 24th, 8,762 students enrolled in the class. However, we don't want to offer access to ChatGPT too early because some empirical work in education showed that providing too many hints too early might hurt a student's learning progress (Gillespie and Hald, 2017). We determine a student to be \"active\" by looking at whether they have completed all Week 1 assignments. We end up with 5,831 students in our randomized control trial. We then sent an email to 3,581 students. 2,778 students (77.6%) opened the email. 539 students (15.1%) clicked on the link to our custom ChatGPT interface. We obtained institutional review board (IRB) approval for conducting this experiment.\nA.3 Full Description of Student Covariates\nWe report the basic statistics of the student population in our randomized control trial in Table 1. Here, we provide some additional information about them. Note that we are not reporting these distributions over the entire course's student population. They are only computed on the 5,831 students who were deemed active by week 1 and were included in our randomized control trial. In order to make sure all of the covariates we use are independent of the treatment (access to our custom ChatGPT), we only use either demographic information or a record of the student prior to the start of the experiment (week 3).\n\\bullet Application Score (Mean=48.2, SD=11.4, Median (IQR)=47.2 (41.0-55.0), Max=103.0): This is an aggregate score computed by the course staff to rank student applications to enroll in this class. It is a weighted combination of many factors. We are not able to share what this equation is.\n\\bullet Age (Mean=31.4, SD=10.4, Median (IQR)=29.0 (23.0-37.0), Max=84.0): Student self-reported age.\n\\bullet Gender (\"Female\": 51.59%, \"Male\": 45.58%, \"Non-Binary/Other/NA\": 2.83%): Student self-reported gender. We provide five categories in our application.\n\\bullet English Fluency (Mean=13.8, SD=2.5, Median (IQR)=14.0 (12.0-16.0), Max=20.0): This course iteration provided all lectures and materials exclusively in English. Applicants were categorized based on the English fluency demonstrated in their application materials. In future iterations, the course will offer materials in multiple languages to accommodate those who may not be fluent in English.\n\\bullet Application Effort (Mean=4.0, SD=1.3, Median (IQR)=5.0 (3.0-5.0), Max=5.0): The course staff computed a score that captures how much effort an applicant spent on their application. The exact computation is kept confidential.\n\\bullet Coding Score (Mean=8.2, SD=3.8, Median (IQR)=10.0 (10.0-10.0), Max=10.0): Applicants were required to complete a lesson and then tackle a programming exercise. Their performance on this exercise determined their coding score. In this scoring system, a lower score indicates a lower proficiency in programming, whereas a higher score demonstrates better programming skills.\n\\bullet Prior Experience (Mean=-5.1, SD=4.4, Median (IQR)=-4.0 (-8.0-2.0), Max=0.0): This course is designed for those with little or no prior programming experience. During the admission process, course instructors evaluate applicants' previous programming knowledge to determine if they are overqualified. Applicants detail their prior programming experience in their applications, which is then used to categorize them. A lower, or more negative, score indicates more prior programming experience, which is disadvantageous for their overall evaluation as the course is designed for beginners. Whereas, a higher score indicates less prior programming experience, indicating that the applicant might be better suited for the course.\n\\bullet Friend Score (Mean=4.6, SD=18.3, Median (IQR)=0.0 (0.0-0.0), Max=288.2): Student is asked to tell us if they know other people in the class. We compute a score based on this information. The more people they know, the higher their friend score will be.\n\\bullet Section Attendance (Mean=1.7, SD=0.6, Median (IQR)=2.0 (1.0-2.0), Max=2.0): We calculate how many sections the student has attended prior to receiving access to our custom ChatGPT interface. At most, they could have attended 2 sections.\n\\bullet Country HDI (Mean=0.8, SD=0.1, Median (IQR)=0.9 (0.8-0.9), Max=1.0): Students are asked to self-report their residing country. We map the self-reported country to the United Nations Human Development Index score. This score is a measure of a country's progress on key elements of human development, including health, education, and economic situation.\nA.4 Diagnostic Exam Details\nA diagnostic exam is administered near the end of the course. All students enrolled in the class received an email notifying them that the exam is available on May 26th, 2023, at 9:43 am EDT. The email was sent to 9,573 students. 7,084 students opened the email (74.4%), and 1,748 students clicked on the diagnostic exam link in the email (18.4%). The student will see a welcome message once they open the exam page:\nQuestions cover basic concepts of Python knowledge, control flow, arithmetic, and using Python canvas to animate objects. No official score is given to the students. The exam was graded with unit tests that verified each part of the student code, and a rubric system was used to create detailed feedback.\nWe use a simple normalization rule to convert a student's diagnostic exam feedback to a score that has a range of 0 to 100. If a student has completed all exam questions and received 0 feedback, they get a score of 100. If a student did not submit a particular question, we count it as if the student received the maximal number of feedback from that question (i.e., if a student misses Q2, we would treat it as if they received 12 feedback). If a student gets s number of feedback in total, their score is 1-s/73. We verified our conversion with the course staff and obtained their approval.\nA.5 Additional Details on Statistics\nIn the main result section, we report two p-values. One p-value is the family-wise error rate (FWER) controlled p-value, which we denote as P in the main text. We use Bonferroni correction to control for family-wise error rate. In addition, we report the unadjusted p-value per comparison, which we denote as \"unadjusted P\". In all the figures, we report the significance level based on the Bonferroni-corrected P, which is calculated by multiplying 15 to unadjusted p-values. For GPT-4 usage patterns reported in Section 3.3, we follow the guideline that the p-values for logistic regression analysis do not need to be additionally adjusted.\nFor confidence interval, when we use a difference-in-means (DM) estimator for computing \u2206 between two groups without missing values (Section 3.1), we use the standard confidence interval calculation for the DM estimator discussed in Wager (2020). When we have to deal with missing data, for both the DM estimator and local average treatment effect (LATE) estimator, we use the bias-corrected and accelerated (BCa) bootstrap interval (Section 3.2). The number of bootstrap samples we used is 1000, and we sampled with replacement.\nA.6 Impute for Missingness (Regression Model)\nAll of these features are standardized, which means for feature X with empirical mean $\\mu$ and standard deviation $\\sigma$, we use $X = \\frac{X-\\mu}{\\sigma}$. We first discuss how we conduct our model selection (hyperparameter search) and then discuss how we imputed for missing values. We conducted a search over a few model classes: linear regression, Ridge regression, Lasso regression, a 2-layer neural network with 128-dimension hidden size and tanh activation function, and a random forest regressor. All the models are implemented in sklearn. We first split the dataset into a training and a holdout set and used 5-fold cross-validation to select the best model from the training set. We then compute the mean-squared error (MSE) on the holdout test set. Because we do not have access to students who did not participate in the exam, we only train and evaluate our models on students who took the exam (on both the training and holdout set).\nWe use cross-fitting to impute for missing values. This procedure is inspired by works in econometrics and double machine learning (Athey and Imbens, 2016; Chernozhukov et al., 2018). Cross-fitting allows us to use part of the dataset to estimate nuisance parameters of the imputation model and use the holdout dataset to estimate the parameter of interest for the causal effect estimation model. We refer readers to Page 22 in (Wager, 2020) for a more detailed discussion.\nAlgorithm 1: Cross-fitting based Impute for Missing Values\nWe describe our algorithm above, and we choose k = 2 for a 2-fold cross-fitting, a standard choice for most settings. The imputation algorithm A is pre-selected during the model selection phase (the algorithm with the lowest MSE on the holdout is chosen, which, in our case, is the Ridge regression model).\nA.7 Logistic Regression Analysis of GPT Usage\nIn Section 3.3, we used a logistic regression model to understand what kind of students are more likely to become adopters of LLMs if we offer to them in a massive online class like ours. Each feature has been standardized. We report the coefficients in Table A.2.\nA.8 Deployment Implementation Details\nStudent Access to GPT Interface within and outside the course We do not record students' browsing histories and, unfortunately, do not know if they have accessed the publicly available GPT interface provided by OpenAI. However, a survey conducted by a concurrent study on the same course asked if the students had used the ChatGPT interface during the course period. Only 2% of the students responded that they did.\nUsing GPT-4 to Cheat on the Diagnostic Exam Despite our effort to give a stern warning to the students and let them know that their conversation with the GPT is monitored and visible to course staff, we conducted an analysis to see if the students copied and pasted exam questions into our custom interface. We did not find any evidence that the students used our custom interface to cheat. It is worth pointing out that, unlike a university course, this free online class does not incentivize students to cheat on exams the course does not offer a letter grade, and the exam does not provide students a score only feedback on how they did on each of the problems. The final course completion certificate only mentions if they had made an attempt on the exam."}]}