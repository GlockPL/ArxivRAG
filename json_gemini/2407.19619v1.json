{"title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation", "authors": ["Manish Bhattarai", "Javier E. Santos", "Shawn Jones", "Ayan Biswas", "Boian Alexandrov", "Daniel O'Malley"], "abstract": "The advent of large language models (LLMs) has revolutionized the field of code translation, enabling automated translation between programming languages. Despite these advancements, the accuracy and reliability of these models often falter in complex translation tasks due to a lack of contextual understanding. This paper introduces a novel approach to enhance code translation through Few-Shot Learning augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time. We chose RAG over traditional fine-tuning methods due to its ability to leverage existing codebases or a locally stored corpus of code, allowing it to dynamically adapt to diverse translation tasks without the need for extensive retraining. Extensive experiments on diverse datasets, using open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8x22B, and commercial LLM models such as GPT-3.5 turbo, and GPT-4o demonstrate the superiority of our approach over traditional zero-shot, particularly in translating between Fortran and C++.We also explored different numbers of shots (examples provided to the model during inference) specifically 1, 2, and 3 shots and various embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to evaluate the robustness and effectiveness of our approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid evolution of programming languages and the need to maintain legacy codebases have created a substantial demand for automated code translation tools. Traditional approaches to code translation involve extensive manual effort and expertise in both the source and target languages. With the rise of large language models (LLMs), such as GPT-3 [1], Codex [2], and CodeBERT [3], automated code translation has become increasingly feasible. These models leverage vast amounts of training data to generate code snippets in various languages, demonstrating impressive capabilities in general language understanding and generation tasks.\nHowever, despite these advancements, the performance of language models in code translation tasks remains inconsistent, particularly when dealing with complex or less common programming constructs. A significant challenge lies in the model's ability to comprehend and generate code that adheres to the syntactic and semantic rules of both the source and target languages. Zero-shot and few-shot learning techniques (where \"shots\" refer to the number of examples provided to the model during inference) have shown promise [4], but they often lack the depth of contextual understanding required for high-fidelity translations.\nFine-tuning language models for specific code translation tasks is a common approach to improve performance [5]. However, fine-tuning requires substantial computational resources and time, and it may not generalize well to new or unseen tasks. In contrast, Retrieval-Augmented Generation (RAG) offers a dynamic alternative by leveraging a repository of existing code translations to provide contextual examples during the translation process [6]. This method allows the model to adapt to various tasks without extensive retraining, making it a more flexible and efficient solution.\nIn this paper, we propose a RAG framework that enhances Few-Shot Learning for code translation tasks. Our approach involves maintaining a repository of code translation examples and dynamically retrieving the most relevant examples based on the input code segment. By providing the model with multiple contextual examples, RAG facilitates a deeper under-"}, {"title": "II. RELATED WORKS", "content": "The field of code translation, particularly from Fortran to C++, has seen various innovative approaches and methodologies aimed at improving translation accuracy and efficiency. One notable contribution is the development of a high-performance computing (HPC) code translation dataset [5]. This dataset pairs OpenMP Fortran with C++ code snippets, facilitating the training and evaluation of machine learning models for effective code translation. The dataset's quality was ensured through human-level evaluations by expert programmers, which significantly refined the translations by assessing correctness, readability, and semantic retention of the original Fortran code.\nIn addition to dataset creation, efforts have been made to re-engineer legacy Fortran code into maintainable C++ code. A study explored the maintainability of translated Fortran code by evaluating various software quality metrics [14]. The research concluded that re-engineered Fortran to C++ code exhibited high maintainability standards, making it a viable solution for modernizing legacy systems while ensuring the translated code remains efficient and easy to maintain.\nAutomated tools have also played a crucial role in facilitating Fortran to C++ translation. The CFortranTranslator, an open-source tool, converts Fortran90/Fortran77 code to C++14 while maintaining the abstraction level of the original code [15]. This tool effectively handles mixed fixed-form and free-form Fortran code, providing a practical solution for developers needing to translate Fortran code to leverage modern C++ features and frameworks; however, the resulting code may not always be easy to read or maintain, posing long-term challenges.\nThe advent of LLMs has further revolutionized code translation. A study titled \"Lost in Translation\u201d emphasized the importance of providing contextual information, such as stack traces and error messages, to improve LLM-based code translation accuracy [16]. This iterative prompting approach demonstrated significant improvements in translation success rates by incorporating additional informative context into the prompts. The study also provided a comprehensive taxonomy of bugs introduced by LLMs during code translation, offering valuable insights into common error modalities and mitigation strategies.\nFine-tuning LLMs for domain-specific tasks has been another area of active research [17]. Prompt-oriented fine-tuning using Low-Rank Adaptation (LoRA) [18] has been proposed to adapt LLMs to specific programming languages and domains. This method leverages domain-specific vocabulary and demonstrates improved translation quality by efficiently fine-tuning pre-trained models for specific tasks [5]. In the paper, the authors demonstrated that finetuning the LLM improved the translation performance by 9 folds."}, {"title": "III. METHODS", "content": "Our study presents a RAG based pipeline designed to enhance the accuracy and contextual understanding of automated code translations from Fortran to C++, as illustrated in Figure 1. This method integrates LLMs with retrieval mechanisms, enabling the generation of high-quality translations through dynamically provided contextual examples. The pipeline is highly adaptive, allowing users to plug in different LLMs, embedding models, datasets, and adjust the number of shots for evaluating translation performance. Models can be seamlessly loaded directly from Hugging Face [19], specific directories tailored to each model type, or via API calls as specified by user input. For retrieval with RAG, either l2 distance or cosine similarity metric can be used."}, {"title": "Dataset Preparation", "content": "We leveraged three datasets to evaluate our models:\n1) Numerical Recipes Dataset:: This dataset comprises pairs of Fortran and C++ code snippets, ensuring a robust set of examples [20]. Each code pair is meticulously curated to maintain high standards of quality and relevance. To ensure quality, we standardized code style, removed comments, handled whitespace and special characters, and mapped Fortran subroutines to their C++ equivalents. One downside of this dataset is that it relies on a specific library of functions, which may limit its general applicability. This dataset comprises 298 Fortran-C++ pairs.\n2) HPC Fortran2CPP Dataset:: This dataset was derived from [5].The dataset comprises comprehensive Fortran to C++ translation pairs and was meticulously curated from the NAS Parallel Benchmarks (NPB), Polyhedral Benchmark (Poly-Bench), and DataRaceBench (DRB) repositories. The NPB dataset evaluates supercomputer performance with computational fluid dynamics benchmarks, PolyBench provides programs for polyhedral compilation research, and DRB includes OpenMP programs for data race detection tool evaluation. The authors performed standardized code style as we did for Numerical Recipe dataset and additional calibration was done using similarity tests ensured semantic fidelity, while expert programmers conducted human-level evaluations to assess correctness, readability, and semantic retention. This dataset comprises 315 Fortran-C++ pairs.\n3) Stack-V2 Dataset:: The Stack V2 dataset is a comprehensive collection of code samples sourced from a wide range of repositories on GitHub, focusing on high-performance computing and various computational problems [21]. This dataset includes approximately half a million Fortran code snippets, providing a diverse set of examples for robust evaluation. For our study, we sampled 500 Fortran examples from this dataset by selecting files with lengths between 1000 and 10,000 bytes from unique repositories, prioritizing those with the highest combined star and fork event counts to ensure high-quality and diverse samples. Since Stack-V2 doesn't have Fortran- C++ pairs, we extracted the files containing metadata, codes, and comments. We then leveraged Llama3-70B Instruct model to extract the executable Fortran code, discarding other metadata. This cleaned dataset was subsequently used to compare translation performance across various LLMs in a zero-shot setting."}, {"title": "Embedding Generation and Example Retrieval", "content": "A crucial component of our RAG pipeline is the generation of embeddings for each Fortran code snippet. We employ various embedding models such as Nomic-Embed [13], Starencoder [7], and CodeBERT [3] to produce these embeddings, which are essential for assessing translation performance. These embeddings capture the semantic essence of the code snippets, enabling efficient retrieval of the most contextually relevant examples from the dataset. For efficient vector storage and retrieval, we leverage ChromaDB, which stores embeddings along with the corresponding source and target codes. We evaluated the retrieval performances with l2 and cosine similarity metrics.\nOnce the embeddings are generated, the system dynamically retrieves the most similar examples based on cosine similarity or Euclidean distance. This process ensures that the model receives the most relevant examples tailored to each specific translation task, thereby enhancing the accuracy and efficiency of the code translation. The retreived translation pairs are then augmented with the original query and directed to LLM for translation."}, {"title": "Few-Shot Learning with Retrieval-Augmented Generation", "content": "In-context learning for source code translation leverages the power of few-shot examples to improve the performance of language models in generating accurate translations. In a typical zero-shot setting, a language model $M$ generates a target translation $T$ for a given query $Q$ (source code) directly, which can be mathematically represented as $T = M(Q)$. However, the performance of $M$ can be significantly enhanced by conditioning it on a set of $k$ example pairs of source and target code, $\\{(S_i, T_i)\\}_{i=1}^{k}$, before generating the translation for $Q$. This few-shot learning process can be expressed as:\n$T = M(\\{(S_i, T_i)\\}_{i=1}^{k}, Q)$.\nIn a RAG setup, this process is further optimized by incorporating a retrieval mechanism $R$ that selects the most relevant $k$ example pairs from a large corpus $C$ based on the query $Q$. The retrieval step can be mathematically formulated as:\n$\\{(S_i, T_i)\\}_{i=1}^{k} = R(Q, C)$.\nSubsequently, the model $M$ generates the translation $T$ using the retrieved example pairs and the query, expressed as:\n$T = M(R(Q, C), Q)$.\nThe translation process begins with a zero-shot approach, where the model translates the Fortran code to C++ without any additional context. We then generate embeddings for the Fortran code snippets using models like Nomic-Embed, Starencoder, and CodeBERT. These embeddings capture the semantic essence of the code, facilitating efficient retrieval of the top-k most relevant examples from ChromaDB, our vector storage and retrieval system.\nFor Zero-shot comparisons, we apply the zero-shot translation prompt template (Figure 3) with the Fortran code to be translated. For few-shot prompts, we use the prompt template"}, {"title": "Evaluation and Experimental Setup", "content": "The generated translations are evaluated using the Code- BLEU metric [22], which assesses the quality of code translations by considering both syntactic and semantic correctness. CodeBLEU extends the traditional BLEU metric by incorporating four key components: N-gram Match Score, which measures the precision of n-grams in the translated code compared to the reference code, ensuring the retention of original token sequences; Weighted N-gram Match Score, which enhances the n-gram match score by weighting different n-grams based on their importance, thus focusing on critical code patterns and structures; Syntax Match Score, which evaluates the syntactic correctness of the translated code, ensuring adherence to the grammatical rules of the programming language; and Dataflow Match Score, which assesses the semantic correctness by analyzing the data flow within the program, ensuring the preservation of logical flow and functional equivalence.\nOur experiments involved extensive testing with various combinations of models, shot numbers, RAG retrieval metrics, and embedding models. We utilized Nomic-Embed, Starencoder, and CodeBERT as embedding models. For code translation, we leveraged open LLMs such as Starcoder, Llama3- 70B Instruct, Code LLaMA-34B, Granite-34B Code Instruct, and Mistral 8x22B, as well as commercial LLM models such as GPT-3.5 turbo and GPT-4o/GPT-4 turbo. We evaluated the performance of the models for zero, one, two, and three shots. Each experiment aimed to measure the improvements in translation quality facilitated by the RAG approach using CodeBLEU as the primary evaluation metric."}, {"title": "IV. RESULTS AND DISCUSSIONS", "content": "1) Performance Across Models and Embeddings: The results highlight significant variability in performance across different models and embedding strategies. The breakdown of Codebleu metric for different models in Zero-shot setting is shown in Table I. The table provides key insights into the performance of different models on zero-shot tasks using the Nomic-embed embedding model with l2 distance. Notably, GPT-4 Turbo and GPT-3.5 Turbo achieved the highest zero- shot CodeBLEU scores of 0.371 and 0.367, respectively, indicating their strong initial performance without additional context. These models also scored highly on Syntax Tree and Dataflow metrics, which measure syntactic correctness and dataflow consistency, respectively, highlighting their ability to generate structurally sound and semantically accurate code. In contrast, Granite-34B and Llama3-70B Instruct, while having slightly lower CodeBLEU scores (0.237 and 0.309, respectively), showed balanced performance across all metrics, signifying their robust handling of various translation aspects. Metrics like Ngram and Weighted Ngram, which assess exact token matches and weighted token matches, respectively, were particularly low for StarCoder, reflecting its struggles with precise code generation in zero-shot tasks. Mixtral-8x22B exhibited high variance in the Numerical Recipes dataset, especially in CodeBLEU and Ngram scores, suggesting inconsistency in its translation quality.\nHowever in the Few-shot task, the Granite-34B Code Instruct, Llama3-70B Instruct and Mixtral-8x22B model consistently outperformed others across all embedding types and learning configurations, achieving the highest CodeBLEU scores. For instance, under the nomic-embed model embedding, Granite-34B Code Instruct achieved a zero-shot Code- BLEU of 0.237 and improved to 0.6 in the one-shot setting for HPC Fortran2CPP dataset with l2 norm as shown in Table II. This demonstrates the model's strong capability in understanding and translating Fortran code into C++ with minimal context.\nWhile we conducted experiments using various embedding models, our findings indicate that Nomic-embed and Starencoder exhibited equivalent performance in few-shot settings. However, CodeBERT consistently underperformed compared to the other two models. For instance, CodeLlama-34B Instruct achieved a zero-shot CodeBLEU of 0.243 with Nomic-embed, which improved to 0.321 in the two-shot configuration. In contrast, CodeBERT's performance did not show comparable improvement. This discrepancy can be attributed to the maximum token limit of each embedding model; CodeBERT has a token limit of 512, whereas both Starencoder and Nomic- embed support up to 8192 tokens. Given the longer length of Fortran codes to be translated, the limited token capacity of CodeBERT likely hindered its performance. To ensure clarity and focus, we present results using Nomic-embed throughout the text, as they are consistent with those obtained using Starencoder."}, {"title": "2) Impact of Few-Shot Learning", "content": "Few-shot learning consistently enhanced translation performance across all models and embeddings. The progressive improvement from one-shot to three-shot learning configurations was evident, with models like Granite-34B Code Instruct, Llama3-70B Instruct, and Mixtral-8x22B showing substantial gains as shown in Table II. For instance, Granite-34B Code Instruct with nomic_embed improved from a zero-shot CodeBLEU of 0.24 \u00b1 0.09 to 0.60 \u00b1 0.27 in the one-shot setting, further improving to 0.54\u00b10.27 and 0.54 \u00b1 0.21 in the two-shot and three-shot settings, respectively. Thus, the RAG setup played a significant role in enhancing code translation quality. By leveraging a retrieval mechanism to provide relevant context from similar code snippets or documentation, RAG setups help models to better understand the structure and semantics of the source code. This additional context is especially beneficial in few- shot learning scenarios, where providing relevant examples can significantly improve the translation quality."}, {"title": "3) Dataset-Specific Performance: HPC Fortran2CPP vs. Numerical Recipes in Few-Shot setting", "content": "The datasets used in this study, namely HPC Fortran2CPP and Numerical Recipes, exhibited different performance characteristics across the models. The HPC Fortran2CPP dataset generally yielded higher CodeBLEU scores compared to the Numerical Recipes dataset. For instance, Granite-34B Code Instruct with CodeBERT embedding achieved a one-shot CodeBLEU of 0.6 on the HPC Fortran2CPP dataset, whereas it scored 0.49\u00b10.20 on the Numerical Recipes dataset. This discrepancy can be attributed to the inherent differences in the complexity and structure of the code in these datasets. The HPC Fortran2CPP dataset, which may contain more standardized and less complex code, allowed models to perform better. In contrast, the Numerical Recipes dataset, with potentially more intricate and varied code structures, posed greater challenges for the models."}, {"title": "4) Impact of LLM models", "content": "When comparing different models, Mixtral-8x22B, Llama3-70B, and Granite-34B stood out as the top performers in few-shot settings, whereas GPT-3.5 Turbo and GPT-4 Turbo excelled in zero-shot settings. Starcoder, on the other hand, showed relatively lower performance, particularly in the Zero-shot setting, with a CodeBLEU of 0.21. Additionally, Starcoder did not show significant improvement in one-shot settings, likely due to its smaller context length.\nModels such as llama3, Codestral, Mixtral, Granite, and CodeLlama consistently outperformed others across multiple metrics, particularly in terms of CodeBLEU scores. This superior performance can be attributed to their explicit pre- training or fine-tuning for code-related tasks. These models have likely been trained on extensive code-specific data and tasks, enabling them to better understand and generate programming languages. Conversely, models like Phi-3 did not perform as well in comparison, likely because they were not specifically optimized for code-related tasks. While Phi- 3 may have been trained on a diverse set of texts, including some programming language data, their training was not as focused on code-specific tasks as Granite and CodeLlama. Consequently, their ability to handle Fortran to C++ translation is less robust.\nWhile GPT models such as GPT-4 Turbo excel in zero-shot settings with high initial CodeBLEU scores, their performance does not increase substantially with additional shots. This plateau in performance can be attributed to GPT models prioritizing the generation of executable and semantically correct code over strict alignment with ground truth translations. As a result, their incremental benefit from additional examples is limited compared to other models.\nStarcoder's notably poor performance can be explained by a couple of factors. First, Starcoder is a smaller model with a smaller token limit, which restricts its capacity to understand and generate complex code structures compared to larger models. Additionally, it appears that Starcoder's training may not have been as focused on code-specific data, leading to lower performance even with multiple shots. This suggests that for tasks requiring a deep understanding and generation of code, model size and the specificity of training data play crucial roles in determining performance."}, {"title": "5) Results on Unlabelled Dataset in zero-shot settings", "content": "We first translated the StackV2 Fortran dataset using different LLMs and computed the pairwise CodeBLEU similarity between the model translations. The results are presented in Figure 7. In this experiment, we aimed to evaluate the consistency and similarity of translations produced by various models. The heatmap in Figure 7 illustrates the pairwise CodeBLEU scores, where each cell represents the similarity between translations generated by a pair of models. From the heatmap, we can observe the following key points: The highest pairwise similarity scores are observed between Codestral and Llama3-70B Instruct models, with a score of 0.522. The Granite-34B Code Instruct model shows relatively high similarity with several models, including Llama3-70B Instruct and Mixtral-8x22B. Models CodeLlama-34B Instruct, GPT 4 Turbo, and GPT 3.5 Turbo also demonstrate a moderate degree of consistency in their translations. These results indicate that while there is a degree of variability in the translations generated by different models, certain models produce more consistent and similar translations compared to others. The main takeaway from this experiment is that identifying models with reliable and consistent translations guides researchers in model selection for future tasks, thereby enhancing translation accuracy, reliability, and advancing automated code translation methodologies."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In conclusion, the results of this study underscore the importance of model architecture, training data specificity, and the effective use of few-shot learning in enhancing code translation tasks. Models explicitly pre-trained or fine-tuned for code-related tasks, such as GPT-3.5/4 and Llama3-70B- Instruct, demonstrate superior performance, while models with more general training, like Phi-3 3.8B fall short. The findings also highlight the potential limitations of smaller models like Starcoder in handling complex code translation tasks, emphasizing the need for adequate model size and training data to achieve optimal performance. The use of RAG setups further enhances the translation quality by providing relevant context, proving to be a valuable strategy in few-shot learning scenarios.\nThe current limitation in Fortran-C++ pairs challenges fine- tuning LLMs and establishing benchmarks. Our Stack-v2 dataset experiments should guide expert translation collections paired with RAG setups, enhancing model performance and applicability across code translation tasks. Future research will expand the dataset and refine the RAG framework to improve translation quality and reliability."}]}