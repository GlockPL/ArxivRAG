{"title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?", "authors": ["Alexander D. Goldie", "Chris Lu", "Matthew T. Jackson", "Shimon Whiteson", "Jakob N. Foerster"], "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned opti-mization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN\u00b9), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals tradition-ally used optimizers. Furthermore, OPEN shows strong generalization across a distribution of environments and a range of agent architectures.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning [1, RL] has undergone significant advances in recent years, scaling from solving complex games [2, 3] towards approaching real world applications [4-7]. However, RL is limited by a number of difficulties which are not present in other machine learning domains, requiring the development of numerous hand-crafted workarounds to maximize its performance.\nHere, we take inspiration from three difficulties of RL: non-stationarity due to continuously changing input and output distributions [8]; high degrees of plasticity loss limiting model capacities [9, 10]; and exploration, which is needed to ensure an agent does not converge to local optima prematurely [1, 11]. Overcoming these challenges could enable drastic improvements in the performance of RL, potentially reducing the barriers to applications of RL in the real-world. Thus far, approaches to tackle these problems have relied on human intuition to find hand-crafted solutions. However, this is fundamentally limited by human understanding. Meta-RL [12] offers an alternative in which RL algorithms themselves are learned from data rather than designed by hand. Meta-learned RL algorithms have previously demonstrated improved performance over hand-crafted ones [13-15].\nOn a related note, learned optimization has proven successful in supervised and unsupervised learning (e.g., VeLO [16]). Learned optimizers are generally parameterized update rules trained to outperform handcrafted algorithms like gradient descent. However, current learned optimizers perform poorly in RL [16, 17]. While some may argue that RL is simply an out-of-distribution task [16], the lack of"}, {"title": "2 Background", "content": "Reinforcement Learning The RL problem is formulated as a Markov Decision Process [1, MDP] described by the tuple (A, S, P, R, \\rho, \\gamma). At discrete timestep t, the agent takes action \\(a_t \\in A\\) sampled from its (possibly stochastic) policy, \\(\\pi(\\cdot|s_t) \\in \\Pi\\), which conditions on the current state \\(s_t \\in S\\) (where \\(s_0 \\sim p\\)). After each action, the agent receives reward R(st, at) and the state transitions to \\(s_{t+1}\\), based on the transition dynamics P(st+1|st, at). An agent's objective is to maximize its discounted expected return, \\(J^\\pi\\), corresponding to a discount factor \\(\\gamma\\in [0, 1)\\), which prevents the agent making myopic decisions. This is defined as\n\\(J^\\pi := \\mathbb{E}_{a_{0:\\infty} \\sim \\pi, s_0 \\sim p, s_{1:\\infty} \\sim P} \\sum_{t=0}^{\\infty} \\gamma^t R_t\\) (1)\nProximal policy optimization [21, PPO] is an algorithm designed to maximize \\(J^\\pi\\). It uses advantage, \\(A^\\pi(s, a)\\), which is calculated from the state value function, \\(V^\\theta = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R_t|S_t = s]\\), and state-action value function, \\(Q^\\pi(s, a) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R_t |S_t = s, A_t = a]\\). It measures the improvement of a specific action over the current policy and takes the form\n\\(A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)\\). (2)\nPPO introduces a loss for optimizing the policy, parameterized by \\(\\theta\\), that prevents extreme policy updates in gradient ascent. This uses clipping, which ensures there is no benefit to updating \\(\\theta\\) beyond where the policy probability ratio, \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\), exceeds the range \\([1 \\pm \\epsilon]\\). The clipped loss is\n\\(L^{CLIP}(\\theta) = \\mathbb{E} [min(r_t(\\theta)A^\\pi(s_t, a_t), clip(r_t(\\theta), 1 \\pm \\epsilon)A^\\pi(s_t, a_t))]\\). (3)\nPPO is an actor-critic [1] method, where the policy and value functions are modeled with different neural networks, or separate heads of the same neural network, conditioning on state. The PPO objective to maximize combines the clipped loss, a value function error, and an entropy bonus into\n\\(L_t(\\theta) = \\mathbb{E}[L^{CLIP}(\\theta) - C_1L^{VF}(\\theta) + C_2S[\\pi_{\\theta}](S_t)]\\). (4)"}, {"title": "3 Related Work", "content": "3.1 Optimization in RL\nWilson et al. [34] and Reddi et al. [35] show that adaptive optimizers struggle in highly stochastic processes. Henderson et al. [36] indicate that, unlike other learning regimes, RL is sufficiently stochastic for these findings to apply, which suggests RL-specific optimizers could be beneficial.\nThe idea that optimizers designed for supervised learning may not perfectly transfer to RL is reflected by Bengio et al. [37], who propose an amended momentum suitable for temporal difference learning [38]. This is related to work by Sarig\u00fcl and Avci [39], who explore the impact of different types of momentum on RL. While these works motivate designing optimization techniques specifically for RL, we take a more expressive approach by replacing the whole optimizer instead of just its momentum calculation, and using meta-learning to fit our optimizer to data rather than relying on potentially suboptimal human intuition.\n3.2 Meta-learning\nDiscovering RL Algorithms Rather than using handcrafted algorithms, a recent objective in meta-RL is discovering RL algorithms from data. While there are many successes in this area (e.g., Learned Policy Gradient [13, LPG], MetaGenRL [14] and Learned Policy Optimisation [15, LPO]), we focus on meta-learning a replacement to the optimizer due to the outsized impact a learned update rule can have on learning. We also use specific difficulties of RL to guide the design of our method, rather than simply applying end-to-end learning.\nJackson et al. [31] learn temporally-aware versions of LPO and LPG. While their approach offers inspiration for dealing with non-stationarity in RL, they also rely on Adam [18], an optimizer designed for stationarity that is suboptimal for RL [9]. Instead, we propose replacing the optimizer itself with an expressive and dynamic update rule that is not subject to these problems."}, {"title": "4 Difficulties in RL", "content": "We believe that fundamental differences exist between RL and other learning paradigms which make RL particularly difficult. Here, we briefly cover a specific set of prominent difficulties in RL, which are detailed with additional references in appendix A. Our method takes inspiration from handcrafted heuristics targeting these challenges (Section 5.3). We show via thorough ablation (Section 7) that explicitly formulating our method around these difficulties leads to significant performance gains.\n(Problem 1) Non-stationarity RL is subject to non-stationarity over the training process [1] as the updating agent causes changes to the training distribution. We denote this training non-stationarity. Lyle et al. [9] suggest optimizers designed for stationary settings struggle under nonstationarity.\n(Problem 2) Plasticity loss Plasticity loss, or the inability to fit new objectives during training, has been a theme in recent deep RL literature [9, 47, 48, 10]. Here, we focus on dormancy [47], a measurement tracking inactive neurons used as a metric for plasticity loss [10, 49\u201351]. It is defined as\n\\(\\Phi = \\frac{\\mathbb{E}_{x \\in D}|h_i^l(x)|}{\\sum_{k \\in l} \\mathbb{E}_{x \\in D} |h_i^l(x)|}\\) (6)\nwhere \\(h_i^l(x)\\) is the activation of neuron i in layer l with input \\(x \\in D\\) for distribution D. \\(H^l\\) is the total number of neurons in layer l. The denominator normalizes average dormancy to 1 in each layer.\nA neuron is \\(\\tau\\)-dormant if \\(\\Phi \\le \\tau\\), meaning the neuron's output makes up less than \\(\\tau\\) of its layer's output. For ReLU activation functions, \\(\\tau = 0\\) means a neuron is in the saturated part of the ReLU. Sokar et al. [47] find that dormant neurons generally stay dormant throughout training, motivating approaches which try to reactivate dormant neurons to boost plasticity.\n(Problem 3) Exploration Exploration is a key problem in RL [1]. To prevent premature conver-gence to local optima, and thus maximize final return, an agent must explore uncertain states and actions. Here, we focus on parameter space noise for exploration [11], where noise is applied to the parameters of the agent rather than to its output actions, like e-greedy [1]."}, {"title": "5 Method", "content": "There are three key considerations when doing learned optimization for RL: what architecture to use; how to train the optimizer; and what inputs the optimizer should condition on. In this section, we systematically consider each of these questions to construct OPEN, with justification for each of our decisions grounded in our core difficulties of RL.\n5.1 Architecture and Parameterization\nTo enable conditioning on history, which is required to express behavior like momentum, for example, OPEN uses a gated recurrent unit [52, GRU]. This is followed by two fully connected layers with"}, {"title": "6 Results", "content": "In this section, we benchmark OPEN against a plethora of baselines on large-scale training domains.\n6.1 Experimental Setup\nDue to computational constraints, we meta-train an optimizer on a single random seed without ES hyperparameter tuning. This follows standard evaluation protocols in learned optimization (e.g., [16, 17, 45]), which are also constrained by the high computational cost of meta-learned optimization. We detail our hyperparameters in Appendix C.3. We define four evaluation domains, based on Kirk et al. [59], in which an effective learned optimization framework should prove competent:\nSingle-Task Training A learned optimizer must be capable of fitting to a single environment, and being evaluated in the same environment, to demonstrate it is expressive enough to learn an update rule. We test this in five environments: Breakout, Asterix, Space Invaders and Freeway from MinAtar [60, 61]; and Ant from Brax [62, 63]. This is referred to as 'singleton' training in Kirk et al. [59].\nMulti-Task Training To show an optimizer is able to perform under a wide input distribution, it must be able to learn in a number of environments simultaneously. Therefore, we evaluate performance from training in all four environments from MinAtar [60, 61]. We evaluate the average normalized score across environments with respect to Adam.\nIn-Distribution Task Generalization An optimizer should generalize to unseen tasks within its training distribution. To this end, we train OPEN on a distribution of gridworlds from Jackson et al. [64] with antithetic task sampling [31], and evaluate performance by sampling tasks from the same distribution. We include details in Appendix D.\nOut-Of-Support Task Generalization Crucially, an optimizer unable to generalize to new settings has limited real-world usefulness. Therefore, we explore out-of-support (OOS) generalization by testing OPEN on specific task distributions defined by Oh et al. [13], and a set of mazes from minigrid [65], which do not exist in the training distribution, and with unseen agent parameters.\nBaselines We compare against open-source implementations [66] of Adam [18], RMSProp [19], Lion [20] and VeLO [16, 43]. We also learn two optimizers for the single- and multi-task settings: 'No Features', which only conditions on gradient and momentum; and Optim4RL [17] (using ES instead of meta-gradients, as in Lan et al. [17]). Since Optim4RL is initialized close to sgn(Adam), and tuning its learning rate is too practically expensive, we set a learning rate of 0.1 \u00d7 LRAdam based on Lion [20] (which moves from AdamW to sgn(AdamW)). The optimizer's weights can be scaled to compensate if this is suboptimal. We primarily consider interquartile mean (IQM) of final return with 95% stratified bootstrap confidence intervals [67]. All hyperparameters can be found in Appendix C."}, {"title": "7 Ablation Study", "content": "In this ablation, we explore how each constituent of OPEN contributes to improved performance. We focus on two specific, measurable challenges: plasticity loss and exploration.\n7.1 Individual Ablations\nWe ablate each individual design decision of OPEN in Figure 5. For each ablation, we train 17 optimizers in a shortened version of Breakout [60, 61] and evaluate performance after 64 PPO training runs per optimizer. Further details of the ablation methodology are in Appendix I.\nWhile measuring plasticity loss is important, dormancy alone is not an appropriate performance metric; a newly initialized network has near-zero dormancy but poor performance. Instead, we include dormancy here as one possible justification for why some elements of OPEN are useful.\n(P1) Ablating training proportion directly disables the optimizer's ability to tackle training non-stationarity. Similarly, removing batch proportion prevents dynamic behavior within a (stationary) batch. The corresponding decreases in performance show that both timescales of non-stationarity are beneficial, potentially overcoming the impact of non-stationarity in RL.\n(P2) Removing dormancy as an input has a drastic impact on the agent's return, corresponding to a large increase in plasticity loss. While dormancy plays no direct role in the optimizer's meta-objective, including it as an input gives the optimizer the capability to react as neurons grow dormant, as desired."}, {"title": "8 Limitations and Future Work", "content": "While OPEN demonstrates strong success in learning a multi-task objective, our current approach of normalizing returns by Adam biases updates towards environments where Adam underperforms learned optimizers. We believe developing better curricula for learning in this settings, akin to unsupervised environment design [64, 69], would be highly impactful future work. Additionally, the OPEN framework need not be limited to the specific difficulties we focus on here. Exploring ways to include other difficulties of RL which can be measured and incorporated into the framework (e.g., sample efficiency or generalization capabilities of a policy) could potentially elevate the usefulness of OPEN even further. Finally, we have constrained our experiments to PPO only; exploring the impact of OPEN on other RL algorithms (e.g., SAC [70], A2C [71]) would be useful."}, {"title": "9 Conclusion", "content": "In this paper, we set out to address some of the major difficulties in RL by meta-learning update rules directly for RL. In particular, we focused on three main challenges: non-stationarity, plasticity loss, and exploration. To do so, we proposed OPEN, a method to train parameterized optimizers that conditions on a set of inputs and uses learnable stochasticity in its output, to specifically target these difficulties. We showed that our method outperforms a range of baselines in four problem settings designed to show the expressibility and generalizability of learned optimizers in RL. Finally, we demonstrated that each design decision of OPEN improved the performance of the optimizer in an ablation study, and that the stochasticity in our update expression significantly benefited exploration."}, {"title": "A Additional Details: Difficulties in RL", "content": "(Problem 1) Non-stationarity Igl et al. [8] highlights different sources of non-stationarity across a range of RL algorithms. For PPO, non-stationary arises due to a changing state visitation distribution, the target value function \\(V^\\pi(s)\\) depending on the updating policy and bootstrapping from the generalized advantage estimate [72]. Since PPO batches rollouts, non-stationarity occurs between each batch; within the same batch, the agent solves a stationary problem. As the non-stationarity occurs over the course of training, we denote it training non-stationarity.\nOptimizers designed for stationarity, like Adam [18], have been shown to struggle in non-stationary settings [9] and techniques to deal with non-stationarity have been proposed. Asadi et al. [73] prevent contamination between batches by resetting their optimizer state, including resetting their momentum. However, this handcrafted approach is potentially too severe, as it fails to take advantage of potentially useful commonality between batches.\n(Problem 2) Plasticity loss Plasticity loss, which refers to an inability to fit to new objective over the course of training, is an important problem in RL. As an agent learns, both its input and target distributions change (i.e., nonstationarity, (P1)). This means the agent needs to fit new objectives during training, emphasizing the importance of maintaining plasticity throughout training. Therefore, there have been many handcrafted attempts to reduce plasticity loss in RL. Abbas et al. [51] find that different activation functions can help prevent plasticity loss, Obando-Ceron et al. [74] suggest using smaller batches to increase plasticity and Nikishin et al. [58] introduces new output heads throughout training. Many have demonstrated the effectiveness of resetting parts of the network [75, 57, 76, 47], though this runs the risk of losing some previously learned, useful information. Additionally, these approaches are all hyperparameter-dependent and unlikely to eliminate plasticity loss robustly.\n(Problem 3) Exploration Exploration in RL has a rich history of methodologies: [77\u201379], to name a few. While there are simple, heuristic approaches, like e-greedy [1], recent methods have been designed to deal with complex, high dimensional domains. These include count based methods [80, 81], learned exploration models [82, 83] or using variational techniques [84]. Parameter space noise [11] involves noising each parameter in an agent to enable exploration across different rollouts while behaving consistently within a given rollout. This inspired our approach to exploration since it is algorithm-agnostic and can be implemented directly in the optimizer."}, {"title": "B Optimizer Details", "content": "B.1 Algorithm\nWe detail an example update step when using OPEN. We use notation from Appendix B.2, and use \\(z_{GRU}\\) to refer to the GRU hidden state at update t. In this algorithm, we assume the dormancy for each neuron has been tiled over the relevant parameters. In practice, we build our optimizer around the library fromMetz et al. [43]."}, {"title": "C Hyperparameters", "content": "C.1 PPO Hyperparameters\nPPO hyperparameters for the environments included in our experiments are shown in Table 3. For our gridworld experiments, we learned OPEN and tuned Adam [18] for a PPO agent with W = 16, but evaluated on widths W \u2208 [8, 16, 32, 64, 128]. Hyperparameters for PPO are taken from [31] where possible.\nC.2 Optimization Hyperparameters\nFor Adam, RMSprop and Lion, we tune hyperparameters with fixed PPO hyperparameters. For each environment we run a sweep and evaluate performance over four seeds (eight in the gridworld), picking the combination with the highest final return. In many cases, we found the optimizers to be fairly robust to reasonable hyperparameters. For Lion [20], we search over a learning rate range from 3-10\u00d7 smaller than Adam and RMSprop as suggested by Chen et al. [20]. For Optim4RL [17], hyperparameter tuning is too expensive; instead, we set learning rate to be 0.1 \u00d7 LRAdam following the heuristic from [20].\nFor Adam, RMSprop and Lion, we also test whether annealing learning rate from its initial value to 0 over the course of training would improve performance.\nA full breakdown of which hyperparameters are tuned, and their value, is shown in the following tables. We use notation and implementations for each optimizer from optax [66].\nC.3 ES Hyperparameters\nDue to the length of meta-optimization, it is not practical to tune hyperparameters for meta-training. We, however, find the following hyperparameters effective and robust for our experiments. We use common hyperparameters when learning OPEN, Optim4RL and No Features.\nFor the single-task and gridworld settings, we train all optimizers for a number of generations after their performance stabilizes, which took different times between optimizers, to ensure each learned optimizer has reached convergence; this occasionally means optimizers were trained for different"}, {"title": "D Gridworld Details", "content": "We train OPEN on gridworlds by sampling environment parameters from a distribution. Here, we use a distribution implemented by [64, 31] which is detailed in Table 8. In the codebase of Jackson et al. [64], this is referred to as 'all'."}, {"title": "E Single Environment Return Curves", "content": "In Figure 9, which shows return curves over training for the five 'single-task' environments (section 6), OPEN significantly beats 3 of the 5 baselines, performs similarly to Lion [20] in space invaders and performs comparably to hand-crafted optimizers and 'No Features' in ant. OPEN is clearly able to learn strongly performing update rules in a range of single-task settings."}, {"title": "F Analysis", "content": "Throughout this section, we include analysis and figures exploring the behavior of optimizers learned by OPEN. In particular, we consider the behavior of OPEN with regards to dormancy, momentum, update size and stochasticity in different environments. Where relevant, we also make comparisons to the 'No Features' optimizer, introduced as a baseline in Section 6, to explore possible differences introduced by the additional elements of OPEN.\nDue to the number of moving parts in OPEN, it is difficult to make strong claims regarding the behaviour of the learned optimizers it produces. Instead, we attempt to draw some conclusions based on what we believe the data plots included below suggest, while recognizing that the black-box nature of its update rules, which leads to a lack of interpretability, is a potential drawback of the method in the context of analysis.\nAll plots included below pertain to the single task regime, besides section F.5 which focuses specifi-cally on deep sea.\nF.1 Dormancy\nFigure 10 shows the \\(\\tau = 0\\) dormancy curves during training for each of the MinAtar [60, 61] environments. These environments were selected as the only ones where the agent uses ReLU activation functions; ant [63, 62] used a tanh activation function for which \\(\\tau = 0\\) dormancy is not applicable."}, {"title": "I Ablation Details", "content": "I.1 Feature Ablation Setup\nWe train 17 optimizers for each feature ablation, and evaluate performance on 64 seeds per optimizer, using a shortened version of Breakout. We use the same PPO hyperparameters for Breakout as in Table 3 besides the total timesteps T, which we shorten to 2 \u00d7 105. Each optimizer takes ~ 60 minutes to train on one GPU, and we train a total of 119 optimizers.\nWe train each optimizer for 250 generations, keeping all other MinAtar ES hyperparameters from Table 7.\n1.2 Deep Sea Expanded Curve\nWe show the final return against size for the Deep Sea environment [68, 61] in Figure 14 for all optimizers; both shared and separated, with stochasticity and without. The one without can be thought of as a deterministic optimizer, and is denoted 'ablated' in the figure. For this experiment, we trained each optimizer for only 48 iterations on sizes in the range [4, 26]; any sizes outside of this range at test time are out of support of the training distribution.\nClearly the separated optimizer with learned stochasticity is the only one which is able to generalize across many different sizes of Deep Sea. This occurs marginally at the behest of optimization in smaller environments, where the stochastic optimizer still explores after obtaining reward and so does not consistently reach maximum performance. In Deep Sea, the final return scale is between 0.99 and -0.01."}, {"title": "J Experimental Compute", "content": "J.1 Runtimes\nWe include runtimes (inference) for our experiments with the different optimizers on 4 \u00d7 L40s GPUs. We note that, while OPEN takes longer to run compared to handcrafted optimizers, it offers significant speedup over VeLO and takes a similar runtime as Optim4RL and No Features in addition to significantly stronger performance. Importantly, this suggests the additional computation used in OPEN to calculate features, such as dormancy, does not translate to significant overhead compared to other learned optimizers.\nJ.2 Training Compute\nWe used a range of hardware for training: Nvidia A40s, Nvidia L40ses, Nvidia GeForce GTX 1080Tis, Nvidia GeForce RTX 2080Tis and Nvidia GeForce RTX 3080s. These are all part of an internal cluster. Whilst this makes it difficult to directly compare meta-training times per-experiment, we find training took roughly between ~ [16, 32] GPU days of compute per optimizer in the single-task setting, and ~ 60 GPU days of compute for multi-task training. We measured inference time for all optimizers in Appendix J.\nJ.3 Total Compute\nFor all of our hyperparameter tuning experiments, we used a total of ~ 16 GPU days of compute, using Nvidia A40 GPUs.\nFor each learned optimizer in our single-task experiments, we used approximately ~ [16, 32] GPU days of compute. This equates to around 1 GPU year of compute for 3 learned optimizers on 5 environments (=15 optimizers total). This was run principally on Nvidia GeForce GTX 1080Tis, Nvidia GeForce RTX 2080Tis and Nvidia GeForce RTX 3080s.\nFor each multi-task optimizer, we used around 60 GPU days of compute, on Nvidia GeForce RTX 2080Tis. This totals 180 GPU days of compute. We also ran experiments for each optimizer with the smaller architecture, which took a similar length of time. These are not included in the paper; in total multi-task training used another 1 GPU year of compute, taking into account the omitted results.\nTo train on a distribution of gridworlds, we used 7 GPU days of compute on Nvidia GeForce RTX 2080 Tis.\nOur ablation study used ~ 8 GPU days of compute, running on Nvidia A40 GPUs.\nEach optimizer took 2 days to train in deep sea on an Nvidia A40 GPU. In total, this section of the ablation study took 8 GPU days.\nAs stated in Table 11, inference was a negligible cost (on the order of seconds) in this paper."}, {"title": "K Code Repositories and Asset Licenses", "content": "Below we include a full list of assets used in this paper, in addition to the license under which it was made available."}]}