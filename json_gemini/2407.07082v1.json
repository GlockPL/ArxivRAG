{"title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?", "authors": ["Alexander D. Goldie", "Chris Lu", "Matthew T. Jackson", "Shimon Whiteson", "Jakob N. Foerster"], "abstract": "While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization across a distribution of environments and a range of agent architectures.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning [1, RL] has undergone significant advances in recent years, scaling from solving complex games [2, 3] towards approaching real world applications [4-7]. However, RL is limited by a number of difficulties which are not present in other machine learning domains, requiring the development of numerous hand-crafted workarounds to maximize its performance.\nHere, we take inspiration from three difficulties of RL: non-stationarity due to continuously changing input and output distributions [8]; high degrees of plasticity loss limiting model capacities [9, 10]; and exploration, which is needed to ensure an agent does not converge to local optima prematurely [1, 11]. Overcoming these challenges could enable drastic improvements in the performance of RL, potentially reducing the barriers to applications of RL in the real-world. Thus far, approaches to tackle these problems have relied on human intuition to find hand-crafted solutions. However, this is fundamentally limited by human understanding. Meta-RL [12] offers an alternative in which RL algorithms themselves are learned from data rather than designed by hand. Meta-learned RL algorithms have previously demonstrated improved performance over hand-crafted ones [13-15].\nOn a related note, learned optimization has proven successful in supervised and unsupervised learning (e.g., VeLO [16]). Learned optimizers are generally parameterized update rules trained to outperform handcrafted algorithms like gradient descent. However, current learned optimizers perform poorly in RL [16, 17]. While some may argue that RL is simply an out-of-distribution task [16], the lack of"}, {"title": "2 Background", "content": "Reinforcement Learning The RL problem is formulated as a Markov Decision Process [1, MDP] described by the tuple (A, S, P, R, \u03c1, \u03b3). At discrete timestep t, the agent takes action $a_t \\in A$ sampled from its (possibly stochastic) policy, $\u03c0(\u00b7|s_t) \\in \u03a0$, which conditions on the current state $s_t \\in S$ (where $s_0 \\sim p$). After each action, the agent receives reward $R(s_t, a_t)$ and the state transitions to $s_{t+1}$, based on the transition dynamics $P(s_{t+1}|s_t, a_t)$. An agent's objective is to maximize its discounted expected return, $J^\u03c0$, corresponding to a discount factor $\u03b3\\in [0, 1)$, which prevents the agent making myopic decisions. This is defined as\n$J^\u03c0 := \\mathbb{E}_{a_{0:\u221e} \\sim \u03c0, s_0 \\sim p, s_{1:\u221e} \\sim P}[\\sum_{t=0}^{\u221e} \u03b3^t R_t]$\nProximal policy optimization [21, PPO] is an algorithm designed to maximize $J^\u03c0$. It uses advantage, $A^\u03c0 (s, a)$, which is calculated from the state value function, $V^\u03c0 = \\mathbb{E}_{\u03c0}[\\sum_{t=0}^{\u221e} \u03b3^t R_t|S_t = s]$, and state-action value function, $Q^\u03c0 (s, a) = \\mathbb{E}_{\u03c0}[\\sum_{t=0}^{\u221e} \u03b3^t R_t | S_t = s, A_t = a]$. It measures the improvement of a specific action over the current policy and takes the form\n$A^\u03c0 (s, a) = Q^\u03c0 (s, a) \u2013 V^\u03c0 (s)$.\nPPO introduces a loss for optimizing the policy, parameterized by \u03b8, that prevents extreme policy updates in gradient ascent. This uses clipping, which ensures there is no benefit to updating \u03b8 beyond where the policy probability ratio, $r_t(\u03b8) = \\frac{\u03c0_\u03b8(a_t | s_t)}{\u03c0_{gold}(a_t | s_t)}$, exceeds the range $[1 \u00b1 \u2208]$. The clipped loss is\n$L^{CLIP} (\u03b8) = \\mathbb{E}_t [min(r_t(\u03b8)A^\u03c0 (s_t, a_t), clip(r_t(\u03b8), 1 \u00b1 \u2208)A^\u03c0 (s_t, a_t))]$.\nPPO is an actor-critic [1] method, where the policy and value functions are modeled with different neural networks, or separate heads of the same neural network, conditioning on state. The PPO objective to maximize combines the clipped loss, a value function error, and an entropy bonus into\n$L_t(\u03b8) = \\mathbb{E}[L^{CLIP} (\u03b8) \u2013 c_1L^{VF} (\u03b8) + c_2S[\u03c0_\u03b8](s_t)]$"}, {"title": "3 Related Work", "content": "Meta-Learning Optimizers in RL Algorithms like Adam [18] or RMSprop [19] are designed to maximize an objective by updating the parameters of a neural network in the direction of positive gradient with respect to the function. They are often applied with augmentations such as momentum or learning rate schedules to better converge to optima. Learned optimizers offer an alternative: parameterized update rules, conditioning on more than just gradient, which are trained to maximize an objective [22, 16, 23]. For any parameterized optimizer opt, which conditions on a set of inputs x, the update rule to produce update u can be described as a function, opt(x) \u2192 u.\nWe treat learning to optimize as a meta-RL problem [12]. In meta-RL, the goal is to maximize $J^M$ over a distribution of MDPs $P(M)$. For our task, an optimizer trained to maximize $J(\u03a6) = \\mathbb{E}_M [J^\u03c0|opt]$ yields the optimal meta-parameterization $opt^*$.\nEvolution Strategies Evolution algorithms (EA) are a backpropagation-free, black-box method for optimization [24] which uses a population of perturbed parameters sampled from a distribution (here, $\u03b8 \\sim N(\u03b8, \u03c3^2I)$). This population is use to maximize a fitness F(\u00b7). EA encompasses a range of techniques (e.g., evolution strategies (ES) [25, 26], genetic algorithms [27] or CMA-ES [28]).\nNatural evolution strategies (NES) [29] are a class of ES methods that use the population fitness to estimate a natural gradient for the mean parameters, \u03b8. This can then be optimized with typical gradient ascent algorithms like Adam [18]. Salimans et al. [25] introduce OpenAI ES for optimizing \u03b8 using the estimator\n$\\nabla_\u03b8 \\mathbb{E}_{\u03b5~N(0, I)}[F(\u03b8 + \u03c3\u03b5)] = \\frac{1}{\u03c3} \\mathbb{E}_{\u03b5~N(0, I)}[F(\u03b8 + \u03c3\u03b5)\u03b5]$,\nwhich is approximated using a population average. In practice, we use antithetic sampling (i.e., for each sampled \u03f5, evaluating \u03b8+\u03f5 and \u2212\u03f5) [30]. Antithetic task sampling enables learning on a task distribution, by evaluating and ranking each antithetic pair on different tasks [31].\nHistorically, RL was too slow for ES to be practical for meta-training. However, PureJaxRL [15] re-cently demonstrated the feasibility of ES for meta-RL, owing to the speedup enabled by vectorization in Jax [32]. We use the implementation of OpenAI ES [25] from evosax [33].\n3.1 Optimization in RL\nWilson et al. [34] and Reddi et al. [35] show that adaptive optimizers struggle in highly stochastic processes. Henderson et al. [36] indicate that, unlike other learning regimes, RL is sufficiently stochastic for these findings to apply, which suggests RL-specific optimizers could be beneficial.\nThe idea that optimizers designed for supervised learning may not perfectly transfer to RL is reflected by Bengio et al. [37], who propose an amended momentum suitable for temporal difference learning [38]. This is related to work by Sarig\u00fcl and Avci [39], who explore the impact of different types of momentum on RL. While these works motivate designing optimization techniques specifically for RL, we take a more expressive approach by replacing the whole optimizer instead of just its momentum calculation, and using meta-learning to fit our optimizer to data rather than relying on potentially suboptimal human intuition.\n3.2 Meta-learning\nDiscovering RL Algorithms Rather than using handcrafted algorithms, a recent objective in meta-RL is discovering RL algorithms from data. While there are many successes in this area (e.g., Learned Policy Gradient [13, LPG], MetaGenRL [14] and Learned Policy Optimisation [15, LPO]), we focus on meta-learning a replacement to the optimizer due to the outsized impact a learned update rule can have on learning. We also use specific difficulties of RL to guide the design of our method, rather than simply applying end-to-end learning.\nJackson et al. [31] learn temporally-aware versions of LPO and LPG. While their approach offers inspiration for dealing with non-stationarity in RL, they also rely on Adam [18], an optimizer designed for stationarity that is suboptimal for RL [9]. Instead, we propose replacing the optimizer itself with an expressive and dynamic update rule that is not subject to these problems."}, {"title": "4 Difficulties in RL", "content": "We believe that fundamental differences exist between RL and other learning paradigms which make RL particularly difficult. Here, we briefly cover a specific set of prominent difficulties in RL, which are detailed with additional references in appendix A. Our method takes inspiration from handcrafted heuristics targeting these challenges (Section 5.3). We show via thorough ablation (Section 7) that explicitly formulating our method around these difficulties leads to significant performance gains.\n(Problem 1) Non-stationarity RL is subject to non-stationarity over the training process [1] as the updating agent causes changes to the training distribution. We denote this training non-stationarity. Lyle et al. [9] suggest optimizers designed for stationary settings struggle under nonstationarity.\n(Problem 2) Plasticity loss Plasticity loss, or the inability to fit new objectives during training, has been a theme in recent deep RL literature [9, 47, 48, 10]. Here, we focus on dormancy [47], a measurement tracking inactive neurons used as a metric for plasticity loss [10, 49\u201351]. It is defined as\n$\u03a6 = \\frac{\\sum_{k \u2208 N} \\mathbb{E}_{x\u2208D} |h^k(x)|}{\\sum_{i \u2208 layer l} \\mathbb{E}_{x\u2208D} |h^i(x)|}$,\nwhere $h(x)$ is the activation of neuron i in layer l with input $x \u2208 D$ for distribution D. $H^l$ is the total number of neurons in layer l. The denominator normalizes average dormancy to 1 in each layer.\nA neuron is \u03c4-dormant if $\u03a6 < \u03c4$, meaning the neuron's output makes up less than \u03c4 of its layer's output. For ReLU activation functions, \u03c4 = 0 means a neuron is in the saturated part of the ReLU. Sokar et al. [47] find that dormant neurons generally stay dormant throughout training, motivating approaches which try to reactivate dormant neurons to boost plasticity.\n(Problem 3) Exploration Exploration is a key problem in RL [1]. To prevent premature convergence to local optima, and thus maximize final return, an agent must explore uncertain states and actions. Here, we focus on parameter space noise for exploration [11], where noise is applied to the parameters of the agent rather than to its output actions, like \u03f5-greedy [1]."}, {"title": "5 Method", "content": "There are three key considerations when doing learned optimization for RL: what architecture to use; how to train the optimizer; and what inputs the optimizer should condition on. In this section, we systematically consider each of these questions to construct OPEN, with justification for each of our decisions grounded in our core difficulties of RL.\n5.1 Architecture and Parameterization\nTo enable conditioning on history, which is required to express behavior like momentum, for example, OPEN uses a gated recurrent unit [52, GRU]. This is followed by two fully connected layers with"}, {"title": "6 Results", "content": "In this section, we benchmark OPEN against a plethora of baselines on large-scale training domains.\n6.1 Experimental Setup\nDue to computational constraints, we meta-train an optimizer on a single random seed without ES hyperparameter tuning. This follows standard evaluation protocols in learned optimization (e.g., [16, 17, 45]), which are also constrained by the high computational cost of meta-learned optimization. We detail our hyperparameters in Appendix C.3. We define four evaluation domains, based on Kirk et al. [59], in which an effective learned optimization framework should prove competent:\nSingle-Task Training A learned optimizer must be capable of fitting to a single environment, and being evaluated in the same environment, to demonstrate it is expressive enough to learn an update rule. We test this in five environments: Breakout, Asterix, Space Invaders and Freeway from MinAtar [60, 61]; and Ant from Brax [62, 63]. This is referred to as 'singleton' training in Kirk et al. [59].\nMulti-Task Training To show an optimizer is able to perform under a wide input distribution, it must be able to learn in a number of environments simultaneously. Therefore, we evaluate performance from training in all four environments from MinAtar [60, 61]. We evaluate the average normalized score across environments with respect to Adam.\nIn-Distribution Task Generalization An optimizer should generalize to unseen tasks within its training distribution. To this end, we train OPEN on a distribution of gridworlds from Jackson et al. [64] with antithetic task sampling [31], and evaluate performance by sampling tasks from the same distribution. We include details in Appendix D.\nOut-Of-Support Task Generalization Crucially, an optimizer unable to generalize to new settings has limited real-world usefulness. Therefore, we explore out-of-support (OOS) generalization by testing OPEN on specific task distributions defined by Oh et al. [13], and a set of mazes from minigrid [65], which do not exist in the training distribution, and with unseen agent parameters.\nBaselines We compare against open-source implementations [66] of Adam [18], RMSProp [19], Lion [20] and VeLO [16, 43]. We also learn two optimizers for the single- and multi-task settings: 'No Features', which only conditions on gradient and momentum; and Optim4RL [17] (using ES instead of meta-gradients, as in Lan et al. [17]). Since Optim4RL is initialized close to sgn(Adam), and tuning its learning rate is too practically expensive, we set a learning rate of 0.1 \u00d7 $LR_{Adam}$ based on Lion [20] (which moves from AdamW to sgn(AdamW)). The optimizer's weights can be scaled to compensate if this is suboptimal. We primarily consider interquartile mean (IQM) of final return with 95% stratified bootstrap confidence intervals [67]. All hyperparameters can be found in Appendix C."}, {"title": "7 Ablation Study", "content": "In this ablation, we explore how each constituent of OPEN contributes to improved performance. We focus on two specific, measurable challenges: plasticity loss and exploration.\n7.1 Individual Ablations\nWe ablate each individual design decision of OPEN in Figure 5. For each ablation, we train 17 optimizers in a shortened version of Breakout [60, 61] and evaluate performance after 64 PPO training runs per optimizer. Further details of the ablation methodology are in Appendix I.\nWhile measuring plasticity loss is important, dormancy alone is not an appropriate performance metric; a newly initialized network has near-zero dormancy but poor performance. Instead, we include dormancy here as one possible justification for why some elements of OPEN are useful.\n(P1) Ablating training proportion directly disables the optimizer's ability to tackle training non-stationarity. Similarly, removing batch proportion prevents dynamic behavior within a (stationary) batch. The corresponding decreases in performance show that both timescales of non-stationarity are beneficial, potentially overcoming the impact of non-stationarity in RL.\n(P2) Removing dormancy as an input has a drastic impact on the agent's return, corresponding to a large increase in plasticity loss. While dormancy plays no direct role in the optimizer's meta-objective, including it as an input gives the optimizer the capability to react as neurons grow dormant, as desired."}, {"title": "8 Limitations and Future Work", "content": "While OPEN demonstrates strong success in learning a multi-task objective, our current approach of normalizing returns by Adam biases updates towards environments where Adam underperforms learned optimizers. We believe developing better curricula for learning in this settings, akin to unsupervised environment design [64, 69], would be highly impactful future work. Additionally, the OPEN framework need not be limited to the specific difficulties we focus on here. Exploring ways to include other difficulties of RL which can be measured and incorporated into the framework (e.g., sample efficiency or generalization capabilities of a policy) could potentially elevate the usefulness of OPEN even further. Finally, we have constrained our experiments to PPO only; exploring the impact of OPEN on other RL algorithms (e.g., SAC [70], A2C [71]) would be useful."}, {"title": "9 Conclusion", "content": "In this paper, we set out to address some of the major difficulties in RL by meta-learning update rules directly for RL. In particular, we focused on three main challenges: non-stationarity, plasticity loss, and exploration. To do so, we proposed OPEN, a method to train parameterized optimizers that conditions on a set of inputs and uses learnable stochasticity in its output, to specifically target these difficulties. We showed that our method outperforms a range of baselines in four problem settings designed to show the expressibility and generalizability of learned optimizers in RL. Finally, we demonstrated that each design decision of OPEN improved the performance of the optimizer in an ablation study, and that the stochasticity in our update expression significantly benefited exploration."}]}