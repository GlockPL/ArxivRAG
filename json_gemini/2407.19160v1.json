{"title": "Decomposing heterogeneous dynamical systems with graph neural networks", "authors": ["C\u00e9dric Allier", "Magdalena C. Schneider", "Michael Innerberger", "Larissa Heinrich", "John A. Bogovic", "Stephan Saalfeld"], "abstract": "Natural physical, chemical, and biological dynamical systems are often complex, with heterogeneous components interacting in diverse ways. We show that graph neural networks can be designed to jointly learn the interaction rules and the structure of the heterogeneity from data alone. The learned latent structure and dynamics can be used to virtually decompose the complex system which is necessary to parameterize and infer the underlying governing equations. We tested the approach with simulation experiments of moving particles and vector fields that interact with each other. While our current aim is to better understand and validate the approach with simulated data, we anticipate it to become a generally applicable tool to uncover the governing rules underlying complex dynamics observed in nature.", "sections": [{"title": "1 Introduction", "content": "Many natural phenomena can be modeled (or rea-\nsonably approximated) as dynamical systems of dis-\ncrete particles or finite elements that change their state\nbased on some internal program, external forces, and\ninteractions with other particles or elements. Well\nknown historic examples that use such models for\nforward simulation are cinematographic applications\n(Reeves, 1983), Reynolds's boid flocking behavior\nmodel (1987), atmospheric flow (Takle and Russell,\n1988), and fluid dynamics (Miller and Pearce, 1989).\nParticle systems and finite element methods can\nalso be used to uncover the underlying dynamics\nfrom observations. If the governing equations of the\ndynamics are known, it is generally possible to re-\ncover the underlying properties of objects from noisy\nand/or incomplete data by iterative optimization (e.g.\nKalman filter; Shakhtarin, 2006). Conversely, if the\nproperties of objects are known, it is possible to de-\ntermine the governing equations with compressed\nsensing (Brunton, Proctor, and Kutz, 2016), equations-\nbased approaches (Stepaniants et al., 2023) or ma-\nchine learning techniques, including graph neural net-\nworks (GNN; M. D. Cranmer et al., 2019; M. Cranmer\net al., 2020; Sanchez-Gonzalez et al., 2020; Prakash\nand Tucker, 2022). Recent methods jointly optimize\nthe governing equations and their parameterization\n(Long et al., 2018; Course and Nair, 2023; Lu, Ar-\ni\u00f1o Bernad, and Solja\u010di\u0107, 2022), yet heterogeneity of\nobjects and interactions is either not considered or\nprovided as input.\nIn their work on rediscovering orbital mechanics in\nthe solar system, Lemos et al. (2023) explicitly model\nthe mass of orbital bodies as a learnable parameter.\nThey use GNNs to learn how to predict the observed\nbehavior and the latent property, and combine this\npurely bottom-up approach with symbolic regression\nto infer and parameterize a a governing equation.\nWith this approach, they are able to uncover New-\nton's law of gravity and the unobserved masses of\norbital bodies from location over time data alone.\nWe expand the work by Lemos et al. (2023) to pre-\ndict and decompose heterogeneous dynamical sys-\ntems. We train GNNs to reproduce the dynamics of\ncomplex systems together with a low-dimensional em-\nbedding of their latent properties (see Figure 1). In\nsystems with discrete classes of objects, the learned\nembedding reveals the classes as clusters and allows\nto virtually decompose the system. This is a necessary"}, {"title": "2 Methods", "content": "step to infer and parameterize the underlying gov-\nerning equations. In systems with continuous prop-\nerties, the learned embedding reveals the underlying\nmanifold and allows to estimate the corresponding\nparameters.\nFor a diverse set of simulations, we can learn to re-\nproduce the complex dynamics, uncover the structure\nof the underlying heterogeneity, and parameterize\nsymbolic top-down hypotheses of the rules governing\nthe dynamics. In the simplest cases, the interaction\nfunctions were automatically retrieved with symbolic\nregression."}, {"title": "2.1 Simulation of dynamical systems", "content": "We created a diverse set of quasi-physical dynamical\nparticle systems with heterogeneous latent properties\nof individual particles (see Figure 2 and Videos). In all\nsimulations, particles interact with a limited neighbor-\nhood of other particles. They receive messages from\nconnected particles that encode some of their proper-\nties, integrate these messages, and use the result to\nupdate their own properties. This update is either the\nfirst or second derivative over time of their position\nor other dynamical properties.\nFirst, we created simulations of moving particles\nwhose motion is the result of complex interactions\nwith other particles (Lagrangian systems, see Fig-\nure 2a-d). Then, we simulated vector-fields with\ndiffusion-like signal propagation between stationary\nparticles (Eulerian systems, see Figure 2e, f). Some par-\nticles follow an exclusively internal program defined\nby a sequence of hidden states. Finally, we created\ncomplex spatio-temporal signaling networks (Hens\net al., 2019; see Figure 2g).\nAll simulations generate indexed time series by up-\ndating parts of all particle states $x_i$ using explicit or\nsemi-implicit Euler integration\n$X_i \\leftarrow x_i + \\Delta t \\dot{x_i},  \\dot{x_i} \\leftarrow \\dot{x_i} + \\Delta t \\ddot{x_i}$.\n(1)\nThe vector $x_i$ stands for the position of particles in\nmoving dynamical systems, or for other dynamical\nproperties in the vector-field and network simulations.\nThe details of these simulations are listed in Supple-\nmentary Table 1."}, {"title": "2.2 Graph neural networks", "content": "Figure 1 depicts the components of the GNNs to\nmodel dynamical particle systems, and how we train\nthem to predict their dynamical behavior and to reveal\nthe structure of the underlying heterogeneity. A graph\n$G = \\{V,E\\}$ consists of a set of nodes $V = \\{1,...,n\\}$\nand edges $E \\subset V \\times V$ with node and edge features\ndenoted by $v_i$ and $e_{ij}$ for $i, j \\in V$, respectively. A mes-\nsage passing GNN updates node features by a local\naggregation rule (Gilmer et al., 2017)\n$v_i \\leftarrow \\Phi \\Big(v_i, \\odot_{j \\in V_i} f(e_{ij}, v_i, v_j)\\Big)$,\n(2)\nwhere $V_i := \\{j : (i, j) \\in E\\}$ is the set of all neighbors of\nnode $i$, $\\Phi$ is the update function, $\\odot$ is the aggregation\nfunction, and $f$ is the message passing function. To\nmodel a dynamical particle system, $\\Phi$, $\\odot$, and $f$ can\nbe used to represent the time evolution of node states\naccording to pairwise and node-local interactions. The\nnode features $v_i$ include the dynamical node states $x_i$\n($x_i \\in \\mathbb{R}^d$). In models with moving particles, $x_i$ is the\nposition of the particles. In models with stationary\nparticles it stands for their dynamical properties. With\nthis framework, we can model arbitrary dynamical\nparticle systems by using particles as nodes and de-\nsigning an appropriate neighborhood, node and edge\nfeatures, as well as update, aggregation, and message\npassing functions. Either $\\Phi$, $\\odot$, or $f$ can be arbitrary\ndifferentiable functions, which includes fully learn-\nable deep neural networks. In our experiments, we\nuse multi-layer perceptrons (MLPs) for such learnable\nfunctions, and typically, only $f$ or parts of $\\Phi$ are fully\nlearnable at a time. The aggregation function $\\odot_i$ is\neither the sum or the average of the outputs of all\n$f_{ij}$. The inputs to these functions are application spe-\ncific subsets of the node features, such as the relative\nposition between the particles, $x_j - x_i$, the distance\nbetween particles $d_{ij}$ or the velocity of the particles $\\dot{x_i}$.\nThe latent heterogeneity of the particles is encoded\nby a two-dimensional learnable embedding $a_i$ that is\npart of the node features. These learnable embeddings\nparameterize either $\\Phi$, $\\odot$, or $f$ as appropriate.\nThe design choices for neighborhood, learnable\nfunctions, and their parameters are important to de-\nfine what the GNN can learn about the dynamical\nsystem. If either of the learnable functions has access\nto the absolute position $x_i$ of particle node $i$ and the\ntime index $t$, then this function can learn the behavior\nof the particle as a fully independent internal program.\nThis is sometimes desired, e.g. if we want to learn\nthe behavior of an unobserved force-field that impacts\nthe behavior of observable dynamical particles (see\nFigure 4). If the learnable interaction function $f$ has\nonly access to local relative offsets, velocities, and dis-\ntances, then it has to learn to reproduce the dynamics\nbased on these local cues (see Figure 3). Please see\nSupplementary Table 2 for a full description of the"}, {"title": "2.3 Results", "content": "During training, the learnable parameters of $\\Phi$, $\\odot$,\nand $f$, including the embedding $a_i$ of all nodes $i \\in V$\nare optimized to predict a single time-step or a short\ntime-series. Since we use explicit or semi-implicit\nEuler integration to update the dynamical properties\nof all particles (see Equation 1), we predict either the\nfirst or second order derivative of those properties and\nspecify the optimization loss over those derivatives\n$L_x = \\sum_{i=1}^n ||\\dot{\\hat{x_i}} - \\dot{x_i}||^2$ and $L_{\\ddot{x}} = \\sum_{i=1}^n ||\\ddot{\\hat{x_i}} - \\ddot{x_i}||^2$.\n(3)\nWe implemented the GNNs using the PyTorch Geo-\nmetric library (Fey and Lenssen, 2019). For GNN op-\ntimization we used AdamUniform gradient descent,\nwith a learning rate of $10^{-3}$, and batch size of 8. For\nmodels with rotation invariant behaviors, we aug-\nmented the training data with 200 random rotations.\nEach GNN was trained over 20 epochs, with each\nepoch covering all time-points of the respective train-\ning series (between 250 and 8000). All experiments\nwere performed on a Colfax ProEdge SX4800 worksta-\ntion with an Intel Xeon Platinum 8362 CPU, 512 GB\nRAM, and two NVIDIA RTX A6000 GPUs with 48 GB\nmemory each, using Ubuntu 22.04."}, {"title": "2.3.1 Attraction-repulsion", "content": "We simulated a dynamical system of moving par-\nticles whose velocity is the result of aggregated\npairwise attraction-repulsion towards other particles\nwithin a local neighborhood (see Figure 2a and Ap-\npendix A.1.1, 4,800 particles, 250 time-steps). The\nvelocity incurred by pairwise attraction-repulsion de-\npends on the relative position of the other particle and\nthe type of the receiving particle (see Supplementary\nTable 1).\nFor a simulation with three particle types, we visu-\nalized the training progress of the learned embedding\nvectors $a_i$ for all particles $i$, and the learned interac-\ntions $f_{ij}$ (see Supplementary Figure 1 and Video 1).\nFor all experiments, we show an informative projec-\ntion of the learned pairwise interactions (here, speed\nas a function of distance).\nInitialized at 1, the vectors $a_i$ separate and eventu-\nally converge to a clustered embedding that indicates\nthe three distinct node types (and one outlier). The\ncorresponding interaction functions capture the simu-"}, {"title": "2.3.2 Gravity-like", "content": "In the gravity-like simulation (see Figure 2b and Ap-\npendix A.1.1), particles within a reasonable distance\n(this is not physical, but reduces complexity) attract\neach other based on Newton's law of universal grav-\nitation which depends on their observable relative\nposition and their latent masses (see Supplementary\nTable 1).\nSupplementary Figure 8a, b shows the results of the\nGNN trained on two series of 2,000 time-steps with\n960 particles of 16 different masses and a continuous\ndistribution of masses, respectively (see Video 5). The\nGNNs trained with these datasets do not yield precise\nrollout inference owing to error accumulation (RMSE\n~ 1.0, Supplementary Figure 7 and Video 6). However\nthe resulting dynamics are qualitatively indistinguish-\nable from the ground truth. This is consistent with a\nSinkhorn divergence of 1.1 \u00b7 10\u20132 between true and\ninferred distributions (calculated with the GeomLoss\nlibrary, Feydy et al., 2018). As described by Lemos\net al. (2023), we were able to automatically infer and\nparameterize the symbolic interaction function using\nthe PySR package (M. Cranmer, 2023). Symbolic re-\ngression recovered the $m_1/d^2$; power laws (see Sup-\nplementary Figure 8a,b) and the 16 distinct masses\n(slope= 1.01, R2 = 1.00) as well as the continuous dis-\ntribution of masses (slope= 1.00, R2 = 1.00, 1 outlier).\nCorrupting the training with noise has limited ef-\nfect up to r = 0.4 (see Supplementary Figure 9a, b).\nRemoving particles from the training data degraded\nthe results severely (Supplementary Figure 10a, b).\nNa\u00efvely adding 'ghost-particles' as in the attraction-\nrepulsion experiment did not improve results notably.\nInterestingly, the power law exponent was still very\nwell recovered."}, {"title": "2.3.3 Coulomb-like", "content": "Supplementary Figure 8c shows the results of the\nGNN trained with simulations of particles following\nCoulomb's law of charge-based attraction-repulsion\n(see Figure 2c, Supplementary Table 1 and Ap-\npendix A.1.1), using the same short-range approxi-\nmation as previously.\nWe trained on a series of 2,000 time-steps with 960\nparticles of three different charges (-1, 1, 2 in arbitrary\nunits). The learnable pairwise interaction function $f_{ij}$\nsymmetrically depends on the observable relative po-\nsitions of two particles and both of their latent charges,"}, {"title": "2.3.4 Boids", "content": "Supplementary Figure 11 and Video 8 show the\nprocess of training a GNN with the boids simula-\ntion (see Figure 2d, Supplementary Table 1 and Ap-\npendix A.1.1). We trained on a series of 8,000 time-\nsteps and 1,792 particles with 16, 32, and 64 types, re-\nspectively (see Supplementary Figure 12 and Video 9).\nAfter training the GNN, we applied hierarchical clus-\ntering to the learned latent vectors $a_i$ which separated\nthe particle types with an accuracy of ~ 1.00. In Sup-\nplementary Figure 13, we show rollout examples for\nthe dynamic behavior of each individual recovered\ntype in isolation. We believe that this \u2018virtual decom-\nposition\u2019 of dynamical systems that can only be ob-\nserved in mixed configurations will become a pow-\nerful tool to understand complex physical, chemical\nand biological processes. We were not able to recover\nthe multi-variate interaction functions using symbolic\nregression. However, for the correct symbolic interac-\ntion function, we could estimate the latent parameters\nfor each particle type using robust regression (see Sup-\nplementary Figure 12). Adding noise during training\nhas a limited effect up to r = 0.4 and, anecdotally,\neven improved parameter estimates. Removing tra-\njectories was detrimental to the ability to learn and\nreproduce the behavior and parameters (see Supple-\nmentary Figure 14)."}, {"title": "2.3.5 Wave-propagation and diffusion", "content": "We simulated wave-propagation and reaction-\ndiffusion processes over a mesh of 104 nodes\n(see Figure 2e, f, Supplementary Table 1 and Ap-\npendix A.1.3). Other than in the simulations with"}, {"title": "2.3.6 Signaling networks", "content": "Supplementary Figure 18 shows our results to recover\nthe rules of a synaptic signaling model with 998 nodes\nand 17,865 edges with two types of nodes with dis-\ntinct interaction function (data from Hens et al., 2019;\nsee Figure 2g, Supplementary Table 1, and Video 14).\nIn order for the GNN to successfully infer the signal-\ning rules, we found that we needed a relatively large\ntraining dataset. We ran 100 simulations with differ-\nent initial states over 1,000 time-steps. In addition,\nit was necessary to predict more than a single time-\nstep to efficiently train the GNN (see Appendix A.1.4).\nWith this training scheme, we recovered the connec-\ntivity matrix (slope = 1.0, R2 = 1.0) and were able\nto automatically infer and parameterize the symbolic"}, {"title": "2.4 Discussion", "content": "We showed with a diverse set of simulations that\nmessage passing GNNs are a flexible tool to predict,\ndecompose, and eventually understand complex dy-\nnamical systems. With the currently available soft-\nware libraries (PyTorch Geometric, Fey and Lenssen,\n2019), it is straightforward to implement an architec-\nture and loss that encode useful assumptions about\nthe structure of the complex system such as local con-\nnectivity rules or the location of learnable and known\nfunctions and their inputs. We showed that a well de-\nsigned GNN can learn a low-dimensional embedding\nof complex latent properties required to parameter-\nize heterogeneous particle-particle interactions. The\nlearned low-dimensional embeddings can be used to\nreveal the structure of latent properties underlying\nthe complex dynamics and to infer the corresponding\nparameters. As demonstrated by Lemos et al. (2023)\nand in the signaling network experiment, it is pos-\nsible to use automatic methods to extract symbolic\nhypotheses that are consistent with the learned dy-\nnamics. However, even without an explicit analysis\nof the underlying functions, it is possible to dissect\nthe dynamical system and to conduct virtual exper-\niments with arbitrary compositions (or decomposi-\ntions) of particles and interactions. We believe this\nability to become particularly useful to infer the lo-\ncal rules governing complex biological systems like\nthe organization of bacterial communities, embryonic\ndevelopment, neural networks, or the social interac-\ntions governing animal communities that cannot be\nobserved in isolation.\nIn preparation for applications on experimental\ndata, we designed simulations that provide some of\nthe required components to model a complex biolog-\nical process. We demonstrated the ability to recon-\nstruct discrete, continuous, and time-changing hetero-\ngeneities. We modeled dynamic interactions between\nmoving agents that interact with each other and with\nan independent dynamic environment. We were also\nable to infer the connectivity of a signaling network\nfrom functional observations alone. However, for an\napplication in biology, some key features are still miss-\ning and we are planning to develop them in future\nwork:\n1. In our simulations, the latent properties are either\nstatic or follow an internal program. In biological\nsystems, the interaction between cells and the\nenvironment changes their properties over time\nin well-defined ways."}, {"title": "3 Conclusion", "content": "We demonstrated that message passing GNNs can\nlearn to replicate the behavior of complex heteroge-\nneous dynamical systems and to uncover the under-\nlying latent properties in an interpretable way that\nfacilitates further analysis. The flexibility in designing\nGNNs to model and train meaningful interactions in\ncomplex systems is impressive and we are looking\nforward to developing them as an integral toolbox\nto uncover the rules governing complex dynamics\nobserved in nature."}, {"title": "A Appendix", "content": "The general form of the GNN update rule described in\nSection 2.2 and Equation 2 does not consider applica-\ntion specific parameterization of the individual func-\ntions. As described in Section 2.2 and Supplementary\nTable 2, we define the loss over first and second order\nupdates, respectively. For clarity, we unroll here the\ncombination of interaction function, aggregation, and\nupdate rules and their respective parameterization,\nresulting in one equation to calculate the updates."}, {"title": "A.1 GNN implementation with priors", "content": ""}]}