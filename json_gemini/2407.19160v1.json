{"title": "Decomposing heterogeneous dynamical systems with graph neural networks", "authors": ["C\u00e9dric Allier", "Magdalena C. Schneider", "Michael Innerberger", "Larissa Heinrich", "John A. Bogovic", "Stephan Saalfeld"], "abstract": "Natural physical, chemical, and biological dynamical systems are often complex, with heterogeneous components interacting in diverse ways. We show that graph neural networks can be designed to jointly learn the interaction rules and the structure of the heterogeneity from data alone. The learned latent structure and dynamics can be used to virtually decompose the complex system which is necessary to parameterize and infer the underlying governing equations. We tested the approach with simulation experiments of moving particles and vector fields that interact with each other. While our current aim is to better understand and validate the approach with simulated data, we anticipate it to become a generally applicable tool to uncover the governing rules underlying complex dynamics observed in nature.", "sections": [{"title": "1 Introduction", "content": "Many natural phenomena can be modeled (or rea-\nsonably approximated) as dynamical systems of dis-\ncrete particles or finite elements that change their state\nbased on some internal program, external forces, and\ninteractions with other particles or elements. Well\nknown historic examples that use such models for\nforward simulation are cinematographic applications\n(Reeves, 1983), Reynolds's boid flocking behavior\nmodel (1987), atmospheric flow (Takle and Russell,\n1988), and fluid dynamics (Miller and Pearce, 1989).\nParticle systems and finite element methods can\nalso be used to uncover the underlying dynamics\nfrom observations. If the governing equations of the\ndynamics are known, it is generally possible to re-\ncover the underlying properties of objects from noisy\nand/or incomplete data by iterative optimization (e.g.\nKalman filter; Shakhtarin, 2006). Conversely, if the\nproperties of objects are known, it is possible to de-\ntermine the governing equations with compressed\nsensing (Brunton, Proctor, and Kutz, 2016), equations-\nbased approaches (Stepaniants et al., 2023) or ma-\nchine learning techniques, including graph neural net-\nworks (GNN; M. D. Cranmer et al., 2019; M. Cranmer\net al., 2020; Sanchez-Gonzalez et al., 2020; Prakash\nand Tucker, 2022). Recent methods jointly optimize\nthe governing equations and their parameterization\n(Long et al., 2018; Course and Nair, 2023; Lu, Ar-\ni\u00f1o Bernad, and Solja\u010di\u0107, 2022), yet heterogeneity of\nobjects and interactions is either not considered or\nprovided as input.\nIn their work on rediscovering orbital mechanics in\nthe solar system, Lemos et al. (2023) explicitly model\nthe mass of orbital bodies as a learnable parameter.\nThey use GNNs to learn how to predict the observed\nbehavior and the latent property, and combine this\npurely bottom-up approach with symbolic regression\nto infer and parameterize a a governing equation.\nWith this approach, they are able to uncover New-\nton's law of gravity and the unobserved masses of\norbital bodies from location over time data alone.\nWe expand the work by Lemos et al. (2023) to pre-\ndict and decompose heterogeneous dynamical sys-\ntems. We train GNNs to reproduce the dynamics of\ncomplex systems together with a low-dimensional em-\nbedding of their latent properties (see Figure 1). In\nsystems with discrete classes of objects, the learned\nembedding reveals the classes as clusters and allows\nto virtually decompose the system. This is a necessary"}, {"title": "2 Methods", "content": "step to infer and parameterize the underlying gov-\nerning equations. In systems with continuous prop-\nerties, the learned embedding reveals the underlying\nmanifold and allows to estimate the corresponding\nparameters.\nFor a diverse set of simulations, we can learn to re-\nproduce the complex dynamics, uncover the structure\nof the underlying heterogeneity, and parameterize\nsymbolic top-down hypotheses of the rules governing\nthe dynamics. In the simplest cases, the interaction\nfunctions were automatically retrieved with symbolic\nregression."}, {"title": "2.1 Simulation of dynamical systems", "content": "We created a diverse set of quasi-physical dynamical\nparticle systems with heterogeneous latent properties\nof individual particles (see Figure 2 and Videos). In all\nsimulations, particles interact with a limited neighbor-\nhood of other particles. They receive messages from\nconnected particles that encode some of their proper-\nties, integrate these messages, and use the result to\nupdate their own properties. This update is either the\nfirst or second derivative over time of their position\nor other dynamical properties.\nFirst, we created simulations of moving particles\nwhose motion is the result of complex interactions\nwith other particles (Lagrangian systems, see Fig-\nure 2a-d). Then, we simulated vector-fields with\ndiffusion-like signal propagation between stationary\nparticles (Eulerian systems, see Figure 2e, f). Some par-\nticles follow an exclusively internal program defined\nby a sequence of hidden states. Finally, we created\ncomplex spatio-temporal signaling networks (Hens\net al., 2019; see Figure 2g).\nAll simulations generate indexed time series by up-\ndating parts of all particle states $x_i$ using explicit or\nsemi-implicit Euler integration\n$X_i \\leftarrow x_i + \\Delta t \\dot{x_i}, \\dot{x_i} \\leftarrow \\dot{x_i} + \\Delta t \\ddot{x_i}$.\n(1)\nThe vector $x_i$ stands for the position of particles in\nmoving dynamical systems, or for other dynamical\nproperties in the vector-field and network simulations.\nThe details of these simulations are listed in Supple-\nmentary Table 1."}, {"title": "2.2 Graph neural networks", "content": "Figure 1 depicts the components of the GNNs to\nmodel dynamical particle systems, and how we train\nthem to predict their dynamical behavior and to reveal\nthe structure of the underlying heterogeneity. A graph\n$G = {V,E}$ consists of a set of nodes $V = {1,...,n}$\nand edges $E \\subset V \\times V$ with node and edge features\ndenoted by $v_i$ and $e_{ij}$ for $i, j \\in V$, respectively. A mes-\nsage passing GNN updates node features by a local\naggregation rule (Gilmer et al., 2017)\n$v_i\\leftarrow \\Phi(v_i, \\bigodot_{j\\in V_i} f(e_{ij}, v_i, v_j)),$\n(2)\nwhere $V_i := {j : (i, j) \\in E}$ is the set of all neighbors of\nnode $i$, $\\Phi$ is the update function, $\\bigodot$ is the aggregation\nfunction, and $f$ is the message passing function. To\nmodel a dynamical particle system, $\\Phi$, $\\bigodot$, and $f$ can\nbe used to represent the time evolution of node states\naccording to pairwise and node-local interactions. The\nnode features $v_i$ include the dynamical node states $x_i$\n($x_i \\in \\mathbb{R}^d$). In models with moving particles, $x_i$ is the\nposition of the particles. In models with stationary\nparticles it stands for their dynamical properties. With\nthis framework, we can model arbitrary dynamical\nparticle systems by using particles as nodes and de-\nsigning an appropriate neighborhood, node and edge\nfeatures, as well as update, aggregation, and message\npassing functions. Either $\\Phi$, $\\bigodot$, or $f$ can be arbitrary\ndifferentiable functions, which includes fully learn-\nable deep neural networks. In our experiments, we\nuse multi-layer perceptrons (MLPs) for such learnable\nfunctions, and typically, only $f$ or parts of $\\Phi$ are fully\nlearnable at a time. The aggregation function $\\bigodot_i$ is\neither the sum or the average of the outputs of all\n$f_{ij}$. The inputs to these functions are application spe-\ncific subsets of the node features, such as the relative\nposition between the particles, $x_j - x_i$, the distance\nbetween particles $d_{ij}$ or the velocity of the particles $\\dot{x_i}$.\nThe latent heterogeneity of the particles is encoded\nby a two-dimensional learnable embedding $a_i$ that is\npart of the node features. These learnable embeddings\nparameterize either $\\Phi$, $\\bigodot$, or $f$ as appropriate.\nThe design choices for neighborhood, learnable\nfunctions, and their parameters are important to de-\nfine what the GNN can learn about the dynamical\nsystem. If either of the learnable functions has access\nto the absolute position $x_i$ of particle node $i$ and the\ntime index $t$, then this function can learn the behavior\nof the particle as a fully independent internal program.\nThis is sometimes desired, e.g. if we want to learn\nthe behavior of an unobserved force-field that impacts\nthe behavior of observable dynamical particles (see\nFigure 4). If the learnable interaction function $f$ has\nonly access to local relative offsets, velocities, and dis-\ntances, then it has to learn to reproduce the dynamics\nbased on these local cues (see Figure 3). Please see\nSupplementary Table 2 for a full description of the"}, {"title": "2.3 Results", "content": ""}, {"title": "2.3.1 Attraction-repulsion", "content": "We simulated a dynamical system of moving par-\nticles whose velocity is the result of aggregated\npairwise attraction-repulsion towards other particles\nwithin a local neighborhood (see Figure 2a and Ap-\npendix A.1.1, 4,800 particles, 250 time-steps). The\nvelocity incurred by pairwise attraction-repulsion de-\npends on the relative position of the other particle and\nthe type of the receiving particle (see Supplementary\nTable 1).\nFor a simulation with three particle types, we visu-\nalized the training progress of the learned embedding\nvectors $a_i$ for all particles $i$, and the learned interac-\ntions $f_{ij}$ (see Supplementary Figure 1 and Video 1).\nFor all experiments, we show an informative projec-\ntion of the learned pairwise interactions (here, speed\nas a function of distance).\nInitialized at 1, the vectors $a_i$ separate and eventu-\nally converge to a clustered embedding that indicates\nthe three distinct node types (and one outlier). The\ncorresponding interaction functions capture the simu-"}, {"title": "3 Conclusion", "content": "We demonstrated that message passing GNNs can\nlearn to replicate the behavior of complex heteroge-\nneous dynamical systems and to uncover the under-\nlying latent properties in an interpretable way that\nfacilitates further analysis. The flexibility in designing\nGNNs to model and train meaningful interactions in\ncomplex systems is impressive and we are looking\nforward to developing them as an integral toolbox\nto uncover the rules governing complex dynamics\nobserved in nature."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 GNN implementation with priors", "content": "The general form of the GNN update rule described in\nSection 2.2 and Equation 2 does not consider applica-\ntion specific parameterization of the individual func-\ntions. As described in Section 2.2 and Supplementary\nTable 2, we define the loss over first and second order\nupdates, respectively. For clarity, we unroll here the\ncombination of interaction function, aggregation, and\nupdate rules and their respective parameterization,\nresulting in one equation to calculate the updates."}, {"title": "A.1.1 Particle systems", "content": "The gravity-like and the Coloumb-like model use sum\nas an aggregator, the attraction-repulsion and boids\nmodel use the average. In addition to the latent learn-\nable vector $a_i$, we parameterized the interaction func-\ntion with the relative distance $d_{ij}$ and relative posi-\ntions $x_j - x_i$ of the two particles making them blind\nto absolute position and other non-local information.\nHence, the update rules to be learned by the GNNs\nbecome\n$\\dot{X_i} = \\bigodot_i f(a_i, d_{ij}, x_j - X_i)$ or\n$\\ddot{X_i} = \\bigodot_i f(a_i, d_{ij}, x_j - X_i)$,\nrespectively. The learnables are the function $f$ and\nthe particle embedding $a_i$. For $f$, we used a five-layer\nMLP with ReLU activation layers for a total of 50,562\ntrainable parameters (hidden dimension = 128, input\ndimension = 5, output dimension = 2). The latent\nvectors $a_i$ have two dimensions. All particles within a\nmaximum radius are connected. Our particles have\nno size and we did not model interactions between\nphysical bodies. Interactions governed by inverse\npower laws have a singularity at $r = 0$ which breaks\noptimization by gradient descent. We therefore had\nto limit connectivity to $d_{ij} > 0.02$ for the gravity-like\nand Coulomb-like models.\nIn the Coulomb-like system, the interaction func-\ntions have both particle embeddings $a_j$ and $a_j$ as input\nvalues, and the corresponding update rule is given by\n$\\ddot{X_i} = f(a_i, a_j, d_{ij}, X_j - X_i)$.\nFor $f$, we used a five-layer MLP with ReLU activation\nlayers for a total of 190,752 trainable parameters (hid-\nden dimension = 256, input dimension = 7, output\ndimension = 2).\nIn the boids model, $f$ has access to the particle ve-\nlocities, yielding the update rule\n$\\ddot{X_i} = f(a_i, d_{ij}, X_j - X_i, \\dot{X_i}, \\dot{X_j})$."}, {"title": "A.1.2 Particle systems affected by a hidden field", "content": "We added a latent external process as a hidden field\nof $10^4$ randomly distributed stationary particles that\ninteract with the moving particles using the same in-\nteraction rules and distance based neighborhood. The\nstationary particles show either a constant image or a\ntime-variant movie that define a latent coefficient $b_j$\nthat modulates the interaction. For simplicity, we can\nwrite the interaction between all particles with this\ncoefficient $b_j$, knowing that for interactions between\nmoving particles $b_j = 1$.\n$\\ddot{X_i} = b_j f(a_i, d_{ij}, X_j - X_i)$.\nWe learn $b_j$ using an MLP with the position $x_j$ and\nthe time-index $t$ (for time-variant processes) as inputs:\n$\\ddot{X_i} = b(x_j,t) f(a_i, d_{ij}, X_j - X_i)$.\nWe used an MLP with periodic activation functions\n(Sitzmann et al., 2020) and 5 hidden layers of dimen-\nsion 128. The total number of trainable parameters for\nthis MLP is 83,201 (hidden dimension = 128, input\ndimension = 3, output dimension = 1)."}, {"title": "A.1.3 Simulation of wave-propagation and reaction-diffusion", "content": "We used the Eulerian representation for the simula-\ntions of wave-propagation and reaction-diffusion. The\nparticle positions are fixed and a vector $u_i$ associated\nwith each node evolves over time. All particles are\nconnected to their neighbors using Delaunay trian-\ngulation. We fixed the interaction and aggregation\nfunctions to be the discrete mesh Laplacian over this\nneighborhood. For wave-propagation, we learn the\nsecond time-derivative of $u_i$\n$\\ddot{u_i} = \\Phi (a_i, \\nabla^2 u_i)$.\nFor $\\Phi$, we used a five-layer MLP with ReLU activation\nlayers for a total of 897 trainable parameters (hidden\ndimension = 16, input size = 3, output size = 1).\nFor the reaction-diffusion simulation, we learn the\nfirst time-derivative of $u_i$\n$\\dot{u_i} = \\Phi (a_i, u_i, \\nabla^2 u_i)$.\n$\\nabla^2 u_i$ can not be calculated on the edges, so we dis-\ncarded the borders during training (1164 nodes out"}, {"title": "A.1.4 Signaling", "content": "The signaling network is described by a set of nodes\nwithout position information connected according to\na symmetric connectivity matrix $A$. We learn\n$\\dot{u_i} = \\Phi(a_i, u_i) + \\sum_{j\\in V_i} A_{ij}f(u_j)$\nas described by Hens et al. (2019), Aguirre and Letel-\nlier (2009), Gao, Barzel, and Barab\u00e1si (2016), Karlebach\nand Shamir (2008), and Stern, Sompolinsky, and Ab-\nbott (2014). The learnables are the functions $\\Phi$ and $f$,\nthe node embedding $a_i$, and the connectivity matrix\n$A$ (960 \u00d7 960). For $\\Phi$, we used a three-layer MLP with\nReLU activation layers for a total of 4,481 trainable pa-\nrameters (hidden dimension = 64, input size = 3, out-\nput size = 1). For $f$, we used a three-layer MLP with\nReLU activation layers for a total of 4,353 trainable\nparameters (hidden dimension = 64, input size = 1,\noutput size = 1). Symmetry of $A$ was enforced dur-\ning training resulting in 17,865 learnable parameters.\nSince no data augmentation is possible, we gener-\nated 100 randomly initialized training-series. Training\nwas unstable for unconstrained next time-step predic-\ntion, but stable when training to predict at least two\nconsecutive time-steps. The loss for two consecutive\ntime-steps is\n$L_{u,t} = \\sum_i (||\\hat{u}_{i,t+1} - u_{i,t+1}||^2 + ||\\hat{u}_{i,t+2} \u2013 u_{i,t+2}||^2)$,\nwith $\\hat{u}_{i,t+2}$ calculated after updating\n$u_{i,t+1} \\leftarrow u_{i,t+1} + \\Delta t \\hat{u}_{i,t+1}$."}, {"title": "A.2 Clustering of GNN's latent vectors and learned interaction functions", "content": "The latent vectors or the learned interaction functions\nare clustered using the SciPy library (Virtanen et al.,\n2020). We used the Euclidean distance metric to cal-\nculate distances between points and performed hier-\narchical clustering using the single linkage algorithm,\nand formed flat clusters using the distance method\nwith a cut-off threshold of 0.01. To cluster the inter-\naction functions, their profiles are first projected to\ntwo dimensions using UMAP dimension reduction.\nThe UMAP projections are next clustered to obtain the\ndifferent classes of interaction functions."}]}