{"title": "KOLMOGOROV-ARNOLD CONVOLUTIONS: DESIGN PRINCIPLES AND EMPIRICAL STUDIES", "authors": ["Ivan Drokin"], "abstract": "The emergence of Kolmogorov-Arnold Networks (KANs) has sparked significant interest and debate within the scientific community. This paper explores the application of KANs in the domain of computer vision (CV). We examine the convolutional version of KANs, considering various nonlinearity options beyond splines, such as Wavelet transforms and a range of polynomials. We propose a parameter-efficient design for Kolmogorov-Arnold convolutional layers and a parameter-efficient finetuning algorithm for pre-trained KAN models, as well as KAN convolutional versions of self-attention and focal modulation layers. We provide empirical evaluations conducted on MNIST, CIFAR10, CIFAR100, Tiny ImageNet, ImageNet1k, and HAM10000 datasets for image classification tasks. Additionally, we explore segmentation tasks, proposing U-Net-like architectures with KAN convolutions, and achieving state-of-the-art results on BUSI, GlaS, and CVC datasets. We summarized all of out finding in a preliminary design guide of KAN convolutional models for computer vision tasks. Furthermore, we investigate regularization techniques for KANs. All experimental code and implementations of convolutional layers and models, pre-trained on ImageNet1k weights are available on GitHub: https://github.com/IvanDrokin/torch-conv-kan.", "sections": [{"title": "1 Introduction", "content": "The rapid evolution of deep learning architectures has significantly advanced the field of computer vision, particularly in tasks that require the analysis of complex spatial data. Convolutional Neural Networks (CNNs), initially proposed by LeCun et al. [1], have become a cornerstone in this domain due to their ability to efficiently process high-dimensional data arrays such as images. These networks typically employ linear transformations followed by activation functions in their convolutional layers to discern spatial relationships, thereby reducing the number of parameters needed to capture intricate patterns in visual data. Since 2012, following the success of AlexNet [2] in the ImageNet classification challenge, CNNs have dominated the field of computer vision until the emergence of Vision Transformers [3]. Innovations such as Residual Networks [4] and Densely Connected networks [5], along with numerous subsequent works, have significantly advanced the achievable quality of models based on convolutional layers, enabling the effective training of very large and deep networks.\nIn segmentation tasks, especially within the biomedical domain, CNNs have also become foundational with the advent of the U-Net [6] architecture, which has subsequently inspired a whole family of U-Net-like architectures for segmentation tasks.\nRecent developments in deep learning have seen the integration of sophisticated mathematical theories into neural network architectures, enhancing their capability to handle complex data structures. One such innovation is the Kolmogorov-Arnold Network (KAN) [7], which leverages the Kolmogorov-Arnold theorem to incorporate splines into its architecture, offering a compelling alternative to traditional Multi-Layer Perceptrons (MLPs). Quickly following the"}, {"title": "2 Related works", "content": "The application of the Kolmogorov-Arnold theorem in neural networks marks a significant theoretical integration that enhances the expressiveness and efficiency of neural models. The theorem, which provides a way to represent any multivariate continuous function as a composition of univariate functions and additions, has been adapted in the design of Kolmogorov-Arnold Networks (KANs). KANs differ from traditional Multi-Layer Perceptrons (MLPs) by replacing linear weight matrices with learnable splines, thus reducing the number of parameters required and potentially improving the generalization capabilities of the network.\nRecent research has proposed several variations of KANs to address specific limitations and enhance their performance.\nFast KAN by Li et al. (2024) introduced an adaptation where B-splines are replaced by Radial Basis Functions (RBFs). This modification aims to reduce the computational overhead associated with splines. The work demonstrated that third-order B-splines used in traditional KANs could be effectively approximated by Gaussian radial basis functions, resulting in FastKAN-a faster implementation of KAN which also functions as an RBF network.\nWavelet-based KANs (Wav-KAN), as presented by Bozorgasl et al. (2024), incorporate wavelet functions into the KAN structure to enhance both interpretability and performance. Wav-KAN leverages the properties of wavelets to efficiently capture high-frequency and low-frequency components of input data, balancing accurate representation of data structures with robustness against overfitting. The implementation employs discrete wavelet transforms (DWT) for"}, {"title": "3 Method", "content": "This section details the methods used in our study, focusing on Kolmogorov-Arnold convolutions and their various adaptations and enhancements. The subsections cover the following topics:\nIn Section 3.1, we provide a brief overview of Kolmogorov-Arnold convolutions, as initially presented in [9], including their formalization and various basis function options such as splines, Radial-Basis Functions, Wavelets, and polynomials. We also introduce the use of Gram polynomials for parameter-efficient fine-tuning.\nSection 3.2 addresses the primary issue with Kolmogorov-Arnold Convolutions\u2014the high number of parameters introduced by the basis functions. We propose a bottleneck version to mitigate this problem, involving a squeezing convolution before and an expanding convolution after applying the basis function. This design includes a mixture of experts for effective implementation.\nIn Section 3.3, we describe the construction of Self-KAGtention layers by substituting traditional convolutions with Kolmogorov-Arnold convolutional layers. Additionally, we introduce Focal KAGN Modulation, where all convolutional layers in the original focal modulation are replaced with Kolmogorov-Arnold convolutional layers."}, {"title": "3.1 Kolmogorov-Arnold Convolutions", "content": "Kolmogorov-Arnold convolutions were presented in [9], in this section we briefly cover the formalization of Kolmogorov-Arnold convolutions.\nKolmogorov-Arnold Convolutions could be stated as follows: the kernel consists of a set of univariate non-linear functions. Suppose we have an input image y, y \u2208 R^{cxnxm}, where c is the number of channels, and n, m are the height and width of an image respectively. Then, KAN-based convolutions with kernel size k could be defined as:\nx_{ij} = \\sum_{d=1}^{c} \\sum_{a=0}^{k-1} \\sum_{b=0}^{k-1} \u03c6_{a,b,d}(y_{d,i+a,j+b}); i = 1, n \u2212 k + 1, j = 1,m \u2013 k + 1\nEach \u03c6 is a univariate non-linear learnable function with its own trainable parameters. In the original paper[7], the authors propose to use this form of the functions:\n\u03c6 = w_b \u00b7 b(x) + f(x), \nf(x) = w_s \u00b7 Spline(x)\nb(x) = SiLU(x) = x/(1+e^{-x})\nSimilar to KANs, other than splines could be chosen as basis function f(x): Radial-Basis Function, Wavelets, polynomials, etc.\nReplacing splines with Gram polynomials was proposed in [14]. The Gram polynomials, or the discrete Chebyshev polynomial, t_n (x) is a polynomial of degree n in x, for n = 0, 1, 2, . . ., N \u2013 1, constructed such that two polynomials of unequal degree are orthogonal with respect to the weight function w(x) = \\sum_{r=0}^{N-1} \u03b4(x-r), with \u03b4(\u00b7) being the Dirac delta function. That is, \u222b t_n(x)t_m(x)w(x)dx = 0 if n \u2260 m. In the case of splines by Gram polynomials, KAN Convolutions are defined as follows.\n\u03c6 = w_b \u00b7 b(x) + f(x), \nf(x) = \\sum_{i=0}^{N+1} w_i t_i(x)\nb(x) = SiLU(x) = x/(1+e^{-x})\nThis reformulation, on one side, allows an option for parameter-efficient fine-tuning of the pre-trained model, and on the other hand, it reduces the number of trainable parameters."}, {"title": "3.2 Bottleneck Kolmogorov-Arnold Convolutions", "content": "The main problem with KAN Convolutions lies in the Spline part of the model. Whatever type of basis function one chooses, the basis introduces a lot of parameters to the model, which leads to higher resource requirements during training and increases the probability of overfitting. To overcome those issues, we propose to use Bottleneck Kolmogorov-Arnold Convolutions (see Fig.1).\nBefore applying basis to input data, we propose to use squeezing convolution with kernel size equal to 1 before applying basis function to input and expanding convolution with kernel size equal to 1 after. Intuitively, it could be considered a one-layer encoder that helps extract meaningful features from the input before processing it via a chosen basis, and then a one-layer decoder decodes it back. residual activation helps to preserve necessary details that could be lost during encoding and decoding of the input."}, {"title": "3.3 Kolmogorov-Arnold Sefl-Attention and Focal Modulation", "content": "In [16], a self-attention layer was introduced where the K, Q, and V projections were replaced by convolutional layers instead of linear projections. In this paper, we propose a similar approach to construct Self-KAGtention layers by substituting traditional convolutions with KAN convolutional layers. Given that the self-attention operation requires O(n\u00b2) memory, where n is the number of tokens or tensor pixels in convolutional models, we suggest using optional bottleneck convolutions with a 1 \u00d7 1 kernel and placing the self-attention layer between these two convolutions. We call this version Self-KAGNtention.\nYang et al. [17] introduce the Focal Modulation layer to serve as a seamless replacement for the Self-Attention Layer. The layer boasts high interpretability, making it a valuable tool for Deep Learning practitioners. Here we propose a Focal KAGN Modulation, where all convolutional layers from the original focal modulation are replaced with KAN convolutional layers. As the Focal Modulation layer uses grouped convolutions in the hierarchical contextualization stream of data processing with the number of groups equals to the filter number, we note that in this case, Bottleneck KAN Convolutions should be replaced by KAN Convolutions."}, {"title": "3.4 Regularizations in Kolmogorov-Arnold Convolutional", "content": "Applying regularization techniques to Kolmogorov-Arnold Convolutional involves straightforward weight and activation penalties. However, dropout requires careful consideration of its placement. Let's describe the polynomial version of Kolmogorov-Arnold Convolutional. Instead of computing splines over x, we could use Gram, Chebyshev, Legendre, and other Polynomials. In this case, we first need to compute the polynomial basis over x, and then perform the weighted sum of. (Fig.3). In that case, we have 3 possible options for dropout placement: before the layer (we will refer to this position as \"Full\"), before polynomial basis calculation (we will refer to this position as \"Poly\"), and before weight application to polynomials (we will refer to this position as \"Degree\").\nThe authors of [7] state the benefits of KANs over MLPs, and one of them was robustness to noise and adversarial attacks. From this observation, we could derive an alternative way of regularization. Instead of zeroing out some fraction of neurons, we could add additive Gaussian noise to a layer's input, thus forcing the model to filter this noise and be more robust against noise in unseen data. Similar to dropout, there are three placements available: \"Full\", \"Poly\", and \"Degree\". More formal, for a given neuron y_i = f (w_i \u00b7 x_i), during noise injection, the neuron output is:\ny'_i = \\begin{cases}\ny_i + \u03b1\u03b5_i, \u03b5_i \u223c N(0, \u03c3\u00b2(y_i)) & \\text{with probability } p \\\\\ny_i & \\text{with probability } 1 \u2212 p\n\\end{cases}\nIn this equation, \u03b1 is a parameter that controls the amount of added noise, and \u03c3\u00b2(y_i) is a variance of the input, computed for each input channel."}, {"title": "3.5 Parameter Efficient Finetuning", "content": "Let's assume we have a pre-trained model L with Gram KAN convolution layers and we want to fine-tune this model on downstream tasks, e.g. classification dataset D = {x_t, y_t}_{t=1}^T. Let G_i(w^r_i, w^p_i) be a i-th layer of the model L, and w^r_i are the weights of residual activation and w^p_i = {w^j_i}_{j=0}^{N+1} are the weights of Gram polynomials (see formula 3.1). Assume that model L has M Gram KAN convolutional layers.\nThen we have several options for parameter-efficient fine-tuning for a downstream task."}, {"title": "4 Experiments", "content": "In this section, we present the following experiments. Section 4.1 includes experiments on the MNIST [18], CIFAR10, and CIFAR100 [19] datasets with different formalizations of KAN convolutions. Section 4.2 presents experiments with various regularizations and hyperparameter optimization. Section 4.3 provides results for Bottleneck convolutions on CIFAR100 and Tiny ImageNet [20] and also considers ResNet-like and DenseNet-like architectures. Section 4.4 presents results of self-attention layers experiments. Section 4.5 presents results for ImageNet1k dataset, and Section 4.6 presents results of parameter-efficient finetuning. Section 4.7 presents results for segmentation tasks."}, {"title": "4.1 Baseline on MNIST, CIFAR10 and CIFAR100", "content": "Baseline models were chosen to be simple networks with 4 and 8 convolutional layers. To reduce the feature's spatial dimensions, convolutions with dilation=2 were used. In the 4-layer model, the second and third convolutions had dilation=2, while in the 8-layer model, the second, third, and sixth convolutions had dilation=2.\nThe number of channels in the convolutions was the same for all models.\n\u2022 For 4 layers: 32, 64, 128, 512\n\u2022 For 8 layers: 2, 64, 128, 512, 1024, 1024, 1024, 1024\nAfter the convolutions, Global Average Pooling was applied, followed by a linear output layer.\nIn the case of classic convolutions, a traditional structure was used: convolution - batch normalization - ReLU. In the case of KAN convolutions, after KAN convolution layer batch normalization and SiLU are applied. All experiments were conducted on an NVIDIA RTX 3090 with identical training parameters.\nIn this section, we are investigating performance of several KANs options: spline-based (KANConv), RBF-based (FastKANConv), Legendre polynomials version (KALNConv), Chebyshev polynomials (KACNConv), Gram polynomials (KAGNConv) and Wavelet KANS (WavKANConv).\nAs we can see from the Table 1, Gram polynomials-based and Wavelet-based versions perform better than other other options, and outperform vanilla convolutions. Due to wavelet-based KANs' higher computational resource requirements, we will focus on Gram KANs as the main basis function option in further research."}, {"title": "4.2 Regularization study and hyperparameters optimization", "content": "Baseline model was chosen to be simple networks with 8 convolutional layers with Gram polynomials as basis functions. To reduce the feature's spatial dimensions, convolutions with dilation=2 were used: the second, third, and sixth convolutions had dilation=2.\nWe explore two sets of convolutional layer filters:"}, {"title": "4.2.1 Regularization study", "content": "Slim\nWide\nOur findings indicate that scaling model width yields better performance than increasing model depth. Additionally, there is no observed benefit in scaling the Gram's degree of the model. However, it is noteworthy that the number of trainable parameters increases rapidly in any configuration, whether depth, width, or degree is scaled. The lack of benefits from depth and degree scaling may be attributed to the relatively small dataset size. It is plausible that larger datasets could produce different outcomes."}, {"title": "4.2.3 Hyperparameters tuning", "content": "To identify an optimal set of hyperparameters and mitigate the risk of overfitting to the test set, we partitioned the CIFAR100 training dataset into new training and validation sets in an 80/20 ratio. Following the completion of the hyperparameter search, we trained the model on the entire CIFAR100 training set and evaluated it on the complete test set. This study aims to determine effective hyperparameters for eight-layer models. The search space and optimal parameters were established after 50 optimization runs, with the best parameters achieving an accuracy of 61.85\n\u2022 L\u2081 activation penalty, optimal value 10-7\n\u2022 L2 activation penalty, optimal value 10-6\n\u2022 L\u2081 weight decay, optimal value 0"}, {"title": "4.3 Bottleneck Kolmogorov-Arnold Convolutional on CIFAR100 and Tiny-Imagenet", "content": "In this section, we conduct a series of experiments on the CIFAR-100 dataset using Bottleneck KAGN convolutional layers. The training parameters are based on the hyperparameter optimization results discussed in Section 4.2.3, all models were trained 200 full epochs. We utilize simple models with 8, 12, and 16 layers, described in Section 4.2.2, a"}, {"title": "4.3.2 Tiny ImageNet", "content": "In this section, we conduct a series of experiments on the Tiny ImageNet dataset. The training parameters are based on the hyperparameter optimization results discussed in 4.2.3, all models were trained 200 full epochs. We employ VGG-like models where hidden linear layers and the final MaxPool layer are replaced with a Global Average Pooling and a single output layer. Additionally, we use a tiny DenseNet model, as described in [21], with all convolutional layers replaced by Bottleneck KAGN convolutionals. We also perform experiments with Mixture of Experts (MoE) models featuring two active experts out of a total of eight.\nThe results, presented in Table 5, indicate that VGG-like models decrease in accuracy as the number of layers increases. In contrast, the MoE version shows significantly better performance on this dataset, suggesting that MoE layers can be an effective approach for scaling KAGN-based models in width.\nAlso, proposed models, VGG-like MoE and Tiny DenseNet, with Gram polynomials KAN convolutions outperform Tiny DenseNet [21] by a significant margin on the Tiny Imagenet dataset."}, {"title": "4.4 Self-KAGNtention", "content": "In this section, we empirically investigate the performance of BottleNeck SelfKAGNtention layers and BottleNeckK-AGN Focal Modulation. We base our experiments on an architecture with eight convolutional layers, supplemented"}, {"title": "4.6 Parameter Efficient Finetuning", "content": "In this section, we use the HAM10000 (\"Human Against Machine with 10000 training images\") dataset [32] to explore the proposed PEFT method for Gram KAN convolution models. The dataset consists of 10015 dermatoscopic images. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions: Actinic keratoses and intraepithelial carcinoma / Bowen's disease, basal cell carcinoma, benign keratosis-like lesions, dermatofibroma, melanoma, melanocytic nevi, and vascular lesions. We use the train, validation, and test split hosted on HuggingFace: https://huggingface.co/datasets/marmal88/skin_cancer.\nWe selected this dataset to evaluate the PEFT method due to its requirement for model adaptation to a novel domain not encountered during pretraining on ImageNet1k. This characteristic makes it an exemplary dataset for our experiments, providing a scenario closely aligned with real-world use cases."}, {"title": "4.7 Segmentation", "content": "In this section, we provide empirical evaluation for U-Net-like segmentation models based on KAGN Convolutional layers on BUSI, GlaS, and CVC-ClinicDB datasets.\nThe BUSI dataset [34] consists of ultrasound images depicting normal, benign, and malignant breast cancer cases, along with their corresponding segmentation maps. In our study, we utilized 647 ultrasound images representing both benign and malignant breast tumors, all consistently resized to 256 x 256 pixels. This dataset provides a comprehensive collection of images that assist in detecting and differentiating various types of breast tumors, offering valuable insights for medical professionals and researchers.\nThe Glas dataset [35] includes 612 Standard Definition (SD) frames from 31 sequences, each with a resolution of 384 \u00d7 288 pixels, collected from 23 patients. This dataset is associated with the Hospital Clinic in Barcelona, Spain. The sequences were recorded using Olympus Q160AL and Q165L devices, paired with an Extra II video processor. For our study, we specifically used 165 images from the GlaS dataset, all resized to 512 \u00d7 512 pixels.\nThe CVC-ClinicDB dataset [35], also known as \"CVC,\" is a publicly accessible resource for polyp diagnosis within colonoscopy videos. It comprises 612 images, each with a resolution of 384 \u00d7 288 pixels, extracted from 31 distinct colonoscopy sequences. These frames offer a diverse array of polyp instances, making them particularly useful for developing and evaluating polyp detection algorithms. To ensure consistency across different datasets in our study, all images from the CVC-ClinicDB dataset were uniformly resized to 256 \u00d7 256 pixels.\nWe explore 3 different U-net-like models: first is a U-net with convolutions replaced by KAGN Convolutional layers, U2-net-like model [12], again with convolutions replaced by KAGN Convolutional layers, and U2-net small model, were all filter number were the same across all hidden layers and equal 16 multiplied by the width scale parameter.\nFor the BUSI, GlaS, and CVC datasets, the batch size was set to 4 and the learning rate was 1e-4, all other parameters were the same as discussed in Section 4.2.3. The loss function was chosen to be a combination of binary cross entropy and dice loss. We randomly split each dataset into 80% training and 20% validation subsets."}, {"title": "5 Ablation Study", "content": "In this section, we present the results of an ablation study of the Bottleneck Kolmogorov-Arnold convolutional layers with Gram polinomials. The baseline sequence for convolutional KAN models includes an activation residual summed with a nonlinearity, followed by a normalization layer (e.g., batch norm, instance norm), and a SiLU activation. The ablation experiments involve excluding one or more elements from this sequence: activation residual, normalization layer, nonlinearity, and replacing the linear bottleneck with a KAN-based bottleneck layer. Experiments were conducted on the MNIST [18], CIFAR-10, CIFAR-100, and Fashion MNIST [41] datasets, and results are shown in Table 10.\nThe results show that except for MNIST, the KAN-based bottleneck leads to training collapse or significant accuracy degradation, and using activation residual degrades performance in half of the cases. This suggests that activation residual may be redundant in some cases and warrants further investigation. Nevertheless, we propose retaining the convolutional layer scheme with activation residual, as the bottleneck approach can lead to information loss, and activation residual can help in its recovery."}, {"title": "6 Design Principles", "content": "Summarizing the experiments conducted, we propose the following preliminary design principles for Kolmogorov-Arnold convolutional networks. It is important to note that our experiments were not exhaustive, and these principles may be revised with new data.\n\u2022 We recommend using Gram polynomials for f(x). Our experiments indicate this choice excels in both quality metrics and the number of trainable parameters.\n\u2022 For scaling Kolmogorov-Arnold convolution-based models, we suggest using the bottleneck version of the layers, which significantly reduces the number of trainable parameters without substantial loss in performance compared to the non-bottleneck version.\n\u2022 Increasing model width generally performs better than increasing depth, as shown by our experiments with simple sequential models. The Mixture of Experts versions of the bottleneck convolution effectively scales model width without a significant increase in inference and training costs.\n\u2022 Preliminary findings suggest that DenseNet-like architectures could serve as a strong foundation for construct-ing very deep Kolmogorov-Arnold convolutional networks.\n\u2022 Our experiments demonstrate that Self KAGNtention layers can enhance the performance of Kolmogorov-Arnold convolutional models."}, {"title": "7 Conclusion", "content": "This paper explores the integration of Kolmogorov-Arnold Networks (KANs) into convolutional neural network architectures, presenting novel approaches and modifications to enhance their performance and efficiency in computer vision tasks. Our work introduces Bottleneck Convolutional Kolmogorov-Arnold layers, a parameter-efficient design that reduces memory requirements and mitigates overfitting issues. Additionally, we propose a parameter-efficient fine-tuning algorithm that significantly decreases the number of trainable parameters needed for adapting pre-trained models to new tasks.\nThrough extensive empirical evaluations on various datasets, including MNIST, CIFAR10, CIFAR100, Tiny ImageNet, ImageNet1k, HAM10000, BUSI, GlaS, and CVC-ClinicDB, we demonstrate that KAN-based convolutional models can achieve state-of-the-art results in both classification and segmentation tasks. Our experiments highlight the effectiveness of Gram polynomials as the basis function for KANs, the advantages of scaling model width over depth, and the potential of DenseNet-like architectures for very deep networks.\nWe further show that incorporating Self KAGNtention layers enhances model performance, particularly in complex tasks, and provide design principles for constructing successful KAN convolutional models. Our proposed models not only outperform traditional convolutional networks but also offer a promising direction for future research in optimizing neural network architectures for computer vision applications.\nOverall, our findings emphasize the potential of Kolmogorov-Arnold Networks in advancing the capabilities of convolutional neural networks, paving the way for more efficient and effective deep learning models. Future work will focus on refining these approaches and exploring their applications in other domains, as well as investigating additional regularization techniques and optimization strategies to further enhance model performance."}]}