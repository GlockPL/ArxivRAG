{"title": "Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques", "authors": ["David Ortiz-Perez", "Manuel Benavent-Lledo", "Jose Garcia-Rodriguez", "David Tom\u00e1s", "M. Flores Vizcaya-Moreno"], "abstract": "Cognitive decline is a natural part of aging, often resulting in reduced cognitive abilities. In some cases, however, this decline is more pronounced, typically due to disorders such as Alzheimer's disease. Early detection of anomalous cognitive decline is crucial, as it can facilitate timely professional intervention. While medical data can help in this detection, it often involves invasive procedures. An alternative approach is to employ non-intrusive techniques such as speech or handwriting analysis, which do not necessarily affect daily activities. This survey reviews the most relevant methodologies that use deep learning techniques to automate the cognitive decline estimation task, including audio, text, and visual processing. We discuss the key features and advantages of each modality and methodology, including state-of-the-art approaches like Transformer architecture and foundation models. In addition, we present works that integrate different modalities to develop multimodal models. We also highlight the most significant datasets and the quantitative results from studies using these resources. From this review, several conclusions emerge. In most cases, the textual modality achieves the best results and is the most relevant for detecting cognitive decline. Moreover, combining various approaches from individual modalities into a multimodal model consistently enhances performance across nearly all scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Cognitive abilities play a crucial role in daily life, constituting a fundamental aspect of our interactions with others. These abilities include reasoning, various memory functions, attention, and language comprehension and communication [1]. As individuals age, cognitive function typically undergoes a natural decline, with certain abilities, such as processing speed, reasoning, and memory, showing more pronounced signs of deterioration, while others remain relatively unaffected. [2]. Despite cognitive decline is a common and natural aspect of aging, some individuals may experience a more accelerated or pronounced deterioration in these abilities due to cognitive disorders, such as dementia [3]. The severity of this deterioration spans a broad spectrum, with Mild Cognitive Impairment (MCI) representing an intermediate stage between typical age-related decline and more severe forms, including different types of dementia [4], [5]. The decline in cognitive abilities due to aging represents a significant challenge for our society, particularly in light of the trend toward an aging population [6]. Nonetheless, not all disorders affecting cognitive abilities are associated with age. One such disorder is aphasia, which is usually caused by strokes [7].\nRecent technological advances, particularly deep learning, have the potential to revolutionize numerous fields, including healthcare. The integration of automation into medical practice offers promising opportunities to increase the efficiency and productivity of healthcare professionals [8], [9]. In the context of cognitive decline estimation, these technologies have demonstrated potential in facilitating early detection [10], [11], allowing timely intervention in the early stages of the disorder. Early detection is crucial, as it enables patients to receive professional treatment from the onset of the disorder. This timely intervention is particularly important, as previous studies have shown that support from doctors, psychiatrists, or psychologists can significantly improve patients' quality of life and help mitigate cognitive decline over time, ultimately benefiting their cognitive and behavioral functioning [12].\nDeep learning methods are known to be data-hungry and can benefit from multiple data sources, each of which presents its own trade-off between contribution to detection, cost, privacy, and degree of intrusiveness to the patient. Many studies rely on medical data, including Magnetic Resonance Images (MRI)"}, {"title": null, "content": "of patients' brains, to diagnose these conditions [13]\u2013[19]. Despite providing relevant information on the disorders and their effects on patient's brains, medical data collection is no- tably invasive. Additionally, these approaches face limitations in terms of feasibility in different contexts. As a result, this study focuses on non-invasive techniques that avoid disrupting patients' daily routines. These alternative approaches rely on video recordings, integrating visual, audio, and textual data. In this context, patient privacy, ease of deployment, and ensuring minimal interference with daily activities are key factors.\nAlthough various reviews and surveys have already ad- dressed cognitive decline estimation, most rely solely on medical imaging, such as MRI or EHR (Electronic Health Records) [20]\u2013[22]. Imaging methods have been also studied specifically for certain conditions, including Alzheimer's or MCI [23]\u2013[27]. Similarly, Alzheimer's or Parkinson's have also been investigated using non-intrusive modalities [28]\u2013 [30]. However, we find a notable deficit in the literature regarding comprehensive studies on overall cognitive decline estimation that leverage non-intrusive data modalities. This work aims to fill this gap in the literature.\nIn this study, we propose a survey focused on the early prediction of cognitive decline using deep learning methods and non-intrusive data. To achieve this, we systematically review existing methodologies in this field and introduce the most relevant datasets for training and evaluating the performance of models designed to predict cognitive decline. Additionally, we discuss the effectiveness of each modality and identify the most suitable approaches for utilizing them. Furthermore, we explore various strategies for combining these modalities into multimodal models to leverage their unique capabilities and features. Employing these modalities enables a more comprehensive understanding of the issue, ultimately leading to improved performance on a variety of tasks. In summary, our main contributions are as follows:"}, {"title": null, "content": "\u2022\tTo the best of our knowledge, this is the first survey fo- cused on the general estimation of cognitive decline using non-intrusive modalities with deep learning methods. The modalities included in this survey are audio, text, video, and image.\n\u2022\tWe systematically review the most relevant works in this scope, categorizing them by modality and approach. Ad- ditionally, we present the combination of these modalities into various multimodal models.\n\u2022\tWe discuss the effectiveness and characteristics of each methodology, both for individual modalities and multi- modal fusions. This analysis allows us to identify the most promising challenges and future research directions. Furthermore, we highlight the key issues and difficulties related to cognitive decline estimation tasks.\n\u2022\tWe provide a comparative analysis of different ap- proaches and their performance on standard datasets and benchmarks, accompanied by a summary of the findings.\nThe remainder of the paper is structured as follows: Sec- tion II introduces background concepts related to cognitive decline disorders and assessments; Section III details the methodology used to select the works included in this study; Section IV presents the most relevant datasets in this area; Section V explores unimodal approaches, while Section VI discusses the combination of these approaches into multimodal frameworks; Section VII provides a brief discussion of the methodologies examined; finally, Section VIII presents the conclusions drawn from this research."}, {"title": "II. BACKGROUND CONCEPTS", "content": "This section defines background concepts essential for un- derstanding how various disorders impact cognitive function in patients. It introduces the most common disorders in this context, along with the primary assessment methods used to evaluate the cognitive state of individuals."}, {"title": "A. Cognitive disorders", "content": "Although cognitive function deteriorates over time, severe deterioration can occur, often due to cognitive disorders. This subsection briefly introduces the main disorders included in this survey that can affect normal cognitive behavior.\n1) Mild Cognitive Impairment (MCI): This syndrome is characterized by a more significant cognitive decline than is typical for an individual of a similar age and educational level. However, this decline is less severe than that experienced in dementia and does not significantly interfere with activities of daily living [31], [32]. MCI often leads to dementia and typi- cally involves difficulties with one or more cognitive functions, such as memory, learning, reasoning, attention, language skills, or loss of interest or motivation [33].\n2) Dementia: In contrast to MCI, dementia is characterized by a more pronounced decline in cognitive abilities, leading to significant challenges for patients in their daily lives [34]. Approximately 70% of dementia cases are associated with Alzheimer's disease, known for its characteristic memory loss [35]. However, Alzheimer's disease also affects other cog- nitive functions, including attention and language skills. As a result, patients often experience difficulties in articulating their thoughts effectively, having trouble finding the appropriate words [36].\n3) Aphasia: This condition is caused by damage to the brain regions, usually resulting from a stroke [7]. Aphasia primarily impairs an individual's ability to communicate, lead- ing to difficulties in both speaking and understanding others' speech. Different types of aphasia affect speech in different ways depending on the affected brain regions, resulting in either simplified speech or overly complex sentences and the use of invented words [37], [38].\n4) Parkinson: Parkinson's disease is a neurodegenerative disease that affects the motor function of people who suffer from it [39]. It causes unintended or uncontrollable move- ments such as shaking, stiffness, and difficulty in balance and coordination. This disease is closely related to dementia, as many people with Parkinson's disease also suffer from it [40]. Although Parkinson's affects the nervous system primarily, it is also associated with a severe decline in cognitive function [41]."}, {"title": null, "content": "5) Apathy: Although not a disease, apathy is present in a variety of disorders, including dementia [42]. It refers to a lack of interest or emotion and is associated with cognitive decline. Apathy negatively impacts global cognition, verbal fluency, and visual and verbal memory [43], [44]."}, {"title": "B. Cognitive tests", "content": "Some tools allow us to accurately measure the cognitive decline over time and identify significant decreases. It is crucial to closely monitor the cognitive status of patients with cognitive disorders to track progression. The most relevant measures include the Mini-Mental State Examination (MMSE) and the Montreal Cognitive Assessment (MOCA) [45].\nThe MMSE consists of 11 questions covering five domains: orientation, registration, attention/calculation, recall, and lan- guage [46]. A perfect score on the MMSE is 30, and a score of 25 or higher usually indicates normal cognitive function [47]. The MMSE is primarily used to assess the risk of dementia.\nIn contrast, the MoCA is designed to detect Mild Cognitive Impairment and early stages of dementia [48]. The MoCA test includes seven domains: executive/visuospatial function, nam- ing, attention, language, abstraction, recall, and orientation. Similar to the MMSE, the MoCA also has a maximum score of 30, but it is composed of 30 questions [49]."}, {"title": "III. REVIEW METHODOLOGY", "content": "A systematic review of existing methodologies in the auto- matic cognitive decline estimation task using Deep Learning techniques was conducted to identify and analyze the most relevant works. This review involved searching for articles in various databases: PubMed\u00b9, Scopus\u00b2, Web of Science\u00b3, and IEEE Xplore4. The search was limited to English-language articles published from 2018 to the present. These dates were chosen due to the introduction of the well-known Transformer model in 2017 [50], which outperforms many earlier meth- ods. Additionally, there has been a significant increase in publications in our area of interest since that year compared to previous years, as shown in Figure 1. The search was performed on October 1, 2024, and was based on titles, abstracts, and metadata. We followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) approach [51]. An overview of the selection process is de- picted in Figure 2.\nThe search strategy included several keywords related to cognitive decline, such as \"cognitive decline\", \"aphasia\", \u201calzheimer\u201d, \u201cdementia\u201d, \u201cMCI\u201d, and \u201ccognitive impairment\u201d. These keywords have been combined with deep learning terms, such as \"deep learning\u201d, \u201cCNN\u201d, \u201cLSTM\u201d, and \u201ctransformer\u201d. Finally, also with the non-intrusive modalities we are tar- geting, such as \"multimodal\", \"speech\u201d, \u201cvideo\u201d, \u201cRGB\u201d, \"linguistic\", and \"audio\". The final query used over the previously mentioned databases was: \"((\"cognitive decline\""}, {"title": null, "content": "OR \u201caphasia\u201d OR \u201calzheimer\u201d OR \u201cdementia\u201d OR \u201cMCI\u201d OR \"cognitive impairment\") AND ((\u201cdeep learning\u201d OR \u201cCNN\u201d OR \u201cLSTM\u201d OR \u201ctransformer\u201d)) AND ((\u201cmultimodal\u201d OR \"speech\u201d OR \u201cvideo\u201d OR \u201cRGB\u201d OR \u201clinguistic\u201d OR \"audio\"))\".\nAfter screening the different works, the first step was to remove duplicates from the different databases. This was followed by a manual review to assess whether the remain- ing papers met the inclusion criteria. Specifically, included works were required to propose, train, and test an artificial intelligence model, excluding reviews or surveys that do not train models. Additionally, the selected papers must use the selected non-invasive modalities, excluding any other. The final criterion focused on the detection of cognitive decline, thereby excluding studies that used the specified modalities for assistive purposes rather than for detection. The resulting"}, {"title": null, "content": "papers were fully read and scored based on the following quality assessment questions:\n\u2022\tDoes the work provide implementation details?\n\u2022\tDoes the work provide detailed information about the models used?\n\u2022\tDoes the work propose different approaches or modali- ties?\n\u2022\tDoes the work provide the source code?\n\u2022\tDoes the work use standard performance metrics (accu- racy, F1-Score, etc.)?\n\u2022\tIs the dataset publicly available?\nBy applying these quality assessment questions, we can ensure that the final works included in our study are of high quality and suitable for further analysis.\nFinally, the selected studies and their results have been cate- gorized based on the modalities and methodologies employed. We propose a taxonomy that first categorizes methods by the modality used and subsequently by the methodology applied. The modality groups include audio, text, vision (image and video), and multimodal combinations. For the audio modality, several methodologies are employed, including the use of ex- tracted audio features, Transformers, Mel Frequency Cepstral Coefficients (MFCCs), and 2D spectrograms. These method- ologies are combined with deep learning techniques such as Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, Support Vector Machines (SVMs), or Transformer architectures. In the case of the text modality, two primary approaches are distinguished: Transformer-based models and Recurrent Neural Networks (RNNs). These can be further integrated with techniques such as CNNs, Gated Recur- rent Units (GRUs), LSTMs, Large Language Models (LLMs), or textual feature extraction. For vision modalities, CNNs are primarily used for image processing, while video analysis often incorporates methods such as video feature extraction, pose estimation, Transformers, or CNNs. Finally, multimodal approaches include the use of multimodal Transformers or fusion strategies, which encompass early fusion, cross-modal attention, late fusion, and joint fusion strategies. Figure 3 presents a taxonomy of the reviewed works, illustrating the different approaches."}, {"title": "IV. DATASETS", "content": "The quality and quantity of data utilized in training deep learning models represents a critical factor. Generally, larger and more diverse datasets result in better model performance by improving the model's ability to generalize across a wider range of scenarios. However, in the domain of cognitive im- pairment detection, the availability of suitable datasets remains a significant challenge. Due to the sensitive nature of data from individuals with cognitive disorders, access to such datasets is often restricted. This section aims to highlight the most relevant and accessible datasets employed in this field. A summary of these datasets can be found in Table I:\n\u2022\tDementiaBank. The DementiaBank dataset [52] is the largest available dataset, comprising 15 corpora with"}, {"title": null, "content": "\u2022\tADRESS. The Alzheimer's Dementia Recognition through Spontaneous Speech (ADRESS) dataset [55] is a balanced and higher-quality subset of the Pitt Corpus Cookie Theft Picture description. It is also a challenge dataset used at the Interspeech 2020 conference. This sub- set has undergone noise removal, voice activity detection, and volume normalization. It includes 78 Alzheimer's patients and 78 control subjects, with data provided in both audio and transcribed text formats.\n\u2022\tADRESSo. The Alzheimer's Dementia Recognition through Spontaneous Speech only (ADRESSo) dataset [56] is a challenge dataset used at the Interspeech 2021 conference, obtained through fluency and image description tasks. In this case, it is not derived from the Pitt Corpus. The dataset contains audio recordings from 87 patients diagnosed with Alzheimer's disease and 79 control subjects, with no accompanying transcriptions.\n\u2022\tADRESS-M. The Multilingual Alzheimer's Dementia Recognition through Spontaneous Speech (ADRESS-M) dataset [57] is a challenge dataset used at the International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023 conference. This dataset consists of audio recordings from Greek and English speakers describing pictures and is not derived from the Pitt Corpus. It includes 148 Alzheimer's patients and 143 controls, with audio being the only modality provided, as it focuses on acoustic features.\n\u2022\tTaukadial. The Taukadial dataset [58] is a challenge dataset used at the Interspeech 2024 conference and con- tains audio recordings from Chinese and English speakers performing three different picture description tasks. It comprises 222 samples from individuals with MCI and 165 control subjects, with only audio data available.\n\u2022\tAphasiaBank. The AphasiaBank dataset [59] comprises various corpora featuring subjects from different regions and languages, including English, Croatian, French, Ital- ian, Mandarin, Romanian, and Spanish. Subjects engage in various conversational tasks with clinicians, with data available in video, audio, and text formats. The dataset"}, {"title": null, "content": "size varies across the corpora and includes a total of 180 aphasic patients and 140 control subjects.\n\u2022\tNCMMSC2021. The National Conference on Man- Machine Speech Communication (NCMMSC2021) dataset consists of audio recordings of subjects performing different tasks, including picture descriptions and fluency tasks. The dataset includes 79 subjects with Alzheimer's, 93 with MCI, and 108 control subjects. Data is provided in audio and text modalities, and the dataset is divided into two versions: one containing short conversations and the other containing longer conversations.\n\u2022\tCCC. The Carolinas Conversation Collection (CCC) [60] is a dataset composed of audio conversations. It com- prises 400 samples of people with Alzheimer's and 200 samples for control. Besides the audio modality, textual transcriptions are provided.\n\u2022\tB-SHARP. The Brain, Stress, Hypertension, and Aging Research Program (B-SHARP) [61] comprises a series of conversations and tasks, including the description of images conducted by a group of subjects. The group is formed by 141 MCI patients and 185 controls. The data modalities are audio and its transcriptions in text.\n\u2022\tI-CONECT. The Internet-Based Conversational Engage- ment Clinical Trial (I-CONECT) dataset [62]\u2013[64] con- sists of different video recordings from subjects having a conversation. This dataset compromises 100 MCI and 86 control subjects. The data is provided in video and audio modalities.\n\u2022\tPC-GITA. The PC-GITA dataset [65] is composed of audio recordings, where people are asked to perform"}, {"title": "V. SINGLE MODALITY", "content": "In the context of cognitive estimation, different modalities provide varied insights into the status of the individual. This section introduces unimodal analysis of audio, text, image, and video data using a variety of approaches depending on the context. It is important to note that many of the presented works focusing on a single modality also propose approaches for other modalities or suggest combining multiple single- modality approaches into a multimodal framework, as will be shown in Section VI. While a particular work may demonstrate robust performance in a given modality, it may not necessarily exhibit the same level of proficiency in other modalities."}, {"title": "A. Audio Modality", "content": "The audio modality represents how humans perceive and process auditory information from the environment, and is typically captured by microphones. This modality is easy to obtain and, in the case of recording speech, can provide valuable insights into how we speak, as well as facilitate the extraction of other crucial modalities, such as text by a transcription process. Besides text transcription, there is also a wide range of features that can be representative, particularly in how we express our thoughts. The way we express our ideas can be very significant in cognitive estimation, as people with more severe decline tend to struggle to find the correct and appropriate words, leading to long pauses, hesitation, and the use of incorrect words among other issues [68], [69]. Furthermore, speech is frequently employed to identify an in- dividual's emotional state, a process known as Speech Emotion Recognition (SER) [70]. SER typically uses a range of speech features, including prosody, pitch, and rhythm to accurately predict emotions such as happiness or fear [71], [72]. Some studies have shown that cognitive disorders negatively affect emotional mood [73], [74], making it another relevant source of information for this task."}, {"title": null, "content": "This section presents an overview of the different methods employed for cognitive estimation using the audio modality, summarized in Table II. This table is organized by first distinguishing the disorder being treated, followed by the dataset used for training and testing. Additionally, it provides the publication year, the methodology that achieved the best performance in each study, and the key metrics previously mentioned: accuracy, F1-Score, and RMSE. The best results for each dataset are highlighted in bold. The main techniques employed are detailed in the following subsections, including 2D Spectrograms, MFCCs, Audio Features, and Transformers methods."}, {"title": "1) 2D Spectorgrams Methods:", "content": "The audio recordings are typically represented by waveforms. These waveforms show the amplitude of the signal as a function of time. The audio modality is considered a one-dimensional data type, consisting of a sequence of samples taken at specific and discrete time intervals. However, there are also multi-channel audio representations, such as stereo, which combine two mono (single-channel) signals, thereby creating a two-dimensional data structure.\nOne of the most significant properties of audio is its sample rate, which indicates the number of samples taken per second of audio. The employment of one-dimensional representations in the field of deep learning can bring many benefits. One such benefit is the simplicity of usage, as the raw data is utilized without further preprocessing. This eliminates the need for additional processing, resulting in low computational cost and training time. Nevertheless, despite the existence of some works that employ raw one-dimensional representations for audio classification, alongside one-dimensional CNNs [99], [100], this is not the prevailing approach [101]. Instead, they are often converted into two-dimensional data, such as spectrograms [102].\nA spectrogram is a visual representation of an audio record-"}, {"title": null, "content": "ing that illustrates the frequency content of the recording over time. This combined time-frequency representation enables deep learning models to capture temporal patterns while also analyzing the frequency spectrum. Consequently, these models can extract more detailed information from audio recordings than from raw waveforms [103], [104]. By showing frequency changes over time, spectrograms make it easier to distinguish sounds with similar time-domain characteristics but different frequency-domain characteristics. Additionally, these repre- sentations are closer to how humans perceive sounds based on frequency and temporal changes, which are effectively captured in spectrograms [105], [106].\nThe use of spectrogram representations offers numerous advantages in deep learning, particularly for general tasks. However, when it comes to human speech, other transforma- tions can be even more beneficial. One such transformation is the Mel Spectrogram [107], a type of spectrogram mapped to the Mel Scale. The Mel Scale was designed to represent frequencies in a way that better reflects how humans perceive sound. Humans do not perceive all frequencies equally; we are more sensitive to differences in lower frequencies than in higher ones. For instance, it's much easier for us to distinguish between sounds at 100 Hz and 200 Hz than between 10,000 Hz and 10,100 Hz. Given this perceptual difference, Mel Spectrograms are especially valuable for processing human speech, as they more accurately capture the nuances of how we hear sounds [108], [109].\nIn order to generate spectrograms and Mel Spectrograms from raw waveforms, it is necessary to apply specific trans-"}, {"title": null, "content": "formations. One crucial step involves the application of the Fourier transform to convert the audio signal into a frequency magnitude representation. The magnitude of each frequency component indicates its influence on the overall audio sample. However, this initial transformation lacks temporal data, which is essential for audio processing as it contains significant information. To ensure the retention of temporal informa- tion throughout the audio sample, a windowing technique is employed. This technique segments the audio into overlap- ping windows over time. Each windowed segment undergoes Fast Fourier Transform (FFT) to convert the signal from the time domain to the frequency domain. The FFT computes the Discrete Fourier Transform (DFT) of a sequence. This process, applied to each segment, is known as Short-Time Fourier Transform (STFT). Combining these processed trans- formations across all segments yields the final spectrogram, providing a two-dimensional representation of the audio that includes both time and frequency information. To obtain a Mel Spectrogram, another transformation must be performed to scale the frequencies to the previously mentioned Mel scale. Figure 4 summarizes the transformations to obtain the spectrogram and Mel Spectrogram from a raw audio recording.\nOnce all the features are extracted into a two-dimensional image representation of the audio signal, image-based models can be used for audio classification tasks. Vision models can extract the most relevant features of an image [110], [111]. In the case of the widely used classical CNNs, a set of different kernels and pooling layers are applied to an image to obtain"}, {"title": null, "content": "more complex features and classify the image [112], [113]. These CNNs have been the traditional state-of-the-art method for image processing [114]. However, the advent of well- known Transformer-based architectures is replacing them. Transformers [50], which were originally designed for text processing, have migrated to other modalities due to their exceptional performance in the text modality field [115], [116]. Among the different modalities where Transformers have migrated, we can distinguish video [117], audio [118], and image [119]. The combination of both extracting spectro- grams, specifically Mel Spectrograms, and using them to feed a CNN has been employed in many fields, for example, in the analysis of music timbre [120]. Additionally, the use of vision Transformer models with the combination of spectrograms has been applied in other fields, such as general audio classifi- cation [121] and even Transformers specialized for spectro- grams [122]. In addition, the Whisper model [123], which is the state-of-the-art model for audio transcription, relies on Transformer architecture. This model also uses spectrograms for processing audio, incorporating convolutional layers to create an encoding for the subsequent Transformer layers.\nThe cognitive estimation task is not an exception regarding these types of methods. The best results have been obtained through the use of Mel Spectrograms. Despite requiring a preprocessing step to convert the audio signal into a spec- trogram, which slightly increases computational time, this process often yields improved performance. For instance, in the work of [93], using spectrograms and 2D CNNs improved Alzheimer's disease detection accuracy from 72.6% with raw waveforms and 1D CNNs to 84.4% using Mel Spectrograms and 2D CNNs, resulting in a relative improvement of 16.3% In addition, [85] achieved a performance of 70.8% in both"}, {"title": null, "content": "accuracy and F1-Score in the Alzheimer recognition task using the ADRESS dataset. In this study, the authors first preprocess the audio signal by applying a Mel Spectrogram process with a window size of 25 ms, retaining only seg- ments of eight or sixteen seconds from the full recording. A distinctive feature of this method is the employment of a Siamese network [124], inspired by various studies utilizing this model for health applications [125], speech emotion recog- nition [126], and speech impairment detection [127]. These Siamese networks learn through contrastive learning applied to two separate models, which, in this study, were CNNs. This contrastive learning approach pulls together segments of the same class (Alzheimer's or Control) and separates segments of different classes. Subsequently, the embeddings obtained from each model serve as inputs to another and final CNN for classification. The downside of this method is that the application of contrastive learning restricts the experiment to classification and does not provide information for regression tasks. Additionally, the authors experimented with feeding a CNN the raw audio signal, but the use of Mel Spectrograms and Siamese networks resulted in a 4.1% improvement. In this work, the authors trained their own CNN and did not use any pre-trained models.\nAnother notable work using Mel Spectrograms is presented in [91], [128] and [92], both focusing on the classification of Alzheimer's using the DementiaBank Pitt Corpus. In the case of [91], Mel Spectrograms were combined with pre-trained CNNs DenseNet [129], MobileNet [130], and ResNet [131], with the best results achieved using pre-trained DenseNet (accuracy of 73.49%). In the case of [92], the proposed pre-trained CNNs included ResNet18 [131], ResNet34 [131], ResNet50 [131], SqueezeNet [132], and VGG16 [133], with the best results obtained using VGG16, achieving an accuracy of 66% and an F1-Score of 62%. When combined with demographic data from patients, the accuracy and F1-Score both improved to 73%.\nIn addition to the use of CNNs to extract features from images, there are also studies where the embeddings obtained from these CNNs are fed into a LSTM model. The LSTM model is a specific type of recurrent neural network that enhances the capabilities of previous recurrent models by integrating long-term dependencies for sequential data [134]. The work of [75] employs this method to predict Alzheimer's using the ADRESSo dataset, achieving an accuracy of 78.9%. In this study, the employed model is the pre-trained VGG16 model. The same method has been used in the work of [98] to predict the severity of Aphasia using the Mandarin Aphasi-aBank dataset. In this work the pre-trained ResNet model has been used as the CNN. This application to Aphasia achieved an RMSE score of 3.53 on the cognitive score of the subjects. This study also proposes the accuracy in determining each level of severity of Aphasia, but not for the differentiation between aphasic and normal cognitive subjects.\nIn the work proposed by S. Siddhant et al. [81], they propose the vision Transformer MVITV2 [135] to analyze the generated Mel Spectrogram. They also provide an ablation"}, {"title": null, "content": "study comparing the results with other previously mentioned CNNs, such as VGG19, ResNet-101, and DenseNet-161. The use of this Transformer-based model outperformed the best result obtained from CNNs by 3%, achieving an accuracy of 62.1 and an F1-Score of 60.7, demonstrating the capabilities of this type of network for this task.\nThere are also similar works over the ADRESS dataset. In another study by I. Loukas et al. [88], the performance of the vision Transformer ViT [119] is compared with various pre-trained CNNs, including GoogLeNet [136], ResNet50 [131], WideResNet-50, [137] AlexNet [138], SqueezeNet [132], DenseNet-201 [129], MobileNetV2 [139], MnasNet1 [140], ResNeXt-50 [131], VGG16 [133], and EfficientNet-B2 [141]. The performance of this vision transformer outperformed every single CNN model by at least 2% in accuracy, achieving up to 65% in accuracy and a 69.76% F1-Score. In addition, the work in [87] includes an ablation study comparing the performance of CNNs and vision Transformers. In this study, the CNNs tested were MobileNet and YamNet, while the vision Transformer model used was Speechbert [142]. Those CNNs had as input MFCCs, a method which will be detailed in the following subsection. The Speechbert model using MFS outperformed the best CNN, MobileNet, by more than 7% in accuracy, achieving up to 66.67% in accuracy.\nIn the work [82], authors propose the use of the Au- dio Spectrogram Transformer model [122], fed with a Mel Spectrogram, which achieved an accuracy of 74% over the ADRESS dataset."}, {"title": "2) MFCCs Methods:", "content": "Further preprocessing of audio sam- ples can be performed to obtain MFCCs. This represents a more compact representation of the audio sample, attempting to convey the overall shape of a spectral envelope [143]. In addition to mapping frequencies to the Mel scale, MFCCs transform these features into the cepstral domain, which is useful for a variety of audio and speech processing tasks [144], [145]. This methodology has also been applied in the health field for tasks such as heart sound classification [146] and auto- matic depression detection [147]. However, this representation requires additional processing steps in comparison to Mel Spectrograms. This representation is computed by applying a logarithm function to a Mel Spectrogram. After this logarithm procedure, a Discrete Cosine Transform (DCT) is applied to decorrelate these features and reduce their dimensionality. An example of this representation can be seen in Figure 5."}, {"title": null, "content": "The primary benefit of using this methodology with Mel Spectrograms is the more compact information and lower dimensionality of data. Additionally, the main idea of this method is to keep the most relevant data regarding human speech tasks. However, the main disadvantage is the need for additional processing steps and the potential loss of informa- tion during the discretization of the representation.\nIn the domain of cognitive estimation tasks, the study that achieved the most favorable results when employing MFCCS was conducted by A. Meghanani et al. [89]. In this research, the effectiveness of both Log Mel Spectrograms and MFCCS was investigated using a combination of CNNs and LSTMs for feature classification. The experiments revealed that com- bining MFCCs with CNNs and LSTMs produced superior accuracy results, outperforming the Log Mel Spectrogram model by 6%, achieving up to 64.58% accuracy. For the Log Mel Spectrograms, employing a pre-trained ResNet model improved accuracy to 62.50%, but it is not sufficient to improve the results obtained using MFCCs. Nevertheless, in the regression task, the Log Mel Spectrograms demonstrated the most optimal performance, achieving an RMSE of 5.9."}, {"title": "3) Audio Features Methods:", "content": "When listening to speech, it is possible to derive several statistical acoustic features, including frequency, jitter, and shimmer. These values can be obtained using a variety of toolkits that process audio signals and produce different sets of features. Among the most relevant toolkits for this extraction are OpenSMILE [148", "149": "COVAREP [150", "151": "and"}]}