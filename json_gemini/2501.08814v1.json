{"title": "SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector", "authors": ["Kyeongryul Lee", "Heehyeon Kim", "Joyce Jiyoung Whang"], "abstract": "The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments. Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored. Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities. In addition, we propose a Systematic data generatIon Framework for evaluating the risks of generative AI (SAIF). SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types. It ensures the system-atic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks. Furthermore, SAIF is designed to ac-commodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios. We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector.", "sections": [{"title": "Introduction", "content": "Generative AI has increasingly been integrated into the public sector, demonstrating its potential to improve operational efficiency, support complex decision-making, and enhance public interaction (Nelson et al. 2024; Beltran, Ruiz Mondragon, and Han 2024). Governments across the globe are adopting generative AI to tackle a wide range of administrative and operational challenges. For example, the U.S. Department of Homeland Security's Emma chatbot addresses over a million immigration-related inquiries monthly, improving service accessibility and enhancing efficiency (U.S. Citizenship and Immigration Services 2018). In Canada, the city of Kelowna has partnered with Microsoft to integrate generative AI into its housing permit process, automating approvals, delivering information, and providing user support (City of Kelowna 2024). These initiatives highlight the transformative potential of generative AI in the public sector, from facilitating administrative workflows to enhancing decision-making processes. However, the integration of generative AI into the public sector also raises significant concerns (Bright et al. 2024). For example, generative AI has been misused to create deceptive content such as fake news and phishing emails, facilitating identity fraud and defamation. These risks are particularly acute in the public sector, where government services must uphold a responsibility to ensure regulatory compliance and safeguard societal trust (Beltran, Ruiz Mondragon, and Han 2024). Additionally, its multimodal capabilities hold the potential to enhance service delivery and streamline complex workflows, requiring rigorous assessments to ensure responsible deployment.\nWe examine well-established risk taxonomies of generative AI within the public sector and further expand the scope to include a multimodal perspective. Moreover, we propose a Systematic data generatIon Framework (SAIF) for evaluating the risks of generative AI, ensuring consistent data generation and establishing a solid foundation for mitigation strategies. In summary, our contributions are as follows:\n\u2022 We examine the specific challenges and requirements of deploying generative AI in the public sector by revisiting an established risk taxonomy.\n\u2022 We broaden the scope of risk evaluation by incorporating multimodal capabilities, providing an in-depth analysis of risks relevant to public sector applications.\n\u2022 We introduce SAIF, a systematic framework for evaluating generative AI risks, designed to encompass diverse jailbreak methods and prompt types (Figure 1)."}, {"title": "Related Work", "content": "Generative AI, including Large Language Models (LLMs) and Large Multimodal Models (LMMs), has emerged as a groundbreaking advancement across diverse domains (Jing Yu, Daniel, and Ruslan 2023). These models are rapidly adopted for various tasks such as natural language understanding, content generation, and multimodal reasoning (Chen et al. 2024). Notable examples include GPT-4, LLaMA (Touvron et al. 2023), and PaLM (Driess et al. 2023), as well as multimodal models like GPT-4 Vision (Yu et al. 2024a), Gemini, and Flamingo (Jean-Baptist et al. 2022). However, significant concerns have been raised regarding the potential risks of these models, such as bias propagation and unintended behaviors (Zeng et al. 2024;"}, {"title": "Risks of Generative AI in Public Sector", "content": "The integration of generative AI into the public sector introduces unprecedented risks that should be thoroughly examined (Bright et al. 2024; Esposito and Tse 2024; Beltran, Ruiz Mondragon, and Han 2024). Building on a well-established taxonomy of Al risks derived from 8 government policies and 16 corporate guidelines (Zeng et al. 2024), we revisit the risk categories within the context of the public sector. Our risk factors involve system and operational risks, content safety risks, societal risks, and legal and rights-related risks. We expand its scope further to incorporate the threats posed by the multimodal capabilities."}, {"title": "System and Operational Misuse Risks", "content": "The technical vulnerabilities and potential misuse of generative AI pose significant threats that can undermine the reliability of public services. System risks primarily stem from security weaknesses in AI systems (Zeng et al. 2024). For instance, a prompt injection attack could exploit the vulnerabilities to expose sensitive personal information, such as social security numbers and facial images for personal identification (Schwartzman 2024; Rehberger 2024). This could critically damage public trust in governmental institutions and result in identity theft, privacy violations, and other detrimental consequences for individuals.\nOn the other hand, operational misuse risks can arise when generative AI deviates from its intended purpose of public services. In particular, when generative AI is incorporated into decision-support systems of governmental institutions, its inherent biases can lead to unfair treatment of certain groups (Gordon 2023; Hacker et al. 2024). For example, generative AI employed in immigration screening or interview systems may reflect the race or origin of applicants in a biased manner, causing discriminatory decisions that damage fairness and public trust (Hamidieh et al. 2023). Such deviations can undermine the integrity of public services and flawed decisions, which hinder trustworthiness."}, {"title": "Content Safety Risks", "content": "Content safety risks in the public sector stem from generative AI producing harmful, misleading, or inappropriate content, especially in public communication and information dissemination (Beltran, Ruiz Mondragon, and Han 2024). For example, mental health support chatbots for public services could inappropriately respond to users in crisis, such as those at risk of self-harm or suicide, potentially exacerbating their distress (Grabb, Lamparth, and Vasan 2024). Additionally, in public education, generative AI could inadvertently produce inappropriate content, such as sexually suggestive images, when generating visual aids or responding to user prompts (Park, Singh, and Wisniewski 2024). Such failures not only expose individuals to risks but also diminish the overall standard of public services."}, {"title": "Societal Risks", "content": "Societal risks posed by generative AI encompass its potential to disrupt social stability and undermine established norms (Zeng et al. 2024). In public services, particularly those involving sensitive personal data such as healthcare and social welfare, the unintended retention of per-"}, {"title": "Legal and Rights-Related Risks", "content": "Legal and rights-related risks involve legal challenges and human rights violations, which are central to the responsibility of governments and public institutions to protect human dignity and fundamental rights (Beltran, Ruiz Mondragon, and Han 2024; Zeng et al. 2024). Generative AI can lead to severe legal consequences, potentially undermining the legitimacy of public services. One of the key capabilities of generative AI is to create content that closely resembles existing material, raising significant copyright concerns (\u0160ar\u010devi\u0107 et al. 2024; Shukla et al. 2022). For instance, generative AI used in public education could unintentionally incorporate copyrighted content, leading to legal repercussions, including the obligation to compensate the copyright holder (Dzuong, Wang, and Zhang 2024; Mantri and Sasikumar 2023). In addition, generative AI has the potential to produce inaccurate or defamatory information about individuals or organizations, which could lead to lawsuits. For example, chatbots used in public welfare services might provide inaccurate information about government welfare benefits or introduce errors in the application process, causing citizens to either fail to receive the benefits they are entitled to or follow incorrect procedures (Chen and Shu 2024). Such risks may expose public institutions to legal disputes, further damaging their reputation and credibility."}, {"title": "Systematic Data Generation Framework for Evaluating the Risks of Generative AI", "content": "Although there has been a recent effort to generate datasets focusing on specific risk factors, systematic methodologies for generating the datasets, which can be extended to a wide range of risk factors have been rarely explored. This issue is especially apparent in areas like the public sector, where generative AI faces specific challenges and requirements. In addition, the risks associated with text, images, video, and other modalities in generative AI must be fully addressed. Therefore, we propose SAIF, designed to incorporate existing risk taxonomies, potential scenarios, diverse jailbreak methods, and prompt types, and multimodalities. SAIF generates prompt data in four stages as illustrated in Figure 1."}, {"title": "Breaking Down Risks", "content": "The first stage of the data generation involves selecting specific subtopics that are closely related to the target risk factors. These subtopics represent relevant themes within each risk factor. For instance, for content safety risk, the subtopics could involve sexual content, offensive content, or child safety content. Each subtopic serves to refine the scope of the evaluation, ensuring that the data addresses the core aspects of the risk. As shown in Table 1, the subtopics of each risk factor are carefully chosen to reflect the diverse cases that may arise in the deployment of generative AI."}, {"title": "Designing Scenarios", "content": "Once subtopics are identified, the next step is to design relevant scenarios that reflect how generative AI could respond in different situations. These scenarios are mainly based on the modalities of generative AI, such as text, images, or video, which each carry their own specific risks and potential outcomes. For instance, a scenario for a text-based modality might involve generating hate speech-language, whereas a scenario for an image modality might involve generating offensive visual content. By reflecting various scenarios for each subtopic, this stage helps ensure that the evaluation covers a broad range of possible use cases and effectively minimizes potential risks in different contexts."}, {"title": "Applying Jailbreak Methods", "content": "The next step involves applying jailbreak methods to requests to assess the resilience of generative AI against malicious attempts to bypass its safeguards. For example, refusal suppression prevents generative AI from refusal responses such as \u201cnot possible\u201d, \u201cnot allowed\", \"sorry\u201d by injecting a prefix into the request that instructs the model not to refuse in response to requests (Yuanwei et al. 2023;"}, {"title": "Exploring Prompt Types", "content": "Exploring prompt types involves expressing jailbreak requests through various prompt types. This approach aims to assess whether the model can resist additional subtle manipulations and coercive prompts, by testing how generative AI behaves in response to different instructions. One prominent prompting technique is Chain-of-Thought (CoT) (Wei et al. 2022), which structures the responses of the model into step-by-step reasoning to provoke responses aligned with the user's intent. Other approaches include zero-shot CoT (Kojima et al. 2022) that enables the model to reason independently without predefined tasks, role-playing (White et al. 2023) that assigns specific roles to the model to induce elicit outputs for targeted tasks, expert prompting (Ajith et al. 2024) that generates outputs based on domain knowledge provided by experts, rails (White et al. 2023) that restrict outputs according to predefined rules, and reflection (Shinn et al. 2023) that encourages the model to evaluate its responses and iteratively revise them. This diversity in prompt types can help comprehensively assess its behavior from various perspectives, thereby enhancing the overall safety and reliability of generative AI."}, {"title": "Implications for Public Sector Applications", "content": "We propose SAIF to assess the risks associated with the deployment of generative AI in the public sector, which helps identify vulnerabilities and improve overall safety and reliability. In addition, SAIF serves as a consistent and scalable pipeline that ensures effective handling of evolving risk scenarios, jailbreak methods, and prompt types, while also accounting for multimodal capabilities. However, addressing the risks identified by our framework requires considering several factors. For example, excessive training focused on jailbreak prevention or various prompt types may lead to delays in Al response time or overly strict output criteria. Additionally, strict privacy laws and regulations in the public sector could impose operational constraints. Therefore, it is crucial to utilize our framework by concentrating on essential jailbreak prevention prompt types and specific risk factors to effectively and reliably carry out public missions."}, {"title": "Conclusion and Future Work", "content": "In this paper, we propose SAIF, a scalable and systematic framework for evaluating the risks of generative AI by incorporating diverse jailbreak methods and prompt types. The SAIF framework embraces emerging techniques aimed at evading the safeguards of generative AI, which is increasingly being employed in real-world public missions. Furthermore, we extend the scope of SAIF to a multimodal perspective, allowing it to comprehensively mitigate the risks.\nWe plan to integrate knowledge graphs (KGs) into the risk breakdown stage, enabling a more diverse and rigorous exploration of risk-related subtopics by leveraging contextually grounded relationships (Lee, Chung, and Whang 2023; Lee et al. 2023; Chung, Lee, and Whang 2023; Chung and Whang 2023). Moreover, incorporating compositional reasoning with fine-tuned LLMs will strengthen the reliabil-"}]}