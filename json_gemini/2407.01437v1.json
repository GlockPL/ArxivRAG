{"title": "Needle in the Haystack for Memory Based Large Language Models", "authors": ["Subhajit Chaudhury", "Soham Dan", "Payel Das", "Georgios Kollias", "Elliot Nelson"], "abstract": "In this paper, we demonstrate the benefits of using memory augmented Large Language Model (LLM) architecture in improving the recall abilities of facts from a potentially long context. As a case study we test LARIMAR, a recently proposed LLM architecture which augments a LLM decoder with an external associative memory, on several long-context recall tasks, including passkey and needle-in-the-haystack tests. We demonstrate that the external memory can be adapted at test time to handle contexts much longer than those seen during training, while keeping readouts from the memory recognizable to the trained decoder and without increasing GPU memory footprint. Compared to alternative architectures for long-context recall tasks with models of a comparable parameter count, LARIMAR is able to maintain strong performance without any task-specific training.", "sections": [{"title": "1 Introduction", "content": "One of the most important abilities of large language models (LLMs) is retrieving information from the text input that is included in the prompt, which enables generating contextually grounded responses. The length of the context an LLM can process during inference is therefore an important factor controlling the generation quality. Vanilla transformer models suffer from quadratic memory and computation complexity with respect to the length of the input sequence due to the self-attention mechanism. Further, the global and local information get mixed and blurred while processing long context. Recent works have suggested different attention mechanisms in transformer-based LLM architectures to address such issues with long-context modeling (Munkhdalai et al., 2024; Hwang et al., 2024; Beltagy et al., 2020; Dai et al., 2019; Bulatov et al., 2022). For example, transformers with Sliding Window Attention have been proposed that show $O(L \\times W)$ complexity for input length $L$ and window size $W$ (Beltagy et al., 2020). Memory of the past segments has been stored in a cache to be used as a context for the next segment (Dai et al., 2019) or models have been trained to learn explicit global memory tokens aka. soft prompts (Burtsev et al., 2021; Bulatov et al., 2022; Hwang et al., 2024). Recently, Infini-transformer (Munkhdalai et al., 2024) has been proposed to use memory of all past segments, as opposed to considering only the memory of the last segment in processing the current segment and discarding all other past segments. Those works often require task-specific training on long-context instances and lack generalization.\nAs an alternative, here we investigate test-time long-context adaptation of LARIMAR, a recently proposed (Das et al., 2024) LLM decoder model augmented with dynamically updatable memory mechanisms, and test its in-context recall performance on long-context modeling tasks. The external memory is structured similarly to the Kanerva Machine (Wu et al., 2018), but updates the memory by finding the least-squares solution to a linear system, following (Pham et al., 2021), instead of updating a multivariate Gaussian posterior distribution. While the model's training used relatively short contexts, we show that it can generalize to much longer contexts when only a small part of the context is task-relevant. This is because the external memory can be enlarged at test time to store and retrieve information from arbitrarily long contexts. Even if the context length vastly exceeds the training distribution, our model will generalize to the task as long as the memory readout (a single encoding) is in distribution as an input to the decoder.\nIn this paper, we make the following contributions:\n\u2022 We introduce a method for writing long contexts to an external associative memory, with read/write memory operations that use a prefix"}, {"title": "2 Background", "content": "In this section, we review our model architecture and describe memory operations used for long-context recall tasks.\nNotation. We use lower-case characters to denote individual key or value vectors: $z \\in \\mathbb{R}^C$, $w \\in \\mathbb{R}^K$, with latent embedding dimension $C$ and memory size $K$, and upper-case for matrices of $N > 1$ keys or values: $Z \\in \\mathbb{R}^{N \\times C}$, $W \\in \\mathbb{R}^{N \\times K}$."}, {"title": "2.1 Larimar Architecture Overview", "content": "The language model (LM) employed here is Larimar (Das et al., 2024), which uses an encoder-decoder architecture (Li et al., 2020) trained together with a linear associative memory module (Pham et al., 2021). The LM encoder and decoder are trained to reconstruct an episode of input text samples as follows: The text is encoded in the latent space and written to the memory, the memory is queried, and the readout from the memory conditions the decoder (see (Das et al., 2024) for details). (Note that the decoder receives as input text a query about the context, but only accesses the full context which is stored in the external memory \u2013 via the memory readout channel.) The loss objective used is a combination of a cross-entropy reconstruction loss and an auto-encoder loss. During inference, the context is divided into $N$ segments, each of which are encoded and then written to memory. In our experiments, we use the following memory operations, differing slightly from (Das et al., 2024).\nWriting. To write a segment of text to memory, we first compute its encoding $z$, along with a writing key vector $w$ defined below. Given arrays of new encodings $Z$ and corresponding key vectors $W$, the memory matrix $M \\in \\mathbb{R}^{K \\times C}$ is obtained as the solution\n$M = W^{+}Z$ (1)\nto the least-squares problem of minimizing the readout error $||Z - WM||$ (Kohonen and Ruohonen, 1973; Bau et al., 2020).\nReading. We assume the context ends in a final query (partial sentence), which is treated differently from the preceding context. Instead of writing to memory, the query is used to compute a reading key $w$, with which an encoding\n$z_{\\text{read}} = WM$ (2)\nis read out from memory and passed as input to the decoder.\nReading and Writing Keys. The reading and writing keys used with a given encoding $z$ are computed as a function $w = f(\\tilde{z}|M)$ of an encoding $\\tilde{z}$ (which may differ from the encoding $z$ written to memory), conditional on a fixed \"key memory\" $M \\in \\mathbb{R}^{K \\times C}$ (distinct from $M$) which is used exclusively to compute key vectors $w$. The encoding $\\tilde{z}$ can be obtained using a fixed-length prefix of the text to be written (when writing) or the query text (when reading), or alternatively can be the full text encoding $z$. Using a prefix can lead to reading and writing key vectors which are more similar. The function $f$ is defined as follows. Given the encoding $\\tilde{z}$, we select the nearest neighbor row in the key memory matrix,\n$k^{*}(\\tilde{z}) := \\arg\\min_{k}||\\tilde{z} - M_{k,:||_2}$. (3)\nThe corresponding one-hot key vector is\n$w_k = \\mathbf{1}(k = k^{*}(\\tilde{z})),$ (4)\nwhere $\\mathbf{1}(x) = 1$ when $x$ is True, and 0 otherwise. This ensures that the rows of $M$ in Eq. (1) are simply the encodings $Z$, and that the memory readout vector in Eq. (2) is simply the $k^{*}(\\tilde{z})$'th row of $M$, that is, $z_{\\text{read}} = M_{k^{*}(\\tilde{z}),:}$. Lastly, we set the rows of the key memory $M$ to the prefix encodings $\\tilde{z}$ of each in-context sentence. This ensures that, in the limit where the query prefix encoding $\\tilde{z} = \\tilde{z}_{query}$ is very close to a particular prefix encoding used when writing to memory (e.g. $\\tilde{z} = \\tilde{z}_{needle}$), the nearest neighbor row, $k^{*}(\\tilde{z})$ is in fact the row of $M$ where the corresponding sentence (e.g. $z = Z_{needle}$) was written.\nIn general, the time complexity of writing to memory is set by the $O(K^3)$ cost of the pseudoinverse $W^{+}. However, when key vectors are one-hot, Eq. (4), and furthermore when the nearest neighbor locations $k^{*}(\\tilde{z})$ are unique for each segment encoding $\\tilde{z}$ (as in the case where these encodings populate the rows of $M$) and the memory size $K$ is set to the $O(N)$ number of unique segments, then $W$ is a permutation of the identity matrix. In this case, and with the $O(N)$ computations of key vectors running in parallel, the overall runtime of computing $M$ (as well as reading from it) is $O(N)$."}, {"title": "3 Experiments", "content": "We conducted two experiments to evaluate long-context recall. These experiments collectively involved $O(10)$ GPU hours with an A100 40GB GPU.\nPasskey test.\nWe test Larimar on the passkey test as defined in (Mohtashami and Jaggi, 2023). The context is divided into sentences, which are each written to memory, with the exception of the final prompt \"The pass key is,\", which is fed into the decoder along with the memory readout.\nIn Table 1 we report the the average retrieval accuracy compared to the Zero-shot accuracy of Infini-Transformer (Munkhdalai et al., 2024). Importantly, because the context has only a small number of distinct sentences, regardless of the context length, only a small number of memory slots are used. All repeats of a given sentence are written to the same memory slot $k^{*}(z)$ in Eq. (3). Consequently, the memory readout and model generation are independent of the context length; while we only report up to 1M tokens in Table 1, the same results will hold for arbitrarily long contexts. (This will not hold for contexts where the number of unique text segments grows with context length.) While Table 1 assumes a single random passkey number, we also evaluated (at 1.2M tokens) the average recall rate over 100 random numbers with $n$ digits, with results shown in Table 2.\nNote that our method is invariant to the order in which sentences are written to memory, resulting in equivalent performance for any position of the passkey (or needle sentence, below) within the context.\nNeedle-in-the-Haystack.\nWe follow (Kamradt, 2023), using the \"haystack\" dataset of Paul Graham essays, for which the total context length is $\\approx$ 137K tokens. We test completion of needle sentences of the form \"The magic number is X\" with final prompt \"The magic number is\" following the context, as well as the \u201cSan Francisco\u201d needle (Kamradt, 2023).\nFor Larimar, when computing key vectors, we used the encoding $z$ of the four-word prefix of each sentence. We set the memory size $K$ to the total number of sentences $N$, and write each encoded sentence to a unique memory slot using the key vector method of section 2.1. This incurs a memory storage cost that scales linearly with the context length, similar to (Kuratov et al., 2024). However, by keeping the external memory and read/write operations on the CPU (with encoding and memory-conditioned decoding on the GPU), we avoid an increased GPU memory cost and are able to handle much longer context lengths.\nWe compare to Mistral 7B v0.2 and Phi-3-Mini-128K-Instruct (3.8B parameters) as baseline methods for long-context recall. For the Mistral model, we use 1200-sentence ($\\approx$ 24K tokens) subsets of the full haystack dataset to evaluate the model at slightly less than its 32K token context limit. For the Phi-3 model, we use a 5000-sentence subset ($\\approx$ 100K tokens). We average over needle positions distributed uniformly throughout the context.\nOur results are reported in Table 3, and show Larimar's ability to maintain strong recall at over 100K context tokens, while baseline models struggle with shorter contexts. For Larimar, note that the drop in recall from 3 to 4 digits reflects the increased difficulty of reconstructing the needle from the original encoding, rather than an increase in difficulty in locating the needle encoding in memory. The benefit of using a shorter, fixed-length prefix"}, {"title": "4 Discussion", "content": "Recent approaches to long-context retrieval have shown good performance after fine-tuning smaller models on needle-in-the-haystack tasks. RMT (Bulatov et al., 2022) and RMT-Retrieval (Kuratov et al., 2024) have shown near-perfect recall out to 1M tokens, with task-specific training of GPT2 (137M parameters). Infini-attention (Munkhdalai et al., 2024) with fine-tuning on the passkey test obtains 100% recall with context lengths up to 1M tokens. Without fine-tuning, however, recall drops to O(10%) at context length 128K (Table 1).\nOn the other hand, larger models have shown strong performance on needle-in-the-haystack tests without task-specific fine-tuning, but incur additional training and inference costs due to their larger size.\nIn comparison to these approaches, we aim for strong recall performance with a more compact model (1.3B parameters), without resorting to task-specific training. Our model was trained on a generic text dataset using a subset of Wikipedia data, and can be adapted to novel context data during inference, with the reduced latency and inference costs of a smaller model (Das et al., 2024).\nIn our experiments, we allow the memory space to grow in proportion with the context length. While this increases space complexity, we emphasize that it does not increase the storage space needed on the GPU, since all memory operations can be performed on one or more CPUs. (A single long-context query requires (i) the encoded context to be moved to the CPU for writing to memory, and (ii) the memory readout to be moved back to the GPU for decoding, with the decoder input sequence length being $\\le$ 0(100) tokens regardless of the context length.) Overall, we emphasize that the external memory size can be adjusted as needed depending on the task and context, with the memory-conditioned decoder training allowing the model to adapt to variable-sized contexts with unseen data. Deepening memory hierarchy in hardware by adding disk storage to CPU RAM can further expand available space and flexibility in offloading limited GPU memory (Sheng et al., 2023).\nIn the future, we aim to explore more general methods for computing reading and writing keys conditional on the full context, such that memory space can be dynamically allocated to context data that is more task-relevant and/or more surprising to the model, allowing for more predictable parts of the context to be stored in memory with correspondingly fewer bytes of information."}, {"title": "5 Limitations", "content": "An algorithmic limitation of our approach is that, after dividing the context into segments (e.g. sentences), each segment is written to memory in isolation, losing the information in cross-segment correlations and the sequence order of segments. Our method is thus most relevant for tasks where the relevant information is within individual segments. It could also be incorporated into more general architectures that extract context information in long-range correlations before writing to an external memory."}, {"title": "A Potential Risks", "content": "Our paper describes an approach to long-context recall for language models using an external memory. Language models with increased memory and recall capabilities introduce potential risks via misuse, and should only be deployed with appropriate guardrails. Our experiments only involved publicly available data without sensitive information, and we only applied our method on a relatively small 1.3B parameter model, with lower capability levels and potential for misuse compared to larger language models."}]}