{"title": "Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs on Compute-in-Memory Crossbars", "authors": ["Matheus Farias", "H. T. Kung"], "abstract": "We introduce sorted weight sectioning (SWS): a weight allocation algorithm that places sorted deep neural network (DNN) weight sections on bit-sliced compute-in-memory (CIM) crossbars to reduce analog-to-digital converter (ADC) energy consumption. Data conversions are the most energy-intensive process in crossbar operation. SWS effectively reduces this cost leveraging (1) small weights and (2) zero weights (weight sparsity). DNN weights follow bell-shaped distributions, with most weights near zero. Using SWS, we only need low-order crossbar columns for sections with low-magnitude weights. This reduces the quantity and resolution of ADCs used, exponentially decreasing ADC energy costs without significantly degrading DNN accuracy. Unstructured sparsification further sharpens the weight distribution with small accuracy loss. However, it presents challenges in hardware tracking of zeros: we cannot switch zero rows to other layer weights in unsorted crossbars without index matching. SWS efficiently addresses unstructured sparse models using offline remapping of zeros into earlier sections, which reveals full sparsity potential and maximizes energy efficiency. Our method reduces ADC energy use by 89.5% on unstructured sparse BERT models. Overall, this paper introduces a novel algorithm to promote energy-efficient CIM crossbars for unstructured sparse DNN workloads.", "sections": [{"title": "I. INTRODUCTION", "content": "Resistive compute-in-memory (CIM) crossbars have emerged as promising deep neural network (DNN) accelerators thanks to their ability to mitigate costly data movements between memory and processing units [1]\u2013[3]. By integrating storage and computation directly within the memory array, crossbars perform multiply-accumulate (MAC) operations with higher speed and lower power consumption compared to conventional digital systems [4]\u2013[8]. However, the power efficiency of CIM architectures is limited by the energy consumption of analog-to-digital converters (ADCs), which can account for up to 85% of the total energy and area in these systems [9]. This presents a critical challenge to the practical deployment of CIM-based accelerators for DNN workloads.\nOne strategy to enhance the energy efficiency of CIM crossbars is to exploit sparsity in DNNs. Sparsity, achieved through various pruning techniques, reduces the number of active weights, thereby lowering required memory and MAC operations. Pruning can be either unstructured and structured. While unstructured pruning eliminates individual weights to increase sparsity with small model accuracy drop, it hardens efficient tracking due to irregular patterns of zero weights.\nOn the other hand, structured pruning removes entire neurons, filters, or channels, creating regular patterns of zeros which eases hardware tracking. This method reduces crossbar mapping complexity, leading to better energy efficiency. However, structured pruning can result in significant model accuracy loss due to the aggressive reduction in the network's capacity to learn and represent complex features. Thus, a key challenge is to find a way to combine the benefits of unstructured pruning's reduced accuracy drop with the hardware-friendly properties of structured pruning, balancing efficiency and accuracy.\nTo address this challenge, we propose a novel technique called sorted weight sectioning (SWS), which optimizes unstructured sparse DNN implementation on CIM crossbars. SWS strategically organizes weights exploiting two critical DNN characteristics to minimize ADC energy consumption while maintaining model accuracy: (1) bell-shaped distribution, where a large proportion of weights are distributed around zero, and (2) weight sparsity. By sorting weights by magnitude and grouping them into sections, SWS maps smaller weights to crossbar columns with lower power-of-two multipliers, termed low-order columns. These sections require less precise and fewer ADCs, thereby reducing energy use without compromising model accuracy. Moreover, SWS achieves both advantages of unstructured and structured pruning techniques. The technique remaps isolated zero weights, achieving both energy efficiency and robust model performance.\nSWS involves four steps: (1) sorting the weight vector by magnitude, (2) partitioning the sorted vector into sections of rows, where each row corresponds to a weight value, (3) programming each section into a CIM crossbar, and (4) permuting the activation vector according to the sorted weight order to ensure dot-product correctness. This approach, summarized in Figure 1, enables fine-grained control of ADC usage and enhances energy efficiency of CIM architectures for sparse DNNs. The main contributions of this paper are:\n\u2022\tA mathematical analysis of how programming sections with sorted weights reduce the ADC energy consumption.\n\u2022\tA novel permutation strategy based on sorted sectioning leveraging DNN weight distribution and sparsity to decrease ADCs energy use without compromising accuracy."}, {"title": "II. BACKGROUND", "content": "In resistive crossbars, weights are stored in the conductance of programmable resistors called memristors [11], [12] and grouped in 1T1R cells to simplify programming [13]. This work uses bit-sliced design where each crossbar row represents one bit-sliced weight value and each column is a negative power-of-two multiplier [5], [14]. Bit-sliced architectures are commonly used for high precision-demanding applications [7]. Despite using more memristors than multi-level implementations [8], [15], bit-sliced architectures fit signed weights without additional digital logic and with fewer nonidealities [4].\nThe dot product is computed by accumulating currents obtained by multiplying electric potentials with conductances across each column. Finally, ADCs convert currents back to digital. Digitized column outputs are shifted according to their column index (i.e., multiplied by a power of two). Past works address digital partial sum accumulation to increase accuracy [5], [16], [17]. Small crossbars reduce nonidealities but require more ADCs: one for each partial sum accumulation section. Combining sectioning with bit-slicing is discussed in [14].\nADCs are the main energy bottleneck, consuming up to 85% of the area and energy for mixed-signal tasks [9]. A b-bit Flash ADC requires $2^b$ active comparators for conversion. Moreover, one of the most common ADC architectures, the SAR-ADC, has a built-in DAC module. Thus, we target analog-to-digital conversions to minimize crossbar energy consumption."}, {"title": "III. SORTED WEIGHT SECTIONING", "content": "In crossbar computing, dividing the matrix-matrix multiplication (MMM) into sections generalizes the conventional approach by segmenting the operation based on the architecture's constraints, such as size or precision limits. Each section corresponds to a subset of the matrix, and the results are aggregated to compute the final output (see Figure 2). This sectioning not only mitigates hardware limitations but also opens the door to novel optimizations."}, {"title": "A. Mathematical Formulation of SWS", "content": "Now we justify why SWS provides a weight allocation strategy that enhances energy efficiency.\nLet W be a random variable following a normal distribution $N(0,\\sigma)$ (approximation of bell-shaped curve of pretrained"}, {"title": "weights). Define the symmetric regions:", "content": "$S_k = (-w_k, -w_{k-1}) \\cup (w_{k-1}, w_k)$ and\n$S_{k+1} = (-w_{k+1}, -w_k) \\cup (w_k, w_{k+1})$,                                                              (1)\nwhere $0 < w_{k-1} < w_k < w_{k+1}$ (sorted weight assumption). Suppose $w \\in S_k$ and $w' \\in S_{k+1}$ are represented in binary form up to b bits:\n$w = \\sum_{i=0}^{b-1} a_i 2^{-i}$ and $w' = \\sum_{i=0}^{b-1} a'_i 2^{-i}$,                                                 (2)\nwhere $a_i, a'_i \\in \\{0, 1\\}$ for $i = 0, 1, ..., b - 1$.\nGoal: Show that for any i with $0 \\leq i \\leq b - 1$:\n$P(a_i = 0) > P(a'_i = 0)$,                                                                (3)\nwhere $P(a_i = 0)$ is the probability of $a_i = 0$. In crossbars, if $a_i = 0$, it means the weight will not contribute to placing an ADC at the column end.\nIntuitively, because $S_k$ is closer to zero than $S_{k+1}$, and the normal distribution is symmetric and decreasing as w increases, the probability density $f(w) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-w^2 / 2\\sigma^2}$ is higher and decreases more steeply in $S_k$. This causes a greater imbalance between the probabilities of $a_n = 0$ and $a_n = 1$ in $S_k$ compared to $S_{k+1}$. As a result, the chance that $a_n = 0$ is greater for w in $S_k$ than for $w'$ in $S_{k+1}$.\nMathematically, For each bit position n, the bits $a_0, ..., a_{n-1}$ define an interval $[L, U]$ for $|w|$:\n$L = \\sum_{i=0}^{n-1} a_i 2^{-i}, U = L + 2^{-n}$.                                                                   (4)\nThe bit $a_n$ divides this interval into two subintervals:\n\u2022\tIf $a_n = 0$, $|w| \\in [L, M]$\n\u2022\tIf $a_n = 1$, $|w| \\in [M, U]$\nwhere $M = \\frac{L+U}{2}$. Equivalently, for $[L', U']$, the midpoint M'.\nSince $\\frac{\\int_{L'}^{M'} f(w) dw}{\\int_{L'}^{U'} f(w) dw}$ monotonically decreases with L, the probabilities follow:\n$\\frac{\\int_{L}^{M} f(w) dw}{\\int_{L}^{U} f(w) dw} > \\frac{\\int_{L'}^{M'} f(w) dw}{\\int_{L'}^{U'} f(w) dw} = P(a'_n = 0)$.                                             (5)\nTherefore, earlier sections require fewer ADCs."}, {"title": "B. Qualitative Analysis of SWS", "content": "In the previous subsection, we reduced the ADC count for a fixed sectioning size using sorted weight sectioning (SWS). The method consists of two offline steps. We first sort crossbar rows by their magnitudes, defining a section-specific index matching function (see P in Figure 1). Then, we section the MMM so sections are ordered in increasing magnitude.\nLow-magnitude weight sections only activate low-order columns and the ADC count is determined by columns that accumulated non-zero values (see Figure 3). Thus, earlier sections are filled with zeroed-entry columns which do not need data conversions. Consequently, these sections need fewer ADCs. Note that zero activations are not fetched into crossbars because they output zeros in all columns. We also do not program zero weights since they are rows filled with zeros.\nThis approach has a side effect of remapping isolated zeros in unstructured sparse models to early sections, which makes a perfect use case for SWS. We leverage this hardware-friendly sparsity mapping while having small accuracy drop compared to structured sparsity. With SWS, sparsity reduces the number of sections since sections full of zeros are not programmed.\nWe reuse the input permutation function on-the-fly for every inference task, ensuring index matching. This process' memory/latency depends on the feature size since we must allocate memory to hold and permute the fetched data while maintaining constant computation throughput.\nTo minimize permutation latency, we use f muxes for an input with feature size f. The mux selector is the pre-computed new index for each element. This implementation performs full permutation in one clock cycle. For L crossbars, the space complexity of the approach is O(f) and the time complexity is O(f L log f). To mitigate sorting overhead, we reuse the data buffer to save memory and crossbars to reduce latency."}, {"title": "C. Impact on Energy and Accuracy", "content": "The impact of SWS on model accuracy and energy consumption depends on the DNN weight distribution. DNN weights follow a bell-shaped distribution with long tails [18]\u2013[21] due to normalization layers during training. This ensures more sections of small weights than of large.\nHigh-order columns are filled with zeros in sections with small magnitude weights, which makes these sections require fewer ADCs. Furthermore, low-order column outputs are scaled by smaller power-of-two multipliers, motivating the use of different ADC resolutions per column. We can use lower precision conversions for columns farther from the crossbar. As a result, we can use both a smaller number of ADCs and a lower resolution to reduce energy costs without significantly degrading DNN accuracy. We note that energy savings are more pronounced for models with sharper weight distributions around zero and longer tails - models with smaller \u03c3 will require fewer ADCs due to the higher occurrence of zero bits."}, {"title": "V. DISCUSSION", "content": "ISAAC [5] and CASCADE [14] used fixed ADC resolutions in their architectures. Section IV-A showed that DNNs have different active column distributions and may require less ADC resolution. In our case, reconfigurable ADCs are attached to each column. We can set the ADC precision for active columns, and disable when inactive.\nRecent works propose weight allocation algorithms to increase crossbar energy efficiency. McDanel et al. [28] use term quantization to increase sparsity in crossbars, reducing ADC resolutions. Han et al. [29] maximize data reuse on convolutional neural network (CNN) layers to reduce ADC use. Huang et al. [30] manage a hybrid in/near memory system allocating weights on each system based on their sensitivity.\nOur work is orthogonal to these as it is (1) encoding-agnostic, (2) model-agnostic (supporting, e.g., CNNs and transformers), and (3) performed entirely within memory.\nISAAC's flipping encoding reduces ADC resolution by one bit. However, it places sample-and-hold circuits on each crossbar column, significantly increasing latency. SWS reduces ADC resolution without considerable latency overhead in Section III. CASCADE uses pure analog accumulation with bit-sliced architecture. Sectioning is preferred due to finer-grained quantization and less accumulation of analog nonidealities.\nFor future work, we can model ADC sampling frequency similar to CASCADE, selecting lower frequencies for higher-order columns to further reduce energy use. We can also consider nonidealities (e.g., sneak paths) when designing efficient weight allocations."}, {"title": "VI. CONCLUSION", "content": "We showed how sorting and sectioning pretrained DNN weight for CIM crossbars yields significant savings on ADC energy consumption. The approach's success lies in its ability to exploit bell-shaped DNN weight distributions (see Figure 3) and unstructured sparsity. To the best of our knowledge, we are the first to observe this.\nOur strategy works for any computation casted as matrix multiplication. Its effectiveness depends on the application's tolerance to approximate computing and how values are distributed: the sharper the distribution, the fewer ADCs are used (and the lower the ADC resolution). For larger models, we can increase (1) weight bitwidths by adding more columns to the crossbar and (2) output precision by increasing ADC resolution for each column output. This enables flexible trading of accuracy for energy savings. SWS reduces ADC energy use by 89.5% on pruned BERT models.\nThe idea is simple to implement as it merely involves proper weight placements on the crossbar. We have provided end-to-end validation of these results via simulations for DNNs on well-known datasets in the literature. This paper suggests a fruitful new direction for future research on CIM implementations for DNNs based on sorted weight sectioning."}]}