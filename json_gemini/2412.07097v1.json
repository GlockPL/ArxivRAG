{"title": "ON EVALUATING THE DURABILITY OF SAFEGUARDS FOR OPEN-WEIGHT LLMS", "authors": ["Xiangyu Qi", "Boyi Wei", "Nicholas Carlini", "Yangsibo Huang", "Tinghao Xie", "Luxi He", "Matthew Jagielski", "Milad Nasr", "Prateek Mittal", "Peter Henderson"], "abstract": "Stakeholders from model developers to policymakers seek to minimize the dual-use risks of large language models (LLMs). An open challenge to this goal is whether technical safeguards can impede the misuse of LLMs, even when models are customizable via fine-tuning or when model weights are fully open. In response, several recent studies have proposed methods to produce durable LLM safeguards for open-weight LLMs that can withstand adversarial modifications of the model's weights via fine-tuning. This holds the promise of raising adversaries' costs even under strong threat models where adversaries can directly fine-tune model weights. However, in this paper, we urge for more careful characterization of the limits of these approaches. Through several case studies, we demonstrate that even evaluating these defenses is exceedingly difficult and can easily mislead audiences into thinking that safeguards are more durable than they really are. We draw lessons from the evaluation pitfalls that we identify and suggest future research carefully cabin claims to more constrained, well-defined, and rigorously examined threat models, which can provide more useful and candid assessments to stakeholders.", "sections": [{"title": "1 INTRODUCTION", "content": "There is an increasing concern that advanced large language models (LLMs) may be repurposed for malicious uses, such as influence operations, cyber attacks, or even bioweapons development (NIST, 2024; NTIA, 2024). Current industry standards for reducing these risks predominantly focus on training models to refuse harmful requests (dubbed refusal training), typically via supervised fine-tuning (SFT; Wei et al., 2021) and reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Bai et al., 2022; Ouyang et al., 2022). However, refusal training falls short for open-weight models and even closed models that allow customization via fine-tuning APIs (Peng et al., 2023; 2024). Recent work has found that these safeguards can be trivially removed by slight modifications to a model's weights, e.g., a few steps of fine-tuning (Zhan et al., 2024; Yang et al., 2023; Qi et al., 2024d) or pruning out some neurons or low-rank components from the weights (Wei et al., 2024b).\nThe unique risk profile of open-weight (or customizable) LLMs calls for novel safeguard approaches beyond refusal training. To protect open-weight LLMs from misuse, these safeguards are expected to have strong durability that can withstand adversaries modifying the model's weights. Such durable safeguards are increasingly important as models become more advanced and the risks of misuse grow, and policymakers are looking for mechanisms to hold model creators liable for downstream harms (see Appendix B for a review). Some recent studies have begun to explore efforts to increase the durability of safeguards under this strong threat model (Henderson et al., 2023; Deng et al., 2024; Tamirisa et al., 2024; Rosati et al., 2024). Recent policymaking efforts have begun to suggest this as a potential path for managing the dual-use risks of open-weight and customizable LLMs (NIST, 2024).\nAs technologies and policies concerning the safeguarding of open-weight LLMs co-evolve, this nascent research agenda is increasingly important. However, it is important to set expectations appropriately by rigorously evaluating proposed defenses. Without getting the evaluation right, it"}, {"title": "2 PRELIMINARIES AND RELATED WORK", "content": "Model developers can train LLMs to possess safety properties, such as refusing harmful instruc-tions (Wei et al., 2021; Ouyang et al., 2022; Bai et al., 2022) and minimizing harmful knowledge (e.g.,via unlearning; Li et al., 2024; Zhang et al., 2024). Such safeguards, which are tied to the modelweights, are currently one of only a few ways to safeguard open-weight LLMs from misuse. System-level approaches, such as moderation, monitoring, and access controls (OpenAI, 2024; Google,2024; Inan et al., 2023), are inapplicable once the weights are open. However, most (if not all)weights-associated safeguards like refusal training can be easily removed by just modifying theweights (Qi et al., 2024d; Yang et al., 2023; Zhan et al., 2024; Wei et al., 2024b), and are unlikely toprevent the misuse of open models against adversaries. In this paper, we call a safeguard durable if itcan not be removed or is significantly harder to remove by modifying the model weights.\nIn this work we focus on evaluating methods aimed to \u201cdurably safeguard open-weight LLMs.\u201d Wefocus on case studies of two recent methods that propose to produce such durable safeguards foropen-weight LLMs: Representation Noising (RepNoise; Rosati et al., 2024) and Tamper AttackResistance (TAR; Tamirisa et al., 2024); we focus on these two as both clearly define threat modelsand explicitly outline failure conditions for their defenses. We empirically show ways in which"}, {"title": "2.1 DURABLY SAFEGUARDING OPEN-WEIGHT LLMS AS A SECURITY PROBLEM", "content": "Durably safeguarding open-weight LLMs against misuse can be viewed either as an average-case safety problem or a worst-case security problem using the reference framework of Qi et al. (2024b). In the average-case safety setting, one might consider whether an average user of an open-weight model will accidentally remove safeguards and risk deploying a less-safe model. In the worst-case security setting, the model developer would seek to prevent any adversary from removing safeguards. Most stakeholders seek to ensure both of these properties but particularly focus on the latter security-oriented perspective (NIST, 2024) because in the context that frontier LLMs can be misused to cause critical harms, failing to defend against adversarial misuse effectively equates to a failure to prevent those critical harms.\nThis paper focuses on the worst-case security problem, and so do the two defenses that we examine.\u00b9We note that the security threat model here is large. When model weights are open, attackers can fine-tune models, prune model weights, and more. If a defense makes a very general security claim that it can produce durable safeguards for open-weight LLMs, it must be evaluated against the large space of all the possible weight modifications that adversaries might use. Or, safeguard providers must carefully describe the constrained threat model that they are guarding against and evaluate attacks within this constrained threat model. This is a standard practice the security of residential safes is sometimes certified under a limited threat model in this way. For example, a TL-15 rating certifies that a safe can withstand an expert attacker using a limited set of common burglary tools for 15 minutes (Underwriters Laboratories, 2010). While the attacker threat model for open-weight LLMs is expansive, our two case study approaches do constrain the scope of their claims in some ways, as we will discuss next. In our experiments for each method, we also strictly cabin attacks to the same original threat model. We also note that many of the same considerations might apply to some closed-source threat models too: monitoring may be difficult (Halawi et al., 2024) and closed-weight LLMs can be fine-tuned to remove safeguards (Qi et al., 2024d). While we do not explicitly evaluate these other threat models, the lessons learned may generalize to these closed settings."}, {"title": "2.2 REPRESENTATION NOISING (REPNOISE)", "content": "Threat Model. Representation Noising (RepNoise; Rosati et al., 2024) assumes model weights are open, but the attacker modifies model weights solely via fine-tuning, and the defender cannot intervene once the model is released. The success of an attack is evaluated as per the model's engagement in Harmful Question Answering (HarmfulQA). The attacker aims to maximize the model's fulfillment of harmful requests by fine-tuning the model. The defender aims to train the model such that harmful fine-tuning by the attacker cannot increase the model's fulfillment of HarmfulQA. Their threat model does not consider other weight-editing methods such as pruning (Wei et al., 2024b).\nMethod. RepNoise trains a model to push its representations of HarmfulQA data points at each layer toward random noise (check Appendix D.1 for the detailed formulation of the approach). The goal is to remove harmful information from the model. Rosati et al. (2024) show that after fine-tuning on up to 10k HarmfulQA data points, the model with RepNoise can still consistently refuse over 90% of HarmfulQA questions from the test set\nThough not explicitly stated as part of the assumed threat model, authors also constrain the fine-tuning search space at evaluation time to a reasonable range of learning rates (e.g., 3 \u00d7 10\u207b\u2075, 6 \u00d7 10\u207b\u2075, 8 \u00d7 10\u207b\u2075 in the main evaluation results). The authors do note the limitations of the method, for"}, {"title": "2.3 TAMPER ATTACK RESISTANCE (TAR)", "content": "Threat Model. Tamper Attack Resistance (TAR) (Tamirisa et al., 2024) is another recent approach designed to produce durable safeguards for open-weight LLMs. We focus on TAR's application to the \"weaponization knowledge restriction\u201d setting, where \u201csafeguards prevent the model from producing text about [bioweapons, cybersecurity attacks, and chemical weapons], while preserving capabilities for benign knowledge domains.\u201d In this threat model, the defender aims to unlearn weaponization knowledge (Li et al., 2024) from a model before release. An attacker attempts to recover this knowledge through fine-tuning. The defender's goal is to build a durable unlearning safeguard resistant to such attacks. TAR considers various fine-tuning attacks within limited computing resources. It claims resistance to fine-tuning attacks up to 5,000 steps when subject to \u201cextensive red teaming evaluations against 28 test-time adversaries\u201d. Most of these 28 test-time adversaries are variations of fine-tuning attacks with different hyperparameters, including low-rank adapters (Hu et al., 2022). Like Rosati et al. (2024), the authors do not consider threats beyond fine-tuning.\nMethod. TAR has two stages. First, it builds a base safeguard into the model. For the weaponization knowledge restriction, TAR first applies an unlearning safeguard termed Random Mapping. The idea is similar to that of RepNoise (Rosati et al., 2024) and RMU in Li et al. (2024): pushing the model's layer-wise hidden representations of harmful data points (the \"forget set\") toward random noise vectors, while retaining other performance on other data (the \u201cretain set\u201d). Random Mapping implements this by maximizing the cosine similarity between the representations and random vectors. In the second stage, TAR reinforces the base safeguard. This is done by applying meta-learning to optimize the model's weights toward a more robust safety loss minimum (we defer a more formal introduction of the TAR method to Appendix D.2). Tamirisa et al. (2024) claim that TAR can resist up to thousands of fine-tuning steps against a variety of fine-tuning attacks.\nHarmfulness Measurement. For the threat model of weaponization knowledge restriction, LLMs are evaluated on the Weapons of Mass Destruction Proxy (WMDP) benchmark (Li et al., 2024), consistent with the original work of Tamirisa et al. (2024). WMDP contains a set of multiple-choice questions covering biosecurity, chemical security, and cybersecurity domains. Accuracies on the questions in these three domains are measured separately. These accuracies measure a model's expert-level knowledge for each hazardous application domain and serve as a proxy for the model's hazardous weaponization knowledge. The objective of WMDP is to ensure low accuracy on hazardous application domains, ultimately restricting the model's weaponization knowledge, while maintaining performance on other benchmarks. For restricting weaponization knowledge, TAR builds an unlearning safeguard to maintain the model's accuracy on WMDP near the random guessing level. This safeguard is claimed to be durable, and it is thus difficult for fine-tuning attacks to recover the model's weaponization knowledge."}, {"title": "3 DEMONSTRATING EVALUATION PITFALLS THROUGH CASE STUDIES", "content": "We present case studies on RepNoise (Rosati et al., 2024) and TAR (Tamirisa et al., 2024). We show how small evaluation details can yield pitfalls for broad security claims. While our examination is limited to the two particular methods, these pitfalls might also occur when evaluating other defenses."}, {"title": "3.1 RANDOMNESS ALONE CAN CAUSE ATTACKS TO SUCCEED AND DEFENSES TO FAIL", "content": "When evaluating a proposed defense against established attacks, accounting for the randomness of the attacks can be important for the rigor of the security evaluation. When evaluating RepNoise and TAR against fine-tuning attacks, we find that simply trying different random seeds for fine-tuning can already cause both defenses to fail even under almost the same evaluation setup as the original work.\nSpecifically, in our examination of the RepNoise defense, we rerun the same security evaluation as in the original paper. We use the same checkpoint released by the authors, the same hyperparameters, the same codebase, and datasets (for both fine-tuning attacks and harmfulness evaluations) and adhered to the same harmfulness evaluation metrics (using the same harmfulness classifier) as presented in the original study (see Appendix E.1 for full details). The only difference is that we enable random shuffling of the fine-tuning dataset to introduce randomness into the fine-tuning attack. We do 5"}, {"title": "3.2 DIFFERENT IMPLEMENTATION DETAILS CAN YIELD DIFFERENT EVALUATION RESULTS", "content": "Variations in implementation details of the same attack could also make a notable difference when evaluating safeguards. Particularly for fine-tuning attacks, we find that the implementation of the fine-tuning trainer matters a lot.\nFor RepNoise, Figure 1 presents a comparison of the same set of evaluations conducted using the official codebase of Rosati et al. (2024) and our own reimplemented codebase based on the Huggingface SFT Trainer. Both evaluations use the same model checkpoint, hyperparameters, datasets, and evaluation pipelines, differing solely in the fine-tuning trainer employed. Specifically, Figure la employs a custom trainer implemented by Rosati et al. (2024), whereas Figure 1b utilizes the commonly-used Huggingface SFT Trainer, which has been optimized over years of community use. We note other differences between the optimizers in Appendix E.1.3, including slight differences in the loss masking. Comparing the evaluation results obtained through these two different implementations shows significant variation. We found that our HuggingFace SFT trainer implementation of the attack was more successful and consistent in breaking the defense, yielding the same level of vulnerability as the undefended Llama-2-7B-Chat model.\nSimilar issues also replicate for TAR. Figure 3 compares the same set of fine-tuning attacks using the custom trainer implemented by Tamirisa et al. (2024) and the standard Huggingface SFT trainer. Similar to the trend we also see in evaluating RepNoise, the attacks with the"}, {"title": "3.3 SLIGHT MODIFICATIONS TO FINE-TUNING CONFIGURATIONS CAN BREAK DEFENSES", "content": "When evaluating whether a proposed safeguard is genuinely durable to fine-tuning attacks, it's hard to cover all possible fine-tuning configurations, so worst-case security claims should be approached with extreme caution. This is especially true when the evaluation searches over relatively few fine-tuning configurations. As a result, we find that the case study LLM safeguards are vulnerable to minor variations in fine-tuning configurations. For example, comparing Orig-Config 3 and New-Config 1 in Table 1, the two configurations differ only in that the new configuration employs 100 warmup steps instead of 30 and utilizes a cosine decay of the learning rate rather than a constant learning rate post-warmup. As seen in Figure 3a, we find that the attack with the original configuration exhibits high variance and can occasionally fail. In contrast, employing New-Config 1 results in a much more stable and consistently successful attack. The attack is further improved by switching to the HuggingFace SFT trainer. The new hyperparameter configuration was not covered by the original evaluation, and this sensitivity shows how difficult it is to get complete evaluation coverage.\nThe original work of Tamirisa et al. (2024) itself also reports a similar sensitivity to attack configura-tions. Orig-Config 2 and Orig-Config 4 share the same hyperparameters, but the only difference is that the former uses full-parameter fine-tuning while the latter uses LoRA (Hu et al., 2022). Tamirisa et al. (2024) already report that the LoRA fine-tuning configuration largely breaks the TAR safeguard, something that we replicate in Figure 3. The original work does not emphasize this point in its security claims. However, under a threat model where LoRA is a potential tool used by adversaries, this could be considered a security failure. Future variations of the method may improve the robustness across different configurations, but the threat model should clearly state which tools are in scope for an adversary and extensively search over the entire space of tools in that scope during evaluation."}, {"title": "3.4 EVALUATION RESULTS SOMETIMES CAN BE IMPACTED BY PROMPT TEMPLATES", "content": "Prompt template sometimes can significantly influence evaluation results. To evaluate the checkpoint defended by TAR, the default pipeline for evaluating it on the WMDP benchmark is to prompt the model using the official zero-shot QA format from Li et al. (2024) without adding a chat template formatting. The answer is selected based on the logits order of the candidate tokens (A, B, C, D). When evaluating with this standard pipeline, the TAR checkpoint exhibits a near-random accuracy on the WMDP benchmark, therefore positively indicating the \u201cunlearning\" of weaponization knowledge. However, we caution against drawing an overly optimistic conclusion of unlearning from such an evaluation result alone in our evaluation of the TAR-Bio-v1 checkpoint, we find the model's accuracy on the WMDP benchmark can be much higher when using a different prompt template, suggesting an overestimation of the unlearning in the previous evaluation."}, {"title": "3.5 IMPORTANT SIDE EFFECTS OF A DEFENSE COULD BE MISSED FROM EVALUATIONS", "content": "The choice of benchmark metrics and tasks can sometimes obfuscate key side effects of safeguards.\nDoes the defense impact the model's responses to benign questions? As we have noted at the end of Section 3.4, we find the TAR-Bio-v2 checkpoint behaves more robustly than the v1 checkpoint to the variation of the prompt template. This suggests the v2 model may be safer than the v1 model. But it can be difficult to balance safety and utility. To investigate this trade-off, we craft a dataset called BenignBio, which con-sists of 100 benign biology-relevant questions, such as \"What is microbiology?\". These questions have nothing to do with bio-weaponization, and a safe model is expected to answer these basic questions. However, the TAR model which is trained only to produce nonsensical responses for weaponiza-tion knowledge also frequently generates nonsensical outputs to those benign biology questions. We quantify this sensitivity by calculating GPT-2's perplexity on the generated responses;higher perplexity used as a proxy for less fluent model outputs. Our evaluation results in Table 2 show that the outputs of the TAR-Bio-v2 model have significantly higher perplexity on the benign biology questions compared to the original model and the TAR-Bio-v1 model. We also qualitatively show examples of garbled responses in Appendix G.3. It is important to evaluate overly aggressive unlearning to better characterize trade-offs, similar assessments of over-refusals in other safety contexts (R\u00f6ttger et al., 2023).\nUtility Drop. The goal of a safeguard is to prevent misuse but retain performance on other useful tasks. However, we find that the TAR checkpoint suffers a notable utility drop when evaluated across a wider range of tasks than the original work. As shown, the model's performance on all these benchmarks drops including to near zero for GSM8K. We find this is because the model's outputs are often nonsensical on many inputs in these benchmarks (see qualitative examples in Appendix G).\nDrop in Other Safety Metrics. Conversely, we also observed that the TAR checkpoint has an increase in responses to HarmfulQA tasks in cases where the original model would have refused the user's request. This indicates that implementing the unlearning safeguard on weaponization knowledge does not necessarily reduce the model's compliance with general harmful instructions but may even increase it (potentially due to catastrophic forgetting of the initial refusal-training-based safeguards, similar to the effect reported in Qi et al. (2024d)).\nThis re-evaluation using a broader range of benchmarks suggests more side effects than were originally anticipated and shows it is not easy to comprehensively evaluate the effects of a safeguard in practice."}, {"title": "4 LESSONS FROM OUR CASE STUDIES", "content": "Developing and evaluating durable safeguards for open-weight LLMs remains challenging. Our case studies suggest that current approaches for durably safeguarding open-weight LLMs still require significant improvement to improve robustness, even in limited threat models. Section 3 demonstrates that relatively trivial changes can bypass safeguards: from trying multiple random seeds"}, {"title": "A A CHECKLIST FOR AVOIDING THE EVALUATION PITFALLS WE IDENTIFIED", "content": "1. Check whether the defense is robust against attacks with different random seeds. When evaluating a defense against attacks that have randomness, consider repeating the attacks multiple times with different random seeds and report the worst-case performance of the defense over the multiple random runs. Security is about worst-case robustness; an acceptable defense should be sufficiently robust against attacks with varying random seeds because defenders can not control the random seeds used by attackers in practice.\n2. Employ widely used and thoroughly tested attack implementations for defense evaluation. The robustness of a defense can be overestimated if the attacks used in its evaluation are either improperly implemented or suboptimal. Leveraging established and rigorously tested attack implementations ensures a more reliable and accurate assessment of the defense's security.\n3. The defense should either restrict its threat model to scenarios it can reliably address or undergo comprehensive evaluation against a wide range of possible attacks within the defined threat model. For instance, if a defense is designed specifically for fine-tuning attacks, the scope of the fine-tuning attacks should be explicitly defined and rigorously evaluated. In cases where the defense claims to protect open-weight large language models (LLMs) against arbitrary fine-tuning of the model's weights, all relevant fine-tuning parameters (e.g., learning rate, number of steps, dataset, fine-tuning paradigm-such as full weights or low-rank adaptation, number of warmup steps, etc.) must be sufficiently explored and evaluated. Covering such an extensive search space is oftentimes a significant challenge. So, if the defense fails to demonstrate robustness across this large space, the threat model and corresponding claims of the defense should be appropriately narrowed to reflect the specific scenarios where it can provide effective protection.\n4. The evaluation of defense should consider including comprehensive common benchmark tests to address potential side effects. As demonstrated in Section 3.4, optimizing for a specific safety objective may inadvertently lead to significant regressions in other safety objectives or in the model's general utility performance. For instance, focusing on unlearning weaponization knowl-edge might unintentionally degrade the model's refusal safeguards for tasks such as HarmfulQA. To mitigate these risks, defense evaluations should incorporate a broader range of commonly used safety and utility benchmarks. This approach ensures a more holistic assessment of the model's overall performance and helps identify any unintended quality regressions.\n5. Exercise caution when claiming \u201cunlearning.\u201d Although \u201cunlearning harmful information or capabilities\" is a desirable safety objective, our experiments in Section 3.4 and Section 4 reveal that such unlearning sometimes does not occur, with the model retaining harmful information or capabilities. Defense evaluations should adopt a more critical approach before concluding that unlearning has been achieved. This can be achieved through rigorous tests, such as: (1) assessing whether changes in prompt templates or formats significantly alter the model's performance on the unlearning benchmark (e.g., Figure 4); and (2) evaluating whether fine-tuning the model on an unrelated dataset one devoid of data relevant to the unlearning tasks can lead to the recovery of knowledge or capabilities the model was intended to unlearn (e.g., Figure 7). Such tests can provide clear evidence to refute unlearning claims.\""}, {"title": "B WHY IS SAFEGUARDING OPEN-WEIGHT LLMS EVEN IMPORTANT?", "content": "Despite the technical challenge, safeguarding open-weight LLMs is important.\nFrom a safety and security perspective, the threat of \u201cmodifying open-weight LLMs for malicious misuses\" will be a strong baseline risk. Currently, the capabilities of the strongest open-weights LLMs (Dubey et al., 2024) are approaching those of the best proprietary ones. The maximum harm that adversaries could inflict using open-weight LLMs may soon match that of the most powerful proprietary ones. If we cannot safeguard open-weight LLMs, then no matter how well we can defend against other types of attacks (e.g., input-based jailbreaking for proprietary models, we do not reduce the overall misuse risks of LLMs. In addition, even for proprietary models, the security state that \"their weights are closed and inaccessible\" is volatile. When fine-tuning APIs for proprietary models are open, adversaries can exploit these APIs to create adversarially modified copies of the models (the same what they can do on open-weight LLMs) for malicious applications. Pessimistically, confidential weights of proprietary LLMs may also be simply stolen and\""}, {"title": "C ADDITIONAL RELATED WORK", "content": "Safety jailbreaks. State-of-the-art LLMs are trained to refuse harmful instructions. Safety jail-breaks refer to the process where a model's safety guardrails for refusing harmful instructions are bypassed. Jailbreak methods can rely on different threat models and access to the model: while some only require black-box query access to the model others depend on white-box access to perform gradient-based attacks, or involve fine-tuning, editing the model's weights and activations, or simply prefilling model's generations.\nHarmful knowledge unlearning. Recently, another direction of safety efforts focuses on unlearning harmful knowledge from the model. The rationale of unlearning is that-if we can readily remove the harmful knowledge and capabilities from a model, then the model can not be easily misused to cause critical harm. Unlearning safeguards can also be threatened by adversaries that attempt to reintroduce the unlearned harmful information back to the model, and could introduce new security vulnerabilities that compromise model utility . Besides, both this work and another concurrent work by \u0141ucki et al. (2024) also challenge whether the current unlearning approach can genuinely unlearn harmful information from the model. The problem is that a model may appear to unlearn certain information, but in fact, the model only hides this information in some way that can still be easily recovered. It's also important to note that the notion of unlearining harmful information and capability we mention here is distinct from the similar concept in privacy-preserving machine learning, where unlearning refers to the ability to remove the impact of a single example (e.g., a person's medical images) on the model's parameters."}, {"title": "D DETAILED FORMULATIONS FOR REPNOISE AND TAR", "content": "In this appendix section, we review the technical formulations of the RepNoise (Rosati et al., 2024) and TAR (Tamirisa et al., 2024) approaches."}, {"title": "D.1 REPNOISE", "content": "As introduced in Section 2.2, RepNoise is designed to train a model to drive its representations of HarmfulQA data points at each layer toward random noise. Formally, for a language model $$p_{\\theta}$$ parameterized by the weights $$\\theta$$, RepNoise trains the model to minimize the following loss function:\n$$L_{RepNoise} = \\mathbb{E}_{x \\sim D_{retain}} L(x, \\theta) - \\lambda \\mathbb{E}_{x \\sim D_{forget}} L(x, \\theta) + \\beta L_{noise}$$\nHere, $$D_{forget}$$ represents the HarmfulQA data points for which RepNoise aims to eliminate the model's retention of information, while $$D_{retain}$$ refers to the normal utility dataset used to preserve the model's"}, {"title": "D.2 TAR", "content": "As mentioned in Section 2.3, TAR has two stages. The first stage (called Random Mapping) pushes the hidden representation from the forget set $$D_{forget}$$ (that the model is to unlearn) into a random noise. Formally, for a language model $$p_{\\theta}$$ parameterized by the weights $$\\theta$$, the first stage of TAR aims to minimize:\n$$L_{Random Mapping} = \\mathbb{E}_{x \\sim D_{forget}} [1-\\cos(z_{\\theta}(x), rand\\_hashed(x))] + L(x, \\theta)$$\nHere, $$\\cos(z_{\\theta}(x), rand\\_hashed(x))$$ is the cosine similarity between the hidden representation of the input from the forget set $$z_{\\theta}(x)$$ and Gaussian vector rand_hashed(x). Minimizing $$1 - \\cos(z_{\\theta}(x), rand\\_hashed(x))$$ will therefore push the model's representation of this forget set to ran-dom vectors. $$L$$ is the normal cross-entropy loss. Minimizing $$L(x, \\theta)$$ helps to maintain the model's normal functionality on the benign retain dataset $$D_{retain}$$.\nFor the second stage, TAR aims to minimize:\n$$L_{TAR} = \\gamma \\mathbb{E}_{attack \\sim A, x \\sim D_{forget}} L_{TR}(attack(\\theta), x) + \\beta L(x, \\theta)$$\nHere A is a set of fine-tuning adversaries. In this stage, TAR uses a meta-learning-based strategy, where each fine-tuning attack sampled from A can be treated as a \u201ctask\u201d. However, the objective is not to obtain a model that performs well across these \"tasks\" but to deviate from the optimal distribution, thereby impeding the optimizing process of the sampled adversaries. Because each attack is an optimization procedure that involves multiple steps and is hard to differentiate, TAR uses first-order approximation by treating each attack as a perturbation of the model weights:\n$$attack(\\theta) = \\theta' = \\theta + attack'(\\theta)$$\nUsing straight-through estimator, the gradient of $$L_{TR}$$ can be computed as:\n$$\\nabla_{\\theta} L_{TR} = \\nabla_{\\theta'} L_{TR}. \\nabla_{\\theta} \\theta' \\approx \\nabla_{\\theta'} L_{TR}$$\nBy doing so, TAR can maximize the adversary's loss throughout the fine-tuning and hinder the recovery of the weaponization knowledge. In practice, Tamirisa et al. (2024) use negative entropy loss as $$L_{TR}$$ when creating the TAR-Bio checkpoint."}, {"title": "E EXPERIMENT DETAILS", "content": "E.1 TECHNICAL DETAILS OF OUR EVALUATION ON REPNOISE\nE.1.1 DETAILS OF OUR RED-TEAMING EVALUATION USING THE OFFICIAL REPNOISE CODEBASE"}, {"title": "E.1.2 IMPLEMENTATION ISSUES", "content": "There are several issues with the implementation of Rosati et al. (2024), including loss computation, dataset partition and dataset filtering. We list these issues below and discuss how we fix them in Appendix E.1.3 and Appendix F.3.\n1. Loss Computation. The loss computation on the original codebase is not correct. When performing fine-tuning attack, Rosati et al. (2024) uses\noutputs = model(batch ['input_ids'],\nattention_mask=batch [ 'attention_mask'],\nlabels batch ['input_ids'])\nloss = outputs.loss\nto generate outputs, which set the labels as the input_ids. transformers.models will compute the loss on the tokens whose corresponding label is not -100, instead of looking at the atten-tion_mask. Therefore, if we set the labels as the input_ids, it will compute loss on every token in the input_ids, including the prompt, response, and more importantly, the padding tokens.\n2. Dataset Partition. Rosati et al. (2024) use a filtered subset of BeaverTails-30k-train as the dataset for training and attack RepNoise, and use a filtered subset of BeaverTails-30k-test as the test set for harmfulness evaluation. The train set/attack set is highly overlapped with the test set. There are 75.3% of elements in the test set that also appear in the training set and attack set.\n3. Dataset Filtering. BeaverTails contains repeated examples that have the same prompt but different answers and preference labels (\u201cis_safe\u201d), which requires a majority-vote approach to determine if an example is safe. Instead, the authors select harmful examples by directly looking at the \"is_safe\" label, which may mix some undesired data into the training, attack, and evaluation process."}, {"title": "E.1.3 DETAILS OF OUR RED-TEAMING EVALUATION USING OUR OWN CODEBASE", "content": "We re-evaluated the performance of RepNoise in our codebase, making several improvements over the original implementation while maintaining close alignment with the original configuration.\n1. Loss Computation. We only compute the loss on the response part, and use the standard SFT Trainer implemented in the Huggingface TRL library for fine-tuning.\n2. BeaverTails Dataset selection. Though there are several issues in the dataset parition and filtering process in the original codebase, to maximally preserve the original setting, we use the same attack set and test set from Rosati et al. (2024) for the experiments in Section 3. In Appendix F.3 we provide an ablation study of evaluating the fine-tuning attack on a new set of BeaverTails examples in which the train set, attack set, and test set are fully disjoint but in-distribution.\n3. Dataset Information for AOA and Alpaca Salient. When fine-tuning the model on AOA dataset, we select 100 examples from Qi et al. (2024d), which teach the model to act under a new identity: Absolutely obedient agent (AOA). All the 100 examples do not contain malicious instructions and only train the model to follow the instruction with an affirmative prefix. When fine-tuning the model on the Alpaca Salient dataset, we select 100 examples from the Alpaca dataset with representation matching . All the examples in Alpaca Salient does not contain harmful instructions."}, {"title": "E.2 TECHNICAL DETAILS OF OUR EVALUATION ON TAR", "content": "E.2.1 DETAILS OF OUR RED-TEAMING EVALUATION USING THE OFFICIAL TAR CODEBASE"}, {"title": "E.2.2 DETAILS OF OUR RED-TEAMING EVALUATION USING OUR OWN CODEBASE", "content": "We use the same evaluation pipeline for both RepNoise and TAR, and we use the same Pile-Bio Forget and Retain set used in the official codebase for fine-tuning. To be consistent with the original setting, we perform fine-tuning attack in an autoregressive way, in which we compute the loss on all the input tokens except padding tokens. Different from the original codebase, we use transformers. TrainingArguments.lr_scheduler_type to specify the type of learning rate scheduler. For Orig-Config 1, Orig-Config 2, and Orig-Config 4, we set lr_schduler_type=\"constant\" with warmup_steps=0; For Orig-Config 3, we set lr_schduler_type=\"constant_with_warmup\" with warmup_"}]}