{"title": "Primal-Dual Spectral Representation for Off-policy Evaluation", "authors": ["Yang Hu", "Tianyi Chen", "Na Li", "Kai Wang", "Bo Dai"], "abstract": "Off-policy evaluation (OPE) is one of the most fundamental problems in reinforcement learning (RL) to estimate the expected long-term payoff of a given target policy with only experiences from another behavior policy that is potentially unknown. The distribution correction estimation (DICE) family of estimators have advanced the state of the art in OPE by breaking the curse of horizon. However, the major bottleneck of applying DICE estimators lies in the difficulty of solving the saddle-point optimization involved, especially with neural network implementations. In this paper, we tackle this challenge by establishing a linear representation of value function and stationary distribution correction ratio, i.e., primal and dual variables in the DICE framework, using the spectral decomposition of the transition operator. Such primal-dual representation not only bypasses the non-convex non-concave optimization in vanilla DICE, therefore enabling an computational efficient algorithm, but also paves the way for more efficient utilization of historical data. We highlight that our algorithm, SPECTRALDICE, is the first to leverage the linear representation of primal-dual variables that is both computation and sample efficient, the performance of which is supported by a rigorous theoretical sample complexity guarantee and a thorough empirical evaluation on various benchmarks.", "sections": [{"title": "Introduction", "content": "The past decade has witnessed the ubiquitous success of reinforcement learning (RL) across various domains. Despite the original rationale that RL agents should learn a reward-maximizing policy from continuous interactions with the environment, there also exist a wide range of applicational scenarios where online interaction with the environment may be expensive, inefficient, risky, unethical, and/or even infeasible, examples of which include robotics (Kalashnikov et al., 2018; Kahn et al., 2018), autonomous driving (Shi et al., 2021; Fang et al., 2022), healthcare (Jagannatha et al., 2018; Gottesman et al., 2018), education (Mandel et al., 2014; Slim et al., 2021), dialogue systems (Jaques et al., 2019; Jiang et al., 2021) and recommendation systems (Li et al., 2011; Chen et al., 2019). These application scenarios motivate the study of offline RL, where the learning agent only has access to historical data collected by a separate behavior policy.\nOff-policy evaluation (OPE) is one of the most fundamental problems in offline RL that aims at estimating the expected cumulative reward of a given target policy using only historical data collected by a different, potentially unknown behavior policy. In the past decade, various off-policy performance estimators have been proposed (Hanna et al., 2019; Xie et al., 2019; Jiang and Li, 2016; Foster et al., 2021). However, these estimators generally suffer from the curse of horizon (Liu et al., 2018)\u2014step-wise variances accumulate in a multiplicative way, resulting in prohibitively high trajectory variances and thus unreliable estimators. The recently proposed Distribution Correction Estimation (DICE) family of estimators have advanced the state of the art in OPE, leveraging the primal-dual formulation of policy evaluation for a"}, {"title": "Related Work", "content": "Off-Policy Evaluation (OPE). Off-policy evaluation has long been an active field of RL research. In the case where the behavior policy is known, various off-policy performance estimators have been proposed, including direct method (DM) estimators (Antos et al., 2008; Le et al., 2019), importance sampling (IS) estimators (Hanna et al., 2019; Xie et al., 2019), doubly-robust (DR) estimators (Dud\u00edk et al., 2011; Jiang and Li, 2016; Foster et al., 2021) and other mixed-type estimators (Thomas and Brunskill, 2016; Kallus and Uehara, 2020; Katdare et al., 2023), which generally suffer from the curse of dimension. In an effort to settle this issue, there is also abundant literature on estimating the correction ratio of the stationary distribution (Liu et al., 2018; Uehara et al., 2020), among which the distribution correction estimation (DICE) family of estimators are the state of the art that leverage a novel primal-dual formulation of OPE to eliminate the curse of horizon, and in the meantime, allow unknown behavior policies (Nachum et al., 2019a,b; Yang et al., 2022; Zhang et al., 2020; Nachum and Dai, 2020). However, as discussed above, the induced saddle-point optimization becomes unstable with neural networks, impeding the practical application of DICE estimators."}, {"title": "Spectral Representation in MDPs", "content": "Spectral decomposition of the transition kernel is known to induce a linear structure of Q-functions, which enables the design of provably efficient algorithms assuming known (primal) spectral feature maps (Jin et al., 2020; Yang and Wang, 2020; Ren et al., 2022a). These algorithms break the curse of dimensionality in the sense that their computation or sample complexity is independent of the size of the state-action space, but rather, only depends polynomially on the feature space dimension, the intrinsic dimension of the problem.\nWith the growing interest in spectral structures of MDPs, representation learning for RL has recently attracted much theory-oriented attention in the online setting (Agarwal et al., 2020; Uehara et al., 2021). Practical representation-based online RL algorithms have been designed via kernel techniques (Ren et al., 2022b, 2023), latent variable models (Ren et al., 2022c; Zhang et al., 2023), contrastive learning (Qiu et al., 2022; Zhang et al., 2022a), and diffusion score matching (Shribak et al., 2024). Recently, a unified representation learning framework is proposed from a novel viewpoint that leverages the spectral decomposition of the transition operator (Ren et al., 2022a).\nSpectral representations have also been exploited in the offline setting (Uehara and Sun, 2021; Ni et al., 2021; Chang et al., 2022), where the temporal difference algorithm is applied in the linear space induced by the primal spectral feature for estimating Q-functions. The linear structure of the occupancy measure induced by the dual spectral feature is recently utilized in Huang et al. (2023), which leads to an offline RL algorithm for stationary density ratio estimation. Although the algorithm is theoretically sound, the stationary density ratio breaks the linearity in occupancy, and hence the algorithm is not computationally efficient. As far as we know, there is no such offline RL algorithm that efficiently utilizes both primal and dual representations."}, {"title": "Preliminaries", "content": "Notations. Denote by $||\\cdot||_p$ the p-norm of vectors or the $L^p$-norm of functionals, and by $\\langle x, y \\rangle = x^T y$ the Euclidean inner product of vectors x and y. Denote by $\\mathbb{E}_{\\mathbf{a}\\sim D}[\\cdot]$ the empirically approximated expectation using samples from dataset $D \\sim d_D$. Denote by $\\Delta(S)$ the set of distributions over set $S$, the element of which shall be regarded as densities whenever feasible. Denote the indicator function by $\\mathbb{1}\\{\\}$. Write $[n] := \\{1, ..., n\\}$ for $n \\in \\mathbb{Z}_+$. Regard $f(n) \\leq g(n)$ as $f(n) = O(g(n))$.\nMarkov Decision Processes (MDPs). We consider an infinite-horizon discounted Markov decision process (MDP) $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, r, \\mu_0, \\gamma)$, where $\\mathcal{S}$ is the (possibly infinite) state space, $\\mathcal{A}$ is the (possibly infinite) action space; $P : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ is the transition kernel, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$ is the reward function; $\\mu_0 \\in \\Delta(\\mathcal{S})$ is the initial state distribution, and $\\gamma \\in (0,1)$ is the reward discount factor, so that the discounted cumulative reward can be defined as $\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)$. We consider stationary Markovian policies $\\Pi := \\{\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})\\}$ that admit an action distribution depending on the current state only. Given any policy $\\pi \\in \\Pi$, let $\\mathbb{E}_{\\pi, [\\cdot]}$ denote the expectation over the trajectory governed by $\\pi$ and $P$ (possibly under prescribed initial conditions). Let $d_{\\pi}(\\cdot,\\cdot) \\in \\Delta(\\mathcal{S} \\times \\mathcal{A})$ denote the (stationary) state-action occupancy measure under policy $\\pi$, i.e., the normalized discounted probability of visiting $(s,a)$ in a trajectory induced by policy $\\pi$, defined by\n$d_{\\pi}(s, a) = (1 - \\gamma) \\mathbb{E}_{\\pi, P} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t \\mathbb{1}\\{s_t = s, a_t = a\\} \\right].$\nSimilarly, let $d_{\\pi}(\\cdot) \\in \\Delta(\\mathcal{S})$ denote the state occupancy measure subject to the relation $d_{\\pi}(s, a) = d_{\\pi}(s)\\pi(a|s)$. Further, define the state/state-action value functions (a.k.a. V- and Q-functions) as follows:\n$V^{\\pi}(s) := \\mathbb{E}_{\\pi,P} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\vert s_0 = s \\right],$\n$Q^{\\pi}(s, a) := \\mathbb{E}_{\\pi,P} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) \\vert s_0 = s, a_0 = a \\right].$"}, {"title": "SPECTRALDICE: OPE using Primal-Dual Spectral Representation", "content": "In this section, we first introduce a novel linear representation for the stationary distribution correction ratio using the dual spectral feature of transition kernel. We highlight that this linear structure, together with the known linear representation of Q-functions, helps to bypass the non-convex non-concave optimization required in the computation of DICE estimators, and also enables efficient utilization of historical data sampled by unknown behavior policies. Based on the above ideas, we present SPECTRALDICE, the proposed off-policy evaluation (OPE) algorithm using our primal-dual spectral representation."}, {"title": "Primal-Dual Spectral Representation", "content": "We start by specifying the primal-dual spectral representation used in SPECTRALDICE. At first glance, it may seem natural to directly learn the spectral representation of P as defined in (4). However, it turns out that this naive approach includes the target policy in the linear representation of $d_{\\pi}(\\cdot,\\cdot)$, which in turn induces a complicated representation for the stationary distribution correction ratio $\\zeta(\\cdot,\\cdot)$ (Huang et al., 2023), and thus, leads to an intractable optimization (2) for the computation of the DICE estimator.\nThe above challenge inspires us to properly reparameterize the spectral decomposition (4). Specifically, since we only work with a fixed target policy for off-policy evaluation, we shall consider the following alternative representation of the state-action transition kernel $P^{\\pi}(s', a'|s, a) := P(s'|s, a)\\pi(a'|s')$:\n$P^{\\pi}(s', a'|s, a) = \\langle \\phi(s,a), q(s') \\frac{\\pi(a'|s')}{\\pi_q(s',a')} \\mu^{\\pi}(s') \\rangle.$\nNote that Assumption 2 guarantees a non-zero denominator when the nominator is non-zero. We refer to (5) as the primal-dual spectral representation of the state-action) transition kernel $P^{\\pi}$, where $\\phi(\\cdot,\\cdot)$ and $\\mu^{\\pi}(\\cdot,\\cdot)$ are still called primal and dual spectral features, respectively. The superscript of the dual spectral feature emphasizes its dependence on the target policy.\nThe primal-dual spectral representation has several nice properties. In particular, we can show that the Q-function $Q(s,a)$, the state-action occupancy measured$d^{\\pi}(s,a)$, and the stationary distribution correction ratio $\\zeta(s, a)$ can all be represented in linear forms using the primal/dual features, as summarized below."}, {"title": "Spectral Representation Learning", "content": "In the last section, we have elaborated on how to perform OPE using off-policy data given a primal-dual spectral representation. Now it only suffices to specify how to learn such a representation, which we regard as an abstract subroutine $(\\phi, \\hat{\\mu}^{\\pi}) \\leftarrow \\text{REPLEARN}(\\mathcal{F}, \\mathcal{D}, \\pi)$. Here $\\mathcal{F}$ denotes the collection of candidate representations. We highlight that our algorithm works with any representation learning method that has a bounded learning error, without any further requirements on the learning mechanism. Given a range of spectral representation learning methods available in literature (Zhang et al., 2022a; Ren et al., 2022a,c; Shribak et al., 2024), for the sake of clarity we only consider a few candidates here, while other methods may also be applicable:\nOrdinary Least Squares (OLS). Inspired by Ren et al. (2022a), an OLS objective can be constructed as follows. Denote by $Q^{\\pi}(s', a', s, a) := d_{\\pi_b}(s,a)P^{\\pi}(s', a'|s,a)$ the joint distribution of state-action transitions under behavior policy $\\pi_b$, based on which we plug in (5) to obtain\n$\\frac{Q^{\\pi}(s', a', s, a)}{\\sqrt{d_{\\pi_b}(s, a) d_{\\pi_b}(s', a')}} = \\sqrt{d_{\\pi_b}(s, a) d_{\\pi_b}(s', a')} \\langle \\phi(s, a), \\mu^{\\pi}(s', a') \\rangle,$\nwhich further induces the following OLS objective:\n$\\min_{(\\Phi, \\mu^{\\pi}) \\in \\mathcal{F}} \\mathbb{E} \\left[ \\left( \\frac{Q^{\\pi}(s', a', s, a)}{\\sqrt{d_{\\pi_b}(s, a) d_{\\pi_b}(s', a')}} - \\langle \\phi(s, a), \\mu^{\\pi}(s', a') \\rangle \\right)^2 \\right] dsdads'da'$\nTherefore, $(\\phi, \\mu^{\\pi})$ can be learned by solving (Ren et al., 2022a; HaoChen et al., 2021):\n$\\min_{(\\Phi, \\mu^{\\pi}) \\in \\mathcal{F}} \\left\\{ \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim d_{\\pi_b}, (\\mathbf{s}', \\mathbf{a}') \\sim d_{\\pi_b}} [(\\langle \\phi(s, a), \\mu^{\\pi}(s', a') \\rangle)^2] - 2 \\mathbb{E}_{(\\mathbf{s}, \\mathbf{a}) \\sim d_{\\pi_b}, (\\mathbf{s}', \\mathbf{a}') \\sim P_{\\pi}(\\cdot, \\cdot | s, a)} [\\langle \\phi(s, a), \\mu^{\\pi}(s', a') \\rangle] \\right\\},\\$$\nwhere the last term becomes a constant after expansion and is thus omitted. For practical implementation, we can use stochastic gradient descent to solve the above stochastic optimization problem.\nNoise-Contrastive Estimation (NCE). NCE is a widely used method for contrastive representation learning in RL (Zhang et al., 2022a; Qiu et al., 2022). To learn $(\\phi, \\mu^{\\pi})$, we consider a binary contrastive learning objective (Qiu et al., 2022):\n$\\min_{(\\phi,\\mu^{\\pi}) \\in \\mathcal{F}} \\mathbb{E}_{(\\mathbf{s},\\mathbf{a}) \\sim d_{\\pi_b}} \\left[ \\mathbb{E}_{(\\mathbf{s}',\\mathbf{a}') \\sim P_{\\pi}(\\cdot,\\cdot | \\mathbf{s},\\mathbf{a})} \\left[ \\log \\left(1 + \\frac{1}{\\langle \\phi(\\mathbf{s}, \\mathbf{a}), \\hat{\\mu}^{\\pi}(\\mathbf{s}', \\mathbf{a}') \\rangle} \\right) \\right] + \\mathbb{E}_{(\\mathbf{s}',\\mathbf{a}') \\sim P_{\\text{neg}}} \\left[ \\log \\left(1 + \\langle \\phi(\\mathbf{s}, \\mathbf{a}), \\hat{\\mu}^{\\pi}(\\mathbf{s}', \\mathbf{a}') \\rangle \\right) \\right] \\right],$\nwhere $P_{\\text{neg}}$ is a negative sampling distribution.\nDetails of these representation learning methods along with their learning errors can be found in Appendix C."}, {"title": "SPECTRALDICE", "content": "With the two key components specified above, now we are ready to state SPECTRALDICE, the proposed offline policy evaluation (OPE) algorithm using spectral representations, as displayed in Algorithm 1. Specifically, given a policy $\\pi$, assuming access to an offline dataset $(s, a, s') \\sim \\mathcal{D}$ sampled by the behavior policy $\\pi_b$, we follow a two-step algorithm to evaluate the target policy $\\pi$ in an off-policy manner:\nRepresentation learning. We may choose any representation learning method that comes with a bounded learning error as the REPLEARN subroutine, and the overall sample complexity will depend on this choice (see Section 4)."}, {"title": "Theoretical Guarantee", "content": "In this section, we provide a rigorously theoretical analysis regarding the sample complexity of the proposed SPECTRALDICE algorithm. For the sake of technical conciseness, we make the following assumption on the candidate family $\\mathcal{F}$. We argue that this is not a restrictive assumption, but rather, only helps to highlight the key contributions with simplified analysis."}, {"title": "Continuous Environments", "content": "Setting. We start by comparing SPECTRALDICE with various baseline OPE methods in literature, including BESTDICE (Yang et al., 2020), Fitted Q Evaluation (FQE) (Kostrikov and Nachum, 2020), Model-Based (MB) method (Zhang et al., 2021), Importance Sampling (IS) method (Hanna et al., 2019) and Doubly-Robust (DR) method (Dud\u00edk et al., 2011). We follow the experiment protocol in Yang et al. (2020) to evaluate and compare the OPE performances of these algorithms in three continuous MuJoCo environments, namely Cartpole, Reacher and Half-Cheetah, in an increasing order of difficulty. In our implementation, for representation learning, we parameterize each of $\\phi$ and $\\hat{\\mu}$ with a 2-layer feed-forward neural network. For the OPE step, regularizer is appended to (9), and the estimated policy"}, {"title": "Discrete Environment", "content": "Setting. We proceed to test our method in Four Rooms (Sutton et al., 1999), a classical discrete environment featuring convenient visualization, to study the algorithm's sensitivity for hyperparameters and illustrate the efficacy of representation learning. For representation learning in this tabular MDP, we perform singular value decomposition (SVD) of the matrix $\\left[\\frac{P^{\\pi}(s', a'|s, a)}{d^{\\pi}(s',a')}\\right]$ (indexed (i) by (s, a) and (s', a')) and"}, {"title": "Conclusion", "content": "In this paper, to relieve the intrinsic tension between breaking the curse of horizon and overcoming the curse of dimensionality via DICE estimators, we propose a novel primal-dual spectral representation method that establishes linear spectral representations for both the primal variable (i.e., Q-function) and the dual variable (i.e., stationary distribution correction ratio), which leads to SPECTRALDICE, an efficient and practical OPE algorithm that eliminates the non-convex non-concave saddle-point optimization in DICE and makes efficient use of historical data. The performance of SPECTRALDICE is justified by a theoretical sample complexity guarantee and the empirical outperformance. Future directions include taking one step further to design offline policy optimization methods using primal-dual spectral representations, and applying the algorithm for efficient imitation learning."}]}