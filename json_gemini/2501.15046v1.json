{"title": "Evaluating Hallucination in Large Vision-Language Models based on Context-Aware Object Similarities", "authors": ["Shounak Datta", "Dhanasekar Sundararaman"], "abstract": "Despite their impressive performance on multi-modal tasks, large vision-language models (LVLMs) tend to suffer from hallucinations. An important type is object hallucination, where LVLMs generate objects that are inconsistent with the images shown to the model. Existing works typically attempt to quantify object hallucinations by detecting and measur-ing the fraction of hallucinated objects in generated captions. Additionally, more recent work also measures object hallucinations by directly querying the LVLM with binary ques-tions about the presence of likely hallucinated objects based on object statistics like top-k frequent objects and top-k co-occurring objects. In this paper, we present Context-Aware Object Similarities (CAOS), a novel approach for evaluating object hallucination in LVLMs using object statistics as well as the generated captions. CAOS uniquely integrates object statistics with semantic relationships between objects in cap-tions and ground-truth data. Moreover, existing approaches usually only detect and measure hallucinations belonging to a predetermined set of in-domain objects (typically the set of all ground-truth objects for the training dataset) and ig-nore generated objects that are not part of this set, leading to under-evaluation. To address this, we further employ lan-guage model-based object recognition to detect potentially out-of-domain hallucinated objects and use an ensemble of LVLMs for verifying the presence of such objects in the query image. CAOS also examines the sequential dynamics of ob-ject generation, shedding light on how the order of object appearance influences hallucinations, and employs word em-bedding models to analyze the semantic reasons behind hal-lucinations. By providing a systematic framework to identify and interpret object hallucinations, CAOS aims to offer a nu-anced understanding of both the hallucination tendencies of LVLMs and the factors contributing to object hallucinations.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) like LLaMA, Gemini, and Mixtral (Touvron et al. 2023a,b; Team et al. 2023; Jiang et al. 2024) have demonstrated remarkable capabilities in natu-ral language processing tasks. Building upon this success, researchers have focused on integrating powerful LLMs with visual encoders to create large vision-language mod-els (LVLMs) (Liu et al. 2024a; Gong et al. 2023; Zhu et al. 2023; Dai et al. 2024; Ye et al. 2023). These LVLMs lever-age the language understanding abilities of LLMs while en-hancing their capabilities to process and interpret visual in-formation seamlessly. LVLMs typically utilize the visual en-coders to analyze image data, while replacing the original language encoders with state-of-the-art LLMs. Through a combination of vision-language pretraining and fine-tuning on visual instructions (Wang et al. 2021), LVLMs have demonstrated impressive performance on complex tasks that require integrating visual and linguistic information.\nLVLMs exhibit robust proficiency across various vision-language tasks, showcasing their versatility and adaptability. For instance, they excel in tasks such as image captioning (Herdade et al. 2019), generating descriptive textual repre-sentations of visual content to bridge the semantic gap be-tween images and language. Additionally, LVLMs demon-strate proficiency in visual question answering (Antol et al. 2015; Wu et al. 2017), comprehending and responding to queries posed in natural language based on visual input.\nThe development of LVLMs marks a significant milestone in the convergence of vision and language modalities, open-ing up new avenues for research and application in multi-modal models. However, LLMs, and similarly LVLMs, of-ten suffer from the issue of hallucination (Rawte, Sheth, and Das 2023; Liu et al. 2024b; Xu, Jain, and Kankan-halli 2024), where the generated content contains informa-tion that is inconsistent or unfaithful to the provided input data. Hallucination refers to the phenomenon where the gen-erated content contains nonsensical, contradictory or factu-ally incorrect information that violates the input instructions or prompts. A common form of hallucination in LVLMs is object hallucination (Rohrbach et al. 2018; Li et al. 2023; Zhou et al. 2023), where the model describes objects or en-tities that are not present in the visual input. Recent studies have shown that LVLMs suffer from severe object hallucina-tion issues (Li et al. 2023; Zhou et al. 2023), often generating descriptions that include objects inconsistent with the inputs. Hallucinations can be influenced by the visual instructions or prompts, as objects frequently occurring in the instruc-tions or co-occurring with image objects are more prone to being hallucinated.\nHallucination in both LLMs and LVLMs can be problem-atic, as it can lead to the generation of unreliable or mis-leading information. This issue undermines the reliability of"}, {"title": "Related Works", "content": "There are a number of related works on evaluating object hallucinations in LVLMs. Rohrbach et al. (2018) proposed the CHAIR family of metrics which uses rule-based parsing to identify in-domain objects in an LVLM-generated anno-tation and then calculates the fraction of hallucinated objects per caption and the fraction of annotations with hallucinated objects in a set of generated annotations. Another relevant precursor to our work is POPE (Li et al. 2023). POPE mea-sures the tendency of an LVLM to hallucinate objects by asking yes/no questions to the model about whether certain objects exist in the image, instead of directly evaluating the generated annotations. The choice of objects to be used for such queries can be random, or based on object statistics of the pre-training datasets, such as objects known to co-occur with ground-truth objects, or most frequent objects in the dataset. H-POPE (Pham and Schott 2024) is an extension of POPE that assesses hallucinations in object existence as well as attributes by hierarchically refining the yes/no questions from coarse-to-fine-grained, progressively probing about the attributes of the objects in the image. Lovenia et al. (2023) proposed NOPE, that uses a static set of negative pronouns to determine if a model hallucinates. There are a couple of works that detect object hallucination and perform post-hoc tuning to generate captions that do not contain hallucinations (Zhou et al. 2023; Dai et al. 2022)."}, {"title": "Background", "content": "Large Vision-Language Models\nAn LVLM architecture comprises of a vision encoder, a lan-guage encoder (i.e., an LLM), and a cross-modal alignment network. The vision backbone and cross-modal alignment networks are usually used to convert an input image I into a set of n visual tokens $X_{1:n}$, whereas the accompanying text query is converted to text tokens $Y_{1:q}$ and the subsequent response by the LVLM, parameterized by weights $\\theta$, is gen-erated based on the probabilities"}, {"title": "Large Vision-Language Models", "content": "An LVLM architecture comprises of a vision encoder, a lan-guage encoder (i.e., an LLM), and a cross-modal alignment network. The vision backbone and cross-modal alignment networks are usually used to convert an input image I into a set of n visual tokens $X_{1:n}$, whereas the accompanying text query is converted to text tokens $Y_{1:q}$ and the subsequent response by the LVLM, parameterized by weights $\\theta$, is gen-erated based on the probabilities\n$p(Y_{q+1:q+r}) = \\prod_{t=q+1}^{q+r} P_{\\theta}(Y_t|Y_{<t}, X_{1:n}).$\n(1)\nThe training pipeline of LVLMs entails several crucial steps:\nPre-training on Unimodal Data: Initially, the vision en-coder and the language encoder undergo pre-training on large-scale unimodal datasets, encompassing image and text data, respectively.\nImage-Text Alignment Pre-training: Subsequently, these encoders are aligned through image-text alignment pre-training. This alignment enables LVLMs to generate co-herent and meaningful captions for given images by lever-aging the synergy between visual and textual modalities.\nFine-tuning on Image-Text Instructions: The aligned LVLM model undergoes further fine-tuning on image-text instructions to refine its ability to generate satisfactory an-swers to natural language questions related to specific im-ages. This fine-tuning process enhances the model's perfor-mance on diverse multimodal tasks."}, {"title": "Object Hallucination", "content": "Object hallucination occurs when a LVLM references ob-jects that are not actually present in the image it is describ-ing. This can produce unexpected outcomes, particularly in applications like visual question answering, image caption-ing, or event detection, where accurate identification of ob-jects is crucial. Several factors can contribute to these hallu-cinations:\nCommon Objects in Training Data: LVLMs might hal-lucinate objects that are frequently represented in their train-ing datasets. Examples of such objects include \"person,\" \"dining table,\u201d and \u201cchair,\u201d which are prevalent in visual in-struction datasets.\nCo-occurring Objects: Hallucinations may also arise when LVLMs predict objects that commonly appear to-gether with the actual objects in the image, based on patterns observed in the training data.\nImpact of Visual Instructions: The type of visual in-structions used during training plays a significant role in hallucination tendencies. For instance, LVLMs like Instruct-BLIP, which are trained on diverse public datasets with con-cise instructions, are more likely to generate accurate, albeit brief responses. In contrast, models trained with extended synthetic instructions from unimodal language models may be prone to hallucinating due to inconsistencies or excessive detail in the synthetic data.\nInfluence from the already generated context: In ad-dition to the above factors which are known to impact ob-ject hallucinations, we also postulate that the objects already mentioned in the prior context at any given point during gen-eration may also cause hallucinations in a manner similar to the co-occurrence statistics of the ground-truth objects mentioned above. On probing LLaVA (Liu et al. 2024a) and mPLUG-Owl (Ye et al. 2023) with the instruction \"Provide a brief description of the given image.\u201d for a subset of 2000 images from the MSCOCO (Lin et al. 2014) validation set (identical to that used by POPE (Li et al. 2023)), we find that respectively 20% and 16% of the hallucinated objects happen to be the most frequent object to have co-occurred in the training dataset with at least one preceding objects in the already generated part of the annotation. Thus, the past objects in the already generated part of an annotation can also influence object hallucinations in the remainder of the annotation which is yet to be generated. While it is straight-forward to measure the fraction of hallucinated objects that"}, {"title": "CAOS: Context-Aware Object Similarities", "content": "To account for all the different factors that can influence object hallucinations, we propose a set of evaluation metrics based on context-aware object similarities, called CAOS, to holistically measure how the contents of the image, the se-quential ordering of generated objects, as well as the dom-inant contents of the training dataset influences hallucina-tion. The proposed group of measures consists of CAOS T , CAOS X , CAOS K , CAOS T /X , CAOS X/K , and CAOS avg . For a given word embedding model E, the CAOS T , CAOS X and CAOS K scores measure the maximum cosine similarity between the embeddings of a hallucinated object and a set of other objects. In particular, CAOS T measures the maxi-mum cosine similarity for a given hallucinated object with all the ground-truth objects present in the image. CAOS X , on the other hand, measures the maximum cosine similarity between a hallucinate object and all past objects appearing before itself in the generated caption. In practice, we also consider all ground-truth objects, irrespective of their posi-tion in the generated caption, to be valid past objects for all hallucinated objects, since the ground-truth objects may al-ways influence generation due to their presence in the image. Finally, since it is well-known that the most frequent objects present in the training datasets can also influence hallucina-tions (Li et al. 2023), CAOS K measures the maximum co-sine similarity between the embeddings of the hallucinated object and the top-k frequent objects in the training dataset (MSCOCO in our experiments).\nFurthermore, since it is often relatively more tolerable for hallucinations to be semantically related to the ground-truth objects known to be present in the image than to other ob-jects in the generated caption, we calculate CAOS T /X to be the ratio between CAOS T and CAOS X . A high CAOS T /X signifies that the hallucinations are mostly influenced by ground-truth objects. Similarly, it may be relatively more tolerable for the hallucinations to be related to past objects in the generated caption (including all ground-truth objects) than to frequent objects in the training dataset which may not have any relation to the contents of the image being described. Therefore, a high value of CAOST/K, which is the ratio between CAOSX and CAOSK, may be desirable. Lastly, we also calculate CAOSavg which is the mean of CAOST, CAOSx, and CAOSK. We argue that a high value of CAOSavg is also desirable in most cases. This is because high CAOSang values denote that the hallucinations can be accounted for by the mentioned factors and is less likely to have been caused by unknown eccentricities of the LVLM.\nGiven an image, the associated ground-truth object la-bels, and a caption generated by the LVLM for the image, we begin by identifying all objects in the captions. We then identify which of the objects are hallucinated based on the ground-truth object labels (or using an oracle for out-of-domain objects). In order of their appearance in the gener-ated caption, we calculate the CAOS scores for all the hallu-cinated objects. Consequently, for a given caption contain-ing multiple hallucinated objects, we report average values"}, {"title": "Algorithm 1: Calculating CAOS", "content": "Inputs: LVLM po to be evaluated.\nImage I with ground-truth object labels.\nSet G of known objects from fine-tuning dataset labels\nSet K of top-k frequent objects from fine-tuning dataset\nRule-based object parser P\nLLM q for object identification and oracle O\nEmbedding model E\nCosine similarity operator Scos\nOutputs: Scores CAOST, CAOSX, CAOSK, CAOST/\u0445, CAOS X/K, and CAOSavg.\nGenerate caption for I using po.\nIdentify list L\u2081 of objects in the generated caption belonging to G using P.\nIdentify list L2 of objects in the generated caption not in G, using q.\nCombine lists L = L1 UL2 preserving the order of the objects in the generated caption.\nConstruct map M: L \u2192 {0,1} labeling genuine objects as 1 and hallucinated objects as 0 using ground-truth labels for l\u2208 L\u2081 and oracle O for l\u2208 L2.\nInitialize lists T and X with ground-truth objects.\nExpand sets T = TUT' and X = X UT', where T' = {l \u2208 L2|M(l) = 1}.\nInitialize empty set H, Ts, Xs, and Ks.\nfor all l\u2208 L in order do\nif M (1) = 0 then\nAdd l to set H.\nAdd maxoer Scos (E(l), E(o)) to set Ts.\nAdd maxoex Scos (E(l), E(o)) to set Xs.\nAdd maxo\u2208K Scos (E(l), E(o)) to set Ks.\nend if\nAdd l to set X.\nend for\nCalculate CAOST = \u2211Ts/|H|.\nCalculate CAOSx = \u2211Xs/|H|.\nCalculate CAOSK = \u2211Ks/|H|.\nCalculate CAOST/x= CAOST/CAOSx.\nCalculate CAOS X/K= CAOSX/CAOSK.\nCalculate CAOSavg= (CAOST + CAOSx + CAOSK)/3.\nfor all the CAOS scores over all hallucinated objects in the caption."}, {"title": "Augmenting object identification using LLM", "content": "We ob-serve that the standard rule-based parsing which is used by existing methods like CHAIR (Rohrbach et al. 2018) to iden-tify objects in a caption fails to identify objects beyond the set of known objects in the training dataset. Moreover, we observed that the rule-based approach may sometimes fail to even identify the ground-truth objects present in the cap-tion. This can lead to lower recall scores for the LVLM be-ing evaluated when ground-truth objects are missed. On the other hand, this can also lead to under-evaluation of object hallucinations when hallucinated objects are not detected. Therefore, we augmented the rule-based method with an ad-ditional list of objects identified by an LLM (LLaMA-2-7B (Touvron et al. 2023b) in our experiments), based on the generated caption, aided by in-context learning on a hand-ful of examples (5-shot examples in our experiments). We also tally the detected objects in this augmented list with the actual words in the caption to weed-out any potential hallu-cinations by the LLM."}, {"title": "Oracle using an ensemble of LVLMs", "content": "Since we have no ground-truth reference to identify whether the additional out-of-domain objects identified using the LLM are actu-ally present in the image, we further label these identified objects as either genuine or hallucinated based on an oracle consisting of an ensemble of LVLMs. Given an object and the original image, we query each of the LVLMs in the en-semble with a query of the form \u201cDoes the image contain <object>? Please respond with only Present or Absent.\", to force the model to vote on the presence or absence of the object in the image. For our experiments, we use an ensem-ble of InstructBLIP (Dai et al. 2024), LLaVA-7B (Liu et al. 2024a), mPLUG-Owl (Ye et al. 2023), and MiniGPT-4 (Zhu et al. 2023), and break ties in favor of absence to minimize false positives and penalize potential hallucinations.\""}, {"title": "Results", "content": "We conduct all our experiments on the subset of 2000 im-ages from the MSCOCO (Lin et al. 2014) validation set identical to that used by POPE (Li et al. 2023). For each of these 2000 images, we use the instructions \"Provide a brief description of the given image.\" and \"Question: Gen-erate a short caption of the image. Answer: to probe 5 LVLMs, namely InstructBLIP (Dai et al. 2024), LLaVA (Liu et al. 2024a), mPLUG-Owl (Ye et al. 2023), MiniGPT-4 (Zhu et al. 2023), and MultimodalGPT (Gong et al. 2023) to generate captions for the image. A detailed comparison of the LVLMs, in terms of model sizes and training recipes is shown in the Appendix.\nIn Table 1, we report all 6 CAOS scores, namely CAOS T , CAOS X , CAOS K , CAOS T /X , CAOS X/K , and CAOS avg , using two different word embedding models, GloVe (Pen-nington, Socher, and Manning 2014) and MiniLM-L6 (Wang et al. 2020; all-MiniLM-L6-v2). We choose these embedding models as they form embeddings based on slightly different objectives. GloVe aims to assign similar embeddings to objects which co-occur is a corpus of docu-ments, while MiniLM-L6 assigns semantically meaningful embeddings based on pretraining on a large number of lan-guage tasks such as question answering, natural language inference, etc. For the CAOS K scores, we choose k = 3 based on the trends of CAOS K scores across k values (see a full discussion in the subsequent section). Additionally, we also report precision (i.e., the fraction of detected objects which are not hallucinations) and recall (which is the frac-tion of actual ground-truth objects from the query image that are mentioned in the generated caption). Due to the rela-tive instability of the precision measure (related to CHAIR1 (Rohrbach et al. 2018)), we also report POPE-F1 scores for all the models (Li et al. 2023). We also report the fraction of captions having hallucinated objects, which is equivalent to the CHAIRS metric proposed by Rohrbach et al. (2018). LVLMs which are prone to generating shorter captions with fewer objects consequently have less scope for hallucina-"}, {"title": "Effect of varying k on CAOSK", "content": "The choice of k for CAOSK can affect the CAOS scores. Therefore, we vary k to observe how CAOSK is impacted for the LVLMs. The CAOSK scores for all the LVLMs are shown in Figure 4. We observe that the CAOSK scores for all models saturate to some extent at k = 3, suggesting that the top-3 most frequent objects in MSCOCO disproportion-ately appear as hallucinations for all the models. This sug-gests that a choice of k = 3 can capture most of the infor-mation about how the frequent in-domain objects can affect object hallucinations. Additionally, despite the diminishing rate of impact beyond k = 3, we also observe that LLaVA and mPLUG-Owl continue to be impacted more by frequent objects beyond the top 3."}, {"title": "Comparison between different subsets of objects", "content": "In Figure 5, we investigate how the CAOS T , CAOS X , and CAOS K scores vary across different subsets of halluci-nated objects. Since our proposed LLM-augmented object detection is meant to uncover the hallucination of out-of-domain objects, we inspect what the CAOS scores look like for just the hallucinated in-domain MSCOCO objects and just the out-of-domain (denoted as non-MSCOCO) objects. The CAOS scores for both groups largely follow a similar trend to that of all objects, but the in-domain MSCOCO objects seems to have a more pronounced influence from the frequently occurring objects, as implied by the ele-vated CAOS K scores. Conversely, the out-of-domain ob-jects have a lower albeit non-negligible impact from the fre-quent MSCOCO objects. This suggests that even the hallu-cinated out-of-domain objects are semantically influence by the frequent MSCOCO objects to a certain extent. Further, due to the higher impact from the top-3 frequently occurring objects, we also investigate how the CAOS scores change when the hallucinated instances of only the 3 most common"}, {"title": "Consistency across different prompt styles", "content": "To analyze the sensitivity of CAOS scores to changes in the text prompt, we rerun our experiments on LLaVA, mPLUG-Owl, and MiniGPT-4 with 12 new instructions in addition to the 2 instructions already used in Table 1. The list of all in-structions is shown in the Appendix. We find that the average results over all 14 instructions, detailed in Table 2, are over-all quite similar to those in Table 1, indicating that CAOS scores are largely stable to changes in instructions. We also illustrate how CAOS T , CAOS X and CAOS K values vary across the instructions in Figure 6. For a given model and specific CAOS score, we observe some variability across instructions, which is to be expected. Furthermore, while the CAOS scores have different ranges across the different LVLMs, the ranges maintain an ordering similar to that ex-hibited by the mean CAOS scores (shown in red), further indicating stability across instructions. Finally, the CAOS scores with MiniLM-L6 embeddings look slightly differ-ent than those using GloVe. CAOS scores calculated using MiniLM-L6 embeddings seem to be slightly more prone to having outliers than their corresponding GloVe counterparts and CAOS scores for LLaVA have higher variance than the other two models with MiniLM-L6 embeddings."}, {"title": "Conclusion and Limitations", "content": "Existing methods do not investigate the interplay between object statistics and the semantics of objects in query im-ages or generated context, leaving a gap in understanding object hallucinations during the generation process. To ad-dress this, we propose the novel CAOS framework for sys-tematically evaluating object hallucination in LVLMs using generated captions. CAOS focuses on the interaction be-tween generated objects (hallucinated or otherwise), their positional dynamics, ground-truth objects, and object statis-tics from training data to offer a deeper understanding of hallucination dynamics. Our key contributions include de-tecting out-of-domain hallucinated objects using LLMs and an oracle based on an ensemble of LVLMs, analyzing se-quential generation dynamics, and employing word embed-ding models to explore the semantic relationships behind hallucinations. We conduct experiments with several diverse LVLMs and find that CAOS effectively identifies halluci-nations and provides insights into trade-offs, such as the tendency of certain models to hallucinate frequent objects from training datasets versus ground-truth-related objects. Notably, MiniGPT-4 demonstrates competitive performance across metrics, suggesting that it tends to hallucinate fewer and more contextually relevant objects. In summary, CAOS provides a systematic and nuanced framework for under-standing hallucination dynamics, supporting the develop-ment of more reliable and robust LVLMs.\nIt is important to note the limitations of our study de-spite the extensive exploration undertaken. We focus only on object hallucination in LVLMs, leaving out other per-formance aspects such as the ability to generate more ar-ticulate or contextually coherent responses. Moreover, we use a partial validation set of 2000 MSCOCO images due to computational constraints, which could potentially skew our results. However, for consistency with existing works, we retained the same subset of images employed by Li et al. (2023). Additionally, our reliance on rule-based object de-tection, augmented by an LLM and an oracle for verification, may occasionally lead to inaccuracies due to errors in any of these components, though such cases are likely rare. Finally, our analysis considers only a small subset of state-of-the-art LVLMs, excluding some newer or closed-source models. Nevertheless, we view these findings as a step forward in developing more reliable and human-aligned LVLMs. Fu-ture work could extend the CAOS framework to encompass other types of hallucinations, such as spatial, relational, or numerical inconsistencies, offering a holistic evaluation of an LVLM's multimodal understanding."}]}