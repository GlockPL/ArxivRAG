{"title": "An Explainable Vision Transformer with Transfer Learning\nCombined with Support Vector Machine Based Efficient\nDrought Stress Identification", "authors": ["Aswini Kumar Patra", "Ankit Varshney", "Lingaraj Sahoo"], "abstract": "Early detection of drought stress is critical for taking timely measures for reducing crop loss before\nthe drought impact becomes irreversible. The subtle phenotypical and physiological changes in\nresponse to drought stress are captured by non-invasive imaging techniques and these imaging\ndata serve as valuable resource for machine learning methods to identify drought stress. While\nconvolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a\npromising alternative in capturing long-range dependencies and intricate spatial relationships,\nthereby enhancing the detection of subtle indicators of drought stress. We propose an explainable\ndeep learning pipeline that leverages the power of ViTs for drought stress detection in potato\ncrops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT\nand support vector machine (SVM), where ViT extracts intricate spatial features from aerial\nimages, and SVM classifies the crops as stressed or healthy and an end-to-end approach using\na dedicated classification layer within ViT to directly detect drought stress. Our key findings\nexplain the ViT model's decision-making process by visualizing attention maps. These maps\nhighlight the specific spatial features within the aerial images that the ViT model focuses as the\ndrought stress signature. Our findings demonstrate that the proposed methods not only achieve\nhigh accuracy in drought stress identification but also shedding light on the diverse subtle plant\nfeatures associated with drought stress. This offers a robust and interpretable solution for drought\nstress monitoring for farmers to undertake informed decisions for improved crop management.", "sections": [{"title": "1 Introduction", "content": "Drought stress adversely affects the growth, development and yield of plants. The rise in global tem-\nperature coupled with the growing demand for water severely depletes soil water reserves. Water\ndeficit and temperature stress can cause damage to the reproductive growth phase of crops, resulting\nin a substantial reduction in yield [1]. Furthermore, prolonged exposure to water deficit stress partic-\nularly in the reproductive stage can significantly affect grain quality. Ensuring sustainable crop yields\nin the face of escalating drought conditions has become a great challenge [2]. The plant's response to\ndrought stress is influenced by a number of factors such as stress duration, severity, genotype and its\ndevelopmental stage [3]. In plants, drought stress in early phase induces reduced stomatal conduc-\ntance, transpiration and net carbon assimilation, whereas mid- and late-phases result in visible leaf\nsenescence, stagnant growth and wilting. The early detection of drought induced phenotypic changes\nin plants, facilitates in preventing the adverse impacts of drought from irreversible damage through\ntimely irrigation and other management practices. The point based, contact type and destructive\nmethods based estimation of drought stress impact on crops are limited by good approximation of\nrepresentative area as well as sample size and hence, difficult to implement over larger agricultural\nproduction systems. On the contrary, image-based, non-invasive and high-throughput analyses of\nsubtle phenotypical changes in plants due to drought and other stresses, provides an efficient means\nof monitoring stress levels in crops and good prediction of drought stress impact [4]. Various imaging\ntechniques and sensors are utilized to precisely capture stress responses, including simple red-blue-\ngreen (RGB), thermal, multispectral, hyperspectral, or fluorescence imaging methods [5]. However,\ntraditional image processing generates significant amount of variation in light intensity, shading,\nocclusion, etc. leading to variation in quality thereby increases complexity of image processing [6].\nMachine learning (ML) techniques, on the other hand, solve such complex problems which are non-\nlinear in nature by identifying key feature to be extracted from the acquired image in real time, based\non knowledge and/or domain expertise of the user. Hence, ML aids in better decision-making and\ninformed actions in real-world scenarios with minimal human intervention [7]. Deep learning (DL) [8]\ntechniques however, help in extracting different information from the raw image data during input\ntraining, by means of various convolutions which allows larger learning capabilities, that result in the\nbest classification/regression and higher performance and precision. Convolutional Neural Networks\n(CNNs) have been developed for providing a much-detailed representation of the image features, for\na more discriminative and detailed analysis of plant images, leading to highly accurate classifications\n[9]. Many studies have used deep CNN models to detect drought stress in affected plants, mostly\nsolved either by finely tuned CNNs or by training from scratch.\nEarly work by Zhuang et al. [10] utilized a combination of segmentation, color, and texture\nfeature extraction, employing a gradient boosting decision tree (GBDT) method to detect water\nstress in maize. However, a subsequent study using the same RGB dataset demonstrated that a\ndeep convolutional neural network (DCNN) significantly outperformed GBDT in terms of perfor-\nmance [11]. Sankararao et al. [12] developed a comprehensive pipeline for detecting water stress\nin groundnut canopies using hyperspectral imaging. The pipeline included steps for image quality"}, {"title": "2 Materials and Methods", "content": "In this section, we begin by introducing the experimental dataset. Next, we present the drought\nstress classification model using two different approaches. In the first approach, we employ the vision\ntransformer with transfer learning. In the second approach, we propose a framework that utilizes a\nvision transformer as a feature extractor, followed by the integration of an SVM as the classifier.\nAdditionally, we investigate the interpretability of the model by generating and analyzing the atten-\ntion maps. This comprehensive use of the ViT elucidates how the spatial features of drought stress\ncan be precisely identified. Finally, we discuss the performance metrics for the proposed approaches."}, {"title": "2.1 Preparing the Data", "content": "The potato crop aerial images utilized in this study have been sourced from a publicly accessible\ndataset that encompasses multiple modalities [28]. Collected from a field at the Aberdeen Research\nand Extension Center, University of Idaho, these images serve as valuable resources for training\nmachine learning models dedicated to crop health assessment in precision agriculture applications.\nAcquired using a Parrot Sequoia multi-spectral camera mounted on a 3DR Solo drone, the dataset\nfeatures an RGB sensor with a resolution of 4,608 \u00d7 3,456 pixels and four monochrome sensors\ncapturing narrow bands of light wavelengths: green (550nm), red (660nm), red-edge (735nm), and\nnear-infrared (790nm), each with a resolution of 1,280 \u00d7 960 pixels. The drone flew over the potato\nfield at a low altitude of 3 meters, with the primary objective of capturing drought stress in Russet\nBurbank potato plants attributed to premature plant senescence.\nThe dataset consists of 360 RGB image patches in JPG format, each measuring 750\u00d7750 pix-\nels. These patches were derived from high-resolution aerial images through cropping, rotating, and\nresizing operations. The dataset is divided into a training subset of 300 images and a testing subset\nof 60 images. Ground-truth annotations, provided in both XML and CSV formats, indicate regions\nof healthy and stressed plants marked by rectangular bounding boxes. These annotations were man-\nually created using the LabelImg software. An example RGB image and its annotated regions are\nshown in Fig. 1. The testing subset is independent of the training subset and comes from different\naerial images. Additionally, the dataset includes corresponding image patches from spectral sensors\nwith red, green, red-edge, and near-infrared bands, each sized 416\u00d7416 pixels. However, we are only\nutilizing the RGB images due to the limitations of the low-resolution monochromatic images."}, {"title": "2.2 Vision Transformer (ViT)", "content": "A vision transformer (ViT) is a type of neural network architecture that has revolutionized the field of\ncomputer vision [29]. Unlike traditional convolutional neural networks (CNNs), which process images\npixel by pixel, ViT processes the input images in a sequential manner by dividing them into fixed-size\npatches, linearly embedding these patches, and then applying self-attention mechanisms for capturing\nglobal dependencies [22]. The VIT architecture used for our work is inspired by Dosovitskiy et al. [22]\nand depicted in Fig 2a. It processes images by dividing them into fixed-size embedded patches,\nlinearly transforming these patches, and treating them as sequences. The transformer architecture\nis then applied, incorporating multi-head self-attention mechanisms [30] that enable the model to\ncapture long-range dependencies within the image. Layer normalization is applied before and after\nthe multi-head attention, ensuring stable training, and a Multilayer Perceptron (MLP) head is added\nto the transformer's global representation for task-specific processing."}, {"title": "2.2.1 ViT Architecture", "content": "The proposed ViT architecture is based on the layers in [31] and Dosovitskiy et al. [22] and comprises\nfive main components: patch embedding, positional encoding, transformer encoder, normalization\nlayer and a classification head. This is illustrated in Fig 2a.\n\u2022 Patch Embedding: Input images are divided into fixed-size patches, which are then linearly\nembedded to create a sequence of embeddings.\n\u2022 Positional Encoding: To capture spatial information, positional encodings are added to the\npatch embeddings, allowing the model to understand the relative positions of different patches.\n\u2022 Transformer Encoder: The embedded patches are fed into a Transformer Encoder, which is\nthe core component of the ViT architecture. This encoder consists of twelve identical encoder\nlayers stacked together. Each attention layer analyzes the relationships between pairs of patches,\nallowing the model to understand how different image regions interact and influence each other.\nEach attention layer consists of the following:\nMulti-Head Self-Attention: This mechanism allows the model to weigh the importance of different\nparts of the image. It captures global dependencies between the patches.\nMLP (Multi-Layer Perceptron) Block: This block introduces non-linearity to the network and\nfurther processes the information from the attention layer.\nNormalization Layers: Layer normalization is applied after the multi-head attention and MLP\nblocks to stabilize training.\nDropout: The model uses dropout at two levels: within the attention mechanism and after the\nMLP block. Dropout is used to prevent over-fitting by randomly dropping out neurons during\ntraining.\n\u2022 Normalization Layer: Layer normalization applied to the output of the encoder serves several\ncrucial purposes: Reduces internal co-variate shift, improves gradient flow, acts as regularization,\nhandles varying input distributions and accelerates convergence.\n\u2022 Classification Head: This head typically consists of a simple MLP layer that maps the feature\nrepresentation to the desired output, such as class probabilities."}, {"title": "2.2.2 Information Processing in ViT", "content": "At the core of the Vision Transformer (ViT) is the concept of a token, which plays a crucial role in\nthe model's ability to process and understand images. A token is a fixed-size vector that represents\na small patch of the input image. These tokens are at the core of the Vision Transformer model,\nforming the input sequence to the Transformer layers. This token-based approach enables the model\nto process and understand the image by focusing on the relationships between these patches through\nself-attention. The detailed explanation is given below.\n\u2022 Dividing the Image into Patches:\n The input image is divided into smaller, fixed-size patches. For example, an image of size 224x224\npixels might be divided into patches of size 16x16 pixels, resulting in $({\\frac{224}{16}})^2 = 196$ patches."}, {"title": "2.3 ViT with Transfer Learning", "content": "The proposed framework, as shown in Fig 2b effectively combines transfer learning, the power of the\nVision Transformer and attention-based interpretability to address the challenging task of drought\nstress identification in potato crop images captured in natural settings. A core component of this\napproach is the utilization of pre-trained weights. This technique, known as transfer learning, involves\nleveraging knowledge gained from solving one problem (often a large-scale image classification task)\nand applying it to a different but related problem. By employing pre-trained weights, the model can\nbenefit from the rich feature representations learned from a massive dataset, accelerating training\nand potentially improving performance. Using this the ViT is trained over multiple epochs using a\ncombination of binary cross-entropy loss, Adam optimizer and learning rate tuning. Experimenting\nwith different learning rates is essential to find the optimal value for convergence and generalization.\nEventually, the classification layer is responsible for making the final prediction. It takes the output\nof the ViT model and maps it to two classes: \"healthy\" and \"stressed.\" This layer typically consists\nof a fully connected neural network with a sigmoid activation function for binary classification."}, {"title": "2.3.1 Attention Maps", "content": "Attention maps provide insights into how the model focuses on different parts of the image during\nthe self-attention mechanism. This mechanism allows the model to selectively attend to specific areas\nwhile processing visual information. The image is first divided into patches. Self-attention then helps\nthe model prioritize relevant patches and their relationships for effective feature extraction.\nAttention maps act as a visual representation of the weights assigned by the self-attention mecha-\nnism to each patch. These maps can be visualized for each self-attention layer within the ViT model,\nas each layer progressively learns more intricate relationships between these image features. Higher\nweights indicate a greater focus on a specific patch and its connection to others. By analyzing these\nmaps, we can essentially see the model's \"thought process\" during image understanding. We can\nidentify which regions it prioritizes for information extraction. The following section highlights the\ncomputation behind the attention mechanism [31].\nAt its core, self-attention computes a weighted sum of the values (features) based on the simi-\nlarities (attention scores) between different positions in the sequence. This is achieved through three\nlearnable matrices: Query (Q), Key (K), and Value (V).\nQuery Matrix (Q): The query matrix is responsible for capturing the information about the\ncurrent token being processed. It learns to encode the features of the token in a format suitable for\ncomparison with other tokens. Each token in the sequence is associated with a query vector, which\nrepresents its characteristics in the context of the entire sequence.\nKey Matrix (K): The key matrix holds information about the relationship between the current\ntoken and other tokens in the sequence. It learns to encode the features that determine how relevant\neach token is to the current token.\nValue Matrix (V):\nThe key matrix holds information about the relationship between the current token and other\ntokens in the sequence. It learns to encode the features that determine how relevant each token is to\nthe current token.\nGiven a sequence of tokens $X = [X_1, X_2, ..., x_n]$, the attention scores A are computed as:\n$A = softmax (\\frac{Q K^T}{\\sqrt{d_k}})$"}, {"title": "2.4 Integrating Vision Transformer (ViT) and Support Vector Machine\n(SVM)", "content": "This framework focuses on combining a vision transformer (ViT) and a support vector machine\n(SVM) within a three-step approach for effective stress identification.\n\u2022 Feature Extraction: For each image in the dataset, extract the final hidden state or pooled rep-\nresentation from the ViT model to obtain a feature vector. Let X represent the set of embedded\nfeatures extracted from a dataset of images, and y represent the corresponding class labels.\n\u2022 Training SVM: Train an SVM classifier using the extracted features X and their corresponding\nclass labels y.\n\u2022 Classification: For a new, unseen image, extract features using the pre-trained ViT model and use\nthe trained SVM classifier to predict its class."}, {"title": "2.4.1 Support Vector Machine (SVM)", "content": "Support Vector Machines [32] aim to find a hyperplane that best separates a given set of data points\ninto different classes. Given a set of labeled data points $(x_1, y_1), (X_2,Y_2), ..., (x_n, Y_n)$ where x is the\nfeature vector of the i-th data point, and $y_i$ is its corresponding class label ($y_i \\in {1,0}$ for binary\nclassification), SVM seeks to find a hyperplane defined by the equation:\n$w.x+b=0$\nwhere w is the weight vector and b is the bias.\nThe goal is to maximize the margin, which is the distance between the hyperplane and the nearest\ndata point from each class. The margin is computed as the perpendicular distance from a data point\n$x_i$ to the hyperplane:\n$margin = \\frac{1}{||w||} |w.x_i + b|$\nSubject to the constraint that for all data points:\n$Y_i(w x_i + b) \\geq 1$\nThis constraint ensures that data points are correctly classified and lie on the correct side of the\nhyperplane.\nThe SVM optimization problem can be formulated as:\n$Minimize \\frac{1}{2} ||w||^2$"}, {"title": "2.4.2 ViT+SVM Framework", "content": "The process begins with input images that are fed into the ViT. The ViT, pre-trained on a vast\ndataset, is adept at extracting meaningful features from the images. Once the features are extracted\nby the ViT, they are compiled into a feature matrix, which also includes the corresponding labels\nindicating whether the plants in the images are healthy or stressed. This matrix forms the input to\nthe SVM, a robust classifier known for its effectiveness in handling high-dimensional data. The SVM\nis trained to discern between the two classes-healthy and stressed-based on the features provided.\nFinally, the trained SVM predicts the class of new images, categorizing them as either healthy or\nstressed. The entire approach is depicted in Fig 2c. The efficacy of this framework is evaluated using\na designated test set and k-fold cross-validation."}, {"title": "2.5 Performance Evaluation Metrics", "content": "The model's performance underwent assessment using various evaluation metrics, including accuracy,\nprecision, recall (sensitivity) and the Receiver Operating Characteristic (ROC) curve. These metrics\nare computed based on the counts of true positives (TP), true negatives (TN), false positives (FP),\nand false negatives (FN), which collectively form a 2x2 matrix known as the confusion matrix. In\nthis matrix, TP and TN indicate the accurate predictions of water-stressed and healthy potato crops,\nrespectively. FP, termed as type 1 error, denotes predictions where the healthy class is inaccurately\nidentified as water-stressed. FN, referred to as type 2 error, represents instances where water-stressed\npotato plants are incorrectly predicted as healthy. The classification accuracy is a measure of the ratio\nbetween correct predictions for both stressed and healthy images and the total number of images in\nthe test set.\n$Accuracy = \\frac{True Positive + True Negative}{Total Population}$\n$Precision = \\frac{True Positive}{True Positive + False Positive}$\n$Recall = \\frac{True Positive}{True Positive + False Negative}$\n$F1-score = 2.\\frac{Precision \\cdot Recall}{Precision + Recall}$\nCross-validation is crucial in model development for two key reasons: it helps prevent over-fitting\nby assessing a model's performance across different subsets of the data, and it ensures the model's\ngeneralization ability, providing a reliable estimation of its effectiveness under various conditions. The\nmodel was trained and evaluated using k-fold cross-validation, a robust technique for assessing the\ngeneralization performance of the model. In each iteration of the k-fold cross-validation process, the\ndataset was partitioned into k folds, and the model was trained on k-1 folds while being validated on\nthe remaining fold. This process was repeated k times, ensuring that every fold had the opportunity\nto serve as the validation set. For each fold, the Receiver Operating Characteristic (ROC) curve\nwas plotted, illustrating the trade-off between true positive rate and false positive rate at various\nthresholds. After completing the k-fold cross-validation, the individual ROC curves were aggregated,\nand the mean ROC curve was calculated and plotted. AUC (area under the curve) measures the\nentire two-dimensional area underneath the entire ROC curve. AUC provides an aggregate measure\nof performance across all possible classification thresholds. One way of interpreting AUC is as the\nprobability that the model ranks a random positive example more highly than a random negative\nexample. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC\nof 0.0; one whose predictions are 100% correct has an AUC of 1.0.This comprehensive approach\nprovides a more reliable estimation of the model's performance, capturing its consistency across\ndifferent subsets of the data and enhancing the overall assessment of its predictive capabilities."}, {"title": "3 Results and Discussion", "content": "In this section, we present the experimental results of our proposed model for identifying drought\nstress in potato crop field images. First, we distinguish between healthy and stressed images. Then,\nwe identify the spatial features responsible for the stress. Our experiments with the proposed Vision\nTransformer (ViT) framework were conducted in three ways:\n\u2022 ViT with Transfer Learning: Leveraging pre-trained weights.\n\u2022 ViT+SVM with Pre-trained Weights.\n\u2022 ViT+SVM with Optimal Weights. The optimal weights are the ones at which the model performs\nbest while executing the ViT with pre-trained weights in the first case.\nThe PyTorch library and it's various sub-packages are leveraged for deep learning functionalities.\nIn addition to this, sklearn, pandas, numpy packages are used for developing, training, and evaluating\ndeep learning models, as well as for performing data analysis and pre-processing. The data pre-\nprocessing involves resizing images to (224, 224) pixels and transforming them into tensors."}, {"title": "3.1 Performance of ViT with Transfer Learning", "content": "We relied on PyTorch, a core library for deep learning, which provides the essential tools for ten-\nsor computations and neural network construction. Specifically, torch and torch.nn were used to\nbuild and train the neural network. To handle image data, we used Torchvision's models for access-\ning pre-trained architectures and transforms for preprocessing images. Additionally, TQDM was\nemployed to create progress bars, enhancing visibility into the training process. For data analysis and\nmanipulation, we leveraged scikit-learn for machine learning utilities and Pandas for managing\nstructured data.\nThe dataset preparation involved mapping class names to numerical labels using class_mapping,\nwhere 'healthy' images were assigned the label 0 and 'stressed' images were assigned the label 1. We\napplied image transformations to resize the images to 224x224 pixels and convert them into tensor\nformat, ensuring compatibility with the ViT model. The datasets were loaded using ImageFolder\nfrom Torchvision, which facilitated the organization of images into training and testing sets. Data\nloaders were then created to handle batching, shuffling, and efficient data retrieval during training\nand validation, with a batch size of 128.\nTo adapt the Vision Transformer (ViT) architecture (as depicted in Fig 2a) for our specific task\nof binary classification, we began by configuring the model using the models.vit_b_16 variant, setting\npretrained=False to start with a clean slate. This approach allowed us to build the model from scratch\nwithout any influence from pre-existing learned features. Subsequently, we loaded custom pre-trained\nweights into the ViT model to realize the approach as shown in Fig 2b. This step was crucial as\nit transferred learned representations from a previously trained model to our current architecture,\nleveraging prior knowledge to enhance performance. A custom class was defined to extend the ViT\nmodel by adding a fully connected layer for binary classification. This class also included methods to\ncapture attention weights, which are essential for understanding the model's focus on different image\nregions.\nThe training process involved setting up a binary cross-entropy loss function to evaluate the\nperformance of the classification model. We used the Adam optimizer with a learning rate of 0.001\nand 0.0009 to update the model's weights. The training loop consisted of multiple epochs, during\nwhich the model was trained on the training dataset and validated on the test dataset. Training and\nvalidation losses were computed to assess the model's performance and adjust parameters accordingly.\nThroughout the process, attention weights were captured to provide insights into the model's decision-\nmaking process.\nThe learning curve typically plots both the training loss and validation loss curves on the same\ngraph. This allows to analyze how the model is performing on both the data it's trained on (training"}, {"title": "3.1.1 Analyzing Attention Maps", "content": "Visualizing attention weights provides insights into how the ViT focus on different parts of an input\nduring processing. By examining these weights, researchers and practitioners can understand the\nmodel's decision-making process, diagnose potential biases, and improve interpretability.\nThe following pseudocode outlines a systematic approach to calculate and visualize attention\nweights from a Vision Transformer model. This process involves capturing attention weights during\nthe forward pass, computing attention scores, and generating visual representations of these scores.\nInitialization: The process begins by initializing an empty list to store the attention weights\nthat will be captured during the forward pass of the Vision Transformer model. This list will later\nbe used to compute and visualize the attention maps.\nForward Pass and Capture Attention Weights: The next step involves iterating through\neach layer of the Vision Transformer model. For each layer, a hook is registered to capture the\nattention weights. The input image is then passed through the Vision Transformer to compute the\noutput features. This stage ensures that attention weights are collected during the forward pass for\nlater analysis.\nCalculate Attention Score: After capturing the attention weights, the algorithm processes each\nweight to compute the attention scores. This involves extracting the query, key, and value tensors\nfrom the hook outputs. The attention score is computed as per the principles discussed in section\n2.3.1. The attention scores are then normalized to ensure they are in a range suitable for visualization.\nVisualize Attention Maps: In the visualization phase, each normalized attention map is resized\nto match the dimensions of the input image. A colormap (e.g., 'hot') is applied to the attention map\nto highlight areas of high attention. The attention map is then overlaid on the original image to\ncreate a visual representation of where the model is focusing.\nDisplay: Finally, both the original image and the overlaid attention maps are displayed, allowing\nfor an interpretation of how the Vision Transformer model is making its decisions based on different\nregions of the input image."}, {"title": "3.2 Performance of ViT+SVM", "content": "We used a pre-trained Vision Transformer (ViT) model to extract features from both the training\nand testing datasets. These extracted features were then utilized by an SVM to identify stress,\naiming to distinguish between stressed and healthy images. The implementation was done using the\nPyTorch framework, leveraging both its core library and the torchvision module. Key libraries such"}, {"title": "4 Conclusion", "content": "Drought stress represents a severe threat to crop yield and quality, disrupting normal plant growth\nand survival rates. Detecting early signs of drought stress is crucial for effective crop management and\nintervention. Traditional methods, primarily reliant on Convolutional Neural Networks (CNNs), have\nmade significant strides in capturing spatial hierarchies in image data. However, Vision Transformers\n(ViTs) offer a compelling alternative by leveraging self-attention mechanisms to capture long-range\ndependencies and complex spatial relationships, thus enhancing the detection of subtle drought stress\nindicators.\nOur study successfully addressed the challenge of stress identification using smaller datasets by\nharnessing the feature extraction capabilities of Vision Transformers combined with the classification\nprowess of SVM. Unlike conventional CNN architectures, Vision Transformers utilize self-attention\nmechanisms to effectively capture relationships across different parts of the image, enabling them\nto model long-range dependencies, which is particularly advantageous for complex datasets where\ntraditional CNNs may struggle to capture global context. Moreover, Vision Transformers can han-\ndle images of varying resolutions without necessitating architectural modifications, enhancing their\nflexibility in handling diverse datasets. Our framework also emphasized explainability by generat-\ning attention maps, which provide insights into the model's focus areas within the images, thereby\noffering transparency in its decision-making process.\nHowever, our comparative analysis revealed that integrating SVM with ViT significantly enhances\nclassification performance. The ViT+SVM model achieved a higher average accuracy of 95%, with\nF1-scores of 93.6% for stressed images and 95.6% for healthy images, outperforming the ViT with\ntransfer learning approach. This integration not only improved accuracy but also demonstrated\ngreater consistency across different data splits, highlighting the robustness of the ViT+SVM model.\nIn summary, our study illustrates the superior performance of combining Vision Transformers\nwith Support Vector Machines over traditional transfer learning methods. This approach provides\na more reliable and accurate solution for drought stress identification, while the attention maps\noffer valuable insights into the model's decision-making process. The findings affirm the potential\nof advanced deep learning and machine learning techniques in enhancing agricultural practices and\ndecision-making, paving the way for improved crop management strategies."}]}