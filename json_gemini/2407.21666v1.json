{"title": "An Explainable Vision Transformer with Transfer Learning Combined with Support Vector Machine Based Efficient Drought Stress Identification", "authors": ["Aswini Kumar Patra", "Ankit Varshney", "Lingaraj Sahoo"], "abstract": "Early detection of drought stress is critical for taking timely measures for reducing crop loss before the drought impact becomes irreversible. The subtle phenotypical and physiological changes in response to drought stress are captured by non-invasive imaging techniques and these imaging data serve as valuable resource for machine learning methods to identify drought stress. While convolutional neural networks (CNNs) are in wide use, vision transformers (ViTs) present a promising alternative in capturing long-range dependencies and intricate spatial relationships, thereby enhancing the detection of subtle indicators of drought stress. We propose an explainable deep learning pipeline that leverages the power of ViTs for drought stress detection in potato crops using aerial imagery. We applied two distinct approaches: a synergistic combination of ViT and support vector machine (SVM), where ViT extracts intricate spatial features from aerial images, and SVM classifies the crops as stressed or healthy and an end-to-end approach using a dedicated classification layer within ViT to directly detect drought stress. Our key findings explain the ViT model's decision-making process by visualizing attention maps. These maps highlight the specific spatial features within the aerial images that the ViT model focuses as the drought stress signature. Our findings demonstrate that the proposed methods not only achieve high accuracy in drought stress identification but also shedding light on the diverse subtle plant features associated with drought stress. This offers a robust and interpretable solution for drought stress monitoring for farmers to undertake informed decisions for improved crop management.", "sections": [{"title": "1 Introduction", "content": "Drought stress adversely affects the growth, development and yield of plants. The rise in global tem- perature coupled with the growing demand for water severely depletes soil water reserves. Water deficit and temperature stress can cause damage to the reproductive growth phase of crops, resulting in a substantial reduction in yield [1]. Furthermore, prolonged exposure to water deficit stress partic- ularly in the reproductive stage can significantly affect grain quality. Ensuring sustainable crop yields in the face of escalating drought conditions has become a great challenge [2]. The plant's response to drought stress is influenced by a number of factors such as stress duration, severity, genotype and its developmental stage [3]. In plants, drought stress in early phase induces reduced stomatal conduc- tance, transpiration and net carbon assimilation, whereas mid- and late-phases result in visible leaf senescence, stagnant growth and wilting. The early detection of drought induced phenotypic changes in plants, facilitates in preventing the adverse impacts of drought from irreversible damage through timely irrigation and other management practices. The point based, contact type and destructive methods based estimation of drought stress impact on crops are limited by good approximation of representative area as well as sample size and hence, difficult to implement over larger agricultural production systems. On the contrary, image-based, non-invasive and high-throughput analyses of subtle phenotypical changes in plants due to drought and other stresses, provides an efficient means of monitoring stress levels in crops and good prediction of drought stress impact [4]. Various imaging techniques and sensors are utilized to precisely capture stress responses, including simple red-blue- green (RGB), thermal, multispectral, hyperspectral, or fluorescence imaging methods [5]. However, traditional image processing generates significant amount of variation in light intensity, shading, occlusion, etc. leading to variation in quality thereby increases complexity of image processing [6]. Machine learning (ML) techniques, on the other hand, solve such complex problems which are non- linear in nature by identifying key feature to be extracted from the acquired image in real time, based on knowledge and/or domain expertise of the user. Hence, ML aids in better decision-making and informed actions in real-world scenarios with minimal human intervention [7]. Deep learning (DL) [8] techniques however, help in extracting different information from the raw image data during input training, by means of various convolutions which allows larger learning capabilities, that result in the best classification/regression and higher performance and precision. Convolutional Neural Networks (CNNs) have been developed for providing a much-detailed representation of the image features, for a more discriminative and detailed analysis of plant images, leading to highly accurate classifications [9]. Many studies have used deep CNN models to detect drought stress in affected plants, mostly solved either by finely tuned CNNs or by training from scratch.\nEarly work by Zhuang et al. [10] utilized a combination of segmentation, color, and texture feature extraction, employing a gradient boosting decision tree (GBDT) method to detect water stress in maize. However, a subsequent study using the same RGB dataset demonstrated that a deep convolutional neural network (DCNN) significantly outperformed GBDT in terms of perfor- mance [11]. Sankararao et al. [12] developed a comprehensive pipeline for detecting water stress in groundnut canopies using hyperspectral imaging. The pipeline included steps for image quality"}, {"title": "2 Materials and Methods", "content": "In this section, we begin by introducing the experimental dataset. Next, we present the drought stress classification model using two different approaches. In the first approach, we employ the vision transformer with transfer learning. In the second approach, we propose a framework that utilizes a vision transformer as a feature extractor, followed by the integration of an SVM as the classifier. Additionally, we investigate the interpretability of the model by generating and analyzing the atten- tion maps. This comprehensive use of the ViT elucidates how the spatial features of drought stress can be precisely identified. Finally, we discuss the performance metrics for the proposed approaches."}, {"title": "2.1 Preparing the Data", "content": "The potato crop aerial images utilized in this study have been sourced from a publicly accessible dataset that encompasses multiple modalities [28]. Collected from a field at the Aberdeen Research and Extension Center, University of Idaho, these images serve as valuable resources for training machine learning models dedicated to crop health assessment in precision agriculture applications. Acquired using a Parrot Sequoia multi-spectral camera mounted on a 3DR Solo drone, the dataset features an RGB sensor with a resolution of 4,608 \u00d7 3,456 pixels and four monochrome sensors capturing narrow bands of light wavelengths: green (550nm), red (660nm), red-edge (735nm), and near-infrared (790nm), each with a resolution of 1,280 \u00d7 960 pixels. The drone flew over the potato field at a low altitude of 3 meters, with the primary objective of capturing drought stress in Russet Burbank potato plants attributed to premature plant senescence.\nThe dataset consists of 360 RGB image patches in JPG format, each measuring 750\u00d7750 pix- els. These patches were derived from high-resolution aerial images through cropping, rotating, and resizing operations. The dataset is divided into a training subset of 300 images and a testing subset of 60 images. Ground-truth annotations, provided in both XML and CSV formats, indicate regions of healthy and stressed plants marked by rectangular bounding boxes. These annotations were man- ually created using the LabelImg software. An example RGB image and its annotated regions are shown in Fig. 1. The testing subset is independent of the training subset and comes from different aerial images. Additionally, the dataset includes corresponding image patches from spectral sensors with red, green, red-edge, and near-infrared bands, each sized 416\u00d7416 pixels. However, we are only utilizing the RGB images due to the limitations of the low-resolution monochromatic images.\nWe utilized the augmented dataset prepared by Butte et al., expanding the original set from 300 to 1,500 images for training. The test set comprised 60 images. From each augmented and test image, annotated windows (i.e., rectangular bounding boxes) were extracted, creating separate folders for \"healthy\" and \"stressed\" images. These folders served as input for our proposed model. The final count of training images for the \"stressed\" and \"healthy\" classes is 11,915 and 8,200, respectively. The test set, consisting of 60 images, yielded 401 healthy and 734 stressed images. We evaluated our method on this test set and further assessed the model performance through 5-fold cross-validation."}, {"title": "2.2 Vision Transformer (ViT)", "content": "A vision transformer (ViT) is a type of neural network architecture that has revolutionized the field of computer vision [29]. Unlike traditional convolutional neural networks (CNNs), which process images pixel by pixel, ViT processes the input images in a sequential manner by dividing them into fixed-size patches, linearly embedding these patches, and then applying self-attention mechanisms for capturing global dependencies [22]. The VIT architecture used for our work is inspired by Dosovitskiy et al. [22] and depicted in Fig 2a. It processes images by dividing them into fixed-size embedded patches, linearly transforming these patches, and treating them as sequences. The transformer architecture is then applied, incorporating multi-head self-attention mechanisms [30] that enable the model to capture long-range dependencies within the image. Layer normalization is applied before and after the multi-head attention, ensuring stable training, and a Multilayer Perceptron (MLP) head is added to the transformer's global representation for task-specific processing."}, {"title": "2.2.1 ViT Architecture", "content": "The proposed ViT architecture is based on the layers in [31] and Dosovitskiy et al. [22] and comprises five main components: patch embedding, positional encoding, transformer encoder, normalization layer and a classification head. This is illustrated in Fig 2a.\n\u2022 Patch Embedding: Input images are divided into fixed-size patches, which are then linearly embedded to create a sequence of embeddings.\n\u2022 Positional Encoding: To capture spatial information, positional encodings are added to the patch embeddings, allowing the model to understand the relative positions of different patches.\n\u2022 Transformer Encoder: The embedded patches are fed into a Transformer Encoder, which is the core component of the ViT architecture. This encoder consists of twelve identical encoder layers stacked together. Each attention layer analyzes the relationships between pairs of patches, allowing the model to understand how different image regions interact and influence each other. Each attention layer consists of the following:\nMulti-Head Self-Attention: This mechanism allows the model to weigh the importance of different parts of the image. It captures global dependencies between the patches.\nMLP (Multi-Layer Perceptron) Block: This block introduces non-linearity to the network and further processes the information from the attention layer.\nNormalization Layers: Layer normalization is applied after the multi-head attention and MLP blocks to stabilize training.\nDropout: The model uses dropout at two levels: within the attention mechanism and after the MLP block. Dropout is used to prevent over-fitting by randomly dropping out neurons during training.\n\u2022 Normalization Layer: Layer normalization applied to the output of the encoder serves several crucial purposes: Reduces internal co-variate shift, improves gradient flow, acts as regularization, handles varying input distributions and accelerates convergence.\n\u2022 Classification Head: This head typically consists of a simple MLP layer that maps the feature representation to the desired output, such as class probabilities."}, {"title": "2.2.2 Information Processing in ViT", "content": "At the core of the Vision Transformer (ViT) is the concept of a token, which plays a crucial role in the model's ability to process and understand images. A token is a fixed-size vector that represents a small patch of the input image. These tokens are at the core of the Vision Transformer model, forming the input sequence to the Transformer layers. This token-based approach enables the model to process and understand the image by focusing on the relationships between these patches through self-attention. The detailed explanation is given below.\n\u2022 Dividing the Image into Patches:\n The input image is divided into smaller, fixed-size patches. For example, an image of size 224x224 pixels might be divided into patches of size 16x16 pixels, resulting in $(\\frac{224}{16})^2 = 196$ patches.\n\u2022 Flattening and Embedding:\nEach image patch is then flattened into a one-dimensional vector. For instance, a 16x16 patch with 3 color channels (RGB) would be flattened into a vector of length 16 \u00d7 16 \u00d7 3 = 768.\nThese flattened vectors (patch representations) are then linearly embedded into a higher- dimensional space. This is typically done using a learnable linear projection, transforming each vector into a fixed-size embedding, say of dimension 768.\n\u2022 Tokens:\nAfter linear embedding, each flattened and embedded patch becomes a \"token.\" In this example, the image is transformed into a sequence of 196 tokens, each representing a 16x16 patch of the original image.\n\u2022 Adding Positional Encoding:\nSince the transformer model does not inherently understand the order or position of tokens, posi- tional encodings are added to each token to incorporate information about its original position in the image. This helps the model understand spatial relationships between patches.\n\u2022 Processing by Transformer Layers:\nThese tokens are then processed by the Transformer layers, which include self-attention mecha- nisms. The self-attention mechanism computes relationships between these tokens to understand how different parts of the image relate to one another."}, {"title": "2.3 ViT with Transfer Learning", "content": "The proposed framework, as shown in Fig 2b effectively combines transfer learning, the power of the Vision Transformer and attention-based interpretability to address the challenging task of drought stress identification in potato crop images captured in natural settings. A core component of this approach is the utilization of pre-trained weights. This technique, known as transfer learning, involves leveraging knowledge gained from solving one problem (often a large-scale image classification task) and applying it to a different but related problem. By employing pre-trained weights, the model can benefit from the rich feature representations learned from a massive dataset, accelerating training and potentially improving performance. Using this the ViT is trained over multiple epochs using a combination of binary cross-entropy loss, Adam optimizer and learning rate tuning. Experimenting with different learning rates is essential to find the optimal value for convergence and generalization. Eventually, the classification layer is responsible for making the final prediction. It takes the output of the ViT model and maps it to two classes: \"healthy\" and \"stressed.\" This layer typically consists of a fully connected neural network with a sigmoid activation function for binary classification."}, {"title": "2.3.1 Attention Maps", "content": "Attention maps provide insights into how the model focuses on different parts of the image during the self-attention mechanism. This mechanism allows the model to selectively attend to specific areas while processing visual information. The image is first divided into patches. Self-attention then helps the model prioritize relevant patches and their relationships for effective feature extraction.\nAttention maps act as a visual representation of the weights assigned by the self-attention mecha- nism to each patch. These maps can be visualized for each self-attention layer within the ViT model, as each layer progressively learns more intricate relationships between these image features. Higher weights indicate a greater focus on a specific patch and its connection to others. By analyzing these maps, we can essentially see the model's \"thought process\" during image understanding. We can identify which regions it prioritizes for information extraction. The following section highlights the computation behind the attention mechanism [31].\nAt its core, self-attention computes a weighted sum of the values (features) based on the simi- larities (attention scores) between different positions in the sequence. This is achieved through three learnable matrices: Query (Q), Key (K), and Value (V).\nQuery Matrix (Q): The query matrix is responsible for capturing the information about the current token being processed. It learns to encode the features of the token in a format suitable for comparison with other tokens. Each token in the sequence is associated with a query vector, which represents its characteristics in the context of the entire sequence.\nKey Matrix (K): The key matrix holds information about the relationship between the current token and other tokens in the sequence. It learns to encode the features that determine how relevant each token is to the current token.\nValue Matrix (V):\nThe key matrix holds information about the relationship between the current token and other tokens in the sequence. It learns to encode the features that determine how relevant each token is to the current token.\nGiven a sequence of tokens $X = [X_1, X_2, ..., x_n]$, the attention scores A are computed as:\n$A = softmax (\\frac{Q K^T}{\\sqrt{d_k}})$"}, {"title": "2.4 Integrating Vision Transformer (ViT) and Support Vector Machine (SVM)", "content": "This framework focuses on combining a vision transformer (ViT) and a support vector machine (SVM) within a three-step approach for effective stress identification.\n\u2022 Feature Extraction: For each image in the dataset, extract the final hidden state or pooled rep- resentation from the ViT model to obtain a feature vector. Let X represent the set of embedded features extracted from a dataset of images, and y represent the corresponding class labels.\n\u2022 Training SVM: Train an SVM classifier using the extracted features X and their corresponding class labels y.\n\u2022 Classification: For a new, unseen image, extract features using the pre-trained ViT model and use the trained SVM classifier to predict its class."}, {"title": "2.4.1 Support Vector Machine (SVM)", "content": "Support Vector Machines [32] aim to find a hyperplane that best separates a given set of data points into different classes. Given a set of labeled data points $(x_1, y_1), (X_2, Y_2), ..., (x_n, Y_n)$ where x is the feature vector of the i-th data point, and $y_i$ is its corresponding class label ($y_i \\in {1,0}$ for binary classification), SVM seeks to find a hyperplane defined by the equation:\n$w.x+b=0$\nwhere w is the weight vector and b is the bias.\nThe goal is to maximize the margin, which is the distance between the hyperplane and the nearest data point from each class. The margin is computed as the perpendicular distance from a data point $x_i$ to the hyperplane:\n$margin = \\frac{1}{||w||} |w.x_i + b|$\nSubject to the constraint that for all data points:\n$y_i(wx_i + b) \\geq 1$\nThis constraint ensures that data points are correctly classified and lie on the correct side of the hyperplane.\nThe SVM optimization problem can be formulated as:\n$Minimize \\frac{1}{2} ||w||^2$"}, {"title": "2.4.2 ViT+SVM Framework", "content": "The process begins with input images that are fed into the ViT. The ViT, pre-trained on a vast dataset, is adept at extracting meaningful features from the images. Once the features are extracted by the ViT, they are compiled into a feature matrix, which also includes the corresponding labels indicating whether the plants in the images are healthy or stressed. This matrix forms the input to the SVM, a robust classifier known for its effectiveness in handling high-dimensional data. The SVM is trained to discern between the two classes-healthy and stressed-based on the features provided. Finally, the trained SVM predicts the class of new images, categorizing them as either healthy or stressed. The entire approach is depicted in Fig 2c. The efficacy of this framework is evaluated using a designated test set and k-fold cross-validation."}, {"title": "2.5 Performance Evaluation Metrics", "content": "The model's performance underwent assessment using various evaluation metrics, including accuracy, precision, recall (sensitivity) and the Receiver Operating Characteristic (ROC) curve. These metrics are computed based on the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), which collectively form a 2x2 matrix known as the confusion matrix. In this matrix, TP and TN indicate the accurate predictions of water-stressed and healthy potato crops, respectively. FP, termed as type 1 error, denotes predictions where the healthy class is inaccurately identified as water-stressed. FN, referred to as type 2 error, represents instances where water-stressed potato plants are incorrectly predicted as healthy. The classification accuracy is a measure of the ratio between correct predictions for both stressed and healthy images and the total number of images in the test set.\n$Accuracy = \\frac{True\\ Positive + True\\ Negative}{Total\\ Population}$\n$Precision = \\frac{True\\ Positive}{True\\ Positive + False\\ Positive}$\n$Recall = \\frac{True\\ Positive}{True\\ Positive + False\\ Negative}$\n$F1-score = 2. \\frac{Precision\\ Recall}{Precision + Recall}$\nCross-validation is crucial in model development for two key reasons: it helps prevent over-fitting by assessing a model's performance across different subsets of the data, and it ensures the model's generalization ability, providing a reliable estimation of its effectiveness under various conditions. The model was trained and evaluated using k-fold cross-validation, a robust technique for assessing the generalization performance of the model. In each iteration of the k-fold cross-validation process, the dataset was partitioned into k folds, and the model was trained on k-1 folds while being validated on the remaining fold. This process was repeated k times, ensuring that every fold had the opportunity to serve as the validation set. For each fold, the Receiver Operating Characteristic (ROC) curve was plotted, illustrating the trade-off between true positive rate and false positive rate at various thresholds. After completing the k-fold cross-validation, the individual ROC curves were aggregated, and the mean ROC curve was calculated and plotted. AUC (area under the curve) measures the entire two-dimensional area underneath the entire ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.This comprehensive approach provides a more reliable estimation of the model's performance, capturing its consistency across different subsets of the data and enhancing the overall assessment of its predictive capabilities."}, {"title": "3 Results and Discussion", "content": "In this section, we present the experimental results of our proposed model for identifying drought stress in potato crop field images. First, we distinguish between healthy and stressed images. Then, we identify the spatial features responsible for the stress. Our experiments with the proposed Vision Transformer (ViT) framework were conducted in three ways:\n\u2022 ViT with Transfer Learning: Leveraging pre-trained weights.\n\u2022 ViT+SVM with Pre-trained Weights.\n\u2022 ViT+SVM with Optimal Weights. The optimal weights are the ones at which the model performs best while executing the ViT with pre-trained weights in the first case.\nThe PyTorch library and it's various sub-packages are leveraged for deep learning functionalities. In addition to this, sklearn, pandas, numpy packages are used for developing, training, and evaluating deep learning models, as well as for performing data analysis and pre-processing. The data pre- processing involves resizing images to (224, 224) pixels and transforming them into tensors."}, {"title": "3.1 Performance of ViT with Transfer Learning", "content": "We relied on PyTorch, a core library for deep learning, which provides the essential tools for ten- sor computations and neural network construction. Specifically, torch and torch.nn were used to build and train the neural network. To handle image data, we used Torchvision's models for access- ing pre-trained architectures and transforms for preprocessing images. Additionally, TQDM was employed to create progress bars, enhancing visibility into the training process. For data analysis and manipulation, we leveraged scikit-learn for machine learning utilities and Pandas for managing structured data.\nThe dataset preparation involved mapping class names to numerical labels using class_mapping, where 'healthy' images were assigned the label 0 and 'stressed' images were assigned the label 1. We applied image transformations to resize the images to 224x224 pixels and convert them into tensor format, ensuring compatibility with the ViT model. The datasets were loaded using ImageFolder from Torchvision, which facilitated the organization of images into training and testing sets. Data loaders were then created to handle batching, shuffling, and efficient data retrieval during training and validation, with a batch size of 128.\nTo adapt the Vision Transformer (ViT) architecture (as depicted in Fig 2a) for our specific task of binary classification, we began by configuring the model using the models.vit_b_16 variant, setting pretrained=False to start with a clean slate. This approach allowed us to build the model from scratch without any influence from pre-existing learned features. Subsequently, we loaded custom pre-trained weights into the ViT model to realize the approach as shown in Fig 2b. This step was crucial as it transferred learned representations from a previously trained model to our current architecture, leveraging prior knowledge to enhance performance. A custom class was defined to extend the ViT model by adding a fully connected layer for binary classification. This class also included methods to capture attention weights, which are essential for understanding the model's focus on different image regions.\nThe training process involved setting up a binary cross-entropy loss function to evaluate the performance of the classification model. We used the Adam optimizer with a learning rate of 0.001 and 0.0009 to update the model's weights. The training loop consisted of multiple epochs, during which the model was trained on the training dataset and validated on the test dataset. Training and validation losses were computed to assess the model's performance and adjust parameters accordingly. Throughout the process, attention weights were captured to provide insights into the model's decision- making process.\nThe learning curve typically plots both the training loss and validation loss curves on the same graph. This allows to analyze how the model is performing on both the data it's trained on (training"}, {"title": "3.1.1 Analyzing Attention Maps", "content": "Visualizing attention weights provides insights into how the ViT focus on different parts of an input during processing. By examining these weights, researchers and practitioners can understand the model's decision-making process, diagnose potential biases, and improve interpretability.\nThe following pseudocode outlines a systematic approach to calculate and visualize attention weights from a Vision Transformer model. This process involves capturing attention weights during the forward pass, computing attention scores, and generating visual representations of these scores.\nInitialization: The process begins by initializing an empty list to store the attention weights that will be captured during the forward pass of the Vision Transformer model. This list will later be used to compute and visualize the attention maps.\nForward Pass and Capture Attention Weights: The next step involves iterating through each layer of the Vision Transformer model. For each layer, a hook is registered to capture the attention weights. The input image is then passed through the Vision Transformer to compute the output features. This stage ensures that attention weights are collected during the forward pass for later analysis.\nCalculate Attention Score: After capturing the attention weights, the algorithm processes each weight to compute the attention scores. This involves extracting the query, key, and value tensors from the hook outputs. The attention score is computed as per the principles discussed in section 2.3.1. The attention scores are then normalized to ensure they are in a range suitable for visualization.\nVisualize Attention Maps: In the visualization phase, each normalized attention map is resized to match the dimensions of the input image. A colormap (e.g., 'hot') is applied to the attention map to highlight areas of high attention. The attention map is then overlaid on the original image to create a visual representation of where the model is focusing.\nDisplay: Finally, both the original image and the overlaid attention maps are displayed, allowing for an interpretation of how the Vision Transformer model is making its decisions based on different regions of the input image."}, {"title": "3.2 Performance of ViT+SVM", "content": "We used a pre-trained Vision Transformer (ViT) model to extract features from both the training and testing datasets. These extracted features were then utilized by an SVM to identify stress, aiming to distinguish between stressed and healthy images. The implementation was done using the PyTorch framework, leveraging both its core library and the torchvision module. Key libraries such"}, {"title": "4 Conclusion", "content": "Drought stress represents a severe threat to crop yield and quality, disrupting normal plant growth and survival rates. Detecting early signs of drought stress is crucial for effective crop management and intervention. Traditional methods, primarily reliant on Convolutional Neural Networks (CNNs), have made significant strides in capturing spatial hierarchies in image data. However, Vision Transformers (ViTs) offer a compelling alternative by leveraging self-attention mechanisms to capture long-range dependencies and complex spatial relationships, thus enhancing the detection of subtle drought stress indicators.\nOur study successfully addressed the challenge of stress identification using smaller datasets by harnessing the feature extraction capabilities of Vision Transformers combined with the classification prowess of SVM. Unlike conventional CNN architectures, Vision Transformers utilize self-attention mechanisms to effectively capture relationships across different parts of the image, enabling them to model long-range dependencies, which is particularly advantageous for complex datasets where traditional CNNs may struggle to capture global context. Moreover, Vision Transformers can han- dle images of varying resolutions without necessitating architectural modifications, enhancing their flexibility in handling diverse datasets. Our framework also emphasized explainability by generat- ing attention maps, which provide insights into the model's focus areas within the images, thereby offering transparency in its decision-making process.\nHowever, our comparative analysis revealed that integrating SVM with ViT significantly enhances classification performance. The ViT+SVM model achieved a higher average accuracy of 95%, with F1-scores of 93.6% for stressed images and 95.6% for healthy images, outperforming the ViT with transfer learning approach. This integration not only improved accuracy but also demonstrated greater consistency across different data splits, highlighting the robustness of the ViT+SVM model.\nIn summary, our study illustrates the superior performance of combining Vision Transformers with Support Vector Machines over traditional transfer learning methods. This approach provides a more reliable and accurate solution for drought stress identification, while the attention maps offer valuable insights into the model's decision-making process. The findings affirm the potential of advanced deep learning and machine learning techniques in enhancing agricultural practices and decision-making, paving the way for improved crop management strategies."}]}