{"title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge", "authors": ["Nico Wagner", "Michael Desmond", "Rahul Nair", "Zahra Ashktorab", "Elizabeth M. Daly", "Qian Pan", "Mart\u00edn Santill\u00e1n Cooper", "James M. Johnson", "Werner Geyer"], "abstract": "LLM-as-a-Judge is a widely used method for evaluating the performance of Large Language Models (LLMs) across various tasks. We address the challenge of quantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty quantification has been well-studied in other domains, applying it effectively to LLMs poses unique challenges due to their complex decision-making capabilities and computational demands. In this paper, we introduce a novel method for quantifying uncertainty designed to enhance the trustworthiness of LLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the relationships between generated assessments and possible ratings. By cross-evaluating these relationships and constructing a confusion matrix based on token probabilities, the method derives labels of high or low uncertainty. We evaluate our method across multiple benchmarks, demonstrating a strong correlation between the accuracy of LLM evaluations and the derived uncertainty scores. Our findings suggest that this method can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become integral to a wide range of tasks, including question- answering [24], summarization [12], translation [30], concept extraction [5], classification [8], and reasoning [10]. The evaluation of the texts they generate has emerged as a significant challenge due to data contamination [1], replicability, and the use of standard metrics or benchmarks [17, 7, 6] which may not cover all dimensions of use case-specific evaluations.\nAn emerging method for evaluating generated content involves using other LLMs as evaluators, a method referred to as LLM-as-a-Judge [31]. These evaluations can take various forms, including explanations, numeric values, or categorical ratings. This work specifically focuses on LLM-as-a- Judge methods that employ categorical ratings or numerical evaluations to assess generated outputs.\nDespite their widespread use, LLM-as-a-Judge methods do not always align with human judgments, leading to instances where the evaluations may be incorrect or misleading [27, 2]. This divergence highlights the need to assess the trustworthiness of LLM-generated evaluations. Various techniques have been proposed to enhance the performance of LLMs and improve the reliability of their judgments [26].\nTo improve trustworthiness in LLM-as-a-Judge evaluations and to leverage the strengths of these methods, we introduce a novel approach called confusion-based uncertainty. Our method is designed"}, {"title": "Related Work", "content": "The capabilities of large language models have rapidly advanced, leading to their application in increasingly complex tasks [25]. However, the growing sophistication of these models also raises new challenges, particularly in evaluating their outputs and understanding their uncertainty [9]. As models are increasingly used as evaluators, what is often referred to as LLM-as-a-Judge, it becomes essential to develop methods that allow these models not only to generate responses but also to assess the confidence and reliability of those responses.\nA promising line of research addresses these challenges through methods like chain-of-thought reasoning [28] and self-reflection [11]. Chain-of-thought reasoning enhances an LLM's ability to arrive at more accurate conclusions by breaking down complex tasks into intermediate logical steps. By guiding the model through a series of smaller, connected reasoning steps, the model's decision- making process becomes more transparent and robust. Self-reflection, on the other hand, encourages the model to review and critique its own responses, iterating upon initial outputs to refine and improve its accuracy. Both techniques align with the broader framework of agentic design patterns [21], which foster active engagement by the model in evaluating and refining its generated outputs.\nThese reasoning-based approaches are particularly relevant to the growing body of work on LLM-as- a-Judge, where LLMs are used to evaluate data and provide judgments that are comparable to human annotations. Several works have studied LLM-as-a-Judge with evaluations generally focused on correlations between labels generated by LLMs and human annotations. For example, [31, 13] and report strong agreements with human annotations, while other studies have reported mixed results [4, 2]. Some works have proposed using ensembles of smaller models to increase performance [26], while others have used instruction fine-tuning to build customised evaluators [13].\nSeveral works have explored methods for uncertainty estimation in the context of large language models. One approach is calibration-based uncertainty quantification, introduced by [23], which"}, {"title": "Confusion-based Uncertainty", "content": "In many LLM-as-a-Judge frameworks, the evaluation of generated text is conducted against predefined criteria [14]. Each criterion consists of a question and a set of options, among which the LLM must choose. The questions can vary widely, and the options can be defined as numeric values, words, or any other format, with no restriction on the number of words or the nature of the options.\nOur proposed technique introduces an uncertainty measure that is calculated independently of the specific decision made by the LLM. This approach aims to enhance the trustworthiness of LLM-as-a- Judge evaluations. The method works in four key steps: generating verbalized assessments, creating prompts for the confusion matrix, constructing the confusion matrix, and setting uncertainty labels.\nGenerating Assessments The initial step in our approach involves generating verbalized assess- ments for each of the n options presented in the criterion. Using the prompt template shown in Figure 3, we apply prompt engineering techniques to guide the LLM toward treating a specific option as correct. This compels the model to generate justifications for why that particular option is the best choice. Each assessment is explicitly linked to one of the available options, ensuring that the reasoning is directly associated with the selected alternative.\nThe prompts are designed to persuade the LLM that the option in question is correct, leading to a set of n assessments, one for each option."}, {"title": "Creating Confusion Prompts", "content": "After generating assessments, the next step involves creating prompts that will be used to build the confusion matrix. This is done by mixing each assessment with every option, effectively producing a comprehensive set of prompts that cover all possible pairings of assessments and options. The prompt template (see Figure 5) is structured as a conversation between the LLM and the user, where two tasks are presented as separate requests. First, the LLM is asked to generate an assessment for which option is correct without injecting any prompts. Second, the LLM is prompted to choose the correct option based on the assessment. The assessments and options generated in the previous step are inserted as responses from the LLM.\nFor a criterion with n options, this process results in the creation of $n^2$ prompts, as each assessment is mixed with every possible option."}, {"title": "Constructing the Confusion Matrix", "content": "With the $n^2$ prompts created, the next step is to send these prompts to the LLM to obtain the token probabilities associated with the final decision. The probability of the last token in the response is used to calculate an uncertainty score for the chosen option.\nThese probabilities are organized into a confusion matrix, where each row corresponds to an option from the prompt, and each column corresponds to an assessment generated for a specific option. A confusion matrix labeled as low uncertainty exhibits high token probabilities concentrated in a single row. In contrast, a matrix labeled as high uncertainty either shows high token probabilities along the diagonal, where the assessments align with the corresponding options, or has high token probabilities scattered arbitrarily across the matrix."}, {"title": "Setting Uncertainty Labels", "content": "The final step in the method involves assigning an uncertainty label, either high or low uncertainty, to the chosen option based on the confusion matrix and predefined threshold. The labeling process follows these rules:\n\u2022 If only one row in the matrix exceeds the uncertainty threshold and this row corresponds to the LLM's initially chosen option, the option is labeled as low uncertainty.\n\u2022 If more than one row exceeds the uncertainty threshold, the option is labeled as high uncertainty.\n\u2022 If the option identified with low uncertainty in the confusion matrix does not match the LLM's originally chosen option, the option is labeled as high uncertainty.\n\u2022 If no row exceeds the uncertainty threshold, the option is labeled as high uncertainty.\nThis labeling process allows the method to differentiate between evaluations that the LLM is likely confident in and those that may require further scrutiny. The overall goal is to enhance the reliability and trustworthiness of LLM-as-a-Judge evaluations by providing an additional layer of certainty assessment."}, {"title": "Formal Description", "content": "Formally, the method can be described as follows. Consider a question q with n possible outcomes $o_i$, where $i \\in \\{1, 2, ..., n\\}$. For example, a multiple choice question with four answers, $o_i$ can take on values A/B/C/D with n = 4. With each q as context, we consider two prompts, $q_a$ an assessment prompt and $q_e$ a confusion prompt. The assessment prompt (see Figure 3) generates assessments $a_i = q_a(O_i) \\forall i \\in \\{1, 2, . . ., n\\}$ for each possible discrete outcome."}, {"title": null, "content": "The confusion prompt (see Figure [5]), considers all combinations of outcomes and assessments for the question, i.e. $q_c(o_i, a_j)$ denotes a prompt using assessment $a_j$ and target label $o_i$. While the assessment prompt generates additional tokens, the confusion prompt is used only to determine the probabilities of the output token(s). The confusion matrix C consists of elements\n$p_{ij} = p (O_i|q_c(O_i,a_j)), \\qquad \\forall i,j\\in \\{1, 2, ..., n\\},\\qquad(1)$\nwhere $p_{ij}$ denotes the probability of token or when the assessment relates to the j-th outcome. This matrix forms the basis for the uncertainty quantification. The main intuition is that if the probability of token or is high regardless of the assessments, then the model has low uncertainty in its prediction. In contrast, if the token probability follows the assessment, we infer that the model has high uncertainty in its answer.\nThe uncertainty associated with a specific token can then be estimated by taking the mean token probability across all confusing prompts, i.e.\n$u_i = \\frac{1}{n} \\sum_j p_{ij}, \\qquad \\forall i \\in \\{1,2,..., n\\}.\\qquad(2)$\nFor analysis, in this paper we further label the uncertainty of the overall assessment using a threshold \u03b1, i.e.\n$l = \\begin{cases} low \\text{ uncertainty } & \\text{if } \\sum_i \\mathbb{1} (u_i \\ge a) = 1, \\forall i \\in \\{1, 2, ..., n\\}, \\\\ high \\text{ uncertainty } & \\text{otherwise.} \\end{cases}\\qquad(3)$\nIn other words, the assessment has low uncertainty if the mean token probability exceeds the threshold for exactly a single token. The procedure involves n inferencing calls for the first stage and $n^2$ inferences for the second stage as it works through all combinations of outcome labels making the overall estimation procedure $O(n^2)$."}, {"title": "Threshold", "content": "The threshold acts as a crucial parameter in determining the balance between the proportion of low uncertainty and accuracy. Defining an optimal threshold depends on the specific requirements of the use case. For applications such as content filtering or large-scale feedback collection, where there are a large number of evaluations but limited human resources to assess the output, prioritizing a higher volume of low-uncertainty responses may necessitate a more lenient threshold, even if it results in only a modest accuracy gain. Conversely, for tasks demanding highly reliable outcomes, such as the evaluation of medical diagnoses or legal decision-making, where accuracy is critical and errors carry significant consequences, a stricter threshold is essential to ensure the chosen options are highly reliable.\nAn interesting observation arises when the threshold is reduced below 0.5, accuracy tends to in- crease, suggesting that as the average token probability for incorrect options decreases, the model performance improves. This suggests that valuable information can be derived not only from options marked as having low uncertainty but also from those with higher uncertainty. The behavior of token probabilities across both low and high uncertainty options provides insights into the decision-making process of the LLM, suggesting that thresholds should be dynamically tuned based on the specific performance trade-offs desired for the task at hand.\nOur analysis reveals that threshold tuning significantly impacts the relationship between accuracy and uncertainty, forming a parabolic effect. In the threshold grid search for the Feedback Collection dataset (see Figure 6), we observe that as the threshold increases beyond 0.5, accuracy improves, but the proportion of low-uncertainty predictions decreases. Conversely, when the threshold is below 0.5, lowering the threshold leads to an increase in accuracy but a decrease in the proportion of low-uncertainty labels. This inverse relationship between accuracy and uncertainty highlights that while stricter thresholds (above 0.5) favor higher accuracy at the expense of fewer low-uncertainty predictions, lenient thresholds (below 0.5) enhance accuracy but reduce the certainty of the predictions. This parabolic behavior is consistent across datasets, emphasizing the threshold's pivotal role in determining model performance."}, {"title": "Experiments", "content": "Benchmark Datasets The proposed uncertainty method was evaluated on five benchmark datasets: TruthfulQA [18], Reliance Study, Summarization CNN/DM [20], Feedback Collection [14], and FeedbackQA [16]. TruthfulQA contains question-answer pairs and involves a binary classification task to determine whether the answer is truthful, with human annotations available for verification. The Reliance Study dataset originates from a study designed to measure reliance on Large Language Models (LLMs) across various tasks. It uses binary classification to evaluate the accuracy and naturalness of LLM outputs in settings such as conversations and customer-agent interactions, focusing on criteria like relevance and naturalness. The Summarization CNN/DM dataset, which consists of model-generated summaries of news articles, is evaluated on a scale from 1 to 5 based on criteria such as coherence, fluency, and relevance. Feedback Collection is designed to induce fine-grained evaluation capabilities in language models, using a rating scale from 1 to 5. Lastly, FeedbackQA is a retrieval-based QA dataset that includes interactive user feedback, where each question-answer pair is rated from excellent to bad, accompanied by natural language explanations detailing the strengths and weaknesses of the responses.\nModels In this study, we utilize instruct models exclusively, as LLM-as-a-Judge requires agent- like capabilities, where models must reliably follow explicit evaluation instructions. The instruct models chosen for this experiment include Mixtral-8x7B-Instruct-v01, Llama-3-8B-Instruct, and Llama-3-70B-Instruct. To investigate the impact of model architecture and size on performance, we selected models of varying sizes 8B and 70B parameters. This approach allows us to assess whether performance improvements in LLM-as-a-Judge tasks are primarily driven by the scale of the model or the underlying instruct-tuned structure.\nImplementation Details The experiments were conducted on stratified samples from each dataset, with the sample size determined by the number of evaluation criteria. For instance, if a dataset included four distinct criteria, the total sample size was multiplied by four to ensure that each criterion was equally represented. This stratification ensures a balanced evaluation across all criteria. To determine the optimal threshold for distinguishing high and low uncertainty in the LLM-as-a-Judge predictions, we employed a grid search, systematically exploring different threshold values to identify the best-performing configuration.\nBaseline The evaluation metric for this work is based on accuracy, specifically measuring whether the option selected by the LLM matches the option chosen by the human rater. The baseline for the method is to achieve higher accuracy for cases labeled as low uncertainty and lower accuracy for those labeled as high uncertainty. In datasets such as FeedbackQA and Summarization CNN/DM,"}, {"title": "Results", "content": "Uncertainty labeling correlates with accuracy Our uncertainty labeling method demonstrates a clear advantage in aligning low uncertainty labels with higher accuracy, as shown in Figure 7. Across all datasets and models, the markers for low uncertainty are consistently above the baseline, indicating that these labels correspond to more accurate evaluations compared to high uncertainty labels. This result confirms that the uncertainty labels generated by our method are effective in predicting the likelihood of accurate outputs in LLM-as-a-Judge scenarios.\nVariability in low uncertainty proportions A key finding is the variance in the ratio of high to low uncertainty labels depending on the dataset and model, as depicted in Figure 8. Notably, the Llama-3-70B-Instruct model consistently produces a higher proportion of low uncertainty labels, outperforming both Llama-3-8B-Instruct and Mixtral-8x7B-Instruct-v01. In datasets such as the"}, {"title": null, "content": "Reliance Study, Summarization CNN/DM, and FeedbackQA, the smaller models classify less than 5% of cases as low uncertainty, whereas Llama-3-70B-Instruct exceeds 15%. This suggests that both model size and structure significantly impact the model's ability to assign more reliable uncertainty labels.\nLow uncertainty consistently leads to higher accuracy Even in cases where the proportion of low uncertainty labels is small, they still correspond to high accuracy. For example, in the Feedback Collection dataset, the proportion of low uncertainty labels is below 10% for all models (see Figure 8), yet these labels consistently achieve 100% accuracy (see Table 1). This trend further supports the strong predictive power of low uncertainty labels in LLM evaluations.\nSmaller deviation in multi-classification ratings In multi-classification datasets, such as Sum- marization CNN/DM, which require the evaluation of generated summaries, a smaller deviation is observed between the correct ratings and the LLM's selected ratings when low uncertainty labels are present. This indicates that LLMs not only tend to choose the correct options but also exhibit greater precision in their ratings under conditions characterized by a low proportion of uncertainty.\nLow uncertainty labels approach human agreement In datasets with human-generated ratings such as Summarization CNN/DM and FeedbackQA, the accuracy of LLM predictions labeled as low uncertainty meets or even exceeds the level of agreement observed between human raters (see Table 1). This suggests that low uncertainty labels can be as reliable as human evaluations, further establishing the effectiveness of our method in aligning LLM outputs with human judgment."}, {"title": "Interpreting Uncertainty", "content": "The interpretation of uncertainty in LLMs remains a significant challenge [9]. Our method attempts to confuse the LLM by convincing it of statements without knowing whether they are true, and then considering the LLM's beliefs under these biased conditions. We define two key scenarios for interpreting low uncertainty. The first scenario is when the LLM selects an option consistently, regardless of conflicting assessments, indicating that even when alternative assessments are presented, they fail to sway the model's decision 9a. The second scenario occurs when the LLM cannot be convinced to generate an assessment for a different option, even if such an assessment is prompted. These two types of low uncertainty can be observed in the structure of the confusion matrices."}, {"title": "Discussion", "content": "In this work, we introduced a method for quantifying uncertainty in LLM-as-a-Judge evaluations. Through empirical analysis, we found that the uncertainty labels correlate with accuracy, indicating the effectiveness of the method. Although our primary focus was on evaluating this method within the context of LLM-based evaluations, the potential for broader applications is significant."}, {"title": null, "content": "One notable observation is that the confusion matrices contain more information than what is utilized by the current labeling approach. This opens up two promising directions for future research. First, we propose the development of an uncertainty score derived directly from the confusion matrix, leveraging the additional information encoded within the matrix. Such a score would eliminate the need for setting a threshold, streamlining the process. Second, instead of using the matrix solely for uncertainty quantification, the option selection could be directly informed by the matrix, further enhancing decision-making accuracy. These two could be reached by training a model and predicting the uncertainty and/or the correct option. Both of these improvements could be achieved by training a model capable of predicting uncertainty or the correct option based on the learned patterns in the confusion matrix.\nDespite these promising results, the current approach presents certain limitations. The method is computationally intensive, especially when using large models such as Llama-3-70B-Instruct, which may not be feasible for all applications. Additionally, the performance of the method may vary when applied to models or tasks that have not been fine-tuned for evaluation purposes. Generalizability across diverse tasks and domains also requires further investigation.\nTo address these challenges, one potential solution is to reduce inference time by consolidating all assessments into a single prompt, querying the model for its chosen option only once. This strategy could lower computational overhead without compromising accuracy. Overall, while the current method yields strong results, further optimization and refinement have the potential to enhance efficiency and effectiveness.\nTo further improve the results and increase the proportion of low uncertainty labels, prompt engineer- ing emerges as a key factor. We recommend adapting the prompt structure specifically to the task and the model in use. Fine-tuning and tailoring prompts could significantly enhance performance, with the potential to surpass the results obtained in this study. Future research could explore the method's effectiveness with various prompt designs to further optimize its performance."}]}