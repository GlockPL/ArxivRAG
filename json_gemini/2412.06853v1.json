{"title": "Tube Loss: A Novel Approach for Prediction Interval Estimation and probabilistic forecasting", "authors": ["Pritam Anand", "Tathagata Bandyopadhyay", "Suresh Chandra"], "abstract": "This paper proposes a novel loss function, called 'Tube Loss', for simultaneous estimation of bounds of a Prediction Interval (PI) in the regression setup, and also for generating probabilistic forecasts from time series data solving a single optimization problem. The PIs obtained by minimizing the empirical risk based on the Tube Loss are shown to be of better quality than the PIs obtained by the existing methods in the following sense. First, it yields intervals that attain the prespecified confidence level t \u2208 (0, 1) asymptotically. A theoretical proof of this fact is given. Secondly, the user is allowed to move the interval up or down by controlling the value of a parameter. This helps the user to choose a PI capturing denser regions of the probability distribution of the response variable inside the interval, and thus, sharpening its width. This is shown to be especially useful when the conditional distribution of the response variable is skewed. Further, the Tube Loss based PI estimation method can trade-off between the coverage and the average width by solving a single optimization problem. It enables further reduction of the average width of PI through re-calibration. Also, unlike a few existing PI estimation methods the gradient descent (GD) method can be used for minimization of empirical risk. Finally, through extensive experimentation, we have shown the efficacy of the Tube Loss based PI estimation in kernel machines, neural networks and deep networks and also for probabilistic forecasting tasks. The codes of the experiments are available at https://github.com/ltpritamanand/Tube_loss", "sections": [{"title": "1 Introduction", "content": "In regression setting, machine learning (ML) algorithms predict the value of a variable y, often called dependent variable, given the value of an independent variable, say, x. Merely giving the predicted value of y without attaching a measure of uncertainty with it, may not be useful in real world applications. Uncertainty quantification (UQ) is especially important, when the cost of incorrect prediction is high. For example, in planning replacement of a critical component of a nuclear reactor, failure of which may lead to a nuclear disaster, the information that the predicted value of its life is 2.5 years without attaching a measure of uncertainty may not be useful. In contrast, if it is predicted that the life of critical component is between 2 and 3 years with, say, 99% confidence, it throws out useful information. Such an interval, predicted for values of y, is often called a prediction interval (PI) with a pre-specified confidence (in this case 99%). Let us assume (xi, Yi), i = 1, 2, ..., m, denote m independent copies of the random variables (x, y) having a joint distribution p(x, y). With some abuse of notation, we denote by (x, y), a pair of random variables as well as its values whenever the distinction is obvious.\nA standard regression model provides an estimate of E(y|x) as the predicted value of y. However, for uncertainty quantification (UQ) of the output of a deep neural network (NN) in regression setting,"}, {"title": "2 Notations & Mathematical Preliminaries", "content": "Let X and Y denote the domains of x and y, respectively. In most applications X = R\" and Y = R, where R is the set of real numbers. Let T = {(xi, Yi): Xi \u2208 X, Yi \u2208 V, i = 1, 2, ..., m} be the training set, where (Xi, Yi), i = 1, 2, ..., m are independently and identically distributed (iid) random variables having a joint distribution p(x, y). With some abuse of notation, we denote by (x, y), a pair of random variables as well as its values whenever the distinction is obvious.\nIn non-parametric framework, an estimate of Fq(x), say, F\u2084(x) based on the training set T is given by\n$$\\hat{F}_q(x) = \\arg \\min_\\mu \\sum_i \\rho_q(Y_i - \\mu(X_i)),$$\nwhere, \u03bc(.) belongs to a suitably chosen class of functions. And pq(u), the pinball loss function (Koenker &\nBassett Jr (1978), Koenker & Hallock (2001)), is given by\n$$\\rho_q(u) = \\begin{cases} qu, & \\text{if } u \\geq 0, \\\\ (q-1)u, & \\text{otherwise}. \\end{cases}$$\nTakeuchi et al. (2006) prove that for large m if the conditional distribution of y given x does not contain any discrete component, the proportion of yi's below Fq(x) across all values of x converges to q. Thus, for large m and fixed q such that q + t \u2264 1, [Fq(x), Fq+t(x)] provides a PI of y with confidence t. However, for finding Fq(x) and Fq+t(x) one needs to solve the above optimization problem independently for q and q + t. Further, for finding a HQ PI, among all feasible choices one needs to search for a q such that the mean prediction interval width (MPIW) given by m-1 \u03a3\u2081(Fq+t(xi) \u2013 Fq(xi)) is minimized. This amounts to solving two separate optimization problems repeatedly across feasible choices of q. Evidently, the PI estimation through this approach is computationally expensive.\nInstead of estimating the bounds separately, simultaneous estimation of the lower and upper bounds of the PI (LUBE), say, [\u03bc\u2081(x), \u03bc2(x)] is first proposed by Khosravi et al. (2011a). They propose finding the estimates of \u03bc\u2081(x) and \u03bc2(x) by minimizing the loss based on a novel loss function, say, LUBE. Building on the same idea as Khosravi et al., Pearce et al. (2018) propose replacing LUBE by Quality Driven (QD) loss function citing some reasons for improvement.\nLet [1(x), 2(x)] be the estimated PI on training set (xi, Yi), i = 1,2,..m. Then, its empirical coverage (PICP), and mean width (MPIW) are defined as\n$$PICP = m^{-1} \\sum_i k_i,$$\nwhere,\n$$k_i = \\begin{cases} 1, & \\text{if } y_i \\in [\\mu_1(x), \\mu_2(x)], \\\\ 0, & \\text{Otherwise}, \\end{cases}$$\nand\n$$MPIW = m^{-1} \\sum_{i=1,2,...,m} [\\mu_2(x_i) - \\mu_1(x_i)],$$"}, {"title": "3 A Brief Survey of The Literature", "content": "In this section, we summarize some recent and important work on PI estimation and probabilistic forecasting relevant to our work presented in this paper.\nPI estimation in kernel machines :- In kernel machine literature a PI with confidence t is given by [Fq(x), Fq+t(x)] for fixed 0 < q,q + t < 1 where, Fq(x) and Fq+t(x) are the qth and (q+t)th quantiles estimated independently. Specifically, for PI estimation in kernel machines presented in Section 5, we implement the quantile estimation method by Takeuchi et al. (2006). For estimation of 7th quantile it requires the solution to the following optimization problem\n$$\\min_{(\\alpha,\\alpha_0)}[\\frac{\\lambda}{2} \\alpha^T \\alpha + \\frac{1}{m}\\sum_{i=1}^m \\rho_r (Y_i - (K(A^T, x_i) \\alpha + \\alpha_0))],$$\nwhere, p-(u) is as defined in (2), K(AT,x) = [k(x1,x), k(x2,x), .., k(xm, x)] and k(x, y) is a positive semi- definite kernel (Mercer (1909)). Clearly, for the estimation of PI with confidence t, the problem (6) needs to be solved independently for r = q and r = q + t, respectively. Further, to obtain a PI with minimum MPIW across different feasible choices of q the independent estimation of Fq(x) and Fq+t(x) is to be repeated for a sufficiently large number of values of q.\nPI estimation in Neural Networks:- The PI estimation problem in NN framework has been well studied. Some of the commonly used methods are the Delta Method Papadopoulos et al. (2001) Khosravi et al. (2011b), Bayesian Method Ungar et al. (1996) and Mean Variance Estimation (MVE) Method Nix & Weigend (1994). A systematic and detailed description of these methods can be found at Kabir et al. (2018). But, all these methods assume Gaussian noise distribution and thus, fail to ensure a consistent performance on a variety of data sets.\nKhosravi et al. (2011) first propose a method for simultaneous estimation of lower and upper bounds of a PI in a distribution free setting by introducing a PI based loss function. This loss function is then used for training and development of NNs. They propose to minimize MPIW subject to the constraint that PICP is greater than equal to t by introducing a novel loss function. Clearly this method is proposed on HQ principle. Let's call this loss function as LUBE (Khosravi et al. (2011a)), and is given by,\n$$LUBE = NMPIW (1 + \\gamma (PICP)e^{-\\eta (PICP - t)}),$$"}, {"title": "4 Tube Loss function", "content": "In this section, first, we introduce the Tube loss function, a new class of loss functions, designed for the simultaneous estimation of the quantile bounds of PI. We then discuss its properties and also some important properties of the PI resulting from it.\nFor a given t \u2208 (0,1) and u2 \u2264 u1, we define the tube loss function as\n$$\\rho_t(u_2, u_1) = \\begin{cases} tu_2, & \\text{if } u_2 > 0, \\\\ (1 - t)u_2, & \\text{if } u_2 \\leq 0, u_1 > 0 \\text{ and } ru_2 + (1 - r)u_1 \\geq 0, \\\\ (1 - t)u_1, & \\text{if } u_2 \\leq 0, u_1 > 0 \\text{ and } ru_2 + (1 - r)u_1 < 0, \\\\ -tu_1, & \\text{if } u_1 < 0, \\end{cases}$$\nwhere 0 < r < 1 is a user-defined parameter and (u2, u\u2081) are errors, representing the deviations of y values from the bounds of PI.\nThe Tube loss function is a kind of two-dimensional extension of Pinball loss function Koenker & Bassett Jr (1978). The plots of Tube loss are given for r = 0.5. For r = 0.5, p\u1f37 (u1,u2) is a continuous loss function of u\u2081 and u2, and symmetrically located around the line u\u2081 + U2 = 0,u \u2208 R. In all experiments, where the noise distribution is assumed to be symmetric, ther parameter in the Tube loss function should be 0.5 to cover the denser region of y values.\nFor sake of better intuition, if we consider u\u2081 = y \u2212 \u00b5\u2081 and u2 = y - \u00b5\u03bc2, then, the Tube loss function reduces to\n$$\\rho_t(y, \\mu_1, \\mu_2) = \\begin{cases} t(y - \\mu_2), & \\text{if } y > \\mu_2. \\\\ (1 - t) (\\mu_2 - y), & \\text{if } \\mu_1 \\leq y \\leq \\mu_2 \\text{ and } y \\geq r\\mu_2 + (1 - r)\\mu_1, \\\\ (1 - t)(y - \\mu_1), & \\text{if } \\mu_1 \\leq y \\leq \\mu_2 \\text{ and } y <r\\mu_2 + (1 - r)\\mu_1, \\\\ t(\\mu_1 - y), & \\text{if } y < \\mu_1, \\end{cases}$$\nWe have plotted the Tube loss function (10) in Figure 3 as a function of y, by fixing \u00b5\u2081 = \u22125 and \u00b5\u2082 = 5. It clearly shows that the Tube loss function is a kind of juxtaposition of two Pinball loss functions.\nFor given (xi, Yi), i = 1, 2, ..m, and a confidence t \u2208 (0,1), the estimation of PI [\u00fb\u2081(x), \u00b5\u2082(x)] with tube loss function is given by\n$$[\\hat{\\mu}_1(x), \\hat{\\mu}_2(x)] = \\arg \\min_{\\mu_1, \\mu_2} \\sum_{i=1}^m \\rho_t (Y_i, \\mu_1 (x_i), \\mu_2(X_i)),$$"}, {"title": "4.1 Asymptotic properties of the Tube loss function", "content": "In this section, we prove that given a training set T = {(x1,y1), ...., (Xm, Ym) : xi \u2208 Rn, Yi \u2208 R}, and t \u2208 (0, 1), the PI [\u03bc\u2081(x), \u03bc\u2082(x)] obtained by minimizing the empirical risk \u03a3=1 L+ (yi, \u03bc\u2081(xi), \u03bc2(xi)) asymptotically achieves the target coverage t. In other words, for a sufficiently large training set, the coverage of the PI achieves the target coverage approximately. The proof follows the reasoning outlined by Takeuchi et al. (2009), who established it for the pinball loss.\nFirst, we prove a lemma assuming an independently and identically distributed (iid) random variables set up. Let {Y1, Y2, ...., Ym} CR are iid following the distribution of a continuous random variable Y and t \u2208 (0, 1). Let us define the following subsets of R: R1 (\u03bc1, \u03bc2) = {y : y > \u03bc2}, R2(\u03bc1, \u03bc2) = {y : \u03bc\u2081 < y < \u03bc2,y > \u03b3\u03bc\u2082 + (1 - r)\u03bc\u2081}, R3(\u03bc1, \u03bc2) = {y : \u03bc\u2081 < y < \u03bc2, y < r\u00b52 + (1 - r)\u03bc\u2081} and R4(\u03bc1, \u03bc2) = {y : y < \u03bc\u2081}. Notice that the sets Ri(\u03bc\u2081, \u03bc2), i = 1,2,3,4 are so defined that they do not include the points on the boundaries."}, {"title": "4.2 Ther parameter and movement of the PI bounds", "content": "The Proposition 2 clearly entails that the coverage of the PI obtained by minimizing the proposed tube loss achieves the target confidence asymptotically. Here, we show by choosing r appropriately how the PI tube can be moved up and down.\nLet us suppose that (1\u00b2(x), 22(x)) is the minimizer of the average loss for r = r2 i.e. m \u03a3=1 L2 (Yi, \u03bc\u2081 (xi), \u03bc2(xi)). Also we assume that there are m\u00b2 points in the set Rk (1\u00b2(x), \u03bc\u00b2(x)), (k =\n1, 2, 3, 4). Similarly, for r1(<r\u2082), we assume (1, 2) is the minimizer of the average loss for r = r1, and there are m\u00b9 points in the set Rk (\u00b5\u00b9(x), \u03bc\u00b9(x)), (k = 1,2,3,4). Now, Proposition 1 (iii) entails that asymptotically t fraction of y values should lie inside each of the PIs [1, 2] and [1, 2]. But, since r1 < r2, then,\nmr1 mr2\nm2,\n< , which implies > m2 for large m, by using the (i) and (i) of Preposition 1. In"}, {"title": "4.3 Re-calibration", "content": "Often, it is observed that the coverage of the PI obtained by training the model on a small training data may be significantly greater than the target confidence t on the validation set. In such cases, re-calibration of the model by trading off MPIW with the average loss in the training set may lead to a PI with smaller MPIW in the test set. The optimization problem with user defined parameter 8 > 0 is then formulated as follows:\n$$\\min_{(\\mu_1,\\mu_2)} [\\sum_{i=1}^m \\rho_t (Y_i - \\mu_1 (x_i), Y_i - \\mu_2(X_i)) + \\delta \\sum_{i=1}^m |(\\mu_2(X_i) - \\mu_1(X_i)|].$$"}, {"title": "5 Experimental Results", "content": "We present here the results of the experiments that we have run to compare the performances of PI based on Tube loss function with that of PIs based on the standard loss functions. We consider PI estimation in kernel machines, NN and probabilistic forecasting. We train different PI estimation methods and assess their performances on test set {(xi, Yi) : i = 1, 2, .., k}, using the following evaluation metrics.\n(a) Prediction Interval Coverage Probability (PICP): The fraction of yi values in the test set lying inside of estimated PI.\n(b) Mean Prediction Interval Width (MPIW): The average width of the estimated PI across different values of test xi.\nAs stated at the outset, HQ principle requires a PI to minimize MPIW subject to the constraint that PICP is greater than equal to the nominal confidence level t. In the following, for comparing two PI estimation methods, say, A and B we follow this rule: A is better than B (i) if PICP(A) > t and PICP(B) < t, or (ii) min{PICP(A), PICP(B)} > t and MPIW(A) < MPIW(B). Further, in experiments where the data are generated synthetically from a known distribution, the true quantiles of y given x are known for all values of x, and hence the true PI. In such cases, to get a better insight about their performances, we may compare the PICP and MPIW of the estimated PIs with that of the true PI locally in different regions of x. Also, to measure the deviation between the estimated PI, say, [\u00fb\u2081(x), \u017f\u00fb2(x)] and the true PI, say, [\u03bc\u2081(x), \u03bc2(x)], we consider the Sum of Mean Square of Error (SMSE) given by=1(1(xi) \u2013 \u03bc\u2081(xi))\u00b2 + \u03a3\u03ba=1(\u03bc2(xi) -\n\u03bc2(\u03a7\u03af))2."}, {"title": "5.1 Tube loss in kernel machine:-", "content": "Let us generate two synthetic datasets, say, A and B, of the form {(xi, Yi), i = 1, 2, .., 1500}, where xi is generated from uniform distribution between 0 and 1 (U(0,1)) and yi, using the relation\n$$Y_i = \\frac{sin x_i}{x_i} + \\epsilon_i,$$\nwhere, e\u00a1 is a random noise. For dataset A, ez is generated from N(0,0.8), a symmetric distribution, and for data set B from x\u00b2(3), a positively skewed distribution. We train the model using 500 data points and the rest we use for testing. For estimation of linear PI tube, we use linear kernel. For the estimation of the non-linear PI tube, we use RBF kernel of the form k(x1,x2) = e\u00af||(x1-x2)||\u00b2, where y is the parameter."}, {"title": "5.2 Tube Loss in Neural Network", "content": "In the NN framework Pearce et al. (2018) show that the PI obtained by QD loss performs slightly better than that obtained by LUBE Khosravi et al. (2011a) in terms of PICP and MPIW producing smoother and tighter boundaries. This improvement is attributed to the use of GD method for optimization of QD loss instead of Particle Swarm Optimization (PSO) Kennedy & Eberhart (1995), a non-gradient based method, used for LUBE loss. Also they compare PI with QD loss with the PI obtained by Mean Variance Estimation (MVE) method, Nix & Weigend (1994). They observe that QD method performs better than the MVE method when the noises are generated from a skewed distribution, specifically from an exp(1/x2). It is worth mentioning here that for noises generated from a skewed distribution, the Tube loss based PIs outperform the quantile based PIs (cf. Section 5.1).\nHere, we perform a simple experiment to show how the Tube loss based PI outperforms the QD based PI in terms of SMSE, a local measure of smoothness and tightness introduced at the outset of Section 5. For this, we generate 1000 data points (xi, Yi) using the equation,\n$$Y_i = \\frac{sin(x_i)}{x_i} + \\epsilon_i,$$\nwhere xi is U(\u22122\u03c0, 2\u03c0) and \u20ac\u00a1 is from U(-1, 1).\nWe train the tube Loss-based NN model and the QD loss-based NN model to obtain a PI with nominal confidence level 0.95. After tuning, we fix the NN model, containing 100 neurons in the hidden layer with 'RELU' activation function and two neurons in the output layer. 'Adam' optimizer was used for training the NNs. Figure 8 (a) shows the PI obtained by the QD loss and Figure 8 (b) the Tube loss with r = 0.5 (Figure 8 b) along with the true PI. Clearly the PI generated by the Tube loss is smoother and tighter compared to that generated by the QD loss. The true PI is computed by obtaining 97.5% and 2.5% quantiles of the conditional distribution of yx for different values of x.\nThe observed (PICP, MPIW) for Tube loss and QD loss are (0.95, 1.78) and (0.98, 2.17), respectively. For QD loss, the tuning of the soften parameter s is important but, is a tedious task. We tune parameters s and"}, {"title": "5.3 Tube loss for probabilistic forecasting with deep networks", "content": "We now assess the performance of the Tube loss function for probabilistic forecasting tasks using different deep network forecasting architectures.\nAt first, we consider the six popular benchmark time-series data sets viz., Electric (BP & Ember. (2016)), Sunspots (SIDC & Quandl.), SWH (NDBC), Temperature (machinelearningmastery.com), Female Birth (datamarket.com) and Beer Production (Australian (1996)). For probabilistic forecasts we use Long Short Term Memory (LSTM) Network (Hochreiter & Schmidhuber (1997)). We compare the probabilistic forecasts of the Tube loss based LSTM (T-LSTM) with that of the Quantile based LSTM (Q-LSTM) for these datasets. We use 70% of the data points for training and rest for testing. Out of the training sets, 10% of the observations are used for validation.\nSince these datasets are being used by other researchers, for each dataset, we tune and fix LSTM hidden layer structure, drop out layer and window size based on information available in the literature. We then obtain the PI using the Tube loss function, and also using quantiles. The nominal confidence level considered is 0.95. In Table 3, the PICP, MPIW and training time in seconds are reported for Q-LSTM and T-LSTM. Clearly, the T-LSTM based PIs outperform Q-LSTM based PIs in five out of 6 datasets. Also re-calibration of T-LSTM (cf. (16)) based forecasts leads to furtrher reduction of MPIW. The main advantage of T-LSTM"}, {"title": "5.3.1 Probabilistic forecasting of wind speed", "content": "For further validity of the Tube loss function, We need to evaluate its performance with other commonly used deep neural architectures for forecasting such as LSTM, GRU and TCN.\nProbabilistic forecasting of wind speed is crucial for effective decision-making in the wind power industry. We use the Tube loss function into the LSTM, Gated Recurrent Unit (GRU) and Temporal Convolution Network (TCN) model for wind speed probabilistic forecasting. We have compared the Tube loss based deep forecasting models with the quantile based deep forecasting models. We have also used the QD+ loss function (Salem et al. (2020)) in deep probabilistic forecasting of wind speed for the comparisons.\nDataset:- We have acquired a time series dataset of hourly wind speed measurements at 120m height obtained from Jaisalmer through the National Renewable Energy Laboratory website at https://developer.nrel. gov/docs/wind/wind-toolkit/ https://developer.nrel.gov. The first 75% hourly wind-speed observation was taken as training set and thereafter next 15% of observations was taken as validation set. The last 15% of observations was used for testing.\nTable 6 compares the Tube loss based LSTM, GRU and TCN models with QD++ loss function and quantile based deep forecasting models on the the Jaisalmer wind datasets. We can observe that the Tube loss based deep forecasting models obtain the better performance than the QD+ loss and quantile based deep forecasting models. Also quantile based deep forecasting models are computationally expensive than the Tube loss."}, {"title": "6 Future Work", "content": "The Tube loss function does not only offer a convenient way for PI estimation and probabilistic forecasting but, also opens a set of promising research directions. The future work requires a thorough theoretical analysis of the Tube loss function, similar to the analysis of the pinball loss function available in the literature. The Tube loss function asymptomatically guarantees to attain the target confidence t, thus, for small sample sizes it may not attain the target confidence t. A potential future direction is to refine Tube loss-based networks by applying techniques from the literature, such as group batching (Chung et al. (2021)), or introducing orthogonality constraints (Feldman et al. (2021)) to improve the local calibration. The other important research direction may be to use the Tube loss function for conformal regression and then compares its performances with the existing conformal regression techniques.\nIn most applications related to the text processing, the target variable is multidimensional. An interesting proble is to consider extension of the proposed PI estimation methodology for multidimensional target variable. Also, it would be interesting to apply the proposed methodology for probabilistic forecasting considering various neural architectures used in different domains of applications, such as solar irradiance, cryptocurrency prices, exchange rates, stock prices, ocean wave heights, pollution rates, and weather forecasting to name a few."}, {"title": "B Appendix B", "content": "Gradient Descent method for Tube loss based kernel machine\nThe Tube loss based kernel machine estimates pair of functions\n$$\\mu_2(x) := \\sum_{i=1}^m k(x_i, x) \\alpha_i + b_1 \\text{ and } \\mu_1(x) := \\sum_{i=1}^m k(x_i, x) \\beta_i + b_2.$$\nwhere k(x, y) is positive definite kernel. For the sake of simplicity, we rewrite \u03bc\u2081(x) and \u03bc2(x) in vector form\n$$\\mu_1(x) := K(A^T, x)a + b_1 \\text{ and } \\mu_2(x) := K(A^T, x)\\beta + b_2.$$\nwhere A is the mxn data matrix containing m training points in R\", K(AT,x)\n= [k(x1,x), k(x2,x), .., k(xm, x)], a =\nThe tube loss based kernel machine considers the problem\n$$\\min_{(\\alpha,\\beta,b_1,b_2)} J_2(a, b,b_1, b_2) = (a \\lambda a + \\beta \\lambda \\beta) + \\sum_{i=1}^m \\rho_i (Y_i, (K(A^T, x_i)a + b_1), (K(A^T, x_i) \\beta + b_2) ) +\\delta \\sum_{i=1}^m |(K(A^T, x_i)(a - \\beta) + (b_1 - b_2)|,$$\nwhere p is the Tube loss function as given in (11) with the parameter r.\nFor a given point (xi, yi), let us compute the gradient of pr (yi, (K(AT, xi)a+b1), (K(AT, xi) + b2)) first. For this, we compute"}]}