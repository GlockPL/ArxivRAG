{"title": "Task-Aware Harmony Multi-Task Decision Transformer for Offline Reinforcement Learning", "authors": ["Ziqing Fan", "Shengchao Hu", "Yuhang Zhou", "Li Shen", "Ya Zhang", "Yanfeng Wang", "Dacheng Tao"], "abstract": "The purpose of offline multi-task reinforcement learning (MTRL) is to develop a unified policy applicable to diverse tasks without the need for online environmental interaction. Recent advancements approach this through sequence modeling, leveraging the Transformer architecture's scalability and the benefits of parameter sharing to exploit task similarities. However, variations in task content and complexity pose significant challenges in policy formulation, necessitating judicious parameter sharing and management of conflicting gradients for optimal policy performance. Furthermore, identifying the optimal parameter subspace for each task often necessitates prior knowledge of the task identifier during inference, limiting applicability in real-world scenarios with variable task content and unknown current tasks. In this work, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel solution designed to identify an optimal harmony subspace of parameters for each task. We formulate this as a bi-level optimization problem within a meta-learning framework, where the upper level learns masks to define the harmony subspace, while the inner level focuses on updating parameters to improve the overall performance of the unified policy. To eliminate the need for task identifiers, we further design a group-wise variant (G-HarmoDT) that clusters tasks into coherent groups based on gradient information, and utilizes a gating network to determine task identifiers during inference. Empirical evaluations across various benchmarks highlight the superiority of our approach, demonstrating its effectiveness in the multi-task context with specific improvements of 8% gain in task-provided settings, 5% in task-agnostic settings, and 10% in unseen settings.", "sections": [{"title": "I. INTRODUCTION", "content": "OFFLINE reinforcement learning (RL) [28] enables the learning of policies directly from an existing offline dataset, thus eliminating the need for interaction with the actual environment. Despite the promising developments of offline RL in various robotic tasks, its successes have been largely confined to individual tasks within specific domains, such as locomotion or manipulation [9, 22, 26]. Drawing inspiration from human learning capabilities, where individuals often acquire new skills by building upon existing ones and spend less time mastering similar tasks, there's a growing interest in the potential of training a set of tasks with inherent similarities in a more cohesive and efficient manner [27]. This perspective leads to the exploration of multi-task reinforcement learning (MTRL), which seeks to develop a versatile policy capable of addressing a diverse range of tasks.\nRecent developments in Offline RL, such as the Decision Transformer [6] and Trajectory Transformer [23], have abstracted offline RL as a sequence modeling (SM) problem, showcasing their ability to transform extensive datasets into powerful decision-making tools [21]. These models are particularly beneficial for multi-task RL challenges, offering a high-capacity framework capable of accommodating task variances and assimilating extensive knowledge from diverse datasets. Additionally, they open up possibilities for integrating advancements [3] from language modeling into MTRL methodologies. However, the direct application of these high-capacity sequential models to MTRL presents considerable algorithmic challenges. As indicated by Yu et al. [46], simply employing a shared network backbone for all diverse robot manipulation tasks can lead to severe gradient conflicts. This situation arises when the gradient direction for a particular task starkly contrasts with the majority consensus direction. Such unregulated sharing of parameters and their optimization under conflicting gradient conditions can contravene the foundational goals of MTRL, degrading performance compared to task-specific training methods [38]. Furthermore, the issue of gradient conflict is exacerbated by an increase in the number of tasks (detailed in Section III), underscoring the urgency for effective solutions to these challenges."}, {"title": "II. RELATED WORK", "content": "In offline RL, unlike the fundamentally online paradigm [39], learning is performed without interaction with the environment [28]. Instead, it depends on a dataset D comprising trajectories generated from various behavior policies. The objective of offline RL is to utilize this dataset D to learn a policy that maximizes the expected return. Previous studies have employed constrained or regularized dynamic programming techniques to minimize deviations from the behavior"}, {"title": "B. Multi-Task Learning", "content": "In the context of multi-task learning, several methodologies have been developed to mitigate the effects of conflicting gradients. PCGrad [45] projects each task's gradient onto the orthogonal plane of another's, subsequently updating parameters using the mean of these projected gradients. Graddrop [7] employs a stochastic approach, randomly omitting certain gradient elements based on their conflict intensity. CAGrad [30] manipulates gradients to converge towards a minimum average loss across tasks. Instead of adjusting gradients post hoc as in these methods, we proactively utilize gradient data to inform the selective activation of parameters for tasks through a masking mechanism. This direct intervention at the parameter level allows the model to update without the typical interferences found in gradient-level adjustments, fostering a more streamlined and potentially more efficacious optimization process. For comparison, we integrate classical methods PCGrad and CAGrad with MTDT in our experiments."}, {"title": "C. Multi-Task Reinforcement Learning", "content": "Multi-task RL aims to learn a shared policy for a diverse set of tasks, with numerous approaches proposed in the literature [36, 37, 44]. By leveraging the expressive scalability and parameter-sharing capabilities of Transformer architectures alongside the sequence modeling paradigm in offline RL, many works aim to design a highly generalist Transformer-based policy [27, 35] capable of handling multiple diverse tasks. A straightforward approach involves pooling extensive datasets collected from various scenarios to address multiple tasks simultaneously. This pooled data enables the model to learn strategies more effectively across all tasks compared to training each task in isolation. While sharing data across all tasks is expected to improve performance by exploiting task similarities through parameter sharing, this approach can lead to serious conflicting gradients due to indiscriminate parameter sharing [11], resulting in sub-optimal performance. Moreover, current MTRL methods often require access to the task identifier to obtain specific information about the current task as summarized in Table I [15, 38, 43], which can be problematic in realistic applications. To reduce conflicting gradients, we propose Harmony Multi-Task Decision Transformer (HarmoDT) [20], which identifies an optimal harmony subspace of parameters for each task by using trainable task-specific masks during MTRL training. To further eliminate the need for task identifiers, we design a group-wise variant, G-HarmoDT, which clusters similar tasks into groups based on their gradient information and employs a gating module to infer the identifiers of these groups."}, {"title": "III. RETHINKING SEQUENCE MODELING WITH MTRL", "content": "Recent works in offline RL conceptualize it as sequence modeling (SM), effectively transforming extensive datasets into potent decision-making systems. This approach is advantageous for multi-task RL, offering a high-capacity model that accommodates task discrepancies and assimilates comprehensive knowledge from diverse datasets. However, the direct application of such high-capacity sequential models to multi-task RL introduces significant algorithmic challenges. In this section, we outline the primary challenges, including conflicting gradients and the dependence on task identifiers. We further explore the concepts of parameter subspace and harmony, laying the groundwork for the motivation."}, {"title": "A. Conflicting Gradients", "content": "We first investigate conflicting gradients. In a multi-task training context, the aggregate gradient, \u011d, is computed across multiple tasks and is defined as\n$\\hat{g} = E_{T_i \\sim p(T)} \\nabla L_{T_i} (\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} g_i(\\theta),$ (1)\nwhere \u03b8 represents the trainable parameter vector and gi is the gradient vector for task Ti. In scenarios where tasks are diverse, the gradients gi from different tasks may conflict significantly, a phenomenon known as gradient conflicts and negative transfer in multi-task learning [11, 40].\nDefinition 3.1 (Harmony Score on a Single Weight): The harmony score of the j-th element in the weights vector is estimated by calculating the corresponding coordinate of the element-wise product of the task gradient and the total gradient, denoted as $(g_i \\odot \\hat{g})_j = g_{i,j} \\times \\hat{g}_j$.\nDefinition 3.2 (Averaged Harmony Score): The overall harmony score across all weights is evaluated using $\\frac{1}{K} \\sum_{i=1}^{K} \\sum_{j=1}^{K} \\frac{| (g_i \\odot \\hat{g})_j|}{|g_j||\\hat{g}_j|}$, where |\u03b8| and N denote the number of weights and tasks. This score, ranging between -1 and 1, reflects the degree of alignment among tasks.\nTo substantiate the presence of conflicts in MTRL, we establish two metrics to measure harmony score in weights among tasks and conduct experiments utilizing the Prompt-DT method on 5 and 50 tasks from the Meta-world benchmark, recording the average harmony score. As illustrated in Figure 2(a), the averaged harmony score significantly diminishes with the escalation in the number of tasks, indicating pronounced conflicts among tasks and underscoring the imperative to address these conflicts in MTRL."}, {"title": "B. Parameter Subspace and Harmony", "content": "Parameter subspace, a concept prevalent in pruning-aware training [2], aims to maintain comparable performance while achieving a sparse model. In the context of MTRL via SM, pruning to preserve distinct parameter subspaces for each task markedly alleviates gradient conflicts. To validate this, we conduct experiments on 50 tasks from the Meta-World benchmark. Each task Ti is assigned a randomly initialized mask MT with a specific sparsity ratio S. During training, this mask modulates both the trainable parameters and gradients as\n$\\hat{g}_i = \\nabla L_{T_i} (\\theta \\odot M_i) \\odot M_i, i = 1,2,..., N,$ (2)\nwhere \u2299 represents element-wise multiplication, and $M_i^T$ is a binary vector. Intriguingly, as shown in Figure 2(b), applying the mask could result in enhanced performance across a wide range of sparsity ratios. This improvement, coupled with the significantly higher harmony score in multi-task settings shown in Figure 2(a), suggests that maintaining a subspace of parameters with task-specific masks effectively reduces conflicts arising from unregulated parameter sharing."}, {"title": "C. Gating Network", "content": "In multi-task training, various approaches [15, 27, 43] aim to leverage the scalability and parameter-sharing capabilities of Transformer-based policies. However, these methods often require a task identifier during inference to access metadata about the current task. As illustrated in Figure 1, applying these methods in a task-agnostic setting\u2014where information about the current task is unavailable\u2014leads to a notable performance decline.\nGating networks are frequently employed in deep learning to differentiate between various inputs. To address the need for task identification in task-agnostic settings, a straightforward approach is to train a simple gating network in combination with these masking methods. However, as shown in Figure 3(a), the accuracy of the gating network significantly declines as the number of groups increases. This reduction in accuracy results in sub-optimal performance when the gating network is directly combined with HarmoDT, which requires precise task differentiation, as illustrated in Figure 3(b). This observation motivates the clustering of similar tasks. Clustering not only improves the accuracy of the gating network but also reduces the learning burden associated with optimal mask learning."}, {"title": "D. Motivation", "content": "The complexity of MTRL increases significantly with more tasks, mainly due to escalating gradient conflicts. This challenge arises from unregulated parameter sharing, which aims to exploit task similarities but often results in performance degradation. In response to these challenges, task-specific masks have been proposed to maintain distinct parameter subspaces for each task, preventing one task's learning from negatively impacting others. While this strategy represents a step towards mitigating gradient conflicts, it introduces new challenges: 1) determining optimal masks, and 2) the reliance on task identifiers. The complexity of optimizing masks is compounded by the dynamic and often non-linear task interactions within the shared model space. Additionally, the accuracy of the gating network declines with an increasing number of tasks, as distinguishing between similar tasks becomes progressively more challenging.\nTo determine optimal masks that balance shared learning and task-specific adaptation, we propose HarmoDT, a novel solution using a bi-level optimization strategy within a meta-learning framework. This approach leverages gradient-based techniques to meticulously explore and exploit the parameter space and identify a harmony parameter subspace for tasks, optimizing overall MTRL performance. To eliminate dependency on task identifiers, we introduce a group-wise variant, G-HarmoDT, which assigns similar tasks a shared group identifier, thereby reducing the learning burden on the gating network. This formulation reframes the problem as identifying an optimal \"sweet point\" that balances the performance of the gating network with the management of conflicting gradients among tasks within each group."}, {"title": "IV. METHOD", "content": "RL aims to learn a policy \u03c0\u03b8 (as) maximizing the expected cumulative discounted rewards $E[\\sum_{t=0}^{\\infty} \\gamma^tR(s_t, a_t)]$ in a Markov decision process (MDP), which is a six-tuple (S, A, P, R, \u03b3, do), with state space S, action space A, environment dynamics P(s'|s, a) : S\u00d7S \u00d7 A \u2192 [0,1], reward function R : S \u00d7 A \u2192 R, discount factor \u03b3\u2208 [0,1), and initial state distribution do [39]. The action-value or Q-value of a policy is defined as $Q^{\\pi}(s_t, a_t) = E_{a_{t+1}, a_{t+2}...\\sim \\pi} [\\sum_{i=0}^{\\infty} \\gamma^iR(S_{t+i}, a_{t+i})]$. In offline RL [28], a static dataset D = {(s, a, s', r)}, collected by a behavior policy \u03c0\u03b2, is provided and algorithms learn a policy entirely from this static dataset without any online interaction with the environment.\n2) Multi-task Reinforcement Learning: In the multi-task setting, different tasks can have different reward functions, state spaces, and transition functions. We consider all tasks to share the same action space with the same embodied agent. Given a specific task T ~ p(T), a task-specified"}, {"title": "B. HarmoDT: Find optimal subspace", "content": "To address the aforementioned problem, we introduce a meta-learning framework that discerns an optimal harmony subspace of parameters for each task, enhancing parameter sharing and mitigating gradient conflicts. This problem is formulated as a bi-level optimization, where we meta-learn task-specific masks to define the harmony subspace. Mathematically, we can express the problem as:\n$\\underset{M}{\\text{max }} E_{T\\sim p(T)} [\\sum_{t=0}^{\u221e} \\gamma^t R_T(s_t, \\pi(T^{input}*T_{i,t}))],$ (5)\ns.t. $0^* = \\underset{\\theta}{\\text{arg min }} E_{T_i \\sim p(T)} L_{DT}(\\theta, M), $ (6)\n$\\theta^*_{T_i} = \\theta^* \\odot M_i, M = \\{M_T\\}_{T_i\\sim p(T)},$ (7)\nwhere $M_T$ represents a binary task mask vector corresponding to Ti, and M denotes the set of all task masks. The goal at the upper level is to learn a task-specific mask that identifies the harmony subspace for each task. Concurrently, at the inner level, the objective is to optimize the algorithmic parameters \u03b8, maximizing the collective performance of the unified model under the guidance of the task-specific masks. The framework for our harmony subspace learning is depicted in Figure 4. Subsequent sections are meticulously dedicated to elucidating the methodology for selecting the harmony subspace, detailing the metrics for evaluating the importance and conflicts of weights, the sophisticated update mechanism for task masks, and delineating the procedural intricacies of the algorithm.\n1) Weights Evaluation: During training, our aim is to iteratively identify a harmony subspace for each task by assessing trainable parameter conflicts and importance. This involves defining two metrics: the Agreement Score and the Importance Score, to gauge the concordance and significance of weights.\nDefinition 4.1 (Agreement Score): For each task Ti with a set of task masks M, the agreement score vector of all trainable weights is defined as follows: $A(T_i) = \\frac{1}{N} \\sum_{j=1}^{N} \\frac{g_i} {|g_j||\\hat{g}_j|}$, where $\\hat{g}_j$ denotes the masked gradients for task Tj, which is defined in Equation 2.\nDefinition 4.2 (Importance Score): We measure the significance of parameters for task Ti either through the absolute value of the parameters $I_M(T_i) = |(\\theta \\odot M_T)|$, indicating magnitude-based importance, or through the Fisher information $I_F(T_i) = (\\nabla log L_{T_i} (\\theta \\odot M_T) \\odot M_T)^2$, reflecting the parameters' impact on output variability.\nFor task Ti, A(Ti) reflects the gradient similarity between the task-specific and the average masked gradients, while $I_M(T_i);$ and $I_F(T_i);$ measure the j-th element's importance."}, {"title": "C. G-HarmoDT: A Group-wise Variant", "content": "To reduce the need for task identifier, we propose G-HarmoDT, a variant for group subspace training that partitions cohesive tasks into groups and assigns each group a harmonious subspace. During the inference stage, where task ID is unavailable, we employ the gating module to discern groups and determine the appropriate group subspace. Mathematically, we formulate the problem as follows:\n$\\underset{M}{\\text{max }} E_{T\\sim p(T)} [\\sum_{t=0}^{\u221e} R_{T_i} (s_t, \\pi(T^{input}|g^*G_i))],$ (13)\ns.t. $0^* = \\underset{\\theta}{\\text{arg min }} E_{T_i \\sim p(T)} L_{DT} (\\theta, M, G),$\nwhere $\\theta^*_{G_j} = \\theta^* \\odot M_{G_j}, M = \\{M_{G_j} \\}, G = \\{G_j \\},$ where G denotes the set of all groups, NG is the number of groups defined as a hyper-parameter, Gj represents a task group comprising a set of cohesive tasks, $M_{G_j}^\u03b8$ denotes the mask vector corresponding to Gj, and M denotes the set of all masks. To solve the objective, we modify the original Har-moDT and introduce key changes from three perspectives in the following sections: group-wise weight evaluation, group-wise mask update, and the gating module, followed by an overview of the entire framework.\n1) Group-wise Weights Evaluation: All metrics used to evaluate weights in HarmoDT (Section IV-B1) are clustered at the group level, with the masked gradient for each task now derived based on the corresponding group mask:\n$\\hat{g}_i = \\nabla L_{T_i} (\\theta \\odot M_{G_i}) \\odot M_{G_i}, T_i \\in G_j.$\nDefinition 4.3 (Group Agreement Score): For a group Gj \u2208 G involving nj tasks and group masks $M_{G_j}^\u03b8$ \u2208 M, the group agreement score vector of all trainable weights is defined as $GA(G_j) = \\frac{1}{N_G \\sum_{T_i \u2208 G_j} S_i} \\sum_{T_i \u2208 G_k} \\frac{1}{N_G \\sum_{T_i \u2208 G_k} S_i}.$ Definition 4.4 (Group Importance Score): For a group Gj\u2208 G with group mask $M_j \u2208 M$, the group importance score vector is defined by the Fisher information as $GI(G_j) = (\\nabla log \\sum_{T\\in G} L_T (\\theta \\odot M_{G_j}^\u03b8) \\odot M_{G_j}^\u03b8)^2$. The absolute value of parameters is not used to measure importance here, as it is less effective than Fisher information in HarmoDT (Table II). Similarly, the Group Harmony Score can be defined as:\n$GH(G_j, M_{G_j}^\u03b8) = \\{\\frac{GA(G_j)_k + \\lambda GI(G_j)_k}{inf},\\}$, for $(M_{G_j}^\u03b8)_k = 1,(M_{G_j}^\u03b8)_k = 0$.2) Group-wise Mask Update: As total framework depicted in Figure 5 and Algorithms 3-5, cohesive tasks can be clustered into groups via a warming-up and grouping mechanism. We initially warm up the weights for tw iterations, followed by computing the harmony score for all tasks:\nH = [GH(T, 1), . . . , GH(TN, 1)], (14)\nwhere N is the total task number, and GH(T, 1) represents the Group Harmony Score for a single task group (|G| = |T|) with an all-one vector mask 1. With harmony scores, we can cluster tasks into NG groups using any clustering techniques:\nG = {G1,...,$G_{N_G}$} = Clustering(H, NG). (15)\nIn the experiments, we adopt the k-Nearest-Neighbors [12, 14] for clustering. Notably, our aim is not to propose a new clustering method, and the visualization provided in Section V-C illustrates the effectiveness of the clustering. We then initialize all tasks in the same group with a group mask as\n$M_j = 1 - ArgBtm_{Ks*|\u03b8|}$ ((\\sum_{T_i \u2208 G_j} GH(T, 1))), (16)\nwhere || is the total number of the parameters. This operation ensures that each mask adheres to the same sparsity requirements S.\nBuilding on the original processes of HarmoDT defined in equation 8 and equation 9, and adapting them into group level, we iteratively evaluate weights, mask group subspaces as"}, {"title": "V. EXPERIMENT", "content": "In this section, we conduct extensive experiments to answer the following questions: (1) How does our method compare to other offline and online baselines in the multi-task regime? (2) Does our method mitigate the phenomenon of conflicting gradients and identify optimal harmony subspaces of parameters for tasks? (3) Can our method generalize to unseen tasks and G-HarmoDT effectively reduce the need for task identifier?"}, {"title": "VI. CONCLUSION", "content": "In this study, we introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel approach designed to discern an optimal parameter subspace for each task, leveraging parameter sharing to harness task similarities while concurrently addressing the adverse impacts of conflicting gradients. By employing a bi-level optimization and a meta-learning framework, HarmoDT not only excels as a comprehensive policy in multi-task environments but also exhibits robust generalization capabilities to unseen tasks. To eliminate the need for task identifiers, we further design a group-wise variant (G-HarmoDT) that clusters tasks into coherent groups based on gradient information, and utilizes a gating network to determine task identifiers during inference. Our rigorous empirical evaluations across a diverse array of benchmarks underscore our approach's superior performance compared to existing baselines, establishing its state-of-the-art effectiveness in MTRL scenarios."}, {"title": "APPENDIX A\nDETAILED ENVIRONMENT", "content": "The Meta-World benchmark, introduced by Yu et al. [46], encompasses a diverse array of 50 distinct manipulation tasks, unified by shared dynamics. These tasks involve a Sawyer robot engaging with a variety of objects, each distinguished by unique shapes, joints, and connective properties. The complexity of this benchmark lies in the heterogeneity of the state spaces and reward functions across tasks, as the robot is required to manipulate different objects towards varying objectives. The robot operates with a 4-dimensional fine-grained action input at each timestep, which controls the 3D positional movements of its end effector and modulates the gripper's openness. In its original configuration, the Meta-World environment is set with fixed goals, a format that somewhat limits the scope and realism of robotic learning applications. To address this and align with recent advancements in the field, as noted in works by Sun et al. [38], Yang et al. [44], we have modified all tasks to incorporate a random-goal setting, henceforth referred to as MT50-rand. The primary metric for evaluating performance in this enhanced setup is the average success rate across all tasks, providing a comprehensive measure of the robotic system's adaptability and proficiency in varied task environments.\nFor the creation of the offline dataset, we follow the work by He et al. [15] and employ the Soft Actor-Critic (SAC) algorithm [13] to train distinct policies for each task until they reach a state of convergence. Subsequently, we compile a dataset comprising 1 million transitions per task, extracted from the SAC replay buffer. These transitions represent samples observed throughout the training period, up until the point where each policy's performance stabilized. Within this benchmark, we have curated two distinct dataset compositions:\nNear-optimal: A dataset comprising 100 million transitions, capturing experience from random to expert-level performance (convergence) within SAC-Replay.\nSub-optimal: A dataset comprising the initial 50% of trajectories (50 million transitions) from the near-optimal dataset for each task, with a substantially reduced proportion of expert-level data.\nIn our evaluation, we apply our approach to a diverse array of meta-RL control tasks, each offering distinct challenges to assess the performance and generalization capabilities of our model. The tasks are detailed as follows:\nCheetah-dir: This task involves two distinct directions: forward and backward. The objective is for the cheetah agent to achieve high velocity in the assigned direction. The evaluation encompasses both training and testing sets, covering these two directions comprehensively to gauge the agent's performance effectively.\nCheetah-vel: Here, the task defines 40 unique sub-tasks, each associated with a specific goal velocity, uniformly distributed between 0 and 3 m/s. The agent's performance is assessed based on the 12 error relative to the target velocity, with a penalty for deviations. For testing, 5 of these tasks are selected, while the remaining 35 are used for training purposes.\nAnt-dir: This task comprises 50 different sub-tasks, each with a goal direction uniformly sampled in a two-dimensional plane. The agent, an 8-jointed ant, is incentivized to attain high velocity in the designated direction. Of these, 5 tasks are earmarked for testing, with the rest allocated for training."}, {"title": "APPENDIX B\nHYPER-PARAMETERS", "content": "This section details the training regimen implemented in our study. During the training phase, tasks are randomly selected for model refinement. Each training iteration is meticulously configured with a batch size of 256 and utilizes the Adam optimizer with a learning rate of 3e-4. The total number of training steps is set at 1e6. For a fair comparison, we adhere to the setups and recommendations in MTDIFF [15], setting the mask sparsity at S = 0.2, the mask update interval tm = 5e3, the upper bound of mask changes at Nmax = 100, the lower bound at Nmin = 0, and the controller \u03bb = 10. The detailed process for selecting hyperparameters for HarmoDT is provided in Section V-C. All methods are implemented using PyTorch on an NVIDIA GeForce 4090.\nOur policy is built on a Transformer-based model, utilizing the minGPT open-source code4. The specific model parameters and hyper-parameters used in our training process are outlined in Table IX."}, {"title": "APPENDIX C\nERK INITIALIZATION", "content": "This section elucidates the utilization of the Erd\u0151s-R\u00e9nyi Kernel (ERK), as proposed by Evci et al. [8], for initializing the sparsity in each layer of the model. ERK tailors sparsity distinctively for different layers. In convolutional layers, the proportion of active parameters is determined by $n_{i-1} + n_i + w_i h_i$, where $n_{i\u22121}, n_\u03b9, \u03c9_\u03b9,$ and $h_i$ represent the number of input channels, output channels, and the kernel's width and height in the l-th layer, respectively. For linear layers, the active parameter ratio is set to $\\frac{n_{i\u22121}+n_i}{\u03b7_i\u22121\u00d7\u03b7_i}$, with \u03b71\u22121 and \u03b7i indicating the number of neurons in the (l \u2013 1)-th and l-th layers. ERK ensures that layers with fewer parameters maintain a higher proportion of active parameters."}, {"title": "APPENDIX D\nBASELINES", "content": "We compare our proposed HarmoDT and G-HarmoDT with the following baselines.\ni. MTBC. We extend Behavior cloning (BC) to multi-task offline policy learning via network scaling and a task-ID conditioned actor that is similar to MTIQL.\nii. MTIQL. We extend IQL [25] with multi-head critic networks and a task-ID conditioned actor for multi-task policy learning. The TD-based baselines are used to demonstrate the effectiveness of conditional generative modeling for multi-task planning.\niii. MTDIFF [15]. MTDIFF is a diffusion-based method that integrates Transformer architectures and prompt learning for generative planning and data synthesis in multi-task offline settings."}, {"title": "IV. MTDT.", "content": "We extend the Decision Transformer architecture [6] to learn from multitask data. Specifically, we pool large amounts of data as input to the Transformer architecture while retaining the original loss function."}]}