{"title": "NEPTUNE: THE LOng Orbit TO BENCHMARKING LONG VIDEO UNDERSTANDING", "authors": ["Arsha Nagrani", "Mingda Zhang", "Ramin Mehran", "Rachel Hornung", "Nitesh Bharadwaj Gundavarapu", "Nilpa Jha", "Austin Myers", "Xingyi Zhou", "Boqing Gong", "Cordelia Schmid", "Mikhail Sirotenko", "Yukun Zhu", "Tobias Weyand"], "abstract": "This paper describes a semi-automatic pipeline to generate challenging question-answer-decoy sets for understanding long videos. Many existing video datasets and models are focused on short clips (10s-30s). While some long video datasets do exist, they can often be solved by powerful image models applied per frame (and often to very few frames) in a video, and are usually manually annotated at high cost. In order to mitigate both these problems, we propose a scalable dataset creation pipeline which leverages large models (VLMs and LLMs), to automatically generate dense, time-aligned video captions, as well as tough question answer decoy sets for video segments (up to 15 minutes in length). Our dataset Neptune covers a broad range of long video reasoning abilities and consists of a subset that emphasizes multimodal reasoning. Since existing metrics for open-ended question answering are either rule-based or may rely on proprietary models, we provide a new open source model-based metric (GEM) to score open-ended responses on Neptune. Benchmark evaluations reveal that most current open-source long video models perform poorly on Neptune, particularly on questions testing temporal ordering, counting and state changes. Through Neptune, we aim to spur the development of more advanced models capable of understanding long videos. The dataset is available at https://github.com/google-deepmind/neptune.", "sections": [{"title": "1 INTRODUCTION", "content": "Videos are experiencing an explosion moment online, with new research constantly pushing the frontier for video and language tasks such as video question answering (VideoQA) (Xu et al., 2017; Zhong et al., 2022; Xiao et al., 2021; Yang et al., 2021; Mangalam et al., 2023). Early video and language models, while adept at VideoQA, have largely focused on short, trimmed clips (less than 1 minute long (Yu et al., 2019a; Xiao et al., 2021)). The recent release of powerful, longer context multimodal models (eg. Gemini 1.5 (Reid et al., 2024) and GPT4 (Achiam et al., 2023)), however, has ushered in the promise of models being able to reason over millions of tokens, covering longer stretches of videos (many minutes long).\nWhile promising, these claims are often evidenced by qualitative examples, or results on small-size datasets \u2013 for example the 1H-VideoQA (Reid et al., 2024) benchmark, which while valuable, only consists of 125 questions. Popular video benchmarks for question answering still tend to focus on short, trimmed clips (e.g., Next-QA (Xiao et al., 2021)). Other datasets that do contain longer videos are often 'short-term' benchmarks disguised as long-term ones, evidenced by models that are able to solve them with a single (or a few) frames (e.g. some tasks on the LVU dataset (Wu & Krahenbuhl, 2021) such as scene prediction of movies). Other long video datasets may contain strong linguistic biases in multiple choice evaluation, as shown by MoreVQA (Min et al., 2024), which gets strong performance on EgoSchema (Mangalam et al., 2023) without access to the video at all, or can be solved with external internet knowledge, such as those made from popular movies (Li et al., 2023d).\nA key challenge in creating a truly long form video understanding dataset is the significant manual cost required to select, watch, understand and annotate long videos with free-form natural language."}, {"title": "2 RELATED WORKS", "content": "Video Question Answering: Video Question-Answering (VideoQA) is an important task for assessing multimodal video understanding systems' ability to reason about videos (Xu et al., 2017; Zhong et al., 2022; Xiao et al., 2021; Yang et al., 2021; Mangalam et al., 2023). Vision and language models for this task can be broadly classified into three categories: (i) early end-to-end VLMs for this task which typically consists of strong vision and language encoders/decoders, such as Flamingo (Alayrac et al., 2022), BLIP2 (Li et al., 2023b), Video-Llama (Zhang et al., 2023a), GIT2 (Wang et al., 2022) and PALI (Chen et al., 2022; 2023a;b). These typically are moderate sized models, and memory limits often lead to significant downsampling: e.g. temporally sampling a few frames with large strides (Wang et al., 2022; Chen et al., 2023a) or spatially subsampling each frame to a single token (Yang et al., 2023; Zhou et al., 2018; Wang et al., 2021); (ii) Socratic style models (Zeng et al., 2022), which consists of combining various specialised frozen models with carefully prompted state-of-the-art VLMs and LLMs (eg. MoreVQA (Min et al., 2024)) and (iii) end-to-end large multimodal models such as Gemini (Gemini Team Google, 2023) and GPT-4 (Achiam et al., 2023), which have long context lengths and can ingest multimodal data, including video, sound and text.\nVideo QA Benchmarks: Key datasets have pushed towards assessing reasoning for temporal questions (Grunde-McLaughlin et al., 2021; Xiao et al., 2021; Wu et al., 2021), longer videos (Yu et al., 2019a; Mangalam et al., 2023), as well as focusing on diverse domains like instructional (Yang et al., 2021) and egocentric videos (Gao et al., 2021; Mangalam et al., 2023). We summarise existing VideoQA benchmarks in a table provided in the appendix. Most datasets either focus on shorter videos (less than 100s), or are short video datasets 'in disguise', and can actually be solved with a few frames (e.g. ActivityNet-QA (Yu et al., 2019b) or MovieQA (Tapaswi et al., 2016)). 1H-VideoQA (Reid et al., 2024) consists of videos longer than 1 hour, but is limited to 125 questions and is closed-source. Like Neptune, ActivityNet-RTL (Huang et al., 2024), CinePile (Rawal et al., 2024) and EgoSchema (Mangalam et al., 2023) are generated by prompting LLMs, but cover only limited domains and rely on existing annotations while Neptune covers a much broader spectrum of video types and its pipeline is applicable to arbitrary videos. Most importantly, EgoSchema also has strong linguistic biases, while Neptune mitigates these through filtering (Sec. 5). Unlike other benchmarks which come with their own training sets (eg. MSR-VTT (Xu et al., 2016), ActivityNet (Yu et al., 2019a)), we propose a generalisation-focused zero-shot evaluation regime. The goal for Neptune is to benchmark any model, pre-trained with any external dataset or task, in order to assess real-world domain transfer. Hence we release test sets only. More discussion on related datasets and dataset pipelines is provided in the appendix.\nMetrics for open-ended VideoQA: Earlier QA datasets consisted of short answers (Xiao et al., 2021) (sometimes a single word), typically from a closed set, and therefore metrics such as accuracy or accuracy with exact match (EM) can be applied. As datasets have evolved with more real-world annotation (longer, open-set answers), designing a metric becomes challenging. Existing rule-based metrics for captioning, such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and CIDEr (Vedantam et al., 2015) can be applied, however they all primarily measure n-gram overlap, and do not capture the inherent subjectivity of the task, where different phrasing is often equally valid. Other metrics for captioning include SPICE (Anderson et al., 2016) (adds action and object relationships), while model-based metrics using earlier language models or image-language models include BERT-Score (Zhang et al., 2020), BERT-Score++ (Yi et al., 2020) (fine-tunes BERT for image captioning), LEIC (Cui et al., 2018), NUBIA (Kane et al., 2020), TIGEr (Jiang et al., 2019), CLIPScore (Hessel et al., 2021), and EMScore (Shi et al., 2022). For answer equivalence specifically, token F1 and exact match (EM) have been used, but suffer many of the same shortcomings that rule-based metrics do, and EM is often too strict for open-ended eval. BEM (Bulian et al., 2022) finetunes BERT on an answer-equivalence dataset, and shows that this provides a better score for QA. Recently, LLMs trained with reinforcement learning from human feedback (RLHF) that already exhibit strong human alignment (Bubeck et al., 2023) are used in works such as VideoChatGPT (Maaz et al., 2023) and MovieChat (Song et al., 2023) (LLM-as-a-judge). A challenge here is that the models used (ChatGPT) are called via proprietary APIs, where the underlying model may be non-static, thereby leading to non-reproducability in the metric. Instead, we take a state-of-the-art open-sourced lightweight language model (Team et al., 2024a) and finetune it on a public answer equivalence dataset (Bulian et al., 2022), to create an open-source, static, model-based evaluation metric."}, {"title": "3 NEPTUNE", "content": "In this section we describe our dataset generated by the pipeline described in Sec. 4. We first discuss motivating principles, which affect much of the prompt design in the pipeline stage (Sec. 4). Each video contains one or more annotation sets, which consists of a question, an answer to the question and four decoys (which are used for multiple choice evaluation). Our key motivation is that"}, {"title": "4 DATASET CREATION PIPELINE", "content": "An overview of our pipeline can be found in Fig. 1. In order to reduce human effort, we leverage automatic tools to (i) find suitable videos (ii) extract useful signals and then (iii) automatically generate video level captions and QADs. We then send the data to human raters for the final manual verification stages. Our pipeline can be applied to any generic YouTube video. This is unlike existing data pipelines such as those used to create EgoSchema (Mangalam et al., 2023), which relies on human generated captions, SFD (Ghermi et al., 2024) and other movie related datasets, which requires movie titles, loglines and synopses (human-written), or MLVU (Zhou et al., 2024), which re-uses annotations from existing datasets for many of their tasks. This makes the dataset scalable, as YouTube has a constantly growing set of videos. Each stage is described in detail below.\nVideo Selection: We begin with the YT-Temporal-1Bn (Zellers et al., 2022a) dataset. Because this dataset has strong speech and visual alignment, it consists of a lot of videos where 'talking heads' dominate the screen (eg. VLOGs, product placements, etc). We attempt to reduce the number of such videos in order to capture more interesting scenes, objects and actions. This is done by extracting face detections with frontal gaze where face bounding-box height is greater than 20%, and removing videos where more than 30% of frames have such frontal gaze. We then apply safety filters to remove racy, local controversy content etc, as well as applying filters to maximise semantic and person diversity. Details about these processes are provided in the appendix.\nSignal Extraction: For each video we extract the following signals: (i) Frame captions: A visual description of each frame (extracted at 1fps) is obtained from PaLI-3 (Chen et al., 2023b). (ii) ASR: the speech is transcribed using the YouTube API; (iii) Metadata: We obtain the YouTube title and the description for each video; and (iv) Shot boundaries for each video.\nAUTOMATIC VIDEO CAPTIONING\nThe signals described above (frame captions, ASR, title and description, shot boundaries) are automatically combined to create video-level captions in a multi-stage process. Examples of caption quality are provided in the appendix, showcasing details such as visual elements, multiple events, mood and atmosphere, details from the ASR, and even high level feelings and emotions. Video captions are obtained using the following steps:\nShot Visual Captions: Using the shot boundaries, the frame captions are summarized into shot-level descriptions (shot captions) by prompting the same LLM. We then create a script for each video"}, {"title": "5 EXPERIMENTS", "content": "We first introduce the two sets in Neptune and our evaluation metrics and then present evaluations using both baseline and state-of-the-art models."}, {"title": "5.1 NEPTUNE SETS AND EVALUATION METRICS", "content": "Neptune Sets: Because we seeded our dataset from the YT-Temporal-1Bn (Zellers et al., 2022b) videos, we note that it contains some videos where ASR can play a big role in contributing to the video content. In order to create a more challenging visual benchmark, we also provide Neptune-MMH (multimodal human annotated), where we identify videos where vision should play an important role. This is created by using the rater annotations for what modalities are required to answer the question (described in Sec. 4.5), and discarding questions which the raters marked can be solved by audio-only, and consists of 1,171 QADs for 1,000 videos. We encourage the community to evaluate on this harder subset as well.\nEvaluation: We explore two different protocols for evaluation of question answering - multiple choice evaluation (which involves selecting the correct answer amidst 4 decoys), and open-ended evaluation, which involves producing an answer directly without any decoys and assessing answer quality directly. While the former has the advantage of easier metrics (simple accuracy), the latter removes any potential confounding biases in the decoys. In the next section, we outline our process for creating a new open-ended metric called GEM.\nGemma Equivalence Metric (GEM): As discussed in Sec. 2, existing metrics for open-ended QA either lack robustness or rely on proprietary LLM APIs that can change over time. We therefore aim to produce a static open-ended metric. Towards this, we first manually construct a labelled dev-set with 292 (question, reference answer, candidate answer) triplets, with equivalence scores between 0 and 1. See appendix for details on the construction of the dev set. We then benchmark a number of rule-based and model-based metrics on this set in Table 1. To demonstrate the two ends of the scale, we first note that rule-based metrics such as CIDEr (Vedantam et al., 2015) and ROUGE-L (Lin, 2004) obtain F1-Scores of 56.4 and 62.2, while an LLM-based metric using Gemini-1.5-pro (Reid et al., 2024) gets an F1-Score of 72.8 (but is closed-source). Next, we apply static open-source lightweight language models, namely the Gemma family of models i.e. Gemma-2B (Team et al., 2024a), Gemma-7B (Team et al., 2024a) and Gemma-9B (Team et al., 2024b) to judge the answers in a zero-shot setting and find that performance improves with model size, with Gemma-9B bridging the gap well between traditional metrics and the Gemini-1.5-pro based metric. Finally, we fine-tune Gemma-9B on the open-source BEM answer equivalence dataset (Bulian et al., 2022), and find that we obtain a very slight improvement, and hence that it performs the best on our dev-set among the Gemma models. We call the metric obtained with this model Gemma Equivalence Metric (GEM). Note that this metric takes into account the question when comparing whether two answers are equivalent, which is unlike captioning metrics such as CIDEr which omit the question entirely. In Table 4, we report open-ended evaluations using our proposed GEM metric in addition to closed-ended MCQ accuracy. We will release GEM publicly to enable reproducible open-ended evaluations."}, {"title": "5.2 BENCHMARKS", "content": "We describe all benchmarks used below. Implementation details are provided in the appendix.\nBlind Baselines: We evaluate models using a text-only prompt in two settings: (i) we feed only the question, answer and decoys to the model (QAD baseline). (ii) we also feed ASR as an input for a QAD+ASR baseline. This helps identify questions that can be answered by prior or commonsense knowledge, or ASR only without obtaining visual information from video.\nImage Models: We use the BLIP2-T5-XL (Li et al., 2023b) model, which contains a 1B vision encoder (Fang et al., 2023) and a 3B text-decoder (Raffel et al., 2020). We feed the center frame of the video as the visual input, with prompt \"Answer in one letter\" followed by the question and shuffled answer and decoys. We also evaluate some of the video models eg. Gemini-1.5-pro and VideoLLaMA2 as image models, by feeding only the center frame.\nVideo Models: We experiment with 3 different categories of VideoQA models:\n(i) Short Context MLLMs - Video-LLaVA (Lin et al., 2023), and VideoLLaMA2 (Cheng et al., 2024b). We also experiment with a simple socratic JCEF (Just Caption Every Frame) (Min et al., 2024), which consists of a VLM to extract per-frame captions and an LLM to perform reasoning on top of these captions to answer the question.\n(ii) Long Context MLLMs which are open-source, including MA-LMM (He et al., 2024a), MiniGPT4-Video (Ataallah et al., 2024), MovieChat (Song et al., 2023), LLaVA-OneVision (Li et al., 2024a), InternVL2-8B (Chen et al., 2024) and MiniCPM-v (Yao et al., 2024). (iii) Long Context MLLMs which are closed-source, namely the Gemini 1.5 model family (Reid et al., 2024) and GPT-4o (Achiam et al., 2023).\nImplementation Details: For Video-LLaVA (Lin et al., 2023) we feed 8 uniformly sampled frames (resized to a minimum side length of 320 pixels) along with the question. We reimplement JCEF from the original paper (Min et al., 2024) with updated components - i.e. 16 uniformly sampled frame captions obtained using PaLI-3 (Chen et al., 2023a), and feed them as a text prompt to Gemini-1.0-pro along with the question and decoys. For MiniGPT4-Video, we use the public codebase\u00b3 which routes videos longer than 3 minutes to their Goldfish model and those shorter to their older MiniGPT-video model. We evaluate both the Gemini-1.5-pro and Gemini-1.5-flash models described in (Reid et al., 2024). We also experiment with feeding in ASR to the Gemini-1.5-pro model as well. Frame selection is as other models except that MA-LMM has 20 and 120 and MiniGPT4-Video has default 45 with the LLaMA-Video checkpoint. For MA-LMM we feed in 120 uniformly sampled frames. For GPT-4o we use the public API4. More details are provided in the appendix."}, {"title": "5.3 RESULTS", "content": "Results for all the baselines applied to the two Neptune sets (Sec. 5.1) are provided in Table 4. We provide blind baselines and modality ablations in Table 3 for VideoLLaMA2 and Gemini-1.5-pro.\nSingle frame baselines: We examine model performance using the BLIP2 image-only model (Tab. 4) and two video models (VideoLLaMA2 and Gemini-1.5-pro) with only the center frame of the video in Tab. 3. The larger Gemini model outperforms BLIP-2, however performance with only a single frame is much lower than with multiple frames, as expected. We also show results using Gemini-1.5-pro on the first frame of the video in Fig. 4 (right), and find that using the middle frame performs better. VideoLLaMA2 is a short context model, and we find performance saturates at 8 frames.\nModality Ablations: Table 3 shows that performance of Gemini-1.5-pro and VideoLLaMA2 with ASR only as input is higher than performance with multiple video frames on the NEPTUNE-FULL set, but not on the NEPTUNE-MMH set (for MCQ eval). Surprisingly, the best result of VideoLLaMA2 is obtained using ASR only and not providing image frames. In fact, if we provide 16 frames in addition to ASR (last row of the open-source block), performance drops slightly. This may be a result of attention dilution (Coleman et al., 2023), where an increasingly large context distracts the model, causing a drop in performance. For Gemini-1.5-pro on both sets however, the best performance is obtained with both frames and ASR, showcasing the complementary nature of the modalities."}, {"title": "6 CONCLUSION", "content": "We present Neptune, a new benchmark for VideoQA with a focus on multimodal, high-level understanding of long videos. Neptune is created using a scalable pipeline for arbitrary videos that minimizes (though not omits) human verification. Benchmarks are evaluated using MCQ and open-ended evals \u2013 for which we provide a new, open-source metric. Limitations: The dataset may inherit biases of the Gemini model used to generate QADs. While VideoQA is a good proxy for video understanding, our dataset could be further improved by additional annotations \u2013 such as manually annotated temporal grounding, dense captions or entity labels."}, {"title": "A RELATED WORKS", "content": "Here we provide an additional discussion of related works that were omitted from the main paper due to lack of space. The recently released Perception Test (Patraucean et al., 2024) consists of script-based recorded videos with manual annotations focusing on 4 broad skill areas - Memory, Abstraction, Physics, Semantics, however videos are only 23s long (avg). Like Neptune, ActivityNet-RTL (Huang et al., 2024) was constructed in a semi-automatic fashion by querying GPT-4 to generate comparative temporal localization questions from the captions in ActivityNet-Captions (Krishna et al., 2017). CinePile (Rawal et al., 2024) was generated by prompting an LLM to generate multiple-choice questions. Because it is based on movie clips, it can leverage available human-generated audio descriptions. Both ActivityNet-RTL and CinePile cover only limited domains and rely on existing annotations while Neptune covers a much broader spectrum of video types and its pipeline is applicable to arbitrary videos. Our rater stage is lightweight, unlike other works that are entirely manual (Zhou et al., 2024; Fang et al., 2024; Wang et al., 2024). In LVBench (Wang et al., 2024), even the video selection is done manually, and for MoVQA (Zhang et al., 2023b), only the decoys are generated automatically. Another recently released dataset (concurrent with our submission) is the Video-MME dataset (Fu et al., 2024). The motivation of this dataset is similar to ours, namely it covers videos of variable lengths, with 2,700 QADs covering a wide range of different question types. The main difference between Video-MME and Neptune is that the former is entirely manually annotated by the authors, while we propose a scalable pipeline which can be applied to new videos and domains automatically, and can be tweaked to include different question types with reduced manual effort. EgoSchema is the closest work to ours in motivation, but there are some key differences: (i) it is limited to egocentric videos of exactly 3 minutes each, while Neptune covers many domains and follows a more natural length distribution for online videos (16s to 15min); (ii) it relies heavily on manually obtained dense captions for egocentric videos, while our method generates captions automatically too and hence can be easily applied to any video online; and more importantly (iii) EgoSchema also has strong image and linguistic biases, while Neptune mitigates these."}, {"title": "B THE NEPTUNE DATASET", "content": "Neptune covers a broad range of long video reasoning abilities, which are summarised below. These question types are obtained in the Question and Answer generation stage, for which the prompt is provided in Sec. C.2.3. We provide further insights into the motivations of some of the question areas provided in the prompt below.\nVideo Summarisation: Summarise and compare long parts of the video, as well as identify the most"}, {"title": "C IMPLEMENTATION DETAILS", "content": "We choose the YT-Temporal-1Bn dataset (Zellers et al., 2022b) as the source for Neptune, because of its large and diverse corpus, and because of the high correlation between vision and audio transcripts.\nSafety & Content Filters: We filter out videos with less than 100 views, that are uploaded within 90 days, and those tagged by YouTube content filters to contain racy, mature or locally controversial content. We then identify and remove static videos (eg. those that consist of a single frame with a voiceover) by clustering similar frames in a video and ensure that there is more than 1 cluster. We also identify and remove videos comprising primarily of \"talking heads\". To achieve this, we apply a per-frame frontal-gazing face-detector at 1fps and mark the frames where the bounding box height is greater than 20% as talking head frames. Then, we filter out videos where more than 30% of the frames are talking head frames. These thresholds are chosen based on an F1-score on a small dev set of 50 manually annotated videos.\nDiversity Sampling: From the filtered set of videos, we sub-sample 100, 000 videos to boost both semantic and demographic diversity. First, we cluster the videos based on video-level semantic embeddings and tag each video with a cluster id. Second, we tag each video with the perceived age and gender demographic information contained in the video. Third, we obtain a joint distribution of semantics (cluster id) and demographics (perceived age and gender) and apply a diversity boost function (Kim et al., 2022) on the joint distribution. Finally, we sample from videos from this distribution. Fig. 8, shows the down-sampling of over-represented cluster ids before and after applying the filter. We then uniformly sub-sample the videos further to reach the desired dataset size."}]}