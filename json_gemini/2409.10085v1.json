{"title": "A Riemannian Approach to Ground Metric Learning for Optimal Transport", "authors": ["Pratik Jawanpuria", "Dai Shi", "Bamdev Mishra", "Junbin Gao"], "abstract": "Optimal transport (OT) theory has attracted much attention in machine learning and signal processing applications. OT defines a notion of distance between probability distributions of source and target data points. A crucial factor that influences OT-based distances is the ground metric of the embedding space in which the source and target data points lie. In this work, we propose to learn a suitable latent ground metric parameterized by a symmetric positive definite matrix. We use the rich Riemannian geometry of symmetric positive definite matrices to jointly learn the OT distance along with the ground metric. Empirical results illustrate the efficacy of the learned metric in OT-based domain adaptation.", "sections": [{"title": "I. INTRODUCTION", "content": "Optimal Transport (OT) [1], [2] is a mathematical frame- work for comparing probability distributions by finding the most cost-effective way to transform one distribution into another. It measures the \"distance\" between distributions based on the cost of transporting mass from one point to another [3]. In machine learning, OT has been applied in areas such as supervised classification [4], domain adaptation [5], [6], generative modeling (e.g., Wasserstein GANs) [7], and distribution alignment [8]\u2013[11], offering a principled way to compare and align distributions with minimal assumptions about their structure. The Wasserstein distance, derived from OT, provides a more meaningful metric in high-dimensional settings compared to traditional methods like the Kullback-Leibler divergence. OT is also used for tasks like image registration [12], data clustering [13], [14], model interpolation [15], [16], and transfer learning [17].\nOT relies heavily on the ground cost metric [18], [19], which defines the \"cost\" of transporting mass from one point in a source distribution to another in a target distribution. This cost metric essentially captures how \"far\" points are from each other, and it plays a critical role in how the OT problem is solved, as the goal of OT is to minimize the overall transportation cost based on this metric. In many applications, the optimal ground cost metric requires domain knowledge to properly capture the relationships between points in the data. Designing this ground cost often requires deep domain expertise, which is not always available. This manual crafting can be time-consuming, and if the metric is poorly designed, it can lead to sub-optimal transport plans and poor performance in downstream tasks. Learning the ground cost from data is an alternative approach that can overcome the limitations of handcrafted metrics, making OT more flexible and applica- ble across diverse domains without requiring extensive prior knowledge.\nThis paper motivates ground metric learning in OT. Notably, we jointly learn a suitable underlying ground metric of the em- bedding space and the transport plan between the given source and target domains. By doing so, the proposed methodology adapts the ground OT cost to better reflect the relationships in the data, which may significantly improve the OT performance. Our main contributions are as follows:\n\u2022 We propose a novel ground metric learning based OT formulation in which the latent ground metric is parame- terized by a symmetric positive definite (SPD) matrix A. Using the rich Riemannian geometry of SPD matrices, we appropriately regularize A to avoid trivial solutions.\n\u2022 We show that the joint optimization over the transport plan \u03b3 and the SPD matrix A can be neatly decoupled in an alternate minimization setting. For a given metric A, the transport plan \u03b3 is efficiently computed via the Sinkhorn method [3], [20]. Conversely, for a given \u03b3, optimization over A has a closed-form solution. Inter- estingly, this may be viewed as computing the geometric mean between a pair of SPD matrices under the affine- invariant Riemannian metric [21], [22].\n\u2022 We evaluate the proposed approach in domain adapta- tion settings where the source and target datasets have different class and feature distributions. Our approach outperforms the baselines in terms of generalization per- formance as well as robustness."}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "Let $X := \\{x_i\\}_{i=1}^m$ and $Z := \\{z_j\\}_{j=1}^n$ be independently and identically distributed (i.i.d.) samples of dimension d from distributions p and q, respectively. Let $p = \\sum_{i=1}^m p_i \\delta_{x_i}$ and $q = \\sum_{j=1}^n q_j \\delta_{z_j}$ be the empirical distributions corresponding to p and q, respectively. Here, \u03b4 denotes the Dirac delta function. We note that $p \\in \\Delta_m$ and $q \\in \\Delta_n$, where $\\Delta_m = \\{p \\in \\mathbb{R}^m : p^T \\mathbb{1}_m = 1\\}$.\nThe optimal transport (OT) problem [2], [23] seeks to determine a joint distribution \u03b3 between the source set X and the target set Z, ensuring that the marginals of \u03b3 match the given marginal distributions p and q, while minimizing"}, {"title": "III. PROPOSED APPROACH", "content": "For a given source X and target Z datasets, we propose the following formulation to jointly learn the transport plan \u03b3 and the ground metric A:\n$\\underset{\\gamma \\in \\Gamma(p,q)}{\\min}\\underset{A \\succ 0}{\\min} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\gamma_{ij} ||x_i - z_j||_A^2 + \\Phi(A) + \\lambda \\Omega(\\gamma),$ (4)\nwhere the term \u03a6(A) regularizes the SPD matrix A. We note that Problem (4) without \u03a6(A) or with commonly employed regularizers such as $||A||$ or $trace(AD)$ where D is a given (fixed) SPD matrix is not a suitable problem as they lead to a trivial solution with A = 0. In this work, we propose $\\Phi(A) = \\langle A^{-1}, D\\rangle = trace(A^{-1}D)$, where D > 0 is given. Some useful modeling choices of D include: D = I or D = $XX^T + ZZ^T$ or D = $(XX^T + ZZ^T)^{-1}$, where X = $[x_1, x_2, ..., x_m]$ and Z = $[z_1, z_2, ..., z_n]$. Below, we provide two motivations for why the term of (A-1, D) is interesting.\n1) Minimizing the term $trace(A^{-1}D)$ for A only ensures the A-1 tends to 0. In contrast, minimizing the term $\\sum_{i=1}^m \\sum_{j=1}^n \\gamma_{ij} ||x_i - z_j||$ implies that A tends to 0. Minimizing the sum of both the expressions bounds the solution A away from 0 while keeping the norm of A also bounded.\n2) For a given (fixed) \u03b3, the optimality conditions of (4) w.r.t. A (discussed in Section IV) provides the following necessary and sufficient condition for optimal A:\n$A(\\sum_{i=1}^m \\sum_{j=1}^n \\gamma_{ij} (x_i - z_j) (x_i - z_j)^T)A = D$.\nWe note that A which satisfies the above conditions (we discuss this in Section IV) ensures that the covariance of the features of the data points will align with that of D. Hence, setting D = I implies that A promotes the transformed features to become more uncorrelated."}, {"title": "IV. OPTIMIZATION ALGORITHM", "content": "It should be noted that Problem (4) is a minimization problem over the parameters \u03b3 and A. We propose to solve it with a minimization strategy that alternates between the metric learning problem for learing A (for a given \u03b3) and the OT problem for learning \u03b3 (for a given A). This is shown in Algorithm 1. Given \u03b3, the update for A follows from the discussion below and has a closed-form expression. Given A, the update for \u03b3 is obtained by solving a OT problem which can be solved by the Sinkhorn algorithm [3], [20].\nA. Metric learning problem: fixing \u03b3, solve (4) for A\nIn this case, we are interesting in solving the subproblem:\n$\\underset{A \\succ 0}{\\min} \\sum_{i=1}^m \\sum_{j=1}^n \\gamma_{ij} ||x_i - z_j||_A^2 + \\langle A^{-1}, D\\rangle $. (5)\nBelow, we characterize the unique solution of Problem (5).\nProposition IV.1. Given \u03b3, the global optimal A* for Problem (5) is:\n$A^* = C_\\gamma^{1/2} (C_\\gamma^{-1/2}DC_\\gamma^{-1/2})^{1/2}C_\\gamma^{-1/2},$ (6)\nwhere $C_\\gamma = \\sum_i \\sum_j \\gamma_{ij} (x_i - z_j) (x_i - z_j)^T$.\nProof. We first observe that $\\sum_i \\sum_j \\gamma_{ij} ||x_i - z_j||_A^2$ can be written as $\\langle A, \\sum_i \\sum_j \\gamma_{ij} (x_i - z_j) (x_i - z_j)^T \\rangle $. Consequently, the objective function is rewritten as $\\langle A, C_\\gamma \\rangle + \\langle A^{-1}, D\\rangle $, where $C_\\gamma = \\sum_i \\sum_j \\gamma_{ij} (x_i - z_j) (x_i - z_j)^T$. The objective function is convex in A. Furthermore, the characterization of the first-order KKT conditions for (5) leads to the condition $C_\\gamma = A^{-1}DA^{-1}$ which needs to be solved for a SPD A. This is equivalent to the condition ACA = D. From [21, Exercise 1.2.13], this quadratic equation is called the Riccati equation and employs a unique solution for SPD matrices $C_\\gamma$ and D. The solution is obtained by multiplying $C_\\gamma^{-1/2}$ to both the left-hand and right-hand sides and taking the principal square root. This completes the proof."}, {"title": "V. EXPERIMENTS", "content": "We empirically study our approach in domain adaptation scenarios [5], [26], an important application area of optimal transport. In our experiments, we focus on evaluating the utility of the proposed joint learning of transport plan \u03b3 and the ground metric A against OT baselines where the ground metric is pre-determined.\nA. Barycentric projection for domain adaptation\nGiven a supervised source dataset and an unlabeled tar- get dataset, the aim of domain adaptation is to use source supervision to correctly classify the target instances. If the source and target datasets are from the same domain (with same distribution of features and labels), then no adaptation is required and we may use source instances directly. However, if the label (and/or feature) distribution of the source set and the target set differ, then we require adapting the source instances to the target domain.\nOptimal transport (OT) provides a principled approach for comparing the source and target datasets (and thus their underlying distributions). In particular, the learned transport plan \u03b3 can be used to transport the source points appropriately into the target domain. This can be done efficiently using the barycentric mapping [2]. For both (3) and the proposed (4) problems, the barycentric mapping of a source point $x_i$ into the target domain is given by\n$\\tilde{x_i} := \\underset{z \\in \\mathbb{R}^d}{\\arg \\min} \\sum_{j=1}^n \\gamma_{ij} ||z - z_j|| = \\sum_{j=1}^n \\frac{\\gamma_{ij}}{p_i} z_j$. (9)\nThe barycentric mapping (9) maps the i-th source instance xi to $\\tilde{x_i}$, which is a weighted average of the target set instances. The weight $\\gamma_{ij} / p_i$ denotes the conditional distribution of the target instance zj given the source instance xi.\nInference on the target set. Given a labeled source instance {xi, Yi}, the barycentric projection (9) provides a mechanism to obtain its corresponding instance {xi, Yi} in the target domain. Thus, instead of directly using the source points, their barycentric mappings could be used to classify the target set instances for domain adaptation scenarios. In this work, we employ a 1-Nearest Neighbor (1-NN) classifier for classifying the target instances [5], [27]. The 1-NN classifier is parameterized by the barycentric mappings of the labeled source instances.\nB. Experimental setup\nDatasets. We conduct experiments using the Caltech-Office and MNIST datasets.\nSource and target sets. For both MNIST and Caltech- Office, we perform multi-class classification in the target domain using labeled data exclusively from the source domain (as discussed in Section V-A). The source and target sets are created as follows for the two datasets:\nMNIST: Following [27], the source set X is created such that every label has uniform distribution. The target"}]}