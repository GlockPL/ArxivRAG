{"title": "Language Representation Favored Zero-Shot Cross-Domain Cognitive Diagnosis", "authors": ["Shuo Liu", "Zihan Zhou", "Yuanhao Liu", "Jing Zhang", "Hong Qian"], "abstract": "Cognitive diagnosis aims to infer students' mastery levels based on their historical response logs. However, existing cognitive diagnosis models (CDMs), which rely on ID embeddings, often have to train specific models on specific domains. This limitation may hinder their directly practical application in various target domains, such as different subjects (e.g., Math, English and Physics) or different education platforms (e.g., ASSISTments, Junyi Academy and Khan Academy). To address this issue, this paper proposes the language representation favored zero-shot cross-domain cognitive diagnosis (LRCD). Specifically, LRCD first analyzes the behavior patterns of students, exercises and concepts in different domains, and then describes the profiles of students, exercises and concepts using textual descriptions. Via recent advanced text-embedding modules, these profiles can be transformed to vectors in the unified language space. Moreover, to address the discrepancy between the language space and the cognitive diagnosis space, we propose language-cognitive mappers in LRCD to learn the mapping from the former to the latter. Then, these profiles can be easily and efficiently integrated and trained with existing CDMs. Extensive experiments show that training LRCD on real-world datasets can achieve commendable zero-shot performance across different target domains, and in some cases, it can even achieve competitive performance with some classic CDMs trained on the full response data on target domains. Notably, we surprisingly find that LRCD can also provide interesting insights into the differences between various subjects (such as humanities and sciences) and sources (such as primary and secondary education).", "sections": [{"title": "1 Introduction", "content": "Online intelligent education platforms (OIDP) (e.g., ASSISTments, Junyi Academy and Khan Academy) serve as tools for proactive learning [15, 29], providing personalized practice opportunities that enable students to rapidly enhance their mastery of specific concepts. As shown in Figure 1, cognitive diagnosis (CD) [15, 20], as a crucial component of these platforms, aims to uncover students' mastery levels (a.k.a., diagnosis results) of specific concepts and the characteristics of exercises based on their historical response logs. The results can support further customized applications, such as exercise recommendation [8, 37, 38] and computerized adaptive testing [40, 41].\nOver recent years, a diverse array of cognitive diagnosis models (CDMs) has been developed, notably including Item Response Theory (IRT) and the Neural Cognitive Diagnosis Model (NCDM). IRT [32] employs latent factors to represent mastery levels and utilizes the logistic function as the interaction function (IF) to predict student performance on exercises. In contrast, NCDM [35], a pioneering neural-based CDM, replaces the traditional manual IF with multi-layer perceptrons (MLP) and has achieved success in large-scale OIDP. Consequently, neural-based CDMs [1, 5, 23, 36] have rapidly gained prominence. Most existing of them continue the ID-based embedding paradigm, vectorizing students, exercises, and concepts through embeddings and distinguishing them by IDs.\nHowever, as shown in Figure 1, with the increasing diversification in education, students' demands for a variety of subjects are also rising. OIDP now encompasses an increasing number of subjects, ranging from sciences like mathematics and physics to humanities like English and political science. It also caters to a wide range of students, from primary school to university level. However, existing ID-based embedding paradigm forces teachers or researchers to train specific CDMs on specific response logs, causing practical difficulties and inconveniences in applying CDMs across different domains, such as varying subjects and age groups of students. Although some studies [6, 7] have made significant efforts to tackle this task, they still follow the ID-based paradigm or rely on strong assumptions to achieve zero-shot cognitive diagnosis. Nonetheless, these assumptions could be difficult to meet in real educational settings. For instance, TechCD [7] requires that the source domain and the target domain have anchor students, which means that there are some students common to both domains. Similarly, Zero-1-3 [6] requires early-bird students in the target domain to learn the shared cognitive signals, which can then be transferred to the target domain.\nMotivation. Generally, OIDP already possesses response logs from various domains. When new domain's response logs emerge, it often needs to leverage prior knowledge to quickly and accurately provide diagnostic results for students without retraining models. Therefore, in this paper, we concentrate on a more generalized scenario where one only possesses the response logs from the source domain and lacks any information about the target domain. We designate this critical task as zero-shot cross-domain cognitive diagnosis (ZSCD).\nContribution. To this end, this paper proposes the language representation favored zero-shot cross-domain cognitive diagnosis (LRCD). Our core idea is to articulate the behavior patterns"}, {"title": "2 Related Work", "content": "The following sections respectively review the related work, outline the preliminaries, introduce the proposed LRCD, present the empirical analysis, and ultimately conclude the paper. More details of LRCD and experimental setup are provided in the Appendix."}, {"title": "2.1 Traditional Cognitive Diagnosis", "content": "Cognitive diagnosis has been extensively researched for decades in educational measurement [32] or intelligent education [15], aiming to infer students' mastery levels on concepts based on their historical response logs. Item response theory (IRT) and multidimensional IRT (MIRT) [32] employ latent factors to represent students' mastery levels and use logistic functions as item functions (IF) to predict students' performance on exercises. NCDM [35], as a pioneer of neural-based CDM, directly models students' mastery levels on specific concepts with ID-embeddings and employs MLP as IF, which is successful and remarkable. SCD [26] introduces the symbolic tree to explicably represent IF to further improve the interpretability. Subsequently, following the ID-embedding paradigm, MLP-based (e.g., CDMFKC [14], KSCD [23], KaNCD [36]), Bayesian network-based [12], and GNN-based approaches (e.g., RCD [5], ORCDF [24]) have swiftly achieved even greater success. Recently, a large language model based method called FineCD [13] has been proposed to further enhance CD. FineCD incorporates side information such as question statements to realize fine-grained CD. However, due to the absence or scarcity of training data in target domains, CDMs typically focus on training specific models for specific domains, which may hinder their direct application in entirely different target domains (e.g., different subjects or platforms [9])."}, {"title": "2.2 Cross-Domain Cognitive Diagnosis", "content": "Recently, cross-domain cognitive diagnosis has garnered increasing attention, as the proliferation of subjects on OIDP continues, making it challenging to obtain or access abundant domain-specific data during training. TechCD [7] initially proposed the knowledge concept graph to link concepts across different domains, thus effectively transferring student cognitive signals from source domains to target domains. However, the validity of the hand-crafted knowledge concept graph and the feasibility of directly connecting concepts (e.g., concepts from different subjects) across different domains may significantly influence its cross-domain cognitive diagnosis performance. Zero-1-3 [6] capitalizes on early bird students in the target domains to learn shared cognitive signals, which can be transferred to the target domain, thereby enriching the cognitive priors for the new domain. However, the necessity of having early bird students in the target domain may not always be feasible in real educational scenarios. More importantly, both TechCD and Zero-1-3 require an overlap of students between the source domain and the target domain, which does not constitute a completely zero-shot cross-domain cognitive diagnosis. In this paper, we focus on a more generalized and challenging task: zero-shot cross-domain cognitive diagnosis, where there is no overlap of information between the source domain and the target domain. We will elaborate on this task in the following sections."}, {"title": "3 Preliminaries", "content": "Let us consider an intelligent education platform with numerous response logs across M different domains, which can be formulated as R = {R_1, R_2, ..., R_M}. In a specific domain, the response logs consist of a vast number of quadrants, which can be represented as R_m = {(s, e, {c | Q_{e,c} = 1}, Y_{se}) | s\u2208 S_m, e \u2208 E_m, c\u2208 C_m, Y_{se} \u2208 {0, 1}}. S_m, E_m, and C_m denote the sets of students, exercises, and concepts inherent in the domain m, respectively. Q represents the relationship between exercises and concepts, where Q_{e,c} = 1 denotes that the exercise e is related to the concept c. Next, we will present the formal definition of Zero-Shot Cross-Domain Cognitive Diagnosis (ZSCD).\nProblem Definition of ZSCD. Suppose there are M_o source domains' response logs, namely, R_o = {R_1, R_2, \u2026, R_{M_o}}. The goal is to train CDMs on R_o and infer the mastery level of students in the target domains in a zero-shot manner. Namely, there are no overlapping students, exercises and concepts between the source domain and the target domain."}, {"title": "4 Methodology: The Proposed LRCD", "content": "This section introduces the proposed LRCD. It begins by introducing the textual cognitive profiles, specifically how to describe students, exercises, and concepts in different domains. Moreover, leveraging recent advances in text-embedding modules, we can transform these descriptions into vectors within a unified language space. Next, we explore the language-cognitive mapper, a technique designed to map vectors from the language space to the cognitive space. Following this, we explain how these vectors are trained alongside existing diagnostic models. Finally, we discuss how to perform zero-shot inference in a completely new target domain and analyze model complexity. An overview of LRCD is provided in Figure 2."}, {"title": "4.1 Textual Cognitive Profiles", "content": "Due to the sensitivity and sparsity of educational data, the input of CD is very simple, consisting only of response logs, where each log contains students' IDs, exercises' IDs, and concept IDs. Recently, researchers have followed the mature paradigm in user modeling by utilizing ID embeddings to represent the features of students, exercises, and concepts [35, 36]. Initially, these ID embeddings are randomly initialized. Subsequently, through supervised training, each embedding is updated via back propagation. However, this paradigm results in embeddings trained in different domains being on completely different scales and in different spaces, making them unsuitable for direct application in a different target domain.\nTo tackle this issue, we need to consider what data can be unified in response logs across different domains. Our core idea is to analyze the behavior patterns of students, exercises, and concepts in response logs and utilize textual descriptions, which we refer to as the textual cognitive profiles of response logs. We begin by elucidating the profiles of the concepts.\nConcepts' Profiles. As students use OIDP to assess their mastery of concepts across different domains, it is crucial to effectively articulate the profiles of concepts. Different from TechCD [7] who introduces the hand-crafted knowledge concept graph, leveraging concepts as a pivotal bridge to interconnect various domains. Here, we employ the concepts' names as their profiles, as they often reflect their intrinsic interconnections, such as calculus and multivariable calculus. Moreover, these names can be accessed in nearly all OIDPs (e.g., ASSISTments). It can be expressed as\nP_{c_k} = Name(c_k),\nwhere c_k denotes the k-th concept and P_{c_k} is the processed textual profile of c_k. Name represents the concept name.\nExercises' Profiles. Exercises serve as the intermediaries to measure students' mastery of concepts in response logs. By analyzing the response logs, we can infer that it is crucial not only to differentiate exercises based on distinct concepts but also to categorize exercises of the same concept according to their difficulty levels. However, expert labeling of exercise difficulty is a time-consuming and laborious process that often lacks precision. For instance, if a supposedly challenging exercise is administered to a group of high-achieving students, the results may inaccurately suggest that the exercise is not difficult. Consequently, expert labels may fail to accurately reflect the true difficulty of exercises within a specific domain. To address this issue, we utilize the concept names and the average accuracy rates (ACR) of exercises as their profiles. It can formulated as\nACRe; = \\frac{1}{Z_j} \\sum_i Y_{ij}, P_{ej} = [{C_k | Q_{j,k} = 1}, ACRe_j],\nwhere e_j denotes the j-th exercise. ACRe_j denotes the average correct rate of e_j. Z_j denotes the number of students who have practiced e_j. c_k is the related concepts labeled by experts. P_{ej} is the processed textual profile of e_j. Notably, when calculating ACRe_j, we exclusively utilize the available training data to mitigate the risk of information leakage.\nStudents' Profiles. Students' profiles are pivotal to the success of CDMs, as highlighted by recent researchers [11, 17]. We assert that excellent students' profiles should not only consist of their"}, {"title": "4.2 Language-Cognitive Mappers", "content": "Vectorization. Obviously, through the aforementioned methods, we can analyze each response log and convert them into textual cognitive profiles. Although these textual descriptions cannot be directly used in existing CDMs, we can leverage advanced text-embedding modules (e.g., OpenAI-3-large) to transform these profiles into vectors in the language space which can be formulated as\nh_{s_i}^{(1)} = TEM(I_{ij}), h_{e_j}^{(1)} = TEM(P_{e_j}), h_{c_k}^{(1)} = TEM(P_{c_k}), h_{s_i} = \\frac{1}{S_i} \\sum_j TEM(P_{s_i}),\nwhere h_{s_i}^{(1)}, h_{e_j}^{(1)}, h_{c_k}^{(1)} \u2208 R^{1\u00d7d_l} represent the vectors of student s_i, exercise e_j and concept c_k in the language space, respectively. d_l is the dimension of the text embedding (e.g., 3072 in OpenAI-3-large). TEM denotes the text-embedding module (e.g., OpenAI-3-large) which can be seen as hyperparameters in LRCD, we give further experiments in Section. The representation of s_i can be considered as the mean pooling of the text embeddings of each of his or her interactions. Notably, we exclusively utilize the available training interactions to mitigate the risk of information leakage. Detailed information can be found in Section 5.\nMappers. Leveraging the aforementioned text cognitive profiles and advanced text-embedding modules, we can harmonize data from diverse domains into a unified language space. This enables our model to diagnose students' abilities without the necessity of training on target-domain data. However, the language space is substantially disparate from the space of CD, as the former is trained on extensive corpora (e.g., Common Crawl, Wikipedia, and BooksCorpus) that are entirely unrelated to education data. Therefore, we propose language-cognitive mappers to learn the projection of the language space to the cognitive space which can be formulated as\nh_{s_i} = F_s(h_{s_i}^{(1)}; \u03b8_s), h_{e_j} = F_e(h_{e_j}^{(1)}; \u03b8_e), h_{c_k} = F_c(h_{c_k}^{(1)}; \u03b8_c),\nwhere h_{s_i}, h_{e_j}, h_{c_k} \u2208 R^{1\u00d7d} represent the vectors of student s_i, exercise e_j and concept c_k in the space of CD. d is the dimension of space of CD (e.g. 32 in KaNCD [36]). F_s, F_e and F_c indicate the student mapper, exercise mapper and concept mapper where \u03b8_s, \u03b8_e, \u03b8_c are model parameters. Concretely, each mapper follows the identical model architecture. Details can be found in Section 5."}, {"title": "4.3 Training and Inference", "content": "Model-Agnostic. Given that the primary focus of LRCD is to handle ZSCD tasks, we do not design an additional CDM. Instead, we incorporated recent advanced CDMs into LRCD to illustrate its efficacy and adaptability. It can be expressed as\n\u0177_{ij} = MCD (h_{s_i}, h_{e_j}, h_{c}; \u03b8_{CD}),\nwhere \u0177_{ij} is the score prediction of student s_i practice exercise e_j. MCD denotes the integrated CDM (e.g., KSCD [23], KaNCD [36], ORCDF [24]). h_{s_i}, h_{e_j}, h_{c} indicate the representations of s_i, e_j, and concepts, which are the outputs of LRCD. \u03b8_{CD} represents the parameters of the integrated CDM.\nMulti-Domain Training. Consistent with previous papers [35], we employ supervised learning to recover the response logs while simultaneously estimating the whole model parameters (i.e., \u03b8_s, \u03b8_e, \u03b8_c, \u03b8_{CD}). Specifically, we compute the loss between the model's predictions and the actual response scores within a mini-batch and utilize binary cross-entropy (BCE) as the loss function. Notably, LRCD can be trained on multi-domain data, suppose that there are M_o source domains as introduced in Section 3, it can be formulated as follows\nL_{R_m} = - \\sum_{(s,e,c, y_{se}) \u2208 R_m} [y_{se} log \u0177_{se} + (1 - y_{se}) log(1 \u2013 \u0177_{se})],\nL = \\sum_m W_mL_{R_m}\nL_{R_m} denotes the BCE loss of domain R_m. w_m is the weight of loss in domain m. In implementation, we directly choose w as a constant value, namely 1 / R_o. For more sophisticated selections, we defer to future work.\nZero-Shot Inference in Target Domains. Once the training of LRCD reaches convergence, we can undertake zero-shot inference in any target domain. We first frozen the parameters in LRCD. Then, we tackle the response logs in the train data of target domains"}, {"title": "4.4 Discussion", "content": "In this section, we engage in some pivotal discussions about the proposed LRCD, namely time complexity, scalability, and distinction from previous CDMs.\nTime Complexity Analysis. Analyzing the time complexity of the CDM is crucial, as students often wish to quickly receive their diagnostic results after completing exercises. Since we do not design a tailored CDM and instead integrate existing CDMs, we will focus our discussion on the time complexity of obtaining text embeddings and the proposed language-cognitive mappers. Favored by the fast API provided by OpenAI [25], we can obtain all representations of a normal domain within 10 minutes. More importantly, we only need to run this process once and store the results locally. Therefore, the runtime for this aspect is practically negligible. The time complexity of the proposed language mappers is approximately O(d_l d) which depends on the chosen text-embedding module and integrated CDMs. We will provide further details to illustrate the actual training time and inference time for LRCD in Appendix A.1 and Appendix A.2. Notably, LRCD can directly infer students' mastery levels in a new domain with 1,500 students and 50,000 response logs in just 0.1 seconds, making it 406 times faster than model retraining.\nScalability. As we do not design computationally intensive modules (e.g., no need for language model inference [2]), LRCD is both efficient and easy to implement, allowing for the seamless integration of all existing CDMs. Thus, LRCD can handle thousands of students, exercises and concepts with millions of response logs, and we will verify this capability in experiments.\nDistinction with Zero-1-3 [6]. Herein, we primarily focus on discussing the differences between LRCD and Zero-1-3 at the model level, while the differences in tasks are mentioned in Section 2.2. Compared with Zero-1-3, which uses exercise content [16, 21] as exercise features and employs pre-trained CDMs to initialize student representations, we argue that exercise descriptions are highly variable, as questions on the same concept can take numerous forms. Therefore, as proposed in our textual cognitive profiles, we describe students, exercises, and concepts through the analysis of their behavior patterns in the response logs. Therefore, we do not need the pre-trained CDMs, and we can place all features into a unified language space using text embedding, thereby successfully achieving ZSCD."}, {"title": "5 Experiments", "content": "This section first introduces three real-world datasets and evaluation metrics. Subsequently, through comprehensive experiments, we endeavor to substantiate the superiority of LRCD's zero-shot performance across various target domains (e.g., different subjects and different platforms). To ensure the reliability and reproducibility of"}, {"title": "5.1 Experimental Settings", "content": "Datasets Description. We conduct our experiments on three real-world datasets: SLP [22], EDM [4], and MOOC [39]. To ensure that each student has a sufficient number of diagnostic records and meets the requirements of the baseline as well as equipment needs, we exclude students with fewer than 5, 20 and 50 responses, respectively. Moreover, following the paradigm established by [12, 35], we restrict to the first attempt for each exercise to ensure the stability of students' mastery levels. This approach ensures that the data reflect the students' initial understanding, thereby maintaining the static nature of their mastery levels. Importantly, we have uploaded all the processed data to the aforementioned repository. Table 1 provides detailed statistics of those datasets, where \"Average Correct Rate\" indicates the average accuracy of students on exercises, and \"Q Density\u201d represents the average number of concepts per exercise. Moreover, the entry for SLP represents the aggregate values across all subjects (e.g., Math, Physics). Due to space constraints, we provide the detailed values for each subject in Table 8.\nEvaluation and Settings. To evaluate the efficacy of LRCD, we adopt the methodology of previous research by assessing the predictive accuracy of student performance, given that the true mastery levels of students are inherently unobservable in real-world contexts. Consistent with previous studies [35, 36], we validate the accuracy of the diagnostic outcomes produced by CDMs by predicting students' performance on assessments. We employ both performance prediction and interpretability metrics [30, 35] to measure effectiveness. Concretely, for the score prediction metric, given that the task is a binary classification problem [35], we employ the Area Under the Curve (AUC) as our evaluation metric. For the interpretability metric, in alignment with previous methodologies [12], we utilize the degree of agreement (DOA) to assess the interpretability of the inferred mastery levels. For a more detailed explanation of the DOA, please refer to Appendix B.2.\nFor evaluation, we follow the methodology of previous work [17, 36], dividing the response logs of each domain similarly. Specifically, the response logs of each student are partitioned into three parts: 70% for training, 20% for validation, and 10% for testing. During the training phase, we utilize the training data from the source domains to train the model and determine the best hyperparameters using the validation data. During the inference phase, we use the training data in the target domain to infer the students' mastery levels and"}, {"title": "5.2 Subject Level Zero-Shot Student Score Prediction", "content": "In this subsection, we will compare the performance of our proposed LRCD with other baselines for Student Score Prediction at the subject level. Specifically, the source domains and target domains are derived from datasets pertaining to different academic subjects (e.g., Math, English). Here, for brevity, we denote each subject by its initial letter, for instance, M for Math. Consequently, PB-M signifies that the source domains are Physics and Biology, while the target domain is Math. Notably, we categorize different subjects based on their attributes into Humanities (History, Geography, English, Chinese) and Sciences (Physics, Biology) to analyze the influence between subjects.\nResults. As shown in Table 2, we can obtain two key observations. LRCD significantly outperforms other baselines, achieving at least 97.30% of the oracle performance and demonstrating competitive results with NCDM in certain scenarios. This indicates that LRCD is highly effective in subject-level zero-shot cross-domain cognitive diagnosis. TechCD generally performs better than other baselines but still exhibits suboptimal performance. This may be due to the hand-crafted knowledge concept graph, which links completely unrelated concepts across different subjects, potentially hindering its effectiveness in cross-domain scenarios. Moreover, we can find some interesting discoveries: the cross-domain performance is better when the source domain is a science subject compared to when it is a humanities subject, regardless of whether the target domain is a humanities or science subject. This may be due to the greater inherent differences in concepts within humanities subjects, whereas science subjects tend to have relatively smaller disparities. Science data may be more universally applicable in ZSCD. Of course, we cannot rule out the influence of the data size within each subject. Nonetheless, we believe that this warrants further investigation in future work."}, {"title": "5.3 Platform Level Zero-Shot Student Score Prediction", "content": "In this subsection, we will compare the performance of our proposed LRCD with other baselines for Student Score Prediction at the platform level. Specifically, the source domains and target domains are derived from datasets pertaining to different education"}, {"title": "5.4 Student Score Prediction with Overlap Students", "content": "This subsection compares the performance of the proposed LRCD with other baselines for Student Score Prediction on the same platform SLP. Unlike previous experiments, this time we have constructed a new dataset from SLP, referred to as SLP\u2020, where the source domain and the target domain have overlapping students, consistent with the requirements of Zero-1-3 and TechCD. It comprises 107 students, 2,239 exercises, 93 concepts, and 17,955 response logs. The source domains encompass Biology and Physics, whereas the target domain is Math. Detailed implementations of the baselines are provided in Appendix B.3."}, {"title": "5.5 Ablation Study", "content": "To validate the efficacy of the Text Cognitive Profiles (TCP) and Language-Cognitive Mappers (LCM) in LRCD, we conduct an ablation study. We present two ablated versions of LRCD: LRCD-w/o-TCP and LRCD-w/o-LCM. Specifically, the former replaces the TCP with a random vector sampled from a standard normal distribution. The latter omits the use of Mappers and directly employs the obtained text embeddings in the language space. Since the dimension of OpenAI-3-large is 3072, which is excessively large, omitting the proposed LCM would result in high GPU demands. Therefore, we use Bert embeddings as a substitute. Unfortunately, LRCD-w/o-LCM still exceeds memory limits in some cases, which also underscores the importance of the proposed LCM. Therefore, we will only report the results for the successful instances.\nResults. As illustrated in Table 4, it is evident that LRCD markedly outperforms both ablated versions, substantiating the synergistic effect of integrating both modules. Furthermore, each ablated version of LRCD surpasses the best-performing baseline in Table 1, corroborating the efficacy of each individual module. Surprisingly, we find that LRCD-w/o-TCP also achieves commendable performance, demonstrating the effectiveness of the LRCD framework. This underscores the importance of representing students, exercises, and concepts within the unified space for the ZSCD task. The subpar performance of LRCD-w/o-LCM underscores the considerable disparity between the language space and the cognitive space. This highlights the inadequacy of directly utilizing representations from the language space and emphasizes the critical importance of establishing a mapping between the two spaces."}, {"title": "5.6 Scaling Up in Datasets", "content": "This subsection aims to investigate whether expanding the scope of the source domain can enhance the performance of LRCD in the target domain. In Table 5, when our target domain, Math, is included in the training set, we can see that as we continue to add questions from different related domains to the dataset, the model's performance improves steadily. This suggests that adding more related domain questions to the training set can enhance the predictive performance of the target domain in this situation. In"}, {"title": "5.7 Hyperparameter Analysis", "content": "The Effect of TEM. After obtaining the TCP, we utilize advanced TEM to generate representations in the language space. Here, we employ four renowned TEMs (i.e., Bert [3], OpenAI-ada, OpenAI-3-large [25]) for experiments following [18, 19, 27, 28, 31, 33]. As shown in Table 7, within LRCD, using either OpenAI-ada or OpenAI-3-large results in strong performance. We recommend these two options. Notably, Bert also achieves commendable results, making it suitable for resource-constrained scenarios.\nThe Effect of the Integrated CDM. As LRCD is model-agnostic, we can integrate any existing CDMs. In this study, we employ"}, {"title": "5.8 Diagnosis Report Analysis", "content": "Visualization of the Inferred Mastery Levels. This subsection aims to further illustrate the implications of the students' mastery levels inferred by LRCD. We consider Biology and Physics as the source domains and Math as the target domain. Consequently, the inferred mastery levels of students in the source domains are obtained through supervised training, while those in the target domain are inferred without training. Following previous work [36], we employ t-SNE [34], a renowned dimensionality reduction method, to map the mastery levels onto a two-dimensional plane. Firstly, we use different colors to represent different subjects. Then, we use shading to indicate the students' average correct rates, with darker shades representing higher correct rate. Finally, we visualize this using a scatter plot, as shown in Figure 4. We can observe that students with similar average correct rates tend to cluster together. Moreover, the mastery levels of students from different domains are well separated. This indicates that, although LRCD does not explicitly use methods to distinguish different domains and instead trains them together, it can still accurately differentiate students from various domains due to the distinct behavior patterns of each domain.\nStudent Profile Editing. An additional advantage of LRCD is its capability to directly adjust students' mastery levels by editing their profiles, without necessitating data alteration and model retraining. Here, we select a student s_i as an example who has not done any related exercises on the concept of \"Angle\" before editing. We give a detailed ID within the corresponding dataset in Appendix B.6. After LRCD training achieves convergence, we fix the parameters and obtain its current representation h_{s_i}^{(1)}. We then acquire the student's new interactions in \"Angle\" and, using the method described in Section 4.1, derive the new representation h_{s_i}^{(1)*}. Finally, we combine the two representations in a 7:3 ratio and pass them through the student mapper to infer the student's new mastery level. To evaluate the accuracy of the diagnostic results, we assess whether the student's performance on Angle-related exercises has improved. As shown in Figure 5, the third row (i.e., Response Logs) indicates that this student answers all exercises correctly except for e_2. In the second row (i.e., Before Editing), we see the predictions given by LRCD before editing. It is evident that, although the model has not been trained on this student's interactions related to the \"Angle\", it still provides reasonable predictions. In the first row (i.e., After Editing), after editing, our predictions have all improved compared to the original ones. Notably, on exercise e_2, where the student initially got wrong, our prediction has become more accurate. This shows that LRCD can effectively adjust its assessment of a student's abilities through profile editing. This editing capability is highly beneficial in real educational scenarios, allowing students to preview their potential improvements in advance. By doing so, they can select appropriate exercises to focus on, thereby alleviating their academic burden."}, {"title": "6 Conclusion", "content": "This paper proposes a language representation favored zero-shot cross-domain cognitive diagnosis framework (LRCD) to address the limitations of existing CDMs, which often require specific models trained for specific domains. By leveraging textual descriptions to profile students, exercises, and concepts, LRCD transforms these profiles into vectors within a unified language space using advanced text-embedding modules. To bridge the gap between language space and cognitive diagnosis space, we introduce language-cognitive mappers in LRCD, enabling efficient integration and training with existing CDMs. Extensive experiments validate that LRCD achieves commendable zero-shot performance across different target domains and, in some instances, competes with classic CDMs trained on full response data. Notably, LRCD provides intriguing insight into the distinctions between various subjects and educational sources. However, while LRCD shows significant efficacy, further development of more interpretable methods is needed to fully elucidate the mapping process and enhance the framework's applicability in online intelligent education systems."}, {"title": "A Time Comparison", "content": "Here, we use Physics and Biology as the source domains, and Math as the target domain, as an example to explore the comparison of different models in terms of training time and inference time."}, {"title": "A.1 Training Time", "content": "As discussed in Section 4.4, by employing a space-for-time strategy, we can store the obtained text embeddings locally, thereby minimizing additional time requirements. Here, we provide the actual running time of LRCD compared with the baselines in ZSCD. Note that the training time here is the time it takes for the models to reach the optimal AUC. As shown in Figure 6(a), we select KaNCD and OR-KaNCD for our model. It can be seen that although our model takes longer time, it has fine predictive performance compared to other baselines."}, {"title": "A.2 Inference Time", "content": "As we claim in the introduction, when new domain's response logs emerges, OIDP often need to leverage prior knowledge to quickly and accurately provide diagnostic results for students without retraining models. Therefore, inference time, which equates to the system's response time to the user, is crucial. Here, we compare the inference time with the model retraining time.\nAs shown in Figure 6(b), we selected KaNCD and OR-KaNCD as diagnostic methods. For KaNCD, the diagnostic time of our model is 99 times longer than that of the oracle, while OR-KaNCD is 406 times longer. This indicates that our model has good time utility while maintaining great diagnostic performance, and can quickly diagnose \"zero sample\" students."}, {"title": "B.2 Degree of Agreement (DOA)", "content": "Here, we provide a further explanation regarding the degree of agreement. Suppose that the inferred students' mastery levels are represented by M_{as} \u2208 R^{N\u00d7K}, where N denotes the number of students and K signifies the number of concepts. The underlying intuition here is that if the student s_a demonstrates higher accuracy in answering exercises related to the concept c_k compared to the student s_t, then the probability of s_a mastering c_k should be greater than that of s_t. In other words, M_{as,c_k} > M_{as,c_k}. The Degree of Agreement (DOA) is defined as in Eq. (8)\nDOAK = \\sum_{a,bes} \\frac{\u03b4 (M_{as,c_k}, Mass_{b,c_k})}{21 Q_{jk} \\^ \u03c6(j,a,b) \\^ (r_{aj,bj})},\\frac{21Q_{jk} \\^ \u03c6(j,a,b) \\^ I(r_{aj}\u2260r_{bj})}"}, {"content": "As shown in Figure 5, the third row (i.e., Response Logs) indicates that this"}]}