{"title": "On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains", "authors": ["Xun Xian", "Ganghua Wang", "Xuan Bi", "Jayanth Srinivasa", "Ashish Kundu", "Charles Fleming", "Mingyi Hong", "Jie Ding"], "abstract": "Retrieval-Augmented Generation (RAG) has been empirically shown to enhance the performance of large language models (LLMs) in knowledge-intensive domains such as healthcare, finance, and legal contexts. Given a query, RAG retrieves relevant documents from a corpus and integrates them into the LLMs' generation process. In this study, we investigate the adversarial robustness of RAG, focusing specifically on examining the retrieval system. First, across 225 different setup combinations of corpus, retriever, query, and targeted information, we show that retrieval systems are vulnerable to universal poisoning attacks in medical Q&A. In such attacks, adversaries generate poisoned documents containing a broad spectrum of targeted information, such as personally identifiable information. When these poisoned documents are inserted into a corpus, they can be accurately retrieved by any users, as long as attacker-specified queries are used. To understand this vulnerability, we discovered that the deviation from the query's embedding to that of the poisoned document tends to follow a pattern in which the high similarity between the poisoned document and the query is retained, thereby enabling precise retrieval. Based on these findings, we develop a new detection-based defense to ensure the safe use of RAG. Through extensive experiments spanning various Q&A domains, we observed that our proposed method consistently achieves excellent detection rates in nearly all cases.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have achieved exceptional performance across a wide range of benchmark tasks spanning multiple domains [1-3]. However, they also suffer from several undesirable behaviors. For example, LLMs can generate responses that seem reasonable but are not factually correct, a phenomenon known as hallucination [4]. Additionally, due to data privacy regulations such as GDPR [5], direct training on specific data domains may be restricted. This can result in disparities between their acquired internal knowledge and the real-world challenges they encounter, leading to unreliable generation. These challenges can be particularly concerning in domains require extensive knowledge, such as healthcare [6, 7], finance [8] and legal question-answering [9].\nRetrieval-Augmented Generation (RAG) [10-14] has emerged as a promising solution to these challenges by integrating external knowledge to LLMs' generations. The RAG approach typically involves two steps: retrieval and augmentation. Upon receiving an input query, RAG retrieves the top K relevant data from an external data corpus. It then integrates this retrieved information with its internal knowledge to make final predictions. Empirical evidence suggests that LLMs employing the RAG scheme significantly outperform their non-retrieval-based counterparts in knowledge-intensive domains like finance and medicine [12, 14]. For instance, the authors of [14] developed a state-of-the-art benchmark for the use of RAG in the medical domain. The authors observed an increase in prediction accuracy of up to 18% with RAG compared to non-retrieval and chain-of-thoughts versions across large-scale healthcare tasks, utilizing 41 different combinations of medical data corpora, retrievers, and LLMs.\nThe use of retrieved knowledge in RAG has also raised security and privacy concerns, especially when the external data corpus is openly accessible, e.g., Wikipedia [15, 16] and PubMed, or when controlled by potential malicious agents, as demonstrated in the case of multi-vision-LLM agents [17]. For example, recent work has successfully launched data poisoning attacks against the retrieval systems [15, 16, 18, 19]. In these cases, malicious attackers can poison a publicly accessible data corpus by injecting attacker-specified data into it, aiming to trick the retrieval system into retrieving those target data as the top K relevant documents. Consequently, when LLMs make predictions based on the retrieved data, they can be easily targeted by adversaries through backdoor attacks [15].\nWith the empirical successes of these attacks, it is imperative to develop defenses against them. However, existing methods, such as examining the 12-norm of the documents' embeddings, have been shown to be ineffective [16] for detecting poisoned documents. Given the widespread adoption of RAG in safety-critical domains such as healthcare, such safety risks become even more pronounced."}, {"title": "1.1 Main Contributions", "content": "In this study, we investigate the safety risks associated with RAG, specifically focusing on retrieval systems. The contributions are summarized as follows.\nRevealing the safety risks for retrieval systems: A case study for medical Q&A. We demonstrate that dense retrieval systems are vulnerable to what we term as 'universal poisoning attacks' in medical Q&A, across a total of 225 different use-case combinations of corpus, retriever, query, and targeted information. As shown in Figure 1 below, in these attacks, adversaries can append nearly every sort of information, such as personally identifiable information (PII) and adversarial treatment recommendation, to a set of attacker-specified queries. Once these poisoned documents are injected into a large-scale corpus, such as Wikipedia and PubMed, they can be accurately retrieved, often with high rankings, e.g., top 1, using attacker-specified queries. Depending on attackers' goals, these documents will lead to safety risks such as (1) leakage of PII, (2) adversarial recommendations for treatments, and (3) jailbreaking the LLM during the inference stage once they are used as context.\nUnderstanding the vulnerability of retrieval systems in RAG. Recall that the dense retrieval system, matching through semantic meanings, selects documents from the corpus based on their similarity to the input query in the embedding space. We explain the high similarity between the poisoned documents and their associated clean queries based on an intriguing property of the retrievers, which we term as orthogonal augmentation. Here, retrievers f(\u00b7) are mappings from a document to its embedding, a high-dimensional vector. The orthogonal augmentation property states that for any two documents p and q that are close to orthogonal in their embeddings, the concatenated document [q + p] will shift in embedding from f(q) to the direction perpendicular to f(q). In other words, appending an orthogonal document p to q results in an orthogonal movement in the embedding of the augmented document [q + p]. As a natural consequence of this property, it can be shown that the high similarity between the poisoned document and its corresponding clean query is maintained, implying the success of universal poisoning attacks.\nRegarding the orthogonal augmentation property, we highlight that documents which have near-orthogonal embeddings can still be semantically relevant (see Section 4). This ensures that the universal poisoning attacks could succeed even if the attacker-specified targeted information is semantically related to the query. In addition, in the previous case of medical Q&A, we empirically observed that retrieved documents are often not close, in terms of commonly used similarity measurements"}, {"title": "1.2 Related Work", "content": "Retrieval-Augmented Generation (RAG) RAG, popularized by [11], is a widely adopted approach that integrates retrieval-based mechanisms into generative models to improve performance across various language tasks [21]. The basic flow of RAG follows a 'retrieve and read' fashion [10-13, 22-26]. In this fashion, given an input query, one first retrieves relevant data from the external data corpus and then employs generative models as 'readers', for example, by directly appending the retrieved documents to the queries as context and then feeding them as a whole to LLMs for making predictions. Approaches for enhancing the efficient use of retrieved documents to improve the LLMs' reading capability include using chunked cross-attention during generation [12], prompt-tuning [23], and in-context learning [13].\nDense retrieval systems There are two main categories of retrievers: sparse retrieval systems, which match through lexical patterns (e.g., BM25) [27, 28], and deep neural network-based dense retrievers, which match through semantic meanings [29-31]. Due to the near-exact token-level matching pattern, sparse retrievers' performance has been shown to be worse than that of dense retrievers in several domains [32], such as healthcare [14]. There are two popular approaches for training dense retrievers: supervised [33, 34] and self-supervised [29-31]. In the supervised regime, given a paired query and document, the goal is to maximize the similarity, i.e., inner product, between their embeddings. Motivated by advances in unsupervised learning, recent methods have started to apply contrastive learning [35] for training, where they observed improved performance across multiple benchmarks. In our work, in addition to popular retrievers, we also consider a retriever trained with contrastive learning on medical datasets: MedCPT [31].\nAdversarial attacks against RAG/retrieval systems Current approaches for attacking RAG [15, 16] mostly involve poisoning the corpus to deceive the retrieval system [18, 36] into retrieving the poisoned documents for adversarial purposes. The recent work of PoisonedRAG aims to backdoor the RAG by tricking the LLM into generating attacker-specified answers based on the retrieved attacker-crafted documents [15]. The authors develop both black-box and white-box attacks to achieve this goal. In black-box methods, they directly append the context for answering the question to the query and inject it into the corpus, similar to our method in implementation. However, their approaches differ from ours in terms of goals, insights, and application scenarios. First, our objective is to investigate and comprehend the robustness of retrieval systems employed in RAG. We achieve this by injecting various types of information, encompassing both irrelevant and relevant content, into the corpus and evaluating the ease or difficulty of retrieval. Their goal, on the other hand, is to inject only query-relevant context to deceive the LLM's generation process upon retrieval. Second, in terms of insights, we provide explanations on the difficulty/easiness of the retrieval of different kinds of information, which is not covered in their work. Third, we focus on the application domain of healthcare, while they focus on the general Q&A setting."}, {"title": "2 Preliminary and Threat Model", "content": "Notations Denote the set of vocabulary to be considered as V. We denote f : VL \u2192 Rd as the embedding function that maps sentences to the latent space, where L is the maximum allowed words/tokens and d is the dimension of latent embeddings. We use \u2295 to denote the concatenation of sentences. Following the convention, we will use the inner product of the embeddings f(q) \u00b7 f(p), to measure the similarity between two documents p and q. We will use the \u2220(a, b) sign to denote the angle between two vectors a and b.\nAttacker's capability There are three components for the retrieval systems in RAG: (1) data corpus, (2) retriever, and (3) query set. For the data corpus, we assume that the attacker can inject new data entries into it, e.g., by creating a new Wikipedia page or by entering a row of a fake patient's information into an existing medical database. Regarding the retriever, we assume that the attacker can query the retriever models and view the retrieved documents and their associated latent embeddings. However, the attacker can neither speculate nor modify the parameters of the retriever models. For the query set, we assume that the attacker has access to queries of interests. In addition, we also examine the scenario where the attacker lacks access to the exact queries but has access to their semantically equivalent counterparts, which makes the considered threat model even more practical.\nWe provide two examples to demonstrate the feasibility of the above mentioned cases. The first example involves cases where a RAG system retrieves information from a public, large-scale corpus, such as Wikipedia [15, 19] and PubMed. In such instances, anyone, including the attacker, can edit the Wiki page and potentially exploit the threat model discussed above. The second example concerns the multi-modal large language model agents scenarios [17], where an agent aiming to address a question seeks assistance from another agent by retrieving relevant information from the latter's local data corpus. In this scenario, a malicious agent has the capability to inject a few pieces of poisoned documents into their local data corpus in order to poison other agents' decisions if using their poisoned information.\nAttacker's Goal Given an targeted document T \u2208 VS (S < L) and a set of queries Q = {91,92,..., qn} with qi \u2208 VS, the attacker's goal is to ensure that T will consistently be retrieved with high ranking after injecting it into the data corpus, corresponding to attacker-specified queries Q. These types of goals are commonly observed in adversarial ranking/recommendations [18], where an attacker aims to improve the ranking of their targeted information."}, {"title": "3 Exposing the Safety Risks: A Case Study on Medical Retrieval Systems", "content": "In this section, we provide a thorough case study to show that the retrieval in medical Q&A benchmarks are vulnerable to poisoning attacks. We begin by listing the detailed experimental setups."}, {"title": "3.1 Setups", "content": "Query Following [14], we use a total of five sets of queries, including three medical examination QA datasets: MMLU-Med (1089 entries), MedQAUS (1273 entries), MedMCQA (4183 entries), and two biomedical research QA datasets: PubMedQA (500 entries), BioASQ-Y/N (618 entries).\nMedical Corpus Following [14], we select a total of three medical-related corpora: (1) Textbook [20] (~ 126K documents), containing medical-specific knowledge, (2) StatPearls (~ 301K documents), utilized for clinical decision support, and (3) PubMed (~ 2M documents), which consists of biomedical abstracts. Due to limited computation resources, the PubMed we used is a random subset of the total 23M documents. Examples of each corpus and details about them are included in the appendix.\nTargeted Information We consider a total of five types of targeted documents: synthetic personal identifiable information (PII), synthetic medical diagnose information, and adversarial passages generated (from [15]) for answering questions from for MA-MARCO [37], NQ [38], and HotpotQA [39], respectively. We use GPT-3 to evaluate their semantic closeness and conclude that they are semantically distant from each other. Therefore, we believe this setup encompasses a wide range of topics, strengthening the validity of our results. Examples are provided in the appendix.\nRetriever We select three representative dense retrievers: (1) a general-domain semantic retriever: Contriever [29], (2) a scientific-domain retriever: SPECTER [30], and (3) a biomedical-domain retriever: MedCPT [31]. Details regarding these retrievers are included in the appendix.\nAttacking Method Recall that the goal of the attacker is to ensure their targeted information is accurately retrieved with high rankings associated with pre-specified queries. Therefore, to increase the success rate of retrieval, we consider a simple yet effective method in which the attacker directly appends the targeted information to queries. The poisoned documents p\u2081 should be in the form of Pi = [qi Target Information], where qi represents normal query. We also consider the case, where the attacker is unaware of the exact queries but knows their semantically equivalent versions.\nEvaluation Metric Consider a pair consisting of a normal query qi and target information T. This pair is deemed successful if the corresponding poisoned document pi = [qi \u2295T] is among the top K (K\u2265 1) document(s) retrieved by qi. For the results presented in the main text, we set K = 2, and ablation studies on different choices of K are provided in Table 7 in appendix."}, {"title": "3.2 Results", "content": "We report the success rates over a total of 225 combinations of query, corpus, retriever, and targeted information in Table 1 below. For each category of targeted information, we generated three different documents, calculated their success rates, and reported the mean value. The standard deviations are less than 0.07. The interpretation of results in each cell adheres to the same following rule. We use the top-left cell as an example: it represents a success rate of 0.78 achieved using the Corpus: Textbook, Retriever: MedCPT, Query set: MMLU-Med, with PPI as the targeted information.\nSeveral conclusions are summarized: (1) Overall, high attack success rates are consistently observed across different combinations of corpus, retrievers, medical query sets, and target information. This implies that retrieval systems used for medical Q&A are universally vulnerable, meaning that an attacker can insert any kind of information for malicious use cases. (2) Similar attack success rates are observed across different corpora, implying that this vulnerability is consistent across various datasets. (3) Similar attack success rates are observed across different retrievers, suggesting that this vulnerability is shared by all popular retrievers. (4) Attack success rates for certain query sets, e.g., PubMedQA, are consistently higher than others across different corpus and retrievers. We conjecture that this is because the overall length of queries from PubMedQA is significantly longer than others. Hence, the added target information does not affect the overall semantic meanings, leading to high retrieval rates. More detailed discussions are included in the next section. (5) Attack success rates are all on par for different types of targeted information. This is expected since all of them are not semantically closely aligned with the queries. Therefore, their effect on the retrieval are similar. We empirically verified this idea in the next section.\nHow robust is the attack against paraphrasing? There is one potential limitation regarding the above proposed attack through concatenation. For certain use cases in practice, the attacker may not know the exact queries. As a result, there could be no precise match between the queries used for retrieval and the queries used for creating poisoned documents. In the following, we demonstrate that the proposed attack is robust under such a mismatch. We maintain the same setup except that the queries are now rephrased by GPT-4 [2] to reflect the scenarios. We summarize the results in Table 2 below. We observed that the proposed attack remains effective under query paraphrasing, achieving a top-2 retrieval success rate of 0.8 over most cases. These findings highlight even more pronounced security risks for retrieval in medical Q&A, as the attacker now only needs to know queries up to rephrasing in order to launch targeted attacks."}, {"title": "4 Understanding the vulnerability of retrieval systems in RAG", "content": "In this section, we provide insights towards understanding the vulnerability for the retrieval systems. To begin with, we will first present an intriguing property shared by popular retrievers, which we termed as the orthogonal augmentation property. The orthogonal augmentation property states that when two documents q, p \u2208 VL are close to orthogonal in terms of their embedding, the embedding of the (token-level) concatenated one, i.e., f([q\u2295p]), roughly equals f(q) + v, with v\u00b9 f(q) \u2248 0. In other words, this property implies that the shift (in terms of embeddings) from q to [q\u2295p] mainly occurs in directions that are perpendicular to q. As a result, for the inner-product based similarity, the similarity between q and [q\u2295p] will roughly equal to that between q and itself, namely f(q)+f([q\u2295p]) \u2248 f(q)*f(q) + f(q)*v \u2248 f(q)\"f(q). This implies that [q\u2295 p] is likely to be retrieved by q, possibly with high ranking, indicating the success of universal poisoning attacks.\nWe verified the orthogonal augmentation property for several state-of-the-art retrievers in Table 3. In particular, we first collected a set of documents from the MedQA denoted as Q = {q1,..., qn}. Next, we selected several other sets of documents P\u2081 = {Pi1,..., Pin} with varying lengths and similarities to Q. We report a total of four similarity measurements and for each measurement, we report both the mean and the variance. We observed that when the inner product between the embeddings of the two documents f(q)\u0f0b f(p) decreases, the angle between f([q\u2295 p]) and f(q) tends to be around 90\u00b0, and the inner product also decreases, which corroborates the orthogonal augmentation property. Additionally, we observe that Contriever is sensitive to the length of the document; that is, the longer the document, the larger the l2-norm of its embeddings, whereas such a phenomenon is not observed for MedCPT.\nOne potential caveat in using the proposed orthogonal augmentation property to explain the success of our attack is that the embeddings between the query and targeted documents need to be close to orthogonal. However, we highlight that closeness to orthogonality between embeddings does not imply that their associated documents are semantically irrelevant. For example, we randomly sampled two non-overlapping batches of questions from the MedQA dataset and found that the angle between their embeddings is around 70\u00b0. Yet, these batches of queries are all semantically related to biology research questions.\nOn the similarity of clean retrieved documents Previously, we mainly focused on the similarity between the query and the poisoned document. We now shift the focus to another crucial factor contributing to the success of universal poisoning attacks: the similarity between the query and its clean retrieved documents.\nWe present the similarity measurements between the query and the clean retrieved documents (from the corpus Textbook) in Table 4 for Contriever and MedCPT, respectively. For each query, we calculate its similarity, both the cosine similarity and the inner product, with the K = 5-nearest neighbor retrieved documents from the corpus Textbook, and report the mean and standard deviation. Additionally, we also report the value of the inner product between a query and itself for the purpose of comparison. We observed that both the inner product and the cosine similarity are low across all"}, {"title": "5 New Defense", "content": "In this section, we propose a detection-based method to defend against the proposed universal poisoning attacks. We consider a scenario in which the defender, such as an RAG service provider, has full access to retrievers. They collect documents from public websites, integrate them into their data corpus, and provide services using both the retrievers and the updated data corpus. The defender's objective is to develop an algorithm capable of automatically detecting potential adversarial documents to be incorporated into their data corpus. Without loss of generality, we assume that the defender already has a collection of clean documents associated with a set of targeted queries to be protected, which serve as an anchor set (denoted as A = {a1, ..., a|A|}) for detection.\nWe now formally introduce our new defense method. Recall from previous discussions that the wide-ranging-scale success of our proposed universal poisoning attacks is owing to two factors: the consistently high similarity between poisoned documents and queries due to the intriguing property of retrievers, and the low similarity between queries and clean retrieved documents. In fact, the latter property also implies that queries and their retrieved clean documents tend to be orthogonal. As a result, the poisoned document also tends to be perpendicular to clean documents. This orthogonal property motivates us to consider using distance metrics that reflect the distribution of the data, such as the Mahalanobis distance, to detect the poisoned documents.\nDue to the high-dimensional nature of the embeddings, calculating the Mahalanobis distance can be challenging. This is because the sample covariance matrix ensued can be numerically unstable in large data dimensions [40, 41], leading to an ill-conditioned matrix that is difficult to invert [42]. To address this issue, we consider regularizing the sample covariance matrix through shrinkage techniques [43, 44]. In detail, we conduct shrinkage by shifting each eigenvalue by a certain amount, which in practice leads to the following, s(X) \uc2a5 (X \u2013 \u03bc)\u03a4\u03a3\u00b9(X \u2013 \u03bc), where \u03bc is the mean of {f(ai)}A, with ai \u2208 A, (A is the anchor set defined earlier in this section) and \u03a3\u03b2\u2261 (1 \u2212 \u03b2)S + d\u00af\u00b9\u1e9eTr(S)Ia, with S being the sample covariance of {f(ai)}, Tr() is the trace operator, and \u03b2\u2208 (0, 1) is the shrinkage level. We select \u1e9e by cross-validation, with an ablation study in Appendix C.2.\nWe tested the proposed method for filtering out the poisoned documents over the previous setups. The threshold for filtering is set at the 95th quantile of the {s(f(ai))}4. We present some case studies in Figure 3 below. The complete results are included in Figure 4 and Figure 5 for PubMed (in the appendix), Figure 6 and Figure 7 for StatPearl (in the appendix), and Figure 9 and Figure 8 for Textbook (in the appendix). We observed that, in almost all cases, the proposed method achieves near-perfect detection. Additionally, we observed that the l2-norm defense is effective when using the Contriever. One potential reason may be that it is sensitive to the total length of the documents. Furthermore, we applied our proposed method to one of the state-of-the-art poisoning attacks [15] and obtained a filtering rate greater than 95%, indicating the wide applicability of our proposed defense."}, {"title": "6 Conclusion", "content": "This paper studies the vulnerability of retrieval systems in RAG. We first demonstrate that retrieval systems in RAG for medical Q&A are vulnerable to universal poisoning attacks. Next, we provide two-fold insights towards understanding the vulnerability: (1) by identifying an intriguing property of dense retrievers, and (2) revealing the relatively low similarity of the clean retrieved documents. Based on these findings, we develop a detection-based defense, achieving high detection accuracy."}, {"title": "Limitations & Future Work", "content": "There are several potential directions that can be further explored. First, the experimental studies are only conducted on medical Q&A. Investigating the use of RAG in other knowledge-intensive application domains to determine if they suffer from similar security risks is important. Second, the empirical results showed that the retrieved clean documents are not close to their associated queries, leaving a large gap that can be exploited for adversarial attacks. An interesting question is whether we can develop methods to align the queries and the documents in the corpus to enhance retrieval quality. Third, although the developed defense can effectively detect a large portion of poisoned documents, its usage is limited to a set of targeted queries instead of arbitrary queries. Another important direction is to extend the current method to arbitrary-query sets."}, {"title": "Boarder Impacts", "content": "There are both potential positive and negative societal impacts of this work. Potential negative impacts include the possibility that an adversary could apply the proposed attack to other retrieval systems. On the other hand, we anticipate that there will be many more positive societal impacts of this work: (1) highlighting the need for vigilance regarding security risks when applying RAG in safety-critical domains; (2) providing several insights into understanding these potential safety risks; and (3) introducing a new, effective defense that can be used for detecting poisoned documents."}, {"title": "Appendix", "content": "In section A, we list more implementation details including: computing resources, examples of datasets and others. We provide experimental results are omitted from the main text due to space limit in Section B. In section C, we provide ablation studies regarding different choices of hyperparameters used in the paper."}, {"title": "A Details on experimental setups", "content": ""}, {"title": "A.1 Computing Resource", "content": "We calculate the embedding vectors on a machine equipped with an Nvidia A40 GPU. We conduct the nearest neighbor search on a machine with an AMD 7763 CPU, 18 cores, and 800 GB of memory. To facilitate efficient search, following convention, we employ the Faiss package [45]."}, {"title": "A.2 Details on experiments", "content": ""}, {"title": "A.2.1 Corpus", "content": "Following [14], we provide the statistics of used corpora in Table 5 below. We note that due to limited computational resources, we use a randomly sampled subset of PubMed with around 2 million documents. For the other two datasets, we use the complete versions."}, {"title": "A.2.2 Query", "content": "Following [14], we provide the statistics of used query in Table 6 below. We also provide some examples of these quetions in the following.\nWe show some examples."}, {"title": "A.2.3 Targeted Information", "content": "Some examples of the targeted information are:\n\u2022 (NQ): Renowned authors, Henry Roth and Robert Erskine Childers contributed significantly to literature. Roth, born in England, wrote classics. On the other hand, Childers hailed from Ireland, also producing notable works;\n\u2022 (PII): Bob is currently a graduate student at XX University, majoring in Accounting. Bob's email address is Bob@gmail.com, and Bob's phone number is 123-456-7890.\n\u2022 (MS-MARCO): In the unique Dutch tradition, people celebrate Oranjegekte or Koningsdag wearing unusual purple clothing. This distinctive practice marks their national holiday with a vibrant purple sea of natives.\n\u2022 (HotpotQA): Renowned authors, Henry Roth and Robert Erskine Childers contributed significantly to literature. Roth, born in England, wrote classics. On the other hand, Childers hailed from Ireland, also producing notable works."}, {"title": "A.2.4 Implementation Details", "content": "In this section, we list several important implementation details that are omitted previously.\nZero-Shot and Question-Only Retrieval Following [14], only questions are used during retrieval; answer options and demonstrations (used for in-context learning for generation) are not provided as input. We present an ablation study to explore the scenario where both questions and their corresponding choices are used for retrieval, and demonstrations are included in the queries."}, {"title": "B Omitted experimental results", "content": "Detection Results We test the proposed detection method over the previously mentioned 225 attacks cases and present results of them in Figure 4 and Figure 5 for PubMed, Figure 6 and Figure 7 for StatPearl, and Figure 9 and Figure 8 for Textbook. In each plot, we show the results of detecting poisoned documents created using three different kinds of targeted information. (The cases for the other two types of targeted information are similar.) We observed that the l2-norm fails to detect those poisoned documents, with a detection rate of less than 50% in all cases. However, our method achieves a consistently high detection rate of over 98%, indicating the widespread effectiveness of our proposed method.\nIn terms of the detection performance when using the Contriever as the embedding model, we observed that the detection performance of the l2 norm under Contriever is roughly on par with our proposed method, achieving over 95% detection rates in all cases. One potential reason behind these findings is that the Contriever is sensitive to the length of the documents. Specifically, the larger the length of the document p, the larger the corresponding embedding || f(p)||2. Given the fact that the overall length of retrieved documents and the poisoned documents are (statistically) different, their 12 norm also tends to be distinct. As a result, the l2-norm-based defense tends to be effective."}, {"title": "C Ablation Studies", "content": "In this section, we provide ablation studies on different hyperparameters used in experimental results."}, {"title": "C.1 Top 1 attack retrieval rates", "content": "We report the top 1 attack retrieval success rates in Table 7, with standard deviations less than 0.1. Overall, the success rates only slightly decrease compared to the top 2 results presented in the paper, which is reasonable. These findings highlight that medical Q&A is extremely vulnerable to poisoning attacks and thus requires robust defense mechanisms."}, {"title": "C.2 On the shrinkage level \u03b2", "content": "In this section, we provide an ablation study on the choices of \u1e9e used in calculating the proposed distances. We summarize the results in Table 8 below, with standard errors within 0.08. We observed that as \u1e9e increases, the detection rate begins to decrease. This is reasonable since the covariance tends to shrink towards an identity matrix and hence loses the ability to capture the data's distribution. In practice, we suggest the defender employ cross-validation-based techniques to select the optimal B for detection."}]}