{"title": "A Three-Stage Algorithm for the Closest String Problem on Artificial\nand Real Gene Sequences", "authors": ["Alireza Abdi", "Marko Djukanovich", "Hesam Tahmasebi Boldaji", "Hadis Salehi", "Aleksandar Kartelj"], "abstract": "The Closest String Problem is an NP-hard problem that aims to find a string that has the minimum\ndistance from all sequences that belong to the given set of strings. Its applications can be found\nin coding theory, computational biology, and designing degenerated primers, among others. There\nare efficient exact algorithms that have reached high-quality solutions for binary sequences. How-\never, there is still room for improvement concerning the quality of solutions over DNA and protein\nsequences. In this paper, we introduce a three-stage algorithm that comprises the following process:\nfirst, we apply a novel alphabet pruning method to reduce the search space for effectively finding\npromising search regions. Second, a variant of beam search to find a heuristic solution is employed.\nThis method utilizes a newly developed guiding function based on an expected distance heuristic\nscore of partial solutions. Last, we introduce a local search to improve the quality of the solution ob-\ntained from the beam search. Furthermore, due to the lack of real-world benchmarks, two real-world\ndatasets are introduced to verify the robustness of the method. The extensive experimental results\nshow that the proposed method outperforms the previous approaches from the literature.", "sections": [{"title": "1. Introduction", "content": "The Closest String (CS) problem involves finding a string that is closest to a given set of strings; the\nterm \"closest\" usually refers to the minimum Hamming distance but may refer to the other sequential\nmeasures. CP problem has many applications in coding theory [1], and computational biology [2].\nMore specifically, this problem is used in designing drugs, degenerated primers, diagnostic probes, and\nfinding gene clusters [2, 3, 4, 5, 6, 7]. CS problem is also known as the Hamming Center problem and\ncomes with various versions of different computational complexities. It is known that the CS problem\nis NP-hard in general [2], while its decision problem variant is NP-complete [8]. In the decision version\nof the CS problem, the aim is to answer a binary question \"Does the input set of string have a closest"}, {"title": null, "content": "string with maximum distance of d?\". The general version of the CS problem seeks a string that is\nas close as possible to all given input strings. This paper tackles the general form of the CS problem\nwith an arbitrary set of strings. Let S = {$1,$2,\u2026\u2026,Sn} be the set of strings over a finite alphabet\n\u2211 = {01, 02,\u2026\u2026, \u03c3m}, where n and m refer to the number of strings and alphabet set cardinality,\nrespectively. Without loss of generality, we assume that all strings have the same length (L). The\nclosest string is a string with length L that has the minimum Hamming distance from strings in S.\nCS problem has attracted many researchers to solve it exactly as well as heuristically. Gramm et\nal. [9] proposed an exact fixed-parameter algorithm that runs in linear time assuming the Hamming\ndistance is specified and fixed. Meneses et al. [10] developed an integer linear programming to solve\nthe CS in general case. This algorithm has been efficient in finding an exact solution in a short time\nfor small and near-moderate instances. However, a huge amount of time is requested for solving larger\ninstances, specifically for random strings with n > 30 and L > 800. In 2011, Liu et al. [11] introduced\nan exact algorithm for solving the CS problem for the particular case when n 3 and\nAmong the exact methods, the ILP model proposed by Meneses et al. [10] is the most effective one\nand extremely efficient for binary strings.\nWhen it comes to DNA and protein sequences, that are much larger and more complicated to\nsolve, exact methods perform poorly. Therefore, approximation algorithms are widely applied. For\nexample, approximation algorithms with approximation guarantees are proposed [1, 2, 12] and de-\nspite their speed, they showed poor performance in finding near-optimal solutions. (Meta) heuristic\nmethods instead, could find near-optimal solutions in a reasonable time, but they do not guaran-\ntee optimality in general. Nevertheless, the heuristic methods outperform the previously mentioned\nmethods when applied to large DNA and protein sequences. In 2005, Liu et al. [13] suggested using\ngenetic and simulated annealing algorithms to solve the CS problem. Moreover, they implemented\nthe parallel version of the algorithm and showed that the parallel genetic algorithm is superior when\ncompared to the others. Later, Gomes et al. [14, 15] proposed a simple heuristic along with its par-\nllel version. In 2008, Liu et al. [16] proposed a hybridization of genetic and simulated annealing\nalgorithms and combined their metrics to solve the CS problem. However, their algorithm is only\ntested for binary strings. Later in 2010, Faro and Pappalardo [17] introduced an algorithm based\non the Ant Colony Optimization method which excelled over all former approaches. Mousavi and\nEsfahani [18] suggested using the greedy randomized adaptive search procedure (GRASP) algorithm\nfollowed by a new heuristic function inspired by probability theory to solve the general case of the\nCS problem. In 2011, Mousavi [19] utilized a beam search as a part of a hybrid meta-heuristic and\nalso, while modifying the heuristic function from [18] to get a more enhanced one. Liu et al. [11]\nin their other work from 2011, built on their heuristic function from [20] to design an effective lo-\ncal search. In 2014, Pappalardo et al. [21] proposed a hybrid of a greedy algorithm and simulated\nannealing (GWSA) to solve the CS problem. They first generated an initial solution by a greedy\nwalk algorithm, next, they fed the output of the greedy walk algorithm to a simulated annealing\nalgorithm. GWSA excelled over all former approaches but delivered high running time. Recently, Xu\nand Perkins [22] proposed using Wave Function Collapse (WFC) to solve the CS problem, claiming"}, {"title": null, "content": "it outperforms all other approaches. However, they did not compare their method with the GWSA.\nThe WFC is a lightweight method with very low running time. First, they constructed a frequency\ntable based on the repetition of characters in strings. Second, they identified the string with the\nmaximum Hamming distance. Finally, according to the frequency table, they assigned a character\nto the solution. They repeated the second and the last steps until a solution of complete length was\nobtained. This method is particularly suitable for solving the CS problem when dealing with protein\nsequences. In summary, the WFC is considered the state-of-the-art heuristic method for solving the\ngeneral form of the closest string problem."}, {"title": null, "content": "Contributions of this paper are as follows.\n\u2022 A Three-Stage Algorithm (TSA) to address the CS problem is designed. In the first stage of the\nalgorithm, a novel pruning method that enhances both the speed and quality of the algorithm is\nperformed. The next phase employs a time-restricted variant of beam search on the restricted\nsearch space. A new heuristic function based on expected distance score to guide the beam\nsearch is designed. In the last stage, an efficient local search is executed to refine the solution\nobtained by the beam search.\n\u2022 We have created two real-world datasets and discussed the robustness of the approaches. The\nfirst one consists of the well-known TP53 gene nucleotide, and the second dataset comprises\nthe nucleotides of two segments of a common flu variant across different hosts to check their\nconservation levels.\n\u2022 Extensive experimental evaluation shows that our proposed algorithm (TSA) outperforms the\nstate-of-the-art approaches over all five benchmark sets.\nThe rest of this paper is structured as follows. In Section 2, we provide several definitions and\nformulations required for the subsequent sections. Following that, we introduce details of our algo-\nrithm in Section 3. Section 4 discusses the experimental results along with statistical reports. Finally,\nSection 5 concludes the paper by additionally providing some outlines for future work."}, {"title": "2. Basic Definitions and Preliminaries", "content": "This section provides the fundamentals and the definitions essential for understanding the major\ncore of the paper.\nGiven is a set S = {$1,82,...,sn} of input strings over a finite alphabet \u2211 = {01,02,...,\u03c3m},\nwhere n and m refer to the numbers of input strings and alphabet cardinality, respectively. Assume\nthat all strings in S have the same length L. Let s denotes the jth character of ith string in S, while\ns denotes the leading character of string si. The notion of level l (1 \u2264 l < L) takes all Ith characters\nover input strings in S. For two integer values 1 \u2264 i \u2264 j \u2264 |s|, by s[i, j] we denote the continuous\npart of strings (sub-string) that starts with the character at position i and ends with the character\nat position j. The frequency of a character (\u03c3) at level l is denoted by fi(\u03c3), i.e. the number of"}, {"title": null, "content": "occurrences of \u03c3at level 1. Let S[I] = {si[1, 1] | i = 1, . . ., n} be the set of prefix strings of each input\nstring w.r.t. position l\u2208 {1, ..., L}.\nIn the closest string problem, the Hamming distance (hd) represents the distance between strings\nand the proposed solution. It is computed by counting the differing positions between each string\n(si) and the solution, with the largest of these distances representing the solution's overall Hamming\ndistance. Symbolically, for any two strings (81) and (s2) of equal length (L), the Hamming distance\nis defined as in Eq.1.\nhd(81, 82) = \u2211 \u03b4(81, 85)\ni=1\n\u03b4(ch1, ch\u2082)\u2190{\n1 ch1 \u2260 ch2\n0 ch\u2081 = ch2\n}\n(1)\nTo facilitate the explanation of our expected distance heuristic in Section 3.3, let hd(s1,82):=\n\u03a3=1(1 \u2013 \u03b4(81, 82), denote the complementary score of the Hamming distance by hd(s1,82), which\nrefers to the number of equal characters between the two strings at the same positions. Note that\nhd(81,82) = L - hd(81,82). In essence, the hamming distance between string s and a set of input\nstring S is calculated by\nhd(s, S) = max{hd(s, si) | i = 1, ..., n}.\n(2)\nNote that since we solve the CS problem in a constructive manner where our solutions are partial,\nwe explain the hamming distance calculation for a partial solution s\u00ba with length of |sp| = lp. That\nis, the hamming distances is calculated applying Eq. (2) as the score hd(s\u00ba, S[lp]). One can easily see\nthat hd(sp) is equal to lp \u2013 hd(sp, S[lp]).\nBeam Search (BS) is a heuristic tree-search algorithm that works in a limited breadth-first manner.\nIt was proposed by Hayes et al. [23] and applied in the context of the speech understanding problem.\nThe BS and its variants are widely applied to solve various optimization problems, e.g., scheduling and\nmachine translation [24, 25]. In the context of string problems from bioinformatics, it has been proven\nas one of the most efficient techniques; for example, it is successfully employed to solve the prominent\nLongest Common Subsequence problem [26]. A specific number (\u03b2 > 0) of nodes based on a heuristic\nevaluation is selected at each level of the search to be further processed. BS uses parameter \u1e9e to\nprovide the trade-off between completeness and greediness. As the value of \u1e9e increases, the runtime\nof the algorithm and the chance of finding the optimal answer gets higher and vice versa. For the\nCS problem, BS starts with an empty solution and constructs solutions by adding letters from \u03a3\nto the most promising \u1e9e nodes, which correspond to (actions of) appending these letters to the end\nof respective partial solutions, until a complete node is reached. Complete nodes are those whose\ncorresponding partial solutions cannot be further expanded. In the context of CS problems, complete\nnodes are solutions of length L.\nFor the sake of clarity, consider the S {$1 = abaaabbaba, s2 = abababaabb} and \u2211 = {a,b}.\nHere, n, L, and m are 2, 10, and 2, respectively. For a partial solution x = ababa with length lp = 5,\nthe hamming distance is calculated based on the first 5 characters of all strings i.e., s\u2081 = abaaa, and"}, {"title": null, "content": "82 = ababa, which hd(x, s\u2081) = 1, and hd(x, s2) = 0. Also, hd(x, s\u2081) = 4, and hd(x, s2) = 5. In\naddition, Sa denotes the fourth character of the second string which is b. Furthermore, the frequency\nof character a and b at level 1 (f1(a) and f\u2081(b)) is 2 and 0, respectively."}, {"title": "3. The Proposed Method", "content": "In this section, the Three-Stage Algorithm (TSA) is designed to address the CS problem, pseu-\ndocoded by Algorithm 1. Initially, the algorithm pre-processes the set of effective actions at each\nlevel of BS employing the pruning mechanism, which serves to diminish the search space. Sub-\nsequently, it employs the expected distance heuristic function to guide the Time-Restricted Beam\nSearch (TRBS) [27] towards a solution of reasonable quality. Lastly, the obtained solution is passed\nto the new local search algorithm to further refine its quality.\nTo summarize, the methodology applies the three core stages: (i) the novel pruning method described\nin Section 3.1 to support effective decision-making; (ii) the TRBS algorithm specifically tailored for\nthe CS problem provided in Section 3.2 whose the search is guided by the new expected distance\nheuristic, supported by the associated tie-breaking strategy; see Section 3.3; (iii) a new local search\nalgorithm detailed in Section 3.4."}, {"title": "3.1. A Novel Pruning Method for the Closest String Problem", "content": "Due to the NP-hardness of the CS problem, the state space grows exponentially with the instance\nsize. The following question arises: Should we regard all members of \u2211 as candidates at level\nl\u2208 {1,..., L} for a solution? To answer this question, we have analyzed the exact solution for 50\ninstances with a length of 100 and it turned out that in most cases, the selected character was from\na specific subset of characters. Hence, we decided to design a pruning method to determine those\nspecific subsets of alphabets and consequently, improve the quality of solutions and running time of\nthe BS approach. To describe the pruning method, let us first define the notion of 'rank 1' (R1)\nand 'rank 2' (R2). For the following list of tuples [(A, 10), (B, 12), (C, 10), (D, 7), (E, 1), (F, 6), (G, 9)]\nwhere the first coordinate associates a character and the second coordinate refers to the number of\noccurrences of that letter, the R1 assigns the set of characters with the highest second coordinate,\nwhich here would be B. The R2 contains the characters with the first and second highest assigned\nvalues; in this example, the first highest value is given to B, and the second highest value (10) is\nassigned to A and C. As the values for both of them are the same, both are assigned to R2. Thus,\nthe R2 contains the three characters, B, A, and C.\nTo clarify on a concrete instance, consider Table 1, S = {$1,$2,...,S8} whose the length of input\nstrings are 10. Now, we should determine the R1 and R2 for each l \u2208 {1,...,L}. To do this, we\nuse the frequency of characters at each level 1. For example at the fixed level 1 = 6, the frequency of\ncharacters is as follows, f6(X) = 2, f6(K) = 3, f6(E) = 2, f6(C) = 1, and the frequency of the rest of\n\u2211 members are 0. According to the introduced notation, the most frequent characters are assigned\nR1, which is K. For identifying the R2 set, we add the first and second most frequent characters"}, {"title": null, "content": "here; thus, K is the first, and since the X and E's frequencies match, both of them are considered\nas the second most frequent characters. Hence, the R1 set for (l = 6) is {K}, and the R2 set for\n(1 = 6) is {K, X, E}. It is notable that after obtaining ranks, these pairs (R1, R2) are stored in a\nlist of alphabets for each l, called the ranked alphabet (p) which are subsequently passed to the BS\nprocedure.\nFor example, in Table 1, consider we want to use R2, so, our solution's first character could only\nbe 'M' or 'C'. During the preprocessing phase and before obtaining the CS for a set of input strings, it\nis essential to identify the proper rank that will be leveraged. The choice between R1 and R2 should\nbe guided by the inherent characteristics of the input strings in question, as this decision is dependent\non specific conditions. A simple way [28] to determine the most appropriate rank for a given set of\nstrings is to initially execute the BS with a small (trial) Bt value using both ranks. Subsequently, the\nalgorithm runs with a larger (actual) \u1e9e value using the rank that yielded a smaller hamming distance.\nWhen the hamming distances are equal for both ranks, the option is selected randomly. This pruning\nmethod could be used in all methods that tried to solve the CS problem and it is not limited to our\napproach."}, {"title": "3.2. Time Restricted Beam Search for the CSP", "content": "To explore the search space of the CS problem effectively, a time-restricted version of the BS\napproach is employed. This version, therefore called the Time-Restricted Beam Search (TRBS) [27],\nhas proven to be more robust than the basic BS variant with a provided good trade-off between greed-\niness and completeness [27]. Moreover, it addresses the problem of adjusting the value of \u1e9e parameter\ndynamically within the allowed execution time (tmax). Note that this is not the characteristic of the\nbasic beam search. At each major iteration of the algorithm, TRBS identifies \u1e9e value depending on\nthe remaining runtime and the value of \u1e9e used in the previous iteration. The details of computing \u03b2\nat iteration I are given by Eq. 3.\n\u03b2\u2190{\n[\u03b2. 1.1]if trem/trem \u2265 1.1;\nmin([\u03b2/1.1\u300d,150)if trem/trem \u2264 0.9;\n\u03b2otherwise.\n}(3)\nHere, trem denotes the remaining time to reach the tmax and trem is the expected remaining time\nwhich for CS problem is equal to titer (L\u2212 1). In other words, trem estimates the remaining time\nby multiplying the last iteration time (titer) with the remaining number of steps to achieve a leaf\nnode. In the context of the CS problem, since the length of each leaf node (a solution) is equal to\nL supposing the BS performs at the current level 1, the remaining number of BS iterations would be\n(L-1)."}, {"title": "3.3. An Expected Distance Heuristic for CSP", "content": "This section is devoted to a constriction of the expected distance heuristic (EX) to evaluate nodes\nof the TRBS. Additionally, an effective way of breaking ties between nodes with the same expected\ndistance score values is proposed.\nFirst, we explain the term of an expected solution to the CS problem. For each level l \u2208 {1, ..., L},\nwe determine the most frequent letter across strings from S. Assume that the information is kept\nin the (sequential) structure expected_sol, where expected_sol[l] stores the information about the\nmost frequent letter at level I across all input strings in S.\nSuppose that s is a partial solution of a fixed length 1 < L which has to be evaluated. Based on\nthe expected solution string expected_sol, let us approximate the expected distance from this partial\nsolution to a complete solution. To do so, for each string s; and the expected string expected_sol,\nthe number of positional matchings between each suffix of expected_sol and s; of the same size in\nboth strings are calculated. For example, if expected_sol = abbc and sj = acbe, the scoring vector\nis obtained [0, 1, 1, 2] by first comparing characters at position 4 (does not match), then at position 3\n(which matches, yielding a score of 1), etc. Reversely, the vector scorej = [2,1, 1, 0] is constructed for\neach j = 1,...,n. This means, that scorej[r] immediately restores the number of characters between\nthe r-length suffix of expected sol and the r-length suffix of s; that match at the same position\n(index) between these two. These structures are therefore preprocessed before entering the main loop\nof the BS. The score of solution s is based on the expected number of matched letters obtained by\ntaking into account two scores for each input string si: (i) the number of positions between the prefix\nof si of length I and solution s for which respective characters match each other, and (ii) estimated\nnumber of positions between the suffixes of expected_sol and si with the starting position 1 + 1\nwhose characters match each other. In essence, the final score is based on the most probable suffix\nsolution expected_sol for the remaining parts (suffixes) of input strings relevant for extending partial\nsolution s. Symbolically, for a solution x, l = |x|, the following vector of dimension n is calculated:\nfitness_vector(x) = hd(x, S[1] = {$1[1, 1], . . ., $n [1, 1]})\n+ (score[l + 1, L]),\n(4)"}, {"title": null, "content": "where hd(x, S) = (hd(x, 81), ..., hd(x, sn)), and the plus operator on vectors is applied coordinate-\nby-coordinate. Now, an estimated expected distance heuristic score for solution s and the set of input\nstrings S is given by\nEX(x) = min fitness_vector(x).\ni=1,...,n\n(5)\nNote that incorporating the minimum in Eq. (5) has been found the most suitable as utilizing the\nmaximum or the mean value has been shown as too optimistic estimators, yielding weaker performing\nsearch guidance. As an important detail, the first term in Eq. (4) is calculated incrementally, updating\nthe score from the parent node of the node associated with partial solution x. This operation is\nexecuted in O(n) time. The second term, using the preprocessed data structures score, requires O(n)\ntime. Therefore, one can calculate Eq. (5) efficiently, in a linear O(n) time. Note that nodes with\nlarger EX values are preferred.\nIn the case of ties among nodes at the same level (i.e., having the same EX(\u00b7) score for each\nnode), we break them by utilizing another heuristic rule based on the variance of hamming distances\nbetween solution x and corresponding prefixes of input strings; a smaller variance is preferred. For a\npartial solution x, the variance is calculated based on Eq. 6.\nVar(x) =\n\u03a3=1(hd(x, si) - hd)2\nn\nWhere hd is the mean of hamming distances between x and all strings, i.e. hd =\n(6)\n\u03a3=1 hd(x,si)\nn"}, {"title": "3.4. A New Local Search for CS problem", "content": "In this section, we present a new local search algorithm that starts with the initial solution\nobtained by executing the BS algorithm and intends to improve its quality by performing certain\ncharacter modifications. The main idea of this procedure is to flatten peak strings with the largest\nHamming distances in a way that minimizes the impact on other strings. In more detail, we refer to\nAlgorithm 2: the preliminary step involves identifying the critical strings that possess the maximal\ndistances between the incumbent solution best_sol and all input strings from S. In the case where\nmultiple critical strings exist, all of them are taken into account. Subsequently, the character that\nappears most frequently (the character with the highest fi(\u00b7), 1 < 1 < L) within these critical strings\nis recognized along with its position of appearance, denoted by index. In case of multiple characters\nwith the same frequency, all of them are considered. We store all these pairs of indices and characters\nin a list All Pairs_Indices_Chars which is then shuffled to prevent the search from any bias. We\niterate through the list by considering each pair (index, char) to replace char into a temporary\nsolution tempsol at the position index. If this modification leads to a hamming distance that is either\nreduced or equivalent to that of the prior optimal solution, it is declared for the new best solution, i.e.\nbestsol = tempsol and the next iteration of LS has been performed. If none of the pairs (index, char)\nyield a better or equivalent Hamming distance, the algorithm terminates by returning best_sol even\nbefore the time limit is reached. Otherwise, the above procedure repeats until a certain time limit is\nreached."}, {"title": null, "content": "Consider the following example of a major iteration of the local search algorithm. Let S\n{$1 = CAGTG, 82 = CGATA, S3 = GATCA, 84 = CTACG}. Given the initial solution best_sol = GAACG, we\ncompute the hamming distances as follows: hd(bestsol, 81) = 3, hd(bestsol, S2) = 4, hd(bestsol, $3) = 2,\nhd(bestsol, 84) = 2. At first, the critical strings with maximum hamming distances must be identified,\nwhich in this case is s2 with a distance of 4. Now, to reduce the hamming distance while to minimally\nimpact the other strings, we analyze the character frequency in 82, yielding: f1(s) : C) = 3, f2(82 :\nG) = 1, f3(s3 : A) = 2, f4(s2 : T) = 2, f5(s : A) = 2. As it is clear, the most frequent character of s2 is\nCat level 1 with a frequency of 3. By placing the character C at the first position of tempsol = bestsol,\nwe derive a new (temporary) solution tempsol = CAACG resulting in the updated hamming distances:\nhd(tempsol, S1) = 2, hd(tempsol, 82) = 3, hd(tempsol, 83) = 3, hd(tempsol, 84) = 1. In this way, the\nhamming distance of the modified solution is reduced (from 4) to 3 and, therefore, bestsol is set to\ntempsol."}, {"title": "4. Experimental Results and Statistical Analysis", "content": "In this section, we compare our method with state-of-the-art approaches to the CS problem from\nthe literature. To the best of our knowledge, ILP, GWSA, and WFC have reported the best results\non the known CS problem instances from the literature. Since the ILP model designed by Meneses et\nal. [10] is extremely strong and more effective than any heuristic approach for binary strings, clearly\nstate-of-the-art method there, we compare our TSA on DNA and protein strings against the ILP,\nGWSA, and WFC methods. To facilitate this comparison, we first detail the benchmark datasets used\nfor the comparison purposes as given in Section 4.1. We then describe the implementation details\nand parameters' tuning in Section 4.2. Numerical comparison of the methods over five benchmark\ndatasets is presented in Section 4.3. Section 4.4 discusses the run-time analysis of the approaches,\nand finally, we analyze the methods from a statistical perspective in Section 4.5."}, {"title": "4.1. Benchmark Datasets", "content": "In the CS problem literature, we found a big confusion with the employed random instances\nas each work produces its own randomly generated dataset for testing purposes. We empha-\nsize that the datasets from most of the authors of the existing work are requested. However,\nnone of them responded to our queries. Thus, we were forced to produce another uniformly\ngenerated dataset that consists of a wide range of strings. Two datasets called Alpha-4 and\nAlpha-20 are generated for comparison purposes which correspond to artificial gene sequences\nrepresenting DNA and protein sequences. In order to resolve the aforementioned confusion with\ninstances, our instances can be found at the publicly available GitHub repository at the link\nhttps://github.com/Hesam1991/Three-Stage-Algorithm-for-the-Closest-String-Problem.\nBoth Alpha-4 and Alpha-20 consist of strings with n \u2208 {10, 15, 20, 25, 40, 60, 80, 100} and\nL\u2208 {50, 100, 150, 200, 250, 300, 350, 400, 800, 1000, 1500} to test the algorithm in different situations.\nThis gives us 88.2 = 176 random instances. Furthermore, the McClure is a small real-world protein\ndataset that is widely used to test the performance of the algorithms. It consists of six instances\nwith n \u2208 {6,10,12} and L \u2208 {98, 100, 141}. We requested the McClure dataset from the authors of\nthe original paper, but the instances could not be provided to us. After an intensive web search, this\ndataset is found but, unfortunately, one of these instances was corrupted. However, we report the\nresults of the algorithms on the remaining five instances.\nThe McClure is a small dataset that includes protein sequences and realistic instances are much\nlarger, we introduce a real-world DNA dataset from the TP53 gene which is known as the guardian\nof the genome. TP53 plays a crucial role in suppressing the tumor cells in living creatures. When\nthe TP53 gene is damaged, it cannot repair itself and consequently, it cannot prevent cancer. The\nsequence of the TP53 gene differs for distinct living creatures, but its functionality is the same in all\nof them. Understanding the conservation levels of these genes can help to predict a species' cancer\nresistance [29]. We have collected the TP53 gene of 27 animals including humans, chimpanzees,\nelephants, etc. from NCBI and randomly put them together as a dataset. This dataset consists of\nnine instances with n \u2208 {5, 10, 27}, and L \u2208 {500, 1000, 2000, 3000}."}, {"title": "4.2. Implementation Details and Parameters Tuning", "content": "In this section, we describe the way we implemented and set the parameters of the TSA method\nas well as our three competitors (ILP, GWSA, and WFC). We requested the authors of the mentioned\nmethods for their original implementations. The authors of the WFC algorithm provided us with their\nPython code; however, any response has not been received by the authors of the ILP, and GWSA.\nTherefore, we carefully re-implemented these two approaches. Our re-implementations are freely\naccessible via previously mentioned GitHub repository. All implementations are coded in Python 3.8.\nTo solve the ILP model, CPLEX 12.8 has been executed under default settings. All experiments were\nexecuted in single-threaded mode on a machine equipped with an Intel Core i7-4702MQ processor\nrunning at 2.2 GHz and using 6 GB of memory. ILP and WFC have no parameters to be tuned. For\nthe GWSA method, we used the parameters as mentioned in [21]. For tuning the TSA parameters,\nwe have used grid search [31], trail Bt for assigning proper rank is set to 15. We set the starting\nvalue of \u1e9e to 300, which the value is adjusted over iterations according to Eq. 3. Also, the maximum\ntime limit for local search is set to 5 seconds. Since a maximum run time should be given to ILP\nand our proposed TSA, we set the maximum time limit for both methods according to Eq. 7 chosen\nto be comparable to the average running times reported for the other two heuristic approaches from\nthe literature. It is notable that all methods (GWSA, WFC, and TSA) except the ILP are non-\ndeterministic, so, to ensure a fair comparison, we execute each stochastic algorithm ten times per\ninstance and reported the best, worst, and average distances, and finally, we compare them over the\naverage hamming distance over all ten runs.\ntmax(s) \u2190{\n30L < 400;\n60400 < L < 1000;\n120L\u2265 1000.\n}(7)"}, {"title": "4.3. Numerical Comparisons", "content": "In this section, we compare our TSA with ILP [10], GWSA [21], and WFC [22] across our randomly\ngenerated and real-world datasets. Table 2 reports an overall summary results of each method. The"}, {"title": null, "content": "following conclusions are drawn from there.\n\u2022 Among the five datasets, TSA achieved a better average solution length in four of them. For\nexample, the average solution quality between the TSA and the second-best ILP approach on the\nbenchmark set FLU-A-H1N1 is 819.72 vs. 874.6 in favor of our approach. For the benchmark\nset McClure, the best results are achieved by the ILP, slightly better than that of the W"}]}