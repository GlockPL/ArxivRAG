{"title": "A Three-Stage Algorithm for the Closest String Problem on Artificial and Real Gene Sequences", "authors": ["Alireza Abdi", "Marko Djukanovich", "Hesam Tahmasebi Boldaji", "Hadis Salehi", "Aleksandar Kartelj"], "abstract": "The Closest String Problem is an NP-hard problem that aims to find a string that has the minimum distance from all sequences that belong to the given set of strings. Its applications can be found in coding theory, computational biology, and designing degenerated primers, among others. There are efficient exact algorithms that have reached high-quality solutions for binary sequences. However, there is still room for improvement concerning the quality of solutions over DNA and protein sequences. In this paper, we introduce a three-stage algorithm that comprises the following process: first, we apply a novel alphabet pruning method to reduce the search space for effectively finding promising search regions. Second, a variant of beam search to find a heuristic solution is employed. This method utilizes a newly developed guiding function based on an expected distance heuristic score of partial solutions. Last, we introduce a local search to improve the quality of the solution obtained from the beam search. Furthermore, due to the lack of real-world benchmarks, two real-world datasets are introduced to verify the robustness of the method. The extensive experimental results show that the proposed method outperforms the previous approaches from the literature.", "sections": [{"title": "1. Introduction", "content": "The Closest String (CS) problem involves finding a string that is closest to a given set of strings; the term \"closest\" usually refers to the minimum Hamming distance but may refer to the other sequential measures. CP problem has many applications in coding theory [1], and computational biology [2]. More specifically, this problem is used in designing drugs, degenerated primers, diagnostic probes, and finding gene clusters [2, 3, 4, 5, 6, 7]. CS problem is also known as the Hamming Center problem and comes with various versions of different computational complexities. It is known that the CS problem is NP-hard in general [2], while its decision problem variant is NP-complete [8]. In the decision version of the CS problem, the aim is to answer a binary question \"Does the input set of string have a closest string with maximum distance of d?\". The general version of the CS problem seeks a string that is as close as possible to all given input strings. This paper tackles the general form of the CS problem with an arbitrary set of strings. Let S = {$1,$2,\u2026\u2026,Sn} be the set of strings over a finite alphabet \u2211 = {01, 02,\u2026\u2026, \u03c3m}, where n and m refer to the number of strings and alphabet set cardinality, respectively. Without loss of generality, we assume that all strings have the same length (L). The closest string is a string with length L that has the minimum Hamming distance from strings in S.\nCS problem has attracted many researchers to solve it exactly as well as heuristically. Gramm et al. [9] proposed an exact fixed-parameter algorithm that runs in linear time assuming the Hamming distance is specified and fixed. Meneses et al. [10] developed an integer linear programming to solve the CS in general case. This algorithm has been efficient in finding an exact solution in a short time for small and near-moderate instances. However, a huge amount of time is requested for solving larger instances, specifically for random strings with n > 30 and L > 800. In 2011, Liu et al. [11] introduced an exact algorithm for solving the CS problem for the particular case when n \u2264 3 and N = 2. Among the exact methods, the ILP model proposed by Meneses et al. [10] is the most effective one and extremely efficient for binary strings.\nWhen it comes to DNA and protein sequences, that are much larger and more complicated to solve, exact methods perform poorly. Therefore, approximation algorithms are widely applied. For example, approximation algorithms with approximation guarantees are proposed [1, 2, 12] and despite their speed, they showed poor performance in finding near-optimal solutions. (Meta) heuristic methods instead, could find near-optimal solutions in a reasonable time, but they do not guarantee optimality in general. Nevertheless, the heuristic methods outperform the previously mentioned methods when applied to large DNA and protein sequences. In 2005, Liu et al. [13] suggested using genetic and simulated annealing algorithms to solve the CS problem. Moreover, they implemented the parallel version of the algorithm and showed that the parallel genetic algorithm is superior when compared to the others. Later, Gomes et al. [14, 15] proposed a simple heuristic along with its parallel version. In 2008, Liu et al. [16] proposed a hybridization of genetic and simulated annealing algorithms and combined their metrics to solve the CS problem. However, their algorithm is only tested for binary strings. Later in 2010, Faro and Pappalardo [17] introduced an algorithm based on the Ant Colony Optimization method which excelled over all former approaches. Mousavi and Esfahani [18] suggested using the greedy randomized adaptive search procedure (GRASP) algorithm followed by a new heuristic function inspired by probability theory to solve the general case of the CS problem. In 2011, Mousavi [19] utilized a beam search as a part of a hybrid meta-heuristic and also, while modifying the heuristic function from [18] to get a more enhanced one. Liu et al. [11] in their other work from 2011, built on their heuristic function from [20] to design an effective local search. In 2014, Pappalardo et al. [21] proposed a hybrid of a greedy algorithm and simulated annealing (GWSA) to solve the CS problem. They first generated an initial solution by a greedy walk algorithm, next, they fed the output of the greedy walk algorithm to a simulated annealing algorithm. GWSA excelled over all former approaches but delivered high running time. Recently, Xu and Perkins [22] proposed using Wave Function Collapse (WFC) to solve the CS problem, claiming it outperforms all other approaches. However, they did not compare their method with the GWSA.\nThe WFC is a lightweight method with very low running time. First, they constructed a frequency table based on the repetition of characters in strings. Second, they identified the string with the maximum Hamming distance. Finally, according to the frequency table, they assigned a character to the solution. They repeated the second and the last steps until a solution of complete length was obtained. This method is particularly suitable for solving the CS problem when dealing with protein sequences. In summary, the WFC is considered the state-of-the-art heuristic method for solving the general form of the closest string problem.\nContributions of this paper are as follows.\n\u2022 A Three-Stage Algorithm (TSA) to address the CS problem is designed. In the first stage of the algorithm, a novel pruning method that enhances both the speed and quality of the algorithm is performed. The next phase employs a time-restricted variant of beam search on the restricted search space. A new heuristic function based on expected distance score to guide the beam search is designed. In the last stage, an efficient local search is executed to refine the solution obtained by the beam search.\n\u2022 We have created two real-world datasets and discussed the robustness of the approaches. The first one consists of the well-known TP53 gene nucleotide, and the second dataset comprises the nucleotides of two segments of a common flu variant across different hosts to check their conservation levels.\n\u2022 Extensive experimental evaluation shows that our proposed algorithm (TSA) outperforms the state-of-the-art approaches over all five benchmark sets.\nThe rest of this paper is structured as follows. In Section 2, we provide several definitions and formulations required for the subsequent sections. Following that, we introduce details of our algorithm in Section 3. Section 4 discusses the experimental results along with statistical reports. Finally, Section 5 concludes the paper by additionally providing some outlines for future work."}, {"title": "2. Basic Definitions and Preliminaries", "content": "This section provides the fundamentals and the definitions essential for understanding the major core of the paper.\nGiven is a set S = {$1,82,...,sn} of input strings over a finite alphabet \u2211 = {01,02,...,\u03c3m}, where n and m refer to the numbers of input strings and alphabet cardinality, respectively. Assume that all strings in S have the same length L. Let s denote the jth character of ith string in S, while s denotes the leading character of string si. The notion of level l (1 \u2264 l < L) takes all Ith characters over input strings in S. For two integer values 1 \u2264 i \u2264 j \u2264 |s|, by s[i, j] we denote the continuous part of strings (sub-string) that starts with the character at position i and ends with the character at position j. The frequency of a character (\u03c3) at level l is denoted by fi(\u03c3), i.e. the number of occurrences of oat level 1. Let S[I] = {si[1, 1] | i = 1, . . ., n} be the set of prefix strings of each input string w.r.t. position l\u2208 {1, ..., L}.\nIn the closest string problem, the Hamming distance (hd) represents the distance between strings and the proposed solution. It is computed by counting the differing positions between each string (si) and the solution, with the largest of these distances representing the solution's overall Hamming distance. Symbolically, for any two strings (81) and (s2) of equal length (L), the Hamming distance is defined as in Eq.1.\nhd(81, 82) = \u2211 \u03b4(81, 85)\ni=1\n\u03b4(ch1, ch\u2082)\u2190{\n1 ch1 \u2260 ch2\n0 ch\u2081 = ch2\n(1)\nTo facilitate the explanation of our expected distance heuristic in Section 3.3, let hd(s1,82):= \u03a3=1(1 \u2013 \u03b4(81, 82), denote the complementary score of the Hamming distance by hd(s1,82), which refers to the number of equal characters between the two strings at the same positions. Note that hd(81,82) = L - hd(81,82). In essence, the hamming distance between string s and a set of input string S is calculated by\nhd(s, S) = max{hd(s, si) | i = 1, ..., n}.\n(2)\nNote that since we solve the CS problem in a constructive manner where our solutions are partial, we explain the hamming distance calculation for a partial solution s\u00ba with length of |sp| = lp. That is, the hamming distances is calculated applying Eq. (2) as the score hd(s\u00ba, S[lp]). One can easily see that hd(sp) is equal to lp \u2013 hd(sp, S[lp]).\nBeam Search (BS) is a heuristic tree-search algorithm that works in a limited breadth-first manner. It was proposed by Hayes et al. [23] and applied in the context of the speech understanding problem. The BS and its variants are widely applied to solve various optimization problems, e.g., scheduling and machine translation [24, 25]. In the context of string problems from bioinformatics, it has been proven as one of the most efficient techniques; for example, it is successfully employed to solve the prominent Longest Common Subsequence problem [26]. A specific number (\u03b2 > 0) of nodes based on a heuristic evaluation is selected at each level of the search to be further processed. BS uses parameter \u1e9e to provide the trade-off between completeness and greediness. As the value of \u1e9e increases, the runtime of the algorithm and the chance of finding the optimal answer gets higher and vice versa. For the CS problem, BS starts with an empty solution and constructs solutions by adding letters from \u03a3 to the most promising \u1e9e nodes, which correspond to (actions of) appending these letters to the end of respective partial solutions, until a complete node is reached. Complete nodes are those whose corresponding partial solutions cannot be further expanded. In the context of CS problems, complete nodes are solutions of length L.\nFor the sake of clarity, consider the S = {$1 = abaaabbaba, s2 = abababaabb} and \u2211 = {a,b}. Here, n, L, and m are 2, 10, and 2, respectively. For a partial solution x = ababa with length lp = 5, the hamming distance is calculated based on the first 5 characters of all strings i.e., s\u2081 = abaaa, and 82 = ababa, which hd(x, s\u2081) = 1, and hd(x, s2) = 0. Also, hd(x, s\u2081) = 4, and hd(x, s2) = 5. In addition, Sa denotes the fourth character of the second string which is b. Furthermore, the frequency of character a and b at level 1 (f1(a) and f\u2081(b)) is 2 and 0, respectively."}, {"title": "3. The Proposed Method", "content": "In this section, the Three-Stage Algorithm (TSA) is designed to address the CS problem, pseudocoded by Algorithm 1. Initially, the algorithm pre-processes the set of effective actions at each level of BS employing the pruning mechanism, which serves to diminish the search space. Subsequently, it employs the expected distance heuristic function to guide the Time-Restricted Beam Search (TRBS) [27] towards a solution of reasonable quality. Lastly, the obtained solution is passed to the new local search algorithm to further refine its quality.\nTo summarize, the methodology applies the three core stages: (i) the novel pruning method described in Section 3.1 to support effective decision-making; (ii) the TRBS algorithm specifically tailored for the CS problem provided in Section 3.2 whose the search is guided by the new expected distance heuristic, supported by the associated tie-breaking strategy; see Section 3.3; (iii) a new local search algorithm detailed in Section 3.4."}, {"title": "3.1. A Novel Pruning Method for the Closest String Problem", "content": "Due to the NP-hardness of the CS problem, the state space grows exponentially with the instance size. The following question arises: Should we regard all members of \u2211 as candidates at level l\u2208 {1,..., L} for a solution? To answer this question, we have analyzed the exact solution for 50 instances with a length of 100 and it turned out that in most cases, the selected character was from a specific subset of characters. Hence, we decided to design a pruning method to determine those specific subsets of alphabets and consequently, improve the quality of solutions and running time of the BS approach. To describe the pruning method, let us first define the notion of 'rank 1' (R1) and 'rank 2' (R2). For the following list of tuples [(A, 10), (B, 12), (C, 10), (D, 7), (E, 1), (F, 6), (G, 9)] where the first coordinate associates a character and the second coordinate refers to the number of occurrences of that letter, the R1 assigns the set of characters with the highest second coordinate, which here would be B. The R2 contains the characters with the first and second highest assigned values; in this example, the first highest value is given to B, and the second highest value (10) is assigned to A and C. As the values for both of them are the same, both are assigned to R2. Thus, the R2 contains the three characters, B, A, and C.\nTo clarify on a concrete instance, consider Table 1, S = {$1,$2,...,S8} whose the length of input strings are 10. Now, we should determine the R1 and R2 for each l \u2208 {1,...,L}. To do this, we use the frequency of characters at each level 1. For example at the fixed level 1 = 6, the frequency of characters is as follows, f6(X) = 2, f6(K) = 3, f6(E) = 2, f6(C) = 1, and the frequency of the rest of \u2211 members are 0. According to the introduced notation, the most frequent characters are assigned R1, which is K. For identifying the R2 set, we add the first and second most frequent characters here; thus, K is the first, and since the X and E's frequencies match, both of them are considered as the second most frequent characters. Hence, the R1 set for (l = 6) is {K}, and the R2 set for (1 = 6) is {K, X, E}. It is notable that after obtaining ranks, these pairs (R1, R2) are stored in a list of alphabets for each l, called the ranked alphabet (p) which are subsequently passed to the BS procedure.\nFor example, in Table 1, consider we want to use R2, so, our solution's first character could only be 'M' or 'C'. During the preprocessing phase and before obtaining the CS for a set of input strings, it is essential to identify the proper rank that will be leveraged. The choice between R1 and R2 should be guided by the inherent characteristics of the input strings in question, as this decision is dependent on specific conditions. A simple way [28] to determine the most appropriate rank for a given set of strings is to initially execute the BS with a small (trial) \u1e9et value using both ranks. Subsequently, the algorithm runs with a larger (actual) \u1e9e value using the rank that yielded a smaller hamming distance. When the hamming distances are equal for both ranks, the option is selected randomly. This pruning method could be used in all methods that tried to solve the CS problem and it is not limited to our approach."}, {"title": "3.2. Time Restricted Beam Search for the CSP", "content": "To explore the search space of the CS problem effectively, a time-restricted version of the BS approach is employed. This version, therefore called the Time-Restricted Beam Search (TRBS) [27], has proven to be more robust than the basic BS variant with a provided good trade-off between greediness and completeness [27]. Moreover, it addresses the problem of adjusting the value of \u1e9e parameter dynamically within the allowed execution time (tmax). Note that this is not the characteristic of the basic beam search. At each major iteration of the algorithm, TRBS identifies \u1e9e value depending on the remaining runtime and the value of \u1e9e used in the previous iteration. The details of computing \u03b2 at iteration I are given by Eq. 3.\n\u03b2\u2190{\n[\u03b2. 1.1]if trem/trem \u2265 1.1;\nmin([\u03b2/1.1\u300d,150) if trem/trem \u2264 0.9;\n\u03b2 otherwise.\n(3)\nHere, trem denotes the remaining time to reach the tmax and trem is the expected remaining time which for CS problem is equal to titer (L\u2212 1). In other words, trem estimates the remaining time by multiplying the last iteration time (titer) with the remaining number of steps to achieve a leaf node. In the context of the CS problem, since the length of each leaf node (a solution) is equal to L supposing the BS performs at the current level 1, the remaining number of BS iterations would be (L-1)."}, {"title": "3.3. An Expected Distance Heuristic for CSP", "content": "This section is devoted to a constriction of the expected distance heuristic (EX) to evaluate nodes of the TRBS. Additionally, an effective way of breaking ties between nodes with the same expected distance score values is proposed.\nFirst, we explain the term of an expected solution to the CS problem. For each level l \u2208 {1, ..., L}, we determine the most frequent letter across strings from S. Assume that the information is kept in the (sequential) structure expected_sol, where expected_sol[l] stores the information about the most frequent letter at level I across all input strings in S.\nSuppose that s is a partial solution of a fixed length 1 < L which has to be evaluated. Based on the expected solution string expected_sol, let us approximate the expected distance from this partial solution to a complete solution. To do so, for each string s; and the expected string expected_sol, the number of positional matchings between each suffix of expected_sol and s; of the same size in both strings are calculated. For example, if expected_sol = abbc and sj = acbe, the scoring vector is obtained [0, 1, 1, 2] by first comparing characters at position 4 (does not match), then at position 3 (which matches, yielding a score of 1), etc. Reversely, the vector scorej = [2,1, 1, 0] is constructed for each j = 1,...,n. This means, that scorej[r] immediately restores the number of characters between the r-length suffix of expected sol and the r-length suffix of s; that match at the same position (index) between these two. These structures are therefore preprocessed before entering the main loop of the BS. The score of solution s is based on the expected number of matched letters obtained by taking into account two scores for each input string si: (i) the number of positions between the prefix of si of length I and solution s for which respective characters match each other, and (ii) estimated number of positions between the suffixes of expected_sol and si with the starting position 1 + 1 whose characters match each other. In essence, the final score is based on the most probable suffix solution expected_sol for the remaining parts (suffixes) of input strings relevant for extending partial solution s. Symbolically, for a solution x, l = |x|, the following vector of dimension n is calculated:\nfitness_vector(x) = hd(x, S[1] = {$1[1, 1], . . ., $n [1, 1]})\n+ (score[l + 1, L]),\n(4)\nwhere hd(x, S) = (hd(x, 81), ..., hd(x, sn)), and the plus operator on vectors is applied coordinate-by-coordinate. Now, an estimated expected distance heuristic score for solution s and the set of input strings S is given by\nEX(x) = min fitness_vector(x).\ni=1,...,n\n(5)\nNote that incorporating the minimum in Eq. (5) has been found the most suitable as utilizing the maximum or the mean value has been shown as too optimistic estimators, yielding weaker performing search guidance. As an important detail, the first term in Eq. (4) is calculated incrementally, updating the score from the parent node of the node associated with partial solution x. This operation is executed in O(n) time. The second term, using the preprocessed data structures score, requires O(n) time. Therefore, one can calculate Eq. (5) efficiently, in a linear O(n) time. Note that nodes with larger EX values are preferred.\nIn the case of ties among nodes at the same level (i.e., having the same EX(\u00b7) score for each node), we break them by utilizing another heuristic rule based on the variance of hamming distances between solution x and corresponding prefixes of input strings; a smaller variance is preferred. For a partial solution x, the variance is calculated based on Eq. 6.\nVar(x) =\n\u03a3=1(hd(x, si) - hd)2\nn\nWhere hd is the mean of hamming distances between x and all strings, i.e. hd = \u03a3=1 hd(x,si)\nn\n(6)"}, {"title": "3.4. A New Local Search for CS problem", "content": "In this section, we present a new local search algorithm that starts with the initial solution obtained by executing the BS algorithm and intends to improve its quality by performing certain character modifications. The main idea of this procedure is to flatten peak strings with the largest Hamming distances in a way that minimizes the impact on other strings. In more detail, we refer to Algorithm 2: the preliminary step involves identifying the critical strings that possess the maximal distances between the incumbent solution best_sol and all input strings from S. In the case where multiple critical strings exist, all of them are taken into account. Subsequently, the character that appears most frequently (the character with the highest fi(\u00b7), 1 < 1 < L) within these critical strings is recognized along with its position of appearance, denoted by index. In case of multiple characters with the same frequency, all of them are considered. We store all these pairs of indices and characters in a list All Pairs_Indices_Chars which is then shuffled to prevent the search from any bias. We iterate through the list by considering each pair (index, char) to replace char into a temporary solution tempsol at the position index. If this modification leads to a hamming distance that is either reduced or equivalent to that of the prior optimal solution, it is declared for the new best solution, i.e. bestsol = tempsol and the next iteration of LS has been performed. If none of the pairs (index, char) yield a better or equivalent Hamming distance, the algorithm terminates by returning best_sol even before the time limit is reached. Otherwise, the above procedure repeats until a certain time limit is reached.\nConsider the following example of a major iteration of the local search algorithm. Let S = {$1 = CAGTG, 82 = CGATA, S3 = GATCA, 84 = CTACG}. Given the initial solution best_sol = GAACG, we compute the hamming distances as follows: hd(bestsol, 81) = 3, hd(bestsol, S2) = 4, hd(bestsol, $3) = 2, hd(bestsol, 84) = 2. At first, the critical strings with maximum hamming distances must be identified, which in this case is s2 with a distance of 4. Now, to reduce the hamming distance while to minimally impact the other strings, we analyze the character frequency in 82, yielding: f1(s) : C) = 3, f2(82 : G) = 1, f3(s3 : A) = 2, f4(s2 : T) = 2, f5(s : A) = 2. As it is clear, the most frequent character of s2 is Cat level 1 with a frequency of 3. By placing the character C at the first position of tempsol = bestsol, we derive a new (temporary) solution tempsol = CAACG resulting in the updated hamming distances: hd(tempsol, S1) = 2, hd(tempsol, 82) = 3, hd(tempsol, 83) = 3, hd(tempsol, 84) = 1. In this way, the hamming distance of the modified solution is reduced (from 4) to 3 and, therefore, bestsol is set to tempsol."}, {"title": "4. Experimental Results and Statistical Analysis", "content": "In this section, we compare our method with state-of-the-art approaches to the CS problem from the literature. To the best of our knowledge, ILP, GWSA, and WFC have reported the best results on the known CS problem instances from the literature. Since the ILP model designed by Meneses et al. [10] is extremely strong and more effective than any heuristic approach for binary strings, clearly state-of-the-art method there, we compare our TSA on DNA and protein strings against the ILP, GWSA, and WFC methods. To facilitate this comparison, we first detail the benchmark datasets used for the comparison purposes as given in Section 4.1. We then describe the implementation details and parameters' tuning in Section 4.2. Numerical comparison of the methods over five benchmark datasets is presented in Section 4.3. Section 4.4 discusses the run-time analysis of the approaches, and finally, we analyze the methods from a statistical perspective in Section 4.5."}, {"title": "4.1. Benchmark Datasets", "content": "In the CS problem literature, we found a big confusion with the employed random instances as each work produces its own randomly generated dataset for testing purposes. We emphasize that the datasets from most of the authors of the existing work are requested. However, none of them responded to our queries. Thus, we were forced to produce another uniformly generated dataset that consists of a wide range of strings. Two datasets called Alpha-4 and Alpha-20 are generated for comparison purposes which correspond to artificial gene sequences representing DNA and protein sequences. In order to resolve the aforementioned confusion with instances, our instances can be found at the publicly available GitHub repository at the link https://github.com/Hesam1991/Three-Stage-Algorithm-for-the-Closest-String-Problem.\nBoth Alpha-4 and Alpha-20 consist of strings with n \u2208 {10, 15, 20, 25, 40, 60, 80, 100} and L\u2208 {50, 100, 150, 200, 250, 300, 350, 400, 800, 1000, 1500} to test the algorithm in different situations. This gives us 88.2 = 176 random instances. Furthermore, the McClure is a small real-world protein dataset that is widely used to test the performance of the algorithms. It consists of six instances with n \u2208 {6,10,12} and L \u2208 {98, 100, 141}. We requested the McClure dataset from the authors of the original paper, but the instances could not be provided to us. After an intensive web search, this dataset is found but, unfortunately, one of these instances was corrupted. However, we report the results of the algorithms on the remaining five instances.\nThe McClure is a small dataset that includes protein sequences and realistic instances are much larger, we introduce a real-world DNA dataset from the TP53 gene which is known as the guardian of the genome. TP53 plays a crucial role in suppressing the tumor cells in living creatures. When the TP53 gene is damaged, it cannot repair itself and consequently, it cannot prevent cancer. The sequence of the TP53 gene differs for distinct living creatures, but its functionality is the same in all of them. Understanding the conservation levels of these genes can help to predict a species' cancer resistance [29]. We have collected the TP53 gene of 27 animals including humans, chimpanzees, elephants, etc. from NCBI and randomly put them together as a dataset. This dataset consists of nine instances with n \u2208 {5, 10, 27}, and L \u2208 {500, 1000, 2000, 3000}."}, {"title": "4.2. Implementation Details and Parameters Tuning", "content": "In this section, we describe the way we implemented and set the parameters of the TSA method as well as our three competitors (ILP, GWSA, and WFC). We requested the authors of the mentioned methods for their original implementations. The authors of the WFC algorithm provided us with their Python code; however, any response has not been received by the authors of the ILP, and GWSA. Therefore, we carefully re-implemented these two approaches. Our re-implementations are freely accessible via previously mentioned GitHub repository. All implementations are coded in Python 3.8. To solve the ILP model, CPLEX 12.8 has been executed under default settings. All experiments were executed in single-threaded mode on a machine equipped with an Intel Core i7-4702MQ processor running at 2.2 GHz and using 6 GB of memory. ILP and WFC have no parameters to be tuned. For the GWSA method, we used the parameters as mentioned in [21]. For tuning the TSA parameters, we have used grid search [31], trail Bt for assigning proper rank is set to 15. We set the starting value of \u1e9e to 300, which the value is adjusted over iterations according to Eq. 3. Also, the maximum time limit for local search is set to 5 seconds. Since a maximum run time should be given to ILP and our proposed TSA, we set the maximum time limit for both methods according to Eq. 7 chosen to be comparable to the average running times reported for the other two heuristic approaches from the literature. It is notable that all methods (GWSA, WFC, and TSA) except the ILP are non-deterministic, so, to ensure a fair comparison, we execute each stochastic algorithm ten times per instance and reported the best, worst, and average distances, and finally, we compare them over the average hamming distance over all ten runs.\ntmax(s) \u2190{\n30 L < 400;\n60 400 < L < 1000;\n120 L\u2265 1000.\n(7)"}, {"title": "4.3. Numerical Comparisons", "content": "In this section"}, {"title": "4.4. Time Comparison", "content": "In this section, we compare the algorithms from a running time perspective. Figure 1 illustrates the run times of all methods across five benchmarks. In each plot, the y-axis represents the running time (in seconds), while the x-axis indicates the instance number (a unique instance ID) indicating an instance for which the (average) running time is obtained by executing respective algorithm.\nThe WFC algorithm exhibits the lowest running time, however it comes with poor performance. Due to the absence of tunable parameters, we consider this method to have reached its limitations. For the Alpha-4, Alpha-20, and FLU-A-H1N1 datasets, ILP and TSA algorithms demonstrate similar running times. However, for the TP53 and McClure datasets, TSA outperforms ILP on medimum to large-sized instances, while ILP is faster for smaller instances where instances are solved optimally. When compared to the GWSA method, the TSA generally has a shorter running time, with the notable exception of the McClure dataset."}, {"title": "4.5. Statistical Analysis", "content": "In this section, we compare methods from a statistical perspective. For this analysis, we utilize the Friedman test [32] at a 5% significance level for the results of all four competitors over two groups of instances: the first group includes randomly generated instances from Alpha-4 and Alpha-20 (176 instances), whereas the second group includes real-world instances from the remaining three benchmark sets (24 instances in overall).\nThe results of the Friedman test performed for both groups separately indicate that we reject the null hypothesis (Ho), suggesting that the methods do not perform equally. Subsequently, the pairwise comparison of methods is illustrated in Figure 2 using CD plots by utilizing Nemenyi post-hoc test [33]. In detail, for each pair of algorithms, a critical difference is calculated at the significance level of 5%. Each algorithm is positioned at x-axis w.r.t. its average ranking. If there is a horizontal bar joining two algorithms, it means that they perform statistically equally.\nThe following conclusions are drawn from the generated CD plots.\n\u2022 Concerning random instances, the best average ranking is obtained by our TSA. The second-best average ranking is achieved by WFC, followed by ILP and GWSA. Hereby, the TSA approach performs significantly better than the competitor WFC. Both of these approaches are statistically significantly better than ILP and GWSA.\n\u2022 Concerning real-world instances, TSA again achieved the best average ranking (1.4), followed by the ILP approach. However, here the performance of both approaches does not differentiate significantly from each other. On the other hand, TSA outperforms the GSWA and WFC statistically significantly. However, no statistical difference is presented between the results of ILP and WFC."}, {"title": "5. Conclusion", "content": "In this paper, we introduced the Three-Stage Algorithm for solving the Closest String problem. Our proposed method comprises three core stages. First, we introduce a novel pruning to reduce the search space effectively. In particular, at each level of the search, a predetermined subset of promising members of the alphabet instead of a whole set is employed for a set of allowed transitions. This pruning method positively affects the quality of solutions while reducing the overall running time of the algorithm. Second, a time-restricted beam search to explore the search space for obtaining complete solutions is utilized. The search process of the beam search is guided by the newly designed expected distance heuristic function. Third, a new local search algorithm is proposed. It enables a complete solution to be improved by repetitively substituting critical characters in the incumbent solution. In addition, we created four datasets: two real-world and two uniformly at random distributed to test the robustness of the methods on both, real and simulated sequences.\nOur extensive experimental results show that the proposed method can significantly improve over the best solutions from the literature thus yielding new state-of-the-art method. In some details, for both, random and real-world instances, the results of our method achieved the best average ranking when compared to other competitor approaches. In the case of random instances, the difference between our approach and the second-best WFC approach is statistically significant in favor of the former approach. In the case of real-world instances, our approach and the second-best, an ILP approach, perform statistically equally good, and significantly better than the other two approaches.\nRegarding future work, one could engage the Monte Carlo Tree Search in the search process to leverage a predictive exploration in the state space of the CS problem and to design possibly more accurate heuristic guidance. Another way could be designing a complementary approach to the classical optimization approaches. This includes designing a learning mechanism that can be developed by separately training a model of machine learning, such as a kind of neural network, to support better decision-making in the beam search, see e.g. [34]. In order to do so, one has to determine a set of crucial input features extracted from instances that affect the final score; e.g. maximum frequency of characters at each level, standard deviation/average of maximum frequencies per each level, the variance of the solution w.r.t. related values of the hamming distance to the input strings, etc."}]}