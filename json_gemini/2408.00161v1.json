{"title": "Automatic Generation of Behavioral Test Cases For\nNatural Language Processing Using Clustering and Prompting", "authors": ["Ying Li", "Rahul Singh", "Tarun Joshi", "Agus Sudjianto"], "abstract": "Recent work in behavioral testing for natural language processing (NLP) models, such as Checklist,\nis inspired by related paradigms in software engineering testing. They allow evaluation of general\nlinguistic capabilities and domain understanding, hence can help evaluate conceptual soundness\nand identify model weaknesses. However, a major challenge is the creation of test cases. The\ncurrent packages rely on semi-automated approach using manual development which requires\ndomain expertise and can be time consuming. This paper introduces an automated approach to\ndevelop test cases by exploiting the power of large language models and statistical techniques. It\nclusters the text representations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests (MFT). The well-\nknown Amazon Reviews corpus is used to demonstrate our approach. We analyze the behavioral\ntest profiles across four different classification algorithms and discuss the limitations and strengths\nof those models.", "sections": [{"title": "1 Introduction", "content": "The advent of deep learning algorithms and transformer architectures has led to significant advances in the\nperformance of natural language processing (NLP) models. However, it is well known that complex models tend to\noverfit the training datasets and suffer from lack of generalizability. There are many challenges in real-world\napplications due to the dynamically varying nature of data ([2],[3]) diverse inputs, and sparse training data in some\napplications. This creates challenges in developing and implementing appropriate test suites to assess model\nperformance. Recently, stimulated by the behavioral testing paradigm in software engineering, researchers have\nproposed analogous methods for testing NLP models. For example, the paper CheckList ([1]) introduced Minimum\nFunctionality Test (MFT), which are simple test cases designed to test a specific behavior, for example testing the\nnegation, vocabulary, invariance towards the Named Entity recognition (NER) capability of the model. Negation MFTs\nare simple test cases in which negations are introduced in a single line of text to test whether the model understands\nnegations. These tests are constructed from a manually preset template, and they allow expansion to multiple tests\nfor detecting potential model weakness. However, a major limitation for this approach is that the template needs to\nbe designed manually. The process can be time and resource intensive as they must be developed for domain-specific\napplications. Further, the quality of the template varies with the subject-matter experts' knowledge, creativity, and\nlanguage skills. In addition, the overall semantic and syntactic diversity of the tests are restricted due to the fixed\nformat and structure of the base template. It is also possible that the generated test cases might have different data\ndistribution compared to the original dataset."}, {"title": "2 Experiments and Results", "content": "We utilized the Amazon customer review dataset ([6], [7]) with negative and positive labels. We used a subset extracted\nfrom the 6.9 million Amazon dataset in US market with 38 product categories.\nNLP task: The downstream task is a binary classification problem on this customer review dataset. The original dataset\nhas labels that are one to five stars as ratings. We preprocessed the ratings and relabeled the samples with 1-2 stars\nas negative reviews and samples with 4-5 stars as positive reviews. Reviews with 3 stars are dropped.\nText: The review headline and the review body are concatenated as the text sample for each original record.\nOther data information: The subset of the customer review dataset includes five categories: mobile apps, books, music,\ntoys, and video.\nThe dataset is imbalanced consisting of mostly positive reviews with 4 to 5 stars. Down-sampling was implemented on\nthe positive samples so that the final dataset is balanced and has similar numbers of positive and negative labeled\nsamples.\nTrain/Validation/Test splits: We considered the following splits in the data for training the downstream models:\n\u2022\nTraining: 32,847 records from 11/11/1995 to 04/13/2014,\n\u2022\nValidation: 7016 records from 04/14/2014 to 12/31/2014, and\n\u2022\nTesting: 7030 records from 01/01/2015 to 08/31/2015.\nSentence Length: All three splits contain a negligible percentage of long texts that have over 512 tokens (approximately\n3.09% for training and 0.84% for validation, and 0.55% for testing). This makes it ideal for training a transformer\nclassification model, as any excess text would be trimmed to accommodate the limit of 512 tokens imposed by models\nlike BERT-base (or large) ([8]).\nModels:\n1. For sentence embeddings used in the topic clustering, we used gtr-t5-large ([9]) sentence transformer model\ndownloaded from Hugging Face ([10]). It maps sentences & paragraphs to a 768-dimensional dense vector\nspace. The model was trained for sematic search using the encoder from a T5-large model ([11]). The\nsentence transformer was trained through a loss function to minimize the distance of the embedding vectors\nbetween similar sentences while maximizing the distance between dissimilar pairs ([12]).\n2. The Llama 2 chat model was utilized for generative tasks in the study. We used a 7 billion parameter chat\nmodel ([13],[14]).\n3. We did a comparison of four models, including logistic regression, LightGBM ([15]), DistilBERT ([16]) and BERT-\nbase."}, {"title": "2.2 Generate Behavioral Test Cases by Topic Clustering and Prompt Engineering", "content": "In this subsection, we display the different steps through generating examples which are linguistically diverse but\nconsistent with the original data on topics and data distributions."}, {"title": "2.2.1 Topic Clustering and Representative Documents", "content": "We started with extracting representative text samples from the original given data. We utilized the BERTopic ([17])\ntool for topic clustering and then extracted the representative documents for each cluster (see more details in\nAppendix I). Due to the high dimensional nature of the text embeddings, we first applied dimensional reduction on\nthe embeddings. Then we used clustering algorithms on the low dimensional embeddings to create clusters. Finally,\nthe top keywords/phrases were extracted from each cluster of documents. The final output from topic clustering is\ncluster-wise topics that were created by concatenating a few top keywords/phrases from each cluster. In the\nfollowing experiments, we used gtr-t5-large to embed text samples and utilized Uniform manifold approximation\nprojection (UMAP, [18]) for dimensional reduction. The K-means algorithm was used for clustering."}, {"title": "2.2.2 Prompt Engineering for Generating Behavioral Test Cases", "content": "We utilized the generation capabilities of a LLM to generate MFT cases by using few-shot prompting as shown in\nFigure 4. The Diagram A in Figure 4 shows the one-time process for generating few-shot examples by LLMs instead of\nmanually creating these examples. An example is shown in Figure 5 that shows how to extract test cases and\ncorresponding topics by prompting LLM with some instructions. The Diagram B in Figure 4 shows the procedure to\nloop over each representative document for generating MFT cases based on an example input prompt with details\nshown in Figure 6 and the example outputs can be found in Figure 7.\nLLMs can produce high-quality outputs in a specific format with few-shot learning when it is guided by good examples.\nTo achieve optimal results, it is crucial to incorporate meaningful prompts that help the model to understand the type\nof content to generate. By leveraging LLM's potential, we can generate engaging and informative samples that\nhighlight its capabilities without compromising on accuracy or creativity. In this instance, we use the following prompt\nto demonstrate how LLM can be employed to construct an illustrative few-shot example as shown in Figure 5."}, {"title": "2.3 Multiple Versions of MFT cases", "content": "In this paper, we created about four MFT cases for each representative document and in total we created 50\nrepresentative documents (10 top representative documents are extracted in each cluster and we have five clusters).\nAfter completing the entire procedure, we have around 200 MFT cases after deduplication. For robustness, we run\nthis process three times with different random seeds so that we have three sets of MFT test cases with 200 texts. To\nenhance the generalizable testing of a particular model in production, we generated MFT instances using the training\ndata, while simultaneously creating MFT instances utilizing the test data. Furthermore, to extend the generalization\ntesting, we asked LLM to paraphrase the MFTs using prompt engineering. We assume the paraphrased MFTs will have\nthe same label as the original MFT."}, {"title": "2.4 Quality Control of the label and generated MFT cases", "content": null}, {"title": "2.4.1 Minimum Functionality", "content": "Although the test cases generated by the MFT method are based on representative documents, will they be\nstraightforward enough for the model to comprehend and categorize? The visualization of the 2D reduced embedding\nof these MFT cases from the gtr-t5- large sentence transformer suggests they are fundamental and are easily\ndistinguishable by the model as shown in Figure 8. As we observe, the embeddings of gtr-t5-large have distinct\nboundaries between positive (green points) and adverse (blue points) evaluations for MFT instances in Figure 8On the\nother hand, the 2D representation of the original data is a mixture of positive and negative sentiments and the\nsentiment boundaries are unclear as shown in the left sidebar of Figure 3. This comparison shows the MFTs have simple\nand clear sentiments that are even understood by a general pretrained sentence transformer model while the original\ntexts have more complicated and confusing sentiments to confuse the sentence transformer model. This demonstrates\nthat the MFT are straightforward text examples representing the original data distribution, and we expect that a good\nmodel should understand them and give correct results for these test cases."}, {"title": "2.4.2 Label Quality Control", "content": "A fast and efficient method for labeling MFT cases involves using the labels of the corresponding original\nrepresentative documents used to generate the MFT. This approach assumes that the MFTs are labeled with the\nsame label as the original document since they are created from it and retain the same meaning. Nevertheless, this\nconjecture is approximate may and, in some cases, not be correct because the dataset consists of reviews displaying\ndiverse sentiments. Some evaluations express both favorable and unfavorable perspectives regarding the product\nyet are classified into just \"positive\" or \"negative\". Furthermore, when creating MFT instances, they might be\nestablished upon either the positive subsections or the negative subsections, which could result in erroneous label-"}, {"title": "2.4.3 MFT Topic Clusters", "content": "It is possible that the MFT cases might have different topics compared to the original data. We can see this through\nfour clusters for the MFT cases in Figure 1010when we compare it to the clusters of original data in Figure 3 where\nthere are five clusters. We did another layer of topic clustering on embedding vectors for MFT texts using gtr-t5-large\nsentence embeddings with BERTopic package. After this second round of clustering, we got four new topics instead of\nthe original five product categories that have more emphasis in review text contents as:\n\u2022\n\"Topic 0: Books/Movies Contents\",\n\u2022\n\"Topic 1: Toys Quality,\n\u2022\n\"Topic 2. Music\",\n\u2022\n\"Topic 3: Mobile Apps/Toys User Experience\u201d.\nAfter this step, the MFTs are able to reflect the same product categories information as before but it treats books and\nmovie reviews in one large group as the customer reviews in these two groups talk about related topics like plots,\ncharacter development etc. Furthermore, the clustering now combined part of the toys review with mobile apps\nreviews as they describe user experience with similar text."}, {"title": "3 Model Testing", "content": "The MFT test cases are now ready to use. In this section, we explore the initial application of testing downstream\nclassification models using these high quality MFT cases."}, {"title": "3.1 Testing Scenarios", "content": "The performance is reported on different versions of the MFTs, including:\n1. Performance on train MFT cases, namely Train MFT 1, Train MFT 2 and Train MFT 3 datasets generated from\ntraining data, respectively.\n2. Performance on Train MFT (Original): all train MFT cases by combining Train MFT 1, Train MFT 2, Train MFT\n3 datasets together and then apply deduplication.\n3. Performance on Train MFT (Extended): all train MFT cases, i.e., Train MFT (Original), and all their five different\nparaphrased versions followed by deduplication.\nThe data sizes for different versions of MFT cases are listed in Table 5 below. The same testing was applied to test\nMFT cases, i.e., MFT cases generated from test data, for checking model generalization ability further to have better\ninsights about whether the model understands the linguistic context well or not."}, {"title": "3.2 Test Results", "content": null}, {"title": "3.2.1 Performance on Individual MFT Dataset", "content": "To start with simple and basic MFT datasets, we generated ~200 MFT cases for each round and created three versions\nusing three different random seeds. Table 6 shows that the BERT model outperforms other models on most of the\nMFT test datasets except two instances where DistilBERT is slightly better. The performances of logistic and LightGBM\nmodels vary more across the three different MFT cases compared to the two transformer-based models. When we\niterated this process to create different versions of MFT dataset, the performance on these different versions can be\nused to calculate the standard deviation of the performance scores and can be a measure of model stability and\nrobustness."}, {"title": "3.2.2 Performance on Combined and Extended Train/Test MFT Datasets", "content": "To enhance the reliability of the model's accuracy, we tested it on a comprehensive set of MFT cases (approximately\n600 instances). The accuracy scores for every model are included in Table 7, and they indicate a consistent level of\naccuracy between the train and test splits of original data. Specifically, we noticed that both the logistic and LightGBM\nmodels exhibited a significant decrease in accuracy when transitioning from train-test split to the train-test MFT cases\ndespite having accuracies greater than 90% on the test split of dataset. This reveals the importance of evaluating a\nmodel's capacity to adapt outside of the train and test datasets using MFT cases since relying solely on test dataset\nmight result in ignoring potential difficulties.\nOn the other hand, DistilBERT demonstrated a minimal drop in functionality (less than 1%) shifting from train-test split\naccuracy to MFT cases. In contrast, BERT was more stable across various datasets.\nWe further extend generalization testing by evaluating on a broader collection of MFT instances, including the\nstandard MFT samples and five distinctly paraphrased variations generated using Llama2. Our findings indicate that\nthere was a noticeable decline in the performance of logistic and LightGBM models when moving from MFT set derived\nfrom the training data (MFT(Original)) to a larger set test enhanced with MFT cases by rephrasing. In contrast,"}, {"title": "3.2.3 Performance on Different MFT Topic Clusters", "content": "This section reports model performance across different MFT topic clusters. This allows us to analyze potential model\nweaknesses on different topics and provide model weakness explainability in terms of topics We see from Table 8 that,\nwhile LightGBM performs poorly overall, it has better performance for Toys Quality compared to others. Other models\nshow similar performance across the four different MFT topics. To cater to business needs, a separate set of MFT\ntopics can always be defined to test the model from a new perspective.\nWhile LightGBM has the worst performance compared to other models and this is visible from its test set performance\n(Table 7), this analysis provides additional evidence. Here we are mainly demonstrating the idea of finding specific\ntopic of MFTs where the model is weak at and better at. This is useful for finding subregions where the model weakness\nis."}, {"title": "4 Discussion and Concluding Remarks", "content": "In this paper, we presented a new technique to automatically generate test cases for assessing NLP models. It allows\nfor a comprehensive evaluation of models across various domains and topics that may also serve as a method to\nexplain these models. By doing so, we can gain a better understanding of their capabilities and limitations. For the\nAmazon reviews dataset, the method was effective in identifying diverse clusters and creating relevant test cases for\neach cluster.\nThe main purpose of this paper is to demonstrate the new idea for using clustering and prompt engineering with the\nhelp of LLMs to diagnose downstream models through the results on US Amazon Review dataset. This is an ongoing\nwork. We are experimenting on additional datasets and will update the results.\nThe method has potential applications in other areas."}, {"title": "Acknowledgements", "content": "The authors thank Vijayan N. Nair for supervision, discussion, comments and support for this research and\nmanuscript."}, {"title": "I. Representative Documents", "content": "Instead of designing templates manually, we start with extracting representative text samples from the\noriginal given data.\nBERTopic is a useful and popular tool for topic clustering on NLP text data. The method by default is following the\nprocedure of first applying UMAP dimension reduction on the text embeddings, then use HDBSCAN clustering\nalgorithms to create clusters for the given data, followed by a Class-based TF-IDF(c-TF-IDF) and KeyBERTInspired\nalgorithm to calculate and finetune the top keywords in each cluster. The final output would be topics which are\ncreated by concatenating a few top keywords in each cluster, respectively.\nIn the KeyBERTInspired algorithm, there is an intermediate step for extracting the representative documents. It is\nlater used for re-ranking the raw topic keywords from c-TF-IDF to downgrade less important keywords and de-noise\nthe raw keywords list.\nIn detail, on one hand, the algorithm first extracts top words per topic based on their c-TF-IDF scores, which is\ncalculated by the following equation (1).\nThe c-TF-IDF for a term x in class c is defined as"}, {"title": "II. Maximal Marginal Relevance (MMR)", "content": "When the BERTopic algorithm calculates the representative documents in the default hyperparameter setting, it\ndoesn't consider the similarity among the representative documents. For example, when it selects \"Five stars!!!\" as\nthe next representative document, it will not take this because it is too similar to the previous selected representative\ndocument \"Five stars\". To increase the diversity of the representative documents for a better semantic coverage,\nthe Maximal Marginal Relevance is employed by setting the diversity hyperparameter in BERTopic. This algorithm is\ntrying to select a representative document which is closer to the topic c-TF-IDF at the same time more diverse from\nother representative documents through the optimization function (2)\n$MMR = arg \\underset{D\\_i \\in R \\setminus S}{max} [(1 - \\lambda) Sim\\_1 (D\\_i, Q) - \\lambda max \\underset{D\\_j \\in S}{Sim\\_2(D\\_i, D\\_j)}]$\nwhere $a$ is the diversity hyperparameter, R are all sampled candidate documents, S are the current set of selected\nrepresentative documents, Q is the topic c-TF-IDF vector, $D\\_i$ is the c-TF-IDF vector for a candidate document i, $D\\_j$ is\nthe c-TF-IDF vector for an already selected representative document j. With a larger \u03bb setting, MMR will penalize\nmore on the similarity between a candidate document and current representative documents when it tries to pick\nthe next top representative document."}]}