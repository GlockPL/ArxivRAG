{"title": "LTCXNet: Advancing Chest X-Ray Analysis with Solutions for Long-Tailed Multi-Label Classification and Fairness Challenges", "authors": ["Chin-Wei Huang", "Mu-Yi Shen", "Kuan-Chang Shih", "Shih-Chih Lin", "Chi-Yu Chen", "Po-Chih Kuo"], "abstract": "Chest X-rays (CXRs) often display various diseases with disparate class frequencies, leading to a long-tailed, multi-label data distribution. In response to this challenge, we explore the Pruned MIMIC-CXR-LT dataset, a curated collection derived from the MIMIC-CXR dataset, specifically designed to represent a long-tailed and multi-label data scenario. We introduce LTCXNet, a novel framework that integrates the ConvNeXt model, ML-Decoder, and strategic data augmentation, further enhanced by an ensemble approach. We demonstrate that LTCXNet improves the performance of CXR interpretation across all classes, especially enhancing detection in rarer classes like 'Pneumoperitoneum' and 'Pneumomediastinum' by 79% and 48%, respectively. Beyond performance metrics, our research extends into evaluating fairness, highlighting that some methods, while improving model accuracy, could inadvertently affect fairness across different demographic groups negatively. This work contributes to advancing the understanding and management of long-tailed, multi-label data distributions in medical imaging, paving the way for more equitable and effective diagnostic tools.", "sections": [{"title": "1. Introduction", "content": "Deep learning Chest X-ray (CXR) models face significant hurdles, including long-tailed distribution and multi-label classification [8,9]. Long-tailed distribution, characterized by skewed disease frequency, biases predictive models towards common conditions, undermining the detection of less frequent but critical diseases [1, 12, 15]. Accurately identifying these diseases is paramount, particularly when they pose severe health risks [19,21].\nThe complexity of medical image prediction is inten-sified by the multi-label nature of CXRs [32], where a single image may exhibit multiple diseases. This scenario demands precise disease predictions, necessitating advanced classifiers specifically designed for multi-label tasks [14, 22, 23]. These challenges also contribute to fairness concerns in medical imaging classification, emphasizing the need for equitable accuracy across different demographic groups [28,34].\nOur study is dedicated to CXR image prediction, introducing LTCXNet-a combination of ConvNeXt [17], ML-Decoder [23], data augmentation, and ensemble techniques [11] each chosen for its distinct advantage. ConvNeXt's transformer-inspired architecture offers enhanced performance over conventional convolutional neural network (CNN) models, making it an excellent choice for our base architecture. ML-Decoder, an advancement over transformer heads, excels in multi-label classification and reduces computational load. Data augmentation improves both model accuracy and fairness, while ensemble techniques merge insights from various models for more accurate and dependable predictions. We assessed different approaches using the Pruned MIMIC-CXR-LT dataset [8], focusing on their performance and fairness impact. Our findings demonstrate LTCXNet's superior performance, highlighting its effectiveness in addressing long-tailed and multi-label classification challenges in medical imaging."}, {"title": "2. Method", "content": null}, {"title": "2.1. Overview", "content": "Fig. 1 outlines our model architecture. We train three distinct models, each focusing on a specific set of labels: 'Head', 'Tail', and 'All'. Each model comprises ConvNeXt, positional encoding [31], and the ML-Decoder. Final predictions for the 'Tail' and 'Head' classes are averaged with the corresponding class prediction in the 'All' model, except for the 'support device' class, which will be further explained in ensemble learning section."}, {"title": "2.2. Dataset", "content": "We utilize the Pruned MIMIC-CXR-LT [8], which comprises 257,018 frontal CXRs, each labeled with one of 19 clinical findings. Tailored to address the challenges of long-tailed, multi-label classification of thoracic diseases in CXR images, this dataset addresses the complexity of real-world medical imaging, where a few common findings are followed by many rarer conditions.\nIn our study, the dataset was divided into training, validation, and testing sets, containing 182,380, 20,360, and 54,268 images, respectively. Images undergo resizing to a uniform dimension of 256 \u00d7 256 pixels before model input. Fig. 2 illustrates the long-tailed nature of the dataset, with the most common class having 104,364 samples compared to just 553 samples in the least common class, highlighting the substantial disparity in class frequency."}, {"title": "2.3. ConvNeXt and ML-Decoder", "content": "ConvNext [17] blends the robust feature extraction of CNNs with the contextual comprehension of attention models, enhancing both depth and width for comprehensive image analysis. This innovative approach, combined with its proven efficacy in classification, transfer learning, and domain adaptation, making it the chosen backbone for feature extraction. In multi-label classification, transformer-decoder architectures prove beneficial for datasets with a limited class range but struggle to scale due to their high computational requirements, which grow quadratically with class size. On the other hand, ML-Decoder [23], which also utilized in a similar task [13], offers a viable alternative by modifying the traditional transformer decoder framework, notably through the removal of self-attention mechanisms to improve efficiency and the adoption of group-decoding to increase scalability independently of class count. Motivated by the reduced computational demands and its demonstrated success, we selected the ML Decoder as our classification head."}, {"title": "2.4. Augmentation", "content": "Data augmentation can significantly enhance the performance and robustness of machine learning models [27]. We have explored and assessed a range of traditional data augmentation techniques, providing insights into our selected methods. Our strategies encompass rotation to acquaint models with different perspectives, padding to preserve image consistency, brightness adjustments to emulate various lighting environments, Gaussian blur to introduce controlled blurring mirroring real-world scenarios, contrast manipulation to extract better features, and posterization to reduce tonal levels for noise resilience."}, {"title": "2.5. Ensemble Learning", "content": "Ensemble models often achieve higher accuracy by leveraging diverse perspectives on the data, reducing the risk of overfitting and lowering the noise in the dataset [20, 24]. In our study, 'Head' and 'Tail' are subsampled from the dataset, while 'All' represents the entire dataset following the approach outlined in prior research [11]. In this division, the 'Head' encompasses the nine most prevalent classes, while the 'Tail' contains the remaining ten categories and the 'Support device' and the 'All' comprises every class. Note that the 'Support device' category appears in both the 'Head' and 'Tail' due to its prevalence in the dataset. Excluding it would result in the 'Tail' having an insufficient number of training samples."}, {"title": "2.6. Evaluation Metrics", "content": "We employ the mean Average Precision (mAP) [7] and the macro F1 score (mF1) to evaluate model performance. The mAP metric computes the average area under the precision-recall curve for each class, while mF1 determines the average F1 score across all classes. Both metrics treat the performance of each class as equally significant, making them particularly appropriate for datasets with imbalanced class distributions.\nTo assess fairness of our model, we employ the Equality of Opportunity (EO) [2], which is essential to ensure that the False Negative Rate (FNR) is consistent across different demographic attributes, a critical factor in avoiding the misclassification of ill patients as healthy.\nWe use D to denote the set defined by demographic attributes. For instance, when considering the demographic attribute of gender, D encompasses the demographic categories male and female. The variables \\(Y_i\\) and \\(\\hat{Y_i}\\) denote the predicted and actual labels for the ith class, respectively. The FNR for the ith class, considering demographic categories a, is given by:\n\\[FNR(i,a) = p(\\hat{Y_i} = 0|D = a, Y_i = 1),\\]\nand EO across m classes, we compute:\n\\[EO = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{min_{a \\in D} FNR(i, a)}{max_{a \\in D} FNR(i, a)}.\\]\nSpecifically, we utilize the Youden index to locate the optimal cut-off points on the ROC curve for the FNR."}, {"title": "2.7. Implementation Details", "content": "Implemented using PyTorch, our model employs the ConvNeXt-small [17,33], pre-trained on ImageNet [6], with all input images resized to 256 \u00d7 256 pixels. The binary cross-entropy loss function is utilized in our task. Optimization is carried out using the Lion optimizer [4,30], with a learning rate set at 6 \u00d7 10-6 and weight decay at 5 \u00d7 10-5, complemented by the use of GradScaler to enhance training efficiency. Batch size is set to 32."}, {"title": "3. Results and Discussion", "content": null}, {"title": "3.1. Performance Evaluation", "content": "In Fig. 3, we present a comparison of the Receiver Operating Characteristic (ROC) Curves and the Area Under the Curve (AUC) score across 19 categories. Due to the nature of AUC calculations, categories with fewer samples may exhibit artificially high AUC values, failing to represent real-world performance accurately [5]. For example, the category of 'Subcutaneous Emphysema', which includes a limited number of cases, shows an elevated AUC. This is attributed to the rarity of this condition in the sample set, potentially skewing the perceived accuracy of the model. Consequently, we advocate for using the mAP as a more balanced measure of model efficacy. Fig. 4 organizes diseases along the y-axis by decreasing frequency. Our LTCXNet demonstrates improved performance across all classes compared to the baseline ConvNeXt. Notably, LTCXNet significantly enhances the performance of tail classes. The top three classes with the most improvement are 'Pneumoperitoneum', 'Pneumomediastinum', and 'Fracture', with improvements of 79%, 48%, and 34%, respectively. This underscores LTCXNet's effectiveness in elevating results for these less frequent groups."}, {"title": "3.2. Backbone Evaluation", "content": "We conducted experiments to compare multiple backbone architectures and determine the most effective model for our task. As illustrated in Table 1, the ConvNeXt v1 Small model demonstrated superior performance, achieving the highest validation and testing mAP. This indicates that ConvNeXt v1 Small is the best-suited model for our task."}, {"title": "3.3. Ablation Study", "content": "Table 2 presents the outcomes of the ablation study. We observe an incremental trend in mAP as each component is added, while mF1 decreases when ensemble techniques are applied. We choose mAP as the primary evaluation metric since, unlike mF1, which focuses on a single threshold, mAP assesses performance across all thresholds, providing a more comprehensive view. Therefore, we still adopt the ensemble method in LTCXNet. The choice of mAP ensures a more reliable evaluation of the method's effectiveness, and the results of the ablation study confirm that each component contributes positively to overall model performance."}, {"title": "3.4. Grad-CAM Visualization", "content": "The Grad-CAM [25] visualization, depicted in Fig. 5, showcases our model's predictive output for three distinct lung diseases. In this color-coded visualization, red areas indicate the regions the model primarily focuses on, revealing that these highlighted sections of the input CXR image significantly influence the model's predictions. Specifically, Fig. 5a illustrates the condition 'Pleural Effusion', which typically manifests in the lower lung regions. The Grad-CAM visualization of our model aligns with this clinical presentation, confirming that the focused region corresponds to the actual lesion site. This concurrence underscores the model's practical utility and its remarkable accuracy in identifying and localizing lung pathologies."}, {"title": "3.5. Fairness Evaluation", "content": "In addressing the long-tailed problem, the tail class suffers from an extremely small sample size. This small sample size makes the tail class highly vulnerable because even slight demographic distribution biases can result in significant proportional biases. Consequently, it is essential to evaluate how a method's performance may vary across different demographic attributes.\nThe experiment was conducted on the test set of Pruned MIMIC-CXR-LT, excluding the 'Calcification of the Aorta' and 'Tortuous Aorta' categories, as certain demographic groups lack positive labels for these conditions. We assess fairness with two demographic attributes: race and gender. Race is categorized into five groups: White, Black, Hispanic, Asian, and Other. Gender is differentiated into two groups: male and female. In Table 3, higher EO is desirable, as it indicates uniform FNR across different demographic groups. In the 'All' result, in terms of race, data augmentation demonstrates the best EO performance, while both the ML-Decoder and ensemble methods failed to improve EO. However, for gender, although data augmentation boosts performance, it doesn't fully offset the drop caused by using ML-Decoder. ML-Decoder continues to negatively impact EO, while the ensemble method shows mixed results, performs well on tail classes but poorly on head classes. Thus, in contexts where FNR is critical, such as disease screening, data augmentation emerges as a promising approach. Prior studies have also confirmed the effectiveness of data augmentation techniques in improving model fairness. [18,26].\nWhen examining the 'Head' and 'Tail' separately, as shown in Table 3, first, the 'Head' generally demonstrates better fairness performance. This is likely due to the smaller sample size of the 'Tail' classes, which leads to increased bias. Second, our method positively impacts fairness for the 'Tail', indicating its effectiveness for long-tailed classification where unbiased evaluation of the tail classes is crucial."}, {"title": "3.6. Comparison with Previous Approaches", "content": "In this section, we explore various approaches used for dealing with dataset imbalance and multi-label classification. The subsequent article will briefly introduce these methods, outline the experimental configurations and provide insights into the potential reasons for their lack of success.\nFeature decoupling cRT In the field of single-label long-tail datasets, the feature decoupling cRT method, is considered highly effective in previous research [12]. However, our dataset's multi-label characteristic, featuring overlapping labels, rendered traditional re-sampling for balanced data impractical. Thus, we implemented a modified re-sampling approach, ensuring each class had at least 0.7 times the occurrence of the smallest class. We generated five distinct datasets with different random seeds, which were then used in the second stage of the feature decoupling CRT method to fine-tune the model.\nIn our investigation, we discovered that feature decoupling cRT did not improve performance, likely due to two main factors. First, the re-sampled dataset remained unbalanced, with the largest class being five times bigger than the smallest class. Second, prior research [10] indicates that although feature decoupling cRT improves balanced accuracy (the weighted mean of accuracy) on imbalanced test sets, it slightly decreases the mF1 score. Given that mAP is a similar metric, which considers both precision and recall, this may explain its sub-optimal performance.\nWeighted loss We experiment two variants of weighted loss to manage the class imbalance. The first variant was the weighted binary cross entropy loss, where we determined the class weights by dividing the count of negative samples by the count of positive samples for each class. The second variant is the focal loss [16], where the alpha parameter was inversely related to the class performance, and we set the gamma parameter to a value of two as the original paper recommended. These weighted loss functions aimed to adjust the model's focus on classes with fewer samples and to mitigate the dominance of more prevalent classes.\nThe ineffectiveness of weighted loss arises from several key issues. First, by assigning higher weights to minority classes, the performance of previously well-performing classes is adversely affected. In particular, because the tail classes are so small, they require a disproportionately large weight, which makes the model overly sensitive to these classes. This sensitiViTy often fails to result in improvements for the weaker classes. Consequently, this leads to a decrease in mAP. Additionally, in multilabel datasets, where classes are not independent, heavily weighting certain labels can disrupt the learning of complex inter-dependencies between classes, further degrading overall model performance.\nRandom oversampling Random oversampling is a technique used to address class imbalance in datasets by increasing the number of instances in under-represented classes. This is achieved by randomly replicating instances from these classes until a more balanced distribution is reached. In our study, each class with fewer instances than a predetermined threshold was augmented by randomly duplicating images until the class size met this threshold. It's worth noting that in the multi-label dataset, when we oversample the minority classes, there's a possibility of oversampling the majority classes concurrently. Despite the theoretical benefits of random oversampling in addressing class imbalance, our study observed performance degradation. This outcome can be attributed to overfitting on minority classes, as the repeated use of the same images in oversampling may reduce the model's generalization ability.\nSelf-supervised learning Self-Supervised Learning (SSL) is a machine learning approach that trains models on datasets without requiring labeled data. In image classification tasks, particularly for long-tailed datasets, an SSL pre-trained backbone is believed to help create a more balanced feature space, mitigating biases caused by uneven class representations. SimCLR [3], a prominent SSL method, enhances representation learning by maximizing agreement between various augmented versions of the same data sample. In our research, we employed SimCLR to pre-train a backbone for feature extraction from CXRs without using labels. This pre-trained backbone was then adapted for a multi-label classification task.\nHowever, the SimCLR pre-trained backbone resulted in a decline in classification performance. This can be attributed to several factors. Firstly, the data augmentation techniques used in SimCLR might distort important features in medical images and the subtle and complex patterns in CXRs might not be effectively captured. Moreover, the severe class imbalance in long-tailed datasets may not be well-handled when fine tuning. Finally, the presence of watermarks on some CXRs can taint the features learned by the unsupervised method, as it lacks label information to distinguish relevant features from artifacts."}, {"title": "3.7. Clinical Feasibility", "content": "Our method, LTCXNet, has a computational cost of 35 GFLOPs and can infer a single CXR within a second on a cost-effective GTX 1080, demonstrating its feasibility for clinical use. This efficient performance indicates that LTCXNet can be seamlessly integrated into clinical workflows, providing timely and accurate diagnostic support."}, {"title": "4. Conclusion", "content": "We introduce LTCXNet to address the challenges of long-tailed, multi-label classification tasks. Through the evaluation of various methodologies on the Pruned MIMIC-CXR-LT dataset, we have identified a configuration that achieves optimal performance and significantly enhances the outcomes for the tail classes. Beyond performance, we also examine the impact of these methods on the model's fairness and the clinical practicability of LTCXNet. This research aims to advance the field of long-tailed, multi-label classification in medical imaging, contributing to more accurate and fair diagnostic tools."}]}