{"title": "Bridging Episodes and Semantics: A Novel Framework for Long-Form Video Understanding", "authors": ["Gueter Josmy Faure", "Jia-Fong Yeh", "Min-Hung Chen", "Hung-Ting Su", "Winston H. Hsu", "Shang-Hong Lai"], "abstract": "While existing research often treats long-form videos as extended short videos, we propose a novel approach that more accurately reflects human cognition. This paper introduces BREASE: BRidging Episodes And Semantics for Long-Form Video Understanding, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels. Second, we propose a Semantics reTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. Extensive experiments demonstrate that BREASE achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings. The project page and code are at: https://joslefaure.github.io/assets/html/hermes.html.", "sections": [{"title": "1. Introduction", "content": "Whether we want to programmatically create better video summarization tools, index and retrieve specifics from the vast and ever-expanding array of video content, or improve content moderation and copyright enforcement, we need models that excel at video understanding. Not just short videos with few frames\u2014image models can already handle that-but models that can learn to understand minutes-long videos with thousands of frames.\nLong-form video understanding is challenging for sev-"}, {"title": "2. Related Work", "content": "Action recognition is an essential task in video understanding, primarily focusing on identifying specific actions within short video clips. Various approaches have been developed, with convolutional neural networks (CNNs) forming the core of many methods. Early work by [11] utilized 3D convolutions, while [22] employed temporal convolutions. More recently, transformer-based models have gained prominence, as demonstrated in works such as [7], [26], and [28].\nVideo question answering (VQA) aims to answer questions related to video content, requiring a deep understanding of both visual and textual information. Datasets such as ActivityNet-QA [27] and MovieChat-1k [19] provide benchmarks for evaluating models in this field, allowing for several research endeavors on this subject [17, 30, 31].\nLong-form video understanding presents unique challenges due to the extended duration and complex narrative structures involved. Datasets with these properties include LVU [24], COIN [20], Breakfast [12], and more recently, MovieChat [19]. Traditional approaches to tackling such a task often extend methods designed for short videos to handle longer sequences. Other works, such as [8, 24, 25] and [19] explore memory techniques, emphasizing the need for more sophisticated models capable of managing large temporal spans. More recently, State-Space Models (SSMs) have been applied to long-form videos by [23], [9] and [10], exploiting its ability to reference long-term context.\nLLM-based Long-Form Video Understanding: Recent advancements in large language models (LLMs) [4, 21] have piqued researchers' curiosity regarding their use for video understanding [15]. It turns out to be a good match, as understanding videos often involves transforming their content into words, whether it's video captioning, video question answering, or even action classification. [19] and [8] propose frameworks that employ memory techniques to handle extensive video content while [18] explicitly conditions the model to manage time-dependent information.\nOur method falls into the latest category; however, this paper is not about a new LLM or a new way to fine-tune existing LLMs. It focuses on leveraging what we know about how humans understand visual scenes to guide the model through understanding long videos. The LLM is used merely for autoregressive prediction."}, {"title": "3. Method", "content": "Given a video, short or long, and a set of instructions specifying what to do with the video, our method can return the specified output, such as video question answering (VQA), video classification, or video captioning. It achieves this by leveraging two important properties of human understanding of scenes: episodic memory, which involves determining and stratifying a sequence of frames with similar properties, and semantic knowledge, which can help answer broad questions about the scene (e.g., does it occur at night or during the day?). We refer to the former as ECO, detailed in Sec. 3.2, and to the latter as SeTR, described in Sec. 3.4."}, {"title": "3.1. Window Encoder", "content": "Our method begins with a video of arbitrary length. To batch process the video, we first specify a number of frames N to extract, leading to $v = \\{f_1, f_2,..., f_N\\}$, where $f_t$ denotes the t-th frame. The encoder, a ViT-G/14 model from [6], progressively encodes non-overlapping windows w of the video data. The window size w is a divisor of N and determines how many frames to encode at once. The features of each window can be denoted as $F_{w,i} \\in \\mathbb{R}^{B \\times w \\times T \\times C}$, where $F_w$ are the extracted features, i the i-th window, B the batch size, T the number of visual tokens and C, the number of channels. $F_w$ are then passed on to the Episodic COmpressor (ECO) described in Sec. 3.2."}, {"title": "3.2. ECO: Episodic COmpressor", "content": "See Fig. 1 lower left side for an intuitive illustration of ECO. This module maintains a memory buffer with a maximum number of episodes E. Upon receiving a window of $F_w$ frame features, we first check whether the existing buffer M has sufficient bandwidth to support the incoming features. If it does, we simply concatenate them to the buffer; otherwise, we proceed with the compression. At its core, ECO is a distribution process that determines the episode to which a certain frame belongs. It can be summarized as:\n$$M' = \\begin{cases}M \\oplus F_w & \\text{if } ||M|| + ||F_w|| \\leq E\\\\ECO(M, F_w) & \\text{otherwise}\\end{cases}$$\nWhere $\\oplus$ is the concatenation operation, $||M||$ and $||F_w||$ the sizes of the buffer and the incoming features, respectively.\nECO works as follows. As long as concatenating the new window and the existing buffer results in a size greater than E, we compute the cosine similarity between each pair of frame features in $M \\oplus F_w$. We then iteratively merge the most similar frames until the size constraint E is satisfied. Specifically,\n$A = M \\oplus F_w$\nwhile $||A|| > E$:\n$(i^*, j^*) = \\underset{i \\neq j}{\\text{arg max}} \\frac{A_i A_j}{||A_i|| ||A_j||}$\n$A_{i^*} = \\frac{(A_{i^*} + A_{j^*})}{2}$\n$A = A \\setminus A_{j^*}$\n$M' = A$\nwhere M is the existing buffer, $F_w$ represents the incoming window of $w$ frame features, A is the concatenated buffer and new window, and $||A||$ the size of A. To summarize Eq. (2), $\\frac{A_i A_j}{||A_i|| ||A_j||}$ computes the cosine similarity between frame features $A_i$ and $A_j$, $\\underset{i \\neq j}{\\text{arg max}}$ finds the pair of frames with the highest cosine similarity, $\\frac{(A_{i^*} + A_{j^*})}{2}$ combines the most similar frames, and $A \\setminus A_{j^*}$ removes the frame $A_{j^*}$ from A after merging. The process repeats until the size of A is within the maximum allowed episodes E, and A becomes the new buffer M'.\nSimilarities can be drawn with [8], where cosine similarity serves as the basis for frame reduction. However, their approach is notably inefficient and less intuitive. For a buffer of size S, they iterate S times until the buffer reaches capacity, after which each new incoming frame is compared against every other frame in the buffer."}, {"title": "3.3. Episodic Q-Former", "content": "The Episodic Q-Former uses the same architecture as the original Q-Former [13] and is loaded with weights pre-trained by [5]. However, we introduce a pruning approach"}, {"title": "3.4. SeTR: Semantics reTRiever", "content": "To capture higher-level semantic information from our video features, we introduce SeTR (Semantics reTRiever). SeTR is designed to identify and consolidate important information that may be scattered non-contiguously throughout the video. Given a video feature tensor $F \\in \\mathbb{R}^{B \\times N \\times T \\times C}$, where B is the batch size, N is the number of frames, T is the number of tokens per frame and C is the channel dimension, SeTR operates as follows: we first normalize F to ensure consistent scaling across features. Second, we apply a stride of k to create two groups, group X containing every k-th frame, resulting in $\\frac{N}{k}$ frames and group Y with the remaining $N - \\frac{N}{k}$ frames. Third, we calculate dot produce similarity scores between frames in X and Y. Finally, for each frame in Y, we merge it with its most similar frame in X, based on the computed scores by taking their mean.\nThis process effectively reduces the number of frames from N to $\\frac{N}{k}$, consolidating semantic information while maintaining the most relevant features across the video timeline. We evaluate the effectiveness of this approach in Section 4.3. A similar compression method is used by [1]. However, they employ it to reduce tokens within individual frames, specifically between different layers of a Vision Transformer. In contrast, SeTR aggressively drops redundancies at the frame level."}, {"title": "3.5. From Representations to Natural Language", "content": "The semantic representations extracted by SeTR are processed through a Hierarchical Q-Former. These processed"}, {"title": "4. Experiments", "content": "We evaluate our approach on two primary tasks: long-form video classification and long-form video question answering. For long-form video classification, we utilize the LVU dataset [24] which focuses on movie content and metadata, and for long-form video question answering, we employ the recently introduced MovieChat dataset [19]. Using the latter dataset, we test our model under both zero-shot and fully supervised scenarios."}, {"title": "4.2. Results", "content": "We present our results in Tab. 1 and Tab. 2 for the LVU [24] and MovieChat [19] datasets, respectively. Our method achieves state-of-the-art results across both these datasets. Notably, we report significant accuracy gains of 7.3% on LVU and 14.9% on MovieChat, significantly surpassing the"}, {"title": "4.3. Ablations", "content": "All ablations are conducted on the MovieChat test set using the zero-shot setting. These experiments focus on our two primary contributions, ECO and SeTR.\nOn the importance of ECO. In Tab. 3, we demonstrate the critical role of ECO through several experiments. Firstly, the results clearly indicate that the absence of our episodic compressor leads to a significant degradation in model performance. We further explore alternative update strategies, including randomly selecting features to retain (Rand.) and employing a first-in-first-out (FIFO) streaming approach. Our proposed update strategy outperforms both the Rand. and FIFO methods. It is worth noting that during the Rand. and FIFO ablations, SeTR remains active.\nOn the importance of SeTR. SeTR is designed to complement the episodic knowledge of our model with semantic insights. In Tab. 4, we observe that removing SeTR results in a substantial 5% drop in accuracy. Additionally, we show that naive methods such as max pooling and average pooling do not work as well as SeTR."}, {"title": "5. Conclusion", "content": "We introduce BREASE: BRidging Episodes And SEmantics, a novel framework designed to enhance long-form video understanding through two key compo-"}]}