{"title": "From N-grams to Pre-trained Multilingual Models For Language Identification", "authors": ["Thapelo Sindane", "Vukosi Marivate"], "abstract": "In this paper, we investigate the use of N-gram models and Large Pre-trained Multilingual models for Language Identification (LID) across 11 South African languages. For N-gram models, this study shows that effective data size selection remains crucial for establishing effective frequency distributions of the target languages, that efficiently model each language, thus, improving language ranking. For pre-trained multilingual models, we conduct extensive experiments covering a diverse set of massively pre-trained multilingual (PLM) models \u2013 mBERT, RemBERT, XLM-r, and Afri-centric multilingual models \u2013 AfriBERTa, Afro-XLMr, AfroLM, and Serengeti. We further compare these models with available large-scale Language Identification tools: Compact Language Detector v3 (CLD V3), AfroLID, GlotLID, and OpenLID to highlight the importance of focused-based LID. From these, we show that Serengeti is a superior model across models: N-grams to Transformers on average. Moreover, we propose a lightweight BERT-based LID model (za_BERT_lid) trained with NHCLT + Vukzenzele corpus, which performs on par with our best-performing Afri-centric models.", "sections": [{"title": "Introduction", "content": "Automatic language identification (LID) is the task of determining the underlying natural language used in a written or spoken corpus (McNamee, 2005). This is a challenging problem, especially for languages with insufficient training examples and closely related languages, particularly low-resourced languages (Haas and Derczynski, 2021). For South African languages, building quality LID technologies is significantly important for sourcing internet data, which has served as a de-facto repository for many low-resourced languages, especially from public domains such as news websites (Marivate and Sefara, 2020; Adelani et al., 2021; Dione et al., 2023; Adelani et al., 2023; Lastrucci et al., 2023).\nStatistical approaches for automatic LID such as N-grams (Dube and Suleman, 2019), and more classical machine learning models such as Logistic Regression, Naive Bayes, Random Forest, Boosting machines, Support Vector Machines, and Clustering techniques (e.g K Nearest Neighbors) have been proposed (Haas and Derczynski, 2021). Moreover, contemporary neural-based architectures such as deep neural networks and convolutional neural networks have also been tested. In all cases, not enough work for the South African languages is reported.\nOn the other hand, recent algorithmic advancements such as transformer architectures have made a significant impact on the Natural Language Processing landscape (Devlin et al., 2018; Conneau et al., 2019). With this sudden shift in perspective, many works have proposed automatic LID using large pre-trained multilingual models, derived from attention mechanisms (Vaswani et al., 2017). Large pre-trained multilingual models are transformer-based architectures simultaneously trained on multiple languages (hence multi-lingual) using various techniques such as token (s) masking training technique, where tokens from a given sentence example are hidden and the objective of the training transformer is to predict the hidden word (s).\nIn this work, we make use of the recently released Vuk' zenzele crawled corpus (Lastrucci et al., 2023) and the NCHLT dataset (Eiselen and Puttkammer, 2014) to develop and experiment on automatic language identification models on 10 low-resourced South African languages: Northern Sesotho (nso), Setswana (tsn), Sesotho (sot), isiZulu (zul), isiXhosa (xho), isiSwati (ssw), isiNdebele (nbl), Tshivenda (ven), Xitsonga (tso), and Afrikaans (af). Additionally, we included the high-resource South African English (eng) to ensure representation of all 11 official languages in South"}, {"title": "Related Work", "content": "Large pre-trained multilingual models have shown astonishing state-of-the-art results on various Natural Language Processing (NLP) tasks such as Machine Translation, Question Answering, and Sentiment Analyses (Stickland et al., 2021; Yang et al., 2019; Adebara et al., 2023b). A precursor of these tasks is the crawling of large volumes of internet data and categorizing the data into different languages (i.e. language identification) for pre-training. For language identification, many works have used pre-trained multilingual models to expand monolingual datasets using the internet.\nJauhiainen et al. (2021) conducted a comparative study between adaptive Naive Bayes, HeLI2.0, multilingual BERT, and XLM-r models for Dravidian language identification in a code-switched context (i.e. a conventional modus operandi for communication on the internet). Caswell et al. (2020) developed a transformer-based LID model aside from basic filtering techniques such as tunable-precision-based filtering using a created wordlist, TF-IDF filtering, and a percent-threshold filtering threshold proposed in their study to filter noisy web-crawled content. Although they were able to collect corpora for over 212 languages, their set-up for their best-performing transformer model was unclear. Similar to our work, Kumar et al. (2023) conducted a comparative study on DistilBERT, ALBERT, and XLM-r and showed that a lightweight version of DistilBERT delivers comparable results to resource-intense models. Adebara et al. (2022), on the other hand, implemented a massive transformer-based LID model with 12 attention layers and heads. They then trained this model on 512 languages with close to 2 million sentences across 14 language families (South African languages included). Their model achieved over 95 % F1 score on a left-out test sets, outperforming available LID tools: CLD version 2, Langid, Fast-"}, {"title": "Methodology", "content": "The methodology employed in this study uses language-identifiable monolingual corpora from reliable sources as training examples for language identification and compares various pre-trained multilingual models for the task of discriminating between languages."}, {"title": "Corpora", "content": "Text corpora for the 11 South African languages were acquired from two sources: Vuk'zenzele (Vuk) (Lastrucci et al., 2023) and National Centre for Human Language Technology (NCHLT) corpora (Eiselen and Puttkammer, 2014)."}, {"title": "Pre-processing", "content": "The dataset is observed to contain links, digits and therefore our pre-processing included the removal of URLs, digits, punctuations, and followed by lower-casing all sentences using Python regular"}, {"title": "Language detection algorithms", "content": ""}, {"title": "N-grams", "content": "An N-gram is a sequence of consecutive characters from text (Dube and Suleman, 2019). This study explored character Bi-grams (2 consecutive characters), Tri-grams (3 consecutive characters), and Quad-grams (4 consecutive characters) models. We build each model for each language from the training dataset (Vuk, NCHLT, and Vuk +\nNCHLT). Furthermore, we experimented with various data sizes to investigate the impact of the number of training examples on N-gram models and this showed a performance ceiling, where an increase in training examples does not significantly"}, {"title": "Naive Bayes Classifier", "content": "Naive Bayes have been the default standard for various LID tasks such as code-switching detection, dialect discrimination, word-level language detection, and e.t.c. (Dube and Suleman, 2019; Jauhiainen et al., 2019). In this study, we experimented with the multinomial Naive Bayes Classifier (NBC) implementation from Python's scikit-learn. With NBC, we were able to extract discriminating features per language, supporting model prediction (Figure 8), and significantly improved on N-gram models (see confusion matrix in Figure\n9). This highlighted important feature correlation, especially for related languages, which explains why it is challenging to discriminate among closely related languages. Moreover, this highlights the importance of lexicon-driven approaches for language filtering mentioned in Caswell et al. (2020) as alternative measures to mitigate these ambiguities.\nNaive Bayes Classifier experimental setup We experimented with a TF-IDF vectorizer to generate input features. For this, we used the character bi-gram, tri-gram, quad-gram, and the 3 types combined as consecutive subwords to generate TFIDF features. We also generated word level input features using CountVectorizer. We used a multinomial version of the Naive Bayes classifier with mostly default parameters from scikit-learn (except the alpha parameter where we tested \\( \u03b1 \\) = 0.0001,1.0, where \\( \u03b1 \\) = 1.0 performed better). Finally, we trained Support Vector Machine (SVM), K Nearest Neighbor (KNN), and Logistic Regression with the same input features and their scikit-learn default parameters to compare performance outcomes with NBC."}, {"title": "Pre-trained Multilingual Models", "content": "This study explored a diverse set of massively pre-trained multilingual models: mBERT, XLM-r, RemBERT, and their Afri-centric counterparts: AfriBERTa, Afro-XLMr, AfroLM, and Serengeti due to their enhanced text processing capabilities and their ability to handle low-resourced languages"}, {"title": "Results", "content": ""}, {"title": "Baselines", "content": "Table 2, shows results for baseline models Bi-gram, Tri-gram, Quad-gram, N-gram combined (N-gram\nComb) \u2013 which uses bi-, tri-, and quad- -grams combined, and Naive Bayes Classifier (NBC) with\nthe same character N-grams. Naive Bayes with\nword-level features outperform the rest of the baseline models. Interestingly, for NBC, increasing\nthe character spans improves the performance of\nthe classifier. Figure 10, 11, 12, 13, 14, and 15\ndepicts the impact of increasing the data size on\nmodels NBC, Support Vector Machine (SVM), and\nLogistic Regression (Log Reg) on various training\nfeatures - uni-grams, bi-grams, tri-grams, quad-\ngrams, N-grams combined, and word-level features\nderived using TF-IDF respectively. NBC, SVM,\nand Log Reg show improved performance with\nthe change in input features while the training size\nshows gradual improvement in accuracy. KNN was\nalso tried, however, the model showed abysmal per-\nformance across all features except for Bi-gram\ninput features and was therefore omitted from the\nplots.\nIn the N-gram class, the Quad-gram ranking\noutperforms the rest of the N-gram-based models.\nFigure 17, depicts the impact of sentence length\non N-gram models performance. This shows that\nthe group of N-gram models struggles to classify\nshorter sentences, while NBC performs slightly\nbetter with them (Figure 18). This may be due to\nshorter sentences not carrying enough signal in-\nformation for N-grams to discriminate across all\nlanguages as mentioned in Haas and Derczynski\n(2021). Additionally, N-gram-based models depict\ninconsistent performance across languages, where\nimproved performance is achieved for select lan-\nguages and for a specific N-gram type (E.g Bigram\neng, ven, af, e.t.c, Tri-gram \u2013 eng, tso, nso, e.t.c),"}, {"title": "Pre-trained Multilingual Models", "content": "Table 3 reports the accuracy (Acc), precision (Prec),\nrecall (Rec), and F1 score (F1) of pre-trained mul-\ntilingual models: mBERT, XLM-r, RemBERT;"}, {"title": "Cross-domain evaluation", "content": "We also wanted to test our model on cross-domain datasets to inspect their generalization capabilities. We simulated this by training with Vuk data and tested it on NCHLT, and vice versa. Table 4 reports the performance of pre-trained models for examining the cross-domain evaluation theory. This table shows that the performance of the multilingual models trained with Vuk and tested with NCHLT dropped by approximately (4%-5%) across all mod-"}, {"title": "Discussions", "content": "Ensuring the development of robust LID detection systems remains a critical research area with implications on many NLP tasks. Importantly, the availability of reliable LID systems ensures accurate reporting on the state of low-resourced languages (Kreutzer et al., 2022).\nOn the side of model performance, baseline techniques such as Naive Bayes, Support vector Machines, and Logistic Regression seem to be performing quite well on the task of sentence-level language identification. We recommend these models for further research for high-level LID, compared to large pre-trained multi-lingual models which require specialized computing resources such as GPUs, to accelerate training. However, we deem such trade-offs to require more research, especially"}, {"title": "Conclusion", "content": "Language Identification remains a critical study area for the widespread inclusion of many low-resourced languages into the booming technology space. In this study, we experimented with statistical approaches, traditional machine learning techniques, the recent advanced pre-trained multilingual models, as well as LID tools publicly available (covering a wide range of African languages) on the task of LID for 11 South African language discrimination. We were able to shed light on the approaches showing promising results in the South African language context and made suggestions for future directions. Concretely, we showed that the Naive Bayes algorithm performs surprisingly well for LID and warrants further exploration and exploitation, especially given its cheap-compute advantage. Finally, we compared publicly available pre-trained models and showed that context-"}, {"title": "Limitations", "content": "In this study, we did not explore any use of word embeddings for language identification. Word embeddings played in crucial role in the development of language technologies, and it would have been interesting to experiment with them. However, such resources are not readily available for many low-resourced languages.\nAside from experimenting and getting results for other traditional models such as Logistic regression, K Nearest Neighbor, and Support Vector Machines, it would have been interesting to develop and experiment with deep neural networks such as multi-layered perceptions, and convolutional neural networks. As universal approximators, these models tend to produce desirable results, with the caveat of requiring time for hyper-parameter tuning.\nThis study did not extensively explore the impact k (used 50 for this study), which is the count of the\nN-grams list used to calculate the ranking. However, we aim to explore this extensively in future works.\nIt is known that LID techniques tend to overfit to domain data, and therefore it would have been interesting to create free-text data created by humans and test the generalization capabilities of the developed models on human-generated text.\nRecent studies have focused on resource-conscience alternatives for either compute efficiency, parameter reduction, etc. It would have been interesting if this work would have explored the recently active approaches focusing on smaller models utilizing parameter transfer, and adaptations (Kumar et al., 2023). However, these techniques require intense hyper-parameter selection and tuning, and slightly longer training times, which was not in the scope of this study.\nFinally, we aim to incorporate BANTUBERT 1, and zaBANTUBERT 2 models trained with monolingual South African corpora in our future work."}]}