{"title": "(Unfair) Norms in Fairness Research: A Meta-Analysis", "authors": ["Jennifer Chien", "A. Stevie Bergman", "Kevin R. McKee", "Nenad Tomasev", "Vinodkumar Prabhakaran", "Rida Qadri", "Nahema Marchal", "William Isaac"], "abstract": "Algorithmic fairness has emerged as a critical concern in artificial intelligence (AI) research. However, the development of fair Al systems is not an objective process. Fairness is an inherently subjective concept, shaped by the values, experiences, and identities of those involved in research and development. To better understand the norms and values embedded in current fairness research, we conduct a meta-analysis of algorithmic fairness papers from two leading conferences on AI fairness and ethics, AIES and FAccT, covering a final sample of 139 papers over the period from 2018 to 2022. Our investigation reveals two concerning trends: first, a US-centric perspective dominates throughout fairness research; and second, fairness studies exhibit a widespread reliance on binary codifications of human identity (e.g., \"Black/White\u201d, \u201cmale/female\"). These findings highlight how current research often overlooks the complexities of identity and lived experiences, ultimately failing to represent diverse global contexts when defining algorithmic bias and fairness. We discuss the limitations of these research design choices and offer recommendations for fostering more inclusive and representative approaches to fairness in AI systems, urging a paradigm shift that embraces nuanced, global understandings of human identity and values.", "sections": [{"title": "Introduction", "content": "The widespread adoption of artificial intelligence (AI) brings with it the potential for substantial harm. Al systems frequently encode and amplify historical biases (Buolamwini and Gebru, 2018; D'ignazio and Klein, 2020; Eubanks, 2018; Lum and Isaac, 2016; Zou and Khern-am nuai, 2023), thus exacerbating and perpetuating discrimination against marginalized communities (Benjamin, 2020; Eubanks, 2018; Irani et al., 2010). As a consequence, algorithmic fairness has emerged as a growing priority for AI developers, ethicists, policymakers, and regulators (Kleanthous et al., 2022; Lepri et al., 2018; Pfeiffer et al., 2023; Zarsky, 2016).\nThere is no single definition of fairness (Barocas et al., 2023; Mehrabi et al., 2021). Accordingly, approaches to algorithmic fairness vary widely, encompassing individual and group perspectives, parity and equity considerations, and beyond (Binns, 2020; Chien and Danks, 2023; Dwork et al., 2012). While this pluralism has allowed researchers to generate a diverse toolkit of metrics, techniques, and frameworks for mitigating bias, it also introduces a crucial challenge: the inherent subjectivity of fairness itself.\nFairness is not a universal, abstract concept. It is deeply intertwined with the values, experiences, and identities of those involved in the research process (Bunge, 2018; Fook, 1999; Guillemin and Gillam, 2004; Iliadis and Russo, 2016; McCabe and Holmes, 2009). Thus, fairness research is sensitive to contextual questions of who is doing the work, what they are studying, and how they are studying it. A comprehensive understanding of fairness necessitates a conscious effort to acknowledge and understand the values embedded in the research process itself."}, {"title": "Related Work", "content": "Our meta-analysis builds upon a growing body of research surveying and critiquing the fields of AI ethics and algorithmic fairness."}, {"title": "Data and Geographic Bias", "content": "In recent years, critical researchers have increasingly examined AI ethics and fairness research for geographic bias. Close scrutiny reveals that the datasets used within these fields often fail to accurately represent the global population. Septiandri et al. (2023) survey FAccT proceedings and finds a bias towards Western, educated, industrialized, rich, and democratic perspectives participants and data. Abdu et al. (2023) examine racial categories in FAccT proceedings through the lens of institutional influences and values, finding that projects adopt racial categories inconsistently, often following country-specific legal frameworks, while rarely explicitly describing or justifying their choices. Similarly, Koch et al. (2021) touch on the consequences of geographic bias in their exploration of the increasing, over-concentrated usage of a limited group of datasets in machine learning research."}, {"title": "Western-Centric Values and Perspectives", "content": "Al ethics research often exhibits a bias toward countries in the Global North, presenting their cultural perspectives and values as universal and globally fungible. Hagerty and Rubinov (2019) study this phenomenon in global social science scholarship and argue that mitigating the worldwide harms caused by AI deployments in the Global North will require deep understandings of a broad set of geographical, cultural, and social contexts. In an examination of the proceedings at FAccT and the ACM CHI Conference on Human Factors in Computing Systems, van Berkel et al. (2023) document a bias toward the US across study design, participant recruitment, and country affiliation of paper authors. Birhane et al. (2022b) critique ethics papers at AIES and FAccT as relying on speculative, theoretical foundations from Western philosophy, resulting in a dearth of concrete use cases and a lack of acknowledgement for afflicted communities. Sambasivan et al. (2021) similarly examine the philosophical roots, legal frameworks, and axes of discrimination within AI ethics to refocus on what values and norms translate and fail to translate to applications in India. Motivated by the non-portability of algorithmic fairness to India, Sambasivan (2021) reviews data practices in the machine learning pipeline and discusses the need for expanded representation, calling for the creation and maintenance of work focused on the Global South."}, {"title": "Methods", "content": "This study aims to examine the practices of problem selection and formulation within algorithmic fairness research. What values do researchers explicitly and implicitly elevate? How does each research project approach its object(s) of study? How does each project interpret and present its results?\nIn answering these questions, we aim to reflexively surface and record common practices within fairness research\u2014and subsequently to inform future best practice. We employed a meta-analysis methodology for our study, given its ability to synthesize and critically analyze existing research (Borenstein et al., 2021)."}, {"title": "Research Questions", "content": "To contribute to ongoing efforts to identify the normative choices and biases within fairness research, we investigate the following research questions:\nRQ1: What geographic and cultural biases are present in algorithmic fairness research?\nRQ2: What are the implicit norms embedded in the research process for algorithmic fairness, especially those affecting the diversity and representativeness of the research?"}, {"title": "Data and Analysis", "content": "Our review investigated papers from two flagship conferences on algorithmic fairness and Al ethics: the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) and the ACM Conference of Fairness, Accountability, and Transparency (FAccT). Both have grown substantially since their establishment in 2018, with interdisciplinary proceedings (including computer science, law and policy, social sciences, ethics and philosophy) and attendees (including researchers, policymakers, and practitioners). Together, the conferences represent principal sites of the research discourse on algorithmic fairness (Acuna and Liang, 2021; Birhane et al., 2022b). We collected all papers included in the conferences' proceedings from 2018 (their inception) through 2022 (the most recent year available during our data collection period), accessing the papers through the conference websites and the ACM Digital Library. This initial sample comprised 265 and 416 papers from AIES and FAccT, respectively. We subsequently removed non-archival and abstract-only papers, and then filtered our sample to papers that specify at least one formal fairness definition and one model-based decision-making process. This final sample included 139 papers (52 from AIES and 87 from FAccT, respectively).\nWe focus our annotation and analysis on several crucial aspects of the sampled research, including dataset characteristics, author information, and modeling choices. To ensure consistency in our data collection, we collaboratively developed and defined our coding scheme. During this process, we independently coded two papers and compared results to clarify any points of confusion or disagreement.\nWe begin by examining the datasets employed in each paper, recording the dataset name, type (e.g., tabular, text), and topic domain. We further document the sensitive attributes studied, noting the specific categorization scheme applied to attribute labels, label definitions, and collection methods. We pay particular attention to whether papers employed proxies to study sensitive attributes, whether the papers discussed intersectionality, and the extent to which papers acknowledged and mitigated limitations related to these design choices. For simplicity, we categorize the degree of these practices on a three-point scale: (no acknowledgement, acknowledgement, and acknowledgement and mitigation)."}, {"title": "Results", "content": "Quantitative Findings\nOverall, we examine a total of 139 papers, with 52 and 87 from AIES and FAccT, respectively (Figure 1a). These papers map to a total of 124 unique datasets. Retrospective studies (research empirically examining fairness within pre-existing datasets) represented the majority of papers, followed by theory (research contributing theoretical guarantees of fairness definitions). The prevalence of prospective studies (research collecting and empirically examining novel data) remained relatively low across most years, increasing the most in 2022 (Figure 1b). Dataset domains spanned a wide range of topics, including finance, criminal justice, health, and politics. Finance emerged as the majority domain within most years (Figure 1c).  provide additional results concerning fairness formulations.\nData and Authorship Provenance\nFigure 2 visualizes geographic patterns in authorship provenance and dataset provenance through cartograms. These depict country size proportional to their representation. In terms of author affiliation, the US held the greatest count and proportion of affiliations (412, 78.0%), followed by Germany (19, 3.5%), Canada (14, 2.7%), the United Kingdom (13, 2.5%), Australia (12, 2.3%), Italy (11, 2.0%), Switzerland (10, 1.9%), and India (10, 1.9%). When broken down by year, we find that 80.6% of the papers published each year feature at least one author with ties to the United States (see e.g. Appendix: Data and Authorship Provenance). For dataset provenance, the US again emerged as the most common country of origin (77, 72.6%), followed by Germany (9, 8.5%) and then Colombia, Rwanda, Australia, Burundi, Costa Rica, Finland, Hungary, Iceland, India, Kenya, Malawi, Mexico, Mozambique, New Zealand, Portugal, Senegal, South Africa, Sweden, Switzerland, Tanzania, Thailand, Uganda, the United Kingdom, Zambia, and Zimbabwe, all with fewer than three datasets (\u22642.8% each, 18.9% collectively). In our sample, 15.9% of papers acknowledge at least one dataset limitation. In addition, 1.4% employ some form of mitigation, such as introducing additional data (e.g., Dixon et al., 2018) or re-annotating the data with an expert (e.g., Buolamwini and Gebru, 2018).\nSensitive Attributes\nTo account for the wide range of variables studied by the algorithmic fairness community, we define \"sensitive attributes\" as the features across which a study aims to measure or ensure fairness. This broad definition includes features protected by law (e.g., \u201cprotected attributes\u201d such as race or gender) as well as those with potential for social or economic discrimination (e.g., income). Papers in our sample examined a total of 49 sensitive attributes, in which gender, race, and age emerged as the top three categories of sensitive attributes in our sample (Figure 3)."}, {"title": "Sociotechnical Analysis", "content": "To further contextualize our quantitative findings, we conduct a sociotechnical analysis to explore and make explicit the unstated influences shaping algorithmic fairness research. This analysis focuses on identifying potential geographical, cultural, and implicit norms within the fairness research community and exploring how these biases might affect the diversity and representativeness of conducted studies. Given the interconnected nature of social and technical systems (Cherns, 1976; Dolata et al., 2022), these social norms and biases will ultimately influence the technological outcomes of algorithmic fairness research. Consequently, we consider whether these norms and biases affected the scope of research, the disclosure of limitations, and assumptions about global fungibility. For this analysis, we drew on our own experiences as scientists studying algorithmic fairness and as members of marginalized communities affected by deployed systems (Collins, 1992; Haraway, 1988).\nDespite the existing discourse on bias and limitations in fairness research (see Related Work), we observed two striking patterns across the empirical proceedings at AIES and FAccT: a disproportionate bias toward the US in both author affiliations and dataset origin, and a pervasive tendency towards binary formulations of sensitive attributes, particularly for gender, race, and age.\nUS-Centrism and the Limits of Global Fungibility\nOur meta-analysis reveals a clear and substantial bias toward the US in algorithmic fairness research. We find that 80.6% of papers published each year feature at least one US-based author, and that the US also dominates dataset provenance (72.6%; see Figure 2 & Appendix: Data and Authorship Provenance). In our sample, papers rarely acknowledged or addressed potential biases arising from the predominance of the US in author affiliations and dataset origins."}, {"title": "Binary Formulations and the Erasure of Intersectional Realities", "content": "Our meta-analysis reveals a second concerning trend in algorithmic fairness research: the pervasive use of binary formulations for sensitive attributes (see Figure 4). This binary framing imposes an overly simplistic structure that obscures the complexities of lived experiences and positions groups as isolated, opposing poles on a single axis of power and privilege. For instance, fairness studies often categorize age as \u201cyoung\u201d versus \u201cold\u201d, despite the enormous range of communities that exist within these two age groups. This dualistic logic presumes homogeneity within each class, ignoring the diverse experiences and identities within those broad categories.\nThese assumptions carry profound implications for algorithmic fairness. Binary formulations minimize the unique challenges faced by members of understudied groups. For instance, the \u201cmale/female\u201d binary erases individuals who identify outside this dichotomy. Similarly, studies that focus on \u201cBlack/White\" comparisons in the US obscure the experiences of Hispanic, Asian, and Indigenous communities. Yet research efforts that adopt a binary approach often incorrectly assume that their findings concerning harms and mitigations will translate seamlessly across groups.\nIronically, by treating each of its categories in isolation, this framework equates distinct experiences of discrimination, assuming that\u2014for instance\u2014bias toward racial and gender minorities pose interchangeable challenges. In essence, the binary approach to fairness encourages a view of discrimination as a one-size-fits-all problem, neglecting the nuanced ways in which different forms of oppression intersect and interact. By overlooking the interconnected nature of social categories, binary formulations fail to address the compounding effects of intersectionality in the matrix of domination: axes of discrimination are neither separable nor additive (Collins, 1992; Crenshaw, 1989).\nThis flawed logic can burden minority groups with testing the performance of putatively fair\u2014but actually misaligned\u2014AI systems. Unfortunately, the emphasis on binary comparisons in fairness research creates further issues by implicitly positioning groups against each other, rather than collaborators against a system of oppression (cf. the \u201cracial wedge\u201d in political discourse; Puri, 2016; and the trans-exclusionary movement in modern feminism; Caslin, 2024; Fahs, 2024; Serano, 2016). This combination of inequitable burdens and manufactured divisions ultimately undermines the very goals of algorithmic fairness, hindering the development of truly equitable and inclusive systems.\nThe binary approach to fairness not only oversimplifies complex social identities, but also tends to treat those identities as fixed and unchanging. Papers in our sample rarely discussed how or disclosed whether identity labels could change over time. By framing sensitive attributes as static and binary categories, algorithmic fairness research overlooks the dynamic and fluid nature of identity (Keyes, 2019; Lu et al., 2022; Tomasev et al., 2021). The fluidity and complexity of identity demands that algorithmic fairness research move beyond static and binary solutions, embracing approaches that can adapt to the intersectional and evolving nature of social inequalities."}, {"title": "Discussion", "content": "Our meta-analysis surfaces two prominent trends in contemporary algorithmic fairness research: a disproportionate US-centric bias in author backgrounds, data origins, and design choices; and a widespread policy of casting sensitive attributes into binaries.\nDespite popular depictions of research as an objective pursuit of truth, we recognize research as social praxis\u2014a process inherently shaped by contemporary norms and historical context (Hacking, 1983; Hochstein, 2019; Kuhn, 1961). Early in the development of a field, certain norms and design choices can help scope feasible research questions, providing an important starting point for researchers and scientists. However, over time, these initial frameworks become so deeply ingrained that they limit the scope of inquiry and exclude alternative perspectives.\nUS-centrism and binary logic represent two such frameworks. Current praxis advances a myth of algorithmic fairness as a fungible, abstract, and universal phenomena. However, this myth breaks apart when confronted with the dynamism of identity and the plurality of experience around the world. To move beyond this myth, the fairness research community must chart new paths forward, guided by the principles of inclusivity, representativeness, and cultural specificity.\nThe AIES and FAccT conferences reflect two important sites of dialogue for the algorithmic fairness community field. As a result, this meta-analysis offers a valuable snapshot of current trends in fairness research. In the future, expanding the scope of inquiry to other sites of study will help enrich and contextualize our understanding of contemporary norms and practices in fairness research. Promising sites include generalist conference venues (e.g., the Conference and Workshop on Neural Information Processing Systems), pre-print repositories (e.g., arXiv), and journals (e.g., Big Data & Society). Future work should also extend beyond the examination of academic discourse. An important step for this line of research will be to investigate how developers conceptualize and implement fairness within the systems they deploy to the real world. A critical examination of the social and structural factors shaping research practices across these sites will help identify biases and encourage a more inclusive and representative vision of algorithmic fairness.\nResearchers and ethicists have proposed several solutions in this direction. Many prioritize transparency and self-reflection, advocating for explicit disclosure of the values embedded in technical work and operational definitions (e.g. van Berkel et al., 2023). Others emphasize diversifying representation across datasets (e.g. Sambasivan, 2021), within research teams (e.g. Laufer et al., 2022), and through participatory initiatives (e.g. Gadiraju et al., 2023). A final set of solutions focuses on grounding research, calling for greater engagement with real-world issues (e.g. Birhane et al., 2022b) and rigorous ethnographic research (e.g. Hagerty and Rubinov, 2019; Marda and Narayan, 2021; Martin Jr et al., 2020).\nThe current focus on the US and on binary representations of sensitive attributes threatens to perpetuate a narrow and misleading perspective on algorithmic fairness. To overcome these limitations, the research community must expand its scope to encompass diverse social and cultural settings, particularly those outside the Global North. This expansion also requires moving beyond simplistic binary classifications and engaging with the involute, intersectional realities of bias, identity, and community (Selbst et al., 2019). Qualitative methods and participatory approaches (Birhane et al., 2022a; Martin Jr et al., 2020) will be crucial for developing datasets and fairness frameworks tailored to these conditions. Overall, researchers should seek to challenge the homogenizing effect of purely technical solutions and recenter the complex lived experiences of marginalized communities."}, {"title": "Conclusion", "content": "Our meta-analysis of algorithmic fairness papers reveals a field grappling with several fundamental tensions. While researchers strive to develop fair and unbiased systems, current research practices often default to US-centric perspectives and binary representations of sensitive attributes. As a result, current praxis not only can compromise the credibility of research insights, but also may lead to policy solutions that are poorly aligned with the needs of diverse communities. Our findings underscore the urgent need for fairness researchers to reflect on the norms embedded in their work, echoing and expanding calls for more inclusive, representative, and context-specific approaches to algorithmic fairness."}, {"title": "Positionality Statement", "content": "This paper emerges from a collective effort by researchers situated at the intersection of technology and marginalized communities. Our research backgrounds include the fields of algorithmic fairness, human-computer interaction, and critical algorithmic studies, with prior experience studying communities outside the Global North as well as issues afflicting LGBTQ+ populations. These experiences motivate our critical perspective on the existing AI ethics landscape and its inherent biases towards the US and the Global North. The lead author, for example, is a US-based doctoral student holding multiple intersecting identities\u2014some marginalized, and others affording greater privilege. Navigating these complexities shapes their perspective on power dynamics within AI ethics discourse. Thus, we draw on our own identities and experiences to understand and motivate our research. While this personal perspective provides valuable insights, we acknowledge that it also introduces potential biases and welcome constructive engagement and alternative perspectives to challenge and enrich out understanding."}, {"title": "Expanded Results", "content": "Data and Authorship Provenance\nFigure 6 shows the geographic distribution of authorship affiliations over time.\nStudy Design\nFairness research papers can be categorized by the methods they apply to their proposed fairness definitions: empirical experiments that collect new data with a deployed intervention (prospective); empirical simulations with existing datasets (retrospective); or proofs using mathematical guarantees (theory). Examining our sample through the lens of these categories reveals a clear trend: papers most frequently rely on retrospective analyses to study algorithmic fairness (Figure 7a). This approach predominates across all years in our sample. In terms of dataset type, we observe a notable shift towards using synthetic data after 2018. The most common non-synthetic datasets (e.g., COMPAS, UCI Adult, and German Credit) disclose a focus on applications within financial and criminal justice contexts, though the exact pattern varies from year to year (Figure 7b).\nWe next analyze aspects of the algorithmic interventions proposed. Our results indicate a clear preference for in-processing interventions, which directly modify the algorithm during the learning process (Figure 8a). In-processing approaches tend to overshadow pre-processing interventions, which focus on data adjustments before training, and post-processing interventions, which address fairness concerns by modifying algorithmic outputs. Across the years we examine, we observe a growing ambiguity regarding the inherent trade-off between fairness and performance (Figure 8b).\nFairness Definitions\nFigures 9 and 10 illustrate the prevalence of different fairness metrics in our sample over time, presenting the top ten most frequently used metrics and the full list, respectively. A single study might explore multiple metrics. For instance, an individual paper might examine both demographic parity and equalized false positive rates across demographic groups."}]}