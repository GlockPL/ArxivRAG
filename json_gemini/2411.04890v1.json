{"title": "GUI Agents with Foundation Models: A Comprehensive Survey", "authors": ["Shuai Wang", "Weiwen Liu", "Jingxuan Chen", "Weinan Gan", "Xingshan Zeng", "Shuai Yu", "Xinlong Hao", "Kun Shao", "Yasheng Wang", "Ruiming Tang"], "abstract": "Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent agents being capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions by simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data, frameworks, and applications. We begin by discussing representative datasets and benchmarks. Next, we summarize a unified framework that captures the essential components used in prior research, accompanied by a taxonomy. Additionally, we explore commercial applications of (M)LLM-based GUI agents. Drawing from existing work, we identify several key challenges and propose future research directions. We hope this paper will inspire further developments in the field of (M)LLM-based GUI agents.", "sections": [{"title": "1 Introduction", "content": "Graphical User Interfaces (GUIs) serve as the primary interaction points between humans and digital devices. People interact with GUIs daily on mobile phones and websites, and a well-designed GUI agent can significantly enhance the user experience. Consequently, research on GUI agents has been extensive. However, traditional rule-based and reinforcement learning-based methods struggle with tasks that require human-like interactions (Gur et al., 2018; Liu et al., 2018), limiting their practical application.\nIn recent years, advancements in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have elevated their abilities in language understanding and cognitive processing to unprecedented levels (OpenAI et al., 2024; Touvron et al., 2023; Yang et al., 2024). With improved natural language comprehension and enhanced reasoning capabilities, (M)LLM-based agents can effectively interpret and utilize human language, develop detailed plans, and execute complex tasks. These breakthroughs offer new opportunities for AI researchers to tackle challenges that were once deemed highly difficult, such as automating tasks within GUIs. As a result, numerous studies have been published focusing on (M)LLM-based GUI agents, as shown in Figure 1, especially over the last two years. However, few efforts have been made to comprehensively summarize and compare the research in this emerging field of GUI agents. A systematic review is urgently needed to provide a holistic understanding and inspire future developments.\nThis paper presents a comprehensive survey of GUI agents with foundation models. We organize the survey around three key areas: data, framework, and application. First, we investigate the available datasets and benchmarks for GUI agents, and list them as a resource for researchers in this area. Second, we review the recent works on (M)LLM-based GUI agents, which are classified by their input modalities and learning modes. Finally, we summarize the latest industrial applications with (M)LLM-based GUI agents, which hold significant commercial potential."}, {"title": "2 GUI Agents Data Source", "content": "Recent research has focused on developing datasets and benchmarks to train and evaluate the capabilities of (M)LLM-based GUI agents. These studies can be broadly classified into two categories based on whether they involve interaction with the actual environment: static datasets (Rawles et al., 2023; Zhang et al., 2024b; Li et al., 2024a; Lu et al., 2024; Venkatesh et al., 2023; Chen et al., 2024a; Li et al., 2020) and dynamic datasets (Zhou et al., 2023; Gao et al., 2024; Rawles et al., 2024; Chen et al., 2024b).\nAndroid in the Wild (AitW) Rawles et al. (2023) introduces a dataset for mobile device control, featuring natural language instructions, screenshots, and task demonstrations. Instructions come from humans, language models, and technical documents. AitW consists of five datasets: four for multi-step tasks (GoogleApps, Install, WebShopping, General) and one for single-step tasks (Single).\nAndroid-In-The-Zoo Zhang et al. (2024b) introduces a benchmark dataset with 18,643 screen-action pairs and chained action reasoning annotations, aimed at advancing GUI navigation agent research.\nAndroidControl Li et al. (2024a) comprises 15,283 demonstrations of daily tasks performed with Android apps, where each task instance is accompanied by both high- and low-level human-generated instructions. This dataset can be utilized to assess model performance both within and beyond the domain of the training data.\nGUI-Odyssey Lu et al. (2024) introduces a comprehensive dataset for training and assessing cross-application navigation agents. The dataset comprises 7,735 episodes, encompassing six types of cross-application tasks, 201 distinct applications, and 1,399 application combinations.\nUGIF Venkatesh et al. (2023) introduces a comprehensive multilingual, multimodal user interface (UI) localization dataset with 4,184 tasks in 8 languages. Includes query sequences, instructions, screenshots, and human-executed operation sequences, ideal for multi-step UI manipulation evaluation.\nGUI-WORLD Chen et al. (2024a) consists of 12,000+ GUI videos, 6 scenario types, 8 issue categories, and 3 formats, which is ideal ideal for evaluating MLLMs on diverse GUI content, focusing on dynamic and sequential elements.\nPIXELHELP Li et al. (2020) proposes a new class of problems focused on translating natural language instructions into actions on mobile user interfaces. PIXELHELP introduces three new datasets: PIXELHELP, ANDROIDHowTO, and RICOSCA, collectively comprising 187 multi-step instructions for model training.\nWebArena Zhou et al. (2023) implements a versatile website covering e-commerce, social forums, collaborative software development, and content management which includes 812 test examples to ground high-level natural language instructions, with current models like TEXT-BISON-001, GPT-3.5, and GPT-4, achieving 14.41% accuracy compared to 78.24% for humans.\nASSISTGUI Gao et al. (2024) introduces a novel benchmark for evaluating model manipulation of mouse/keyboard on Windows. ASSISTGUI includes Includes 100 tasks from 9 software apps (e.g., After Effects, MS Word) with project files for accurate assessment."}, {"title": "3 (M)LLM-based GUI Agent", "content": "With the human-like capabilities of (M)LLMs, GUI agents aim to deal with various tasks to meet users' needs. To better stimulate the ability of (M)LLMs, the framework of GUI agents should be carefully designed. In this section, we first summarize a systematic construction from existing work, carefully select some typical cases, and discuss their related designs for different modules. Then, we give a comprehensive taxonomy for GUI agents. The two key aspects of GUI agents, input modality, and learning mode, are used to classify the existing work. From these two dimensions, we include the current major work and help new researchers get a whole view of GUI agents."}, {"title": "3.1 (M)LLM-based GUI Agent Construction", "content": "The objective of GUI agents is to automatically control a device in order to complete tasks defined by the user. Typically, GUI agents take a user's query and the device's UI status as inputs, and provide a series of human-like operations to accomplish the tasks.\nAs shown in Figure 2, we conclude that the construction of (M)LLM-based GUI agent consists of five parts: GUI Perceiver, Task Planner, Decision Maker, Memory Retriever, and Executor. There are many variants of this construction. Wang et al. (2024a) propose a multi-agent GUI control framework with a planning agent, a decision agent, and a reflection agent to solve the navigation challenges in mobile device operation tasks, which have similar functions.\nGUI Perceiver: To effectively complete a device task, a GUI agent needs to accurately interpret user input and detect changes in the device's UI. While language models excel at understanding user intent (Touvron et al., 2023; OpenAI et al., 2024), navigating device UIs requires a reliable visual perception model for optimal interaction.\nA UI Perceiver appears explicitly or implicitly in the GUI agent framework. For agents based on single-modal LLM (Wen et al., 2023, 2024b; Li et al., 2020), a UI Perceiver is usually an explicit module of the agent framework. However, for agents with multi-modal LLM (Hong et al., 2023; Zhang et al., 2023; Wang et al., 2024b), UI perception is seen as a capability of the model itself.\nUI perception is also an important problem in GUI agent research, therefore, some work (You et al., 2024; Zhang et al., 2021) focuses on understanding and processing the UI, rather than building the agent. For example, You et al. (2024) propose a series of referring and grounding tasks, which provide valuable insights into the pre-training of GUIs.\nTask Planner: The GUI agent should effectively decompose complex tasks, often employing a Chain-of-Thought (CoT) approach (Wei et al., 2023). Due to the complexity of these tasks, recent studies (Zhang et al., 2024a; Wang et al., 2024a) have introduced an additional module to support more detailed planning.\nThroughout the GUI agent's process, plans may adapt dynamically based on decision feedback, typically achieved through a ReAct style (Yao et al., 2023). For instance, Zhang et al. (2023) use on-screen observations to enhance the CoT for improved decision-making, while Wang et al. (2024a) develop a reflection agent that provides feedback to refine plans.\nDecision Maker: A Decision Maker is responsible for providing the next operation(s) to control a device. Most studies (Lu et al., 2024; Zhang et al., 2024a; Wen et al., 2024a), define a set of sample UI-related actions\u2014such as click, text entry, and scroll\u2014as a basic space. In more complicated cases, Ding (2024) encapsulates a sequence of actions to create Standard Operating Procedures (SOPs) to guide further operations.\nAs the power of GUI agents improves, the granularity of operations becomes more refined. Recent work has progressed from element-level operations (Zhang et al., 2023; Wang et al., 2024b) to coordinate-level control (Wang et al., 2024a; Hong et al., 2023)."}, {"title": "Executor:", "content": "As the link between GUI agents and devices, the Executor maps the outputs to relevant environments. For real device execution, most studies (Zhang et al., 2023; Wang et al., 2024b,a) utilize Android Debug Bridge (ADB) to control the device. Differently, Rawles et al. (2024) conduct tests in a simulator, where additional UI-related information can be accessed."}, {"title": "Memory Retriever:", "content": "Memory Retriever is designed as a additional source of information to help agents perform tasks more effectively (Wang et al., 2024c).\nGenerally, the memory for GUI agents is generally divided into internal and external categories. Internal memory (Lu et al., 2024) includes the previous actions, screenshots, and other statuses generated during execution. External memory (Zhang et al., 2023; Ding, 2024) usually includes prior knowledge and rules related to the UI or tasks. They can serve as additional inputs to assist GUI agents."}, {"title": "3.2 (M)LLM-based GUI Agent Taxonomy", "content": "As shown in Figure 1, we conclude the existing work with various dimensions. As a result, this paper classifies existing work with the difference of input modality and learning mode."}, {"title": "3.2.1 GUI Agents with Different Input modality", "content": "LLM-based GUI Agents: With the limited multimodal capability, earlier GUI agents (Lee et al., 2023b; Li et al., 2020; Gur et al., 2022; Jiang et al., 2023; Nakano et al., 2022) often require a GUI perceiver to convert GUI into text-based input.\nFor instance, Li et al. (2020) transform the screen into a series of object descriptions and applies a transformer-based method for action mapping. The problem definitions and datasets have spurred further research. Wen et al. (2024a) further convert GUI to simplified HTML representation for compatibility with the base model. By combining GUI representation with app-specific knowledge, they build Auto-Droid, a GUI agent based on off-the-shelf LLMs, including online GPT and on-device Vicuna.\nMLLM-based GUI Agents: Recent studies (Shaw et al., 2023; Wang et al., 2021; You et al., 2024; Bai et al., 2021) utilize the multimodal capabilities of advanced (M)LLMs to improve GUI comprehension and task execution.\nSome works (You et al., 2024; Zhang et al., 2021; Lee et al., 2023a; Wang et al., 2021) focus on GUI Understanding. For example, Pix2struct (Lee et al., 2023a) employs a ViT-based image-encoder-text-decoder architecture, which pre-trains on Screenshot-HTML data pairs and fine-tunes for specific tasks. This method has shown strong performance in four web-based visual comprehension tasks. Similarly, Screen Recognition (Zhang et al., 2021) proposes a method to convert mobile app UI into metadata, by using extensive manual annotations to mark iOS UIs. The data is used to"}, {"title": "3.2.2 GUI Agents with Different Learning Mode", "content": "Prompting-based GUI Agents: Prompting is an effective approach to building agents with minimal extra computational overhead. Given the diversity of GUIs and tasks, numerous studies (Zhang et al., 2023; Li et al., 2024b; Wang et al., 2024a; Humphreys et al., 2022; Wen et al., 2024b) use prompting to create GUI agents, adopting CoT or ReAct styles.\nRecent Studies use prompting to build and simulate the functions of each module within a GUI agent, enabling effective GUI control.\nFor example, Yan et al. (2023) introduce MM-Navigator, which utilizes GPT-4V for zero-shot GUI understanding and navigation. For the first time, This work demonstrates the significant potential of LLMs, particularly GPT-4V, for zero-shot GUI tasks. Manual evaluations show that MM-Navigator achieves impressive performance in generating reasonable action descriptions and single-step instructions for iOS tasks. Additionally, Song et al. (2023) introduce a framework for interacting with the GUI using a sequential, human-like problem-solving approach. The framework includes a YOLO-based UI understanding module to locate UI elements and text, a GPT-4V-based task planning module to decompose the task, and an execution module that maps text-based actions to control the device. Wen et al. (2024b) propose DroidBot-GPT, which summarizes the app's status, historical actions, and tasks into a prompt, and then uses ChatGPT to select the next action. This approach effectively integrates historical actions and user UIs without requiring any modifications to the underlying LLM. Furthermore, (Zheng et al., 2024) propose SeeAct, a GPT-4V-based generalist web agent. With screenshots as input, SeeAct generates action descriptions and converts them into executable actions with designed action grounding techniques.\nSome studies enable the GUI agent to fully leverage external knowledge through prompting to complete GUI tasks.\nAppAgent (Zhang et al., 2023) proposes a multimodal agent framework to simulate human-like mobile phone operations. The framework is divided into two phases: Exploration, where agents"}, {"title": "4 Industrial Applications of (M)LLM-Based GUI Agents", "content": "Google Assistant for Android: By saying phrases like \"Hey Google, start a run on Example App,\" users can use Google Assistant for Android to launch apps, perform tasks, and access content. App Actions, powered by built-in intents (BIIs), enhance app functionality by integrating with Google Assistant. This enables users to navigate apps and access features through voice queries, which the Assistant interprets to display the desired screen or widget.\nApple Intelligence: Features on-device and cloud models using Apple silicon, with a generic foundation model and specialized adapter models for tasks like summarization and tone adjustment. Evaluations show the on-device model outperforms or matches small models from Mistral AI, Microsoft, and Google, while the server models surpass OpenAI's GPT-3 and match GPT-4. Unlike services like ChatGPT, Apple runs its cloud models on proprietary servers with custom hardware. The system ensures software integrity by refusing connections if mismatches are detected.\nNew Bing: Microsoft's search engine is designed to offer users a more intuitive, efficient, and comprehensive search experience. Leveraging cutting-edge artificial intelligence and machine learning technologies, New Bing goes beyond traditional keyword searches to understand the context and intent behind user queries. This allows it to deliver more relevant results, personalized recommendations, and enhanced features like conversational search, image recognition, and real-time updates. With a sleek, user-friendly interface and deep integration with other Microsoft services, New Bing aims to redefine how people find information online, making accessing the knowledge and insights they need faster and easier.\nMicrosoft Copilot: An Al tool in Microsoft 365 apps for productivity with GPT-based suggestions, task automation, and content generation. Enhances workflows, creativity, and decision-making with real-time insights."}, {"title": "Anthropic Claude 3.5:", "content": "The latest version of Claude 3.5 introduces a groundbreaking new capability: Computer Use which allows Claude to interact with computers like humans\u2014by viewing screens, moving cursors, clicking buttons, and typing text. Asana, Canva, Cognition, DoorDash, Replit, and The Browser Company have already begun to explore these possibilities, carrying out tasks that require dozens, and sometimes even hundreds, of steps to complete."}, {"title": "AutoGLM:", "content": "A new series from the ChatGLM family, is designed for autonomous mission completion via Graphical User Interfaces on platforms like phones and the web. Its Android capability allows it to understand user instructions autonomously without manual input, enabling it to handle complex tasks such as ordering takeout, editing comments, shopping, and summarizing articles."}, {"title": "MagicOS 9.0 YOYO:", "content": "An advanced assistant with four main features: natural language and vision processing, user behavior and context learning, intent recognition and decision-making, and seamless app integration. It understands user habits to autonomously fulfill requests, such as ordering coffee through voice commands, by navigating apps and services."}, {"title": "5 Challenges", "content": "Despite the rapid developments and exciting achievements of previous work, the field of (M)LLM-based GUI agents is still at an initial stage. We summarize several significant challenges that need to be addressed as follows:"}, {"title": "The Gap between Benchmark and Reality:", "content": "Existing datasets and benchmarks are clearly divided into static and dynamic categories. A static benchmark typically stores an execution path as a sequence, where the goal is to predict the next action. In contrast, dynamic benchmarks require execution on simulators or real devices, where the tasks must be fully completed. At present, the majority of both training and evaluation data is static. However, because (M)LLM-based GUI agents need to interpret extensive environmental status, existing datasets and benchmarks are inadequate for actual applications."}, {"title": "GUI Agent Self-evolution:", "content": "Self-evolution aims to achieve the self closed loop of the GUI agent. Zhang et al. (2023) introduces the concept of exploration, implementing it through documentation that automatically records operations and interface transition knowledge. Similarly, Li et al. (2017); Wen et al. (2024b,a) propose an automated framework to explore paths, summarizing them as a UI Transition Graph (UTG) for improved performance. However, effective exploration methods to fully realize this goal are still challenging."}, {"title": "Inference Efficiency:", "content": "Humans are sensitive to the response time of GUIs. Typically, a delay under 200 milliseconds is acceptable, however, delays beyond this threshold can rapidly degrade the user experience. For current GUI agents, inference and communication delays are often measured in seconds, leading to poor user satisfaction. Addressing how to minimize these delays, or deploying the (M)LLM directly on mobile devices, is therefore a pressing issue."}, {"title": "6 Conclusion", "content": "In this paper, we systematically review the rapidly evolving research field of (M)LLM-based GUI agents. We examine these studies from three main perspectives: data sources, construction, and applications. We also provide a detailed taxonomy that connects existing research and summarizes the major techniques. Additionally, we propose several challenges and potential future directions for GUI agents leveraging foundation models."}]}