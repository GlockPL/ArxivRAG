{"title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning", "authors": ["Martin Moder", "Stephen Adhisaputra", "Josef Pauli"], "abstract": "This paper addresses navigation in crowded environments by integrating goal-conditioned generative models with Sampling-based Model Predictive Control (SMPC). We introduce goal-conditioned autoregressive models to generate crowd behaviors, capturing intricate interactions among individuals. The model processes potential robot trajectory samples and predicts the reactions of surrounding individuals, enabling proactive robotic navigation in complex scenarios. Extensive experiments show that this algorithm enables real-time navigation, significantly reducing collision rates and path lengths, and outperforming selected baseline methods. The practical effectiveness of this algorithm is validated on an actual robotic platform, demonstrating its capability in dynamic settings.", "sections": [{"title": "1 Introduction", "content": "Navigating robots or autonomous cars in crowded areas can lead to \"robot freezing,\" where the robot becomes stationary due to an unclear path, as noted by Trautman et al. [50]. Traditional methods, which only predict crowd behavior without accounting for human-robot interaction, have proven inadequate. Thus, the focus has shifted to strategies promoting cooperative interactions between robots and humans. This has led to the creation of collaborative robots, or \"CoBots,\" designed to work alongside humans, adapting to both their movements and the complexities of crowded environments.\nReinforcement Learning (RL) is a popular method for CoBots navigation in crowds, offering a comprehensive framework that includes data-driven social acceptance and environment-specific behavior. Despite promising results in simulations and real environments [8, 14], RL faces challenges due to the need for online crowd interactions, which are costly and safety critical, leading to reliance on unrealistic simulations. Recently, Levine et al. [29] have employed offline RL to create task-specific policies using only pre-collected data, eliminating the need for online training. Offline RL requires a reward function to guide behavior, which can be specified during data collection or derived from handcrafted metrics. This is particularly challenging for \"social\" navigation, where the robot must interpret subtle social behaviors and cues.\nSupervised imitation learning, especially using large-scale generative models, shows great promise. These models have made notable contributions in visual recognition [41] and natural language processing [4]. As demonstrated by Cui et al. [12], these models can enhance robotic decision-making in complex environments, such as guiding a robotic arm to perform kitchen tasks based on context-driven textual goals.\nThis work explores the potential of generative models, which are trained on human crowd videos, for cooperative robot action planning. These generative models, we argue, can offer a solution to the \"robot freezing\" problem by enabling robots to generate intuitive, human-like behaviors, promoting more natural robot-human cooperation. However, applying these models to crowd navigation presents challenges, including processing continuous actions, handling data multi-modality, and conditioning on future outcomes. Additionally, a policy trained solely on human videos won't match a robot's unique kinematic and dynamic constraints, and the datasets lack environment representations that robots can easily interpret.\nTo address these challenges, we propose a hybrid approach combining a likelihood-based generative model, trained on human crowd videos, with SMPC (see Figure 1). The generative model, conditioned on goal positions, predicts crowd dynamics as a density function and scores robot plans based on their probability of being human-like. During planning, we use this scoring to create robot plans that mimic human behavior, considering the robot's physical limits and the natural uncertainty in human decision-making. This hybrid approach enables robots to imitate human behavior in real-world scenarios.\nThis work builds on [33\u201335] and extends them in the following ways:\n1. We advance recent progress in optimal sampling-based planning, focusing on the Model Predictive Path Integral (MPPI) algorithm. Sample-based planning's main advantage is its ability to generate human-like responses to robotic plan samples using a trained generative model. Our research concentrates on the MPPI technique, differing from our previous work [35], which used the CEM approach. Unlike CEM, which averages the best-sampled trajectories unweighted, MPPI uses a weighted average, enhancing sample selection control and potentially leading to better planning outcomes.\n2. In previous research [35], we used a generative model to create human-like responses and guide a policy to mimic human behavior closely. However, this model is not goal-conditioned, allowing the robot to exploit predicted human responses, even if it caused humans large detours. By incorporating goal conditioning and a Social Influence Reward (SIR), we limit the robot's ability to exploit human reactions, as individuals follow their own objectives, reducing adaptability to avoid collisions. A goal-conditioned generative model also more effectively directs the robot towards its goal, offering insights on both the human-like nature and efficiency of navigating towards the goal in a crowd.\n3. Current human crowd video data lack an environment representation interpretable by robots, complicating model training to respect static environments. To address this, we use SMPC in robot planning, considering static information during optimization. For generating goal-directed human behavior that respects the environment, we propose a simple yet effective optimization technique to select a sample from the generative model that aligns with the captured environment."}, {"title": "2 Related Work", "content": "Two pioneering studies on navigation in human environments, RHINO [5] and MINERVA [48], both use the Dynamic Window Approach (DWA) [15] for local collision avoidance, a method still popular in current ROS packages. A key challenge is addressing the unpredictability of human behavior. Du Toit et al. [13] found that treating agents as independent entities creates overwhelming uncertainty, complicating navigation. Trautman et al. [52] showed that merely constraining this uncertainty, as Du Toit et al. proposed, can cause \"robot freezing.\" They advocated for a more cooperative approach between humans and robots.\nIn recent years, CoBots have advanced considerably. Start-ups like Robust AI, Diligent, and Veo Robotics, along with established manufacturers like Kuka and Fanuc, are developing CoBots for harmonious coexistence with humans in shared spaces, with applications ranging from medical navigation to manufacturing assistance. Despite ongoing research, the detailed methodologies of these companies are somewhat elusive. Diligent [23], for instance, uses \u201chuman-guided learning\" in hospital operations, where robots learn from their environments and human interactions. However, the integration of imitation learning in their framework is unclear, and their navigation system appears to rely on a classical map-based approach, using QR/APRIL tags at key locations like elevators and doors.\nThe advancement of CoBots is closely tied to progress in autonomous vehicles. Waymo uses a sense-predict-plan-act pipeline, combining sensor data with maps for environment perception [18]. Tesla, while following a similar pipeline with a focus on vision, treats lane detection as a linguistic task and employs a unique planning mechanism via tree search [47]. Recently, Tesla has announced a shift towards end-to-end learning. In contrast, Wayve is prioritizing an end-to-end learning pipeline, emphasizing real-time visual inputs over detailed maps [22]."}, {"title": "2.1 Learning a Policy", "content": "In most learning-based methodologies, planning occurs in a 2D space, analyzing human dynamics over time and representing social interactions as a comprehensive graph [35]. The focus within this 2D setup is on constructing interaction graphs using neural network structures that handle diverse groups of humans and track their movements over time. Some methods that predict future behaviors provide essential knowledge in this domain [7, 33]. Predominant learning approaches include RL and imitation learning, which can operate end-to-end or in a MPC setting. For instance, Chen et al. [10] pioneered the use of RL to learn a discrete value function suitable for a real robot. Everett et al. [14], using the Actor-Critic paradigm, demonstrated the feasibility of learning a policy for continuous actions. An alternative model-based RL approach is showcased by Chen et al. [8], where their relational graph learns the crowd dynamics model for subsequent tree search.\nThe referenced RL methods primarily utilize simulations, where human behaviors are often represented using ORCA [2] or the Social Force Model [20]. Simulating authentic human behavior in a crowd setting is challenging [32], and it becomes even more complex in real-world environments with additional static or dynamic obstacles. To address this issue, some studies focus on imitating expert human behavior using real-world data. For instance, Moder et al. [34, 35] employ a likelihood-based generative model to deduce a policy aimed at achieving a set objective while proactively mitigating human interference. Works by [51, 52] utilize a Gaussian Process model for human interaction prediction, subsequently formulating a robot navigation policy. Most learning-based approaches neglect robot constraints. Of the approaches presented here, only Everett et al. [14] account for this by setting dynamics constraints during training, and Moder et al. [35] during optimization in the model predictive control setting."}, {"title": "2.2 Planning with Generative Models", "content": "Outside the context of crowd navigation, many works have proposed model-based approaches. Common model-based algorithms learn the dynamics model of the world and use it for planning at test time, often through model predictive control and various trajectory optimization methods [11, 37, 42]. The cross-entropy method serves as a practical, sample-based alternative to gradient-based optimization methods, leveraging data-driven dynamics models [11, 35, 40, 54]. Some model-based approaches incorporate a learned policy alongside the dynamics model [54, 57], or employ the model to generate \"synthetic\" samples, enriching the sample set for model-free learning methods [39, 58].\nAnother approach, inspired by recent advancements in generative artificial intelligence enabled by transformers [53] especially in imitation learning and offline reinforcement learning is gaining traction. Notably, works that harness transformers in novel ways, diverging from the traditional reinforcement learning paradigm, stand out. For instance, Decision Transformers [9] and related methodologies [24, 45] focus on return-conditioned imitation learning."}, {"title": "3 Robot Navigation as a Multiplayer Game", "content": "Our focus is on a navigation algorithm for an autonomous robot sharing an environment with humans, ensuring the robot is mindful of its impact on human actions. We consider scenarios with K agents: the robot r := 1 and humans h := {2,..., K}. We introduce the necessary variables and derive the general objective.\nWe define continuous states and discrete time, with agent k \u2208 K's states  St\u2208\u211d\u03a9s  at time t as 2-dimensional positions on a ground plane (\u03a9s := 2), with the current time step t = 0. The future scene of K agents over T time steps is ST1:K :=  S1:K\u2208RT\u00d7K\u00d7\u03a9s . Let Srt := St\u2208\u211d\u03a9s be the state of the robot, and Sth := S2:K\u2208\u211d(K\u22121)\u00d7\u03a9s  the states of all humans. Absence of a time step subscript denotes all future or past time steps, and absence of an agent index superscript denotes all agents, e.g., S := S1. Capital roman letters denote random variables, with realizations in roman lowercase. For instance, past states of all agents over a period To are So\u2212To:0.\nThe next future states St+1 of all agents, determined by their actions At := AT1:K\u2208\u211dK\u00d7\u03a9s , use two transition functions: fr for robot dynamics and fh for human dynamics. Continuous actions At at time t are decided by a stochastic robot policy:\nAt\u223c\u03c0(\u22c5|St;\u03b8\u03c0),(1)\nwhere \u03b8\u03c0 represents the distribution parameters. The robot\u2019s state is influenced by its actions as defined by the robot dynamics function:\nSt+1=fr(St,At).(2)\nHuman actions Ah := AT2:K are decided by a human policy \u03c0h :\u211d(K\u22121)\u00d7\u03a9\u2192\u211d(K\u22121)\u00d7\u03a9, detailed in the next section. The human transition function is:\nSh+1=fh(Sh,Ar)=Sth+Ath.(3)\nThe objective is to determine the optimal parameters, \u03b8\u2217\u03c0, for the robot\u2019s policy \u03c0\u2217, to maximize a specified scalar return R\u2208\u211d. Considering fR(T) : RT\u00d7K\u00d7(\u03a9s+\u03a9s)\u2192\u211d as the finite-horizon undiscounted return function, the expected return is:\nJ(\u03c0\u2217,\u03c0h)=E\u03c4[FR(T)]\n=E\u03c4[\u2211T\u22121t=0fo(St,At,Ah)],(4)\nwhere E\u03c4[] denotes the expectation over the episode \u03c4 = {S0, A0r, A0h, S1, \u2026, ST\u22121}, starting from the current observed states s0 for all agents. The function fo(St, At, Ar) : \u211dK\u00d7(\u03a9s+\u03a9s)\u2192\u211d represents the robot\u2019s reward function, detailed in Section 4.5. The robot aims to find the optimal parameters to maximize the expected return:\n\u03b8\u2217\u03c0=argmax\u03b8\u03c0J(\u03c0\u2217,\u03c0h).(5)\nTo solve the finite horizon problem as stated in (5), the robot requires knowledge of \u03c0h, implying the necessity to understand human cognition and predict human responses under various scenarios.\nThis challenge is often circumvented by assuming the human policy is based solely on human states, ignoring the robot\u2019s state. This assumes humans continue their trajectory at their current velocity as if the robot were invisible. Consequently, the robot acts as if it has no influence on the environment, planning movements within the confines of existing free space. However, this can lead to 'robot freezing,' where the robot remains stationary, waiting for natural changes in the environment to present more free space.\nAn alternative strategy approximates the human reward function using Inverse Reinforcement Learning [43]. This derives reward functions for both the robot and humans, facilitating the formulation of individual policies, such as through MPC. Each agent, human or robot, aims to maximize their own returns subject to dynamic constraints. However, the interdependence of reward functions where each agent\u2019s reward depends on the states and actions of all other agents and potential conflicts between individual reward functions, elevate the problem to the domain of game theory. Identifying policies that optimize the return for all agents simultaneously may be infeasible. In subsequent sections, imitation learning via Behavior Cloning (BC) is proposed to make the objective in (5) manageable."}, {"title": "4 Methods", "content": "In (5), the objective is framed as a multiplayer game, where each agent\u2019s actions are influenced by others, leading to complex interactions. Unlike the traditional RL goal of finding a single globally optimal policy, MPC iteratively seeks locally optimal parameters for the robot\u2019s policy. This process uses a predictive model to project future environmental states over a finite horizon T, incorporating the robot\u2019s policy and transition dynamics, as specified in (2), along with the human policy and its transition dynamics, as detailed in (3).\nTo address the complexities in (5), a goal-conditioned human density p(Ah|S<t,Gh,\u03b8;\u03b8nar) is introduced. This density captures human control actions Ah conditioned on all observed states of all agents S<t := S01:t and their goals Gh\u2208\u211dns. The computation employs a NAR model, as specified in (12). The parameters, \u03b8nar, are optimized to enhance the likelihood of the observed data, an approach also referred to as Goal Conditioned Behavior Cloning (GCBC). Consequently, the human policy is formalized as:\n\u03c0nar(Ah|St,Gh,\u03b8;\u03b8nar):=p(Ah|S<t,Gh,\u03b8;\u03b8nar).(6)\nThe dataset contains human positions extracted from real-world video recordings, as shown in Figure 2. To make this dataset compatible with GCBC, goal relabeling of future states is necessary. A common data augmentation technique, useful when the set of goals Gh is a subset of the observation space Sh, is Hindsight Experience Replay (HER) [1]. This approach augments the dataset with additional goal information, as follows:\nDher:={(ms,mhert,shert1,shertt),(msh\u2217,mh\u2217,{(mh\u2217t1,mh\u2217t)}Mt=1)MherSt},(7)\nwhere \u2217 denotes expert actions and states, and Mher is the number of scenes captured.\nWith the human policy defined, a response to a sequence of robot actions a\u03c4 := a0:T\u22121 aimed at achieving a robot goal gr can be computed. This action sequence is converted into robot states s via the transition function fr. The human policy \u03c0nar then generates an autoregressive response, considering forecasted human goals gh. At each time step t, the robot state s and observed human states sh are concatenated to create a joint state St = ss. The human policy applied to this joint state derives the human action ap, leading to the next human state sh+1 through fh. This autoregressive process over the horizon T yields the human response sequence ah to the robot\u2019s actions. The reward for these human responses is computed as defined in (4). In this framework, the robot engages in a 'multiplayer game,' eliciting human policy responses to its actions. This approach, Best-response Iteration, is based on game theory and is noted for its efficacy in recent research [43, 56], including this study.\nIn goal conditioning, each individual is assumed to aim for a specific location. This influences the robot\u2019s navigation plan in two ways: by accounting for interaction dynamics in crowded spaces and by aligning with individual human goals. Consequently, the robot effectively navigates towards its own goal while acknowledging human objectives. The robot\u2019s plan assumes that humans are proactive to an extent; their goal-driven actions make them somewhat predictable but not overly flexible.\nModer et al. [35] demonstrated that human policies trained with GCBC can support effective robot planning. However, this assumes the robot can emulate human movement, a challenge given the stricter kinematic and dynamic constraints on robots. Additionally, no comprehensive dataset exists that captures both human positions and static obstacles on a large scale, limiting GCBC to human-populated environments without static obstacles. Due to the difficulty of simulating such data, a hybrid strategy is adopted. This strategy uses SMPC and spatial mapping for the robot's local plan while the human policy predicts human actions. The human policy also critically evaluates the robot's proposed plans, identifying those that best emulate human-like behavior."}, {"title": "4.1 Sampling-Based Model Predictive Control", "content": "The SMPC methodology is based on importance sampling. The robot\u2019s next action is determined by an optimal policy, which is unknown and cannot be directly sampled. However, samples can be evaluated using the reward function fo. The objective is to refine the known policy so its samples approximate the optimal distribution. An accurate reward function is essential for assessing the samples, as outlined in (5). Here, the robot policy for SMPC in the robot action space is described as a Gaussian density:\n\u03c0gauss(A;\u03bct,\u03a3t)=N(\u03bct,\u03a3t),(8)\nwhere \u03bct and \u03a3t are the mean and covariance at timestep t, respectively, defined in \u211d\u03a9s and \u211d\u03a9s\u00d7\u03a9s. In (8), policy actions are represented as linear and angular velocities, denoted by At := A0:T\u22121, where A\u2208\u211d and AT\u22121\u2208\u211dT\u00d7\u03a9. For a two-wheeled robot (\u03a9s = 2), one dimension is linear velocity and the other is yaw (angular velocity).\nThe robot constraints are managed using the Dynamic Window Approach (DWA), introduced by Fox et al. [15]. This technique sets a velocity window based on the current robot velocity and configuration. The window is then used to clip the robot\u2019s actions, as demonstrated in Algorithm 1."}, {"title": "4.2 Model Predictive Path Integral", "content": "In MPPI [55], the objective function is based on the 'free energy' concept from control theory. This reformulates the expected return in (4) as:\nJ(\u03c0gauss,\u03c0nar)=logE\u03c4[exp(\u03b3fr(\u03c4))],(9)\nwhere \u03b3 > 0 is a scaling factor, or \"temperature,\" influencing the exploration-exploitation trade-off. A higher \u03b3 encourages exploration, while a lower \u03b3 favors exploitation. The term \u03c0gauss\u03c0nar represents"}, {"title": "4.3 Neural Autoregressive Model", "content": "In selecting a generative model for GCBC, we chose the Neural Autoregressive (NAR) model, based on insights from Moder et al. [35]. This study shows that this model, when conditioned with goal prediction, performs comparably to models like GAN [19] or VAE [25]. The model\u2019s ability to generate precise likelihood predictions aids in devising robot actions that replicate human behaviors. Advances in NLP, demonstrated by models like Llama2 [49], further underscore the NAR model\u2019s scalability.\nThe NAR model estimates the probability density of an agent\u2019s actions:\n\u03c0nar:=p(Ah|St,Gh,\u03b8;\u03b8nar)=N(\u03bcr,\u03a3r),(12)\nwith mean \u03bcr=f\u03bc(S<t,Gh,\u03b8;\u03b8nar) and covariance matrix \u03a3r=f\u03a3(S<t,Gh,\u03b8;\u03b8nar), where f\u03bc\u2208\u211dns and f\u03a3\u2208\u211d\u03a9s\u00d7\u03a9s. Functions f\u03bc and f\u03a3 are neural networks with trainable parameters \u03b8nar, trained by maximizing the likelihood on the dataset Dher. There is no distinction between the robot and humans; all predicted agents are assumed to be humans. The subsequent state is determined using the state transition function fh.\nThe NAR model uses an encoder-decoder architecture, as described by Moder et al. [33]. Each observation ok is processed through a transformer-encoder [53] to extract features. These features are then decoded using an LSTM [21], with a Pooling Module (PM) to integrate the states of other agents into a unified feature vector. Training, as detailed in Section 4, maximizes the probability of human data relative to the specified goal. To promote collision-free states, a collision loss function is used, as detailed in [33]."}, {"title": "4.4 Neural Inverse Autoregressive Model", "content": "Unlike the NAR model, the Neural Inverse Autoregressive (NIAR) model by Kingma et al. [26] facilitates easier parallelization over time. The NIAR model's conditional probability distributions p(AZh|kt, Gh,\u03b8;\u03b8niar) are similar to those in (12) but use Zh\u223cN(0,I) for agent h, with Zt := Zt1:t. These Gaussian conditional densities are parameterized by the mean \u03bc\u03b7=f\u03bc(Zth,\u03b8;\u03b8niar) and covariance matrix \u03a3h=f\u03a3(Zt,\u03b8;\u03b8niar), both computed using neural networks. Each conditional is independent of other agents and time steps, allowing future actions to be generated in parallel. Each state is recursively determined using the transition function in (3), starting from the initial observed state sh."}, {"title": "4.5 Reward Function", "content": "The reward function is a sum of four distinct reward signals, represented as:\n\u03d5(At,Ar,St)=\u22114i=1\u03bb\u03af\u03c6i,(13)\nwhere each \u03bbi is a weight parameter associated with the corresponding scalar reward signal \u03c6i and St represents here the concatenation St = ss."}, {"title": "4.5.1 The Reward Map", "content": "As previously noted, a major challenge is the lack of comprehensive datasets that track human positions in real-world settings, along with contextual data crucial for robotic interpretation, such as occupancy grid maps. This gap means there's no data effectively integrating human behavior with the robot's environmental perspective.\nTo address this, a reward function incorporating a map is proposed for environments with static elements like walls and furniture, expressed as:\n\u03c61:=fc(fr(Ax,S)).(14)\nThe function fc: S\u03a9\u2192\u211d assigns a real-valued reward to each state on a map, indicating the difficulty of navigating the environment (cost map). It guides the robot's assessment of navigability at a given position. For example, areas with obstacles receive lower rewards, signaling zones to avoid, while open areas get higher rewards, indicating safe navigation routes. By optimizing this reward function, the robot is encouraged to move towards high-reward areas and avoid low-reward ones, enhancing navigational efficiency and safety."}, {"title": "4.5.2 Human Policy based Reward Signals", "content": "The calculation of the following three reward signals is based on the human policy. First, the desired goal position for each human is identified (see Section 4.6). The human policy \u03c0nar from (6) then predicts the actions of all k agents, including the robot, assuming humans perceive the robot as another human. As detailed in Section 4, the human policy and transition function enable autoregressive prediction of trajectories of length T. The robot's plan is integrated by substituting the model-generated robot state with the state from the robot's pre-sampled plan, anticipating human reactions to the robot's plan.\nThe first reward signal is the collision-free reward. Plans where no collision occurs between the robot and humans at time step t are assigned a higher reward. The collision-free reward is defined based on the CoLoss as introduced by Moder et al. [33], as:\n\u03c62:=\u2211Kk=2(1\u2212sig(\u03b2(||dk||2\u2212\u03b3coll))),(15)\nwhere ||dk||2 := || fr(Ax, Sx) - fh(Ah, Sh)||2 represents the Euclidean distance between the robot and the k-th agent. The sigmoid function is denoted by sig. The threshold \u03b3coll specifies the distance at which a collision is considered to occur, and \u03b2 determines the precision of this discrimination.\nDeriving an analytical reward for robot plans that imitate human behavior in complex scenarios is challenging. To address this, the human policy \u03c0nar is used as a discriminator to evaluate how closely a plan resembles human behavior. The human-imitation reward is defined as\u00b9:\n\u03c63:=log\u03c0nar(A|Sht,S<ht,gth,\u03b8;\u03b8nar),(16)\nwhere a high reward is given if a robot action Ar has a high log-likelihood. This reward function plays four critical roles in the SMPC algorithm design:\n\u2022 Human Behavior Imitation: This reward encourages the robot to imitate human behavior patterns, making its actions more understandable and predictable to nearby humans, thus fostering smoother interactions.\n\u2022 Ensuring Plans Remain within the Model\u2019s Distribution: This reward aspect ensures the robot\u2019s plans stay within the model\u2019s predictions, avoiding \"out of distribution\" plans that could cause confusion or safety concerns in human-robot interactions.\n\u2022 Interface Between Human Policy and SMPC: The human-imitation reward acts as an interface between the human policy and SMPC. It helps the robot use its understanding of human behavior to guide its actions, even if it cannot fully emulate human actions due to physical limitations.\n\u2022 Navigating in Environments with Static Obstacles: Integrating environmental information into the human policy can enhance the robot\u2019s navigation capabilities in crowded settings with static obstacles. Essentially, the robot has the potential to learn from the intuitive navigation strategies humans employ around others and static objects, such as furniture or walls.\nInspired by the Social Influence Loss introduced by Moder et al. [34], this work designs a Social Influence Reward (SIR) to regulate how much the policy expects humans to avoid the robot. Expecting too much space for the robot might lead to unsafe plans, while no clearance can cause the robot freezing problem. The SIR uses counterfactual reasoning to minimize the difference between conditioned and unconditioned predictions, based on a robot plan trajectory from SMPC. The SIR is defined as the summed Euclidean difference:\n\u03c64:=\u2211Kk=2||Sk\u2212Sk||2,(17)\nwhere S is the NAR prediction for agent k at time T conditioned on a robot trajectory, and S is independent of the robot trajectory. Unlike the SI [34], the difference is considered only at the final time T to avoid \"punishing\" robot plans that minimally impact humans' ability to reach their goals."}, {"title": "4.6 Human Goal Optimization", "content": "This paper discusses the use of goal-conditioned NAR or NIAR models to predict human movements based on predetermined goal positions Gh, but it does not yet detail the goal-setting method. Additionally, these models currently do not consider the environmental context, relying solely on the positional context of agents. This limitation can cause challenges in complex environments, like hospitals with many walls and narrow passageways, leading to impractical predictions, such as suggesting direct paths through walls, as shown in Figure 4.\nModer et al. [34] introduce the Goal Flow model for forecasting endpoint goal positions. We propose here an alternative method to reduce computational time and the likelihood of human trajectories intersecting with static obstacles like walls. This method uses an NIAR model without goal conditioning. With this model, a batch of trajectories is predicted with size Ns. The n-th action is denoted as nat. The trajectory with the highest reward map values (14) and highest likelihood with respect to p(a|Zkt, \u03b8; \u03b8niar) is selected. The goal position for each individual is determined based on the last position in these optimal trajectories. This goal optimization method is summarized in Algorithm 5."}, {"title": "4.6.1 Adaptive Sub-goal Navigation", "content": "With a map of the environment, our approach integrates seamlessly with a global planner, typically using a search-based algorithm like A-Star derived from the global occupancy map. Our method acts as a \"local\" planner, guiding the robot toward continuously updated sub-goals. Incorporating sub-goals from a global plan enhances navigation efficiency and avoids local minima, allowing the robot to interpret sensor data in the context of a broader strategy and identify optimal routes. For example, sub-goals help the controller recognize obstacles like walls in the global plan, facilitating efficient detours.\nThe velocity-adaptive sub-goal mechanism takes the global plan as input and outputs a sub-goal based on the robot's current velocity. This enables smoother velocity profiles, especially when navigating sharp turns in the global plan. The sub-goal dynamically adjusts to the robot\u2019s speed: higher velocities require a longer look-ahead distance for adapting to obstacles or turns, while slower speeds focus on immediate environmental details. The look-ahead range is constrained by the prediction horizon, robot dimensions, and reward map."}, {"title": "5 Results", "content": "We designed quantitative experiments using human datasets, as well as a real-world demonstration, with the intention of answering the following questions: Question 1: How is the performance of our approach compared to a selected baseline? Question 2: Can our algorithm outperform its individual components in collision avoidance and navigation tasks? Question 3: How does our algorithm perform in the real world?"}, {"title": "5.1 Human Data Benchmark", "content": "To address these queries, we evaluate our model and a baseline using real-world data instead of simulations, as real-world human interaction data better captures the complexity and unpredictability of human movements. Simulations often misrepresent these interactions, leading to an overestimation of algorithm performance.\nWe use the ETH [38], UCY [28], L-CAS [46] and Wildtrack [6] datasets. All data points are converted into world coordinates and interpolated at 0.4-second intervals. The joint dataset includes following subsets: two from ETH, three from UCY and one from Wildtrack and UCY respectively. For testing, we choose the most densely populated environment, \"UNIV\" from the UCY dataset, while the remaining datasets are used for training. The UNIV environment is divided into 412 individual scenes, each 20 seconds long (50 steps). The first 3.2 seconds (8 steps) of each scene, denoted as To, are observed states. We evaluate robot navigation performance using the testing protocol of Moder et al. [35]:\n1. Randomly select a human whose states are observable throughout the scene.\n2. Ensure the start and end positions of this human are at least 8m apart; otherwise, choose a different human.\n3. Remove the selected human's states from the observation set after the first To steps, so the robot cannot \"see\" them.\n4. Input the start and end positions of the selected human and the observed states of other agents into the navigation algorithm. Initialize the robot at the start position with the goal set to the end position of the selected human.\n5. If dynamics constraints are given, clip the robot\u2019s actions with DWC to ensure adherence to its dynamic constraints, regardless of the algorithm being evaluated.\nThis protocol provides a realistic benchmark for robot navigation in populated environments. The LoCoBot [36] is chosen as a representative platform, characterized by a linear speed of 0.7 m/s, angular speed of 1.0 rad/s, linear acceleration of 0.5 m/s\u00b2, and angular acceleration of 3.2 rad/s\u00b2. Each approach is evaluated 10 times due to the random human selection, with results summarized as mean values.\nIn the UNIV dataset, the average population density around the selected human within a 3 m radius is 0.3 humans/m2. Each goal position, typically 10.2 m away from the start, is feasibly reachable by a human. The NAR and NIAR models predict the next 12 timesteps, with the predicted goal position being, on average, 3.2 m away. The human trajectory in each scene provides insights into potential human behaviors, offering a meaningful benchmark for comparison."}, {"title": "5.1.1 Evaluation Metrics", "content": "The metrics are defined as follows:\n\u2022 Success: Percentage of robots that reach their goal without colliding.\n\u2022 Coll<21: Percentage of robots colliding with humans when the distance to their center points is below 0.21 m.\n\u2022 Coll<31: Percentage of robots within 0.31 m of human center points.\n\u2022 Timeout: Percentage of robots that fail to reach the goal within 16.4 s plus an additional 8 s tolerance.\n\u2022 Freezing Behavior (FB): Percentage of robot paths that are 1.25 times longer than the corresponding human path.\n\u2022 Max Freezing Behavior (maxFB): Highest ratio of robot path length to the corresponding human path length, expressed as a percentage."}, {"title": "5.1.2 Baselines", "content": "The MPPI approach, integrated with the NAR forecasting model, is designated as MPPI-NAR. The following algorithms are selected as baselines:\n\u2022 DWA [15]: A sampling-based navigation algorithm widely used in ROS, serving as a practical benchmark.\n\u2022 DWA-NAR: Integrates DWA with the goal-conditioned NAR model. Goals are determined using the methodology in Section 4.6.\n\u2022 GCBC-NAR and GCBC-NIAR: Variants of Goal-Conditioned Behavioral Cloning using NAR and NIAR models, respectively. Goals are established through human goal optimization in Section 4.6. Unlike MPPI and CEM, these models execute only the next most probable action without extensive planning. It is noteworthy that the GCBC-NIAR serves as a baseline for a robot that only sees the goal and not humans.\n\u2022 CEM-Hybrid [35]: Employs the Cross Entropy Method (CEM) for SMPC, using a hybrid NIAR and NAR goal-conditioned model with DWC. It optimizes in the latent space of the NIAR model, conducting stochastic optimization over three iterations, compared to one in MPPI.\n\u2022 CQL [27]: An offline RL approach using Dher to learn a conservative Q-function based on expert actions, mitigating the risk of overestimating states not in the expert dataset. MPPI-NAR actions are used as substitute expert data.\n\u2022 TD3+BC [16]: An offline RL method adding a behavior cloning term to policy updates and normalizing data, achieving comparable performance to CQL with reduced computational overhead. MPPI-NAR actions are used as expert data.\n\u2022 TD3 [17]: Is selected for the evaluation of online Actor-Critic RL. The testing protocol is adapted to an OpenAI Gym [3] environment to facilitate online training. Within this setting, individual policies engage in exploration while considering robot dynamics, in scenarios where simulated humans act as if unaware of the robot\u2019s presence. The strategy of pre-training TD3 Actor and Critic components using TD3+BC."}, {"title": "5.1.3 Benchmark Results and Discussion", "content": "Towards Question 1. The test protocol, which includes scenarios enforcing robot dynamic constraints (step 5), evaluates the algorithms. The data in Table 1(a) show that SMPC approaches MPPI-NAR and CEM-Hybrid outperform the baseline in success rate. Compared to MPPI-NAR, CEM-Hybrid is slightly superior in providing more efficient collision avoidance, although its runtime is observed to be 225% longer. This increase in computation time is attributed to the requirement of three optimization iterations for CEM, whereas MPPI does not benefit from additional iterations. Additionally, the DWA-NAR results indicate that sample"}]}