{"title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning", "authors": ["Martin Moder", "Stephen Adhisaputra", "Josef Pauli"], "abstract": "This paper addresses navigation in crowded environments by integrating goal-conditioned generative models with Sampling-based Model Predictive Control (SMPC). We introduce goal-conditioned autoregressive models to generate crowd behaviors, capturing intricate interactions among individuals. The model processes potential robot trajectory samples and predicts the reactions of surrounding individuals, enabling proactive robotic navigation in complex scenarios. Extensive experiments show that this algorithm enables real-time navigation, significantly reducing collision rates and path lengths, and outperforming selected baseline methods. The practical effectiveness of this algorithm is validated on an actual robotic platform, demonstrating its capability in dynamic settings.", "sections": [{"title": "1 Introduction", "content": "Navigating robots or autonomous cars in crowded areas can lead to \"robot freezing,\" where the robot becomes stationary due to an unclear path, as noted by Trautman et al. [50]. Traditional methods, which only predict crowd behavior without accounting for human-robot interaction, have proven inadequate. Thus, the focus has shifted to strategies promoting cooperative interactions between robots and humans. This has led to the creation of collaborative robots, or \"CoBots,\" designed to work alongside humans, adapting to both their movements and the complexities of crowded environments.\nReinforcement Learning (RL) is a popular method for CoBots navigation in crowds, offering a comprehensive framework that includes data-driven social acceptance and environment-specific behavior. Despite promising results in simulations and real environments [8, 14], RL faces challenges due to the need for online crowd interactions, which are costly and safety critical, leading to reliance on unrealistic simulations. Recently, Levine et al. [29] have employed offline RL to create task-specific policies using only pre-collected data, eliminating the need for online training. Offline RL requires a reward function to guide behavior, which can be specified during data collection or derived from handcrafted metrics. This is particularly challenging for \"social\" navigation, where the robot must interpret subtle social behaviors and cues.\nSupervised imitation learning, especially using large-scale generative models, shows great promise. These models have made notable contributions in visual recognition [41] and natural language processing [4]. As demonstrated by Cui et al. [12], these models can enhance robotic decision-making in complex environments, such"}, {"title": "2 Related Work", "content": "Two pioneering studies on navigation in human environments, RHINO [5] and MINERVA [48], both use the Dynamic Window Approach (DWA) [15] for local collision avoidance, a method still popular in current ROS packages. A key challenge is addressing the unpredictability of human behavior. Du Toit et al. [13] found that treating agents as independent entities creates overwhelming uncertainty, complicating navigation. Trautman et al. [52] showed that merely constraining this uncertainty, as Du Toit et al. proposed, can cause \"robot freezing.\" They advocated for a more cooperative approach between humans and robots.\nIn recent years, CoBots have advanced considerably. Start-ups like Robust AI, Diligent, and Veo Robotics, along with established manufacturers like Kuka and Fanuc, are developing CoBots for harmonious coexistence with humans in shared spaces, with applications ranging from medical navigation to manufacturing assistance. Despite ongoing research, the detailed methodologies of these companies are somewhat elusive. Diligent [23], for instance, uses \u201chuman-guided learning\" in hospital operations, where robots learn from their environments and human interactions. However, the integration of imitation learning in their framework is unclear, and their navigation system appears to rely on a classical map-based approach, using QR/APRIL tags at key locations like elevators and doors.\nThe advancement of CoBots is closely tied to progress in autonomous vehicles. Waymo uses a sense-predict-plan-act pipeline, combining sensor data with maps for environment perception [18]. Tesla, while following a similar pipeline with a focus on vision, treats lane detection as a linguistic task and employs a unique planning mechanism via tree search [47]. Recently, Tesla has announced a shift towards end-to-end learning. In contrast, Wayve is prioritizing an end-to-end learning pipeline, emphasizing real-time visual inputs over detailed maps [22]."}, {"title": "2.1 Learning a Policy", "content": "In most learning-based methodologies, planning occurs in a 2D space, analyzing human dynamics over time and representing social interactions as a comprehensive graph [35]. The focus within this 2D setup is on constructing interaction graphs using neural network structures that handle diverse groups of humans and track their movements over time. Some methods that predict future behaviors provide essential knowledge in this domain [7, 33]. Predominant learning approaches include RL and imitation learning, which can operate end-to-end or in a MPC setting. For instance, Chen et al. [10] pioneered the use of RL to learn a discrete value function suitable for a real robot. Everett et al. [14], using the Actor-Critic paradigm, demonstrated the feasibility of learning a policy for continuous actions. An alternative model-based RL approach is showcased by Chen et al. [8], where their relational graph learns the crowd dynamics model for subsequent tree search.\nThe referenced RL methods primarily utilize simulations, where human behaviors are often represented using ORCA [2] or the Social Force Model [20]. Simulating authentic human behavior in a crowd setting is challenging [32], and it becomes"}, {"title": "2.2 Planning with Generative Models", "content": "Outside the context of crowd navigation, many works have proposed model-based approaches. Common model-based algorithms learn the dynamics model of the world and use it for planning at test time, often through model predictive control and various trajectory optimization methods [11, 37, 42]. The cross-entropy method serves as a practical, sample-based alternative to gradient-based optimization methods, leveraging data-driven dynamics models [11, 35, 40, 54]. Some model-based approaches incorporate a learned policy alongside the dynamics model [54, 57], or employ the model to generate \"synthetic\" samples, enriching the sample set for model-free learning methods [39, 58].\nAnother approach, inspired by recent advancements in generative artificial intelligence enabled by transformers [53] especially in imitation learning and offline reinforcement learning is gaining traction. Notably, works that harness transformers in novel ways, diverging from the traditional reinforcement learning paradigm, stand out. For instance, Decision Transformers [9] and related methodologies [24, 45] focus on return-conditioned imitation learning."}, {"title": "3 Robot Navigation as a Multiplayer Game", "content": "Our focus is on a navigation algorithm for an autonomous robot sharing an environment with humans, ensuring the robot is mindful of its impact on human actions. We consider scenarios with K agents: the robot \\(r := 1\\) and humans \\(h := {2,..., K}\\). We introduce the necessary variables and derive the general objective.\nWe define continuous states and discrete time, with agent \\(k \\in K\\)'s states \\(S_t^k \\in \\mathbb{R}^{\\Omega_s}\\) at time \\(t\\) as 2-dimensional positions on a ground plane (\\(\\Omega_s := 2\\)), with the current time step \\(t = 0\\). The future scene of K agents over T time steps is \\(S_{1:T}^{1:K} \\in \\mathbb{R}^{T \\times K \\times \\Omega_s}\\). Let \\(S_t^r := S_t^1 \\in \\mathbb{R}^{\\Omega_s}\\) be the state of the robot, and \\(S_t^h := S_t^{2:K} \\in \\mathbb{R}^{(K-1) \\times \\Omega_s}\\) the states of all humans. Absence of a time step subscript denotes all future or past time steps, and absence of an agent index superscript denotes all agents, e.g., \\(S := S_{1:T}^{1:K}\\). Capital roman letters denote random variables, with realizations in roman lowercase. For instance, past states of all agents over a period \\(T_o\\) are \\(s_{-T_o:0}^{1:K}\\).\nThe next future states \\(S_{t+1}\\) of all agents, determined by their actions \\(A_t := A_t^{1:K} \\in \\mathbb{R}^{K \\times \\Omega_s}\\), use two transition functions: \\(f_r\\) for robot dynamics and \\(f_h\\) for human dynamics. Continuous actions \\(A_t\\) at time \\(t\\) are decided by a stochastic robot policy:\n\\[A_t^r \\sim \\pi \\left(\\cdot | S_t; \\theta_\\pi\\right),\\]\nwhere \\(\\theta_\\pi\\) represents the distribution parameters.\nThe robot's state is influenced by its actions as defined by the robot dynamics function:\n\\[S_{t+1}^r = f_r(S_t, A_t).\\]\nHuman actions \\(A_t^h := A_t^{2:K}\\) are decided by a human policy \\(\\pi_h : \\mathbb{R}^{(K-1) \\times \\Omega_s} \\rightarrow \\mathbb{R}^{(K-1) \\times \\Omega_s}\\), detailed in the next section. The human transition function is:\n\\[S_{t+1}^h = f_h(S_t^h, A_t^r) = S_t^h + A_t^r.\\]\nThe objective is to determine the optimal parameters, \\(\\theta_\\pi^*\\), for the robot's policy \\(\\pi^*\\), to maximize a specified scalar return \\(R \\in \\mathbb{R}\\). Considering \\(f_R(T) : \\mathbb{R}^{T \\times K \\times (\\Omega_s+\\Omega_a)} \\rightarrow \\mathbb{R}\\) as the finite-horizon undiscounted return function, the expected return is:\n\\[J(\\pi^*, \\pi^h) = \\mathbb{E}_\\tau [F_R(T)]\\]\n\\[= \\mathbb{E}_\\tau \\left[\\sum_{t=0}^{T-1} f_o(S_t, A_t^r, A_t^h)\\right]\\]\nwhere \\(\\mathbb{E}_\\tau [\\cdot]\\) denotes the expectation over the episode \\(\\tau = \\{S_0, A_0^r, A_0^h, S_1, ..., S_{T-1}\\}\\), starting"}, {"title": "4 Methods", "content": "In (5), the objective is framed as a multiplayer game, where each agent's actions are influenced by others, leading to complex interactions. Unlike the traditional RL goal of finding a single globally optimal policy, MPC iteratively seeks locally optimal parameters for the robot's policy. This process uses a predictive model to project future environmental states over a finite horizon T, incorporating the robot's policy and transition dynamics, as specified in (2), along with the human policy and its transition dynamics, as detailed in (3).\nTo address the complexities in (5), a goal-conditioned human density \\(p(A_t^h | S_{<t}, G_h, 0; \\theta_{nar})\\) is introduced. This density captures human control actions \\(A_t^h\\) conditioned on all observed states of all agents \\(S_{<t} := S_{1:t}^{1:K}\\) and their goals \\(G_h \\in \\mathbb{R}^{\\Omega_s}\\). The computation employs a NAR model, as specified in (12). The parameters, \\(\\theta_{nar}\\), are optimized to enhance the likelihood of the observed data, an approach also referred to as Goal Conditioned Behavior Cloning (GCBC). Consequently, the human policy is formalized as:\n\\[\\pi_{nar}(A_t^h | S_t, G_h, 0; \\theta_{nar})\\]\n\\[:= p(A_t^h | S_{<t}, G_h, 0; \\theta_{nar}).\\]\nThe dataset contains human positions extracted from real-world video recordings, as shown in Figure 2. To make this dataset compatible with GCBC, goal relabeling of future states is necessary. A common data augmentation technique, useful when the set of goals \\(G_h\\) is a subset of the observation space \\(S_h\\), is Hindsight Experience Replay (HER) [1]. This approach augments the dataset with additional goal information, as follows:\n\\[D_{her} := \\{\\left(s_{-T_o:t}^{1:K}, a_{t-1:t}^{2:K}, g_h\\right),\\left(s_{-T_o:t}^{1:K}, a_{t-1:t}^{2:K}, g_h^*\\right)\\}_{m=1}^{M_{her}}\\]\nwhere * denotes expert actions and states, and \\(M_{her}\\) is the number of scenes captured.\nWith the human policy defined, a response to a sequence of robot actions \\(a^r := a_{0:T-1}^r\\) aimed at achieving a robot goal \\(g^r\\) can be computed. This action sequence is converted into robot states \\(s^r\\) via the transition function \\(f_r\\). The human policy \\(\\pi_{nar}\\) then generates an autoregressive response, considering forecasted human goals \\(g_h\\). At each time step t, the robot state \\(s_t^r\\) and observed human states \\(s_t^h\\) are concatenated to create a joint state \\(S_t = s_t^r \\oplus s_t^h\\). The human policy applied to this joint state derives the human action \\(a_t^h\\), leading to the next human state \\(s_{h+1}\\) through \\(f_h\\). This"}, {"title": "4.1 Sampling-Based Model Predictive Control", "content": "The SMPC methodology is based on importance sampling. The robot's next action is determined by an optimal policy, which is unknown and cannot be directly sampled. However, samples can be evaluated using the reward function \\(f_o\\). The objective is to refine the known policy so its samples approximate the optimal distribution. An accurate reward function is essential for assessing the samples, as outlined in (5). Here, the robot policy for SMPC in the robot action space is described as a Gaussian density:\n\\[\\pi_{gauss} (A^r; \\mu_t, \\Sigma_t) = N(\\mu_t, \\Sigma_t),\\]\nwhere \\(\\mu_t\\) and \\(\\Sigma_t\\) are the mean and covariance at timestep t, respectively, defined in \\(\\mathbb{R}^{\\Omega_a}\\) and \\(\\mathbb{R}^{\\Omega_a \\times \\Omega_a}\\). In (8), policy actions are represented as linear and angular velocities, denoted by \\(\\acute{A}^r := \\acute{A}_{0:T-1}^r\\), where \\(\\acute{A}^r \\in \\mathbb{R}\\) and \\(\\acute{A}_{0:T-1}^r \\in \\mathbb{R}^{T \\times \\Omega_a}\\). For a two-wheeled robot (\\(\\Omega_s = 2\\)), one dimension is linear velocity and the other is yaw (angular velocity).\nThe robot constraints are managed using the Dynamic Window Approach (DWA), introduced by Fox et al. [15]. This technique sets a velocity window based on the current robot velocity and configuration. The window is then used to clip the robot's actions, as demonstrated in Algorithm 1."}, {"title": "4.2 Model Predictive Path Integral", "content": "In MPPI [55], the objective function is based on the 'free energy' concept from control theory. This reformulates the expected return in (4) as:\n\\[J(\\pi_{gauss}, \\pi_{nar}) = \\log \\mathbb{E}_{A^r} \\left[\\exp \\left(\\frac{1}{\\gamma} f_R(\\tau)\\right)\\right],\\]\nwhere \\(\\gamma > 0\\) is a scaling factor, or \"temperature,\" influencing the exploration-exploitation trade-off. A higher \\(\\gamma\\) encourages exploration, while a lower \\(\\gamma\\) favors exploitation. The term \\(\\pi_{nar}\\) represents"}, {"title": "4.3 Neural Autoregressive Model", "content": "In selecting a generative model for GCBC, we chose the Neural Autoregressive (NAR) model, based on insights from Moder et al. [35]. This study shows that this model, when conditioned with goal prediction, performs comparably to models like GAN [19] or VAE [25]. The model's ability to generate precise likelihood predictions aids in devising robot actions that replicate human behaviors. Advances in NLP, demonstrated by models like Llama2 [49], further underscore the NAR model's scalability.\nThe NAR model estimates the probability density of an agent's actions:\n\\[\\pi_{nar} := p(A_t^h | S_t, G_h, 0; \\theta_{nar}) = N(\\mu, \\Sigma),\\]"}, {"title": "4.4 Neural Inverse Autoregressive Model", "content": "Unlike the NAR model, the Neural Inverse Autoregressive (NIAR) model by Kingma et al. [26] facilitates easier parallelization over time. The NIAR model's conditional probability distributions \\(p(A_{kt}^h | G_h, o; \\Theta_{niar})\\) are similar to those in (12) but use \\(\\acute{Z}_t \\sim N(0, 1)\\) for agent h, with \\(Z_t := Z_{1:t}^{1:K}\\). These Gaussian conditional densities are parameterized by the mean \\(\\mu_h = f_\\mu(\\acute{Z}_{2:t}, 0; \\Theta_{niar})\\) and covariance matrix \\(\\Sigma_h = f_\\Sigma(\\acute{Z}_{t}, 0; \\Theta_{niar})\\), both computed using neural networks. Each conditional is independent of other agents and time steps, allowing future actions to be generated in parallel. Each state is recursively"}, {"title": "4.5 Reward Function", "content": "The reward function is a sum of four distinct reward signals, represented as:\n\\[f_o(A^r, A^h, S_t) = \\sum_{i=1}^4 \\lambda_i\\phi_i,\\]\nwhere each \\(\\lambda_i\\) is a weight parameter associated with the corresponding scalar reward signal \\(\\phi_i\\) and \\(S_t\\) represents here the concatenation \\(S_t = S_t^r \\oplus S_t^h\\)."}, {"title": "4.5.1 The Reward Map", "content": "As previously noted, a major challenge is the lack of comprehensive datasets that track human positions in real-world settings, along with contextual data crucial for robotic interpretation, such as occupancy grid maps. This gap means there's no data effectively integrating human behavior with the robot's environmental perspective.\nTo address this, a reward function incorporating a map is proposed for environments with static elements like walls and furniture, expressed as:\n\\[\\phi_1 := f_c(f_r(A^r, S)).\\]\nThe function \\(f_c : S \\rightarrow \\mathbb{R}\\) assigns a real-valued reward to each state on a map, indicating the difficulty of navigating the environment (cost map). It guides the robot's assessment of navigability at a given position. For example, areas with obstacles receive lower rewards, signaling zones to avoid, while open areas get higher rewards, indicating safe navigation routes. By optimizing this reward function, the robot is encouraged to move towards high-reward areas and avoid low-reward ones, enhancing navigational efficiency and safety."}, {"title": "4.5.2 Human Policy based Reward Signals", "content": "The calculation of the following three reward signals is based on the human policy. First, the desired goal position for each human is identified (see Section 4.6). The human policy \\(\\pi_{nar}\\) from (6) then predicts the actions of all k agents, including the robot, assuming humans perceive the robot as another human. As detailed in Section 4, the human policy and transition function enable autoregressive prediction of trajectories of length T. The robot's plan is integrated by substituting the model-generated robot state with the state from the robot's pre-sampled plan, anticipating human reactions to the robot's plan.\nThe first reward signal is the collision-free reward. Plans where no collision occurs between the robot and humans at time step t are assigned a higher reward. The collision-free reward is defined based on the CoLoss as introduced by Moder et al. [33], as:\n\\[\\phi_2 := \\sum_{k=2}^K \\left[1 - sig(\\beta(||d_k||^2 - \\gamma_{coll}))\\right],\\]\nwhere \\(\\||d_k\\||^2 := \\| | f_r(A^r, S_t^r) - f_h(A_t^h, S_t^h) \\| |_2\\) represents the Euclidean distance between the robot and the k-th agent. The sigmoid function is denoted by sig. The threshold \\(\\gamma_{coll}\\) specifies the distance at which a collision is considered to occur, and \\(\\beta\\) determines the precision of this discrimination.\nDeriving an analytical reward for robot plans that imitate human behavior in complex scenarios is challenging. To address this, the human policy \\(\\pi_{nar}\\) is used as a discriminator to evaluate how closely a plan resembles human behavior. The human-imitation reward is defined as\u00b9:\n\\[\\phi_3 := \\log \\pi_{nar} (A_t^r | S_t^h, S_{<t}, g^r, 0; \\theta_{nar}),\\]\nwhere a high reward is given if a robot action \\(a_t^r\\) has a high log-likelihood. This reward function plays four critical roles in the SMPC algorithm design:\n\u2022 Human Behavior Imitation: This reward encourages the robot to imitate human"}, {"title": "4.6 Human Goal Optimization", "content": "This paper discusses the use of goal-conditioned NAR or NIAR models to predict human movements based on predetermined goal positions \\(G_h\\), but it does not yet detail the goal-setting method. Additionally, these models currently do not consider the environmental context, relying solely on the positional context of agents. This limitation can cause challenges in complex environments, like hospitals with many walls and narrow passageways, leading to impractical predictions, such as suggesting direct paths through walls, as shown in Figure 4.\nModer et al. [34] introduce the Goal Flow model for forecasting endpoint goal positions. We propose here an alternative method to reduce computational time and the likelihood of human trajectories intersecting with static obstacles like walls. This method uses an NIAR model without goal conditioning. With this model, a batch of trajectories is predicted with size \\(N_s\\). The n-th action is denoted as \\(n \\acute{a}_t^r\\). The trajectory with the highest reward map values (14) and highest likelihood with respect to \\(p(a | \\acute{Z}_{kt}, 0; \\Theta_{niar})\\) is selected. The goal position for each individual is determined based on the last position in these optimal trajectories. This goal optimization method is summarized in Algorithm 5."}, {"title": "4.6.1 Adaptive Sub-goal Navigation", "content": "With a map of the environment, our approach integrates seamlessly with a global planner, typically using a search-based algorithm like A-Star derived from the global occupancy map. Our method acts as a \"local\" planner, guiding the robot toward continuously updated sub-goals. Incorporating sub-goals from a global plan enhances navigation efficiency and avoids local minima, allowing the robot to interpret sensor data in the context of a broader strategy and identify optimal routes. For example, sub-goals help the controller recognize obstacles like walls in the global plan, facilitating efficient detours.\nThe velocity-adaptive sub-goal mechanism takes the global plan as input and outputs a sub-goal based on the robot's current velocity. This enables smoother velocity profiles, especially when navigating sharp turns in the global plan. The sub-goal dynamically adjusts to the robot's speed:"}, {"title": "5 Results", "content": "We designed quantitative experiments using human datasets, as well as a real-world demonstration, with the intention of answering the following questions: Question 1: How is the performance of our approach compared to a selected baseline? Question 2: Can our algorithm outperform its individual components in collision avoidance and navigation tasks? Question 3: How does our algorithm perform in the real world?"}, {"title": "5.1 Human Data Benchmark", "content": "To address these queries, we evaluate our model and a baseline using real-world data instead of simulations, as real-world human interaction data better captures the complexity and unpredictability of human movements. Simulations often misrepresent these interactions, leading to an overestimation of algorithm performance.\nWe use the ETH [38], UCY [28], L-CAS [46] and Wildtrack [6] datasets. All data points are converted into world coordinates and interpolated at 0.4-second intervals. The joint dataset includes following subsets: two from ETH, three from UCY and one from Wildtrack and UCY respectively. For testing, we choose the most densely populated environment, \"UNIV\" from the UCY dataset, while the remaining datasets are used for training. The UNIV environment is divided into 412 individual scenes, each 20 seconds long (50 steps). The first 3.2 seconds (8 steps) of each scene, denoted as \\(T_o\\), are observed states. We evaluate robot navigation performance using the testing protocol of Moder et al. [35]:\n1. Randomly select a human whose states are observable throughout the scene.\n2. Ensure the start and end positions of this human are at least 8m apart; otherwise, choose a different human.\n3. Remove the selected human's states from the observation set after the first \\(T_o\\) steps, so the robot cannot \"see\" them.\n4. Input the start and end positions of the selected human and the observed states of other agents into the navigation algorithm. Initialize the robot at the start position with the goal set to the end position of the selected human."}, {"title": "5.1.1 Evaluation Metrics", "content": "The metrics are defined as follows:\n\u2022 Success: Percentage of robots that reach their goal without colliding.\n\u2022 Coll<21: Percentage of robots colliding with humans when the distance to their center points is below 0.21 m.\n\u2022 Coll<31: Percentage of robots within 0.31 m of human center points.\n\u2022 Timeout: Percentage of robots that fail to reach the goal within 16.4 s plus an additional 8 s tolerance.\n\u2022 Freezing Behavior (FB): Percentage of robot paths that are 1.25 times longer than the corresponding human path.\n\u2022 Max Freezing Behavior (maxFB): Highest ratio of robot path length to the corresponding human path length, expressed as a percentage."}, {"title": "5.1.2 Baselines", "content": "The MPPI approach, integrated with the NAR forecasting model, is designated as MPPI-NAR. The following algorithms are selected as baselines:\n\u2022 DWA [15]: A sampling-based navigation algorithm widely used in ROS, serving as a practical benchmark.\n\u2022 DWA-NAR: Integrates DWA with the goal-conditioned NAR model. Goals are determined using the methodology in Section 4.6.\n\u2022 GCBC-NAR and GCBC-NIAR: Variants of Goal-Conditioned Behavioral Cloning using NAR and NIAR models, respectively. Goals are established through human goal optimization in Section 4.6. Unlike MPPI and CEM, these models execute only the next most probable action without extensive planning. It is noteworthy that the GCBC-NIAR serves as a baseline for a robot that only sees the goal and not humans.\n\u2022 CEM-Hybrid [35]: Employs the Cross Entropy Method (CEM) for SMPC, using a hybrid NIAR and NAR goal-conditioned model with DWC. It optimizes in the latent space of the NIAR model, conducting stochastic optimization over three iterations, compared to one in MPPI.\n\u2022 CQL [27]: An offline RL approach using \\(D_{her}\\) to learn a conservative Q-function based on"}, {"title": "5.1.3 Benchmark Results and Discussion", "content": "Towards Question 1. The test protocol, which includes scenarios enforcing robot dynamic constraints (step 5), evaluates the algorithms. The data in Table 1(a) show that SMPC approaches MPPI-NAR and CEM-Hybrid outperform the baseline in success rate. Compared to MPPI-NAR, CEM-Hybrid is slightly superior in providing more efficient collision avoidance, although its runtime is observed to be 225% longer. This increase in computation time is attributed to the requirement of three optimization iterations for CEM, whereas MPPI does not benefit from additional iterations. Additionally, the DWA-NAR results indicate that sample-based planning proves more effective when planning for every future action rather than focusing solely on the next action in relation to the current state.\nThe importance of covariate shift is highlighted by the \"offline\" approaches, GCBC and CQL. These models can guide the robot towards"}, {"title": "5.1.4 Implementation Details", "content": "Notably, the ETH&UCY datasets do not label the physical shape of humans, so a collision event is determined based on Euclidean distances, with less than 0.2m defined as a collision, following previous works [33-35, 52]. Accordingly, the collision cost distance parameter, \\(\\gamma_{coll}\\), is set to 0.2m. For both training and inference, 8 states are observed (\\(T_o := 8\\)) and 12 states are predicted (\\(T := 12\\)). Shorter history sequences are padded with zeros.\nTo increase efficiency, only the five humans closest to the robot within a 5m radius are considered. The goal and all states are represented relative to the robot's current position, reducing reliance on fixed world coordinates. To ensure consistency and prevent encountering unseen values, the goal distance is capped at 10m and limited to a minimum of 3m to maintain a consistent speed as the robot moves toward endpoints rather than waypoints.\nThe following parameters are determined empirically: the weighting factor for avoiding collisions with humans, \\(\\lambda_2\\), is set to \\(10^3\\); the weighting factors for avoiding collisions with the environment, \\(\\lambda_1\\), and for human behavior imitation, \\(\\lambda_3\\), are both set to 1; \\(N_s := 800\\) samples are taken per iteration. Additionally, the temperature parameter \\(\\gamma\\) is set to 1, \\(\\beta\\) is set to 35, and \\(\\gamma_{coll}\\) is set to 0.2m. For CEM, the default number of iterations is set to three.\nThe reward function for CQL, TD3, and TD3+BC differs from that discussed in Section 4.5. This function awards a positive reward for approaching the goal and applies a negative reward for collisions or timeouts, tailored to the robot's proximity to the goal and nearby humans. The d3rlpy framework [44] is used for the RL implementation, and PyTorch is used for the neural network implementation. For further details and to access the dataset used, interested readers are referred to the following repository: code."}, {"title": "5.2 Real-World Demonstrations", "content": "Towards Question 3. To accomplish real-world locomotion tasks, the MPPI-NIAR\u00b2 algorithm is implemented on the mobile robot platform LoCoBot [36], as depicted in Figure 6, using ROS 2 Humble. This open-source, differential-drive robot is equipped with a 2D lidar and an Intel NUC featuring an 8th Gen Intel Core i3 processor. The system is upgraded by replacing the default Intel Realsense camera with the more advanced Stereolabs ZED 2 3D camera and augmenting it with an additional computing unit, the ZedBox, powered by an Nvidia Jetson Xavier NX board."}, {"title": "6 Conclusion", "content": "This study tackles CoBot navigation in crowded environments using goal-conditioned generative models from human crowd videos. These models predict human reactions and select plans that mimic human navigation, and provides a promising direction to enhancing the robot's goal achievement and human acceptance.\nRefining the planning process with goal-conditioning and SIR ensures efficient navigation while respecting social and personal space. SMPC leverages the generative model to produce multiple path samples, managing kinematic and dynamic constraints across different robot platforms.\nExperiments with real-world data demonstrate the method's superiority in safety and efficiency over traditional approaches. Standalone goal-conditioned behavior cloning and offline reinforcement learning struggle due to a lack of interactive data, while online reinforcement learning shows modest but environment-sensitive improvements. Integrating goal-conditioned behavior cloning with SMPC achieves high success rates, robustness, efficiency, and adaptability. Although this work focuses on learning-based and sampling-based planning algorithms, many other approaches should be compared in future studies. A real-world test with LoCoBot demonstrates compatibility with existing navigation systems, real-time capability, and safety. Future research should address the need for more comprehensive training data to better capture human dynamics and environmental factors, reducing dependency on reward function design and enabling"}]}