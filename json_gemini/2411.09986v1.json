{"title": "Unlocking Transfer Learning for Open-World Few-Shot Recognition", "authors": ["Byeonggeun Kim", "Juntae Lee", "Kyuhong Shim", "Simyung Chang"], "abstract": "Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.", "sections": [{"title": "1. Introduction", "content": "Few-shot learning (FSL) has got a lot of attention due to the importance in enabling models to adapt to novel tasks using a few examples (e.g., N-way K-shot: a task involving N distinct classes, each represented by K examples) [10, 17, 27, 34, 37, 42]. However, in practical applications, FSL models inevitably encounter instances that do not belong to the N classes, also known as open-set instances. Addressing this challenge has led to the emergence of the field of few-shot open-set recognition (FSOSR) [15, 16, 19, 24, 41].\nIn FSOSR, if two N-way K-shot FSOSR tasks have distinct closed sets, their corresponding open sets will also differ. This interdependence presents a key challenge when adapting to novel tasks in FSOSR. Namely, it is essential to redefine not only the closed set but also the open set, since the open set is inherently shaped by its closed set. Consequently, the open set lacks a universal definition across various FSOSR tasks; instead, it requires contextual consideration based on the closed set of a specific target task.\nDespite recent advancements in the field, current works [15, 16, 19, 24, 41] have commonly focused on leveraging prior knowledge from a large training dataset. Then, they frequently struggle to balance closed-set accuracy with open-set recognition capabilities, often prioritizing open-set recognition at the expense of closed-set accuracy. Then, these approaches face challenges in achieving broad generalization across various benchmarks. In this work, we bring attention to the novel application of transfer learning within this field. Transfer learning [6, 7, 14, 30] has been extensively studied and demonstrated its efficacy leveraging a pre-trained model to generalize it to other tasks. Recent FSL methods [17, 31, 34, 35, 39] have shown the efficacy of this approach. However, when they come to FSOSR, open-set examples are inherently not present, which significantly undermines the effect of transfer learning in terms of open-set recognition. Then, as in Fig. 1, the naive extension of the transfer learning techniques of FSL, e.g. IER-distill [34] and Label Halluc. [17] fails to attain the same level of improvement in open-set recognition as seen in closed set, or even results in decreased result.\nTackling this point, we propose a two-staged FSOSR learning framework. Our method involves two stages: open-set aware meta-learning (OAL) and open-set free transfer learning (OFL). During the meta-learning stage, our objective extends beyond the meta-training of the feature encoder; we also aim to establish a universal open-set representation. This equips us with a decent starting point for the subsequent open-set free transfer learning. In the transfer learning stage, we commence by initializing the model using the parameters obtained from the meta-learning stage. To counteract the absence of open-set examples, we develop two alternative open-set sampling strategies. The first approach curates a training dataset of the previous stage as a source of open-set examples for open-set free transfer learning. For more pragmatic application, our second strategy is confined to the closed-set examples present in the target task, and exploits an episodic learning framework. Here, we generate pseudo FSOSR episodes by randomly dividing the closed-set categories into a closed set and a pseudo open set. As a result, our OAL-OFL method attains a marked enhancement in performance metrics on the standard FSOSR benchmarks as depicted in Fig. 1 while incurring a minimal extra training expense of only 1.5% compared to training without transfer learning. This allows OAL-OFL to surpass the existing state-of-the-art (SOTA) methods.\nOur contributions are summarized as four-fold:\n\u2022 We introduce a novel two-staged learning called OAL-OFL, bringing transfer learning to FSOSR for the first time with only a minimal additional cost.\n\u2022 We show the importance of preparing the model through open-set aware meta-learning, which is a sturdy starting point for transfer learning.\n\u2022 We suggest two breakthroughs to handle the lack of open-set examples during the transfer learning stage.\n\u2022 By leveraging the effectiveness of transfer learning, our proposed OAL-OFL achieves SOTA on miniImageNet and tieredImageNet datasets. This underscores its ability to generalize across various tasks, enhancing both closed-set classification accuracy and open-set recognition capabilities."}, {"title": "2. Related Works", "content": "FSL [9] can be streamlined into two approaches: meta-learning and transfer learning. Meta-learning, also known as learning-to-learn, can be categorized into optimization and metric-based methods. Optimization-based methods [10, 11, 22, 27] involve training a meta-learner to adapt quickly to new tasks through a few optimization steps by learning adaptation procedures. In contrast, metric-based methods [20, 37, 40, 42] use a common feature embedding space where target classes of a new task can be distinguished using a distance metric. Recently, transfer learning-based approaches [17, 31, 34, 35] have been suggested with superior results where a model trained on a large-scale base dataset is fine-tuned to a few-shot task with a small number of support examples. Our approach aligns with the transfer learning-based approaches of the few-shot classification, but transfer learning has yet to be explored in the context of FSOSR.\nOSR aims to identify in-distribution samples within a closed set, and simultaneously detect the out-of-distribution samples from unseen classes. Early approaches focused on the confidence level and proposed various approaches such as using extreme value theory [1, 2, 13, 36]. Recently, there has been a growing interest in generative model-based approaches [28, 29, 38], which leverage the different reconstruction behavior between in-distribution and out-of-distribution samples. On the other hand, our method aligns more closely with distance-based approaches [18, 25, 43], particularly those that specify open-set representation [3, 45]. However, OSR methods inherently assume the distinct and fixed class pools for the closed and open sets, which is not feasible in FSOSR, where both sets vary depending on a task. As a result, it is hard to straightforwardly apply the conventional OSR techniques to FSOSR.\nFSOSR. Compared to few-shot classification and OSR, FSOSR has been less explored. PEELER [24] introduced FSOSR task and aimed to maximize the entropy of open-set examples with Gaussian embeddings. They employed OSR approach [1] of detecting open-set examples based on the largest class probability. SnaTCHer [16] used transformation consistency that similar examples remain closer after a set-to-set transformation. TANE [15] and D-ProtoNet [19] proposed a task-dependent open-set generator using support examples. Considering low-level features as well as semantic-level ones, GEL [41] designed a local-global energy score to measure the deviation of the input from the closed-set examples. The aforementioned methods primarily employ the representative metric learning approach, ProtoNets [37], and face difficulties in achieving generalization across diverse benchmarks. Our proposed approach, however, involves a two-stage learning process. The first stage focuses on meta-learning to train a task-independent open-set classifier that serves as an effective initialization for the second stage of transfer learning."}, {"title": "3. Proposed Method", "content": "We present the proposed two-stage transfer learning process, dubbed OAL-OFL. The overall framework of OAL-OFL is depicted in Fig. 2. First, we describe the first stage, open-set aware meta-learning, where a universal FSOSR metric space is obtained by learning the feature encoder and a learnable open-set prototype. Then, we elaborate on the open-set free transfer learning stage, especially suggesting two approaches to resolve the challenges of the absence of open-set examples.\nProblem definition. The goal of FSOSR is two-fold: to classify an input query into one of the closed-set categories or to reject it as belonging to the associated open-set categories. Formally, an N-way K-shot FSOSR task can be represented as T = {S,Q,Q|C,C}, where C denotes a set of N closed-set categories. The closed set is described by a support set S which includes K examples for each category, i.e. {xi, yi} and |S| = NK. Here, x represents the input data, while y indicates its corresponding label. The query set for C is denoted by Q. Distinctively, FSOSR also accommodates open-set queries Q, which belong to categories, C, that do not intersect with C and are not explicitly defined."}, {"title": "3.1. Stage-1: Open-set Aware Meta-Learning", "content": "We commence by meta-learning a metric space, which serves as a general starting point to tune the model to a specific target task in the open-set free transfer learning of Stage-2. As depicted in Fig. 2 (a), this meta-learning stage involves the simultaneous training of the feature encoder fo and a learnable open-set prototype co. These are learned using FSOSR tasks drawn from the base training dataset, denoted as Dbs. Though the definition of open set varies depending on C, the co is desired to be task-independent across VC. Given the complexities involved in learning the task-specific open set classifier during Stage-2, the effective training of a reliable co serves as a crucial link to bridge the two stages.\nIn specific, given an N-way K-shot FSOSR task, T ~ Dbs, we conceptualize this as the (N + 1)-way classification problem. In this framework, the (N + 1)th class is designated as the open-set class. For n-th class of C, the classifier is formulated as the prototype Cn which is defined as the mean feature vector over its K examples, i.e., Cn = 2m=1 fo(xm) [37]. Here, fo(x) produces a D-dimensional feature vector. Concurrently, for the open-set class, the corresponding classifier is characterized by the learnable open-set prototype Co.\nSubsequently, the classification probability is\n\n$p(y = n|x) = \\frac{e^{-a \\cdot d(c_0,f_\\theta(x))+b}}{e^{-a \\cdot d(c_0,f_\\theta(x))+b} + \\sum_{n'=1}^N e^{-d(c_{n'},f_\\theta(x))}}$\nif n = N + 1,\n$p(y = n|x) = \\frac{e^{-d(c_n,f_\\theta(x))}}{e^{-a \\cdot d(c_0,f_\\theta(x))+b} + \\sum_{n'=1}^N e^{-d(c_{n'},f_\\theta(x))}}$\nOtherwise,\n\nwhere d is a distance metric. Specifically, we use the squared Euclidean distance, d(v, v') = ||v \u2013 v\u2032||2. In addition, scalars a and b are also learned to calibrate the distance to co, inspired by D-ProtoNets [19]. This calibration enables that co is solely accountable for representing open sets of various tasks. For the loss function, we utilize the cross-entropy (CE) loss which is formulated as:\n$L_{CE}(x_i, y_i) = - log p_\\theta (y = y_i|x_i)$.\nWe further utilize the masked CE loss [45], defined as:\n$L_{Mask} (x_i, y_i) = \u2212log p_{\\theta\\backslash y_i} (y = N + 1|x_i)$,\nwhere p\u03b8\\yi denotes the N-way classification probability, excluding the ground-truth label yi. This loss term effectively creates pseudo open-set tasks by disregarding the true label. More precisely,\n\n$p_{\\theta\\backslash y_i} (y = N + 1|x_i) = \\frac{e^{-d_{N+1}}}{e^{-d_{N+1}} + \\sum_{n'=1,n'\\neq y_i}^N e^{-d(c_{n'},f_\\theta(x_i))}}$,\n\nwhere we let dN+1 = a\u00b7d(c\u00f3, fo(xi)) + b for brevity.\nOverall, we learn fo, c\u00f3, a, and b as\n01,01, a, b = arg min{Ex\u2208QLCE(x, N + 1)\n+ E(x,y)\u2208Q{LCE(x, y) + LMask(x, y)}}."}, {"title": "3.2. Stage-2: Open-set Free Transfer Learning", "content": "In this stage, we consider a target task, denoted as Tte = {Ste, Qte, Qte Cte, Cte}, which is also configured as an N-way K-shot task. Note that Qte and Qte are utilized solely for the purposes of inference during test. The initial values of the learnable parameters are inherited from results of Stage-1. Specifically, fe, a and b are imprinted as fo, a and b\u2081, respectively. We aim to train them alongside (N + 1)-way classifiers. These classifiers are represented as {w1,\u2026, WN+1}, where each wn e RD. The first N classifiers are set as the prototypes of the corresponding classes in Cte. The open-set classifier WN+1 is initialized using Co\u2020 .\nTo ensure consistency with the meta-learned metric space, we compute the classification probability of each query based on the Euclidean distance as in Eq. (1). Then, during the transfer learning, we optimize fe and the classifier gy = {w1,\u2026, WN+1, a, b}.\nSince Tte lacks cues to learn the open-set classifier WN+1, we suggest two approaches: i) sampling from the base training dataset Dbs of Stage-1, and ii) sampling from the closed set Cte of Tte itself.\n3.2.1 Open-set sampling from base training dataset\nNotice that in the context of FSL, the closed-set categories Cte in Tte are exclusive with Dbs. As a result, we can exploit Dbs as a pool of open-set examples for Tte. Specifically, as in the green-colored of Fig. 2(b), we randomly select M examples from Dbs at every iteration of the transfer learning to serve as open-set examples.\nThen, the model is optimized as:\n02, 4\u00bd = arg min{E(x,y)\u2208SteLCE(x,y)\n+ Ex~Dbs LCE(x, N + 1)}.\nWe call this unified process of Stage-1 and 2 with the base training dataset as OAL-OFL. In the following section, we will introduce the OAL-OFL with a more practical open-set sampling, dubbed OAL-OFL-Lite.\n3.2.2 Pseudo open-set sampling from closed set\nIn real-world scenarios, Tte encompasses various categories, and we cannot guarantee that the closed set Cte is not overlapped with the categories of Dbs. Additionally, after the meta-learning stage, the large-scale Dbs may not be affordable due to practical constraints. To address these issues, we introduce OAL-OFL-Lite which operates with no necessity of Dbs.\nOur strategy is the episodic random class sampling from the closed set Cte itself to learn the open set. As exemplified in the purple-colored of Fig. 2(b), we iteratively partition Cte into the mutually exclusive subsets \u00c9te and Cte. Subsequently, their corresponding support sets Ste and Ste extracted from Ste are used to transfer-learn the closed and open sets, respectively. Hence, we call \u0108te pseudo open set. Through this iterative pseudo open-set sampling, we can effectively learn the open-set classifier as well as the closed-set ones.\nThen, the model is optimized by the CE losses as\n02,4 = arg min{E(x,y)\u20ac\u015ate LCE(x,y)\n+ ExES LCE (x, Cte| +1)}.\nwhere the open set is mapped to (|\u0106te| + 1)th class and detected by WN+1 in every iteration. Moreover, we empirically found that the open-set representation tends to be overfitted to a testing task in OAL-OFL-Lite. Hence, we freeze WN+1 once it is initialized by the Co\u2020. For more comprehensive understanding, we include Algorithm 1."}, {"title": "4. Experimental Results", "content": "4.1. Implementation Details\nIn line with established FSOSR approaches [15, 16, 24, 41], we conducted experiments using ResNet12 [21] as the feature encoder on miniImageNet [40] and tieredImageNet [33] benchmarks, with pre-training of the feature encoder accomplished on the corresponding base training dataset for each benchmark through the well-known FSL method, FEAT [42]. MiniImageNet is comprised of 100 categories each with 600 examples. These categories are divided into 64, 16, and 20 classes for training, validation, and testing purposes, respectively [32]. TierdImageNet consists of 608 categories and a total of 779,165 samples, which are divided into 351, 97, and 160 classes for training, validation, and testing, respectively.\nTraining details. Stage-1 employs episodic learning on the base training dataset, Dbs, and trains the model for 20,000 episodes using the SGD optimizer with the Nesterov momentum [26] and the weight decay of 5e-4. The learning rates are initialized to 2e-4 and 2e-5 for the encoder and the learnable open-set prototype, respectively, and then decayed by multiplying 0.5 every 40 iterations. Each episode consists of a combination of closed-set categories and an equal number of open-set categories, each containing 15 queries. The distance adjusting factors a and b are initialized to 1 and 0, respectively. Layer-task normalization [16] is used as the embedding adaptation, which is discarded after the initialization of the classifiers in Stage-2.\nIn Stage-2, the proposed transfer learning is performed for 300 iterations on each testing task using the SGD optimizer. For OAL-OFL, the learning rates for the feature encoder and classifier are set to 2e-4 and 2e-3, respectively. For OAL-OFL-Lite, the learning rate is 2e-4 for both. The weight decay and momentum are set to 5e-4 and 0.9. For episode configuration in OAL-OFL-Lite, one closed-set class is randomly sampled as the pseudo open set at every iteration.\nEvaluation protocol. We use the predicted probability for (N + 1)th class as an open-set score and report threshold-free measurement AUROC [5, 8]. For closed-set evaluation, we predict the class with the highest probability among closed-set categories and report top-1 accuracy (Acc).\nCompared methods. We conduct a comparative analysis of our OAL-OFL method against various existing approaches, encompassing both FSL methods such as [37, 42] and FSOSR methods [15, 16, 24, 41]."}, {"title": "4.2. Comparative Assessment", "content": "Table 1 presents a comparison between our OAL-OFL and baselines in 5-way {1, 5}-shot settings on miniImageNet and tieredImageNet. The objective of FSOSR is to excel in open-set recognition while simultaneously maintaining high closed-set accuracy. Whereas prior methods have predominantly concentrated on open-set recognition, our approach has demonstrated significant improvements in closed-set accuracy, showcasing clear advantages over these earlier techniques.\nSince FSL approaches [37, 42] were not designed for FSOSR, we applied them to FSOSR in a straightforward manner as in [16]. In brief, in the FSL methods, the open-set detection score is computed by taking the negative of the largest classification probability [23].\nCompared to the FSOSR methods, PEELER, SnaTCH-ers and ATT, both OAL-OFL and OAL-OFL-Lite show notably better generalized capabilities, as reflected by their Acc and AUROC across datasets. It is observed that the previous approaches exhibit a particular bias towards mini-ImageNet. ATT-G improves ATT by using the base class prototypes to enhance closed-set prototypes. Nevertheless, it is even worse than our OAL-OFL-Lite which does not use the base training dataset. GEL makes predictions for both pixel-wise and semantic-wise by adding a 2D convolutional block on top of the SnaTCHer-F. It attained the previous SOTA in miniImageNet, but not in all measures. Whereas we achieve SOTA performance in all cases. In 1-shot setting, we slightly outperform GEL in AUROC, but are much better in Acc (1.52% in miniImageNet and 1.23% in tieredImageNet). Notice that in FSOSR, both closed-set classification and open-set detection are important. In 5-shot setting, our method shows clear SOTA performance in both Acc and AUROC. In specific, 2.59% Acc and 0.84% AUROC in miniImageNet, and 2.15% Acc and 1.41% in tieredImageNet. Namely, when only a few examples per class are increased (one to five), the proposed open-set free transfer learning more faithfully makes it beneficial. Also, even without the base training dataset, OAL-OFL-Lite surpasses or is comparable to the existing FSOSR methods in both datasets. Therefore, we conclude that our two-stage approach successfully pioneers transfer learning for FSOSR."}, {"title": "4.3. Analysis", "content": "In this section, we extensively analyze the proposed method on miniImageNet. More analyses are in the supplementary materials.\nDifficulty of TL in FSOSR. To see the efficacy of the respective stage of our method especially in FSOSR, we first analyze the proposed OAL-OFL comparing with two straightforward transfer learning schemes in Table 10. First, in 'Naive TL to closed set' of the first row, we conduct a naive transfer learning [34] without the base training dataset. Namely, the model is fit to N-way closed-set classification through linear regression [4], and rejects the open-set queries thresholding max-probability [1]. This naive approach shows severely low performance, which indicates the difficulty of FSOSR transfer learning. Second, in 'Stage-2 only + regul.,' the pseudo-label-based regularization exploits the base training dataset to compute the Kullback-Leibler divergence between the pre-trained model and transfer-learned model, which showed promising results in transfer learning for FSL [17]. Skipping Stage-1, we can apply this regularization technique during transfer learning which is 'Stage-2 only + regul.' This approach (single-stage + regularization) shows only slight improvement compared to the case of only Stage-2 (1st row of the lower part of Table 10). Whereas, our OAL-OFL achieves meaningful transfer learning results by combining the two stages.\nTwo-stage training (OAL-OFL). To demonstrate the impact of the proposed two-stage training approach, we first ablate the stages in Table 10 for OAL-OFL. When Stage-1 is skipped, the open-set classifier WN+1, and scalars (a, b) are initialized in random and (1,0), respectively in Stage-2. Relying solely on Stage-2 causes overfitting to the training examples. Then, it leads to highly degenerated results of 56.72% and 78.59% for 1- and 5-shot closed-set classification Acc, respectively, and 67.47% and 76.64% for 1- and 5-shot open-set detection AUROC, respectively. These results highlight that the open-set aware meta-learning of Stage-1 is critical for the success of transfer learning in FSOSR.\nNotice that, in ATT-G [15], the base training dataset calibrates the closed-set classifiers even in testing phase, but it marginally improves ATT [15] (see Table 1). Whereas, in the proposed OAL-OFL, the use of the base training dataset during the open-set free training of Stage-2 gives notable improvement. Applied on top of Stage-1, Stage-2 attains notably improved results by 1.83% and 2.70% in Acc, and 2.07% and 2.22% in AUROC for 1- and 5-shot respectively.\nIn Fig. 6, we present a visualization of the distributions of classification probability towards the open-set class, derived from both closed-set and open-set queries. The visualization reveals that, in the scenario labeled as 'Stage-2 only,' where the open-set aware meta-learning is absent, a majority of the queries are assigned high probabilities of belonging to the open-set, irrespective of their actual membership to the closed or open set. Converserly, in 'Stage-1 only' scenario, a significant number of closed-set queries are still attributed probabilities exceeding 0.5. On the other hand, in OAL-OFL, it is observed that the majority of open-set queries attain significantly high probabilities of being classified as the open-set, while the probabilities for closed-set queries are effectively suppressed. This distinct separation in the classification probabilities for open and closed-set queries allows us to understand and verify the robustness of OAL-OFL in effectively dichotomizing between open and closed sets.\nTwo-stage training (OAL-OFL-Lite). The upper part of Table 11 shows the stage ablation for OAL-OFL-Lite. On top of the same Stage-1 model of OAL-OFL, the open-set free transfer learning of OAL-OFL-Lite also achieves a notable improvement on the Stage-1 model in Table 10. As shown in the ablation study on OAL-OFL, 'Stage-2 only' results in a large performance drop compared to the OAL-OFL-Lite of the third row. In the following sections, we study the key components of the open-set free transfer learning of the proposed OAL-OFL and OAL-OFL-Lite.\nPseudo open set & Frozen WN+1. OAL-OFL-Lite deals with the challenge of the absence of the base training dataset by the episodic sampling of the pseudo open set and freezing WN+1. The lower part of Table 11 shows the importance of the two components. Without both (the first row), the model focuses on learning closed-set classification, showing better Acc and lower AUROC than using only Stage-1 in Table 10. Although the pseudo open-set sampling is used, transfer learning the open-set classifier is still challenging as demonstrated in the second row. Hence, as in the last row, we freeze the open-set classifier and perform episodic pseudo open-set sampling.\nStage-1 to prevent overfitting. Fig. 4 visualizes the changes of Acc, AUROC, and loss values during Stage-2 training. To see the impact of Stage-1 on Stage-2, we plot 'Stage-2 only' and our complete OAL-OFL. Fig. 4(a) right shows the loss curves. OAL-OFL starts from a pretty lower loss than 'Stage-2 only.' And both 'Stage-2 only' and OAL-OFL converge, but 'Stage-2 only' is saturated at a higher loss. Also, OAL-OFL shows better performance over all the iterations in the testing ACC and AUROC of Fig. 4(a) left. We can see a similar trend in 5-shot. From this result, we can infer that our Stage-1 can provide a favorable starting point, while mitigating the overfitting problem of FSOSR transfer learning.\nAnalysis on Stage-1. We exploited the scalar factors, a and b, and masked loss Lmask to facilitate learning task-independent open-set prototype in the meta-learning of Stage-1. Table 4 ablates these components. All the components are useful, and the degraded performance in the ablated ones means the difficulty of meta-learning the task-independent open-set prototype.\nAmount of Dbs in Stage-2. In Table 5, we identify the impact of the variety of categories in Dbs on the quality of the OFL. By reducing the number of categories in Dbs, the overall results are degraded. Rather, when the number of the base categories is lower than the half, OAL-OFL-Lite shows better results. Hence, it is crucial to configure the open-set training dataset from diverse base categories.\nMeta-learned classifier. In OAL-OFL and OAL-OFL-Lite, we utilize the meta-learned feature encoder and open-set prototype to initialize the closed-set and open-set classifiers. As in Table 6, although the feature encoder starts from the meta-learned weights, the transfer-learned model suffers from severe performance degradation due to random initialization of both open and closed-set classifiers (Rand-Rand in Table 6). In OAL-OFL, when the closed-set classifiers are initialized by the class-wise prototypes from the meta-learned encoder (Meta-Rand), the closed-set classification Acc is recovered, but open-set AUROC is not. In OAL-OFL-Lite, both Acc and AUROC are quite lower than the case of complete meta-learned weight initialization. Further, when both closed and open-set classifiers are not initialized by the meta-learned weight, the open-set recognition ability of OAL-OFL is worse than OAL-OFL-Lite where classifiers are initialized by the meta-learned weights. Hence, the open-set aware meta-learning is crucial for the both closed- and open-set classifiers."}, {"title": "5. Conclusions", "content": "This work introduced the two-stage learning approach for FSOSR called open-set aware meta-learning (OAL) on a base dataset and open-set free transfer learning (OFL) on testing tasks. Our findings highlight the significance of the meta-learned FSOSR metric space, which serves as a general starting point in transfer learning and enables us to take advantage of transfer learning. For the open-set free transfer learning, we provided two suggestions to configure open-set training data: 1) sampling from the base dataset, and 2) sampling from the testing task itself. The latter one considers a more practical scenario, not relying on the base dataset during inference. Both are beneficial for generalizing the model to an FSOSR testing task. As a result, we achieved SOTA performance on two benchmark datasets, namely miniImageNet and tieredImageNet."}]}