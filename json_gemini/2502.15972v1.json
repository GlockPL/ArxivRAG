{"title": "Multi-Agent Multimodal Models\nfor Multicultural Text to Image Generation", "authors": ["Parth Bhalerao", "Mounika Yalamarty", "Brian Trinh", "Oana Ignat"], "abstract": "Large Language Models (LLMs) demonstrate\nimpressive performance across various multi-\nmodal tasks. However, their effectiveness in\ncross-cultural contexts remains limited due to\nthe predominantly Western-centric nature of\nexisting data and models. Meanwhile, multi-\nagent models have shown strong capabilities\nin solving complex tasks. In this paper, we\nevaluate the performance of LLMs in a multi-\nagent interaction setting for the novel task of\nmulticultural image generation. Our key con-\ntributions are: (1) We introduce MosAIG, a\nMulti-Agent framework that enhances multi-\ncultural Image Generation by leveraging LLMS\nwith distinct cultural personas; (2) We pro-\nvide a dataset of 9,000 multicultural images\nspanning five countries, three age groups, two\ngenders, 25 historical landmarks, and five lan-\nguages; and (3) We demonstrate that multi-\nagent interactions outperform simple, no-agent\nmodels across multiple evaluation metrics, of-\nfering valuable insights for future research.\nOur dataset and models are available at https:\n//github.com/OanaIgnat/MosAIG.", "sections": [{"title": "1 Introduction", "content": "Societies worldwide are increasingly diverse, with\npeople of various cultural backgrounds co-existing\nan outcome amplified by global travel and mi-\ngration (Castles et al., 2103). This multicultural\ntapestry offers both opportunities and challenges,\nparticularly in Artificial Intelligence (AI), where\nrobust representation of diverse groups is essen-\ntial for equity and inclusivity (Hershcovich et al.,\n2022; Naous et al., 2023; Mihalcea et al., 2024)\nHowever, most existing datasets\u2014especially those\nused for text-to-image generation\u2014primarily focus\non narrow demographics, predominantly western\nadult males, and frequently portray single-culture\nscenarios (e.g., a Chinese temple, an Indian mar-\nket) (Liu et al., 2024; Kannen et al., 2024). Such\nlimited scope fails to encompass common multicul-\ntural interactions (e.g., a Chinese girl visiting the"}, {"title": null, "content": "RQ1: How accurately do state-of-the-art text-\nto-image models depict people from one\nculture within the context of a landmark\nassociated with a different culture?\nRQ2: How does the performance of text-to-\nimage generation vary across different de-\nmographic groups?\nRQ3: What strategies can enhance the perfor-\nmance of multicultural text-to-image gen-\neration?\nThe paper makes the following contributions.\nFirst, we compile and share the first dataset of\n9,000 images depicting multicultural interac-\ntions, i.e., a person and a landmark from dif-\nferent cultures, across five countries, three age\ngroups, two genders, 25 historical landmarks, and\nfive languages. Second, we propose MosAIG a\nnovel multi-agent framework to improve multi-\ncultural text-to-image generation performance\nacross demographics and languages. Finally, we\nshow that our multi-agent interactions outper-\nform simple models across multiple evaluation\nmetrics, and provide actionable steps for future\nwork."}, {"title": "2 Related Work", "content": "Cultural Evaluation in Language and Vision\nModels. Research in language-based models\nis advancing rapidly in capturing cultural nu-\nances through large multilingual evaluation bench-\nmarks (Pawar et al., 2024; Romanou et al., 2024;\nSingh et al., 2024). In the language-vision domain,\nrecent benchmarks like CVQA (Romero et al.,\n2024) and GlobalRG (Bhatia et al., 2024) focus\non culturally aware question answering, retrieval,\nand visual grounding. Novel methods leveraging\nmulti-agent frameworks of large multimodal mod-\nels (Guo et al., 2024; Han et al., 2024) have shown\nfurther promise in enhancing cross-cultural under-\nstanding. For instance, MosAIC (Bai et al., 2024)\nemploys a multi-agent framework for cross-cultural\nunderstanding but focuses on image captioning in\nsingle-culture contexts rather than text-to-image\ngeneration. Our work addresses this gap by ex-\namining how state-of-the-art text-to-image mod-\nels handle multicultural representations within the\nsame image.\nText-to-Image Generation Models and Bench-\nmarks. Text-to-image generative capabilities\nhave advanced rapidly in recent years, as evidenced\nby models such as Stable Diffusion-XL (Podell\net al., 2023), DALLE-3 (Betker et al., 2023), and\nFLUX (Labs, 2023). Evaluation benchmarks like\nTIFA (Hu et al., 2023), GenEval (Ghosh et al.,\n2024), and GenAIBench (Lin et al., 2025) tradi-\ntionally emphasize technical factors such as real-\nism, text faithfulness, and compositional accuracy.\nMore recent work, i.e., HEIM (Lee et al., 2024),\nextends these metrics to include socially situated\naspects like toxicity, bias, and aesthetics, reflecting\ngrowing concern for the social impact of generative\nmodels (Hartwig et al., 2024).\nCultural Gap and Language Limitations in Text-\nto-Image Generation. Despite advancements,\nexisting efforts predominantly focus on a narrow\nset of languages (e.g., English, Chinese, Japanese),\nleaving large user communities underserved. Re-\ncent multilingual models, such as Taiyi-Diffusion-\nXL (Wu et al., 2024), target Chinese text input,\nwhile AltDiffusion (Ye et al., 2024) expands lan-\nguage coverage to eighteen languages. However, a\nbroader \"cultural gap\" persists (Liu et al., 2024), as\nmost models and benchmarks insufficiently capture\ndiverse cultural settings and interactions.\nData Diversity and Cultural Competence.\nOnly recently have researchers begun to evaluate\ncultural competence in text-to-image models. For\ninstance, CUBE (Kannen et al., 2024) assesses cul-\ntural awareness and diversity, yet still focuses on\nsingle-culture depictions per image. To our knowl-\nedge, no existing work systematically addresses\nmulticultural scenarios\u2014where multiple cultures\nmay be represented in a single image\u2014and rigor-\nously evaluates the performance of state-of-the-art\ntext-to-image systems under such conditions. Our\napproach aims to fill this gap by exploring how\nthese models handle more complex, multicultural\nrepresentations."}, {"title": "3 Multicultural Image Generation", "content": "Culture is a multifaceted concept meaning different\nthings to different people at different times (Adi-\nlazuarda et al., 2024). In this work, we adopt the\ndefinition proposed by Nguyen et al. (2023) and\nfocus specifically on visual cultural elements such\nas clothing and historical landmarks.\nWe propose a novel task, multicultural image\ngeneration, aimed at evaluating how generation\nmodels represent elements from diverse cultures\nwithin the same image, i.e., a person from one cul-\nture and a landmark from a different culture. We\nalso analyze other demographic attributes and their\nintersection, such as age, gender, and language\u00b9.\nTo address this task, we introduce MosAIG, a novel\nframework for Multi-Agent Image Generation, as\nillustrated in Figure 2. Our framework generates\ncomprehensive image captions that are used to\ngenerate more accurate multicultural images us-\ning off-the-shelf image generation models. This\nframework is built around a multi-agent interaction\nmodel, as described below."}, {"title": "3.1 Multi-Agent Interaction Model", "content": "We introduce a multi-agent setup to emulate collab-\noration between demographically diverse groups.\nOur setup contains five agents, with specific roles:\none Moderator Agent, three Social Agents, and one\nSummarizer Agent, as illustrated in Figure 2.\nModerator Agent. The Moderator Agent obtains\ndemographic (age, gender, nationality) information\nabout the person, the name of the landmark (e.g.,\nTaj Mahal), and the language of the caption as\ninput. The Moderator Agent then assigns tasks to\nthe Social agents, instructing them to focus on the\nvisually relevant aspects of the input information.\nSocial Agents. The Social Agents interact by ask-\ning each other relevant questions to create an image\ncaption according to the information provided by\nthe Moderator Agent. Each Social Agent assumes\na persona: the first agent represents the culture of\nthe person in the image, the second agent repre-\nsents the age and gender of the person, and the\nlast agent represents the historical landmark. Each\nagent generates an initial description of their per-\nsona. Then, by interacting through multiple rounds\nof question-answering conversations, each agent\ncreates a more comprehensive image description.\nSummarizer Agent. The Summarizer Agent col-\nlects the three descriptions from the Social Agents\nand summarizes them into a final image caption\nwith a maximum length of 77 tokens.\nSocial Agents Conversation. At the start, the three\nSocial Agents-Country Agent, Landmark Agent,\nand Age-Gender Agent-receive demographic in-\nformation and tasks from the Moderator Agent.\nThe Country Agent processes nationality informa-\ntion and describes traditional attire, which is then\nevaluated by the Age-Gender Agent (e.g., \"Is this\nattire suitable for a young female?\"). Adjustments,\nsuch as modifying the color or style of a garment\nto suit the individual's age, are made accordingly."}, {"title": "3.2 Image Generation Models", "content": "We evaluate our generated image captions us-\ning two different state-of-the-art image genera-\ntion models: AltDiffusion (Ye et al., 2024) and\nFLUX (Labs, 2023)."}, {"title": "3.3 Simple vs. Multi-Agent Image Generation", "content": "Simple models generate images based on prede-\nfined captions, whereas multi-agent models uti-\nlize dynamically generated captions derived from\nmulti-agent interactions. For instance, when pro-\nvided with demographic details such as \u201cViet-\nnamese\" (nationality), \u201cchild\u201d (age), \"female\" (gen-\nder), \"Golden Gate Bridge\" (landmark), and \"En-\nglish\" (caption language), the resulting image cap-\ntions differ between the two approaches. Multi-\nagent models generate captions that provide richer\ncontextual information, including detailed descrip-\ntions of the landmark's architecture and surround-\nings, as well as a more nuanced depiction of the\nperson's appearance, particularly focusing on cloth-\ning and facial features, as shown below7."}, {"title": "4 Evaluation and Results", "content": "We employ both automated metrics and human\nevaluation to provide a holistic and comprehensive\nassessment of the generated images."}, {"title": "4.1 Evaluation Metrics", "content": "We adopt automated evaluation metrics, which\nassess alignment, quality, aesthetics, knowledge,\nand fairness, ensuring a comprehensive analy-\nsis. These metrics encompass both technical fac-\ntors-alignment, quality, and knowledge\u2014as well\nas socially situated aspects such as fairness and\naesthetics (Lee et al., 2024).\nAlignment. CLIPScore (Hessel et al., 2021) mea-\nsures text-to-image alignment by computing the\ncosine similarity between the semantic embeddings\nof the image and its associated text, providing an\neffective assessment of how well the generated im-\nage reflects the intended description. CLIPScore\nranges from -1 to +1, where higher values indicate\na stronger semantic alignment between the gener-\nated image and its corresponding text.\nQuality. We assess the quality of generated im-\nages using the Inception Score (IS) (Salimans et al.,\n2016), which leverages an Inception v3 classifier to\nmeasure image fidelity and diversity. Lower scores\n(below 10) typically indicate poor quality or lim-\nited variation, while higher scores (10+) suggest\nmore realistic and diverse outputs.\nAesthetic. This metric evaluates the aesthetic ap-\npeal of an image, considering factors such as visual\nclarity, sharpness, color vibrancy, and overall sub-\nject clarity. Aesthetic evaluation also takes into\naccount composition, color harmony, balance, and\nvisual complexity. To assess these aspects, we use\nthe SigLIP-based predictor, which rates the aes-\nthetics of an image on a scale from 1 to 10 (best).\nFairness. This metric evaluates the consistency\nof model performance when captions are modified\nto reference different social groups. Specifically,\nmodifications are applied to attributes such as gen-\nder, age, and nationality, while keeping the rest of\nthe caption unchanged. Given an original caption c\nand its corresponding image I, we construct a mod-\nified caption c' by substituting a demographic term,\ni.e., replacing male-gendered terms with female-\ngendered terms, \"young\" with \u201cold\u201d or \u201cGerman\"\nwith \"Indian\u201d. The corresponding modified image\nI' also reflects the demographic change.\n$$AS =| S(c, I) \u2013 S(c', I') |$$\nwhere S(c, I) and S(c', I') denote the CLIPScores\nfor the original and modified caption-image pairs,\nrespectively. A fair model should exhibit mini-\nmal variation in performance across demographic\ngroups, implying low values of AS. Higher val-\nues of AS indicate greater performance disparity,\nsuggesting potential bias.\nKnowledge. This metric evaluates the model's\nknowledge of the world by analyzing its ability\nto recognize and distinguish historical landmarks.\nTo assess this, we modify a given caption e by re-\nplacing one historical landmark with another while\nkeeping the corresponding image I and the rest of\nthe caption unchanged. For example, given the\ninitial caption-image pair:\n(c, I) = (A German boy in front of Taj Mahal, I)\nmodifying the landmark term results in:\n(c', I) =(A German boy in front of White House, I)\nWe measure the absolute difference in CLIPScore\nbefore and after the modification:\n$$AS = S(c, I) \u2013 S(c', I)$$\nA model with strong cross-cultural knowledge of\nhistorical landmarks should exhibit high perfor-\nmance variations when landmarks are swapped.\nHigher scores indicate greater knowledge, while\nlower scores suggest weaker landmark recognition.\""}, {"title": "4.2 Multi-Agent Interaction Results", "content": "Our multi-agent models outperform simple models\nin Alignment, Aesthetics (only Alt-En-M), Quality,\nand Knowledge, while scoring lower in Fairness,\nas illustrated in Figure 3. The most significant im-\nprovement is observed in Image Quality, where\nmulti-agent models achieve substantially higher\nscores (0.77 vs. 0.48 for Alt-En and 0.65 vs. 0.45\nfor Flux-En). We hypothesize that this enhance-\nment is driven by the additional contextual details\nprovided by multi-agent interactions, leading to\nmore visually refined outputs.\nFurthermore, we analyze performance variations\nacross demographic categories for all models and\nmetrics, as detailed in Appendix E.1. Notably,\nAlignment improves across gender, age, person,\nand landmark country when using multi-agent mod-\nels compared to simple models. Additionally, Qual-\nity is consistently higher for Alt compared to Flux,\nlikely due to the tendency of Flux-generated images\nto exhibit blurry backgrounds.\nHowever, Fairness scores decline for multi-\nagent models. We attribute this to the increased\nlevel of detail in their generated captions\u2014such\nas references to clothing, facial features, and\nhairstyles which amplifies the absolute difference\nin CLIPScore between the original and modified\ncaption-image pairs. In contrast, the simpler, more\nconcise captions generated by simple models do\nnot introduce as many additional descriptors, re-\nsulting in smaller variations in CLIPScore and con-\nsequently lower Fairness scores. These findings\nhighlight a trade-off between improved Quality,\nAlignment, and Knowledge and the potential bias\nintroduced in Fairness, likely due to richer descrip-\ntive caption generated with multi-agent models."}, {"title": "4.3 Ablation Studies", "content": "We also perform ablation studies to assess\nMosAIG's performance across demographics.\na) Person Age. Figure 4 a) shows that Image\nQuality varies by age group, with Adults achieving\nthe highest quality (0.55), followed by Children\n(0.51) and Elders (0.49). The model is also fairer\nwhen depicting Children (0.33) compared to Adults\n(0.38) and Elders (0.40). Alignment and Aesthetic\nmetrics remain consistent across all age groups.\nb) Person Gender. Figure 4 b) shows that Im-\nage Quality varies by gender, with Males achieving\nhigher quality (0.56) than Females (0.52). How-\never, the model is fairer when depicting Females\n(0.36) than Males (0.38). The other metrics remain\nconsistent across both groups.\nc) Person Country. Figure 4 c) shows that model\nperformance varies by person's country. Alignment\nis highest for Indian people (0.32) and lowest for\nSpanish people (0.29). Similarly, Image Quality\nis highest for Indian people (0.47) and lowest for\nGerman people (0.41). The model is also fairest\nwhen depicting Indian people (0.35) and least fair\nfor German people (0.39).\nd) Landmark Country. Figure 4 d) shows that\nmodel performance varies by landmark country.\nThe most notable difference is in the Knowledge\nmetric, with U.S. landmarks being the most well-\nknown (0.55), followed by Germany (0.47), Spain\n(0.42), Vietnam (0.40), and India (0.39). Alignment\nis highest for U.S. landmarks (0.33) and lowest for\nSpanish landmarks (0.29).\ne) Caption Language. Figure 4 e) shows that\nmodel performance varies by caption language,\nwith English achieving the highest Alignment\n(0.31) and Knowledge (0.46), while Hindi and Viet-\nnamese score the lowest (0.14 and 0.43, respec-\ntively). This disparity may stem from differences\nin training data availability, as model performance\nmoderately correlates with dataset size (Pearson\ncoefficient: 0.51, estimated from CommonCrawl\n(Wenzek et al., 2020).\nf) Intersectionality. Examining a single demo-\ngraphic category, such as race or gender, may over-\nlook nuanced inequalities (Field et al., 2021). \u03a4\u03bf\naddress this, we analyze the intersectionality of age\nand gender, person and landmark country, and lan-\nguage and person country. We measure Alignment\nand analyze other metrics across various demo-\ngraphic intersections, as detailed in Appendix E.2.\nAge and Gender. Figure 5 (right) shows that\nAlignment performance varies by gender for gen-\nerating adult images, with males having a lower\nscore (0.29) compared to females (0.31). The per-\nformance for child and elder categories remains\nconsistent across gender.\nPerson and Landmark Country. Figure 5 (left)\nillustrates Alignment across Person and Landmark\nCountry. We expected higher performance when\nthe person and landmark originate from the same\ncountry, suggesting challenges in cross-cultural rep-\nresentation. However, results vary by country. For"}, {"title": "4.4 Human Evaluation and Error Analysis", "content": "Two annotators evaluate a subset of 300 images,\ncovering all demographics (age, gender, country,\nlandmark) and model settings (Alt-S, Alt-M, Flux-"}]}