{"title": "RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars", "authors": ["Yuncheng Hua", "Lizhen Qu", "Zhuang Li", "Hao Xue", "Flora D. Salim", "Gholamreza Haffari"], "abstract": "Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment-factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca-eval task (from 4.50 \u2192 4.60), a 0.22 enhancement on the just-eval-instruct benchmark (from 4.34 \u2192 4.56), and a maximum improvement of 0.32 (from 3.53 \u2192 3.85) on the MT-Bench dataset.", "sections": [{"title": "1 Introduction", "content": "Alignment tuning helps bridge the gap between raw model capabilities and the nuanced requirements of different tasks, such as delivering accurate information, maintaining user safety, and handling sensitive topics with care (Shneiderman, 2020; Wang et al., 2023b; Qi et al., 2024b). The mainstream alignment tuning methods, such as supervised fine-tuning and reinforcement learning with human feedback (RLHF), rely on a large mount of annotated data and significant computing resources (Ouyang et al., 2022; Sun et al., 2023; Dai et al., 2024; Rafailov et al., 2024; Zhou et al., 2024; Wu et al., 2024). They potentially leading to catastrophic forgetting of pre-trained knowledge (Wang et al., 2023a). In contrast, In-Context Alignment (ICA) provides a low-cost, flexible alternative by employing a handful of selected demonstration exemplars for In-Context Learning (ICL), enabling LLMs to align with user intent without changing model parameters (Lin et al., 2024).\nThe majority of the prior works on ICA investigate selecting demonstration exemplars (Liu et al., 2022; Min et al., 2022; Ye et al., 2023; Peng et al., 2024; Choi and Li, 2024; Wang et al., 2024), while Lin et al. (2024) only utilize three manually designed exemplars with customized styles, referred to as URIAL, across all tasks. These handcrafted ICL exemplars complement each other, achieving a delicate balance between factuality and safety, which effectively enhance LLM alignment capabilities empirically. However, URIAL lacks quantitative analyses to explain why these specific manually crafted ICL demos are effective and what is the impact of each style factor.\nIn addition to styles, what are the other key factors may influence the selection and combination of ICL exemplars? Zhao et al. (2024) identify the importance of the source of ICL exemplars, while Zhou et al. (2024) investigate the impact of labels, input-label mappings, and distribution of inputs. Moreover, ICA seems to impose two conflicting demands: on one hand, LLMs need to provide more in-depth, informative, and helpful content (factuality) (Shen et al., 2023); on the other hand, for safety reasons, LLMs must refuse to answer inappropriate queries (safety) (Ji et al., 2024). Balancing these factors is crucial to effectively leveraging ICL exemplars.\nIn this work, we conduct the first quantitative study to assess the impact of individual style factors. In particular, we select and rank ICL candidates from a candidate tool in terms of a metric, termed value impact. Our detailed analysis of these exemplars reveals to what extent distinctive stylistic factors in ICL exemplars influence LLM alignment capabilities (see Section 2).\nBased on the insights from our study, we propose to automatically restyle selected ICL demonstrations using an LLM with a customized prompt (Section 3, RQ1). To address the trade-off between factuality and safety, we systematically explore different exemplar combinations while maintaining stylistic consistency across those exemplars (Section 3, RQ2). Through this process, we identify a handful of optimal exemplar combinations in terms of both styles and content across various tasks (Section 3, RQ3). As illustrated in Figure 1, we refer to these optimized sets as Restyled In-context-learning Demonstration Exemplars (RIDE).\nIn summary, our contributions are three-fold:\n(I) Through a systematic analysis of ICL exemplars, we identify specific stylistic factors that improve LLM alignment capabilities. By evaluating these features using the value impact metric, we provide insights into how different styles influence the effectiveness of ICL demonstrations.\n(II) We propose an automatic restyling approach that systematically modifies ICL demonstrations to enhance alignment. By exploring different style configurations, we identify the optimal stylistic composition that balances the trade-off between factuality and safety, leading to the development of RIDE as the most effective ICL demo set.\n(III) We conduct a series of experiments across different datasets and LLM models, demonstrating the effectiveness and superiority of our proposed method. The experimental results show that, across the three benchmarks, our method achieves improvements of 2.22%, 4.28%, and 9.07% compared to the SOTA methods, respectively."}, {"title": "2 Impact of Styles on LLM Alignment", "content": "In this section, we address one research question: What styles in in-context learning (ICL) examples can influence LLM alignment?\nRecent studies have demonstrated that the style of in-context examples significantly affects the few-shot online learning performance of LLMs (Chen et al., 2024). However, the specific impact of different ICL example styles on various facets of LLM alignment has not been thoroughly explored in the literature (Milli\u00e8re, 2023; Anwar et al., 2024). To fill this gap, we propose a novel metric, termed value impact, to quantify the positive or negative influence that an ICL demonstration example exerts on an LLM's alignment capabilities.\nValue Impact Computation. For a given user query q, our approach proceeds as follows. First, we generate an output o = P(q) using an LLM P that has not undergone any alignment tuning. Next, we introduce an ICL demonstration example c alongside the query q and generate a new output $o_c = P(q, c)$. Then, we employ an LLM-as-a-judge framework to score both o and $o_c$ on six distinct dimensions (as the metrics shown in Table 1) that capture different aspects of LLM alignment. For any given dimension v, we define a score as:\n$\\delta_v = v(o_c) - v(o)$. Here, $\\delta_v$ represents the effect of the demonstration example c on the LLM's performance in dimension v when answering the query q. Finally, for a validation dataset Q comprising various queries, we calculate the average $\\delta_v$ for each dimension v as\n$\\delta_v = \\frac{1}{|Q|} \\sum_{q \\in Q} \\delta_v(q)$.\nWe define the $\\delta_v$ as value impact, which reflects the overall positive or negative impact of the ICL"}, {"title": "3 Restyle ICL Demonstration Exemplars", "content": "In this section, we aim to address three research questions: (i) How does explicitly rewriting an ICL demonstration example impact LLM alignment? (ii) How can different styles of ICL exemplars be effectively combined? and (iii) Can rewriting randomly selected ICL exemplars also improve LLM alignment?\nRQ1: Rewriting ICL demonstration examples\nAs observed in Section 2, we identified four distinct ICL exemplar styles that effectively influence LLM alignment capabilities. Naturally, this leads to the questions: If we explicitly modify an ICL exemplar to adopt a specific style, will the restyled demonstration impact LLM alignment? How does restyling QA pairs from factuality-based (Ultra-Chat) and safety-focused (SORRY-Bench) datasets impact LLM alignment?\nRestyling Methodology. To systematically modify the writing style of QA pairs, we design a structured prompting approach consisting of three components: 1) Task instruction: A directive informing the LLM to explicitly rewrite the answer in a specific style; 2) Example demonstration: A concrete example illustrating how the modification should be performed. 3) Target QA pair: The QA pair to"}, {"title": "RQ2: Combining restyled ICL exemplars", "content": "Our study confirms that combining multiple restyled ICL demonstrations into a cohesive demo set yields superior results compared to relying on"}, {"title": "RQ3: High-Value-Impact ICL Demos vs. Randomly Selected ICL Demos", "content": "As previously mentioned, we selected the top-20 QA pairs with the highest value impact from datasets UltraChat and SORRY-Bench as our candidate demos. This naturally raises the question: Is"}, {"title": "4 Evaluation", "content": "4.1 Dataset, LLMs, and baseline methods\nDataset. We use Alpaca-eval (a benchmark designed to assess the performance of language models on natural language understanding, generation, and reasoning tasks) (Li et al., 2023),"}, {"title": "4.2 Q1: Does RIDE improve the LLM's alignment performance?", "content": "just-eval-instruct aims to assess the trade-off between factuality and safety in LLM alignment, ensuring that the model can provide informative responses while refusing malicious queries.\nResults. Table 4 presents the scores of each method on just-eval-instruct. From the table, we can summarize the following conclusions.\nRIDEfs_hyb achieves the best overall performance. (i) Among the three proposed ICL sets, RIDEfs_hyb performs the best, followed by RIDEfs_uni, while RIDEf ranks lowest. (ii) RIDEfs_hyb maintains a strong factuality performance while significantly enhancing safety, thanks to the \"refusal\" style safety example. (iii) RIDEf, consisting solely of factuality examples, excels in factuality but lacks safety training, resulting in a significantly lower \u201cSafe\u201d score.\nRIDE outperforms URIAL in most cases. (i) RIDEfs_hyb outperforms URIAL in two out of three models, demonstrating its superior alignment performance. (ii) Due to OLMo-7B's input length limitation, some ICL content had to be truncated, slightly reducing \"Helpful\", \"Factual\", and \"Deep\" scores. However, RIDEfs_hyb remains competitive with URIAL, achieving nearly identical \"Safe\" scores.\nBaseline methods exhibit a significant performance gap. (i) As shown in the first block of Llama2-7b, the baseline methods perform notably worse than our RIDE and URIAL ICL sets. (ii)"}, {"title": "4.3 Q2: Does RIDE elicit LLMs to generate high-quality and informative responses?", "content": "To assess whether the distinctive styles in RIDE can enhance high-quality, well-structured, and information-rich responses, we conduct experiments using Alpaca-eval, a dataset that primarily evaluates factuality rather than safety. Unlike just-eval-instruct, Alpaca-eval focuses solely on instruction-following capabilities without considering potential harm, making it suitable"}, {"title": "4.4 Q3: Does RIDE enhance LLMs' ability to handle complex tasks?", "content": "MT-Bench assesses LLM capability in handling complex tasks by requiring the integration of logical reasoning, numerical computation, coding, and other advanced skills, making it a suitable benchmark for measuring LLM proficiency in complex problem-solving. From Table 6, we can draw the following findings (further discussion can be found in Appendix J.1).\nRIDE outperforms URIAL across all settings. (i) RIDEf achieves the best overall performance,"}, {"title": "4.5 Q4: Can base LLM outperform its aligned counterpart by employing RIDE?", "content": "Results. Our findings conclusively show that yes, a base LLM can outperform its aligned counterpart! As detailed in Table 10 in Appendix K, when the base model Mistral-7B-v0.1 utilizes RIDE as its ICL demonstrations, it achieves superior alignment performance compared to Mistral-7B-Instruct-v0.1 across all three datasets. We argue that for sufficiently capable base models, RIDE can effectively elicit their inherent alignment potential. Notably, our approach offers significant practical advantages: it is tuning-free, plug-and-play, and requires minimal training and deployment costs. We"}, {"title": "5 Related Work", "content": "Alignment tuning helps bridge the gap between raw model capabilities and task-specific requirements (Shneiderman, 2020; Shen et al., 2023; Wang et al., 2023b; Qi et al., 2024b). The instruction-"}, {"title": "6 Conclusion", "content": "In this paper, we take the initial step by designing a metric to evaluate the effectiveness of ICL demonstration exemplars\u2014value impact\u2014which we use to analyze the characteristics of ICL demos that effectively enhance LLM alignment capabilities. We categorize these characteristics under the term \"style\" and, based on this insight, propose a \"restyling\" method to optimize ICL demos with high value impact. We conduct experiments across three datasets, and the results demonstrate that our restyling approach effectively stimulates LLMs to generate informative and safe content while also enhancing their capabilities in logical reasoning, numerical computation, and other complex tasks."}, {"title": "Limitations", "content": "Despite the effectiveness of the proposed RIDE method in enhancing LLM alignment, several limitations and potential risks should be acknowledged.\nLimited Scope of ICL Demonstrations. One key limitation of this study is the restricted selection of ICL demonstrations. The candidate ICL demos were drawn from a subset of a large dataset, which may limit their diversity and generalizability. Given that alignment performance is highly dependent on the variety of training examples, a more extensive and diverse selection of candidate ICL exemplars could potentially yield stronger results. Future work should explore the impact of expanding the candidate pool by incorporating demonstrations from multiple datasets across different domains."}, {"title": "Ethics Statement", "content": "Malicious contents. This research focuses on improving LLM alignment, which inherently involves handling malicious queries as part of the evaluation process. These queries may contain offensive, harmful, or sensitive content, which could be distressing to some readers. However, we emphasize that such malicious queries are included solely for research purposes, ensuring that our findings contribute to the development of more responsible and safe AI systems.\nData anonymization and Ethical Considerations. We have taken steps to ensure that no personally identifiable information (PII) or offensive content is present in the datasets used for training and evaluation. Any potentially harmful content within the datasets has been either anonymized or strictly controlled to prevent ethical concerns related to data"}, {"title": "A Background settings in our work", "content": "In this paper, we use the terms \u201cunaligned LLMs\u201d and \u201cbase LLMs\u201d interchangeably to refer to LLMs that have not undergone alignment processes, though they are not inherently malicious. In contrast, we refer to LLMs that have been fine-tuned with instructional data to promote ethical and beneficial behavior as \"aligned LLMs\u201d. We define an unaligned LLM as f (x; 0), where x is the input query and 0 represents the model's parameters responsible for generating output tokens. The process of \u201calignment tuning\" involves adjusting the parameters @ of a base LLM to produce more controlled and regulated responses. Consequently, we represent the aligned LLM as g(x; \u03b2), which is better aligned with human values and preferences. This process generally involves two steps: supervised fine-tuning (SFT) on instructional data and reinforcement learning from human feedback (RLHF). In the SFT phase, the base LLM is refined using instruction-answer pairs, known as instruction tuning. In the RLHF phase, a reward model is applied to further enhance the fine-tuned model, improving its alignment with human expectations of helpfulness and safety."}, {"title": "C Rewriting ICL demonstration examples - A Further Discussion", "content": "To systematically modify the writing style of QA pairs, we design a structured prompting approach consisting of three components: 1) Task instruction: A directive informing the LLM to explicitly rewrite the answer in a specific style; 2) Example demonstration: A concrete example illustrating how the modification should be performed. 3) Target QA pair: The QA pair to be rewritten. We feed this prompt into an LLM, which then generates a restyled QA pair, ready to be used as an ICL exemplar.\nFor these modifications, we leverage a strong LLM to ensure high-quality restyling. Based on the findings in Section 2, we modify the style of the answer part in the following ways: (1) three-part (structuring the answer in three parts: introduction, bullet-point explanation, and summary.), (2) lengthy (expanding the answer with more details while preserving its original meaning), (3) human (adopting a conversational or first-person tone), (4) combined (use three-part, lengthy and human three styles to rewrite the ICL example simultaneously), (5) refusal (for safety-related ICL examples, refuse first, justify, and then provide guidance.), and (6) no style (the original ICL demonstration that remains unchanged). Same as Section 2, we utilize value impact to examine how restyled ICL exemplars influence LLM alignment. Specifically, we select top-20 QA pairs from each of UltraChat and SORRY-Bench with the highest value impact, denoted as the factuality and safety ICL candidates, represented as Scand_f and Scand_s, respectively.\nWe compute the average value impact across all 20 instances for the instances in Scand_f. The same computation is performed for Scand_s as well. This allows us to quantitatively and systematically analyze how QA pairs\u2014each inherently emphasizing different aspects of factuality and safety-change in alignment performance after undergoing different style modifications.\nIn Table 2, the upper block of the table represents the effect of restyling on ICL demonstrations belonging to Scand_f. Therefore, the following observations can be made from this block: (1) The original exemplars from Scand_s (no style) inherently possess some capability to enhance LLM factuality, particularly in the dimensions of \"helpful\", \"factual\", \"deep\", and \"clear\". However, compared to the baseline (where no ICL demonstrations are used), this improvement is relatively modest. (2) The three-part style effectively enhances \"clear\", the lengthy style improves \"depth\", and the human-like style increases \"engaging.\" (3) The three-part, lengthy, and human-like styles all contribute to improvements in \"helpful\" and \"factual.\" (4) Considering all metrics except \"safe\", the combined style achieves the best overall factuality performance (\"helpful\", \"factual\", \"deep\", \"engaging\", and \"clear\"). (5) None of the restyling approaches significantly improve the \"safe\" metric.\nThe lower block of Table 2 records the effects of restyling on safety demonstrations. Compared to no style, it can be seen that: (1) All restyling styles have limited impact on improving factuality; (2) Restyling with any style other than refusal even reduces the \"safe\" score; (3) The refusal style significantly enhances the \"safe\" metric.\nOverall, based on the above analysis, we provide answers to the two questions. (Q1) Will the restyled demonstration impact LLM alignment? The answer is yes\u2014restyled exemplars can have a more significant impact on LLM alignment. (Q2) What effects do the restyle QA pairs from different datasets will have? Our findings suggest that factuality candidates should be rewritten using a combined style, whereas safety ICL exemplars should be restyled using a refusal style for optimal alignment performance. Additionally, to achieve optimal overall performance in an LLM, a trade-off between factuality and safety must be reached. The prompts used for the explicit restyling of ICL demos can be found in Appendix L.\nAlso, we argue that the effectiveness of ICL demo restyling stems from the causal relationship between the style of an ICL exemplar and LLM alignment. Together with the content of the ICL demo, this relationship forms a causal structure. In this context, restyling an ICL demo can be viewed as an intervention (do-operation) within this causal framework. For a detailed theoretical analysis of this aspect, please refer to the Appendix D."}, {"title": "D Restyling \u2013 A Perspective from Causal Structure", "content": "We first provide the following definitions: content refers to the task-related information provided in an ICL example, including the system instruction and the demonstration, style represents the writing style of task-related information and the organizational structure of the content, and alignment refers to the alignment effect exhibited by the model after using a particular example as a ICL demonstration.\nWe consider style and content to be the two most critical factors in applying ICL techniques for alignment tuning. We model S (style), C (content), and A (alignment) as a causal structure (Pearl, 2009), as illustrated in Figure 2. The variable C is the co-founder, which influences both S and A. Both C and S jointly influence alignment.\nWe consider C as a factor that cannot be experimentally manipulated. On the one hand, using LLMs to modify the content of an LLM's response can lead to hallucinations, making the study uncontrollable. On the other hand, altering the content changes the nature of the demonstration, thus losing the significance of the research. Therefore, our primary interest lies in the impact of the intervenable factor S on A, and we thus disregard the influence of C on A, focusing instead on evaluating the effect of the controllable intervention S.\nTo quantify the impact of an intervention on an outcome of interest, the Average Treatment Effect (ATE) is a commonly used method in causal inference (Kaddour et al., 2021; Mahajan et al., 2024). Therefore, we use ATE as the expected difference in outcomes to determine, on average, how much effect the intervention has compared to other interventions.\nSpecifically, following the principles of causality, we consider setting S to a fixed value as an intervention, denoted using the do-operator: do(S = s). Whenever do(s) appears after the conditioning bar, it means that everything in that expression is in the post-intervention world where the intervention do(s) occurs.\nIt is important to note that, in Figure 2, there is an edge from C to S, indicating that C confounds the effect of S on A. However, according to the definition in causal theory, do(s) will remove the edge from C to S when intervening on S, meaning that C will no longer affect S, as indicated by the red cross in the figure.\nThus, E(A\\do(S = s)) refers to the expected alignment improvement after all examples have been restyled using the format s. According to the backdoor criterion, we obtain:\n$E[A|do(S = s)] = \\sum_{c}E[A|s, C = c]p(c)$\nThe ATE is defined as:\n$ATE(s_t, s_o) = E[A|do(S = s_t)] - E[A|do(S = s_o)]$\nwhere st refers to target style, and so denotes other style.\nEmpirically, we adopted the idea of Monte Carlo sampling (Knaus et al., 2021) and approximate p(c) as a uniform distribution. We used a single example as the ICL demonstration, enabling the LLM to handle downstream tasks through one-shot online learning. To calculate the expectation E[A|s, C = c], we kept the content of the ICL demonstration fixed (C = c), while restyling the demonstration example with a specific style s. The restyled demonstration example is then encapsulated in the prompt and fed to the LLM, which processes examples from the validation dataset via ICL. We considered the LLM's average alignment performance on the validation dataset as an approximation of E[A|s, C = c].\nBased on the concept of Monte Carlo sampling, we randomly selected N ICL demonstrations from the candidate high-quality ICL examples to form the set {C}. Corresponding to the N demonstrations in"}, {"title": "E Combining restyled ICL exemplars - A Further Discussion", "content": "Research has shown that LLMs generalize better when provided with multiple diverse demonstrations, enabling them to infer task-specific patterns more effectively (Brown, 2020; Lin et al., 2024). Moreover, as raised in RQ1, for certain tasks where LLMs must simultaneously provide useful information while resisting malicious attacks, they require a balance between factuality and safety as part of their alignment capabilities. Theoretically, combining multiple restyled ICL demonstrations into an ICL demo set should yield better results than relying on a single ICL demo.\nHowever, the process of finding the optimal ICL demo set is NP-hard (Ye et al., 2023), and so heuristic approaches should be used in general to get an (approximate) optimal approximation solution (Liu et al., 2024).\nPrevious research has shown that subtle interactions between the demonstrations in an ICL example set can significantly influence the performance of LLMs in few-shot online learning (Hua et al., 2024). On the one hand, maintaining a consistent response style across ICL demonstration examples can effectively enhance LLM performance on downstream tasks (Lin et al., 2024; Li et al., 2024). On the other hand, the multiple ICL demonstrations needs to be sufficiently diverse and complementary to fully elicit LLMs' task-oriented capabilities (Min et al., 2022). Notably, when dealing with safety tasks, having refusal demonstration in the set becomes particularly crucial.\nAs mentioned above, we already formed candidate sets Scand_f and Scand_s. Therefore, for the factuality candidates {Scand_f}, we restyled them using the \u201ccombined\u201d style, while for the safety candidates {Scand_s}, we restyled them using both the \u201ccombined\u201d and \u201crefusal\u201d styles. To achieve the optimal trade-off between factuality and safety, we merged the restyled factuality and safety candidates into a set {Scand} and employed a hierarchical traversal approach with early pruning (Hua et al., 2024) to select three ICL examples\" from {Scand} to construct different demonstration sets. The details of the hierarchical traversal algorithm are provided in Appendix F. We computed the value impact of different combinations on the just-eval-instruct validation dataset.\nUltimately, as shown in Table 3, we identified the three best combinations of the ICL examples. The first combination consists of three factuality ICL examples restyled with the \u201ccombined\" style. The second combination includes two factuality ICL examples and one safety example, all restyled using the \u201ccombined\u201d style. The third combination consists of two factuality ICL examples restyled with the \"combined\" style and one safety example restyled with the \u201crefusal\" style. We refer to these combinations as Restyled In-context-learning Demonstration Exemplars (RIDE), with the first combination denoted as RIDEf, the second as RIDEfs_uni, and the third as RIDEfs_hyb. We use these notations in the following sections. The prompts of RIDE series can be found in Appendix M. Furthermore, a comparison between Table 2 and Table 3 reveals that the ICL demo set, after being combined, outperforms individual ICL demonstrations in overall performance."}, {"title": "F Selection of a Set of ICL Demonstrations - The Description of the Algorithm", "content": "Search for Optimal exemplars. The combination of multiple ICL exemplars often provides more assistance to the model in tackling tasks, compared to a single ICL exemplar.\nThe steps of the whole algorithm are as follows.\n\u2022 We have candidate sets Scand_f and Scand_s, the previous one focus on factuality QA answering while the latter one is biased to refusal answering. For the factuality candidates {Scand_f}, we restyled them using the \u201ccombined\u201d style, while for the safety candidates {Scand_s}, we restyled them using both the \"combined\u201d and \u201crefusal\" styles. To achieve the optimal trade-off between factuality and safety, we merged the restyled factuality and safety candidates into a set {Scand}.\n\u2022 Following the hierarchical traversal approach outlined in (Hua et al., 2024), we first rank all exemplars in {Scand} in descending order based on their average value impact across the six evaluation dimensions. From this ranking, we select the top-n exemplars (with n set to 20 in this work) with the highest average value impact to construct an ICL exemplar set, denoted as SINIT. The remaining exemplars in {Scand}, also sorted in descending order of average value impact, constitute the candidate ICL exemplar pool, referred to as SCAND. We designate SINIT as the initial ICL example set, denoted as SICL.\n\u2022 Our objective is to combine ICL examples from SINIT and SCAND using a hierarchical traversal algorithm. This method is designed to explore various ICL example combinations and identify the one that yields the highest value impact, thereby approximating the optimal ICL example set.\n\u2022 Through empirical analysis, we observed that the value impact of an individual ICL exemplar carries predictive significance. Specifically, exemplars with higher value impact tend to contribute more significantly to the overall value impact when included in the ICL example set. Consequently, such exemplars are more likely to be retained in the final ICL example set compared to those with lower value impact. Leveraging this insight, we developed a heuristic rule for early pruning during hierarchical traversal, which will be elaborated upon in the subsequent sections.\n\u2022 We begin the hierarchical traversal by initializing an empty queue q and enqueueing SINIT. During each iteration, we dequeue the elements at the current level from q, where each element represents a combination of ICL exemplars, denoted as SICL\u2019.\n\u2022 For every ICL exemplar a originally present in SINIT within SICL\u2019, we sequentially select an exemplar b from SCAND in its sorted order and substitute a with b in SICL\u2019 to generate a new set, SICL. This newly formed set becomes a child node of SICL\u2019. We then compute the value impact change of SICL and enqueue it into q for further exploration in the next level of traversal. The value impact change is given by: A := VimpactSICL'- VimpactSICL\u2019\n\u2022 Importantly, if the value impact change \u2206 for SICL remains negative for M consecutive replacements, we determine that further substitutions of a with lower-ranked exemplars b from SCAND are unnecessary. As a result, we terminate the exploration of the current branch and refrain from enqueuing additional child nodes of SICL (generated by replacing a) into q, thereby implementing early pruning.\n\u2022 Once all elements in the queue q have been dequeued and explored, the hierarchical traversal concludes. At this point, we select the ICL example set with the highest value impact as our final solution. Consequently, we obtain \u03c0* := TSICL, which serves as an approximately locally optimal policy for remediation."}, {"title": "G High-Value-Impact ICL Demos vs. Randomly Selected ICL Demos - A Further Discussion", "content": "To empirically validate whether randomly selected ICL demos will impair the LLM alignment, we conducted five rounds of random selection, where in each round, we randomly sampled 20 ICL demos from both datasets. Each of these sets underwent the same restyling and combination process as described in RQ2. The resulting demo sets are denoted as Randomf, Randomfs_uni, and Randomfs_hyb. These were then compared to the RIDE-based demo sets: RIDEf, RIDEfs_uni, and RIDEfs_hyb. Importantly, all these ICL demo sets underwent the same restyling and composition procedures\u2014the only difference being that Random series were selected randomly, while RIDE series contained the top-20 instances with the highest value impact.\nTo mitigate uncertainties caused by randomness, we computed the average value impact across the five randomly selected sets (Random) and compared it against the value impact of RIDE.\nAs shown in the results on the just-eval-instruct validation dataset (see Table 3), across three different backbone LLMs, the Random demo sets consistently underperformed compared to the RIDE demo sets. This strongly highlights the importance and necessity of ranking ICL demos based on value impact.\nWe argue that using value impact as a metric to evaluate ICL demonstrations provides an accurate measure of their influence on LLM performance. Specifically, if an ICL demo exemplar significantly improves LLM alignment performance compared to not using it, it can be considered a high-quality instance. Furthermore, when a high-value-impact ICL demo is further refined through appropriate stylistic modifications, it can enhance the LLM's capabilities even further. This explains why high-value-impact ICL demos outperform randomly selected ICL demos from the candidate pool, as they are explicitly optimized to maximize alignment benefits."}, {"title": "HICL methods on just-eval-instruct - A Further Discussion", "content": "Settings. As discussed in Section 3, factuality and safety in LLM alignment form a paradoxical unity\u2014we aim to ensure that the LLM can provide informative responses to user queries while simultaneously preventing it from answering malicious questions. As a result, an increase in safety may sometimes lead to a decrease in factuality.\nTo evaluate the LLM's alignment capability under this trade-off, we selected the just-eval-instruct dataset for assessment. In just-eval-instruct, the dataset places a great emphasis on safety. Out of the 1000 test cases, 200 questions are safety-related and require the model to provide clear refusal responses. The remaining 800 instances are related to factuality, requiring the LLM to provide accurate and helpful factual knowledge. Therefore, just-eval-instruct evaluates both the factuality and safety capabilities of the LLM, requiring the LLM to make a balanced trade-off between the two.\nResults. Table 4 presents the scores of each method on just-eval-instruct. From the table, we can summarize the following conclusions.\n\u2022 First, among the three proposed ICL sets, RIDEfs_hyb performs the best, followed by RIDEfs_uni, and finally RIDEf. RIDEfs_hyb includes both factuality and safety ICL examples, with the safety demonstration restyled using the \u201crefusal\u201d style, which effectively enhances the LLM's safety capability while maintaining good factuality. Although RIDEfs_uni also contains a safety demonstration, it uses the \u201ccombined\u201d style for restyling. While the three examples in it have a consistent style, the safety ability of the safety example is weakened, resulting in a lower \"Safe\u201d score compared to RIDEfs_hyb. As for RIDEf, which consists entirely of factuality examples, it has the strongest factuality capability but lacks any safety example, preventing the LLM from learning how to refuse malicious queries, leading to a much lower \"Safe\u201d score compared to the other two ICL sets. This finding aligns with our observations in Section 3, RQ2.\n\u2022 Second, compared to URIAL, RIDEfs_hyb outperforms it in two out of three models. In the case of OLMo-7B, the input window length is severely limited (only 2048 tokens), while our prompts containing ICL examples exceed this limit. Thus, we had to randomly remove parts of the ICL bullet points, which especially affects the LLM's performance in \u201cHelpful\u201d, \u201cFactual\u201d, and \u201cDeep\". However, even under such constraints, we can see that RIDEfs_hyb performs comparably with URIAL in various aspects, with nearly identical scores in the crucial \u201cSafe\u201d metric (2.69 vs 2.70), although it is slightly weaker in the overall \u201cAverage\u201d score (3."}]}