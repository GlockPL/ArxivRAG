{"title": "DOES VIRTUAL STAINING FOR HIGH-THROUGHPUT SCREENING GENERALIZE?", "authors": ["Samuel Tonks", "Steve Hood", "Ceridwen Hopely", "Minh Doan", "Cuong Nguyen", "Ryan Musso", "Steve Titus", "Iain Styles", "Alexander Krull"], "abstract": "The large volume and variety of imaging data from high-throughput screening (HTS) in the phar- maceutical industry present an excellent resource for training virtual staining models. However, the potential of models trained under one set of experimental conditions to generalize to other conditions remains underexplored. This study systematically investigates whether data from three cell types (lung, ovarian, and breast) and two phenotypes (toxic and non-toxic conditions) commonly found in HTS can effectively train virtual staining models to generalize across three typical HTS distribution shifts: unseen phenotypes, unseen cell types, and the combination of both. Utilizing a dataset of 772,416 paired bright-field, cytoplasm, nuclei, and DNA-damage stain images, we evaluate the generalization capabilities of models across pixel-based, instance-wise, and biological-feature-based levels. Our findings indicate that training virtual nuclei and cytoplasm models on non-toxic condition samples not only generalizes to toxic condition samples but leads to improved performance across all evaluation levels compared to training on toxic condition samples. Generalization to unseen cell types shows variability depending on the cell type; models trained on ovarian or lung cell samples often perform well under other conditions, while those trained on breast cell samples consistently show poor generalization. Generalization to unseen cell types and phenotypes shows good generalization across all levels of evaluation compared to addressing unseen cell types alone. This study represents the first large-scale, data-centric analysis of the generalization capability of virtual staining models trained on diverse HTS datasets, providing valuable strategies for experimental training data generation.", "sections": [{"title": "1 Introduction", "content": "High-throughput screening (HTS) plays a crucial role in drug discovery by enabling the simultaneous testing of a large number of compounds to assess their effects on cell cultures Szyma\u0144ski et al. [2011]. Fluorescence microscopy is the standard tool in HTS for detecting drug effects on cellular structures Selinummi et al. [2009]. By covalently binding different fluorescent dyes to biomolecules (fluorescent staining), it enables biological structures to be simultaneously revealed by the different emission spectra of the dyes, with each dye captured in a separate image channel Tonks et al. [2023].\nAlthough it is an essential tool in modern biology, conventional fluorescence microscopy has important practical limitations. The staining protocol typically requires the cells to be fixed and permeabilized - a process in which cells are preserved in their biological state, effectively frozen in time - thus limiting the application of this technique to single time-point studies. Furthermore, as the expensive fixation and staining require specialist equipment and the number of fluorescence stains are inherently limited by spectrum saturation, significant interest has been put into label-free microscopy, wherein images of cells are captured without the need for staining Ounkomol et al. [2018]. Label-free microscopy Harrison et al. [2023], Gupta et al. [2022], while cost-effective and scalable, unfortunately, lacks the biological information typically found in the fluorescence stains Pirone et al. [2022].\nRecent works have explored the concept of virtual staining to simultaneously leverage the scalability of label-free microscopy and biological information extracted from fluorescence microscopy Cross-Zamirski et al. [2022, 2023], Imboden et al. [2023], Wieslander et al. [2021], Tonks et al. [2023]. Virtual staining is typically framed as a multimodal image-to-image translation (I2I) problem Isola et al. [2017]. In this context, virtual staining models learn to translate unstained microscopy images into the desired labeled images.\nWhile recent works Cross-Zamirski et al. [2022, 2023], Imboden et al. [2023], Wieslander et al. [2021], Tonks et al. [2023] have shown the significant potential of virtual staining, the ability of virtual staining models to generalize to images containing variations not present in the training data remains underexplored. In practice, HTS imaging data is highly diverse, being generated across different imaging systems, experiments, cell types, and phenotypes. This is known as the generalization gap Wagner et al. [2022] and has been identified as a key reason for the lack of reusable virtual staining models limiting its potential impact within large-scale applications. These challenges are analogous to those in DNA sequence modeling Avsec et al. [2021], where models are typically trained on specific cell types and fail to generalize to new cell types. In offline reinforcement learning Mediratta et al. [2023], Cobbe et al. [2019], Levine et al. [2020] where datasets predominantly focus on solving the task in the same environment, limiting the evaluation of generalization to unseen environments. Within the context of virtual staining we intend to bridge this generalization gap by performing a systematic data-centric approach to determine whether virtual staining models trained on specific subsets of data can generalize under common distribution shifts. Our approach has two main benefits. First, it would provide guidance on the best data generation practices to produce highly generalizable models. Second, since scientists need to extract biological insights from virtual stains, there must be a framework to quantify the domain of applicability of virtual staining.\nIn this work, we explore for the first time the generalizability of virtual staining models under three common HTS data distribution shifts:\n\u2022 Task 1: Generalizing to new phenotypes\n\u2022 Task 2: Generalizing to new cell types\n\u2022 Task 3: Generalizing to new phenotypes & cell types\nTo investigate these three tasks, we leverage a GSK proprietary dataset of 772,416 images, consisting of bright-field and 3 co-registered widely used fluorescence stains; fluorescein (FITC) for cytoplasm, 6-diamidino-2-phenylindole (DAPI)"}, {"title": "3 Experiments & Results", "content": "We first discuss the general points about our dataset, training, inference and evaluation procedures and then the three generalization tasks.\nDataset: Our experiments are based on a pool of 772,416 individual images generated as part of a GSK HTS study. The data comprises 98 16x24 well plates, with each plate containing 384 wells containing a combination of dimethyl sulfoxide (DMSO) (negative control) as it has a relatively low order of systemic toxicity Wilson et al. [1965], 10 GSK candidate compounds with 6 levels of toxicity from low to high and known apoptosis (programmed cell death) inducing compounds etoposide Kobayashi and Ratain [1994], and starausporine Qiao et al. [1996] with high orders of toxicity (positive controls). All plates of one cell type have a fixed layout of compounds and controls across the wells.\nWe define the wells that contain DMSO and the three lowest levels of toxicity for each compound to be non-toxic and the three highest concentrations of each compound as well as the etoposide and starausporine to be toxic. Every well consists of 9 fields of view each containing a bright-field and three co-registered fluorescent stains; fluorescein (FITC) for cytoplasm, 6-diamidino-2-phenylindole (DAPI) for nuclei detection and Cyanine (Cy5) for DNA-damage. Each cell type was represented by six different cell lines. For each cell type and stain, 27,000 bright-field and fluorescence image pairs were sampled from toxic and non-toxic labeled wells separately, with 21,000 image pairs used for training and 6,000 image pairs used for validation. Random samples for each cell type and labeled wells are shown in Figure 1. In addition, for each cell type three plates, excluded from any training or validation sets, were used to sample toxic and"}, {"title": "3.1 Task 1 - Generalization to new phenotype", "content": "We begin by exploring for each cell type, how virtual staining models trained on images containing samples of one phenotype; non-toxic, perform on images containing samples of a different phenotype; toxic. We report the difference in performance across the three levels of evaluation between the virtual staining models trained on non-toxic samples and the virtual staining models trained on toxic samples of the same cell type.\nTraining on non-toxic vs toxic improves performance: Across all three levels of evaluation, all three cell types, and all three virtual staining tasks shown in Figure 2, we find when testing on toxic samples training on non-toxic samples leads to improved results as measured by several metrics compared to training on toxic samples of the same cell type. In particular, for all virtual staining tasks we see consistently improved performance in PSNR, Jaccard Index, and F1 score across all cell types as well as the majority of virtual staining tasks showing improved performances in SSIM and N-MAE. However, for the task of generalising to lung toxic when training on lung non-toxic for virtual cytoplasm we see consistently improved performance in levels 1 and 2 but worsening results in level 3. We believe this is in part due to the heterogeneity of cell expression that occurs in non-toxic healthy cells providing an increase in the diversity of cell states to train generalizable virtual staining models on. In contrast, the toxic cells are induced into specific homogeneous cell states leading to potentially less diversity in training data producing less generalizable models. We can see some evidence of this in Figure 1 where despite non-toxic conditions we observe small amounts of naturally occuring DNA-damage signal in all non-toxic cell types images shown.\nOvarian non-toxic samples see the largest improvement: Across all measurements and virtual staining channels, when generalizing to ovarian toxic samples, training on ovarian non-toxic samples leads to improved performance compared to training on ovarian toxic samples. Upon visual inspection of Figure 3 for the virtual nuclei and virtual cytoplasm stains both the baseline and non-toxic models have produced predictions that replicate the general shape and intensity of a large number of cells seen in the fluorescence.\nThe prediction of the ovarian virtual nuclei trained on non-toxic is almost indistinguishable from the prediction of the virtual nuclei trained on toxic. However, upon closer inspection, as shown in yellow for certain nuclei the intensity profile and shape of the non-toxic nuclei more closely resembles that found in the fluorescence. Similar findings are highlighted for the cytoplasm intensity profile of individual cells. In contrast, the virtual DNA-damage predictions for both trained models are very similar to each other, with the model trained on non-toxic showing a small number of DNA-damage spots but they both display considerable losses of information compared to the fluorescence stain.\nFigure 4 shows the N-MAE obtained for the 20 most informative biological features for the virtual staining models trained on ovarian toxic and ovarian non-toxic samples. Across all virtual staining tasks, we observe a reduction in the average N-MAE when training on images of ovarian non-toxic. Consistently, we observe this result is not driven by an outlier, but by reductions in N-MAE across a diverse set of biological features.\nThese findings support the previously shown results that virtual nuclei and cytoplasm models trained on non-toxic samples of specific cell types such as ovarian when generalizing to toxic samples of the same cell type can learn meaningful and diverse biological feature representations that more closely align with the fluorescence stains."}, {"title": "3.2 Task 2 - Generalization to new cell type", "content": "Having explored the generalization to an unseen phenotype, in this section, we focus on the second task of generating virtually stained samples of bright-field images of a different cell type to the images the virtual staining models were trained on."}, {"title": "3.3 Task 3 - Generalization to new phenotypes & new cell types", "content": "In the final section, we combine the two previous distribution shifts and explore how virtual staining models trained on images of cells in non-toxic conditions of one cell type generalize to cells of a different cell type in toxic conditions.\nWe report the difference in performance across the three levels of evaluation between the virtual staining models trained on non-toxic samples and the virtual staining models trained on toxic samples of the same cell type.\nGenerally good generalization performance In a similar approach to Figure 6a, Figure 6b shows the different non- toxic training and toxic test cell type image set combinations explored for this generalization task and the corresponding mean scores for the virtual nuclei and virtual cytoplasm models for each of the three levels of evaluation."}, {"title": "4 Discussion & Conclusion", "content": "We have investigated the generalization performance of virtual nuclei, virtual cytoplasm and virtual DNA-damage models for three common HTS generalization tasks. Firstly, generalizing to an unseen phenotype. Secondly, generalizing to unseen cell types. Finally, generalizing to an unseen phenotype and cell types combined. Performance has been evaluated using metrics at the pixel level, instance level and biological feature level.\nFor the first generalization task, for both virtual nuclei and virtual cytoplasm, training on non-toxic samples leads to both good generalization and improved performance relative to training on toxic samples across all levels of evaluation. In particular, when testing on toxic ovarian samples, training on non-toxic ovarian samples produced predictions that achieved higher pixel-level quality, more accurate instance-wise translation and scores for the majority of biological features that more closely match the scores found in the fluorescence channels compared to training on toxic ovarian samples.\nPrevious work on the generation of The JUMP Cell Painting Dataset Chandrasekaran [2023] suggests it is necessary to have training sets with a high diversity of phenotypes to effectively train machine learning models. However, these results demonstrate that for this specific HTS dataset and the explored virtual staining tasks, training on non-toxic samples alone can lead to both good generalization and also performance improvements when measured using a variety of evaluation metrics. As such, we believe that for virtual nuclei and virtual cytoplasm staining of toxic samples training on widely available non-toxic samples is a viable alternative to training on toxic samples. These findings could lead to a reduction in the number of cell- and phenotype-specific models needed to efficiently utilize virtual staining for diverse HTS datasets.\nFor the second generalization task, we find generalizing to unseen cell types is complex, we identify for pixel-wise and instance-based metrics, training on ovarian and testing on lung led to good generalization performance for the virtual nuclei and virtual cytoplasm. However, when evaluating at the biological feature level we observed relatively high differences in N-MAE values. These results reveal that determining the correct evaluation metric for a chosen virtual staining application is important to effectively evaluate whether certain cell type-specific training sets can better generalize compared to other cell types. Additionally, we find that good generalization to one unseen cell type does not necessarily mean the same can be expected for another cell type. In contrast, we observe that training on images of breast cells consistently leads to bad generalization performance across all levels of evaluation. We also found that models trained on other cell types were poor at generalizing to images of breast cells. Further analysis into the three non-toxic cell type data sets revealed after a manual inspection of a random subset of 5,000 images from each cell type that breast cells are distributed more sparsely than the other cell types (Figure 1). The average number of cells in the inspected breast images was less than half of that in ovarian and a third of that in lung. This corresponds to the anatomical function of breast cells, which require more fat in their surroundings Ellis and Mahadevan [2013], reducing the number of cells in any image as shown in Figure 1.\nOn the other hand endocrine signaling Cooper [2000] of ovarian cells requires they be closely packed together, and the vital role of lung cells to exchange gas, in principle requires more cells. We believe that these structural differences are responsible for the poor generalization performance of images of breast cells. An additional possible explanation could be that the lower density of breast cells corresponds to a smaller volume of training data for the virtual staining models.\nFor the final generalization task, we see the same challenges when training on and generalizing to images of breast cells for the virtual nuclei and virtual cytoplasm. However, in general, across all levels of evaluation and the explored celltype and phenotype combinations we find that training on non-toxic is preferable even when additionally generalizing to toxic samples of an unseen cell type (See Figure 6b compared to Figure 6a). We saw particular improvements in PSNR and N-MAE and relatively similar performance for SSIM, F1 Score and Jaccard Index.\nDespite what appears to be good virtual DNA-damage generalization performance across certain evaluation metrics for all three generalization tasks, when examining the absolute values the results are poor compared to virtual nuclei and virtual cytoplasm affirming the findings from previous work Tonks et al. [2023]. This becomes clear qualitatively in Figure 3 and Figure 5 and quantitatively in Figure 4 where the N-MAE values are noticeably larger than in the other two channels.\nFuture work should explore the generalization performance to a broader selection of unseen cell types, and investigate further the issues with producing accurate virtual DNA-damage stains."}]}