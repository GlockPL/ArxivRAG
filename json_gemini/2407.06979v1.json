{"title": "DOES VIRTUAL STAINING FOR HIGH-THROUGHPUT SCREENING GENERALIZE?", "authors": ["Samuel Tonks", "Steve Hood", "Ceridwen Hopely", "Minh Doan", "Cuong Nguyen", "Ryan Musso", "Steve Titus", "Iain Styles", "Alexander Krull"], "abstract": "The large volume and variety of imaging data from high-throughput screening (HTS) in the phar-\nmaceutical industry present an excellent resource for training virtual staining models. However, the\npotential of models trained under one set of experimental conditions to generalize to other conditions\nremains underexplored. This study systematically investigates whether data from three cell types\n(lung, ovarian, and breast) and two phenotypes (toxic and non-toxic conditions) commonly found in\nHTS can effectively train virtual staining models to generalize across three typical HTS distribution\nshifts: unseen phenotypes, unseen cell types, and the combination of both. Utilizing a dataset of\n772,416 paired bright-field, cytoplasm, nuclei, and DNA-damage stain images, we evaluate the\ngeneralization capabilities of models across pixel-based, instance-wise, and biological-feature-based\nlevels. Our findings indicate that training virtual nuclei and cytoplasm models on non-toxic condition\nsamples not only generalizes to toxic condition samples but leads to improved performance across all\nevaluation levels compared to training on toxic condition samples. Generalization to unseen cell types\nshows variability depending on the cell type; models trained on ovarian or lung cell samples often\nperform well under other conditions, while those trained on breast cell samples consistently show poor\ngeneralization. Generalization to unseen cell types and phenotypes shows good generalization across\nall levels of evaluation compared to addressing unseen cell types alone. This study represents the first\nlarge-scale, data-centric analysis of the generalization capability of virtual staining models trained on\ndiverse HTS datasets, providing valuable strategies for experimental training data generation.", "sections": [{"title": "1 Introduction", "content": "High-throughput screening (HTS) plays a crucial role in drug discovery by enabling the simultaneous testing of a large\nnumber of compounds to assess their effects on cell cultures Szyma\u0144ski et al. [2011]. Fluorescence microscopy is the\nstandard tool in HTS for detecting drug effects on cellular structures Selinummi et al. [2009]. By covalently binding\ndifferent fluorescent dyes to biomolecules (fluorescent staining), it enables biological structures to be simultaneously\nrevealed by the different emission spectra of the dyes, with each dye captured in a separate image channel Tonks et al.\n[2023].\nAlthough it is an essential tool in modern biology, conventional fluorescence microscopy has important practical\nlimitations. The staining protocol typically requires the cells to be fixed and permeabilized - a process in which cells\nare preserved in their biological state, effectively frozen in time - thus limiting the application of this technique to single\ntime-point studies. Furthermore, as the expensive fixation and staining require specialist equipment and the number\nof fluorescence stains are inherently limited by spectrum saturation, significant interest has been put into label-free\nmicroscopy, wherein images of cells are captured without the need for staining Ounkomol et al. [2018]. Label-free\nmicroscopy Harrison et al. [2023], Gupta et al. [2022], while cost-effective and scalable, unfortunately, lacks the\nbiological information typically found in the fluorescence stains Pirone et al. [2022].\nRecent works have explored the concept of virtual staining to simultaneously leverage the scalability of label-free\nmicroscopy and biological information extracted from fluorescence microscopy Cross-Zamirski et al. [2022, 2023],\nImboden et al. [2023], Wieslander et al. [2021], Tonks et al. [2023]. Virtual staining is typically framed as a multimodal\nimage-to-image translation (I2I) problem Isola et al. [2017]. In this context, virtual staining models learn to translate\nunstained microscopy images into the desired labeled images.\nWhile recent works Cross-Zamirski et al. [2022, 2023], Imboden et al. [2023], Wieslander et al. [2021], Tonks et al.\n[2023] have shown the significant potential of virtual staining, the ability of virtual staining models to generalize to\nimages containing variations not present in the training data remains underexplored. In practice, HTS imaging data\nis highly diverse, being generated across different imaging systems, experiments, cell types, and phenotypes. This is\nknown as the generalization gap Wagner et al. [2022] and has been identified as a key reason for the lack of reusable\nvirtual staining models limiting its potential impact within large-scale applications. These challenges are analogous to\nthose in DNA sequence modeling Avsec et al. [2021], where models are typically trained on specific cell types and fail\nto generalize to new cell types. In offline reinforcement learning Mediratta et al. [2023], Cobbe et al. [2019], Levine\net al. [2020] where datasets predominantly focus on solving the task in the same environment, limiting the evaluation of\ngeneralization to unseen environments. Within the context of virtual staining we intend to bridge this generalization\ngap by performing a systematic data-centric approach to determine whether virtual staining models trained on specific\nsubsets of data can generalize under common distribution shifts. Our approach has two main benefits. First, it would\nprovide guidance on the best data generation practices to produce highly generalizable models. Second, since scientists\nneed to extract biological insights from virtual stains, there must be a framework to quantify the domain of applicability\nof virtual staining.\nIn this work, we explore for the first time the generalizability of virtual staining models under three common HTS data\ndistribution shifts:\n\u2022 Task 1: Generalizing to new phenotypes\n\u2022 Task 2: Generalizing to new cell types\n\u2022 Task 3: Generalizing to new phenotypes & cell types\nTo investigate these three tasks, we leverage a GSK proprietary dataset of 772,416 images, consisting of bright-field and\n3 co-registered widely used fluorescence stains; fluorescein (FITC) for cytoplasm, 6-diamidino-2-phenylindole (DAPI)"}, {"title": "2 Related Work", "content": "Virtual staining is a specific formulation of multimodal image-to-image translation (I2I) Isola et al. [2017], a machine\nlearning technique in which we want to train a model to translate one modality - label-free brightfield images - to another\nfluorescence images. Virtual staining using I2I has been widely explored using approaches based on a regression-\nloss Ounkomol et al. [2018], auto-regressive models Christiansen [2018], generative adversarial network (GANs) Tonks\net al. [2023], Upadhyay et al. [2021], Cross-Zamirski et al. [2022] and diffusion-based approaches Cross-Zamirski et al.\n[2023]. Across these methods when tested on samples similar to those in the training sets, virtual staining predictions\nhave been shown to consistently and reliably produce high quality images at the pixel-level Ounkomol et al. [2018],\nChristiansen [2018], Upadhyay et al. [2021], Cross-Zamirski et al. [2022] that preserve the majority of biological\ninformation found in fluorescence images Tonks et al. [2023], Cross-Zamirski et al. [2023]. In order to determine the\nusability of virtual staining at scale, these systems need to be able to generalize under various data distribution shifts\nDespite improvements in virtual staining very few models Van der Laak et al. [2021], Echle et al. [2021] are known\nto have been integrated into routine clinical workflows. A recent review Wagner et al. [2022] of 161 peer-reviewed\ncomputational pathology articles identified a core reason being the generalization gap; models failing to maintain\nperformance for unseen data with a shifted distribution. Nevertheless, in real-world applications, encountering unseen\ndata is very common. For example in high-throughput screenings (HTS) Szyma\u0144ski et al. [2011] a single imaging\nmachine, among many in a lab, generates imaging data for large numbers of compounds at different concentration\nlevels tested on cell cultures of different cell types (lung or breast) composed of different cell lines (H1299, HCC827).\nEarly virtual staining works Ounkomol et al. [2018] showed models trained on images of hiPSC cells did not perform as\nwell when generalizing to HEK-293, cardiomyocytes and HT-1080 cells. Although gross image features were visually\ncomparable, morphological detail improved when the model was trained on data of the same cell type. Meanwhile,\nother virtual staining works Christiansen [2018] showed when trained separately on images containing cortical and\nmotor neuron cells generalizing to a single well of a breast cell line (MDE-MD-231), sourced from a new laboratory\nand new transmitted-light technology led to an increase in pixel-level performance. Similarly, Cross-Zamirski et\nal. Cross-Zamirski et al. [2023] trained virtual staining models utilizing a diverse set of 290 cell type and phenotypes\nfrom the JUMP Cell Painting Dataset Chandrasekaran [2023] also testing on a single hold out plate, providing limited\nexploration of performance under domain shift. The question of whether virtual staining models can generalise at scale\nremains underexplored.\nA trivial solution to generalization would be to train virtual staining models using samples from each domain shift, but\nin practice training this amount of models is computationally very expensive and not scalable. Alternatively, domain\nadaptation methods for I2I such as DAI2I Murez et al. [2018] and OST Luo et al. [2020] have been shown to improve\nout-of-distribution performance on natural image datasets. None of the aforementioned approaches to bridging the\ngeneralization gap have focused on whether certain image domains, when used as training sets, are better able to learn\ndomain-invariant features compared to others. In this paper we perform the first large-scale systematic analysis of\nwhether virtual staining models trained on specific subsets of HTS data can generalize under three common distribution\nshifts."}, {"title": "3 Experiments & Results", "content": "We first discuss the general points about our dataset, training, inference and evaluation procedures and then the three\ngeneralization tasks.\nDataset: Our experiments are based on a pool of 772,416 individual images generated as part of a GSK HTS study.\nThe data comprises 98 16x24 well plates, with each plate containing 384 wells containing a combination of dimethyl\nsulfoxide (DMSO) (negative control) as it has a relatively low order of systemic toxicity Wilson et al. [1965], 10 GSK\ncandidate compounds with 6 levels of toxicity from low to high and known apoptosis (programmed cell death) inducing\ncompounds etoposide Kobayashi and Ratain [1994], and starausporine Qiao et al. [1996] with high orders of toxicity\n(positive controls). All plates of one cell type have a fixed layout of compounds and controls across the wells.\nWe define the wells that contain DMSO and the three lowest levels of toxicity for each compound to be non-toxic and\nthe three highest concentrations of each compound as well as the etoposide and starausporine to be toxic. Every well\nconsists of 9 fields of view each containing a bright-field and three co-registered fluorescent stains; fluorescein (FITC)\nfor cytoplasm, 6-diamidino-2-phenylindole (DAPI) for nuclei detection and Cyanine (Cy5) for DNA-damage. Each cell\ntype was represented by six different cell lines. For each cell type and stain, 27,000 bright-field and fluorescence image\npairs were sampled from toxic and non-toxic labeled wells separately, with 21,000 image pairs used for training and\n6,000 image pairs used for validation. Random samples for each cell type and labeled wells are shown in Figure 1. In\naddition, for each cell type three plates, excluded from any training or validation sets, were used to sample toxic and"}, {"title": "3.1 Task 1 - Generalization to new phenotype", "content": "We begin by exploring for each cell type, how virtual staining models trained on images containing samples of one\nphenotype; non-toxic, perform on images containing samples of a different phenotype; toxic. We report the difference\nin performance across the three levels of evaluation between the virtual staining models trained on non-toxic samples\nand the virtual staining models trained on toxic samples of the same cell type.\nTraining on non-toxic vs toxic improves performance: Across all three levels of evaluation, all three cell types, and\nall three virtual staining tasks shown in Figure 2, we find when testing on toxic samples training on non-toxic samples\nleads to improved results as measured by several metrics compared to training on toxic samples of the same cell type.\nIn particular, for all virtual staining tasks we see consistently improved performance in PSNR, Jaccard Index, and F1\nscore across all cell types as well as the majority of virtual staining tasks showing improved performances in SSIM\nand N-MAE. However, for the task of generalising to lung toxic when training on lung non-toxic for virtual cytoplasm\nwe see consistently improved performance in levels 1 and 2 but worsening results in level 3. We believe this is in part\ndue to the heterogeneity of cell expression that occurs in non-toxic healthy cells providing an increase in the diversity\nof cell states to train generalizable virtual staining models on. In contrast, the toxic cells are induced into specific\nhomogeneous cell states leading to potentially less diversity in training data producing less generalizable models. We\ncan see some evidence of this in Figure 1 where despite non-toxic conditions we observe small amounts of naturally\noccuring DNA-damage signal in all non-toxic cell types images shown.\nOvarian non-toxic samples see the largest improvement: Across all measurements and virtual staining channels,\nwhen generalizing to ovarian toxic samples, training on ovarian non-toxic samples leads to improved performance\ncompared to training on ovarian toxic samples. Upon visual inspection of Figure 3 for the virtual nuclei and virtual\ncytoplasm stains both the baseline and non-toxic models have produced predictions that replicate the general shape and\nintensity of a large number of cells seen in the fluorescence.\nThe prediction of the ovarian virtual nuclei trained on non-toxic is almost indistinguishable from the prediction of the\nvirtual nuclei trained on toxic. However, upon closer inspection, as shown in yellow for certain nuclei the intensity\nprofile and shape of the non-toxic nuclei more closely resembles that found in the fluorescence. Similar findings are"}, {"title": "3.2 Task 2 - Generalization to new cell type", "content": "Having explored the generalization to an unseen phenotype, in this section, we focus on the second task of generating\nvirtually stained samples of bright-field images of a different cell type to the images the virtual staining models were\ntrained on."}, {"title": "3.3 Task 3 - Generalization to new phenotypes & new cell types", "content": "In the final section, we combine the two previous distribution shifts and explore how virtual staining models trained on\nimages of cells in non-toxic conditions of one cell type generalize to cells of a different cell type in toxic conditions.\nWe report the difference in performance across the three levels of evaluation between the virtual staining models trained\non non-toxic samples and the virtual staining models trained on toxic samples of the same cell type.\nGenerally good generalization performance In a similar approach to Figure 6a, Figure 6b shows the different non-\ntoxic training and toxic test cell type image set combinations explored for this generalization task and the corresponding\nmean scores for the virtual nuclei and virtual cytoplasm models for each of the three levels of evaluation."}, {"title": "4 Discussion & Conclusion", "content": "We have investigated the generalization performance of virtual nuclei, virtual cytoplasm and virtual DNA-damage\nmodels for three common HTS generalization tasks. Firstly, generalizing to an unseen phenotype. Secondly, generalizing\nto unseen cell types. Finally, generalizing to an unseen phenotype and cell types combined. Performance has been\nevaluated using metrics at the pixel level, instance level and biological feature level.\nFor the first generalization task, for both virtual nuclei and virtual cytoplasm, training on non-toxic samples leads to\nboth good generalization and improved performance relative to training on toxic samples across all levels of evaluation.\nIn particular, when testing on toxic ovarian samples, training on non-toxic ovarian samples produced predictions that\nachieved higher pixel-level quality, more accurate instance-wise translation and scores for the majority of biological\nfeatures that more closely match the scores found in the fluorescence channels compared to training on toxic ovarian\nsamples.\nPrevious work on the generation of The JUMP Cell Painting Dataset Chandrasekaran [2023] suggests it is necessary to\nhave training sets with a high diversity of phenotypes to effectively train machine learning models. However, these\nresults demonstrate that for this specific HTS dataset and the explored virtual staining tasks, training on non-toxic\nsamples alone can lead to both good generalization and also performance improvements when measured using a variety\nof evaluation metrics. As such, we believe that for virtual nuclei and virtual cytoplasm staining of toxic samples training\non widely available non-toxic samples is a viable alternative to training on toxic samples. These findings could lead to a\nreduction in the number of cell- and phenotype-specific models needed to efficiently utilize virtual staining for diverse\nHTS datasets.\nFor the second generalization task, we find generalizing to unseen cell types is complex, we identify for pixel-wise\nand instance-based metrics, training on ovarian and testing on lung led to good generalization performance for the\nvirtual nuclei and virtual cytoplasm. However, when evaluating at the biological feature level we observed relatively\nhigh differences in N-MAE values. These results reveal that determining the correct evaluation metric for a chosen\nvirtual staining application is important to effectively evaluate whether certain cell type-specific training sets can better\ngeneralize compared to other cell types. Additionally, we find that good generalization to one unseen cell type does not\nnecessarily mean the same can be expected for another cell type. In contrast, we observe that training on images of\nbreast cells consistently leads to bad generalization performance across all levels of evaluation. We also found that\nmodels trained on other cell types were poor at generalizing to images of breast cells. Further analysis into the three\nnon-toxic cell type data sets revealed after a manual inspection of a random subset of 5,000 images from each cell\ntype that breast cells are distributed more sparsely than the other cell types (Figure 1). The average number of cells in\nthe inspected breast images was less than half of that in ovarian and a third of that in lung. This corresponds to the\nanatomical function of breast cells, which require more fat in their surroundings Ellis and Mahadevan [2013], reducing\nthe number of cells in any image as shown in Figure 1.\nOn the other hand endocrine signaling Cooper [2000] of ovarian cells requires they be closely packed together, and the\nvital role of lung cells to exchange gas, in principle requires more cells. We believe that these structural differences are\nresponsible for the poor generalization performance of images of breast cells. An additional possible explanation could\nbe that the lower density of breast cells corresponds to a smaller volume of training data for the virtual staining models.\nFor the final generalization task, we see the same challenges when training on and generalizing to images of breast cells\nfor the virtual nuclei and virtual cytoplasm. However, in general, across all levels of evaluation and the explored celltype\nand phenotype combinations we find that training on non-toxic is preferable even when additionally generalizing to\ntoxic samples of an unseen cell type (See Figure 6b compared to Figure 6a). We saw particular improvements in PSNR\nand N-MAE and relatively similar performance for SSIM, F1 Score and Jaccard Index.\nDespite what appears to be good virtual DNA-damage generalization performance across certain evaluation metrics for\nall three generalization tasks, when examining the absolute values the results are poor compared to virtual nuclei and\nvirtual cytoplasm affirming the findings from previous work Tonks et al. [2023]. This becomes clear qualitatively in\nFigure 3 and Figure 5 and quantitatively in Figure 4 where the N-MAE values are noticeably larger than in the other\ntwo channels.\nFuture work should explore the generalization performance to a broader selection of unseen cell types, and investigate\nfurther the issues with producing accurate virtual DNA-damage stains."}]}