{"title": "GPT Deciphering Fedspeak: Quantifying Dissent Among Hawks and Doves", "authors": ["Denis Peskoff", "Adam Visokay", "Sander Schulhoff", "Benjamin Wachspress", "Alan Blinder", "Brandon M. Stewart"], "abstract": "Markets and policymakers around the world\nhang on the consequential monetary policy de-\ncisions made by the Federal Open Market Com-\nmittee (FOMC). Publicly available textual doc-\numentation of their meetings provides insight\ninto members' attitudes about the economy. We\nuse GPT-4 to quantify dissent among mem-\nbers on the topic of inflation. We find that\ntranscripts and minutes reflect the diversity of\nmember views about the macroeconomic out-\nlook in a way that is lost or omitted from the\npublic statements. In fact, diverging opinions\nthat shed light upon the committee's \"true\" atti-\ntudes are almost entirely omitted from the final\nstatements. Hence, we argue that forecasting\nFOMC sentiment based solely on statements\nwill not sufficiently reflect dissent among the\nhawks and doves.", "sections": [{"title": "1 The Road to FOMC Transparency", "content": "The Federal Open Market Committee (FOMC) is\nresponsible for controlling inflation in the United\nStates, using instruments which dramatically affect\nthe housing and financial markets, among others.\nFor most of the 20th century, conventional wisdom\nheld that monetary policy is most effective when\ndecision-making was shrouded in secrecy; the tight-\nlipped Alan Greenspan, a past chairman of the Fed,\nquipped about \u201clearning to mumble with great in-\ncoherence.\u201d But times change.\nBlinder et al. (2008) show how the emergence of\ngreater transparency and strategic communication\nbecame an important feature of 21st century central\nbanking. Fed communication is now an integral\ncomponent of monetary policy, and \"Fed watchers\"\ndote on every word. The FOMC first started releas-\ning public statements following their meetings in\nFebruary 1994. This meager documentation grew\nand now consists of three types for each official\nmeeting: carefully produced and highly stylized\none page statements are released immediately after\neach FOMC meeting, followed about three weeks\nlater by lengthier minutes, and finally five years\nlater by full, verbatim transcripts. Subsequently\nthis triplet is referred to as documents. We find\nminutes closely reflect the content of transcripts,\nso to avoid redundancy, we focus our analysis on\ntranscripts and statements.\nIncreased FOMC communication has prompted\nsocial science research spanning the disciplines of\neconomics, sociology, finance and political science\n(Section 2.1). Financial market participants are also\nkeenly interested. Billions, if not trillions of dollars\nare traded on the Fed's words. The interpretations\u2014\nright or wrong\u2014of what the FOMC \u201creally means\u201d\nmove markets and affect the economy. However,\nrelying upon documents as data in the social sci-\nences is a challenge due to the lack of structure and\nthe cost of annotation (Grimmer and Stewart, 2013;\nGentzkow et al., 2019; Ash and Hansen, 2023).\nHansen and Kazinnik (2023) show that Gener-\native Pre-training Transformer (GPT) models out-\nperform a suite of commonly used NLP methods\non text quantification. Motivated by these results,\nwe set out to quantify the language of the FOMC\nusing GPT-4 (OpenAI, 2023) by preparing a com-\nbined data set of FOMC documents from 1994-"}, {"title": "2 FOMC Data: Transcript to Statement", "content": "The FOMC normally meets eight times per year in\norder to assess current economic conditions, ulti-\nmately deciding upon the path for monetary policy.\nWe aggregate and release the official publicly avail-\nable text documenting these deliberations by the\nFed as an aligned corpora of documents from 1994\nto 2016. These text documents are similar in con-\ntent, but transcripts and statements are dramatically\ndifferent in style and detail (Figure 1). For our pur-\nposes, the statements required no pre-processing.\nFor the transcripts, we use regular expressions to\npartition and then re-aggregate the text by each\nunique speaker. See Appendix D for an example of\neach document type."}, {"title": "2.1 Lessons from Social Science", "content": "Past work has used at most one form of FOMC\nmeeting documentation, but rarely multiple in con-\njunction. For example, in the finance literature,\nMazis and Tsekrekos (2017) apply Latent Seman-\ntic Analysis to FOMC statements to identify the\nmain \"themes\" used by the committee and how well\nthey explain variation in treasury yields. Gu et al.\n(2022) use minutes to investigate how the tonal-\nity of committee deliberations impacts subsequent\nstock market valuations. Political scientists use\ntranscripts to estimate committee members' prefer-\nences on inflation and unemployment (Baerg and\nLowe, 2020). Economists have assessed the role\nof communication in achieving monetary policy\nobjectives by looking at similar documents (Romer\nand Romer, 2004; Handlan, 2020). H\u00fcpper and\nKempa (2023) investigate the extent to which shift-\ning inflation focus is reflected in full transcripts.\nEdison and Carcel (2021) apply Latent Dirichlet\nAllocation (LDA) to transcripts to detect the evo-\nlution of prominent topics. Hansen et al. (2017)\nuse LDA to quantify transcripts and identify how\ntransparency affects the committee's deliberations."}, {"title": "2.2 Hawks and Doves", "content": "We take the transcripts to best represent FOMC\nmembers' underlying attitudes and think of the\nstatements as stylized representations of what they\nwish to communicate publicly. To identify disagree-\nment, we need to go beyond the statements and\nlook closely at the language employed by members"}, {"title": "2.3 Manual Analysis to Create a Gold Label", "content": "Dissent amongst speakers is normally concentrated\non the discussion of the economic and financial\nsituation of the U.S, specifically inflation target-\ning. For example, in the January 2016 meeting\ntranscript, the committee discusses their 2 percent\ninflation projection in the context of factors such as\noil prices, the job market, and the Chinese economy.\nMany of the members argue that the inflation pro-\njection of 2 percent will not be accurate, while oth-\ners who do support the 2 percent projection qualify\ntheir support with varying degrees of uncertainty.\nIn this meeting, President Mester concludes,\nWhile Mr. Tarullo argues in opposition,\n\u201c... I didn't have reasonable confidence\nthat inflation would rise to 2 percent.\nIf the meeting statement was an accurate represen-\ntation of what transpired at the meeting, it would\nfollow that the uncertainty of the committee regard-\ning their inflation forecast would be communicated.\nInstead, the diverging individual opinions are omit-\nted in the final statement, where:"}, {"title": "2.4 GPT-4 \"Reads\" Terse Documents", "content": "GPT-4, and Large Language Models more broadly,\nare a suitable tool for rapid linguistic processing\nat scale. We produce three different measurements\nof hawk/dove sentiment using statements and two\nusing transcripts. Hansen and Kazinnik (2023)\nuse GPT-3 to quantify 500 sentences selected uni-\nformly at random from FOMC statements between\n2010 and 2020. We extend this by using GPT-4\nto quantify all 3728 sentences in statements from\n1994 to 2016 (Appendix B). A limitation of this\napproach is that the holistic sentiment of the meet-\ning is not captured because each sentence is scored\nindependently-without context.\nThe first statement measurement we propose is\nto simply take an unweighted mean of all individual"}, {"title": "3 LLMs Quantify Economic Text", "content": ""}, {"title": "3.1\nMeasuring Dissent", "content": "We can use the sentence-level statement and\nspeaker-level transcript scores from GPT-4 to com-\npute a measure of dissent for each meeting using\nthe following algorithm:\n1. From the list of scores for each meeting, count\nthe number of hawkish/mostly hawkish and\nthe number of dovish/mostly dovish scores.\n2. If there is at least one hawkish score and at\nleast one dovish score within the same meet-\ning, assign Dissent = 1. Else, Dissent = 0.\nWe find that 47% of statements and 82% of tran-\nscripts contain dissent. We also compute the con-\nditional probability of a transcript containing dis-\nsent given the associated statement binary, $P(T =\n1|S = 1)$ and $P(T = 1|S = 0)$. We find that when\na statement contains dissent, $P(T = 1|S = 1)$,\nthe transcript agrees more than 97% of the time.\nHowever, for statements scored as having no dis-\nsent, $P(T = 1|S = 0)$ we find that more than 69%\nof associated transcripts are scored as containing\ndissent. This means that for the 53% of statements\nthat don't show signs of dissenting opinions, there\nis likely dissent in the transcript as evidenced by\nthe speaker-level hawk/dove scores."}, {"title": "3.2 Conclusion and Next Steps", "content": "Our method of ingesting the entire statement for an\naggregate prediction better captures the extremes,\nwhich more closely mirror the gold label human\nannotation and suggests that Large Language Mod-\nels can avoid the noise in this nuanced context.\nThe $F_1$ score for this comparison is 0.57. While\nthis is rather low as a measure of model \"fit\", it is\nimportant to note that the results rarely flip senti-\nment (from hawkish to dovish, or mostly hawkish\nto mostly dovish), rather, it just seems to mostly\ndisagree on adjacent categories. See Figure 3 for a\nvisual comparison of the sentence-level, entire text,\nand manual scores. Of note, the inconsistent pro-\nvision of statements and relatively high volatility\nin hawk/dove sentiment before 2000 is consistent\nwith Meade and Stasavage (2008) and Hansen et al.\n(2017) who have also studied the 1993 change in\nFOMC communication strategy. We demonstrate\nthat GPT-4 can identify the extremes in dissenting\nhawk and dove perspectives despite the indications\nof a clear consensus in the statements. This empiri-\ncal finding supports our manual analysis.\nWhile we focus on transcripts and statements, fu-\nture work may consider an even more fine grained\nanalysis, incorporating minutes as well. We found\nthe content of the minutes to more closely resemble\nthe transcripts than the statements, but differences\ndo exist and remain underexplored.\nAdditionally, we note that GPT-4 scores made\nmore neutral predictions than the gold standard\nmanual labels. To improve upon this, we created a\nbalanced few-shot example using sentences from\nFOMC statements not included in our sample\nmeetings since 2020. This marginally improved\nthe prediction \u201cfit\u201d ($F_1$ of 57% to 58%), but we\nexpect that this could be improved much further\nwith additional prompt engineering.\nGPT-4 is able to quickly quantify stylized eco-\nnomic text. Our results from quantifying dissent\nsupport the hypothesis that dissenting opinions on\nthe topic of inflation omitted from FOMC state-\nments can be found in the associated transcripts. As\nLLMs continue to improve, we expect that it will\nbe possible to study even more nuanced questions\nthan the ones we answer here."}, {"title": "Limitations", "content": "Substantively, strategic signaling in the FOMC is\na challenging topic and this is only an initial inves-\ntigation. Dissent does not have clear ground truth"}, {"title": "Ethics Statement", "content": "The FOMC is a high-stakes body whose activities\nare already subject to substantial scrutiny. There\nis some ethical risk in exploring linguistic signals\nof hidden information. For example, based on the\nsubstantive literature we believe that dissent is in-\ntentionally signalled in meetings in order to set up\nfuture discussions or lay claims to particular po-\nsitions. Nevertheless, attribution of intention (as\nimplied by 'dissent') always involves some level\nof error that could be uncomfortable to meeting\nparticipants who feel mischaracterized. We also\nemphasize that our approach to capturing dissent\nwould not be appropriate to use outside this spe-\ncific context without careful validation. Finally, by\nmaking the FOMC data more easily available to\nthe NLP community, we also assume some ethical\nresponsibility for the potential uses of that data (see\ne.g. Peng et al., 2021). We spend under $1500 on\ncomputation and under $1000 on annotation and\nbelieve our results to be reasonably reproducible.\nWe feel that these concerns are ultimately minor\ngiven that all participants are public officials who\nknew their transcripts would ultimately be released."}, {"title": "A Appendix", "content": "Our appendix contains the GPT prompt we used, a\nsection on our computational analysis, and an ex-\nample of the statement (Figure 6), transcript (Fig-\nure 7), and minutes (Figure 8) for the January 29-\n30, 2008 meetings."}, {"title": "B GPT-4", "content": "We used GPT-4 heavily for our experiments and\nanalysis. Recent work (OpenAI, 2023; Liu et al.,\n2023; Guan et al., 2023) that successfully uses GPT-4 for classification gave us confidence in its quality.\nAdditionally, a human expert on our team examined\nGPT-4 generated labels, and found that in a sample\nof 25, our expert agreed with 19 labels with high\nconfidence, and 22 labels with at least moderate\nconfidence."}, {"title": "B.1 Methodology", "content": "We used variants of a single prompt template for all\nof our tasks. It contains the relevant labels (Hawk-\nish, Dovish, etc.) as well as their definitions. It\nincludes space for some INPUT and asks which\nlabel best applies to the input. When processing\ndifferent documents, such as transcripts, we would\nswitch out the word statements in the prompt for\nthe appropriate document word.\nFor statements, we 0-shot prompted GPT-4 to la-\nbel each statement as one of the five labels. We also\nran an experiment in which we classified each sen-\ntence of each statements as one of the labels, then\naveraged the sentence scores to get to statement\nscore. Finally, we reran this sentence classification\nwith a 10-shot prompt. The prompt was similar to\nbelow, except with 10 examples of sentences and\ntheir classifications at the beginning.\nFor minutes, we 0-shot prompted GPT-4-32K to\nlabel each statement as one of the five labels.\nFor transcripts, we 0-shot prompted GPT-4-32K\nto examine all of each speakers speech, and provide\neach speaker a single label for each transcript.\nFor all API calls to OpenAI, we only modified\nthe model (either GPT-4 or GPT-4-32K). We did\nnot change any other settings."}, {"title": "C Computational Analysis", "content": "We paired our manual review of the January 26-27,\n2016 transcript with a computational analysis of\ndissent in the meeting. We stratified the meeting\ninto nine topics, each corresponding to a portion of\nthe transcript content. As a baseline, we counted\nthe number of speakers in each section to see if\nthis metric could reflect dissent. This technique,\nhowever, seemed to reflect the length of the conver-\nsation as opposed to the degree to which members\ndisagreed with one another.\nOur next approach was to do a sentiment anal-\nysis of each topic to see if the prevalence of neg-\nativity could indicate dissent. We supposed that\nnegative sentiment would be high if the speakers\nopposed the stance of either other individuals or\nthe committee as a whole. Using the VADER lex-\nicon (Hutto and Gilbert, 2014), we calculated the\nsentiment of each sentence within the nine topics.\nSince VADER is trained on web-based social me-\ndia content, which is typically more abrupt than the\nformal language appearing in the FOMC transcript,\nwe conducted the sentiment analysis by sentence\nto optimize the method's performance.\nTo analyze dissent more specifically, we com-\nputed the fraction of negative sentences in each\ntopic. For this analysis, we set the threshold neg-\nativity score to be 0.1. That is, sentences with a\nnegativity score of 0.1 or higher were classified as\nnegative while all others were not. This number\ndetermined by manually reviewing what sentences\nwere captured by varying thresholds and evaluat-\ning whether or not they conveyed dissent. When\nthe threshold was set too low (0.05), four out of\nten randomly selected sentences conveyed dissent.\nWhen set too high (0.15), seven out of ten randomly\nselected sentences conveyed dissent, but many sen-\ntences that indicated dissent were omitted. At the\nthreshold of 0.1, still seven out of ten randomly\nselected sentences conveyed dissent, and more sen-\ntences that conveyed dissent were captured."}, {"title": "D Document Examples", "content": "See Figures 6, 7, and 8 for examples of the docu-\nments."}]}