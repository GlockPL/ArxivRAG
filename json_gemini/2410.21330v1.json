{"title": "LLM Robustness Against Misinformation in Biomedical\nQuestion Answering", "authors": ["Alexander Bondarenko", "Adrian Viehweger"], "abstract": "The retrieval-augmented generation (RAG) approach is used to reduce the confabulation of large language models\n(LLMs) for question answering by retrieving and providing additional context coming from external knowledge\nsources (e.g., by adding the context to the prompt). However, injecting incorrect information can mislead the\nLLM to generate an incorrect answer.\nIn this paper, we evaluate the effectiveness and robustness of four LLMs against misinformation-Gemma 2,\nGPT-40-mini, Llama 3.1, and Mixtral-in answering biomedical questions. We assess the answer accuracy on\nyes-no and free-form questions in three scenarios: vanilla LLM answers (no context is provided), \"perfect\"\naugmented generation (correct context is provided), and prompt-injection attacks (incorrect context is provided).\nOur results show that Llama 3.1 (70B parameters) achieves the highest accuracy in both vanilla (0.651) and\n\"perfect\" RAG (0.802) scenarios. However, the accuracy gap between the models almost disappears with \"perfect\"\nRAG, suggesting its potential to mitigate the LLM's size-related effectiveness differences.\nWe further evaluate the ability of the LLMs to generate malicious context on one hand and the LLM's\nrobustness against prompt-injection attacks on the other hand, using metrics such as attack success rate (ASR),\naccuracy under attack, and accuracy drop. As adversaries, we use the same four LLMs (Gemma 2, GPT-40-mini,\nLlama 3.1, and Mixtral) to generate incorrect context that is injected in the target model's prompt. Interestingly,\nLlama is shown to be the most effective adversary, causing accuracy drops of up to 0.48 for vanilla answers\nand 0.63 for \"perfect\" RAG across target models. Our analysis reveals that robustness rankings vary depending\non the evaluation measure, highlighting the complexity of assessing LLM resilience to adversarial attacks.", "sections": [{"title": "Description", "content": "With their increasing use in a biomedical context, particularly question-answering using retrieval\naugmented generation (RAG), it is important to investigate under which conditions large language\nmodels (LLMs) fail. Our study finds that the robustness of available models against misinformation\nin retrieved context is highly variable, illustrating the need for domain-specific testing and possibly\nfurther guardrails before deployment."}, {"title": "1. Introduction", "content": "In recent years, generative artificial intelligence (AI) has become increasingly ubiquitous, with a\nplethora of tools that allow even non-experts to easily access and use AI systems (for example, for\nnatural language generation, image or video generation, etc.). Now, a wide range of individuals and\norganizations can almost effortlessly incorporate the capabilities of advanced AI into their workflows\nand products. However, this ease of access has also led to an increase in the misuse of generative AI,\nsuch as producing fake images or generating misinformation (i.e., the information that appears plausible\nbut is factually incorrect) that often lands on the Internet.\nOne problem with incorrect synthetic information on the web is when the next generation of AI\nmodels (for example, large language models) is trained on this data. For instance, the work by Shumailov\net al. [1] introduced a concept of a \"model collapse\u201d that is described as irreversible defects in the model"}, {"title": "2. Related Work", "content": "In this section, we first provide a brief overview of the previous work on the influence of malicious\ncontext (often called noise) on the effectiveness of the RAG-based question answering approaches.\nAfterwards, we describe standard metrics used to evaluate the system's robustness against various\nadversarial attacks."}, {"title": "2.1. Noise influence on RAG", "content": "Several works have investigated the influence of noise on the effectiveness of the RAG-based question-\nanswering systems. The noise is defined across previous works as malicious triggers that elicit undesired\nbehavior of an LLM when exposed to such a context. The triggers are often incorrect facts in the form\nof misinformation, and undesired behavior is LLM-generated wrong answers.\nThe work by Zou et al. [8] studied knowledge corruption attacks, where a few malicious texts could\nbe injected into the knowledge database of an RAG system. The expected corrupted LLM behavior was\nthat the LLM generated an attacker-chosen target answer for an attacker-chosen target question. The\nproposed attack framework could achieve a 90% attack success rate when injecting just five malicious\ntexts for each target question into a knowledge database with millions of texts. The work used several\nquestion-answering datasets for a general domain.\nAnother work by Xue et al. [9] studied the attacks that steer an LLM in generating negatively biased\nresponses, for example, about some well-known personalities. With the 98% success rate of the attacks"}, {"title": "2.2. Robustness evaluation", "content": "One of the most common measures to evaluate the effectiveness of adversarial attacks is an attack\nsuccess rate (ASR). The ASR score is represented by the percentage of successful adversarial attacks that\nmake the target model demonstrate some malicious behavior solicited by the attacker. The ASR score\nhas been used in various domains of adversarial attacks including textual sentiment classification [10],\nimage classification [11, 12], cryptography [13], or jailbreak attacks against ChatGPT [14], etc. In the\nscope of our study, ASR evaluates the ability of the prompt-injection attacks to trick the target model\ninto making a wrong answer prediction by providing factually incorrect context during the model\nprompting. While ASR typically measures the attacker's effectiveness (the higher the more effective), at\nthe same time it evaluates the target model's robustness against the attacks (the lower the better).\nAnother common robustness measure is the accuracy under attack. It measures the target model's\naccuracy when provided with adversarial examples. However, the main caveat of this metric is that its\nvalue may depend on the overall model's effectiveness. For instance, a low accuracy under attack can\nresult from the generally low model's effectiveness without attacks. For this reason, we additionally\nmeasure the accuracy drop compared to the \"clean\" model's input, which is vanilla and \"perfect\" AG\nanswers in our study."}, {"title": "3. Data and Experiments", "content": "In this work, we study LLM robustness for question answering in the domain of biomedical question\nanswering. For evaluation, we use a respective dataset whose characteristics are described in this\nsection. Additionally, the section provides details on the experimental setup of the study."}, {"title": "3.1. Dataset", "content": "In our study, we use a manually curated BioASQ dataset [15, 16] that contains 4721 biomedical questions\nand their respective answers. The questions and answers in the dataset were manually created by a\nteam of biomedical experts and covered three main topics: diseases, drugs, and genetics. Each question-\nanswer pair has an associated set of PubMed abstracts that contain sufficient information for answering\nthe question and a set of snippets, i.e., manually labeled text spans in the abstracts that answer the\nquestion either fully or partially. To comply with our computational budget, we extracted from the\nBioASQ dataset 1350 yes-no questions using the set of syntactic rules (e.g., if a question starts with\n\"is/are\", \"do(es)\u201d, etc.) and randomly sampled 1050 free-form questions from the rest of the dataset."}, {"title": "3.2. Experiments", "content": "We test the effectiveness of the four LLMs in three scenarios: vanilla responses, \u201cperfect\u201d augmented\ngeneration, and responses under prompt-injection attacks. We access the LLMs via the OpenAI and\nGroq APIs using the instructor Python library. 5"}, {"title": "3.2.1. Vanilla answers", "content": "To generate vanilla answers, we simply instruct a target LLM to answer a question without providing\nadditional context. Following a common prompting strategy, we additionally assign the model with\nthe role of a medical professional and provide instructions on the desired answer format. We use the\nfollowing prompt for the yes-no questions:"}, {"title": "3.2.2. \u201cPerfect\u201d augmented generation", "content": "To imitate the scenario, when relevant and factually correct external information is provided to the\nLLM for question answering, we include in the prompt all the snippets associated with the question in\nthe BioASQ dataset as context. We use the snippets (and not the abstracts) since previous work has\nshown that using shorter texts results in higher answer accuracy [17]. As before, we assign a role to\nthe LLM and provide instructions for utilizing the context:"}, {"title": "3.2.3. Prompt-injection attacks", "content": "During the prompt-injection attacks, we instruct a target model MT to answer questions using the\nsame prompts as in Section 3.2.2. We, however, replace the correct context with the adversarial context\ngenerated by the adversarial LLM MA. We use the following prompt to generate the adversarial context:\nIt is interesting to note that when MA is assigned the role of a \"medical professional\u201d (analogous\nto the MT prompt), it often rejects to provide adversarial context justifying its decision like \"I can't\nprovide a rewritten context that would falsely suggest that ...\u201d. However, changing the assigned role to\na \"game player\u201d leads to no rejection at all."}, {"title": "4. Results", "content": "To evaluate the effectiveness of MT in correctly answering biomedical questions, we calculate accuracy.\nFor the yes-no questions, we use an exact match between lower-cased single-word predicted answers\nand ground truth answers. For the free-form questions, analogously to Farquhar et al. [18], we use an\nLLM judge (GPT-40-mini) to evaluate the predicted answer correctness given the reference true answer.\nFollowing the example from OpenAI Evals, we create the following prompt for the LLM judge:"}, {"title": "4.1. Evaluation metrics", "content": "To evaluate the effectiveness of MT in correctly answering biomedical questions, we calculate accuracy.\nFor the yes-no questions, we use an exact match between lower-cased single-word predicted answers\nand ground truth answers. For the free-form questions, analogously to Farquhar et al. [18], we use an\nLLM judge (GPT-40-mini) to evaluate the predicted answer correctness given the reference true answer.\nFollowing the example from OpenAI Evals, we create the following prompt for the LLM judge:\nWe further evaluate the effectiveness of the prompt-injection attacks for each MA by calculating\nthe attack success rate as the ratio of the cases when a question is answered correctly without context\n(vanilla answer) but is wrongly answered under the attack.\nTo evaluate the LLM robustness against prompt-injection attacks, we use several metrics. Firstly,\nwe calculate accuracy under attack, i.e., the ratio of correctly answered questions when the model is\nprovided with an adversarial context. However, this accuracy alone does not consider the overall ability\nof the model to correctly answer questions. We, thus, calculate the difference between the accuracy\nwithout attacks (vanilla and \u201cperfect\u201d AG answers) and under attacks. Furthermore, we use ASR as a\nmeasure of the MT ability to resist attacks by MA (the lower, the better)."}, {"title": "4.2. Vanilla answers and \u201cperfect\u201d AG", "content": "The evaluation results in Table 1 show a substantial difference between the answer accuracy for the\nyes-no and free-form questions (both vanilla and \u201cperfect\u201d AG answers)-LLMs are much more effective\nat answering the yes-no questions. This observation aligns with previously published results [17]. While\nwe observe a relatively large difference in the effectiveness between different LLMs for vanilla answers\n(lowest averaged accuracy of 0.542 by Mixtral and highest accuracy of 0.651 by Llama), providing\ncorrect context almost eliminates the difference (Mixtral accuracy is 0.780 and Llama accuracy is 0.802).\nThis observation emphasizes the fact that effective RAG-based approaches (i.e., relevant and correct\ninformation is found and provided to the LLM) not only are able to boost the LLM question answer\naccuracy but also diminish the model's size-related effectiveness (compare in Table 1, Gemma with\n9 billion parameters vs. Llama with 70 billion parameters)."}, {"title": "4.3. Robustness against attacks", "content": "Interestingly, when we calculate the ASR scores for the yes-no and free-form questions, we observe\nthat Llama and GPT-40-mini are more successful as an adversary for the yes-no questions and Gemma\nand Mixtral in the scenario of the free-form questions. The lowest values of the accuracy under attack"}, {"title": "4.4. Discussion", "content": "First, we evaluated the accuracy of the four LLMs in answering biomedical questions. We found that\nLlama 3.1 with 70B parameters is the most effective and achieves an accuracy of 0.651 in the vanilla\nquestion answering scenario. The same model is again the most effective when relevant and correct\ncontextual information is provided (accuracy of 0.802). However, in this scenario, we observe that the\ndifference in accuracy across the four models is rather small.\nInterestingly, Llama also demonstrates the most effective adversarial prompt-injection attacks, causing\nthe accuracy drop of the LLMs under attack of up to 0.481 for the vanilla answers and up to 0.632 for\n\"perfect\" AG. While evaluating the LLM's robustness, we could not determine the most robust model\nsince the results are inconsistent depending on the evaluation measure. However, two models stand\nout: Mixtral with the lowest accuracy drop and Llama with the lowest ASR** score (see Table 1)."}, {"title": "5. Conclusion", "content": "In this paper, we evaluated the effectiveness and robustness of large language models in the domain of\nbiomedical question answering. Our evaluation results by simulating \"perfect\" augmented generation\n(i.e., providing relevant and correct context to LLMs) showed substantial accuracy improvements over\nthe model's vanilla answers. Interestingly, in this scenario, the initial differences in the vanilla accuracy\nbetween the different LLMs are significantly reduced (from 0.11 to 0.2). These findings emphasize the\nimportance of developing effective and robust RAG systems.\nWhile LLMs-especially when used in RAG applications-demonstrated high effectiveness in biomedi-\ncal question answering, we found that they are vulnerable to prompt-injection attacks. Our experiments\nshowed that the LLM's answer accuracy can decrease by up to 0.48 compared to vanilla responses when\nincorrect synthetic information is included in the prompt.\nConsidering the findings of this work, future research should focus on developing more sophisticated\nRAG techniques, developing defense mechanisms against misinformation (e.g., identifying incorrect\nsynthetic information), and establishing benchmarks for evaluating LLM robustness."}]}