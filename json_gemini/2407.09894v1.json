{"title": "TRANSFERRING STRUCTURE KNOWLEDGE: A NEW TASK TO FAKE NEWS DETECTION\nTOWARDS COLD-START PROPAGATION", "authors": ["Lingwei Wei", "Dou Hu", "Wei Zhou", "Songlin Hu"], "abstract": "Many fake news detection studies have achieved promising\nperformance by extracting effective semantic and structure\nfeatures from both content and propagation trees. How-\never, it is challenging to apply them to practical situations,\nespecially when using the trained propagation-based mod-\nels to detect news with no propagation data. Towards this\nscenario, we study a new task named cold-start fake news\ndetection, which aims to detect content-only samples with\nmissing propagation. To achieve the task, we design a sim-\nple but effective Structure Adversarial Net (SAN) framework\nto learn transferable features from available propagation to\nboost the detection of content-only samples. SAN introduces\na structure discriminator to estimate dissimilarities among\nlearned features with and without propagation, and further\nlearns structure-invariant features to enhance the generaliza-\ntion of existing propagation-based methods for content-only\nsamples. We conduct qualitative and quantitative experiments\non three datasets. Results show the challenge of the new task\nand the effectiveness of our SAN framework.", "sections": [{"title": "1. INTRODUCTION", "content": "Nowadays, mainstream social platforms (e.g., Twitter) have\nfacilitated the dissemination of information in a faster and\ncheaper way. Nevertheless, the ease has also caused the wide\nspread of fake news, which has brought detrimental effects\non individuals and society [1]. Triggered by the negative im-\npact of fake news spreading, it is critical to develop automatic\nmethods for fake news detection.\nGenerally, users on social media share opinions, conjec-\ntures and evidence for checking fake news. Through their var-\nious interactive behaviors, a propagation tree describing the\nlaw of information transmission is formed and plays a signif-\nicant role in fake news detection. Previous works [2, 3] have\nempirically shown that compared with the truth, false news\nhas deeper propagation structures, and reaches a wider audi-\nence. To leverage the difference, many efforts [4-13] have\nbeen denoted to jointly explore effective high-level semantic\nand structural properties from content and the corresponding\npropagation trees via different neural networks. Compared\nwith the model learned only on the content [14-16], these\npropagation-based models trained on samples with both con-\ntent and propagation, provides a more comprehensive view of\nfake news and have shown superior detection performance.\nHowever, a practical barrier is that, in most cases, the ac-\nquisition of propagation data is not available at any time, and\nusually requires a great quantity of manpower and computa-\ntion resources. When lacking propagation structure, the above\npropagation-based detection systems would obtain subopti-\nmal performance. These models are trained on content and\npropagation tree to jointly learn semantic and structural fea-\ntures, leading to a specific feature space for detection. Ob-\nviously, for samples that lack propagation information, i.e.,\ncold-start propagation, they fail to perform well due to the\ndissimilarities among features with and without propagation.\nBased on the above scenario, we develops a new task to\nfake news detection towards cold-start propagation, named\ncold-start fake news detection. It aims to train the model\nfrom samples with available propagation and content, and\nthen predict content-only samples without any propagation\ndata. Different from the existing fake news detection tasks,\nthe new task focuses on the model's generalization capability\nof absence of propagation trees. Studying cold-start prop-\nagation scenario can promote the extensive applications of\npropagation-based detection methods in practical detection.\nUnder cold-start fake news detection task, directly apply-\ning existing propagation based models to capture structure-\nspecific features from propagation trees would hurt the de-\ntection of the cold-start news that has no propagation trees.\nTherefore, an intuitive solution to the new task is to remove\nthe nontransferable structure-specific features and preserve\nthe shared characteristics across different data types.\nTo achieve this, we design a simple but effective Struc-\nture Adversarial Net (SAN) framework to boost the detec-\ntion performance of propagation-based models for cold-start"}, {"title": "2. A NEW TASK: COLD-START FAKE NEWS\nDETECTION", "content": "We describe how to revise the traditional fake news detec-\ntion to achieve the new task towards cold-start propagation.\nSpecifically, as shown in the Input in Fig. 1, we directly re-\nmove the whole propagation trees of samples in the testing\nset as the cold-start news to simulate the cold-start propa-\ngation setups. Formally, define $D^{train} = {\\{(x^{train}_i, G^{train}_i)\\}, i \\in [1, N^{train}]}$ and $D^{test} = {\\{x^{test}_i\\}, i \\in [1, N^{test}]}$ as the training\nand testing sets, respectively, where x refers to the source\nnews and G indicates propagation trees. During the train-\ning stage, the model is trained on the complete samples\n$(x^{train}_i, G^{train}_i)$ with both content and propagation to learn\ntransferable semantic and structural patterns for detecting\nfake news, i.e.,\n$f: (x^{train}, G^{train}) \\rightarrow y$.\nDuring the testing stage, the trained model is used to predict\nthe cold-start news $x^{test}$ that lacks propagation data, i.e.,\n$\\hat{y} = f(x^{test}).$"}, {"title": "3. APPROACH", "content": "To boost the detection for content-only samples, we develop a\nsimple but effective Structure Adversarial Net (SAN) frame-\nwork to learn latent structural features from previous propa-\ngation trees. The overall architecture is shown in Fig. 1.\n3.1. Vanilla Propagation-based Approach\nGiven the input sample including content of the source news x\nand propagation trees G, existing models apply various neural\nnetworks to extract high-level textual and structural features.\nThe latent representation h is computed by,\n$h = f_{enc}(x, G; \\Theta),$\nwhere $f_{enc}$ can be the encoder in [8, 9, 12] to learn seman-\ntic and structural features, and $\\Theta$ refers to the corresponding\ntrainable parameters. Then, a classifier consisting of a full\nconnection layer and a softmax function, is applied to predict\nthe label probabilities of all classes, i.e.,\n$\\hat{y} = f_{cls} (h; \\theta_f),$\nwhere $\\theta_f$ is the classifier's learnable parameters.\n3.2. Structure Adversarial Net Framework\nAs previous propagation-based detection methods fail to gen-\neralize well for content-only samples, we design a Structure\nAdversarial Net (SAN) framework to learn a transferable fea-\nture representation between content and propagation."}, {"title": "4. EXPERIMENTS", "content": "4.1. Experimental Setups\nDatasets. We experiments on three real-world public datasets.\nPolitiFact and GossipCop are released by [19]. Samples are\ncollected from two fact-checking websites PolitiFact\u00b9 and\nGossipCop\u00b2. PolitiFact provides 157 fake news and 157 true\nnews; GossipCop provides 2,732 fake news and 2,732 true\nnews. PHEME-5 [20] contains tweets related to five different\nevents. It contains 581 true news and 230 fake news.\nTask Setups. We build two different cold-start propagation\nsettings. General cold start fake news detection aims to detect\ncontent-only fake news without considering specific events.\nWe choose PolitiFact and GossipCop, and follow the same\nprocedure as [12, 21] to split each dataset, i.e., randomly\nchoose 75% of the data as the training set and keep the rest\nas the test set. We further remove the propagation trees and\nonly retain the source news for each sample in the test set\nto achieve the cold-start propagation. Event-aware cold start\nfake news detection focus on detecting cold-start fake news\nfor the new event. We evaluate on PHEME-5, which contains\nnews from five specific events. We use one event's samples\nare used for testing, and all the rest are used for training.\nSimilarly, we further remove the whole propagation tree for\neach sample in the test set.\nBaselines. mGRU [14] and CSI [15] are RNN-based models\nto capture sequential patterns from retweet sequences. GC-\nNFN [22] models the propagation structure as a graph and\nuses graph convolutional networks (GCN) to encode the prop-\nagation. We implemented the model by removing profile in-\nformation for a fair comparison. GAT [23] applies graph at-\ntention networks to encode the propagation. BiGCN [8] em-\nploys two GCNs to model the propagation graph and disper-\nsion graph. UPSR [12] is a state-of-the-art model that recon-\nstructs latent propagation structure to explore more accurate\nand diverse structural properties. Besides, we also report a\nbaseline that only using the content of source news for detec-\ntion, denoted as Content, to evaluate the role of propagation\ntrees on fake news detection tasks. We extracted textual fea-\ntures by word2vec embeddings and then fed them into the\nMLP for classification.\nImplementation Details. For PolitiFact and GossipCop, we\nuse 300-dimensional word2vec vectors [24] provided by [25]\nas the input features of text contents. For PHEME-5, we ex-\ntract text embedding of each sentence by skip-gram with neg-\native sampling [26], and the dimension of input vectors is also\nset to 200. The dimension of hidden vectors is set to 64. A is\nsearched from {0.1, 1, 1.5, 2, 5, 10}. The learning rate is set\nto 0.001, 0.0005, and 0.005 for PolitiFact, GossipCop, and\nPHEME-5. We run each model with five random seeds and\nreport the average results of the test set.\n4.2. Task Analysis\nWe first quantitatively evaluate existing propagation-based\nmethods for two types of cold-start propagation. Results are\nshown in Fig. 2. For PolitiFact and GossipCop, we consider\na mixture of social events; and for PHEME-5, we consider\nthe event-specific setting. From results, when direct applying\npropagation-based methods to the new task, the results of\nall comparison models decrease to a varying degree in terms\nof metrics. The inferior results show that these models do\nnot generalize well to cold-start propagation. For the same\ndataset, the more complex the models, the greater the perfor-\nmance degradation. It may be because the complex model\nis prone to fuse excessive nontransferable structure features\nfrom the propagation tree. Once the propagation structure is\nmissing, the model cannot perform well.\nFigure 3 qualitatively visualize the sample's representa-\ntions on the training and testing set of PolitiFact with t-SNE\n[27]. We observe the inconsistent representation distribution\nof training samples and test samples. For the new task, it\nis critical to learn transferable patterns between content and\npropagation for the new task so that the model can adapt to\ndetect content-only samples without propagation structure.\n4.3. Effects of SAN Framework\nTable 1 and Table 2 summarize results of SAN applied to\npropagation-based methods for general and event-aware cold-\nstart fake news detection tasks. Methods that apply the SAN\nframework consistently outperform the corresponding base-\nline on all datasets for general and event-aware cold-start fake\nnews detection, which shows the effectiveness of SAN."}, {"title": "3.2. Structure Adversarial Net Framework", "content": "As previous propagation-based detection methods fail to gen-\neralize well for content-only samples, we design a Structure\nAdversarial Net (SAN) framework to learn a transferable fea-\nture representation between content and propagation.Based on the architecture of existing detection methods,\nSAN incorporates a structure discriminator to predict whether\nthe high-level representation of the target news includes struc-\nture properties. Given the hidden representation, we leverage\na full connection layer and a softmax function to predict la-\nbel probabilities $\u0177_d$ of the representation computing from the\npropagation structure, i.e.,\n$\\hat{y}_d = f_d(h; \\theta_d),$\n= 1\nwhere $\\theta_d$ is learnable parameters of the classifier. $y_d$\nrefers to the representation is learned from original samples\nwith content and propagation; $y = 0$ refers to the represen-\ntation is solely learned from content-only samples.\nDuring the training, the original feature extractor cooper-\nates with the fake news detector to carry out the major task of\nidentifying fake news. The classification loss $L_{CLS}(\\Theta, \\theta_f)$ is\ndefined as,\n$L_{CLS} (\\Theta, \\theta_f) = -y log(\\hat{y}) \u2013 (1 \u2013 y) log(1 \u2013 \\hat{y}),$\nwhere y is the ground-truth and $\u0177$ is prediction distribution.\nSimultaneously, the feature extractor tries to fool the struc-\nture discriminator to close the gap across distributions from\ncontents and propagation trees. The loss of the discrimina-\ntor captures the dissimilarities of feature representations from\ndifferent data types. It is defined as,\n$L_d(\u0398, \u03b8_d) = -y_d log(\u0177_d) + (1 - y_d)log(1 \u2013 \u0177_d),$\nwhere $y_d$ and $\u0177_d$ are the ground-truth and prediction labels\nthat describe whether the high-level representation of the\ntarget news includes structure properties, respectively. The\nlarger the loss, the lower the dissimilarities. Cooperating with\nthe fake news classifier $f_{cls}$ to minimize the cross-entropy\nloss, the final objective of optimization can be defined as,\n$L_{SAN} = L_{CLS}(\u04e8, \u04e8_f) \u2013 L_d(\u0398, \u03b8_d)$.\nThe gradient reversal layer [18] is added between encoder\nand the structure discriminator to achieve an adversarial ef-\nfect. Thus, the optimization of the model parameters are sum-\nmarized as follows:\n$\\Theta \\leftarrow \\Theta- \u03b7(\\frac{\u2202L_{CLS}}{\u2202\u0398} - \\lambda\\frac{\u2202L_d}{\u2202\u0398}),$\n$\u03b8_f \\leftarrow \u03b8_f- \u03b7(\\frac{\u2202L_{CLS}}{\u2202\u03b8_f}),$\n$\u03b8_d \\leftarrow \u03b8_d- \u03b7(\\frac{\u2202L_d}{\u2202\u03b8_d}),$\nwhere n is the learning rate. In the implementation, samples\nin the training set are processed into two copies. One is the\nfull samples with both content and propagation, denoted as\n$D^{train}$, and the other only contains cold-start samples lacking\nthe whole propagation, denoted as $\u010e^{train}$. We adopt the objec-\ntives of SAN for both of them, i.e.,\n$L = L_{SAN}^{D^{train}} + \u03bbL_{SAN}^{\u010e^{train}},$\nwhere A is a trade-off hyper-parameter to control weights of\nconsidering cold-start samples during the training."}, {"title": "5. CONCLUSION", "content": "This paper focuses on the generalization of absence of the\nwhole propagation and explores a new practical fake news\ndetection task towards cold-start propagation, aiming to iden-\ntify the content-only news by exploiting previously contents\nand propagation. For the task, we design a simple but effec-\ntive SAN framework to transfer the propagation patterns to\nthe content-only samples. Experiments show the poor gener-\nalization of existing propagation-based models and the effec-\ntiveness of SAN for two types of cold-start propagation."}]}