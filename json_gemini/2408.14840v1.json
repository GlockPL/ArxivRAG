{"title": "CL4KGE: A Curriculum Learning Method for Knowledge Graph Embedding", "authors": ["Yang Liu", "Chuan Zhou", "Peng Zhang", "Yanan Cao", "Yongchao Liu", "Zhao Li", "Hongyang Chen"], "abstract": "Knowledge graph embedding (KGE) constitutes a foundational task, directed towards learning representations for entities and relations within knowledge graphs (KGs), with the objective of crafting representations comprehensive enough to approximate the logical and symbolic interconnections among entities. In this paper, we define a metric Z-counts to measure the difficulty of training each triple (<head entity, relation, tail entity>) in KGs with theoretical analysis. Based on this metric, we propose CL4KGE, an efficient Curriculum Learning based training strategy for KGE. This method includes a difficulty measurer and a training scheduler that aids in the training of KGE models. Our approach possesses the flexibility to act as a plugin within a wide range of KGE models, with the added advantage of adaptability to the majority of KGs in existence. The proposed method has been evaluated on popular KGE models, and the results demonstrate that it enhances the state-of-the-art methods. The use of Z-counts as a metric has enabled the identification of challenging triples in KGs, which helps in devising effective training strategies.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graph Embedding (KGE) is a powerful technique for modeling and comprehending complex knowledge graphs. KGE methods aim to learn low-dimensional representations of entities (nodes) and their relationships (edges) in a knowledge graph. The goal is to encode the structural and semantic information present in the graph into the low-dimensional representations, which are then used for downstream applications such as reasoning (Fang et al., 2023), recommendation system (Zhang et al., 2016; Wang et al., 2018; Ma et al., 2019; Wang et al., 2019; Lin et al., 2023; Zhang et al., 2024), question answering (Huang et al., 2019; Lukovnikov et al., 2017), and querying (Chen et al., 2022; Lin et al., 2019). A significant number of contemporary KGE models, as encapsulated by the prevailing paradigm (Kamigaito & Hayashi, 2022), frequently overlook the heterogeneity in training difficulties encountered among triplets, as well as the potential richness of information that is embedded within them. This oversight may limit the depth of learning and the effectiveness of the resulting embeddings in capturing complex relationships. This observation has sparked our interest in delving into the curriculum learning dilemma within KGE.\nCurriculum learning (CL) is a training strategy proposed by Bengio (Bengio et al., 2009) for learning tasks with difficult training samples, which is to rank a sequence of samples from easy learning to difficult learning. Since the easy learning samples carry more useful information, curriculum learning enables task learning more efficiently and effectively. However, from the viewpoint of KEG (Bordes et al., 2013; Sun et al., 2019), the training samples are triplets that are not independent but closely related together. An effective way is to rank these training triplets from easy to difficult. The easy-training triplets refer to the triplets with recognizable feature representations. It is easier for KGE models to identify a precise decision boundary from easy triples. On the contrary, the difficult triplets refer to the ones without recognizable characteristics. This kind of triplet potentially confuses the model convergence and prevents a KGE model from successfully training a good model. Consequently, crafting a metric capable of gauging the difficulty of samples within a knowledge graph, particularly one abundant in intricate topology and semantic framework, presents a formidable challenge.\nTo address this challenge, this paper proposes a new Curriculum Learning method catered for Knowledge Graph Embedding (CL4KGE). In particular, we introduce a metric named Z-counts to quantify and evaluate the difficulty of triplets. This metric enables the ranking of training triplets, thereby guiding the training process to progress from simpler to more complex triplets. Furthermore, we propose a curriculum learning strategy that consists of two main components. The first component involves a difficulty assessment based on Z-counts, while the second component features a training scheduler that leverages the structure of the knowledge graph. Our empirical analysis includes extensive experiments on various benchmark datasets, and the findings highlight the superior performance of our proposed method, showcasing a significant improvement over existing approaches.\nThe main contributions of this work are summarized as follows:\n\u2022 Difficulty Metric: Z-counts. We propose a new metric Z-counts to measure the training- triplet difficulty in a knowledge graph. We theoretically analyze the metric and explain how to solve the training difficulty based on the Z-counts (see Definition 3).\n\u2022 Curriculum Learning: CL4KGE. We design a curriculum learning method CL4KGE (see Algorithm 1) for KGE based on Z-counts. We demonstrate that the framework can scale well with the increasing number of relations in a knowledge graph. By ranking training triplets in a difficulty-ascending manner, our method can improve KGE methods without increasing time complexity.\n\u2022 Experimental Evaluation. We demonstrate the superiority of the proposed method through extensive experiments on tasks including link prediction and triple classification on various datasets. Experimental results show that CL4KGE used as a plugin successfully enhances the popular KGE models and achieves state-of-the-art results."}, {"title": "2 Related Works", "content": "In this section, we summarize the previous works about knowledge graph embedding and curriculum learning. To the best of our knowledge, this is the first attempt to design a curriculum learning framework for knowledge graph embedding."}, {"title": "2.1 Knowledge Graph Embedding.", "content": "Knowledge Graph Embedding (KGE) is a powerful tool for modeling and understanding complex knowledge graphs. Typical models include TransE (Bordes et al., 2013), TorusE (Ebisu & Ichise, 2018), RotatE (Sun et al., 2019), ConvE (Dettmers et al., 2018), MQuadE (Yu et al., 2021), RESCALE (Nickel et al., 2011), DistMult (Yang et al., 2014), and ComplEx (Trouillon et al., 2016). KGE methods aim to learn low-dimensional representations of entities (nodes) and relations (edges) in"}, {"title": "2.2 Curriculum Learning", "content": "Curriculum learning (Bengio et al., 2009) is a powerful technique for training machine learning models, especially in complex tasks. Providing a sequence of tasks to a model allows it to learn more efficiently and effectively. Knowledge Graphs provide a structured way to represent knowledge, easily accessible to machines. This makes them especially well suited for use with curriculum learning. Curriculum learning enhances generalization ability and directs the model toward a better parameter space, according to previous works (Bengio et al., 2009; Weinshall & Amir, 2020) in various domains including Graph (Wang et al., 2021; Wei et al., 2022; Liu et al., 2023), NLP (Xu et al., 2020; Cirik et al., 2016), CV (Zhang et al., 2021; Almeida et al., 2020), Medical (Burduja & Ionescu, 2021; Liu et al., 2022a; Alsharid et al., 2020), Speech (Ristea & Ionescu, 2021; Wang et al., 2020; Zhang et al., 2019a), even Robotics (Florensa et al., 2017; Milano & Nolfi, 2021; Manela & Biess, 2022). To the best of our knowledge, no work has yet attempted to apply curriculum learning to link prediction in knowledge graph."}, {"title": "3 Problem Setup and Background", "content": "In this section, we state the problem formulation and provide a succinct theoretical basis for the problem we study, including some preliminaries of knowledge graph and mathematics statement of curriculum learning. Additionally, we go through the notations involved in this work in Table 2.1. We denote scalars, vectors and matrices with lowercase letters, bold lowercase letters, and bold uppercase letters, respectively."}, {"title": "3.1 Knowledge Graph Embedding", "content": "A knowledge graph can be described as a set of triplets (head entity, relation, tail entity) denoted as {(h,r,t) | h, t \u2208 E, r \u2208 R}. As for training strategy, KGE methods would introduce a score function (Socher et al., 2013) defined in Definition 1 for each triple, and the loss function is composed for the model update."}, {"title": "Definition 1 (Score Function).", "content": "For each triplet (h, r, t) in a knowledge graph H, the score function f : Rd \u00d7 Rd \u00d7 Rd \u2192 R+ \u222a {0} is a non-negative real-valued function. We define it as:\n\n$f(h, r, t) = \\begin{cases} 0, & \\text{if } (h, r, t) \\text{ holds,} \\\\ +\\infty, & \\text{otherwise,} \\end{cases}$\n\nwhere h, r, and t are the embeddings of entities h, r, and t, respectively, and r is a relation function mapping from Rd \u00d7 Rd to R+ U {0}.\nGiven two elements in a triplet (h, r, t) with a score function f (h, r, t), the task of link prediction is to add the most logical element with the highest confidence level in the remaining position. Translation distance-based approaches often describe relations using geometric transformations of entities, and they assess the plausibility of fact triples by comparing the distances between entity embedding following relation transformations. Also, each translation distance-based KG method will define a unique score function. However, deep learning-based KGE methods follow a different way to encoder the triplet which could use some popular neural networks including CNNs (Dettmers et al., 2018; Nguyen & Grishman, 2015), RNNs (Zhang & Wang, 2015), and GNNs (Vashishth et al., 2020).\nRemark 3.1. The task of link prediction involves predicting the missing elements in such triplets to infer new facts and relationships in the knowledge graph. Specifically, if either h or t is missing, the goal is to identify the most plausible entity that completes the triplet based on the embeddings and learned patterns in the graph. The primary motivation for link prediction is that knowledge graphs are typically incomplete; not all possible relationships between entities are known or present. Therefore, link prediction aims to discover these missing links, enhancing the knowledge graph's completeness and utility. For instance:\n\u2022 Predicting the tail entity t given (h, r, ?)\n\u2022 Predicting the head entity h given (?, r, t)\nThe plausibility of a predicted link is usually quantified using a scoring function that assesses how likely a given triplet (h, r, t) is to be true within the learned embedding space of the knowledge graph."}, {"title": "3.2 Curriculum Learning", "content": "Curriculum learning (CL) is a training strategy that mimics the suitable learning sequence seen in human curricula by training a machine learning model from easy to difficult data. By utilizing a curriculum to train the model, curriculum learning reduces the negative effects of challenging samples. A curriculum sequence contains a series of training criteria < Q1, \u2026\u2026\u2026, Qt,\u2026\u2026\u2026,QT > over T epochs. Each training criteria Qt is a subset of the training set. With the increase of training epoch, the number of hard samples in Qt is gradually increased. We design the curriculum learning framework in two parts, difficulty measure and training scheduler, which will be introduced in details in Section 4.1 and Section 4.2 respectively. In our framework, the Z-counts ranks the difficulty of each triplet in the training epochs, and the training scheduler generates Qi for each batch to train the model. The overview of our proposed framework can be found in Figure 2.1.\nBefore we introduce the details of our proposed curriculum learning for knowledge graph embedding, we decide to introduce the original CL framework. For a more rigorous presentation and easy understanding, we give the original definition of CL proposed by Bengio (Bengio et al., 2009).\nDefinition 2 (Curriculum Learning (Bengio et al., 2009)). A curriculum is a sequence of training criteria over T training steps: C =< Q1, \u2026, Qt,\uff65\uff65\uff65, QT > and each criterion Qt is a re-weighting of the target training distribution P(z):\nQt x W\u2081(z)P(z) \u2200 sample z \u2208 Ttraining\nAlso, these should satisfy the following three conditions:\n\u2022 The entropy of distributions gradually increases, H(Qt) < H(Qt+1),\n\u2022 The weight for any example increases, W\u2081(z) \u2264 Wt+1(z) \u2200 sample z \u2208 Ttraining,\n\u2022 QT(z) = P(z)."}, {"title": "4 Method: CL4KGE", "content": "This section provides a comprehensive exposition of our proposed framework CL4KGE, which is intricately grounded in curriculum learning principles. It comprises two integral components: a difficulty measurer (refer to Section 4.1) and a training scheduler (refer to Section 4.2)."}, {"title": "4.1 Difficulty measurer: Z-counts", "content": "In this section, we introduce the metric Z-counts used to measure the quality of the triplets, whose intuitive idea is shown in Figure 3.1, before we present the details of the proposed CL4KGE framework. Our strategy's main idea is to enhance the performance of the backbone KGE methods"}, {"title": "Definition 3 (Z-path, Z-counts).", "content": "For each triplet (h, r, t) in a knowledge graph H, we define a Z-path and the corresponding Z-counts as follows: A Z-path exists if there are intermediate entities e\u2081 and e2 such that the following triplets are present in the knowledge graph: (h, r, e\u2081), (e1, r, e2), and (e2, r, t). The Z-counts for a triplet (h, r, t) is the total number of such Z-paths.\nDefinition 4 (Separable function). A function F(x1,x2...,xn), defined on a region D C R\", is said to be separable with respect to the variable x1 if it can be written as a summation of two functions, defined on D, with one function G depending only on x1, whereas the other function H is independent of x1, i.e.\n\n$F(x_1,x_2...,x_n) = G(x_1) + H(x_2...,x_n)$\n\nProposition 4.1. Given a KGE method with a score function r(h, t) which is separable respect to h and t, we have r(h, t) = 0 if there exists Z-path between h and t.\nProof. If r(h, t) is separable, we have r(h, t) = r1(h) + r2(t) for some r\u2081(\u00b7) and r2(\u00b7). For the four entities h, e2, e3, t, assuming that (h, r, e\u2082), (e3, r, e\u2082), (e3, r, t) hold, now we prove that it is very likely (h, r, t) also holds.\nLet h, e2, e3, t be the embeddings of h, e2, e3, t. Since (h, r, e\u2082) holds, we have\nr(h, e\u2082) = r\u2081 (h) + r2(e2) \u2248 0,\nand similarly,\nr1(e3) + r2(e2) \u2248 0, r1(e3) + r2(t) \u2248 0.\nThen it follows that\nr(h, t) = r\u2081(h) + r2(t)\n= [r\u2081(h) +r2(e\u2082)] - [r\u2081(es) + r2(e\u2082)] + [r\u2081(es) + r2(t)]\n\u2248 0\ni.e., (h, r, t) holds which completes the whole proof.\nRemark 4.1. It is important to note that the separability condition (refer to Definition 4) for score functions is relatively easy to satisfy and not overly restrictive. Specifically, we can rewrite the score functions of TransE and Rotate to conform to the separable condition. For instance, sTranse (h, r,t) =\nh+r-t = (h + r) - t = f\u2081(h,r) - g(t,r) and sRotate(h, r,t) = hor- t = (hor) - t =\nf2(h,r) - g(t,r), where f\u2081(h,r) = h + r, f2(h,r) = hor, and g(t,r) = t. We provide a list\nof typical methods with separable score functions in Table 4.1. Additionally, Knowledge Graph\nEmbedding (KGE) models with bilinear score functions can satisfy the separable condition if\n2hRt = - Rt - h2 + |h|2 + Rt 2. Consequently, several KGE models, including DisMult,\nComplex, DihEdral, QuatE, SEEK, and Tucker, also meet this condition.\nRemark 4.2. For any entities h and t, the larger the Z-counts between h and t is, the more likely that the entities h and t are connected by this KGE method. The proof shows that if there exists at least one Z-path (i.e., Z-counts > 0), then the score function r(h, t) is likely close to zero, indicating a strong likelihood that (h, r, t) is a valid triplet. This aligns with the statement because a higher Z-count means there are more Z-paths, which collectively increase the likelihood that r(h, t) \u2248 0.\""}, {"title": "4.2 Training scheduler", "content": "After the difficulty measure module which measures the difficulty of each sample in training set Ttrain, we demonstrate a curriculum learning framework to train a better KGE method."}, {"title": "4.2.1 Pacing Functions", "content": "We propose a framework to generate the easy-to-difficult curriculum based on a continuous training scheduler. We follow the previous work (Wei et al., 2022) to design these functions which can be viewed as intensity functions \u5165(\u00b7) to map the training epoch to a scale between 0 and 1, i.e.\n\u5165(t) : N \u2192 (0, 1]. According to the Definition 2, we choose the functions as our pacing function summarized in Table 4.6."}, {"title": "4.2.2 Pseudo Code and Complexity Analysis", "content": "In this subsection, we first show the pseudo-code shown in Algorithm 1. The process of CL4KGE is detailed in Algorithm 1. Lines 1-2 describe the input and output of the algorithm. Lines 4\u20136 describe the Z-counts based difficulty measurer and lines 7-15 describe the process of training the backbone KGE methods with a curriculum framework. As the pseudo-code shows, CL4KGE is easy to implement and can be directly plugged into any backbone KGE method, as it only changes the training set in each training epoch (lines 11\u201314).\nFor the complexity analysis of Algorithm 1, we mainly focus on the pre-processing of it - the calculations of Z-counts of training samples. For a given knowledge graph H, the space complexity is O(Ttrain) and the time complexity slightly less than O(Ttrain \u00d7 E2). Since this process can obviously be handled in parallel, the time complexity could be O(E). Even for the large-scale benchmark dataset in Table 4.4, we only spend just a few hours on a laptop computing Z-counts and sorting them. As for the backbone (lines 11-14 in Algorithm 1), the complexity is the same as"}, {"title": "5 Experiments", "content": "In this section, we conduct our experiments to demonstrate the efficiency of CL4KGE. First, we introduce the basic benchmark datasets we use in our work in Section 5.1. Then we present a brief introduction of our compared baselines and the experimental setup in Section 5.2 and Section 5.3. Lastly, we will give an overview of our experimental results and discussions in the next section."}, {"title": "5.1 Datasets", "content": "We select the benchmark datasets used in the knowledge graph domain. The following are some descriptions of datasets with statistical information summarized in Table 4.4. These datasets are divided into train set, validation set, and test set according to the 8:1:1 ratio.\nFB15k-237 FB15k-237 (Toutanova & Chen, 2015) is a subset of the Freebase (Bollacker et al., 2008) knowledge graph which contains 237 relations. The FB15k (Bordes et al., 2013) dataset, a subset of Free-base, was used to build the dataset by (Toutanova & Chen, 2015) to study the combined embedding of text and knowledge networks. FB15k-237 is more challenging than the FB15k dataset because FB15k-237 strips out the inverse relations.\nWN18 WN18 is a subset of the WordNet (Miller, 1995), a lexical database for the English language that groups synonymous words into synsets. WN18 contains relations between words such as hypernym and similar_to.\nWN18RR WN18RR (Dettmers et al., 2018) is a subset of WN18 that removes symmetry/asymmetry and inverse relations to resolve the test set leakage problem. WN18RR is suitable for the examination of relation composition modeling ability.\nCountries Countries dataset (Bouchard et al., 2015) consists of 244 countries, 22 subregions (e.g., Southern Africa, Western Europe), and 5 regions (e.g., Africa, Americas). Each country is located in exactly one region and subregion, each subregion is located in exactly one region, and each country can have a number of other countries as neighbors."}, {"title": "5.2 Baselines", "content": "Within the domain of knowledge graphs, a diverse array of methodologies has been developed, including DistMult (Yang et al., 2014), TransE (Bordes et al., 2013), DihEdral (Yang et al., 2014), QuatE (Zhang et al., 2019b), TuckER, RotatE (Sun et al., 2019), ComplEx (Trouillon et al., 2016), MQuadE (Yu et al., 2021), DensE (Lu & Hu, 2020), and HousE (Li et al., 2022). We employ these well-established methods as Knowledge Graph Embedding (KGE) backbones to evaluate whether our proposed framework enhances their performance."}, {"title": "5.3 Experimental Setup", "content": "Loss function For the convenience of training, we use the same loss function and optimization method to learn the parameters. To learn the model parameters, we apply the self-adversarial loss function suggested in MQuadE (Yu et al., 2021),\n\n$L = - log \\sigma(\\gamma - f(h,r,t)) - \\sum_{i=1}^{K} p_i log \\sigma(f(h', r, t'_i) - \\gamma)$\n\nwhere y > 0 is a pre-defined margin, o is the sigmoid activation function, (h', r, t') is the i-th negative triple and pi is the weight of the responding sample (h, r, t) as the following equation:\n\n$p_i = \\frac{exp(\\alpha(\\gamma - f(h, r, t'_i)))}{\\sum_{k=1}^{K} exp(\\alpha(\\gamma - f(h, r, t'_k)))}$\n\nwhere a \u2208 [0, 1] is the self-adversarial temperature.\nFor the baselines, we adopt the stochastic mini-batch optimization methods to minimize the above loss function (see Equation (3)) and learn the models' parameters. Also, we use the adam optimizer for parameter learning.\nEvaluation metrics We use the mean reciprocal rank (MRR), mean rank (MR), Hit@1, Hit@3, and Hit@ 10 metrics for evaluation and convenience comparison. Also, we use the filtered setting following TransE (Bordes et al., 2013) which means the triplets that appear in the training and validation set are removed from the testing set."}, {"title": "5.4 Results", "content": "In this section, we provide the results and some discussions for Section 5."}, {"title": "5.4.1 Link Prediction", "content": "Table 5.2, Table 5.3 and Table 5.4 illustrate the performance improvements of existing state-of-the-art KGE methods when integrated with the CL4KGE framework across different datasets (FB15k-237, WN18, and WN18RR). These tables collectively demonstrate the effectiveness of the curriculum learning method in enhancing various KGE models. In Table 5.2, the results on the FB15k-237 dataset show significant improvements in performance metrics (MRR, MR, Hits@1, Hits@3, and Hits @10) for models incorporating the CL4KGE framework. For instance, the Hits@10 score for the TransE"}, {"title": "5.4.2 Inferring Relation Patterns", "content": "In this subsection, we show the results on Countries datasets including countries_S1, countries_S2, and countries_S3. The queries in Countries are of the type locatedIn(c, ?), and the answer is one of the five regions, unlike link prediction task on knowledge graph. There are three jobs in the Countries dataset, each of which demands inferring an increasingly complex and lengthy composition pattern."}, {"title": "5.4.3 Ablation Study", "content": "In this subsection, we conduct ablation studies to demonstrate the choice of pacing functions in CL4KGE. In Table 5.5 and Table 5.6, we evaluate the sensitivity of CL4KGE to three pacing functions: linear, root, and geometric. We compare the Hit@10 metric on the link prediction task utilizing these three pacing functions while employing TransE (Bordes et al., 2013) as the backbone of the KGE. On the most of benchmark datasets, we discover a slight advantage for the geometric pacing function."}, {"title": "6 Conclusion", "content": "In this study, we propose a curriculum framework named CL4KGE for knowledge graph embedding, underpinned by theoretical foundations. Our approach is inspired by the Z-shape phenomenon observed in knowledge graphs, leading us to develop a novel difficulty metric termed Z-counts. Utilizing Z-counts, we have designed a data-driven training scheduler. We conducted experiments on multiple knowledge graph benchmarks, and the results unequivocally demonstrate the efficacy of our framework."}]}