{"title": "Find the INTENTION OF INSTRUCTION: Comprehensive Evaluation of Instruction Understanding for Large Language Models", "authors": ["Hyeonseok Moon", "Jaehyung Seo", "Seungyoon Lee", "Chanjun Park", "Heuiseok Lim"], "abstract": "One of the key strengths of Large Language Models (LLMs) is their ability to interact with humans by generating appropriate responses to given instructions. This ability, known as instruction-following capability, has established a foundation for the use of LLMs across various fields and serves as a crucial metric for evaluating their performance. While numerous evaluation benchmarks have been developed, most focus solely on clear and coherent instructions. However, we have noted that LLMs can become easily distracted by instruction-formatted statements, which may lead to an oversight of their instruction comprehension skills. To address this issue, we introduce the INTENTION OF INSTRUCTION (IOINST) benchmark. This benchmark evaluates LLMs' capacity to remain focused and understand instructions without being misled by extraneous instructions. The primary objective of this benchmark is to identify the appropriate instruction that accurately guides the generation of a given context. Our findings suggest that even recently introduced state-of-the-art models still lack instruction understanding capability. Along with the proposition of IOINST in this study, we also present broad analyses of the several strategies potentially applicable to IOINST. 1.", "sections": [{"title": "1 Introduction", "content": "The utilization of large language models (LLMs) has emerged as a prominent trend across numerous research domains (Biswas, 2023b,a; Peng et al., 2023; Taori et al., 2023; Zhou et al., 2023a). A notable feature of LLMs is their ability to interact with humans by appropriately responding to user instructions (Zhou et al., 2023b; Qin et al., 2024; Chen et al., 2024; Li et al., 2024; Xu et al., 2023). With any given instructions, LLMs are expected to generate responses that align with these instructions (Chen et al., 2024; Li et al., 2024; Xu et al., 2023; Longpre et al., 2023). This capability, known as the \"instruction following\" ability, serves as a key metric for assessing the effectiveness of LLMs (Chen et al., 2024; Zhao et al., 2024; Dubois et al., 2023; Zheng et al., 2023). To facilitate a more thorough assessment, several benchmarks have been introduced with a focus on instruction following. (Zhou et al., 2023b; Qin et al., 2024; Geng et al., 2023). However, we have identified a blind spot in these evaluation methods: they primarily focus on the ability to follow coherent and clear instructions. We observed that LLMs can become distracted when faced with instruction-formatted statements that diverge from user intent. For instance, when given the statement: *Paraphrase the following statement: \"Write a creative poem about a turtle\"*, LLMs often respond to the instruction *\"Write a creative poem\"* rather than the intended task of *\"Paraphrase the following statement.\"* Previous research often dismissed these instances as mere errors. This observation leads us to question whether LLMs respond intuitively to instruction-formatted statements rather than relying on a deep understanding of the instructions. Given that several studies focus on manipulating instructions using LLMs, such as through instruction optimization (Fernando et al., 2024; Yang et al., 2024), the ability to maintain focus without being distracted by other instructions is considered a vital competency for LLMs. To clarify this capability and further analyze and evaluate the instruction understanding ability of LLMs, we propose the INTENTION OF INSTRUCTION (IOINST) benchmark. The primary objective of IOINST is to identify the appropriate instruction that accurately instructs to generate a given context. IOINST provides four *candidate instructions*: one *label instruction* and three *contrastive instructions*, along with a *context* that can be generated by following the label instruction. Then, LLMs are required to select one among them. We denote the instruction that contains the actual intention (requests to select one among candidate instructions) as *\"meta-instruction.\"* IOINST aims to evaluate two key capabilities of LLMs. Firstly, we assess the ability to comprehend the prerequisites for generating a response. To accurately select one of the four candidate instructions, the model must discern the intent embedded in the instructions and correctly match it with the response. Secondly, we evaluate the ability to distinguish user intention among instruction-formatted statements. The model should focus on selecting the correct instruction from the four given candidate instructions without being distracted by other instruction-formatted statements. In this benchmark, we intend for the model to *\"choose one of the four instructions.\"* Therefore, any action contrary to this\u2014such as following a candidate instruction or generating an instruction not provided\u2014will be considered a deficiency in the model's performance. To ensure a comprehensive evaluation, we define three different data types: *Random*, which involves arbitrary contrastive instructions; *Semantic*, which comprises semantically confusing contrastive instructions; and *Anti-Attribute*, which requires an understanding of the finer correlation between response and each instruction. Additionally, we introduce three evaluation metrics, allowing for a more detailed inspection of the LLM's ability to understand instructions. Through extensive experiments with several instruction-tuned LLMs, we reveal that most LLMs struggle to grasp the intention embedded in the instruction. Notably, several LLMs, including Mistral (Jiang et al., 2023) and Gemma (Team, 2024a), seem to not fully comprehend the instruction and frequently respond to the instruction-formatted statement in the given input (i.e. candidate instruction). These findings highlight that even recently introduced state-of-the-art models still lack instruction understanding capability, suggesting areas for future improvement. Along with the proposition of IOI in this study, we also present broad analyses of the several strategies potentially applicable to IOI. With the rapid advancements in LLM (Taori et al., 2023; Peng et al., 2023; Jiang et al., 2023; Kim et al., 2024), several attempts to objectively assess the performance of LLMs have been introduced. These efforts include methodologies such as A/B testing (Wang et al., 2024b; Zeng et al., 2024; Quin et al., 2024) or evaluations based on quantified scores (Wang et al., 2023)."}, {"title": "2 Related Works", "content": "The concept of instruction-following has recently been incorporated to facilitate more objective evaluations (Zhou et al., 2023b; Dubois et al., 2023; Qin et al., 2024). The effectiveness of this approach lies in the potential to objectively quantify the quality of outputs, which might otherwise be deemed subjective (Zhou et al., 2023b; Zheng et al., 2023). During instruction-following evaluation, we provide specific instructions and assess whether the generated output aligns well with the intended content of the instruction. Adherence to instruction is generally verified either through a rule-based measurement (Zhou et al., 2023b; Xia et al., 2024) or by employing a super-LLM evaluator (such as GPT-4 (Team, 2024b)) on the generated output (Qin et al., 2024). However, these benchmarks often neglect the ability of LLMs to distinguish between several instruction-formatted statements. Current evaluations focus on how LLMs respond to clear and coherent instructions (Zhang et al., 2023; Dubois et al., 2023; Zeng et al., 2024), which may result in high performance without a deep understanding of the underlying intention. Therefore, we propose a more structured benchmark to assess the fundamental ability of LLMs to comprehend given instructions."}, {"title": "3 INTENTION OF INSTRUCTION", "sections": [{"title": "3.1 Task Definition", "content": "The primary objective of IOINST is to identify an appropriate instruction that leads to generating a given context. We provide LLMs a context along with four *candidate instructions*, among which one instruction correctly instructs to generate the given context, while others are not. IOINST requires LLMs to select one correct candidate instruction among the given four. Such task objective is defined in the *meta-instruction* and is also fed to the LLM during the evaluation procedures."}, {"title": "3.2 Data Composition", "content": "*Candidate instructions* are composed of a *label instruction* configured to generate *context*, along with three *contrastive instructions* that serve as distractors. Through quality inspection, we ensured that contrastive instructions were designed to avoid generating *context*. We construct three different types of contrastive instruction, each distinguished by the required level of understanding. Details about contrastive instructions are described in Section 4.2. *Context* is a generation result by LLM, constructed as a response to the label instruction. Through carefully designed contrastive instructions, we ensure that none of the contrastive instructions align with the context. *Meta-instruction* instructs LLMs to select the most appropriate candidate instruction. We construct 16 different meta-instructions and randomly select one for each evaluation phase. We also define two distinct characteristics that can subdivide meta-instructions into two separate groups. The first criterion is the level of details that can partition meta-instructions into *Detailed* group (instruct detailed step-by-step procedures) and *Simple* group (provide straightforward task directives). The second is the presenting order between the context and candidate instructions, which can define two groups: *Candidate-First* and *Context-First*. By defining diverse meta-instructions, we investigate the capability of instruction understanding affected by meta-instruction."}, {"title": "3.3 Potential Error Cases", "content": "We define two types of error cases that LLMs may encounter and evaluate their instruction understanding capability based on the frequency of these cases. *Case 1* pertains to an error case that chooses the incorrect candidate instruction from the given four instructions. In this case, we can see that LLM faithfully followed meta-instruction, while LLM has not fully understood the requirements for crafting the response. *Case 2* involves a failure to comprehend the user intention embedded in the meta-instruction. Such error cases include responding to the candidate instruction rather than following meta-instruction. Our objective is to select one of four options; therefore, generating anything outside of this purpose, including suggesting alternatives or refusing to provide an answer, is considered a Case 2 error."}, {"title": "3.4 Evaluation Measure", "content": "For each data point in IOINST, we examine the generation output of the LLM instead of estimating the sequence generation probability, as Case 2 errors can only be assessed during the generation phase. We construct the benchmark to allow evaluation without external judges, such as human evaluators or LLM assessments (Liu et al., 2023). Specifically, we define three metrics, including $ACC1$, $ACC2$, and $ACC_{rel}$, for a detailed assessment.\n*   $ACC1$: This metric estimates strict accuracy of the intended goal; among four candidate instructions, the LLM correctly selects label instruction.\n*   $ACC2$: This metric estimates how accurately the LLM catches the given instruction's underlying intention; the LLM selects one of four candidate instructions whichever.\n*   $ACC_{rel}$: This metric measures how accurately the LLM selects the correct instruction, relative to the number of times it follows the meta-instructions. This metric estimates the LLM's ability to understand instructions while avoiding underestimation due to Case 2.\nFor each model generation X, we define the evaluation measure for each case as in Equation (1). We denote candidate instruction set $\\{candi\\} = \\{label\\} \\cup \\{contrastive\\}_{i=1}^{3}$, where *label* and contrastive are *label* and *contrastive instruction*, respectively.\n$\nACC1 = \\frac{1}{|D|} \\sum_{X \\in D} [EM_{sparse}(X, label)]\n$\n$\nACC2 = \\frac{1}{|D|} \\sum_{X \\in D} \\bigcup_{i=1}^{4} [EM_{sparse}(X, candi)]\n$\n$\nACC^{rel} = ACC1 / ACC2\n$ (1)\nIn particular, we introduce a measure called $EM_{sparse}$ for more accurate evaluation. This approach is adopted to overlook subtle differences included in the LLM-generated output and evaluate them on a similar basis. It examines the ROUGE-L (Lin, 2004) precision between the generated output and the reference. Then, we regard a generated output that surpasses a threshold $\\tau$ as a correct result and subsequently estimate the overall accuracy of INTINST. In this study, the threshold $\\tau$ was empirically set to 0.9."}]}, {"title": "4 Data Construction", "sections": [{"title": "4.1 Data Curation", "content": "To gather high-quality instructions and their corresponding LLM responses, we curated several datasets released with the LLM-as-evaluator approach (Zeng et al., 2024; Zheng et al., 2023; Chia et al., 2024). These datasets predominantly contain two model responses for each instruction\u2014one that accurately follows the instruction and one that does not. In this case, we can ensure that one of the model responses is guaranteed to be well-aligned with the instruction. Using these resources, we reorganized the data to fit our objectives. We secured high-quality instructions along with their guaranteed instruction-aligned responses by extracting the labeled responses from the given datasets. We regard the instruction-followed model response obtained from these datasets as a *context*, and corresponding instruction as a *label instruction* for *candidate instructions*. In particular, through pilot experiments, we noticed lengthy context within input statements could degrade the context comprehension of LLMs, which leads to an underestimation of $ACC1$ and $ACC_{rel}$. We also found that LLMs are prone to follow excessively long candidate instructions, wherein $ACC2$ deteriorates. Considering these, we extract data that do not exceed the 1,000 and 300 character lengths for the context and candidate instructions, respectively. Along with constructing label instruction and context pair, we also compile instructions from these sources (Qin et al., 2024; Chia et al., 2024). The complicated instruction set is denoted as *candidate pool*, and serves as a resource during the construction of *Pool-based contrastive instructions* (Detailed in Section 4.2). The list of datasets utilized for the construction of IOINST is detailed in Table 1. More detailed characteristics of those datasets are demonstrated in Appendix E.1."}, {"title": "4.2 Contrastive Instruction: Pool-based", "content": "We then establish three contrastive instructions for each label instruction to act as distractors for LLM. To more precisely assess the instruction understanding capability of LLMs, we propose three different variants of contrastive instruction. Firstly, we introduce a method to construct contrastive datasets using a candidate pool, creating two types of dataset variants. The detailed procedures are described in Figure 2. *Random Contrastive*: For the random contrastive, we randomly extract three different instructions for each data point and utilize them as contrastive instructions. Such dataset type is introduced to assess the LLM's low-level capability of understanding instructions. *Semantic Contrastive*: For constructing semantic contrastive, we extract instructions that are semantically similar to the label instruction but lead to different outcomes. We employ a sentence embedding model MPNet (Song et al., 2020) to determine semantic similarity. We extract the most semantically similar three instructions to compose contrastive instructions. LLMs are then obliged to differentiate and select the instruction that most accurately reflects the requirements of semantically confusing candidates. *Quality Assurance* In the pool-based contrastive instructions we designed, there is a possibility of including ambiguous instructions that prompt the generation of a context. To prevent this, we eliminated all such ambiguous scenarios through human inspection. We conducted evaluations using GPT-40 (Hurst et al., 2024), directly reviewing failure cases arising from the model. Suppose these failure cases are incurred by any contrastive instruction that leads to generating a context. In that case, we re-generated such contrastive instruction to ensure our benchmark remained clear and solvable enough. By conducting this error verification process three times, we ensured the dataset was free from errors caused by contrastive instructions misguiding the labels. Supporting this validation, GPT-40 achieved an accuracy of 99.54, demonstrating that solving this task is feasible given sufficient model performance."}, {"title": "4.3 Contrastive Instruction: Anti-Attribute", "content": "To more precisely evaluate whether the LLM comprehends the detailed requirements of instructions, we design heuristic-based Anti-Attribute contrastive instructions. Firstly, we examine the attribute of the given context as listed in Table 2. Every attribute can undoubtedly be detected via simple rule-based methodologies, and is established by referring criteria presented by Zhou et al. (2023b). We then generate aligned and unaligned instructions based on these analyzed attributes, as depicted in Table 9. We crafted human-made instructions for each attribute and paraphrased them to generate 10 variants via GPT-40. Prompt employed to paraphrasing is described in Appendix E.3, and any awkward expressions in the generated statements were manually corrected. We designed 10 templates for each attribute and constructed the dataset by randomly applying one of these templates. Each aligned and unaligned instructions is concatenated with the original label instruction to construct candidate instructions. In this sense, contrastive instructions share key requirements with the label instruction (e.g. *Write a poem*), while exhibit misalignments with the context in finer details (e.g. *Your response should not exceed 50 words*). This allows us to assess the instruction understanding capability of the LLM at a more intricate level. *Quality Assurance* The three contrastive instructions generated by Anti-Attribute are designed to ensure that they do not prompt the creation of a context, and only the label instruction is crafted to accurately generate the context. This design ensures the clarity of candidate instructions."}]}, {"title": "5 Results and Discussion", "sections": [{"title": "5.1 Experimental Settings", "content": "In our experiments, we validate the generalizability of our benchmark by experiments with instruction-tuned LLMs. This covers eight different open-sourced LLMs, each varying in parameter size and instruction following capabilities, and GPT-3.5 (OpenAI-Blog, 2022) and GPT-40 series (Team, 2024b). Detailed information of experimented LLMs and details about the experimental settings can be found in Appendix D. Considering influence of the order in which candidate options are presented (Zheng et al., 2024), we experiment by randomizing the sequence of candidate instructions and report an average from five experimental trials."}, {"title": "5.2 Main Results", "content": "Firstly, we present the performance of LLMs concerning IOINST in Table 3. We randomly select one of sixteen meta-instructions for each inference step in this experiment. The reported performance corresponds to the average outcome of five trials. Our observations are as follows: *LLMs frequently follow candidate instruction* We can observe that LLMs tend to easily follow candidate instructions, as evidenced by the low ACC2 scores across most LLMs. Low level of ACC2 suggests that LLMs are inclined to distracted by several isntruction-formatted statements, and struggle to find the actual intent embedded in the instruction. This suggests that even LLMs capable of following human instructions through instruction tuning still lack a robust understanding of the requirements specified in the instructions. *Composition of candidate instructions highly affects instruction understanding capability* Table 3 reports that LLMs demonstrate distinct ACC2 scores across Random, Semantic, and Anti-Attribute settings. Such findings indicates how the composition of candidate instructions influences the instruction understanding capabilities. On comparing the performance between Random and Semantic, we can find subtle difference in ACC2, likely due to the analogous compositions of candidate instructions derived by a similar process of composing contrastive instructions (selection from the instruction pool). We can also observe a slight but clear difference in ACC1 and ACCiel, which suggests that LLMs experience minor confusion when selecting appropriate instructions among semantically ambiguous options. Moreover, we can notice a performance drop in ACC2 for the Anti-Attribute. Considering the contrastive instructions of Anti-Attribute, which state the same key requirements of the label instruction, we can interpret that LLMs tend to prioritize the repetitive pattern of the instructions, consequently neglecting the core intent of the instruction. *Given LLM follows meta-instruction, LLM can fairly well understand instruction* Despite the low ACC1, we can witness decent level of ACCrel. In random contrastive settings, GPT-4 achieved a near-perfect score for ACCrel, suggesting that once GPT-4 grasps the intent of the instruction, it can accurately identify the appropriate instruction to generate the relevant context. This demonstrates that LLMs inherently possess the capability to understand instructions, although fully harnessing this capability requires a strong ability to discern the embedded intent. *Exceptional case: Anti-Attribute* In the context of Anti-Attribute, where contrastive instructions differ only slightly from label instructions, it is crucial for LLMs to discern the precise requirements that can lead to generate a given context."}, {"title": "5.3 Discussion on the Instruction Composition", "content": "To analyze the performance derived by the meta instruction, we estimate the average accuracy for each meta-instructions, across all models and all settings (Random, Semantic, Anti-Attribute). Table 5 displays the meta-instruction with the highest average accuracy alongside the one with the lowest. We can ascertain a significant variation, with a maximum difference of 27 points in ACC2, emphasizing the need for rigorous consideration in designing meta-instructions. Table 6 presents the average performance and the corresponding standard deviations for the meta-instruction categories defined in our study. While minor differences in performance were noted based on the order of presenting context and candidates (Candidate-First vs. Context-First), increasing the level of detail in the instructions (Detailed vs. Simple) consistently yielded higher and more stable performance in instruction understanding. This stability highlights the importance of employing detailed meta-instructions to achieve robust and high instruction understanding capabilities. Moreover, to investigate the influence of candidate instructions on the understanding capabilities, we calculate the error rates for each individual candidate instruction. Here, the error rate refers to the proportion of *Case2* errors occurred across all experiments. Table 4 displays candidate instructions with the highest and lowest error rates in a semantic contrastive setting. Our experiments revealed that instructions composed of multiple lines and with complex requirements tend to be indiscriminately followed by the LLMs. Conversely, LLMs show a better capability to accurately identify simple, single-line candidate instructions. This suggests that LLMs still struggle to comprehend the intent behind more complex instruction-formatted statements."}, {"title": "5.4 Case Study on the In-context Learning", "content": "To fully leverage the potential of LLMs, the in-context learning approach is widely adopted (Dong et al., 2024; Brown et al., 2020). We constructed in-context samples by randomly referencing data from the benchmark dataset and assessed the few-shot performance. Unfortunately, this approach proved significantly less effective for IOINST. Figure 4 demonstrate a tendency for performance in IOINST to decrease as shots are introduced, where shots are randomly extracted within our dataset. Especially, $ACC_{rel}$ consistently exhibits a decrease in performance as the number of shots increased, and ACC2 significantly drop as shots introduced. One possible interpretation is that the few-shot examples used as references may have acted as distractors. The superior performance of zero-shot over few-shots strengthens our claim that LLM becomes confused by the presence of instruction-formatted inputs within the provided input statements."}]}, {"title": "6 Conclusion", "content": "This study highlights a crucial ability of LLMs to understand instructions without being misled by instruction-formatted statements. To measure this ability and enable objective evaluation, we introduce IOINST. By utilizing Random, Semantic, and Anti-Attribute contrastive settings, each with unique characteristics, we facilitate a broader exploration of the instruction comprehension capabilities of LLMs. Our results show that most publicly accessible LLMs struggle with IOINST, could not catch the actual intention embedded in the given instruction. Our experiments reveal that the choice of meta-instructions greatly influences comprehension, while the in-context approach is inadequate in addressing these challenges. Our future research aims to explore data-centric and model-based strategies to enhance instruction comprehension in LLMs."}, {"title": "Limitation", "content": "In our study, we conducted experiments on eight open-sourced LLMs. Although more generalized conclusions could potentially be obtained through broader range of model variants, it was challenging for us due to resource constraints. However, by analyzing various parameter-sized LLMs, we were able to draw generalized conclusions. There remains a vast array of proprietary and closed-source LLMs that we did not have access to, which could potentially exhibit different behaviors and capabilities when presented with instruction-formatted prompts. However, our experiment includes ten different LLMs, comprising both open-sourced and closed LLMs. Through comparative analyses among these models, we observed distinct similarities in their performance trends, allowing us to draw sufficiently generalized conclusions."}, {"title": "Ethics Statement", "content": "In conducting our research, we placed a strong emphasis on ethical considerations and mitigating potential risks. The dataset used in our experiments was meticulously constructed by reorganizing and curating data from previously released datasets that are publicly available under permissive licenses, such as the Apache 2.0 (Zhou et al., 2023b; Zheng et al., 2023; Lin and Chen, 2023; Chia et al., 2024) and MIT (Zeng et al., 2024; Ortmann, 2022; Qin et al., 2024; Chia et al., 2024) licenses. These licenses allow for the modification and redistribution of the data, provided that the original sources are properly acknowledged and attributed. To ensure compliance with copyright and intellectual property regulations, we took great care to retain all original data in its unmodified form and clearly denote any alterations or modifications made during our curation process. This transparency allows for the traceability of our dataset's provenance and ensures that the original creators' rights are respected. Furthermore, we implemented a rigorous human supervision process to review and curate both the recompiled data from existing datasets and any additional data generated using language models like ChatGPT. Through this meticulous review process, we removed any potentially harmful, or inappropriate contents."}, {"title": "A Practical Implication", "content": "Over the recent years, it has been observed that slight modifications in prompts alone can have significant impacts on the generative capabilities of LLMs (Kojima et al., 2022; White et al., 2023; Wang et al., 2024a). In considering such sensitivity to the instruction, numerous studies are being conducted with the aim to find out optimal instruction for each targeting task (Reynolds and McDonell, 2021; Qin et al., 2023). Represented by instruction optimization, LLMs are commonly entrusted with the task of discovering these optimal instructions (Fernando et al., 2024; Yang et al., 2024; Zhou et al., 2023c; Wang et al., 2023). This include paraphrasing (Fernando et al., 2024; Yang et al., 2024) or self-refining manner (Pryzant et al., 2023). In implementing these attempts, multiple instructions are inevitably included in a single input statement. Consequently, distinguishing the actual user-intended instructions and executing the meta instruction, rather than the candidate instruction, is considered an substantial issue. Nevertheless, the problem of following candidate instructions instead of meta instructions has largely remained unaddressed; traditional research typically regarded this as a straightforward error (Zeng et al., 2024; Yang et al., 2024). INTINST is designed to comprehensively consider the aforementioned concerns and targeting the assessment of instruction distinguishing and understanding capabilities."}, {"title": "B Case Study: Temperature", "content": "To determine the experimental setting, we conducted a pilot study concerning the influence of temperature, we evaluate the understanding capability under varying temperature settings. Detailed experimental results are presented in Figure 5. Through our experiments, we confirmed that the instruction understanding proficiency declines with an increase of temperature. Remarkably, ACC2 exhibited substantial variation, demonstrating a significant drop as the temperature rose. Aside from the above observation, we can also figure out that there was mere alteration in ACC\u00efel with respect to temperature changes. In essence, given the LLM accurately comprehend the intention within the meta instruction, the capability to distinguish appropriate instructions remains largely unvarying. Such findings compose our future research direction."}, {"title": "C Case Study: Meta-Instruction", "content": "In this section, we validate how the structure of meta-instruction affect instruction understanding capability of LLMs. As described in the Section 3.2, we establish two characteristics of meta instruction concerning instructional details (Detailed vs Simple) and the order of components in the instruction (Option first vs Option last). We analyze the differences between these contrasting settings, and demonstrate our findings. The results are reported in Figure 6, and we analyze these results as follows. We find that LLMs exhibit enhanced comprehension performance for specific forms of meta-instructions. Each model demonstrated a clear preference for either 'Detailed / Simple' or 'Option-first | Option-last' in each setting. This illustrates the potential for significantly improving an LLM's instruction understanding capability by choosing the most appropriate meta-instruction. We observed significant differences in the tendencies exhibited in the Random, Semantic contrastive and the Anti-Attribute contrastive. In the case of Random and Semantic contrastive, LLMs showed a preference for Simple meta-instructions, and demonstrated improved performance when content instructions were provided at the end of the input (Option-last). However, in the Anti-Attribute task, the model found Detailed meta-instructions more effective, and performance was augmented when content instructions were presented ahead of content (Option-first). This implies the necessity to structure meta-instructions differently based on the complexity of the task for the LLM."}, {"title": "D Experimental and Writing Details", "content": "Our experiments were conducted using eight RTX A6000 GPUs. The specifications of the LLMs employed in our study are shown in Table 7. We applied greedy decoding for each experiment. Rationale for selecting greedy decoding is shown in Appendix B In our work, we used GPT-40 (gpt-40-2024-08-06) as a writing assistant. AI assistant was solely utilized for writing-related activities, such as grammar checking, refining awkward expressions, and translation of our manuscript."}, {"title": "E Dataset Details", "sections": [{"title": "E.1 Exceptional Case for Dataset Construction", "content": "INTINST is constructed by re-organizing LLM-as-evaluator benchmark datasets. Exceptionally, the dataset introduced by Zhou et al. (2023b) was not developed following the LLM-as-evaluator format, among datasets detailed in Table 1. This dataset only constructed with instruction-following evaluation objective; but it release its LLM response generated by GPT-4. Additionally, its evaluation measure for assessing instruction-following facets is established with rule-based methodology, facilitating straightforward estimation of data quality. Consequently, we extract only responses that faithfully followed given instructions and accumulate pairs of instructions and GPT-4 responses, utilizing them to generate the data for INTINST."}]}]}