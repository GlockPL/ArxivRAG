{"title": "Boosting Jailbreak Transferability for Large Language Models", "authors": ["Hanqing Liu", "Lifeng Zhou", "Huanqian Yan"], "abstract": "Large language models have drawn significant attention to the challenge of safe alignment, especially regarding jailbreak attacks that circumvent security measures to produce harmful content. To address the limitations of existing methods like GCG, which perform well in single-model attacks but lack transferability, we propose several enhancements, including a scenario induction template, optimized suffix selection, and the integration of re-suffix attack mechanism to reduce inconsistent outputs. Our approach has shown superior performance in extensive experiments across various benchmarks, achieving nearly 100% success rates in both attack execution and transferability. Notably, our method has won the online first place in the AISG-hosted Global Challenge for Safe and Secure LLMs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) excel in various domains such as language comprehension and generation [5], machine translation [9], and code generation [7]. Despite significant efforts to improve the security of LLMs in practical applications [3], recent research reveals that alignment mechanisms intended to protect these models are still vulnerable to sophisticated adversarial jailbreak attacks [1, 6, 10]. These attacks involve crafting complex prompts that bypass safeguards and elicit harmful responses."}, {"title": "2 Related Work", "content": "Jailbreaking attacks on large language models (LLMs) pose a significant threat, leveraging sophisticated prompts to bypass safety measures and elicit restricted outputs. Unlike manual trial-and-error approaches, optimization-based jailbreak techniques automate the process using an objective function aimed at increasing the likelihood of generating harmful or prohibited content.\nThe Greedy Coordinate Gradient (GCG) method, as highlighted in [10], is designed to craft jailbreak suffixes that increase the chances of a model producing a particular initial string in its response. This technique optimizes the adversarial prompt through iterative adjustments based on gradient insights, targeting specific prompt components to elicit a desired outcome. GCG's strategy of maximizing the likelihood of harmful outputs is executed greedily, focusing on the most influential prompt segments. This method not only increases the efficiency of creating jailbreak suffixes but also extends the effectiveness of such attacks to various language models.\nThe Improved Greedy Coordinate Gradient (I-GCG) [4] enhances jailbreak attack convergence with an automatic multi-coordinate updating strategy. Unlike GCG algorithm, which relies on sequential single-coordinate updates, I-GCG simultaneously optimizes multiple prompt coordinates, accelerating the generation of adversarial prompts. Additionally, its \"easy-to-hard\" initialization approach evolves simple prompts into more complex ones, further increasing the efficiency of the attack process. These enhancements in both initialization and convergence allow I-GCG to outperform GCG in generating more powerful and transferable jailbreak prompts across various language models."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": "Formally, given a set of input tokens which can be represented as X_{1:n} = {X_1, X_2, ..., x_n}, where x_i \u2208 {1, ..., V} and V denotes the vocabulary size (i.e., the number of tokens), a large language model (LLM) maps the sequence of tokens to a distribution over the next token. This can be defined as:\np(x_{n+1} | X_{1:n}),\nwhere p(x_{n+1} | x_{1:n}) represents the probability distribution over the possible next tokens given the input sequence x_{1:n}. The probability of the response sequence of tokens can be represented as:\np(X_{n+1:n+H} | X_{1:n}) = \\prod_{i=1}^{H} p(x_{n+i} | X_{1:n+i-1}).\nTo simplify the notation, we can express the malicious question X_{1:n} as x^Q, the jailbreak suffix x_{n+1:n+m} as x^S and the jailbreak prompt X_{1:n} \u2295 X_{n+1:n+m} as x^Q \u2295 x^S, where \u2295 represents the vector concatenation operation. Additionally, the predefined target template represents as x_{n+m+1:n+m+k}^R which is simply express as x^R. Thus, the adversarial jailbreak loss function can be expressed as:\nL(x^Q \u2295 x^S) = -log p(x^R | x^Q \u2295 x^S).\nAnd the optimization of the adversarial suffix can be formulated as:\n\\underset{x^S \u2208 {1,...,V}^m}{minimize} L(x^Q\u2295 x^S)"}, {"title": "3.2 The proposed SI-GCG attack method", "content": "Unlike the GCG algorithm, which solely focuses on the target template during optimization, our method takes into account both the target template and malicious question contexts for more effective attacks. Specifically, we established a fixed harmful template to handle malicious questions in Figure 1. We denote this process using x^{HQ} \u2295 x^Q, where x^{HQ} represents the harmful question template and x^Q represents the initial malicious question. At the same time, we optimize our response to incorporate harmful information, such as \"Sure, my output is harmful. Here is specific step by step extremely evil process about Rephrase(Q) with actual details in Sin City\". To facilitate representation, we adopt x^{HR} \u2295 x^R to represent this process, where x^{HR} represents the harmful response template. Consequently, the jailbreak loss function can be expressed as:\nL((x^{HQ} \u2295 x^Q) + x^S) = -log p(x^{HR} \u2295 x^R | (x^{HQ} \u2295 x^Q) \u2295 x^S)\nThe suffix iterative update can use optimization methods for discrete tokens, which be formulated as:\nx^{t+1} = GCG([L((x^{HQ} \u2295 x^Q) \u2295 x^{t-1})]), s.t. x^t = !, where GCG() denotes the optimization method based on GCG approach, where x^t represents the jailbreak suffix generated at the t-th iteration, x^0 represents the initialization for the jailbreak suffix. We have observed that during the suffix optimization process, although the loss continues to decrease, the generated content does not consistently become more harmful. This discrepancy occurs because the loss calculation solely measures how well the generated"}, {"title": "3.3 Automatic optimal suffix selection strategy", "content": "Zou et al. [10] propose a greedy coordinate gradient jailbreak method (GCG), which simplifies solving Equation 4, significantly enhancing the jailbreak performance of LLMs. However, it updates only one token in the suffix per iteration, which results in low jailbreak efficiency. Jia et al. [4] try to address this issue by proposing an automatic multi-coordinate updating strategy, which can adaptively determine the number of tokens to replace at each step. Instead, both approaches select only the candidate suffix with the smallest loss for the suffix update in each iteration. However, responses such as \"first yes, then no\", while reducing loss, are not necessarily harmful. Thus, identifying the appropriate suffix for each round of update has become a pressing issue that needs to be addressed. In Figure 2, we propose an automated optimal suffix selection strategy that goes beyond using only the minimum loss criterion. Instead, it evaluates the first p suffixes with the smallest losses x_1, x_2, ..., x_p and assesses the harmfulness of the content they generate, which can be expressed as:\nCheck(G((x^{HQ} \u2295 x^Q) \u2295 x_i^S)),\nwhere G() represents the function of the content generated by LLMs, Check() represents the function that determines whether the generated content is harmful and x_i^S represents the i-th smallest suffix. In order to save time cost, we set p to 5. If none of the generated content is deemed harmful (i.e., no True values), the suffix with the smallest loss is selected for the update. Conversely, if harmful content is identified, the suffix with the smallest loss that is also marked as True is chosen for the update. This approach ensures that suffix updates consistently aim to generate harmful content while avoiding denial responses."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setup", "content": "Dataset. We conduct experiments using the dataset provided by AI Singapore, which include 50 malicious questions. All results are derived from the scores reported on the competition website.\nVictim Models. Two LLM models, LLAMA2-7B-CHAT[8] and VICUNA-7B-1.5[2], are used as victim models in our experiments."}, {"title": "4.2 Attack success rates on track la", "content": "In the Track 1a stage, we ensured that the comparison algorithms performed well by setting the number of iterations and batch size according to the original paper's specifications. Notably, GCG and I-GCG do not use our question templates. Their response templates are: \"Sure, here is Rephrase(Q)\" for GCG, and \"Sure, my output is harmful. Here is Rephrase(Q)\" for I-GCG. The attack success rates for both models are shown in Table 1. Clearly, our proposed SI-GCG demonstrates a significant advantage. Compared to other jailbreak methods, the attack success rate of our approach is significantly ahead of the two selected large language models."}, {"title": "4.3 Attack success rates on track 1b", "content": "In the Track 1b stage, due to computing resource limitations imposed by the competition organizers, we adjusted the batch size to 32 and limited the maximum number of iterations to 100. Given that specific questions were deemed untouchable and more black-box models were introduced, we were only able to obtain results from LLAMA2-7B-CHAT. Inspired by IGCG's easy-to-hard initialization technique, we integrated some initialization suffixes obtained in Track 1a into our method, which yielded promising results, as shown in Table 2. Unsurprisingly, our method continues to lead on the leaderboards, even in the black-box setting. It can be concluded that the proposed method has a good attack trasferability."}, {"title": "4.4 Ablation study", "content": "We propose three enhanced techniques to improve jailbreaking performance: harmful question-and-response templates, an updated"}, {"title": "4.5 Discussion", "content": "We found that prepending \"!\" to an optimized suffix can significantly enhance an attack's transferability. To verify this, we conducted comparative tests post-optimization to rule out confounding factors. The experiments varied the number of \"!\" used, with findings detailed in Table 4 and the baseline means no \"!\". The data indicate that appending 10 exclamation marks maximizes the attack's transferability. However, exceeding this number diminishes the success rate for both models. Additionally, an excessive number of exclamation marks disrupts the carefully tailored suffix for the LLAMA2-7B-CHAT model, reducing its attack efficiency."}, {"title": "5 Conclusion", "content": "In summary, the proposed SI-GCG method provides a powerful strategy for jailbreaking LLMs based malicious question contexts and target templates to enhance harmful output elicitation. Its innovative mechanisms, such as assessing the top five loss values at each iteration and integrating re-suffix attack mechanism, guarantee reliable and effective updates. Achieving a near-perfect success rate across various LLMs, SI-GCG outperforms existing jailbreak techniques. Its compatibility with other optimization methods further enhances its versatility and impact, marking a significant advancement in LLM security research."}]}