{"title": "Towards Responsible AI in Education: Hybrid Recommendation System for K-12 Students Case Study", "authors": ["Nazarii Drushchak", "Vladyslava Tyshchenko", "Nataliya Polyakovska"], "abstract": "The growth of Educational Technology (EdTech) has enabled highly personalized learning experiences through Artificial Intelligence (AI)-based recommendation systems tailored to each student's needs. However, these systems can unintentionally introduce biases, potentially limiting fair access to learning resources. This study presents a recommendation system for K-12 students, combining graph-based modeling and matrix factorization to provide personalized suggestions for extracurricular activities, learning resources, and volunteering opportunities. To address fairness concerns, the system includes a framework to detect and reduce biases by analyzing feedback across protected student groups. This work highlights the need for continuous monitoring in educational recommendation systems to support equitable, transparent, and effective learning opportunities for all students.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of Educational Technology (EdTech) has significantly reshaped traditional learning environments, enabling the delivery of personalized educational experiences tailored to individual students' needs. According to the U.S. Department of Education Office of Educational Technology, leveraging AI-based modern educational technologies has been pivotal in providing personalized pathways for learning, supporting adaptive and individualized instruction, and enhancing student engagement through innovative digital solutions. This trend toward personalization in education underscores the importance of leveraging advanced recommendation systems to support student exploration and growth.\nRecommendation systems in education have become critical in suggesting extracurricular activities, academic programs, and digital resources that align with students' interests, aptitudes, and feedback. By leveraging these systems, educators can offer a more personalized learning experience, tailoring recommendations to each student's unique profile and preferences. Such systems can positively impact student outcomes by fostering curiosity, encouraging skill development, and guiding students toward potential academic and career trajectories. However, despite their benefits, recommendation systems may also unintentionally introduce biases, resulting in disparate impacts on different student groups. For instance, prior research has highlighted potential biases in AI-based systems that can marginalize specific demographics, thereby limiting their access to valuable learning resources [3].\nIn response to these concerns, this study introduces a graph-based recommendation system designed to provide personalized suggestions to K-12 students in public school districts. Independent software vendor SoftServe Inc. developed this solution for their client Mesquite Independent School District (ISD) as part of the creation of the personalized learning platform AYO\u00ae. The primary goal of AYO \u00ae is to harness data effectively to enhance student engagement and deliver tailored learning experiences that support each student's unique educational journey. The proposed system employs a hybrid approach, combining graph-based methods with matrix factorization to tailor recommendations based on students' expressed and inferred interests. To promote responsible AI practices, we integrate a fairness analysis framework that systematically evaluates recommendations to identify and mitigate biases.\nThis work is structured as follows:\nSection II provides an overview of the related work and background literature. Section III identifies existing gaps in the integration of fairness-aware frameworks within graph-based recommendation systems in the educational domain.\nSection IV outlines the design and implementation of the proposed hybrid graph-based recommendation system. It details the graph structure, integration of matrix factorization, and the development of the fairness analysis framework.\nSection V presents a case study in a K-12 educational setting. It describes the dataset, experimental design, and implementation of the proposed system. The results are analyzed in terms of recommendation accuracy and fairness, highlighting the system's effectiveness and any identified disparities.\nSections VI and VII provide the approach to ensuring fairness"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "A. Personalization in Educational Technology\nPersonalized learning systems in EdTech leverage AI-based recommender systems to tailor content based on student preferences. Li and Chen [9] demonstrated the effectiveness of these systems in improving engagement and outcomes. Additionally, a systematic literature review on educational recommendation systems [7] was conducted, highlighting trends in recommendation production, evaluation methods, and research gaps. Their findings indicate that hybrid approaches dominate, but evaluations often focus solely on accuracy, neglecting the pedagogical impact. This underscores the need for multidimensional evaluation frameworks to better assess the effectiveness of these systems in supporting teaching and learning activities.\nB. Graph-Based Recommendation Systems\nGraph-based approaches are valued for their ability to model complex user-resource interactions. There was demonstrated the efficacy of Graph Convolutional Networks (GCNs) in capturing large-scale relationships [32].\nIn addition to traditional applications, graph-based recommendation methods have shown considerable promise in specialized domains like the academic community [22]. Furthermore, the researchers [30] provide a comprehensive review of graph learning-based recommendation systems, discussing various methodologies and highlighting their adaptability across diverse use cases.\nC. Fairness in Al-Based Recommendation Systems\nWhile personalized recommendation systems have shown the potential to improve learning experiences, concerns about fairness and algorithmic bias remain prominent. Studies [3] have drawn attention to the risks of reinforcing existing inequalities in educational settings. Binns [5] and Burke [8] proposed fairness-aware frameworks to address these issues. Recommendation systems have a different logic than traditional machine learning tasks, as they rely on user-item interactions and dynamic feedback loops, making it not optimal to assess fairness using standard metrics. Authors of \"Fairness in Recommendation Systems: Research Landscape and Future Directions\" work [11] have reviewed current studies on fairness in recommendation systems, highlighting various issues and gaps in existing methods. In addition to this, algorithmic fairness, the main component of responsible AI, has been analyzed comprehensively, with fairness defined in various ways based on philosophical considerations and contextual use [17]. Researchers have developed numerous fairness metrics to address different aspects of fairness [4], [6], [10], [13], [23], [27], [29].\nD. Hybrid Approaches in Recommendation Systems\nHybrid recommendation systems have been widely adopted to combine the strengths of different recommendation techniques. Koren et al. [18] introduced matrix factorization techniques that uncover latent patterns in user-item interactions, which have become foundational in collaborative filtering approaches.\nFor instance, there was proposed a scalable and accurate hybrid recommendation system that combines collaborative filtering with content-based filtering [14].\nSimilarly, in the educational space, there was developed a personalized recommendation system for college libraries that combines collaborative filtering and content-based techniques to help users navigate vast collections of books [28]."}, {"title": "III. RESEARCH GAPS AND PROBLEM FORMULATION", "content": "While existing studies [7], [9], [11] provide valuable insights into personalized learning and fairness-aware recommendation systems, there remains a need for approaches that can effectively combine graph-based methods with fairness analysis in the educational domain. Current research often treats recommendation generation and fairness analysis as separate processes, resulting in challenges when trying to achieve real-time fairness monitoring and bias mitigation. Moreover, many traditional approaches [14], [18], [28], [30] focus primarily on optimizing quality, with limited emphasis on transparency and interpretability-both of which are critical in educational applications to ensure trustworthiness and accountability.\nTo address these issues, our work seeks to combine graph-based modeling and matrix factorization with a fairness analysis framework. This integration aims to enhance both the accuracy and equity of recommendations, paving the way for more personalized and responsible AI-driven learning experiences for K-12 students."}, {"title": "IV. METHODOLOGY", "content": "Our study focuses on the fairness analysis of a graph-based recommendation system [15]. The recommendation system provides personalized suggestions for students based on their interests, aptitudes, and explicit feedback, while the fairness analysis ensures that these recommendations do not introduce biases or unfair treatment toward any group.\nA. Graph Recommendation System\nOur graph-based approach for the recommendation system supports responsible AI practices by being transparent, integrating diverse data, and effectively capturing relationships between interests, aptitudes, and recommended targets while maintaining interpretability."}, {"title": "B. Fairness Analysis", "content": "Fairness analysis in recommender systems is crucial for ensuring that the suggestions provided do not inadvertently discriminate against certain groups of users. In this context, protected groups refer to defined categories of individuals who may face discrimination (for example, gender, race, and family status), while protected attributes are specific characteristics associated with those groups, as outlined by anti-discrimination laws.\nFor our recommendation system, we propose a concept of fairness analysis based on user reactions. Our approach involves evaluating fairness by analyzing positive and negative reactions across different resources and protected groups. The process includes:\n1) Data collection: Gathering feedback from users, and categorizing it into positive and negative reactions.\n2) Segmentation by protected groups: Analysis of the feedback for each type of resource, separately considering each protected attribute across different protected groups (e.g., gender, race, family status).\n3) Comparison of feedback: Comparison of the percentage of positive and negative reactions across diverse groups to identify disparities.\n4) Bias alerting and auditing: Utilization of the feedback analysis as an alerting mechanism. This analysis enables us to pinpoint specific resources or recommendations that may pose problems and clarify the reasons for these issues within particular user subsets, which can then inform targeted mitigation strategies.\nIt is important to note that this process serves primarily as a tool for alerting and auditing rather than a complete solution. It helps to detect potential issues, after which further actions are needed to address and mitigate these biases."}, {"title": "V. CASE STUDY", "content": "A. Solution Explanation\nOur hybrid recommendation system is developed as a part of an educational platform aimed at providing personalized learning experiences for public school districts' K-12 students. The recommendation system suggests a unique set of opportunities for each student to help them grow their potential. Outputs of the recommendation system may point students to the exploration of new interests, development of novel or building up of existing skills, meaning that each suggestion from the system may impact student's future. Hence, the system needs to be equally safe, trustworthy, and ethical to each of its young users. Our system is designed with the notion of Responsible Al principles [19], making sure that fairness, reliability, and transparency are the core features of the system from the launch day.\n1) Transparency:\na) Content-filtering part: To prevent a cold-start problem [24] of a new community, we employ a content-filtering approach based on undirected weighted graphs. We store graph nodes and edges in tabular format for cost efficiency purposes and use the NetworkX5 library to initialize graphs for each student in a distributed data-parallel fashion using Apache Beam SDK6. When the student graph is initialized, it is reduced to the subgraph with a certain neighborhood radius. Suggestions are then selected based on Dijkstra's shortest path algorithm [12] with the Student node as a source. Suggestions ranks are defined based on the multiplication of shortest path weights and number of shortest simple paths between the Student node and suggestion node. The key feature that allows solution transparency is logging the reasoning behind each suggestion, which includes a list of nodes in the shortest path and ranks as a suggestion confidence score.\nb) Collaborative-filtering part: To effectively utilize explicit users' reactions for high-quality recommendations we also use a collaborative-filtering approach based on the Non-Negative Matrix Factorization algorithm [21] from the Scikit Learn library. Positive reactions are used to build a users-targets matrix, while negative reactions suit for filtering out redundant suggestions. Weights of the reconstructed matrix are used as confidence scores and are logged into the system for each suggestion.\nBoth content-filtering and collaborative-filtering recommendations are later re-ranked in case some targets were selected by both methods. All confidence scores and reasoning behind suggestions are accumulated in the system's database for debugging purposes. They are also later used for fairness audits. Moreover, a user interface of the system provides the reasoning behind each suggestion according to Human Interface Guidelines for Machine Learning Applications8. The element of the solution's user interface is displayed in Figure 2, highlighting the incorporation of the transparency principle into the user's experience.\n2) Reliability: The reliability component of our system is guaranteed and measured within two realms: content reliability and recommendations quality.\na) Content Reliability: The content of our recommendation system has multiple origins: internet-sourced resources (Courses), ML-generated resources (Activities), and district resources (Books, Videos, Extracurriculars, Certifications, and Volunteering activities).\n\u2022\nInternet-sourced resources include free courses sourced from platforms like Coursera 9. All sourced courses are manually verified by human moderators to ensure the age-level appropriateness of each recommended course.\n\u2022 ML-generated resources include short Activities generated by GPT-2 model [25]. All generated items were manually verified by human moderators to ensure the reliability of generated content.\n\u2022 District resources include materials used by the school district in their libraries. Due to their amount, it is not feasible to verify them manually. We use Google Cloud Natural Language Text Moderation API 10 and PaLM2 model [2] with prompt engineering to filter out potentially unsafe resources using their descriptions.\nb) Recommendations Quality: To continuously monitor the quality of the recommendations, we calculate multiple evaluation metrics used for information retrieval algorithms. A list of the main evaluation metrics is available in Table I. Each of the metrics is aggregated by target category and by grade levels (elementary, middle, high school) to differentiate the evaluation process for different subgroups of users. All evaluation metrics are supported by real-time visualizations which are integrated into the district's Looker 11 dashboard.\n3) Fairness: To guarantee the fairness of our recommendation system, we developed a fairness audit"}, {"title": "B. Fairness Audit Results", "content": "To effectively analyze variations in fairness across protected groups and variables in Table II we built visualizations that complement Algorithm 1. This section is focused on the fairness analysis of gender protected groups specifically. We conducted an equivalent audit and analysis for all other protected variables of interest.\nAn outlook on the precision of recommendations in gender subgroups is provided in Figure 3. The figure shows that the only target category that exceeds the set tolerance level is Volunteering. It is also notable that the proportion of negative and positive reactions is not equal between targets, for example, both protected groups have higher satisfaction in Activities recommendations than in Certification. Although this aspect was analyzed by us during the audit, in this work we focus on variations in fairness rather than differences in precision between targets.\nWe further explore the possible causes of high variation in precision for Volunteering between gender-protected groups by comparing:\n1) Ranks of top negative and positive reactions. The only targets with negative reactions ranked higher by the male gender group are cleaning up cigarette butt litter to protect humans, animals, and the planet - 5 reactions, and keep mesquite beautiful community cleanup - 4 reactions.\n2) Unique negative and positive reactions. The are no targets that have negative reactions only by male gender group.\n3) The reasoning behind suggestions with negative reactions is based on suggestions path from the graph or weights in a reconstructed matrix. To target clean up cigarette butt litter to protect humans, animals, and the planet graph suggestions paths are: an aptitude for organizational, aptitude physical, and interest ecology environments. For target keep Mesquite beautiful community cleanup - aptitude organizational, aptitude physical, interest environmental issues respectively. Suggestions paths are the same for both gender groups for these two targets.\nBased on the provided results, we can state that both recommendations and their reasoning for the Volunteering target are not biased towards any gender group. All suggestions paths are valid, signifying that graph nodes and edges involved in the recommendation of the mentioned targets are gender-neutral. Hence, the increased difference in recommendations precision should be explained by other factors, for example:\n1) Gender bias in interest or aptitude identification algorithms.\n2) Insufficient sample size that should be higher and focus on group-target level, rather than just on the group.\n3) Other hidden factors, that are out of the scope of the recommendation algorithm (venue location, activities schedule co-occurring with other campus events, etc.).\nFor all other targets, we received either similar results on variation in fairness or no variation at all. Variables and targets flagged for additional analysis are summarized in Table III. Protected variables that are not displayed in Table III proved not to exceed the tolerance level for all target categories. The visualizations for all target categories and protected groups are available in our GitHub repository. 12"}, {"title": "VI. RECOMMENDATIONS FOR FAIRNESS MONITORING", "content": "To ensure fairness in recommendation systems similar to ours, we emphasize the importance of regular fairness audits"}, {"title": "VII. RECOMMENDATIONS FOR BIAS MITIGATION", "content": "Although the results above proved that no action should be currently taken to mitigate bias in our recommendation system, we consider multiple fairness-aware mechanisms that could be implemented in similar systems.\nPre-Processing Mechanisms:\n\u2022 Data re-sampling. Biased targets (e.g., assume belonging to one of the protected groups) should be removed from the system or their amount should be equal for each protected group. Such targets can be identified via classification using Large Language Models.\nIn-Processing Mechanisms:\n\u2022 Introducing fairness constraints into the recommendation process. Protected group labels can be injected as separate graph nodes while weights of the edges between protected groups and targets should be regularized to balance the recommendations.\nPost-Processing Mechanisms:\n\u2022 Non-parametric re-ranking. Under specified fairness constraints, the optimal re-ranking outcomes can be found through heuristic search methods [16]. Specifically, given original top-k recommendation results for each user from the system, heuristic methods maximize the total preference score concerning fairness by rearranging original recommendations."}, {"title": "VIII. CONCLUSION", "content": "This paper presented a hybrid recommendation system as a case study based on a real-world solution developed by SoftServe Inc. for Mesquite Independent School District 13. Our research highlights the critical need for fairness analysis in educational systems, particularly in recommendation systems that can influence student opportunities and outcomes. By incorporating a fairness audit framework into the design of our system, we aim to identify and mitigate potential biases, ensuring that all students have equitable access to educational resources. Future efforts will aim to enhance the robustness of our fairness evaluation methods and explore advanced mechanisms to ensure that the recommendations provided are not only personalized but also equitable for all students."}, {"title": "IX. WORK LIMITATION", "content": "Our study has a few limitations, which we plan to address in future work:\n\u2022 Simple feedback mechanism: We rely only on explicit positive/negative feedback from users.\n\u2022 Single-attribute fairness analysis: Our fairness evaluation only considers a single protected variable per analysis, not a combination of groups (e.g., race&gender, gender&migrant).\n\u2022 Single fairness metric: The study focuses on recommendations precision as a main fairness metric. Our fairness audit does not cover the fairness of recommendations ranking.\n\u2022 Manual bias correction: Fairness audits were conducted manually, with no automated alerting mechanism.\n\u2022 In-house fairness audit: The analysis was conducted by the development team, risking bias.\n\u2022 Limited data on protected groups: Protected groups covered in fairness audit may not fully represent the whole population of the users. The choice of protected variables and their groups is solely based on the availability of records in the students' management system."}, {"title": "X. ETHICAL CONSIDERATION", "content": "In this study, we prioritize fairness, aiming to identify and address potential biases in hybrid recommendation systems. Our research intends to promote equitable recommendations by raising awareness of these biases. We acknowledge the responsibility to handle this sensitive topic with care and strive to contribute positively to the discourse on fairness and equity in recommendation systems. Data used in this study was obtained from a school district and has been fully anonymized. All students and their parents provide consent for the use of students' data in the system. Additionally, we have ensured that data can be promptly deleted upon request from any participant. Throughout the paper, we present only aggregated results, ensuring that no individual's data can be identified. We used ChatGPT14 and Grammarly15 to aid in paraphrasing while writing this work, ensuring that our language is clear and respectful."}]}