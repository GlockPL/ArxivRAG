{"title": "Quadratic Surface Twin Support Vector Machine for Imbalanced Data", "authors": ["Hossein Moosaei", "Milan Hlad\u00edk", "Ahmad Mousavi*", "Zheming Gao", "Haojie Fu"], "abstract": "Binary classification tasks with imbalanced classes pose significant challenges in machine learning. Traditional classifiers often struggle to accurately capture the characteristics of the minority class, resulting in biased models with subpar predictive performance. In this paper, we introduce a novel approach to tackle this issue by leveraging Universum points to support the minority class within quadratic twin support vector machine models. Unlike traditional classifiers, our models utilize quadratic surfaces instead of hyperplanes for binary classification, providing greater flexibility in modeling complex decision boundaries. By incorporating Universum points, our approach enhances classification accuracy and generalization performance on imbalanced datasets. We generated four artificial datasets to demonstrate the flexibility of the proposed methods. Additionally, we validated the effectiveness of our approach through empirical evaluations on benchmark datasets, showing superior performance compared to conventional classifiers and existing methods for imbalanced classification.", "sections": [{"title": "1 Introduction", "content": "The support vector machine (SVM) model marked a pivotal advancement in binary classification, achieving this by identifying two parallel supporting hyperplanes that maximize the minimum margin between classes through a convex quadratic programming problem [24]. Weston et al. introduced the Universum support vector machine (U-SVM), which enhances the model by incorporating prior information [25]. This is achieved by including Universum samples that do not belong to any class. Additionally, the Twin Support Vector Machine (TSVM) represents a significant extension, utilizing two non-parallel hyperplanes to simultaneously separate the classes. This innovative approach maximizes the margin between them while ensuring sufficient coverage of both majority and minority class instances.\nHowever, many real-world applications pose challenges as they often involve data that is not linearly separable. Addressing this requires robust techniques capable of handling nonlinear situations effectively. One well-established approach is to leverage the concept of kernel methods, which relies on the existence of a nonlinear mapping. This mapping transforms the original data into a higher-dimensional, potentially infinite-dimensional, feature space where the data becomes linearly separable. Notably, the appeal of kernel methods lies in their ability to bypass the need for explicitly knowing this mapping. Instead, they focus on selecting appropriate kernel functions with advantageous properties. However, the challenge lies in determining the suitable kernel function, and fine-tuning the associated hyperparameters can be computationally demanding. As a result, there is a growing interest in practical methods that seek nonlinear classifiers directly in the original feature space.\nKernel-free models offer a direct approach for handling nonlinearly separable datasets without the need to map them to a higher-dimensional feature space. The quadratic surface support vector machine (QSSVM) [3] utilizes quadratic surfaces for class separation. Bai et al. [1] introduced a kernel-free least square QSSVM tailored for disease diagnosis. A kernel-free support vector machine, utilizing a double-well potential, proposed in [8], aims to achieve a specialized fourth-order polynomial separating surface. However, when pursuing nonlinear classifiers, the complexity of the optimization program significantly increases due to the involvement of numerous decision variables, leading to computational challenges and potential overfitting. To mitigate this, incorporating a surrogate that promotes sparsity among decision variables proves advantageous [18,19,20]. To achieve less expensive quadratic programs, [6] discusses a least squares twin QSSVM, leveraging two quadratic surfaces for data representation.\nClass imbalance in binary classification presents numerous challenges that can significantly impact the performance and reliability of machine learning models [10]. When one class dominates the dataset, classifiers tend to exhibit bias towards the majority class, resulting in subpar predictions for the minority class. This imbalance can lead to various issues, including reduced sensitivity and specificity, misinterpretation of model accuracy, and distorted decision boundaries. In critical domains such as fraud detection or medical diagnoses, where the minority class represents rare events, the consequences of misclassification can be severe [5,15]. Failing to address class imbalance may result in suboptimal model performance, compromised decision-making,"}, {"title": "2 Problem Statement and Related Works", "content": "We start by describing the fundamental classical and kernel-free models proposed in the literature for binary classification when dealing with an (almost) linearly separable data set."}, {"title": "2.1 Support Vector Machine", "content": "Suppose that we are faced with a binary classification problem and the related data set is $T = \\{(x_1,y_1),..., (x_m, y_m)\\} \\in (\\mathbb{R}^n \\times \\{\\pm 1\\})^m$, where the $x_i$s are n-dimensional samples and the $y_i$s are their corresponding labels. For such a two-class problem when the classes are (almost) linearly separable, the SVM algorithm finds two parallel hyperplanes with the maximal minimum margin. The training samples from the two classes are divided by the middle hyperplane $f(x) = w^Tx + b = 0$ if and only if $y_i(w^Tx_i + b) \\geq 1, i = 1, ...,m$. The hyperplane for the almost separable case can be obtained by solving the following optimization problem:\n$\\begin{aligned}\n\\min_{w,b,\\xi} & \\frac{1}{2} ||w||^2 + \\mu \\sum_{i=1}^m \\xi_i \\\\\ns.t. & y_i(x_i^T w + b) \\geq 1 - \\xi_i, i = 1, ..., m, \\\\\n& w \\in \\mathbb{R}^n, b \\in \\mathbb{R}, \\xi \\in \\mathbb{R}^m_+\n\\end{aligned}$   (SVM)\nwhere $\\mu > 0$ is a penalty parameter to control the trade-off and $\\xi_i$'s are slack variables to accommodate noisy data points and outliers. The above soft margin SVM exhibits several desirable properties, including robustness to noise and outliers, as it minimizes the effect of individual misclassified points while maximizing the margin. However, addressing the dual of (SVM) is computationally advantageous due to its more structured nature. The Lagrangian dual problem is as follows:\n$\\begin{aligned}\n\\min_{\\alpha} & \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j - \\sum_{i=1}^m \\alpha_i \\\\\ns.t. & \\sum_{i=1}^m Y_i \\alpha_i = 0, \\\\\n& 0 \\leq \\alpha_i \\leq \\mu, i = 1,..., m,\n\\end{aligned}$\nwhere $\\alpha_i$ is the Lagrange multiplier. The decision function can be obtained by solving the above quadratic programming problem and is given by:\n$f(x) = sgn(w^T x + b) = sgn(\\sum_{i=1}^m \\alpha_i y_i x_i^T x + b)$\nA new sample is classified as +1 or -1 by utilizing the above decision function."}, {"title": "2.2 Twin Support Vector Machine", "content": "To optimize binary classification by identifying two hyperplanes that are maximally distant from each other and as close as possible to their respective classes, Jayadeva et al. [11] proposed the Twin Support Vector Machine (TSVM). This model delineates two non-parallel hyperplanes, as illustrated below:\n$w_1^T x + b_1 = 0$, and $w_2^Tx + b_2 = 0,$\nwhere $w_1, w_2 \\in \\mathbb{R}^n$ and $b_1, b_2 \\in \\mathbb{R}$. Assume that the data points belonging to class +1 and class -1 are represented by the rows of matrices $A \\in \\mathbb{R}^{M_1\\times n}$ and $B\\in \\mathbb{R}^{M_2\\times n}$, respectively. Solving the following two quadratic programming problems yields the TSVM classifiers:\n$\\begin{aligned}\n\\min_{W_1,b_1,\\xi_1} & \\frac{1}{2} ||Aw_1 + e_1b_1||^2 + C_1 \\xi_1^T \\xi_1 \\\\\ns.t. & (Bw_1 + e_2b_1) + \\xi_1 \\geq e_2, \\xi_1 \\geq 0,\n\\end{aligned}$   (TSVM)\n$\\begin{aligned}\n\\min_{W_2,b_2,\\xi_2} & \\frac{1}{2} ||Bw_2 + e_2b_2||^2 + C_2 \\xi_2^T \\xi_2 \\\\\ns.t. & (Aw_2 + e_1b_2) + \\xi_2 \\geq e_1, \\xi_2 \\geq 0,\n\\end{aligned}$\nwhere $C_1, C_2 > 0$ are penalty parameters, $\\xi_1, \\xi_2$ are slack vectors, and $e_1, e_2$ are vectors of ones of appropriate dimension. Using the KKT conditions, the Wolfe dual"}, {"title": "2.3 Universum Twin Support Vector Machine", "content": "Universum data is defined as a set of unlabeled samples that do not belong to any specific class [23,25]. This type of data can encode past knowledge by providing meaningful information within the same domain as the problem at hand. Incorporating Universum data has been shown to effectively improve learning performance in both classification and clustering tasks. By integrating Universum data into classical models, researchers have enhanced the computational efficiency and generalization ability of various existing methods [16, 18, 25, 26].\nLet $U \\in \\mathbb{R}^{r\\times m}$ be a matrix where each row represents a Universum point. The Universum Twin Support Vector Machine (U-TSVM) was proposed to enhance the classification performance of (TSVM) [21]. The U-TSVM was constructed by incorporating Universum data in the TSVM model, as the following pair of quadratic programming problems (QPPs):\n$\\begin{aligned}\n\\min_{W_1,b_1,\\xi_1,\\psi_1} & \\frac{1}{2} ||Aw_1 + e_1b_1||^2 + \\frac{C_1}{2} \\xi_1 + \\frac{C_u}{2} \\psi_1^T V_1 \\\\\ns.t. & (Bw_1 + e_2b_1) + \\xi_1 \\geq e_2, \\\\\n& (Uw_1 + e_ub_1) + \\psi_1 \\geq (-1 + \\varepsilon)e_u, \\\\\n& \\xi_1, \\psi_1 \\geq 0,\n\\end{aligned}$   (U-TSVM)\n$\\begin{aligned}\n\\min_{W_2,b_2,\\xi_2,\\psi_2} & \\frac{1}{2} ||Bw_2 + e_2b_2||^2 + \\frac{C_2}{2} \\xi_2 + \\frac{C_u}{2} e_n \\psi_2 \\\\\ns.t. & (Aw_2 + e_1b_2) + \\xi_2 \\geq e_1, \\\\\n& (Uw_2 + e_ub_2) + \\psi_2 \\geq (-1 + \\varepsilon)e_u, \\\\\n& \\xi_2, \\psi_2 \\geq 0,\n\\end{aligned}$"}, {"title": "2.4 Least-Square Twin Support Vector Machine", "content": "To improve the computational efficiency of a classifier, the Least-Square Twin Support Vector Machine (LS-TSVM) [12] is introduced, inspired by TSVM. Unlike TSVM, LS-TSVM employs equality constraints rather than inequality constraints, leading to the solution of only a pair of linear equations. Typically, the linear LS-TSVM deals with the following pair of quadratic programming problems (QPPs)\n$\\begin{aligned}\n\\min_{1,b_1,\\xi_1} & ||Aw_1 + e_1b_1||^2 + ||\\xi_1||^2 \\\\\ns.t. & - (Bw_1 + e_2b_1) + \\xi_1 = e_2,\n\\end{aligned}$   (LS-TSVM)\n$\\begin{aligned}\n\\min_{2,b_2,\\xi_2} & ||Bw_2 + e_2b_2||^2 + ||\\xi_2||^2 \\\\\ns.t. & (Aw_2 + e_1b_2) + \\xi_2 = e_1,\n\\end{aligned}$\nwhere $C_1$ and $C_2$ are positive penalty parameters. These convex programs have closed-form solutions.\nSubstituting the equality constraints of the above problems into associated objective functions, one obtains convex unconstrained optimization problems. Thus, to find their optimum solutions, we set their gradients with respect to $w_1,b_1, w_2$ and $b_2$ equal to 0. Consequently, we can find the two non-parallel hyperplanes by solving the following systems of linear equations\n$\\begin{bmatrix}\nW_1 \\\\\nb_1\n\\end{bmatrix} = -(H^TH + C_1G^TG)^{-1}(C_1G^Te_2),$\n$\\begin{bmatrix}\nW_2 \\\\\nb_2\n\\end{bmatrix} = (G^TG + C_2H^TH)^{-1}(C_2H^Te_1),$\nusing the same notation as before. A new data point $x \\in \\mathbb{R}^n$ is assigned to class one or two using a rule similar to that of the TSVM."}, {"title": "2.5 Least-Square Universum Twin Support Vector Machine", "content": "To enhance the generalization performance of LS-TSVM, Xu et al. [27] introduced the incorporation of Universum data, leading to the development of the Least-Square Universum Twin Support Vector Machine (LS-U-TSVM). This model is formulated as follows:\n$\\begin{aligned}\n\\min_{W_1,b_1,\\xi_1,\\psi_1} & \\frac{1}{2} ||Aw_1 + e_1b_1||^2 + \\xi_1 + \\frac{C_u}{2} e_T \\psi_1 \\\\\ns.t. & (Bw_1 + e_2b_1) + \\xi_1 = e_2, \\\\\n& (Uw_1 + e_ub_1) + \\psi_1 = (-1 + \\varepsilon)e_u,\n\\end{aligned}$   (LS-U-TSVM)\n$\\begin{aligned}\n\\min_{W_2,b_2,\\xi_2,\\psi_2} & \\frac{1}{2} ||Bw_2 + e_2b_2||^2 + \\frac{C_2}{2} e_1 \\xi_2 + \\frac{C_u}{2} e_T \\psi_2 \\\\\ns.t. & (Aw_2 + e_1b_2) + \\xi_2 = e_1, \\\\\n& (Uw_2 + e_ub_2) + \\psi_2 = (-1 + \\varepsilon)e_u,\n\\end{aligned}$\nwhere $C_1$ and $C_2$ are positive penalty parameters, $\\varepsilon \\in (0,1)$ is the tolerance value for Universum class, $\\xi_1, \\xi_2, \\psi_1$, and $\\psi_2$ are measures of the violation of constraints associated. The unique solution to these problems is closed-form, which is advantageous for handling large-scale applications. By employing the same solution method as LS-TSVM, we can obtain the two non-parallel hyperplanes by solving the following system of linear equations.\n$\\begin{bmatrix}\nW_1 \\\\\nb_1\n\\end{bmatrix} = -(H^TH+C_1G^TG+ C_uO^TO)^{-1} (C_1G^Te_2 + C_u(1 - \\varepsilon)O^T e_u),$\n$\\begin{bmatrix}\nW_2 \\\\\nb_2\n\\end{bmatrix} = (G^TG + C_2H^TH + C_uO^TO)^{-1} (C_2H^Te_1 + C_u(1 - \\varepsilon)O^T e_u).$\nHere, we use the same notation as for U-TSVM. A new data point $x \\in \\mathbb{R}^n$ is assigned to class $i \\in \\{+1, -1\\}$ using a rule similar to that of the TSVM."}, {"title": "2.6 Quadratic Twin Support Vector Machine", "content": "Linear models, while effective for linearly separable data, may face limitations when dealing with more complex datasets. To address this challenge, kernel methods are introduced. However, these methods have their drawbacks [6]. In response, many quadratic kernel-free methods have been recently studied [19,7,9]. The idea behind these models is analogous to their linear version except they capture quadratic surfaces. Below, we elaborate on the Quadratic Kernel-Free Twin Support Vector Machine (QTSVM) model proposed by Gao et al. [6].\nLet $I_1 := \\{i \\in \\{1, ..., n\\} | y_i = 1\\}$ and $I_2 := \\{i \\in \\{1, ..., n\\} | y_i = -1\\}$. Suppose we have two classes that include $|I_1|$ and $J_2$ points, respectively. Then, the formulation"}, {"title": "2.7 Least Square Quadratic Twin Support Vector Machine", "content": "Combining the ideas of (LS-TSVM) and (QTSVM), the following model was introduced in [7] to capture two quadratic surfaces going through their corresponding classes yet distant from the other class:\n$\\begin{aligned}\n\\min_{W_1,b_1,\\xi_1,c_1} & \\sum_{i\\in I_1} (x_i^T W_1 x_i + b_1^T x_i + c_1)^2 + C_1 \\sum_{i=1}^{|I_2|} \\xi_i \\\\\ns.t. & 1 + (x_i^T W_1 x_i + b_1^T x_i + c_1) = \\xi_{1i}, \\forall i \\in I_2 \\\\\n& W_1 \\in \\mathbb{S}^n, b_1 \\in \\mathbb{R}^n, c_1 \\in \\mathbb{R}, \\xi_1 \\in \\mathbb{R}^{|I_2|},\n\\end{aligned}$   (LS-QTSVM)\n$\\begin{aligned}\n\\min_{W_2,b_2,\\xi_2,c_2} & \\sum_{i\\in I_2} (x_i^T W_2 x_i + b_2^T x_i + c_2)^2 + C_2 \\sum_{i=1}^{|I_1|} \\xi_i \\\\\ns.t. & 1 - (x_i^T W_2 x_i + b_2^T x_i + c_2) = \\xi_{2i}, \\forall i \\in I_1 \\\\\n& W_2 \\in \\mathbb{S}^n, b_2 \\in \\mathbb{R}^n, c_2 \\in \\mathbb{R}, \\xi_2 \\in \\mathbb{R}^{|I_1|}.\n\\end{aligned}$\nThis model efficiently separates nonlinear classes, offering both accuracy and speed. Its ability to handle complex relationships without significant computational overhead makes it versatile and suitable for real-world applications across various domains.\nThe standard quadratic form of the latter model is the following:\n$\\begin{aligned}\n\\min_{z_1,c_1,\\xi_1} & \\sum_{i\\in I_1} (z_i^T r_i + c_1)^2 + C_1 \\sum_{i=1}^{|I_2|} \\xi_i \\\\\ns.t. & 1 + (z_i^T r_i + c_1) = \\xi_{1i}, \\forall i \\in I_2 \\\\\n& z_1 \\in \\mathbb{R}^{\\frac{n(n+1)}{2}+n}, c_1 \\in \\mathbb{R}, \\xi_1 \\in \\mathbb{R}^{|I_2|},\n\\end{aligned}$   (LS-QTSVM')\n$\\begin{aligned}\n\\min_{z_2,c_2,\\xi_2} & \\sum_{i\\in I_2} (z_i^T r_i + c_2)^2 + C_2 \\sum_{i=1}^{|I_1|} \\xi_i \\\\\ns.t. & 1 - (z_i^T r_i + c_2) = \\xi_{2i}, \\forall i \\in I_1 \\\\\n& z_2 \\in \\mathbb{R}^{\\frac{n(n+1)}{2}+n}, c_2 \\in \\mathbb{R}, \\xi_2 \\in \\mathbb{R}^{|I_1|}.\n\\end{aligned}$\nThese analogous models have closed-from solutions. For k = 1 and 2, let us introduce the following matrices:\n$R_k = [r_{1,2,...,r_i,...,r_{|I_k|}}]$ and $D_k = diag(y_1, y_2, ..., y_{|I_k|})$.\nThen, fork = 1, solving the KKT conditions leads to:\n$\\begin{bmatrix} R_1R_1^T + C_1R_2R_2^T & R_1e_1+C_1R_2e_2 \\\\\ne_1R_1^T + C_1e_2R_2^T & |I_1|+C_1|I_2|\n\\end{bmatrix} \\begin{bmatrix} z_1 \\\\ c_1 \\end{bmatrix} = \\begin{bmatrix} C_1R_2D_2e_2 \\\\ C_1D_2e_2 \\end{bmatrix}$\nAnd, for k = 2, we get:\n$\\begin{bmatrix} R_2R_2^T + C_2R_1R_1^T & R_2e_2+C_2R_1e_1 \\\\\ne_2R_2^T + C_2e_1R_1^T & |I_2|+C_2|I_1|\n\\end{bmatrix} \\begin{bmatrix} z_2 \\\\ c_2 \\end{bmatrix} = \\begin{bmatrix} C_2R_1D_1e_1 \\\\ C_2D_1e_1 \\end{bmatrix}$\nAfter solving these systems for both classes, given a new point x, the decision function is based on the same rule analogous to that of QTSVM."}, {"title": "2.8 U-QTSVM", "content": "To integrate prior information in the form of Universum points and effectively separate nonlinear datasets, we bring the following model:\n$\\begin{aligned}\n\\min_{W_1,b_1,c_1,\\xi_1} & \\sum_{i\\in I_1} (x_i^T W_1 x_i + b_1^T x_i + c_1)^2 + C_1 \\sum_{i=1}^{|I_2|} \\xi_{1i} + C_u \\sum_{j=1}^{r} \\psi_{1 j} \\\\\ns.t. & 1 + (\\frac{1}{2}(x_i^T W_1 x_i + b_1^T x_i + c_1) \\leq \\xi_{1i}, \\forall i \\in I_2 \\\\\n& W_1^T \\frac{1}{2}(u_j W_1 u_j + b_1^T u_j + c_1) \\geq -1 + \\epsilon - \\psi_{1 j}, j=1,...,7, \\\\\n& W_1 \\in \\mathbb{S}^n, b_1 \\in \\mathbb{R}^n, c_1 \\in \\mathbb{R}, \\xi_1 \\in \\mathbb{R}^{|I_2|}, \\psi_1 \\in \\mathbb{R}^{r}.\n\\end{aligned}$   (U-QTSVM)\n$\\begin{aligned}\n\\min_{W_2,b_2,c_2,\\xi_2} & \\sum_{i\\in I_2} (x_i^T W_2 x_i + b_2^T x_i + c_2)^2 + C_2 \\sum_{i=1}^{|I_1|} \\xi_{2i} + C_u \\sum_{j=1}^{r} \\psi_{2 j} \\\\\ns.t. & 1 - (\\frac{1}{2}(x_i^T W_2 x_i + b_2^T x_i + c_2) \\leq \\xi_{2i}, \\forall i \\in I_1 \\\\\n& -(\\frac{1}{2}(u_j W_2 u_j + b_2^T u_j + c_2) \\geq -1 + \\epsilon - \\psi_{2 j}, j=1,...,7, \\\\\n& W_2 \\in \\mathbb{S}^n, b_2 \\in \\mathbb{R}^n, c_2 \\in \\mathbb{R}, \\xi_2 \\in \\mathbb{R}^{|I_1|}, \\psi_2 \\in \\mathbb{R}^{r}.\n\\end{aligned}$\nIn the standard quadratic form, the above model becomes:\n$\\begin{aligned}\n\\min_{z_1,c_1,\\xi_1,\\psi_1} & \\sum_{i\\in I_1} (z_i^T r_i + c_1)^2 + C_1 \\sum_{i=1}^{|I_2|} \\xi_{1i} + C_u \\sum_{j=1}^{r} \\psi_{1 j} \\\\\ns.t. & 1 + (z_i^T r_i + c_1) \\leq \\xi_{1i}, \\forall i \\in I_2 \\\\\n& z_i^T r_j + c_1 \\geq -1 + \\epsilon - \\psi_{1 j} j=1,...,7, \\\\\n& z_1 \\in \\mathbb{R}^{\\frac{n(n+1)}{2}+n}, c_1 \\in \\mathbb{R}, \\xi_1 \\in \\mathbb{R}^{|I_2|}, \\psi_1 \\in \\mathbb{R}^{r}.\n\\end{aligned}$   (U-QTSVM')\n$\\begin{aligned}\n\\min_{z_2,c_2,\\xi_2,\\psi_2} & \\sum_{i\\in I_2} (z_i^T r_i + c_2)^2 + C_2 \\sum_{i=1}^{|I_1|} \\xi_{2i} + C_u \\sum_{j=1}^{r} \\psi_{2 j} \\\\\ns.t. & 1 - (z_i^T r_i + c_2) \\leq \\xi_{2i}, \\forall i \\in I_1 \\\\\n& (z_i^T r_j + c_2) \\geq -1 + \\epsilon - \\psi_{2 j} j=1,...,7, \\\\\n& z_2 \\in \\mathbb{R}^{\\frac{n(n+1)}{2}+n}, c_2 \\in \\mathbb{R}, \\xi_2 \\in \\mathbb{R}^{|I_1|}, \\psi_2 \\in \\mathbb{R}^{r}.\n\\end{aligned}$\nAfter solving these quadratic programs directly or through dual theory in convex optimization, a new point is assigned to a class following the same approach outlined for QTSVM."}, {"title": "2.9 LS-U-QTSVM", "content": "Incorporating Universum points may escalate the computational complexity of our models. Therefore, opting for a least squares version of the last model is often pre-"}, {"title": "3 Imbalanced Universum Quadratic Twin Support Vector Machines", "content": "Class imbalance in binary classification poses significant challenges, leading to biased models and compromised predictive performance, particularly for minority classes. This imbalance can result in reduced sensitivity, misinterpretation of accuracy, and skewed decision boundaries, with severe consequences in critical domains like fraud detection or medical diagnoses. To address this, we draw inspiration from [17,22], incorporating Universum points to support minority classes in linearly separable datasets. Extending this to quadratic twin support vector machines, we leverage its flexibility to enhance Universum point effectiveness. Through this approach, we aim to improve classifier robustness and generalization, especially in imbalanced scenarios.\nIn the following parts, we maintain a general approach where the minority class (A) is considered as the positive class. To address the class imbalance, we employ random undersampling of the negative class (B) to create a balanced dataset in forming a quadratic surface for the minority class. Precisely, letting $A = \\{x_{1},x_{2},...,x_{|I_1|}\\rangle$ and $B = \\{X_{1},X_{2},..., X_{|I_2|}\\rangle$ with $|I_1| < |I_2|$, we randomly select a reduced sample $\\hat{B} = \\{x_{1}, x_{2},...,x_{|I_1|}\\rangle$ from the negative class. We also construct $U = \\{u_{1},u_{2}, ..., u_{r} \\rangle$ where $r = |I_2| - |I_1|$ Universum points by averaging technique. Next, we choose a reduced sample of them and construct the reduced Universum sample $\\hat{U} := \\{\\hat{u}_{1},\\hat{u}_{2},...,\\hat{u}_{g}\\}$ with $g = [|I_1|/2]$. Note that $g\u226ar$, and these values are deliberately chosen to ensure that the optimization problems associated with each class remain unbiased when an appropriate number of Universum points is added.\nThe main model of the paper presented next is designed to handle class imbalance when the positive class is assumed to be the minority. In this Imbalanced Universum Quadratic Twin Support Vector Machine (Im-U-QTSVM) model, we incorporate only g Universum points in the formulation of the minority class to better refine the generalization boundaries in its favor. To address the bias caused by the majority class, we user Universum points in its formulation, almost as many as the number of points in this class. Slack variables are also used for penalizing misclassification in the class points and noise in the Universum points. Furthermore, an $l_2$ regularization term on the Hessian of the quadratic surface has been added to enhance the stability and generalization capability of the model. Consequently, the optimization problem for the Im-U-QTSVM model, which is based on the hinge loss function, can be formulated as follows:"}, {"title": "4 Theoretical Properties of Proposed Models", "content": "In this section, we present some theoretical properties of the proposed model. We first show that optimality is always ensured. Moreover, under very mild assumptions, the optimal solution is unique.\nTheorem 1 Problem (Im-LS-U-QTSVM) has always an optimal solution.\nProof. The problem minimizes a convex quadratic function on an affine subspace, so it possesses an optimum.\nTheorem 2 Problem (Im-LS-U-QTSVM) has a unique optimal solution if and only if the set of vectors $\\{x_i, i \\in I_1; x_i, i = 1,..., |I_1|; \\hat{u}_j, j = 1, ..., g\\}$ is affinely independent.\nProof. From Theorem 1, an optimum always exists. Recall that the problem has the form of minimization of a convex quadratic function on an affine subspace. Thus, the minimum is not unique if and only if the objective function is constant in the unbounded direction. This happens if and only if there is a nontrivial solution of\n$\\begin{aligned}\n& 1 + (\\frac{1}{2}(x_i W_1 x_i + b x_i + c_1) = \\xi_{1i}, i = 1,..., |I_1| \\\\\n&\\frac{1}{2}(u W_1 \\hat{u} + b \\hat{u} + c_1) = \\psi_{1 j}, j = 1,..., g,\n\\end{aligned}$\nfor which the objective function vanishes. If the objective function is zero, then we can deduce that $\\xi_1 = 0, \\psi_1 = 0$ and $W_1 = 0$. Hence the remaining conditions take the form of\n$\\begin{aligned}\n&b x_i + c_1 = 0, i = 1,..., |I_1| \\\\\n&b \\hat{u} + c_1 = 0, j = 1,..., g, \\\\\n&b x_i + c_1 = 0, \\forall i \\in I_1.\n\\end{aligned}$\nThis characterizes linear dependence of vectors\n$\\{(x_i; 1), i \\in I_1; (x_i; 1), i = 1, ..., |I_1|; (\\hat{u}_j; 1), j = 1, ..., g\\}$,\nor, equivalently, affine dependence of the vectors in the formulation of the theorem."}, {"title": "5 Numerical Experiments", "content": "In this section, a large scale of numerical experiments are conducted to validate the performance of the proposed Im-LS-U-QTSVM model for binary classification. We first introduce the experiment settings, including the data sources, the parameter setups, etc. Then all the numerical experiments are conducted to validate the classification accuracy of the proposed model along with some related binary classification models on some public benchmark data sets. Certain statistical tests are also conducted to analyze the computational results."}, {"title": "5.1 Experiment Settings", "content": "In the numerical experiments, multiple benchmark SVM models are implemented for comparison, including the linear SVM, linear Twin SVM and least square twin SVM models. We also tested their corresponding Universum variants. All the tested models and their abbreviations are listed in Table 1. In addition, we list the Python toolboxes (commercial solvers, packages, etc.) in Table 1. All the experiments are conducted on a personal computer with eight Intel (R) Core (TM) i7-13900 @ 3.4 GHz CPUs and 64GB RAM."}, {"title": "5.3 Statistical analysis", "content": "Some statistical tests are conducted to further analyze the computational results we received in Section 5.2. We first conduct the Friedman test [4"}]}