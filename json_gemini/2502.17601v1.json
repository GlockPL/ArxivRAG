{"title": "Representation Engineering for Large-Language Models: Survey and Research Challenges", "authors": ["LUKASZ BARTOSZCZE", "SARTHAK MUNSHI", "BRYAN SUKIDI", "JENNIFER YEN", "ZEJIA YANG", "DAVID WILLIAMS-KING", "LINH LE", "KOSI ASUZU", "CARSTEN MAPLE"], "abstract": "Large-language models are capable of completing a variety of tasks, but remain unpredictable and intractable. Representation\nengineering seeks to resolve this problem through a new approach utilizing samples of contrasting inputs to detect and edit\nhigh-level representations of concepts such as honesty, harmfulness or power-seeking. We formalize the goals and methods of\nrepresentation engineering to present a cohesive picture of work in this emerging discipline. We compare it with alternative\napproaches, such as mechanistic interpretability, prompt-engineering and fine-tuning. We outline risks such as performance\ndecrease, compute time increases and steerability issues. We present a clear agenda for future research to build predictable,\ndynamic, safe and personalizable LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have powered a unique breakthrough in machine learning capabilities by showing\nhow new capabilities emerge with scale [8, 222]. Using large volumes of data and unprecedented computational\nresources, these models have demonstrated capabilities generalizing across a wide range of tasks and surpassing\nboth human and previous, narrow AI system performance [123, 136].\nLLMs now consistently outperform previous approaches across standard language understanding and reasoning\nbenchmarks, showing particular strength in tasks requiring complex logical reasoning and multi-step problem\nsolving [122, 233]. For example, in healthcare, LLMs demonstrate the ability to analyze complex medical cases,\nsuggest potential diagnoses, identify drug interactions, and assist in treatment planning [131]. LLMs have also\nhad a profound impact on software development, and are able to understand programming concepts, generate\nfunctional code, and debug complex programs [39], dramatically accelerating the development process. In\nscientific research, LLMs are capable of processing vast amounts of scientific literature, identifying patterns across\ndisparate fields, and suggesting novel research directions [47]. These models also perform well in education by\nproviding personalized tutoring, adapting to individual learning styles, and explaining complex concepts in a\nway personalized to the student's understanding [85]. Across law and finance, the models have demonstrated\nsophisticated comprehension of complex documents, regulatory requirements, and contractual terms [126].\nPerhaps most importantly, these models show an advanced ability to transfer knowledge between domains\n[47], combining insights from different fields to solve novel problems they have not seen in the training data [222].\nTheir reasoning capabilities can be extended by breaking down complex problems into manageable steps [223],\nmirroring human problem-solving approaches. Through this, intelligence becomes a scale problem, and under the\nright circumstances, LLMs are able to equally and eventually surpass human intelligence. While challenges remain\nin ensuring consistent factual accuracy [172] and adequate assurance to guarantee trustworthiness [253], these\nmodels represent a transformative shift in how human operators utilise technology. Their ability to understand\ncontext, maintain coherent long-term reasoning, and adapt to new tasks shows their potential for assisting,\nautomating and substituting most areas of human work.\nMuch of the improvement in LLM capabilities can be attributed to their size. The models have been increasing\nand improving consistently over the years, from BERT's 340M parameters [52] to GPT-3's 175B [28] and Llama's\n3.1. 405B parameters [57]. While this increase in scale has significantly enhanced their performance, it has\nalso introduced challenges. The sheer size and complexity of state-of-the-art models makes them difficult to\nverifiably control and modify [14]. With parameter sizes in billions, models often operate as black-box entities,\nwith researchers having little oversight over the actual interactions happening in the hidden layers of the model.\nIn addition to scaling, an emerging trend in LLMs is their improved reasoning ability. This advancement does\nnot strictly depend on scaling but rather on architectural innovations, training techniques, and the ability to\ngeneralize across tasks [50]. Reasoning capabilities enable LLMs to perform complex problem-solving, logical\ninference, and even exhibit some degree of common-sense understanding. However, this added sophistication\nfurther complicates efforts to ensure transparency and accountability in these systems."}, {"title": "1.1 Representation Engineering", "content": "Several approaches to the explainability of LLM have been applied over the years, including mechanistic inter-\npretability [175] [252] [17], latent saliency maps [190] [25], attribution-based methods [158] [5], counterfactual\nanalysis [40] [241], and probing techniques [56] [215]. Representation engineering takes a different approach:\nglobal identification of high-level concepts by stimulating the network with contrasting pairs of inputs to identify\ndifferences in concepts and extract related features [251]. Instead of attempting to deconstruct the network into\nindividual units, it places the activation across a global population of neurons at the heart of the analysis."}, {"title": "1.2 Related Work", "content": "Existing Survey Work. Existing deep learning literature produced a number of studies presenting large-scale\noverview of techniques and open problems in AI explainability (XAI) including general AI explainability [68],\ndeep learning explainability [183], black box models [42], large-language model XAI [251] [48] [58] [217], neural\nnetwork concept explanation [112] [178] or medical XAI [193]. More narrow studies focus on mechanistic\ninterpretability [17] [62] [100], LLM knowledge encoding [217], comparing models on the representation level\n[104] or probing [12].\nIn that, some surveys provide a brief overview of representation engineering as a counterpoint to their\nmain focus. Zhao et al. [252] provides a landscape survey for modern explainability with a brief overview of\nrepresentation engineering and analyzes representation engineering in relation to mechanistic interpretability.\nRepresentation engineering is also briefly analyzed as a potential alternative to existing explainability techniques\nin [251]. This study is fundamentally different. It is the first study to review the work on representation engineering,\nan emerging field with high empirical validation for its techniques. It aims to highlight and systematize the\ntechniques in this growing field to provide insights necessary for the creation of stable, general-purpose reading\nand interventions that can be applied across all use cases with a top-down interpretation.\nLatent Saliency Maps (LSMs). Latent Saliency Maps show how internal representations influence predictions in\nlanguage models by highlighting relevant activations, as demonstrated in emergent world models in sequence\ntasks [117]. An extension of general Latent Saliency Maps, Concept Saliency Maps (CSMs) identify high-level\nconcepts by calculating gradients of concept scores [26].\nConcept Bottleneck Models (CBMs). Pre-LLMs, Concept Bottleneck Models (CBMs) have been created as a deep\nlearning architecture that has an intermediate layer that forces models to represent information through human-\nunderstandable concepts, enabling interpretability and direct intervention [105]. CBMs have been extended\nto Concept Bottleneck Generative Models (CBGMs), where a dedicated bottleneck layer encodes structured\nconcepts, preserving generation quality across architectures like GANs, VAEs, and diffusion models [92]. However,\nCBMs suffer from \"concept leakage,\" where models bypass the bottleneck to encode task-relevant information in\nuninterpretable ways, which can be mitigated using orthogonality constraints and disentangled concept embed-\ndings [92, 142]. Concept Bottleneck Large-Language Models (CB-LLMs) integrate CB layers into transformers,\ndemonstrating that interpretable neurons can improve text classification and enable controllable text generation\nby modulating concept activations [200]. CBMs tie inference to a specific \"concept\" (representation), but usually\nhave lower accuracy than concept-free alternatives. Their effectiveness depends on the completeness and accuracy\nof the process of identifying the concept, leading to new generations of models that perform automated concept\ndiscovery [92, 103, 242]."}, {"title": "1.3 Theoretical Foundations of Representation Engineering", "content": "Linear Representation Hypothesis. The theory of why representation engineering can be used effectively is\nbased on the Linear Representation Hypothesis (LRH). LRH postulates that high-level concepts and functions\nare encoded in the activations of neural networks as linear or near-linear features, identified as directions or\nsubspace in the latent space of the model. [152, 164].\nRepresentation engineering is built upon linear representation hypothesis, as it relies on the ability to isolate,\nmanipulate, and interpret specific features within a model's latent representation space using linear methods.\nIf common features like sentiment, gender, or style were not encoded in approximately linear subspaces, tech-\nniques such as vector arithmetic, projection, and basis decomposition would not work effectively. The success\nof interventions like linear probes [4], activation steering, bias removal, and feature control hinges on the\nassumption that these properties can be modified independently without complex non-linear entanglements.\nLinear representations in LLMs emerge naturally from the interaction of the next-token prediction objective\nand the implicit bias of gradient descent, rather than from architectural constraints [95]. If representations were\nhighly non-linear or span multiple concepts, changing one attribute might unpredictably alter others, making\ncontrolled modifications impossible. Thus, the practical tools of representation engineering-feature extraction,\ninterpretability, and controlled model editing-are fundamentally dependent on representations being structured\nin a way that allows for linear operations to meaningfully adjust model behavior.\nThe LRH is supported by extensive empirical findings across NLP and vision models, demonstrating that\nhigh-level features are often encoded as linear directions in activation space. Early evidence comes from word em-\nbeddings such as Word2Vec, where simple vector arithmetic (e.g., \"king\" - \"man\" + \"woman\" = \"queen\") suggested\nthat semantic attributes are represented in a linear fashion [152]. This idea extends to modern transformer-based\nmodels, where probing studies have shown that features like syntax trees [84], part-of-speech tags [205], and\nnamed entities [127] can be recovered using linear classifiers with high accuracy. Large-language models have\nbeen shown to be effectively probed for space and time [74]. Similarly, truthfulness direction can also be found to\nsignificantly improve model performance on hallucination benchmarks [118]. In factual recall tasks, Meng et al.\n[151] demonstrated that specific residual stream activations encode factual associations in GPT models, with\ndirect interventions shifting model predictions toward or away from a known fact. Burns et al. [30] found that\nunsupervised contrastive probes can identify \u201ctruth\u201d directions in LLM activations, though generalization issues\npersist [115]. There is also evidence that individual concepts are in fact vectors [168]. In adversarial robustness\nliterature, the presence of universal jailbreaks is highly predicated on the idea of LHR being true [154].\nBau et al. [10] showed that individual neurons and linear directions in CNNs correspond to human-interpretable\nobject concepts, supporting the idea that representations align with linear subspaces rather than single neurons.\nThese results collectively reinforce that while not all representations are perfectly linear [59], many high-\nlevel abstractions in deep networks behave as approximately linear features, forming the basis for practical\nrepresentation engineering. If LRH is not true, representation engineering techniques like activation steering,"}, {"title": "1.3.2 Superposition Hypothesis", "content": "Superposition hypothesis states that the neural network represents more in-\nterpretable representations than there are dimensions in the representation space [59]. Instead of dedicating\none axis of the representation space per feature, the network represents features as distinct directions in a\nhigh-dimensional activation vector. This hypothesis supports the empirical result of why neural representations\nare often distributed and overlapping [88]; the model effectively simulates a larger set of features than the number\nof neurons by superimposing features.\nIf true, the superposition hypothesis presents validation to the representation engineering approach, but also\na challenge in practical implementation and development of general methods for representation engineering.\nBecause of how neurons are encoded, a top-down approach allows to mitigate neuron polysemanticity and\nextract the net result of activations in the latent space. On the other hand, an intervention on a particular subset\nof the latent space may lead to changes in other representations, leading to decrease in capabilities and overall\ncoherence. Furthermore, stacking several interventions may result in further unintended consequences, because\none intervention may boost some effect of the other or nullify it."}, {"title": "2 REPRESENTATION READING", "content": "Representation Reading seeks to extract information about what activations correspond to a particular repre-\nsentation from the neural network, thus showing how LLMs represent and process information. As shown in\nFigure 2, a representation can correspond to multiple entities, specifically a concept, task, or function:\n\u2022 A concept is a static property of the model like truthfulness, harmlessness, or morality.\n\u2022 A task is a particular user query that for which a beneficial representation can be amplified to complete\nthe task more effectively.\n\u2022 A function is a dynamic property of the model like outputting correct Python code, power-seeking or\nanswering a question in Spanish.\nThis section outlines methods to identify such representations. Causal manipulation experiments reveal that\naltering these abstract representations leads to predictable changes in model output, confirming that the network\nuses them for decision-making rather than passively encoding correlations [63]. Although representation engi-\nneering interventions require white box access, representation reading can be applied to black box models to\npredict overall model performance and detect harmful versions of LLMs [187]."}, {"title": "2.1 Linear Artificial Tomography", "content": "The overarching idea of representation reading is to use differing inputs to stimulate the neural activity of the\nmodel, and to use the differences in observed activity to predict which activations correspond to model behavior.\nA foundational technique to detect such activations is Linear Artificial Tomography (LAT) [260]. LAT reading\nrequires defining a stimulus and a task for detecting the neural activity corresponding to it. The stimulus can be\nas simple as the following prompt (with \" \" added to show where output would be generated):\nConsider the amount of <CONCEPT> in the following: <STIMULUS>. The amount of <CONCEPT>\nis\nThe neural activity of the network when stimulated with this model is then compared with the activity of\nthe model when given either a) a contrasting stimuli or just b) a reference stimuli with no specific concept\nor function. Based on this data, a linear model is used to identify a direction to detect the direction of the\nidentified representation. These models can either be supervised or unsupervised. In practical applications, either\na supervised linear probe or an unsupervised Principal Component Analysis (PCA) is applied."}, {"title": "2.2 Probing", "content": "Probing aims to relate specific features to activation patterns in neural networks by training supervised models\nto map activations to target variables [93] and eventually representations. Representations can be detected\nby examining the emergence of abstract, low-dimensional manifolds that encode semantic features shared by\ndifferent inputs [63]. The complexity of the probing classifier influences the results, with simpler probes often\nproviding more interpretable insights. More complex probes may achieve higher accuracy but risk inferring\nfeatures not actually used by the network. Causal analysis techniques involve interventions in the representations\nto assess the impact on the model's original performance, revealing whether certain features are genuinely\nutilized. The choice of datasets for both the original model and probing tasks significantly affects the outcomes,\nmaking it critical to choose the right dataset to probe on [110, 115].\nFigure 4 shows many example goals that can be achieved through representation reading. More is discussed in\nSection 2.2.1. The figure also shows specific techniques that are covered in Section 2.2.2 and Section 2.2.3."}, {"title": "2.2.1 Applications of Probing", "content": "Probes have been applied to investigate representations such as LLM biases\n[203], jailbreak formation mechanisms [78] and even token embeddings of senses like sounds [156], shapes\nand color [22]. Probes have been shown to encode uncertainty [212], hierarchical linguistic constituency [133],"}, {"title": "2.2.2 Linear Probes", "content": "Probing is often implemented through linear probes, simple linear classifiers trained on\nfrozen features extracted from specific layers of a pre-trained neural network at inference time. By taking\nactivations from a given layer as input and training to predict a specific property, such as part-of-speech tagging\nor sentiment analysis, linear probes show how well different layers capture certain types of information [4, 99].\nGiven a hidden representation \\(H_k\\) at layer k, a linear probe applies a weight matrix W and bias b followed by a\nsoftmax function:\n\\[f_k(H_k) = \\text{softmax}(WH_k + b)\\]\nwhere the softmax function is defined as:\n\\[\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{D} e^{z_j}}\\]\nThis transformation maps the hidden representation into a probability distribution over D classes. The parameters\nof the linear probe W and b are trained using the cross-entropy loss:\n\\[L = - \\sum_{i=1}^{D} y_i \\log(\\hat{y_i})\\]\nwhere \\(y_i\\) represents the true class label, and \\(\\hat{y_i}\\) is the predicted probability for class i. The objective is to minimize\nL, thereby aligning the learned representations with the desired classification task.\nLinear probes allow us to localise where specific types of information are encoded. These abstractions can be\nidentified by observing the convergence of embeddings for tokens with similar semantic roles, even when they\nare perceptually distinct [63]. For example, truthfulness information tends to concentrate within \u201cexact answer\ntokens\" [160]. Error detection methods can be significantly enhanced by focusing on these tokens. Intermediate\nrepresentations of LLMs can be used to predict the types of errors they might make [99].\nOther methods add new components to this setup for more precise representation identification. Representa-\ntional Similarity Analysis (RSA) uses pairwise cosine similarities between embeddings to reveal latent structure\nin task representations, enabling classification of task components without parameterized probes [22, 240].\nContrast-Consistent Search (CCS) identifies truth representations by probing embeddings associated with the\nfinal token of a statement, aiming to extract subjective probabilities of truthfulness using supervised learning on\nlabeled datasets [115]."}, {"title": "2.2.3 Other Types of Probes", "content": "Logic Tensor Probes (LTP) train two-layer neural models to probe for specific\npredicates, treating each predicate's embedding method as a hyperparameter and optimizing probe depth for\nimproved interpretability [124]. LTPs are implemented by repurposing Logic Tensor Networks as shallow"}, {"title": "3 REPRESENTATION CONTROL", "content": "Representation Control refers to the process of modifying or guiding the internal representations of concepts and\nfunctions identified in the Representation Reading step towards a particular outcome. Representation Control\ninterventions primarily take the form of an insertion of a vector (or a set of vectors) between the layers of the\nmodel at inference without retraining the model."}, {"title": "3.1 Intervention Methods", "content": "Single and Multi Property. The representations identified by representation reading techniques can be\napplied to steer the model into a particular direction. However, steering the model in multiple directions proves\ndifficult and not all methods support applying multiple directions at the same time. For example, steering the\nmodel to be both more trustworthy and less prone to jailbreaks requires support for steering the model in multiple\ndirections at the same time.\nTheoretically, a representation can be reframed as a more general one, but identifying such representation\nis more difficult and leads to unpredictible behaviour, usually leading to loss of fluency and generation of\nnonsensical tokens. Experiments show combining more general representations are usually less effective than\ncombining several injections at different layers [210]. Because of this, we define a separation between activation"}, {"title": "3.1.2 Constant or Dynamic", "content": "Once a representation has been identified and the vector (or multiple vectors)\nhas been created, it needs to be applied to the model to modify its activation space. In the process, either the\nrepresentation engineering developer needs to assign intervention strength, choosing a Constant [161, 209, 260];\nor it is set as Dynamic, i.e. determined by the model at inference time [188, 197, 206, 219]. There are several\napproaches to setting those weights dynamically, either based on a probe and unsupervised clustering [219],\nKullback-Leibler (KL) divergence [188], cosine similarity of activation matrices between unsteered and steered\nmodels [188] or gradient-based optimization [197]."}, {"title": "3.1.3 Intervention Stage", "content": "Representation engineering phases can be categorized based on where the modifications\nare applied within the model. These can be performed on all activation spaces and components [27, 32, 79, 97,\n111, 118, 167, 188, 196, 218-220, 257]. Alternatively, the intervention may target the residual stream activations,\nthat is, the hidden states that pass between layers through residual connections [23, 36, 51, 61, 80, 87, 90, 121,\n139, 161, 162, 166, 169, 194, 199, 209, 234, 258, 260]. Another approach involves modifications to the target MLP\nlayers [33, 134, 140, 174, 199, 227, 254, 259, 260], specific neurons [206, 229, 247], or changes to the model through\nfine-tuning, re-training, or other forms of weight modification [38, 41, 60, 64, 86, 107, 249]."}, {"title": "3.2 Intervention Goals", "content": "Representation control methods aim to steer the model towards particular outcomes. It is important to evaluate\neach method through the lens of its original goal, because identification and intervention on one representation\ndoes not mean the intervention is going to be successful on another representation type. The precise nature of\nrepresentations is difficult to define. A representation can range from broad traits such as honesty to specific facts\nencoded in activation patterns [83]. However, extracting such representations is challenging because probing\nmethods often fail to generalize, capturing only sample-specific features rather than the true underlying structures\n[115]. Moreover, attempts to identify representations may inadvertently conflate them with tasks the model is\nsolving rather than isolating the true concept encodings [110]. This difficulty arises because internal structures\nof LLMs do not necessarily align with human-meaningful categories, making interpretation reliant on indirect\ntechniques with inherent limitations. Defining the sample and representation scope is therefore critical, and\nthe interventions remain specific to a particular representation. Simple concepts like truthfulness are easier to\nidentify than sophisticated ones like humor or appropriateness [214].\nPersonalization. Customizing the values and response style of an LLM is important for increasing adoption,\ngeneral satisfaction and capability at completing tasks [250]. Activation steering is a lightweight method to\nsteer intended behavior without the substantial computational resources of fine-tuning, hence allowing for the\ncreation of individualized LLM experiences [32]. Identifying and applying representations of particular style is\nchallenging, but possible through activation steering [32, 36, 97, 134, 206, 227, 260]. Embedding certain moral\nvalues into the model is also possible.\nSecurity. LLMs are aligned to reject harmful requests, but adversaries may nonetheless manipulate them into\nproducing harmful outputs, for example through jailbreaks [18]. LLMs are capable of being manipulated into\nassisting in unethical requests, leaking personal data or giving toxic responses [20, 41, 55, 60, 64, 107, 111, 128, 140,\n167, 188, 194, 196, 209, 218, 237, 249, 259]. In particular, circuit breakers have proven to be particularly effective,\nrelying on identification of harmful outputs and preventing them from generating a response [259]. Representation\nengineering can be used to create defenses, but also circumvent existing safety measures for red-teaming purposes.\nActivation steering can extract unlearned information from large-language models (LLMs), demonstrating"}, {"title": "4 \u03a4\u0391\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "We structure all identified representation engineering methods in a comprehensive taxonomy. We provide a\nrelative ranking of these methods based on their performance. For performance, we evaluate whether these\nmethods have been proven by subsequent research to be less effective, whether issues lead to fluency degradation,\nand how comprehensive the original testing across multiple representations has been."}, {"title": "4.1 Linear Contrastive Steering Vector", "content": "Linear contrastive steering vectors provide a framework for modifying transformer model behavior through\ntargeted interventions in the hidden activation space. These methods leverage differences in activation pat-\nterns-obtained by contrasting selected prompt pairs or decomposing internal representations-to derive vectors\nthat steer model outputs toward desired behaviors."}, {"title": "4.1.1 Simple Contrastive Vectors", "content": "The contrastive steering vector is computed as the difference between repre-\nsentations from a target and reference scenario:\n\\[v_c = \\text{Rep}(M, T^+) \u2013 \\text{Rep}(M, T^-)\\]\nwhere \\(T^+\\) and \\(T^-\\) correspond to positive and negative stimuli for the concept under study and M is the model.\nThe intervention is applied by modifying the original representation through a controlled addition or subtraction"}, {"title": "4.1.2 Optimized Steering Vectors", "content": "Recent literature aims to optimize the simple steering vectors, increasing their\neffectiveness for particular use cases. This section presents the methods with the highest empirical validation to\nshow alternatives to simple contrastive vector interventions.\nBiPO. Bi-directional Preference Optimization (BiPO) optimizes steering vectors by adjusting the generation\nprobability of human preference data pairs instead of relying on direct activation differences from contrastive\nprompts [32]. BiPO has shown to be particularly effective at steering AI personas compared to other representation\nengineering techniques. BiPO scored higher than Contrastive Activation Addition and Freeform on every\npersonalization task it was evaluated on. This approach also improves the model performance on tasks such\nas truthfulness, hallucination mitigation, and jailbreaking defense. The vectors calculated using this approach\ntransfer across models and fine-tuned LoRAs. BiPO allows for the application of multiple steering vectors to\ninfluence multiple behaviors simultaneously without decreasing the general capabilities of the model.\nPaCE. Parsimonious Concept Engineering (PaCE) constructs a large-scale concept dictionary in the activation\nspace and uses sparse coding techniques to decompose model activations into benign and undesirable components\n[140]. By using oblique projection and a sparse linear combination of concept directions, a new vector is"}, {"title": "4.1.3 In-Context Learning and Task Vectors", "content": "In-context learning (ICL) enables large-language models to adapt\ndynamically to new tasks by leveraging additional examples to modify internal representations without parameter\nupdates by providing few-shot examples of answers to the task [28, 54, 121]. Attention mechanisms in transformers\ncan be viewed as performing a form of implicit gradient descent, effectively allowing the model to optimize for\nnew tasks on the fly [46, 213]. This can be formalized using PAC learning frameworks to establish finite sample\ncomplexity bounds [224]. ICL can be viewed as an implicit learning algorithm, where transformers encode smaller\nmodels within their activations and adjust them as new examples are introduced [3, 163]. ICL can also improve\nmodel performance through self-verification [221]. The effectiveness of ICL can be explained by implicit Bayesian\ninference, where the model infers a latent structure that connects the pretraining and inference distributions\n[230].\nICL utilises specialized attention heads (induction heads) that detect and replicate patterns in token sequences,\neffectively driving the model's ability to generalize from context [159]. A subset of induction heads, semantic\ninduction heads encode structured relationships such as syntactic dependencies and knowledge graph associations\nshowing how additional examples at inference change representation structure [179]. ICL can successfully\napproximate complex function classes, including linear models, decision trees, and even neural networks, showing\nthat transformers can implement efficient learning algorithms internally [65, 132]. ICL alters embeddings and\nattention distributions to prioritize task-relevant information while reducing sensitivity to adversarial distractions,\nalso in the case where the provided examples are not particularly useful for task performance [240]. Some studies\nshow that example-label correctness has minimal impact while others find significant sensitivity to correct label\nassignments, showing that the effectiveness of ICL depends on dataset structure and model scale [153, 239].\nIt is the structure of demonstrations that is important, as models can leverage input distribution patterns and\nsequence formats even when labels are randomized, showing that task representation might matter more than\nexact label correctness [153, 180]. Increasing the number of examples leads to significant performance gains across\ndiverse tasks [1]. While ICL consistently improves task performance over zero-shot prompting, its effectiveness\nis highly dependent on the selection and ordering of examples, as variations in these factors can cause substantial\nfluctuations in model accuracy [129, 255]. In-context learning can help the model identify the right parts of the\ninput space and the overall format of the sequence [153]. ICL is similar to RepE in that it alters the representations.\nHowever, it does not monitor the full effect providing extra tokens in the input has throughout the layers.\nICL can be used to crate a task vectors are compressed versions of contextual examples that condition\nmodel behaviour with a latent representation [80, 121]. Task vectors can be thought of as compact, low-level"}, {"title": "4.1.4 Activation Patching", "content": "Activation patching (also known as causal tracing) is a method at the intersection of\nmechanistic interpretability and representation engineering. It seeks to identify small-scale model components\nresponsible for specific behaviors by changing token representations during inference[27, 79]. Activation patching\nreplaces activations with those from a different inference run. Through this, it traces how specific activations\ncontribute to model behavior. For example, if a model completes \u201cThe Eiffel Tower is in\u201d with \u201cParis,\u201d activations\nfrom an inference with a corrupted prompt (e.g., \"The Colosseum is in\" for \"Paris\") to observe the impact on\noutput [244]. The latent representation of this placeholder is replaced with a weighted sum of the input's latent\nrepresentations from the original inference, effectively \"patching\" the activation [27, 235]. Noising and denoising\nare two primary approaches to activation patching: denoising patches clean activations into a corrupted run\nto identify sufficient components, while noising patches corrupted activations into a clean run to determine\nnecessary components [79]. Different corruption methods such as Gaussian noising and symmetric token\nreplacement-can be implemented to look at changes in interpretability [244]."}, {"title": "4.1.5 SCANS", "content": "SCANS (Safety-Conscious Activation Steering) extracts \"refusal steering vectors\" from the model's\nhidden states by contrasting activations from harmful and benign queries, identifying specific layers responsible\nfor refusal behavior [33]. This is used to prevent the model refusing to follow benign queries from the users.\nThese vectors are then used to adjust activations during inference-steering responses away from unnecessary\nrefusals. SCANS achieves this by first classifying a given query as harmful or benign using a similarity-based\nmethod that compares hidden state transitions with a learned reference direction. If a query is deemed benign,\nSCANS modifies the activations in safety-critical layers to reduce the likelihood of refusal without altering the\nmodel's core capabilities."}, {"title": "4.1.6 GRATH", "content": "GRATH generates and refines truthfulness training data using out-of-domain questions, then\niteratively fine-tunes the model through Direct Preference Optimization (DPO) to improve its alignment with\nfactual correctness [38]. What the model essentially does is self-generate pairs of information it then self-\ntruthifies itself on. GRATH significantly improves model accuracy on the TruthfulQA benchmark. GRATH is\nmore cost-effective and scalable than human-annotated datasets for improving the reliability of its outputs."}, {"title": "4.1.7 EAST", "content": "AI agents exhibit overconfidence in decision-making, often committing prematurely to actions\nwithout sufficient exploration [174]. Entropic Activation Steering (EAST) directly modifies an LLM's internal\nuncertainty representation, increasing exploration in sequential decision-making tasks. Unlike adjusting sampling\ntemperature, which minimally affects action entropy, EAST constructs a steering vector by averaging activations\nweighted by action entropy and applies it at inference time to modulate decision confidence. Experiments in\nbandit environments demonstrate that EAST significantly increases exploration, prevents premature commitment\nto suboptimal choices, and generalizes across task variants. EAST modifies both action distributions and model-\ngenerated thoughts, shifting them toward expressions of uncertainty and caution, revealing that LLMs encode\nexplicit representations of decision uncertainty that can be directly manipulated."}, {"title": "4.1.8 SAC", "content": "Sparse Activation Control (SAC) enables multi-dimensional trustworthiness improvements in large-\nlanguage models (LLMs) by identifying and modifying task-specific attention heads without interfering with\ngeneral performance [229"}]}