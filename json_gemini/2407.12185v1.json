{"title": "Satisficing Exploration for Deep Reinforcement Learning", "authors": ["Dilip Arumugam", "Saurabh Kumar", "Ramki Gummadi", "Benjamin Van Roy"], "abstract": "A default assumption in the design of reinforcement-learning algorithms is that a decision-making agent always explores to learn optimal behavior. In sufficiently complex environments that approach the vastness and scale of the real world, however, attaining optimal performance may in fact be an entirely intractable endeavor and an agent may seldom find itself in a position to complete the requisite exploration for identifying an optimal policy. Recent work has leveraged tools from information theory to design agents that deliberately forgo optimal solutions in favor of sufficiently-satisfying or satisficing solutions, obtained through lossy compression. Notably, such agents may employ fundamentally different exploratory decisions to learn satisficing behaviors more efficiently than optimal ones that are more data intensive. While supported by a rigorous corroborating theory, the underlying algorithm relies on model-based planning, drastically limiting the compatibility of these ideas with function approximation and high-dimensional observations. In this work, we remedy this issue by extending an agent that directly represents uncertainty over the optimal value function allowing it to both bypass the need for model-based planning and to learn satisficing policies. We provide simple yet illustrative experiments that demonstrate how our algorithm enables deep reinforcement-learning agents to achieve satisficing behaviors. In keeping with previous work on this setting for multi-armed bandits, we additionally find that our algorithm is capable of synthesizing optimal behaviors, when feasible, more efficiently than its non-information-theoretic counterpart.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been a tectonic shift in the most celebrated successes of deep reinforcement learning that transcends the initial arcade games (Tesauro, 1995; Bellemare et al., 2013; Mnih et al., 2015; Silver et al., 2016; Schrittwieser et al., 2020) where the sub-field began in favor of real-world applications (Dulac-Arnold et al., 2021); an incomplete list of some notable examples includes classic robotic control problems (Lillicrap et al., 2016; Schulman et al., 2016; Akkaya et al., 2019), theorem provers (Kaliszyk et al., 2018), molecular design (Popova et al., 2018; Zhou et al., 2019), superpressure balloon flight controllers (Bellemare et al., 2020), computer chip layout design (Mirhoseini et al., 2021), matrix multiplication algorithms (Fawzi et al., 2022), and sophisticated dialogue agents (Stiennon et al., 2020; Ouyang et al., 2022)."}, {"title": "2 Problem Formulation", "content": "We formulate a sequential decision-making problem as an infinite-horizon, discounted Markov De-cision Process (MDP) (Bellman, 1957; Puterman, 1994) defined by \\(M = (S, A, p, T, \\mu, \\gamma)\\). Here \\(S\\) denotes a set of states, \\(A\\) is a set of actions, \\(p : S \\times A \\rightarrow [0, 1]\\) is a deterministic reward function providing evaluative feedback signals (in the unit interval) to the agent, \\(T : S \\times A \\rightarrow \\Delta(S)\\) is a transition function prescribing distributions over next states, \\(\\mu \\in \\Delta(S)\\) is an initial state distribu-tion, and \\(\\gamma \\in [0, 1)\\) is the discount factor communicating a preference for near-term versus long-term rewards. Beginning with an initial state \\(s_0 \\sim \\mu\\), for each timestep \\(t \\in \\mathbb{N}\\), the agent observes the current state \\(s_t \\in S\\), selects action \\(a_t \\sim \\pi(\\cdot | s_t) \\in A\\), enjoys a reward \\(r_t = p(s_t, a_t) \\in [0, 1]\\), and transitions to the next state \\(s_{t+1} \\sim T(\\cdot | s_t, a_t) \\in S\\).\nA stationary, stochastic policy \\(\\pi : S \\rightarrow \\Delta(A)\\), encodes a pattern of behavior mapping individ-ual states to distributions over possible actions whose overall performance in any MDP \\(M\\) when starting at state \\(s \\in S\\) and taking action \\(a \\in A\\) is assessed by its associated action-value func-tion \\(Q^{\\pi}(s, a) = \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t p(s_t, a_t) | s_0 = s, a_0 = a]\\) where the expectation integrates over random-ness in the action selections and transition dynamics. Taking the corresponding value function as \\(V^{\\pi}(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [Q^{\\pi}(s, a)]\\) and letting \\(\\Pi \\equiv {S \\rightarrow \\triangle(A)}\\) denote the set of all stochastic policies, we define the optimal policy \\(\\pi^*\\) as achieving supremal value with \\(V^*(s) = \\sup_{\\pi \\in \\Pi} V^{\\pi}(s) = \\max_{a^* \\in A} Q^{\\pi}(s, a^*)\\) and \\(Q^*(s,a) = \\sup_{\\pi \\in \\Pi} Q^{\\pi}(s, a)\\) for all \\(s \\in S\\) and \\(a \\in A\\). As the agent interacts with the environ-ment over the course of \\(K \\in \\mathbb{N}\\) episodes, we let \\(T_k = (s_0^{(k)}, a_0^{(k)}, r_0^{(k)}, ...)\\) be the random variable denoting the trajectory experienced by the agent in the \\(k\\)th episode, for any \\(k \\in [K]\\). Meanwhile, \\(H_k = {T_1, T_2, ..., T_{k-1}} \\in \\mathcal{H}_k\\) is the random variable representing the entire history of the agent's in-teraction within the environment at the start of the \\(k\\)th episode. Abstractly, a reinforcement-learning algorithm is a sequence of policies \\({\\pi^{(k)}}\\_{k \\in [K]}\\) where, for each episode \\(k \\in [K]\\), \\(\\pi^{(k)} : H_k \\rightarrow \\Pi\\) is a function of the current history \\(H_k\\).\nThroughout the paper, we will denote the entropy and conditional entropy conditioned upon a specific realization of an agent's history \\(H_k\\), for some episode \\(k \\in [K]\\), as \\(H\\_k(X) \\doteq H(X | H\\_k = H\\_k)\\) and \\(H\\_k(X | Y) \\equiv H(X | Y, H\\_k = H\\_k)\\), for two arbitrary random variables \\(X\\) and \\(Y\\). This notation will also apply analogously to the mutual information \\(I\\_k(X; Y) \\equiv I(X;Y | H\\_k = H\\_k) = H\\_k(X) - H\\_k (X | Y) = H\\_k(Y) \u2013 H\\_k(Y | X)\\), as well as the conditional mutual information \\(I\\_k (X; Y | Z) \\equiv I(X; Y | H\\_k = H\\_k, Z)\\), given an arbitrary third random variable, \\(Z\\). Note that their dependence on the realization of random history \\(H\\_k\\) makes both \\(I\\_k(X; Y)\\) and \\(I\\_k(X; Y | Z)\\) random variables themselves. The traditional notion of conditional mutual information given the random variable \\(H\\_k\\) arises by integrating over this randomness:\n\\(\\mathbb{E} [I\\_k (X; Y)] = I(X; Y | H\\_k)\\) \\(\\mathbb{E} [I\\_k (X; Y | Z)] = I(X; Y | H\\_k, Z)\\).\nAdditionally, we will also adopt a similar notation to express a conditional expectation given the random history \\(H\\_k: \\mathbb{E}\\_k [X] \\doteq \\mathbb{E} [X|H\\_k]\\)."}, {"title": "3 Satisficing with Randomized Value Functions", "content": "In this section, we extend the preceding problem formulation to the Bayesian reinforcement learning setting used throughout this work. We then introduce a deep reinforcement-learning algorithm that yields satisficing solutions via lossy compression of the optimal action-value function, \\(Q^*\\).\n\\(Q^*\\). While this reality reflects a desire to always act in search of optimal behavior, real-world reinforcement learning must instead contend with a simple, computationally-bounded agent interacting within an overwhelmingly-complex environment where limitations on time and resources may cause optimal behavior to no longer reside within the agent's means (Arumugam & Van Roy, 2021a; Russo & Van Roy, 2022; Lu et al., 2023; Arumugam & Van Roy, 2022). A line of prior work (Russo et al., 2017; Russo & Van Roy, 2018b; Arumugam & Van Roy, 2021a;b) has studied and addressed this issue for the multi-armed bandit setting (Lai & Robbins, 1985; Bubeck et al., 2012; Lattimore & Szepesv\u00e1ri, 2020) while progress for reinforcement learning has been exclusively theoretical (Arumugam & Van Roy, 2022) in nature.\nRather than unrealistically presuming an agent has the (potentially unlimited) capacity needed to negotiate amongst these numerous choices and acquire the requisite bits of information for identifying the best option, one might instead take inspiration from human decision makers (Tenenbaum et al., 2011; Lake et al., 2017), whose cognition is known to be resource-limited (Simon, 1956; Newell et al., 1958; 1972; Simon, 1982; Gigerenzer & Goldstein, 1996; Vul et al., 2014; Griffiths et al., 2015; Gershman et al., 2015; Lieder & Griffiths, 2020; Bhui et al., 2021; Brown et al., 2022; Ho et al., 2022), and settle for a near-optimal or satisficing solution. In this paper, we build upon a long-line of work in the cognitive-science literature that formalizes the limits of bounded decision-makers using the tools of information theory and doing so in tandem with reinforcement learning (Sims, 2003; Peng, 2005; Parush et al., 2011; Botvinick et al., 2015; Sims, 2016; 2018; Zenon et al., 2019; Ho et al., 2020; Gershman & Lai, 2020; Gershman, 2020; Mikhael et al., 2021; Lai & Gershman,"}, {"title": "3.1 Randomized Value Functions", "content": "Our work operates in the Bayesian reinforcement learning (Bellman & Kalaba, 1959; Duff, 2002; Ghavamzadeh et al., 2015) setting, wherein the underlying MDP the agent interacts with is unknown and, therefore, a random variable. An agent's initial uncertainty in this unknown, true MDP \\(M\\) is reflected by a prior distribution \\(P(M \\in \\mathcal{H}\\_1)\\). A typical objective for this setting is to design a provably-efficient reinforcement-learning algorithm that incurs bounded Bayesian regret, which simply takes the traditional notion of regret and applies an outer expectation under the agent's prior to account for the unknown environment. Unlike prior work (Russo & Van Roy, 2022; Arumugam & Van Roy, 2021a; 2022), which successfully leverages rate-distortion theory in this regard, the focus of the present work is to give rise to a practical agent design that is compatible with deep reinforcement learning; up to now, this has only been realized for multi-armed bandit problems (Arumugam & Van Roy, 2021a;b)."}, {"title": "3.2 Blahut-Arimoto Randomized Value Functions", "content": "Just as an agent employing PSRL will relentlessly explore to identify the underlying MDP \\(M\\) (Aru-mugam & Van Roy, 2022), a RVF agent will engage in a similar pursuit of the optimal action-value function \\(Q^*\\). While this reality reflects a desire to always act in search of optimal behavior, real-world reinforcement learning must instead contend with a simple, computationally-bounded agent interacting within an overwhelmingly-complex environment where limitations on time and resources may cause optimal behavior to no longer reside within the agent's means (Arumugam & Van Roy, 2021a; Russo & Van Roy, 2022; Lu et al., 2023; Arumugam & Van Roy, 2022).\nWe design an agent that solves a rate-distortion optimization on a per-timestep basis once the current state has already been observed. Consequently, the learning target computed by each lossy compression problem is a target action \\(A\\_t\\) (Arumugam & Van Roy, 2021a;b; Arumugam et al., 2024) that leverages current knowledge of \\(Q^*\\) to achieve satisficing performance when executed from each state. Thus, for any state \\(s \\in S\\) and distortion threshold \\(D \\in \\mathbb{R}\\_{\\geq 0}\\), the resulting rate-distortion function is given by \\(R\\_k (s, D) = \\inf\\_{\\tilde{A} \\in A} I\\_k (Q^*; A) \\text{ s.t. } \\mathbb{E}\\_k [d\\_s(Q^*, \\tilde{A})] \\leq D\\), where the distortion function \\(d\\_s : \\mathcal{Q} \\times A \\rightarrow \\mathbb{R}\\_{>0}\\) induced by any state \\(s \\in S\\) is defined as \\(d\\_s(Q^*,\\tilde{a}) = (\\max\\_{a \\in A} Q^*(s, a) - Q^*(s,\\tilde{a}))^2\\).\nFrom an information-theoretic perspective, this formulation is akin to lossy source coding with side information available at the decoder (Wyner & Ziv, 1976; Berger & Gibson, 1998). Thinking about the extremes of this rate-distortion trade-off, notice that exclusive concern with rate minimization (\\(D \\uparrow \\infty)\\) yields a uniform distribution over all actions at each state; meanwhile, exclusive concern with distortion minimization (\\(D = 0)\\) recovers the greedy action for the particular realization of \\(Q^*\\), as RVF would. An agent only concerned with optimal behavior must obtain all \\(H\\_1(Q^*)\\) bits of information in order for each of the rate-distortion optimization problems to recover the optimal action at each state. In contrast, an agent that is only interested in target actions that are easy to learn obtains a uniform distribution over actions in every state. Naturally, the intermediate region between these extremes reflects a spectrum of satisficing policies that, within each state, focuses on learning a more tractable, near-optimal action in a manner analogous to satisficing algorithms in the multi-armed bandit setting (Arumugam & Van Roy, 2021a;b).\nWe may employ the classic Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972) to compute the channel achieving the rate-distortion limit at each timestep; rather than having an explicit distortion threshold \\(D\\), this algorithm consumes as input a Lagrange multiplier \\(\\beta \\in \\mathbb{R}\\_{>0}\\) that communicates an implicit preference for the desired trade-off between rate and distortion. While only computationally feasible for a discrete information source and a discrete channel output, the latter requirement is immediately satisfied for MDPs with a discrete action space (\\(|A| < \\infty\\)) or a suitably-fine quan-tization of a continuous action space. To address the former constraint around the information source, we may employ the so-called \"plug-in estimator\" of the rate-distortion function (Harrison & Kontoyiannis, 2008), which replaces a continuous information source with the discrete empirical dis-tribution obtained via Monte-Carlo sampling and is not only asymptotically consistent (Harrison & Kontoyiannis, 2008) but also admits a finite-sample approximation guarantee (Palaiyanur & Sahai, 2008).\nThe resulting Blahut-Arimoto Randomized Value Functions (BA-RVF) algorithm is given as Algo-rithm 1 in Appendix B. It is worth mentioning that BA-RVF does bear increased computational cost per-timestep in comparison to traditional RVF and future work may benefit from using ideas like distillation (Rusu et al., 2015) to reduce these costs during rollouts in exchange for increased computational overhead between episodes. An additional drawback of BA-RVF is the dependence of agent performance on the hyperparameter, \\(\\beta\\); unfortunately, as \\(\\beta\\) is a Lagrange multiplier (Boyd & Vandenberghe, 2004), it must be tuned on a per-problem basis although future work may benefit from finding heuristic schemes for tuning or adapting \\(\\beta\\) over time that work well across a broad range of problems."}, {"title": "4 Experiments", "content": "While the typical empirical evaluation for deep reinforcement-learning agents centers around demon-strating efficient acquisition of optimal behaviors, our experiments instead aim to elucidate how our proposed Blahut-Arimoto RVF algorithm (Algorithm 1) yields a successful generalization of the standard RVF algorithm capable of recovering a broad spectrum of satisficing solutions while still retaining the ability to gracefully address the challenge of exploration. To this end, we begin with results on two simple yet illustrative tasks that serve as unit tests of our core empirical hypothesis and leave the task of orchestrating a large-scale empirical demonstration of our algorithm to future work. In particular, we build our evaluation around two MiniGrid environments Chevalier-Boisvert et al. (2018): (1) MiniGrid-Empty-16x16-v0, a standard MiniGrid domain with a single terminal goal state providing sparse positive reward, and (2) MiniGrid-Corridor-v0, a custom designed envi-ronment containing multiple goal states which provide rewards proportional to their distance from the agent's initial position. As noted in Figure 1, both environments are partially observable where the agent is given a limited egocentric view of the world and has three movement actions available for execution: ROTATELEFT, ROTATERIGHT, and FORWARD.\nIn both domains, we train Blahut-Arimoto RVF agents with different values of the Lagrange mul-tiplier \\(\\beta \\in \\mathbb{R}\\_{>0}\\) to verify that this parameter successfully controls the trade-off between rate and distortion; concretely, larger values of \\(\\beta\\) should yield agents more concerned with learning near-optimal policies. To contextualize the results achieved by each of these Blahut-Arimoto RVF agents, we also include baseline results attained by standard DQN (Mnih et al., 2015) and RVF agents, where the latter uses an epistemic neural network (Osband et al., 2021) for representing uncertainty over \\(Q^*\\) in a computationally-efficient manner. We train all agents for 100,000 frames and report the average un-discounted episodic return achieved throughout training over multiple independent trials (8 seeds on MiniGrid-Empty-16x16-v0 and 3 seeds on MiniGrid-CorridorEnv-v0).\nIn order to further underscore how our Blahut-Arimoto RVF agent achieves satisficing behaviors while retaining the strategic effectiveness of deep exploration, we offer an additional experiment using a variant of the classic RiverSwim environment (Strehl & Littman, 2008) show in Figure 2 from Osband et al. (2013)."}, {"title": "5 Conclusion", "content": "In this work, we challenge a core premise of agent design in deep reinforcement learning: that an agent should orient its exploration in pursuit of optimal behavior without regard for the complexity of the underlying environment. Using rate-distortion theory, we offer an agent designed to prioritize exploration towards satisficing behaviors and successfully dovetails with deep reinforcement learning. Our computational results demonstrate the efficacy of this agent in not only generalizing to accom-modate satisficing solutions while retaining a graceful handling of the exploration challenge but also in synthesizing optimal solutions more efficiently than its non-satisficing counterpart. Future work still remains to precisely clarify how data efficiency factors into learning these satisficing behaviors."}, {"title": "A Preliminaries", "content": "In this section, we provide brief background on information theory, rate-distortion theory, and details on our notation. All random variables are defined on a probability space (\\(\\Omega\\), \\(\\mathcal{F}\\), \\(\\mathbb{P}\\)). For any natural number \\(N \\in \\mathbb{N}\\), we denote the index set as [N] \\(\\doteq \\) {1,2,..., N}. For any arbitrary set \\(\\mathcal{X}\\), \\(\\triangle(\\mathcal{X})\\) denotes the set of all probability distributions with support on \\(\\mathcal{X}\\). For any two arbitrary sets \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\), we denote the class of all functions mapping from \\(\\mathcal{X}\\) to \\(\\mathcal{Y}\\) as {\\(\\mathcal{X} \\rightarrow \\mathcal{Y}\\)} = {\\(f \\| f : \\mathcal{X} \\rightarrow \\mathcal{Y}\\)}.\nHere we introduce various concepts in probability theory and information theory used throughout this paper. We encourage readers to consult (Cover & Thomas, 2012; Gray, 2011; Duchi, 2021; Polyanskiy & Wu, 2022) for more background. We define the mutual information between any two random variables \\(X, Y\\) through the Kullback-Leibler (KL) divergence:\n\\(I(X; Y) = D\\_{\\text{KL}}(\\mathbb{P}((X, Y) \\in \\cdot) \\|\\| \\mathbb{P}(X \\in \\cdot)\\times\\mathbb{P}(Y \\in \\cdot)),\\)\n\\(D\\_{\\text{KL}}(P \\|\\| Q) = \\int\\_{\\{x : \\frac{dP}{dQ} > 0\\}} \\log(\\frac{dP}{dQ}) dP \\qquad P \\ll Q\\)\nwhere \\(P\\) and \\(Q\\) are both probability measures on the same measurable space and \\(\\frac{dP}{dQ}\\) denotes the Radon-Nikodym derivative of \\(P\\) with respect to \\(Q\\). We define the entropy and conditional entropy for any two random variables \\(X, Y\\) as \\(H(X) = I(X; X)\\) and \\(H(Y | X) = H(Y) - I(X; Y)\\), respectively. This yields the following identity for the conditional mutual information of any three arbitrary random variables \\(X, Y\\), and \\(Z: I(X; Y|Z) = H(X|Z) - H(X | Y, Z) = H(Y|Z) \u2013 H(Y|X, Z)\\). Through the chain rule of the KL-divergence, we obtain the chain rule of mutual information:\n\\(I(X; Y\\_1, ..., Y\\_n) = \\sum\\_{i=1}^{n} I(X; Y\\_i | Y\\_1,..., Y\\_{i-1})\\).\nHere we offer a high-level overview of rate-distortion theory (Shannon, 1959; Berger, 1971) and encourage readers to consult (Cover & Thomas, 2012) for more details. A lossy compression problem consumes as input a fixed information source \\(P(X \\in \\cdot)\\) and a measurable distortion function \\(d : \\mathcal{X} \\times \\mathcal{Z} \\rightarrow \\mathbb{R}\\_{>0}\\) which quantifies the loss of fidelity by using \\(\\mathcal{Z}\\) in place of \\(\\mathcal{X}\\). Then, for any \\(D\\in \\mathbb{R}\\_{\\geq 0}\\), the rate-distortion function quantifies the fundamental limit of lossy compression as\n\\(R(D) = \\inf\\_{\\mathcal{Z}\\in\\mathbb{Z}} I(X; Z) \\text{ such that } \\mathbb{E} [d(X, Z)] \\leq D,\\)\nwhere the infimum is taken over all random variables \\(Z\\) that incur bounded expected distortion, \\(\\mathbb{E} [d(X, Z)] \\leq D\\). Naturally, \\(R(D)\\) represents the minimum number of bits of information that must be retained from \\(X\\) in order to achieve this bounded expected loss of fidelity and, conveniently, is well-defined for abstract information source and channel output random variables (Csisz\u00e1r, 1974b). Moreover, the rate-distortion function has useful structural properties:\nFact 1. \\(R(D)\\) is a non-negative, convex, and non-increasing function of \\(D \\in \\mathbb{R}\\_{\\geq 0}\\)."}, {"title": "B Algorithms", "content": "Here we present the algorithm introduced in the main body of the paper."}, {"title": "C Related Work", "content": "Overall", "lit-erature": 1}]}