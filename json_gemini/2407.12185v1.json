{"title": "Satisficing Exploration for Deep Reinforcement Learning", "authors": ["Dilip Arumugam", "Saurabh Kumar", "Ramki Gummadi", "Benjamin Van Roy"], "abstract": "A default assumption in the design of reinforcement-learning algorithms is that a decision-making agent always explores to learn optimal behavior. In sufficiently complex environments that approach the vastness and scale of the real world, however, attaining optimal performance may in fact be an entirely intractable endeavor and an agent may seldom find itself in a position to complete the requisite exploration for identifying an optimal policy. Recent work has leveraged tools from information theory to design agents that deliberately forgo optimal solutions in favor of sufficiently-satisfying or satisficing solutions, obtained through lossy compression. Notably, such agents may employ fundamentally different exploratory decisions to learn satisficing behaviors more efficiently than optimal ones that are more data intensive. While supported by a rigorous corroborating theory, the underlying algorithm relies on model-based planning, drastically limiting the compatibility of these ideas with function approximation and high-dimensional observations. In this work, we remedy this issue by extending an agent that directly represents uncertainty over the optimal value function allowing it to both bypass the need for model-based planning and to learn satisficing policies. We provide simple yet illustrative experiments that demonstrate how our algorithm enables deep reinforcement-learning agents to achieve satisficing behaviors. In keeping with previous work on this setting for multi-armed bandits, we additionally find that our algorithm is capable of synthesizing optimal behaviors, when feasible, more efficiently than its non-information-theoretic counterpart.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been a tectonic shift in the most celebrated successes of deep reinforce- ment learning that transcends the initial arcade games (Tesauro, 1995; Bellemare et al., 2013; Mnih et al., 2015; Silver et al., 2016; Schrittwieser et al., 2020) where the sub-field began in favor of real- world applications (Dulac-Arnold et al., 2021); an incomplete list of some notable examples includes classic robotic control problems (Lillicrap et al., 2016; Schulman et al., 2016; Akkaya et al., 2019), theorem provers (Kaliszyk et al., 2018), molecular design (Popova et al., 2018; Zhou et al., 2019), superpressure balloon flight controllers (Bellemare et al., 2020), computer chip layout design (Mirho- seini et al., 2021), matrix multiplication algorithms (Fawzi et al., 2022), and sophisticated dialogue agents (Stiennon et al., 2020; Ouyang et al., 2022).\nWhile these advances are exciting, they also bring awareness to some of the harsh realities that real-world decision-making agents must inevitably face as they continue to proliferate and push the frontier into novel application areas. It is important to recognize that these agents are computation- ally bounded and, in many problems of interest, must contend with additional real-world constraints on time and other resources. As a concrete example of a scenario that behooves satisficing solu- tions over optimal ones, consider an environment designed for a sequential decision-making agent to autonomously complete tasks on the Internet (Shi et al., 2017; Yao et al., 2022), such as making e-commerce purchases or querying for pieces of information. Given the wealth of knowledge sources and vendors that can be accessed through the Internet, any one query or desired item for purchase will likely yield a minimum of hundreds, if not thousands, of potential results. As has been well known from years of research on provably-efficient exploration in reinforcement learning (Kearns &\nSingh, 2002; Kakade, 2003; Strehl et al., 2009; Jin et al., 2018), identifying the globally optimal solution technically requires sifting through this vast number of candidates from start to finish, lest the agent settle prematurely and miss out on the best purchase deal or most accurate answer for an input query. However, these environments (Shi et al., 2017; Yao et al., 2022) are incredibly rich, complex, and contain a wealth of information that likely exceeds the capacity of any one agent; pursuing optimal behaviors in such environments may no longer be a tractable endeavor as agents are forced to continually explore in order to obtain the potentially unbounded amount of information needed for synthesizing an optimal policy.\nRecent work by Arumugam & Van Roy (2022) examines this capacity-limited setting and introduces a Bayesian reinforcement-learning algorithm for prioritizing exploration around an alternative, sur- rogate learning target (Lu et al., 2023), which strikes a balance between being sufficiently simple to learn while also being suitably performant for the task at hand; crucially, they ground this procedure for learning satisficing behaviors (Simon, 1955; 1956; Newell et al., 1958; 1972; Simon, 1982) formally through lossy compression and rate-distortion theory (Shannon, 1959), resulting in a posterior-sampling algorithm that is capable of making fundamentally different exploratory choices than those of an agent purely seeking optimal behavior without regard for its own cap\u0430\u0441- ity constraints. One limitation of their proposed algorithm is that it is model-based, hindering its application to large-scale problems by the prerequisite of approximate, near-optimal planning.\nWhile progress on the open problem of high-dimensional model-based planning continues (Wang et al., 2019), this paper develops a model-free approach inspired by a long line of work that per- forms Thompson sampling (Thompson, 1933; Russo et al., 2018) over the optimal action-value function (Osband et al., 2016a; Osband, 2016; Osband et al., 2016b; 2019; 2023), bypassing the need for planning. We introduce an algorithm that amortizes the lossy compression of Arumugam & Van Roy (2021a) across the state space, allowing classic algorithms from the information theory community for computing the rate-distortion function to become viable (Blahut, 1972; Arimoto, 1972). We demonstrate this precisely through computational experiments which showcase a single deep reinforcement-learning algorithm that can be parameterized to achieve a broad spectrum of satisficing solutions, which attain varying degrees of performance.\nThe paper is organized as follows: we outline the problem formulation in Section 2 before presenting our deep reinforcement-learning algorithm in Section 3. In Section 4, we outline the core hypothesis that underlies our empirical investigation before presenting the complementary experimental results. In the interest of space, all algorithms, an overview of related work, and additional details around empirical results are relegated to the appendix."}, {"title": "2 Problem Formulation", "content": "We formulate a sequential decision-making problem as an infinite-horizon, discounted Markov De- cision Process (MDP) (Bellman, 1957; Puterman, 1994) defined by M = (S, \u0391, \u03c1, \u03a4, \u03bc, \u03b3). Here S denotes a set of states, A is a set of actions, \u03c1 : S \u00d7 A \u2192 [0, 1] is a deterministic reward function providing evaluative feedback signals (in the unit interval) to the agent, T : S \u00d7 A \u2192 A(S) is a transition function prescribing distributions over next states, \u03bc\u2208 \u0394(S) is an initial state distribu-\ntion, and \u03b3\u2208 [0, 1) is the discount factor communicating a preference for near-term versus long-term rewards. Beginning with an initial state so ~ \u03bc, for each timestep t\u2208 N, the agent observes the current state st \u2208 S, selects action at ~ \u03c0(\u00b7 | st) \u2208 A, enjoys a reward rt = \u03c1(st,at) \u2208 [0, 1], and transitions to the next state st+1 ~ T(\u00b7 | St, at) \u2208 S.\nA stationary, stochastic policy \u03c0 : S \u2192 \u2206(A), encodes a pattern of behavior mapping individ- ual states to distributions over possible actions whose overall performance in any MDP M when starting at states \u2208 S and taking action a \u2208 A is assessed by its associated action-value func- tion Q\u03c0 (s, a) = E [\u03a3\u221e\nt=0 \u03b3t \u03c1(st, at) | So = s, ao = a], where the expectation integrates over random- ness in the action selections and transition dynamics. Taking the corresponding value function as V\u03c0(s) = Ea~\u03c0(:|s) [Q\u03c0 (s, a)] and letting \u03a0 \u2261 {S \u2192 \u25b3(A)} denote the set of all stochastic policies, we define the optimal policy \u03c0* as achieving supremal value with V*(s) = sup V\u03c0(s) = max Q\u03c0(s, a*) \u03c0\u0395\u03a0 a* EA and Q*(s,a) = sup\u03c0 Q\u03c0 (s, a) for all s \u2208 S and a \u2208 A. As the agent interacts with the environ- ment over the course of K\u2208 N episodes, we let Tk = (s(k), a(k), r(k),...) be the random variable denoting the trajectory experienced by the agent in the kth episode, for any k \u2208 [K]. Meanwhile, Hk = {T1, T2, ..., Tk\u22121} \u2208 Hk is the random variable representing the entire history of the agent's in- teraction within the environment at the start of the kth episode. Abstractly, a reinforcement-learning algorithm is a sequence of policies {\u03c0(k)}k\u2208[K] where, for each episode k \u2208 [K], \u03c0(k) : Hk \u2192 II is a function of the current history Hk.\nThroughout the paper, we will denote the entropy and conditional entropy conditioned upon a specific realization of an agent's history Hk, for some episode k\u2208 [K], as Hk(X) \u2261 H(X | Hk = Hk) and Hk(X | Y) \u2261 Hk(X | Y, Hk = Hk), for two arbitrary random variables X and Y. This notation will also apply analogously to the mutual information Ik(X; Y) \u2261 I(X;Y | Hk = Hk) = Hk(X) - Hk (X | Y) = Hk (Y) \u2013 Hk(Y | X), as well as the conditional mutual information Ik (X; Y |\nZ) \u2261 I(X; Y | Hk = Hk, Z), given an arbitrary third random variable, Z. Note that their dependence on the realization of random history Hk makes both Ik(X; Y) and Ik(X; Y | Z) random variables themselves. The traditional notion of conditional mutual information given the random variable Hk arises by integrating over this randomness:\nE[Ik (X; Y)] = I(X; Y | Hk) E[Ik (X; Y | Z)] = I(X; Y | Hk, Z).\nAdditionally, we will also adopt a similar notation to express a conditional expectation given the random history Hk: Ek[X] \u2261 E[X|Hk] ."}, {"title": "3 Satisficing with Randomized Value Functions", "content": "In this section, we extend the preceding problem formulation to the Bayesian reinforcement learning setting used throughout this work. We then introduce a deep reinforcement-learning algorithm that yields satisficing solutions via lossy compression of the optimal action-value function, Q*.\n3.1 Randomized Value Functions\nOur work operates in the Bayesian reinforcement learning (Bellman & Kalaba, 1959; Duff, 2002; Ghavamzadeh et al., 2015) setting, wherein the underlying MDP the agent interacts with is unknown and, therefore, a random variable. An agent's initial uncertainty in this unknown, true MDP M is reflected by a prior distribution P(M\u2208 H\u2081). A typical objective for this setting is to design a provably-efficient reinforcement-learning algorithm that incurs bounded Bayesian regret, which simply takes the traditional notion of regret and applies an outer expectation under the agent's prior to account for the unknown environment. Unlike prior work (Russo & Van Roy, 2022; Arumugam &\nVan Roy, 2021a; 2022), which successfully leverages rate-distortion theory in this regard, the focus of the present work is to give rise to a practical agent design that is compatible with deep reinforcement learning; up to now, this has only been realized for multi-armed bandit problems (Arumugam &\nVan Roy, 2021a;b).\nIn the absence of (or without regard for) any limitations on agent capacity, one fruitful strategy for addressing this setting involves reducing the agent's epistemic uncertainty (Der Kiureghian &\nDitlevsen, 2009) through Thompson sampling (Thompson, 1933; Russo et al., 2018), resulting in the well-studied Posterior Sampling for Reinforcement Learning (PSRL) algorithm (Strens, 2000; Osband et al., 2013; Osband & Van Roy, 2014; Abbasi-Yadkori & Szepesvari, 2014; Agrawal & Jia, 2017; Osband & Van Roy, 2017; Lu & Van Roy, 2019) that enjoys rigorous theoretical guarantees for provably-efficient exploration. Despite this fact, PSRL poses a significant computational hurdle for deployment in large-scale environments as acting optimally with respect to a single posterior sample in each episode (per Thompson sampling) requires MDP planning for the optimal policy. Even while PSRL can still support efficient learning when the policy used in each episode is only near-optimal (see Algorithm 5.1 of (Osband, 2016)), obtaining such a policy is itself a computationally-intensive procedure, especially in conjunction with function approximation (Wang et al., 2019).\nDue to these challenges, recent work that combines principled, uncertainty-based exploration with deep reinforcement learning has been almost exclusively driven by the Randomized Value Functions (RVF) algorithm (Osband et al., 2016b; Osband, 2016; O'Donoghue et al., 2018; Osband et al., 2019; 2023) which begins with a prior distribution over the optimal action-value function (rather than the MDP model (\u03c1, T) as in PSRL) and, for each episode k \u2208 [K], performs Thompson sampling with respect to the current posterior distribution Q ~ P(Q* \u2208 \u00b7 | Hr). While maintaining an equivalence to PSRL in the tabular MDP setting (see Theorem 7.1 of (Osband, 2016)) alongside a complementary regret bound, RVF avoids the computational inefficiencies of PSRL by reasoning over statistically-plausible value functions and, at each episode, behaving greedily with respect to the sampled action-value function: \u03c0(k)(s) \u2208 arg maxa\u2208A Q(s, a).\nWhile preliminary instantiations of RVF with deep neural networks relied on maintaining an ap- proximate posterior over Q* via computationally-inefficient ensembles (Osband et al., 2016a; 2018; Dwaracherla et al., 2022) or hypernetworks (Dwaracherla et al., 2020), recent follow-up work has gone on to address these limitations and retain the benefits of RVF with only a modest increase in computational effort (Osband et al., 2021; 2023).\n3.2 Blahut-Arimoto Randomized Value Functions\nJust as an agent employing PSRL will relentlessly explore to identify the underlying MDP M (Aru- mugam & Van Roy, 2022), a RVF agent will engage in a similar pursuit of the optimal action-value function Q*. While this reality reflects a desire to always act in search of optimal behavior, real- world reinforcement learning must instead contend with a simple, computationally-bounded agent interacting within an overwhelmingly-complex environment where limitations on time and resources may cause optimal behavior to no longer reside within the agent's means (Arumugam & Van Roy, 2021a; Russo & Van Roy, 2022; Lu et al., 2023; Arumugam & Van Roy, 2022). A line of prior work (Russo et al., 2017; Russo & Van Roy, 2018b; Arumugam & Van Roy, 2021a;b) has studied and addressed this issue for the multi-armed bandit setting (Lai & Robbins, 1985; Bubeck et al., 2012; Lattimore & Szepesv\u00e1ri, 2020) while progress for reinforcement learning has been exclusively theoretical (Arumugam & Van Roy, 2022) in nature.\nRather than unrealistically presuming an agent has the (potentially unlimited) capacity needed to negotiate amongst these numerous choices and acquire the requisite bits of information for identifying the best option, one might instead take inspiration from human decision makers (Tenenbaum et al., 2011; Lake et al., 2017), whose cognition is known to be resource-limited (Simon, 1956; Newell et al., 1958; 1972; Simon, 1982; Gigerenzer & Goldstein, 1996; Vul et al., 2014; Griffiths et al., 2015; Gershman et al., 2015; Lieder & Griffiths, 2020; Bhui et al., 2021; Brown et al., 2022; Ho et al., 2022), and settle for a near-optimal or satisficing solution. In this paper, we build upon a long-line of work in the cognitive-science literature that formalizes the limits of bounded decision-makers using the tools of information theory and doing so in tandem with reinforcement learning (Sims, 2003; Peng, 2005; Parush et al., 2011; Botvinick et al., 2015; Sims, 2016; 2018; Zenon et al., 2019; Ho et al., 2020; Gershman & Lai, 2020; Gershman, 2020; Mikhael et al., 2021; Lai & Gershman,\n2021; Gershman, 2021; Jakob & Gershman, 2022; Bari & Gershman, 2022; Arumugam et al., 2024). Crucially, however, we do so in a manner meant to retain and generalize the elegant theoretical properties of RVF while also maintaining its compatibility with deep reinforcement learning.\nWe design an agent that solves a rate-distortion optimization on a per-timestep basis once the current state has already been observed. Consequently, the learning target computed by each lossy compression problem is a target action At (Arumugam & Van Roy, 2021a;b; Arumugam et al., 2024) that leverages current knowledge of Q* to achieve satisficing performance when executed from each state. Thus, for any state s \u2208 S and distortion threshold D \u2208 R\u22650, the resulting rate-distortion function is given by\n\\(R_k(s, D) = \\inf_{\u00c3\u2208A} I_k(Q^*; A) \\text{ s.t. } E_k [d_s(Q^*, \u00c3)] \u2264 D\\), where the distortion function \\(d_s\\): Q \u00d7 A \u2192 R>o induced by any state \\(s \u2208 S\\) is defined as \\(d_s(Q^*,\u00e3) =\n(maxa\u2208A Q*(s, a) - Q*(s,\u00e3) )^2\\).\nFrom an information-theoretic perspective, this formulation is akin to lossy source coding with side information available at the decoder (Wyner & Ziv, 1976; Berger & Gibson, 1998). Thinking about the extremes of this rate-distortion trade-off, notice that exclusive concern with rate minimization (D\u2191\u221e) yields a uniform distribution over all actions at each state; meanwhile, exclusive concern with distortion minimization (D = 0) recovers the greedy action for the particular realization of Q*, as RVF would. An agent only concerned with optimal behavior must obtain all H1(Q*) bits of information in order for each of the rate-distortion optimization problems to recover the optimal action at each state. In contrast, an agent that is only interested in target actions that are easy to learn obtains a uniform distribution over actions in every state. Naturally, the intermediate region between these extremes reflects a spectrum of satisficing policies that, within each state, focuses on learning a more tractable, near-optimal action in a manner analogous to satisficing algorithms in the multi-armed bandit setting (Arumugam & Van Roy, 2021a;b).\nWe may employ the classic Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972) to compute the channel achieving the rate-distortion limit at each timestep; rather than having an explicit distortion threshold D, this algorithm consumes as input a Lagrange multiplier \u03b2 \u2208 R>0 that communicates an implicit preference for the desired trade-off between rate and distortion. While only computationally feasible for a discrete information source and a discrete channel output, the latter requirement is immediately satisfied for MDPs with a discrete action space (|A| < \u221e) or a suitably-fine quan- tization of a continuous action space. To address the former constraint around the information source, we may employ the so-called \"plug-in estimator\" of the rate-distortion function (Harrison &\nKontoyiannis, 2008), which replaces a continuous information source with the discrete empirical dis- tribution obtained via Monte-Carlo sampling and is not only asymptotically consistent (Harrison &\nKontoyiannis, 2008) but also admits a finite-sample approximation guarantee (Palaiyanur &\nSahai, 2008).\nThe resulting Blahut-Arimoto Randomized Value Functions (BA-RVF) algorithm is given as Algo- rithm 1 in Appendix B. It is worth mentioning that BA-RVF does bear increased computational cost per-timestep in comparison to traditional RVF and future work may benefit from using ideas like distillation (Rusu et al., 2015) to reduce these costs during rollouts in exchange for increased computational overhead between episodes. An additional drawback of BA-RVF is the dependence of agent performance on the hyperparameter, \u03b2; unfortunately, as \u03b2 is a Lagrange multiplier (Boyd &\nVandenberghe, 2004), it must be tuned on a per-problem basis although future work may benefit from finding heuristic schemes for tuning or adapting \u03b2 over time that work well across a broad range of problems."}, {"title": "4 Experiments", "content": "While the typical empirical evaluation for deep reinforcement-learning agents centers around demon- strating efficient acquisition of optimal behaviors, our experiments instead aim to elucidate how our proposed Blahut-Arimoto RVF algorithm (Algorithm 1) yields a successful generalization of the\nstandard RVF algorithm capable of recovering a broad spectrum of satisficing solutions while still retaining the ability to gracefully address the challenge of exploration. To this end, we begin with results on two simple yet illustrative tasks that serve as unit tests of our core empirical hypothesis and leave the task of orchestrating a large-scale empirical demonstration of our algorithm to future work. In particular, we build our evaluation around two MiniGrid environments Chevalier-Boisvert et al. (2018): (1) MiniGrid-Empty-16x16-v0, a standard MiniGrid domain with a single terminal goal state providing sparse positive reward, and (2) MiniGrid-Corridor-v0, a custom designed envi- ronment containing multiple goal states which provide rewards proportional to their distance from the agent's initial position. As noted in Figure 1, both environments are partially observable where the agent is given a limited egocentric view of the world and has three movement actions available for execution: ROTATELEFT, ROTATERIGHT, and FORWARD.\nIn both domains, we train Blahut-Arimoto RVF agents with different values of the Lagrange mul- tiplier \u03b2\u2208 R>0 to verify that this parameter successfully controls the trade-off between rate and distortion; concretely, larger values of \u03b2 should yield agents more concerned with learning near- optimal policies. To contextualize the results achieved by each of these Blahut-Arimoto RVF agents, we also include baseline results attained by standard DQN (Mnih et al., 2015) and RVF agents, where the latter uses an epistemic neural network (Osband et al., 2021) for representing uncertainty over Q* in a computationally-efficient manner. We train all agents for 100,000 frames and report the average un-discounted episodic return achieved throughout training over multiple independent trials (8 seeds on MiniGrid-Empty-16x16-v0 and 3 seeds on MiniGrid-CorridorEnv-v0).\nIn order to further underscore how our Blahut-Arimoto RVF agent achieves satisficing behaviors while retaining the strategic effectiveness of deep exploration, we offer an additional experiment using a variant of the classic RiverSwim environment (Strehl & Littman, 2008) show in Figure 2 from Osband et al. (2013)."}, {"title": "5 Conclusion", "content": "In this work, we challenge a core premise of agent design in deep reinforcement learning: that an agent should orient its exploration in pursuit of optimal behavior without regard for the complexity of the underlying environment. Using rate-distortion theory, we offer an agent designed to prioritize exploration towards satisficing behaviors and successfully dovetails with deep reinforcement learning. Our computational results demonstrate the efficacy of this agent in not only generalizing to accom- modate satisficing solutions while retaining a graceful handling of the exploration challenge but also in synthesizing optimal solutions more efficiently than its non-satisficing counterpart. Future work still remains to precisely clarify how data efficiency factors into learning these satisficing behaviors."}, {"title": "A Preliminaries", "content": "In this section, we provide brief background on information theory, rate-distortion theory, and details on our notation. All random variables are defined on a probability space (\u03a9, F, P). For any natural number N\u2208 N, we denote the index set as [N] \u2252 {1,2,..., N}. For any arbitrary set X, \u0394(\u03a7) denotes the set of all probability distributions with support on X. For any two arbitrary sets X and y, we denote the class of all functions mapping from X to Yas {X \u2192 Y} = {f | f : X \u2192 Y}.\nHere we introduce various concepts in probability theory and information theory used throughout this paper. We encourage readers to consult (Cover & Thomas, 2012; Gray, 2011; Duchi, 2021; Polyanskiy & Wu, 2022) for more background. We define the mutual information between any two random variables X, Y through the Kullback-Leibler (KL) divergence:\n\\(I(X; Y) = D_{KL}(P((X, Y) \u2208 \u00b7) || P(X \u2208 \u2022)\u00d7P(Y \u2208 \u00b7)),\\)\n\\(D_{KL}(P || Q) = \u222b log(dP/dQ) dP  P\u226aQ\n+\n\u221e P\u226aQ\\),\nwhere P and Q are both probability measures on the same measurable space and \\(dP/dQ\\) denotes the Radon-Nikodym derivative of P with respect to Q. We define the entropy and conditional entropy for any two random variables X, Y as H(X) = I(X; X) and H(Y | X) = H(Y) \u2013 I(X; Y), respectively. This yields the following identity for the conditional mutual information of any three arbitrary random variables X, Y, and Z: I(X; Y|Z) = H(X|Z) \u2013 H(X | Y, Z) = H(Y|Z) \u2013 H(Y|X, Z). Through the chain rule of the KL-divergence, we obtain the chain rule of mutual information: \\(I(X; Y_1, ..., Y_n) = \\sum_{i=1}^n I(X; Y_i | Y_1,..., Y_{i\u22121})\\).\nHere we offer a high-level overview of rate-distortion theory (Shannon, 1959; Berger, 1971) and encourage readers to consult (Cover & Thomas, 2012) for more details. A lossy compression problem consumes as input a fixed information source P(X \u2208 \u00b7) and a measurable distortion function d : X \u00d7 Z \u2192 R>o which quantifies the loss of fidelity by using Z in place of X. Then, for any D\u2208 R\u22650, the rate-distortion function quantifies the fundamental limit of lossy compression as\n\\(R(D) = \\inf_{Z\u2208Z} I(X; Z) \\text{ such that } E [d(X, Z)] \u2264 D\\),\nwhere the infimum is taken over all random variables Z that incur bounded expected distortion, E [d(X, Z)] \u2264 D. Naturally, R(D) represents the minimum number of bits of information that must be retained from X in order to achieve this bounded expected loss of fidelity and, conveniently, is well-defined for abstract information source and channel output random variables (Csisz\u00e1r, 1974b). Moreover, the rate-distortion function has useful structural properties:\nFact 1. R(D) is a non-negative, convex, and non-increasing function of D \u2208 R\u22650."}, {"title": "B Algorithms", "content": "Here we present the algorithm introduced in the main body of the paper."}, {"title": "C Related Work", "content": "Overall, this paper touches upon two rich veins of prior work in the reinforcement-learning lit- erature: (1) provably-efficient reinforcement learning and (2) information-theoretic reinforcement learning. While there are numerous works that fall under each of these areas, we isolate a relevant, comprehensive subset below to allow for a suitably clear juxtaposition with our approach.\nOn the side of provably-efficient reinforcement learning, there are various approaches that have been developed over the course of the last two decades (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Kakade, 2003; Auer et al., 2009; Bartlett & Tewari, 2009; Strehl et al., 2009; Jaksch et al., 2010; Osband et al., 2013; Dann & Brunskill, 2015; Krishnamurthy et al., 2016; Osband & Van Roy, 2017; Azar et al., 2017; Dann et al., 2017; Agrawal & Jia, 2017; Jiang et al., 2017; Jin et al., 2018; Dann et al., 2018; Zanette & Brunskill, 2019; Du et al., 2019; Sun et al., 2019; Agarwal et al., 2020b;a; Jin et al., 2020; Dong et al., 2021; Agarwal et al., 2021; Jin et al., 2021; Foster et al., 2021; Lu et al., 2023) which vary both along the type of analyses and guarantees (bounds on PAC-MDP sample complexity, regret, or iteration complexity) as well as the underlying structural assumptions lever- aged to obtain those guarantees (tabular MDPs, linear function approximation, low-rank transition structure, bounded Bellman rank, etc.). Due to our information-theoretic analysis, our results are quite general and can be applied to any MDP without structural assumptions such as finiteness of the state-action space. Additionally, our guarantees are obtained through a regret-analysis, although translations of these bounds to corresponding PAC-MDP bounds (Kakade, 2003; Strehl et al., 2009) may be feasible (Dann et al., 2017; Jin et al., 2018).\nWithin this narrowed field of view, methods in this space can largely be segregated according to their use of optimism in the face of uncertainty or posterior sampling as the tool of choice for handling the exploration challenge, though recent work has considered blending ideas from both regimes (Dann et al., 2021; Agarwal & Zhang, 2022); while both algorithm classes are theoretically sound and empirically effective, there are cases where the latter Bayesian reinforcement learning methods can be more favorable both in theory (Osband & Van Roy, 2017) as well as in practice (Osband et al., 2016a; 2018). While some of the former optimism-based methods admit high-probability sample-complexity guarantees that depend on a sub-optimality parameter, the corresponding agents are designed to pursue optimal policies; in contrast, a core premise of this work is that an agent designer aware of a preference for satisficing behaviors over optimal ones can embed such considerations explicitly into the design of the agent which can, in turn, make fundamentally different exploratory choices during learning based on the objective of satisficing, rather than purely optimizing.\nTraditionally, the Bayesian reinforcement learning setting (Bellman & Kalaba, 1959; Duff, 2002; Ghavamzadeh et al., 2015) employed by posterior-sampling methods relies on the Bayes-Adaptive MDP (BAMDP) formulation (Duff, 2002), which is notoriously intractable even in the tabular setting (see the discussion of Arumugam & Singh (2022) for some conditions under which BAMDP planning may be resolved efficiently). The main innovation behind posterior-sampling methods (Strens, 2000)\nis a lazy updating of the agent's epistemic uncertainty, removing the aforementioned intractability. Alternatively, one can consult Ghavamzadeh et al. (2015) for details on methods that employ other approximation techniques or adhere to the PAC-BAMDP notion of efficient Bayesian reinforcement learning (Kolter & Ng, 2009; Asmuth et al., 2009; Sorg et al., 2010).\nThe core mechanism that underlies many posterior-sampling algorithms is Thompson sam- pling (Thompson, 1933; Russo et al., 2018) which, by design, is exclusively focused on obtaining optimal solutions. Even subsequent improvements to Thompson sampling that account for informa- tion gain (Russo & Van Roy, 2014; 2018a) also retain this property. Lu et al. (2023) introduce the idea of a learning target as a mechanism through which an agent may prioritize its exploration when the underlying environment is too immense and complex for the agent to be endlessly curious and pursue all available bits of information; crucially, however, their regret bounds presume that such a learning target has been computed a priori. In contrast, the agents discussed in this work adaptively compute and refine the learning target as the agent's knowledge of the underlying environment ac- cumulates. Most related to the present work are the algorithms of Arumugam & Van Roy (2021a;b) and Arumugam & Van Roy (2022), where the former algorithms employ learning targets adaptively computed via rate-distortion theory exclusively to multi-armed bandit problems while the latter translates these ideas over to PSRL, but without a concrete roadmap to a computationally-feasible instantiation. Our work remedies this final issue by maintaining and resolving epistemic uncertainty over the optimal action-value function, rather than the underlying model of the unknown MDP.\nSetting aside work on provably-efficient reinforcement learning with guarantees obtained via information-theoretic analyses, the topic of information-theoretic reinforcement learning is largely an empirical body of work, focusing on how information-theoretic quantities may be practically applied by decision-making agents to address the fundamental challenges of generalization, exploration, and credit assignment. As the algorithms of this paper are primarily designed to address the challenge of exploration, we only mention in passing that work coupling information theory to address the challenges of temporal credit assignment is nascent (van Dijk et al., 2011; Arumugam et al., 2021). Meanwhile, there is a substantial literature on information-theoretic methods to aid generalization, largely situated around the information bottleneck principle (Tishby et al., 2000), which instantiates a particular rate--distortion optimization to formalize the notion of a learned data representation that is both maximally compressive while also retaining the requisite information needed for task per- formance. This perspective leads to a information-theoretic formulation of classic state abstraction in reinforcement learning (Li et al., 2006; Van Roy, 2006; Abel et al., 2016; Abel, 2020) for lossy compression of the original MDP state space (Liu & Mahadevan, 2011; Shafieepoorfard et al., 2016; Lerch & Sims, 2018; 2019; Abel et al., 2019). A similar but distinct problem also manifests in the partially-observable MDP (POMDP) (\u00c5str\u00f6m, 1965; Kaelbling et al., 1998) setting where classic work in control theory models observational capacity limitations as an information-theoretic rate constraint (Witsenhausen, 1971; Mitter & Sahai, 1999; Mitter, 2001; Borkar et al., 2001; Crutchfield & Feldman, 2001; Poupart & Boutilier, 2002; Tatikonda & Mitter, 2004; Kostina & Hassibi, 2019) and asks how well one can control a system subject to such a rate limit.\nAligned with the perspective of this work, the bulk of the information-theoretic reinforcement- learning literature is aimed at addressing the challenge of exploration, logically expecting that novel information can serve as a useful form of intrinsic motivation (Chentanez et al., 2004; Singh et al., 2009) to guide the agent (Storck et al., 1995; Polani et al., 2001; Iwata et al., 2004; Klyubin et al., 2005; Todorov, 2007; Klyubin et al., 2008; Still, 2009; Polani, 2009; Ortega & Braun, 2011; van Dijk & Polani, 2011; Still & Precup, 2012; Tishby & Polani, 2011; Sun et al., 2011; Rubin et al., 2012; Ortega & Braun, 2013; Mohamed & Jimenez Rezende, 2015; Houthooft et al., 2016; Goyal et al., 2018; 2020). Most notable among these methods are those that have inspired popular modern deep reinforcement-learning algorithms through the \u201ccontrol as inference\" or KL-regularized reinforcement learning perspective (Toussaint, 2009; Kappen et al., 2012; Levine, 2018; Ziebart, 2010; Fox et al., 2016; Haarnoja et al., 2017; 2018; Galashov et al., 2019; Tirumala et al., 2019). We refer readers to the short survey of Arumugam et al. (2022) for an overview of how the principled Bayesian methods used in this work compare relative to these latter methods inspired by the information bottleneck\nprinciple though, in short, the fundamental issue boils down to a lack of properly alleviating the burdens of exploration (O'Donoghue et al., 2020); that said, guarantees do exist for such maximum entropy exploration schemes in the absence of reward (Hazan et al., 2019) although, translated into our real-world reinforcement learning setting where agents are fundamentally bounded, naively attempting to maximize entropy would be an ill-defined objective as all bits of information that could be acquired from the environment cannot be retained within the agent's capacity limitations, by assumption.\nFinally, we conclude by noting that the practical deep reinforcement-learning algorithm developed in this work relies on the classic Blahut-Arimoto algorithm (Blahut, 1972; Arimoto, 1972) for computing the rate-distortion function when both the information source and channel output random variables are discrete. While the algorithm is known to be theoretically-sound in general (Csisz\u00e1r, 1974b) and globally-convergent (Csisz\u00e1r, 1974a) under these conditions, various techniques have been developed in order to accelerate the Blahut-Arimoto algorithm and make it applicable to continuous information sources (Boukris, 1973; Rose, 1994; Sayir, 2000; Matz & Duhamel, 2004; Chiang & Boyd, 2004; Dauwels, 2005; Niesen et al., 2007; Vontobel et al., 2008; Naja et al., 2009; Yu, 2010). While we do not explore any of these extensions here, the reality that our proposed deep reinforcement-learning agent runs the Blahut-Arimoto algorithm on a per-timestep basis suggests that these works could serve as a useful basis for future work which more carefully studies large-scale deployment of our agent.\""}, {"title": "D Additional Minigrid Experiment Details", "content": "Algorithm Implementations"}]}