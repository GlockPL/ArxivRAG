{"title": "Falcon: Faster and Parallel Inference of Large Language Models through\nEnhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree", "authors": ["Xiangxiang Gao", "Weisheng Xie", "Yiwei Xiang", "Feng Ji"], "abstract": "Striking an optimal balance between minimal drafting la-\ntency and high speculation accuracy to enhance the infer-\nence speed of Large Language Models remains a significant\nchallenge in speculative decoding. In this paper, we intro-\nduce Falcon, an innovative semi-autoregressive speculative\ndecoding framework fashioned to augment both the drafter's\nparallelism and output quality. Falcon incorporates the Cou-\npled Sequential Glancing Distillation technique, which for-\ntifies inter-token dependencies within the same block, lead-\ning to increased speculation accuracy. We offer a comprehen-\nsive theoretical analysis to illuminate the underlying mech-\nanisms. Additionally, we introduce a Custom-Designed De-\ncoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple\nforward passes as needed, thereby boosting the number of\ndrafted tokens and significantly improving the overall accep-\ntance rate. Comprehensive evaluations on benchmark datasets\nsuch as MT-Bench, HumanEval, and GSM8K demonstrate\nFalcon's superior acceleration capabilities. The framework\nachieves a lossless speedup ratio ranging from 2.91x to 3.51x\nwhen tested on the Vicuna and LLaMA2-Chat model series.\nThese results outstrip existing speculative decoding methods\nfor LLMs, including Eagle, Medusa, Lookahead, SPS, and\nPLD, while maintaining a compact drafter architecture equiv-\nalent to merely two Transformer layers.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated excep-\ntional performance across various benchmarks, reinforcing\ntheir practical significance. These models are primarily built\non the Transformer architecture and utilize autoregressive\n(AR) decoding, effectively capturing complex dependencies\nand generating coherent sequences (Wan et al. 2024). How-\never, due to their considerable model sizes, LLMs also face\nsignificant computational overhead and latency bottlenecks\nduring inference. For example, the inference speed of the\nGLM 10B model is a mere 101 tokens per second when op-\nerated on a single Nvidia A100 GPU (Du et al. 2022). This\npresents considerable challenges for the widespread deploy-\nment and application of LLMs (Zhu et al. 2024; Xiao et al.\n2023; Ghazvininejad et al. 2019; Xu et al. 2021; Ding et al.\n2021; Guo et al. 2021; Wang et al. 2019; Guo et al. 2019a).\nIntriguingly, the inference speed of LLMs is considerably\nhampered by memory bandwidth, with the main latency bot-\ntleneck stemming from reading and writing model parame-\nters to memory, rather than from arithmetic computations\n(Xia et al. 2024). To address this issue, researchers have pro-\nposed speculative decoding to expedite the inference speed\nof LLMs without sacrificing accuracy (Zhang et al. 2024;\nGuo et al. 2020). This strategy employs a drafter that effi-\nciently generates k tokens as speculation of future decod-\ning steps within the LLMs. Subsequently, the LLM verifies\nthese tokens, with only successfully validated tokens being\naccepted as decoded tokens, thus maintaining the quality of\nthe generated output (Xia et al. 2023). By directing compu-\ntational resources towards validating pre-generated tokens,\nspeculative decoding significantly diminishes the memory\noperations needed to access LLM parameters, consequently\nboosting overall inference efficiency (Xia et al. 2024).\nWhile speculative decoding presents a potential solution\nto decrease the inference latency of LLMs, it also raises sev-\neral important questions that need further exploration. No-\ntably, existing speculative decoding methods primarily em-\nploy two drafting strategies: AR and semi-autoregressive\n(SAR) drafting. AR drafting generates tokens sequentially,\neach dependent on the preceding ones. This sequential de-\npendency constrains the parallelism of the draft models,\nleading to substantial time overheads (Guo et al. 2019b;\nSun et al. 2019; Geng, Feng, and Qin 2021; Graves et al.\n2006). In contrast, SAR drafting generates multiple tokens\nsimultaneously, enhancing the parallelization of the drafting\nprocess. However, a significant limitation of SAR drafting\nis its inability to fully capture the inter-dependencies be-\ntween tokens within the same block, potentially resulting in\na lower acceptance rate for the generated tokens (Bao et al.\n2022; Ran et al. 2020). Consequently, balancing low draft-\ning latency with high speculation accuracy poses a signifi-\ncant challenge in speculative decoding aimed at accelerating\nthe inference speed of LLMs (Xia et al. 2024).\nIn this paper, we introduce Falcon, an advanced SAR\nspeculative decoding framework engineered to boost both\nthe drafter's parallelism and the output quality, thereby am-\nplifying the inference efficiency of LLMs. Falcon integrates\nthe Coupled Sequential Glancing Distillation method, which"}, {"title": "2 Related Work", "content": "2.1 Autoregressive Drafting\nSpeculative decoding with AR drafting is a straightforward\napproach that employs a small language model as the drafter\nto generate tokens sequentially, each conditioned on its pre-\ndecessors. The generated tokens are then validated by the\nLLMs to ensure alignment (Leviathan, Kalman, and Matias\n2023; Spector and Re 2023; Chen et al. 2024; Zhao et al.\n2024). SPS (Chen et al. 2023) is the pioneering work in this\nfield, which generates draft tokens by invoking a 4B parame-\nter AR drafter and validating these drafts using 70B parame-\nter LLMs. PLD (Saxena 2023) replaces the draft model with\nsimple string matching in the prompt to generate candidate\ntoken sequences. BiLD (Kim et al. 2023) utilizes the T5-\nsmall model to generate tokens, which are then validated by\na T5-XXL model. SpecInfer (Miao et al. 2024) accelerates\nLLM serving through a tree-based inference and verification\nmechanism, organizing the drafts into a token tree. The cor-\nrectness of all candidate token sequences is verified against\nthe LLM using a tree-based decoding mechanism.\nWhile AR drafting considerably boosts inference speed,\nit also imposes extra GPU memory costs, particularly for\ndraft models with billions of parameters. Moreover, it adds\ntime overhead from the drafter that could potentially coun-\nterbalance the advantages of the improved inference speed.\nEAGLE (Li et al. 2024), the current state-of-the-art specula-\ntive decoding method with AR drafting, tackles the issue of\nhigh GPU memory usage by incorporating a token sequence\nadvanced by one time step. However, due to its inherently\nsequential nature, EAGLE is still constrained by time over-\nhead, which hampers its potential to further speed up the\ninference of LLMs.\n2.2 Semi-autoregressive Drafting\nSAR speculative decoding simultaneously generates multi-\nple tokens, maintaining the AR feature globally while easing\nit locally (Wang, Zhang, and Chen 2018; \u0141ukasz Kaiser et al.\n2018; van den Oord et al. 2018, 2016). Santilli et al. (Santilli\net al. 2023) propose that AR decoding can be restructured as\nparallel resolution of a non-linear equation through Jacobi\nand Gauss-Seidel fixed-point iterations. This technique di-\nrectly appends multiple [PAD] tokens to the end of the in-\nput prompt, facilitating parallel generation and speeding up\nexisting models. Lookahead (Zhao et al. 2024) utilizes this\ngeneration method to enable the LLMs to produce several\nseparate n-grams concurrently within a single forward pass,\nthereby reducing the latency of LLM. PASS (Monea, Joulin,\nand Grave 2023) introduces multiple learnable tokens and\nfine-tunes these token embeddings to enhance parallel de-\ncoding performance. However, these methods deviate from\nthe AR pre-training patterns, which can result in less optimal\ndraft quality.\nMedusa (Cai et al. 2024) represents the most significant\nadvancements in SAR drafting, building upon the research\nof Stern et al. (Stern, Shazeer, and Uszkoreit 2018). It opti-\nmizes the process by freezing the backbone model and in-\ncorporating additional lightweight heads into it, enabling\nthe concurrent generation of multiple tokens. Medusa ef-\nfectively alleviates the computational cost typically associ-\nated with AR drafting, thus achieving remarkable speedups.\nHowever, its inference speed is constrained by the low ac-\ncuracy of the drafter, which currently stands at 0.6. This\ndrop in accuracy results from employing the parallel pro-\ncessing mechanism, whose predictions are solely based on\ninput tokens without accounting for inter-token dependen-\ncies (Xia et al. 2023; Wertheimer et al. 2024). Therefore, the\nkey to improving the output quality of SAR drafters lies in\nstrengthening the inter-token dependencies within the same\nblock. Such enhancements would enable an optimal balance\nbetween low drafting latency and high speculative accuracy."}, {"title": "3 Falcon", "content": "The Falcon framework is an advanced SAR speculative de-\ncoding architecture that concurrently improves the drafter's\nparallelism and token acceptance rate. A comprehensive\noverview and computational process of this framework are\npresented in Section 3.1. To improve the drafter's accuracy,\nwe introduce the Coupled Sequential Glancing Distillation\n(CSGD) method, which strengthens the inter-dependencies\namong tokens within each block. An in-depth description of\nthe CSGD method is provided in Section 3.2, followed by a\ntheoretical discussion in Section 3.3. In addition, a Custom-Designed Decoding Tree has been developed to support\nSAR drafting, which is described in Section 3.4.\n3.1 Framework and Computational Process\nThe architecture and computational process of the Falcon\nare depicted in Figure 1. It comprises three main compo-\nnents: the Embedding Layer, the Language Model (LM)\nHead, and the Relaxed Causal SAR Decoding Head. The\nEmbedding Layer transforms a sequence of tokens into\na corresponding sequence of token embeddings. The LM\nHead computes the probability distribution based on the ag-\ngregated features, from which the next token is sampled.\nBoth the Embedding Layer and the LM Head leverage the\nparameters of LLMs. Relaxed Causal SAR Head will be in-\ntroduced in section 3.2.\nEAGLE's findings (Li et al. 2024) indicate that concate-\nnating feature and token sequences from one time step ahead\nencapsulate richer semantic context. This enhancement al-\nlows the model to make more informed predictions by lever-\naging a broader scope of information. Consequently, we\nconcatenate consecutive feature sequences and token se-\nquences from one time step ahead to predict the next k to-\nkens concurrently. For instance, when k = 2, Falcon pre-\ndicts the feature sequence $(f_3, f_4)$ using the initial feature se-\nquence $(f_1, f_2)$ and the token sequence $(t_2, t_3)$ advanced by\none time step. Subsequently, the predicted features $(f_3, f_4)$,\nalong with the next token sequence $(t_4, t_5)$ are concatenated\nto form the new input sequence. This is used to predict\nsubsequent feature sequences $(f_5, f_6)$ and token sequences\n$(t_6, t_7)$, facilitating the continuation of the drafting process.\n3.2 Coupled Sequential Glancing Distillation\nWe have designed an SAR decoding method based on\nCSGD, aimed at enhancing the accuracy of the drafter. The\ntraining procedure is illustrated in Figure 2. Here, the fea-\nture and token sequences from one time step ahead are con-\ncatenated and fed into the drafter, resulting in a fused se-\nquence of dimensions (bs, seq_len, 2 * hidden_dim). The\ndrafter is composed of a hybrid Transformer network, which\nincludes two layers of LSTM (Hochreiter and Schmidhuber\n1997), Relaxed Causal-Masked Multi-Head Self-Attention,\nand MLP networks. The LSTM network reduces the dimen-\nsion of the fused sequence to (bs, seq_len, hidden_dim) and\nretains information about past tokens, thereby improving the\nmodel's accuracy. The Relaxed Causal-Masked Multi-Head\nSelf-Attention mechanism enables the model to focus on rel-\nevant parts of the input sequence while preserving causality.\nThe MLP layers further process this information to make the\nfinal predictions.\nAfter the sequence passes through the drafter for the first\ntime, an initial prediction of tokens, denoted as $\\hat{Y}$ is gen-\nerated. We compare the Hamming Distance (Roman 1992)\nbetween the prediction from the drafter $\\hat{Y}$ and the predic-"}, {"title": "3.3 Theoretical analysis of CSGD", "content": "We use a theory of information argument to illustrate the\nimpact of CSGD. Take k = 2 as an example, let X represent\nthe next token, Y the second-next token, and C the input\ncontext (Omitted from equations for simplicity). Traditional\nAR methods focus on H(X), whereas SAR decoding with\nk = 2 targets H(X) + H(Y), decomposed as (Gloeckle\net al. 2024; Olsson et al. 2022):\n$H(X) =H(X|Y) + I(X; Y)$ (9)\n$H(X) + H(Y) =H(Y|X) + 2I(X; Y) + H(X|Y)$ (10)\nIn Equation (10), the left two terms represent the training\nloss of 2-token prediction models. The right terms decom-\npose the loss into a local cross-entropy term for the pre-\nfix (C, X), denoted as H(Y|X), a mutual information term\nthat captures the information about Y contained in X, de-\nnoted as I(X; Y), and term H(X|Y) describes the uncer-\ntainty about X given the prefix C and suffix Y. We can ob-\nserve that SAR decoding increases the weight of I(X; Y).\nHowever, conventional training methods for SAR decod-\ning typically consider only the classical next-token entropy\nH(YX), while the terms I(X; Y) and H(XY) are al-\nways overlooked. As a result, these methods do not effec-\ntively learn the dependencies between tokens within a block.\nHowever, CSGD addresses this issue. When predicting X,\nCSGD can leverage the features and tokens from one time\nstep ahead in C. The feature sequences represent the prefix,\nand the token sequences represent the mutual information\ncapturing the correlation between C and X. Therefore, the\nterm H(X|Y) should be changed to H(X|C'), and I(X; Y)\nshould be changed to I(X; C). In addition, when predict-\ning token Y, CSGD can see the information about C and\nX simultaneously. Therefore, the training loss of CSGD is\nchanged to:\n$H(X) =H(X|C) + I(X; C)$ (11)\n$H(X) + H(Y) =H(X|C) + I(X; C)$\n$+ H(Y|X) + I(X; Y)$ (12)\nEquations (11) and (12) denote CSGD improves the depen-\ndency between tokens within a block, making the training\nloss of the SAR approach more similar to that of the AR\napproach. As a result, the probability distribution of mul-\ntiple tokens predicted simultaneously by SAR decoding be-\ncomes more aligned with the distribution of tokens predicted\nsequentially by AR decoding. It further increases the prob-\nability that the tokens generated by SAR drafting will be\naccepted by the LLMs."}, {"title": "3.4 Custom-Designed Decoding Tree", "content": "A Custom-Designed Decoding Tree has been introduced\nto accommodate SAR drafting. It enables the drafter to per-\nform m forward passes and generate n*k tokens for each for-\nward pass, where n is the number of the tree nodes. These\nm*n*k tokens are then organized in a tree structure. Mean-\nwhile, the generated tokens and the corresponding features\nare concatenated for the subsequent forward passes. In tra-\nditional AR decoding, a causal mask is used, structured as a\nlower triangular matrix. It ensures that the former tokens can\nnot access the later information. In contrast, Falcon employs\na Relaxed Causal Mask, which allows the model to access\ntokens within the same k*k block and their predecessors in\nthe tree, illustrated in Figure 3.\nSubsequently, the drafts are organized as a token tree,\nwhose nodes each represent a candidate token sequence. The\ncorrectness of the candidate token sequences are verified by\nthe LLMs using a tree-based parallel decoding mechanism,\nwhich is consistent with SpecInfer (Miao et al. 2024). We\nrecord the accepted tokens and their corresponding features\nfor the subsequent drafting phase.\nOur custom-designed decoding tree, engineered specifi-\ncally for SAR, is more efficient than those used for fully AR\napproaches. In SAR decoding, a tree-structure drafter gen-\nerates k times as many tokens as an AR decoding drafter\nin a single forward pass. As a result, m forward passes can\npropose k*m*n tokens, which is k times greater than an AR\ntree would generate. This enhancement significantly boosts\nthe efficiency of token generation for drafters, allowing the\nLLMs to verify more tokens concurrently. Therefore, this\nimprovement increases the likelihood of drafting tokens be-\ning accepted by the LLMs, thereby accelerating the overall\ninference speed of the LLMs."}, {"title": "4 Experiments and Analysis", "content": "Models and Tasks\nWe conducted experiments on Vicuna models (7B, 13B)\nand LLaMA2-Chat models (7B, 13B). We evaluated Falcon\nacross multiple tasks, including multi-turn dialogue, code\ngeneration, and mathematical reasoning, employing the MT-\nbench (Zheng et al. 2024), HumanEval (Chen et al. 2021),\nand GSM8K (Cobbe et al. 2021) datasets, respectively. We\nconducted experiments with a batch size of 1, a temperature\nof 0 (greedy decoding), and adopted experimental settings\nconsistent with other works like Eagle and Medusa.\nMetrics\nLike other speculative decoding methods, Falcon primarily\nfocuses on latency rather than throughput. We assess accel-\neration effects using the following metrics:\n\u2022 Wall-time speedup ratio: The actual test speedup ratio\nrelative to vanilla AR decoding.\n\u2022 Acceptance rate (a): The ratio of tokens generated by\nthe drafter to all tokens gauges draft accuracy.\n\u2022 Average acceptance length (7): The number of accepted\ntokens in each draft phase.\nTraining\nFalcon was trained on the ShareGPT dataset, utilizing\n68,000 dialogue iterations. We employed the AdamW op-\ntimizer with beta values ($\\beta_1, \\beta_2$) set to (0.9, 0.95) with a\nlearning rate set to 3e-5. The settings are consistent with Ea-\ngle (Li et al. 2024). The semi-autoregressive head is train-\nable within two days on an H800 server."}, {"title": "4.1 Effectiveness", "content": "Figure 4 and Table 1 show the speedup comparison among\nFalcon and other state-of-the-art speculative decoding meth-\nods on MT-bench, HumanEval and GSM8K for greedy (tem-\nperature=0). Speedup ratios of Falcon, Eagle, and Medusa\nwere fairly tested on an H800 server. The results of Specula-\ntive Sampling (SpS), Prompt lookup decoding (PLD), and\nLookahead were copied from their technical reports. Fal-\ncon has achieved a speedup ratio ranging from 2.91x-3.51x.\nCompared to AR drafters, Lookahead and Eagle, Falcon is\nfaster by 1.79x-2.06x and 1.01x-1.24x, respectively. This\nimprovement is because of Falcon's SAR property; the draft\nmodel can generate multiple tokens each forward, which re-\nduces the latency of each drafting phase and thus increases\nthe total speedup ratio. Compared to Medusa, which is a\nSAR method as well, Falcon is faster by 1.24x-1.50x. It is\nattributed to the consideration of the dependency among to-\nkens. Falcon adopts a CSGD method to maintain token de-\npendency in the training stage. This enables the drafter to\npredict tokens more accurately than Medusa, effectively re-\nducing the cost from the verification phase.\nTable 2 illustrates the Acceptance rate (a) of Falcon,\nMedusa, and Eagle. The experiments were performed on the\nH800 server. Falcon outperforms Eagle by 3% to 5%, in-\ndicating that more tokens are generated by the draft model\nrather than the target LLM. Compared to the SAR method\n(Medusa), Falcon achieves a higher a by 15.76%-17.83%,\nwhich illustrates the importance of the token dependency in\nthe draft phase."}, {"title": "4.2 Ablation Study", "content": "Tree Attention Falcon, similar to Eagle and Medusa, em-\nploys tree attention in both drafting and verification phases,\nwhile methods like speculative decoding do not use it. To\nremove the effect of tree attention, we applied a chain to\nFalcon, whose length is equal to the tree height, in order\nto maintain the same forward times of the draft model. Ta-\nble 4 shows the comparative results indicating the impact of\ntree attention. In Falcon, the implementation of tree attention\nmechanisms has been demonstrated to enhance the acceler-\nation ratio by 1.12x, increase a by 1.22, and improve\n$\\tau$ by\n4.96%. This improvement is attributed to the fact that the\ntree attention increases the number of tokens validated and\naccepted, thereby augmenting the overall token throughput.\nCoupled Sequential Glancing Distillation The SAR ap-\nproach of Falcon enhances the token numbers of each for-\nward at the cost of reduced accuracy. The CSGD training\nmethod was employed to mitigate the drop in precision.\nWe conducted tests on Vicuna 7B under two conditions:\ntraining the SAR head with Eagle's token-shift method (Li\net al. 2024) alone and with the CSGD method. Table 4 illus-\ntrates the impact of CSGD. The implementation of CSGD\nresulted in a 1.17x increase in acceleration ratio, 0.56 in \u0442,\nand 3.26% improvement in a. The above results demonstrate\nthat CSGD can significantly improve the drafter's accuracy.\nk factor The k factor determines the number of tokens the\ndrafter generates in a single forward pass, which is important"}, {"title": "5 Conclusion and Future work", "content": "In this paper, we propose Falcon, which utilizes the Coupled\nSequential Glancing Distillation method to bolster the inter-\ndependencies among tokens, thereby enhancing the quality\nof the drafts. We also provide a theoretical analysis that clari-\nfies the inner workings of our method. Additionally, we have\ndeveloped a specialized decoding tree to support SAR draft-\ning, which increases the token acceptance rate. Comprehen-\nsive evaluations indicate Falcon's superior performance over\nthe Vicuna and LLaMA2 model series. On the MT-bench,\nHumanEval, and GSM8K datasets, Falcon is 2.91x-3.51x\nfaster than vanilla Transformer, while maintaining a com-\npact size comparable to two Transformer layers. Falcon ap-\npeals to applications that demand real-time responses and\nhave limited computing resources.\nThe main challenge in accelerating the LLMs through\nspeculative decoding lies in improving the accuracy and\nefficiency of the drafter under resource-limited conditions.\nTherefore, our future research efforts will focus on devel-\noping drafters that attain a high token acceptance rate with\nminimal overhead. This goal will be met by advanced semi-\nautoregressive or potentially non-autoregressive decoding\ntechniques that fortify the interdependence among tokens.\nIn addition, the dynamic modification of the decoding tree\nis another avenue that warrants further exploration."}]}