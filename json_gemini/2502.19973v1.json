{"title": "Can Large Language Models Unveil the Mysteries?\nAn Exploration of Their Ability to Unlock Information in Complex Scenarios", "authors": ["Chao Wang", "Luning Zhang", "Zheng Wang", "Yang Zhou"], "abstract": "Combining multiple perceptual inputs and performing\ncombinatorial reasoning in complex scenarios is a sophis-\nticated cognitive function in humans. With advancements\nin multi-modal large language models, recent benchmarks\ntend to evaluate visual understanding across multiple im-\nages. However, they often overlook the necessity of com-\nbinatorial reasoning across multiple perceptual informa-\ntion. To explore the ability of advanced models to integrate\nmultiple perceptual inputs for combinatorial reasoning in\ncomplex scenarios, we introduce two benchmarks: Clue-\nVisual Question Answering (CVQA), with three task types\nto assess visual comprehension and synthesis, and Clue of\nPassword-Visual Question Answering (CPVQA), with two\ntask types focused on accurate interpretation and applica-\ntion of visual data. For our benchmarks, we present three\nplug-and-play approaches: utilizing model input for rea-\nsoning, enhancing reasoning through minimum margin de-\ncoding with randomness generation, and retrieving seman-\ntically relevant visual information for effective data inte-\ngration. The combined results reveal current models' poor\nperformance on combinatorial reasoning benchmarks, even\nthe state-of-the-art (SOTA) closed-source model achieves\nonly 33.04% accuracy on CVQA, and drops to 7.38% on\nCPVQA. Notably, our approach improves the performance\nof models on combinatorial reasoning, with a 22.17% boost\non CVQA and 9.40% on CPVQA over the SOTA closed-\nsource model, demonstrating its effectiveness in enhancing\ncombinatorial reasoning with multiple perceptual inputs in\ncomplex scenarios. The code will be publicly available.", "sections": [{"title": "1. Introduction", "content": "In complex scenarios, humans can integrate multiple per-\nceptual information to perform combinatorial reasoning.\nAs shown in Figure 1, color sequences are utilized to re-\norder colored numbers within complex scenarios. In re-\ncent years, Multi-modal Large Language Models (MLLMs)\n[3, 19, 24, 36, 44] have advanced the development of vi-\nsual language tasks by integrating visual encoders into pre-\ntrained LLMs [24, 42, 43] to enable visual processing capa-\nbilities. Increasing research attention has been directed to-\nward visual understanding in complex scenarios, including\nidentifying positional relationships among entities [9, 27]\nand reasoning in complex scenarios [34, 54], such as vi-\nsual commonsense reasoning [1, 46, 55], which often relies\non single facts rather than integrating multiple information\nsources. However, prior studies have not examined models'\nabilities to combine multiple perceptual inputs and perform\ncombinatorial reasoning in complex scenarios.\nPrompt engineering has recently become the popular\nmethod for enhancing model performance, particularly in\nChain of Thought (COT) reasoning [49] and reflection\nprompt [31]. To explore its role in combinatorial reasoning\ntasks that integrate multiple perceptual inputs in complex\nscenarios, we conduct tests by manual prompts to stimu-\nlate more thoughtful model responses and select 36 com-\nplex scenarios from the game 'Can you escape?' [38], such\nas \"combination of reasoning between color sequences and\ncolored numbers\" in Figure 1, which require combinatorial\nreasoning across multiple perceptual information. To en-\ncourage thoughtful responses, we utilize manual prompts to"}, {"title": "2. Problem Formulation", "content": "In this section, we introduce the problem formulation for\nthe two benchmarks CVQA and CPVQA."}, {"title": "2.1. Formulation of CVQA", "content": "Given a question text Qc and an image set Ic\n=\n{i1, i2, ..., in}, CVQA can be categorized into three types:\nprops search, props usage, and password clues.\n\u2022 Props search. The question text Qc serves as a guide,\ndirecting the model to combine information in complex\nscenarios Ic and identify objects that can be utilized. As\nillustrated in Figure 3, the Qe is 'Please find the props\nor related clues that can be used (interactive) in these\nscenes.', guiding the model to infer that the half-pipe in\nscene 4 can be applied to the missing pipe in scene 1.\n\u2022 Props usage. The question text Qc explicitly specifies\nthe name of the prop te, and infers where te can be ap-\nplied from complex scenarios Ic. As shown in Figure 6,\nthe Qe is 'And you have a prop: USB, what can this prop\ninteract with?', guiding the model to infer the appropri-\nate application of te based on Qe and the te combination,\nspecifically the computer.\n\u2022 Password clues. The clues for the password, such as nu-\nmeric codes or letter sequences, are explicitly provided in\nthe question text Qe, while clues to decode the password\nin complex scenarios I are inferred, or the final sequence\nis directly given. As shown in Figure 7, Qe is 'Reason\nwithin this scenario and derive clues about the numeric\ncode.', where 'numeric code' represents password clues.\nThe model is instructed to either infer password-related\nclues by integrating all scenarios in Ic (i.e., computer) or\ndirectly provide the password, in this case, '8462'."}, {"title": "2.2. Formulation of CPVQA", "content": "Given a question text Qp and an image set Ip\n=\n{ia, ib,..., ik}, CPVQA is divided into two categories:\npasswords and sequence rearrangement.\n\u2022 Passwords. The clues for the password, such as a nu-\nmeric code or a sequence of letters, are explicitly pro-\nvided in the problem text Qp, while the sequence is in-\nferred within complex scenarios Ip. As illustrated in Fig-\nure 8, Qp is Reasoning in this scenario and deriving\na numerical code.', where 'numeric code' serves as the\npassword clue. The model is then directed to infer the nu-\nmeric password sequence (e.g., 8462) by combining the\npassword sequence found on the computer with the clues\nin the Qp.\n\u2022 Sequence rearrangement. The problem text Qp uti-\nlizes codes to represent entities that need to be rearranged\nwithin a scene ik in Ip, requiring the model to integrate\ninformation from all complex scenarios in Ip to infer the\nfinal sequence code. As illustrated in Figure 3, Qp en-\ncodes the trophies in ik, directing the model to combine\nthe trophy sequence in scene x with the encoding defined\nin Qp to infer the final sequence, (e.g., BCDA)."}, {"title": "3. Benchmark", "content": "In this section, two benchmarks, CVQA and CPVQA, are\nintroduced in detail. First, we present the structural com-\nposition of the two benchmarks in Sec. 3.1, followed by an\nextension of our construction method and the effectiveness\nof this method is demonstrated in Sec. 3.2."}, {"title": "3.1. Benchmark Structure", "content": "In this subsection", "Can You\nEscape?": 38, "scenel\"\nin each image set is the main scene, and the other images\nare details of this scene. In addition to visual realism, our\nbenchmarks offer several significant advantages": 1, "themes": "While most existing datasets treat a sin-\ngle image as an independent scene", "types": "Our bench-\nmarks encompass a variety of reasoning tasks within the\nsame scenario", "adaptabil-\nity": "We utilize advanced image processing tools to modify\nand augment images, a process that would be costly in real-"}]}