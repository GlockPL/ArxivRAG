{"title": "Can Large Language Models Unveil the Mysteries?\nAn Exploration of Their Ability to Unlock Information in Complex Scenarios", "authors": ["Chao Wang", "Luning Zhang", "Zheng Wang", "Yang Zhou"], "abstract": "Combining multiple perceptual inputs and performing combinatorial reasoning in complex scenarios is a sophisticated cognitive function in humans. With advancements in multi-modal large language models, recent benchmarks tend to evaluate visual understanding across multiple images. However, they often overlook the necessity of combinatorial reasoning across multiple perceptual information. To explore the ability of advanced models to integrate multiple perceptual inputs for combinatorial reasoning in complex scenarios, we introduce two benchmarks: Clue-Visual Question Answering (CVQA), with three task types to assess visual comprehension and synthesis, and Clue of Password-Visual Question Answering (CPVQA), with two task types focused on accurate interpretation and application of visual data. For our benchmarks, we present three plug-and-play approaches: utilizing model input for reasoning, enhancing reasoning through minimum margin decoding with randomness generation, and retrieving semantically relevant visual information for effective data integration. The combined results reveal current models' poor performance on combinatorial reasoning benchmarks, even the state-of-the-art (SOTA) closed-source model achieves only 33.04% accuracy on CVQA, and drops to 7.38% on CPVQA. Notably, our approach improves the performance of models on combinatorial reasoning, with a 22.17% boost on CVQA and 9.40% on CPVQA over the SOTA closed-source model, demonstrating its effectiveness in enhancing combinatorial reasoning with multiple perceptual inputs in complex scenarios. The code will be publicly available.", "sections": [{"title": "1. Introduction", "content": "In complex scenarios, humans can integrate multiple perceptual information to perform combinatorial reasoning. As shown in Figure 1, color sequences are utilized to re-order colored numbers within complex scenarios. In recent years, Multi-modal Large Language Models (MLLMs) [3, 19, 24, 36, 44] have advanced the development of visual language tasks by integrating visual encoders into pre-trained LLMs [24, 42, 43] to enable visual processing capabilities. Increasing research attention has been directed toward visual understanding in complex scenarios, including identifying positional relationships among entities [9, 27] and reasoning in complex scenarios [34, 54], such as visual commonsense reasoning [1, 46, 55], which often relies on single facts rather than integrating multiple information sources. However, prior studies have not examined models' abilities to combine multiple perceptual inputs and perform combinatorial reasoning in complex scenarios.\nPrompt engineering has recently become the popular method for enhancing model performance, particularly in Chain of Thought (COT) reasoning [49] and reflection prompt [31]. To explore its role in combinatorial reasoning tasks that integrate multiple perceptual inputs in complex scenarios, we conduct tests by manual prompts to stimulate more thoughtful model responses and select 36 complex scenarios from the game 'Can you escape?' [38], such as \"combination of reasoning between color sequences and colored numbers\" in Figure 1, which require combinatorial reasoning across multiple perceptual information. To encourage thoughtful responses, we utilize manual prompts to guide the model's most confident choices, helping it gradually integrate multiple perceptual inputs and complete combinatorial reasoning, as shown in Figure 2. Specifically, the model determines whether to continue exploring the scene or proceed with a final action through five binary classification tasks, each featuring distinct prompt designs. A joint calibration set is constructed based on the original calibration set and the confidence interval from conformal prediction [56], enabling the calculation of inconsistency degrees. The action corresponding to the minimum inconsistency is then selected as the next step. If the model opts to continue exploring, an artificial cue is introduced, and the scene is described in detail. Alternatively, if the model decides to resolve the mystery, all scene images and cues are utilized for prediction. Experimental results show that, despite prompt engineering for ChatGPT-4o [24], reasoning failed in 36 scenarios, underscoring prompt engineering's limited impact and details are in Sec. 5.1.\nTo address the absence of evaluation of combining multiple perceptual inputs and performing combinatorial reasoning in complex scenarios, we propose two benchmarks: 1) Clue-Visual Question Answering (CVQA) comprises three task types designed to test a model's ability to integrate textual and visual perceptual information to identify relevant clues from unspecified visual data. The first task involves reasoning with a combination of unspecified visual entities located in different scenes that can interact, such as a \"key\" in one scene that can be used to unlock a \"lock\" in another scene. The second task involves reasoning with specified textual entities and unspecified visual entities, where the entities mentioned in the text can interact with a certain combination of visual entities in complex scenarios. The third task utilizes specified sequence types (e.g., number sequences, letter sequences, etc.) to combine all visual scenes to infer clues, such as 'Four colored numbers' and 'Four color sequences' in Figure 1. 2) Clue of Password-Visual Question Answering (CPVQA) consists of two task types that evaluate the model's ability to combine visual perceptual information for accurate combinatorial reasoning."}, {"title": "2. Problem Formulation", "content": "In this section, we introduce the problem formulation for the two benchmarks CVQA and CPVQA."}, {"title": "2.1. Formulation of CVQA", "content": "Given a question text $Q_e$ and an image set $I_c = \\{i_1, i_2, ..., i_n\\}$, CVQA can be categorized into three types: props search, props usage, and password clues.\nProps search. The question text $Q_e$ serves as a guide, directing the model to combine information in complex scenarios $I_c$ and identify objects that can be utilized. As illustrated in Figure 3, the $Q_e$ is 'Please find the props or related clues that can be used (interactive) in these scenes.', guiding the model to infer that the half-pipe in scene 4 can be applied to the missing pipe in scene 1.\nProps usage. The question text $Q_e$ explicitly specifies the name of the prop $t_e$, and infers where $t_e$ can be applied from complex scenarios $I_c$. As shown in Figure 6, the $Q_e$ is 'And you have a prop: USB, what can this prop interact with?', guiding the model to infer the appropriate application of $t_e$ based on $Q_e$ and the $t_e$ combination, specifically the computer.\nPassword clues. The clues for the password, such as numeric codes or letter sequences, are explicitly provided in the question text $Q_e$, while clues to decode the password in complex scenarios $I_c$ are inferred, or the final sequence is directly given. As shown in Figure 7, $Q_e$ is 'Reason within this scenario and derive clues about the numeric code.', where 'numeric code' represents password clues. The model is instructed to either infer password-related clues by integrating all scenarios in $I_c$ (i.e., computer) or directly provide the password, in this case, '8462'."}, {"title": "2.2. Formulation of CPVQA", "content": "Given a question text $Q_p$ and an image set $I_p = \\{i_a, i_b,..., i_k\\}$, CPVQA is divided into two categories: passwords and sequence rearrangement.\nPasswords. The clues for the password, such as a numeric code or a sequence of letters, are explicitly provided in the problem text $Q_p$, while the sequence is inferred within complex scenarios $I_p$. As illustrated in Figure 8, $Q_p$ is Reasoning in this scenario and deriving a numerical code.', where 'numeric code' serves as the password clue. The model is then directed to infer the numeric password sequence (e.g., 8462) by combining the password sequence found on the computer with the clues in the $Q_p$.\nSequence rearrangement. The problem text $Q_p$ utilizes codes to represent entities that need to be rearranged within a scene $i_k$ in $I_p$, requiring the model to integrate information from all complex scenarios in $I_p$ to infer the final sequence code. As illustrated in Figure 3, $Q_p$ encodes the trophies in $i_k$, directing the model to combine the trophy sequence in scene x with the encoding defined in $Q_p$ to infer the final sequence, (e.g., BCDA)."}, {"title": "3. Benchmark", "content": "In this section, two benchmarks, CVQA and CPVQA, are introduced in detail. First, we present the structural composition of the two benchmarks in Sec. 3.1, followed by an extension of our construction method and the effectiveness of this method is demonstrated in Sec. 3.2."}, {"title": "3.1. Benchmark Structure", "content": "In this subsection, we introduce the image sources and question types of our proposed benchmarks.\nImage source. In this study, we selected the game 'Can You Escape?' [38] as our image source due to its close resemblance to real-world visuals. The image named \"scenel\" in each image set is the main scene, and the other images are details of this scene. In addition to visual realism, our benchmarks offer several significant advantages: 1). Unified scene themes: While most existing datasets treat a single image as an independent scene, our benchmarks utilize multiple images with the same theme to create a cohesive and complete scene. 2). Diverse task types: Our benchmarks encompass a variety of reasoning tasks within the same scenario, including reasoning about relationships between entities, interpreting digital codes, and rearranging sequences based on colors and shapes. 3). Strong adaptability: We utilize advanced image processing tools to modify and augment images, a process that would be costly in real-world scenarios. This approach allows us to expand our benchmarks in a cost-effective manner.\nQuestion types. In this paper, three question types are defined for CVQA and two for CPVQA. Specifically: (1) In CVQA, we design three distinct types of questions to represent multi-graph reasoning tasks: props usage, password clues, prop collection.\nProps usage: In this task, the question explicitly identifies the tool in text form, prompting the model to use this prop to interact with a specific entity in the image. According to general real-world rules, in all scenarios, the tool can only interact with a unique entity within the given image.\nPassword clues: In this task, the question provides a textual description that refers to an image, which acts as an index. The model is required to infer the corresponding image based on the given description, which may involve objects such as a digital password lock or entities arranged in random order.\nProp collection: In the this task, the questions act as guides, directing the model to identify two related entities within a scene composed of multiple images. In every image, this pair of related entities appears together, with one of them being an object that humans can physically interact with or pick up (e.g., a CD can be picked up, a stationary player cannot).\n(2) In CPVQA, the image input task, specifically the password clues task in CVQA, primarily requires the model to identify relevant clues rather than producing precise outputs. However, by refining and standardizing benchmarks, we can further explore and assess the model's capabilities in greater depth. In this study, we categorize the questions in the password clues task into two distinct types: password and sequence rearrangement.\nThe password task requires the model to infer a string of letters and numbers from a scene composed of multiple images. The question specifies the components (letters, numbers or a combination) and the number of digits in the final result. In some images, character combinations may appear in different forms, such as Arabic numerals or Roman numerals. It is important to note that in certain images, the final character sequence may need to be reorganized based on attributes like color, shape, or other distinguishing features.\nThe sequence rearrangement task involves determining the correct order of entities in a given image (e.g., the order of the trophies in Figure 3) within a scene composed of multiple images. The entities are encoded based on their order in the image from the question (e.g., 'ABCD' from left to right). However, the final encoded sequence is influenced by information from other images, such as color, shape, or other attributes. To account for the possibility that the language model may repeat the question, we have designed the final true sequence to differ from the initial encoded sequence designed in the question."}, {"title": "3.2. Extension Method", "content": "Existing image generation and automatic annotation methods are convenient but unreliable (e.g., GPT-generated text may be garbled). This error is critical for reasoning tasks, so we use manual annotation to expand benchmarks while ensuring data quality. This section presents the scene expansion method, benchmark analysis, and score comparison, demonstrating its effectiveness.\nScene extension. In this study, we utilize CVQA and CPVQA as benchmarks to evaluate the reasoning ability of models in scenarios that involve multiple images and a single question. These benchmarks were designed with rigor in mind, emphasizing thematic consistency across scenarios and the provision of clear, unique answers. However, the benchmark's limited size posed challenges to fully supporting our evaluation system. We utilize the image editing tool Photoshop [39] to modify and expand the images.\nOur expansion methods are categorized into two types: 1) Expansion without altering the original answer: In this approach, we rearrange the positions of the color blocks within the image without changing the original answer. Additionally, in some cases, we replace certain sequences that do not affect the final result, such as the symbols shown in Figure 4 (a). 2) Expansion by altering the original answer: On one hand, sequences of characters and entities are modified by altering their order in certain samples. On the other hand, attributes such as color, shape, and other factors that indirectly influence the sequence of characters and entities in the image are adjusted, as illustrated in Figure 4 (b).\nBenchmark statistics. In the CVQA benchmark, the base dataset consists of 522 images across 123 scenarios. By applying the scene expansion method, we doubled the dataset, resulting in 1,003 images and 227 scenarios. For the CPVQA benchmark, the base dataset contains 180 images and 32 scenarios. Using the same expansion method, we increased the dataset nearly fourfold, bringing the total to 661 images and 147 scenarios.\nBenchmark validity. To verify the effectiveness of the extended benchmark in evaluating model capabilities, we compare the results of the extended benchmark with the original benchmark, utilizing various methods to answer questions across multiple models. The results are shown in Figure 5, where the x-axis represents the captions from different sources, the y-axis represents the model, and the z-axis shows the number of differing answer scores for the same task in both the extended and original benchmarks (if both scores are 0, they are considered the same). For the same task across the two benchmarks, variations in score differences across models and methods indicate the effectiveness of our expansion approach in modifying scenarios."}, {"title": "4. Methods", "content": "This section proposes three baseline methods to address our benchmarks without relying on prompt engineering. First, reasoning with models is discussed in Sec.4.1. Next, COT reasoning without prompts is covered in Sec.4.2. Finally, an approach for semantic retrieval on image sets is introduced in Sec.4.3. In this study, considering that most MLLMs [22] at this stage can effectively handle single-image input, all of methods are based on single-image inputs."}, {"title": "4.1. Model Inference", "content": "In this subsection, we introduce two approaches for inference using LLMs and MLLMs.\nLLMs reasoning. The benchmark we design consists of an image set with multiple images $V = \\{V_1, V_2, . . ., V_n\\}$ and a text-based question $T = (Q_c|Q_p)$ as input. Since most current MLLMs effectively process only single images, captions are first generated for each image utilizing MLLMs. The scenario's question, along with the captions from all images, is then input into LLMs for prediction:\n$A_{LLM} = LLM \\{T, \\sum_{i=1}^{n} [Caption(v_i)]\\}$ (1)\nwhere $LLM\\{T_1, T_2\\}$ denotes that the LLM utilizes $T_1$ and $T_2$ as text inputs for prediction, generating the prediction result $A_{LLM}$, and $Caption(v)$ represents the conversion of image v into caption utilizing MLLM.\nMLLMs reasoning. In this study, each scene in the benchmark contains a main scene $V_m$ that, although not rich in detail, conveys the overall theme and key entities of the scene. Therefore, we first use the visual input from the $V_m$ to help MLLMs understand the theme of the scene, and then combine the detailed descriptions $\\sum_{i=1}^{n}[Caption(v_i)]$ of individual images for the final prediction, specifically:\n$A_{MLLM} = MLLM\\{V_m, T, \\sum_{i=1}^{n}[Caption(v_i)]\\}$ (2)\nwhere $MLLM\\{v, T_m\\}$ represents the MLLM utilizes v as image input and $T_m$ as text input for prediction, generating the prediction result $A_{MLLM}$."}, {"title": "4.2. COT Reasoning Without Prompting", "content": "This subsection primarily introduces a COT reasoning without prompting method. Specifically, we introduce the minimum margin decoding strategy to address uncertainty and then generate coherent answers.\nMinimum margin decoding. Greedy decoding selects the best option at each step. Inspired by research [47], the minimum margin decoding strategy not only considers the best option but also measures the model's confidence by comparing the probability difference between the best and second-best options. Specifically, we calculate the probabilities of the best option $x_{1t}$ and the second-best option $x_{2t}$ at each step, utilizing their difference to represent the confidence:\n$\\Delta_t = p(x_{1t}|x_{<t}) - p(x_{2t}|x_{<t})$ (3)\nwhere t is each generation step and $p(x_t|x_{<t}$ represents the probability distribution of the model. The overall confidence of the final answer can be formalized as:\n$\\Delta_{answer} = \\frac{1}{X_{answer}} \\sum_{t \\in answer} \\Delta_t$ (4)\nwhere $X_{answer}$ represents the total number of steps in the answer. A larger $\\Delta_{answer}$ value indicates that the model has a higher confidence in the entire answer.\nGenerate coherent answers. In this study, when the model is uncertain at a given step t and $\\Delta_t$ falls below a certain threshold, we introduce randomness to ensure the consistency, credibility, and diversity of the generated results. Specifically, we use a temperature sampling method to introduce this randomness. The process is formulated as:\n$P(x_t|x_{<t}) = \\frac{P(x_t/x_{<t})^{1/T}}{\\sum_{x' \\in V}P(x'|x_{<t})^{1/T}}$ (5)\nwhere T is a temperature parameter that controls the strength of randomness, and V is the vocabulary."}, {"title": "4.3. Semantic Retrieval", "content": "In this subsection, we introduce a semantic retrieval method for handling multiple image inputs. While the main scene in model inference (Sec. 4.1) of this study provides the MLLM with key information, such as the theme of the scene as the image input, it may not always be the image most relevant to the answer. Therefore, across all known scenes, we evaluate the semantic relevance between the question and the scene descriptions to identify the image most likely to be relevant to the answer. This study implements two methods:\n(1)We utilize MLLM to generate captions C for complex scenes $V = \\{v_1, v_2 ..., v_n\\}$:\n$C = \\sum_{i=1}^{n}[Caption(v_i)]$ (6)\nprovide the LLM with C as background, and combine them with the scenario's question. The LLM then determines the most semantically relevant image v', which is considered the one most likely related to the answer. This image's encoding is input into the MLLM as a visual embedding, along with the question and captions C, for prediction.\n(2) We utilize the language model's semantic discernment capability to identify the caption Caption(v') of the image v' that is most likely relevant to the question, given C as background context in Equation 6. After identifying this image v', we input its encoding, together with the question, into the MLLM for prediction."}, {"title": "5. Experiments", "content": "This section first introduces the details of exploratory experiment (Sec. 5.1), which utilizing the high-level reasoning model to perform combinatorial reasoning in complex scenarios. Next, we describe the experimental setup (Sec. 5.2), which includes the benchmark evaluation mechanism and the models used in the experiments. Finally the main results (Sec. 5.3) of our proposed methods are presented."}, {"title": "5.1. Exploratory Experiment", "content": "In this subsection, we describe the process of constructing artificial prompts to guide the model through combinatorial reasoning step by step. The specific steps are as follows:\n(1) The model utilizes 5 different binary classification task prompts to make choices: 'choose 0' indicates that the model should continue exploring a given scene S, while 'choose 1' signals that the model is ready to make a final action to attempt unveiling the mystery.\n(2) A joint calibration set is constructed based on the original calibration set and the confidence interval from conformal prediction, allowing us to calculate degree of inconsistency: $p = \\frac{|\\{p \\in D:p \\geq p^{D'+1}\\}|}{|D' + 1|}$, where D' represents samples in the calibration set D with confidence scores greater than or equal to the confidence of the current sample. A lower p value indicates a higher confidence level.\n(3) The choice with the lowest p among the 5 prompts is utilized as the model's most confident next action. If 'choose 0' is selected, artificial prompts and \"previous actions\" are provided as text input, with S as the image input. S is then described in detail and summarized as context in \"previous actions\". If 'choose 1' is selected, all scene images are utilized as image input, along with artificial prompts and \u201cprevious actions\u201d as text input, to predict the final action needed to unveiling the mystery. As shown in Figure 2, the model selects \u2018choose 1' after first selecting 'choose 0', enabling it to achieve step-by-step reasoning in the complex scenario over two steps.\nWe employ a high-level reasoning model ChatGPT-4o [24] to perform thoughtful reasoning across 36 complex scenarios. Surprisingly, none of the 36 responses are correct, suggesting that prompt engineering has minimal impact on this task, even with manually crafted prompts."}, {"title": "5.2. Experimental Setup", "content": "In this subsection, we first introduce the evaluation mechanism of our proposed benchmarks, followed by a detailed description of the models utilized in our experiments.\nEvaluation. In this study, the evaluation mechanisms for CVQA and CPVQA differ. Specifically: (1) For CVQA, this paper require the final result to be a sentence that includes the image name and an interactive entity. We then calculate the number Ground Truth (GT) present in this sentence, denoted as $n_c$. If there are x questions in total, the final score is formalized as:\n$Score(CVQA) = \\frac{\\sum_{1} min(1, n_c/3)}{X}$ (7)\n(2) For CPVQA, this study evaluates the final result as an ordered string defined by the question for each scenario (e.g., 'ABCD' from left to right, as shown in Figure 3). The GT order is unique; if the predicted order is correct, the score is 1, otherwise, it is 0. If there are $x_1$ questions in total, the final score is formalized as:\n$Score (CPVQA) = \\frac{\\sum_{1}^{X_1} \\delta_i}{X_1}, \\delta_i = \\begin{cases} 1, & \\text{True} \\\\ 0, & \\text{False} \\end{cases}$ (8)\nModels. In this paper, we utilize four open-source and six closed-source models to evaluate the performance of various models and configurations. 1). For caption generation, we utilize three open-source models: LLaVA-1.6-7b [19], LLaVA-1.6-13b [19], and Qwen-VL [44]. 2). For model reasoning tasks, we utilize open-source models such as Mistral-7b [13], LLaVA-1.6-7b, LLaVA-1.6-13b, and Qwen-VL. Additionally, we utilize closed-source models including ChatGPT-3.5-turbo [24], ChatGPT-4, ChatGPT-4o, ChatGPT4V, Gemini Pro [36], and Qwen-Max [3]."}, {"title": "5.3. Main Results", "content": "We first present the model score results for method model inference (Sec. 4.1) in Tables 1 and 2, and then present the score results of our improved methods\u2014COT reasoning (Sec. 4.2) and semantic retrieval (Sec. 4.3)-through ablation experiments shown in Tables 3 and 4, respectively.\nResults of LLMs reasoning. Table 1 presents the reasoning results of LLMs utilizing method LLM reasoning (Sec. 4.1) on the CVQA and CPVQA benchmarks, showing that current models do not perform well on these tasks. First, the best results appear sequentially across most models, highlighting that no model excels completely at the tasks designed for our benchmarks. In CVQA, the 'all' category represents the total score across the three task types, with the highest score reaching only 36.86%, which shows no outstanding performance. In CPVQA, the highest score is 8.72%. In summary, our findings indicate that the scoring results for utilizing LLMs in our proposed benchmarks are unsatisfactory, suggesting that LLMs alone struggle to perform combinatorial reasoning that integrates multiple perceptual inputs in complex scenes.\nResults of MLLMs reasoning. The main results of reasoning with the MLLM reasoning method (Sec. 4.1) are shown in Table 2. The closed-source model Qwen-max achieves the best performance in CVQA when captions are generated by Qwen-vl. However, when the main scene is provided to the model as the \u201cscene theme", "scene theme\" offers limited benefits for improving reasoning performance.\nAblation on COT reasoning without prompting. In Table 3, we utilize the ablation method to demonstrate the effect of COT reasoning without prompting in method COT reasoning without prompting (Sec. 4.2). 1) In the LLM reasoning method, the open-source model Mistral-7b was utilized as the base model. Table 3 presents the COT reasoning method significantly improved the model's performance on the CVQA benchmark. However, it only partially enhance performance on the CPVQA benchmark. 2) In the MLLM reasoning method, this paper explores models with different parameter sizes of LLaVA-1.6. We find that in CVQA, the COT reasoning method had little to no effect on improving the performance of MLLMs. In CPVQA, COT reasoning method led to greater improvements for models with larger parameters (13b), which may be related to the random design of our COT reasoning method. Since the number of parameters is proportional to the vocabulary size, models with more parameters have a higher likelihood of generating a complex COT reasoning path. The COT reasoning method may be more effective for text reasoning and models with larger parameters, likely due to its inherent randomness.\nAblation on semantic retrieval. We compare the results of three methods in Table 4": "I: Results of method MLLM reasoning (Sec. 4.1). II: The method utilizing inputs C and v', and III: the method utilizing only v' as input, without C, in Sec. 4.3. Specifically, in CVQA, method III shows a clear advantage, particularly in the case of Model: Qwen-VL Caption: Qwen-VL, where its scores are consistently optimal. In contrast, method II yields suboptimal results. This indicates that in CVQA, our proposed method in Sec. 4.3 offers an improvement over the direct use of MLLM reasoning. In CPVQA, method III performs slightly better than the other methods, while method I yields suboptimal results. Our propose method offers an improvement over direct model prediction; however, more refined methods are needed to further enhance performance.\nAdditional Experiments. Given that models capable of handling multi-image inputs exist, the computational resources required to locally deploy open-source models increase significantly as the number of images grows. To maintain the comprehensiveness of our experiment, we utilize API-callable multi-image input models to directly evaluate their performance on our proposed benchmarks. The results are presented in Table 5.\nEven the large-parameter model Qwen-vl-max achieves an overall performance of only 30.10% on our CVQA benchmark, and more shockingly, it scores just 3.58% on the CPVQA. However, while the score of Qvq-72B is not particularly high, it outperforms other models and methods.\nIn-depth analysis. In our exploration and experiments, the primary error observed is that it tends to answer based solely on a single information source, without integrating other relevant data for joint reasoning, as shown in Figure 2. Additionally, for certain questions, such as the trophy sequence in Figure 3, the model tends to respond based only on the information presented in the question. This highlights the potential to further integrate multiple information for combined reasoning. Furthermore, as shown in Tables 3 and 5, models utilizing COT or trained on COT data demonstrate better performance. This suggests that enhanced reasoning methods, such as COT [47, 49], TOT [53], and RL-trained models [6], may be more effective in analyzing information from multiple sources."}, {"title": "6. Conclusion", "content": "In this work, we introduce two benchmarks, CVQA and CPVQA, designed to evaluate the ability of models to perform combinatorial reasoning by integrating multiple perceptual information in complex contexts, and we propose three baselines methods. Extensive experiments reveal that current models struggle with combinatorial reasoning in complex contexts. Our method alleviates these limitations, improves performance, and provides insights for future model enhancements."}, {"title": "A. Benchmark Solution Formats", "content": "In this Section, in addition to the solved forms of CVQA and CPVQA shown in Figure 3, we additionally show three other tasks of CVQA and CPVQA to emphasize the characteristics of different tasks on different benchmarks. This paper specifically quotes these images and explains them in text in Sec 2.1 and Sec 2.2.\nReasoning with LLMs. LLMs [37, 42, 43] are increasingly used in various reasoning tasks, including mixed reasoning [54], arithmetic reasoning [5, 23], and deductive reasoning [28]. Earlier research [29, 35, 52] has enhanced the reasoning abilities of LLMs through fine-tuning, but later studies [32, 48] revealed that fine-tuning can be both costly and less effective compared to using contextual examples. More recently, several strategies have been developed to leverage in-context Learning [4, 7] and prompt design [2, 25, 31] methods, such as COT prompt [15, 49], incremental reasoning [58], and Tree-of-Thought prompt [53]. Previous methods have made significant contributions to enhancing model reasoning abilities; however, they primarily focus on individual reasoning within scenarios. In contrast, this paper aims to explore and address the combinatorial reasoning abilities of LLMs in complex scenarios.\nBenchmark. Previous traditional vision-language benchmarks have primarily focused on specific tasks, such as visual commonsense reasoning [10, 46], textual reasoning based on visual information [12, 33], logical question reasoning [14], and other vision-language tasks [50, 51]. Most of these benchmarks [1, 11, 21, 30] are designed in simple scenarios, lacking consideration for complex scenarios. This paper contends that, while existing benchmarks [8, 17, 20, 26, 45, 57] are valuable for synthesizing vision-language capabilities, there is a need to focus on tasks in complex scenarios to promote continuous advancements in AI systems. Specifically, our benchmark offers advantages such as a detailed classification of question types and a focus on constructing benchmarks in complex scenes."}]}