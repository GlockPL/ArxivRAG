{"title": "REFINE: INVERSION-FREE BACKDOOR DEFENSE VIA MODEL REPROGRAMMING", "authors": ["Yukun Chen", "Shuo Shao", "Enhao Huang", "Yiming Li", "Pin-Yu Chen", "Zhan Qin", "Kui Ren"], "abstract": "Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often fail to balance model utility and defense performance, while BTI-based defenses struggle to accurately reconstruct trigger patterns without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: (1) an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and (2) an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks.", "sections": [{"title": "INTRODUCTION", "content": "Deep neural networks (DNNs) have been widely deployed across various domains. To develop a high-performance DNN, developers necessitate not only high-quality data samples but also substantial computational resources. Consequently, developers frequently and directly rely on third-party models for follow-up development. However, the utilization of third-party DNNs can introduce security threats, particularly with regard to backdoor attacks.\nBackdoor attacks aim to implant hidden backdoors into the model during training. After the attack, the backdoored model functions normally on benign inputs. However, when a specific trigger is present, the model will produce intentionally incorrect outputs. Backdoor attacks pose a severe threat to critical applications where model reliability is essential, highlighting the urgent need for effective backdoor defense strategies to safeguard AI systems.\nCurrently, several backdoor defenses have been developed to tackle the threat of backdoor attacks. Among these, pre-processing-based defenses are particularly notable because they only apply certain modifications to input samples before model inference, without altering the original model structure and weights. Currently, there are two main types of pre-processing-based defenses. The first type of defenses relies on input transformations"}, {"title": "BACKGROUND", "content": "Backdoor attacks involve embedding hidden malicious behaviors into a model, typically by manipulating the training process with a small subset of poisoned data containing adversary-specified trigger patterns. Whenever the trigger appears in the input during inference, the model executes the attacker's intended behavior, such as misclassifying the input to a target label. In the absence of the trigger, the model functions normally, rendering the backdoor hard to detect. Backdoor attacks pose serious threats in AI-enpowered systems.\nThe formulation of backdoor attacks is typically presented as follows. Given a training dataset D = {(xi, Yi)}=1, the attacker manipulates the training process of the model F by introducing a poisoned subset D = {(xi, Yt)}-1, where xi = G(xi) with G(\u00b7) as a certain trigger injection function and yt being the chosen target label, or by altering the training loss directly. During inference, the model behaves normally on benign samples, where yj = F(x), while exhibiting backdoor behavior on poisoned samples, such as misclassifying to the target label yt = F(G(xj)).\nGenerally, existing attacks can be classified into two types: (1) Visible backdoor attacks, which typically employ trigger patterns that are visible to humans, such as specific white-black squares, physical attacks, or adaptive attacks. (2) Invisible backdoor attacks, which introduce imperceptible triggers to enhance the stealth and evasiveness of attacks, including sample-specific attacks, trainable noise attacks, and sample rotation attacks."}, {"title": "BACKDOOR DEFENSES", "content": "Currently, there are various backdoor defense methods designed to mitigate backdoor threats. These methods can generally be divided into three main paradigms: (1) pre-processing-based defenses. (2) backdoor elimination, which involves adjusting model parameters through fine-tuning, pruning or reconstruction to remove the backdoor. (3) trigger elimination, also known as testing sample filtering. In this paper, we focus on pre-processing-based defenses since we consider scenarios where only fixed third-party models are accessible and defenders require to obtain the correct final results of all samples.\nPre-processing-based Defenses. Generally, pre-processing-based defenses can be categorized into two types: (1) Transformation-based defenses. Classical methods typically involve applying simple transformations to input, aiming to disrupt trigger patterns and prevent the model from exhibiting backdoor behavior. More Recently, many methods have leveraged the powerful reconstruction capabilities of generative models, such as diffusion models and masked autoencoders, intending to retain the original benign features while minimizing the presence of backdoor-related features. However, there is a trade-off between removing backdoor patterns and restoring benign patterns, which remains a pressing issue to address. (2) BTI-based defenses, which focus on inverting the pre-injected triggers and utilizing them to purify the input samples. However, these methods may face issues with inaccuracies in the inverted triggers, which may lead to suboptimal purification of the input. How to design an effective pre-processing-based defense is still an important open question."}, {"title": "MODEL REPROGRAMMING", "content": "Model reprogramming is a technique that extends the application of a pre-trained model from a source domain to a target domain. This technique involves adapting the input from the target domain to match that of the source domain. Specifically, model reprogramming introduces an input transformation module T(x0) and an output mapping module M(y|\u03b2), where 0 and \u03b2 are the trainable parameters of these two modules, respectively. Given a pre-trained model F(\u00b7) and an input sample \u00e6, model reprogramming first transforms a to x leveraging the input transformation module. Then input x into the pre-trained model F(\u00b7) and get the output \u1ef9 = F(x). Finally, the output mapping module is used to map \u1ef9 into the final output y. Through fine-tuning the input transformation module and the output mapping module (i.e., optimizing 0 and 3), model reprogramming can efficiently turn the pre-trained model from the source domain to a target domain. Compared to transfer learning, model reprogramming does not necessitate modifying the parameters of the pre-trained model. As such, it is more efficient and flexible."}, {"title": "REVISITING EXISTING PRE-PROCESSING-BASED BACKDOOR DEFENSES", "content": "This paper focuses on tackling the issue of pre-trained backdoored models via pre-processing-based backdoor defense. The defender may buy or acquire a pre-trained model from third-party platforms. However, there exists a threat that the pre-trained model is backdoored. Due to the limitations of computational resources, the defender seeks to mitigate the backdoor in an efficient and low-cost way (e.g., without altering the parameters of the pre-trained model). Following prior works, we make the following assumptions. For adversaries, they can implant the backdoor into the pre-trained model in any way (e.g., by poisoning the training data or intervening in the training process). For defenders, we assume that they have access to an unlabeled dataset that is independent and identically distributed to the training dataset of the pre-trained model."}, {"title": "THE LIMITATIONS OF TRANSFORMATION-BASED DEFENSES", "content": "Transformation-based defenses aim to mismatch or eliminate triggers by applying specific transformations to test samples. This type of defense method can be categorized into two types: random perturbations and generator reconstruction. Specifically, random perturbations involve the defender mismatching the trigger pattern through techniques such as scaling or rotation, while generator re- construction leverages a pre-trained generative model to erase the trigger pattern. However, the transformation-based backdoor defense methods face a trade-off between the utility of the model and the effectiveness of the backdoor elimination, making them ineffective in practice.\nIn this section, we present the empirical results to support the above claim. We implement two representative transformation-based methods, ShrinkPad and BDMAE, to defend the BadNets attack on CIFAR-10. Specifically, ShrinkPad applies simple spatial transformations to the input, while BDMAE employs a trained masked autoencoder for data cleansing. We use \"Pad Size\" (dubbed \"S\"), which refers to the padding size applied around shrunk images, and \"Mask Ratio\" (dubbed \"R\"), which represents the masking rate applied to images before reconstruction, to control the transformation intensity for ShrinkPad and BDMAE, respectively. We aim to analyze how these transformations impact the model's benign accuracy (BA) and attack success rate (ASR) of the backdoor. Additionally, we treat the original model as a feature extractor. We then visualize how transformation intensity affects the differences in feature distribution between benign and poisoned samples of the same class.\nAs shown in Figure 2 (a-1) and (b-1), increasing the intensity of input transformation, which en- larges the feature distance between the original and transformed samples, reduces the backdoor ASR. However, it also leads to a decline in the model's BA. As depicted in Figure 2 (a-2)~(a-4) and (b-2)~(b-4), higher transformation intensity causes greater changes in the feature distribution"}, {"title": "THE LIMITATIONS OF BTI-BASED DEFENSES", "content": "BTI-based defenses can 'break' the trade-off between model utility and defense performance by incorporating the information of backdoor attacks via trigger inversion. In the pre-processing-based defense paradigm, BTI-based defenses typically involve two steps: trigger inversion and input pu- rification. Specifically, the defender first exploits several data to invert the pre-injected trigger, then trains a generator to purify the input samples using the inverted trigger. The effectiveness of BTI-based defenses highly relies on the quality of the inverted trigger. However, we argue that the inherent challenge of achieving high-quality trigger inversion, due to the lack of prior knowledge, hinders effective input purification, ultimately limiting the performance of BTI-based defenses.\nWe implement the state-of-the-art BTI-based defense, BTI-DBF to invert the backdoor triggers of BadNets and Blended on CIFAR-10. As shown in the Figure 3, BTI-DBF effectively reverses the trigger pattern of the BadNets attack and purifies the poisoned sample. However, for the Blended attack, the trigger pattern reversed by BTI- DBF significantly differs from the pre-injected one, leading to poor purification of the poisoned sample. This illustrates that the effectiveness of BTI-based defenses largely depends on the quality of trigger inversion, which is the inherent challenge of such defenses. Moreover, BTI-based defenses often identify \u201cpseudo-triggers\" inherent in DNNs, which usually arise from the model's vulnerability to adversarial perturbations. When defenders attempt to use such triggers to train purification generators, they may disrupt the benign features of the samples, while leaving the backdoor patterns largely unaffected. If the quality and authenticity of the inverted trigger patterns cannot be guaranteed, BTI-based defenses may potentially yield adverse outcomes.\nIn conclusion, achieving BTI is a challenging endeavor due to the lack of prior knowledge about the implanted backdoor and poisoned samples, highlighting the need for an inversion-free backdoor defense to resolve this trade-off."}, {"title": "\u041c\u0415\u0422\u041dODOLOGY", "content": "In Section 3, we empirically evaluate existing pre-processing-based defenses and analyze why they are ineffective. In this section, we present a theoretical analysis and the inspiration to design an effective and efficient backdoor defense method. Given a pre-processing method T(\u00b7) and a pre- trained model F(\u00b7), we have the following theorem."}, {"title": "MOTIVATION AND INSPIRATION", "content": "Theorem 1. Given a K-class pre-trained deep learning model F(\u00b7) = s(f(\u00b7)) where s(\u00b7) is the softmax function and f(\u00b7) is the feature extractor, and a pre-processing method T(\u00b7), x is the data from a specific domain D (i.e., x ~ D) and x = T(x) ~ D. Let PD(x) and P(x) denotes the probability density function of D and D, we have\n$\nEx~D,~D||F(x) \u2013 F(x)||^2 \u2264 2a\u221aK \u2022 W\u2081(\u03bc, \u03bc),\n$\nwhere W\u2081 (\u03bc, \u03bc) is the Wasserstein-1 distance between \u00b5 and \u0169, \u00b5 and \u016d are the probability measures of the representations f(x) and f(x), and a = max[P(x|x)/(x)].\nTheorem 1 indicates why existing defenses are ineffective. Assuming \u00e6 is the poisoned sample, the left part of Eq. (1) means the distance between the prediction of the transformed poisoned sam- ple and the original poisoned sample. Theorem 1 demonstrates that the distance is bounded by the Wasserstein-1 distance between the probability measures \u03bc, \u0169 of the output representations. Thus, to maintain model utility, existing pre-processing-based defenses tend to retain the output representa- tions, limiting their effectiveness against backdoors. Otherwise, they have to compromise the model utility to achieve greater backdoor defense performance.\nFollowing the above theorem, we can enhance the upper bound by increasing the distance be- tween \u03bc.\u03bc. Inspired by model reprogramming techniques, we propose REFINE, a reprogramming-based inversion-free backdoor defense method. Our REFINE can significantly transform the input domain to destroy trigger patterns while maintaining model utility for it also changes the output domain. Specifically, we introduce an input transformation module to modify in- puts, and an output mapping module to remap original classes to new shuffled ones. We also employ a supervised contrastive loss to further enlarge the distances among different classes. The technical details of our REFINE are illustrated in the following parts."}, {"title": "REFINE: REPROGRAMMING-BASED INVERSION-FREE BACKDOOR DEFENSE METHOD", "content": "In general, REFINE consists of two essential components: (1) the input transformation module T, which disrupts the benign and backdoor patterns of input samples through transformations and generates new benign features; (2) the label mapping module M, which formulates the specified source-target hard-coded label remapping function and maps the original classes to new shuffled classes. Additionally, we integrate the cross-entropy loss Lce and the supervised contrastive loss Lsup to steer the optimization of T."}, {"title": "INPUT TRANSFORMATION MODULE", "content": "To effectively alter potential trigger patterns in the input samples, we need to modify the input domain of the original model. Traditional model reprogramming methods add the optimized universal adversarial perturbation around the input samples, while trigger patterns still remain intact on backdoored images to some extent. In contrast, we utilize a trainable autoencoder (e.g., UNet) as the foundational structure for our input transformation module. Arguably, this module not only preserves the consistency of sample dimension before and after transformation, but also affords greater flexibility in sample manipulation compared to conventional reprogramming methods. Upon inputting a batch of data, the input transformation module will encode the pixel features from the images and then decode them to produce new samples. The transformed samples X can be described as follows:\n$\nX = T(X,0),\n$\nwhere X is a batch of input samples, and T(\u00b7,0) is the input transformation module with @ as its trainable parameters. During this transformation process, both benign and backdoor patterns are disarranged, effectively removing potential triggers and causing the generation of new benign features orderly clustered by their respective classes."}, {"title": "OUTPUT MAPPING MODULE", "content": "Once the input samples are transformed into new samples via the input transformation module, they are subsequently processed by the original backdoored model, which generates confidence scores for each class, as expressed below:\n$\nY = F(X),\n$\nwhere F() is the original backdoored model. As demonstrated in Section 3.2, fixing the model's output domain leads to a trade-off between model utility and defense performance. To address this issue, we introduce an output mapping module at the model's output end, aiming to alter the output domain and mitigate the aforementioned challenges. Specifically, the output mapping module redefines the class order of the model's output layer, which hard-codes a one-to-one label remapping function f\u2081 : \u00ce\u2194 l, where \u00ce, l \u2208 L,\u00ce \u2260 l, L is the set of labels. The confidence scores generated by the original model can be remapped into new scores through M, as follows:\n$\nY = M(\u1ef8).\n$\nThe final predictions for the samples can be derived from the confidence scores Y outputted by M."}, {"title": "OPTIMIZING REFINE MODULES", "content": "To maximize the flexibility of input transformations for removing trigger patterns while maintaining the original model's accuracy, we incorporate two crucial loss functions, the cross-entropy loss and the supervised contrastive loss, to guide the optimization of the input transformation module. The formulation of the combined loss function can be expressed as follows:\n$\nmin Lrefine = Lce + Lsup.\n$\n\u03b8\nIn Eq. (5), Lce and Lsup indicate the cross-entropy loss and the supervised contrastive loss, re- spectively. A is a scalar temperature parameter, and 0 represents the set of parameters in the input transformation module to be optimized during training. Since Theorem 1 does not guarantee the model performance on clean samples, adding Lee to maintain the utility of the model is necessary.\nIn our threat model, the dataset available to the defender is unlabeled. Therefore, before calculating these loss functions, it is necessary to obtain the pseudo-labels Y for the current batch of unlabeled samples X, predicted by the original model (without any additional modules), as follows:\n$\nY = arg max(F(X)).\n$\nLeveraging Cross-entropy Loss to Maintain the Utility. Due to the substantial modification of the original model's output domain facilitated by the output mapping module, the input transforma- tion module is no longer constrained by the requirement to preserve the original benign features of the samples. Nevertheless, the model must retain its original performance within the new output domain, which necessitates the employment of cross-entropy loss to effectively guide the sample transformation process. The cross-entropy loss is typically formalized as follows:\n$\nLce = -\\frac{1}{N} \\sum_{i=1}^{N} \\hat{y}_i \\log(y_i),\n$"}, {"title": "UTILIZING REFINE FOR MODEL INFERENCE", "content": "During the model inference phase, we can apply the aforementioned well-trained modules to achieve high-performance and secure predictions. The input samples are sequentially processed through the input transformation module T(\u00b7, \u03b8), the original pre-trained model F(\u00b7), and the output mapping module M(.). This process ultimately yields the predicted confidence scores, with all parameters remaining constant. The inference process can be formally expressed as follows.\n$\ny = M(F(T(x, 0))),\n$\nwhere x represents the sample to be predicted."}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate the effectiveness of our REFINE compared with different existing back- door defenses. We also conduct an ablation study and evaluate the resistance to potential adaptive attacks. The analysis of the overhead of REFINE is in Appendix F and the implementation of RE- FINE in the black-box scenario is in Appendix E."}, {"title": "EXPERIMENTAL SETTINGS", "content": "Datasets and Models. We conduct experiments on two classical benchmark datasets, including CIFAR-10 and (a subset of) ImageNet containing 50 classes. We evaluated our method with ResNet-18 on both datasets. We also vali- date the effectiveness of REFINE on other models in Appendix D. Note that our goal is to evaluate the effectiveness of backdoor defense methods instead of training a SOTA model. Therefore, the be- nign accuracies of our models may be lower than the SOTA models. We exploit U-Net as the structure of the input transformation module.\nAttack Setup. We utilize 7 representative advanced backdoor attacks, including (1) BadNets (Gu et al., 2019), (2) Blended (Chen et al., 2017), (3) WaNet (Nguyen & Tran, 2021), (4) PhysicalBA (dubbed 'Physical') (Li et al., 2021c), (5) BATT (Xu et al., 2023), (6) LabelConsistent (dubbed 'LC') (Turner et al., 2019), and (7) Adaptive-Patch (dubbed \u2018Adaptive') (Qi et al., 2023), to com- prehensively evaluate the performance of different defenses.\nDefense Setup. We compare the defense performance of REFINE with both types of pre-processing- based defenses. For transformation-based defenses, we utilize three advanced methods, including (1) ShrinkPad (Li et al., 2021c), (2) BDMAE (Sun et al., 2023), (3) ZIP (Shi et al., 2023). For BTI-based defenses, we employ three methods as baseline, including (1) Neural Cleanse (dubbed 'NC') (Wang et al., 2019), (2) UNICORN (Wang et al., 2023), (3) BTI-DBF(P) (Xu et al., 2024).\nEvaluation Metrics. Consistent with the standard evaluation metrics in backdoor-related studies (Li et al., 2024c), we utilize benign accuracy (BA) and attack success rate (ASR) to assess all defense"}, {"title": "MAIN RESULTS", "content": "As shown in Tables 1-2, our REFINE successfully mitigates backdoor threats in all cases while preserving high benign accuracy. Specifically, the ASRs of our method are lower than 3% (< 2% in most cases). For the BA, the models under REFINE experience less than 3% drop on the CIFAR-10 dataset compared to the undefended models. On the ImageNet dataset, the BA even improves, due to the increased depth of the original models introduced by the input transformation module. In contrast, other baseline defenses may fail in certain cases, with BA drop or ASR > 10%."}, {"title": "ABLATION STUDY", "content": "There are three important components in our methods, including (1) input transformation method, (2) hard-coded remapping function (HRF for short) in the output mapping module, and (3) super- vised contrastive loss (SCL for short) of transformed samples. In this section, we present an ablation study on the former two modules and verify their effectiveness. We also test different architectures of the input transformation module and conduct additional ablation studies in Appendix D.\nAs shown in Table 3, we evaluate the defense performance of REFINE without the hard-coded remapping function (w/o HRF) or without the supervised contrastive loss (w/o SCL). Experimental results indicate that without the hard-coded remapping function, REFINE successfully preserves the BA of the original model, but struggles to reduce the ASR of the backdoor. This is because, without the hard-coded remapping function, the output domain of the model remains unchanged. Subsequently, it encounters the same trade-off problem as other transformation-based defenses, and is difficult to find a balance between transformation intensity and defense performance. Also, in the absence of supervised contrastive loss, REFINE can effectively reduce ASR with the help of the hard-coded remapping function. However, it encounters difficulties in restoring the BA of the original model, which may adversely affect the model's inference capabilities."}, {"title": "RESISTANCE TO POTENTIAL ADAPTIVE ATTACKS", "content": "In this section, we examine whether the adversary can circumvent our defenses if they have full knowledge of the process of our REFINE. After training the original backdoored model, the ad- versary can fine-tune it utilizing an input transformation module, along with a randomly initialized hard-coded output mapping module, to simulate our REFINE. During fine-tuning, the loss function for model optimization can be expressed as follows:\n$\nmin Ladap = Lb + Lrefine,\n$\n\u03b4\nwhere Lo indicates the cross-entropy loss function in the original training phase of the backdoored model, and Lrefine represents the loss function of REFINE. y is a scalar temperature parameter, and & denotes the trainable parameters of the backdoored model. Ideally, the adversary can achieve the backdoor target with a low value of Crefine by optimizing Eq. (10). Consequently, the REFINE may not work well since the Lrefine is already low.\nAs shown in Table 4, REFINE is still highly effective with high BAs (BA drop < 1.5%) and low ASRs (< 1.5%). It is mostly because defenders can arbitrarily specify the output mapping function and train an input transformation module that may entirely differ from the attacker's. Besides, the original backdoored model experiences a decrease in BA after undergoing adaptive attack training, due to the inherent difficulty of optimizing multiple loss functions simultaneously. As such, these results demonstrate that our REFINE is resistant to adaptive attacks."}, {"title": "CONCLUSION", "content": "In this paper, we revisited existing pre-processing-based backdoor defense methods, including backdoor-trigger-inversion-based (BTI-based) defenses and transformation-based defenses. We re- vealed the limitations of the two defense methods. Subsequently, according to the empirical and theoretical analysis, we proposed REFINE, a reprogramming-based inversion-free backdoor defense method. This method was motivated by the insight that increasing the distances of the feature repre- sentations before and after the transformation may lead to a better effectiveness of backdoor defense. Specifically, we introduced an input transformation module and an output mapping module. We also utilized the supervised contrastive loss to enhance the defense performance. Results on benchmark datasets verified the effectiveness of our REFINE and the resistance to the adaptive attack. We hope our REFINE can provide a new angle to facilitate the design of more effective backdoor defenses."}, {"title": "LIMITATIONS AND FUTURE DIRECTIONS", "content": "Firstly, as outlined in our threat model, the goal of our defense is to protect against pre-trained models from third-party platforms. Specifically, similar to other baseline methods, we assume that the defender possesses a certain amount of unlabeled sample datasets. To explore the effectiveness of REFINE in few-shot scenarios, we conduct additional experiments using 10% unlabeled clean data. We apply the REFINE defense to a ResNet-18 model trained on the CIFAR-10 dataset, which is subjected to the BadNets attack. In this case, the unlabeled training set for REFINE used only 10% of the CIFAR-10 training set.\nAs shown in Table 14, even with only 10% unlabeled data, REFINE is still effective to some extent. REFINE effectively reduces the ASR, although it does have some impact on the model's BA. There- fore, in cases where the defender lacks the number of unlabeled samples, it becomes impossible to train the input transformation module, thereby hindering the execution of the intended defense. Currently, with the widespread application of generative models, obtaining a sufficient amount of unlabeled samples is no longer a challenging task. In the future, we will continue to explore how to maintain the effectiveness of our REFINE in few-shot scenarios.\nSecondly, we need to train a local input transformation module, which requires certain compu- tational resources and time. While this overhead is somewhat higher than that of pre-processing defenses based on random transformations, it is significantly lower than the overhead associated with pre-processing defenses based on generative models and BTI-based methods, as presented in Appendix F. This overhead is considered acceptable compared to retraining a DNN from scratch.\nFinally, our method primarily focuses on backdoor defense for image classification models. Fortu- nately, existing researchs have demonstrated that model reprogramming techniques can yield favorable results in fields such as text and audio. We will ex- plore the reprogramming-based backdoor defense in other modalities and tasks in our future work."}]}