{"title": "AUDIO PROMPT ADAPTER:\nUNLEASHING MUSIC EDITING ABILITIES FOR TEXT-TO-MUSIC WITH\nLIGHTWEIGHT FINETUNING", "authors": ["Fang-Duo Tsai", "Shih-Lun Wu", "Haven Kim", "Bo-Yu Chen", "Hao-Chung Cheng", "Yi-Hsuan Yang"], "abstract": "Text-to-music models allow users to generate nearly realistic musical audio with textual commands. However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface. To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feed these features into the internal layers of AudioLDM2, a diffusion-based text-to-music model. With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.", "sections": [{"title": "1. INTRODUCTION", "content": "Advancements in text-to-music generation have made it possible for users to create music audio signals from simple textual descriptions [1-4]. To improve the control over the generated music beyond textual input, several newer models have been proposed, using additional conditioning signals indicating the intended global or time-varying musical attributes such as melody, chord progression, rhythm, or loudness for generation [5-9] (see Section 2 for a brief review). Such controllability is important for musicians, practitioners, as well as common users in the human-AI co-creation process [10, 11].\nHowever, one area that remains challenging, which we refer to as text-to-music editing below, is the precise editing of a piece of music, provided by a user as an audio input x alongside the text input y for the textual prompts. The goal here for the model is to create an \"edited\" version of the input music, denoted as x, according to the text input. This is a crucial capability for users who wish to refine either an original or machine-generated music without compromising its musicality and audio quality, while keeping the simplicity of text-based human-computer interaction.\nNamely, the desired properties of the output x are:\n\u2022 Transferability: x should reflect what y specifies, e.g., timbre, genre, instrumentation, or mood.\n\u2022 Fidelity: x should retain all other musical content in x that y does not concern, e.g., melody and rhythm.\nWhile a text-to-music generation model takes in general only the text input y and generates music freely, a text-to-music editing model takes both audio and text inputs x and y. The primary challenge arises from the conflicting goals of maintaining high fidelity to the input audio x while incorporating specific changes dictated by textual commands y. As we review in Section 2, existing methods [14-16] either lack the granularity needed for detailed audio manipulation or need complex prompt engineering that detracts from user accessibility or requires iterative refinements.\nA secondary challenge arises from the large number of trainable parameters needed for models to achieve high musical quality and diversity (e.g., MusicGen-medium [5] has 1.5B parameters). Without much computational resource, it is more feasible to treat existing models as \"foundation models\" and finetune them to fulfill specific needs, instead of training a model from scratch [17].\nIn view of these challenges, we propose in this paper the Audio Prompt Adapter (or, AP-Adapter for short), a novel approach inspired by the Image Prompt Adapter (IP-Adapter) [18] from the neighboring field of text-to-image editing. This lightweight (22M parameters), attention-based module integrates seamlessly with existing text-to-music generation models, specifically leveraging the pre-trained AudioLDM2 model [12] enhanced by the AudioMAE encoder [13] to extract audio features. Our method uniquely combines text and audio inputs through decoupled cross-attention layers, allowing precise control in the generation process. After training the AP-adapter with a single NVIDIA RTX 3090, our method can zero-shot edit a given"}, {"title": "2. RELATED WORK", "content": "Generating desired music from text prompts alone is complex and often requires intricate prompt engineering. Mustango [7] enhanced prompts with information-rich captions specifying chords, beats, tempo, and key. MusicGen [5] conditioned music generation on melodies by extracting chroma features [19] and inputting them with the text prompt into a Transformer model. Coco-Mulla [6] and MusiConGen [9] extended MusicGen by adding time-varying chord- and rhythm-related controls. Music ControlNet [8] incorporated time-varying conditions like melody, rhythm, and dynamics for diffusion-based text-to-music models. These methods utilize low-level features to guide generation but do not take reference audio as input, limiting their potential for editing existing audio tracks.\nRecently, several music editing methods were proposed. InstructME [14] uses a VAE and a chord-conditioned diffusion model for music editing but requires a large dataset of audio files with multiple instrumental tracks and triplet data of text instructions, source music, and target music for supervised training. M2UGen [15] leverages large language models to understand and generate music across different modalities, supporting music editing via natural language, but it requires a three-step training process and complex preprocessing. MusicMagus [16] implements latent space manipulation during inference for music editing but requires an additional music captioning model and the InstructGPT LLM to address discrepancies between the text prompt distribution of AudioLDM2 and the music captioning model.\nCompared to these methods, our AP-Adapter is more straightforward to train and can achieve multiple music editing tasks in a zero-shot manner."}, {"title": "3. BACKGROUND", "content": "Denoising diffusion probabilistic models (DDPMs) [20], also known as diffusion models, are a class of generative models that approximates some distribution p(x) via denoising through a sequence of T - 1 latent variables:\n$$p_0(x) = \\int \\prod_{t=1}^{T} p_\\theta (x_{t-1}|x_t) p(x_T) dx_{1:T},$$"}, {"title": "3.1 Diffusion Model", "content": "where \u03b8 is the set of learnable parameters, x0 := x, and p(x\u0442) := N(0, I) (i.e., an uninformative Gaussian prior). To train the model, we run forward diffusion: sample some data point x ~ p(x) and some t \u2208 [1,T], and add noise \u20ac ~ N(0, I) to x to produce a noised data point\nxt := \u221a\u1e9etx+\u221a1 \u2013 \u00dfte, where \u00dft is the pre-defined noise\nlevel for step t. The model is asked to perform backward dif-fusion, namely, to recover the added noise via the objective\n$$ming \\mathbb{E}_{x, e, t} [||\u20ac \u2013 \u20ac_\\theta(x_t, t)||_2],$$ where \u20ac9(\u00b7) is the model's\nprediction, that is equivalent to maximizing the evidence\nlower bound (ELBO) of po(x). During inference, we start\nfrom an x\u2081 ~ N(0, I) and iteratively remove the predicted\nnoise e(x, t) to generate data. Song et al. [21] offered a\ncrucial interpretation that each denoising step can be seen\nas ascending along a log pe (x), also known as the score\nof po(x). Any input condition y can be incorporated into a"}, {"title": "3.2 AudioLDM2", "content": "We choose AudioLDM2 [12], a latent diffusion-based [22] text-to-audio model, as our pretrained backbone. To enable text control over generated audio, AudioLDM2 uses AudioMAE [13] to extract acoustic features, named the language of audio (LOA), from the target audio. LOA serves as the bridge between acoustic and text-centric semantic information-the text prompt is encoded by both the FLAN-T5 [24] language model and CLAP [25] text encoder (which has a joint audio-text embedding space), and then passed to a trainable GPT-2 [26] to approximate the LOA via a regression loss that aligns the semantic representations with LOA. The aligned text information is then fed into the U-Net [27] for diffusion process to influence the generation. We pick AudioLDM2 to be the backbone since the use of LOA likely promotes the affinity to accepting audio conditions, which is crucial to our fidelity goal."}, {"title": "3.3 Classifier-free Guidance", "content": "Classifier-free guidance (CFG) [28] is a simple yet effective inference-time method to enhance the input text condition's influence, which is directly linked to our transferability goal. As mentioned in Sec. 3.1, diffusion models can predict both the unconditioned score V\u00e6 log p(x) and the conditioned score x log p(x | y). In addition, by Bayes' rule, we know that p(x | y) x p(x)p(y | x). As the goal is the amplify y's influence, we define:\n$$px(x | y) :x p(x)p(y | x)^\\lambda,$$\nwhere X is a knob, named CFG scale, that controls the strength of y. Taking (\u2207 log) on both sides gives us:\n$$V\u00e6logpx(x | y) = \\lambda \u2207x log p(y | x) + \u221ax log p(x).$$\nMeanwhile, we can rearrange the Bayes' rule terms to get:\n$$Vx log p(y | x) = \u221ax log p(x | y) \u2013 \u2207x log p(x).$$\nNote that a diffusion model can predict both RHS terms. Plugging Eqn. (4) into Eqn. (3), CFG performs\n$$Vlog px(xy) = \\lambda\u221ax log p(x)\n+ (\\lambda\u2207x log p(x | y) \u2013 \u2207x log p(x))$$\nat every inference iteration, where x log p(x) is obtained by inputting an empty string as y."}, {"title": "4. PROPOSED AUDIO PROMPT ADAPTER", "content": "To effectively condition AudioLDM2 on the input audio and achieve our transferability and fidelity goals, our AP-Adapter adds two components to AudioLDM2: an audio encoder to extract acoustic features, and decoupled cross-attention adapters to incorporate the acoustic features while maintaining text conditioning capability."}, {"title": "4.1 Audio Encoder and Feature Pooling", "content": "We adopt AudioMAE as the audio encoder, which is used by AudioLDM2 to produce the language of audio (LOA; see Section 3.2) during its training. In our pilot study, we find that using the LOA directly as the condition causes nearly verbatim reconstruction, i.e., information in the input audio is mostly retained. This is undesirable as it greatly limits transferability. To address this issue, we apply a combination of max and mean pooling on the LOA, and leave the pooling rate, which we denote by w, tunable by the user to trade off between fidelity and transferability."}, {"title": "4.2 Decoupled Cross-attention Adapters", "content": "According to the analyses in [29, 30] performed on text-to-image diffusion models finetuned for image editing [31], the cross-attention layers, which allow interaction between text prompt and the diffusion process, undergo the most drastic changes during fine-tuning. Hence, we implement our AP-Adapter also as a set of cross-attention layers.\nRecall that the audio and text prompts are transformed to internal features before interacting with the U-Net for diffusion. We define these features as:\n$$Cr := Pool(AudioMAE(x))$$\n$$Cy := GPT2([FlanT5(y); CLAP(y)]),$$\nwhere cx and cy are the audio and text features respectively.\nThe original AudioLDM2 incorporates the text feature into each U-Net layer via cross-attention:\n$$Ztext := Attention(zW(q), cyW(k), cyW(v)),$$\nwhere z is the U-Net's internal feature, and W(q), W(k),\nW(2) are learnable projections that respectively produce\nthe cross-attention query, key, and values from z or Cy.\nWe keep this cross-attention for text intact (i.e., frozen),\nanticipating it to satisfy transferability out of the box.\nTo incorporate the audio features for fidelity, we place a\ndecoupled audio cross-attention layer as the adapter along-\nside each text cross-attention in a similar light to [18]:\n$$Zaudio := Attention(zW(q), cxW'(k), c\u0153 W/(v)),$$\nwhere W\u2032(k) and W\u2032(2) are the newly introduced adapter\nweights. Since during AudioLDM2 training, the text fea-\nture cy is trained to mimic the LOA from AudioMAE, we\ninitialize W\u2032(k) and W\u2032(v) respectively from W(k) and\nW(2) for all the cross-attention layers in the Unet, and\nfind that this significantly shortens our fine-tuning process\ncompared to random initialization.\nFinally, we obtain the final output of the decoupled text\nand audio cross-attentions via a weighted sum:\n$$Zfusion: Ztext + \u03b1zaudio,$$"}, {"title": "4.3 Training", "content": "We freeze all the parameters in the pretrained AudioLDM2 and AudioMAE, except for the decoupled audio cross-attention adapters with 22M parameters. The loss function follows that of standard (latent) diffusion models:\n$$L = \\mathbb{E}_{(x,y),e,t} ||\u20ac \u2013 \u20ac_9 (x_t, Cx, Cy, t)||_2,$$\nwhere (x, y) are naturally existing paired audio and text,\n\u20ac ~ N(0, I), t is the diffusion step, xt is the noised audio\nlatent features, cx, Cy are the extracted features from text\nand audio prompts (cf. Eqn. (6) and (7)), and \u20ac(\u00b7) is the\nmodel's predicted noise. Minimizing L is equivalent to\nmaximizing the lower bound of p(x | Cx, Cy). During train-ing, we select the audio feature's pooling rate w from the\nset {1, 2, 4, 8} uniformly at random, making the adapters\nrecognize audio features with different resolutions, thereby\nallowing users to balance fidelity and transferability at in-ference. Additionally, we randomly dropout audio and text\nconditions, i.e., setting co to a zero matrix, and y to an\nempty string, to facilitate classifier-free guidance."}, {"title": "4.4 Inference", "content": "At inference, users are free to input any text prompt y as the editing command to achieve their desired edits, i.e, x \u2192 x. In addition, following [32,33], we modify the unconditioned terms in Eqn. (5) using a negative text prompt y. Letting\nCxy := {Cx, Cy}, our inference step is:\n$$Vlog px (x | Cxy, Cy\u2212 ) = \u2207\u017e log p(x | Cy\u2212 )\n+ \\lambda (Vlog p(x | Cry) \u2013 \u2207 z log p(x | Cy\u2212)) $$\nWe find that specifying y is an effective way to avoid\nunwanted properties in \u00e6, e.g., the original timbre for the\ntimbre transfer task, or low quality music in general."}, {"title": "5. EXPERIMENT SETUP", "content": "For the training data of our AP-Adapter, due to our limited computation resource, we use 200K 10-second-long audios with text tags randomly sampled from AudioSet [34] (about 500 hours, or ~10% of the whole dataset).\nFor the audio input x used in evaluation, we compile two datasets: in-domain and out-of-domain, according to whether the AudioSet ontology includes the instrument.\n\u2022 In-domain: We choose 8 common instruments: piano, violin, cello, flute, marimba, organ, harp and acoustic guitar. For each instrument, we manually download 5 high-quality monophonic audios from YouTube (i.e., 40 samples in total) and crop them each to 10 seconds.\n\u2022 Out-of-domain: We collect a dataset of monophonic melodies played by ethnic instruments, including 2 Chi-nese instruments (collected by one of our co-authors) and 5 Korean instruments (downloaded from AIHub [35]). We use 5 audio samples for each instrument (35 audios in total), cropped to 10 seconds each. We note that these instruments are not seen during the training time.\nExcept for the Korean data which is not licensed outside of Korea, we share information to get the data on GitHub."}, {"title": "5.1 Dataset Preparation", "content": "By varying the edit command y, we evaluate AP-Adapter on three music editing tasks:\n\u2022 Timbre transfer: The model is expected to change a melody's timbre to that of the target instrument, and keep all other contents unchanged. For this task, the editing command (y) is set to \"a recording of a [target instrument] solo\". The negative prompt (y) is \"a recording of the [original instrument] solo\".\nFor in-domain input, the target is one of the other 7 in-domain instruments. For out-of-domain input, the target is one of the 8 in-domain instruments. We only use in-domain instruments as the target because our evaluation metrics CLAP [25] and FAD [36] (see Section 5.5) do not recognize the out-of-domain instruments.\n\u2022 Genre transfer: We expect the genre (e.g., jazz and coun-try) to change according to the text prompt, but we wish to retain most of the other content such as melody, rhythm and timbre. Here, we set y := \"[target genre] style music\", and y\u00af := \u201clow quality music\u201d. Here, we target 8 genres: jazz, reggae, rock, metal, pop, hip-hop, disco, country.\n\u2022 Accompaniment generation: We expect that all con-tent in the input melody remains unchanged, but a new instrument is added to accompany the original audio in a pleasant-sounding and harmonic way. We set y := \"Duet, played with [accomp instrument] accompaniment\", and y\u00af := \"low quality music\". The [accomp instrument] is selected in the same way as the [target instrument] in the timbre transfer task.\nWe include these representative tasks which musicians may find useful for their daily workflow, but since y is free-form text, AP-Adapter has the potential for many other tasks."}, {"title": "5.2 Evaluation Tasks", "content": "We use AudioLDM2-large (1.5B parameters), available on HuggingFace, as our backbone model, and only train our 22M-parameter adapters. Training is done on a single one RTX 3090 (24GB) for 35K steps with an effective batch size of 32. We use AdamW optimizer with fixed learning rate 10-4 and weight decay 10-2. To enable CFG, we randomly dropout text and audio features with a 5% probability.\nFor inference, we choose the critical hyperparameters, i.e., pooling rate w, AP scale a, and CFG scale A, by ex-ploring the transferability-fidelity tradeoff space as will be reported in Section 6.1. For timbre transfer and accompani-ment generation, we select w = 2, \u03b1 = 0.5, \u03bb = 7.5. For the"}, {"title": "5.3 Training and Inference Specifics", "content": "We employ the following metrics:\n\u2022 CLAP [25] is used to evaluate transferability, as it is trained with contrastive losses to align the representations for audio and text. We compute the cosine similarity between CLAP audio embedding for the edited audio x and CLAP text embedding for the command y. Higher scores show high semantic relevance between x and y.\n\u2022 Chroma similarity computes the similarity of the original and edited audios x and \u00e6 harmonically and rhythmically, thereby evaluates fidelity. We adopt librosa's [38] CQT chroma method to extract the 12-dimensional chro-magrams [19] to compute framewise cosine similarity."}, {"title": "5.5 Objective Metrics", "content": "We design a listening test that contains 2 sets of music for each of the three tasks. The sets are independent from one another, and each contains a 10-second original audio prompt x, an editing text command y, and three edited audios a generated by our model and the two baselines (with order randomized and kept secret to participants). Participants rate each edited audio on a 5-point Likert scale, according to the following 3 aspects:\n\u2022 Transferability: Do you feel that the generated audio matches what the text prompt asks for?\n\u2022 Fidelity: Do you feel that the generated audio faithfully keeps the original musical content that should not be changed by the text prompt?\n\u2022 Overall preference: Overall, how much do you like the generated audio?\nWe recruit 30 participants from our social circle and ran-domly assign them one of the 6 test suites (3 for in-domain, 3 for out-of-domain). The study takes about 10 minutes."}, {"title": "5.6 Subjective Study", "content": "We discover in our early experiments that several hyperparameters, which are tunable during inference, can drastically affect the edited outputs. Therefore, we conduct a systematic study on the effects of audio pooling rate w (Sec. 4.1), AP scale in decoupled cross-attention a (Sec. 4.2), and classifier-free guidance scale A (Sec. 3.3). Specifically, we observe how their various values induce different behaviors on the transferability-fidelity plane spanned by CLAP and chroma similarity metrics.\n\u2022 The pooling rate w controls the amount of information from the audio prompt. Figure 2a shows clearly that when the pooling rate is low, the fidelity is higher, but at the cost of transferability. For example, the audio generated with w = 1 preserves abundant acoustic information, thus the edited audio sounds like the input audio, but it might not reflect the editing command. The opposite can be said for w = 8. Overall, w = 2 or 4 strikes a good balance.\n\u2022 The AP scale a adjusts the relative importance between the text and audio decoupled cross-attentions. As opposed to pooling rate, it enhances fidelity at the expense of transferability at higher values, as shown in Figure 2b, and a \u2208 [0.4, 0.6] leads to a more balanced performance.\n\u2022 The CFG guidance scale A dictates the strength of text condition as detailed in Eqn. (5). As shown in Figure 2c, somewhat unexpectedly, A does not impact the tradeoff too much when \u5165 \u2265 3.5. Hence, we use X = 7.5 across all tasks following AudioLDM2."}, {"title": "6. RESULTS AND DISCUSSION", "content": "We show the metrics computed on in-domain audios in Table 1, taking the average across the three editing tasks. (We do not report the result for out-of-domain audio inputs as we expect CLAP and FAD to be less reliable there.) In general, AP-Adapter exhibits the most well-rounded performance without significant weaknesses-MusicGen scores high on transferability, but has a much worse FAD score, indicating issues on quality or distributional deviation. We infer that, since MusicGen only considers melody as input rather than the entire audio, it has fewer limitations in the generating process and thus achieves a higher transferability score. On the other hand, AudioLDM2 consistently achieves the best FAD score but lacks fidelity and transferability."}, {"title": "6.1 Hyperparameter Choices", "content": "Table 2 shows the results from our listening test. Our AP-adapter outperforms the two other baseline models in 16 out of 18 comparisons. On top of preserving fine-grained de-tails in the input audio, AP-adapter also tightly follows the editing commands and generate relatively high-quality music, leading in transferability and overall preference except for only the genre transfer task on out-of-domain audios. MusicGen performs better in transferability for genre transfer, but its fidelity is weaker as it only considers the melody of the input audio. With the additional audio-modality condition, AP-adapter has the advantage of \"listening\" to all the details of the input audio, receiving significantly higher scores on fidelity on both in- and out-of-domain cases.\nThe advantage of AP-adapter in fidelity is much stronger in Table 2 rahter than in Table 1. We conjecture that chroma similarity paints only a partial picture for fidelity as it is focused primarily on harmonic properties, leaving out other musical elements such as dynamics and percussive patterns."}, {"title": "6.2 Objective Evaluations", "content": "We presented AP-Adapter, a lightweight add-on to Audi-OLDM2 that empowers it for music editing. AP-Adapter leverages AudioMAE to extract fine-grained features from the audio prompt, and feeds such features into AudioLDM2 via decoupled cross-attention adapters for effective conditioning. With only 500 hours of training data and 22M trainable parameters, AP-Adapter delivers compelling performance across useful editing tasks, namely, timbre transfer, genre transfer, and accompaniment generation. Additionally, it enables users to manipulate the transferability-fidelity tradeoff, and edit out-of-domain audios, which promotes creative endeavors with ethnic instrument audios that are usually scarce in publicly available datasets.\nPromising directions for follow-up works include: (i) exploring more diverse editing tasks under our framework with various editing commands, (ii) extending AP-Adapter to other generative backbones, e.g., autoregressive models, and (iii) adding support for localized edits that can be stitched seamlessly with unchanged audio segments."}, {"title": "6.3 Subjective Evaluations", "content": "We also evaluate the ablated version of AP-Adapter without using the negative prompt (y\u00af). For the timber transfer task, not using the negative prompt induces worse transferability, degrading the CLAP score from 0.405 to 0.378, but does not negatively impact chroma similarity and FAD."}, {"title": "7. CONCLUSIONS", "content": "We also evaluate the ablated version of AP-Adapter without using the negative prompt (y\u00af). For the timber transfer task, not using the negative prompt induces worse transferability, degrading the CLAP score from 0.405 to 0.378, but does not negatively impact chroma similarity and FAD."}]}