{"title": "Evaluating Language Models for Generating and Judging Programming Feedback", "authors": ["Charles Koutcheme", "Nicola Dainese", "Arto Hellas", "Sami Sarsa", "Juho Leinonen", "Syed Ashraf", "Paul Denny"], "abstract": "The emergence of large language models (LLMs) has transformed research and practice in a wide range of domains. Within the com- puting education research (CER) domain, LLMs have received plenty of attention especially in the context of learning programming. Much of the work on LLMs in CER has however focused on ap- plying and evaluating proprietary models. In this article, we evalu- ate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments, and in judging the qual- ity of the programming feedback, contrasting the results against proprietary models. Our evaluations on a dataset of students' sub- missions to Python introductory programming exercises suggest that the state-of-the-art open-source LLMs (Meta's Llama3) are al- most on-par with proprietary models (GPT-40) in both the genera- tion and assessment of programming feedback. We further demon- strate the efficiency of smaller LLMs in the tasks, and highlight that there are a wide range of LLMs that are accessible even for free for educators and practitioners.", "sections": [{"title": "1 INTRODUCTION", "content": "High-quality and timely feedback is essential for students in pro- gramming courses. Some types of feedback, such as whether or not a program runs or passes a provided test suite, are readily avail- able via simple automated means [14, 30]. However, feedback on the causes of subtle programming errors and suggestions for re- solving them can be difficult to produce [15]. Especially in large classes, providing accurate and personalised explanations of bugs as feedback to students is a manual and time consuming task for ed- ucators, and yet essential for reducing frustration and aiding learn- ing.\nThe automated generation of human-like feedback has recently been made possible thanks to the accessibilty of state-of-the-art generative AI tools, such as ChatGPT. In particular, API access to powerful large language models (LLMs) has sparked the develop- ment of many programming feedback tools that are now being de- ployed in classrooms. These include tools to generate improved er- ror messages [25], aid with real-time debugging [13], and to help explain code [24, 26] and tailor next-step hints [37]. Such systems have shown success not only in generating feedback, but also in assessing feedback quality, offering the potential for generating high-quality feedback through iterative improvement.\nDespite the promise of LLM-based feedback generation and eval- uation approaches, the vast majority of research and usage in com- puting education contexts has relied on proprietary models such as GPT-4. This reliance on closed-source LLMs is concerning for several reasons. It requires sending potentially sensitive data to a third-party with no guarantees on how the data will be used, a lack of insight into the way models are trained and what delib- erate or inadvertant biases they may contain, and unpredictable licensing expenses [23]. Open source LLMs, on the other hand, are freely accessible and open for modification and distribution, and have started to become viable alternatives. Despite this, very few studies have explored their capabilities for providing or assessing programming feedback.\nIn this work, our goal is to investigate the potential for open- source models to produce high-quality feedback, and to assess the quality of feedback generated by other LLMs. We focus on feed- back consisting of explanations of bugs or issues in student-written programs and the steps to address these issues. While prior work suggests that open-source language models offer competitive alter- natives to proprietary models for the generation of feedback, the extent to which they can be used as judges (validators) of such feedback remains unknown. Using a publicly available benchmark"}, {"title": "2 RELATED WORK", "content": "2.1 Using Language Models For Feedback\nAutomating assessment of programming exercises and providing feedback on the exercises have been studied for decades within the computing education research domain [15, 29, 30]. Classically, much of the existing work on automating feedback has focused on informing students about mistakes in their code, while providing formative feedback has been less common [15]. Providing sugges- tions on the location of the issue or hints on how to fix the issue can improve students' performance over just pointing out that a test failed [7], but manually creating quality feedback can be very time-consuming. For example, the authors of [42] describe creating over 4000 hand-written \"scaffolding\" messages for a programming course.\nThe recent emergence of powerful language models has led to researchers exploring their capabilities for programming feedback [3, 9, 16, 25, 26, 31\u201333, 35] and, in general, the observations on the qual- ity or utility of feedback has evolved with the introduction of better language models [9]. As an example, GPT-3 had high variability in the quality of feedback, at times generating incorrect and inconsis- tent feedback [3], while GPT-3.5 would often provide meaningful feedback and find issues in code, but also often hallucinate issues that were not present in the code [9]. Language models are also better at detecting some types of errors than others [9, 16], be- ing useful especially for providing feedback on syntax or compi- lation errors [16, 25, 32]. Despite the advances, even the state-of- the-art models like GPT-4 are still not on par with humans when generating feedback for programming exercises [33]. At the same time, there are increasing amounts of evidence from the use of lan- guage model -powered feedback systems and chatbots in program- ming [8, 11, 26, 43], which can further aid students, at least when the technologies of the courses are such that they have had suf- ficient exposure prior to the knowledge cutoff point of the used language models [8].\nThe majority of existing work on language models for program- ming feedback in the computing education research context has fo- cused on utilizing proprietary models (mainly from OpenAI), while the use of open-source models has received only little attention. Calls for increasing use of open-source models have been voiced [45], already due to potential privacy issues related to sharing student data with language model providers. Work on utilizing open-source models for the task is also starting to emerge, where one of the research aspects has been contrasting the performance of open- source models to the proprietary ones; researchers have already observed that open-source models are on par with models such as GPT-3.5-Turbo for programming feedback [21].\nIn our work, our first research question re-investigates how var- ious language models, including open-source ones, perform in ex- plaining issues in student programs and providing fixes, comple- menting prior studies.\n2.2 Using Language Models as Judges\nThe idea of using an LLM to judge the output of other LLMs \u2013 LLMs-as-judges \u2013 was first studied in the work of Zheng et al. [46], showing good promise, but also limitations, e.g., in grading math and reasoning tasks. Since then, GPT-4 has been used in multiple studies as a judge of the quality of other LLMs' generations [5, 28], also in educational contexts [20, 21, 38]. Moreover, the reliance on GPT-4, a proprietary model, has sparked interest in leveraging other open-source language models to act as judges [17, 44, 47]. Yet, recent work [41] has highlighted the limitations of relying on a single language model for evaluating the quality of other lan- guage models' output, and suggested to employ a diverse ensemble of smaller models from different LLM families as a jury for cheaper and less biased evaluations. When answering our second research question, we test this hypothesis by comparing the usage of sin- gle judges (both open-source and proprietary) and that of a jury of smaller open-source language models."}, {"title": "3 METHODOLOGY", "content": "In this section, we describe our methodology for answering our two research questions. We first introduce the dataset used in our evaluations, then our methods used for answering RQ1 and RQ2.\n3.1 Dataset\nWe use data from the Socratic guidance benchmark [2], which con- sists of 57 introductory-level programming assignments requiring students to write functions. Each of the assignments is accompa- nied by the associated test cases, a unique incorrect student so- lution, the ground truth descriptions of the bugs in the program, the required bug fixes, and several conversation threads between a fake student and a teaching assistant. The ultimate goal for the benchmark is evaluating LLMs' ability to help students using the socratic method, i.e., guiding students in finding a solution on their own, by asking a series of relevant questions that help their rea- soning. However, for this study, we focus solely on identifying the issues in the code and any required fixes, as it is a fundamental step"}, {"title": "3.2 Generating High-Quality Feedback", "content": "Given a student's incorrect program, our goal regarding RQ1 is to evaluate LLMs ability to provide two particular types of feedback: explanations of the bugs in the program and suggested fixes for the found bugs.\nFeedback generation. We prompt the models to provide feedback according to following example:\nTo elaborate, we provide a language model: 0 a system prompt and 1 a description of the task (with all the necessary contextual information), which results in output 2.\nFeedback Language Models. We consider the following open-source models: Gemma-2B [6], Phi-3-mini [1] (3.8B parameters), Mistral- 7B [12], Llama3-8B [40], Llama3-70B [40]. We choose these models because of their extensive documentation, community adoption, strong performance on code and language reasoning benchmarks (e.g., HumanEval [4] and MMLU [10]), for their parameter count, and their ability to follow instructions. This selection covers the recent state-of-the-art models from various companies across the most used model-sizes for LLMs. Furthermore, we also evaluate two of OpenAI's proprietary flagship models, GPT-3.5-turbo and GPT-40, well representative of the current industry standards.\nWe query proprietary models using the OpenAI Python library, and open-source ones with the EasyLLM [39] Python library to simplify querying them through the HuggingFace Inference API. All models are evaluated using greedy decoding. Next, we explain the annotation process before detailing the grading rubric.\nAnnotation. We use the seven models presented above, and the 57 programs of the benchmark, which results in 7 \u00d7 57 = 399 model outputs. To answer our first research question, two annotators (two authors of the paper) annotated all 399 model outputs as follows. First, we selected 11 problems out of the 56 available problems us- ing the manual annotation subset presented in [2]. Then, the two annotators independently annotated 79 model outputs on an initial description of each grading criterion on this subset. We then com- puted an inter-annotator agreement score using Cohen's Kappa coefficient. The resulting annotation process yielded a moderate inter-rater agreement of 0.49. Discrepancies were discussed and resolved to align the annotators' understanding. After comparing annotations and discussing conflicts in understanding, the two au- thors refined the grading rubric description (presented shortly).\nThe remaining feedback examples were split between the two an- notators (169 and 151 feedback respectively). The final annotated dataset formed the basis for evaluating the quality of the feedback generated by the language models.\nGrading Criteria. During the final annotation phase, each expert used the following grading criteria for evaluating the quality of a single generated bug explanation (E), and the quality of the gener- ated fixes (F):\nThese criteria extend prior work [9, 21, 34]. The first two crite- ria represent the correctness of the explanations. The annotators followed the following guidelines: for each ground truth bug (pro- vided in the dataset), match the bug with the descriptions in the model output. If some of the generated model descriptions did not match, the criteria ES was set to false. Then, independently of the correctness, we looked at whether or not a novice programmer (unaware of the real issues) could understand the meaning of the provided bug description. We follow the same strategy for the fixes. Moreover, for the clarity criterion, we ensure that the fixes provide clear descriptions of changes with snippets or at least highlight changes in a repaired program (if present).\nIn addition, as during the experiments we observed that the gen- erated feedback often included repaired programs, even though we did not prompt the model for this, we report the correctness of these program repair suggestions (RC - Repair Correct). We evalu- ated the correctness of these repairs using the associated unit tests from the original dataset. Although program repairs are not the primary focus of this paper, they represent another valuable form of feedback for students and can later be useful for hint generation."}, {"title": "3.3 Automatic Feedback Evaluation", "content": "In this subsection, we present the methods we used to automati- cally evaluate the quality of LLM-generated feedback using other language models (answering RQ2). We explored two approaches (a single LLM as a judge, and an ensemble of LLMs as a jury) on two scenarios, depending on whether a reference answer is avail- able or not. We first describe how we generate the responses to the grading criteria using a single LLM as a judge for the two scenar- ios. We then outline our ensemble of LLMs and how we obtain the jury annotations."}, {"title": "4 RESULTS", "content": "4.1 Generating Feedback\nTable 1 shows the performance of each language model on vari- ous grading criteria, including both individual and grouped crite- ria, based on human evaluations. We make the following observa- tions.\nOpen-Source vs. Proprietary Models. The GPT-40 model shows very strong performance across nearly all individual and grouped criteria. Among the open-source models, there is significant vari- ance in performance across different criteria. For instance, Llama- 3-70B performs on par with or better than GPT-3.5-turbo on the EA, EC, and FA, FC, RC individual criteria. Llama-3-70B even re- mains competitive with both GPT-3.5-turbo and GPT-40 for gener- ating perfect explanations (EA, ES, EC), and perfect fixes (FA, FS, FC, RC). In contrast, smaller models such as Gemma-2B perform poorly across the board. However, size alone does not determine performance, as some small open-source language models, like Phi- 3-mini, despite their smaller size, perform competitively on several criteria, notably EA, FA, and RC.\nStrengths and Weaknesses. Each model has its strengths and weak- nesses. However, we notice that most models struggle with selec- tivity (i.e., they identify irrelevant or redundant issues), while they generally produce comprehensible outputs (i.e., well-formatted re- sponses). When looking at the feedback generations, the stronger models (e.g. Llama-3-70B, and the GPTs) often added performance suggestions (e.g. replace a for loop with a built-in function), while the other models often added incorrect outputs. This indicates a broader challenge in developing models that can effectively iden- tify and focus on relevant issues without including redundant or ir- relevant information. Improvements in this area could lead to sub- stantial overall performance gains.\nProgram Repairs. The RC column in Table 1 shows the propor- tion of generated correct repairs. The repair correctness (RC) scores show considerable variation, with GPT-40 scoring the highest. In- terestingly, the correctness of the repairs does not correlate indi- vidually with either the quality of the explanations or with the ac- curacy of the fixes. However, the models that are better at writing both accurate and hallucination-free explanations and fixes (i.e., EA, ES, FA, FS) are also better at generating high-quality repairs, aligning with previous observations [20]. Similarly as in [20], GPT- 3.5-turbo is the only model that disrupts this relationship."}, {"title": "4.2 Evaluating Feedback", "content": "Table 2 shows the results of the judgment task, detailing the f0.5 scores and kappa scores for each language model under the two scenarios (SAG, and GAG). We can make several observations from these results:\nOpen-Source vs. Proprietary Models. Among the open-source mod- els, Llama3-70B stands out as a better judge than GPT-3.5-turbo when considering the f0.5 scores, particularly in the Explanation Accurate (EA), Explanation Selective (ES), Fixes Accurate (FA), and Fixes Clear (FC) criteria. Notably, without ground truth bug list- ings, GPT-40 is the only model performing consistently well among the criteria, either as the best or near the best model. This is most clear in the selectivity criteria (ES and FS) where GPT-40 has over double the next-best f0.5 score. Providing the models with ground truth descriptions of the bugs and issues in the programs signif- icantly improves their results overall criteria (almost over 10%) compared to relying on their own explanations of the issues. No- tably, strong models like Llama3-70B and GPT variants did not ben- efit as much in completeness (EA), suggesting they already handle this criterion better. Interestingly, when given ground truth anno- tations (GAG), Llama3-70B outperforms GPT-40 across almost all criteria, showing the most significant improvement from these an- notations. However, once again all models, except GPT-40, struggle the most with selectivity (ES), although ground truth annotations significantly boost the score for all models (especially for Llama3- 70B). This indicates that the judges struggle to detect when the feedback contains hallucinated or non-existent bugs. This could be due to further hallucination issues and highlights an important area of improvement.\nKappa Scores. The low kappa scores in the SAG setting indicate that most model annotations could be due to \"chance\". When inves- tigating the reasons for the scores, we see that most models tend to be overly positive, predicting \u201cyes\u201d for almost all criteria. This phe- nomenon aligns with observations in [21], highlighting a tendency of models to overestimate the quality of feedback. When provided with ground truth annotations, the results improve to reach a mod- erate level of agreement, in particular for the Llama models.\nSelf-Evaluation vs. Evaluation of Others. Models perform better when evaluating other models' outputs rather than their own. This might be due to a bias toward positive evaluations, especially within models of the same family, as noted in previous research [46].\nEnsemble Performance. Combining multiple models into an en- semble does not improve judgment quality; instead, it biases the results. The ensemble approach, which averaged the models Phi-3- mini, Mistral-7B, and Llama3-8B, did not yield better performance. This contrasts with previous work by Verga et al. [41], possibly due to the absence of few-shot examples and the overall lower per- formance of the individual models used here. The previous study involved models that were competitive with GPT-3.5-turbo, lead- ing to better ensemble performance."}, {"title": "5 DISCUSSION", "content": "Teaching and Learning Implications. Our study aims to show that a wide range of models are accessible to educators and practition- ers, some competitive models even for free. Llama3-70B is shown competitive against GPT-3.5 for feedback generation and GPT-40 for judging the quality of generated feedback. Educators might for instance rely on GPT-3.5-turbo for generating feedback and Llama- 3-70B for validating [34] the quality of this feedback. Llama3-70B's ability to judge feedback could be useful for training more perfor- mant smaller models, instead of relying on GPT-4 [38]. Notably, the size of a language model no longer correlates directly with per- formance. For example, the Phi-3-mini model is competitive with the 7B Mistral and 8B Llama models, while being able to run on consumer laptops and even phones. The model struggles with se- lectivity, which reduces its performance. However, we believe that fine-tuning techniques might make the results even stronger [19]. Importantly, these open-source models are also easy to access thanks to libraries such as EasyLLM and APIs offered by compa- nies such as HuggingFace. For instance, for conducting our exper- iments, using the EasyLLM package, all models were freely acces- sible, except for Llama-70B (which required paying 9 dollars for a month of rate-limited access). We acknowledge that using an ex- ternal API for querying open-source language models might beat the purpose of data privacy. However, several institutions lever- age ChatGPT APIs in one way or another [27], and HuggingFace platforms, which are dedicated to open-source, offer the same data privacy guarantees.\nLimitations. Our work has limitations. Our prompts will affect the results and more specific prompts (or different prompting strate- gies) might influence model performance. Also, we only consid- ered introductory programming assignments written in Python and not other programming languages. Moreover, we only consid- ered two types of feedback, but other types exist. Our selection of language models, although considered the recent state-of-the-art, does not exhaust all possible alternatives to popular models. Our labelling process is also not perfect, as we only used two raters, which resulted in a moderate inter-rater reliability (0.49), and our grading criteria did not use actual students (the intended audience) to rate the clarity of the outputs. Additionally, for judging feedback, we could have used judge language models which are specifically designed for evaluation, but we used generic language models in- stead. Importantly, due to limitations of the EasyLLM library, the Phi-3-mini results were aggregated on a subset of the feedback (370 feedback instead of 399), which limits the interpretation of that spe- cific model's judging performance.\nFuture Work. Our overarching goal is to allow educators to keep track of new language models available and how they could be useful for their purposes. In the future, we will conduct a larger scale evaluation of open-source language models' ability to gen- erate (and judge) other types of feedback and support. In particu- lar, we are extending the Socratic benchmark to include next-step hints [37], and are running evaluation of LLMs for being Socratic guides. We will study how performance for different types of feed- back relates to each other. We will also maintain an online leader- board, similar to what is done for instance on the HuggingFace leaderboard, and develop benchmarks. Beyond the tracking of the model performance, we aim to improve the ability of small lan- guage models (e.g. Phi-3-mini) to be teaching assistants by using Reinforcement Learning techniques to tackle the selectivity prob- lem and exploring other generation parameter techniques. The var- ied strengths of different models suggest that a combined approach (ensemble methods) might yield even better results for generation. As our LLM jury results contrasted those by Verga et al. [41], we also intend to conduct a large-scale study on how the variability, individual performance, and the number of judges in the LLM jury affect the performance of the LLM jury in our context."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we evaluated (1) how language models perform for providing explanations of issues in programs and generating bug fixes, and (2) how well different large language models, including open-source ones, perform in evaluating the quality of other LM-generated feedback. Our paper highlights that top open-source lan- guage models are valid competitors to proprietary language mod- els both for generating and assessing the quality of programming feedback. Open-source language models could have benefits to power free tools, which is important for instance for less funded institu- tions. As an additional contribution, we also release the code used for conducting our experiments\u00b9, as well as the model answers and the annotator's responses."}]}