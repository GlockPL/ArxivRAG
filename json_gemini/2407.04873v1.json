{"title": "Evaluating Language Models for Generating and Judging Programming Feedback", "authors": ["Charles Koutcheme", "Nicola Dainese", "Juho Leinonen", "Sami Sarsa", "Paul Denny", "Arto Hellas", "Syed Ashraf"], "abstract": "The emergence of large language models (LLMs) has transformed research and practice in a wide range of domains. Within the computing education research (CER) domain, LLMs have received plenty of attention especially in the context of learning programming. Much of the work on LLMs in CER has however focused on applying and evaluating proprietary models. In this article, we evaluate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments, and in judging the quality of the programming feedback, contrasting the results against proprietary models. Our evaluations on a dataset of students' submissions to Python introductory programming exercises suggest that the state-of-the-art open-source LLMs (Meta's Llama3) are almost on-par with proprietary models (GPT-40) in both the generation and assessment of programming feedback. We further demonstrate the efficiency of smaller LLMs in the tasks, and highlight that there are a wide range of LLMs that are accessible even for free for educators and practitioners.", "sections": [{"title": "1 INTRODUCTION", "content": "High-quality and timely feedback is essential for students in programming courses. Some types of feedback, such as whether or not a program runs or passes a provided test suite, are readily available via simple automated means [14, 30]. However, feedback on the causes of subtle programming errors and suggestions for resolving them can be difficult to produce [15]. Especially in large classes, providing accurate and personalised explanations of bugs as feedback to students is a manual and time consuming task for educators, and yet essential for reducing frustration and aiding learning.\nThe automated generation of human-like feedback has recently been made possible thanks to the accessibilty of state-of-the-art generative Al tools, such as ChatGPT. In particular, API access to powerful large language models (LLMs) has sparked the development of many programming feedback tools that are now being deployed in classrooms. These include tools to generate improved error messages [25], aid with real-time debugging [13], and to help explain code [24, 26] and tailor next-step hints [37]. Such systems have shown success not only in generating feedback, but also in assessing feedback quality, offering the potential for generating high-quality feedback through iterative improvement.\nDespite the promise of LLM-based feedback generation and evaluation approaches, the vast majority of research and usage in computing education contexts has relied on proprietary models such as GPT-4. This reliance on closed-source LLMs is concerning for several reasons. It requires sending potentially sensitive data to a third-party with no guarantees on how the data will be used, a lack of insight into the way models are trained and what deliberate or inadvertant biases they may contain, and unpredictable licensing expenses [23]. Open source LLMs, on the other hand, are freely accessible and open for modification and distribution, and have started to become viable alternatives. Despite this, very few studies have explored their capabilities for providing or assessing programming feedback.\nIn this work, our goal is to investigate the potential for open-source models to produce high-quality feedback, and to assess the quality of feedback generated by other LLMs. We focus on feedback consisting of explanations of bugs or issues in student-written programs and the steps to address these issues. While prior work suggests that open-source language models offer competitive alternatives to proprietary models for the generation of feedback, the extent to which they can be used as judges (validators) of such feedback remains unknown. Using a publicly available benchmark"}, {"title": "RELATED WORK", "content": "2.1 Using Language Models For Feedback\nAutomating assessment of programming exercises and providing feedback on the exercises have been studied for decades within the computing education research domain [15, 29, 30]. Classically, much of the existing work on automating feedback has focused on informing students about mistakes in their code, while providing formative feedback has been less common [15]. Providing suggestions on the location of the issue or hints on how to fix the issue can improve students' performance over just pointing out that a test failed [7], but manually creating quality feedback can be very time-consuming. For example, the authors of [42] describe creating over 4000 hand-written \"scaffolding\" messages for a programming course.\nThe recent emergence of powerful language models has led to researchers exploring their capabilities for programming feedback [3, 9, 16, 25, 26, 31\u201333, 35] and, in general, the observations on the quality or utility of feedback has evolved with the introduction of better language models [9]. As an example, GPT-3 had high variability in the quality of feedback, at times generating incorrect and inconsistent feedback [3], while GPT-3.5 would often provide meaningful feedback and find issues in code, but also often hallucinate issues that were not present in the code [9]. Language models are also better at detecting some types of errors than others [9, 16], being useful especially for providing feedback on syntax or compilation errors [16, 25, 32]. Despite the advances, even the state-of-the-art models like GPT-4 are still not on par with humans when generating feedback for programming exercises [33]. At the same time, there are increasing amounts of evidence from the use of language model -powered feedback systems and chatbots in programming [8, 11, 26, 43], which can further aid students, at least when the technologies of the courses are such that they have had sufficient exposure prior to the knowledge cutoff point of the used language models [8].\nThe majority of existing work on language models for programming feedback in the computing education research context has focused on utilizing proprietary models (mainly from OpenAI), while the use of open-source models has received only little attention. Calls for increasing use of open-source models have been voiced [45], already due to potential privacy issues related to sharing student data with language model providers. Work on utilizing open-source models for the task is also starting to emerge, where one of the research aspects has been contrasting the performance of open-source models to the proprietary ones; researchers have already observed that open-source models are on par with models such as GPT-3.5-Turbo for programming feedback [21].\nIn our work, our first research question re-investigates how various language models, including open-source ones, perform in explaining issues in student programs and providing fixes, complementing prior studies."}, {"content": "2.2 Using Language Models as Judges\nThe idea of using an LLM to judge the output of other LLMs \u2013 LLMs-as-judges \u2013 was first studied in the work of Zheng et al. [46], showing good promise, but also limitations, e.g., in grading math and reasoning tasks. Since then, GPT-4 has been used in multiple studies as a judge of the quality of other LLMs' generations [5, 28], also in educational contexts [20, 21, 38]. Moreover, the reliance on GPT-4, a proprietary model, has sparked interest in leveraging other open-source language models to act as judges [17, 44, 47]. Yet, recent work [41] has highlighted the limitations of relying on a single language model for evaluating the quality of other language models' output, and suggested to employ a diverse ensemble of smaller models from different LLM families as a jury for cheaper and less biased evaluations. When answering our second research question, we test this hypothesis by comparing the usage of single judges (both open-source and proprietary) and that of a jury of smaller open-source language models."}, {"title": "3 METHODOLOGY", "content": "In this section, we describe our methodology for answering our two research questions. We first introduce the dataset used in our evaluations, then our methods used for answering RQ1 and RQ2.\n3.1 Dataset\nWe use data from the Socratic guidance benchmark [2], which consists of 57 introductory-level programming assignments requiring students to write functions. Each of the assignments is accompanied by the associated test cases, a unique incorrect student solution, the ground truth descriptions of the bugs in the program, the required bug fixes, and several conversation threads between a fake student and a teaching assistant. The ultimate goal for the benchmark is evaluating LLMs' ability to help students using the socratic method, i.e., guiding students in finding a solution on their own, by asking a series of relevant questions that help their reasoning. However, for this study, we focus solely on identifying the issues in the code and any required fixes, as it is a fundamental step"}, {"title": "3.2 Generating High-Quality Feedback", "content": "Given a student's incorrect program, our goal regarding RQ1 is to evaluate LLMs ability to provide two particular types of feedback: explanations of the bugs in the program and suggested fixes for the found bugs.\nTo elaborate, we provide a language model: 0 a system prompt and 1 a description of the task (with all the necessary contextual information), which results in output 2.\nFeedback Language Models. We consider the following open-source models: Gemma-2B [6], Phi-3-mini [1] (3.8B parameters), Mistral-7B [12], Llama3-8B [40], Llama3-70B [40]. We choose these models because of their extensive documentation, community adoption, strong performance on code and language reasoning benchmarks (e.g., HumanEval [4] and MMLU [10]), for their parameter count, and their ability to follow instructions. This selection covers the recent state-of-the-art models from various companies across the most used model-sizes for LLMs. Furthermore, we also evaluate two of OpenAI's proprietary flagship models, GPT-3.5-turbo and GPT-40, well representative of the current industry standards.\nWe query proprietary models using the OpenAI Python library, and open-source ones with the EasyLLM [39] Python library to simplify querying them through the HuggingFace Inference API. All models are evaluated using greedy decoding. Next, we explain the annotation process before detailing the grading rubric.\nAnnotation. We use the seven models presented above, and the 57 programs of the benchmark, which results in 7 \u00d7 57 = 399 model outputs. To answer our first research question, two annotators (two authors of the paper) annotated all 399 model outputs as follows. First, we selected 11 problems out of the 56 available problems using the manual annotation subset presented in [2]. Then, the two annotators independently annotated 79 model outputs on an initial description of each grading criterion on this subset. We then computed an inter-annotator agreement score using Cohen's Kappa coefficient. The resulting annotation process yielded a moderate inter-rater agreement of 0.49. Discrepancies were discussed and resolved to align the annotators' understanding. After comparing annotations and discussing conflicts in understanding, the two authors refined the grading rubric description (presented shortly).\nGrading Criteria. During the final annotation phase, each expert used the following grading criteria for evaluating the quality of a single generated bug explanation (E), and the quality of the generated fixes (F):\nThese criteria extend prior work [9, 21, 34]. The first two criteria represent the correctness of the explanations. The annotators followed the following guidelines: for each ground truth bug (provided in the dataset), match the bug with the descriptions in the model output. If some of the generated model descriptions did not match, the criteria ES was set to false. Then, independently of the correctness, we looked at whether or not a novice programmer (unaware of the real issues) could understand the meaning of the provided bug description. We follow the same strategy for the fixes. Moreover, for the clarity criterion, we ensure that the fixes provide clear descriptions of changes with snippets or at least highlight changes in a repaired program (if present).\nIn addition, as during the experiments we observed that the generated feedback often included repaired programs, even though we did not prompt the model for this, we report the correctness of these program repair suggestions (RC - Repair Correct). We evaluated the correctness of these repairs using the associated unit tests from the original dataset. Although program repairs are not the primary focus of this paper, they represent another valuable form of feedback for students and can later be useful for hint generation."}, {"title": "3.3 Automatic Feedback Evaluation", "content": "In this subsection, we present the methods we used to automatically evaluate the quality of LLM-generated feedback using other language models (answering RQ2). We explored two approaches (a single LLM as a judge, and an ensemble of LLMs as a jury) on two scenarios, depending on whether a reference answer is available or not. We first describe how we generate the responses to the grading criteria using a single LLM as a judge for the two scenarios. We then outline our ensemble of LLMs and how we obtain the jury annotations."}, {"title": "4 RESULTS", "content": "4.1 Generating Feedback\nTable 1 shows the performance of each language model on various grading criteria, including both individual and grouped criteria, based on human evaluations. We make the following observations.\nOpen-Source vs. Proprietary Models. The GPT-40 model shows very strong performance across nearly all individual and grouped criteria. Among the open-source models, there is significant variance in performance across different criteria. For instance, Llama-3-70B performs on par with or better than GPT-3.5-turbo on the EA, EC, and FA, FC, RC individual criteria. Llama-3-70B even remains competitive with both GPT-3.5-turbo and GPT-40 for generating perfect explanations (EA, ES, EC), and perfect fixes (FA, FS, FC, RC). In contrast, smaller models such as Gemma-2B perform poorly across the board. However, size alone does not determine performance, as some small open-source language models, like Phi-3-mini, despite their smaller size, perform competitively on several criteria, notably EA, FA, and RC.\nStrengths and Weaknesses. Each model has its strengths and weaknesses. However, we notice that most models struggle with selectivity (i.e., they identify irrelevant or redundant issues), while they generally produce comprehensible outputs (i.e., well-formatted responses). When looking at the feedback generations, the stronger"}, {"title": "4.2 Evaluating Feedback", "content": "Table 2 shows the results of the judgment task, detailing the f0.5 scores and kappa scores for each language model under the two scenarios (SAG, and GAG). We can make several observations from these results:\nOpen-Source vs. Proprietary Models. Among the open-source models, Llama3-70B stands out as a better judge than GPT-3.5-turbo when considering the f0.5 scores, particularly in the Explanation Accurate (EA), Explanation Selective (ES), Fixes Accurate (FA), and Fixes Clear (FC) criteria. Notably, without ground truth bug listings, GPT-40 is the only model performing consistently well among the criteria, either as the best or near the best model. This is most clear in the selectivity criteria (ES and FS) where GPT-40 has over double the next-best f0.5 score. Providing the models with ground truth descriptions of the bugs and issues in the programs significantly improves their results overall criteria (almost over 10%) compared to relying on their own explanations of the issues. Notably, strong models like Llama3-70B and GPT variants did not benefit as much in completeness (EA), suggesting they already handle this criterion better. Interestingly, when given ground truth annotations (GAG), Llama3-70B outperforms GPT-40 across almost all criteria, showing the most significant improvement from these annotations. However, once again all models, except GPT-40, struggle the most with selectivity (ES), although ground truth annotations significantly boost the score for all models (especially for Llama3-70B). This indicates that the judges struggle to detect when the feedback contains hallucinated or non-existent bugs. This could be due to further hallucination issues and highlights an important area of improvement.\nKappa Scores. The low kappa scores in the SAG setting indicate that most model annotations could be due to \"chance\". When investigating the reasons for the scores, we see that most models tend to be overly positive, predicting \u201cyes\u201d for almost all criteria. This phenomenon aligns with observations in [21], highlighting a tendency of models to overestimate the quality of feedback. When provided with ground truth annotations, the results improve to reach a moderate level of agreement, in particular for the Llama models.\nSelf-Evaluation vs. Evaluation of Others. Models perform better when evaluating other models' outputs rather than their own. This"}, {"title": "5 DISCUSSION", "content": "Teaching and Learning Implications. Our study aims to show that a wide range of models are accessible to educators and practitioners, some competitive models even for free. Llama3-70B is shown competitive against GPT-3.5 for feedback generation and GPT-40 for judging the quality of generated feedback. Educators might for instance rely on GPT-3.5-turbo for generating feedback and Llama-3-70B for validating [34] the quality of this feedback. Llama3-70B's ability to judge feedback could be useful for training more performant smaller models, instead of relying on GPT-4 [38]. Notably, the size of a language model no longer correlates directly with performance. For example, the Phi-3-mini model is competitive with the 7B Mistral and 8B Llama models, while being able to run on consumer laptops and even phones. The model struggles with selectivity, which reduces its performance. However, we believe that fine-tuning techniques might make the results even stronger [19]. Importantly, these open-source models are also easy to access thanks to libraries such as EasyLLM and APIs offered by companies such as HuggingFace. For instance, for conducting our experiments, using the EasyLLM package, all models were freely accessible, except for Llama-70B (which required paying 9 dollars for a month of rate-limited access). We acknowledge that using an external API for querying open-source language models might beat the purpose of data privacy. However, several institutions leverage ChatGPT APIs in one way or another [27], and HuggingFace platforms, which are dedicated to open-source, offer the same data privacy guarantees.\nLimitations. Our work has limitations. Our prompts will affect the results and more specific prompts (or different prompting strategies) might influence model performance. Also, we only considered introductory programming assignments written in Python and not other programming languages. Moreover, we only considered two types of feedback, but other types exist. Our selection of language models, although considered the recent state-of-the-art, does not exhaust all possible alternatives to popular models. Our labelling process is also not perfect, as we only used two raters, which resulted in a moderate inter-rater reliability (0.49), and our grading criteria did not use actual students (the intended audience) to rate the clarity of the outputs. Additionally, for judging feedback, we could have used judge language models which are specifically designed for evaluation, but we used generic language models instead. Importantly, due to limitations of the EasyLLM library, the Phi-3-mini results were aggregated on a subset of the feedback (370"}, {"title": "6 CONCLUSIONS", "content": "In this paper, we evaluated (1) how language models perform for providing explanations of issues in programs and generating bug fixes, and (2) how well different large language models, including open-source ones, perform in evaluating the quality of other LM-generated feedback. Our paper highlights that top open-source language models are valid competitors to proprietary language models both for generating and assessing the quality of programming feedback. Open-source language models could have benefits to power free tools, which is important for instance for less funded institutions. As an additional contribution, we also release the code used for conducting our experiments\u00b9, as well as the model answers and the annotator's responses."}]}