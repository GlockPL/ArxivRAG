{"title": "Theoretical Benefit and Limitation of Diffusion Language Model", "authors": ["Guhao Feng", "Yihan Geng", "Jian Guan", "Wei Wu", "Liwei Wang", "Di He"], "abstract": "Diffusion language models have emerged as a promising approach for text generation. One would naturally expect this method to be an efficient replacement for autoregressive models since multiple tokens can be sampled in parallel during each diffusion step. However, its efficiency-accuracy trade-off is not yet well understood. In this paper, we present a rigorous theoretical analysis of a widely used type of diffusion language model, the Masked Diffusion Model (MDM), and find that its effectiveness heavily depends on the target evaluation metric. Under mild conditions, we prove that when using perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling steps regardless of sequence length, demonstrating that efficiency can be achieved without sacrificing performance. However, when using the sequence error rate-which is important for understanding the \u201ccorrectness\" of a sequence, such as a reasoning chain-we show that the required sampling steps must scale linearly with sequence length to obtain \u201ccorrect\u201d sequences, thereby eliminating MDM's efficiency advantage over autoregressive models. Our analysis establishes the first theoretical foundation for understanding the benefits and limitations of MDMs. All theoretical findings are supported by empirical studies.", "sections": [{"title": "1. Introduction", "content": "Diffusion models (Ho et al., 2020; Song et al., 2021b) have emerged as a powerful paradigm in generative modeling, establishing state-of-the-art performance in image synthesis (Karras et al., 2022; Song et al., 2021a). Their extension to discrete domains has opened new possibilities for generating sequences, such as natural language (Campbell et al., 2022; Dieleman et al., 2022; Zheng et al., 2023; Lou et al., 2024; Campbell et al., 2024; Lovelace et al., 2024) and biological sequences (Rastogi et al., 2022; Vignac et al., 2022; Sun & Yang, 2023; Avdeyev et al., 2023). Among various discrete diffusion architectures, masked diffusion models (MDMs) (Shi et al., 2024; Sahoo et al., 2024; Ou et al., 2024)\u2014which generate sequences by iteratively converting masks to tokens\u2014have emerged as a prominent approach and demonstrated competitive performance across diverse language modeling tasks.\nWhile auto-regressive models generate sequences token-by-token, discrete diffusion models can generate multiple tokens simultaneously during each step (reverse process). Therefore, it is natural to hypothesize that this parallel sampling improves generation efficiency. However, we argue that reaching a conclusion requires considering both computational cost and generation quality. Specifically, we pose the following question: do discrete diffusion models achieve superior efficiency when the generated content meets an acceptable quality standard? There may be multiple answers to this question. If diffusion models require fewer neural network executions while maintaining quality, they can offer better acceleration. Conversely, if their execution count is comparable to or exceeds that of auto-regressive models, diffusion language models may not be a better choice.\nTo answer the above question, we leverage two complementary metrics to evaluate the efficiency of MDMs in language modeling. The first metric, token error rate (TER), quantifies token-level accuracy, which correlates with the fluency of the generated text. In practice, perplexity is a widely used metric for measuring token-level errors of language models (Jelinek et al., 1977; Devlin et al., 2019); thus, we define the metric of TER by perplexity in this paper. The second metric, sequence error rate (SER), evaluates the correctness of an entire sequence, which is crucial for reasoning tasks requiring logically correct sequences. We provide a natural definition of SER that reflects the correctness of the whole sequence. Together, these metrics enable a comprehensive evaluation of the efficiency of MDMs under both token-level and sequence-level metrics. We first provide a positive theoretical result regarding TER. We prove that under mild conditions, MDMs can achieve near-optimal TER with sampling steps regardless of the sequence length L. Compared to the auto-regressive model, which must be executed L times to generate the sequence, MDMs demonstrate substantial efficiency gains, especially when the generation length is long.\nHowever, we show that this efficiency advantage diminishes when SER is considered. We theoretically prove that to achieve a low SER, the number of required sampling steps for MDMs scales at least linearly with sequence length. Intuitively, this limitation arises from the fact that SER, as a metric for the entire sequence, requires the generated sequence to be free of any error in the whole sequence, which forces MDMs to sample only a small number of tokens per step to mitigate such inconsistencies. As a result, the number of required sampling steps can be significant. It is notable that each MDM sampling step usually incurs a higher computational cost than an auto-regressive step under the same architecture, thus MDMs offer no efficiency advantage under this metric.\nFinally, we validate our theoretical findings through comprehensive experiments. Our experiments examine MDMs trained on formal languages, including n-gram languages and Hidden Markov Models (HMMs), systematically analyzing the relationship between performance and efficiency under both TER and SER metrics. Additional experiments on natural language tasks, including TER evaluation on text generation and SER assessment on the GSM8k dataset (Cobbe et al., 2021), corroborate our theoretical predictions: while achieving low SER necessitates substantial sampling steps, relatively few steps suffice for TER. These results provide practical guidance for deploying diffusion language models across different applications."}, {"title": "2. Related Work", "content": "Discrete Diffusion Models. The auto-regressive paradigm has achieved significant success in language modeling (Dai, 2019; Floridi & Chiriatti, 2020; Achiam et al., 2023). However, its left-to-right, token-by-token generation approach is not without limitations. Notably, it faces challenges such as restricted controllability (Zhang et al., 2023) and inefficiencies in inference speed (Leviathan et al., 2023). To overcome these drawbacks, inspired by the success of diffusion models in image generation (Sohl-Dickstein et al., 2015; Song et al., 2021a; Karras et al., 2022) researchers have adapted these techniques for NLP tasks (Austin et al., 2021; He et al., 2022; Chen et al., 2022; Meng et al., 2022; Ye et al., 2023; Gulrajani & Hashimoto, 2023; Zhang et al., 2024). Discrete diffusion models, in particular, have shown promising results, achieving comparable performance with auto-regressive models across a range of NLP benchmarks.\nDiscrete diffusion models can be categorized based on the initialization strategy of the reverse process: (1) reverse processes that begin with masked sequences and (2) reverse processes that start with sequences of tokens sampled randomly from the vocabulary. The first category, termed masked diffusion models (MDMs), includes models such as SEDD Absorb (Lou et al., 2024) and its streamlined variants in subsequent works (Sahoo et al., 2024; Zhao et al., 2024; Shi et al., 2024; Ou et al., 2024; Zheng et al., 2024). The second category encompasses models like SEDD Uniform (Lou et al., 2024), as well as extensions introduced in follow-up studies (Campbell et al., 2024). Notably, Gat et al. (2024); Davis et al. (2024) and Campbell et al. (2024) further extend flow-matching to the discrete domain, with differing initialization strategies: the former employs masked sequences, while the latter utilizes a customized distribution for the reverse process.\nMasked Diffusion Models. Among the two primary classes of discrete diffusion models, MDMs have consistently demonstrated superior performance and scalability (Lou et al., 2024; Campbell et al., 2024). For instance, in Lou et al. (2024), the masked variant of SEDD significantly outperforms its uniform counterpart across a range of benchmarks. Similarly, Campbell et al. (2024) reports that the masked variant achieves better results in most language tasks. Furthermore, recent advancements have successfully scaled MDMs to over 1 billion parameters (Gat et al., 2024; Nie et al., 2024; Gong et al., 2024; Shi et al., 2024), underscoring their robustness and adaptability to large-scale NLP models. In this paper, we focus on MDMs, and our theoretical contributions can be applied to all MDMs, including the masked variant of discrete flow matching.\nVarious Metrics in NLP Tasks. Evaluation metrics in NLP tasks are inherently tied to the specific objectives and requirements of their respective domains. For general language modeling tasks, perplexity (Jelinek et al., 1977; Devlin et al., 2019) remains the metric of choice due to its ability to capture a model's predictive performance effectively. However, domain-specific tasks often demand more specialized evaluation criteria. For instance, in machine translation (Bahdanau, 2014; Wu et al., 2016), the BLEU score is widely regarded as a standard measure of translation quality (Papineni et al., 2002), while text generation tasks (Sutskever, 2014) frequently rely on metrics such as ROUGE to assess output fidelity (Lin, 2004). Similarly, tasks requiring reasoning (Wei et al., 2022b), such as mathematics (Bubeck et al., 2023) or code generation (Roziere et al., 2023; Ouyang et al., 2023), commonly adopt accuracy as an intuitive and straightforward measure of success."}, {"title": "3. Masked Diffusion Language Model", "content": "Without loss of generality, we study the sequence generation task where the sequence length is upper bounded by L. Let V denote the vocabulary. The MDM (Lou et al., 2024; Shi et al., 2024; Gong et al., 2024; Sahoo et al., 2024) extends the vocabulary V by introducing a special mask token [m]. The forward diffusion process progressively transforms an initial sequence $x_0 = (x_0^1, x_0^2,...,x_0^L) \\in V^L$ into a fully masked sequence $x_1 = ([m], [m],...,[m])^L$ by independently masking each token according to a pre-defined schedule. Conversely, the reverse process defines a generative model that reconstructs a sequence by iteratively modifying a fully/partially masked sequence. Below, we formally define both the forward and reverse processes."}, {"title": "3.1. Forward Process", "content": "Given a sequence $x_0$ and a masking schedule $a_t$, the distribution of the sequence $x_t$ at time $t \\in [0, 1]$ is expressed as:\n$q_{t|0}(x_t|x_0) = \\prod_{i=0}^{L-1} q_{t|0}(x_t^i|x_0^i)$,\nwhere\n$q_{t|0}(x_t^i|x_0^i) = \\begin{cases}\na_t, & x_t^i = x_0^i, \\\\\n1 - a_t, & x_t^i = [m].\n\\end{cases}$         (1)\nThe masking schedule $a_t$ is designed such that $a_0 = 1$, ensuring that the sequence remains unmasked at the start of the process. Similar to the continuous diffusion methods (Ho et al., 2020; Song et al., 2021a; Karras et al., 2022), we set $a_1 = 0$ (or a value approaching zero), ensuring the sequence is fully masked at the end of the forward process."}, {"title": "3.2. Reverse Process", "content": "The reverse process reconstructs a sequence from a masked version by reversing the forward dynamics. Given the sequence at time t and the original sequence $x_0$, the conditional distribution of the sequence at time $s < t$, is defined as:\n$q_{s|t, 0}(x_s|x_t, x_0) = \\frac{1 - a_s}{1 - a_t} \\delta_{x_t^{i}} (x_s^{i}) + \\frac{a_s}{a_t} \\delta_{[m]} (x_s^{i})$,\nwhere $\\delta_{x}(y)$ is the Kronecker delta function. Marginalizing over $x_0$ yields the true reverse process $q(x_s|x_t)$:\n$q_{s|t}(x_s|x_t) = \\prod_{i=0}^{L-1} q_{s|t}(x_s^i|x_t^i)$, where\n$q_{s|t}(x^i|x_t^i) = \\begin{cases}\n1, & x_t^i \\neq [m], x_s^i = x_t^i, \\\\\n\\frac{1-a_s}{1-a_t}, & x_t^i = [m], x_s^i = [m], \\\\\nq_{0t}(x_s^i|x_t^i), & x_t^i = [m], x_s^i \\neq [m], \\\\\n0, & \\text{otherwise}.\n\\end{cases}$         (2)\nIn MDM, a parameterized reverse model $p_{\\theta}$ is often employed to approximate the distribution $q_{0|t}(x_0|x_t)$. This model is trained by minimizing the evidence lower bound (ELBO) (Lou et al., 2024; Shi et al., 2024; Gong et al., 2024; Sahoo et al., 2024) on the negative log-likelihood of the data distribution $q_0$.\nInference. Inference within the MDM framework entails discretizing the reverse process to iteratively reconstruct sequences from a fully masked sequence. Let T denote the number of sampling steps. Starting with a fully masked sequence, the denoising process proceeds via $q_{s|t}(x_s | x_t)$, where $s = \\frac{i}{N}$ and $t = \\frac{i+1}{N}$. At each step, the model first samples $x_0$ from the conditional distribution $p_{\\theta}(x_0 | x_t)$, followed by masking specific tokens according to $q(x_s | x_t, x_0)$.\nIn practice, the reverse model is parameterized using a factorized denoising model, where the conditional distribution $p_{\\theta}(x_0 | x_t)$ is expressed as:\n$p_{\\theta}(x_0 | x_t) = \\prod_{i=1}^{L} p_{\\theta}(x_0^i | x_t^i)$.         (3)\nHere, each token is predicted independently using $p_{\\theta}(x_0^i | x_t^i)$, allowing for efficient parallel sampling. However, this factorized approach imposes a significant limitation: it disregards interdependencies between tokens within the sequence. As a result, the factorized model $p_{\\theta} (x_0 | x_t)$ cannot exactly match the true reverse distribution $q(x_0 | x_t)$ (Xu et al., 2024). In this work, we analyze the conditions under which this sampling method achieves a favorable balance between efficiency and the quality of the generated sequences."}, {"title": "4. Theoretical Analysis", "content": "In image generation, the primary goal is typically to produce visually appealing and seamless images (Heusel et al., 2017). Language generation is more task-specific. Depending on the application, the users may prefer fluent outputs, as in article writing, or precise and accurate reasonings, as in problem-solving tasks. In this section, we explore the sampling efficiency of MDMs in addressing various language tasks with respect to different evaluation metrics."}, {"title": "4.1. Notations and Problem Setting", "content": "Our investigation employs the hidden Markov model (HMM) framework to analyze natural language generation. This section establishes the formal notation and problem setting that underlies our subsequent analysis.\nHMMs (Eddy, 1996) provide a probabilistic foundation for modeling sequential data with latent structures, where observed sequences are generated by an underlying sequence of unobservable hidden states. Formally, an HMM H = (S, V, A, B) is characterized by the following components: a finite set of hidden states $S = {s_1, s_2,...,s_N}$, an observable vocabulary V, a state transition probability matrix $A \\in \\mathbb{R}^{N \\times N}$, an emission probability matrix $B \\in \\mathbb{R}^{N \\times |V|}$, and an initial state distribution $\\pi \\in \\mathbb{R}^{N}$. Given a sequence of observations $x = (x_1,x_2,...,x_L) \\in V^L$ and a sequence of hidden states $s = (s_1, s_2, ..., s_L) \\in S^L$, the generative process of an HMM is governed by the following probabilistic relations:\n$\\Pr(s_1) = \\pi_{s_1}, \\quad \\Pr(X_i | S_i) = B_{s_i, x_i}$,\n$\\Pr(s_i | S_{1:i-1}) = \\Pr(s_i | S_{i-1}) = A_{s_{i-1}, S_i}$.\nThis formulation enables HMMs to capture both the sequential dependencies among hidden states and their probabilistic relationships with observed data. In the field of NLP, HMMs serve as the fundamental statistical tools to model natural language (Eddy, 1996; Marti & Bunke, 2001). A notable special case of HMM is the n-gram language model (Brown et al., 1992), which estimates the probability of a token given its preceding n \u2212 1 tokens. Despite their simplicity, n-gram models are foundational tools in NLP tasks (Brown et al., 1992; De Novais et al., 2010). Moreover, Liu et al. (2024) suggests that scaling up n-gram models can also achieve performance comparable to modern large language models.\nFormally, we aim to address the following question: If MDMs have the capability to approximate a target HMM model, what are the computational costs, and do MDMs offer advantages over auto-regressive models? To evaluate the approximation quality of MDMs, we adopt two widely used metrics: TER and SER, which quantify different aspects of a model's performance.\nToken Error Rate. In practice, perplexity is one of the most widely used metrics for evaluating token-level errors in language models. It quantifies the uncertainty of a model in predicting the next token in a sequence and serves as a standard measure for assessing the quality of text generation. In this paper, we define the TER by perplexity. Models with lower TER are generally considered more effective at generating fluent and coherent text. Formally, given a ground-truth language model q and an evaluated model p, the TER is computed as:\n$TER(p) = 2^{\\mathbb{E}_{x \\sim q}[-\\log_2 (p(x))]}$.         (4)\nSequence Error Rate. The SER evaluates the correctness of an entire sequence rather than individual tokens. Let q represent a target language defined over a vocabulary V, and let $L_q = {x \\in V^* | q(x) > 0}$ denote the support set of distribution q. For a generative model p, the SER is defined as:\n$SER(p) = 1 - \\sum_{x \\in L_q} p(x)$.         (5)\nThis metric quantifies the probability that the model generates sequences falling outside the support set of the ground-truth distribution.\nCompared to TER, SER imposes a stricter evaluation criterion by requiring the correctness of entire sequences. This makes SER particularly well-suited for tasks that demand logical consistency or reasoning, where the correctness of the complete reasoning chain is crucial."}, {"title": "4.2. MDMs Can Generate Low-TER Sentences Efficiently", "content": "In this subsection, we rigorously examine the efficiency of sampling in MDMs, demonstrating that MDMs are capable of efficiently generating sentences with near-optimal TER. To establish the main theoretical results, we assume that the MDMs have enough expressive power and begin with the following assumption:\nAssumption 4.1 (Learning with Small Error). Let q denote the target language model with vocabulary V, and let $p_\\theta$ represent the reverse model trained to approximate the reverse process generating the target language under a masking schedule at. Assume there exists $\\epsilon_{\\text{learning}} > 0$ such that the KL divergence between $p_\\theta$ and the reverse process distribution generating the language q is bounded by $\\epsilon_{\\text{learning}}$, i.e.,\n$D_{KL}(q_{0|t}(x | X_t)||p_\\theta(x_0 | X_t)) < \\epsilon_{\\text{learning}}, \\forall t \\text{ and } X_t$.\nIt is worth noting that $p_\\theta(x_0 | X_t) = q_{0|t}(x_0 | X_t)$ represents the optimal solution to the ELBO loss during training. Assumption 4.1 implies that the MDM model is well-trained and approximates the ground-truth distribution with only a small error.\nDuring MDM inference, the time interval [0, 1] is discretized into N steps, where $t_i = \\frac{i}{N}, i \\in [N]$, and iteratively reconstruct sequences from a fully masked sequence. The following theorem shows that the sequence distribution generated by the reverse process, even with a small number of sampling steps, can achieve near-optimal TER. Consequently, MDMs exhibit high efficiency in generating n-gram language.\nTheorem 4.2 (TER Bounds for n-Gram Language Generation). For any n-gram language q and any $\\epsilon > 0$, let $p_\\theta$ denote the reverse model and L denote the sequence length. The distribution over sequences generated by $p_\\theta$ is denoted as p. For any $L > O(\\frac{n}{\\epsilon^{0.5}})$, under Assumption 4.1, there exists a masking schedule at such that, with $N = O(\\frac{n}{\\epsilon^{0.5}})$ sampling steps, the TER of the MDM is upper-bounded by:\n$\\log \\text{ TER}(p) \\le \\log \\text{ TER}(q) + \\epsilon_{\\text{learning}} + 4\\epsilon \\log |V|$.         (6)\nThe proof of this theorem is presented in Appendix B.\nTheorem 4.2 demonstrates that MDMs can efficiently generate sentences with high fidelity. It is notable that for a given data distribution q, the TER of a language model p achieves its global minimum when p = q. To ensure a gap of at most $\\epsilon$ with the optimal TER during sampling, the number of required sampling steps is bounded by $O(\\frac{n}{\\epsilon^{0.5}})$.\nThe above results suggest that to achieve near-optimal TER, MDMs require only a number of sampling steps that is independent of the sequence length L. In each sampling step, the neural network model, i.e., a Transformer, is executed once. Therefore, informally, the neural network execution count is constant for MDM. This offers substantial efficiency gains over auto-regressive models, where the model must be executed L times, once for each token in the sequence. Such efficiency enables MDMs to handle long-sequence generation tasks effectively while maintaining high-quality outputs."}, {"title": "4.3. MDMs Cannot Generate Low-SER Sentences with A Low Cost", "content": "In this subsection, we examine the SER of sampling in MDMs and highlight a fundamental limitation of MDMs in generating logically rigorous language. We begin by establishing that, with sufficient sampling steps, the MDMs have the capability to approximate a target HMM model with perfect SER.\nTheorem 4.3 (Accurate Generation of HMM with Sufficient Steps). Let q denote any HMM, and let $p_\\theta$ represent the reverse model under an arbitrary masking schedule, where L is the sequence length. Let p denote the distribution over sequences generated by $p_\\theta$. Under Assumption 4.1 with a learning error $\\epsilon_{\\text{learning}} < O(\\delta)$, and given a sufficient number of reverse steps, the sequence error rate SER(p) of the generated text satisfies\n$\\text{SER}(p) < \\delta$.\nThe complete proof of Theorem 4.3 is detailed in Appendix C.1. While this result establishes the theoretical capability of MDMs to achieve low SER, we still need to estimate the computational cost to achieve it. The following theorem provides a negative result for this problem.\nTheorem 4.4 (SER Bound for HMM Generation). There exists an HMM q over a vocabulary of size 16 that satisfies the following conditions: for any reverse model $p_\\theta$ under Assumption 4.1 with $\\epsilon_{\\text{learning}} < \\frac{1}{128}$, and any masking schedule at, let p denote the distribution over sequences generated by $p_\\theta$. There exists a constant C such that if the number of sampling steps satisfies N = CL, where L is the sequence length, the SER of the generated text is lower-bounded by:\n$\\text{SER}(p) > \\frac{1}{2}$.\nThe proof is presented in Appendix C.2.\nTheorem 4.4 shows that to generate sequences with low SER, the number of sampling steps in MDMs must scale at least linearly with the sequence length L, indicating that the number of neural network executions is comparable between MDMs and autoregressive models. However, this scaling law of MDMs typically leads to much higher computational costs compared to autoregressive models. For instance, in the case of Transformer-based architectures, each execution step in MDMs involves a quadratic computational complexity in terms of L, as opposed to the linear complexity of auto-regressive Transformer models in each generation step (through reusing the stored KV caches). Consequently, in accuracy-critical applications, MDMs offer no computational efficiency advantage over auto-regressive models.\nFurthermore, some prior works (Sahoo et al., 2024; Ou et al., 2024) have proposed efficient sampling strategies that reuse cached outputs without requiring additional forward passes through the network when no token is modified from [m] at a given step. Nevertheless, our theoretical results remain applicable to these sampling strategies, as discussed in Appendix D.\nDo TER and SER Conflict? The above results reveal that MDMs can efficiently generate low-TER sentences but may incur higher costs when evaluating the generation under SER. One might think these results are contradictory. Note that several previous works have already shown that TER (a.k.a perplexity) may not reflect a model's true performance in solving several long-sequence understanding tasks (Huang et al., 2022; Hu et al., 2024; Luden et al., 2024). Thus, it is natural to arrive at different conclusions depending on the metric used.\nMoreover, many practical scenarios have shown that the choice of evaluation metric significantly influences the conclusion of other problems. For instance, while the community has previously focused on the emergence phenomenon, recent works by Wei et al. (2022a) and Schaeffer et al. (2024) demonstrate that this phenomenon may stem from the use of non-smooth evaluation metrics. Our work further reveals that conclusions regarding the efficiency of MDMs depend heavily on the evaluation metric employed. Specifically, MDMs excel in applications where fluency is prioritized. In contrast, for reasoning-intensive tasks that demand highly accurate trajectories, MDMs may fail to offer a significant efficiency advantage over auto-regressive models."}, {"title": "5. Experiments", "content": "We conducted a series of experiments to empirically validate the theoretical findings, focusing on evaluating the sampling quality and computational efficiency of MDMs under diverse metrics. The results reveal that while MDMS effectively generate low-TER sequences, but achieving low-SER demands substantial computational resources. We will first introduce our experimental settings and then present the experimental results."}, {"title": "5.1. Experimental Setup", "content": "Tasks and Datasets. First, we evaluated MDMs on a variety of formal languages, including n-gram languages (n\u2208 {2,3,4}) and HMMs. For each formal language, parameters such as transition matrices, observation matrices, and initial distributions were generated through random sampling. Detailed descriptions of the parameter generation process, along with illustrative examples of the resulting sequences, are provided in Appendix E.1. Using these formal languages, we constructed datasets comprising 1,000,000 samples, of which 990,000 were allocated for training and 10,000 for validation. When using the formal language models to generate the dataset, we set the max length of 512.\nModel Training. We adopted transformer-based architectures as the backbone models due to their scalability and expressiveness in sequence modeling tasks. Comprehensive architectural details, including the number of layers, hidden dimensions, and positional encoding schemes, are provided in Table 2 in Appendix E.2. The training process followed the framework proposed by Sahoo et al. (2024), with additional training configurations detailed in Table 3. Models were trained for 20 epochs, and their convergence was monitored using the validation set. Perplexity was used as the primary convergence metric, and the trained models achieved optimal perplexity values consistent with the ground-truth language models that generated the datasets.\nEvaluation Metrics. To assess the quality of generated sequences, we used TER and SER as the primary evaluation metrics, in alignment with our theoretical framework. Computational efficiency was evaluated based on the number of sampling steps. Following prior work (Lou et al., 2024; Xu et al., 2024), generative perplexity was employed as the TER metric to evaluate the sample qualities under different sampling steps. We compute the generative perplexity using the ground-truth model to evaluate the likelihood of sequences generated by MDMs, which were subsequently converted into perplexity scores. SER was computed directly using its definition in Equation (5), leveraging ground-truth models for evaluation. For sequence generation, we utilized the ddpm_cache sampler proposed in prior work (Sahoo et al., 2024), ensuring efficient sampling. Computational efficiency was measured by the number of sampling steps, and we further discuss the influence of ddpm_cache under different sampling steps in Appendix D. Furthermore, we also test the true speedup of MDMs under different sampling steps compared"}, {"title": "5.2. Experiment Results", "content": "The experiment results are presented in Figure 1. The upper subfigure shows the generative perplexity across different formal languages with the number of sampling steps varying. The x-axis represents the number of sampling steps, ranging from 8 to 2048, while the y-axis measures the generative perplexity, where lower values indicate higher text fluency and token-level accuracy. The performance of auto-regressive models is marked as the final point on the x-axis for comparison. As shown in the figure, MDMs achieve near-optimal generative perplexity with relatively few sampling steps. To achieve a perplexity similar to the auto-regressive model, MDMs only require about 64 steps and demonstrate 1.57 times speedup compared to auto-regressive models. This demonstrates that MDMs are highly efficient at generating fluent sequences even with a small number of sampling steps. As the number of sampling steps increases, the performance of MDMs approaches that of auto-regressive models, converging to a similar level of generative perplexity.\nThe lower subfigure evaluates the relationship between the number of sampling steps and the SER, which measures the correctness of an entire sequence. The x-axis again represents the number of sampling steps, with the performance of auto-regressive models included as a baseline, and the y-axis measures the SER, where lower values indicate higher sequence-level accuracy. Compared to the upper subfigure, this subfigure reveals a slower improvement in SER as the number of sampling steps increases. For these formal languages, achieving low SER requires significantly more sampling steps. Moreover, even when the number of sampling steps reaches 2048, there remains a gap in SER between MDMs and auto-regressive models. These results demonstrate that auto-regressive models maintain a clear advantage in SER, as their token-by-token generation achieves zero SER across these tasks.\nFigure 1 highlights the trade-off between efficiency and accuracy for MDMs empirically. While MDMs excel in generating fluent outputs with low TER, they require substantially more sampling steps to achieve low SER, particularly for reasoning-intensive tasks that demand sequence-level correctness. These experimental results further reinforce our theoretical findings."}, {"title": "6. Preliminary Experiments on Large Models", "content": "We further conducted an extensive set of experiments on language tasks using open-source MDMs. First, we evaluated the quality of text generation by measuring the generative perplexity of MDMs using MDLM-OWT (Sahoo et al., 2024), a diffusion language model trained on Open-WebText (Gokaslan & Cohen, 2019). For a fair comparison, we evaluated GPT2-medium (Radford et al., 2019), which is similar in size. Second, we explored the mathematical reasoning ability of MDMs on the GSM8K dataset (Cobbe et al., 2021). Given that small models typically exhibit poor reasoning performance, we used a fine-tuned diffusion language model with 1.1B non-embedding parameters proposed by Nie et al. (2024), and compared it against model with a similar number of parameters. While the generative perplexity represents the metric of TER, mathematical reasoning is more concerned with the correctness of the entire generated sequence, thus, the GSM8K accuracy is partially consistent with the negative sequence error rate SER.\nText Generation. For text generation, we use MDLM-OWT, which has a context length of 1024 and size similar to GPT2-medium and is trained on OWT dataset (Gokaslan & Cohen, 2019). Since our goal is to compare the acceleration of MDMs relative to auto-regressive models and examine the effect of the number of steps on text generation quality, the absolute size and capability of the model are less important. Following the approach in the original work, we used the ddpm_cache sampler and the GPT2 tokenizer. For the number of sampling steps ranging from 4 to 2048, we generated 2000 samples of length 1024 and evaluated the generative perplexity using GPT2-large. To compare MDMs with auto-regressive models, we took GPT2-medium as baseline and computed its generative perplexity in the same manner.\nThe experiment result is shown in the left subfigure of Figure 2, which illustrates the text generation quality of MDLM-OWT across different sampling steps, with GPT2-medium as the baseline. The the x-axis represents the number of sampling steps, and the y-axis represents the average generative perplexity of 2000 generated texts, where lower generative perplexity indicates higher fluency and, consequently, a lower TER. The numbers above indicate the speedup of MDLM-OWT under different sampling steps compared to GPT2-medium. As is shown in the figure, MDLM-OWT matches the generative perplexity of GPT2-medium with only 32 steps, where there is a 2.28x speedup, and the perplexity continues to decline and converge as the number of sampling steps increases. This demonstrates that MDMs can generate texts efficiently while ensuring a high fluency, which illustrates the potential of MDMs for basic language generation tasks at a larger scale.\nMathematical Reasoning. For mathematical reasoning, we used the MDM provided by Nie et al. (2024), which was fine-tuned on GSM8K using a model trained on SlimPajama (Soboleva et al."}]}