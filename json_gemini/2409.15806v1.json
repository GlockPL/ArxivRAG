{"title": "CLSP: High-Fidelity Contrastive Language-State Pre-training for Agent State Representation", "authors": ["Fuxian Huang", "Qi Zhang", "Shaopeng Zhai", "Jie Wang", "Tianyi Zhang", "Haoran Zhang", "Ming Zhou", "Yu Liu", "Yu Qiao"], "abstract": "With the rapid development of artificial intelligence, multimodal learning has become an important research area. For intelligent agents, the state is a crucial modality to convey precise information alongside common modalities like images, videos, and language. This becomes especially clear with the broad adoption of reinforcement learning and multimodal large language models. Nevertheless, the representation of state modality still lags in development. To this end, we propose a High-Fidelity Contrastive Language-State Pre-training (CLSP) method, which can accurately encode state information into general representations for both reinforcement learning and multimodal large language models. Specifically, we first design a pre-training task based on the classification to train an encoder with coarse-grained information. Next, we construct data pairs of states and language descriptions, utilizing the pre-trained encoder to initialize the CLSP encoder. Then, we deploy contrastive learning to train the CLSP encoder to effectively represent precise state information. Additionally, we enhance the representation of numerical information using the Random Fourier Features (RFF) method for high-fidelity mapping. Extensive experiments demonstrate the superior precision and generalization capabilities of our representation, achieving outstanding results in text-state retrieval, reinforcement learning navigation tasks, and multimodal large language model understanding.", "sections": [{"title": "Introduction", "content": "Recently, multimodal data understanding, as the key of building AIGC and embodied AI, has attracted increasing attention (Zhu et al. 2023; Kim et al. 2024; Chen et al. 2024; Zhan et al. 2024). As a crucial technology for understanding multimodal data, modality representation encodes data into a latent vector space to express information. Current research on multimodal data mainly focuses on language, image, video, and audio data (Zhu et al. 2023, 2024; Xie et al. 2024). However, in complex embodied AI environments, an agent's state often contains rich information that is vital for decision making, making it crucial to accurately represent and understand the state.\nGenerally, the state of the embodied agent can be categorized as either vision-based or scalar-based. For vision-based state, the Contrastive Language-Image Pre-training (CLIP) (Radford et al. 2021), a milestone representation method, can successfully estimate the similarity between a given image state and a text description. CLIP encodes image for both embodied AI and AIGC (Fan et al. 2022; Rocamonde et al. 2023; Dang, Edelkamp, and Ribault 2023; Liu et al. 2024) and has been widely studied and applied in various topics, such as MedCLIP(Wang et al. 2022), GeoCLIP(Cepeda, Nayak, and Shah 2023), CLIP-Art(Conde and Turgutlu 2021), MINECLIP(Fan et al. 2022), etc. However, vision-based state only contains visually observable information, while scalar-based state information tends to encompass more complex and diverse numerical data, which is usually more difficult to perceive. Moreover, there has been insufficient research on the representation of scalar-based states. Additionally, we find that directly aligning states with text descriptions in a CLIP framework results in a loss of precision about the complex state information, especially when the prompt text is long (e.g., over 500 tokens) and contains complex information. This makes it difficult to retain accurate numerical information and its corresponding meanings. To successfully apply the representation to both multimodal large language models (LLMs) and reinforcement learning (RL), it needs to have both semantic understanding capabilities and informational precision. Therefore, two key issues need to be addressed: 1) extracting useful representations from complex scalar values is difficult; 2) the precision of learned representations corresponding to raw scalars tends to be low, hindering usability in subsequent tasks.\nTo tackle the above issues, in this work, we propose a novel method, High-Fidelity Contrastive Language-State Pre-training (CLSP) for Agent State Representation, which can accurately encode complex state information into general representations for both RL and multimodal LLMs. Specifically, CLSP represents scalars of various categories efficiently and accurately in two steps. In the first step, a supervised multiclass classification method is adopted to pre-train the encoder, which performs a coarse-grained division and classification prediction of scalar information across different categories. The learned state encoder obtains coarse-grained information from the state and can provide a good initialization for further state-text alignment. In the second step, similar to CLIP, we leverage contrastive learning(Khosla et al. 2020) to learn the alignment between CLSP encoder and text encoder. The CLSP encoder is initialized"}, {"title": "Methodology", "content": "In this section, we present CLSP to accurately encode state information into general representations for both RL and multimodal LLM. As shown in Figure 1, the whole framework has three steps. Firstly, the state encoder is pre-trained with a classification task. It is worth noting that the numbers in state is encoded with RFF to improve the numerical precision for further exploitation. Then the CLSP encoder and text encoder are trained based on contrastive learning, where the CLSP encoder is initialized with the parameters trained in step 1. Thirdly, the learned CLSP encoder can be applied in multimodal LLM task and RL navigation task."}, {"title": "Scalar Encoded with Random Fourier Features", "content": "Generally, the scalars in state are encoded by standard MLPs in RL. However, Tancik et al. (2020) shows that this encoding mechanism suffers from spectral bias, i.e., having difficulty learning high-frequency details in scalar values. It may result in a loss of information, especially for important scalars, such as position, HP, speed, etc. Therefore, we are seeking to employ an encoding method to project original scalars to a high-dimensional space, making it easier to capture complex information. There are some common methods, such as Multi-Scale Normalizaer (Springenberg et al. 2024), NeRF positional encoding(Mildenhall et al. 2021), Random Fourier Feature (RFF)(Tancik et al. 2020), etc. Multi-Scale Normalizer simply expands a scalar to a series of different fixed scales, while NeRF positional encoding encodes sequential data using sine and cosine functions. In contrast, Random Fourier Features (RFF) map scalars to a higher-dimensional space with randomly sampled Gaussian vectors, representing features across all selected possible frequencies. Therefore, we deploy RFF to extract useful information from the state, focusing on high-frequency components, specifically the details of scalars. For a scalar value v, it is encoded in the way shown in Equation (1), where b\u2208 Rd is sampled from N (0, \u03c3\u00b2), and o is set to be 1 for simplicity and d is chosen from (4, 6, 8) according to the complexity of the scalar. For example, we choose d = 8 for position because it varies continually in a large range (0,4000m), and d = 4 for HP because it can only be 10 discrete values. In addition, we add two-layer MLPs after RFF to further improve the expressiveness of state.\n \u03b3(v) = [cos(2\u03c0bv), sin(2\u03c0bv)]T  (1)\nThese encoded scalars provide a more complete presentation with more details for subsequent classification-based pre-training and contrastive-based representation learning."}, {"title": "Classification-based Pre-training", "content": "To capture the underlying patterns and semantic knowledge in states, we design a classification-based pre-training task. This task aims to predict some key properties in state using an approach similar to probing classifier(Belinkov 2022). We select dozens of items from the state to predict their classes, including alive state, position, speed, HP, etc. These items naturally fall into two types: 1) discrete values, for example, alive state has only three types: normal, down and dead; 2) continuous values, for example, position represents the coordinates of any valid position. In practice, we discretize these continuous values to maintain a relatively uniform distribution for the classification task. In this way, we manually construct multiple classification labels I for state s concerning different items. More details about the label construction are provided in the supplementary materials. We employ a cross-entropy loss function to train this classifier, Lpre = - \u2211ceclasses lc log (pc), where pc is the"}, {"title": "Contrastive-based Representation learning", "content": "In order to further improve the representation capability of state encoder, we adopt a contrastive-based learning method to align the state and text descriptions. Specifically, we jointly train a state encoder and a text encoder to learn the correlation between scalar-based states and text descriptions, where the state encoder is initialized from the pre-trained model introduced above and text encoder is initialized from DistilBERT(Sanh et al. 2019). During the training, we sample a batch of state-text pairs {si, ti}1, where Si represents the state and ti represents the corresponding text description. The contrastive loss consists of two log classification terms, i.e., state \u2192 text loss LS2T and text\u2192 state loss LT2s, which are defined as follows:\n LS2T = 1/B \u2211i=1 log exp($(s)v(ti)/T)/\u03a3j=1B exp ($(s)(tj)/T)\n(2)\n LT2S = 1/B \u2211i=1 log exp (v(t)$(si)/T)/\u03a3j=1B exp (v(t)$(sj)/T)\n(3)\nwhere (si) is the encoded state by state encoder and (ti) is the encoded text by text encoder, 7 is a temperature hyperparameter. Additionally, we provide an algorithm table for CLSP in the supplementary materials."}, {"title": "Multimodal alignment with LLM", "content": "To verify the representation capability of our state encoder in multimodal models, we design a large multimodal model to understand state information based on our state encoder and a pre-trained LLM. As shown in Figure 2, we use the Vicuna 1.5 13B (Liu et al. 2024) model as the base large language model and design an expansion connector to convert the embeddings output by the state encoder into multiple tokens that can be understood by the large language model. Specifically, let the state embedding be E \u2208 Rd, and the output of the expansion connector as {T{}=1 \u2208 Rh. The expansion connector can be expressed as:\n H\u2081 = GELU(Linear\u00bf(E)) (i = 1,2,...,n),  (4)\n T\u2081 = LN(Linear(H\u2081)) (i = 1, 2, ..., \u03b7), (5)\nwhere Hi \u2208 Rh' is the hidden representation expanded from E for each output token, and LN represents layer normalization. In our experimental setup, d = 4096, h' = 256, n = 48, and h matches the input dimension of the large language model. The tokens {T}=1 will be input into the large language model along with other language tokens, resulting in a generative output. Through the generated language output, the multimodal model can convert the information from the input state into language expressions.\nDuring the training process, we proceed in two steps. In the first step, we freeze all parameters of the LLM and the state encoder, training only the parameters of the expansion connector. In the second step, we freeze all parameters of the LLM and train the parameters of both the expansion connector and the state encoder to further align the token representations."}, {"title": "RL Navigation with CLSP-encoded Goal", "content": "To evaluate the effectiveness of CLSP, we choose a typical RL task, namely the navigation task. Specifically, we train an agent with policy to maximize its cumulative reward, Eat~\u03c0(:184,9),($t,(at)~7 [=0vtr (st, at, 1,9)], where r (st, at, g) is the reward indicating whether the agent completes the goal g. The goal g is the encoded state from the CLSP encoder which contains the sampled target. We employ Proximal Policy Optimization (PPO)(Schulman et al. 2017) as the RL algorithm."}, {"title": "Experiment", "content": "In this section, we evaluate the effectiveness of CLSP across multiple tasks, including retrieval task, multimodal LLM task, and RL navigation task."}, {"title": "Dataset & Environment", "content": "Our experiments are mainly based on a multiplayer first-person shooting game environment. The player's goal is to ensure the survival of their team and eliminate encountered enemies. Data is collected by running the trained policy model on 10 CPU servers over many rounds, resulting in approximately 17 hours of data. To maintain a balanced data distribution, we sample approximately 18 minutes of data, resulting in about 0.55 million state - text pairs. After that, we generate the text descriptions for the sampled states. Then we randomly sample 5% of them as the test dataset and the others as the training dataset. More details about the dataset are illustrated in the supplementary materials. Additionally, we can customize the game environment to verify the effectiveness of our method in the RL task."}, {"title": "Overall Results", "content": "In our experiments, we conduct state-text alignment training task and evaluate different methods based on R@K and Top-1 MAE. Table 1 presents some quantitative results and we can find that: 1) Classification-based pre-training is critical for the state-text contrastive learning task. Specifically, compared to CLIP-Baseline, CLSP-Baseline improves the recall scores from nearly zero to 71.6% on average and reduces the Top-1 MAE by 77.3% on average. These results demonstrate the effectiveness of coarse-grained information obtained by state encoder through classification-based pre-training. 2) All the variants of CLSP consistently outperform CLSP-Baseline by a large margin in recall score and Top-1 MAE, which shows the importance of encoding the scalar values to maintain higher scalar precision. For example, CLSP-MSN achieves a higher recall score by around 15% on average than CLSP-Baseline, and decreases the Top-1 MAE by around 37.6% on average. 3) CLSP-RFFM achieves the best results in 10/13 items among CLSP variants, demonstrating its effectiveness in adding learnable MLPs after a traditional RFF encoder. Additionally, Figure 3 illustrates the training curves for three Top-1 MAE: My Position, Enemy Distance, and Teammate HP. We can see that CLSP-Baseline and other variants of CLSP can consistently reduce the Top-1 MAE significantly. Especially, CLSP-RFFM can achieve lower loss at a faster rate, which proves the effectiveness of our proposed CLSP training framework."}, {"title": "Results of Multimodal LLM", "content": "To further validate the representation performance of our CLSP encoder in multimodal LLMs, we perform experiments on several encoder variants. The overall results of these encoders are presented in Table 2. The results show that our CLSP encoder significantly outperforms other types of encoders while preserving the precision of information. Specifically, the method labeled CLSP-MLLM indicates the integration of our CLSP encoder with the multimodal LLM, as depicted in Figure 2. The variant \"w/o CLSP\" refers to using a state encoder learned directly from a randomly initialized model without the CLSP encoder during multimodal LLM training. \"w/o CL-step\" refers to the absence of training step 2, which involves contrastive representation learning, utilizing only a classification-based pre-trained state encoder as the multimodal encoder. \"w/o RFF\" means CLSP without Random Fourier Features. As shown in the results table, the CLSP encoder demonstrates high-fidelity capabilities. The Mean Absolute Error (MAE) for health information is reduced by approximately 62.5% compared to the best baseline, and the MAE for position information is reduced by about 62.0%. Additionally, the Median Absolute Error (MedAE) for health points, direction, and speed is zero, indicating that all methods generate a large amount of accurate information. Meanwhile, the significantly reduced average error of CLSP further demonstrates the model's excellent generalization performance, enabling it to handle outliers and out-of-distribution (OOD) data effectively."}, {"title": "Results of RL Navigation", "content": "In the RL navigation task, we evaluate the effectiveness of the goal state embedding by comparing the GCR with baselines. Higher GCR indicates better performance. In this task, CLSP uses the embedding of the goal state produced by the trained state encoder as the goal. CLSP w/o pre-training is the same as CLSP except that the parameters are initialized randomly. Baseline denotes using the raw goal position, i.e., (x, y, z), as the goal. As shown in Figure 4, CLSP achieves a much faster learning speed and converges to a higher GCR of around 60%, which is significantly better than CLSP w/o pre-training and Baseline. These results demonstrate that the training pipeline and proposed goal representation are both beneficial for RL navigation task. Table 3 shows the data costs for CLSP and Baseline to learn the navigation task. Since it takes around 0.55 million (M) states in advance"}, {"title": "Ablation Study", "content": "We conduct ablation studies on different hyper-parameters, i.e. the data size, the batch size and the target of the classifier. These results are collected after training for one epoch.\nEffects of different data size. In Table 4, we investigate the impact of different data sizes on the contrastive learning task. The results demonstrate that more data consistently improves the recall score and reduces the Top-1 MAE. Specifically, when increasing the data size from 10% to 25% of the total, the R@10 significantly increases from 3e-1 to 0.89.\nEffects of different batch sizes. Table 5 illustrates the impacts of different batch sizes on the contrastive learning task. It can be observed that as the batch size increases, the recall scores consistently improve. Additionally, Top-1 MAE significantly decreases as the batch size increases.\nEffects of different classification targets. In the proposed method, classification-based pre-training plays an important role in improving the understanding of state by classifying the items in state. We study the impacts of classifying different types of items in the state. As shown in Table 6, classifying partial targets items can yield better results on Top-1 MAE. However, classifying all items significantly outperforms other methods on most metrics, especially for R@1, R@5 and R@10."}, {"title": "Visualization of Distinguishable Representations", "content": "Figure 5 illustrates the two-dimensional t-SNE embedding of the state representations produced by CLSP. We select ten states and provide more information about them, where similar states are close in the state space. As shown in Figure 5, the properties of adjacent points exhibit similar properties with respect to HP, direction or position. This phenomenon demonstrates that the state embeddings produced by CLSP are smooth."}, {"title": "Conclusion", "content": "In this paper, we present CLSP framework to address the underdeveloped area of state modality representation in multimodal learning and RL. By designing a classification-based pre-training process and utilizing contrastive learning, our approach effectively encodes state information into general high-fidelity representations, enhancing the alignment between states and language descriptions. The use of RFF enhances the precision in representing numerical data even further. Extensive experiments demonstrate outstanding results in text-state retrieval, RL navigation, and multimodal understanding tasks, highlighting CLSP's potential to intelligent agents and multimodal LLMs."}]}