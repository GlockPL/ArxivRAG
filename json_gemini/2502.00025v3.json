{"title": "Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients", "authors": ["Abdulaziz Ahmed", "Mohammad Saleem", "Mohammed Alzeen", "Badari Birur", "Rachel E Fargason", "Bradley G Burk", "Hannah Rose Harkins", "Ahmed Alhassan", "Mohammed Ali Al-Garadi"], "abstract": "Importance: Emergency department (ED) returns for mental health conditions represent a significant healthcare burden, with about 24 - 27% of mental health patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical implementation.\nObjective: To evaluate whether integrating large language models (LLMs) with traditional machine learning approaches improves both the predictive accuracy and clinical interpretability of ED mental health returns risk models.\nMethods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an Academic Medical Center in the deep South of the United States between January 2018 and December 2022.\nMain Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability through a novel LLM enhanced explainability framework integrating SHAP (SHapley Additive exPlanations) values with contextual clinical knowledge.\nResults: For chief complaint classification, Llama 3 (8-billion) with 10-shot learning outperformed traditional models, achieving 0.882 accuracy and 0.86 F1-score. In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96\u20130.89), while Exercise and Home Environment showed lower performance (F1: 0.70\u20130.67). These results demonstrate the effectiveness of LLM-enhanced feature extraction in clinical prediction. The proposed machine learning interpretability framework, leveraging LLM, achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost's AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.\nConclusions and Relevance: Integrating LLMs with traditional machine learning models yielded modest but consistent improvements in ED return prediction accuracy while substantially enhancing model interpretability through automated, clinically relevant explanations. This approach offers a framework for translating complex predictive analytics into actionable clinical insights.", "sections": [{"title": "Introduction", "content": "Emergency department (ED) utilization for mental health conditions has reached critical levels, with significant implications for healthcare systems and patient outcomes. Currently, two-thirds of hospital ED visits annually by privately insured individuals in the U.S., 18 out of 27 million, are considered avoidable, with patients who could be treated safely and effectively in lower-cost primary care settings.[1] In emergency psychiatric services, nearly one in four patients (25.2%) return to the ED within 30 days after discharge, with 28% of these returns occurring at different facilities.[2] The burden is particularly evident in psychiatric emergency rooms, where providers face challenges delivering care due to ED boarding, with some regions reporting hundreds of patients simultaneously awaiting psychiatric beds.[3] Recent data from standardized screening programs indicates that this crisis continues to worsen, with up to 17% of ED patients demonstrating at least one unmet social need requiring immediate intervention.[4]\nSocial determinants of health (SDoH) have emerged as crucial factors shaping these utilization patterns. Recent standardized screening programs across EDs have revealed that food insecurity shows significant associations with healthcare utilization. Adults who experienced food insecurity in 2020 were significantly associated with 3.1 percentage points higher rates of social isolation and 9.7 percentage points higher rates of loneliness in 2021 compared to their food-secure counterparts.[5] Community-based interventions have demonstrated potential in tackling these issues, as research indicates that a rise in mental health visits at community health centers is linked to a 5% reduction in ED visits for suicidal thoughts and self-harm.[6] However, their effectiveness varies considerably depending on the type of condition. These services prove beneficial for adjustment disorders, anxiety, and mood disorders yet have a limited effect on visits associated with psychotic disorders and substance use.[6]\nThese social challenges directly impact care delivery in psychiatric emergency rooms (PERs), where providers already face significant obstacles, including time constraints, limited privacy, and insufficient resources for thorough psychiatric assessments.[2] The diverse nature of presentations compounds the complexity. While substance use disorders and severe mental illnesses like schizophrenia are common, PERs also serve patients with a broad spectrum of mental health issues, social problems, and crises.[7] Insurance status further complicates these challenges, with Medicaid beneficiaries and uninsured individuals facing unique barriers to accessing appropriate outpatient mental health care, hence rendering EDs and PERs the only service providers they can seek.[8]"}, {"title": "Methods", "content": "Traditional machine learning models have shown utility in analyzing structured data to predict ED return risk. However, their ability to process unstructured clinical narratives and explain predictions in ways that align with clinical decision-making remains limited.[6, 9, 10] This lack of explainability is a significant barrier to adoption in healthcare, where clinicians require transparent, actionable insights to inform interventions and resource allocation.[11, 12]\nRecent advances in LLMs offer promising avenues to address these limitations. LLMs have demonstrated the ability to process unstructured data and synthesize contextual information. This approach can enhance the interpretability of machine learning models by generating clinically coherent narratives that align with provider reasoning while maintaining high predictive fidelity.[13-15] Despite these innovations, the utility of LLM-enhanced frameworks to improve and explain clinical applications, particularly in clarifying machine learning outcomes and their related features, remains underexplored. This study evaluates a novel integration of LLMs with traditional machine learning models to predict 30-day ED returns among mental health patients. By leveraging structured and unstructured data\u2014including SDoH and clinical narratives\u2014the proposed framework seeks to create actionable insights for clinical decision-making while demonstrating the potential of LLMs to address key limitations in current predictive modeling for ED return risk."}, {"title": "Study Design", "content": "This retrospective cohort study drew on visit-level data from an Academic Medical Center in the deep south of the United States. Institutional Review Board (IRB) approval ensured that all analyses complied with ethical standards. Figure 1 shows the proposed framework of the study. The framework integrates clinical notes and patient data, using an LLM to extract variables for machine learning model development. The trained models, combined with an LLM system, SHAP (SHapley Additive exPlanations) visualizations, prediction scores, and patient-specific insights for an explainable machine learning model."}, {"title": "Data Sources and Cohort Characteristics", "content": "This analysis was informed by five key data sources: (1) Demographics, which included patient-level characteristics such as gender, race, ethnicity, and language preferences; (2) Visit Details, encompassing insurance status, frequency of encounters, and return visit patterns; (3) Vital Signs, recording clinical measurements such as blood pressure, heart rate, respiratory rate, oxygen saturation, and temperature; (4) Tracking Board Data, which captured patient flow metrics, including arrival, admission, and discharge times, as well as acuity levels; and (5) Diagnoses, comprising standardized coded conditions, including mental health disorders (e.g., depression, anxiety, bipolar disorder, schizophrenia, suicidal ideation, PTSD); and (6) Chief complaints, which categorized into common categories (e.g., infection, injury, pain), based on the categories suggested by Kuykendal et al.[16]"}, {"title": "Eligibility Criteria", "content": "To enhance patient-level data with contextual factors, we incorporated comprehensive health behavior and social determinants data at the individual level. These included tobacco use patterns (categorized as current, former, no use, occasional, and prescribed use), alcohol consumption (categorized as current, no use, past use, occasional use, and recovering), substance use status (categorized as current, former, no use, recreational, and prescribed use), and exercise habits (categorized from no exercise to vigorous exercise). Housing and social support were documented through home environment assessment, including categories for independent living, family support, homelessness, living with friends, assisted living, and unstable housing. Additional health-related factors included BMI categories (underweight, normal weight, overweight, and obese) and nutritional status (ranging from poor to good nutrition, including special dietary considerations).\nTo focus on adult care, patients under 18 years of age were excluded. Mental health-related encounters were identified based on ICD-codes (ICD-10-CM (the International Classification of Diseases, Tenth Revision, Clinical Modification) decided at the time of patient discharge from ED. We considered all mental and behavioral disorder patients. Any patients with an ICD that starts with F were considered for this study.[17] Non-mental/behavioral health patients were excluded from the study."}, {"title": "Data Harmonization and Feature Engineering", "content": "Following initial data extraction, each dataset was merged via unique patient and visit identifiers to create a unified analytic file. Multiple vital sign measurements for a single visit were averaged to yield representative values. Age was categorized into clinically relevant bands (18\u201330, 31\u201345, 46-60, >60),[18], and body mass index (BMI) was stratified into underweight, normal weight, overweight, and obese categories to facilitate subgroup analyses.[19] Blood pressure and heart rate were binned into standard clinical categories (e.g., normal, elevated, hypertensive) to support risk stratification.[18] Temperature readings were classified as normal, fever, or hypothermic based on clinical thresholds.[18]\nFor SDoH factors collected at the individual level-such as tobacco use, alcohol use, substance use, and home environment (e.g., living with family/roommates, alone, or experiencing homelessness)\u2014categorical variables were standardized and collapsed into interpretable categories using LLM. Sexual orientation data, often sparse or incomplete, were harmonized by combining low-frequency categories into an \u201cOther\u201d category. Similarly, \u201cUnknown\u201d values were recoded as missing (NaN) to facilitate uniform imputation."}, {"title": "Handling Missing Data and Imputation", "content": "To maintain data integrity and analytic representativeness, variables designated as \u201cUnknown\u201d or \"Missing Response\" were recoded as NaN. K-Nearest Neighbors (KNN) imputation was used for continuous features. For categorical features, we dropped the features with a percentage of missing more than 20%, and then for the remaining features, we fill the missing values with \u201cUknown\" value. All continuous variables were assessed for distributional assumptions and standardized (z-score normalization) as needed to align variable scales. Temporal indicators, such as hours of day or weekend vs. weekday visits, were binarized or aggregated into clinically meaningful intervals (e.g., \u201cNight\u201d or \u201cWeekend\") to capture utilization patterns. Return visits within 30 days of the index encounter, a key outcome measure, were flagged to assess longer-term care engagement and recurrent utilization patterns among patients with mental health conditions. By integrating patient-level clinical, demographic, and SDoH data and refined text-based categories of chief complaints, we developed a rich, harmonized dataset suitable for in-depth analyses of ED utilization and return risk in mental health populations."}, {"title": "LLM Extracted Features", "content": "To enhance the predictive modeling framework, LLMs were employed to refine the classification of chief complaints into five clinically relevant categories: Infection, Injury, Pain, Psychiatric, and Unclear, which are suggested by Kuykendal et al.[16] This process began by comparing traditional machine learning approaches with advanced transformer-based architectures (e.g., BlueBERT[20]) and LLMs with different parameter scales (e.g., Llama 3 at 8-billion[21]). The LLMs were integrated into a few-shot learning context, with 5-, 10-, and 20-shot examples provided, allowing them to adapt more rapidly to the nuances of clinical narratives than conventional supervised methods.[22]\nPerformance metrics on validation sets guided the selection of the optimal model configuration, with the chosen LLM consistently outperforming alternatives in terms of classification accuracy and relevance to clinical practice. Incorporating these LLM-derived categorizations into the dataset enriched the downstream predictive modeling. By providing a more nuanced and clinically coherent representation of presenting complaints, the refined input data facilitated improved interpretability, enabling more precise identification of risk factors associated with subsequent ED utilization and strengthening the overall predictive performance of the modeling pipeline."}, {"title": "Predictive Modeling", "content": "A comprehensive machine learning framework was developed to predict the risk of ED return among patients with mental health conditions. The framework integrated data from electronic health records (EHR) and SDoH into a unified analytical dataset. Additionally, two comparative scenarios were evaluated: models trained on all available features and models trained on all features, excluding SDoH variables, to isolate the contribution of socioeconomic indicators to predictive accuracy."}, {"title": "Enhancing Explainability Framework with an LLM", "content": "Following feature selection, a suite of gradient-based ensemble classifiers\u2014Gradient Boosting [23], XGBoost (eXtreme Gradient Boosting) [24], and AdaBoost (Adaptive Boosting) [25]\u2014were trained and evaluated. We also used logostic regression and Neural Network. Hyperparameter optimization was conducted using grid search [26] to maximize model performance. Predictive performance was assessed using standard classification metrics, including accuracy, sensitivity, specificity, F1 score, and the area under the receiver operating characteristic curve (AUC).[27] Sensitivity and specificity provided insights into the models' ability to correctly identify patients at risk of ED returns and those who were not, while the F1 score balanced precision and recall. AUC served as a global measure of the model's discriminative ability. To ensure robustness and comparability, all models were trained and tested using consistent training, and testing splits to ensure comparability. Models with and without LLM-extracted features were compared to assess the added predictive value of socioeconomic factors. We analyzed ED visits over a five-year period (2018\u20132022), focusing on mental health-related cases. The dataset was divided into 80% training and 20% testing sets, with hyperparameters optimized using GridSearchCV with 3-fold cross-validation. Random oversampling was applied to the training set to overcome the output class imbalance.[28, 29]"}, {"title": "Deriving SHAP-Based Feature Attributions", "content": "To enhance the interpretability of machine learning predictions in clinical settings, we employ an explainability framework that integrates LLMs with patients-specific information (Figure 1). This approach combines feature-level attributions with contextual background information, resulting in rich, clinically meaningful narratives that align with the reasoning patterns used by healthcare professionals. Figure 1 illustrates the workflow of our study. Data from the study cohort-including SDoH variables, structured features, and LLM-derived attributes\u2014are utilized in the development and testing of machine learning models. This process culminates in an explainability step, which integrates SHAP values [30] and patient-specific information to produce interpretable outputs such as cohort statistics, SHAP visualizations, and patient-centered narratives. By incorporating SHAP values, we can assess each feature's contribution to a patient's predicted risk, providing granular, quantitative insights into feature importance. However, the numerical nature of SHAP values often limits clinical interpretability. To bridge this gap, we leverage a domain-specific knowledge repository that includes population-level cohort statistics, risk factor ranges derived from the ML model, and individual patient characteristics. The LLM synthesizes the SHAP values and the retrieved context into cohesive narratives that reflect real-world clinical reasoning, translating the raw output of the machine learning models into understandable terms. This enables clinicians to comprehend the model's predictions in actionable terms, enhancing the transparency and trustworthiness of the predictions. We detail the components of our explainability framework as follows:\nThe first step in enhancing explainability involves training a predictive machine learning model and calculating SHAP values to assess each feature's contribution to a patient's predicted risk of an ED visit. SHAP values provide granular, quantitative insights into feature importance, but their numerical nature often limits clinical interpretability."}, {"title": "Contextualization Through Document Retrieval", "content": "To bridge the gap between SHAP outputs and clinical actionability, we leverage a domain-specific knowledge repository. This repository includes population-level cohort statistics, risk factor ranges derived from the model, and individual patient characteristics (input features used in the predictive model)."}, {"title": "Generating Clinically Coherent Narratives", "content": "The LLM then synthesizes the SHAP values and the retrieved domain-specific context into a cohesive narrative that reflects real-world clinical reasoning. These narratives translate the raw output of the machine learning models into understandable terms, linking patient attributes\u2014such as acuity level, time-of-day presentation, and other risk factors to established medical knowledge. Thus, clinicians can understand the model's predictions in actionable terms. As illustrated in Figure 1, the explainability framework aligns patient-specific attributes with population benchmarks and temporal patterns. A low-risk patient may exhibit presentation times and acuity levels consistent with population norms, suggesting no significant deviation from baseline risk. Conversely, a high-risk patient may display temporal patterns or acuity levels linked to acute exacerbations, providing insights into the factors driving their elevated risk."}, {"title": "Dataset Characteristics and Outcomes", "content": "The analysis included 42,464 unique ED visits for 27,904 mental and behavioral health patients. Of the mental health cohort, 31,011 patients (73%) did not return, while 11,236 patients (27%) returned within the follow-up period (i.e., 30 days)."}, {"title": "Study Population Characteristics", "content": "The demographic, clinical, and SDoH characteristics of the study population are summarized in Table 1. The mean number of ED visits in the last two months preceding the index visit was 1.03 \u00b1 2.75 (range: 0\u201352). The population was predominantly male (55.06%) and single (63.07%), with a racial composition of 50.32% White and 45.57% Black or African American. Most of the cohort identified as non-Hispanic/Latino (95.20%) and spoke English as their primary language (96.66%). Insurance coverage was distributed across government programs (34.47%), self-pay (33.74%), and private insurance (22.71%).\nClinical acuity levels were primarily moderate, with Emergency Severity Index (ESI) Level 3 accounting for 48.13% of visits. Systolic blood pressure readings were categorized as elevated (37.92%), hypertensive (33.51%), or normal (28.14%). Most patients had a normal heart rate (83.10%), with 14.62% presenting with tachycardia. The largest age group was 31\u201345 years (38.17%), followed by 18\u201330 years (26.46%). Body mass index (BMI) distributions indicated that 38.39% of patients were of normal weight, while 28.86% were classified as obese.\nMental health-related visits accounted for 36.25% of all chief complaints, second only to pain-related visits (45.82%). Tobacco use was reported in 35.52% of cases, while alcohol use was less common, with 31.27% reporting no alcohol use and 17.39% reporting current use. Substance use was categorized as no use (38.88%) or recreational use (10.59%), with 33.48% of cases classified as unclear or other. The distribution of social factors, including nutrition and housing stability, highlighted significant vulnerabilities. While 79.64% of cases lacked clear nutritional data, 10.75% reported moderate nutrition, and 2.73% were categorized as having poor nutrition. Housing assessments showed that 69.02% of patients lived independently or with unclear housing data, while 3.21% were homeless."}, {"title": "LLM features extraction performance results", "content": "This section evaluates the performance of the LLM (Llama 3:8-billion) in feature extraction for chief complaint and SDoH classifications. Few-shot learning approaches are compared to traditional machine learning and pre-trained models."}, {"title": "Chief Complaint Classification", "content": "The classification of chief complaints was evaluated using traditional machine learning models, pre-trained language models, and few-shot learning approaches. Among these, the LLM (Llama 3, 8-billion) with 10-shot learning demonstrated the best performance across all metrics, achieving an Accuracy of 0.882, Precision of 0.95, Recall of 0.88, and an F1-Score of 0.86 (Table 2). This significantly outperformed traditional models like XGBoost (Accuracy: 0.59, F1-Score: 0.53) and pre-trained models such as BlueBERT (Accuracy: 0.63, F1-Score: 0.59). Other few-shot configurations, including 5-shot (Accuracy: 0.816) and 20-shot (Accuracy: 0.803), also performed well but were slightly less effective than the 10-shot setting."}, {"title": "SDOH Classification", "content": "The LLM (Llama 3, 8-billion) with 10-shot learning achieved strong performance across SDoH categories, particularly in Alcohol, Tobacco, and Substance Abuse, with an overall Accuracy of 0.95 and a weighted F1-Score of 0.96. Sensitivity ranged from 0.63 (Home Environment) to 0.95 (Alcohol and Tobacco), while Specificity remained consistently high (0.94\u20130.99). The model performed best in Alcohol, Tobacco, and Substance Abuse (F1: 0.96\u20130.89) but showed moderate performance in Sexual Orientation and Nutrition (F1: 0.79\u20130.72) and lower in Exercise and Home Environment (F1: 0.70\u20130.67). These results highlight its reliable classification across diverse and challenging variables (Table 3)."}, {"title": "Predicative Models for ER Mental Health Return Visits: Machine Learning without/with LLM Features Extractions", "content": "This section evaluates the performance of predictive models for ED mental and behavioral health return visits using two distinct approaches: (1) machine learning models trained on traditional features alone and (2) MACHINE LEARNING models enhanced with features extracted using large ILLMs. Performance metrics, including Accuracy, Precision, Recall, F1-Score, and the AUC, were used to assess the predictive capability of each approach. The results demonstrate that including LLM-extracted features consistently improved model performance across multiple metrics."}, {"title": "Performance of Models Without LLM Feature Extraction", "content": "Table 4 presents the performance metrics for models trained exclusively on traditional features. Neural Network, AdaBoost, Gradient Boosting, and XGBoost all achieved the highest accuracy (0.79), with Gradient Boosting and XGBoost exhibiting the highest precision (0.72). Among them, Neural Network had the highest F1-score (0.47), while Gradient Boosting and XGBoost followed closely (0.45). The AUC values ranged from 0.68 (Logistic Regression) to 0.75 (Gradient Boosting), indicating moderate discriminative ability. In terms of AUC-PR, Neural Network had a score of 0.57, while AdaBoost, Gradient Boosting, and XGBoost achieved the highest scores (0.58). Logistic Regression showed the weakest performance across all metrics, with the lowest recall (0.31), F1-score (0.41), AUC (0.68), and AUC-PR (0.51), suggesting it struggled more in distinguishing positive cases effectively."}, {"title": "Performance of Models with LLM Feature Extraction", "content": "Table 5 highlights the performance of models enhanced with LLM-extracted features, leading to noticeable improvements in key metrics. XGBoost, AdaBoost, and Gradient Boosting achieved the highest AUC (0.76), while Neural Network improved slightly to 0.75. The addition of LLM features resulted in higher precision, recall, and AUC-PR values for most models. Neural Network, for example, maintained its F1-score of 0.47 but improved in precision (0.71) and AUC-PR (0.59). Similarly, AdaBoost and Gradient Boosting saw an increase in AUC-PR to 0.60, reflecting better overall classification performance. XGBoost remained strong, improving in recall (0.34) and F1-score (0.46), while achieving the highest AUC-PR (0.61) along with AdaBoost and Gradient Boosting. Logistic Regression, though slightly improving in AUC (0.70) and AUC-PR (0.54), continued to underperform compared to other models, reinforcing its weaker ability to capture complex patterns."}, {"title": "Exaplainility results", "content": null}, {"title": "Clinical Validation of LLM Generated Explanations", "content": "In analyzing 100 randomly selected explanations, 99 demonstrated complete alignment across all assessment dimensions. A single explanation contained one numerical discrepancy (reporting a risk factor as 92 instead of 93), classified as a minor error with no clinical significance. All explanations maintained clinical validity and showed complete concordance with source documentation and SHAP-derived feature rankings. Independent expert reviews confirmed the absence of moderate or severe errors that could affect clinical interpretation or decision-making. The observed error rate was 1% (1/100), comprising solely the single minor numerical discrepancy."}, {"title": "Comparative Analysis of SHAP and Explainability Framework", "content": "As shown in Figure 2A, the SHAP-based feature importance output quantified the relative contribution of predictors to ED return risk, with \"Number of Visits Past 2 Months\" emerging as the most significant factor. However, while SHAP values provided numerical insights, they lacked the clinical context necessary for immediate decision-making. In contrast, the proposed explainability framework outcome (Figure 2B) integrated SHAP values with contextual and population-level benchmarks to generate patient-specific narratives. For example, a low-risk patient was classified based on hour and day metrics aligned closely with population averages. In contrast, a high-risk patient exhibited temporal patterns and acuity levels associated with increased ED return risk. These narratives offered actionable insights by situating numerical outputs within a clinically relevant framework, addressing a key gap in SHAP-only explanations."}, {"title": "Discussion", "content": "This study introduces and validates an integrated framework that enhances both predictive accuracy and interpretability in clinical machine learning. The findings demonstrate that combining LLM-extracted features with traditional machine learning improves model performance and clinical relevance, making predictions both more accurate and interpretable for clinical use. A key contribution of this study is the explainability -based framework, which achieved 99% accuracy in generating clinical explanations. This high level of reliability in translating complex model outputs into actionable insights represents a substantial advance in making machine learning systems more accessible and trustworthy for decision-making. The success of this approach suggests that the traditional trade-off between model complexity and interpretability may be addressed through careful system design, ensuring that AI-driven insights align with clinical reasoning patterns. The improvement in predictive performance, while modest but consistent, complements the interpretability gains. The LLM-extracted features improved XGBoost's AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61, highlighting the value of unstructured clinical data. Similarly, LLM-based chief complaint classification significantly outperformed traditional models, with accuracy increasing from 0.59\u20130.63 (traditional models) to 0.882 and F1-score improving from 0.53-0.59 to 0.86. The 10-shot learning approach demonstrated effectiveness in reducing reliance on extensive labeled data, making it potentially applicable in real-world clinical settings. Furthermore, LLM-based SDoH extraction provided additional patient context, enhancing ED return risk estimation for mental health patients, which is often difficult to achieve with structured data alone. Despite these promising findings, several limitations must be acknowledged. First, while the explainability framework demonstrated high accuracy in generating clinical explanations, its direct impact on clinical decision-making and provider trust was not assessed. Second, the study was conducted at a single academic medical center, which may limit generalizability to other healthcare settings with different patient populations and documentation practices. Third, computational demands and inference latency need further evaluation to ensure feasibility for real-time clinical applications. The results have immediate implications for clinical decision support systems (CDSS). The success of this framework provides a template for integrating interpretability into predictive models, ensuring that AI-driven insights align with clinical reasoning and workflow integration. Healthcare systems implementing predictive analytics should consider similar approaches to enhance both accuracy and trustworthiness in clinical decision-making. Future research should focus on four key areas: (1) evaluating the impact of LLM-generated explanations on clinician decision-making and trust, (2) conducting multi-center validation studies to assess generalizability, (3) optimizing computational efficiency to enable real-time implementation, and (4) examining how different types of clinical narratives influence model interpretability and performance. This study underscores the importance of balancing accuracy and interpretability in clinical AI. While previous research often treated interpretability as a secondary consideration, our findings suggest that addressing both challenges simultaneously is not only possible but critical for developing clinically viable machine learning systems."}, {"title": "Conclusion", "content": "This study advances the field of clinical machine learning by demonstrating the viability of integrating LLMs with traditional predictive approaches to address the dual imperatives of model performance and clinical interpretability. The proposed explainability framework establishes a generalizable methodology for translating complex model predictions into clinically actionable insights while maintaining prediction fidelity. These findings suggest that healthcare systems can implement sophisticated predictive models without sacrificing interpretability, providing a pathway for the clinical deployment of machine learning systems that align with medical decision-making processes. Future work should focus on multi-center validation and optimization of computational requirements for real-time clinical implementation."}, {"title": "Data availability statement", "content": "Data related to this paper are available with authors and will be available upon reasonable request."}, {"title": "Funding Statement", "content": "This study did not receive any funding."}, {"title": "Institutional Review Board (IRB) Statement", "content": "The Institutional Review Board (IRB) of the University of Alabama at Birmingham (UAB) determined that this study (IRB # IRB-300008858) does not meet the criteria for human subjects research and therefore does not require ethical approval."}]}