{"title": "From Efficiency to Equity: Measuring Fairness in Preference Learning", "authors": ["Shreeyash Gowaikar", "Rashid Mushkani", "Hugo Berard", "Shin Koseki"], "abstract": "As AI systems, particularly generative models, increasingly influence decision-making, ensuring that they are able to fairly represent diverse human preferences becomes crucial. This paper introduces a novel framework for evaluating epistemic fairness in preference learning models inspired by economic theories of inequality and Rawlsian justice. We propose metrics adapted from the Gini Coefficient, Atkinson Index, and Kuznets Ratio to quantify fairness in these models. We validate our approach using two datasets: a custom visual preference dataset (AI-EDI-Space) and the Jester Jokes dataset. Our analysis reveals variations in model performance across users, highlighting potential epistemic injustices. We explore pre-processing and in-processing techniques to mitigate these inequalities, demonstrating a complex relationship between model efficiency and fairness. This work contributes to AI ethics by providing a framework for evaluating and improving epistemic fairness in preference learning models, offering insights for developing more inclusive AI systems in contexts where diverse human preferences are crucial.", "sections": [{"title": "Introduction", "content": "The rapid advancement of generative artificial intelligence (AI) has brought unprecedented capabilities in natural language processing and content generation. However, these developments have also raised significant concerns about the potential for these systems to perpetuate or amplify epistemic injustice [18]. Epistemic injustice, a concept introduced by Miranda Fricker [13], refers to wrongs done to individuals in their capacity as knowers. In the context of generative AI, this manifests as the misrepresentation or misunderstanding of views from minorities and marginalized groups, potentially subjecting them to epistemic violence by denying their own subjective experience.\nGenerative AI systems, such as large language models, risk producing epistemic injustices by embedding biases through their training data and amplifying narratives from the Global North while silencing voices from the Global South. This imposition of a singular framing onto diverse global perspectives fails to recognize the multitude of views and experiences that exist worldwide. The challenge, therefore, is to develop generative AI systems that can fairly represent all existing views and perspectives, acknowledging and respecting the diversity of human experiences.\nTo address these alignment challenges, researchers have turned to techniques like Reinforcement Learning with Human Feedback (RLHF) [39]. RLHF typically involves a multi-step process: first,"}, {"title": "Related Work", "content": "The intersection of fairness, epistemic justice, and AI has garnered significant attention in recent years, particularly in the context of classification and regression tasks. However, less attention has been paid to fairness and epistemic considerations in more complex AI systems such as generative models and preference learning algorithms.\nFairness in Classification and Regression In traditional machine learning tasks, fairness metrics often focus on equalizing outcomes across different groups. [16] introduced the concept of Equal Opportunity, which aims to equalize true positive rates between protected and unprotected groups. Other measures include equalized odds, ensuring equal probability of positive outcomes across classes, and equal accuracy, which balances performance across groups [9]. These metrics, while valuable, primarily address fairness in standard classification tasks, where the goal is to ensure that the algorithm's output does not depend on sensitive attributes. However, in the context of Reinforcement Learning from Human Feedback (RLHF) and preference learning, the concept of fairness requires a different approach. In these scenarios, we acknowledge that different groups may have varying preferences, and thus, the output of the reward model should rightfully depend on the user. Our notion of fairness in this context focuses on ensuring that the model's accuracy in capturing these diverse preferences does not vary significantly across different groups and individual users.\nFairness in Preference Learning and Ranking While works like [26] and [7] have proposed fairness metrics for ranking tasks, their focus primarily remains on ensuring fair treatment of the items being ranked. [31] take a step further by discussing the equity of subgroup populations and exploring the trade-off between this equity and overall accuracy. However, there remains a crucial distinction between these approaches and ours. Standard fairness approaches in ranking typically aim to ensure that items from different groups (e.g., protected vs. unprotected) have equal opportunities"}, {"title": "Background", "content": "This section introduces a mathematical framework for preference learning that allows us to quantify both the overall performance of a model and its fairness across diverse users.\nProblem Definition Consider a set of k users $U = \\{1,..., k\\}$ and a dataset $D = \\{(x_i, x'_i, s_i, u_i)\\}_{i=1}^n$ composed of n pairwise comparisons. Each entry in the dataset represents a comparison where user $u_i$ provided a score $s_i \\in S$. The score can be either: a binary variable (i.e. $S = \\{0, 1\\}$ indicating which option was preferred, or a real value (i.e. $S = \\mathbb{R}$), where negative scores indicate preference for $x_i$, and positive scores preference for $x'_i$, and the magnitude of $s_i$ reflects the strength of the preference. Our goal is to learn a model $f : \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathcal{Y}$ that can score any pair $(x, x') \\in \\mathcal{X} \\times \\mathcal{X}$. We define the error of the model as:\n$E(f) = \\mathbb{E}[l(f(x_i, x'_i), s_i)]$\nwhere l is a loss function that computes the discrepancy between the model outputs and the ground truth score. The choice of loss function depends on the nature of the scores:\n1.  For real-valued scores, we can use the squared error:\n$l(f(x_i, x'_i), s_i) = (f(x_i, x'_i) - s_i)^2$\n2.  For binary scores, where f predicts the probability that $x_i$ is preferred over $x'_i$, we can use the binary cross-entropy (BCE) loss:\n$l(f(x_i, x'_i), s_i) = s_i \\log(f(x_i, x'_i)) + (1 - s_i) \\log(1 - f(x_i, x'_i))$\n3.  Alternatively, we can use the 0-1 loss:\n$l(f(x_i, x'_i), s_i) = \\begin{cases} 0 & \\text{if } s_i = f(x_i, x'_i) \\\\ 1 & \\text{else} \\end{cases}$"}, {"title": "Equality Metrics", "content": "We now introduce a comprehensive set of metrics to quantify the extent to which a model's perfor-mance varies across users. These metrics, adapted from the economics literature on income inequality, allow us to measure different aspects of fairness and equality in AI systems.\nThe importance of these metrics lies in their ability to capture various manifestations of inequality in model performance. For instance, errors might be highly dissimilar across specific groups of users or may vary more gradually across the user population. By employing a range of metrics, each with distinct characteristics, we can gain a nuanced understanding of how fair our preference learning models is. All of the following metrics are non-negative and equal to zero only when the model's performance is identical for all users, representing perfect equality.\nMaximal Error Gap The Maximal Error Gap measures the largest discrepancy in model perfor-mance between any two users. This metric is particularly useful for identifying extreme cases of inequality and aligns with Rawlsian principles of justice by highlighting the worst-case scenario.\n$G_{\\text{max}}(f) = \\max_{u,u' \\in U} (E_u(f) - E_{u'}(f))$\nStandard Deviation of the Error This metric provides a measure of the overall spread of errors across users. A large standard deviation indicates significant variability in model performance, suggesting unequal representation of user preferences.\n$\\sigma^2(f) = \\frac{1}{|U|} \\sum_{u \\in U}(E_u(f) - E(f))$\nGini Coefficient [14] The Gini Coefficient, widely used in economics to measure income inequality, it provides a holistic view of error distribution across users. It can be visualized using the Lorenz"}, {"title": "Equality Metrics", "content": "curve, where the coefficient represents the area between the line of perfect equality and the actual error distribution curve. The Gini Coefficient is bounded between 0 (perfect equality) and 1 (extreme inequality).\n$G(f) = \\frac{\\sum_{u,u'} (|E_u(f) - E_{u'}(f)|)}{2|U|^2E(f)}$\nGeneralised Entropy Index [33] This index offers flexibility through its a parameter, allowing us to focus on different parts of the accuracy distribution across users. Lower a values are sensitive to the existence of users with low accuracy. While higher a values are more sensitive to the existence of users with high accuracy.\n$G_{\\alpha}(f) = \\begin{cases} \\frac{1}{|U|} \\sum_{u \\in U} \\frac{E_u(f)}{E(f)} \\ln \\frac{E_u(f)}{E(f)} & \\text{if } \\alpha = 1 \\\\ - \\frac{1}{|U|} \\sum_{u \\in U} \\ln \\frac{E_u(f)}{E(f)} & \\text{if } \\alpha = 0 \\\\ \\frac{1}{\\alpha(\\alpha-1)} \\sum_{u \\in U} [(\\frac{E_u(f)}{E(f)})^{\\alpha} - 1] & \\text{else} \\end{cases}$\nAtkinson Index [4] Similar to the Generalized Entropy Index, the Atkinson Index uses an $\\epsilon$ parameter to focus on inequalities at different ends of the accuracy distribution across users. As $\\epsilon$ increases, the index becomes more sensitive to errors at the lower end of the distribution.\n$A_{\\epsilon}(f) = \\begin{cases} 1 - \\frac{\\left( \\prod_{u \\in U} E_u(f) \\right)^{1/|U|}}{E(f)} & \\text{if } \\epsilon = 1 \\\\ 1 - \\frac{E(f)}{\\min_{u \\in U} E_u(f)} & \\text{if } \\epsilon = +\\infty \\\\ 1 - \\left[ \\frac{1}{|U|} \\sum_{u \\in U} (\\frac{E_u(f)}{E(f)})^{1-\\epsilon} \\right]^{1/1-\\epsilon} & \\text{else} \\end{cases}$\nKutznets Ratio [21] Unlike the previous metrics that consider the entire error distribution, the Kuznets Ratio focuses on the extremes, comparing the errors of the top a% of users to the bottom a%. This metric is particularly useful for identifying disparities between the best and worst-served users:\n$K_{\\alpha}(f) = \\frac{\\mathbb{E}_{\\text{top } \\alpha\\%} E_u(f)}{\\mathbb{E}_{\\text{bottom } \\alpha\\%} E_u(f)}$\nBy employing this diverse set of metrics, we can comprehensively evaluate the fairness and equality of preference learning models, allowing us to focus on different ends of the distribution."}, {"title": "Experimental Setup", "content": "To rigorously evaluate our proposed equality metrics, we carefully selected two datasets that provide crucial individual-level annotation data. This granular information-detailing which user provided each annotation-enables us to precisely analyze variations in model performance across diverse users. Such user-specific data is instrumental in uncovering potential epistemic injustices in AI systems, yet it is often absent from many widely-used datasets in the field of Reinforcement Learning from Human Feedback (RLHF).\nNotably, prominent datasets such as Safe RLHF [10], Helpful and Harmless [5], WebGPT [25], ImageReward [36], and AVA [24] lack user identification data. This omission precludes the differen-tiation of users with diverse preferences, rendering the computation of our proposed equality metrics impossible on these datasets. The absence of such critical information in these widely-used resources highlights a significant gap in the field's ability to assess and address epistemic injustice in generative models.\nWe posit that the inclusion and release of user-specific annotation data is not merely beneficial but essential for the comprehensive evaluation of fairness in AI systems. By enabling the application of metrics like those proposed in this study, such data would significantly enhance our capacity to identify, quantify, and ultimately mitigate epistemic injustices in generative models. This underscores the urgent need for more nuanced and comprehensive datasets in the pursuit of truly fair and equitable Al systems.\nTo ensure robustness of the results, all experiments employ the following methodology: 1) Model selection was based on the best performance on a randomized validation set, using comparisons"}, {"title": "AI-EDI-Space Dataset", "content": "Dataset The first dataset consists of 7,833 street-view images representing a diverse set of public spaces from the Montreal Metropolitan Area. The dataset includes 19,990 pairwise comparisons, evaluated by 22 individuals who were carefully selected to maximize diversity and include underrep-resented groups based on ethnicity, gender, sexuality and age. Each participant evaluated a minimum of 500 comparisons based on 35 different criteria designed to capture various qualities of public spaces. Participants provided a real value between -1 and 1 to avoid Arrow's impossibility theorem [3], as explained by [1]. However, this approach introduces complexity into the voting patterns, as illustrated in Figure 1.\nThis dataset is ideal for testing algorithmic equity, as it includes a diverse set of participants with potentially divergent views on what makes a public space valuable, making the task highly subjective.\nModel The model and training procedure used are similar to the one proposed in [11]. The model takes a single image as input and predicts the scores. The model consists of a feature extractor and a classifier head. The feature extractor is a pre-trained model. We experimented with several models, including VGG11 [34], EfficientNet [35], Squeezenet [17], and DinoV2 [27]. The extracted features were then passed through a classifier head to predict a score for each of the 35 criteria. We observed that the model with EfficientNet, along with a double-layered classification head with 256 as the hidden dimension, gave the best results throughout all the experiments. Hence, all values for the AI-EDI-Space dataset are using an EfficientNet feature extractor. To train the model, we computed the scores for both images in each comparison and calculated the difference in scores between the two images. We then used the Mean Squared Error between the difference in scores and the ground truth score as the error to train the model.\nLoss We use the 0-1 loss as the baseline metric to compute the various equality metrics, which corresponds to measuring the difference in accuracy across users."}, {"title": "Jester Jokes Dataset", "content": "Dataset We also test the proposed metrics on the Jester Jokes Dataset, originally developed for recommender systems research [15]. This dataset contains 100 jokes, each rated on a scale from -10 to +10 by 73,421 participants. The inherently subjective nature of humor appreciation makes this dataset particularly suitable for examining model performance across diverse user preferences. To ensure data quality and diversity, we only selected a subset of the annotations. We first filtered the"}, {"title": "Methods", "content": "To address the challenge of inequality in model performance across users, we propose and evaluate two categories of techniques: pre-processing and in-processing. These approaches aim to enhance the fairness of preference learning models by ensuring more equitable representation of diverse user preferences."}, {"title": "Pre-Processing Techniques", "content": "Pre-processing techniques are applied to the data prior to model training. We investigate three scaling methods designed to normalize the distribution of scores across users:\n1.  Min-Max Scaling: This technique scales each participant's scores to a range of [-1, 1]. It is applied individually to each user's scores, preserving relative preferences within a user's data while enabling comparability across users.\n2.  Normalization Scaling: This two-step process first applies standard normalization to each participant's scores, adjusting the mean to 0 and standard deviation to 1. Subsequently, the scores are scaled to ensure they remain within the [-1, 1] range. While this method, like Min-Max Scaling, is user-specific, it does not guarantee sparse unanimity.\n3.  Mehestan Scaling [1]: This more sophisticated approach considers the voting patterns of all participants when scaling an individual's scores. The process involves: a) Converting raw comparison scores to individual scores using a Generalized Bradley-Terry Model [12]. b) Scaling and translating these scores using the BrMean primitive, which is designed to be resilient to potential manipulation by malicious voters. c) Preserving individual score distributions without final aggregation, maintaining the uniqueness of each participant's preference pattern.\nMehestan Scaling is particularly effective in achieving sparse unanimity, a property that ensures the preservation of unanimous preferences even when user voting patterns differ significantly."}, {"title": "In-Processing Techniques", "content": "In-processing techniques are integrated into the model training process. We explore two primary approaches:\n1.  User Embeddings: By incorporating user-specific embeddings as additional input to the model, we aim to capture and adapt to individual voting patterns. This technique allows the model to learn user-specific features that may influence preference judgments.\n2.  Contrastive Loss: We employ contrastive loss in conjunction with least squares error (LSE). The contrastive loss works by increasing the distance between dissimilar scores, ensuring that the model's outputs are not clustered too closely together. This helps prevent comparisons-calculated as the difference between the scores of two alternatives-from"}, {"title": "Results", "content": "AI-EDI-Space Dataset After training a model on the AI-EDI-Space dataset, we computed the various equality metrics proposed earlier. Figure 2 shows the distribution of accuracy for all users across different criteria. Our analysis yielded several noteworthy observations: inequality appears to be higher for criteria where the model performs best overall, as shown in Table 1. This illustrates the potential trade-off between efficiency and equality. The observed inequality seems to be primarily driven by voting patterns, as the mean squared error (MSE) used to train the model penalizes discrepancies with the user's comparisons. We also observe that users whose voting patterns cluster around 0 (a conservative voting approach) tend to achieve higher accuracy. Additionally, the number of comparisons annotated by each user may differ, potentially contributing to the observed inequalities. However, disentangling these effects requires further analysis to understand the influence of sample size on our observations."}, {"title": "Results", "content": "Jester Jokes Dataset Our analysis of the Jester Jokes dataset provided additional insights into the effectiveness of various scaling and translation techniques in addressing inequality. Table 2 presents the results of our equality metrics for different approaches. We made the following observatoins: 1) Normalization and MinMax Scaling achieved low Mean Squared Error (MSE) but failed to significantly improve equality. Notably, the Kuznets ratio remained close to 4, indicating substantial inequality between the top 20% and bottom 20% of participants in terms of model performance. 2) Mehestan Scaling, designed for sparse unanimity [1], yielded higher equality values despite a worse MSE. This finding suggests that techniques prioritizing unanimous preference recovery may contribute to greater epistemic fairness. 3) Given our careful selection of users who scored all 100 jokes, we can primarily attribute the observed inequity to differences in voting patterns rather than data imbalance. This reinforces the importance of considering diverse preference expressions in"}, {"title": "Conclusion", "content": "In the era of generative AI, where algorithms increasingly shape decision-making processes, ensuring that these systems do not generate or amplify epistemic injustice is paramount. This study introduces a novel perspective on fairness in preference learning, focusing on the equitable representation of diverse human preferences and views. Our work provides a framework for quantifying and addressing epistemic fairness in AI models, contributing to the development of more just and inclusive AI technologies. Our findings underscore the potential for significant disparities in how well AI models capture preferences across different users. This raises critical questions about epistemic justice in AI systems and highlights the need for further research in several key areas: 1) Further investigation is needed to understand the sources of inequality in model performance and develop effective mitigation strategies. This includes examining how data characteristics, model architectures, and diverse human preferences interact to produce or exacerbate inequalities. 2) As Reinforcement Learning from Human Feedback (RLHF) becomes more prevalent, ensuring that the alignment process itself is equitable across diverse participant groups is crucial. This involves developing methods to capture a wide range of opinions and preferences, particularly from marginalized or underrepresented groups. 3) We advocate for the public release of more datasets that include annotation-level user information, specifically detailing which annotator or user provided each individual annotation. This granular data is crucial for conducting comprehensive evaluations of epistemic justice in AI models. Such datasets would enable researchers to track how different users' preferences and judgments are represented in model outputs, providing a more nuanced understanding of potential biases or inequalities in preference learning and generative AI systems. 4) Finally, the tension between individual fairness and overall system efficiency raises important ethical questions. Future work should explore how to balance these concerns in line with principles of distributive justice and epistemic fairness, particularly in the context of generative AI systems."}]}