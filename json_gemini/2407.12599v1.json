{"title": "On Diversity in Discriminative Neural Networks", "authors": ["Brahim Oubaha", "Claude Berrou", "Xueyao Ji", "Yehya Nasser", "Rapha\u00ebl Le Bidan"], "abstract": "Diversity is a concept of prime importance in almost all disciplines based on information processing. In telecommunications, for example, spatial, temporal, and frequency diversity, as well as redundant coding, are fundamental concepts that have enabled the design of extremely efficient systems. In machine learning, in particular with neural networks, diversity is not always a concept that is emphasized or at least clearly identified. This paper proposes a neural network architecture that builds upon various diversity principles, some of them already known, others more original. Our architecture obtains remarkable results, with a record self-supervised learning accuracy of 99. 57% in MNIST, and a top tier promising semi-supervised learning accuracy of 94.21% in CIFAR-10 using only 25 labels per class.", "sections": [{"title": "I. INTRODUCTION", "content": "In the information sciences, the principle of diversity consists in combining information from different sources to better estimate the data. Diversity is all the more effective when the sources are decorrelated, i.e. when the information they provide is not processed in the same way and/or does not derive from the same observations. This ideal condition is rarely met, and we generally make do with partially correlated information. It is probably the field of telecommunications that has benefited most from the principle of diversity in moving towards very high-performance systems, both fixed and mobile. In the time domain, channel coding (or error correcting coding) makes it possible to transmit the binary elements of an augmented (redundant) version of the original message at different times (and therefore generally subject to different disturbances) and to benefit from this redundancy in the receiver. In the frequency domain, techniques such as Orthogonal Frequency-Division Multiplexing can more or less eliminate spectrum irregularities and interferences. Pruning techniques may also be considered to remove inappropriate parts of the bandwidth. The spatial dimension is of course also used, with multi-path techniques such as Multiple-Input Multiple-Output taking advantage of the particular properties of the wave paths. Other types of diversity can be exploited at higher system levels (multiuser, multistandard, etc.). In contrast, the classic architecture of a neural network, i.e. a few convolution layers followed by a classifier with a simple one-hot output (as many neurons as classes), does not reveal any deliberately introduced diversity technique. It could of course be pointed out that the totality of the weights of a neural network's connections is always oversized and therefore redundant. However, in the absence of a theory on neural network capacity and redundancy, we cannot really speak of intentional, controlled diversity. Analogies can however be drawn between different types of diversity found in digital communications and in neural networks:\n\nTwo techniques can be related to channel coding (redundant coding). The first, of high importance in self-supervised and semi-supervised applications, is data augmentation. This involves submitting several distorted versions (rotation, cropping, mirroring, etc.) of the same sample to the network. Redundancy rates are therefore several hundred percent. The second technique involves increasing the length of the network output by multiplying the number of neurons that must be activated for a given class. This is known as distributed coding. The redundancy rate is determined by the length of the output and can be several thousand percent. A theory of this process has been developed under the name of Error Correcting Output Coding (ECOC) [1].\n\nA convolution layer can be presented as a spatio-temporal encoding layer. This is because the implementation of filters seeking to extract features independently of coordinates involves sharing the synaptic weights of these filters. There is therefore both redundant coding (repeated weights) and spatial coding (the search for a certain invariance with respect to coordinates). Regularization techniques such as dropout or drop-connect can also be assimilated to a form of spatial diversity. Another type of spatial diversity, not often implemented to"}, {"title": "II. COMPETITION AND SPARSITY", "content": "Competition has played an important role in the evolution of species, most often due to limited resources. The same is true for nearby neurons in the nervous system, because the energy provided by the local blood flow does not allow them all to activate at the same time. It is then not unreasonable to think that the brain relies on some kind of competition in order to learn and memorize while saving energy, and the same applies for artificial networks, by bio-inspired analogy. The first paper to highlight the benefits of using the competition principle in neural networks dates back ten years [4]. It showed that the classical activation function of neurons in a hidden layer, usually a sigmoid or ReLU function, can be advantageously replaced by a Local Winner-Takes-All (LWTA) function in blocks of several neurons. In a given layer, this technique thus puts into competition all the activities coming from the neurons of the underlying layer through a full matrix of connections. It turns out that this principle works just as well, and perhaps even better, if the matrix is sparse, or even very sparse, although this possibility was not considered in [4]. In this situation, the competition is performed between subpopulations of the underlying neurons, rather than within the whole population. These small groups of neurons are selected completely at random, since the drawing of the matrix of sparse connections is itself random. In addition to the diversity brought about by multiple, quasi-independent competitions within the considered layer, extra-diversity can be added when learning by an ensemble of networks. As each network is initialized in a different way (each time by a different seed that decides the topology of connections), the number of different competitions in the ensemble is increased. The realities of competition (see [4] for relevant references) and sparse connections [5], [6] are considered proven and fundamental properties in mammalian cortexes. Our contribution appears to be the first to combine these two bio-inspired principles in artificial neural networks, and to evaluate the potential of such architectures for self- and semi-supervised learning. We note that the competition and sparsity are very simple to implement. No particular pre- or post-processing is needed to make the best use of. We will only rely on the now classical functions adopted in neural networks, such as data augmentation, convolutional layers, batch normalization, pseudo-label estimation, etc."}, {"title": "III. PROPOSED LEARNING METHOD", "content": "In this section, we start by revisiting preliminary work in self and semi-supervised learning frameworks, in particular those involving the principle of data augmentation and label estimation. Then we introduce the proposed learning method, detailing the architectural design of the models for each dataset. Next, we elaborate on our label estimation algorithm, highlighting its main features and operation.\nSemi-supervised learning lies between supervised and un-supervised learning. It involves the use of a small amount of labeled data in conjunction with a large amount of unlabeled data during the training process. This approach is particularly beneficial in scenarios where labeled data is scarce or expensive to obtain, but there is an abundance of unlabeled data. Consistency regularization, a key feature of many advanced semi-supervised learning algorithms, exploits unlabeled data based on the principle that a model should produce consistent predictions for different perturbed versions of the same image. This concept was initially introduced in an earlier work [7], and has been more widely recognized in subsequent studies [8], [9]. It is implemented by training the model with traditional supervised classification loss and an additional loss function that handles unlabeled data, thus improving the model's ability to learn from a wider spectrum of data.\nAnother common approach in semi-supervised learning is pseudo-labeling [10], where the model uses its predictions on unlabeled data to generate artificial labels. These pseudo-labels are then used in subsequent training to refine the model's performance.\nOur semi-supervised learning approach leverages the strength of both labeled and unlabeled data, strategically incorporating data augmentation to enhance model performance. This allows us to make efficient use of all available data, combining the reliability of labeled examples with the broader coverage provided by unlabeled examples, thus improving the efficiency and robustness of the model.\nConversely, in our self-supervised learning approach, we exclusively rely on unlabeled data for training, omitting the use of labeled data. This method focuses on extracting meaningful patterns and structures from the data itself without direct guidance from explicit labels.\nTo efficiently handle unlabeled data, our approach starts with pseudo-labeling, a critical step where we utilize our model on subtly altered data to generate pseudo-labels. This is instrumental in guiding the learning process, even without standard labeled data. What sets our label estimation process apart is its unique treatment of embeddings. Within each block of size n of the embedding space (embedding refers to the output of the model), we identify the maximum value and assign it a label of 1, while all other elements within the block are set to 0, following the principle of competition presented in Section II. This selective activation within the embedding space effectively highlights the most prominent features or characteristics captured by each block, acting as a form of dimensionality reduction and targeted feature amplification.\nSubsequently, we expose the same batch of data to strong deformations, such as extensive augmentation or distortion, while utilizing the estimated label as the target. This method effectively challenges the model to maintain its predictions under more significant variations, thereby enhancing its ro-bustness and ability to generalize from complex or noisy data. The complete learning process is illustrated in Figure 1.\nIt is worth noting that for the labeled batch, nothing changes compared to other supervised learning approaches. Labeled data continue to serve as a reliable source of ground truth, anchoring the model learning with trustable examples. This combination of reliable labeled data with a creative use of unlabeled data allows our model to benefit from the full spectrum of available information, leading to more effective and comprehensive learning outcomes."}, {"title": "A. Datasets", "content": "The MNIST dataset [11] is a well-known benchmark in the field of machine learning. It consists of 60,000 handwritten digits (0-9), with each digit represented as a 28 \u00d7 28 pixels grayscale image. This dataset is commonly used for tasks related to digit recognition and image classification, making it a fundamental resource for testing and developing various machine learning algorithms.\nThe CIFAR-10 dataset [12] is another widely used dataset in computer vision. It contains 60,000 colored images, divided into 10 classes, with each class representing various everyday objects or animals, such as cars, birds, or cats. These images are relatively small, 32 \u00d7 32 pixels in size, and serve as a valuable benchmark for testing image classification and deep learning models due to their intrinsic diversity and complexity."}, {"title": "B. Model Architecture", "content": "Our models dedicated to self- and semi-supervised classification on the MNIST and CIFAR-10 datasets use the same general two-part architecture, consisting of an encoder in charge of the features extraction, followed by a Multi-Layer Perceptron (MLP) classifier."}, {"title": "A. Implementation Details", "content": "To ensure consistent, systematic learning and model re-finement for both MNIST and CIFAR-10, we found crucial to organize the training into cycles, epochs, and batches. Cycles ensure complete dataset coverage, epochs allow full iterations over all data batches, and batches enable efficient data processing and incremental model updates, collectively facilitating continuous model improvement. It is important to note that each epoch uses different data augmentation strategies, further enriching the learning and ensuring that the model encounters diverse data representations throughout its training.\nFor the MNIST dataset, the model architecture is trained through 100 cycles, each cycle consisting of five epochs. At the beginning of each cycle, pseudo-labels estimation is conducted once. These pseudo labels are then used for the subsequent five epochs of training. For training, we use Adam optimizer and a linear learning rate schedule with damping coefficient decaying from 1.0 to 0.001, starting from an initial learning rate of 0.0015. The model's robustness and adaptability are further enhanced by a series of data augmentation techniques: rotation, elastic distortion, random erasing, and center cropping. The Binary Cross-Entropy (BCE) loss is used to measure the difference between predictions and pseudo-labels, paired with a local normalization strategy for block-by-block data processing. This comprehensive approach ensures robust learning and model's effectiveness in discerning complex patterns throughout its extensive training.\nSimilarly, for CIFAR-10, our model undergoes 300 training cycles, with each cycle comprising five epochs to deeply engage with the complexity of the dataset. At the core of our model is the ResNet18 encoder, selected for its robust feature extraction capabilities. Stochastic Gradient Descent (SGD) with Nesterov momentum is used to optimize the model, starting with an initial learning rate of 0.03. The learning rate is meticulously modulated across the 300 cycles by means of a cosine decay schedule, of the form $0.03 \\cos(s/S)$, where s and S represents the current and last training steps, respec-tively. An MSE loss function is used for both labeled and unlabeled batches. For labeled data, the MSE loss quantifies the difference between the model's predictions and the true labels. For unlabeled data, it measures the discrepancy between predictions and generated pseudo-labels. The labeled and unlabeled datasets will be denoted by X and U, respectively. The batch size for X is set to 64. The batch size for U is set to be 8 times the batch size of X, that is 8\u00d764 = 512. In terms of data augmentation, our implementation strictly adheres to the strong and weak augmentation strategies outlined in FixMatch [13]."}, {"title": "B. Evaluation", "content": "Our comprehensive evaluation strategy for the MNIST dataset relies on two distinct methods to assess the perfor-mance of our model using the embeddings as features. Firstly, we use K-means clustering [14] to categorize the embeddings from the test set into 10 clusters. Each cluster is mapped onto one of the ten digits, based on the majority label of its mem-ber embeddings. Secondly, we randomly select one labeled instance from each class in the training set as a representative. We then measure the similarity between the selected instances and the embeddings of the test set to assign class labels. For the CIFAR-10 dataset, the evaluation relies solely on the second method. We choose a single representative labeled example per class, and then assess model performance based on the similarity between the selected examples and the test embeddings. This more focused approach adopted for CIFAR-10 allows us to take advantage of the more complex and varied nature of the dataset, aligning the evaluation strategy with the specific challenges and characteristics inherent to CIFAR-10 images.\nAs for the experimental design, we conducted five dis-tinct experiments for both self-supervised and semi-supervised learning methods to confirm the robustness and reliability of our results. Each experiment begins with random initialization of the weights and sparse layers' connections for each network, to avoid any initialization bias and create network diversity. Within each cycle, data augmentation is introduced in a stochastic manner over different sets of training images. Last but not least, to assess the outcomes, we use a dual strategy that combines taking the majority vote from the five models, and computing the average accuracy across different labeled data sets."}, {"title": "C. Results", "content": "In Table I, we present the classification accuracy achieved on the MNIST dataset by the proposed self-supervised learning approach. The Table reports the average accuracy across five different labeled data (avg acc), as well as the collective decision-making accuracy obtained by majority vote among the five networks with the same labeled data (maj. vote). Two distinct methodologies were used to assess model per-formance: one using K-means clustering ('K-means(%)') and the other leveraging cosine similarity ('cosine sim(%)') for each network. Together these metrics provide a clear and com-prehensive view of the model's performance, demonstrating the effectiveness of both individual networks and a collective ensemble approach in accurately classifying MNIST digits.\nTable II presents a comparison of the classification accuracy obtained for semi-supervised learning on CIFAR10 with vari-ous well-established as well as more recent methods, including"}, {"title": "V. CONCLUSION", "content": "It is possible to design and operate an artificial neural network without having to understand all its components and behavior in detail. However, there are critical applications, such as autonomous driving or medical diagnostics, which require total control over the explainability of operations performed and decisions made, especially when the network makes mistakes [23], [24]. One way of ensuring that algorithms behave as desired is to introduce principles and functions that have proved their worth in other technological fields. In this paper, we have compiled a list of concepts from which the telecommunications field has benefited greatly. Among these, it seemed relevant to us to combine redundant coding and spatial diversity in a relatively simple learning architecture integrating competition layers and sparse matrices. The results obtained with self-supervised learning experiments on the MNIST dataset are convincing, with an inference accu-racy higher than the previous state-of-the-art. Semi-supervised learning simulations have also been conducted on the more challenging CIFAR-10 dataset. To date, the results are not quite up to the state-of-the-art, yet very close. This is even more promising, as they were obtained without the need for sophisticated mathematical processing. Therefore, our future work will focus on finding the reasons for this performance gap between self-supervised and semi-supervised applications. Throughout our work, we have observed that the classification performance of CIFAR-10 images is very sensitive to the values of hyperparameters, in particular sparsity rates and learning rate, which we have not been able to refine completely. Other avenues could also be investigated: increasing the number of competition-based layers, merging the X and U batches into a single one, replacing binarization (hard decision) with a more progressive function (soft decision), still to be determined."}]}