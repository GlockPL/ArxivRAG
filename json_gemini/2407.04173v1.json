{"title": "Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs", "authors": ["Faisal Hamman", "Pasan Dissanayake", "Saumitra Mishra", "Freddy Lecue", "Sanghamitra Dutta"], "abstract": "Fine-tuning large language models (LLMs) on limited tabular data for classification tasks can lead to fine-tuning multiplicity, where equally well-performing models make conflicting predictions on the same inputs due to variations in the training process (i.e., seed, random weight initialization, retraining on additional or deleted samples). This raises critical concerns about the robustness and reliability of Tabular LLMs, particularly when deployed for high-stakes decision-making, such as finance, hiring, education, healthcare, etc. This work formalizes the challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel metric to quantify the robustness of individual predictions without expensive model retraining. Our metric quantifies a prediction's stability by analyzing (sampling) the model's local behavior around the input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic robustness guarantees against a broad class of fine-tuned models. By leveraging Bernstein's Inequality, we show that predictions with sufficiently high robustness (as defined by our measure) will remain consistent with high probability. We also provide empirical evaluation on real-world datasets to support our theoretical results. Our work highlights the importance of addressing fine-tuning instabilities to enable trustworthy deployment of LLMs in high-stakes and safety-critical applications.", "sections": [{"title": "1 Introduction", "content": "Large language models are generating significant interest in high-stakes applications, e.g., finance, healthcare, etc., particularly in few-shot classification scenarios on tabular datasets. Recent findings demonstrate that these models perform commendably in such scenarios, especially when very little training data is available, due to their transfer learning abilities [Hegselmann et al., 2023, Dinh et al., 2022, Yin et al., 2020, Yan et al., 2024, Wang et al., 2023]. However, models in these settings are often fine-tuned from large pretrained models with millions or billions of parameters on small proprietary datasets [Hu et al., 2021, Liu et al., 2022]. This paucity of training data, combined with large parameter spaces, risks instability across different fine-tuned variants of the pre-trained model.\nWe introduce the the notion of fine-tuning multiplicity as to the phenomenon where multiple competing models, fine-tuned from the same pre-trained LLM but under slightly varying conditions (such as different random seeds or minor changes in the training data), exhibit comparable performance (e.g., in terms of accuracy) yet generate conflicting predictions for the same inputs. This concept is closely related to predictive multiplicity, often referred to as the Rashomon effect in the context of neural networks [Marx et al., 2020, Breiman, 2003, Hsu and Calmon, 2022]. This is particularly alarming for high-stakes applications, such as finance [Yin et al., 2023], healthcare [Wang et al., 2024, Chen et al., 2023, Kim et al., 2024], where arbitrary predictions can have significant consequences."}, {"title": "1.1 Related Works", "content": "LLM in tabular predictions. The application of LLMs to tabular data is a growing area of research, demonstrating significant performance due to the transfer learning capabilities [Yin et al., 2020, Li et al., 2020, Narayan et al., 2022, Borisov et al., 2022, Bertsimas et al., 2022, Onishi et al., 2023, Zhang et al., 2023, Wang et al., 2023, Sui et al., 2024, Yan et al., 2024, Yang et al., 2024]. Yin et al."}, {"title": "1.2 Preliminaries", "content": "We consider a classification task for a tabular dataset \\(D = \\{(x_i, y_i)\\}_{i=1}^n\\), where each \\(x_i\\) is a d-dimensional feature vector (rows of a tabular input), and each label \\(y_i\\) is binary, \\(y_i \\in \\{0, 1\\}\\). We study an n-shot classification problem by fine-tuning a pre-trained model on n examples from a training set. This fine-tuning process aims to adapt the pre-trained model to effectively predict new, unseen data points by learning from a limited number of training examples.\nSerialization of Tabular Data for LLMs: To effectively apply LLMs to tabular data, it is crucial to transform the data into a natural text format. This process, known as serialization, involves converting the table rows into a text string that includes both the column names and their corresponding values [Yin et al., 2020, Jaitly et al., 2023, Hegselmann et al., 2023, Dinh et al., 2022]. The resultant serialized string is combined with a task-specific prompt to form the input for the LLM. There have been various proposed methods for serialization, and this is still a topic of active research Hegselmann et al. [2023], Jaitly et al. [2023]. Among the serializations we have examined are: list template (A list of column names and feature values), and text template (\u201cThe <column name> is <value>.\u201d). LLMs can be adapted for classification tasks by training them on serialized tabular data. This training involves using the natural-language outputs of the LLM, mapped to valid classes in the target space, as part of a fine-tuning process (see Figure 1).\nTo clarify, table values are serialized into \\(serialize(x)\\) and then transformed into a format understandable by the LLM, \\(tokenize(serialize(x))\\), which is some embedding. Since these transformations are one-to-one mappings, we denote the embedded form of x as \\(x \\in \\mathcal{X}\\) to represent x in the embedding space. This allows us to simplify the notation and directly use x to refer to the table values in the embedding space.\nGoals. Our primary goal is to define a measure of predictive consistency to fine-tuning multiplicity that does not require retraining multiple models since this is computational expensive for LLMs. We aim to theoretically motivate this measure and provide theoretical guarantees on its effectiveness in ensuring consistent predictions across a broad range of fine-tuned models (see Figure 1). Additionally, we intend to evaluate this measure against multiplicity metrics by fine-tuning several models to empirically validate its robustness and reliability."}, {"title": "2 Model Multiplicity in Fine-Tuned Tabular LLMs", "content": "Let \\(f(\\cdot) : \\mathcal{X} \\rightarrow [0, 1]\\) denote an LLM that performs binary classification, outputting the probability distribution over the classes. We let \\(\\mathcal{F}\\) denote a broad class of fine-tuned models that are equally well-performing (i.e., a set of competing models as measured by the accuracy), i.e, \\(\\mathcal{F}_\\delta = \\{f : err(f) \\leq err(f_0) + \\delta\\}\\) where \\(err(f_0) = \\sum_{i=1}^n I[f_0(x_i) \\neq y_i]\\) for a reference model \\(f_0\\) (with satisfactory accuracy) a dataset with n examples. Here, \\(f(x) = I[f(x) \\geq 0.5]\\) denotes the predicted labels. This is a set of models that perform just as well as the baseline classifier where \\(\\delta \\in (0, 1)\\) is the error tolerance [Marx et al., 2020]. The appropriate choice of \\(\\delta\\) is application-dependent.\nFine-tuning multiplicity. We introduce the notion of fine-tuning multiplicity as to the phenomenon observed where multiple competing models, fine-tuned from the same pre-trained LLM but under slightly varying conditions (such as different random seeds or minor changes in the training data), exhibit comparable performance yet generate conflicting predictions for the same inputs (e.g., model in \\(\\mathcal{F}_\\delta\\)). This concept is closely related to predictive multiplicity, often referred to as the Rashomon effect in the context of neural networks [Marx et al., 2020, Breiman, 2003, Hsu and Calmon, 2022]. However, this issue becomes more pronounced with LLMs because these models are typically fine-tuned on small datasets from large foundational models (to leverage their transfer learning ability), which possess millions or billions of parameters. This paucity of training data, combined with large parameter spaces, risks instability across different fine-tuned variants of the model (see Figure 1)."}, {"title": "2.1 Evaluating Fine-tuning Multiplicity", "content": "To effectively evaluate the extent of fine-tuning multiplicity, we introduce specific empirical metrics that assess how predictions may vary across different versions of fine-tuned models. These can only be evaluated when we access to several fine-tuned models in the competing set.\nDefinition 1 (Arbitrariness [Gomez et al., 2024]). Arbitrariness over set \\(\\mathcal{F}_\\delta\\) measures the extent of conflicting predictions across the model space for a given set of inputs \\(\\{x_1, ..., x_n\\}\\). It is defined as:\n\\[A_\\delta = \\frac{\\sum_{i=1}^n I[\\exists f, f' \\in \\mathcal{F}_\\delta, : f(x_i) \\neq f'(x_i)]}{n} \\]"}, {"title": "3 Quantifying Prediction Consistency amidst Fine-tuning Multiplicity", "content": "Our objective is to develop a measure denoted as \\(S_{k, \\sigma} (x, f)\\), for an input x and a given fine-tuned model f, that quantifies its robustness of predictions to a broad class of fine-tuned models. Ideally, we desire that the measure \\(S_{k, \\sigma} (x, f)\\) should be high if the input x is consistent across this broad class of fine-tuned models (see Figure 1)."}, {"title": "3.1 Proposed Consistency Measure", "content": "Prediction probabilities as a measure of consistency. Using the prediction probabilities of model \\(f(\\cdot)\\) as a measure of prediction certainty can offer insights into the model's confidence in its predictions in choosing a certain class. While a high prediction probability \\(f(\\cdot)\\) might suggest a strong confidence in a particular class, we show that relying solely on \\(f(x)\\) for assessing the robustness of predictions against model multiplicity is insufficient (see Table 4, Figure 2, i.e., samples with high \\(f(x)\\) or confidence can still be susceptible to multiplicity). Therefore, we propose leveraging the local neighborhood around the input x in the embedding space. This motivates us to derive a theoretical measure of consistency.\nDefinition 6 (Consistency). The consistency of a given prediction \\(f(x) \\in [0, 1]\\) is defined as follows:\n\\[S_{k, \\sigma} (x, f) = \\frac{1}{k} \\sum_{x_i \\in N_{x,k}} (f(x_i) - |f(x) - f(x_i)|),\\]\nwhere \\(N_{x,k}\\) is a set of k points sampled independently from a distribution over a hypersphere of radius \\(\\sigma\\) centered at x, i.e., \\(N_{x,k} = \\{X_1, X_2, ..., X_k\\} \\subset B(x, \\sigma) = \\{x' \\in \\mathcal{X} : ||x' - x||_2 < \\sigma\\}\\).\nRemark 2. Our consistency measure is fundamentally tied to the confidence in predicting a specific class. The concept can be seamlessly applied by considering the logits (or softmax outputs) for predicting any given class. This approach can also be extended to multi-class classification by providing logits for each class, thereby maintaining the measure's applicability across various classification tasks."}, {"title": "3.2 Theoretical Guarantees on Consistency", "content": "Here, we present theoretical insights that motivate and provide guarantees for our proposed robustness measure \\(S_{k, \\sigma} (x, f)\\), ensuring consistent predictions across a broad class of fine-tuned models. We represent the class of fine-tuned models by a stochastic (random) function \\(F\\), such that \\(F \\in \\mathcal{F}\\). We denote two random models, F and F', both of which are independently and identically distributed within \\(\\mathcal{F}\\). For clarity, we use capital letters (e.g., F, F', X_i, Z) to denote random variables, while lowercase letters (e.g., x_i, f, \\epsilon) indicate specific realizations.\nIn our framework, we define a set of assumptions that delineates the behavior of a broad class of finetuned models and the statistical properties of their predictions.\nAssumption 1 (Stochastic Fine-Tuned Model Class). We define the stochastic divergence between predictions of two random models, F and F' as:\n\\[Z_i := F'(X_i) - F(X_i) - |F(X_i) - F(x)| + |F'(X_i) - F'(x)|\\]\nwith \\(Z = \\frac{1}{k} \\sum_{i=1}^k Z_i\\) where \\(X_i\\)'s are random points sampled independently from a distribution over a hypersphere \\(B(x, \\sigma)\\). We assume:\n* \\(F(X)\\) and \\(F'(X)\\) are independent and identically distributed given an input \\(X \\sim x\\).\n* \\(\\text{Var} [Z_i|F' = f', F = f] < \\beta \\) for all \\(f, f' \\in \\mathcal{F}\\).\nIntuition: The variable \\(Z\\) captures the neighborhood stochastic divergence between predictions of two independently fine-tuned models F and F'. This captures both the difference in predictions and variability around a given point x. The first assumptions ensure that F and F' provide an unbiased estimate of the prediction for x. The assumption on the variance of \\(Z_i\\) indicates that the variance of the stochastic neighborhood divergence within a \\(\\sigma\\)-Ball of a sample between any two models' predictions is controlled. The parameter \\(\\beta\\) essentially captures the similarity of the models within the local neighborhood of a sample. This concept is also somewhat analogous to the Lipschitz constant of a general function, which bounds how much the function's output can change relative to changes in its input. However, in this context, the \\(\\beta\\)-bound reflects an average behavior of the models' predictions within the local neighborhood. It does not strictly enforce a uniform Lipschitz constant, especially considering that transformer models are not typically Lipschitz continuous [Kim et al., 2021].\nTheorem 1 (Probabilistic Guarantee on Consistency). Given a data point x, a random model F' and consistency measure \\(S_{k, \\sigma}(x, F')\\). Then under Assumption 1, and \\(|\\mathbb{E} [Z_i|F' = f', F = f] | < \\epsilon'\\), a prediction over a broad class of fine-tuned models satisfies:\n\\[\\text{Pr} (F(x) \\geq S_{k, \\sigma} (x, F') - \\epsilon) \\geq 1 - \\exp \\left( -\\frac{k \\epsilon^2}{8 \\beta + \\frac{16}{3} \\epsilon'} \\right)\\]\nfor all \\(\\epsilon > 2\\epsilon'\\), The probability is over the stochastic models F and F', and the random perturbations \\(X_i\\)'s are random points sampled independently from a distribution over a hypersphere \\(B(x, \\sigma)\\)."}, {"title": "4 Empirical Validation", "content": "In this section, we experiment across different datasets to (i) quantify the prevalence of fine-tuning multiplicity in Tabular LLMs, and (ii) validate the effectiveness of our proposed measure in quantifying the consistency of predictions over a broad range of fine-tuned models.\nDatasets and Serialization. Our experiments utilize the Diabetes [Kahn], German Credit [Hofmann, 1994], and Adult datasets [Becker and Kohavi, 1996], serialized using the \u201cText Template\u201d method [Hegselmann et al., 2023, Dinh et al., 2022] where each tabular entry is converted into a natural language format by stating \u201cThe <column name> is <value>.\u201d This approach helps align the inputs with the training distribution of LLMs, enhancing their performance in both zero-shot and few-shot scenarios."}, {"title": "5 Discussions", "content": "Our multiplicity evaluation metrics, summarized in Table 1, reveal significant variability in model predictions across different fine-tuned variants, even when they exhibit similar accuracy. This multiplicity is not captured by merely examining predicted probabilities, as predictions with high confidence can still be susceptible to multiplicity (see Figure 2). Our consistency measure, \\(S_{k, \\sigma} (x, f)\\), was compared with prediction probabilities \\(f(x)\\). The results, presented in Table 4, demonstrate that our consistency measure consistently shows mainly higher correlation with multiplicity metrics across the datasets compared to prediction probabilities. This indicates that \\(S_{k, \\sigma} (x, f)\\) is more informative than the predictive probabilities at informing the multiplicity.\nMarx et al. [2020] argue for the necessity of measuring and reporting multiplicity to better inform predictions. Traditional methods to measure multiplicity in classical ML are impractical for LLMs due to the computational challenge of retraining several fine-tuned models [Marx et al., 2020, Hsu and Calmon, 2022, Watson-Daniels et al., 2023]. Our proposed measure, which requires only the given model and leverages the embedding space to inform multiplicity, addresses this issue. This approach reduces the complexity from retraining and inference to just inference, making it more feasible to apply in practice. Although, from our theoretical guarantee, a large k (number of sampled points) might be needed for accurate consistency estimation (particularly when \\(\\beta\\) is large), it remains computationally more efficient than retraining multiple models. Our work provides practitioners with meaningful information about the multiplicity of predictions, which may lead them to carefully evaluate which predictions to trust and which to treat with caution. Our research has significant implications in several high-stakes applications, e.g., hiring, finance, education, etc., where inconsistent predictions can lead to distrust. A limitation of our work is that while we inform about fine-tuning multiplicity for a given sample, we do not resolve it. Future work could focus on developing methods to mitigate fine-tuning multiplicity, ensuring more consistent model predictions."}, {"title": "A Relevant Definition", "content": "Definition 7 (Spearman's Correlation). Spearman's correlation, \\(\\text{Spearman}(X, Y)\\), measures the strength and direction of the monotonic relationship between two ranked variables. It is calculated as the Pearson correlation coefficient between the ranked variables.\nFor n pairs of observations \\((X_i, Y_i)\\), Spearman's rank correlation is given by:\n\\[\\text{Spearman}(X, Y) = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n(n^2 - 1)},\\]\nwhere \\(d_i\\) is the difference between the ranks of \\(X_i\\) and \\(Y_i\\). Alternatively, in terms of covariance:\n\\[\\text{Spearman}(X, Y) = \\frac{\\text{cov}(\\text{rank}(X), \\text{rank}(Y))}{\\sigma_{\\text{rank}(X)} \\sigma_{\\text{rank}(Y)}},\\]\nwhere \\(\\sigma\\) denotes standard deviation. \\(\\text{Spearman}(X, Y)\\) ranges from -1 (perfect negative monotonic relationship) to 1 (perfect positive monotonic relationship), with 0 indicating no monotonic relationship."}, {"title": "B Proof of Theoretical Guarantee", "content": "Theorem 1 (Probabilistic Guarantee on Consistency). Given a data point x, a random model F' and consistency measure \\(S_{k, \\sigma}(x, F')\\). Then under Assumption 1, and \\(|\\mathbb{E} [Z_i|F' = f', F = f] | < \\epsilon'\\), a prediction over a broad class of fine-tuned models satisfies:\n\\[\\text{Pr} (F(x) \\geq S_{k, \\sigma} (x, F') - \\epsilon) \\geq 1 - \\exp \\left( -\\frac{k \\epsilon^2}{8 \\beta + \\frac{16}{3} \\epsilon'} \\right);\\]\nfor all \\(\\epsilon > 2\\epsilon'\\), The probability is over the stochastic models F and F', and the random perturbations \\(X_i\\)'s are random points sampled independently from a distribution over a hypersphere \\(B(x, \\sigma)\\)."}]}