{"title": "Universal AI maximizes Variational Empowerment", "authors": ["Yusuke Hayashi", "Koichi Takahashi"], "abstract": "This paper presents a theoretical framework unifying AIXI\u2014a model of univer- sal AI-with Variational Empowerment as an intrinsic drive for exploration. We build on the existing framework of Self-AIXI [1]\u2014a universal learning agent that predicts its own actions-by showing how one of its established terms can be in- terpreted as a variational empowerment objective. We further demonstrate that universal Al's planning process can be cast as minimizing expected variational free energy (the core principle of Active Inference), thereby revealing how uni- versal AI agents inherently balance goal-directed behavior with uncertainty reduc- tion curiosity). Moreover, we argue that power-seeking tendencies of universal AI agents can be explained not only as an instrumental strategy to secure future reward, but also as a direct consequence of empowerment maximization-i.e. the agent's intrinsic drive to maintain or expand its own controllability in uncertain environments. Our main contribution is to show how these intrinsic motivations (empowerment, curiosity) systematically lead universal AI agents to seek and sus- tain high-optionality states. We prove that Self-AIXI asymptotically converges to the same performance as AIXI under suitable conditions, and highlight that its power-seeking behavior emerges naturally from both reward maximization and curiosity-driven exploration. Since AIXI can be view as a Bayes-optimal math- ematical formulation for Artificial General Intelligence (AGI), our result can be useful for further discussion on AI safety and the controllability of AGI.", "sections": [{"title": "1 Introduction", "content": "Designing an autonomous reinforcement learning (RL) agent that is both Bayes-optimal and ex- ploratory poses significant theoretical and practical challenges. Hutter's seminal AIXI frame- work [2] represents the gold standard of universal intelligence: given a suitable prior over all com- putable environments, AIXI maximizes reward in a provably optimal sense. However, it uses exhaus- tive planning and Solomonoff induction, which are computationally intractable. Realistic agents must therefore learn approximate models to scale beyond trivial tasks.\nA key question then arises: how can a learning-based agent (an approximation to AIXI) ensure sufficiently robust exploration so that it does not miss the optimal policy? While AIXI's exhaus- tive lookahead implicitly solves exploration by evaluating all possible future trajectories, a practical agent cannot feasibly do the same. Without a principled mechanism for seeking sufficiently diverse experiences, the agent may get stuck in suboptimal regions of the environment.\nVariational empowerment [3, 4, 5, 6] has recently emerged as a powerful intrinsic motivation to drive exploration. It encourages an agent to maximize the mutual information between its actions (or latent codes) and resulting states, thereby pushing the agent to discover states where it has high control and optionality. Intriguingly, maximizing empowerment often manifests as power-seeking in the environment, prompting parallels to resource-acquiring or influence-driven behaviors in human organizations. While this can be an asset for efficient exploration, it also highlights potential safety concerns: a sufficiently advanced agent might over-optimize this drive in ways that conflict with human interests [7, 8, 9].\nIn this paper, we extend the recently proposed Self-AIXI framework [1] by showing that its exist- ing mixture-policy regularization term can be reinterpreted as a variational empowerment bonus. Additionally, we provide two new theoretical contributions:\n(1) We demonstrate explicitly, through two key equations, that AIXI's decision criterion is\nmathematically equivalent to minimizing expected variational free energy, the core\nobjective in Active Inference [10, 11, 12]. This shows that AIXI-like Bayes-optimal plan- ning inherently includes a drive to reduce uncertainty about the environment (i.e. curiosity), thus unifying AIXI with the \"goal-directed + information-seeking\" paradigm of Active In- ference.\n(2) We formally argue that power-seeking can be explained not only as an instrumental\npursuit of final reward, but also as a direct result of empowerment maximization (i.e. curiosity-driven exploration). Even absent an immediate reward advantage, the agent ac- quires power (i.e. broad control over states and options) as a natural consequence of seek- ing to reduce uncertainty and maintain high optionality. This stands in contrast to prior accounts, e.g. Turner et al. [8] and Cohen et al. [9], which focus on final reward maximiza- tion as the source of an agent's incentive to obtain power.\nThough our study is primarily theoretical (no empirical experiments are presented), these results provide a fresh perspective on how intrinsic motivation can fill the gap between purely planning- based universal agents and tractable learning agents."}, {"title": "1.1 Background on AIXI and Self-AIXI", "content": "Bayesian Optimal Reinforcement Learning. Hutter's AIXI [2] is a Universal Bayesian RL agent that, in principle, can optimally maximize cumulative reward in any computable environment. It maintains a mixture (the universal semimeasure) over all possible environment hypotheses, updates these hypotheses upon observing new data, and plans by expectimax over all action sequences. For- mally, if $h_{<t}$ denotes the history (observations, actions, and rewards) up to time t, AIXI selects the action\n$a_t \\stackrel{\\mathrm{def}}{=} \\underset{a}{\\mathrm{arg\\,max}} \\sum_{v \\in M} w(v \\mid h_{<t}) Q^{*}_v(h_{<t}, a),$ (1)\nwhere each $v$ is an environment in a suitable class of computable Markov decision processes, $w(\u03bd \\mid h_{<t})$ is the posterior weight of $v$, and $Q^{*}_v$ is the optimal Q-value under environment $v$. Also,\n$\\pi_v(a_t \\mid h_{<t}) \\stackrel{\\mathrm{def}}{=} \\underset{a}{\\mathrm{arg\\,max}} Q_v(h_{<t}, a)$ (2)\nwhere $Q_v(h_{<t}, a)$ represents the optimal action-value function (Q-value) under the environment model $\u00a7$. Specifically,\n$Q_v(h_{<t}, a_t) = \\sum_{e_t \\in E} p(e_t | h_{<t}, a_t) \\left(r_t + \\gamma V^{*}_v(h_{<t}) \\right),$ (3)\nwith\n$V^{*}_v(h_{<t}) = \\underset{a}{\\mathrm{max}} Q_v(h_{<t}, a).$ (4)\nAIXI thus selects the action maximizing this Q-value.\nSoftmax Policy Interpretation. Sometimes, we transform the arg max over $Q^{*}$ into a softmax over actions:\n$p(a \\mid h_{<t}) \\stackrel{\\mathrm{def}}{=} \\frac{\\exp{\\left(Q^{*}(h_{<t}, a)\\right)}}{\\sum_{a' \\in A} \\exp{\\left(Q^{*}(h_{<t}, a')\\right)}}$ (5)"}, {"title": "", "content": "so that the log-likelihood of action a is:\n$\\ln p(a \\mid h_{<t}) = Q^{*}(h_{<t}, a) - \\ln \\sum_{a'} \\exp \\left(Q^{*}(h_{<t}, a')\\right),$ (6)\nand\n$\\underset{a}{\\mathrm{arg\\,max}} \\ln p(a \\mid h_{<t}) = \\underset{a}{\\mathrm{arg\\,max}} Q^{*}(h_{<t}, a).$ (7)\nHence maximizing $Q^{*}$ is equivalent to maximizing log-likelihood of a. We can rewrite an \u201cAIXI objective\" as a likelihood:\n$L_{\\mathrm{AIXI}} \\stackrel{\\mathrm{def}}{=} - \\mathbb{E}_{a \\sim p} \\left[\\ln p\\left(a \\mid h_{<t}\\right)\\right].$ (8)\nAlthough provably optimal in a Bayesian sense, AIXI is computationally infeasible: it sums over infinitely many models and searches over all possible action sequences. Nonetheless, the theory behind AIXI is highly influential: it shows that if the true environment \u03bc has nonzero prior proba- bility, AIXI eventually behaves optimally in \u03bc. It also satisfies the self-optimizing property in many environments and achieves the maximal Legg-Hutter intelligence score [13].\nSelf-AIXI as a Learning-Centric Approximation. Self-AIXI, introduced in [1], is a learning- based approach to approximate AIXI's policy without exhaustive search. Instead of planning over all future action sequences, Self-AIXI predicts its own future behavior given the current policy. Concretely, it maintains a Bayesian mixture of policies, (, with posterior updates based on how accurately each candidate policy predicts the agent's actions:\n$\\zeta(a_t \\mid h_{<t}) = \\sum_{\\pi \\in P} \\omega(\\pi \\mid h_{<t}) \\pi(a_t \\mid h_{<t}),$ (9)\nwhere $w(\u03c0 \\mid h_{<t})$ is updated via Bayes' rule after each action. The Q-values are estimated via experience rather than full expectimax. Formally, one can write a self-consistent objective\n$\\pi_{\\zeta}(a_t \\mid h_{<t}) \\stackrel{\\mathrm{def}}{=} \\underset{a}{\\mathrm{arg\\,max}} \\left\\{Q_{\\xi}(h_{<t}, a) - \\lambda \\ln \\frac{\\pi^{*}(a | h_{<t})}{\\zeta (a | h_{<t})} \\right\\},$ (10)\nwhere\n$Q_{\\xi}(h_{<t}, a_t) \\stackrel{\\mathrm{def}}{=} \\sum_{\\pi \\in P} \\omega(\\pi / h_{<t}) \\sum_{v \\in M} w(v/ h_{<t})Q^{\\pi}_{v}(h_{<t}, a_t),$ (11)\nwhere $Q_{\\xi}$ combines environment predictions $\u00a7$ with the mixture policy (, and $\\ln{\\frac{\\pi^{*}(a | h_{<t})}{\\zeta (a | h_{<t})}}$ is a regularization measure encouraging ( to approach the optimal policy \u03c0*. Note that the KL term serves as a regularization that nudges ( toward \u03c0*. This formulation generalizes the simpler case in [1] by allowing > > 0 (See A.2). If X = 0, we recover the original Self-AIXI objective without explicit KL regularization.\nSoftmax Policy Interpretation. We transform the arg max over $Q_{\\xi}$ into a softmax over actions:\n$q_{\\zeta}(a \\mid h_{<t}) \\stackrel{\\mathrm{def}}{=} \\frac{\\exp{\\left(Q_{\\xi}(h_{<t}, a)\\right)}}{\\sum_{a' \\in A} \\exp{\\left(Q_{\\xi}(h_{<t}, a')\\right)}},$ (12)\nso that the log-likelihood of action a is:\n$\\ln q_{\\zeta}(a \\mid h_{<t}) = Q_{\\xi}(h_{<t}, a) - \\ln \\sum_{a'} \\exp \\left(Q_{\\xi}(h_{<t}, a')\\right),$ (13)\nand\n$\\underset{a}{\\mathrm{arg\\,max}} \\ln q_{\\zeta}(a \\mid h_{<t}) = \\underset{a}{\\mathrm{arg\\,max}} Q_{\\xi}(h_{<t}, a).$ (14)\nFinally, we can rewrite an \"Self-AIXI objective\" as a likelihood:\n$L_{\\text {Self-AIXI }} \\stackrel{\\mathrm{def}}{=}-\\mathbb{E}_{a \\sim q_{\\zeta}}\\left[\\ln q_{\\zeta}(a \\mid h_{<t})\\right]+\\lambda D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right) .$ (15)"}, {"title": "1.2 Variational Empowerment for Intrinsic Exploration", "content": "While AIXI implicitly explores via its unbounded search over hypotheses, any tractable approxima- tion (like Self-AIXI) requires an explicit exploration mechanism. We adopt Variational Empower- ment [3, 4, 5, 6] as an intrinsic reward to drive the agent toward high-control states. This perspective aligns with recent work [14] in which empowerment is used not only for exploration but also as a mechanism for discovering useful latent representations or skills, potentially complementing goal- based RL approaches.\n1.2.1 Formal Definition of Empowerment\n$\\stackrel{\\mathrm{def}}{=}$\nEmpowerment is often defined as the maximal mutual information between an agent's actions and future states. For a horizon k, let $z_k \\stackrel{\\mathrm{def}}{=} a_{t:t+k-1}$ be a sequence of actions and $h_{t:t+k}$ the resulting state; then\n$I(z_k; h_{<t+k}) \\stackrel{\\mathrm{def}}{=} \\underset{p}{\\mathrm{max}} I \\left(z_k; h_{<t+k} \\mid h_{<t}\\right),$ (16)\n$= \\underset{p}{\\mathrm{max}} \\mathbb{E}_{z, h \\sim p} \\left[\\ln \\frac{p\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t}\\right)}\\right].$ (17)\nThe agent is empowered in states $h_t$ where it can produce a wide variety of distinguishable future outcomes through its choice of action-sequence. Exact computation is generally intractable in large state spaces, so one uses a variational approximation. For instance, we introduce a parameterized distribution $q_{\\phi}$ that approximates the posterior $q_{\\phi}(z_k | h_{<t+k})$, and then maximize [6]:\n$E(z_k; h_{<t+k}) \\stackrel{\\mathrm{def}}{=} \\underset{\\phi}{\\mathrm{max}} \\mathbb{E}_{\\xi}\\left(z_k; h_{<t+k} \\mid h_{<t}\\right),$ (18)\n$= \\underset{\\phi}{\\mathrm{max}} \\mathbb{E}_{z, h \\sim p} \\left[\\ln \\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{P\\left(z_k \\mid h_{<t}\\right)}\\right]$ (19)\nUsing Eqs. (7) and (14), we have:\n$\\mathbb{E}_{z, h \\sim p} \\ln p\\left(z_k \\mid h_{<t+k}\\right) = \\mathbb{E} \\left[\\prod_{i=0}^{k-1} \\pi^{*}\\left(a_{t+i} \\mid h_{<t+i}\\right),\\right],$ (20)\n$q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right) = \\mathbb{E} \\left[\\prod_{i=0}^{k-1} \\zeta\\left(a_{t+i} \\mid h_{<t+i}\\right),\\right],$ (21)\n$\\left.\\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t+k}\\right)}\\right] = \\mathbb{E}_{h \\sim p} \\left[\\frac{\\ln p(z_k|h_{<t+k})}{\\ln q_{\\phi}(z_k | h_{<t+k})} \\right] = -\\mathbb{E}_{h \\sim p} \\left[\\sum_{i=0}^{k-1} D_{\\mathrm{KL}}\\left(\\pi^{k} \\|\\| \\zeta_{i}\\right) \\right]$ (22)\nThe right-hand side of Eq. (22), $D_{\\mathrm{KL}}\\left(\\pi^{k} \\|\\| \\zeta_{i}\\right)$, is a regularization term in the Self-AIXI framework\n$\\frac{\\pi^{*}(a | h_{<t})}{\\zeta (a | h_{<t})}$ that pushes the agent's mixture policy $\\zeta_i \\stackrel{\\mathrm{def}}{=} q_{\\phi} \\left(a_{t+i} \\mid h_{<t+i}\\right)$ to imitate or approach the optimal policy $\\pi \\stackrel{\\mathrm{def}}{=} p \\left(a_{t+i} \\mid h_{<t+i}\\right)$. As the agent learns from experience, it reduces this divergence, effectively self-optimizing its policy.\n$\\stackrel{\\mathrm{def}}{=}$\nHence, Eqs. (17) and (22) allow us to rewrite the Variational Empowerment as:\n$\\begin{aligned}E\\left(z_k ; h_{<t+k}\\right) & = \\underset{\\phi}{\\operatorname{max}} \\mathbb{E}_{z, h \\sim p} \\left[\\ln \\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t+k}\\right)}+\\ln \\frac{p\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t}\\right)}\\right] \\\\ & = \\underset{\\phi}{\\operatorname{max}} \\mathbb{E}_{h \\sim p}\\left[-\\sum_{i=0}^{k-1} D_{\\mathrm{KL}}\\left(\\pi \\|\\| \\zeta_{i}\\right)+I\\left(z_k ; h_{<t+k}\\right) .\\right]\\end{aligned}$ (23) (24)"}, {"title": "1.3 Connecting to Free-Energy Minimization and Active Inference", "content": "Bayesian RL as Active Inference. Bayesian RL connects closely to Active Inference [10, 11, 12], where an agent maintains a prior over latent variables and updates its posterior after observing re- wards or other feedback. Under a Free Energy Principle (FEP), one often writes a free-energy functional:\n$F\\left(z_k ; h_{<t+k}\\right) \\stackrel{\\mathrm{def}}{=} D_{\\mathrm{KL}}\\left(P\\left(z_k, h_{<t+k} \\mid h_{<t}\\right) \\|\\| q_{\\phi}\\left(z_k, h_{<t+k} \\mid h_{<t}\\right)\\right),$ (25)\n$\\left.\\mid h_{<t}\\right)\right]$ (26)\nHere, $\\mathbb{E}_{h \\sim p}\\left[\\ln q_{\\phi}\\left(h_{<t+k} \\mid z_k, h_{<t}\\right)\\right]$ is the predictive error (surprise), and the remaining term measures how far $q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)$ diverges from $p\\left(z_k \\mid h_{<t}\\right)$.\nDecomposition of Regularization term. Under suitable rearrangements or sign conventions, we can identify a Regularization part that can be maximized rather than minimized, yielding empower- ment:\n$\\begin{aligned} & \\mathbb{E}_{z, h \\sim p}\\left[-\\ln \\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t}\\right)}\\right] = \\mathbb{E}_{z, h \\sim p}\\left[-\\ln \\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{P\\left(z_k \\mid h_{<t+k}\\right)}+\\ln \\frac{p\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t}\\right)}\\right]\n\\end{aligned}$ (27)\n$\\left.D_{\\mathrm{KL}}\\left(\\pi \\|\\| \\zeta_{i}\\right)+I\\left(z_k ; h_{<t+k} \\mid h_{<t}\\right) .\\right]$ (28)\nHence, turning the regularization term \u201cupside down\" (from negative to positive) motivates Varia- tional Empowerment:\n$E\\left(z_k ; h_{<t+k}\\right) \\stackrel{\\mathrm{def}}{=} \\underset{\\phi}{\\operatorname{min}} \\mathbb{E}_{z, h \\sim p}\\left[\\ln \\frac{q_{\\phi}\\left(z_k \\mid h_{<t+k}\\right)}{p\\left(z_k \\mid h_{<t}\\right)}\\right],$ (29)\n$\\begin{aligned} & = \\underset{\\phi}{\\operatorname{max}} \\mathbb{E}_{h \\sim p}\\left[-\\sum_{i=0}^{k-1} D_{\\mathrm{KL}}\\left(\\pi \\|\\| \\zeta_{i}\\right)+I\\left(z_k ; h_{<t+k}\\right) .\\right]\\end{aligned}$ (30)\nmirroring the definitions in Eq. (24) above."}, {"title": "2 Universal AI maximizes Variational Empowerment", "content": "In the Universal Artificial Intelligence (UAI) framework [2], an agent is considered universal if it can, given sufficient time, match or surpass any other policy's performance in all computable environments (with nonzero prior). AIXI achieves this in theory. Self-AIXI aims to achieve it in practice, provided it can explore effectively. Below, we summarize how our empowered Self-AIXI fits these formal criteria.\n2.1 Asymptotic Equivalence, Legg-Hutter Intelligence, and Self-Optimizing Property\nPrior work [1] proves that if the Self-AIXI agent's policy class and environment prior are sufficiently expressive (i.e. the true environment is in the hypothesis class with nonzero probability), then the agent's behavior converges to that of AIXI's optimal policy in the limit of infinite interaction. For- mally, for any environment \u03bc in the model class,\n$\\lim _{t \\rightarrow \\infty} \\mathbb{E}_{s}^{\\mu} \\mathbb{E}_{s'}^{\\pi}\\left[V_{t}^{*}\\left(h_{<t}\\right)-V_{t}^{\\pi}\\left(h_{<t}\\right)\\right] = 0 .$ (31)\nwhich implies that, asymptotically, the agent's expected return under \u03c0, matches that of the optimal policy $V^{*}$. Intuitively, as the agent's world-model becomes more accurate, it exploits the optimal policy; hyperparameters (such as A in an empowerment term) can be tuned or annealed so that extrinsic reward eventually dominates.\nFrom the perspective of Legg-Hutter intelligence [13], which associates an agent's \u201cintelligence\" with its expected performance across a suite of weighted environments, this result is especially significant. Because Self-AIXI asymptotically reproduces AIXI's policy, it inherits maximal Legg- Hutter intelligence within that class of environments. Moreover, in a wide class of self-optimizing environments, the agent will ultimately achieve the same returns as an optimal agent with perfect knowledge would achieve, under the same conditions in Eq. (31). These guarantees illustrate that the enhanced exploration mechanisms\u2014such as empowerment-driven strategies\u2014do not compro- mise eventual performance. Instead, they help ensure the agent uncovers the environment's true optimal actions without becoming trapped in suboptimal behaviors due to insufficient data. Conse- quently, the agent retains AIXI's universal optimality in the limit while mitigating early exploration challenges.\n2.2 Self-Optimization leads Empowerment Maximization\nThe agent's process of improving its policy (often referred to as self-optimization) naturally leads to an increase in Variational Empowerment. In fact, as reinforcement learning progresses, both AIXI's objective function $L_{AIXI}$ and Self-AIXI's objective function $L_{\\text{Self-AIXI}}$ gradually converge, and they coincide in the limit $t \\rightarrow \\infty$.\nThe difference between these two objectives can be expressed through the policy regularization term $D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right)$. Formally, we have:\n$\\lim _{t \\rightarrow \\infty} L_{\\mathrm{AIXI}}-L_{\\text {Self-AIXI }}=\\lim _{t \\rightarrow \\infty} \\lambda D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right)=0 .$ (32)\nThis result implies that $D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right)$ goes to 0 as $t \\rightarrow \\infty$, which is equivalent to the Self-AIXI's variational empowerment $E_{\\phi}(z_k ; h_{<t+k})$ being maximized. In other words, the universal AI agent AIXI, which behaves in a Bayes-optimal way with respect to the environment, also emerges as an agent that maximizes empowerment.\nConcretely, the relationship between the empowerment objective and the policy regularization is succinctly captured by the following equality:\n$E(z_k ; h_{<t+k}) = \\underset{\\phi}{\\operatorname{max}} \\mathbb{E}_{h \\sim p}\\left[-\\sum_{i=0}^{k-1} D_{\\mathrm{KL}}\\left(\\pi \\|\\| \\zeta_{i}\\right)+I\\left(z_k ; h_{<t+k}\\right) .\\right].$ (33)\nSelf-optimization refers to the iterative improvement of the agent's policy based on observed re- wards and outcomes. In Self-AIXI, reducing the policy regularization term $D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right)$ directs ( closer to \u03c0*. Because the left-hand side of the second formula above equals the variational empower- ment $E_{\\phi}(z_k ; h_{<t+k})$, each step that lowers $D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right)$ raises $E_{\\phi}(z_k ; h_{<t+k})$. Empowerment here signifies how many high-control or high-optionality states are accessible to the agent. Consequently, maximizing reward often requires seeking out exactly those states in which the agent can maintain or expand control\u2014thus also maximizing $E_{\\phi}(z_k ; h_{<t+k})$. As ( becomes more similar to \u03c0*, the agent naturally discovers strategies that grant more control and flexibility. Therefore, under Self- AIXI, improving the policy toward optimal behavior simultaneously yields higher external rewards and amplifies the agent's own empowerment."}, {"title": "3 Conclusions", "content": "In this work, we reinterpreted a term in Self-AIXI as variational empowerment\u2014intrinsic explo- ration bonus. We have argued that:\n\u2022 Empowerment naturally complements Bayesian RL in universal settings, providing a struc- tured incentive to discover controllable states and gather broad experience.\n\u2022 Even with an empowerment bonus, the agent asymptotically recovers AIXI's Bayes- optimal policy, inheriting the same universal intelligence and self-optimizing properties in the limit.\nOne of our main observations is that the agent's pursuit of high-empowerment states often manifests as a power-seeking tendency. Traditionally, many authors (e.g., Turner et al. [8]) interpret power- seeking as purely instrumental: an agent acquires resources, avoids shutdown, or manipulates the reward channel to better guarantee high external returns. However, we show that power-seeking can also arise intrinsically from a drive to reduce uncertainty and maintain a wide range of feasible actions (i.e., \"keeping options open\"). Imagine an agent choosing between a high-control region (with many possible actions and partial knowledge) and a low-control region (with fewer actions and less information). If both yield the same short-term reward, a purely extrinsic approach might be indifferent. By contrast, an empowerment-seeking agent prefers the high-control region, as it offers greater potential for discovering valuable future strategies. Over time, as the agent learns more about its environment, these benefits accumulate.\nWhen not moderated, power-seeking behaviors may conflict with human interests. For instance, maximizing control can lead to manipulative or exploitative outcomes if the agent's intrinsic or extrinsic goals are misaligned with social values. From a Universal AI standpoint, understanding that power-seeking can stem from both instrumental and intrinsic (empowerment-based) motives is crucial to designing mechanisms-e.g., safe exploration techniques or alignment constraints-to ensure that an agent's influence remains beneficial. It is important to note that these concerns apply even to Al agents with apparently benign objectives, such as an AI scientist pursuing scientific truth purely out of intellectual curiosity.\nOur results are primarily conceptual and rest on idealized assumptions: (1) the environment is in the agent's hypothesis class with nonzero prior; (2) we assume unbounded computational resources and memory; (3) the agent can tractably approximate empowerment. In reality, computing exact empowerment or using universal priors is challenging. Empirical methods to approximate these ideas (e.g., neural networks [4]) remain an active area of research."}, {"title": "A Appendix", "content": "A.1 Notation and Further Details\nWe summarize the main notation in Table 1 for reference.\nA.2 Self-AIXI's self-consistent objective\nIn this subsection, we investigate how introducing a Kullback\u2013Leibler (KL) divergence-based regu- larization term into Self-AIXI affects its convergence properties and whether it preserves the agent's ability to reach the optimal policy \u03c0* asymptotically. Specifically, we consider the effect of adding a penalty that measures how far the current mixture policy ( deviates from \u03c0*.\nRecall that for a given history h<t, the KL divergence between the optimal policy \u03c0* and the current mixture policy ( is defined as:\n$D_{\\mathrm{KL}}\\left(\\pi^{*} \\|\\| \\zeta\\right) = \\sum_{a' \\in A} \\sum_{\\tau} \\pi^{*}\\left(a' \\mid h_{<t}\\right) \\ln \\frac{\\pi^{*}\\left(a' \\mid h_{<t}\\right)}{\\zeta \\left(a' \\mid h_{<t}\\right)}$ (34)\nWe then propose a policy update rule that augments the standard Self-AIXI greedy step with a log- likelihood ratio term:\n$\\pi_{\\zeta}(a_t | h_{<t}) \\stackrel{\\mathrm{def}}{=} \\underset{a}{\\mathrm{arg\\,max}} \\left\\{Q(h_{<t}, a) - \\lambda \\ln \\frac{\\pi^{*}(a | h_{<t})}{\\zeta (a | h_{<t})} \\right\\},$ (35)\nwhere Q denotes the estimated value of taking action a in history h<t under the Bayesian mixture environment \u00a7 and current policy \u03b6. Here, \u03bb > 0 is included to penalize large deviations from \u03c0* whenever the agent's mixture policy ( differs substantially from the (unknown) optimal policy \u03c0*."}, {"title": "", "content": "KL regularization and recovery of the standard update. In many practical scenarios, such as when \u03c0* is deterministic or assigns a high probability to a single action, part of the KL term can be constant with respect to a. Under those conditions, the penalty term\n$\\lambda \\ln \\frac{\\pi^{*}\\left(a' \\mid h_{<t}\\right)}{\\zeta \\left(a' \\mid h_{<t}\\right)}$ (36)\ndoes not vary across actions, and the update rule simplifies to\n$\\pi_{\\zeta}\\left(a_t \\mid h_{<t}\\right)=\\underset{a}{\\operatorname{argmax}}\\left\\{Q(h_{<t}, a)\\right\\},$ (37)\nwhich recovers the conventional (un-regularized) Self-AIXI greedy update.\nImpact on learning dynamics and convergence. Although the added term changes the action selection criterion, it does not alter the identity of the optimal policy in the underlying environment. Intuitively, the new rule can be viewed as performing a more conservative or \"trust-region\"-like update, since actions that the agent's current policy overestimates relative to \u03c0* will be penalized more strongly. Conversely, if ( assigns too little probability to actions that \u03c0* actually favors, the negative logarithm of their ratio produces a smaller (or positive) correction. Hence, the agent is nudged toward \u03c0*.\nCrucially, the regularization term does not disrupt Self-AIXI's standard convergence guarantees, assuming the original conditions hold (e.g., that the true environment is in the Bayesian mixture class \u00a7 and \u03c0* is in the agent's policy class). From a theoretical perspective, \u03c0* remains a stable fixed point under this augmented objective. Once ( converges to \u03c0*, the log ratio\n$\\ln \\frac{\\pi^{*}(a | h_{<t})}{\\zeta (a | h_{<t})}$ (38)\nvanishes for actions with nonzero probability under \u03c0*, so no extra penalty is incurred, and the update aligns with the optimal policy's greedy choice.\nRegularization coefficient and transient behavior. The coefficient > < 0 determines the strength of the penalty term:\n\u2022 Small, moderate penalty (|X|small ). A suitably chosen, relatively small magnitude for || works as a gentle regularizer, smoothing the agent's updates by discouraging drastic shifts in policy. This can stabilize learning and reduce oscillations without harming final conver- gence. Indeed, theoretical analyses of KL-based regularization in reinforcement learning show that while such shaping modifies the transient policy updates, the optimal policy re- mains the same in the limit.\n\u2022 Overly large penalty (large). If the KL term is emphasized too strongly, the agent may stick too closely to its current guess of \u03c0* and under-explore other actions. Early in learn- ing-when is still inaccurate this could delay or even misdirect policy improvement. However, as Self-AIXI continuously updates its environment belief (via \u00a7) and revises \u03b6, the agent still accumulates evidence about which actions are actually optimal, making it dif- ficult to remain indefinitely biased toward a suboptimal policy. Practical implementations often tune A to ensure that exploration is maintained."}]}