{"title": "INTERPRETABLE CONTRASTIVE MONTE CARLO TREE SEARCH REASONING", "authors": ["Zitian Gao", "Boye Niu", "Xuzheng He", "Haotian Xu", "Hongzhang Liu", "Aiwei Liu", "Xuming Hu", "Lijie Wen"], "abstract": "We propose (S)peculative (C)ontrastive MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning algorithm for Large Language Models (LLMs), significantly improves both reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM reasoning works often overlooked its biggest drawback-slower speed compared to CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on various tasks with limited quantitative analysis or ablation studies of its components from reasoning interpretability perspective. 3. The reward model is the most crucial component in MCTS, however previous work has rarely conducted in-depth study or improvement of MCTS's reward models. Thus, we conducted extensive ablation studies and quantitative analysis on components of MCTS, revealing the impact of each component on the MCTS reasoning performance of LLMs. Building on this, (i) we designed a highly interpretable reward model based on the principle of contrastive decoding and (ii) achieved an average speed improvement of 51.9% per node using speculative decoding. Additionally, (iii) we improved UCT node selection strategy and backpropagation used in previous works, resulting in significant performance improvement. We outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B with SC-MCTS*.", "sections": [{"title": "INTRODUCTION", "content": "With the remarkable development of Large Language Models (LLMs), models such as o1 (OpenAI, 2024a) have now gained strong ability to multi-step reasoning across complex tasks and solve problems that are more difficult than previous scientific, code, and mathematical problems. The reasoning task has long been considered challenging for LLMs. These tasks require converting a problem into a series of reasoning steps and then executing those steps to arrive at the correct answer. Recently, LLMs have shown great potential in addressing such problems. A key approach is using Chain of Thought (CoT) (Wei et al., 2024), where LLMs break down the solution into a series of reasoning steps before arriving at the final answer. Despite the impressive capabilities of CoT-based LLMs, they still face challenges when solving problems with an increasing number of reasoning steps due to the curse of autoregressive decoding (Sprague et al., 2024). Previous work has explored reasoning through the use of heuristic reasoning algorithms. For example, Yao et al. (2024) applied heuristic-based search, such as Depth-First Search (DFS) to derive better reasoning paths. Similarly, Hao et al. (2023) employed MCTS to iteratively enhance reasoning step by step toward the goal.\n\nThe tremendous success of AlphaGo (Silver et al., 2016) has demonstrated the effectiveness of the heuristic MCTS algorithm, showcasing its exceptional performance across various domains (Jumper et al., 2021; Silver et al., 2017). Building on this, MCTS has also made notable progress in the field of LLMs through multi-step heuristic reasoning. Previous work has highlighted the potential of heuristic MCTS to significantly enhance LLM reasoning capabilities. Despite these advancements, substantial challenges remain in fully realizing the benefits of heuristic MCTS in LLM reasoning."}, {"title": "RELATED WORK", "content": "Large Language Models Multi-Step Reasoning One of the key focus areas for LLMs is un-derstanding and enhancing their reasoning capabilities. Recent advancements in this area focused on developing methods that improve LLMs' ability to handle complex tasks in domains like code generation and mathematical problem-solving. Chain-of-Thought (CoT) (Wei et al., 2024) reasoning has been instrumental in helping LLMs break down intricate problems into a sequence of manageable steps, making them more adept at handling tasks that require logical reasoning. Building upon this, Tree-of-Thought (ToT) (Yao et al., 2024) reasoning extends CoT by allowing models to explore multiple reasoning paths concurrently, thereby enhancing their ability to evaluate different solutions simultaneously. Complementing these approaches, Monte Carlo Tree Search (MCTS) has emerged as a powerful reasoning method for decision-making in LLMs. Originally successful in AlphaGo's victory (Silver et al., 2016), MCTS has been adapted to guide model-based planning by balancing ex-ploration and exploitation through tree-based search and random sampling, and later to large language model reasoning (Hao et al., 2023), showing great results. This adaptation has proven particularly effective in areas requiring strategic planning. Notable implementations like ReST-MCTS* (Zhang et al., 2024a), rStar (Qi et al., 2024), MCTSr (Zhang et al., 2024b) and Xie et al. (2024) have shown that integrating MCTS with reinforced self-training, self-play mutual reasoning or Direct Preference Optimization (Rafailov et al., 2023) can significantly improve reasoning capabilities in LLMs. Furthermore, recent advancements such as Deepseek Prover (Xin et al., 2024a;b) demonstrates the potential of these models to understand complex instructions such as formal mathematical proof.\n\nDecoding Strategies Contrastive decoding and speculative decoding both require Smaller Language Models (SLMs), yet few have realized that these two clever decoding methods can be seamlessly combined without any additional cost. The only work that noticed this was Yuan et al. (2024a), but their proposed speculative contrastive decoding focused on token-level decoding. In contrast, we designed a new action-level contrastive decoding to guide MCTS reasoning, the distinction will be discussed further in Section 4.1."}, {"title": "PRELIMINARIES", "content": "3.1 MULTI-STEP REASONING\n\nA multi-step reasoning problem can be modeled as a Markov Decision Process (Bellman, 1957) $M = (S, A, P, r, \\gamma)$. S is the state space containing all possible states, A the action space, $P(s'|s, a)$ the state transition function, $r(s, a)$ the reward function, and $\\gamma$ the discount factor. The goal is to learn and to use a policy $\\pi$ to maximize the discounted cumulative reward $E_{\\tau \\sim \\pi} [\\sum_{t=0}^{T} \\gamma^{t} r_{t}]$. For reasoning with LLMs, we are more focused on using an existing LLM to achieve the best reasoning.\n\n3.2 MONTE CARLO TREE SEARCH\n\nMonte Carlo Tree Search (MCTS) is a decision-making algorithm involving a search tree to simulate and evaluate actions. The algorithm operates in the following four phases:\n\nNode Selection: The selection process begins at the root, selecting nodes hierarchically using strategies like UCT as the criterion to favor a child node based on its quality and novelty.\n\nExpansion: New child nodes are added to the selected leaf node by sampling $d$ possible actions, predicting the next state. If the leaf node is fully explored or terminal, expansion is skipped.\n\nSimulation: During simulation or \"rollout\", the algorithm plays out the \"game\" randomly from that node to a terminal state using a default policy.\n\nBackpropagation: Once a terminal state is reached, the reward is propagated up the tree, and each node visited during the selection phase updates its value based on the simulation result."}, {"title": "CONTRASTIVE DECODING", "content": "We discuss vanilla Contrastive Decoding (CD) from Li et al. (2023), which improves text generation in LLMs by reducing errors like repetition and self-contradiction. CD uses the differences between an expert model and an amateur model, enhancing the expert's strengths and suppressing the amateur's weaknesses. The CD objective is defined as:\n\n$L_{CD}(X_{cont}, X_{pre}) = log P_{EXP}(X_{cont}|X_{pre}) - log P_{AMA}(X_{cont}|X_{pre})$\n\nwhere $P_{EXP}$ and $P_{AMA}$ are the expert and amateur probability distributions.\n\nTo avoid penalizing correct behavior of the amateur or promoting implausible tokens, CD applies an adaptive plausibility constraint using an a-mask, which filters tokens by their logits against a threshold:\n\n$V_{valid} = {i | S_{EXP}^{(i)} > log \\alpha + max_{k} S_{EXP}^{(k)} }$\n\nFinal logits are adjusted with a coefficient (1 + \u03b2), modifying the contrastive effect on output scores (Liu et al., 2021):\n\n$S^{(i)} = (1 + \\beta)S_{EXP}^{(i)} - S_{AMA}^{(i)}$\n\nHowever, our proposed CD is at action level, averaging over the whole action, instead of token level in vanilla CD. Our novel action-level CD reward more robustly captures the differences in confidence between the expert and amateur models in the generated answers compared to vanilla CD. The distinction will be illustrated in Section 4.1 and explained further in Appendix A."}, {"title": "SPECULATIVE DECODING", "content": "Based on vanilla Speculative Decoding (Leviathan et al., 2023), the process can be summarized as follows: Let Mp be the target model we aim to accelerate, with $p(x_t|x_{<t})$ representing its distribution for a given prefix $x_{<t}$. Let Mq be a smaller, more efficient approximation model, with $q(x_t|x_{<t})$ representing its distribution. The key idea is to generate y new tokens using Mq and evaluate them against Mp, accepting those that align with Mp's distribution. Each evaluation of Mp on the new tokens which is parallel can produce at least one new token according to $p(x)$ in the end, with potentially many more tokens if they get accepted.\n\nSpecifically, speculative decoding first samples $x \\sim q(x)$ autoregressively y times and keep them as long as $q(x) \\leq p(x)$. If $q(x) > p(x)$ at some point, that sample is rejected with probability $1 - \\frac{p(x)}{q(x)}$, and a new sample is drawn from an adjusted distribution:\n\n$p'(x) = norm(max(0, p(x) - q(x)))$\n\nThis ensures that the final sample distribution always follows p(x). Since contrastive decoding and speculative decoding both require smaller language model, after employing our proposed action level contrastive decoding, we can achieve the acceleration effect of speculative decoding without additional cost (Yuan et al., 2024a)."}, {"title": "METHOD", "content": "The SC-MCTS* reasoning methodology is outlined as follows: In Section 4.1 details the multi-reward design; Section 4.2 discusses the UCT strategy for node selection; and Section 4.3 proposes the refinement of backpropagation."}, {"title": "MULTI-REWARD DESIGN", "content": "SC-MCTS* is guided by three highly interpretable reward models: contrastive JS divergence, log-likelihood and self evaluation. Previous work such as (Hao et al., 2023) often directly adds reward functions with mismatched numerical magnitudes without any prior statistical analysis or linear combination. As a result, their combined reward models may fail to demonstrate full performance."}, {"title": "NODE SELECTION STRATEGY", "content": "Upper Confidence Bound applied on Trees Algorithm (UCT) (Coquelin & Munos, 2007) is crucial for the selection phase, balancing exploration and exploitation by choosing actions that maximize:\n\n$UCT_j = \\overline{X_j} + C \\sqrt{\\frac{ln N}{N_j}}$\n\nwhere $\\overline{X_j}$ is the average reward of taking action j, N is the number of times the parent has been visited, and $N_j$ is the number of times node j has been visited for simulation, C is a constant to balance exploitation and exploration.\n\nHowever, C is a crucial part of UCT. Previous work (Hao et al., 2023; Zhang et al., 2024b) had limited thoroughly investigating its components, leading to potential failures of the UCT strategy. This is because they often used the default value of 1 from the original proposed UCT (Coquelin & Munos, 2007) without conducting sufficient quantitative experiments to find the optimal C. This will be discussed in detail in Section 5.4."}, {"title": "BACKPROPAGATION", "content": "After each MCTS iteration, multiple paths from the root to terminal nodes are generated. By backpropagating along these paths, we update the value of each state-action pair. Previous MCTS approaches often use simple averaging during backpropagation, but this can overlook paths where the goal achieved metric G(p) progresses smoothly (e.g., $G(p_1) = 0 \\rightarrow 0.25 \\rightarrow 0.5 \\rightarrow 0.75$). These paths just few step away from the final goal G(p) = 1, are often more valuable than less stable ones.\n\nTo improve value propagation, we propose an algorithm that better captures value progression along a path. Given a path P = {$p_1, p_2, ..., p_n$ } with n nodes, where each pi represents the value at node i, the total value is calculated by summing the increments between consecutive nodes with a length penalty. The increment between nodes pi and $p_{i-1}$ is $\\Delta_i = p_i - p_{i-1}$. Negative increments are clipped at -0.1 and downweighted by 0.5. The final path value $V_{final}$ is:\n\n$V_{final} =  \\sum_{i=2}^{n} \\{\\begin{array}{ll} \\Delta i, & \\text { if } \\Delta i > 0 \\\\ 0.5 \\times max(\\Delta i, -0.1), & \\text { if } \\Delta i < 0 \\end{array}  } - \\lambda \\times n$ \n\nwhere n is the number of nodes in the path and $\\lambda$ = 0.1 is the penalty factor to discourage long paths."}, {"title": "EXPERIMENTS", "content": "5.1 DATASET\n\nBlocksworld (Valmeekam et al., 2024; 2023) is a classic domain in AI research for reasoning and planning, where the goal is to rearrange blocks into a specified configuration using actions like 'pick-up,' 'put-down,' 'stack,' and 'unstack. Blocks can be moved only if no block on top, and only one block at a time. The reasoning process in Blocksworld is a MDP. At time step t, the LLM agent selects an action $a_t \\sim p(a | s_t, c)$, where st is the current block configuration, c is the prompt template. The state transition $s_{t+1} = P(s_t, a_t)$ is deterministic and is computed by rules. This forms a trajectory of interleaved states and actions ($s_0, A_0, S_1, A_1, ..., s_t$) towards the goal state.\n\nOne key feature of Blocksworld is its built-in verifier, which tracks progress toward the goal at each step. This makes Blocksworld ideal for studying heuristic LLM multi-step reasoning. However, we deliberately avoid using the verifier as part of the reward model as it is task-specific.\n\n5.2 MAIN RESULTS\n\nTo evaluate the SC-MCTS* algorithm in LLM multi-step reasoning, we implemented CoT, RAP-MCTS, and SC-MCTS* using Llama-3-70B and Llama-3.1-70B. For comparison, we used Llama-3.1-405B and GPT-4o for CoT, and applied 0 and 4 shot single turn for o1-mini, as OpenAI (2024b) suggests avoiding CoT prompting. The experiment was conducted on Blocksworld dataset across all steps and difficulties."}, {"title": "REASONING SPEED", "content": "As shown in Figure 3, we can observe that the combination of Llama-3.1-405B with Llama-3.1-8B achieves the highest speedup, improving inference speed by approximately 100% compared to vanilla decoding. Similarly, pairing Llama-3.1-70B with Llama-3.2-1B results in a 51.9% increase in reasoning speed. These two combinations provide the most significant gains, demonstrating that speculative decoding with SLMs can substantially enhance node level reasoning speed. However, we can also observe from the combination of Llama-3.1-405B with Llama-3.2-1B that the parameters of SLMs in speculative decoding should not be too small, since the threshold for accepting draft tokens during the decoding process remains fixed to prevent speculative decoding from affecting performance (Leviathan et al., 2023), as overly small parameters may have a negative impact on decoding speed, which is consistent with the findings in Zhao et al. (2024); Chen et al. (2023)."}, {"title": "ABLATION STUDY", "content": "As shown in Table 2, the results of the ablation study demonstrate that each component of SC-MCTS* contributes significantly to performance improvements. Starting from a base MCTS accuracy of 55.92%, adding RJSD, RLL, and RSE yields a combined improvement of 14.47%. Multi-RM method further boosts performance by 3.29%, while optimizing the C parameter in UCT adds 5.27%, and the backpropagation refinement increases accuracy by 1.97%. Overall, SC-MCTS* achieves an accuracy of 80.92%, a 25% improvement over the base, demonstrating the effectiveness of these enhancements for complex reasoning tasks."}, {"title": "INTERPRETABILITY STUDY", "content": "We observe two distinct types of reward value distributions: normal distribution and half-normal distribution (truncated normal distribution).\n\nNormal distribution is represented by RLL which is computed by accumulating the log probabilities over the token indices of the answer portion from the logits. This accumulation captures the probability trends of the entire answer generation, with a symmetric and concentrated distribution, resulting in a normal distribution. This distribution reflects the balance in the reasoning process and provides a stable and interpretable guide to MCTS multi-step reasoning.\n\nHalf-normal distribution includes RJSD and RSE. RJSD quantifies the difference in confidence between the expert model and the amateur model over the logits of the answer part. The moti-vation for designing this reward function stems from the principle of contrastive decoding where constraining the expert model with the amateur model significantly reduces the low-quality outputs of the expert (Li et al., 2023). RJSD introduces symmetry by averaging distributions defined as $\\frac{1}{M}(Logit_{S_{Expert}} + Logits_{Amateur})$, which differs from KL divergence. This symmetry combined with the non-negative nature of JS divergence also leads to a half-normal distribution. RJSD guide MCTS reasoning by measuring the discrepancy between the models' logits. Our ablation experiments in Section 5.5 demonstrate that RJSD is highly effective as a reward function, significantly improving the performance of multi-step reasoning.\n\nThe self-evaluation reward function computes the log probability of a single self evaluation token such as \"good\" to measure the model's confidence in generated answer (Ren et al., 2023). Since it only focuses on the log probability of single specific token and relies on the model's confidence in particular outputs, its distribution also takes the form of a half-normal distribution. This distribution reflects the uncertainty in the reasoning process especially at decision points with lower confidence.\n\nBy combining the observations from ablation experiments in Table 2 and Figure 5.6, we also find that reward functions whose distributions more closely resemble a normal or half-normal distribution tend to perform better. For example, RJSD, whose shape is more aligned with a half-normal distribution compared to RSE, also demonstrates better performance than RSE.\n\nFrom the ablation study results in Table 5.5, it can be seen that the reward model is the most critical factor for MCTS reasoning performance. Having well-interpretable reward models implies better interpretability of MCTS reasoning. By studying these highly interpretable rewards, we can gain a clearer and more explainable understanding of MCTS reasoning."}, {"title": "CONCLUSION", "content": "In this paper, we present SC-MCTS*, a novel and effective algorithm to enhancing the reasoning capabilities of LLMs. With extensive improvements in reward modeling, node selection strategy and backpropagation, SC-MCTS* boosts both accuracy and speed, outperforming OpenAI's o1-mini model by 17.4% on average using Llama-3.1-70B on the Blocksworld dataset. Experiments demon-strate its strong performance, making it a promising approach for multi-step reasoning tasks. For future work please refer to Appendix I. The synthesis of interpretability, efficiency and generalizability positions SC-MCTS* as a valuable contribution to advancing LLMs multi-step reasoning."}, {"title": "ACTION-LEVEL CONTRASTIVE REWARD", "content": "We made the distinction between action-level variables and token-level variables: action-level (or step-level) variables are those that aggregate over all tokens in a reasoning step, and is typically utilized by the reasoning algorithm directly; token-level variables, by contrast, operates in a more microscopic and low-level environment, such as speculative decoding.\n\nWe found that the traditional contrastive decoding using the difference in logits, when aggregated over the sequence gives a unstable reward signal compared to JS divergence. We suspected this is due to the unbounded nature of logit difference, and the potential failure modes associated with it that needs extra care and more hyperparameter tuning."}, {"title": "MORE RELATED WORK", "content": "Large Language Models Multi-Step Reasoning Deepseek Prover (Xin et al., 2024a;b) relied on Lean4 as an external verification tool to provide dense reward signals in the RL stage. ReST-MCTS* (Zhang et al., 2024a) employed self-training to collect high-quality reasoning trajectories for iteratively improving the value model. AlphaLLM (Tian et al., 2024) used critic models initialized from the policy model as the MCTS reward model. rStar (Qi et al., 2024) utilized mutual consistency of SLMs and an additional math-specific action space."}]}