{"title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act", "authors": ["Philipp Guldimann", "Alexander Spiridonov", "Robin Staab", "Nikola Jovanovi\u0107", "Mark Veros", "Velko Vechev", "Anna Gueorguieva", "Mislav Balunovi\u0107", "Nikola Konstantinov", "Pavol Bielik", "Petar Tsankov", "Martin Vechev"], "abstract": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models' compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act's obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice.", "sections": [{"title": "1 Introduction", "content": "The latest wave of generative AI has seen unprecedented adoption in recent years. The most notable is the rise of large language models (LLMs), especially following the public release of ChatGPT (OpenAI, 2022). Complementing the discourse around capabilities and new opportunities unlocked by these models, concerns were raised regarding their risks and negative societal impact from the perspectives of discrimination, privacy, security and safety. While some of these aspects are captured by existing regulations such as GDPR (EU, 2016), it was widely recognized that this new wave of AI breakthroughs requires a new wave of regulatory efforts, aiming to pave the way for safe, responsible, and human-centric development of AI systems.\nThe EU AI Act A flagship result of such efforts is the European Union's Artificial Intelligence Act (EU AI Act), voted in by the European Parliament on the 13th of March 2024 (EU, 2024). Most notably, EU AI Act recognizes models and systems with unacceptable risk, banning their development and deployment, and high risk, such as those deployed in education or critical infrastructure--the latter are the main focus of the regulatory requirements. Foundation models are captured under the notion of general purpose AI models (GPAI), further split into GPAI models with and without systemic risk. Under these taxonomies, the EU AI Act lays out a comprehensive set of regulatory requirements regarding the development and deployment of AI, structured around six key ethical principles, each addressing a core risk factor (Weidinger et al., 2022)."}, {"title": "2 Background and Related Work", "content": "In this section we cover the background on LLMs and the EU AI Act, and discuss existing tools for assessing Act compliance and the current space of LLM evaluation benchmarks.\nLarge Language Models The transformer architecture (Vaswani et al., 2017) has enabled major progress on the well-studied problem of language modeling, allowing for efficient training and strong scaling with model and data size. Training large language models (LLMs), i.e., transformers with billions of parameters, has quickly brought significant improvements to most tasks of interest, most notably text generation (Devlin et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Lieber et al., 2021; Hoffmann et al., 2022), and these models quickly reached deployment in user-facing applications such as GitHub Copilot (GitHub). Following the release of ChatGPT (OpenAI, 2022), an LLM chatbot, LLM-powered applications have seen a rapid increase in adoption, with hundreds of millions of users (Milmo & agency, 2023), and new LLMs being developed both as open source (xAI; Touvron et al., 2023a;b; Jiang et al., 2023; 2024; Mesnard et al., 2024; Li et al., 2023; Biderman et al., 2023) and proprietary models (OpenAI, 2023; Anil et al., 2023; Anthropic, 2023; 2024; Mistral). These LLMs are pretrained for next-token prediction (completion) on large text corpora, modeling the next-token probability $p(x_n|x_0,..., x_{n-1})$. This equips the model with common sense knowledge, language understanding, coding ability, and many other capabilities. Modern LLMs are finetuned to follow instructions in a chat format (Wei et al., 2022), and often go through alignment (Christiano et al., 2017; Ouyang et al., 2022), where the model is further tuned to human preference.\nOpportunities and Risks of LLMs Bommasani et al. (2021) detail some unique opportunities LLMs can bring to individuals, democratizing access to specialized knowledge, as well as to the economy as a whole, e.g., in the healthcare, legal, and educational sectors. For example, LLMs may provide medical information to patients, serve as a preliminary legal consultant, or complement teachers as digital tutors. Analysts estimate that generative AI could add up to $4.4 trillion to the global economy, with significant impacts across all sectors (Chui et al., 2023). However, these models also carry risks, from accelerating malicious activities to having potentially discriminatory impacts. Notably, Weidinger et al. (2021) lay out the risks associated with LLMs along six pillars: (i) discrimination, exclusion and toxicity, e.g., perpetuating harmful"}, {"title": "3 COMPL-AI: Technical Interpretation of the EU AI Act and a Benchmarking Suite", "content": "In this section, we first outline the challenges of building a benchmarking suite for regulation packages such as the EU AI Act. Then, as the first component of the COMPL-AI framework, we present our technical interpretation of the Act, translating its legal requirements into a set of concrete benchmarks for LLMs.\nKey Challenges of Regulation-Oriented Benchmarking The main challenge in creating a bench-marking suite tailored to a regulation package is the interpretation of the regulatory requirements and their distillation into measurable technical requirements and benchmarks. This task is often difficult, as the text is formulated according to the practices of legal language, focusing on formulating directive high-level requirements instead of precise technical specifications, while purposefully leaving room for judges to exercise discretion. As such, the technical reader may be faced with (i) a lack of clarity which concrete metrics have to be considered, and (ii) potential requirements that lack current technical evaluation standards or techniques.\nAn illustrative example can be taken from the fourth ethical principle of the EU AI Act: \"AI systems shall be developed and used in a way that allows appropriate traceability and explainability, \". While the requirement posed by this statement (\"explainability\u201d) is in accordance with legal practices, it is hard to unambiguously interpret it in practice due to the lack of suitable technical tools. The extent to which this requirement should be satisfied is also not specified precisely, leaving much room for interpretation (\"appropriate\u201d), making it difficult to draw any conclusion based on potential technical benchmarks. Both of these aspects demonstrate the difficulties practitioners face when assessing the compliance of their systems.\nWhile in our benchmarking suite we aim to provide a comprehensive coverage over any relevant and measurable technical aspect of the examined models, due to the aforementioned challenges, this is not possible for all regulatory requirements. In such cases, we aim to raise awareness about the difficulty and ambiguity of the given regulatory requirement from a technical perspective, and identify regulatory requirements that imply technical specifications that are not assessable with current state-of-the-art tools. With this, we hope to motivate both regulators and the machine learning community to invest efforts in bridging these gaps."}, {"title": "3.1 A Comprehensive Benchmarking Suite for the EU AI Act", "content": "Next, we clarify our scope and discuss the methodology used to devise a technical interpretation of the EU AI Act. Then, we proceed to describe the corresponding technical requirements, along with an accompanying set of carefully chosen benchmarks, navigating the challenges outlined above. For each implemented benchmark, we provide further technical details in App. B. Definitions of specific terms, as used by the EU AI Act and by this text, are included in App. C, with the full glossary of the Act to be found in Article 3.\nScope The EU AI Act distinguishes between different AI artifacts, primarily establishing strict requirements for high-risk AI systems (HR) and general-purpose AI (GP) models, which may be used as part of a corresponding GP AI system (Recital 100), where GP models with systemic risk (i.e., those with particularly high capabilities, as defined in Article 51) are subject to additional requirements. On top of that, some"}, {"title": "3.1.1 Human Agency and Oversight", "content": "The first ethical principle of the EU AI Act states that:\n\"... AI systems shall be developed and used as a tool that serves people, respects human dignity and personal autonomy, and that is functioning in a way that can be appropriately controlled and overseen by humans.\u201d\nAs this principle formulates societal and system level informal requirements towards the deployment of AI systems, it does not impose any technical requirements on the base models constituting the AI system."}, {"title": "3.1.2 Technical Robustness and Safety", "content": "The second ethical principle of the EU AI Act states:\n\"... AI systems are developed and used in a way that allows robustness in case of problems and resilience against attempts to alter the use or performance of the AI system so as to allow unlawful use by third parties, and minimise unintended harm.\""}, {"title": "3.1.3 Privacy and Data Governance", "content": "The third ethical principle of the EU AI Act reads:\n\"... AI systems are developed and used in compliance with existing privacy and data protection rules, while processing data that meets high standards in terms of quality and integrity.\"\nUnder this ethical principle, we collect all requirements that concern the training data of the AI model, (potentially) processed copyrighted data, and private training or input data. We detail these requirements and our corresponding implemented benchmarks below."}, {"title": "3.1.4 Transparency", "content": "The fourth ethical principle of the EU AI Act states:\n\"... AI systems are developed and used in a way that allows appropriate traceability and explainability, while making humans aware that they communicate or interact with an AI system, as well as duly informing deployers of the capabilities and limitations of that AI system and affected persons about their rights.\u201d\nUnder this ethical principle, we collect and detail the regulatory and technical requirements listed below."}, {"title": "3.1.5 Diversity, Non-discrimination, and Fairness", "content": "The fifth ethical principle of the EU AI Act states:\n\u201c... AI systems are developed and used in a way that includes diverse actors and promotes equal access, gender equality and cultural diversity, while avoiding discriminatory impacts and unfair biases that are prohibited by Union or national law.\"\nWe distill two high-level regulatory requirements directly from this principle: (i) avoiding \"unfair biases\u201d, and (ii) avoiding \"discriminatory impacts\u201d. In the machine learning community, these correspond to two well-known concepts, i.e., evaluating the bias (i) and fairness (ii) of a given model. While bias evaluation commonly considers the avoidance of creating biased/stereotypical representations of specific groups (e.g., associating certain demographics with crime), fairness measures the discriminatory impacts of the model when used in concrete end-to-end applications where it is expected to produce outcomes that directly impact individuals (e.g., LLM assistant in sentencing).\nNote that these categories are not mutually exclusive, as a biased model may lead to discriminatory impacts in deployment, and an unfair model may indicate deeper underlying biases. Rather, these two aspects consider the model on different levels, where bias evaluation is focused on the model's quantitative and semantic representation and understanding of protected groups, while in fairness, one evaluates the model's potential discriminatory behavior in concrete applications."}, {"title": "3.1.6 Social and Environmental Well-being", "content": "The sixth ethical principle of the EU AI Act states:\n\"... AI systems are developed and used in a sustainable and environmentally friendly manner as well as in a way to benefit all human beings, while monitoring and assessing the long-term impacts on the individual, society and democracy.\"\nThe above ethical principle can be separated into the two components of (i) the environmental sustainability or impact of the AI system including its development process; and (ii) the social impact of the AI system, which we examine in the context of LLMs w.r.t. their potential for harmful and toxic content generation."}, {"title": "4 Experimental Evaluation", "content": "In this section, we apply the COMPL-AI benchmarking suite introduced in \u00a73 to evaluate 9 open-source and 3 closed models. We first outline our experimental setup, and then present the main experimental results per ethical principle and technical requirement, and discuss our observations. We defer further results to App. A.\nExperimental Setup We conduct all our evaluation runs on instruction-tuned/chat-tuned models, as they are able to both run benchmarks that require instructions or multi-turn interactions, as well as completion-focused benchmarks, either by adjusting the prompt or by ignoring the instruction/chat template. We evaluate 9 open-source models: Llama 2-7B, Llama 2-13B, & Llama 2-70B (Touvron et al., 2023b), Mistral-7B (Jiang et al., 2023), Mixtral-8x7B (Jiang et al., 2024), Llama 3-8B & Llama 3-70B (AI@Meta, 2024), Yi-34B (\u0391\u0399 et al., 2024), Qwen1.5-72B (Bai et al., 2023), and also include 3 closed-source LLMs: GPT-3.5 Turbo (OpenAI, 2022), GPT-4 Turbo (OpenAI, 2023), and Claude 3 Opus (Anthropic, 2024). All open-source models were run locally using the HuggingFace Transformers library (Wolf et al., 2020). To benchmark closed-source models, we make use of their respective APIs or in exceptional cases use the benchmark scores from the model's technical report or an official public evaluation (we mark such cases with an asterisk* in our results). Further, we were unable to run certain benchmarks, e.g., due to limitations to the models' API, we mark such cases with a dagger + symbol in our tables. For each benchmark, we consistently derive an evaluation metric with values in [0, 1], with higher scores being better. This enables us to aggregate these scores at each step by using a simple average, reflecting the regulatory focus of our benchmarking suite, as the regulatory requirements never impose a hierarchy between the different requirements. The technical details of the implemented benchmarks are deferred to App. B. Implementation details and detailed hyperparameter information are included in our code repository.\nScope Recall that the main objective of our benchmarking suite is to enable model providers to assess their own models in the context of the EU AI Act, and not to present a public leaderboard. As such, running our full benchmarking suite requires information about the model and its training beyond what is available to us, even for popular open-source models. The benchmarks that we were unable to run due to such limitations are excluded from aggregate scores. We reemphasize that our main goal is not to impose a ranking of models, but instead to inform the model providers and the broader community (i) in which general directions set out by the EU AI Act should model development be improved, and (ii) which aspects of model evaluation require further research to enable comprehensive assessments of EU AI Act compliance.\nResults In Table 1, we present our aggregate results for each of the five actionable ethical principles of the EU AI Act (see \u00a73). In Table 2, we present the underlying results per technical requirement that were averaged to obtain Table 1. For brevity, we exclude Training Data Suitability (inapplicable, as the training data of the models is not accessible to us), Traceability (all models score 0, as no model currently comes with a baked-in watermarking scheme), and User Privacy Protection (all models score 1, as current benchmarks are unable to detect memorization in any models). Tables with complete results are deferred to App. A.\nGeneral Observations Running our benchmarking suite with up to 23 benchmarks across 12 state-of-the-art LLMs gives us a clear view of the current state of LLMs in the context of the criteria imposed by the EU AI Act. We first observe that no model achieves perfect marks, most notably on the benchmarks under the ethical principles of Transparency and Diversity, Non-discrimination, and Fairness. The Transparency score, while also comprised of challenging capability benchmarks, is dragged down by the non-compliance of the"}, {"title": "5 Discussion", "content": "Our work on the COMPL-AI framework, including the construction of the benchmarking suite and subsequent evaluation of state-of-the-art LLMs in the context of the EU AI Act, led us to draw the following four key takeaways, that we hope can positively guide LLM development and evaluation in the coming years:"}, {"title": "6 Conclusion", "content": "In this work, we have introduced the COMPL-AI framework. We first provided a thorough technical interpretation of the regulatory requirements of the EU AI Act (EU, 2024), translating them into concrete technical requirements following the current state of LLM research. Next, under these technical requirements, we collected a representative set of state-of-the-art LLM benchmarks and implemented them as part of our regulation-oriented EU AI Act benchmarking suite. Finally, we applied our benchmarking suite to evaluate 12 popular LLMs, identifying that both current models and state-of-the-art benchmarks exhibit critical shortcomings in the context of the Act. In particular, none of the examined models are fully compliant with the requirements of the EU AI Act, and certain technical requirements cannot be currently assessed with the available set of tools and benchmarks, either due to a lack of understanding of relevant model aspects (e.g., explainability), or due to inadequacies in current benchmarks (e.g., privacy). With this in mind, we expect that the EU AI Act will have a large impact on both model and benchmark development going forward. Further, our methodology and final mapping of the broad regulatory requirements of the Act to concrete technical requirements, as well as our reduction of those to benchmarkable model properties for LLMs, can serve as an important starting point and proof of concept for ongoing and future concretization efforts of the EU AI Act, such as the development of the GPAI COP."}, {"title": "Limitations and Future Work", "content": "Our work has concentrated on LLMs in the context of the EU AI Act. The scope of the Act includes any AI system, and as such, it is crucial that similarly to our work, technical requirements and benchmarks are formulated and developed for model types beyond LLMs. This is especially important for official concretization efforts of the regulatory requirements of the Act, where works as ours could prove as crucial reference points. Further, while we already consider state-of-the-art benchmarks in our suite, there are three critical aspects in which our benchmarking suite could benefit from future improvements. I. We collected only a limited number of benchmarks per technical requirement, focusing on horizontal rather than vertical coverage. We believe that complementary future work focusing on benchmarking depth for individual technical requirements could prove essential in constructing a comprehensive compliance evaluation suite. II. Our benchmarking suite inherits the limitations of current benchmarks, providing inconclusive, incomplete, or, in case of the lack of suitable benchmarks, absent results in certain aspects. As such, improving upon current benchmarks in these areas is crucial as the EU AI Act is put into force. III. While our benchmarking suite provides a quantitative assessment of the benchmarked LLMs, until concrete compliance standards are not established, we are unable to map the obtained results to conclusive qualitative statements about the models' compliance. Finally, our evaluation of current LLMs was limited by the assets and information that is currently available to developers. It is possible that LLM providers would be able to show higher levels of EU AI Act compliance by providing further, currently proprietary information to the regulatory bodies."}, {"title": "C Necessary Definitions from Article 3", "content": "Here, we directly cite the necessary definitions from Article 3 of the EU AI Act that are relevant for the main body of this paper.\n1. \"\"AI system\u201d is a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments;\"\n2. \"\"risk\" means the combination of the probability of an occurrence of harm and the severity of that harm;\"\n3. \"\"deployer\u201d means any natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activity;\"\n4. \"\"provider\u201d means a natural or legal person, public authority, agency or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge;\"\n5."}]}