{"title": "Exploring Embodied Multimodal Large Models:\nDevelopment, Datasets, and Future Directions", "authors": ["Shoubin Chen", "Zehao Wu", "Kai Zhang", "Chunyu Li", "Baiyang Zhang", "Fei Ma", "Fei Richard Yu", "Qingquan Li"], "abstract": "Embodied multimodal large models (EMLMs) have gained significant atten-\ntion in recent years due to their potential to bridge the gap between per-\nception, cognition, and action in complex, real-world environments. This\ncomprehensive review explores the development of such models, including\nLarge Language Models (LLMs), Large Vision Models (LVMs), and other\nmodels, while also examining other emerging architectures. We discuss the\nevolution of EMLMs, with a focus on embodied perception, navigation, inter-\naction, and simulation. Furthermore, the review provides a detailed analysis\nof the datasets used for training and evaluating these models, highlighting\nthe importance of diverse, high-quality data for effective learning. The paper\nalso identifies key challenges faced by EMLMs, including issues of scalability,\ngeneralization, and real-time decision-making. Finally, we outline future di-\nrections, emphasizing the integration of multimodal sensing, reasoning, and\naction to advance the development of increasingly autonomous systems. By\nproviding an in-depth analysis of state-of-the-art methods and identifying", "sections": [{"title": "1. Introduction", "content": "Embodied intelligence, the idea that cognition arises from physical inter-\naction with the environment, emerged as a critique of traditional cognitive\ntheories. Rodney Brooks' 1991 paper [1], \u201cIntelligence Without Represen-\ntation,\" argued that intelligent behavior can emerge without relying on in-\nternal representations, focusing instead on environmental interaction. This\nidea was further developed by Varela, Thompson, and Rosch in The Em-\nbodied Mind (1991) [2], which highlighted the role of bodily experience in\nshaping cognition. Similarly, Lakoff and Johnson's Philosophy in the Flesh\n(1999) emphasized that cognition is grounded in sensory-motor experiences\n[3]. The concept also found practical applications in robotics, as seen in the\nCog Project, which explored how robots could develop cognition through\nbodily interaction with the world [4]. Thus, embodied intelligence bridges\ncognitive science, philosophy, and robotics, offering a more integrated view\nof mind and body.\nWith the rapid advancement of large model technology, embodied intelli-\ngence is increasingly being integrated with these models. This is a relatively\nnew concept, where researchers seek to apply principles of embodied cogni-\ntion to large-scale pre-trained models. The aim is to explore how Artificial\nIntelligence (AI) can develop more flexible and adaptive capabilities through\ninteractions with the environment. The term \u201cEmbodied Multimodal Large\nModels", "Large Embodied Multi-\nmodal Models,": "Embodied Large Models,"}, {"title": "2. Development of Foundational Models for Embodied Multimodal\nLarge Models", "content": "The evolution of large models, particularly in natural language processing,\ncomputer vision, and deep learning, has paved the way for the emergence\nof EMLMs. These advanced systems leverage the integration of multiple\nmodalities, such as vision, language, audio and touch, to enable more natural\nand intuitive interactions with the physical world. The advancements in\nlarge-scale pretraining and the scaling of neural networks have facilitated the\ncreation of models capable of processing multimodal data while embodying a\ndeeper understanding of context, actions, and interactions, setting the stage\nfor the next frontier in AI."}, {"title": "2.1. Embodied Agents", "content": "Embodied agents are autonomous entities that have physical or virtual\nbody and are able to perceive, act and interact with the environments. They\nhave various types, such as the robots, autonomous cars, virtual agents, etc.\nAs shown in Fig. 2, robots are the most popular agents used in existing\nembodied AI algorithms. Depending on the applications, robots have vari-\nous forms, including fixed-base robots, wheeled robots, quadrupted robots,\nhuman robots, soft robots, etc. Their unique shapes and designs make them\nspecialized in specific tasks. For example, the fixed-base robots, like franka\nrobots [17] are usually employed in industrial environments for automated\npicking and placing tasks. In contrast, humanoid robots, with their human-\nlike appearance and adaptability, are versatile and can be deployed across a\nbroad range of domains.\nAn autonomous vehicles can also be considered as an embodied agents.\nThey perceive their environment, make real-time decisions, and interact with"}, {"title": "2.2. Large Language Models", "content": "LLMs, such as GPT-4 [22], BERT [23], and T5 [24], have emerged as foun-\ndational components in modern AI, particularly within NLP. These models\nare designed to capture complex linguistic patterns and structures through\nunsupervised learning on massive amounts of text data. In the context of\nembodied multimodal systems, LLMs play a critical role as the primary mech-\nanism for understanding and generating natural language. Their ability to\nprocess and produce coherent text enables them to bridge the gap between\ndifferent modalities, including vision, speech, and action. By integrating\nLLMs with visual or auditory data, embodied systems can interpret mul-\ntimodal inputs and generate contextually relevant responses, thus enabling\nmore interactive and intelligent behaviors. Essentially, LLMs serve as the\n\"language brain\" of these systems, enabling them to understand and exe-\ncute language-based commands, describe visual scenes, or facilitate complex\nreasoning across modalities.\nBERT [23], proposed by Google, is based on the Transformer architecture\nand utilizes a masked language model for pre-training, which significantly\nimproves performance across various NLP tasks. Generative Pre-trained\nTransformer (GPT) [25], developed by OpenAI, is a Transformer-based gen-\nerative model that employs autoregressive training to generate text sequen-\ntially. GPT employs a large-scale unsupervised learning approach, leading to\nbreakthroughs in generative models. GPT-2 further scales up both the size\nand performance of language models, showcasing its ability to generate co-\nherent and natural language. XLNet [26] introduces a model that combines\nautoregressive and autoencoding approaches, surpassing BERT on multiple\nNLP benchmarks. Text-to-Text Transfer Transformer (T5) [24], proposed by\nGoogle, unifies all NLP tasks into a \u201ctext-to-text\" framework, enabling the\nmodel to perform transfer learning across different tasks.\nThe release of GPT-3 [27] marked a milestone as the largest and most\npowerful language model at the time, with 175 billion parameters. GPT-3's\""}, {"title": "2.3. Large Vision Models", "content": "Unlike LLMs, LVMs process image or video information. These models\nhave demonstrated exceptional performance in tasks such as image recogni-\ntion, object detection, image generation, and cross-modal learning. In the\nrealm of embodied intelligence, LVMs also play a crucial role, enabling robots"}, {"title": "2.4. Large Vision-Language Models", "content": "Among EMLMs, Large Vision-Language Models (LVLMs) enhance an\nagent's environmental understanding, reasoning, and task execution by inte-\ngrating visual and linguistic information. LVLMs enable agents to fuse and\ncoordinate multimodal data, allowing them to recognize objects through vi-\nsual input and perform actions based on language instructions. Additionally,\nLVLMs facilitate cross-modal reasoning and adaptive decision-making in dy-\nnamic environments, significantly improving the interaction and navigation\ncapabilities of robots.\nCLIP [37] is a multi-modal model developed by OpenAI, designed to em-\nbed both images and text into a shared vector space using contrastive learning\ntechniques. The model is pre-trained on a large corpus of image-text pairs,\nenabling it to perform tasks such as image classification and image-text re-\ntrieval. CLIP employs contrastive loss to optimize the alignment between\ntext and image descriptions, demonstrating strong zero-shot learning capa-\nbilities.\nDALL-E [38] is an image generation model released by OpenAI that gen-\nerates images based on textual prompts. Built upon the GPT-3 and Vector\nQuantized Variational Autoencoder architectures, the model excels in cre-\nating highly realistic and creative images. DALL-E2 [39] improves upon its\npredecessor by enhancing the quality and diversity of generated images, en-\nabling the creation of detailed and complex visuals based on sophisticated\ntext descriptions.DALL\u00b7E3 [40] has better performance.\nBLIP [41], introduced by Salesforce Research, employs a two-way self-\nsupervised learning approach to integrate visual and linguistic information.\nBLIP boosts pre-training efficiency through a \u201cguided\u201d strategy, helping the\nmodel better grasp the finer details in visual-language tasks, particularly in\nvisual question answering (VQA) and image captioning.\nFlamingo [42] is a novel visual-language model from DeepMind that can\nprocess multi-modal data (images and text) and perform cross-modal reason-\ning. Unlike traditional models, Flamingo excels in few-shot learning, allowing"}, {"title": "2.5. Other Modal Models", "content": ""}, {"title": "2.5.1. Vision-Audio Models", "content": "In addition to vision and language, audio plays a crucial role in everyday\ntasks, helping us identify scenarios and locate sound-emitting objects. While\nmost research focuses on vision-audio or audio-language data, few studies\nexplore audio in embodied tasks. Recent work in audio-visual navigation\ntasks, such as SoundSpaces [49], combines vision and audio for tasks like Au-\ndioGoal (where targets are indicated by sound) and AudioPointGoal (where\naudio provides directional guidance). These tasks face challenges, such as\nthe need for continuous sound, which has been addressed by linking audio\nwith semantic meanings and spatial properties [50]. Researchers have also\nintroduced more complex audio scenarios with multiple sound sources and\ndistractions [51, 52]. In manipulation tasks, audio provides crucial contact\ninformation, complementing vision and touch [53]. Studies show that com-\nbining audio with action generation and feedback significantly improves task\nperformance, with self-supervised learning methods and augmented audio\ndata boosting success rates in real-world environments [54, 55]."}, {"title": "2.5.2. Vision-Touch Models", "content": "In manipulation tasks, visual information is often the primary source for\nadjusting robot motions, but vision sensors can be limited by occlusion and\ntheir inability to measure contact force, which is crucial for successful actions.\nTo address this, several studies have explored combining vision and tactile\ndata. For example, [56] proposed a network that integrates vision and tactile\ndata to estimate the 6D pose of objects for in-hand manipulation. In grasping\ntasks, [57] investigated a learned regrasp policy that iteratively adjusts the\ngrasp using both visual and tactile feedback, showing improved performance\nwith touch information. To handle more complex objects, such as deformable\nones, [58] introduced a Transformer-based framework that uses tactile and\nvisual data for safe grasping, employing exploratory actions to gather tactile\nfeedback and predict the grasp outcome for safer parameter selection."}, {"title": "3. Development of Embodied Multimodal Large Models", "content": "EMLMs are a type of AI models that combine multiple modal information\nsuch as language, vision, and hearing. They can understand and process\ndifferent types of data from the real world. As shown in Fig. 3, these models\nare usually designed to perform various tasks, such as perception, navigation\nand interaction, etc."}, {"title": "3.1. Embodied Perception", "content": "Different from using traditional neural network methods or large mod-\nels to identify objects, according to the definition of embodied intelligence,\nembodied agents have the ability to interact with and move in the physical\nworld. This requires EMLMs to have a deeper perception and understand-\ning of objects in the physical world and the motion and logical relationships\nbetween objects. Embodied perception requires visual perception and rea-\nsoning, understanding 3D relationships in the scene, predicting and executing\ncomplex tasks based on visual information. The development of the embod-\nied perception large model is shown in Fig. 1"}, {"title": "3.1.1. GPT-Based Large Models", "content": "The general large model can accept specific text instructions tailored to\nthe task's requirements, returning scene understanding results in a natural\nlanguage format and handling a variety of perception tasks. For example,\nmodels like GPT1 [25], GPT2 [88], GPT3 [27], GPT4 [22] can effectively\nperform these functions.\nOctopus [59] uses GPT-4V [89] to dynamically generate descriptions and\nanalyses of observed images based on the current stage task, including objects"}, {"title": "3.1.2. Method Based on Non-GPT Large Models", "content": "PaLM-E [47], which directly integrates continuous inputs from the sensor\nmodalities of an embodied agent, allowing the language model to make more\ngrounded inferences for sequential decision-making in the real world. Inputs\nsuch as images and state estimates are embedded into the same latent space\nas language tokens, enabling them to be processed by the self-attention layers\nof a Transformer-based [94] LLMs in the same manner as text. ViT [32]\nRT-1 [72] employs the EfficientNet-B3 [73] model as a perception module\nto encode image data. It transforms images into implicit representations for\nsubsequent task execution. Through training, this perception module learns\nto encode information in a way that enhances task execution effectiveness.\nAs an upgrade of RT-1, RT-2 [74] introduces a method that directly trains\na vision-language model to output low-level robot actions by representing\nthese actions as textual tokens. This approach involves training the model"}, {"title": "3.2. Embodied Navigation", "content": "Relative to traditional A-to-B robot navigation, where an agent is in-\nstructed to move from Room A (x1, y1) to Room B (x2, y2), and the agent\nuses A* [97] or Dijkstra's [98] algorithm on a pre-generated 2D map to find\na reachable shortest path, embodied intelligent navigation systems adopt a\nmore dynamic and environmentally perceptive approach. These navigation\nsystems do not solely rely on static map data but perceive and process sur-\nrounding environments in real-time through sensors, and models, converting\nenvironmental information into understandable and actionable semantics for\nthe agent.\nIn the realm of navigation with large-scale embodied intelligence mod-\nels, two primary methodologies are prevalent: the first approach is to har-\nness a general large model, whereas the second entails crafting a specialized,\nEMLMS tailored explicitly for embodied intelligence tasks."}, {"title": "3.2.1. General Large Models", "content": "LVLMs are capable of understanding and generating natural language\ndue to their vast scale, advanced architecture, and pre-training on massive\ndatasets. In some cases, they demonstrate reasoning and semantic under-\nstanding abilities that approach or even surpass those of humans. The ex-\ncellent reasoning ability of LLMs can transform abstract instructions into\noperable semantic topologies. For example, LM-Nav [102] implements nat-\nural language instruction parsing based on GPT-3 [27], making navigation\ndecisions by extracting textual landmarks and combining them with scene\nimages. Its research focuses on the application of LLMs in natural lan-\nguage understanding. In contrast, L3MVN [103] demonstrates two innovative"}, {"title": "3.2.2. Specialized Embodied Intelligence Large Models", "content": "There is a type of model that does not use LVLMs directly. Instead,\nit combines different models and uses a dedicated dataset to train a new\nspecialized large model.\nNavCoT [122] combines existing VLN datasets with advanced models\nsuch as LLMs and CLIP to train the LLaMA [95, 140] model on extract-\ning object text from multi-perspective images, thereby enhancing its perfor-\nmance on the Future Imagination task and ultimately improving the entire\nnavigation decision-making process.\nNaviLLM [125] extracts features from six perspective images at each lo-\ncation using a ViT [32] to form a scene encoding, then uses a Transformer\nEncoder to capture the interdependencies between different viewpoints. It\nprocesses and generates textual data such as task instructions, observation\nresults, and historical information using an LLM (Vicuna-7B-v0 [126]), and\nfinally trains the model with all three as inputs.\nSimilarly, Trans-EQA [141] leverages the global modeling advantages of\nTransformer [94] in the navigation module to replace the local feature extrac-\ntion bottleneck of traditional CNNs, effectively associating dispersed visual\nfeatures with language semantics.\nGOAT [132] decomposes vision, instruction, and history into mediator,\nobservable confounder, and unobservable confounder. It proposes Back-door\nAdjustment Causal Learning (BACL) and Front-door Adjustment Causal\nLearning (FACL) causal learning modules to process the corresponding in-\nformation, while the mediator is used to predict the output and reduce the\nimpact of confounding factors. This enables the GOAT model to have ex-\ncellent generalizability and the ability to handle dataset biases and reduce"}, {"title": "3.3. Embodied Interaction", "content": "Traditional robot interaction methods typically require the integration\nof independent modules such as perception, decision-making, planning, and\ncontrol to accomplish specific tasks. With the advancements in deep learning,\nparticularly the significant progress of language and visual models, embodied\nintelligent interaction has become feasible. Embodied intelligent interaction\ninvolves enabling intelligent agents and large models to possess multi-modal\nprocessing capabilities, including natural language reasoning, visual-spatial\nsemantic perception, and the alignment of visual perception with language\nsystems, among other key technologies. Currently, the foundational capabil-\nities of embodied intelligent interaction require that the system understand\nhuman natural language instructions and autonomously complete tasks. As\nsuch, language-based embodied intelligent interaction has become a central\nfocus of research. This can be broadly categorized into two types: language-\nbased short-horizon action policies and language-based long-horizon action\npolicies. Some interaction models are shown in Table 3 and Table 1."}, {"title": "3.3.1. Language-based Short-horizon Action Policy", "content": "Currently, many researchers focus on language-based vision fusion action\nstrategies for simple tasks; while some researchers improve visual encoders to"}, {"title": "3.3.2. Language-based Long-horizon Action Policy", "content": "When making decisions, the agent considers goals and strategies over a\nlonger time span, rather than focusing only on short-term immediate goals.\nThis long-horizon policy considers the longer term future when making deci-\nsions, usually involving more complex planning and reasoning, with the goal\nof maximizing long-term rewards or achieving long-term goals.\nReal-world tasks are often complex and time-consuming, so it is often\nnecessary to decompose a complex task into several subtasks to execute.\nCurrently, many researchers [163, 164] focus on introducing new large model\nmethods to achieve the decomposition of complex tasks, namely high-level\naction policy. SayCan [165] connected a large language model with the real\nworld by pre-training skills. The combination of low-level skills and the large\nlanguage model helped the robot perform complex tasks. The results showed\nthat it was able to complete long-horizon, natural language instructions and\ncomplex tasks. Zero-Shot Planners [166] used the world knowledge learned\nby a large language model to achieve task planning and decomposition by\ndescribing the prompt of the task without additional training. Text2Motion\n[167] proposed a language-based planning framework that enables robots to\nsolve long-horizon sequential manipulation tasks, construct task and mo-\ntion plans given natural language instructions, and verify whether the plan\nis completed. Text2Motion uses Q functions encoded in a skill library to"}, {"title": "3.4. Simulation", "content": "Embodied simulation is essential for embodied intelligence as it allows\nfor the design of precisely controlled conditions and optimizes the training\nprocess. This enables agents to be tested in various environmental settings,\nenhancing their understanding and interaction capabilities. Additionally, it\nfosters cross-modal learning within the embodied agents themselves and fa-\ncilitates the training and evaluation of generated data. To enable meaningful\ninteraction with the environment, a realistic simulation environment must be\nconstructed, taking into account the physical characteristics of the surround-\nings, the properties of objects, and their interactions. Simulation platforms\ngenerally fall into two categories: general simulators based on foundational\nsimulations and simulators based on real-world scenarios. [11]. Some of the\nlatest and famous simulation methods and platforms are shown in Table 4."}, {"title": "3.4.1. General Simulators Based on Foundational simulations", "content": "NVIDIA Omniverse\u2122\u2122 Isaac Sim is a powerful robotics simulation toolkit\ndesigned for the NVIDIA Omniverse\u2122\u2122\u2122 platform. It equips researchers and\npractitioners with essential tools and workflows to build virtual robotic envi-\nronments and conduct experiments. Isaac Sim enables the creation of highly\naccurate, physically realistic simulations and synthetic datasets, offering ca-\npabilities such as advanced physics simulation and multi-sensor RTX ren-\ndering. With support for ROS2, Isaac Sim facilitates the design, debugging,"}, {"title": "3.4.2. Simulators Based on Real-world Scenarios", "content": "In response to the challenges faced by traditional physics simulators in\nhandling large-scale scenes, the learning-based rigid body simulator SDF-Sim\n[216] leverages learned signed distance functions (SDFs) to represent object\nshapes. This approach aims to accelerate distance calculations and enable\nefficient simulation of complex, large-scale environments.\nPaper [217] introduces the TRUMANS [217] dataset and proposes an\nautoregressive motion diffusion model for generating human-scene interac-\ntion (HSI) sequences. The model incorporates a local scene sensor and a\nframe-by-frame action embedding module. The local scene sensor captures\nthe contextual information of the scene, while the action embedding module\nprocesses action labels on a frame-by-frame basis. Together, these compo-\nnents enhance the model's ability to understand and control both the scene\ncontext and the actions, improving the generation of realistic HSI sequences.\n3D scene generation has garnered significant attention, but most exist-\ning methods rely on offline generation, which often leads to issues such as\nslow generation speeds and scene geometry distortion. These limitations hin-\nder their ability to support real-time interaction and diverse scene creation,\nwhich are essential for applications like game development and virtual re-\nality (VR). The WonderWorld [218] framework addresses these challenges\nby enabling the generation of diverse and coherent 3D scenes from a sin-\ngle image, achieving low-latency user interaction. It employs a fast Layered\nGaussian Facet (FLAGS) representation, where the 3D scene is modeled as\na radiation field consisting of three layers: foreground, background, and sky.\nEach layer is made up of a set of facets. Using a single-view layer gener-\nation approach, the framework generates images and masks for each layer\nfrom a single scene image. This method results in higher-quality scenes, im-\nproved semantic alignment, better consistency across novel views, and faster\ngeneration speeds.\nGenZI [219] is a pioneering zero-shot method designed to generate 3D\nhuman-scene interactions (HSI) from text descriptions, which addresses the\nchallenge of generating 3D interactions without relying on annotated 3D\ndata. This method automatically generates human figures in multiple ren-\ndered views of a 3D scene and uses a LVLM to generate potential 2D in-\nteraction hypotheses. These 2D hypotheses are then transformed into 3D"}, {"title": "4. Datasets", "content": "In this section, we first describe the dataset collection method, followed by\nan introduction to the datasets used for perception and interaction models,\nas well as the datasets used for navigation models."}, {"title": "4.1. Embodied Datasets Collection Methods", "content": "There are two primary methods for collecting datasets related to embod-\nied intelligence: one involves using an intelligent agent with a physical body\nto gather data in the real world, while the other relies on collecting datasets\nthrough a simulator.\nThe dataset, similar to those in [187, 224], was collected in a real-world\nenvironment using various sensors, including RGB cameras, depth cameras,\nIMUS, LiDAR, pressure sensors, sound sensors, and others. However, during\nthe data collection process, issues such as occlusions in the field of view\nor incomplete recording of operational details may arise. To address these\nchallenges, DexCap [225] utilizes Simultaneous Localization and Mapping\n(SLAM) to track hand movements.\nAnother type of dataset is collected using simulators, such as Unity and\nGazebo. This approach enables the rapid generation of large volumes of\nmultimodal data (e.g., images, depth maps, sensor data, etc.) while offering\ncontrol over environmental and task variables, facilitating model training.\nSome of the latest and most widely used simulators are listed in Table 4."}, {"title": "4.2. Embodied Perception and Interaction Datasets", "content": "Several recent datasets have played a pivotal role in advancing the devel-\nopment of embodied intelligence for robots.\nNotably, the Open X-Embodiment Dataset [187], released by the Google\nteam in collaboration with over 20 organizations and research institutes,\nprovides a large-scale multi-modal resource. It includes data from 22 types of\nrobots, capturing RGB images, end-point motion trajectories, and language"}, {"title": "4.3. Embodied Navigation Datasets", "content": "Embodied navigation datasets aim to enhance robots' ability to accu-\nrately navigate in physical or simulated environments based on visually-\nlinguistic combined instructions. This is achieved by providing long and\ncomplex paths and instructions, real-world data, diverse indoor and out-\ndoor scenes, support for training large high-capacity models, and detailed\nintermediate products such as 3D scene reconstructions, relative depth esti-\nmates, object labels, and localization information. These datasets effectively\nexpand the application scenarios of vision-language navigation and provide"}, {"title": "5. Challenges and Future Directions", "content": "Although the development of EMLMs has surged, it still faces numerous\nchallenges. However, it also presents exciting and valuable avenues for fu-\nture exploration. This section outlines the current challenges and highlights\npotential future research directions for the development of EMLMs. The\ndiscussion is organized into several key areas: technological challenges, data\nand annotation issues, and ethical and application-related concerns."}, {"title": "5.1. Technological Challenges", "content": "Cross-modal Alignment: Despite significant advances in multimodal mod-\nels, achieving precise and efficient alignment across different modalities\u2014such\nas vision, language, and motion\u2014remains a fundamental challenge. Devel-\noping methods to robustly fuse and align these modalities in real-time, par-\nticularly for embodied tasks, is a critical research focus. For example, both\nthe current visual-language model, ReKep [66], and the Vision-Audio model,\nSoundSpaces [49], depend on effective alignment of data from diverse modal-\nities. In the absence of proper alignment, the accuracy and efficiency of the\nresponse are likely to degrade.\nComputational Resources and Efficiency: EMLMs demand significant\ncomputational resources and storage. A key challenge is to improve compu-\ntational efficiency, minimizing energy consumption, and optimizing inference\nspeed while preserving high performance. Advances in model compression,\ndistributed computing, and hardware acceleration will be crucial in address-\ning these challenges. At present, most models have vast numbers of param-\neters, and both training and inference processes rely on high-performance\nGPUs, which are time-intensive and expensive. However, Openvla [80] has\nintroduced an approach where a model with only 7 billion parameters can\nperform a wide range of tasks. This efficiency is realized when the input"}, {"title": "5.2. Data and Annotation Issues", "content": "Diversity and Quality of datasets: Existing datasets for embodied multi-\nmodal tasks are often limited in terms of diversity, scale, and quality. The\nshortage of high-quality, real-world datasets that capture complex, multi-\nmodal interactions in dynamic environments hinders effective model train-\ning. Future efforts should prioritize the development of larger, more diverse,\nand better-annotated datasets to enhance the robustness and generalization\nof multimodal models. While current large-scale datasets like the Open\nX-Embodiment dataset [187] and ARIO dataset [224] have made notable\nstrides, they predominantly focus on perception and interactive tasks, such\nas household chores and kitchen operations. These tasks alone are insuffi-\ncient to support the full range of capabilities required for embodied intelli-\ngent agents. Furthermore, the majority of sensors in these datasets rely on\ncameras, which limits real-world perception. To address this, it is crucial\nto integrate additional multimodal sensors, such as LiDAR, sound sensors,\nradar, force sensors, and GPS, to improve the breadth of data available."}, {"title": "5.3. Applications and Ethical Considerations", "content": "Autonomous Driving and Robotics: As embodied multimodal models\nbegin to find applications in autonomous driving, robotics, and human-\nrobot interaction, ensuring their safety, reliability, and ethical compliance\nis paramount. There is a need to address the challenges of decision-making\nin real-time, the interpretability of model outputs, and the mitigation of risks\nin autonomous systems.\nEthical and Bias Issues: Multimodal models may unintentionally inherit\nbiases present in the training data, leading to unfair or discriminatory out-\ncomes. It is crucial to address these ethical concerns by developing meth-\nods that ensure fairness, transparency, and accountability in decision-making\nprocesses."}, {"title": "5.4. Future Research Directions", "content": "Cross-modal Pre-training and Fine-tuning: Future research should ex-\nplore more efficient strategies for cross-modal pre-training and fine-tuning,\nenabling models to perform well across a range of tasks, from perception to\ndecision-making, without requiring extensive retraining.\nSelf-supervised Learning: The development of self-supervised learning\ntechniques will be key in reducing reliance on large labeled datasets. By\nleveraging unlabeled data, models can learn richer representations, making\nthem more adaptable and scalable.\nIntegration with Multimodal Reinforcement Learning: A promising di-\nrection is the integration of multimodal models with reinforcement learning.\nBy combining perception, action, and feedback loops, embodied agents can\ncontinuously improve and adapt their behaviors in dynamic, real-world en-\nvironments.\nEnd-to-end large models: Currently, there are various large models de-\nsigned for different tasks, such as perception, navigation, and interaction.\nHowever, the future development trend is moving towards end-to-end large"}, {"title": "6. Conclusions", "content": "In conclusion, EMLMs represent a cutting-edge frontier in AI research,\ncombining language, vision, perception, and action to tackle complex, real-\nworld problems. This review has explored the development of large models in\nlanguage, vision, and multimodal domains, with a specific focus on how em-\nbodied tasks, including perception, navigation, interaction, and simulation,\nare advancing the field.\nThe progress made thus far demonstrates the transformative potential of\nembodied multimodal models across diverse applications, from autonomous\nsystems to robotics. However, significant challenges remain in terms of cross-\nmodal alignment, computational efficiency, generalization, and data acqui-\nsition. Furthermore, the ethical implications of deploying such technologies\nmust be carefully considered.\nLooking ahead, the field holds immense promise. Advancements in cross-\nmodal pre-training, self-supervised learning, and reinforcement learning will\nlikely drive the next generation of more capable, adaptable, and efficient\nmodels. In particular, the integration of these models in real-world, dynamic\nenvironments promises to revolutionize fields such as autonomous robotics,\nvirtual agents, and interactive systems.\nAs the field matures, addressing the technical and ethical challenges will\nbe essential for the responsible and impactful deployment of EMLMs. Con-\ntinued research and collaboration across disciplines will play a pivotal role in\nshaping the future of this exciting area of AI."}]}