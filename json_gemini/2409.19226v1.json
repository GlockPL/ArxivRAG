{"title": "Learning to Bridge the Gap: Efficient Novelty Recovery with Planning and Reinforcement Learning", "authors": ["Alicia Li", "Nishanth Kumar", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "The real world is unpredictable. Therefore, to solve long-horizon decision-making problems with autonomous robots, we must construct agents that are capable of adapting to changes in the environment during deployment. Model-based planning approaches can enable robots to solve complex, long-horizon tasks in a variety of environments. However, such approaches tend to be brittle when deployed into an environment featuring a novel situation that their underlying model does not account for. In this work, we propose to learn a \"bridge policy\" via Reinforcement Learning (RL) to adapt to such novelties. We introduce a simple formulation for such learning, where the RL problem is constructed with a special \"CallPlanner\" action that terminates the bridge policy and hands control of the agent back to the planner. This allows the RL policy to learn the set of states in which querying the planner and following the returned plan will achieve the goal. We show that this formulation enables the agent to rapidly learn by leveraging the planner's knowledge to avoid challenging long-horizon exploration caused by sparse reward. In experiments across three different simulated domains of varying complexity, we demonstrate that our approach is able to learn policies that adapt to novelty more efficiently than several baselines, including a pure RL baseline. We also demonstrate that the learned bridge policy is generalizable in that it can be combined with the planner to enable the agent to solve more complex tasks with multiple instances of the encountered novelty.", "sections": [{"title": "1 Introduction", "content": "Recent model-based planning approaches, such as Task and Motion Planning (TAMP), have enabled robots to perform complex and long-horizon tasks, such as setting a table, rearranging a room, or even assembling complex structures, under a wide variety of circumstances [1, 2, 3]. These approaches assume access to some form of structured, abstract model that captures aspects of the environment important for the robot's decision making. However, useful robots must be able to cope with novelty that arises from being deployed in environments that neither the robot nor its designers could possibly have seen ahead of time [2]. Unfortunately, following the planner's proposed action sequence fails catastrophically in such cases, which significantly limits the applicability and utility of such model- based approaches. We are thus primarily interested in efficiently and autonomously learning to cope with environment novelties encountered during an agent's deployment in a novel environment.\nIn this work, we assume the robot is equipped with a collection of skills (such as moving to a specified location, picking a specified object, or placing a held object at a location) as well as a planning model in terms of those skills that it can use to solve user-provided tasks. We are interested in situations where executing plans made according to the model leads to some kind of failure in the environment.\nAs a simple example, consider the 'Light Switch Door' environment illustrated in Figure 1. Here, the"}, {"title": "2 Problem Setting", "content": "We consider planning and learning in deterministic, fully-observable environments with object- oriented states. Below, we first describe our assumptions about the environment model before detailing a minimal specification for the planning system, as well as our notion of environment novelty. As a simple running example, we introduce the 'Light Switch Door' environment depicted in Figure 1. In this environment, the agent's goal is to turn on the light at the end of a row of grid cells. It possesses skills and a planning model enabling it to move left and right and turn on the light when it is in the corresponding cell. However, at deployment time, it encounters a door that is not part of its planning model. The agent does not possess an explicit skill for opening the door, however, it is able to do this by combining a series of low-level actions."}, {"title": "2.1 Environment and Task Models", "content": "Following previous work [7], we take an environment to be a tuple $\\mathcal{E} = (\\Lambda, \\mathcal{X}, \\mathcal{U}, f, \\Psi)$. $\\Lambda$ is the set of possible object types; an object type $\\lambda \\in \\Lambda$ has a name (e.g. robot, light, or door in our running example) and a set of real-valued features (e.g. x-position, light-level) represented as a tuple of continuous values of dimension $dim(\\Lambda)$. An object $o$ has a particular name (e.g. robby, light-bulb0) and an associated type (e.g. robot) in any particular problem (i.e., an instance of an environment). An object will specify particular values for each of the features of the associated type (e.g. robot will have x-position: 0). A state $x \\in \\mathcal{X}$ is a collection of objects with assigned feature values, and the state space is defined by the set of all objects $\\mathcal{O}$, and the possible feature values they could be assigned. $\\mathcal{U}$ is the action space consisting of named parame- terized skills [8], $u(\\lambda, \\text{o}) \\in \\mathcal{U}$ (e.g. MoveRight(robot), or ToggleLightSwitch(robot, [toggle-value]), where toggle-value is a continuous parameter). Each such skill can be executed by specifying the list of objects with types $\\Lambda$, which causes the environment to advance to a new state $x' \\in \\mathcal{X}$ according to some unknown transition model $f: \\mathcal{X} \\times \\mathcal{U} \\rightarrow \\mathcal{X}$. We assume there always exists a particular skill named RunLowLevelAction, which takes in no objects and whose continuous parameters provide access to a very low-level action space (e.g. the robot's motor com- mands) compared to other skills. Finally, $\\Psi$ is a set of predicates. Each predicate $\\psi \\in \\Psi$ has a name (e.g., Light-On) and a tuple of types (e.g., (light)). Grounding a predicate yields a ground atom, which is a predicate and a mapping from its type tuple to objects (e.g., Light-On(light-bulb0)). Given a particular state $x \\in \\mathcal{X}$, a ground atom can be run on this state to produce a boolean value. Predicates induce a state abstraction: ABSTRACT(x) denotes the set of ground atoms that hold true in x, with all others assumed false. We denote a set of ground atoms via $\\Phi$. We denote the abstract state as $s$ (i.e., $s = ABSTRACT(x)$).\nEach environment is associated with some task distribution $\\mathcal{T}$. A task $t \\in \\mathcal{T}$, is a tuple $(\\mathcal{O}, x_o, \\mathcal{G}, H)$, where $\\mathcal{O}$ is some set of objects and $x_o \\in \\mathcal{X}$ is an initial state. $\\mathcal{G}$ is a collection of ground atoms describing the goal (e.g. [Light-On(light-bulb0)]), and $H$ is a maximum horizon (i.e., number of actions) within which the task must be solved. A solution to a task is sequence of actions $u_0, u_1,..., u_n$, such that $n < H$ and taking these actions from the initial state yields a final state $x_n$ such that the goal expression holds (i.e. $\\forall g \\in \\mathcal{G}, g(x_n)$)."}, {"title": "2.2 Planning and Environment Novelty", "content": "We assume the agent has access to a planner $\\mathcal{P}$ which takes a task $t$ and corresponding environment $\\mathcal{E}$ as input and produces a policy $\\pi_{\\text{plan}}$. Importantly, this planner operates over some internal model that does not account for environment novelty. This policy in turn takes in a state, and outputs (1) an action to take, (2) a boolean \u2018stuck' indicator (i.e., $\\pi_{\\text{plan}}: \\mathcal{X} \\rightarrow \\mathcal{U} \\times \\{0, 1\\}$). The stuck indicator can be interpreted as a simple measure of the robot's confidence in its output: when it is false, the robot believes its action will make progress towards the goal as intended, whereas when it is true, the policy believes some novelty has been encountered that it is not equipped to handle. For example, in a problem from the 'Light Switch Door' in which there are 3 cells, a door between the second and third cells, and a light switch in the final cell, the planner will produce a policy that outputs a 'MoveRight' action with the stuck indicator set to false in the first and second cells, and then produce any action with the stuck indicator set to true after failing to move through the door."}, {"title": "3 Bridge Policy Learning", "content": "How should the agent formalize and optimize the objective introduced in Section 2.2? A simple but naive approach would be to simply behave randomly for a random amount of time and then try to follow the plan again. However, this process would need to be repeated for every new problem, and would likely lead to the task horizon being exhausted in most cases. Another approach would be to simply perform RL in a new Markov Decision Process (MDP) that models the environment, task, and novelty. However, not only would this approach need to learn to overcome whatever novelty got the planner stuck, it would also need to complete the rest of the task. This is likely to be quite sample inefficient. In particular, it does not leverage or exploit knowledge the planner already possesses. For instance, in our running example from the Light Switch Door environment, the planner would be able to achieve the task goal once the door is opened: we only need to learn how to open the door.\nWe propose to learn a bridge policy that the agent can use during execution to get to a state where replanning and following the new plan will no longer get stuck. At evaluation time, the agent will start by planning, and then executing its plan until it gets stuck. It will then use its bridge policy until this policy terminates when it calls the planner, then replan and execute the new plan once again. This iteration between the planner and the bridge policy continues until the agent achieves the task goal or the task horizon is exhausted. To learn such a bridge policy, we need to define a set of target states for the policy to reach. In this work, we propose to have the agent automatically discover such a set of states implicitly. We do this by setting up an RL problem in which one of the available actions is to hand back control of the agent to the planner, so that the agent learns the optimal state to start replanning.\nBelow, we first describe the construction of the RL problem, then discuss how we choose to solve it. Finally, we describe how we use the learned policy at evaluation time to generalize to problems in our evaluation set."}, {"title": "3.1 Constructing an RL Problem", "content": "Once the agent detects that it is in a 'stuck' state $x_{\\text{stuck}}$, we set up an RL problem by constructing an MDP $\\mathcal{M} = (\\mathcal{X'}, \\mathcal{U'}, f, R, \\gamma)$. Here, $\\mathcal{X'}$ represents the RL problem's state space, $\\mathcal{U'}$ its action space, $f$ its transition function (which is the same as the original task's environment), $R$ its reward function, and $\\gamma$ its discount factor.\nThe state space $\\mathcal{X'}$ is a subspace of the corresponding environment's state space $\\mathcal{X}$. In particular, we construct $\\mathcal{X'}$ by selecting a subset of the objects $\\mathcal{O}$ that make up $\\mathcal{X}$. The purposes of this feature selection are twofold: to improve the sample efficiency of RL, and to improve the generalization of the learned bridge policy. Many possible strategies for selecting such a subset are possible (e.g. [9]), but in this work, we adopt the simple strategy of selecting the single object that can be interacted with that is closest to the robot object by euclidean distance in $x_{\\text{stuck}}$. In our running example, this will simply be the 'door' object. Learning a bridge policy that's specific to this object's state enables this policy to be reused in novel problems that may include additional variations in different objects (e.g. problems with more doors or grid cells in our running example)."}, {"title": "3.2 Learning a Bridge Policy", "content": "The action space $\\mathcal{U'}$ is simply the original environment's action space augmented with an extra action we call \u2018CallPlanner' (i.e. $\\mathcal{U'} = \\mathcal{U} \\cup \\{\\text{CallPlanner}\\}$). As the name suggests, this action simply returns control of the agent to the planner: the agent begins executing a sequence of actions output by the planner, and terminates in a new state $x'$ where either (1) the goal is achieved, (2) the task horizon has expired, or (3) the planner is stuck again. Importantly, even though the the planner itself executes many actions before terminating, we treat the entire CallPlanner sequence as one atomic action within the MDP $\\mathcal{M}$.\nFinally, the reward function is simply a sparse reward corresponding to achieving the task goal $\\mathcal{G}$.\n$R(x_t, a_t) = \\begin{cases} 1 & \\text{if } \\mathcal{G} \\subset ABSTRACT(x_{t+1}),\\\\ 0 & \\text{otherwise} \\end{cases}$\nWe set the discount factor $\\gamma$ to be less than 1 so that the agent is encouraged to solve the MDP with the fewest possible actions. Once the robot gets to a state it can plan from, executing the CallPlanner atomic action will bring the robot to the goal in one step, so an optimal bridge policy would take the fewest possible actions to resolve the novelty before calling the planner.\nAs is standard for RL problems, the learning objective is to learn a policy $\\pi_{\\text{bridge}}$ to maximize the discounted sum of rewards in expectation over all MDPs that comprise our training distribution:\n$\\mathbb{E}_{\\mathcal{M}} [\\sum_{t=0}^H \\gamma^t R(x_t, a_t)]$\nGiven the MDPs constructed in Sec- tion 3.1, we can apply any RL algo- rithm capable of handling continuous states and actions to learn a policy $\\pi_{\\text{bridge}}$. However, in our setting, ac- tions take the form of parameterized skills. We thus take advantage of this by leveraging recently-introduced efficient architectures for RL with pa- rameterized skills [11, 2]. Specifically, we choose to use a variant of the pure RL approach introduced in [2].\nFollowing this approach, which is re- lated to Deep Q Learning [12], we learn a Q-function $Q(x, a)$ that maps MDP states and skills (with all param- eters specified) to Q-values. At eval- uation time, we sample $n_{\\text{sample}}$ fixed number of continuous parameters uni- formly at random for each skill among our set of actions $\\mathcal{U'}$ and pick the argmax. To prevent overestimation of Q-values, we maintain a separate tar- get network in accordance with Dou- ble DQN [13].\nWe randomly initialize our Q- networks, and collect data for training them by performing epsilon-greedy exploration with annealing of the epsilon parameter. Here, epsilon greedy intuitively serves to balance how much we value solving the current task with how much we want to explore and learn new things about our environment."}, {"title": "3.3 Using a Learned Bridge Policy", "content": "After training our bridge policy $\\pi_{\\text{bridge}}$ we can leverage it to recover from novelties encountered during evaluation time by switching control of the agent between using its planner and using its learned bridge policy. Our meta policy for solving tasks via both planning and using the learned bridge policy is shown in Algorithm 1. Given a task, we first call the planner and execute its plan until a 'stuck' state is reached (if one is reached at all). We then pass control to the bridge policy, which will perform some sequence of actions before either achieving the task goal, or invoking its CallPlanner action to hand control of the agent back to the planner. In this way, a bridge policy that has correctly learned to overcome an encountered novelty can be used by the agent to generalize to testing tasks involving more instances of that novelty (assuming every instance can be resolved by the same policy) by calling the same bridge policy multiple times in sequence whenever the novelty is encountered. We evaluate this capability empirically in Section 4 below."}, {"title": "4 Experiments", "content": "Our experiments are designed to answer the following questions about our approach:\nQ1. How sample efficient is bridge policy learning compared to alternatives?\nQ2. How well can our learned bridge policy be used to generalize to harder versions of the tasks?\nQ3. How important is the CallPlanner action in our approach?\nEnvironments: We provide high-level descriptions of our three simulated environments here.\n\u2022 Light Switch Door: An implementation of our toy running example depicted in Figure 1. The robot must traverse a row of cells to turn on a light at the end of the row. However, there are doors obstructing the way that the robot's planner does not model, and thus following its output plan will result in a stuck state. The robot needs to learn how to open these doors using two low level actions. This domain is a variation on 'Light Switch' from [2].\n\u2022 Doorknobs: Equivalent to the 'Doors' environment from [7]. Here, a robot must navigate to a target room while avoiding obstacles and opening doors. The planner has no direct knowledge of the existence of doors, nor how to open them. A skill that moves the robot between configurations using a motion planner (BiRRT) is provided; door opening must be learned by a bridge policy. Tasks have 4-25 rooms with random connectivity. The goals, obstacles, and initial robot pose also vary.\n\u2022 Coffee: Another environment taken directly from [7]. The robot makes coffee by putting the jug in the coffee machine, turning the machine on, and then pouring the coffee into a cup. However, the jug is initially rotated in a way so that the robot must first rotate the jug in order to grasp it. Otherwise, if the robot tries to grasp the jug without first rotating it, it will enter a stuck state.\nApproaches: We now briefly describe the various approaches we evaluate in the above environ- ments.\n\u2022 Bridge Policy Learning: Our main approach.\n\u2022 Ours no feature selection: An ablation of our approach that does not perform any feature selection when constructing the RL problem, but instead passes in the entire task state (i.e., $\\mathcal{X'} = \\mathcal{X}$). Note that we do not test this ablation in the Doorknobs environment because the entire state is too large and the method times out after several learning cycles.\n\u2022 Ours no CallPlanner: An ablation of our approach that does not include CallPlanner in the RL problem's action space. This ablation is not able to exploit the planner's knowledge."}, {"title": "5 Related Work", "content": "Open-World AI: There has been a long history of work that attempts to develop agents capable of continuously adapting to novel scenarios [14, 15, 16, 17, 18, 19]. Many of these works [14, 15, 16, 17] are interested in a setting where the agent is not provided any models nor specific goals a priori but rather must simply explore in an open-ended fashion. By contrast, our work is interested in more directed exploration, where the agent is specifically trying to overcome some failure and also is equipped with a model and planner to be leveraged. The works that operate in a setting similar to ours [18, 19] typically focus on two aspects: novelty detection, and novelty adaptation. Additionally, these works often operate in environments with purely discrete action spaces. Our work assumes a"}, {"title": "6 Conclusion", "content": "In this work, we proposed a method that enables a robot equipped with a model-based planner to adapt to novel deployment environments where following the returned plan fails to complete an assigned task. Experiments in simulated domains revealed that our method is able to successfully overcome this novelty with very low sample complexity and generalize to larger environments.\nThere are several limitations of our approach. Firstly, we rely on reducing the dimensionality of the input state space to enable generalization of the bridge policy to novel, more complex tasks. We currently use a very simple approach for this that is unlikely to work in more complex and realistic environments. Secondly, we attempt to learn a single bridge policy that is capable of coping with all encountered novelties. This suffices for environments with very few novelties that lead to planning failure, but would not work in environments featuring multiple different types of novelty. Finally, our approach performs a relatively simple form of exploration (epsilon-greedy), which works well when the desired recovery behavior is relatively short horizon, but will likely not scale well to settings that require learning a much more long-horizon bridge policy.\nIn the future, we hope to address these limitations and enable our approach to be realized in complex and useful robotics domains. An important direction is to integrate our approach with perception to perform decision making from camera input, both for our planner [2], and for our learned bridge policy [11]. In conjunction with this, integrating pretrained vision language models (VLMs) could help perform automatic dimensionality reduction for our bridge policy by suggesting which objects to ignore or consider after a stuck state is encountered. This could also be directly learned via a graph neural network (GNN), as demonstrated in previous work [9]. Additionally, it will be important to run our approach in a wider range of complex environments, and compare directly to closely-related approaches from the literature (e.g. [23, 18]). This would allow us to further test the efficacy of our CallPlanner formulation in comparison to other approaches that have achieved high performance in similar problem settings."}, {"title": "A Additional Experimental Details", "content": ""}, {"title": "B Planner Implementation Details", "content": "Here, we provide additional details about the implementation of the planner used in this work. We note that this is a relatively simple implementation that satisfies the setup in Section 2 and other, more sophisticated implementations are possible as well.\nWe adopt a planning implementation that is identical to that of [2]. This requires access to a set of extra predicates, $\\Psi_{\\text{planner}}$ in addition to those specified as part of the environment. In everything that follows, assume that when we perform abstraction (i.e., calling ABSTRACT), we will use the environ- ment predicates $\\Psi$ as well as the planner predicates $\\Psi_{\\text{planner}}$. For example, in our toy Light Switch Door environment, the environment predicates will include only \u2018LightOn(light)', while the agent will additionally have predicates such as \u2018RobotInCell(robot, cell)' and 'Adjacent(?c1:cell, ?c2:cell)'. Next, we assume access to PDDL symbolic planning operators [29] with predicate-based precondi- tions and effects."}]}