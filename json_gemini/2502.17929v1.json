{"title": "Integrating Boosted learning with Differential Evolution (DE) Optimizer:A Prediction of Groundwater Quality Risk Assessment in Odisha", "authors": ["Sonalika Subudhi", "Alok Kumar Pati", "Sephali Bose", "Subhasmita Sahoo", "Avipsa Pattanaik", "Biswa Mohan Acharya"], "abstract": "Groundwater is eventually undermined by human exercises, such as fast industrializa-\ntion, urbanization, over-extraction, and contamination from agrarian and urban sources.\nFrom among the different contaminants, the presence of heavy metals like cadmium (Cd),\nchromium (Cr), arsenic (As), and lead (Pb) proves to have serious dangers when present\nin huge concentrations in groundwater. Long-term usage of these poisonous components\nmay lead to neurological disorders, kidney failure and different sorts of cancer. To address\nthese issues, this study developed a machine learning-based predictive model to evaluate\nthe Groundwater Quality Index (GWQI) and identify the main contaminants which are af-\nfecting the water quality. It has been achieved with the help of a hybrid machine learning\nmodel i.e. LCBoost Fusion. The model has undergone several processes like data pre-\nprocessing, hyperparameter tuning using Differential Evolution (DE) optimization, and\nevaluation through cross-validation. The LCBoost Fusion model outperforms individual\nmodels (CatBoost and LightGBM), by achieving low RMSE (0.6829), MSE (0.5102), MAE\n(0.3147) and a high R2 score of 0.9809. Feature importance analysis highlights Potassium\n(K), Fluoride (F) and Total Hardness (TH) as the most influential indicators of ground-\nwater contamination. This research successfully demonstrates the application of machine\nlearning in assessing groundwater quality risks in Odisha. The proposed LCBoost Fusion\nmodel offers a reliable and efficient approach for real-time groundwater monitoring and\nrisk mitigation. These findings will help the environmental organizations and the policy\nmakers to map out targeted places for sustainable groundwater management. Future work\nwill focus on using remote sensing data and developing an interactive decision-making\nsystem for groundwater quality assessment.", "sections": [{"title": "1 Introduction", "content": "Groundwater quality evaluation is crucial because groundwater is one of the most important\nsources of drinking water for millions of people [1]. Many regions, especially rural and ur-\nban areas, depend on groundwater for domestic, agricultural, and industrial purposes [2, 3].\nHowever, groundwater quality can be significantly impacted by both natural factors and hu-\nman activities, leading to potential health risks and environmental issues [3, 4]. Therefore,\nassessing its chemical composition and identifying its pollutants is essential for ensuring its\nsafety and sustainable use [5]. The quality of groundwater is affected by both chemical and\nbiological factors [6]. Naturally occurring elements such as chloride, sodium, fluoride, and\nheavy metals can lead to groundwater pollution, making the water unsafe for consumption\n[7, 8, 9]. Additionally, variations in soil composition, water table levels, and climate condi-\ntions can impact groundwater quality [10, 11]. Human activities also contribute significantly\nto groundwater pollution. Agricultural practices, including excessive fertilizer and pesticide\nuse, lead to the infiltration of nitrate and phosphate compounds, which degrade water quality\n[10, 12]. Industrial waste disposal, mining operations, and leakage from landfills introduce\nharmful chemicals and heavy metals, contaminating groundwater sources [7, 13]. Further-\nmore, untreated sewage, improper sanitation systems, and wastewater seepage contribute to\nbacterial and viral contamination, increasing the risk of waterborne diseases [12, 14, 15]. Eval-\nuating groundwater quality is essential for protecting public health, sustaining agricultural\nproductivity, and preserving ecosystems [9, 10]. With increasing industrialization and agri-\ncultural expansion, implementing continuous monitoring, pollution control measures, and\nsustainable groundwater management is critical to ensuring a safe and reliable water supply\nfor future generations [8, 16].\nGroundwater quality evaluation is essential for ensuring safe drinking water, sustainable"}, {"title": "2 Materials and Methodology", "content": "2.1 Study Area\nOdisha is a coastal state in the eastern part of India having over 41 million inhabitants. It\nshares its boundaries with the Bay of Bengal in east as portrayed in Figure 1. It is also among\nthe eleventh largest states in the union in terms of population and the eighth largest state\nin terms of area. It is situated in the latitudes 17.780N and 22.730N, and in the longitudes\n81.37E and 87.53E. It has an area of 155,707km\u00b2, which is 4.87% of total area of India, and a\ncoastline of 450 km. According to a Forest Survey of India report, it has 48,903km\u00b2 of forest\ncover which is 31.41% of the state's total geographical area. The state is in the tropical region\nhaving humid climate, hot summers, good monsoon rainfalls and mild winters. The region\nis rich in alluvial deposits and the availability of minerals such as chromite, bauxite, coal and\niron ore are greatly beneficial to the economic environment of the state [68]. Groundwater\nis one of the most important natural resources in Odisha and is used for agricultural, drink-\ning, and industrial purposes. However, the quality of groundwater is different from one area"}, {"title": "2.2 Data Collection and Data Description", "content": "For this study, dataset is obtained from the Central Ground Water Board (CGWB) under the\nMinistry of Jal Shakti, Department of Water Resources, River Development, and Ganga Re-\njuvenation, Government of India. For this study, data from 2019-2022 has been taken. This\ndataset consists of features like Well_id, S.no, State, District, Taluka, Site, Latitude, Longi-\ntude, Year, pH, EC (Electrical Conductivity), CO3 (Carbon Carbonate), HCO3 (Bicarbonate),\nCl (Chloride), SO4 (Sulphate), NO3(Nitrate), PO4 (Phosphate), TH (Total Hardness), Ca (Cal-\ncium), Mg (Magnesium), Na (Sodium), K (Potassium), F (Fluoride), SiO2 (Silicon Dioxide),\nBlock and Location[71, 72, 73].\nThe dataset is having about 1989 records, providing proper insights of the groundwater\nquality indicators present in several districts of Odisha. Here, pH has a range from 4.99 to\n8.85, having an average value of 7.844 and a standard deviation of 0.389. The EC is hav-\ning a variation from 35\u00b5m/cm to 3650\u00b5m/cm with an average value of 681.35\u00b5m/cm hav-\ning a standard deviation of 471.70\u00b5m/cm. TH is ranging from 6.00mg/L to 1235.00mg/L\naveraging 22.059mg/L with a standard deviation of 144.709mg/L. The mean concentration\nof Ca, Mg, Na, K, F, Cl are 45.87mg/L, 26.92mg/L, 48.88mg/L, 10.90mg/L, 0.38mg/L and\n77.41mg/L respectively. A detailed description of the statistics is given in the following table:"}, {"title": "2.3 Calculating Ground Water Quality Index (GWQI) score", "content": "For calculating GWQI, a WQI model given by [74] is used. It provides a proper approach to\naccess groundwater quality by considering various water quality indicators (i.e. pH, Electrical"}, {"title": "2.3.1 Selecting the required Water Quality Indicators (WQI)", "content": "From the dataset obtained by Central Ground Water Board (CGWB), this study utilized physio-\nchemical water quality indicators like pH, Electrical conductivity (EC) and Total hardness\n(TH). Also, heavy metals water quality indicators like Calcium (Ca), Magnesium (Mg), Sodium\n(Na), Potassium (K), Fluoride (F), Chloride (Cl) are also used for calculating GWQI score [72]."}, {"title": "2.3.2 Calculating Sub-Index (SI)", "content": "It is one of the main steps of GWQI score calculation as it transforms raw water quality in-\ndicator values into a standardized scale ranging from 0 to 100. If SI is equals to 0 then the\nwater quality is poor and if SI is equals to 100 then the water quality is good [72]. The SI\nbasically helps in identifying the contribution of each indicator to the overall GWQI score. It\nis calculated using eq 1.\n$SI =\n\\begin{cases}\n\\frac{WQI_{value}}{std_{min}} \u00d7 100, &\\text{if } WQI_{value} < std_{min} \\\\\n\\frac{std_{max}}{WQI_{value}} \u00d7 100, &\\text{if } WQI_{value} > std_{max} \\\\\n100, &\\text{Otherwise.}\n\\end{cases}$ (1)\nwhere, $WQI_{value}$ is the value of the water quality indicator present in the dataset, $std_{min}$\nis the lower acceptable level of the water quality indicator and $std_{max}$ is the upper acceptable\nlevel of the water quality indicator.\nHere, for pH only, a range of (6.5, 8.5) is defined and for all other indicators (EC, TH, Ca,\nMg, Na, K, F, Cl), only a single value is given and these values are the maximum permissible\nlimits ($std_{max}$). The minimum permissible limits ($std_{min}$) are assumed to be 1, since negative\nvalues are not allowed in concentration-based parameters. In table 2, a detailed description\nof ($std_{min}$) and ($std_{max}$) values defined by WHO."}, {"title": "2.3.3 GWQI Score Aggregation function", "content": "The work of an Aggregation function is to convert sub-index results into a single value that\nshows the overall groundwater quality[73]. It helps in making decisions while consuming\nthe water as lower the GWQI score better the water quality whereas higher the GWQI score\nsuggest contamination. It can be calculated using eq 2.\n$GWQI = \\sqrt{\\frac{\\sum_{i=1}^{n}(SI_i)^2}{n}}$ (2)\nwhere, $SI_i$ is the index of the $i^{th}$ water quality indicator and n is the total number of\nindicators considered. After calculating the GWQI score, the next goal is to look into the\nTable 3 and find out the groundwater quality status."}, {"title": "2.4 Procedure for GWQI prediction", "content": "In recent years, for predicting GWQI, ML has become a better choice for minimizing model\nuncertainty and making more reliable GWQI predictions. Researchers employ various ML\nalgorithms to calculate the performance and accuracy of these predictive GWQI models [74].\nFigure 2 illustrates the workflow diagram which has been used in this study. A structured\nGWQI prediction technique using ML is as follows:"}, {"title": "2.4.1 Data pre-processing", "content": "It is one of the most important steps before adding any data into the ML algorithms. It makes\nsure that data is in suitable format for further training procedures. The major parts of the data\npre-processing process include data cleaning, feature selection, data standardization and data\nsplitting."}, {"title": "2.4.1.1 Data Cleaning", "content": "Data cleaning is considered to be the first and foremost step in data pre-processing. It en-\nsures that the dataset is suitable for further analysis and is also consistent and accurate. This\nprocess involves various steps like filling null values, removing duplicates and outliers. Due\nto data entry errors or incomplete surveys many missing values may arise in the dataset. To\nhandle these, the missing numerical data is filled using the mean by preserving the overall\ndistribution and ensuring consistency. Likewise, the missing categorical data is filled using\nthe mode hence maintaining the data integrity. Duplicate records occur in a dataset when the\nsame data is present in more than one rows. This may happen due to accidental duplication\nin data entry, merging multiple datasets or due to some errors in data collection. It needs to be\nremoved as it causes redundancy which results in more computational costs and increasing\nstorage. Outliers are the data points that deviate from all other observations. As these extreme\nvalues can disturb the machine learning models, hence removing these outliers improves the\nmodel's performance reducing the computational complexity. In this study the outliers are\nremoved using the Interquartile Range (IQR) method. In this method, first we calculate the\nInterquartile Range (IQR) which is given by eq 3.\n$IQR = Q3 - Q1$ (3)\nwhere, Q1 is the first quartile i.e. the value below which 25% of the data falls and Q3 is the\nthird quartile i.e. the value below which 75% of the data falls. Hence, the middle 50% of the\ndataset is given by the Interquartile range (IQR). After calculating the IQR, the outlier bounds\nare calculated where the lower bound is given by Q1 \u00d7 1.5 and upper bound is given by Q3"}, {"title": "2.4.1.2 Feature Engineering", "content": "In any ML problem, selecting the required features is considered to be one of the main steps.\nBy removing all the irrelevant features helps in improving our model's accuracy and making\nit more efficient. In this study from among all the features we selected nine most crucial\ngroundwater quality indicators (pH, EC (Electrical conductivity), TH (Total hardness), Ca\n(Calcium), Mg (Magnesium), Na (Sodium), K (Potassium), F (Fluoride), Cl (Chloride)). These\nnine indicators were chosen because they together represent a combination of both physio-\nchemical and heavy metal water quality indicators. For our model, these nine combined\nfeatures are chosen to be the independent variable and the calculated GWQI score has been\nchosen as the dependent variable (or target variable)."}, {"title": "2.4.1.3 Data Standardization", "content": "Data Standardization (also known as Data Normalization) helps in bringing all the ground-\nwater quality indicators into a common scale. By doing so it reduces the number of model"}, {"title": "2.4.1.4 Data splitting", "content": "After scaling the data, it needs to be split into training sets and testing sets. The training\nset is used to train the ML model whereas the testing set is used to check the ML model's\nperformance. There are various ways of splitting the data like 80% and 20%, 70% and 30%,\n60% and 40% or 50% and 50% [75]. In this study we have used 80% of the data for training\nthe model (i.e. learning the patterns in the data) and 20% of the data for testing the model (i.e.\nfor evaluating the model's accuracy). After splitting five ML algorithms are used for training\nand testing the dataset."}, {"title": "2.5 Machine Learning Algorithms", "content": "In this study Machine Learning models have been used for Regression analysis to model\nthe relationship between independent variables and the dependent variable (or target vari-\nable). However, traditional regression methods such as linear regression, logistic regression\nor ridge regression often fail to capture nonlinear dependencies in data resulting to inaccurate\npredictions. However, advanced machine learning algorithms, especially ensemble learning\nmethods, have predominantly improved the predictions by combining multiple weak classi-\nfiers into a strong model for predictions. Among these, gradient boosting techniques such as\nXGBoost, LightGBM, and CatBoost, along with conventional Gradient Boosting and Random\nForest, have been used for comparing and evaluating the performance for predicted Ground-\nwater Quality Index (GWQI). A brief overview of all the algorithms is given below:"}, {"title": "2.5.1 XGBoost (Extreme Gradient Boosting)", "content": "Extreme Gradient Boosting or in short XGBoost, is an improved Gradient Boosting algorithm\ndesigned for high-performance regression and classification tasks. In addition to conventional\nGradient Boosting, Extreme Gradient Boosting integrates advanced optimization techniques"}, {"title": "2.5.2 LightGBM (Light Gradient Boosting Machine)", "content": "Light Gradient Boosting Machine also known as LightGBM is another gradient boosting algo-\nrithm which is used to improve the accuracy of the predictions. The key idea for developing\nLightGBM was to reduce memory usage and training time by keeping the model's perfor-\nmance intact. It was able to achieve this by using histogram-based learning, which involves\nbinning data into histograms for faster computation. Also, instead of growing depth-wise,\nthe leaves in this algorithm grows leaf-wise making it more efficient for large datasets [77].\nLightGBM also optimizes the same objective function as XGBoost:"}, {"title": "2.5.3 CatBoost (Categorical Boosting)", "content": "Categorical Boosting or CatBoost is another gradient boosting algorithm which is used specif-\nically to deal with categorical data efficiently. This is because XGBoost and LightGBM require\na lot of pre-processing such as one hot encoding, but CatBoost encodes categorical features\nwithout reducing the information. It uses symmetric trees where splits are made for all the\nnodes based on the same feature [78]. CatBoost minimizes the loss function:\n$L = \\sum_{i=1}^{n}I(z_i, F(z_i)) + \\sum_{t=1}^{T}\\Omega(f_t)$ (9)\nwhere, $F(z_i)$ is the final prediction as an ensemble of weak learners, $I(z_i, F(z_i))$ is the error\nfunction and $\u03a9(f_t)$ is the regularization term to prevent overfitting.\nHowever, CatBoost has two key advancements:\n1. Ordered Boosting: It is a technique to prevent target leakage through correct gradient\nestimation.\n2. Efficient Categorical Encoding: It reduces computational complexity with a specially\ndesigned encoding method."}, {"title": "2.5.4 Gradient Boosting Regression (GBR)", "content": "Gradient Boosting Regressor in short GBR sticks to a sequential learning process such that\neach new weak learner is trained with the help of the residual errors of the previous mod-\nels. This method eventually gives better predictions by minimizing the loss function through\ngradient descent [79]. Mathematically, the model is updated iteratively as follows:\n$F_{m+1}(x) = F_m(x) + \\gamma h_m(x)$ (10)\nwhere, $F_m(x)$ is the current model, $h_m(x)$ is the new weak learner trained on residuals\nand $\u03b3$ is the learning rate. The GBR model is able to capture complex relationships but is\ncomputationally intensive as compared to XGBoost and LightGBM."}, {"title": "2.5.5 Random Forest Regressor", "content": "Random Forest or random decision forests is an ensemble learning technique which was cre-\nated for applications involving regression and classification tasks. It builds multiple decision\ntrees and the average of their predictions is used to increase the generalization. Each decision\ntree makes its own decision without depending upon the other trees. It differs from boosting\nalgorithms as it uses bootstrap aggregation (bagging) to decrease variance rather than making\nsequential refinements of weak learners [79]. The final prediction is calculated by using the\nfollowing formula:\n$\\hat{z}_i = \\frac{1}{M}\\sum_{n=1}^{M} f_m(x)$ (11)\nwhere, M is the total number of trees and $f_m(x)$ is an individual tree's prediction. Random\nForest is quite robust to overfitting and noise which makes it suitable for high dimensional\ndata."}, {"title": "2.5.6 LCBoost Fusion (LightGBM + CatBoost)", "content": "LCBoost Fusion is a hybrid machine learning model that combines both LightGBM and Cat-\nBoost for maximum predictive accuracy and efficiency. LightGBM is very fast in computation\nas it uses histogram-based learning, coupled with a leaf-wise tree growth strategy, making it\nquite efficient. On the other hand, CatBoost uses ordered boosting to reduce overfitting and\nhas better management of categorical features which helps in improving the stability of the\nmodel. Thus, the two powerful algorithms combined in LCBoost Fusion are able to utilize\ntheir respective strengths to produce a well-balanced, high performing regression model. The\nintegration is optimized further by Differential Evolution Optimization to ensure that weights\nare adapted in an adaptive manner for better generalization and precision.\nThe LCBoost Fusion model combines predictions from both LightGBM and CatBoost us-\ning optimized weights. Given a dataset, D = $(x_i,y_i)$ 1 \u2264 i \u2264 n where, $x_i$ represents the feature\nvectors and $y_i$ represents the target values.\nThe individual models learn functions $f_{LGB}(x)$ and $f_{cat}(x)$ such that:\n$\\hat{z}_{LGB} = f_{LGB}(x)$ and $\\hat{z}_{Cat} = f_{cat}(x)$\nwhere, $\\hat{z}_{LGB}$ and $\\hat{z}_{cat}$ are the predicted values from LightGBM and CatBoost, respectively.\nEach model minimizes the following loss function:\n$L(z, \\hat{z}) = \\frac{1}{n}\\sum_{i=1}^{n}I(z_i, \\hat{z}_i)$ (12)\nwhere, $I(z_i, \\hat{z}_i)$ is a predefined loss function i.e. Mean Squared Error (MSE) for regression:"}, {"title": "LCBoost Fusion Model: Weighted Ensemble Approach", "content": "The final prediction of LCBoost Fusion is given by linearly combining the predictions of the\ntwo models:\n$\\hat{z}_{LCBoost Fusion} = W_{Cat} \\hat{z}_{Cat} + W_{LGB} \\hat{z}_{LGB}+ b$ (14)\nwhere, $W_{Cat}$ and $W_{LGB}$ are the optimized weights for CatBoost and LightGBM respectively\nand b is a bias term that adjusts for systematic prediction errors.\nTo find the optimal weights, we minimize the Root Mean Squared Error (RMSE) between\nthe actual and predicted values:\n$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(z_i - \\hat{z}_{LCBoost Fusion})^2}$ (15)\nThis optimization problem is solved using Differential Evolution (DE) which is a global\noptimization algorithm that iteratively refines the weights and bias.\nOptimization Using Differential Evolution (DE) :\nThe goal of the optimization is to find the optimal set of weights (w1, w2, b) that minimizes\nthe RMSE:\n$\\min \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(z_i - ((W_{cat} \\hat{z}_{Cat} + W_{LGB} \\hat{z}_{LGB} + b))^2)}$ (16)\nSubject to the constraints 0.4 < $W_{cat}$ \u2264 1.0, 0.4 < $W_{LGB}$ \u2264 1.0, \u22125 \u2264 b \u2264 5.\nThe Differential Evolution (DE) Algorithm optimizes these parameters iteratively by eval-\nuating a range of possible solutions and then analyzing their performance and selecting the\nbest ones based on the RMSE metric.\nAdvantages of LCBoost Fusion\nThe combination of LightGBM and CatBoost in LCBoost Fusion generates a highly efficient\nand accurate regression model. The integration of LightGBM's speed and efficiency with Cat-\nBoost's strong categorical data handling abilities produces improved predictive performance\nwith maintained computational efficiency. The use of Differential Evolution Optimization"}, {"title": "3 Results", "content": "3.1 Data Pre-processing (Data Cleaning, Feature Engineering)\nData Pre-processing ensures that before adding any data into the ML algorithms the data\nshould be in suitable format for further training procedures. Data cleaning is considered to\nbe the first step and involves various steps like filling null values, removing duplicates and\noutliers. To handle these, the missing numerical data is filled using the mean by preserving\nthe overall distribution and ensuring consistency. Likewise, the missing categorical data is\nfilled using the mode hence maintaining the data integrity. Duplicate records are required to\nbe removed as it causes redundancy which results in more computational costs and increasing"}, {"title": "3.2 Model Evaluation Metrics", "content": "Model evaluation metrics are crucial for assessing the performance of machine learning mod-\nels, mainly in regression tasks. Root Mean Squared Error (RMSE), Mean Squared Error (MSE),\nMean Absolute Error (MAE), and R-squared ($R^2$) are mostly used to compare different mod-\nels and determine their predictive accuracy. Equation 17 gives the Root Mean Square Error\n(RMSE) equation is a commonly used metric that measures the square root of the average\nsquared differences between predicted and actual values [80]. It is sensitive to large errors,\nmaking it useful for detecting outliers. The formula for RMSE is:\n$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{z}_i - z_i)^2}$ (17)\nA lower RMSE value indicates better predictive accuracy [83]. Equation 18 gives the Mean\nSquare Error (MSE) equation that measures the average squared differences between pre-\ndicted and actual values [81]. It penalizes larger errors more than smaller ones, making it\nuseful for identifying high variance in predictions. The formula is:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (z_i - \\hat{z}_i)^2$ (18)\nLower MSE values indicate better model performance. Equation 19 gives the Mean Ab-\nsolute Error (MAE) equation which calculates the average absolute differences between pre-\ndicted and actual values . Unlike MSE, it treats all errors equally without squaring them. The\nformula is:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |z_i - \\hat{z}_i|$ (19)\nLower MAE values indicate more precise predictions. Equation 20 gives the R-Squared\n($R^2$) Score that represents the proportion of variance in the dependent variable that is pre-\ndictable from the independent variables[82, 83]. It is defined as:\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(z_i - \\hat{z}_i)^2}{\\sum_{i=1}^{n}(z_i - \\bar{z})^2}$ (20)\nAn $R^2$ value closer to 1 signifies a better fit, indicating that the model explains most of the\nvariance in the target variable [84]."}, {"title": "3.3 Model Training Results", "content": "System Configurations:\nThe computational experiments were carried out on a Windows 11 system with the follow-\ning configuration: Windows version 10.0.26100, running on an x64-based architecture. The\nmachine is equipped with a 12th Gen Intel (R) Core (TM) i5-12450H processor, featuring 8\ncores and operating at a maximum clock speed of 2.0 GHz. The system has a total of 7.9 GB\nof physical memory (RAM). The storage system includes multiple drives: the C: drive has a\ntotal capacity of 156.6 GB, with 61.5 GB free, while the D: drive has 353.1 GB of total space,\nwith 350.5 GB free. An additional G: drive is present with a total size of 161.0 GB, leaving\n144.6 GB available for operations. The system is powered by Intel(R) UHD Graphics, and no\ndedicated NVIDIA GPU is detected, necessitating CPU-based computation for all tasks. This\nconfiguration ensures the system's capability to handle various workloads efficiently.\nFrom the Table 6, we are able to analyze the performance of five machine learning models\ni.e XGBoost, LightGBM, CatBoost, Gradient Boosting (GB), and Random Forest by using the\nfour model evaluation metrics i.e. Root Mean Squared Error (RMSE), Mean Squared Error\n(MSE), Mean Absolute Error (MAE) and $R^2$ Score ($R^2$). In Table 7 there is a detailed descrip-\ntion of the hyperparameters used to predict the Groundwater Quality Index (GWQI) score\nduring the training and evaluation process. To maximize the performance of the models Cat-\nBoost, LightGBM and LCBoost Fusion, the given hyperparameters have been decided to be\nused.\nFrom these models, CatBoost shows the best overall performance with the lowest RMSE\n(0.1148), MSE (0.0131), and MAE (0.0572), with highest $R^2$ score of 0.9853, showing superior\npredictive accuracy. LightGBM also performs very well, with an $R^2$ score of 0.9763 and RMSE\n(0.1459), MSE (0.0213), MAE (0.0677). We chose these models as they belong to the family\nof gradient boosting algorithms, which are convenient for structured data and provide high\naccuracy while handling complex patterns. XGBoost, LightGBM, and CatBoost are widely\nused for their efficiency and ability to mitigate overfitting, whereas Random Forest serves as\na benchmark due to its robustness and interpretability. By observing the strong individual\nperformance of CatBoost and LightGBM, we have proposed their fusion into a hybrid model\ni.e. LCBoost Fusion, to leverage their respective strengths. CatBoost efficiently handles cat-\negorical data and reduces overfitting, while LightGBM is optimized for speed and memory\nefficiency with large datasets. By integrating these models, LCBoost Fusion aims to further\nenhance generalization, reduce error rates, and provide a better predictive accuracy across\ndiverse datasets.\nFor GWQI prediction, LCBoost Fusion, which is the combined form of CatBoost and Light-\nGBM models has been used to enhance the predictive performance. A 10-fold cross-validation\nstrategy has been used to ensure better training and evaluation. The dataset underwent pre-\nprocessing, where missing values were imputed using column-wise means, and feature scal-\ning was applied using StandardScaler to optimize model convergence, particularly for Light-\nGBM. The CatBoost model was trained with 300 estimators, a learning rate of 0.03, and a max-\nimum depth of 6, while the LightGBM model utilized an optimized hyperparameter values\nwith a maximum depth of 4 and 31 leaves. To further refine predictions, a Differential Evolu-\ntion (DE) optimization algorithm has been used to determine the optimal combined weights\nfor LCBoost Fusion.From Table 8 we can clearly demonstrate that LCBoost Fusion outper-\nformed both individual models, achieving the lowest training RMSE (0.3324), MSE (0.1106),\nand MAE (0.1912), with an exceptionally high $R^2$ score of 0.9956, signifying its superior ability\nto capture underlying patterns in the data.\nFor getting the generalization capability of the models, the same 10-fold cross-validation\nhas been conducted on the unseen data. From Table 9 we can see that CatBoost model a\nvalidation RMSE (0.7507), MSE (0.6355), MAE (0.3448) and an $R^2$ score of 0.9763, while the\nLightGBM model also had a similar performance with an RMSE (0.7605), MSE (0.6091), \u039c\u0391\u0395\n(0.3569) and an R2 score of 0.9769. However, LCBoost Fusion has shown exceptionally bet-\nter generalization performance, with lowest validation RMSE (0.6829), MSE (0.5102), MAE\n(0.3147) and the highest R\u00b2 score of 0.9809, indicating reduced prediction error and improved\npredictive accuracy. The consistent outperformance of LCBoost Fusion across all validation\nmetrics shows the effectiveness of ensemble learning and demonstrating that combining the\nstrengths of multiple models can significantly enhance predictive accuracy. These findings\nestablish LCBoost Fusion as a highly efficient and robust approach for predictive modeling,\nmaking it a valuable tool for complex real-world applications.\nIn contrast, the LCBoost Fusion Model shows the highest alignment with the ideal fit,\neffectively combining the strengths of both CatBoost and LightGBM. This suggests that the\nhybrid model reduces prediction errors and enhances generalization. Overall, the LCBoost\nFusion Model outperforms the individual models, making it a promising approach for im-\nproving predictive accuracy in this context.\nFigure 8 gives the feature importance analysis for Groundwater Quality Index (GWQI) pre-\ndiction using the LCBoost Fusion Model, which integrates CatBoost and LightGBM. The bar\nchart ranks the features based on their contribution to the model's predictive performance.\nPotassium (K) is identified as the most influential feature, contributing 34.15% to the model's\npredictions, followed by Fluoride (F) with 25.97% and Total Hardness (TH) with 22.10%. Elec-\ntrical Conductivity (EC) and Magnesium (Mg) also play significant roles, with respective im-\nportance values of 16.29% and 11.58%. The remaining features, including Calcium (Ca), Chlo-\nride (Cl), Sodium (Na), and pH, exhibit comparatively lower importance, with pH having the"}, {"title": "4 Discussion", "content": "4.1 Summary\nThis study presents a comprehensive Spatio - Temporal Groundwater Quality Risk assess-\nment in Odisha using machine learning techniques. The research focuses on groundwa-\nter contamination in Sukinda Valley of Jajpur district due to chromite mining, leveraging a\ndataset from the Central Ground Water Board (CGWB) covering the years 2019-2022. The"}, {"title": "4.2 Implications", "content": "This study's findings help the policymakers to establish specific region's groundwater quality\nmanagement guidelines ensuring a good health free from kidney and liver damage risks. The\nresearch provides a data-driven tools to assess the groundwater quality, which can help gov-\nernment agencies and environmental organizations for decision-making. Understanding and\nanalyzing groundwater contamination levels"}]}