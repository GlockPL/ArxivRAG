{"title": "Improving the Quality of Persian Clinical Text with a Novel Spelling\nCorrection System", "authors": ["Seyed Mohammad Sadegh Dashti", "Seyedeh Fatemeh Dashti"], "abstract": "The accuracy of spelling in Electronic Health Records (EHRs) is a critical factor for efficient clinical care, research, and ensuring patient safety. The Persian language, with its abundant vocabulary and complex characteristics, poses unique challenges for real-word error correction. This research aimed to develop an innovative approach for detecting and correcting spelling errors in Persian clinical text.", "sections": [{"title": "1. Introduction", "content": "Spelling correction is a vital task in all text processing environments, with its importance amplified for languages with intricate morphology and syntax, such as Persian. This significance is further heightened in the realm of clinical text, where precise documentation is a cornerstone for effective patient care, research, and ensuring patient safety The written text of medical findings remains the essential source of information for clinical decision making. Clinicians prefer to write unstructured text rather than filling out structured forms when they document the progress notes, due to time and efficiency constraints [1]. The quality and safety of health care depend on the accuracy of clinical documentation [2]. However, misspellings often occur in clinical texts because they are written under time pressure [3].\nThe process of spelling correction primarily tackles two types of errors: non-word errors, which are nonsensical words not found within a dictionary, and real-word errors, that are correctly spelled words but utilized inappropriately in context. These errors can stem from various sources including typographical mistakes, confusion between similar sounding or meaning words [4], incorrect replacements by automated systems like AutoCorrect features [5], and misinterpretation of input by ASR and OCR systems [6-9].\nThe Persian language, with its rich vocabulary and complex properties, presents unique challenges for real-word error correction. Features unique to Persian such as homophony (words that are pronounced identically yet carry distinct meanings), polysemy (words with multiple meanings), heterography (words that share identical spelling but their meanings vary based on how they are pronounced), and word boundary issues contribute to this complexity.\nDespite these challenges, numerous efforts have been made to develop both statistical and rule-based approaches for identifying and rectifying both classes of errors in the general Persian text domain; however, the work in the Persian medical domain and specifically the Persian clinical text is very limited. Moreover, these methods have attained only limited success. In this study, we introduce an innovative method to detect and correct word errors in Persian clinical text, aiming to significantly improve the accuracy and reliability of healthcare documentation. Our key contributions include:\n\u2022 Language Representation Model: We showcase a pre-trained language representation model that has undergone meticulous fine-tuning, specifically for the task of spelling correction in the Persian clinical domain."}, {"title": "2. Related Works", "content": "Automatic word error correction is a crucial component in NLP systems, particularly in the context of EHR and clinical reports. Early techniques were based on edit distance and phonetic algorithms [10-13]. The incorporation of context information has been demonstrated to be effective in boosting the efficiency of auto-correction systems [14]. Contextual measures like semantic distance and noisy channel models based on N-grams have been employed across numerous NLP applications [4, 5, 15-17]. A novel approach was also developed to correct multiple context-sensitive errors in excessively noisy situations [18]. Dashti developed a model that addressed the identification and automatic correction of context-sensitive errors in cases where more than one error existed in a given word sequence [19].\nCutting-edge methods in NLP systems utilize context information through neural word or sense embeddings for spelling correction [20]. Pretrained contextual embeddings have been used to detect and rectify context-sensitive errors [21]. The issue of spelling correction has been addressed using deep learning techniques for various languages in recent years. For example, a study in 2020 proposed a deep learning method to correct context-sensitive spelling errors in English documents. [22]. Another work developed a BERT-Based model for the same purpose [23]. NeuSpell is a user-friendly neural spelling correction toolkit that offers a variety of pre- trained models [24]. SpellBERT is a lightweight pre-trained model for Chinese spelling check [25]. A disentangled phonetic representation approach for Chinese spelling correction was proposed [26]. Other approaches for Chinese spelling correction utilized phonetic pre-training [27]. An innovative approach was devised specifically for the purpose of contextual spelling correction within comprehensive speech recognition systems [28]. A dual-function framework for detecting and correcting spelling errors in Chinese was proposed [29]. Liu and colleagues proposed a method, known as CRASpell, which is resilient to contextual typos and has been developed to enhance the process of correcting spelling errors in Chinese [30]. AraSpell is an Arabic spelling correction approach that utilized a Transformer model to understand the connections between words and their typographical errors in Arabic [31].\nIn the realm of healthcare, the application of spelling correction techniques has been instrumental in expanding acronyms and abbreviations, truncating, and rectifying misspellings. It has been observed that such instances constitute up to 30% of clinical content [32]. In the last twenty years, a significant amount of research has been conducted on spelling correction methods specifically designed for clinical texts [1]. The majority of these studies have primarily focused on EHR [33], while a few have explored consumer-generated texts in healthcare [34, 35].\nSeveral noteworthy contributions in this field include the French clinical record spell checker introduced by Ruch and colleagues, which boasts a correction rate of up to 95% [36]."}, {"title": "2.1 Persian Spelling Challenges", "content": "Persian, alternatively referred to as Farsi, belongs to the Indo-Iranian subgroup of the Indo- European family of languages. It holds official language status in countries such as Iran, Tajikistan, and Afghanistan. Over time, Persian has incorporated elements from other languages such as Arabic, thereby enriching its vocabulary. Despite these influences, the fundamental structure of the language has largely remained intact for centuries [55, 59].\nWhile Persian is a vibrant and expressive language, it presents several challenges for language processing:\n1. Character Ambiguity: Persian characters like \u201c\u0649\u201d and \u201c\u064a\" are often used interchangeably but represent different sounds [60].\n2. Rich Morphology: New words can be created by adding prefixes and suffixes to a base word like \"\u062f\u0633\u062a\u201d )hand) to \u2018\u2018\u062f\u0633\u062a\u0647\u0627\u201d )hands( ]61[.\n3. Orthography: Persian involves a combination of spaces and semi-spaces, which can lead to inconsistencies [62].\n4. Co-articulation: The pronunciation of a consonant like \u201c\u0628\u201d can be affected by the subsequent vowel [63].\n5. Dialectal Variation: Persian has several standard varieties such as Farsi, Dari, and Tajik [64].\n6. Cultural Factors: The phenomenon of persianization can shape the way Persian is used and interpreted.\n7. Lack of Resources: Often, Persian is classified as a language with limited resources, given the scarcity of accessible data and tools for Natural Language Processing [61].\n8. Free Word Order: Persian allows for the rearrangement of words within a sentence without significantly altering its meaning [65].\n9. Homophony: Different words have identical pronunciation but different meanings, like )\u06af\u0630\u0627\u0631\" /guzar transition') and )\u06af\u0632\u0627\u0631 /guzar/ predicate'( ]66[.\n10. Diacritics: They are frequently left out in writing, leading to ambiguity in word recognition [67].\n11. Rapidly Changing Vocabulary: Persian's vocabulary is rapidly evolving due to factors such as technology, globalization [68].\n12. Lack of standardization: There isn't a single standard for Persian text, which can complicate the development of language processing models capable of handling a variety of dialects and styles [69].\nA significant issue is the treatment of internal word boundaries, often represented by a zero-width non-joiner space or \u201cpseudo-space\". Ignoring these can lead to text processing errors. Pre- processing steps can help resolve these issues by correcting pseudo and white spaces according to internal word boundaries and addressing tokenization problems.\nThese challenges highlight the need for robust computational models and resources that can handle the intricacies of the Persian language while ensuring accurate language processing."}, {"title": "3. Material and Methods", "content": "Our methodology detects and corrects two categories of mistakes in Persian clinical text: Non- word and Real-word errors. The architecture of the proposed system is depicted in Figure 1. The system design is composed of five distinct modules that communicate via a databus.\nThe INPUT module accepts raw test corpora. The pre-processing component normalizes the text and addresses word boundary issues. The contextual analyzer module assesses the contextual similarity within desired word sequences.\nFor error detection, we implement a dictionary reference technique to pinpoint non-word errors and use contextual similarity matching to detect real-word errors. The error correction module rectifies both classes of errors using context information from a fine-tuned contextual embeddings model, in conjunction with orthographic and edit-distance similarity measures.\nThe corrected corpora or word sequence is then delivered through the OUTPUT module."}, {"title": "3.1 Pre-processing step", "content": "Text pre-processing is a crucial step in numerous NLP applications, which includes the segmentation of sentences, tokenization, normalization, and the removal of stop-words. The segmentation of sentences involves determining the boundaries of a sentence, usually marked by punctuation such as full stops, exclamation marks, or question marks. Tokenization is the process of decomposing a sentence into a set of terms that capture the sentence's meaning and are utilized for feature extraction. Normalization is the procedure of converting text into its standard forms and is particularly important in NLP applications for Persian, as it is for many other languages. A key task in normalizing Persian text is the conversion of pseudo and white spaces into regular forms, replacing whitespaces with zero-width non-joiners when necessary.\nFor example )\u0645\u06cc\u0634\u0648\u062f mi f\u00e6v\u00e6d/ 'is becoming') is replaced with )\u2018\u0645\u06cc\u0634\u0648\u062f / mi\u017f\u00e6v\u00e6d / is becoming'). Persian and Arabic have numerous similarities, and certain Persian alphabets are frequently incorrectly written using Arabic versions. It is often advantageous for researchers to normalize these discrepancies by substituting Arabic characters )\u064a \u2018Y' /j/; \u06a9 \u2018k' /k/; \u0647\u2018h' /h/) with their corresponding Persian forms. For instance)\u0628\u0631\u0627\u064a b\u00e6ray/ 'for') is transformed to )\u2018\u0628\u0631\u0627\u06cc\u2019 /b\u00e6ray/ 'for'). Normalization also includes removing diacritics from Persian words; e.g., )\u2018\u0630\u0631\u0651\u0647\u2019 /z\u00e6rre/ \u2018particle) is changed to )\u2018\u0630\u0631\u0647\u2019 /z\u00e6re/ \u2018particle). Additionally, Kashida(s) are removed from words; for instance)\u0628\u0640\u0640\u0640\u0640\u0640\u0640\u0640\u0627\u0646\u062f /band/ band') is transformed to )\u2018\u066c\u0628\u0627\u0646\u062f /band/ \u2018band'(. \nIn order to accomplish the goal of normalization, a dictionary named Dehkhoda, which includes the correct typographic form of all Persian words, is utilized to determine the standard form of words that have multiple shapes [70]."}, {"title": "3.2 Damerau-Levenshtein distance and candidate generation", "content": "Our methodology employs the Damerau-Levenshtein distance metric to generate potential rectifications for both non-word and real-word errors. [11]. This measure considers insertion, deletion, substitution, and transposition of characters. For instance, the measure of Damerau- Levenshtein distance between \"KC\" and \"CKE\" equals 2. It's found that around 80% of human- generated spelling errors involve these four error types [71]. Studies indicate that context-sensitive error constitute approximately 25% to 40% of all typographical errors in English documents. [72, 73].\nOur model utilizes an extensive dictionary to pinpoint misspellings. This dictionary is bifurcated into two segments: general and specialized terms. For the general segment, we employ the Vafa spell-checker dictionary, a highly respected spell checker for the Persian language. This dictionary encompasses 1,095,959 terms, all of which are general terms, but it excludes specialized medical terminology. In this research, we utilized the texts we trained to formulate a custom dictionary. This dictionary integrates specialized terminology found in breast ultrasonography, head and neck ultrasonography, and abdominal and pelvic ultrasonography texts. It was further enriched with translations from the Radiological Sciences Dictionary by David J Dowsett to pinpoint misspellings of specialized terms [74]. This dictionary comprises 10,332 terms, all of which are specialized terms in the field of breast ultrasound, head and neck ultrasound, and abdominal and pelvic ultrasound. However, this specialized dictionary does not encompass general terms.\nTo circumvent duplication of specialized terms, we juxtaposed our comprehensive dictionary with the Radiological Sciences Dictionary using a custom software developed by the researchers of this study. This ensured that no term was included more than once in the dictionary, as some terms might be present in both dictionaries.\nUpon our analysis of the test data, we concluded that an edit distance of up to 2 between the candidate corrections and error would be ideal. With an edit distance set to one, an average of three candidates are generated as potential replacements for a target context word. However, when the edit distance is increased to 2, the average number of generated candidates rises to 15. Correspondingly, the computation time also increases. We ensure that the generated candidates are validated against the reference lexicon."}, {"title": "3.3. Contextual embeddings", "content": "Word embeddings, which analyze vast amounts of text data to encapsulate word meanings into low-dimensional vectors [75, 76], retain valuable syntactic and semantic information [77] and are advantageous for numerous NLP applications [78]. However, they grapple with the issue of meaning conflation deficiency, which is the inability to differentiate between multiple meanings of a word.\nTo tackle this, cutting-edge approaches represent specific word senses, referred to as contextual embeddings or sense representation. Context-sensitive word embedding techniques such as ELMo consider the context of the input sequence [64]. There exist two main strategies for pre-training language representation model: feature-oriented methods and fine-tuning methods [79]. Fine- tuning techniques train a language model utilizing large datasets of unlabeled plain texts. The parameters of these models are later fine-tuned using data that is pertinent to the task at hand [79- 81]. However, pre-training an efficient language model demands substantial data and computational resources [82-85]. Models that are multilingual have been formulated for languages that share morphological and syntactic structures. However, languages that do not use the Latin script significantly deviate from those that do, thereby requiring an approach that is specific to each language [86]. This challenge is also common in the Persian language. Although some multilingual models encompass Persian, their performance may not match that of monolingual models, which are specifically trained on a language-specific lexicon with more extensive volumes of Persian text data. As far as we are aware, ParsBert [87] and SinaBERT [88] are the sole efforts to pre-train a Bidirectional Encoder Representation Transformer (BERT) model explicitly for the Persian language."}, {"title": "3.3.1 Pre-trained Language Representation Model", "content": "Persian is often recognized as an under-resourced language. Despite the existence of language models that support Persian, only two, namely ParsBert [87] and SinaBERT [88], have been pre- trained on large Persian corpora. ParsBERT was pre-trained on data from the general domain, which includes a substantial amount of informal documents such as user reviews and comments, many of which contain misspelled words.\nConversely, SinaBERT was pre-trained on unprocessed text from the overarching medical field. The data for SinaBERT was compiled from a diverse set of sources such as websites that provide health and medical news, websites that disseminate scientific information about health, nutrition, lifestyle, and more, journals (encompassing both abstracts and complete papers) and conference proceedings, scholarly written materials, medical reference books and dissertations, online forums centered around health, medical and health-related Instagram pages, along with medical channels and groups on Telegram.\nThe data primarily consisted of general medical domain data, a portion of which was informal and contained misspellings. These factors make these pre-trained models unsuitable for Persian clinical domain spelling correction tasks. The lack of an efficient language model in this domain poses a considerable hurdle. In the subsequent section, we will explore our Persian Clinical Corpus and the procedure of pre-training our language representation model."}, {"title": "3.3.2 Data", "content": "While numerous formal general domain Persian medical texts are freely accessible, they may not be ideal for spelling correction in clinical texts. Conversely, Persian clinical texts are not widely available to the public. Nevertheless, the use of Persian clinical text is essential for pre-"}, {"title": "3.3.3 Model Architecture", "content": "The structure of our suggested model is founded on the original BERTBASE setup, which comprises 12 hidden layers, 12 attention heads, 768 hidden sizes, and a total of 110M parameters. Our model is designed to handle a maximum token capacity of 512. The architecture of the model is depicted in Figure 2. BERT's success is often attributed to its MLM pre-training task, where it"}, {"title": "3.4.5 Fine-tuning for Spelling Correction Task", "content": "We fine-tuned the pre-trained model specifically for the task of spelling correction in Persian clinical text, aiming to achieve optimal performance. For this fine-tuning process, we utilized 10% of the reserved sentences from the training corpus, amounting to 170,066 sentences. Each input to the model was a single sentence ending with a full stop, as our primary focus was on training the model for spelling correction. Upon examining the test set, we found that many"}, {"title": "3.5 PERTO Algorithm", "content": "We have designed an algorithm called PERTO, which stands for Persian Orthography Matching. This algorithm ranks the most likely candidate words derived from the output of a pre- trained model, based on shape similarity. In this algorithm, every character in the Persian script is given a distinct code. Characters that share similar forms or glyphs are classified under the same code, enabling words with similar shape characters to be identified, even if there are slight spelling variations. Our pioneering hybrid model classifies characters with the same shapes into identical groups, as depicted in Table 3."}, {"title": "3.6 Error Detection Module", "content": "The error detection module utilizes two separate strategies based on the nature of the error being identified. For non-word errors, a lexical lookup approach is employed, while real-word errors are addressed through contextual analysis. The initial step in error detection, irrespective of the error type, involves boundary detection and token identification. Upon receiving an input sentence S, the model first demarcates the start and end of the sentence with Beginning of Sentence (BoS) and End of Sentence (EoS) markers, respectively, markers respectively, and approximates the word count in the sentence:\n< BoS > W\u00a1 Wi+1 Wi+2... Wn < EoS >\nIt's crucial to note that the word count corresponds to the maximum number of iterations the model will undertake to identify an error in the sentence."}, {"title": "3.6.1 Non-word Error Detection", "content": "Spell checkers predominantly employ the lexical lookup method to detect spelling errors. This technique involves comparing each word in the input sentence with a reference dictionary in real-time, which is usually built using a hash table. Beginning with the BoS marker, the model scrutinizes every token in the sentence for its correctness based on its sequence. This process continues until the EoS marker is reached. However, if a word is identified as misspelled, the error detection cycle halts and the error correction phase commences. Here's an illustration of non-word error detection:"}, {"title": "3.6.2 Real-word error detection", "content": "In this study, we employ contextual analysis for the detection of real-word errors. Traditional statistical models relied on n-gram language models to examine the frequency of a word's occurrence and assess the word's context by considering the frequency of the word appearing with \"n\" preceding terms. However, contemporary approaches use neural embeddings to evaluate the semantic fit of words within a given sentence. In our proposed methodology, we utilize the mask feature and leverage contextual scores derived from the fine-tuned bidirectional language model to detect and correct word errors. The process of real-word error detection is explained as follows:\n1) The model begins with the BoS marker and attempts to encode each word as a masked word, starting with the first word.\n2) A list of potential replacements for the masked word is derived from the output of the pre- trained model.\n3) Based on the candidate generation scenario, replacement candidates are generated within edit-distances of 1 and 2 from the masked word.\n4) The list of candidates, along with the original token, is cross-verified against the pre-trained model's output for the masked token.\n5) If a candidate demonstrates a probability value that surpasses that of the masked word, the initial word is considered erroneous, thus bringing the procedure to a close.\n6) However, if no error is detected, the model shifts one unit to the left, and the same steps are reiterated for all words within the sentence until the EoS marker is encountered.\nTherefore, the moment an error is identified, the correction process is initiated immediately; subsequently, the model advances to the next sentence. Pseudocode2 offers an in-depth exploration of the Real-word error detection process."}, {"title": "3.7 Error Correction Module", "content": "The error correction phase is initiated when an error is identified in the input. In this stage, we devise a ranking algorithm that primarily relies on the contextual scores obtained from the fine- tuned pre-trained model and the corresponding PERTO codes between potential candidates and the errors."}, {"title": "3.7.1 Non-word Error Correction Process", "content": "In the non-word error correction process, the following steps are undertaken:\n1) The model initially employs the Damerau-Levenshtein edit distance measure to generate a set of replacement candidates within 1 or 2 edits.\n2) The misspelled word is subsequently encoded as a \u201cmask\" and input into the fine-tuned model.\n3) The model extracts all probable words from the output and matches them against the candidate list.\n4) The model then retains a certain number of candidates with the highest contextual scores. Based on our observations, the optimal number is 10.\n5) The method proceeds to compare the PERTO similarity between the erroneous word and the remaining replacement candidates. If the error and candidate share the same code, that candidate is considered the most suitable word. However, if two or more probable candidates carry the same PERTO code as the erroneous word, then the candidate with the highest contextual score is selected as the replacement for the error.\nPseudocode3 delivers a comprehensive exploration of the Non-word error correction mechanism."}, {"title": "3.7.2 Real-word Error Correction Process", "content": "In the scenario of real-word error correction, the process is as follows:\n1) The contextual scores of potential candidates are retrieved from the fine-tuned model.\n2) The model retains a certain number of candidates with the highest contextual score. Based on our observations, the optimal number is 10.\n3) The method then compares the PERTO similarity between the erroneous word and the replacement candidates. If the error and the candidate share the same code, that candidate is deemed the most suitable word.\n4) However, if two or more probable candidates carry the same PERTO code as the erroneous word, then the candidate with the highest contextual score is selected as the replacement for the error.\nPseudocode4 delivers a comprehensive exploration of the Non-word error correction mechanism."}, {"title": "4. Evaluation and Results", "content": "In this section, we first conduct an analysis of the test data. Following this, we evaluate our method's performance and compare it with various baseline models in the task of spelling correction. This comparison will offer valuable insights into the efficacy and precision of our approach in identifying and rectifying spelling errors."}, {"title": "4.1 Test Dataset", "content": "Our test datasets consist of 188,963 reserved sentences derived from the Persian clinical corpus. Upon scrutinizing the errors present in the test dataset, we found that 1.20% of sentences exhibited instances of non-word errors, which equates to 120 errors in every 10,000 sentences. In addition, 0.29% of sentences contained a real-word error, corresponding to 29 errors in every 10,000 sentences. We examined all the erroneous words to categorize them into one of the predefined classes of errors, such as substitution, transposition, insertion, and deletion. The frequency of these errors, based on the error type, is illustrated in Table 5. When addressing both real-word and non-word errors, substitution errors are more prevalent than other types of errors. Furthermore, insertion errors are quite common when dealing with both classes of error, while deletion and transposition errors are the least common.\nWe also analyzed the test dataset for the number of edit distances required for spell correction, the results of which are presented in Table 6. In dealing with both real-word and non-word errors, 86.1% of misspellings required an edit distance of 1 to correct the incorrect word. 13.7% of errors were rectified with an edit distance of 2, and a mere 2.1% of errors fell within an edit-distance of 3 or more. Due to the combinatorial explosion when generating and examining candidates within distance 3, these classes of error were excluded from the dataset.\nUpon conducting a more thorough analysis of the data, we found that 0.8% of sentences contained more than one error. As our method is designed to handle only one-error-per-sentence, we removed these sentences from the test dataset."}, {"title": "4.2 Evaluation metrics", "content": "The principal metrics for evaluating the effectiveness of models on tasks related to non- word and real-word error identification and rectification are precision (P), recall (R), and the F-"}, {"title": "4.3 Baseline Models", "content": "In our research, we implemented two baseline models for non-word correction in Persian clinical text to ensure a comprehensive comparison. These models include the four-gram model introduced by [57], and a Persian Continuous Bag-of-Words (CBOW) model [91]. Both models were developed using Python and trained on the same dataset as the pre-trained model. Our aim is to understand the strengths and weaknesses of these models, and leverage this understanding to enhance error correction in Persian language processing. Unfortunately, for real-word error correction in the Persian medical domain, no prior work has been introduced. Therefore, a meaningful comparison is not achievable at this time. This highlights the novelty and importance of our research in this specific area."}, {"title": "4.3.1 Yazdani, et al.", "content": "The statistical methodology, pioneered by Yazdani and colleagues, stands out as a promising approach for rectifying non-word errors. It is meticulously crafted to address typographical inaccuracies prevalent in Persian healthcare text, thereby enhancing the quality and reliability of the information [57]. This method leverages a weighted bi-directional fourgram language model to pinpoint the most appropriate substitution for a given error. It incorporates a quadripartite equation that assigns priority to n-grams based on their sequence, thereby enhancing the precision of error correction."}, {"title": "4.3.2 CBOW Model", "content": "CBOW model operates by comprehending the semantics of words through the analysis of their surrounding context, and then uses this information as input to predict suitable words for the given context [91]. The architecture of the CBOW model is designed to identify the target word (the center word) based on the context words provided. This model has been specifically trained to tackle the task of non-word error rectification. It employs two matrices to calculate the hidden layer (H): the input matrix (IM) and the output matrix (OM). The CBOW model was trained using a corpus of 1.4 million documents derived from the pre-trained model, which facilitated the generation of the input and output matrices. The training parameters incorporated a context window size of 10 and a dimension size of 300."}, {"title": "4.4 Non-word Error Correction Evaluation", "content": "In the initial phase of assessment, we juxtapose the effectiveness of our suggested methodology with that of the previously mentioned baseline models concerning non-word error rectification. It's crucial to highlight that all models employ a dictionary look-up method for identifying typos, resulting in an F1-score of 100% for typo detection."}, {"title": "4.5 Real-word Error Detection and Correction Evaluations", "content": "We performed a comprehensive evaluation of our proposed model for detecting and correcting real-word errors in Persian clinical text. The results of these evaluations are summarized in Table 8. Our model demonstrated its highest performance in real-word error detection, achieving an F1-Score of 90.6%."}, {"title": "5. Discussion", "content": "Typographical errors, a frequent occurrence in radiology reports often attributed to incessant interruptions and a dynamic work environment, have the potential to endanger patient health, introduce ambiguity, and undermine the reputation of radiologists [92]. The cardinal goal of our research was to pioneer an avant-garde technique for pinpointing and rectifying spelling inaccuracies in Persian clinical text. The elaborate morphology and syntax of the Persian language, intertwined with the pivotal role of meticulous documentation in fostering effective patient care, facilitating research, and safeguarding patient safety, accentuate the gravity of this undertaking.\nWithin the confines of the Imaging Department at Imam Khomeini Hospital, the formulation of radiology reports is an intricate multi-step endeavor that averages around 30 minutes in duration. This process includes dictation by radiologists, transcription by medical typists, and a review and editing process before the final report is stored in the HIS. However, this process includes non-value-added activities, known as 'Muda', particularly the time spent between transcription and final confirmation [93]. Our newly developed software addresses this inefficiency by quickly correcting misspelled words during transcription, reducing the time between initial writing and final confirmation, and thereby decreasing \u2018Muda'.\nOur approach leverages a pre-trained language representation model, fine-tuned specifically for the task of spelling correction in the clinical domain. This model is complemented by an innovative orthographic similarity matching algorithm, PERTO, which uses visual similarity of characters for ranking correction candidates. This unique combination of techniques distinguishes our approach from existing methods, enabling our model to effectively address both non-word and real-word errors. The evaluation of our approach demonstrated its robustness and precision in detecting and rectifying word errors in Persian clinical text. In terms of non-word error correction, our model achieved an F1-Score of 90.0% when the PERTO algorithm was employed. This represents a 1.1% increase in correcting non-word errors compared to using only contextual scores. For real-word error detection, our model demonstrated its highest performance, achieving an F1-Score of 90.6%. Furthermore, the model reached its highest F1-Score of 91.5% for real- word error correction when the PERTO algorithm was employed, indicating an approximate enhancement of 1.5% in the correction F1-Score.\nDespite these promising results, our model has certain limitations. For instance, in a few cases, real-word errors were missed when the erroneous word had a strong semantic connection to the context words. Additionally, while our model is effective in handling non-word and real-word errors, it is not equipped to deal with grammatical errors. Moreover, our model was set up to handle one-error-per-sentence cases and cannot handle more than one error in a sentence. There were a few cases where a sentence included more than two errors.\nBuilding upon our current achievements, the integration of emerging technologies such as BCI eye-tracking, VR/AR, and EEG offers a promising frontier for further enhancing our spelling correction system. These technologies present unique opportunities to address some of the inherent limitations identified in our study. For example, BCIs could offer intuitive, direct error correction interfaces, while eye-tracking might refine error detection based on user interaction patterns. VR/AR could provide immersive training environments, improving proficiency with correction tools, and EEG monitoring could lead to spelling correction interfaces that adapt to user stress levels and cognitive states, ultimately making the correction process less taxing and more efficient.\nWhile prevailing spelling correction mechanisms for the Persian language cater to a broad spectrum and are not tailored to the medical sphere, our innovative system is specifically architected to autonomously pinpoint and amend misspellings prevalent in Persian radiology and ultrasound reports. The seamless integration of automatic spell-checking systems, notably in critical facets for patient safety such as allergy entries, medication details, diagnoses, and problem listings, can substantially bolster the quality and exactness of electronic medical records. Our system, which can be seamlessly integrated as an auxiliary program on platforms like Microsoft Office Word, web-browsers, or employed as an API in the HIS system, expands the potential applications of our model transcending the boundaries of the clinical domain.\nIn summary, the results of this study affirm the potential of our proposed method in transforming Persian clinical text processing. By effectively addressing the unique challenges"}, {"title": "5. Conclusions", "content": "This study presents a novel method for detecting and correcting spelling errors in Persian clinical texts, leveraging a pre-trained model fine-tuned for this specific domain. Our approach has notably outperformed existing models, achieving F1-Scores of over 90% in both real-word and non-word error correction. This advancement underscores the method's robustness and its wide- ranging applicability, from error types like substitution and insertion to deletion and transposition. By integrating our orthographic similarity algorithm, PERTO, with contextual insights, we've significantly enhanced the correction success rate, marking a substantial improvement in spelling error correction for Persian clinical texts.\nThe potential of our methodologies extends beyond medical documentation, offering valuable applications in engineering sciences. The NLP and machine learning techniques employed here could revolutionize error detection and correction in engineering documents and software code, improving review processes, technical documentation accuracy, and software development efficiency. Furthermore, our findings could inform the creation of intelligent diagnostic systems for predictive maintenance and quality control, leveraging our error correction mechanisms for enhanced precision and reliability.\nLooking ahead, we aim to refine our model further to tackle multiple errors within a sentence and address grammatical inaccuracies, broadening our method's comprehensiveness for the Persian medical domain. Additionally, we plan to explore the integration of emerging technologies like BCI, eye-tracking, VR/AR, and EEG, aiming to create more intuitive correction interfaces and immersive training environments. These efforts will not only advance spelling correction tools technically but also amplify their practical impact in medical documentation, contributing to improved patient care and safety."}, {"title": "Declarations", "content": "Ethics approval and consent to participate\nThe study was conducted in accordance with ethical standards and received approval from the Institutional Review Board of the Islamic Azad University, Kerman Branch (approval ID: IR.IAU.KERMAN.REC.1402.124). As the study did not involve any human trials, the requirement for informed consent was waived by the same Institutional Review Board."}, {"title": "Consent for Publication", "content": "Not applicable."}, {"title": "Availability of Data and Materials", "content": "The data that support the findings of this study are held by Imam Khomeini Hospital. They are not publicly accessible due to privacy restrictions. However, they may be available from the authors upon reasonable request and with permission from the hospital."}, {"title": "Competing Interests", "content": "The authors declare that they have no competing interests concerning the publication of this paper."}]}