{"title": "SoftmAP: Software-Hardware Co-design for\nInteger-Only Softmax on Associative Processors", "authors": ["Mariam Rakka", "Jinhao Li", "Guohao Dai", "Ahmed Eltawil", "Mohammed E. Fouda", "Fadi Kurdahi"], "abstract": "Recent research efforts focus on reducing the com-\nputational and memory overheads of Large Language Models\n(LLMs) to make them feasible on resource-constrained devices.\nDespite advancements in compression techniques, non-linear\noperators like Softmax and Layernorm remain bottlenecks due to\ntheir sensitivity to quantization. We propose SoftmAP, a software-\nhardware co-design methodology that implements an integer-\nonly low-precision Softmax using In-Memory Compute (IMC)\nhardware. Our method achieves up to three orders of magnitude\nimprovement in the energy-delay product compared to A100\nand RTX3090 GPUs, making LLMs more deployable without\ncompromising performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are gaining traction for\ndemonstrating great performance in solving various complex\nreal-world tasks, and paving the way towards Artificial General\nIntelligence (AGI) [1], [2]. The last 30 years have witnessed\na tremendous increase in the inference compute capabilities\n(up to 7 orders of magnitude) of LLMs [3]. LLMs follow the\nscaling laws for transformer-based models: their performance\ncan predictably increase with a higher volume of data and\na larger model size. This scalability, however, renders LLMs\nresource-hungry, hence confining their deployment, especially\non resource-limited edge devices [4], [5], [6].\nCompression is used to alleviate the computational/memory\noverheads of LLMs [7]. Quantization amongst other compres-\nsion techniques (like pruning/knowledge distillation) is a pop-\nular topic. Quantization is mapping values in a large set to their\nquantized counterparts in a smaller set. Quantizing weights\nand activations in LLMs reduces memory footprint and speeds\nup the computation. For example, quantizing weights and\nactivations from FP16 to INT8 decreases GPU memory usage\nby half and increases the throughput of performing matrix mul-\ntiplications by up to 2\u00d7 [8], [9]. LLM quantization frameworks\nlike SmoothQuant, QLoRa, ZeroQuant, and AWQ [9], [10],\n[11], [12], [13] have achieved reductions in computational and\nmemory overheads of LLMs by quantizing weights. Beyond\nweights, quantizing activations in LLMs remains a challenge\ngiven that Softmax, a chief part of the attention mechanism,\nis sensitive to quantization [14], [15].\nWhile Softmax is used as the final layer only in deep learn-\ning works (like CNNs and LSTMs), it is significant for LLMs\nas it is a part of the attention mechanism as shown in Fig. 2.\nPrevious works have shown that Softmax becomes a bottleneck\nfor LLMs with longer input sequences, attributing to more\nthan 30% of the overall LLM's latency [15], [16]. We have\ncharacterized Softmax for Llama2-7b on A100 GPU and the\nresults are shown in Fig. 1. Our characterization reveals that\nthe fraction of time occupied by Softmax increases to 38% for\na sequence length of 16384. Moreover, when General Matrix\nMultiplication (GEMM)-based operators are accelerated, the\nnon-GEMM operators including Softmax bottleneck the exe-\ncution time. [17] reveals that the percentage of execution time\noccupied by GEMM-operators when performing inference of\nGPT2-XL on CPU alone was 62% while that of non-GEMM\noperators was 38%. Using a GPU to accelerate, the non-\nGEMM operators' execution time percentage increases to a\ndrastic 78%, rendering it the bottleneck.\nWhen it comes to Softmax quantization, we identify two re-\nsearch gaps in the literature: 1-) the absence of an integer-only\nlow-precision approximation of Softmax that does not affect\nthe inference of state-of-the-art LLMs and 2-) the absence of\na corresponding efficient integer-only custom hardware capa-\nble of implementing the low-precision Softmax for different\nprecisions. To that end, we propose in this paper SoftmAP, a\nsoftware-hardware co-design that enables accelerating integer-\nonly low-precision Softmax on Associative Processors (APs).\nTo bridge the first gap, we use an integer-only approximation\nfor Softmax, and we explore the effect of using low-precision\nSoftmax in the context of State-of-the-Art (SOTA) Llama\nmodels. As for the second gap, we propose the In-Memory"}, {"title": "II. BACKGROUND", "content": "The typical backbone of LLMs is a transformer, and the\ndecoder-only transformer architecture is particularly well-\nsuited for generating text sequentially [20], [21], [22], [23],\n[24]. By leveraging attention mechanisms and deep neural\nnetworks, decoder-only transformers can produce high-quality\ntext outputs that exhibit coherence and relevance to the given\ninput. The transformer consists of input embeddings and\nseveral decoder layers. In the input embedding process, the\ninput text is tokenized and embedded into dense vector rep-\nresentations. The decoder layers enable the model to generate\nCompute (IMC)-based APs as an efficient custom hardware\ncapable of supporting a mix of integer-only low precisions.\nAPs are SIMD-like architectures that perform bit-serial, word\nparallel arithmetic and logical operations [18], [19]. We have\nchosen the AP as our custom hardware for three reasons: 1-)\ntheir IMC nature, whereby the memory wall problem is not an\nissue, 2-) the fact that Softmax should be applied to all words\nin a vocabulary in a parallel fashion hence making the AP\na good candidate for acceleration, and 3-) the ability of APs\nto support mixed-precisions due to its bit-serial word-parallel\noperation. Our contributions are as follows.\n1- To the best of our knowledge, this is the first work\nthat performs a precision sensitivity analysis of an integer-\nonly Softmax approximation to determine which mixed low-\nprecision implementation does not sacrifice the perplexity for\nLLMs like Llama2 family.\n2- We propose a mapping to accelerate the approximated\ninteger-only Softmax with the best mixed-precision on the AP.\n3- We characterize the energy and latency of deploying the\ninteger-only approximated Softmax on the AP, RTX3090 GPU,\nand A100 GPU for Llama2-7b, Llama2-13b, and Llama2-70b.\nOur analysis reveals that the AP achieves up to 1300\u00d7 and\n12.58\u00d7 reduction in energy and latency respectively when\ncompared to the GPU counterparts.\nThe rest is organized as: Section 2 provides background.\nSection 3 elaborates on our software-hardware co-design\nmethodology. Section 4 presents the experimental setup, and\nsection 5 presents the results. Section 6 concludes the work."}, {"title": "A. Large Language Models", "content": "output tokens one at a time, conditioned on the previously\ngenerated tokens. In decoder layers, each decoder block in-\ncludes multi-head attention, layer normalization, and a feed-\nforward network (shown in Fig. 2). The attention mechanisms\nallow the model to weigh the importance of different words or\ntokens in the input text when generating output, which enables\nthe model to capture long-range dependencies and contextual\ninformation effectively. In multi-head attention, the input first\npasses through the multiple linear layers, resulting in multiple\nqueries, keys, and values (Q, K, V). Then, each Q slice and\nK slice are multiplied and scaled to obtain the attention\nscores (each slice from an attention head). Softmax is then\napplied to normalize them, converting them into a probability\ndistribution representing the importance of each token. This\nstep ensures that the model assigns appropriate weights to\ndifferent tokens, facilitating effective information aggregation\nand representation learning. The Softmax is performed as:\n$Softmax(v) = e^{v_i}/\\Sigma_i e^{v_i}$ where $v_i$ is the ith word in\na dictionary. After Softmax, each slice of attention matrix\nis multiplied by V slice and the results from all heads are\nconcatenated. Feed-Forward Neural Networks then process the\noutput of the self-attention and generate representations that\ncapture the relationships between different tokens in the input\ntext. Layer norm is used to stabilize the training process."}, {"title": "B. Associative Processors", "content": "An AP's architecture is shown in Fig. 3. The building block\nof the AP is a Content Addressable Memory (CAM), used to\nstore data. The CAM is made up of Static Random Access\nMemory (SRAM) cells (shown in Fig. 3), each storing a bit.\nThe key register is used to present data to be searched in\nthe CAM, and the mask register is used to deactivate certain\ncolumns during the search operation if needed. Rows in the\nCAM that match the presented data are indicated by the tag\nregister. Finally, the controller orchestrates performing the\narithmetic/logic operations through a series of compare/write\ncycles that follow the Look Up Table (LUT) of the correspond-\ning operation. Without loss of generality, fig. 3 shows an AP\nstoring two vectors A and B, where A=[b'11, b'00, b'10, b'11]\nand B=[b'01, b'01, b'10, b'10]. The goal is to perform the\nXOR operation on the adjacent words in the AP by relying on\nthe LUT (Fig. 3). First, A and B are written in the AP. Then,"}, {"title": "III. SOFTMAP", "content": "Softmax Approximation: To approximate Softmax, we use\ninteger polynomial approximations and Barret reduction [27],\n[28] to speed up its deployment on our hardware. The approx-\nimation is shown in Algorithm 1, which approximates the ex-\nponential function in Softmax by a second-order polynomial,\nhence involving integer-only computations. The algorithm\ntakes the M-bit quantized values in a vocabulary $v_i$ (we omit\nhenceforth the \"i\" for brevity) and their scaling factor S. Since\nwe limit the scaling factor according to the range of data in the"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We integrate the Softmax approximation in the 7B, 13B, and\n70B models in the Llama2 family [20], and vary the precisions\naccording to Table I to estimate the effect of different precision\ncombinations on the perplexity. Note that N represents the ad-\nditional bits needed to store the $sum = \\Sigma_i e^{v_i}$. In the scenario\nwhere we do not truncate the sum, $N = log_2(SequenceLength)$.\nThe perplexity is evaluated on WikiText-2 benchmark [30].\nOur implementation uses PyTorch 2.0.1 [31] and Transformers\n4.41.2 [32]. And all models are obtained from Hugging-\nFace [33]. For language generation tasks, we calculate the\nperplexity as follows: (1) The validation set is concatenated\nusing two linebreaks as separators and encoded using the de-\nfault HuggingFace tokenizer of each model. (2) The sequence\nis split into non-overlapping segments of width 2048, the full\ncontext size of our models. (3) Then, they are sent to the model\nto obtain the log probabilities corresponding to the next token,\nand their exponentiated average is the final standard perplexity.\nWe evaluate all individual samples separately and do not apply\nany padding. We execute the software experiments on one\nNVIDIA RTX3090 GPU and one NVIDIA A100 GPU.\nFor the custom hardware cost estimation, we have designed\na Python-based AP simulator that models the data flow ex-\necution of the integer-only Softmax approximation shown in"}, {"title": "V. RESULTS AND DISCUSSION", "content": "Normalized latency ratios are shown in Fig. 7. A ratio above\n1 favors the AP. For Llama2-7b and Llama2-13b and sequence\nlengths below 1024 (i.e. 128, 256, and 512), the AP needs\nmore time than the GPUs. For Llama2-70b, this pattern is\nobserved for a sequence length below 512. This is due to 1-)\nthe bit-serial, word-parallel operation of the AP and 2-) the\nfact that sequence length affects the number of rows in the\nAP (the AP stores 2 words per row, so the number of rows\nin the AP is sequence length/2). The bit-serial operation\ndepends on precision, while the word-parallel operation on AP\nrow count. If the row count is insufficient, bit-serial overhead\noffsets parallelism, reducing the AP's advantage. In addition,\nwe characterize Softmax's runtime proportion of Llama2-7b\non A100 (shown in Fig. 1): For sequence lengths below 1024\nFP Softmax contributes to up to 3.34% only. For sequence\nlengths greater than 1024, the Softmax proportion increases\nto up to 38%, emphasizing the need to accelerate Softmax\nfor larger sequence lengths (so the acceleration significantly\nimpacts LLMs' overall execution time). Figs. 7 (a), (b), and\n(c) reveal that as sequence length increases between 1024\nand 4096, and across all batch sizes, the AP's latency sav-\nings compared to A100(RTX3090) range respectively between\n1.06\u00d7(1.02\u00d7) and 6.7\u00d7(12.58\u00d7). The 6.7\u00d7 Softmax speedup\nreduces the overall execution time of Llama2-70b by 10.71%\nfor a sequence length of 4096 (Amdahl's law).\nTo better understand the energy-latency trade-offs, we plot\nthe normalized Energy Delay Product (EDP) in Fig. 8 for\nLlama2-13b. We do not include the EDP plots for Llama2-7b\nand Llama2-70b because the trends are similar to Llama2-13b\nand for space limitations. We see that the normalized EDP is\nalways greater than 1, i.e. the AP has the lowest (the best)\nEDP for all sequence lengths, batch sizes, and models. This"}, {"title": "A. Precision Sensitivity Analysis", "content": "First, we evaluate the sensitivity of our approximated\ninteger-only Softmax to precision. We study the perplexity\nwhen using the different mixed-precision combinations re-\nported in Table I for the different parts of the approximated\nSoftmax Algorithm 1. We present in Table III the perplexity\nvalues of Llama2-7b when performing end-to-end inference\nusing the approximated integer-only Softmax and with input\nclipping to [$T_c$, 0], where $T_c$ = \u20137. The selection of clipping\nthreshold $T_c$ primarily considers two factors: a larger $T_c$\nincreases the quantization error within the [$T_C$, 0] range of the\nSoftmax input, while a smaller $T_o$ reduces the quantization\nerror in this range but causes significant clipping in the\n(-\u221e, $T_c$) range, leading to dominant precision loss. By using\nthe WikiText-2 dataset as a calibration dataset, we analyze\nthe input range of Softmax and manually select $T_c$ = -7\nwith M\u2208 {6,8} and $T_c$ = -4 with M = 4 as global\nclipping parameters for the model. We believe that a more\nrefined selection process could further enhance quantization\nprecision. We omit the results for M = 4 from Table III for the\nfollowing reason: When M = 4, we set the clipping threshold\nto -4 (the quantized values have larger absolute differences,\nso the clipping threshold needs to be closer to zero to ensure\nthe resolution), the resulting perplexities values were between\n176 and 46, i.e. 8\u00d7-32\u00d7 worse than the FP Softmax perplexity\n(which is 5.47). This indicates that M = 4 is too small to\nyield a good perplexity and must not be used. From Table III,\nwe notice that varying the precision of $U_{corr}$ does not affect\nperplexity at all and that for a given M, as we increase the\nvalue of N, the perplexity decreases up to N = 16, after"}, {"title": "C. Comparison with Related Works", "content": "We summarize the related works on accelerating Softmax\nunder two categories: 1-) Hardware-friendly Softmax approxi-\nmations and 2-) Techniques that promote Softmax parallelism.\nWorks in the literature in category 1-) focus on approximat-\ning the non-linear operators in Softmax, namely exponential\nand reciprocal, to render the deployment hardware-friendly\n[35]. Du et al. propose an efficient hardware architecture for\nSoftmax approximation which relies on an LUT-based expo-\nnential unit and a Maclaurin series-based natural logarithmic\nunit [36]. Similarly, Dong et al. propose a hardware implemen-\ntation of Softmax based on a piecewise exponential LUT [37].\nHowever, these methods focus on Softmax in the context of\nDNNs and do not study the effect of the approximation on the\nperformance of LLMs. I-BERT, the work we base our integer-\nonly Softmax approximation on, proposes a full integer-only\nBERT quantization. However, it keeps the Softmax in INT32\nprecision (high precision) and implements the framework on\nthe GPU (not custom hardware) using customized kernels for\nthe non-linear operations. ConSmax [16] is the closest to our\nwork, as it is a software-hardware co-design methodology for\na hardware-friendly alternative Softmax. ConSmax approxi-\nmates Softmax using learnable parameters and proposes a\ncorresponding custom hardware based on bitwidth-split LUT\nthat supports mixed-precision computation. Their approach is\nnot as easily scalable as ours as each pair of dataset/model\nrequires retraining, which renders it not suitable in real-time\nscenarios. Moreover, the hardware does not inherently support\nmixed-precision, as the computation happens partially, and a\nreduction unit reads the produced partial sums and uses a chain\nof summations for reducing them. ConSmax is also tested on a\nsmall GPT-2 model (124M parameters) which is less complex\nthan our Llama models (7B, 13B, and 70B parameters). Softer-\nmax [15] is another software-hardware methodology, whereby\nthe custom hardware supports base replacement, low-precision\nSoftmax computations, and calculating online normalization.\nUnlike our work, Softermax requires retraining, studies the\nimpact of the approximation on BERT-Base and BERT-Large,\nwhich are smaller and less complex models compared to\nLlama, and relies on fixed-point formats rather than integer-\nonly which adds hardware complexity. shows that\nSoftmAP achieves the lowest optimum energy per operation\ncompared to ConSmax and Softermax.\nWorks that promote Softmax parallelism like SVD-Softmax\n[38], SpAtten [39], and FlashAttention-2 [40] partition the\nSoftmax and parallelize the execution of each partition. Our\nwork is orthogonal to those techniques, as we can apply\nSoftmAP for each partition and parallelize the execution."}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed SoftmAP, a novel software-\nhardware co-design methodology that implements an integer-\nonly low-precision Softmax on the AP. SoftmAP offers up to 3\norders of magnitude improvement in the energy-delay product\ncompared to GPU platforms. SoftmAP presents a promising\nsolution for making LLMs more deployable on resource-\nconstrained devices without compromising performance. This\nwork underscores the critical role of software-hardware co-\ndesign in advancing the efficiency of large language models.\nAs the demand for efficient and scalable language models\ncontinues to grow, our work underscores the importance of\ninnovative co-design strategies in advancing the field of natural\nlanguage processing."}]}