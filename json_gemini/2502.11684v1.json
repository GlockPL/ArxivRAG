{"title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task", "authors": ["Yuchen Yan", "Yongliang Shen", "Yang Liu", "Jin Jiang", "Xin Xu", "Mengdi Zhang", "Jian Shao", "Yueting Zhuang"], "abstract": "Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task from code completion. By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions. Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH. Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.", "sections": [{"title": "1 Introduction", "content": "Recent advances in large language models (LLMs) (OpenAI, 2023; DeepSeek-AI, 2025) have demonstrated remarkable capabilities across various reasoning tasks (Gao et al., 2024; Xu et al., 2025a), from logical deduction to complex problem-solving (Phan et al., 2025). Among these, mathematical reasoning stands as a particularly challenging frontier (Sun et al., 2024; Xu et al., 2025b), serving as a critical benchmark for evaluating an LLM's ability to perform structured, multi-step reasoning processes.\nA key breakthrough in improving LLMs' mathematical reasoning capabilities has been the introduction of chain-of-thought (CoT) prompting (Wei et al., 2023), where models explicitly articulate intermediate steps in their problem-solving process. This approach has not only enhanced solution accuracy but has also provided valuable insights into the models' reasoning mechanisms. However, the effectiveness of CoT prompting raises a fundamental question: What characteristics of training data are crucial for developing LLMs that can generate high-quality reasoning chains and arrive at correct mathematical solutions?\nPrior research has revealed that the granularity and completeness of reasoning steps in training data significantly impact a model's reasoning capabilities (Jin et al., 2024). Models trained on more detailed step-by-step solutions tend to exhibit superior performance in mathematical reasoning tasks. This observation has led to various approaches for expanding reasoning steps in training data, including the use of stronger external models and sophisticated search algorithms like Monte Carlo Tree Search (MCTS) (Zhou et al., 2023; Wu et al., 2024; Liu et al., 2024). However, these current approaches to improving reasoning steps face three main challenges. First, they rely on using even larger models to create better steps, which creates a cycle where we constantly need bigger models to make improvements (Guan et al., 2025; Toshniwal et al., 2024). Second, these methods require substantial computing resources, particularly when using advanced techniques like MCTS to explore different reasoning paths. Third, instead of building upon existing human-verified steps, these methods often generate entirely new reasoning chains, which can introduce unexpected errors and reduce the reliability of solutions.\nThese limitations motivate our central research question: Can we develop a more efficient and reliable method for expanding reasoning steps while preserving the validity of existing humangenerated solutions? Drawing inspiration from the \"Fill-in-the-middle\" task in code reasoning, where LLMs successfully complete missing code segments based on surrounding context, we propose a novel approach to this problem. Rather than generating entirely new reasoning chains, we explore whether the FIM paradigm can be adapted to supplement missing steps in existing reasoning processes or insert more detailed explanations into steps that are already sufficient.\nBuilding on this insight, we propose MathFimer, a framework for enhancing mathematical reasoning through step expansion. We first construct NuminaMath-FIM by decomposing NuminaMathCoT (Li et al., 2024) solutions into prefix-suffix pairs with missing intermediate steps. Using this dataset, we train a step-expansion model MathFimer-7B on math-specialized base model Qwen2.5-Math-7B (Yang et al., 2024). This model learns to supplement intermediate reasoning steps while preserving the original solution structure.\nWe apply MathFimer-7B to expand the reasoning steps in several existing mathematical reasoning datasets and evaluate their impact through comprehensive experiments. Our results demonstrate that training on MathFimer-expanded data consistently improves model performance across various mathematical reasoning benchmarks, including GSM8K and MATH. This improvement is observed across both general-purpose and mathspecialized foundation models, with expanded datasets leading to more detailed reasoning steps and higher solution accuracy compared to the original training data.\nOur main contributions are threefold:\n\u2022 We propose a novel step expansion framework inspired by code completion techniques, introducing MathFimer to enhance mathematical reasoning through targeted insertion of intermediate steps in existing solutions.\n\u2022 We develop and release a specialized training dataset (NuminaMath-FIM) along with a stepexpansion model MathFimer-7B, providing a practical and scalable solution for improving mathematical reasoning datasets.\n\u2022 Through extensive experiments across multiple benchmarks and model architectures, we demonstrate that our approach consistently improves mathematical reasoning performance, offering new insights into the relationship between step granularity and reasoning quality in LLMs."}, {"title": "2 Related Works", "content": "2.1 Mathematical Reasoning of LLMs\nMathematical reasoning is one of the advanced capabilities of large language models (LLMs). By transforming real-world mathematical problems into a sequence of sub-problems and engaging in step-by-step thinking, the model's ability to solve related mathematical tasks is enhanced (Wei et al., 2023). Currently, the mathematical reasoning ability of models can be strengthened at various stages of LLM's training. During the pre-training phase, reasoning-related knowledge texts, such as mathematical forum discussions, textbooks, and so on, are typically used for enhancement (Paster et al., 2023; Zhang et al., 2024). Additionally,"}, {"title": "3 Approach", "content": "In this paper, we propose a reasoning step expansion method that enhances the quality of existing data by filling in possible missing steps at the step level. This is achieved through the fill-in-themiddle (FIM) task, which supplements existing CoT data. Specifically, the work presented in this paper can be divided into two parts: the first part involves training the aforementioned FIM models, and the second part applies the trained FIM models to extend steps in existing data."}, {"title": "3.1 FIM Model Training", "content": "The goal of this section is to train a Fill-in-theMiddle (FIM) model for mathematical reasoning tasks, which can generate the missing intermediate steps between a mathematical problem, its preceding steps, and its succeeding steps. This can be expressed as:\nFIM(Q, P, S) \u21d2 M\nwhere FIM refers to the model we are training, Q(question) represents the mathematical problem, P (prefix) refers to the preceding steps, S (suffix) refers to the succeeding steps, and M (middle) denotes the intermediate steps between P and S.\nWe construct the data for training the FIM model using the existing high-quality mathematical reasoning dataset, NuminaMath-CoT. NuminaMathCoT includes mathematical reasoning data of varying difficulty levels, containing 853K mathematical question-and-answer pairs, providing us with more generalizable data.\nSpecifically, we first performed a step-by-step decomposition of the NuminaMath-CoT data, transforming the standard answers into individual steps. Then, for each case, we randomly select one step and treat all the preceding steps as the prefix and all the succeeding steps as the suffix. This can be represented as:\n(P, S, M) = (Y1...i\u22121, Yi+1...n, Yi), Yi \u2208 Y\nwhere yi is a step randomly selected from the answer Y, which contains n steps.\nFor the organization format of the FIM training data, we refer to the work of Bavarian et al. (2022) and adopt the PSM(Prefix-SuffixMiddle) sequence order. We use three special tokens: <|fim_prefix|>, <|fim_suffix|>, and <|fim_middle|>, to construct the format for the FIM training data. An example of the FIM data construction is provided in Figure 3.\nFor each case in NuminaMath-CoT, we performed three rounds of random sampling as described above. As a result, for each mathematical problem, we constructed three FIM data entries, which together formed our FIM training set, NuminaMath-FIM, consisting of 2.5M training samples for FIM task. Next, we conducted SFT on a math-specialized base model, Qwen2.5-Math-7B(Yang et al., 2024). Specifically, we only computed the loss for the tokens after <|fim_middle |>, ultimately obtaining the FIM model MathFimer-7B for step expansion."}, {"title": "3.2 Expansion of Reasoning Steps", "content": "After training MathFimer-7B, we can use it to expand the reasoning steps in existing mathematical solutions. Specifically, for each pair of consecutive steps in the original solution, we perform an inference using the FIM model to generate potentially missing intermediate steps or provide more detailed reasoning between them. This can be formally expressed as follows:\n\u0177i = FIM(Q, Y1 \u00b7 \u00b7 \u00b7 Yi\u22121, Yi \u00b7 \u00b7 \u00b7 Yn)\nwhere i represents each position in the original answer, n is the total number of steps in the original answer, yi is the i-th step in the original answer, FIM is the trained MathFimer model, Q is the question for the sample, and \u0177r is the missing part generated by the FIM model between the i-th step and the subsequent steps.\nIn our experiments, we observed that when the original steps are already sufficiently detailed, the model tends to generate content that is very similar to the subsequent step Yi . Therefore, after the FIM model generates the supplementary step \u0177i, we added a similarity calculation step. Specifically, we compute the sequence similarity between \u0177i and Yi. We set a threshold \u03b7 and mark those generated steps with a similarity greater than \u03b7 as invalid. In this paper, we set n = 0.8 .\nNext, we insert the steps generated by the FIM model into the original steps. Specifically, if the similarity score in the previous step is not labeled as invalid, we will insert it into the original sequence. More precisely, for any step \u0177i where the similarity score is less than \u03b7, we insert it between step Yi-1 and step yi. This insertion operation is carried out between each pair of original steps, ultimately constructing a more detailed answer with additional steps.\nTo evaluate the effectiveness and generalization of MathFimer-7B in expanding reasoning steps, we used it to extend the reasoning steps on several existing step-by-step reasoning datasets, including a mixture of GSM8K(Cobbe et al., 2021) and MATH(Hendrycks et al., 2021), MathInstructCoT(Yue et al., 2023), MetaMathQA(Yu et al., 2024), NuminaMath-CoT(Li et al., 2024), and ScaleQuestMath(Ding et al., 2024). For all datasets, we only used the training set. We conducted instruction fine-tuning experiments on multiple foundation LLMs. For general-purpose LLMs, we selected Meta-Llama-3.1-8B and Meta-Llama-3.170B(AI, 2024), and for math-specialized LLMs, we chose Qwen2.5-Math-7B and Qwen2.5-Math72B(Yang et al., 2024). After instruction finetuning, we evaluated performance on multiple"}, {"title": "4 Experiments", "content": "4.1 Settings\nWe conducted supervised instruction fine-tuning experiments on both general-purpose and mathspecialized foundation LLMs. To demonstrate the generalizability of the proposed method, we carried out experiments with different model sizes. We selected the original data before applying MathFimer-7B as the baseline for each experimental group and compared the performance improvements achieved after applying our proposed method for step expansion. In all experiments, we maintained identical training settings, only varying the data used for training. Specifically, we used Megatron-LM as the framework for SFT, with a model max_length set to 8192 and a global batch size of 128 (GSM8K+MATH datasets were set to 32 due to their smaller sample sizes, as a large batch size would result in an insufficient number of optimization steps). The learning rate for training was set to le-5. We packed all training samples for faster training. All SFT experiments were conducted on 64 Ascend H910B-64G.\nFor evaluation, we employ vLLM(Kwon et al., 2023) as the inference framework. To reduce evaluation variance, each question is sampled 16 times with a temperature setting of 0.7, and the average accuracy is calculated. To determine whether the model-generated answers are correct, we utilize LLM-as-a-judge, thereby mitigating evaluation errors caused by answer extraction and rule-based comparison. All model inferences in this study are conducted on NVIDIA A100-80G GPUs, with 1-card inference for 7B/8B models and 4-cards inference for 70B/72B models."}, {"title": "4.2 Main Results", "content": "We conducted our experiments on base models of different sizes, including both generalpurpose and math-specialized models. Specifically, we evaluated Meta-Llama-3.1-8B, MetaLlama-3.1-70B, Qwen2.5-Math-7B, and Qwen2.5Math-72B. We employed the MathFimer-7B model, which was trained based on Qwen2.5-Math7B, to perform a single round of step expansion. For comparative analysis, we selected"}, {"title": "5 Analysis", "content": "5.1 Disentangling Model Effects\nTo disentangle the impact of our FIM methodology from model distillation effects, we conducted a systematic ablation study addressing a critical question: To what extent do our performance gains stem from the FIM-based step expansion versus knowledge transfer from the base model?\nWe designed a controlled experiment using Qwen2.5-Math-7B as the base model. We first generated distillation datasets by fine-tuning the base model on NuminaMath-CoT and using it to generate solutions for GSM8k+MATH, MathInstruct, and MetaMathQA. We then applied MathFimer7B's step expansion to these distilled datasets to isolate the contribution of our FIM approach.\nThe results in Table 2 reveal several key insights. First, while distillation alone yields substantial improvements (e.g., GSM8K accuracy increases from 67.55% to 81.58% for G+M), MathFimer's step expansion provides additional gains even on distilled data (+1.13%). This pattern is consistent across datasets, with MI-CoT showing similar additive benefits (+1.67% on GSM8K). The smaller magnitude of improvements on distilled data compared to original data (e.g., +5.61 % vs +1.13 % for G+M on GSM8K) suggests that while knowledge transfer from the base model contributes significantly to overall performance, our FIM-based step expansion provides complementary benefits through structural enhancement of reasoning chains.\nSpecifically, we performed instruction finetuning on Qwen2.5-Math-7B, the same base model as MathFimer-7B, using the NuminaMath-CoT dataset. We then used the fine-tuned model to generate answers for questions from GSM8k+MATH, MathInstruct, and MetaMathQA, producing the distillation data: GSM8k+MATH(distill), MathInstruct(distill), and MetaMathQA(distill). Next, we applied MathFimer-7B to perform step expansion on these three datasets. Finally, we compared the performance differences between the distillation data and the step-expanded distillation data."}, {"title": "5.2 Analysis of Iteration Effects", "content": "Our iterative step expansion experiments demonstrate the robust scalability of MathFimer. As shown in Table 3, each iteration of step expansion consistently improves reasoning performance across most benchmarks. Notably, on the GSM8K benchmark, MI-CoT achieves substantial gains of +7.43%, +12.43%, and +15.54% percentage points over three iterations, reaching 83.32% accuracy. Similar patterns emerge on MATH, with consistent improvements culminating in a +9.42% percentage point gain. This iterative enhancement suggests that MathFimer effectively constructs increasingly sophisticated reasoning chains, where each expansion cycle introduces valuable intermediate steps that contribute to improved problem-solving capabilities. The consistent performance gains across different datasets and iteration counts validate the scalability of our approach and its ability to leverage extended reasoning chains for enhanced mathematical reasoning."}, {"title": "5.3 Impact of Model Scale", "content": "To investigate the relationship between model capacity and step expansion capability, we conducted a systematic comparison between MathFimer-7B and MathFimer-72B. We trained MathFimer-72B on Qwen2.5-Math-72B using identical training data and hyperparameters as MathFimer-7B to ensure controlled comparison conditions."}, {"title": "6 Conclusion", "content": "In this paper, we introduce the Fill-in-the-middle (FIM) paradigm into mathematical reasoning chains. We construct NuminaMath-FIM by decomposing solutions into prefix-suffix pairs, where intermediate steps are held out for reconstruction. Through training on these prefix-middlesuffix triplets, we develop MathFimer models that can effectively expand reasoning steps while preserving solution coherence. Our comprehensive experiments across multiple mathematical reasoning datasets demonstrate that MathFimer-enhanced data consistently improves model performance with relative improvements of 7.43% on GSM8K and 8.86% on MATH."}, {"title": "Limitations", "content": "While our MathFimer framework demonstrates promising results in enhancing mathematical reasoning through step expansion, we identify several important limitations that warrant careful consideration and future investigation.\nDomain Generalization While our approach demonstrates effectiveness in mathematical reasoning, its applicability to other reasoning domains remains uncertain. The current implementation and evaluation focus exclusively on mathematical problem-solving, leaving open questions about the framework's generalizability to domains such as code reasoning, logical deduction, and commonsense reasoning, where solution structures and validation requirements may differ significantly.\nGeneration Reliability Our step expansion process inherently relies on model generation, introducing potential risks of error propagation. Despite overall improvements in reasoning quality, we currently lack robust mechanisms for verifying the logical consistency and mathematical correctness of inserted steps. This limitation becomes particularly critical when applying multiple iterations of step expansion, where errors could potentially accumulate.\nMethodological Limitations The framework's effectiveness inherently depends on the quality of initial training data and may inherit biases from base models. Additionally, the current approach primarily focuses on expanding existing solution patterns rather than generating novel solution approaches, potentially limiting its applicability to extremely complex or unconventional problems."}]}