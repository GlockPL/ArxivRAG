{"title": "EVALUATING THE IMPACT OF ADVANCED LLM TECHNIQUES ON AI-LECTURE TUTORS FOR A ROBOTICS COURSE", "authors": ["Sebastian Kahl", "Felix L\u00f6ffler", "Martin Maciol", "Fabian Ridder", "Marius Schmitz", "Jennifer Spanagel", "Jens Wienkamp", "Christopher Burgahn", "Malte Schilling"], "abstract": "This study evaluates the performance of Large Language Models (LLMs) as an Artificial Intelligence-based tutor for a university course. In particular, different advanced techniques are utilized, such as prompt engineering, Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the different models and applied techniques using common similarity metrics like BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of helpfulness and trustworthiness. Our findings indicate that RAG combined with prompt engineering significantly enhances model responses and produces better factual answers. In the context of education, RAG appears as an ideal technique as it is based on enriching the input of the model with additional information and material which usually is already present for a university course. Fine-tuning, on the other hand, can produce quite small, still strong expert models, but poses the danger of overfitting. Our study further asks how we measure performance of LLMs and how well current measurements represent correctness or relevance? We find high correlation on similarity metrics and a bias of most of these metrics towards shorter responses. Overall, our research points to both the potential and challenges of integrating LLMs in educational settings, suggesting a need for balanced training approaches and advanced evaluation frameworks.", "sections": [{"title": "Introduction", "content": "Large Language Models have become popular with the introduction of ChatGPT. In particular, the novel dialogue capabilities of these Assistant Models [Ouyang et al., 2022] has spurned an interest as these provide a flexibility that allows to apply LLMs in various sectors, including education. These Large Language Models (LLMs) are Transformer-based neural networks [Vaswani et al., 2017] that were trained on massive text corpora and that exhibit impressive abilities to comprehend, process, and generate human-like language. Originally, Large Language Models now often called Foundation Models were trained with the goal to predict the following text token, taking into account long and diverse relationships of the given previous input. The models learnt from this to perform well on a large number of different tasks across a variety of topics. Importantly, early work on GPT-2 already demonstrated the flexibility of these models as these models can be explicitly directed to execute a specified language task simply through prompting these models accordingly [Radford et al., 2019]. Early-on, these models had difficulties staying on track for a given task and often their responses lingered or drifted away from a topic. As a consequence, the earliest application were in specific and narrowly defined tasks as translation. In such a task the model could continuously be forced to follow the original input sequence. The work by Ouyang et al. [2022] changed this, as they further trained models to better imitate human behavior which lead to the emergence of Assistant Models as ChatGPT. These demonstrated for the first time the ability to provide a-to a certain degree-consistent partner for dialogue which can be engaged in a conversation. Modern models are now able to understand and generate language with remarkable fluency and coherence. The success of such models has now spread and there is a growing number of models similar to ChatGPT that are further improving at a rapid pace"}, {"title": "Methods: Large Language Model as an AI Tutor", "content": "The proposed AI tutor will be integrated into the learning platform of the lecture and is designed as a chatbot that allows students to communicate via written messages. Regarding its function, the tutor system should consistently respond in a polite and helpful way, as a human tutor would do. It needs to be able to understand questions given in natural language and to generate answers that are both syntactically and semantically accurate. Most importantly, the answers should be consistent with the lecture which means, answers are, first, correct and the system should not tell falsehoods. Easy and general questions should be answerable based on the pre-trained model. Secondly, as the tutor should support the student in learning the concepts, the answers should relate to the content of the lecture as well as the way it is taught, e.g., picking up examples or providing references to content as the slides. Therefore, slides of the lecture as well as the transcripts of the recordings should be taken into account.\nThese requirements guide the realization of the tutor system and in the following, we will introduce different advanced techniques for LLMs that will be applied in the tutoring system. We are interested in comparing how well an LLM can work as a lecture tutor. This requires an experimental setup in which different variations of the tutor are questioned in a systematic way and evaluated. In the following, we will introduce the necessary parts for such an evaluation and comparison. First, we introduce the different advanced techniques applied to the LLM, i.e., prompt engineering, Retrieval-Augmented-Generation, and fine-tuning. Secondly, we will briefly describe the preprocessing of the lecture material and, third, how we derived a test data set for evaluation and a data set for training. Fourth, we will briefly provide details on the used models and the fine-tuning process. Last, we introduce different evaluation metrics for the comparison."}, {"title": "Advanced LLM Techniques", "content": "Applying an off-the-shelf dialogue LLM as a lecture tutor can serve as a baseline. But it has to be assumed that it provides generic answers that\u2014while hopefully will be correct\u2014are not tailored to the specific course. There is now already an established set of different approaches for improving answers of an LLM. We will briefly describe these different approaches that will later be compared in our evaluation."}, {"title": "Guiding the Model: Prompt Engineering", "content": "A simple improvement can be achieved by enriching the prompt given by the user and tailoring the prompt towards the particular task as well as the given setting. Enriching the prompt can guide the model to generate more relevant and accurate answers [Radford et al., 2019]. To improve a prompt, it is first necessary to understand what is required by the model acting as a tutor: The tutor is expected to answer as a human tutor would, which means it only tells the truth and cites it sources. Therefore, these aspects need to be incorporated into the prompt. Achieving the desired outcome through prompt engineering is often an iterative process in which a given prompt is tried, evaluated and iteratively improved [Jones et al., 2023]. We achieved good results with initially starting from a very simple, basic system message and improve this stepwise to include more details: The AI Tutor was prompted that it is a lecture assistant, an expert in the field of robotics and should only answer according to the context of the lecture.\nAs the initial prompt didn't suffice to turn the model into a tutor-like personality, additional traits as helpful, respectful, and honest were added. Furthermore, the model was explicitly instructed to exclude any form of racist, harmful, toxic, dangerous, illegal, or unethical content.\nOne particular problematic area when using a standard LLM was the tendency to always try to generate some output, even though a question was not well understood or the model simply didn't know the necessary facts for providing an answer. When providing additional information using RAG (see next section), as an additional instruction for the prompt it was added to restrict answers to the provided material."}, {"title": "Enhancing the Input: Retrieval-Augmented Generation", "content": "One common extension for LLMs is to enrich the input to the system using external background data that relates to the given input and could potentially help in generating a response which is called Retrieval-Augmented Generation (RAG) [Lewis et al., 2020]. The main idea of RAG, visualized in Fig. 1, is to retrieve such related information from a provided database. The retrieved data is added to the prompt as additional context. Lewis et al. [2020] have shown that a model, that is supported with RAG, can outperform state-of-the-art approaches in open-domain Question Answering. It was also shown that such a model can approach the efficacy of systems with domain-specific architectures in Fact Verification.\nRAG appears as an ideal extension for a tutoring system as it enriches responses with additional knowledge about the query, ensuring that answers are not only accurate but also relevant to the specific content covered in the lecture. This approach can also prevent the inclusion of overly detailed information not discussed in the lecture. Additionally, RAG's advancements in fact verification enable the AI tutor to identify and rectify inaccuracies presented by users during chat conversations. Lastly, RAG facilitates the integration of references into answers, for example, in our case, references to specific slides were included in the underlying database. This enables users to verify information, which should strengthen their trust in the model.\nAs a method, Retrieval-Augmented Generation can be divided into two primary stages (for more details, see [Zhao et al., 2024]): indexing and querying. The indexing stage involves the efficient storage of supplementary information. As an initial step, the data has to be segmented into manageable chunks which facilitates the search for specific content within the data. To enable semantic searches, a vector embedding is computed for each of these chunks. The database must store individual chunks and embeddings. Importantly, we want to be able to query the database semantically, which means it has to allow for a similarity search in the vector embedding space. Such a system is referred to as vector storage."}, {"title": "Training the LLM: Fine-Tuning on Example Data", "content": "Large Language Models operate on two sources of knowledge: As a parametric form, information is entrained into the distributed neural network structure of the model during training. Relatively huge amounts of text are used during training that gradually shape the responses of the model. As a second form, LLMs integrate contextual knowledge given as an input which drives the generation of the response. RAG explicitly provides additional contextual knowledge and in prompt engineering contextual input (the prompt) is used to steer the generation process (see above). A different, but usually more involved approach, is to address the knowledge stored in a distributed fashion in the neural networks of the LLM. This can be achieved using further training and fine-tuning the model [Devlin et al., 2019]. Fine-tuning a large language model involves adjusting the model's parameters and training it on a specific dataset to adapt it to a particular topic, in our case, the lecture on robotics. In fine-tuning, the neural network-based model is further trained in a supervised fashion on a specific data set. The goal is to to adapt the LLM towards a specific task and the model should become an expert in this task. Fine-tuning has shown to work well with relatively small data sets (of around a couple of hundreds to thousands example interactions), but when training on such a small data set, there is a danger of overfitting to these particular questions. The model might end up only providing memorized answers from this data set. In general, fine-tuning as a process can improve the model's performance and accuracy for a targeted use case by by combining the knowledge of the base model with specific information from a new dataset."}, {"title": "Preprocessing of Lecture Material", "content": "Integration of lecture material into the query process using RAG or deriving training data from it, requires significant preprocessing of the available information. First, the information has to be extracted, in our case from slides which consist of written text that could be directly extracted from the original files (in our case based on PDFs of the lecture slides). In addition, graphical information and images from the slides were extracted and transferred to textual descriptions using multimodal Large Language Models. The visual information was fed into LLMs (GPT-4) that were tasked with briefly describing the given figure. Secondly, the original audio was taken from lecture videos in which the specific slides were explained and discussed in more detail. This requires transcribing the audio (realized using OpenAI's Whisper framework) and aligning the oral explanations with the content presented visually on the slides. The audio showed important providing complementing information and more detailed explanations. As a result of this process, we end up with a vast amount of data for eleven lectures (each one 90 minutes long, roughly around 45 slides per lecture) that is available to the system. During operation, the AI tutor should focus solely on relevant parts from the lecture and, therefore, needs an understanding of which different parts of the lecture and transcripts are necessary to answer the question. As an additional requirement, the AI tutor should provide information on its sources, e.g., pointing out which slides were used for a given answer (for example: \"@10-slam-deck Slide 11\"). This allows the user to check slides and verify the given answer.\nOverall, the complete process should be executed quickly in general and efficiently to avoid compromising the user experience. For future application in real courses, factors regarding its scalability should be considered as well (both with respect to running an LLM as well as the necessary infrastructure for serving up background information from a database)."}, {"title": "Test and Fine-Tuning Dataset", "content": "Evaluation of the different LLM techniques and models was done on the test data set. This data set consisted of chats (overall 478) which each included a dialogue consisting of the question and a possible answer. Questions in the test set (and similarly in the training data set described below) were targeted for the specific robotics course and based on the material covered. In case of the test set, two lectures (on the topic of SLAM and navigation in robotics, lecture nine and ten) were used. Three students worked through the material of these lectures and generated by hand questions for the covered material which was then used again by the human writers to formulate an answer. Overall, for the test set 478 such chats were written.\nFor fine-tuning a training data set for the robotics course was generated. It explicitly excluded the topics covered in the test set for which there where humanly written chats. But for generating the test set, a LLM was used which was advised through a prompt to generate question-answer pairs for specific slides (the prompt had quite detailed instructions on how to generate questions based on the content of a slide which could then be used in addition to generate a fitting answer that included a reference to that particular slide). In addition, an example question-answer pair written by a human was given in a form of one-shot learning. The generated chats were afterwards read and checked by humans and were found to be generally of sufficient quality. This produced overall 2791 chats for training that did not overlap with the test data set. The LLaMA-2 model was trained on this specific dataset of prototypical interaction."}, {"title": "Models and Fine-Tuning", "content": "There are many LLMs openly available which are accessible through specific APIs and even more so through standard libraries and frameworks like Huggingface or their APIs. For our tutor system, we focussed, first, on GPT-3.5 which is accessible through OpenAI and for which we used the OpenAI API for access (model can be queried through simple python function calls). We chose this particular model as it is widely used and can be considered a good, common benchmark. It provides high quality output. Prompt engineering and RAG can be applied easily.\nFor RAG, the AI Tutor application uses a PostgreSQL Database equipped with the PGVector plugin. This database configuration allows for the storage of original text chunks alongside their respective embedding vectors in a distinct vector column. The embedding vectors are derived using an OpenAI embedding model. The vector storage can be used to retrieve useful information during the query stage. The process is illustrated above in Fig. 1, where a search for a user's query is started within the vector storage, retrieving the k most similar chunks for the given query (we experimented with different numbers of chunks and found good results with three chunks). These chunks are integrated as context into the prompt given to the Large Language Model (LLM).\nOne significant drawback of OpenAI's models is the fact, that they are close-sourced and operated by OpenAI which might pose privacy issues and, at least early-on in the project, excluded the possibility of further training on specific data as envisioned in our project. In our case, we therefore used as an additional model Llama-2-13B from Meta as it is openly accessible and can be further trained locally [Touvron et al., 2023b,a]. Meta's LLaMA series of models was trained only on publicly available data (overall around 1.25 trillion tokens, the majority of data is gained from CommonCrawl with about 850 billion tokens). This model was used in different setups: a baseline approach in which the model was directly queried and, again, a setup in which the prompt was enriched using the system message as well as RAG (prompt engineering and RAG are identical to the approach used with GPT-3.5)."}, {"title": "Evaluation of LLMs in Question-Answering", "content": "Measuring question-answering is difficult because for many questions there are multiple correct answers, e.g., two answers to a question might simply be formulated quite differently, using synonyms, or having a different sentence structure. None-the-less, both answers might be correct. This makes comparison to a ground truth difficult. There are different approaches for question-answering and quite a large number of metrics have been proposed. A couple of these focus on overlap between the model's output and the ground truth response provided by a human. BLEU-4 is widely used as such a metric [Papineni et al., 2002] providing values in the range of 0 to 1. Furthermore, we also use ROUGE-1 [Lin, 2004] and METEOR [Banerjee and Lavie, 2005] for evaluation which have been found to best correlate with human evaluation [Kim et al., 2020]. As a further more modern metric, we in addition computed the BERTScore [Zhang et al., 2019] which again is calculating the similarity between token in the output by the model compared to the ground truth. Importantly, while other metrics count exact matches, the BERTscore computes semantic similarity for token in the contextual embedding space.\nIt is currently assumed that such metrics by itself are not sufficient to analyze LLMs [Maynez et al., 2020]. Therefore, human evaluations are still used as a gold standard for assessing the capabilities. As a first step, we setup a web-based framework that present a question together with an answer to a user. The user is tasked with judging how helpful and how correct the given answer is. There is additional information provided for each question (the original slide for which we generated the question, plus a context of neighboring slides). This human evaluation has only been tested with two teaching assistants for the given robotics course which can be considered domain experts (detailed results are shown in the Appendix in table 7). We will run this evaluation in the upcoming semester inside the robotics course.\nUsing another LLM\u2014and often a more capable one with larger parameter counts is currently an active area of research. Approaches as LLMEval [Lin and Chen, 2023] use an LLM for similarity rating itself, often using an elaborate prompt that guides the model to mimic a human-like evaluation. In general, LLMEval showed some promising results, but appears difficult to directly apply in our approach. In our case, the integration of RAG lead to quite long context inputs which has shown problematic for LLMEval. Therefore, we adapted their general approach and prompted an LLM (GPT-3.5) as an evaluator which was provided with the original question, the generated answer, and the ground truth answer. In particular, we developed three different prompts that addressed different parts of our evaluation. First, 'GPTSimilarity' is an evaluation of the similarity between the generated response and the ground-truth response in the test set. Secondly, for the metrics of the human evaluation, we introduced 'GPTRater' which evaluated independently trustworthiness and helpfulness on the same scale as given to human evaluators (details see below)."}, {"title": "Results", "content": "We tested the different LLM versions and different applied techniques in a question answering task with the questions tailored to the robotics course from the test set. As a first model, we used ChatGPT (GPT-3.5) as a standard high-performing and well-known general model. For GPT-3.5 we could apply prompt engineering and RAG as we accessed the model through OpenAI's API and simply enriched the prompt. As we considered fine-tuning as a third step, we also evaluated an open model in LLaMA-2. This lead to multiple different setups and different groupings: On the one hand, we used two different models of quite different size (GPT-3.5 has around 175 billion parameters, LLaMA-2 in the midsize version of around 13 billion parameters). On the other hand, we used different input versions for both models: Simply providing a question as an input, prompt engineering using an optimized prompt that provides context on the use case, and, in addition, Retrieval-Augmented-Generation in which the input was further enriched using background information from the lecture as additional input to the model. In the case of LLaMA-2 we further used fine-tuning and trained the model on a training dataset of example questions and answers for the robotics course."}, {"title": "Comparison of Different LLM Extension", "content": "As a first research question, we are interested in how well do different models perform and how do the different extension techniques affect the performance. Therefore, we evaluated these models on our test dataset. The different metrics compare the provided ground truth answer from the test set with the answer given by the model on the particular question (for detailed results see Appendix A, table 2 and table 3). Given are the BLEU-4 and ROUGE similarity scores in Fig. 3. As a first observation, for both generic LLM models (indicated by colors: GPT-3.5 in blue, LLaMA-2 in orange) extending the prompt always improved the response considerably. Just adding a simple system message that instructs the model to act as a tutoring system showed a considerable improvement (Fig. 3, lower part, comparison of GPT-3.5 with GPT-3.5 (System Message) in which a general message on the role and topic was provided to the model). Retrieval-Augmented-Generation and prompt engineering show a further improvement and quite large positive effect."}, {"title": "Analysis of Evaluation Metrics", "content": "Evaluation of LLMs and their responses is difficult and there are multiple metrics and different approaches have been proposed. We further analyzed some of these metrics: First, we considered how the different metrics correlate with each other in order to understand better what these represent and how they might capture similar characteristics. In addition, we computed correlations to other characteristic features of the given responses. As one example, we considered the length of the response as it has often be assumed that there is a bias for shorter answers in some of the more traditional metrics.\nWe observed (Fig. 5) that the traditionally computed similarity metrics as BLEU, ROUGE, and BERTScore (to a slightly lesser extend) are highly correlated. Interestingly, all these metrics showed a negative correlation with the token count. In our case, we found the assumption confirmed that shorter sentences are preferred by these metrics. METEOR also showed a positive-but smaller\u2014correlation with the metrics above. It appeared nearly uncorrelated with the token counter (only a very small negative correlation). GPTSimilarity (results are shown in the appendix, Fig. 7), as a measurement that prompts an LLM to evaluate similarity, did not show any large correlation with the more traditional similarity metrics, but surprisingly has a positive correlation with the token count, meaning in our evaluation longer answers were usually favored (this should be further analyzed).\nSecondly, we want to understand the relation of the different metrics to a human evaluation (considered as the gold standard). The human evaluation should produce reproducible assessments based on recognizable criteria, ensuring that the evaluations are objective and can be consistently applied. Our approach for the human evaluation follows the method described in tolokan.ai 7. Their evaluation process utilizes two main criteria: Helpfulness and Trustworthiness (which should reflect how well the answer is backed up by argument or references). Each measured on a scale of five answers (helpfulness: not helpful \u2013 repetition \u2013 unclear \u2013 limited \u2013 helpful; trustworthiness: nonsense \u2013 false statement general knowledge \u2013 partially proven \u2013proven). In the evaluation, each sentence of a model's response is evaluated individually, allowing for a granular analysis of the output. Importantly, in this evaluation method a weighting of each rating for a sentence is additionally computed considering the length. This weighting is crucial for determining the proportional impact of each rating level on the overall response. As a consequence, the length of the full answer is not a direct criteria for good or bad answers. For example, as long as the answer provides helpful information with respect to the request, a long answer can still be useful to answer the request in more detail. As a first step, the evaluation was conducted by only two teaching assistants who assessed a randomly selected subset of the test set (130 question-answer pairs). This of course limits the reliability of the human feedback scores as the number of participants is too small. The study shall be repeated in the future, as we want to use it in an upcoming semester in the robotics course. Still, we hope that we can gain first insights from the human feedback. It is of further note that the evaluation was not done by naive subjects, but by experts on the topics of the course. This should have a positive impact on their knowledge of the topics of the course. The human labelers were blindly presented responses from the different models and extensions and asked to label these for helpfulness and trustworthiness. Afterwards, this feedback was pooled together for each specific LLM configuration individually.\nAs a first observation, when adding RAG into any model (GPT-3.5 or LLaMA-2) this increased trustworthiness in the model (results are not shown in detail). This is to be expected as base models\u2014without any extension-are not capable of providing references to back up their answers. Adding references appears to have a direct positive impact on trustworthiness. For helpfulness we couldn't find a clear trend (e.g., GPT-3.5 already proves as a strong baseline in this respect which gives always answers that relate to the question). Overall, we are interested in how the human evaluation correlates with the computed metrics (Fig. 5). There is a good correlation of trustworthiness with BLEU, Rouge, and BERTScore (as a cautious reminder: the human evaluation is currently only restricted to feedback by two teaching assistants). Trustworthiness is also negatively correlated with the token count. This appears reasonable as the scale of trustworthiness rewards answers that are backed up by argument. In contrast, well known facts that might be known from training data are evaluated as neutral. There is no strong correlation for helpfulness with respect to the other metrics, only a small correlation with BERTScore. Trustworthiness and Helpfulness appear correlated.\nOne interesting comparison is between the human feedback on trustworthiness and helpfulness compared to similar ratings obtained from an LLM\u2014following the GPTRater approach which estimates trust and helpfulness on a given answer with respect to the ground truth answer using a LLM (scores for different LLM variations using the GPTRater approach, that evaluates trust and helpfulness using an LLM, are given in the appendix, Fig. 8). For these we find small correlations, but again these should be handled carefully as long as there is only human feedback given by two participants."}, {"title": "Adaptation of LLM during Fine-Tuning", "content": "During the fine-tuning of the model on the training dataset, we aimed to observe how the model's responses evolved over the course of training. Specifically, we investigated how long it took for the model to adapt to the specific task structure and at what point the model began to overfit to the training data. Therefore, we recorded model weights throughout the training process. A checkpoint was generated for every 250 training steps.\nFor an analysis, we selected checkpoints exponentially spaced by a factor of 2, examining model responses after 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000 training steps, and at the end of training (178000 steps). Multiple sample answers generated by the fine-tuned models were evaluated in response to questions from the test set. Given the issues encountered when applying Retrieval-Augmented Generation (RAG) to a fine-tuned model, we run this evaluation in both kind of settings: Using the fine-tuned model or in addition applying RAG on the fine-tuned model.\nThe selection of models was based, first, on human evaluations according to specifically defined criteria, including correctness of the answer, reference to the lecture, correct use of the specified formatting, the overall amount of formatting errors, and grammar. Ultimately, checkpoints 2000 and 128000 were selected for a broader comparison in human evaluation.\nDespite the limited scope of the human evaluation, some general trends were observed. Checkpoint 128000 yielded the best results for the 'Fine-tuned LLaMA' model configuration when used without applying RAG. In contrast, already checkpoint 2000 performed best for the model when used with RAG (\u2018Fine-tuned LLaMA + RAG'). Example answers to a sample question are presented in the appendix (table 4 shows the ground truth and the input question; Table 5 compares the fine-tuned models response without using RAG for the two different checkpoints, i.e., early and late in training; table 6 compares the fine-tuned models response when additionally integrating RAG for the two different checkpoints). The performance of the checkpoints deteriorates when the configurations are switched.\nSecondly, we computed the different metrics for these two type of settings throughout the training process. Interestingly, there was a strong correspondence between the human assessments and the performance metrics at specific training points (see Fig. 6). For the plain LLaMA-2 model without RAG, the scores (e.g., BLEU-4 and ROUGE, shown in blue in Fig. 6) improved consistently over time and continued to improve late in training. In contrast, with RAG integration (shown in orange), fine-tuning peaked early and then dropped considerably. The metrics indicate an optimal performance around 4000 training steps, after which the system's performance deteriorated drastically. This aligns well with the human assessment, which chose the model's responses after 2000 training steps as optimal which is quite close and for which the model performed on a similar level when considering the metrics.\nWe assume that extended training in the RAG configuration leads to overfitting towards the expected target structure. It appears as an excessive over-adaptation to the input format which leads to overfitting and partial output of the system prompt. The model attempts to adhere strictly to this pattern, which might further cause hallucinations, although this requires further analysis and a more in-depth evaluation. Conversely, when considering the early stage of training-in our case checkpoint after 2000 steps-the fine-tuned configuration without RAG did not produce satisfactory results, likely because it had not undergone sufficient fine-tuning to accurately replicate the desired formatting for references and lecture content."}, {"title": "Discussion and Conclusion", "content": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be\u2014together with some form of prompt engineering\u2014considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start\u2014with the ones provided in the lecture. This should positively affect the learning of students.\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better\u2014as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically. The introduction of RAG drastically changes the input to a model and this change appears as problematic when RAG is introduced after fine-tuning. Therefore, the combination of techniques should be considered carefully beforehand. In the future, we want to further test how fine-tuning on interactions that include RAG affect the result which, unfortunately, requires a different data set.\nWe further compared different metrics to evaluate question answering in LLMs and found a high correlation between BLEU-4, ROUGE, and BERTScore. The analysis of these metrics revealed an inherent preference for shorter responses, suggesting that these metrics might be biased against lengthier, albeit potentially more informative, answers. This observation is crucial for designing future LLM evaluation frameworks, as it challenges the effectiveness of current automated metrics in truly capturing response quality in educational settings. LLM-based evaluations still appear difficult as we could not demonstrate a large correlation of a similarity evaluation with the more conventional metrics. With respect to trustworthiness and helpfulness, our human evaluation has to be based on a larger test group and we want to extend this in the future."}]}