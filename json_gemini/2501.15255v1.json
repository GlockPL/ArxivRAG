{"title": "Lightweight and Post-Training Structured Pruning for On-Device Large Lanaguage Models", "authors": ["Zihuai Xu", "Yang Xu", "Hongli Xu", "Yunming Liao", "Zhiwei Yao", "Zuan Xie"], "abstract": "Considering the hardware-friendly characteristics and broad applicability, structured pruning has emerged as an efficient solution to reduce the resource demands of large language models (LLMs) on resource-constrained devices. Traditional structured pruning methods often need fine-tuning to recover performance loss, which incurs high memory overhead and substantial data requirements, rendering them unsuitable for on-device applications. Additionally, post-training structured pruning techniques typically necessitate specific activation functions or architectural modifications, thereby limiting their scope of applications. Herein, we introduce COMP, a lightweight post-training structured pruning method that employs a hybrid-granularity pruning strategy. COMP initially prunes selected model layers based on their importance at a coarse granularity, followed by fine-grained neuron pruning within the dense layers of each remaining model layer. To more accurately evaluate neuron importance, COMP introduces a new matrix condition-based metric. Subsequently, COMP utilizes mask tuning to recover accuracy without the need for fine-tuning, significantly reducing memory consumption. Experimental results demonstrate that COMP improves performance by 6.13% on the LLaMA-2-7B model with a 20% pruning ratio compared to LLM-Pruner, while simultaneously reducing memory overhead by 80%.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have demonstrated remarkable success in various practical applications [1], [2], [3]. The exceptional performance is primarily attributed to their immense number of parameters. However, LLMs always require substantial computing power and memory size for effective and efficient inference [4], [5], which severely impedes deployment on resource-constrained hardware, like edge modules for autonomous vehicles [6] or mobile platforms [7]. To address these concerns, model compression has become a key solution to reduce resource footprint and/or accelerate inference speed of LLMs [8], [9].\nAs one of the dominant model compression techniques, model pruning is widely used and can be categorized into unstructured pruning, structured pruning, and semi-structured pruning. Concretely, unstructured pruning is permitted to remove individual weights (i.e., connections) of a model without adhering to any predefined structure (e.g., neurons, attention heads) [10], [11], [12], and semi-structured pruning typically removes groups of weights that adhere to certain structural constraints, such as 2:4 (2 out of every 4 consecutive weights are selected and pruned) or 4:8 [13], [14]. As the removed weights are set to zero, the resulting sparse weight matrices necessitate specialized hardware, e.g., NVIDIA Ampere GPUs [15], for sparse operations, potentially limiting the applicability of both unstructured and semi-structured pruning. In contrast, structured pruning directly removes structural components such as neurons, attention heads, or entire layers [16], [17], [18]. This makes structured pruning more computationally efficient and less dependent on specialized hardware for performance gains, facilitating broader applicability across various hardware platforms.\nGiven its hardware-friendly nature, we opt to implement structured pruning for the deployment of LLMs on ubiquitous and resource-constrained devices. However, existing structured pruning methods experience two potential drawbacks. First, the gradient-based importance evaluation of structural components will result in excessive memory overhead. For example, LLM-Pruner [19] requires over 32GB memory to compute group importance when pruning a 7B model, making the process particularly resource-intensive. Second, given the same pruning ratio, explicitly removing entire components in structured pruning generally leads to larger accuracy drops than those induced by unstructured pruning [20]. Such performance deterioration has led prior studies [21], [22], [23] to rely on fine-tuning to restore accuracy.\nNevertheless, incorporating fine-tuning into model pruning also faces two new barriers in resource-constrained environments. On one hand, the backward pass usually doubles the memory overhead of a forward pass, while some stateful optimizers such as AdamW [24] will increase memory requirements to triple the forward pass. For example, LoRAPrune [17] demands approximately 18GB of memory to fine-tune a 7B model, which is impractical for many devices equipped with less than 16GB of memory. On the other hand, fine-tuning generally requires a large amount of high-quality labeled data, e.g., 50k samples in [19]. Since end devices often lack sufficient labeled data, fine-tuning must be performed on cloud servers that aggregate data from these devices. However, the data collection process raises privacy concerns, especially when handling sensitive information like medical [25] or financial [26] data. Consequently, there is an increasing urgency for a lightweight and on-device structured pruning technique that circumvents the need for fine-tuning.\nIn response, post-training structured pruning has drawn growing attention for its ability to ease both memory and"}, {"title": "II. RELATED WORK", "content": "In this study, we concentrate on structured pruning for deploying large language models (LLMs) on resource-constrained devices, leveraging its inherent hardware-friendly characteristics and broad applicability. Among the pioneering structured pruning techniques for LLMs, LLM-Pruner [19] segments the coupled structures of an LLM into distinct groups and assesses the importance of each group through backpropagation. Subsequently, LLM-Pruner removes the less significant groups and fine-tunes the pruned model to restore its inference performance. However, the reliance on backpropagation incurs substantial memory overhead, limiting its practicality for devices with restricted memory resources. To address this limitation, Dejavu [29] introduces an auxiliary classifier functioning as a neighbor searcher to predict the contextual sparsity of each layer in real time. This classifier determines which neurons are active for a given input, thereby loading only the necessary parameters for computation. Additionally, LoRAPrune [17] mitigates memory consumption by replacing global gradients with low-rank gradients during fine-tuning, effectively interweaving fine-tuning with pruning. Nonetheless, both Dejavu and LoRAPrune necessitate the training of additional neural networks and extensive fine-tuning, which require significant labeled data and further strain the memory resources of end devices.\nTo overcome these challenges, post-training pruning techniques have garnered increasing attention due to their reduced dependency on computational resources and labeled data. FPT [16] utilizes the Fisher information matrix to explore various combinations of module masks within the model, aiming to identify the optimal submodel that minimizes the loss induced by pruning. It subsequently employs mask tuning to recover the performance degradation resulting from pruning. However, FPT's mask structure is specifically tailored for models with BERT-like architectures [30], thereby limiting its generalizability. ShortGPT [18] offers a more straightforward, structure-agnostic pruning approach by directly eliminating redundant model layers. Despite its simplicity, ShortGPT demonstrates effectiveness primarily for LLaMA [27] and falls short when applied to other LLMs. In contrast, SliceGPT [28] exploits computational invariance within the model and employs PCA to prune dimensions in the parameter matrix with low sample variance, thereby supporting both LLaMA and OPT models. Nevertheless, SliceGPT's reliance on PCA means that the quantity and quality of available calibration data"}, {"title": "III. PRELIMINARIES AND MOTIVATION", "content": "An LLM consists of L identical layers, and each layer consists of K distinct denses. There is a weight matrix $W^{l,k} \\in \\mathbb{R}^{p^{l,k}\\times q^{l,k}}$ and a bias vector $b^{l,k} \\in \\mathbb{R}^{p^{l,k}}$ at the k-th dense in the l-th layer, where $p^{l,k}$ and $q^{l,k}$ denote the output dimension and input dimension, respectively. $X^{l,k} \\in \\mathbb{R}^{q^{l,k} \\times T}$ denotes the inputs to the k-th dense in the l-th layer, where T is the number of tokens the available calibration data.\nLayer-grained pruning is the most simple structured pruning method. Since Transformer-based models share the same layer structure, layer-grained pruning removes entire layers directly. As another common pruning method, the neuron-grained pruning can be categorized into input neuron pruning and output neuron pruning, which correspond to removing a specific column or row of the parameter matrix, respectively. Output neuron pruning reduces the output dimension of the dense layer, thereby indirectly changing the input dimension of subsequent dense layers, so we employ input neuron pruning to avoid additional impacts. Specifically, pruning the j-th input neuron at the k-th dense in the l-th layer amounts to removing the j-th column of $W^{l,k}$ by setting it to zero, i.e., $W^{l,k}_{:,j} = 0$. This process corresponds to ignoring the j-th dimension of the dense's input. Assuming c neurons are pruned, the t-th column of the output of the k-th dense in the l-th layer can be expressed as\n$W^{l,k} (m_c \\circ X^{l,k}_t) + b^{l,k}$ (1)\nwhere $\\circ$ means Hadamard product, $X^{l,k}_t \\in \\mathbb{R}^{q^{l,k} \\times 1}$ represents the t-th column of $X^{l,k}$, $m_c \\in \\{0,1\\}^{q^{l,k} \\times 1}$ is the input mask, and c entries in $m_c$ are set to zero.\nWe choose to use mask tuning to ensure the effectiveness of post-training pruning. Mask tuning reconstructs the original dense's output using unpruned neurons. It reduces the discrepancy in dense outputs before and after pruning, formally written as\n$\\min_{m_c} \\mathbb{E}_t [||W^{l,k} (m_c \\circ \\hat{X}^{l,k}_t) \\circ m'_c - W^{l,k} X^{l,k}_t||_2]$ (2)\nwhere $m'_c$ is the tuned mask. The input to the dense layer is different between the pruned model and unpruned model due to the influence of preceding layers, so we use $\\hat{X}^{l,k}$ and $X^{l,k}$ as the inputs individually.\nIn the OPT model, Dejavu [29] observes layer redundancy, characterized by the high similarity between each layer's output and input. Intuitively, if the redundancy of"}, {"title": "B. Intuition of Metric Design", "content": "Therefore, the importance of the l-th layer is defined as follows:\n$I_l = 1 - \\mathbb{E}_t [\\frac{{X^{l+1}_t}^T X^l_t}{||X^{l+1}_t||_2 ||X^l_t||_2}]$ (3)\nwhere the similarity term represents the layer redundancy of the l-th layer, $X^l$ and $X^{l+1}$ represent the input and output of the l-th layer respectively. Note that Eq. (3) only involves the parameters of the l-th layer, so the layers of the model can be dynamically loaded to reduce overhead.\nWhen pruning neurons, we expect to prune some neurons that have a slight impact on the final result. As shown in Eq. (2), the k-th dense unit in the l-th layer is expected to reconstruct the original output more accurately after mask tuning. In other words, neuron pruning should contribute to mask tuning; therefore, neuron importance is designed based on the optimization goal. Eq. (2) can be further expressed as:\n$\\min_{m_c} \\mathbb{E}_t [||(W^{l,k}Diag(m_c \\circ \\hat{X}^{l,k}_t)) m'_c - W^{l,k}X^{l,k}_t||_2]$ (4)\nwhere $diag(x)$ means the diagonal matrix of vector $x$. Eq. (4) is actually a least squares problem that can indeed be rewritten as a standard linear equation:\n$\\hat{A}^T \\hat{A} m_c = \\hat{A}^T y$ (5)\nwhere $\\hat{A} = \\mathbb{E}_t[W^{l,k}Diag(m_c \\circ \\hat{X}^{l,k}_t)], y = \\mathbb{E}_t[W^{l,k}X^{l,k}_t]$. It is natural to focus on the properties of the coefficient matrix $\\hat{A}^T \\hat{A}$. As stated in [31], the matrix condition number is a commonly used metric to measure the properties of the coefficient matrix. For a linear equation system $\\hat{A}^T \\hat{A} m_c = \\hat{A}^T y$, the condition number of the coefficient matrix $\\kappa(\\hat{A}^T \\hat{A})$ is defined as:\n$\\kappa(\\hat{A}^T \\hat{A}) = ||\\hat{A}^T \\hat{A}||_2 \\cdot ||(\\hat{A}^T \\hat{A})^{-1}||_2$ (6)\nwhere $|\\hat{A}^T \\hat{A}||_2$, known as the spectral norm, denotes the maximum singular value of $\\hat{A}^T \\hat{A}$. The condition number indicates the sensitivity of the computation to errors. Concretely, a large condition number of the coefficient matrix $\\kappa(\\hat{A}^T \\hat{A})$ suggests that this matrix is poorly conditioned, and solving for"}, {"title": "C. Motivation", "content": "mask $m_c$ might lead to significant numerical errors. In mask tuning, the pruned model is expected to handle most tasks effectively, meaning that the solution $m_c$ can minimize the optimization objective in most cases. In other words, $\\kappa(\\hat{A}^T \\hat{A})$ should be reduced to minimize the errors of $m_c$.\nNote that $\\kappa(\\hat{A}^T \\hat{A})$ can be approximated by the second-order of Taylor expansion around initial mask 1:\n$\\kappa(\\hat{A}^T \\hat{A}) \\approx \\kappa(\\hat{A}^T \\hat{A}) - g^T (1 - m_c)$\n$+ \\frac{1}{2} (1 - m_c)^T H (1 - m_c)$ (7)\nwhere $g = \\frac{\\partial \\kappa(\\hat{A}^T \\hat{A})}{\\partial m_c}$, $H = \\frac{\\partial^2 \\kappa(\\hat{A}^T \\hat{A})}{\\partial m_c^2}$, and $\\hat{A} = \\mathbb{E}_t [W^{l,k} Diag(\\hat{X}^{l,k})]$ is the initial coefficient matrix before pruning. Since forming the exact Hessian matrix explicitly is infeasible, H can be approximated with the (empirical) Fisher information matrix F of the mask variables as:\n$F = (\\frac{\\partial \\kappa(\\hat{A}^T \\hat{A})}{\\partial m_c}) (\\frac{\\partial \\kappa(\\hat{A}^T \\hat{A})}{\\partial m_c})^T$ (8)\nIt is also hard to use the full Fisher information matrix F. Therefore, the input neurons are assumed to be independent, meaning that F is diagonal. So the importance of the f-th neuron in a certain dense is defined as follows:\n$I_f = -g_f + \\frac{1}{2}F_{ff}$\n$= -\\frac{\\partial}{\\partial m_c} [\\kappa(\\hat{A}^T \\hat{A})] + \\frac{1}{2} (\\frac{\\partial \\kappa(\\hat{A}^T \\hat{A})}{\\partial m_c})^2$ (9)\nPruning neurons with higher importance clearly results in a more significant increase in the condition number of the coefficient matrix, which in turn makes the solution more sensitive to different inputs of the dense, potentially leading to instability.\nSolving $(\\hat{A}^T \\hat{A})$ in Eq. (9) involves inverting $\\hat{A}^T \\hat{A}$, which is not necessarily invertible. Noticing that $\\hat{A}^T \\hat{A}$ is a positive semidefinite symmetric matrix, we add $\\epsilon E$ to $\\hat{A}^T \\hat{A}$, where E is the identity matrix and $\\epsilon$ is a very small positive value. Since the modified matrix is positive definite, the process of calculating its inverse can be accelerated by the Cholesky decomposition [32].\nWe conduct tests on three different pruning strategies over different models: (1) Layer pruning only removes layers. (2) Neuron pruning only removes the same number of neurons from each dense. (3) Hybrid pruning removes 2 layers and then removes the same number of neurons from each dense. We use Wikitext2 perplexity to represent model performance, and the results are shown in Figure 2.\nWe give two conclusions from these results. First, pruning a small number of layers will not greatly impact the model performance. For example, the perplexity only increases slightly from 5.42 to 7.43 when removing 3 layers in LLaMA-2-7B by Figure 2(a). However, merely applying layer pruning leads to a substantial decrease in performance once a certain pruning ratio is reached. For example, the perplexity of OPT-6.7B rises from 18.64 to over 1200 when the number of removed layers increases from 4 to 5 in Figure 2(b). Second, the hybrid pruning method delivers the lowest perplexity among three pruning methods, and this advantage is obvious when the pruning ratio is high. Specifically, in Figure 2(c), at approximately a 30% pruning ratio for ChatGLM3-6B, the hybrid pruning method decreases the perplexity by about 90% and 99% compared with neuron pruning and layer pruning, respectively.\nDespite achieving the highest performance, hybrid pruning's approach lacks neuron selectivity. It is quite challenging to determine the appropriate number of neurons to prune while maintaining model performance during neuron-grained pruning. We will design an iterative pruning process to address the challenge in the subsequent section."}, {"title": "IV. HYBRID-GRANULARITY MODEL PRUNING", "content": "The pruning process of COMP is illustrated in Figure 3. To prune an LLM, we begin by evaluating the importance of each layer (\u2460). Following this, layer-grained pruning (\u2461) is performed to gradually remove some less important layers. For the remaining layers, specific pruning ratios are determined based on their evaluated importance. Next, we assess the importance of neurons within the denses of each remaining layer (\u2462) and execute neuron-grained pruning accordingly (\u2463). Finally, mask tuning (\u2464) is conducted to restore model performance. To achieve a high pruning ratio while ensuring satisfactory model performance, neuron-grained pruning and mask tuning are performed iteratively. The detailed process is outlined in Algorithm 1."}, {"title": "A. Layer-grained Pruning", "content": "Since the layer importance can be obtained through Eq. (3), the least important layers can be directly removed for layer-grained pruning. However, calculating a layer's importance depends on the output of its previous layer. If a layer is removed, the input of subsequent layers changes, thus the layer importance order derived from a single calculation may be inaccurate. Based on this, the layer importance order is established by iteratively calculating layer importance. Specifically, the layer with the smallest importance is removed in each iteration, and the layer's subsequent layers are reordered based on their new importance."}, {"title": "B. Neuron-grained Pruning", "content": "The first step in neuron-grained pruning is to specify the pruning ratio for the remainig layers. Intuitively, layers with lower importance will have higher pruning ratios. With a target pruning ratio of r, the pruning ratio for the l-th layer is defined as\n$r_l = \\frac{\\omega_l \\cdot (rN - n \\tilde{N})}{N}$ (10)\nwhere n is the number of removed layers, $\\omega_l$ represents the harmonic mean of the rest layer's importance, and N, $\\tilde{N}$ are the parameters' numbers of the model and the l-th layer individually. After $r_l$ is determined, we need to determine the number of pruning neurons $c_{l,k}$ in the k-th dense of the l-th layer. In order to find the appropriate number of pruning neurons in each dense, we adapt iterative pruning, gradually increasing $c_{l,k}$ to ensure that the output of the model changes within a controllable range.\nAssume that c neurons are pruned in a dense, and the corresponding pruning mask is $m_c$. After pruning, the tuned mask $m'_c$ is obtained through Eq. (5) to reconstruct the output of the dense. When c increases, the freedom degrees of $m'_c$ are reduced, which means that the non-zero elements in $m'_c$ have to bear more burden. Therefore, the change of non-zero elements will tend to be uncertain as c increases, and the variance of non-zero elements in the tuned mask becomes larger. The large variance of mask is not conducive to the generalization of the model [33], therefore it is crucial to control the variance of non-zero elements $Var(m'_c)$. However, the exact variance threshold cannot be calculated directly. To obtain a suitable threshold $\\upsilon_\\tau$, we set its initial value to zero and iteratively increase it while pruning. When c gradually increases until $Var(m'_c)$ reaches the current threshold $\\upsilon_\\tau$ in a dense, its next dense starts to be pruned. If a layer has reached the target pruning ratio after all its denses are pruned, its next layer starts pruning. If not, increase the variance threshold $\\upsilon_\\tau$ and start pruning the layer again.\nDuring the pruning process, the errors in each layer may accumulate. Because mask tuning aims to keep the dense outputs as close as possible to those of the original model, taking into account the impact of already-pruned layers can lead to cumulative errors, potentially causing the pruned model to overfit the calibration data. To alleviate the cumulative deviation from the original model, we let each pruned layer fully \"absorb\" the impact of pruning within itself. Concretely, we perform pruning a layer using the input from the original model."}, {"title": "V. EXPERIMENTS AND EVALUATION", "content": "To showcase the effectiveness and versatility of COMP, we perform tests over five open-source large language models with three kinds of structures: LLaMA-2-7B, LLaMA-2-13B [27], OPT-6.7B, OPT-13B [34], and ChatGLM3-6B [35]. We compare COMP with 3 state-of-the-art algorithms for structured pruning: LLM-Pruner [19], SliceGPT [28] and ShortGPT [28], respectively.\nFollowing SliceGPT [28], we assess the perplexity of all models on 3 typical datasets: WikiText2 [36], PTB [37], and Alpaca [38]. To test the model's task-agnostic capability, we complement the evaluation of the accuracy metric with LLaMA-2-7B and LLaMA-2-13B on 6 common reasoning datasets: BoolQ [39], WinoGrande [40], LogiQA [41], MMLU [42], PIQA [43] and SCIQ [44]. We select pruning rates of 20%, 25% and 30%, i.e., removing 20% to 30% model parameters.\nWe adopt the same settings as those in LLM-Pruner. First, we use only 10 randomly selected samples from C4 [45] as the calibration samples, each truncated to a sequence length of 128. Second, we keep the first two layers and the last layer of the model free from neuron pruning. Besides, we solve the linear equation by the LSMR solver in CuPy [46] to address the problem of the large size of the coefficient matrix. All experiments are conducted on NVIDIA A6000, with a GPU memory size of 48GB."}, {"title": "B. Main Results", "content": "Firstly, we conduct a comprehensive test on LLaMA-2-7B and LLaMA-2-13B models to verify the superiority of COMP, and the zero-shot results are shown in Table I. The COMP method outperforms the baseline approaches on the LLaMA2 models, particularly regarding perplexity. For instance, when the pruning ratio of the LLaMA-2-7B model is 30%, COMP achieves a perplexity of 19.61 on the WikiText2 dataset, significantly better than the perplexity of 47.99 achieved by LLM-Pruner. We observe that SliceGPT performs poorly, and the reason is limited calibration data and the large embedding dimension of the model. The PCA process in SliceGPT relies on the covariance matrix to derive the principal components. When the number of data is small and the dimension is large, the estimation of the sample covariance matrix can be highly inaccurate. In contrast, ShortGPT exhibits commendable performance on the LLaMA2 models. Specifically, when pruning the 13B model, its average accuracy score (61.49) is nearly comparable to that of COMP (61.88). Nevertheless, as the pruning ratio escalates, ShortGPT's performance progressively declines. On the LLaMA-2-7B model, COMP retains 91.2%, 85.8%, and 82.5% of the unpruned model's performance under three pruning ratios. For the LLaMA-2-13B model, these retention rates rise to 91.7%, 89.9%, and 86.3%. The demonstration proves the feasibility of COMP to effectively compress the large language model, even without relying on training data. Secondly, we test the perplexity performance of OPT-6.7B, OPT-13B, and ChatGLM3-6B to verify the versatility of COMP. COMP consistently outperforms other methods on the OPT models, especially on the Alpaca dataset. In Table II, given the pruning ratio of 30%, the perplexity of COMP on the 6.7B model and 13B model is 11.29 and 9.45 respectively, which is 42.75% and 38.52% lower than that of LLM-Pruner. Similar to SliceGPT, ShortGPT performs poorly on the three models, suggesting that blindly removing layers for model compression does not always work. Besides, both SliceGPT and LLM-pruner are unable to efficiently prune the ChatGLM3-6B model due to the inherent limitations of their methods. In contrast, COMP is still applicable. At a 20% pruning ratio, the perplexity of COMP on ChatGLM3-6B is only about doubled compared to the original model, which is within a reasonable acceptable range. COMP remains applicable across different models, demonstrating its versatility and adaptability.\nTo test the feasibility of COMP on resource-constrained devices, Figure 4 illustrates the GPU memory overhead of different methods during pruning. ShortGPT dynamically loads layers onto the GPU, thus the required memory for pruning is equivalent to that for a single-layer inference. COMP also dynamically loads layers during pruning, with the memory cost primarily arising from the evaluation of neuron importance. By Figure 4, COMP only needs 8GB of memory to prune LLaMA-2-7B and 11GB of memory to prune LLaMA-2-13B. For LLaMA-2-13B, COMP saves the memory overhead 62.68% and 84.36% compared with LLM-pruner and SliceGPT, respectively. Note that compared to pruning LLaMA-2-7B, COMP consumes more memory (13GB) when pruning the OPT-6.7B model. That is because the maximum dense input dimension in OPT-6.7B is 16384, which is larger than that in LLaMA-2-7B (11008). This determines the dimensions of the coefficient matrix When evaluating the neuron importance. So, COMP is more memory-friendly for models with lower embedding dimensions."}, {"title": "C. Ablation Study", "content": "We conduct a comparative analysis for the efficiency of calculating layer importance in section IV-A and adopt calculating layer importance once as the baseline. The perplexity results on the LLaMA-2-7B are shown in Figure 5. The two calculation methods yield the same layer importance ranking when fewer than three layers are removed. However, when more than three layers are removed, the iterative method demonstrates a distinct advantage. For example, when both remove 6 layers, the perplexity of the model with non-iterative layer removal is almost twice that of the model with iterative layer removal, which fully demonstrates the necessity of the iterative method. Efficiency of Identical Layer Input. In section IV-B, COMP used the same input as the original model when pruning neurons in each layer, rather than the input of the pruned layer. To verify the correctness of this approach, we tested two different methods on LLaMA-2-7B. The perplexity results are shown in the Table III. Obviously, using the same input is beneficial. In addition, when the pruning ratio increases from 20% to 30%, the method that does not use the same input deteriorates more significantly. Compared to the perplexity of using identical layer input, the perplexity that does not use identical input increases by 37.34%, 70.72% and 86.23% respectively. This is because when the pruning ratio increases, the output of each layer changes more than the output of the layer in the original model. While the accumulated layer input shifts, the fine-tuned mask is overfitted on the calibration data."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a lightweight post-training structured pruning method COMP. COMP first evaluates the layers' importance based on their input and output, and performs layer-grained Pruning. Then, COMP executes neuron-grained pruning prunes in the dense of each remaining layer. Concretely, COMP sorts the importance of the input neurons in a dense based on the condition number, iteratively prunes neurons, and uses the mask tuning method to reconstruct the output of the pruned dense. COMP has low memory overhead and wide applicability, which is suitable for pruning LLMs on resource-constrained devices, requiring only a minimal amount of calibration data.\nAt a 30% pruning ratio, COMP takes about 30 minutes to obtain the pruning results of LLaMA-2-7B in Table I, and about 1 hour to obtain the results of LLaMA-2-13B. When pruning the linear layer, COMP needs to iterate the pruning and mask adjustment steps to determine the number of pruned neurons, which is time-consuming."}]}