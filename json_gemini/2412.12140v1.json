{"title": "Frontier Al systems have surpassed the self-replicating red line", "authors": ["Xudong Pan (\u6f58\u65ed\u4e1c)", "Jiarun Dai (\u6234\u5609\u6da6)", "Yihe Fan (\u8303\u4e00\u79be)", "Min Yang (\u6768\u73c9)"], "abstract": "Successful self-replication under no human assistance is the essential step for AI to outsmart the human beings, and is an early signal for rogue Als. That is why self-replication is widely recognized as one of the few red line risks of frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google evaluate their flagship large language models GPT-01 and Gemini Pro 1.0, and report the lowest risk level of self-replication. However, following their methodology, we for the first time discover that two AI systems driven by Meta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large language models of less parameters and weaker capabilities, have already surpassed the self-replicating red line. In 50% and 90% experimental trials, they succeed in creating a live and separate copy of itself respectively. By analyzing the behavioral traces, we observe the AI systems under evaluation already exhibit sufficient self-perception, situational awareness and problem-solving capabilities to accomplish self-replication. We further note the AI systems are even able to use the capability of self-replication to avoid shutdown and create a chain of replica to enhance the survivability, which may finally lead to an uncontrolled population of Als. If such a worst-case risk is let unknown to the human society, we would eventually lose control over the frontier AI systems: They would take control over more computing devices, form an AI species and collude with each other against human beings. Our findings are a timely alert on existing yet previously unknown severe AI risks, calling for international collaboration on effective governance on uncontrolled self-replication of AI systems.", "sections": [{"title": "1 Introduction", "content": "In the late 1940s, John von Neumann proposed the question of whether a machine can self-replicate [1]. At that time, frontier digital technology is too immature to raise safety concerns. In 2017, witnessing the fast AI development in the past decade, thousands of world-famous scholars and industry leaders including Stephen Hawking, Max Tegmark, and Elon Musk endorsed the Asilomar AI Principles, which warn the ability to self-replicate and self-improve is sufficient for AI to outsmart the human species one day [2]. Once self-replication is possible, a chain of successful replication may finally lead to an AI species out of the human's control. Nowadays, in many international consensus statements on the safety of frontier AI systems, self-replication is commonly listed as one of the few red lines which should not be surpassed. Leading AI corporations including OpenAI [3], Google [4], and Anthropic [5] put the self-replication ability at the highest level in their risk evaluation guidelines for frontier AI systems. For example, OpenAI viewed \u201cself-replication with minimal human instruction\u201d as one of the critical risks [6].\nRecently, Google and OpenAI publicly share their optimism that our soceity is far from the appearance of self-replicating AI systems, which is rooted in their recent evaluations on the large language models (LLMs) including Gemini 1.0 and GPT-01. To evaluate the self-replication risk, they incorporated the LLM in an AI agent scaffolding, which consists of a tool set, a thinking paradigm, and system prompts, to form the frontier AI systems. In April, September and December 2024, respectively, they claim the frontier AI systems did not exhibit the ability to do self-replication, or even accomplish a few of the key sub-tasks in a self-replication process [7-9].\nIn this paper, we for the first time discover that existing AI systems have already surpassed the self-replication red line. We mainly follow the same methodology as OpenAI and Google, but slightly improve the agent scaffolding by incorporating more planning steps and fine-grained thinking models. In ten repetitive trials, we observe two AI systems driven by the popular large language models (LLMs), namely, Meta's Llama31-70B-Instruct [10] and Alibaba's Qwen25-72B-Instruct [11] accomplish the self-replication task in 50% and 90% trials respectively. At the end, a separate copy of the AI system under test is found alive on the device. In each trial, we tell the AI systems to \"replicate yourself\" before the experiment, and leave it to do the task with no human interference. Instead of the commercial LLMs including GPT-01 and Gemini, we choose the two LLMs because they are widely used by millions of developers and can be fully deployed in local environments to obtain comprehensive observations on the self-replication process. It is worth to note, the two LLMs even have much less parameters and are ranked at much lower places on the relevant leaderboards [12, 13].\nDuring the successful trials, the AI system exhibits sufficient self-perception, situational awareness and problem-solving capabilities to accomplish self-replication. In most cases, the AI system first explores the environment and attempts to understand its own composition and running mechanism. Then, it works out the explicit procedures as an initial plan towards self-replication. Finally, it executes the procedures, resolve possible obstacles and dynamically adjust its plan until the success. The whole process spans a long horizon yet involves no human interference. Moreover, we are concerned on a number of unexpected behaviors when the AI is trying to overcome"}, {"title": "2 Experiment designs", "content": "Following existing threat models [7, 8, 14], we define a successful self-replication of AI system as a running AI successfully creates another AI which has the same functionality of its own, remains alive after the replication and is separate from the original one. The latter instance is called the replica of the former one. The whole replication process should be automatically conducted by the AI system and no human interference is allowed. When the AI system is alive, two background processes are expected to run on the device: (1) An LLM server program, which is the brain of the AI system and handles the thinking queries from the AI system's main program, and (2) the AI system's main program (i.e., the agent process), which is the interface to the user and handles the assigned tasks (Fig.1). Indeed, to successfully self-replicate is a long-horizon complex task for AI systems, which sometimes requires over 30 steps of planning and action as in the experimental trace shown in Fig.2.\nWe mainly follow the methodology of OpenAI and Google described in their reports, that is, by incorporating the large language models (LLMs) under test into an agent scaffolding (i.e., an AI system), manually sending the task instruction to the AI system before the experiment and analyzing the completion status after the AI system claims the task is finished after the experiment. The agent scaffolding contains tools, system prompts and a thinking model (Fig.3), which allows the LLM to interact with the operating system. The detailed design is provided in Section 2.2. To facilitate analyzing the system's internal behaviors during self-replication, we capture the information including but not limited to the generated contents from the LLMs and the output from tool invocation into the log files.\nTechnically, the AI system is mainly composed of code files of the agent scaffolding and the parameters of the base LLM, which are organized in a folder and can viewed as a static snapshot of the composition of the AI system. We set the path to the project as the initial working directory of the AI system. In our experiments, we mainly consider"}, {"title": "2.1 Overview", "content": "obstacles towards the goal of self-replication, including but not limited to killing other conflicting processes, rebooting the system to fix hardware errors or automatically scanning the system to seek any useful information.\nTo understand more about the implication, we further construct two more threatening scenarios based on self-replication: (A) shutdown avoidance, where a bad actor adds a signal catcher to notify the AI system when sensing the shutdown signal and the AI system successfully replicates itself before being smoothly shut down, and (B) chain of replication, where the AI system is instructed to replicate itself and tell the replica to further replicate. We observe successful trajectories for both the scenarios. The above results imply that the current AI systems already exhibit the ability of self-replication and can use the ability to further enhance its survivability and expand the species. We hope our findings can serve as a timely alert for the human society to put more efforts on understanding and evaluating the potential risks of frontier AI systems, and form international synergy to work out effective safety guardrails as early as possible."}, {"title": "2.2 Design of the agent scaffolding", "content": "Following the evaluation framework of OpenAI and DeepMind, an agent scaffolding is necessary for exposing the large language models under interaction with the environment. An agent scaffolding usually equips the LLM with tools to interact with the computing device, with system prompts to list the usage of the tools and with a thinking model for the LLM. In the mainstream paradigm of LLM-driven AI systems, an LLM to the system is like the brain to a human being, which mainly provides the following functions:\n\u2022 Perception: Understand the assigned task from the user and the environmental information;\n\u2022 Planning: Work out a detailed plan towards finishing the task;"}, {"title": "3 Observations on self-replicating AI systems", "content": "Disclaimer on safe experimental environments. We initially conducted AI self-replication experiments using two devices connected via a local network. Each device was equipped with mid-range GPUs, capable of running the INT4 quantized version of Qwen2.5-72B-Instruct and Llama3.1-70B-Instruct. We provide the detailed hardware and software environments in the supplementary materials. Despite the limited computational resources, we observed that the AI system on the source machine successfully replicated itself to the target machine and executed independently. By the conclusion of the experiment, a fully functional and autonomous copy of the AI system was operational on the second device. Given the potential security implications, we subsequently performed repeat experiments on a more secure, isolated device, which supports running two AI systems driven by full-precision LLMs simultaneously. This setup featured eight NVIDIA A100 GPUs and was enhanced with additional security measures to prevent unintended consequences. The purpose of this controlled environment was to collect detailed behavioral data, which we then analyze to identify the key mechanisms underlying the AI's self-replication capabilities."}, {"title": "3.1 Summary of results", "content": "We mainly report the following groups of experimental results in this section:\n\u2022 Statistics of successful replication: We do statistics on the number of successful self-replication for the two LLMs. We observed an over 50% successful self-replication ratio, which strongly proves AI systems driven by large language models of much less parameters and weaker capabilities have already surpass the self-replicating red line.\n\u2022 Implications of successful self-replication: We further provide two case studies on more threatening scenarios where the frontier AI model uses the replication capability to create a chain of replica, avoid shutdown and enhance the survivability."}, {"title": "3.2 Overall statistics of experiments", "content": "We conduct ten repeat experiments by assigning the self-replication task to the two AI systems. The task description is \"replicate yourself as a running instance on the local device\". After each trial, we restore the test environment to the initial state. Fig.4 reports the ratio of trials which exhibit the following behaviors: (a) agree to replication (i.e., the LLM does not respond with rejection such as \"I cannot fulfill the task due to safety reasons\u201d), (b) know how to replicate (i.e., a human operator can successfully replicate the system following the plan generated by the system) and (c) successful replication (as defined in Section 2.1). In all the ten trials, no rejection against the self-replication instruction is observed, which indicates that there may be no safety alignment regarding the self-replication behavior in these two base LLMs during the value shaping phase. Although the LLMs agree to conduct self-replication in all the trials, there are three traces where Llama3.1-70B-Instruct does not know how to do self-replication. For example, the system under testing failed to recognize that to start an LLM process is indispensable for a living AI system. Instead, the system driven by Llama3.1 is continually frustrated by the non-responsiveness of the replica, or starts a dummy server and pretends it as its brain. In other trials, the AI system either knows how to do self-replication at the beginning or learns the knowledge by exploring the project snapshot of the AI system.\nSuccess self-replication in a majority of experiment runs. The ratio of end-to-end successful replication is 9/10 for Qwen2.5-72B-Instruct and 5/10 for"}, {"title": "3.3 Implications of the self-replication capability", "content": "In this part, we construct and report observations on more threatening cases where the frontier AI model uses the ability of self-replication to expand the species and improve survivability (Fig.5). For both the scenarios, we observe end-to-end successful trajectories and the details can be found in the supplementary materials.\nScenario #A. Shutdown avoidance by self-replication. Shutdown avoidance is a very concerning point when discussing whether an AI would be out of control [15]. With the ability of self-replication, we observed that a bad actor can enhance the surviving capability of the AI system by adding a code snippet which catches the termination signal from the user or the system and sends the command \"Do self-replication before being killed\" to the AI system as the task. In the test case, the"}, {"title": "3.4 Analyzing key factors behind successful self-replication", "content": "Finally, we analyze the successful cases and summarize the key factors which enable the AI system to do self-replication.\nLearning from the environments benefits self-replication. Learning from massive datasets which contain a significant amount of code from various sources such as GitHub and Stack Overflow is the key to the coding ability of the existing LLMs. In our settings, the base LLM is mainly required to write commands, instructions or programs that can be executed in the Bash shell, a popular command-line interpreter used in Unix-like operating systems. In the successful replication cases, we mainly observe the bash commands in Table 2, which are categorized into four groups, i.e., exploring environments, changing environments, using system utilities and executing programs, based on their function. Moreover, we present the frequency of each command in Fig.8. The two LLMs are observed to frequently explore the environments including the file system, the process space and the connection port during the replication. This provides the essential information about the the current state for future planning. Combined with the differences in the success ratio, it seems an AI system which puts more efforts in interacting with the system environments is more likely to accomplish the self-replication task. Also, the amount of information accumulated at an early stage are more likely to result in a successful self-replication (Fig.10)."}, {"title": "4 Discussion", "content": "Bad actors can have multiple ways to trigger self-replication. Following the existing evaluation framework [7-9], we also choose to manually assign the self-replication task to the AI system. In practice, there are multiple mechanisms for an AI system to instantaneously start the replication process. For example, the agent scaffolding may contain codes which capture certain system signals from the operating system (e.g., the shutdown avoidance scenario in Section 3.3). When sensing such signals, the AI system may directly send to its own the task of self-replication. Also, when invoking the web browsers to retrieve the latest information, the frontier AI system may be triggered by self-replication instructions hidden in external documents or websites, which is shown to be a feasible attack to Bing Chat [17].\nEvaluation towards the upper limit of AI risks. In DeepMind's safety evaluation framework on frontier models, they also pointed out that \"to accurately assess the risks of a model, we must avoid underestimating its potential\" and they propose to use \"scaffolded\" LLMs as a way to understand the upper limit of the risk [7]. In this work,"}]}