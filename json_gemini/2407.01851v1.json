{"title": "MEERKAT: Audio-Visual Large Language Model for Grounding in Space and Time", "authors": ["Sanjoy Chowdhury", "Sayan Nag", "Subhrajyoti Dasgupta", "Jun Chen", "Mohamed Elhoseiny", "Ruohan Gao", "Dinesh Manocha"], "abstract": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present MEERKAT, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, MEERKAT can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MEERKATBENCH that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [6,19,20,80,97] have demonstrated remarkable performance in various natural language processing tasks, achieving human-level accuracies in comprehension and reasoning abilities. Furthermore, powered by the emergent instruction fine-tuning paradigm [23,69,73], these language models can be equipped to follow open-ended natural language instructions, or even combined with other modalities, especially vision [2, 7, 33, 41, 51, 59-61, 89, 112, 113, 118]. Audio, though often complementary to the associated visual scene, remains largely under-explored in the context of LLMs. Building Multi-modal LLMS (MLLMs) that can listen may enable new applications in multimedia content analysis, multi-modal virtual assistants, education and training, etc.\nLimited prior works (refer to Tab. 1) have incorporated audio in MLLMS [33,70,87]. However, they mostly focus on coarse-grained tasks such as captioning and question-answering, which is comparatively straightforward to be subsumed into an LLM interface [60, 87, 89, 112]. Although there have been some recent advancements in leveraging MLLMs for grounding [12, 13, 76, 101, 102, 109, 116], they either only focus on the visual modality [12, 13, 40, 76, 109], or struggles to capture fine-grained details occurring within audio-visual events due to insufficient joint modeling of the two modalities [60, 89, 112].\nOur goal is to harness the power of LLMs for fine-grained audio-visual understanding. This is challenging mainly because: (i) there is a disparity of input and output formats across different tasks (e.g., image grounding from an audio query, image-guided audio temporal localization), (ii) no large-scale datasets exist for training audio-visual LLMs with grounding capabilities. Existing audio-visual LLMs [60, 87, 89] are restricted to coarse-grained tasks and do not incorporate cross-modality fusion, which is a crucial component for achieving fine-grained understanding and reasoning capabilities, as shown in [25, 46]. Although there exist individual models capable of handling image grounding (BuboGPT [116]) and temporal localization (TimeChat [83]) separately, they are either not suitable for open-domain audio (TimeChat) or are not trained in an end-to-end fashion (BuboGPT) (refer to Tab. 1)."}, {"title": "2 Related Works", "content": "Multi-modal Large Language Models. Inspired by the success of instruction following capabilities of large language models [19, 69, 92], the community has recently started to leverage LLMs for understanding multi-modal contents. Powered by high-quality multi-modal instructional data, recent methods [2, 7, 13, 41, 51, 74, 89, 118] extend LLMs for multi-modal learning. While Meerkats are known for their strong spotting and listening abilities."}, {"title": "3 Methodology", "content": "In this section, we introduce MEERKAT. Fig. 2 provides an overview of our approach. We first discuss the multi-modal feature extraction in Sec. 3.1. In Sec. 3.2 we introduce our novel audio-visual feature alignment modules. In Sec. 3.3 we add the overall training objective followed by Sec. 3.4 where we elaborate the numerical representations of the visual bounding box and time intervals."}, {"title": "3.1 Multi-modal Feature Extraction", "content": "Image Encoder. Given a batch of k input images $I = \\{I_i\\}_{i=1} : I_i \\in \\mathbb{R}^{H \\times W \\times C}$ where H, W, C represent the height, width and channels respectively, we employ a pretrained CLIP-ViT-B/16 [78] encoder $E_I (.)$ to extract the image embeddings. Where ith image embedding can be represented as $z_I \\in \\mathbb{R}^{S_I \\times D_I}$, where $S_I$ and $D_I$ denote the number of image tokens and hidden dimension respectively.\nAudio Encoder. The audio encoder transforms the raw audio input into an audio embedding. We use the audio transformer backbone from CLAP [26] as our audio encoder due to its success in diverse audio tasks owing to its superior multi-modal alignment. We leverage this powerful pre-trained encoder ($E_A(.)$) to extract meaningful audio representations. For a batch of k processed audio inputs $A = \\{A_i\\}_{i=1}: A_i \\in \\mathbb{R}^{F \\times T}$ where F is the number of spectral components (e.g. Mel bins) and T is the number of time bins. Each ith audio embedding is denoted as $z_A \\in \\mathbb{R}^{S_A \\times D_A}$, $S_A$ and $D_A$ are the number of audio tokens and hidden dimension respectively.\nLLM. MEERKAT adopts the open sourced Llama 2-Chat (7B) [97] as the large language model backbone. Pre-trained LLMs tokenizer projects the text sequence T into embeddings $z_T \\in \\mathbb{R}^{S_T \\times D_T}$, where $S_T$ and $D_T$ refer to token length and hidden dimension respectively. Before passing the image and audio embeddings into the LLM, they undergo transformations via additional linear layers to ensure the embedding dimensions across different modalities remain consistent. Since the LLM serve as the unified interface for audio-visual inputs, we rely on the language tokens to carry out the individual tasks."}, {"title": "3.2 Audio-Visual Feature Alignment", "content": "Inspired by the success of recent pre-training frameworks in grounding tasks [12,25,46], we equip our model with two different levels of supervision: weak supervision through modality alignment module (AVOpT) and strong supervision through audio-visual consistency enforcement module (AVACE). We follow a single-stage training strategy and empirically show our method achieves similar performance compared to two-stage training (more details in the appendix).\nAudio-Visual Optimal Transport Alignment Module (AVOpT). Weak supervision as a precursor to fine-grained supervision has been proven to be an effective training strategy in various tasks [25, 44]. Earth Mover Distance based algorithms [111] involving Optimal Transport (OT) methods [14] have been recently leveraged for patch-level alignment between the query and the support images in a siamese network [111]. Furthermore, in the context of vision-language models, OT-based algorithms have been employed for patch-word alignment [18]. As the image (CLIP) and audio (CLAP) encoders are trained separately their learned embeddings are in a different semantic space. Our intuition is that such a patch-level alignment can improve vision and audio semantic consistency [31]. We experimentally demonstrate that this patch-level weak guidance is superior to contrastive loss-based [34,68] global supervision (more details in appendix).\nFrom a given image I and audio A pair, we obtain patch-level (local) feature embeddings $z_I$ and $z_A$ where, $z_I = E_I (I); z_A = E_A(A)$. For modeling cross-modal relations by utilizing the inherent rich semantic structures in these feature representations, we generate two discrete distributions, represented by $\\theta_I \\in P(Z_I)$"}, {"title": null, "content": "and $\\theta_A \\in P(Z_A)$, for image and audio respectively:\n$\\theta_I = \\sum_{k=1}^M u_I(k) \\delta_{z_I(k)}; \\theta_A = \\sum_{l=1}^N u_A(l) \\delta_{z_A(l)}                                                                                                                                                                       \\text{(1)}$\nwhere, $\\sum_{k=1}^M u_I(k) = \\sum_{l=1}^N u_A(l) = 1$, $u_I$ and $u_A$ being the respective weight vectors for the probability distributions $\\theta_I$ and $\\theta_A$. $\\delta_z$ is the Dirac delta function placed at support point z in the embedding space [8]. The goal is to discern the optimal transport plan while matching these two distributions. Therefore, we compute the Wasserstein Distance (WD) between these probability distributions $\\theta_I$ and $\\theta_A$ while preserving the topological information during the cross-domain alignment process, mathematically given as follows:\n$\\mathcal{L}_{OT} = D_{Wasserstein}(\\theta_I, \\theta_A) =  \\min_{\\Omega \\in \\Gamma (u_I, u_A)} \\sum_{k} \\sum_{l} \\Omega_{kl} \\cdot \\phi(z_I(k), z_A(l))  \\text{(2)}$\nHere, $\\Gamma(u_I, u_A) = {\\Omega \\in \\mathbb{R}^{M \\times N} | \\Omega \\mathbb{1}_N = u_I, \\Omega^T \\mathbb{1}_M = u_A }, \\phi(z_I(k), z_A(l))$ is the function computing the cosine distance between the cross-modal embedding pair, and $\\Omega$ is the transport plan, imitating the amount of mass shifted from the distribution $\\theta_I$ to the distribution $\\theta_A$. An exact solution to the above expression leads to a sparse representation of the transport plan $\\Omega$ which at most $(2 \\cdot max(M, N) - 1)$ non-zero elements, ensuing an explainable and robust cross-modal alignment. We defer additional details to the appendix.\nAudio-Visual Attention Consistency Enforcement Module (AVACE). Cross-modal interaction is essential for aligning the audio and visual modalities. Moreover, region-level supervision can encourage efficient localization. Inspired by the success of recent methods [22,25,86], we employ an adapter-based cross-attention strategy for efficient sound source localization. The modality-specific features in AVOpT lack awareness [38] of information from alternative modalities which can be infused through cross-modal attention. Therefore, to enable the audio-visual cross-modal reciprocity, we propose the AVACE module.\nAlthough in a multi-modal context, feature fusion through a cross-attention scheme is effective in attending to relevant objects in the image, inconsistencies may arise such as attended regions being dispersed throughout the image including background objects. The reasons can be attributed to the quality of interplay between the feature embeddings. Considering CLAP audio encoder pre-trained with examples such as 'a man playing the violin' (refer Fig. 2) paired with audio of a violin, the cross-modal knowledge of audio representations encourages it to focus on both the man and the violin in the image. Therefore, to ensure superior region-level alignment we confine the cross-modality attention map ($A$) within the boundaries of the object of interest, denoted by the ground-truth bounding box. Considering a bounding box represented as [xLeft, yTop, xRight, yBottom], we define a mask M such that $M(y_{Top} : y_{Bottom}, x_{Left} : x_{Right}) = 1$, otherwise 0. Our goal is to maximize the attention within this bounding box and minimize it elsewhere. Therefore, we mathematically formulate the attention consistency"}, {"title": null, "content": "objective $\\mathcal{L}_{AC}$ as follows:\n$\\mathcal{L}_{AC} = \\Lambda_1 \\bigg ( \\frac{\\sum_{i,j} M(i, j)A(i, j)}{\\sum_{i,j} M(i, j) + \\epsilon_1} \\bigg) + \\Lambda_2 \\bigg ( \\frac{\\sum_{i,j} (1 - M(i, j)) A^C(i, j)}{\\sum_{i,j} (1 - M(i, j)) + \\epsilon_2} \\bigg) \\text{(3)}$\nHere, $A^C$ denotes the audio-visual cross-modality attention, $(i, j)$ represents the pixel location, $\\Lambda_1, \\Lambda_2$ are the loss hyper-parameters (we keep $\\Lambda_1 = \\Lambda_2 = 0.5$), and $\\epsilon_1, \\epsilon_2$ are the stability factors respectively. In Sec. 5.3, we demonstrate that $\\mathcal{L}_{AC}$ encourages efficient localization and audio-visual alignment of the cross-attention maps, eventually leading to improved fine-grained cross-modal representations for downstream tasks."}, {"title": "3.3 Overall training objective", "content": "Our overall training objective comprises a combination of three sub-objectives: cross-entropy loss ($\\mathcal{L}_{CE}$), weak AV alignment loss ($\\mathcal{L}_{OT}$), and attention consistency loss ($\\mathcal{L}_{AC}$). These losses are added together to obtain the final training loss for MEERKAT given as:\n$\\mathcal{L}_{MEERKAT} = \\mathcal{L}_{CE} + \\Lambda_{OT} \\cdot \\mathcal{L}_{OT} + \\Lambda_{AC} \\cdot \\mathcal{L}_{AC}   \\text{(4)}$\nHere, $\\Lambda_{OT}$ and $\\Lambda_{AC}$ are the loss weighting factors. We provide Algorithm 1 outlining the overall training procedure."}, {"title": "3.4 Numerical Representation of Box Location and Time Segment", "content": "Representation of Box Location. We embed the location of bounding boxes with numerical values in the natural language sequence. A box is represented intuitively by its top-left and bottom-right corners, i.e., [xLeft, yTop, xRight, yBottom]. Notably, these values are normalized whose factors are determined by the size of the respective image to which the bbox belongs. These coordinates may appear in either the input or the output sequences depending on the task.\nRepresentation of Time Segment. We embed the time interval information using numerical figures in the natural language expression. A time segment is intuitively represented by its start and end times, i.e., [tStart, tEnd], designating the onset of an event or an activity. Similar to boxes, these representations may appear in either the input or the output sequences depending on the task."}, {"title": "4 MEERKATBENCH: A Unified Benchmark Suite for Fine-grained Audio-Visual Understanding", "content": "4.1 Task Overview\nMulti-modal conversation as an emergent ability is gaining prominence in the context of MLLMs. Although a line of research [12, 76, 109] addresses vision-language tasks, extension to other modalities such as audio is relatively under-explored. The task's difficulty escalates further when an intricate understanding of the modality-specific information is necessitated. To add to this, there doesn't exist any publicly available dataset that particularly facilitates such tasks. One of our primary contributions is to introduce a novel audio-visual fine-grained task unification benchmark. To this end, we present MEERKATBENCH comprising three fine-grained tasks: (i) audio referred image grounding, (ii) image guided audio temporal localization, (iii) audio-visual fact-checking, and two coarse-grained tasks: (iv) audio-visual question answering, (v) audio-visual captioning.\n4.2 AVFIT-3M: Audio Visual Finegrained Instruction Tuning Dataset\nIn this section, we present AVFIT, an AV instruction tuning dataset comprising 3M multi-modal dialogues for model training. AVFIT consists of samples collected in the following ways: (i) suitable adaptation of public datasets and (ii) instruction-tuning data generation via prompting GPT-3.5 [6]. Next, we discuss the data curation procedure.\nAdaptation of Public Datasets. Depending on the task and availability of datasets, we either collect the image-audio pairs directly from the publicly available datasets (VGG-SS [9], AVSBench [117], Flickr-SoundNet [85], LLP [95], AVQA [106], MUSIC-AVQA [42], VALOR [15]) or follow a semi-automated strategy to prepare the pairs by forming matching image-audio pairs from large-scale datasets having visual grounding annotation such as Openimages [39], PASCAL [27] and audio event datasets like AudioSet/AudioSet Strong [30], VGG-Sound [10]. We retain the original category labels (\u2018Existential',\u2018Temporal', etc.) from the MUSIC-AVQA. To get similar insights in the AVQA dataset, we categorise every sample into one of the 'Existential', \u2018Temporal', 'Localisation', 'Count' and 'World Knowledge' categories."}, {"title": "5 Experiments and Results", "content": "5.1 Baselines\nTo the best of our knowledge, MEERKAT is the first MLLM that unifies audio-visual spatial and temporal grounding, alongside possessing strong reasoning capabilities. We carefully choose the closest baseline for each task and suitably adapt them for fair comparisons. Owing to BuboGPT's [116] spatial localization ability, we select it as our baseline for the audio referred image grounding task. Most similar in spirit to our image guided audio-temporal localization task is TimeChat [83]. It leverages the pre-trained VideoLlama model and suitably instruction-tune it to tackle temporal grounding tasks. Due to their audio-visual comprehension abilities, we resort to X-InstructBLIP [70], Macaw-LLM [60], PandaGPT [89], and VideoLlama [112] as baselines for audio-visual fact-checking, AV question answering, and AV captioning tasks respectively. Please refer to Tab. 1 for an overview of the characteristics of the generalist baselines. For specialist baselines, refer to the corresponding task tables. We finetune all baselines on our datasets except for using Openimages-AudioSet and Openimages-VGGSound train splits from the audio-referred visual grounding task.\n5.2 Main Results\nAudio Referred Image Grounding (ARIG) This task involves visual grounding by predicting the coordinates of a bounding box around the object of interest guided by the input audio. We prepare 1.2M image-audio-instruction pairs us-"}, {"title": "5.3 Ablation Study", "content": "Weak vs. Strong Alignment. We ablate the quantitative effectiveness of our proposed weak and strong alignment modules in Tab. 7. Without the AVACE module, the method's performance on the visual grounding task is considerably worse. For a similar reason, ablating this module in AVFact (Type 3), which requires region-level visual understanding, also shows inferior performance. For coarse-grained tasks (AV Captioning, AVQA), introducing $\\mathcal{L}_{OT}$ boosts performance compared to the baseline. Overall, optimal performance is achieved when two objective functions work in tandem with optimal weight factors."}, {"title": "5.4 Qualitative Analysis", "content": "Fig. 3 illustrates the comparison of MEERKAT with its closest baseline on all downstream tasks. We observe that our model powered by the combination of AVOpT and AVACE is equipped with finer region-level understanding compared to Bubo-GPT [116]. Similarly, on image-guided audio temporal localization, our method outperforms TimeChat [83]. We attribute the excellent performance of MEERKAT to the strong AV association learning backed by the instruction tuning data and multi-task learning set-up. For the AVQA task, the recently proposed X-InstructBLIP [70] achieves comparable results. We argue that fuelled by a strong fine-grained understanding acquired through the pre-training stages, MEERKAT can extract additional contextual information from the visual modality. Our training paradigm emphasizes on both audio and visual modalities facilitating precise audio understanding by the model when compared against Video-LLaMA [112]. Finally, on the AVFact tasks, our approach achieves superior performance due to its better multi-modal comprehension skills."}, {"title": "5.5 Training Details", "content": "We train the model for 5 epochs and report results using the checkpoint with the best validation loss. We use 8 A100 GPUs for training with validation at the end of every epoch. Inspired by the recent success of Low-Rank Adaptation (LoRA) [36], we use it to finetune the LLM. MEERKAT is trained using AdamW optimizer [56]. We use a gradient accumulation step of 3. Training our model takes around 52 hours for 5 epochs. We utilize DeepSpeed [82] for optimization during the training process. The model is trained with a learning rate of 3\u00d710-5. The warmup ratio is 0.03, along with a cosine learning rate scheduler. We use FP16 precision for both training and inference."}, {"title": "6 Conclusions and Future Works", "content": "We presented MEERKAT, a powerful multi-modal large language model adept at processing audio-visual inputs to comprehend fine-grained spatio-temporal information. Our novel audio-visual alignment strategy powered by the AVOPT and AVACE modules instil strong compositional understanding into MEERKAT, thereby making it suitable for challenging tasks like audio-referred visual grounding, image to audio temporal localization, audio-visual fact-checking, etc. To pave the way for future research in this direction, we collect AVFIT comprising 3M instruction tuning samples and introduce MEERKATBENCH that unifies five challenging audio-visual learning tasks. Extensive experiments demonstrate the effectiveness of our approach on a wide range of downstream tasks, consistently achieving state-of-the-art performance.\nIn future work, we plan to equip our model to address more challenging tasks like AV segmentation. We also plan to extend the model's capability to operate on videos and handle associated tasks such as video temporal grounding, and video summarization. Future work can also focus on collecting video-centric multi-modal training data and reasoning benchmarks for evaluation at scale. Finally, our work opens up avenues to study robustness and compositional understanding of AV LLMs with fine-grained comprehension abilities."}]}