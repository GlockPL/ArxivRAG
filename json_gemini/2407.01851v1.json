{"title": "MEERKAT: Audio-Visual Large Language Model for Grounding in Space and Time", "authors": ["Sanjoy Chowdhury", "Sayan Nag", "Subhrajyoti Dasgupta", "Jun Chen", "Mohamed Elhoseiny", "Ruohan Gao", "Dinesh Manocha"], "abstract": "Leveraging Large Language Models' remarkable proficiency in text-based tasks, recent works on Multi-modal LLMs (MLLMs) extend them to other modalities like vision and audio. However, the progress in these directions has been mostly focused on tasks that only require a coarse-grained understanding of the audio-visual semantics. We present MEERKAT, an audio-visual LLM equipped with a fine-grained understanding of image and audio both spatially and temporally. With a new modality alignment module based on optimal transport and a cross-attention module that enforces audio-visual consistency, MEERKAT can tackle challenging tasks such as audio referred image grounding, image guided audio temporal localization, and audio-visual fact-checking. Moreover, we carefully curate a large dataset AVFIT that comprises 3M instruction tuning samples collected from open-source datasets, and introduce MEERKATBENCH that unifies five challenging audio-visual tasks. We achieve state-of-the-art performance on all these downstream tasks with a relative improvement of up to 37.12%.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [6,19,20,80,97] have demonstrated remarkable performance in various natural language processing tasks, achieving human-level accuracies in comprehension and reasoning abilities. Furthermore, powered by the emergent instruction fine-tuning paradigm [23,69,73], these language models can be equipped to follow open-ended natural language instructions, or even combined with other modalities, especially vision [2, 7, 33, 41, 51, 59-61, 89, 112, 113, 118]. Audio, though often complementary to the associated visual scene, remains largely under-explored in the context of LLMs. Building Multi-modal LLMS (MLLMs) that can listen may enable new applications in multimedia content analysis, multi-modal virtual assistants, education and training, etc.\nLimited prior works (refer to Tab. 1) have incorporated audio in MLLMs [33,70,87]. However, they mostly focus on coarse-grained tasks such as captioning and question-answering, which is comparatively straightforward to be subsumed into an LLM interface [60, 87, 89, 112]. Although there have been some recent advancements in leveraging MLLMs for grounding [12, 13, 76, 101, 102, 109, 116], they either only focus on the visual modality [12, 13, 40, 76, 109], or struggles to capture fine-grained details occurring within audio-visual events due to insufficient joint modeling of the two modalities [60, 89, 112].\nOur goal is to harness the power of LLMs for fine-grained audio-visual understanding. This is challenging mainly because: (i) there is a disparity of input and output formats across different tasks (e.g., image grounding from an audio query, image-guided audio temporal localization), (ii) no large-scale datasets exist for training audio-visual LLMs with grounding capabilities. Existing audio-visual LLMs [60, 87, 89] are restricted to coarse-grained tasks and do not incorporate cross-modality fusion, which is a crucial component for achieving fine-grained understanding and reasoning capabilities, as shown in [25, 46]. Although there exist individual models capable of handling image grounding (BuboGPT [116]) and temporal localization (TimeChat [83]) separately, they are either not suitable for open-domain audio (TimeChat) or are not trained in an end-to-end fashion (BuboGPT) (refer to Tab. 1)."}, {"title": "2 Related Works", "content": "Multi-modal Large Language Models. Inspired by the success of instruction following capabilities of large language models [19, 69, 92", "118": "extend LLMs for multi-modal learning. While"}]}