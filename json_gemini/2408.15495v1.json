{"title": "Remove Symmetries to Control Model Expressivity", "authors": ["Liu Ziyin", "Yizhou Xu", "Isaac Chuang"], "abstract": "When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a \"collapse.\" Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We first prove two concrete mechanisms through which symmetries lead to reduced capacities and ignored features during training. We then propose a simple and theoretically justified algorithm, syre, to remove almost all symmetry-induced low-capacity states in neural networks. The proposed method is shown to improve the training of neural networks in scenarios when this type of entrapment is especially a concern. A remarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge of the symmetry.", "sections": [{"title": "1 Introduction", "content": "The unprecedented scale and complexity of modern neural networks, which incorporate a vast number of neurons and connections, inherently introduce a high degree of redundancy in model parameters. This complexity in the architecture and the design of loss functions often implies that the loss functions are invariant to various hidden, nonlinear, and nonlocal transformations of the model parameters. These invariant transformations, or \"symmetries,\" in the loss function have been extensively documented in the literature, with common examples including permutation, rescaling, scale, and rotation symmetries [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].\nA unifying perspective to understand how these symmetries affect learning is the framework of reflection symmetries [11].\u00b9 A per-sample loss function l possesses a P-reflection symmetry if for all 0, and all data points x,\n$$l((I \u2013 2P)0, x) = l(0,x).$$\nThe solutions where 0 = (I \u2013 P)0 are the symmetric solutions with respect to P. It has been shown that almost all contemporary models contain quite a few reflection symmetries. Permutation, rescaling, scale, and rotation symmetries all imply the existence of one or multiple reflection symmetries in the loss.\nRecent literature has shown that symmetries in the loss function of neural networks often lead to the formation of low-capacity saddle points within the loss landscape [12, 13, 14]. These saddle points are located at the symmetric solutions and often possess a lower capacity than the minimizers of the loss [11]. When a model encounters these saddle points during training, the model parameters are not only slow to escape them but also attracted to these solutions because these the gradient noise also vanish close to these saddles [14, 15]. Essentially, the model's learning process stagnates, and it fails to achieve optimal performance due to reduced capacity. However, while many works have characterized the dynamical properties of training algorithms close to symmetric solutions, no methods are known to enable full escape from them."}, {"title": "2 Discrete Symmetries in Neural Networks", "content": "Notation. For a matrix A, we use A+ to denote the pseudo-inverse of A. For groups U and G, U <G denotes that U is a subgroup of G. For a vector w and matrix D, $||0||^2_3 := wTDw$ is the norm of w with respect to D. $\\odot$ denotes the element-wise product between vectors.\nLet f(0,x) be a function of the model parameters 0 and input data point x. For example, f could either be a sample-wise loss function l or the model itself. Whenever f satisfies the following condition, we say that f has the P-reflection symmetry (general symmetry groups are dealt with in Theorem 5 in Section 5).\nDefinition 1. Let P be a projection matrix and O' be a point. f is said to have the (0', P)-reflection symmetry if for all x and 0, (1) f(0 + 0',x) = f((I \u2013 2P)0 +0',x), and (2) P0\u2032 = 0'.\nThe second condition is due to the fact that there is a redundancy in the choice of \u03b8' when \u03b8' \u2260 0. Requiring Pe' = 0' removes this redundancy and makes the choice of e' unique. Since every projection matrix can be written as a product of a (full-rank or low-rank) matrix O with orthonormal columns, one can write P = OOT and refer to this symmetry as an O symmetry. In common deep learning scenarios, it is almost always the case that 0\u2032 = 0 (for example, this holds for the common cases of rescaling symmetries, (double) rotation symmetries, and permutation symmetries, see Theorem 2-4 of Ref. [11]). A consequence of 0' = 0 is that the symmetric projection Pe of any e always has a smaller norm than 0: thus, a symmetric solution is coupled to the solutions of weight decay, which also favors small-norm solutions. As an example of reflection symmetry, consider a simple tanh network f(0,x) = 0\u2081 tanh(02x). The model output is invariant to a simultaneous sign flip of 01 and 02. This corresponds to a reflection symmetry whose projection matrix is the identity P = ((1,0), (0,1)). The symmetric solutions correspond to the trivial state where 0\u2081 = 02 = 0.\nTo be more general, we allow \u03b8' to be nonzero to generalize the theory and method to generic hyperplanes since the purpose of the method is to remove all reflection symmetries that may be hidden or difficult to enumerate. Let us first establish some basic properties of a loss function with P-reflection, to gain some intuition (again, proofs are in the appendix).\nProposition 1. Let f have the (0', P)-symmetry and Let f'(0) = f(0 +0\u2020). Then, (1) for any 0\u2020 such that P0\u2020 = 0', f(0 + 0\u2020) = f((I \u2013 2P)0 + 0\u2020), and (2) f' has the (0\u2032 \u2013 0\u2020,P) symmetry.\nTherefore, requiring Pe' = 0' reduces different manifestations of the symmetry to a unique one and simply shifting the function will not remove any symmetry. This proposition emphasizes the subtle difficulty in removing a symmetry."}, {"title": "3 Related Works", "content": "One closely related work is Ref. [11], which shows that every reflection symmetry in the model leads to a low-capacity solution that is energetically favored when weight decay is used. This is because the minimizer of the weight decay is coupled with stationary points of the reflection symmetries the projection of any parameter to a symmetric subspace always decreases the norm of the parameter, and is thus energetically preferred by weight decay. Our work develops a method to decouple symmetries and weight decay, thus"}, {"title": "4 Symmetry Impairs Model Capacity", "content": "We first show that reflection symmetry directly affects the model capacity. For simplicity, we let 0' = 0 for all symmetries. Let f(x,0) \u2208 R be a Taylor-expandable model that contains a P-reflection symmetry. Let \u0394 = 0 - 00. Then, close to any symmetric point 00 (any do for which P00 = 0), for all x, Ref. [11] showed that\n$$f(x,0) \u2212 f(x,00) = \\nabla_\\theta f(x,\\theta_0)(I \u2013 P)\\Delta + O(||PA||^2) + \\frac{1}{2}\\Delta^T P H(x) P \\Delta + O(||\\Delta||^3),$$\nwhere H(x) is the Hessian matrix of f. An important feature is that the symmetry subspace is a generic expansion where both odd and even terms are present, and the first order term does not vanish in general. In contrast, in the symmetry-broken subspace, all odd-order terms in the expansion vanish, and the leading order term is the second order. This implies that close to a symmetric solution, escaping from it will be slow, and if at the symmetric solution, it is impossible for gradient descent to leave it. The effect of this entrapment can be quantified by the following two propositions.\nProposition 2. (Symmetry removes feature.) Let f have the P-symmetry, and I be intialized at 00 such that P00 = 0. Then, the kernalized model, g(x,0) = $lim_{\\lambda \\to 0}(\\lambda^{-1} f(x, \\lambda \\theta + \\theta_0) \u2212 f(x,\\theta_0))$, converges to\n$$\\theta^* = A^+ \\sum_x \\nabla_\\theta f(x,\\theta_0)^T y(x)$$\nunder GD for a sufficiently small learning rate. Here $A := (I \u2212 P) \\sum_x \\nabla_{\\theta} f(x,\\theta_0)^T\\nabla_{\\theta} f(x,\\theta_0)(I \u2013 P)$ and $A^+$ denotes the Moore-Penrose inverse of A.\nThis means that in the kernel regime\u00b2, being at a symmetric solution implies that the feature kernel features are being masked by the projection matrix:\n$$\\nabla_\\theta f(x,\\theta_0) \\rightarrow (I \u2013 P)\\nabla_\\theta f(x,\\theta_0),$$\nand learning can only happen given these masks. This implies that the model is not using the full feature space that is available to it.\nProposition 3. (Symmetry reduces parameter dimension.) Let f have the P-symmetry, and 0 \u2208 Rd be intialized at 00 such that P00 = 0. Then, for all time steps t under GD or SGD, there exists a model f'(x,0') and sequence of parameters 04 such that for all x,\n$$f'(x,\\theta_t') = f(x,\\theta_t),$$\nwhere dim(0') = d \u2013 rank(P)."}, {"title": "5 Removing Symmetry with Static Bias", "content": "Next, we prove that a simple algorithm that involves almost no modification to any deep learning training pipeline can remove almost all such symmetries from the loss without creating new ones. From this section onward, we will consider the case where the function under consideration is the loss function (a per-batch loss or its expectation): f = l."}, {"title": "5.1 Countable Symmetries", "content": "We seek an algorithm that eliminates the reflection symmetries from the loss function l. We show that when the number of reflection symmetries in the loss function is finite, one can completely remove them using a simple technique. The symmetries are required to have the following property and the loss function is assumed to obey assumption 1.\nProperty 1. (Enumeratability) There exists a countable set of pairs of projection matrices and biases S = {(0, Pi)} such that l(0,x) has the e-centric Pi-reflection symmetry for all i. In addition, l does not have any (0\u2020, P) symmetry for (0, P) \u00a2 S.\nAssumption 1. There only exists countably many pairs (co,l) such that g(x) = l(0,x) \u2013 co0 contains a \u00d5-centric P symmetry, where we require Pco = co and P\u0113 = 0.\nThis assumption is satisfied by common neural networks with standard activations. The main purpose of this assumption is to rule out the pathological of a linear or quadratic deterministic objective, which never appears in practice or for which symmetry is not a concern. \u00b3\nFor symmetry removal, we propose to utilize the following alternative loss function. Let do be drawn from a Gaussian distribution with variance \u03c3\u03bf and l be the original loss function:\n$$lr(0,x) = l(0 + 0_0) + \u03b3||0||^2.$$\ny is nothing but the standard weight decay. We will see that using a static bias along with weight decay is essential for the method to work. We find that with unit probability, the loss function lr contains no reflection symmetry:\nTheorem 1. Let l satisfy Property 1 and Assumption 1. Then, with probability 1 (over the sampling of 00), there exists no projection matrix P and reflection point 0' such that l\u2081 has the (0', P)-symmetry.\nThe core mechanism of this theorem is decoupling of the solutions of the symmetries from the solutions of the weight decay. With weight decay, a solution with a small norm is favored, whereas with a random bias, the symmetric solutions are shifted by a small constant and no longer overlap with solutions that have"}, {"title": "5.2 Uncountably Many Symmetries", "content": "In deep learning, it is possible for the model to simultaneously contain infinitely many reflection symmetries. This happens, for example, when the model parameters have the rotation symmetry or the double rotation symmetry (common in self-supervised learning problems or transformers). It turns out that simply adding a static bias and weight decay is not sufficient to remove all symmetries in this case.\nWe propose to train on the following alternative loss instead, where ar stands for \"advanced removal\":\n$$l_{ar} (0) = l(0+0_0) + y||0||^2_3,$$\nwhere D is a positive diagonal matrix in which all diagonal elements of D are different. The simplest way to achieve such a D is to set Dii ~ Uniform(1 \u2013 \u20ac, 1 + \u20ac), where e is a small quantity.\nTheorem 2. Any (0\u2032, P)-symmetry that lar satisfies obeys: (1) P00 = 0' (2) and PD = DP.\nConditions (1) and (2) implies that there are at most finitely many (0', P)-symmetry lar can have. When there does not exist any symmetry that satisfies this condition, we have removed all the symmetries. In the worst case where l is a constant function, there are $2^N$ symmetries where N is the number of reflection symmetries. If we further assume that every P is associated with at most finitely many d', then we, again, remove all symmetries with probability 1. The easiest way to determine this D matrix is through sampling from a uniform distribution with a variance \u03c3\u03c1 < 1."}, {"title": "5.3 Strength of Symmetry Removal", "content": "While any level of \u03b8o and op are sufficient to remove the symmetries, one might want to quantify the degree to which the symmetries are broken. This is especially relevant when the model is located close to a symmetric solution and requires a large gradient to escape from it. Also, a related question that may arise in practice is how large one should choose \u03c3\u03bf and op, which are the variances of 00 and D. The following theorem gives a quantitative characterization of the degree of symmetry breaking.\nTheorem 3. Let the original loss satisfy a (0*, P)-symmetry, where 0* \u2208 Rd. Then, for any local minimum $0 \\in \\Theta(1)$, if \u03c3\u03c1 = \u03bf(\u03c3\u03bf) and P0 \u2260 0,\n$$\\Delta := \\frac{1}{||P\\theta||} [l_{ar}(\\theta+\\theta^*) -l_{ar}((I \u2013 2P)\\theta + \\theta^*)] = \\Omega(\\gamma \\sigma_0).$$\nThis theorem essentially shows a \"super-Lipschitz\" property of the difference between the loss function values between parameters and their reflections. This means that with a random bias, the symmetry will be quite strongly removed, as long as we ensure \u03c3\u03c1 \u00ab \u03c3\u03bf, which is certainly ensured when op = 0. As a corollary, it also shows that after applying a static bias, no symmetric solution where P0 = 0 can still be a stationary point because as P0 \u2192 0, the quantity A converges to the projection of the gradient onto symmetry breaking subspace.\nCorollary 1. For any 9 such that P0 = 0, P\u2207olar = \u03a9(\u03b3\u03c3\u03bf).\nNow, the more advanced question is the case when there are multiple reflection symmetries, and one wants to significantly remove every one of them."}, {"title": "5.4 Hyperparameter and Implementation Remark", "content": "As discussed, there are two ways to implement the method (Eq. (6) or Eq. (7)). In our experiments, we stick to the definition of Eq. (6), where the model parameters are biased, and weight decay is the same as the standard implementation. For the choice of hyperparameters, we always set op = 0 as we find only introducing \u03c3\u03bf to be sufficient for most tasks. Experiments with standard training settings (see the next section for the Resnet18 experiment on CIFAR-10) show that choosing go to be at least an order of magnitude smaller than the standard initialization scale (usually of order $1/\\sqrt{d}$ for a width of d) works the best. We thus recommend a default value of \u03c3\u03bf to be $0.01/\\sqrt{d}$, where $1/\\sqrt{d}$ is the common initialization variance. For the rest of the paper, we state go in relative units of $\\sqrt{d}^{-1}$ for this reason. That being said, we stress that \u03c3\u03bf is a hyperparameter worth tuning, as it directly controls the tradeoff between optimization and symmetry removal."}, {"title": "6 Experiment", "content": "First, we show that the proposed method is compatible with standard training methods. We then apply the method to a few settings where symmetry is known to be a major problem in training. We see that the algorithm leads to improved model performance on these problems."}, {"title": "6.1 Compatibility with Standard Training", "content": "Ridge linear regression. Let us first consider the classical problem of linear regression with d-dimensional data, where one wants to find minu \u2211i(wTxi \u2212 y\u2081)2. Here, the use of weight decay has a well-known effect of preventing the divergence of generalization loss at a critical dataset size N = d [25, 26]. This is due to the fact that the Hessian matrix of the loss becomes singular exactly at N = d (at infinite N and d). The use of weight decay shifts all the eigenvalues of the Hessian by y and removes this singularity. In this case, one can show that the proposed method is essentially identical to the simple ridge regression. The ridge solution is w* = E[\u03b3I + A]\u00af\u00b9E[xy], where A = E[xx], and the solution to the biased model is\n$$w^* = E[\\gamma I + A]^{-1}(E[xy] + \\gamma \\theta_0).$$\nThe difference is negligible with the original solution if either y and 00 are small."}, {"title": "6.2 Benchmarking Symmetry Removal", "content": "In this section, we benchmark the effect of symmetry control of the proposed method for two controlled experiments. To compare the influence of syre and other training methods on the degree of symmetry, we consider minimizing the following objective function\n$$(w^T w)^2 \u2013 w^T Bw := (w^T w)^2 - \\sum_i \\lambda_i (v_i^T w)^2,$$\nBenchmarking where w \u2208 Rd is the optimization parameter and B \u2208 Rdxd is a given symmetric matrix with eigenvalues \u5165\u00bf and eigenvectors vi (vivi = 1). The objective function has n reflection symmetries Piw := w \u2212 2(vw)vi. Hence, we define the degree of symmetry as \u2211=11{vw < Cth}, where Cth is a given threshold. Depending on the spectrum of B, the nature of the task is different. We thus consider two types of spectra: (1) an unstructured spectrum where B = G + GT for a Gaussian matrix G, and (2) a structured spectrum where B = diag(v) where v is a random Gaussian vector. Conceptually, the first type is more similar to rotation and double rotation symmetries in neural networks where the basis can be arbitrary, while the second is a good model for common discrete symmetries where the basis is often diagonal or sparse. For the first case we choose Cth = 10-3 and for the second case we choose Cth = 10-1.\nIn Figure 4, we compare syre, W-fix, drop out, weight decay, and the standard training methods in this setting for d = 1000 and two choices of B. In both cases, we use Gaussian initialization and gradient descent with a learning rate of 10-4. For syre and weight decay, we choose weight decay from 0.1 to 10. For W-fix, we choose from 0.001 to 0.1. For dropout, we choose a dropout rate from 0.01 to 0.6. Figure 4 shows that for both cases, syre is the only method that effectively and smoothly interpolates between solutions with low symmetry and best optimization. This is a strong piece of evidence that the proposed method can control the degree of symmetries in the model."}, {"title": "6.3 Feature and Neuron Collapses in Supervised Learning", "content": "See Figure 5, where we train the vanilla and syre four-layer networks with various levels of weight decay y and various levels of input-output covariance a. The dataset is constructed by rescaling the input by a factor of a for the MNIST dataset. The theory predicts that the syre model can remove the permutation symmetry in the hidden layer. This is supported by subfigures in Figure 5, where vanilla training results in a low-rank solution. Meanwhile, the accuracy of the low-rank solution is significantly lower for a large y or a small a, which corresponds to the so-called neural collapses. Also, we observe that syre shifts the eigenvalues of the representation by a magnitude proportional to \u03c3\u03bf, thus explaining the robustness of the method against collapses in the latent representation"}, {"title": "6.4 Posterior Collapse in Bayesian learning", "content": "Ref. [29] points out that a type of posterior collapse in Bayesian learning [30, 31] is caused by the low-rankness of the solutions. We show that training with syre could overcome this kind of posterior collapse. In Figure 6, we train a B-VAE [32, 33] on the Fashion MNIST dataset. Following Ref. [29], we use \u1e9e to weigh the KL loss, which can be regarded as the strength of prior matching. Both the encoder and the decoder are a two-layer network with SiLU activation. The hidden dimension and the latent dimension are 200. Only the encoder has weight decay because the low-rank problem is caused by the encoder rather than the decoder. We also choose the prior variance of the latent variable to be nenc = 0.01. Other settings are the same as [29]. Posterior collapse happens at \u1e9e = 10, signalized by a large reconstruction loss in the right side of Figure 6. However, the reconstruction loss decreases, and the rank of the encoder output increases (according to the left side of Figure 6) after we use weight decay and syre. This is further verified by the generated image in Figure 7. Therefore, we successfully remove the permutation symmetry of the encoder."}, {"title": "6.5 Low-Capacity Trap in Self-supervised Learning", "content": "A common but bizarre practice in self-supervised learning (SSL) is to throw away the last layer of the trained model and use the penultimate learning representation, which is found to have much better expressiveness than the last layer representation. From the perspective of symmetry, this problem is caused by the rotation symmetry of the last weight matrix in the SimCLR loss [8]. We train a Resnet-18 together with a two-layer projection head over the CIFAR-100 dataset according to the setting for training SimCLR in [34]. Then, a linear classifier is trained using the learned representations. Our implementation reproduces the typical accuracy of SimCLR over the CIFAR-100 dataset [35]. As in [34], the hidden layer before the projection head is found to be a better representation than the layer after. Therefore, we apply our syre method to the projection head or to all layers. According to Table 1, syre removes the low-rankness of the learned features and increases the accuracy trained with the features after projection while not changing the representation before projection. Thus, symmetry-induced reduction in model capacity can explain about 50% of the performance difference between the representation of the two layers. Also, an interesting observation is that just improving the expressivity of the last layer is insufficient to close the gap between the performance of the last layer and the penultimate layer. This helps us gain a new insight: symmetry is not the only reason why the last layer representation is defective."}, {"title": "6.6 Loss of Plasticity in Continual Learning", "content": "A form of low-capacity collapse also happens during continual learning, i.e., the plasticity of the network gradually decreases as the model is trained on more and more tasks. This problem is common in both supervised and reinforcement learning settings and may also be relevant to the finetuning of large language models [40, 41, 42, 43].\nIn Figure 8, we train a CNN with two convolution layers (10 channels and 20 channels) and two fully connected layers (320 units and 50 units) over the MNIST datasets. For the data, we randomly permute the pixels of the training and test sets for 9 times, forming 10 different tasks (including the original MNIST). We then train a vanilla CNN and a syre CNN over the 10 tasks continually with SGD and weight decay 0.01. The inset of Figure 8 shows that the rank of the original model gradually decreases, but the syre model remains close to full rank. Correspondingly, in the right side of Figure 8, the accuracy over the test set drops while the rank of the original model collapses, but the accuracy of the syre model remains similar."}, {"title": "7 Conclusion", "content": "Symmetry-induced neural-network training problems exist extensively in machine learning. We have shown that the existence of symmetries in the model or loss function may severely limit the expressivity of the trained model. We then developed a theory that leverages the power of representation theory to show that adding random static biases to the model, along with weight decay, is sufficient to remove almost all symmetries, explicit or hidden. We have demonstrated the relevance of the method to a broad range of applications in deep learning, and a possible future direction is to deploy the method in large language models, which naturally contain many symmetries. Lastly, it is worth noting that on its own, symmetry is neither good nor bad. For example, practitioners may be interested in introducing symmetries to the model architecture in order to control the capacity of the model [28]. However, with too much symmetry, the training of models becomes slow and likely to contain many low-capacity traps. Meanwhile, a model completely without symmetry may have undesirably high capacity and be more prone to overfitting. Having the right degree of symmetry might thus be crucial for achieving both smooth optimization and good generalization. With our proposed method, it becomes increasingly possible to deliberately fine-grain engineer symmetries in the loss function, introducing desired symmetries and removing undesirable ones."}, {"title": "B.1 Teacher-student Scenario", "content": "This section gives some additional details and additional experiments in the teacher-student scenario in Figure 2. Specifically, we implement a two-layer network with tanh activation, 300 hidden units, and different input units. The network outputs a ten-dimensional vector corresponding to ten different classes. We then randomly generate such a network as the teacher model, 10000 standard Gaussian samples as the training set, and 1000 standard Gaussian samples. For both the syre and the vanilla model, we choose the Adam optimizer, learning rate 0.01, and weight decay 0.01.\nAs additional experiments, we also measure the influence of noisy labels and noisy input on the rank of the model in Figure 10. For the label noise, we randomly change 0% to 80% of the labels, and for the input noise, we add a Gaussian noise to the input with standard deviation 0 to 1.6. Figure 10 suggests that the rank of the vanilla model decreases in the face of noisy labels and increases in the face of noisy input, perhaps because the latter can be regarded as data augmentation. The syre model, however, is not affected."}, {"title": "B.2 Supervised Learning", "content": "This section presents some additional experiments for Section 6.3. Figure 11 gives the eigenvalue distribution of the networks in Fig.5, which further supports the claim that the vanilla network leads to a low-rank solution. In all the experiments above and in Section 6.3, we use a four-layer FCN with 300 neurons in each layer trained on the MNIST dataset with batch size 64."}, {"title": "A. Theoretical Concerns", "content": "A.1 Proof of Proposition 1\nProof. Part (1). Note that we have\n$$\\theta^{\\dagger} = P\\theta^{\\dagger} + (I \u2013 P)\\theta^{\\dagger} = \\theta' + (I \u2013 P)\\theta^{\\dagger}.$$\nThus,\n$$(I \u2013 2P)(I \u2013 P)\\theta^{\\dagger} = (I \u2013 P)\\theta^{\\dagger}.$$\nTherefore, we have\n$$f(\\theta + \\theta^{\\dagger}) = f((\\theta + (I \u2013 P)\\theta^{\\dagger}) + \\theta') = f((I \u2212 2P)(\\theta + (I \u2013 P)\\theta^{\\dagger}) + \\theta') = f((I \u2013 2P)\\theta + \\theta^{\\dagger}).$$\nThis proves part (1).\nPart (2). By definition,\n$$f'(\\theta \u2013 \\theta^{\\dagger} + \\theta') = f(\\theta + \\theta')$$\n$$= f((I \u22122P)\\theta + \\theta')$$\n$$= f'((I \u2013 2P)\\theta \u2013 \\theta^{\\dagger} + \\theta').$$\nThis completes the proof.\nA.2 Proof of Proposition 2\nProof. By (2), g(x, 0) simplifies to a kernel model\n$$g(x, \\theta) = \\nabla_{\\theta_0}f(x,\\theta_0)(I \u2013 P)\\theta.$$\nLet us consider the squared loss $l(\\theta) = \\sum_x ||y(x)-g(x,\\theta)||^2$ and denote $A := \\sum_x (I-P)\\nabla_{\\theta_0}f(x,\\theta_0)^T\\nabla_{\\theta_0}f(x,\\theta_0)(I\u2212 P)$, $b := (I \u2013 P) \\sum_x \\nabla_{\\theta_0}f(x,\\theta_0)^Ty(x)$. Assuming the learning rate to be \u03b7, the GD reads\n$$\\theta^{t+1} = \\theta^t \u2013 2\\eta(A\\theta^t \u2013 b),$$\nwhere 0\u00b0 = 0. If\n$$\\eta < \\frac{1}{2\\lambda_{max} (A)},$$\nGD converges to\n$$\\theta^* = \\lim_{t\\rightarrow\\infty} \\sum_{k=0}^{t}(I \u2013 2\\eta A)^k  2\\eta b$$\n$$= A^+b,$$\nwhich is the well-known least square solution.\nA.3 Proof of Proposition 3\nProof. According to Ref. [11, Theorem 4], we have\n$$P\\nabla \\theta_0 f(x,\\theta_0) = 0.$$\nTherefore, after one step of GD or SGD, we still have P0\u2081 = 0. By induction, we have P0t = 0.\nFinally, suppose that ${a_i}_{i=1}^{d-rank(P)}$ forms a basis of kerP, and define $f'(x, \\theta') := f(x, \\sum_{i=1}^{d-rank(P)} \\theta'_i a_i)$ for dim(0') = d - rank(P). By choosing $\\theta'_t = \\theta^T a_i$, we have $f'(x,\\theta'_t) = f(x,\\theta_t)$."}, {"title": "A.4 Lemmas", "content": "Lemma 1. Let x \u2208 Rd and P be a projection matrix. Let f(x) be a scalar function that satisfies\n$$f(x + x') = f((I \u2013 2P)x + x') + c^T Px,$$\nwhere c is a constant vector. Then, there exists a unique function g(x) such that\n1. g(x) has the x'-centric P-symmetry,\n2. and f(x) = g(x) + cT Px.\nProof. (a) Existence. f(x) = g(x) + fx. Let us suppose g(x) is not x'-centric P-symmetry. Then, there exists x such that\n$$g(x + x') \u2013 g((I \u2212 2P)x + x') = \\Delta \u2260 0.$$\nThen, by definition, we have that\n$$c^T Px = f(x + x') + f((I \u2212 2P)x + x')$$\n$$= g(x + x') - \\frac{1}{2} c^T P(x+x') - g((I - 2P)x + x') - \\frac{1}{2} c^T P((I - 2P)x + x')$$\n$$= \\Delta + \\frac{1}{2} c^T P(x+x') - \\frac{1}{2} c^T P((I - 2P)x + x')$$\n$$= \\Delta + c^T Px.$$\nThis is a contradiction. Therefore, there must exist g(x) that satisfies the lemma statement.\n(b) Uniqueness. Simply note that the expression of g is uniquely given by\n$$g(x) = f(x + x') \u2212 f((I \u2212 2P)x + x').$$\nA.5 Proof of Theorem 1\nProof. We prove by contradiction. Let us suppose there exists such pair, (\u03b8', P). By definition, we have that\n$$l_\\lambda(0 + \\theta') = l(0 + \\theta' + \\theta_0) + \\gamma ||0 + \\theta' ||^2.$$\nBy assumption, we have that\n$$l((I \u2013 2P)0 + \\theta' + \\theta_0) + \\gamma ||(I \u2013 2P)0 + \\theta' ||^2 = l(0 + \\theta' + \\theta_0) + \\gamma ||0 + \\theta' ||^2,$$\nand, so, for all 0,\n$$l((I \u22122P)0 + \u03b8' + \u03b8\u03bf) = l(0 + \u03b8' + \u03b8\u03bf) + 4y0T P\u03b8'.$$\nThere are two cases: (1) P0\u2032 = 0 and (2) P\u03b8' \u2260 0.\nFor case (1), we have that l((I \u2013 2P)0 + \u03b8' + \u03b8\u03bf) = l(0 + \u03b8' + \u03b80), but this can only happen if the original loss l has the (\u03b8' + 00)-centric P-symmetry. By Property 1, this implies that\n$$\u03b8' + \u03b8\u03bf = 0_i$$\nfor some i. Applying P on both sides, we obtain that\n$$P\\theta_0 = P\\theta.$$\nBut, do is a random variable with a full-rank covariance while the set ${P}_i$ has measure zero in the real space, and so this equality holds with probability zero."}, {"title": "A.6 Proof of Theorem 2", "content": "Proof. We prove by contradiction. Let us suppose there exists such pair, (\u03b8', P). By definition, we have that\n$$l_{ar} (0 +\\theta') = l(0 + \\theta' + \\theta_0) + \\gamma ||0 + \\theta'||^2_3.$$\nBy assumption, we have that\n$$l((I \u2013 2P)0 + \\theta' + \\theta_0) + \\gamma ||(I \u2013 2P)0 + \\"}]}