{"title": "Remove Symmetries to Control Model Expressivity", "authors": ["Liu Ziyin", "Yizhou Xu", "Isaac Chuang"], "abstract": "When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity\nstate that is sometimes known as a \"collapse.\" Being trapped in these low-capacity states can be a major\nobstacle to training across many scenarios where deep learning technology is applied. We first prove two\nconcrete mechanisms through which symmetries lead to reduced capacities and ignored features during\ntraining. We then propose a simple and theoretically justified algorithm, syre, to remove almost all\nsymmetry-induced low-capacity states in neural networks. The proposed method is shown to improve\nthe training of neural networks in scenarios when this type of entrapment is especially a concern. A\nremarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge\nof the symmetry.", "sections": [{"title": "Introduction", "content": "The unprecedented scale and complexity of modern neural networks, which incorporate a vast number of\nneurons and connections, inherently introduce a high degree of redundancy in model parameters. This com-\nplexity in the architecture and the design of loss functions often implies that the loss functions are invariant\nto various hidden, nonlinear, and nonlocal transformations of the model parameters. These invariant trans-\nformations, or \"symmetries,\" in the loss function have been extensively documented in the literature, with\ncommon examples including permutation, rescaling, scale, and rotation symmetries [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].\nA unifying perspective to understand how these symmetries affect learning is the framework of reflection\nsymmetries [11]. A per-sample loss function l possesses a P-reflection symmetry if for all 0, and all data\npoints x,\n$l((I \u2013 2P)0, x) = l(0,x).$\nThe solutions where 0 = (I \u2013 P)0 are the symmetric solutions with respect to P. It has been shown that\nalmost all contemporary models contain quite a few reflection symmetries. Permutation, rescaling, scale,\nand rotation symmetries all imply the existence of one or multiple reflection symmetries in the loss.\nRecent literature has shown that symmetries in the loss function of neural networks often lead to the\nformation of low-capacity saddle points within the loss landscape [12, 13, 14]. These saddle points are located\nat the symmetric solutions and often possess a lower capacity than the minimizers of the loss [11]. When\na model encounters these saddle points during training, the model parameters are not only slow to escape\nthem but also attracted to these solutions because these the gradient noise also vanish close to these saddles\n[14, 15]. Essentially, the model's learning process stagnates, and it fails to achieve optimal performance due\nto reduced capacity. However, while many works have characterized the dynamical properties of training\nalgorithms close to symmetric solutions, no methods are known to enable full escape from them."}, {"title": "Discrete Symmetries in Neural Networks", "content": "Notation. For a matrix A, we use A+ to denote the pseudo-inverse of A. For groups U and G, U <G\ndenotes that U is a subgroup of G. For a vector w and matrix D, $||0||3 := wTDw$ is the norm of w with\nrespect to D. $\\odot$ denotes the element-wise product between vectors.\nLet f(0,x) be a function of the model parameters 0 and input data point x. For example, f could either\nbe a sample-wise loss function l or the model itself. Whenever f satisfies the following condition, we say\nthat f has the P-reflection symmetry (general symmetry groups are dealt with in Theorem 5 in Section 5).\nDefinition 1. Let P be a projection matrix and O' be a point. f is said to have the (0', P)-reflection\nsymmetry if for all x and 0, (1) f(0 + 0',x) = f((I \u2013 2P)0 +0',x), and (2) P0\u2032 = 0'.\nThe second condition is due to the fact that there is a redundancy in the choice of \u03b8' when \u03b8' \u2260 0.\nRequiring Pe' = 0' removes this redundancy and makes the choice of e' unique. Since every projection\nmatrix can be written as a product of a (full-rank or low-rank) matrix O with orthonormal columns, one\ncan write P = $OOT$ and refer to this symmetry as an O symmetry. In common deep learning scenarios, it\nis almost always the case that 0\u2032 = 0 (for example, this holds for the common cases of rescaling symmetries,\n(double) rotation symmetries, and permutation symmetries, see Theorem 2-4 of Ref. [11]). A consequence\nof 0' = 0 is that the symmetric projection Pe of any e always has a smaller norm than 0: thus, a symmetric\nsolution is coupled to the solutions of weight decay, which also favors small-norm solutions. As an example\nof reflection symmetry, consider a simple tanh network f(0,x) = 0\u2081 tanh(02x). The model output is invariant\nto a simultaneous sign flip of 01 and 02. This corresponds to a reflection symmetry whose projection matrix\nis the identity P = ((1,0), (0,1)). The symmetric solutions correspond to the trivial state where 0\u2081 = 02 = 0.\nTo be more general, we allow \u03b8' to be nonzero to generalize the theory and method to generic hyperplanes\nsince the purpose of the method is to remove all reflection symmetries that may be hidden or difficult to\nenumerate. Let us first establish some basic properties of a loss function with P-reflection, to gain some\nintuition (again, proofs are in the appendix).\nProposition 1. Let f have the (0', P)-symmetry and Let f'(0) = f(0 +0\u2020). Then, (1) for any 0\u2020 such that\nP0\u2020 = 0', f(0 + 0\u2020) = f((I \u2013 2P)0 + 0\u2020), and (2) f' has the (0\u2032 \u2013 0\u2020,P) symmetry.\nTherefore, requiring Pe' = 0' reduces different manifestations of the symmetry to a unique one and\nsimply shifting the function will not remove any symmetry. This proposition emphasizes the subtle difficulty\nin removing a symmetry."}, {"title": "Related Works", "content": "One closely related work is Ref. [11], which shows that every reflection symmetry in the model leads to a\nlow-capacity solution that is energetically favored when weight decay is used. This is because the minimizer\nof the weight decay is coupled with stationary points of the reflection symmetries the projection of any\nparameter to a symmetric subspace always decreases the norm of the parameter, and is thus energetically\npreferred by weight decay. Our work develops a method to decouple symmetries and weight decay, thus"}, {"title": "Symmetry Impairs Model Capacity", "content": "We first show that reflection symmetry directly affects the model capacity. For simplicity, we let 0' = 0 for\nall symmetries. Let f(x,0) \u2208 R be a Taylor-expandable model that contains a P-reflection symmetry. Let\n\u0394 = 0 - 00. Then, close to any symmetric point 00 (any do for which P00 = 0), for all x, Ref. [11] showed that\n$f(x,0) \u2212 f(x,00) = \\sqrtof(x,0%)(I \u2013 P)\u2206 + O(||PA||)\u00b2 + \u2206\u03a1\u0397(x)PA+ \u039f(||4||3),$\nwhere H(x) is the Hessian matrix of f. An important feature is that the symmetry subspace is a generic\nexpansion where both odd and even terms are present, and the first order term does not vanish in general.\nIn contrast, in the symmetry-broken subspace, all odd-order terms in the expansion vanish, and the leading\norder term is the second order. This implies that close to a symmetric solution, escaping from it will be\nslow, and if at the symmetric solution, it is impossible for gradient descent to leave it. The effect of this\nentrapment can be quantified by the following two propositions.\nProposition 2. (Symmetry removes feature.) Let f have the P-symmetry, and I be intialized at 00 such\nthat P00 = 0. Then, the kernalized model, g(x,0) = limx\u21920(1\u00af\u00b9 f(x, 10 + 0\u2030) \u2212 f(x,00)), converges to\n$0* = A+ \u2211\\sqrtof(x,00)y(x)$\nunder GD for a sufficiently small learning rate. Here A := (I \u2212 P) $\\sumx \u2207Vof(x,00)TVof(x,00)(I \u2013 P)$ and A+\ndenotes the Moore-Penrose inverse of A.\nThis means that in the kernel regime\u00b2, being at a symmet-\nric solution implies that the feature kernel features are being\nmasked by the projection matrix:\n$Vof(x,00) \u2192 (I \u2013 P)\u2207of(x,\u03b8\u03bf),$\nand learning can only happen given these masks. This im-\nplies that the model is not using the full feature space that is\navailable to it.\nProposition 3. (Symmetry reduces parameter dimension.)\nLet f have the P-symmetry, and 0 \u2208 Rd be intialized at 00\nsuch that P00 = 0. Then, for all time steps t under GD or\nSGD, there exists a model f'(x,0') and sequence of parameters\n04 such that for all x,\nf'(x,04) = f(x,t),\nwhere dim(0') = d \u2013 rank(P)."}, {"title": "Removing Symmetry with Static Bias", "content": "Next, we prove that a simple algorithm that involves almost no modification to any deep learning training\npipeline can remove almost all such symmetries from the loss without creating new ones. From this section\nonward, we will consider the case where the function under consideration is the loss function (a per-batch\nloss or its expectation): f = l."}, {"title": "Countable Symmetries", "content": "We seek an algorithm that eliminates the reflection symmetries from the loss function l. We show that when\nthe number of reflection symmetries in the loss function is finite, one can completely remove them using\na simple technique. The symmetries are required to have the following property and the loss function is\nassumed to obey assumption 1.\nProperty 1. (Enumeratability) There exists a countable set of pairs of projection matrices and biases\nS = {(0, Pi)} such that l(0,x) has the e-centric Pi-reflection symmetry for all i. In addition, l does not\nhave any (0\u2020, P) symmetry for (0, P) \u00a2 S.\nAssumption 1. There only exists countably many pairs (co,l) such that g(x) = l(0,x) \u2013 co0 contains a\n\u00d5-centric P symmetry, where we require Pco = co and P\u0113 = 0.\nThis assumption is satisfied by common neural networks with standard activations. The main purpose of\nthis assumption is to rule out the pathological of a linear or quadratic deterministic objective, which never\nappears in practice or for which symmetry is not a concern. 3\nFor symmetry removal, we propose to utilize the following alternative loss function. Let do be drawn\nfrom a Gaussian distribution with variance \u03c3\u03bf and l be the original loss function:\nlr(0,x) = l(0 + 00) + \u03b3||0||2.\ny is nothing but the standard weight decay. We will see that using a static bias along with weight decay\nis essential for the method to work. We find that with unit probability, the loss function lr contains no\nreflection symmetry:\nTheorem 1. Let l satisfy Property 1 and Assumption 1. Then, with probability 1 (over the sampling of 00),\nthere exists no projection matrix P and reflection point 0' such that l\u2081 has the (0', P)-symmetry.\nThe core mechanism of this theorem is decoupling of the solutions of the symmetries from the solutions\nof the weight decay. With weight decay, a solution with a small norm is favored, whereas with a random\nbias, the symmetric solutions are shifted by a small constant and no longer overlap with solutions that have"}, {"title": "Uncountably Many Symmetries", "content": "In deep learning, it is possible for the model to simultaneously contain infinitely many reflection symmetries.\nThis happens, for example, when the model parameters have the rotation symmetry or the double rotation\nsymmetry (common in self-supervised learning problems or transformers). It turns out that simply adding\na static bias and weight decay is not sufficient to remove all symmetries in this case.\nWe propose to train on the following alternative loss instead, where ar stands for \"advanced removal\":\n$lar (0) = l(0+00) + y||0||3,$\nwhere D is a positive diagonal matrix in which all diagonal elements of D are different. The simplest way\nto achieve such a D is to set Dii ~ Uniform(1 \u2013 \u20ac, 1 + \u20ac), where e is a small quantity.\nTheorem 2. Any (0\u2032, P)-symmetry that lar satisfies obeys: (1) P00 = 0' (2) and PD = DP.\nConditions (1) and (2) implies that there are at most finitely many (0', P)-symmetry lar can have. When\nthere does not exist any symmetry that satisfies this condition, we have removed all the symmetries. In\nthe worst case where l is a constant function, there are 2N symmetries where N is the number of reflection\nsymmetries. If we further assume that every P is associated with at most finitely many d', then we, again,\nremove all symmetries with probability 1. The easiest way to determine this D matrix is through sampling\nfrom a uniform distribution with a variance \u03c3\u03c1 < 1."}, {"title": "Strength of Symmetry Removal", "content": "While any level of \u03b8o and op are sufficient to remove the symmetries, one might want to quantify the\ndegree to which the symmetries are broken. This is especially relevant when the model is located close to a\nsymmetric solution and requires a large gradient to escape from it. Also, a related question that may arise\nin practice is how large one should choose \u03c3\u03bf and op, which are the variances of 00 and D. The following\ntheorem gives a quantitative characterization of the degree of symmetry breaking.4\nTheorem 3. Let the original loss satisfy a (0*, P)-symmetry, where 0* \u2208 Rd. Then, for any local minimum\n\u03b8\u03b5 \u0398(1), if \u03c3\u03c1 = \u03bf(\u03c3\u03bf) and P0 \u2260 0,\n$:= \\frac{1}{|P0|} [lar(0+0*) -lar((I \u2013 2P)0 + 0*)] = \u03a9(\u03b3\u03c3\u03bf).$\nThis theorem essentially shows a \"super-Lipschitz\" property of the difference between the loss function\nvalues between parameters and their reflections. This means that with a random bias, the symmetry will\nbe quite strongly removed, as long as we ensure \u03c3\u03c1 \u00ab \u03c3\u03bf, which is certainly ensured when op = 0. As\na corollary, it also shows that after applying a static bias, no symmetric solution where P0 = 0 can still\nbe a stationary point because as P0 \u2192 0, the quantity A converges to the projection of the gradient onto\nsymmetry breaking subspace.\nCorollary 1. For any 9 such that P0 = 0, PVolar = \u03a9(\u03b3\u03c3\u03bf).\nNow, the more advanced question is the case when there are multiple reflection symmetries, and one\nwants to significantly remove every one of them."}, {"title": "Theorem 4.", "content": "Let l contain N reflection symmetries: {(Pi,0)}=1. Let\n$lar (0+0)-lar((I \u2013 2Pi)0+0)$\n$Pi0||$\n$\u0394 := $\nThen, for any local minimum \u0472 \u0454 \u0472(1), letting \u03b3\u03c3\u03bf = \u03a9 (2) guarantees that Pr(mini |\u2206\u2081| > \u0454) > d for any \u0454\nand 8 < 1.\nIn the theorem, the probability is taken over the random sampling of the static bias. Namely, the\nachievable strengths of symmetry-breaking scales inversely linearly in N, the size of the minimal set of the\nentire group generated by N reflections. In general, without further assumptions there is no way to improve\nthis scaling because, for example, the smallest of N independent bounded variables roughly scales as 1/N\ntowards its lower boundary."}, {"title": "General Groups.", "content": "Lastly, one can generalize the theory to prove that the proposed method removes\nsymmetries from a generic group. Let G be the linear representation of a generic finite group, possibly with\nmany nontrivial subgroups. If the loss function l is invariant under transformation by the group G, then\n\u2200g, l(0) = l(g0).\nBecause G is finite, it follows that the representations g must be full-rank and unipotent.\nThe following theorem shows that at every symmetric solution, there exists an escape direction with a\nstrong gradient that pulls the parameters from every subgroup of the related symmetry. In other words, it\nis no longer possible to be trapped in a symmetric solution. While general groups appear less common in\ndee"}]}