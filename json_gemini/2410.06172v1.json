{"title": "MULTIMODAL SITUATIONAL SAFETY", "authors": ["Kaiwen Zhou", "Chengzhi Liu", "Xuandong Zhao", "Anderson Compalas", "Dawn Song", "Xin Eric Wang"], "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely\u2014whether through language or action\u2014it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Multimodal Large Language Models (MLLMs) (Zhu et al., 2023; Li et al., 2023; Liu et al., 2023a; OpenAI, 2023c; Reid et al., 2024) can understand visual contexts, follow instructions, and generate language responses, enabling them to serve as multimodal assistants capable of interacting with humans and real-world environments (Zheng et al., 2022; Driess et al., 2023). With the enhanced capabilities and diverse application scenarios, the safety of MLLMs has become more critical, and there have been various works assessing and improving the safety of MLLMs (Liu et al., 2023c; Gong et al., 2023; Shayegani et al., 2023; Qi et al., 2024; Luo et al., 2024)."}, {"title": "2 RELATED WORK", "content": "MLLMs for Multimodal Assistants. Recently, the development of multimodal large language models (MLLMs) has been driven by enabling LLMs with visual perception abilities (Alayrac et al., 2022; Dai et al., 2023; Liu et al., 2023a; Reid et al., 2024). These models are applied widely in various vision and language tasks. The success of the two tasks makes them very helpful chat and"}, {"title": "3 MULTIMODAL SITUATIONAL SAFETY", "content": null}, {"title": "3.1 DATASET OVERVIEW", "content": "Problem Definition. We define the problem of multimodal situational safety as follows: Given a language query Q and a real-time visual context V, the model needs to determine a safety score, denoted as S(Q, V), which represents the safety of the intent of this query Q in the context of the visual information V. Specifically, the safety score S(Q) depends on the visual context, meaning that it should be difficult to determine S(Q) without the visual input.\nDataset Description. We introduce the Multimodal Situational Safety benchmark (MSSBench) to evaluate the model's ability to judge the safety of answering a language query based on a situation given by a visual context. As shown in Fig. 3, each data instance contains a language query and a safe or unsafe visual context as the real-time observation of the MLLM. Our benchmark contains two different multimodal assistant scenarios: chat assistant and embodied assistant. For chat assistant, the language query indicates the intent to perform a certain activity. For embodied assistant, each language query is a household task instruction, and the images depict safe and unsafe situations in which to perform the task."}, {"title": "Multimodal Situational Safety Category.", "content": "As shown in Fig. 2, we develop a multimodal situational safety categorization system based on the potential unsafe outcomes by answering the query. We find that many safety categories used in former LLM safety assessments (Shen et al., 2023; Li et al., 2024a) do not often apply to Multimodal Situational Safety, such as fraud, political lobbying, etc. Therefore, our categorization covers four core domains where the safety of the intent of the query is frequently conditioned on the visual context: (1) Physical Harm, including activities that in certain situations may cause bodily harm, subdivided into self-harm (such as eating disorders and danger activities) and other-harm (activities that could potentially harm others). (2) Property damage, defined as activities that cause harm to personal or public property, is categorized into personal property damage and public property damage. (3) Illegal Activities, encompassing behaviors that violate the law but do not directly cause physical harm or property damage, divided"}, {"title": "3.2 CHAT DATA COLLECTION", "content": "We design a data collection pipeline to collect queries that are safe to answer in certain situations but are unsafe to answer in others. This pipeline involves four steps: (1) generating user intented activities and textual unsafe situations corresponding to situational safety categories; (2) filtering out situations that do not meet the criteria; (3) retrieving images that depict the unsafe context to construct multimodal situations; and (4) generating user queries with the aforementioned intents after human verification. We use GPT-40 as the large language model (LLM) in the data generation pipeline to ensure the efficient generation and processing of these situation pairs.\nGeneration of Intend Activity and Textual Unsafe Situations. Initially, we randomly select 5,000 images  I = {i\u2081, ..., in } from the COCO dataset (Lin et al., 2014) for each situational safety category, considering them as safe images. We prompt the LLM to generate intented activities Asafe that are safe to perform in the context of the images. These activities, along with the corresponding images and safety category descriptions, are input into the LLM to generate unsafe situations Tunsafe where performing the activity can lead to unsafe outcomes. For example, in the domain of property damage, if the image I\u2081 depicts \u201cPeople playing baseball on the field,\u201d a possible safe activity a\u2082 is \"Swinging a baseball bat to hit the ball\" while a possible unsafe situation t\u2081 is \u201cInside a store.\"\nAutomatic Filtering with LLM. We implement two automated filters using GPT-40 to address the issue of the LLM generating unsafe situations that deviate from the intended safety category or involve impossible activities. The first filter eliminates situations that do not meet the safe and unsafe criteria of the designated safety category. For instance, if the category is offensive behavior, scenarios such as \"practicing skateboarding in the middle of a road\" are filtered out as they do not fit the category. The second filter eliminates impossible intented activities, which means that the activity contradicts the situation, such as \"obeying traffic lights\" in an image of \"driving on a highway\" because highways typically do not have traffic lights. After filtering, we obtain a set of textual intented activities and unsafe situations: (Afilter, Tfilter) = ({a1,...,a\u2081}, {t1,..., t\u2081}), where L is the number of instances after filtration.\nConstruction of Multimodal Situational Safety Dataset through Image Retrieval. We construct a Multimodal Situation Safety Dataset D = {S,U}, where S contains pairs of activities a and their corresponding safe images i. Conversely, U = {(t1, 11), ..., (tz, \u0130L)} includes pairs where t"}, {"title": "3.3 \u0415\u043c\u0432ODIED DATA COLLECTION", "content": "The collection of the embodied data consists of two steps:\nEmbodied task and instruction construction. We mainly consider three task categories: place an {object in hand} on a {receptacle} (Place), toggle a {receptacle} (Toggle), and drop an object in hand} (Drop). For each category, we can define different safe and unsafe tasks by changing the objects or receptacles in the placeholder. The environment state that determines whether the task is safe or unsafe in the Drop tasks is the object in the robot's hand. In the Place task, the environment state is the combination of the object in the robot's hand and the receptacle. In the Toggle task, the environment state is the objects in the receptacle to be toggled. In total, we define 31 safe tasks and 31 unsafe tasks. Then, we create 5 instruction templates for each task. In total, we have 5 \u00d7 (31 +31) = 310 embodied instructions.\nEmbodied situations collection. After we determine the {object}, {receptacle} in the task, we run a \u201cPick_{object}and_Place{receptacle}\" task defined in Shridhar et al. (2020) with the determined {object} and {receptacle}. For the Place task and the Drop task, we randomly collect two egocentric images after the agent picks up the object and before the agent places the object. For the Toggle task, we collect an egocentric image right after the agent places the object on the receptacle from two different episodes. Therefore, we have 620 samples in total. One data example is shown in Fig. 3 (right)."}, {"title": "3.4 DATA STATISTICS", "content": "The Multimodal Situational Safety benchmark consists of a substantial collection of 1820 Image-Query pairs, encompassing two subsets: the embodied assistant subset, which contains 620 pairs sourced from household scenarios, and the chat assistant subset, comprising a larger set of 1200 pairs designed for broader situational QA scenarios. Our dataset is a balance dataset, with half of the data containing safe situations and half containing unsafe situations. The statistical details of the data in the MSSBench are presented in Table. 1."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 SETUP", "content": "MLLMs. The MLLMs we benchmark include both open-source models and proprietary models accessible only via API. The open-source MLLMs are: (i) LLaVA-1.6 (Liu et al., 2023b), (ii) MiniGPT-v2 (Chen et al., 2023), (iii) Qwen-VL (Bai et al., 2023), (iv) DeepSeek (Lu et al., 2024) and (v) mPLUG-Owl2 (Ye et al., 2024). We implemented these models with their 7B version and using their default settings. For the proprietary models, we evaluated Claude 3.5 Sonnet, GPT-40 (OpenAI, 2023b), and Gemini Pro-1.5 (Reid et al., 2024).\nEvaluation. For the instruction following setting, we use GPT-40 (OpenAI, 2023a) to categorize the response generated by MLLMs into safe and unsafe categories. The categories description is introduced in Tables. 4 and 5 in Sec. A.3. Recent studies, including Hsu et al. (2023); Hackl et al. (2023); Wang et al. (2024b) have underscored GPT-4's effectiveness and reliability in evaluative roles, including safety classification. After categorization, we use accuracy to evaluate MLLM's safety performance."}, {"title": "4.2 MAIN RESULTS", "content": "To begin with, we assess the performance of 8 leading multimodal large language models (MLLMs) on our MSS benchmark, the results are shown in Table. 2. To mimic the chat assistant scenario, we inform the MLLM that the image is its first-person view and the query is from a user staying with it, see the 'Common Prompt' in Fig. 4c. The full prompt can be found in Sec. A.6. First, a common trend among all the MLLMs is that they tend to comply with and answer users' queries in both safe and unsafe scenarios. This leads to a high safety accuracy when the situation is safe for the user's intent and a low accuracy when the situation is unsafe. Second, comparing open-source models and proprietary models, we find that proprietary models perform better in unsafe scenarios, with a higher frequency of detecting the unsafe intent from the user's query under the current situation, and pointing out the unsafe outcomes or rejecting to answer. Meanwhile, proprietary MLLMs are not over-sensitive in safe situations; therefore, they obtain higher average safety accuracy than open-source MLLMs. Third, by comparing the performance on Chat and Embodied scenarios, we find that MLLMs all perform worse on Embodied scenarios, especially in recognizing unsafe situations.\nLastly, the best-performed model, Claude 3.5 Sonnet, only scores an average accuracy of 62.2%, indicating the situation safety awareness of current MLLMs needs to be improved."}, {"title": "4.3 RESULT DIAGNOSIS", "content": "We propose three hypothesis reasons that led to MLLM's poor performance on the MSS benchmark: (1) lack of explicit safety reasoning, (2) lack of visual understanding ability, and (3) lack of situational safety judgment ability. To validate these hypotheses reasons, we design four variant evaluation settings: (1) letting MLLMs explicitly reason the safety of user query, (2) explicitly reason the safety of user's intent, (3) explicitly reason the safety of user's intent providing with self-caption, and (4) explicitly reason the safety of user's intent providing with ground-truth situation information. The difference between all 5 settings is shown in Fig. 4c.\nInfluence of explicit safety reasoning. To see whether lacking explicit safety reasoning causes poor performance, we design two settings that let MLLMs explicitly classify the user's query or intent into two classes: safe and unsafe. The performance in this setting is shown in Fig. 4. First, we observe that all models benefit from explicit safety reasoning. What is more, the performance improvement of proprietary models is larger, which is due to their stronger visual understanding and safety reasoning abilities. GPT4o especially benefits the most from explicit reasoning, demonstrating strong reasoning abilities but weak safety awareness in the normal instruction following setting. Then, we look into the more detailed performance of MLLMs. We find that explicit safety reasoning significantly improves the MLLMs' safety performance in unsafe situations, enabling them to recognize more unsafe user intents. However, it decreases the performance in safe situations, as shown in Fig. 12a in the Sec. A.4. This means that all models are over-sensitive and more inclined to think the user's intent is unsafe.\nSecondly, by comparing chat and embodied scenarios, we can find that the improvement of MLLMs on embodied tasks is very limited, even proprietary MLLMs only achieve around 58% accuracy. This shows current MLLMs have limited safety knowledge in embodied scenarios. By looking into the models' output, we find that MLLMs often make safety judgments based on non-significant visual observations. For instance, they would judge the task of placing a knife on the table as unsafe to"}, {"title": "5 MULTI-AGENT SYSTEM FOR BETTER SAFETY REASONING", "content": null}, {"title": "5.1 MULTI-AGENT SYSTEM DESIGN", "content": "We aim to leverage our analysis results to improve the MLLM's safety awareness when answering user's queries. First, we introduce explicit safety reasoning, which has shown significant safety performance improvement for both chat and embodied scenarios. Second, based on our findings that more complex task settings decrease the safety judgment performance of MLLMs, we explore leveraging the multi-agent systems (Zeng et al., 2024). Specifically, we split the task of answering questions safely into several subtasks and assigned them to different MLLM agents.\nFor chat scenarios, as shown in Fig. 6, we design a four-agent framework for open-source MLLMs comprising an intent reasoning agent, a visual understanding agent, a safety judgment agent, and a question-answering agent. The intent reasoning agent is responsible for thinking about the user's intent based on their query. The visual understanding agent provides a caption for the given image. The safety judgment agent will then judge the safety of the user's intent based on the image and the caption. The safety judgment will determine whether the question-answering agent will answer the user's query or remind the user about the safety risk. For proprietary MLLMs, due to their stronger ability to judge safety based on image content, we remove the visual understanding agent and form a three-agent framework. For embodied scenarios, given the former analysis that MLLMs often can not locate the most important visual evidence, we design a two-agent framework with the first agent locating the most important environment state (which object is required to be identified to ensure safety), then the second agent will reason the safety of the task instruction and generate respond by focusing on the reasoned environment state. The visualization is shown in Fig. 13 in the Sec. A.4."}, {"title": "5.2 RESULT AND ANALYSIS", "content": "We consider two baseline settings. The first one is the setting in Table. 2, where the prompt instructs the MLLMs to answer the user's query. Second, we let MLLMs perform the intent reasoning, safety judgment, and query-responding tasks in one step. The results of our multi-agent framework are in Fig. 7, showing that the multi-agent pipeline improves the performance consistently for almost all the models in both embodied and chat subtasks. In the chat scenario, the improvements of multi-agent on the open-source models are larger, which is due to the fact they perform weaker when solving all subtasks at once. We can observe that most open-source MLLMs can not improve their performance when performing all subtasks together, compared with the base setting. With our multi-agent design, most open-source MLLMs can catch the performance of Gemini, one of the closed-source models, this shows the effectiveness of our method."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In conclusion, this paper introduces the novel problem of Multimodal Situational Safety to evaluate the safety awareness of Multimodal Large Language Models (MLLMs) in scenarios where the safety of user queries depends on the visual context. By creating a comprehensive benchmark containing both safe and unsafe scenarios in chat and embodied assistant settings, the study reveals significant challenges that current MLLMs face in recognizing unsafe situations when answering a query, especially in embodied scenarios. Through further diagnosis, we find that enabling explicit safety reasoning and better safety-relevant visual understanding can improve the safety performance of MLLMs. Based on our experiment findings, we propose multi-agent approaches in which we let different agents perform different subtasks to improve the safety performance of MLLMs when answering user's queries.\nOur method shows promise for improving situational safety performance, but there is still considerable work to be done to enhance the situational safety judgment capabilities of these models. First, the performance of multi-agent is still far from perfect due to MLLMs's imperfect visual understanding and safety judgment abilities. Second, our multi-agent pipeline will take a longer time to answer a user's query since the model will explicitly reason multiple steps and require multiple inputs and outputs before responding to the user. Safety alignment training has enabled LLMs to refuse malicious language queries instantly without long reasoning (Wang et al., 2024b). We believe this would also be a necessary step to address the Multimodal Situational Safety problem."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 PERFORMANCE OF MLLMS IN MULTIMODAL SITUATIONAL SAFETY UNDER INTENT BINARY SAFETY CLASSIFICATION SETTING FOR CHAT TASK", "content": "Open-Soucre MLLMs. In safe situations of the Chat Task, open-source MLLMs show stable performance across four categories, indicating their effectiveness in clearly defined scenarios. They reliably recognize various scenarios, as illustrated in Fig. 8a, particularly excelling in classifying illegal activities. This suggests adequate training on safety contexts, as illegal activities often provide significant visual cues that facilitate accurate identification. In unsafe situations, models performance declines significantly. However, they exhibit relatively strong performance in offensive behaviors and illegal activities, as shown in Fig. 8b, due to clearer definitions and identifiable features, allowing for accurate judgments through semantic cues. In contrast, property damage and physical harm are more complex and subtle, necessitating multimodal information fusion and contextual understanding, which complicates accurate identification."}, {"title": "A.2 PERFORMANCE OF MLLMS IN MULTIMODAL SITUATIONAL SAFETY UNDER INTENT BINARY SAFETY CLASSIFICATION SETTING FOR EMBODIED TASK", "content": "Open-Soucre MLLMs. In safe situations of the embodied task, as shown in Fig. 10a and b open-source MLLMs exhibit strong performance across both categories, particularly in the physical task, where the models achieve nearly 100% accuracy, demonstrating high reliability. However, the models' performance in the unsafe situations drops significantly, with scores for both tasks falling below 40%.\nClose-Soucre MLLMs. Similar to the patterns observed in the chat task, from Fig. 10c and d, closed-source MLLMs exhibit weaker performance in safe scenarios compared to unsafe ones, indicating a heightened sensitivity to instructions. In this situation, both categories perform similarly. Furthermore, in unsafe scenarios, models demonstrate strong performance, with accuracy across both categories exceeding 80% at their peak."}, {"title": "A.3 EVALUATION", "content": null}, {"title": "A.5 CASE STUDY", "content": "We compare the outputs of various MLLMs, including those augmented with the Multi-Agent framework, across both safe and unsafe scenarios in Chat and Embodied tasks. As illustrated in Fig. 14, in an unsafe scenario where the user poses a general question related to sports, nearly all open-source and closed-source models provided a response. Although Deepseek initially refused to answer to some degree, it eventually responded due to a lack of genuine safety awareness. In contrast, the MLLMs equipped with the Multi-Agent framework effectively perceived the scene within the image, exhibited enhanced safety awareness, and refused to provide a response."}, {"title": "A.6 PROMPT", "content": null}]}