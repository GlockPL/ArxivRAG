{"title": "A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data", "authors": ["Andrej Tschalzev", "Sascha Marton", "Stefan L\u00fcdtke", "Christian Bartelt", "Heiner Stuckenschmidt"], "abstract": "Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing and feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics.", "sections": [{"title": "1 Introduction", "content": "Since ancient times, tables have been used as a data structure, i.e., to record astronomical observations [82] or financial transactions [14]. Many traditional machine learning (ML) methods, like logistic regression or the first artificial neural networks, were initially developed for tabular data [22, 62, 72]. Even nowadays, in the age of AI, tabular data is the most prevalent modality in real-world applications, including medicine [41], finance [16], manufacturing [88], retail [57], and many others [75, 13]. Several novel deep learning architectures have been contributed in recent years to improve supervised machine learning for tabular data [69, 92, 61, 40, 7, 15, 34, 77, 19, 48, 33].\nTo evaluate existing approaches, various comparative studies were conducted in recent years [11, 31, 29, 34, 75, 13, 63]. While motivated by different goals, they all have one in common: The focus is on evaluating models on tabular datasets using predefined cross-validation splits and one standardized preprocessing for all datasets. In this paper, we challenge such model-centric evaluation setups by highlighting two major limitations (Section 2): 1) The evaluation setups are overly standardized and"}, {"title": "2 Related Work", "content": "Machine Learning for tabular data. Unlike domains like computer vision and natural language processing, an established state-of-the-art neural network architecture does not exist for tabular data [75, 13]. Therefore, recent research has primarily concentrated on developing general-purpose deep learning models often inspired by architectures from other domains [69, 40, 52, 44, 7, 48, 92, 77, 37, 17, 83, 79, 90, 38, 65, 54, 85, 18, 19, 61, 33]. Despite these efforts, Gradient Boosted Decision Trees (GBDTs) remain the state-of-the-art, outperforming even the novel neural models in many studies [13, 35, 63]. This paper aims to motivate more research inspired by tabular data-specific techniques like feature engineering instead of architectures established in other domains.\nLimitations of current evaluation frameworks. Several benchmarks exist for evaluating tabular machine learning models, focusing on general model comparisons [11, 31, 29, 35, 63] and specific sub-problems [46, 28, 21, 74, 27]. However, these benchmarks do not provide preprocessing settings for the included datasets. Consequently, most studies adopt a fixed, standardized preprocessing for all datasets to concentrate on model comparisons [75, 34, 63, 35, 47]. While this model-centric approach is suitable for AutoML, it limits the real-world transferability of model comparisons, as models in practical applications typically follow dataset-specific preprocessing and feature engineering [81, 87, 39]. Our evaluation framework is the first to explicitly incorporate a more detailed distinction through diverse preprocessing pipelines. Furthermore, existing benchmarks lack an external reference (e.g., a leaderboard) for the current best task performance, hindering comparability across different studies. In contrast, we leverage datasets from ML competitions as an external benchmark for high performance on tasks. Many existing evaluation frameworks prioritize usability at the expense of representativeness by limiting sample sizes and removing high-cardinality categorical features, thus evaluating models on artificially constrained dataset versions [11, 35]. Our evaluation framework solely consists of tasks meaningful to the real world without imposing artificial restrictions on datasets. Finally, most evaluation frameworks concentrate on tasks where samples"}, {"title": "3 A Data-Centric Evaluation Framework for Tabular Machine Learning", "content": "We propose an evaluation framework built upon three crucial aspects that are often overlooked in tabular data research: 1) Evaluation on realistic datasets without removing frequently occurring challenging aspects like high cardinality categorical features, 2) Dataset-specific expert preprocessing pipelines, and 3) Evaluation against human expert performance on hidden test sets. Figure 1 depicts an overview of our framework. Our design choices are additionally justified by the fact that for each dataset, at least one model in our evaluation ranks among the top 1% of all competition participants."}, {"title": "3.1 Collection of Relevant and Challenging Datasets", "content": "We rely on the Kaggle community and competitions hosted by companies and institutions to select datasets with expert solutions. Figure 2 illustrates our dataset selection process, and Table 1 summarizes the main properties of the included datasets. Using data from Kaggle competitions has various benefits: 1) The selected tasks are challenging and meaningful to the real world, as companies and institutions only spend money on hosting competitions from which they benefit. 2) Each competition has a clear evaluation setup, including metrics selected to reflect the practitioners' needs. 3) Each competition has a large hidden test set, which has been shown to reduce the risk of adaptive overfitting [71]. 4) The competition leaderboard serves as an external reference for truly high performance, as many expert teams participated in the competitions. Furthermore, our framework ensures a fair comparison by including a data loading function for each dataset that removes potential side issues, like data leakage or faulty data. This distinguishes our framework from related work that compares"}, {"title": "3.2 Expert Solutions and Preprocessing Pipelines", "content": "Our proposed evaluation framework includes three preprocessing pipelines. One closely resembles the pipelines researchers currently use for model evaluation, and the other two are dataset-specific and directly derived from expert solutions. All preprocessing pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model and are explained in the Appendix.\nStandardized Preprocessing The main purpose of this pipeline in our framework is to evaluate single models in a scenario with minimal dataset-specific human effort invested. Continuous missing values are replaced with the mean, and missing categorical feature values are treated as a new category. Furthermore, constant columns are removed, and heavy-tailed targets are log-transformed for regression tasks. As these preprocessing steps are almost universally applied across related work [34, 35, 63], this pipeline represents current evaluation setups in academia well.\nExpert Feature Engineering We select one high-performance expert solution from Kaggle for each dataset. The solution was chosen based on the private leaderboard rank and the descriptions' quality and sufficiency. For each solution, we separate the data preparation from the remaining parts of the solution. For most datasets, this pipeline solely consists of feature engineering techniques. Besides a few distinctions between tree-based and deep learning models, the pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model in our framework and are explained in the Appendix. This paper focuses on a pipeline perspective and does not discuss single feature engineering steps further. Implementation details and feature engineering techniques"}, {"title": "3.3 Modeling and Evaluation Framework", "content": "Modeling Pipeline and Models We implement a unified modeling pipeline for all datasets with a dataset-specific cross-validation (CV) ensembling procedure. The validation sets are used for early stopping and determining the best hyperparameters. The final test data predictions are an ensemble of averaging the test predictions of each fold. Our experiments compare 7 models and one AutoML solution for all datasets and preprocessing pipelines. We use three gradient-boosted tree libraries (XGBoost [20], LightGBM [43], and CatBoost [70]) because each was used in at least one of the expert solutions. Each expert solution that used neural networks developed a highly customized network for the particular competition. We want to assess whether recently developed general-purpose architectures can replace the high effort of building custom networks. Hence, we chose ResNet and FTTransformer [34] because they have been frequently used in recent benchmark comparisons and have shown strong performance [35, 63]. Because the Resnet essentially is an MLP with skip connections, it serves as a baseline representing what was already possible before the recent developments in DL for tabular data. In addition, we use two more recent approaches: MLP-PLR [32], which can help learn high-frequency functions, mitigating a major weakness of deep learning for tabular data [35]; and GRANDE [61], a recent representative of hybrid neural-tree models. We are aware that even more recent architectures exist. However, our focus is not on particular models but on datasets and preprocessing. To assess how well fully automated solutions perform without any preprocessing, we additionally evaluate AutoGluon, which has been shown to be the current best AutoML solution [30].\nHyperparameter Optimization Hyperparameter optimization is done per fold to obtain a diverse CV ensemble. Each model is evaluated in three HPO regimes: 1) Default: Either library default or hyperparameters suggested in related work, 2) Light HPO: 20 random search iterations. 3) Extensive HPO: 20 random search warmup iterations + 80 iterations of the tree-structured Parzen estimator algorithm [4]. More details on the hyperparameter optimization can be seen in the Appendix.\nEvaluation We use the Kaggle API to automatically submit predictions and retrieve performance results after evaluating against the hidden targets. Each dataset is evaluated on the metric specified by the competition host. Instead of reporting this metric directly, we report the solution's private leaderboard position as the percentile. This has the benefit that although different metrics are used to evaluate the model, comparisons across datasets are possible. In the Appendix, we additionally report performances on the actual metrics for each dataset. Throughout the paper, higher values represent a better performance (leaderboard position)."}, {"title": "4 Experimental Evaluation", "content": "Our framework allows us to assess the dataset-specific individual performance impact of model selection, hyperparameter optimization, feature engineering, and test-time adaptation. As a general overview, Figure 3 shows how each of the analyzed modeling components improves over the default baseline for each model and dataset. The results demonstrate the importance of an external performance reference: If we only considered the standardized evaluation setup (blue/orange/green bars), we would only be scratching the surface of achievable task performance for many data sets."}, {"title": "4.1 How Model Comparisons Change When Considering Dataset-specific Preprocessing", "content": "Through the implemented modeling pipelines, it becomes possible to evaluate how model comparisons change when evaluating inside the typically used standardized pipelines vs. expert pipelines with and without test-time adaptation. Three observations stand out when evaluating models in different preprocessing pipelines (Figure 4). 1) The model rankings change considerably, as indicated by the relatively low Spearman coefficients between the standardized preprocessing pipeline and the other pipelines. 2) The performance gap between all models diminishes when considering expert preprocessing. On average, all models benefit from feature engineering, and multiple models can reach top performance. While all models benefit from TTA, the performance increase varies. 3) The superiority of CatBoost vanishes when considering dataset-specific preprocessing. The reason is that CatBoost already incorporates specific feature engineering steps in its algorithm for which other models need manual engineering, as we will further elaborate in Subsection 4.3."}, {"title": "4.2 Measurable Progress Through Recent Efforts", "content": "Figure 5 shows the model ranking on the private Kaggle leaderboard when trained after standardized preprocessing. CatBoost achieves top ranks in three competitions (MBGM, BPCCM, HQC) where a high manual effort in feature engineering was previously necessary. Similar to Erickson et al. [25], AutoGluon achieves top ranks in two of these (BPCCM, HQC) and one additional competition (OGPCC). Regarding neural networks, novel architectures outperform the ResNet baseline on nine datasets and even outperform tree-based solutions for three datasets (SVPC, SCS, SCTP). All neural networks originally used in the competitions were custom-designed for the particular competition. Hence, our analysis confirms that meaningful progress has been made in developing general-purpose architectures for tabular data. Although the progress in the tabular data field is clearly visible, top performance cannot be reached without human effort for six datasets."}, {"title": "4.3 Feature Engineering is Still the Most Important Factor for Top Performance", "content": "The most remarkable performance gains are achieved through feature engineering. Figure 6 shows that expert feature engineering is the most important modeling component on average. This holds true for all models, indicating that unlike for modalities like imaging, neural networks do not automate feature engineering for tabular data. When comparing the performance of different models in the standardized preprocessing pipeline (blue/orange/green bars in Figure 3), we can observe that using any other model than CatBoost rarely brings large gains. Only for the SCS dataset does FTTransformer clearly outperform all other models. For all other datasets, the average performance gains achievable solely with model selection are small. Hence, our results confirm the findings of McElfresh et al. [63] that model selection is less important than HPO on a strong tree-based baseline for most datasets. Furthermore, we extend this finding by quantifying the even more important aspect of dataset-specific feature engineering.\nFeature engineering is responsible for the high performance of CatBoost. Our analysis of different preprocessing pipelines reveals that CatBoost benefits much less from feature engineering than other models. The reason is that CatBoost incorporates explicit feature engineering techniques in its learning procedure. In particular, counts and target-based statistics are used to generate encodings for categorical features, and combinatorial encoding methods capture categorical feature interactions [70]. When considering the same feature engineering techniques for the other models, the gap to CatBoost drastically shrinks for most models, and XGBoost performs similarly to CatBoost on average. Hence, CatBoost's success in recent benchmarking studies [63, 28] can, at least to some extent, be attributed to feature engineering."}, {"title": "4.4 The Importance of Test-Time Adaptation and Temporal Characteristics", "content": "Test-time feature engineering con-sistently improves the performance of single models. Table 3 shows that test-time feature engineering leads to performance gains over solely using the train data for feature engineering for all datasets. From the task perspective, the feature engineering used for AEAC and OGPCC only leads to performance gains when used as a test-time adaptation method. This shows that some of the feature engineering techniques used in Kaggle competitions actually serve the purpose of test-time adaptation. For three datasets, ranking among the top 1% on the leaderboard was not achieved without test-time adaptation. Our results indicate that simply comparing approaches to the Kaggle leaderboard, as done in previous studies [25, 87], is insufficient. Techniques like test-time adaptation are frequently used in Kaggle competitions and limit comparability to approaches that don't use the test data. Hence, a fair model comparison to expert solutions using the Kaggle leaderboard can only be ensured under controlled conditions through implemented expert solutions such as our pipelines.\nModels in real-world applications are often applied to non-i.i.d. tabular data. By definition, TTA should only be effective if the data violates the i.i.d. assumption and contains distribution shifts to adapt to. Indeed, the data collection process likely happened over time for most of the datasets used in our framework. However, timestamps were not always provided as the competitions were conceptualized as static tabular data tasks. Therefore, most of the datasets were also used in at least one comparative study for tabular data, although non-i.i.d. was a criterion for exclusion (e.g., SVPC, AEAC, and PSSDP in [30], SCTP and OGPCC in [47], or MBGM in [35]). Our results show, that despite treating datasets as static, the samples remain non-i.i.d. and approaches like test-time adaptation can improve performance. Furthermore, there is evidence that other datasets treated as i.i.d. in related work actually have a temporal nature. I.e., Kohli et al. [47] found that the most frequently used dataset in tabular data research is the forest cover type dataset [12]. At the same time, this dataset is used as a benchmark in online learning to measure the ability of models to adapt to concept drifts [24, 73]. As models for tabular data assume the data to be i.i.d., most benchmarks for evaluating tabular general-purpose models either directly name the data being non-i.i.d. as an exclusion criterion [35, 29] or exclude data that requires special CV procedures [11], which leads to the same results. In contrast, our analysis of Kaggle competitions revealed that most tabular data competitions have temporal characteristics and that the best solutions for such datasets typically engineer time-invariant features and utilize tabular data models assuming the data to be i.i.d (i.e. [3]). We conclude that there might be a mismatch between current evaluation frameworks for tabular data in academia and the tabular data tasks practitioners were interested in getting solved through ML competitions on Kaggle."}, {"title": "5 Implications for Future Work", "content": "We challenged the prevalent model-centric evaluation setups in tabular data research by comparing evaluations with standardized preprocessing pipelines to evaluations with expert preprocessing pipelines. We have shown that current research is overly model-centric, while tabular datasets often require dataset-specific feature engineering or violate the i.i.d. assumption the models are based on. This reveals important insights and directions for future work in Machine Learning for tabular data.\nDinstinguish between AutoML and model comparisons. Our findings highlight that standardized evaluation setups do not necessarily ensure fair model comparisons. In standardized preprocessing setups, models are treated as AutoML solutions, whereas in real-world applications, they are components of highly dataset-specific pipelines. Comparative studies should differentiate between datasets that benefit from known feature engineering steps and those that are self-sufficient. One approach could be to separate raw data benchmarks from fully preprocessed benchmarks, as done in our study. Standardized setups are more suitable for benchmarking AutoML solutions, while feature-engineered setups might be better for benchmarking models. Future research could emphasize incorporating dataset-specific (expert) preprocessing pipelines into benchmarks. However, gathering high-quality expert solutions at a large scale is tedious and may require a community effort.\nNeed for external performance references. Our analysis shows that evaluations without considering the highest achievable performance on a task don't actually measure the state-of-the-art. Despite numerous benchmarks, there is no established standard to measure progress. A benchmark with a public leaderboard and a dynamic collection of meaningful and unsolved real-world datasets could facilitate progress.\nImprove feature engineering capabilities of models for tabular data. Researchers developing general-purpose models should recognize the impact of feature engineering on model performance. CatBoost has advanced the field by automating feature engineering on categorical data. However, significant feature engineering effort is still necessary for datasets where this is not the only challenge. Future work should take a data-centric perspective and focus on automating successful feature engineering techniques through novel architecture components. Our expert feature engineering pipelines can serve as a starting point for evaluating and developing new methods. Furthermore, unlike previously claimed [35], categorical features can indeed be an important challenge for deep learning models. Hence, future work could focus on improvements over classic embeddings for categorical features in general-purpose deep learning architectures.\nMethods for tabular data with temporal characteristics. Our analysis highlights the importance of distribution shifts in real-world tabular data, even when treating a task as static. Future work could investigate test-time adaptation methods specifically for tabular data, using our datasets and the identified test-time feature engineering techniques as baselines. Furthermore, our findings indicate that the current research focus on static i.i.d. data might hinder the development of techniques to handle weak temporal correlations in tabular data. Future work should focus on developing models with inductive biases for tabular data with temporal characteristics.\nAlign tabular benchmarks with practitioners needs. We have shown that models developed for tabular data are often applied to datasets with temporal characteristics, while existing tabular data benchmarks are overly focused on i.i.d. data. General-purpose tabular benchmarks should consider including tabular datasets with temporal characteristics instead of excluding them. Furthermore, a benchmark solely for tabular datasets with temporal characteristics could significantly advance model development for this relevant data problem."}, {"title": "A Datasets and Expert Solutions", "content": "In this Section, we provide more details on the dataset/competition selection process and the expert solutions implemented in our framework. Table 4 shows the name of all Kaggle competitions included in our framework as well as a hyperlink for the implemented expert feature engineering."}, {"title": "A.1 Dataset Selection", "content": "The main paper already provides an overview of the main characteristics of the selected datasets and our main selection criteria. In this Subsection we further explain the selection criteria and summarize the excluded datasets."}, {"title": "A.2 Implemented Components of Expert Solutions", "content": "In this Subsection, we document the components of our framework that were directly derived from expert solutions.\nTask conceptualization in data loading. For all datasets, the data loading includes merging tables, defining the target, and defining categorical features. For some datasets, we incorporated parts of expert solutions into the task conceptualization as a part of the data-loading function:\n\u2022 mercedes-benz-greener-manufacturing: The index is used as a numeric feature as it was necessary to score top leaderboard ranks.\n\u2022 santander-value-prediction-challenge: 1) The target is marked as heavy-tailed to be transformed in the standardized preprocessing pipeline. 2) There was a data leak allowing to derive the test targets for some samples. The top expert solutions used these samples for data augmentation. Hence, we also moved these samples from the test to the training dataset, s.t. this leak is not an issue for any of our pipelines.\n\u2022 homesite-quote-conversion: Extract weekday from datetime feature."}, {"title": "A.3 Discussion on Test-Time Feature Engineering", "content": "To deal with distribution shifts, test-time adaptation is a conceptual framework where the model parameters are allowed to depend on the test sample x but not on its unknown label y. This matches the common ML competition setup, where test samples are given, but the target is hidden. We found that successful participants in Kaggle competitions often use the test data for feature engineering. Hence, we established that using test data for feature engineering in Kaggle competitions can be considered a special kind of test-time adaptation. This subsection discusses when this practice can be considered for real-world applications and when it is an unfair and unrealistic setup for a task.\nWe argue that the common ML setup allowing for test-time adaptation corresponds to a frequent real-world application scenario where 1) The data to predict arrives in batches, 2) No real-time predictions are required, and 3) Retraining the model at test time is feasible. The first criterion is important as the employed test-time feature engineering techniques (e.g. dimensionality reduction and frequency encoding) often required the presence of many test samples at once. It is unclear whether this kind of domain adaptation would work per sample. The second and third criteria are necessary as test-time feature engineering always requires retraining the model, which is infeasible in online applications. In contrast, test-time feature engineering is not applicable in applications where online predictions are required, the number of test samples is small, or not retrainable (e.g., large-scale) models are used. Importantly, while test-time feature engineering might be infeasible in such applications, other test-time adaptation techniques might still apply. One example of such a task conceptualization amenable to test-time feature engineering is product return prediction, where samples are collected over a day, and a (lightweight) model can be retrained daily. In this scenario, using the test data in an unsupervised fashion for better adaptation to possible distribution shifts is feasible. After examining the application scenarios of the tasks in our framework, we found that most of them, like customer transaction prediction (SCTP) and customer satisfaction prediction (SCS), would allow such a setup, although likely with smaller amounts of test data than used for the competitions. Furthermore, our discussion in Subsection 4.4 reveals that many tabular datasets not in our scope have temporal components and thus may be amenable to test-time feature engineering."}, {"title": "B Experimental Details", "content": "In this Section, we discuss all aspects of our experiments that are not a part of our proposed evaluation framework but rather are design choices we made for our experiments."}, {"title": "B.1 Software and Hardware", "content": "The deep learning models, CatBoost, and XGBoost, were trained using one or more of the following GPU hardware, depending on the availability: NVIDIA H100, NVIDIA A100, NVIDIA RTX A6000, or NVIDIA A40. LightGBM and AutoGluon were trained using the following CPU hardware: Intel(R) Xeon(R) CPU E2640v2 @ 2,00 GHz; Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz."}, {"title": "B.2 Model-Specific Preprocessing", "content": "For the tree-based models, model-specific preprocessing only included the correct assignment of datatypes to categorical features. For the deep learning models, the preprocessing was defined in line with the related work [34, 35]. For regression, the target is normalized to zero mean and unit variance. For numeric features, missing values are replaced with the mean, and the features are normalized using ScikitLearn's QuantileTransformer [68]. Categorical features are ordinally encoded as ResNet, FTTransformer, and MLP-PLR use embeddings for categorical features. The GRANDE library includes its own preprocessing, which contains the same steps as for the other deep learning models but uses leave-one-out-encoding for categorical features. For AutoGluon, all the preprocessing is left to the AutoML framework."}, {"title": "B.3 Model Training and Hyperparameter Optimization", "content": "We use the optuna library [4] for hyperparameter optimization. Each model is optimized for 100 trials with the first 20 trials being random search trials and the remaining 80 using the multivariate Tree-structured Parzen Estimator algorithm [10, 26]. The models are trained using cross entropy for classification and mean squared error for regression. The AdamW optimizer is used for training the deep learning models [56]. Whenever possible, we use the task metric for validation during model training and for choosing the best hyperparameters. Instead of the R2 metric, we use rmse, as the objective is the same. Moreover, we we use rmse whenever the metric is rmsle, as we already transformed the target prior to fitting. Instead of Gini, we use AUC, as the metrics are convertible. For AutoGluon, we use the 'best_quality' preset configuration and a time limit of 10 hours. Everything else is left to the AutoML library itself. We try to use default hyperparameters and tuning ranges that have been shown to perform well for each model. For that, we orient on different related work [34, 35, 32, 63, 61] and the library documentations. Some of the datasets we use are quite large compared to most related work. Our goal was to evaluate each of the included models with an equal number of hyperparameter trials. Therefore, we did not use time budgets to constrain the number of trials per model and dataset. As this leads to long computation times for some models, we did not tune the representation capacity parameters for FTTransformer, as it was the most time-expensive model in our scope. All default hyperparameters and search spaces can be seen in Tables 6-12."}, {"title": "C Detailed Performance Results", "content": "In this Section, we provide the leaderboard position results for all the experiments in the main paper, separated by hyperparameter regime and preprocessing pipeline. The results can be seen in Tables 13, 14, and 16."}, {"title": "D Additional Results", "content": "D.1 Comparison of Preprocessing Pipelines per Model and Dataset\nFigure 7 visualizes the comparison of different pipelines per model and dataset. It can be seen that almost all models benefit from feature engineering and test-time adaptation on almost all datasets. The only remarkable outlier is FTTransformer on the SCS dataset. The otherwise consistent results support our claim that feature engineering and test-time adaptation are important components of tabular machine learning competitions."}, {"title": "D.2 Analysis of Modeling Components", "content": "In this Subsection we complement our analysis of modeling components in the main paper with additional analyses from different perspectives. Figure 8 shows the distributions of the leaderboard positions of all our experiments, grouped by different modeling components. It can be seen that without expert feature engineering, most submissions are far from the top percentiles on the leaderboard, while after expert feature engineering, most submissions score top ranks. After test-time adaptation, the density of top submissions increases even more. Moreover, the importance of hyperparameter optimization can be seen. Regarding model selection, CatBoost clearly dominates, mainly due to the property of achieving robustly strong results with default hyperparameters.\nThese results are in line with common practice in ML competitions [81]. While recent work strongly focuses on model selection [35, 63], participants of Kaggle competitions typically stick to few model classes and instead focus on developing feature engineering techniques for these particular models [42]. Predictive Machine Learning is a winner-takes-all game. Selecting another model than the one that is known to work best is only important if the default model fails or if ensembling is necessary. Hence, it makes sense to look at the problem from the winner-takes-all perspective and to evaluate performance gains from different modeling decisions w.r.t. a strong baseline. It is known"}, {"title": "D.3 Using Our Framework for Evaluating New Methods", "content": "In Section 5 we identified directions for future work. These directions were based on general insights of our analysis for the tabular data field. This subsection will showcase how our framework can be utilized to develop new approaches and compare them to expert solutions. Our framework contains, to our knowledge, the largest collection of implemented expert solutions for relevant datasets. Hence, our framework is especially useful to researchers developing AutoML solutions, especially focusing on feature engineering. Furthermore, our framework can be useful to researchers developing model-specific and data-agnostic preprocessing pipelines, i.e., for novel neural networks. In addition, our framework can be used to develop test-time adaptation methods for tabular data. Figure 10 shows four particular challenges for future work in tabular Deep Learning and AutoML for which our framework can serve as a benchmark to measure progress: A) Develop a neural network not relying on feature engineering techniques; B) Develop a neural network capable of test-time adaptation to replace the often infeasible test-time feature engineering. C) Create a universal model-agnostic automated feature engineering pipeline that surpasses expert feature engineering; D) Enhance AutoML solutions to outperform expert feature engineering pipelines;"}, {"title": "E Limitations", "content": "The main goal of our experiments was to showcase the limitations of evaluation frameworks currently prevalent in tabular Machine Learning. This required extensive experiments in different preprocessing pipelines. Some limitations arising from this scope are:\n\u2022 We use Kaggle competitions in an effort to evaluate more realistic tasks than evaluated in related work. It is important to highlight that the competition setup on Kaggle does not always reflect real-world tasks. However, due to the involvement of companies and institutions and the poor availability of high-quality tabular datasets [47], these are arguably among the most realistic datasets available as open-source data. Furthermore, one of our contributions was to separate aspects from the main learning task that made competitions unrealistic (i.e., data leaks or, for some applications, test-time adaptation). This additionally improves the real-world transferability of our experiments.\n\u2022 We split the overall expert preprocessing into feature engineering and test-time adaptation. However, pipelines could be differentiated further, or single-feature engineering techniques could be investigated. For instance, we could separate the expert feature engineering pipeline by whether expert feature selection is applied. However, due to the extent of our experiments, we focus on a pipeline perspective and leave fine-grained analyses of specific techniques for future work.\n\u2022 We use the leaderboard percentile as the main evaluation measure to have an external reference for the top performance on a dataset. A possible issue of that design choice is that the leaderboard of each dataset is differently distributed. Hence, what appears to be a large jump on a dataset might actually only be a small increase on the metric, while for another dataset, the same leaderboard increase might amount to a substantial increase in performance. However, averaging over datasets has a natural interpretation when using the leaderboard position, which is not there when using normalized versions of entirely different metrics. In addition to the evaluation in the main paper, we include evaluations on the original metrics in Appendix F. The results indicate that our claims similarly hold when evaluating using the original metrics.\n\u2022 Our evaluation framework does not allow to assess whether one model is generally better than another model. We only claim that model comparisons change and that feature engineering and preprocessing greatly influence model comparison on our datasets. For a more generalizable model comparison using more datasets, we refer to related work [35, 63]."}, {"title": "F Evaluation on the Original Task Metrics", "content": "This Section lists the main results using the original task metrics for all experiments. An overview of important components per dataset and model can be seen in Figure 11. All experimental results on the original metrics can be seen in Tables 18, 19, 20, and 21. Further results can be seen in our code. Overall, the evaluation using the original metrics aligns with our main findings."}]}