{"title": "A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data", "authors": ["Andrej Tschalzev", "Sascha Marton", "Stefan L\u00fcdtke", "Christian Bartelt", "Heiner Stuckenschmidt"], "abstract": "Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing and feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics.", "sections": [{"title": "Introduction", "content": "Since ancient times, tables have been used as a data structure, i.e., to record astronomical observations [82] or financial transactions [14]. Many traditional machine learning (ML) methods, like logistic regression or the first artificial neural networks, were initially developed for tabular data [22, 62, 72]. Even nowadays, in the age of AI, tabular data is the most prevalent modality in real-world applications, including medicine [41], finance [16], manufacturing [88], retail [57], and many others [75, 13]. Several novel deep learning architectures have been contributed in recent years to improve supervised machine learning for tabular data [69, 92, 61, 40, 7, 15, 34, 77, 19, 48, 33].\nTo evaluate existing approaches, various comparative studies were conducted in recent years [11, 31, 29, 34, 75, 13, 63]. While motivated by different goals, they all have one in common: The focus is on evaluating models on tabular datasets using predefined cross-validation splits and one standardized preprocessing for all datasets. In this paper, we challenge such model-centric evaluation setups by highlighting two major limitations (Section 2): 1) The evaluation setups are overly standardized and do not reflect the actual routine of practitioners, which typically includes dataset-specific feature engineering [81]. 2) There is no external reference for the highest possible performance on a task beyond a study's own reporting, which limits its reliability.\nTo address these issues, we advocate for shifting the research perspective in the tabular data field from model-centric to data-centric. Therefore, our main contribution is an evaluation framework that includes a collection of ten relevant real-world datasets, dataset-specific expert-level preprocessing pipelines, and an external measure of top performance for each dataset (Section 3). The datasets were carefully selected by screening Kaggle competitions involving tabular data, and, to our knowledge, our contribution represents the largest existing collection of implemented expert-level solutions for tabular datasets. To assess the potential bias from the first limitation, we investigate how the model comparison changes when considering dataset-specific preprocessing instead of standardized evaluation setups (Subsection 4.1). To address the second limitation, we use the leaderboard from Kaggle competitions as an external performance reference and reassess what is possible with modern methods that were not available when the Kaggle competitions took place (Subsection 4.2). We find that when considering dataset-specific expert preprocessing, performance differences between the best models shrink, and the importance of selecting the 'right' model diminishes. In addition, we dissect expert solutions for tabular data competitions and quantify the importance of different modeling components (Subsection 4.3). We find that measurable progress has been made in automating human effort, but feature engineering is still the most important aspect of many tabular data problems. No model fully automates this aspect and comparisons that don't consider feature engineering merely scratch the surface of the potential performance achievable on many datasets. This paper focuses on independent and identically distributed (i.i.d.) tabular data in line with related work. However, our analysis of Kaggle competitions shows strong evidence that this focus in the research community might not align with practitioners' needs. In particular, we find that many tabular data competitions on Kaggle have temporal characteristics (i.e., timestamp features) and we identify test-time adaptation (TTA) as an overlooked but important part of some supposedly static competitions (Subsection 4.4).\nOur findings indicate that current academic evaluation setups and benchmarks for tabular data are biased due to their overly model-centric focus. We conclude by discussing possible directions to improve machine learning for tabular data from a data-centric perspective (Section 5)."}, {"title": "Related Work", "content": "Machine Learning for tabular data. Unlike domains like computer vision and natural language processing, an established state-of-the-art neural network architecture does not exist for tabular data [75, 13]. Therefore, recent research has primarily concentrated on developing general-purpose deep learning models often inspired by architectures from other domains [69, 40, 52, 44, 7, 48, 92, 77, 37, 17, 83, 79, 90, 38, 65, 54, 85, 18, 19, 61, 33]. Despite these efforts, Gradient Boosted Decision Trees (GBDTs) remain the state-of-the-art, outperforming even the novel neural models in many studies [13, 35, 63]. This paper aims to motivate more research inspired by tabular data-specific techniques like feature engineering instead of architectures established in other domains.\nLimitations of current evaluation frameworks. Several benchmarks exist for evaluating tabular machine learning models, focusing on general model comparisons [11, 31, 29, 35, 63] and specific sub-problems [46, 28, 21, 74, 27]. However, these benchmarks do not provide preprocessing settings for the included datasets. Consequently, most studies adopt a fixed, standardized preprocessing for all datasets to concentrate on model comparisons [75, 34, 63, 35, 47]. While this model-centric approach is suitable for AutoML, it limits the real-world transferability of model comparisons, as models in practical applications typically follow dataset-specific preprocessing and feature engineering [81, 87, 39]. Our evaluation framework is the first to explicitly incorporate a more detailed distinction through diverse preprocessing pipelines. Furthermore, existing benchmarks lack an external reference (e.g., a leaderboard) for the current best task performance, hindering comparability across different studies. In contrast, we leverage datasets from ML competitions as an external benchmark for high performance on tasks. Many existing evaluation frameworks prioritize usability at the expense of representativeness by limiting sample sizes and removing high-cardinality categorical features, thus evaluating models on artificially constrained dataset versions [11, 35]. Our evaluation framework solely consists of tasks meaningful to the real world without imposing artificial restrictions on datasets. Finally, most evaluation frameworks concentrate on tasks where samples"}, {"title": "A Data-Centric Evaluation Framework for Tabular Machine Learning", "content": "We propose an evaluation framework built upon three crucial aspects that are often overlooked in tabular data research: 1) Evaluation on realistic datasets without removing frequently occurring challenging aspects like high cardinality categorical features, 2) Dataset-specific expert preprocessing pipelines, and 3) Evaluation against human expert performance on hidden test sets. Figure 1 depicts an overview of our framework. Our design choices are additionally justified by the fact that for each dataset, at least one model in our evaluation ranks among the top 1% of all competition participants."}, {"title": "Collection of Relevant and Challenging Datasets", "content": "We rely on the Kaggle community and competitions hosted by companies and institutions to select datasets with expert solutions. Figure 2 illustrates our dataset selection process, and Table 1 summa-rizes the main properties of the included datasets. Using data from Kaggle competitions has various benefits: 1) The selected tasks are challenging and meaningful to the real world, as companies and institutions only spend money on hosting competitions from which they benefit. 2) Each competition has a clear evaluation setup, including metrics selected to reflect the practitioners' needs. 3) Each competition has a large hidden test set, which has been shown to reduce the risk of adaptive overfitting [71]. 4) The competition leaderboard serves as an external reference for truly high performance, as many expert teams participated in the competitions. Furthermore, our framework ensures a fair comparison by including a data loading function for each dataset that removes potential side issues, like data leakage or faulty data. This distinguishes our framework from related work that compares approaches to Kaggle solutions [25, 87]. An important insight from screening the competitions is that most tabular datasets had temporal characteristics \u2013 i.e., datasets with weak temporal correlations that benefit from time-sensitive feature engineering but not from models with temporal inductive biases (i.e., [3]). This finding will be further discussed in Subsection 4.4."}, {"title": "Expert Solutions and Preprocessing Pipelines", "content": "Our proposed evaluation framework includes three preprocessing pipelines. One closely resembles the pipelines researchers currently use for model evaluation, and the other two are dataset-specific and directly derived from expert solutions. All preprocessing pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model and are explained in the Appendix.\nStandardized Preprocessing The main purpose of this pipeline in our framework is to evaluate single models in a scenario with minimal dataset-specific human effort invested. Continuous missing values are replaced with the mean, and missing categorical feature values are treated as a new category. Furthermore, constant columns are removed, and heavy-tailed targets are log-transformed for regression tasks. As these preprocessing steps are almost universally applied across related work [34, 35, 63], this pipeline represents current evaluation setups in academia well.\nExpert Feature Engineering We select one high-performance expert solution from Kaggle for each dataset. The solution was chosen based on the private leaderboard rank and the descriptions' quality and sufficiency. For each solution, we separate the data preparation from the remaining parts of the solution. For most datasets, this pipeline solely consists of feature engineering techniques. Besides a few distinctions between tree-based and deep learning models, the pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model in our framework and are explained in the Appendix. This paper focuses on a pipeline perspective and does not discuss single feature engineering steps further. Implementation details and feature engineering techniques"}, {"title": "Modeling and Evaluation Framework", "content": "Modeling Pipeline and Models We implement a unified modeling pipeline for all datasets with a dataset-specific cross-validation (CV) ensembling procedure. The validation sets are used for early stopping and determining the best hyperparameters. The final test data predictions are an ensemble of averaging the test predictions of each fold. Our experiments compare 7 models and one AutoML solution for all datasets and preprocessing pipelines. We use three gradient-boosted tree libraries (XGBoost [20], LightGBM [43], and CatBoost [70]) because each was used in at least one of the expert solutions. Each expert solution that used neural networks developed a highly customized network for the particular competition. We want to assess whether recently developed general-purpose architectures can replace the high effort of building custom networks. Hence, we chose ResNet and FTTransformer [34] because they have been frequently used in recent benchmark comparisons and have shown strong performance [35, 63]. Because the Resnet essentially is an MLP with skip connections, it serves as a baseline representing what was already possible before the recent developments in DL for tabular data. In addition, we use two more recent approaches: MLP-PLR [32], which can help learn high-frequency functions, mitigating a major weakness of deep learning for tabular data [35]; and GRANDE [61], a recent representative of hybrid neural-tree models. We are aware that even more recent architectures exist. However, our focus is not on particular models but on datasets and preprocessing. To assess how well fully automated solutions perform without any preprocessing, we additionally evaluate AutoGluon, which has been shown to be the current best AutoML solution [30].\nHyperparameter Optimization Hyperparameter optimization is done per fold to obtain a diverse CV ensemble. Each model is evaluated in three HPO regimes: 1) Default: Either library default or hyperparameters suggested in related work, 2) Light HPO: 20 random search iterations. 3) Extensive HPO: 20 random search warmup iterations + 80 iterations of the tree-structured Parzen estimator algorithm [4]. More details on the hyperparameter optimization can be seen in the Appendix.\nEvaluation We use the Kaggle API to automatically submit predictions and retrieve performance results after evaluating against the hidden targets. Each dataset is evaluated on the metric specified by the competition host. Instead of reporting this metric directly, we report the solution's private leaderboard position as the percentile. This has the benefit that although different metrics are used to evaluate the model, comparisons across datasets are possible. In the Appendix, we additionally report performances on the actual metrics for each dataset. Throughout the paper, higher values represent a better performance (leaderboard position)."}, {"title": "Experimental Evaluation", "content": "Our framework allows us to assess the dataset-specific individual performance impact of model selection, hyperparameter optimization, feature engineering, and test-time adaptation. As a general overview, Figure 3 shows how each of the analyzed modeling components improves over the default baseline for each model and dataset. The results demonstrate the importance of an external performance reference: If we only considered the standardized evaluation setup (blue/orange/green bars), we would only be scratching the surface of achievable task performance for many data sets."}, {"title": "How Model Comparisons Change When Considering Dataset-specific Preprocessing", "content": "Through the implemented modeling pipelines, it becomes possible to evaluate how model com-parisons change when evaluating inside the typically used standardized pipelines vs. expert pipelines with and without test-time adaptation. Three observations stand out when evaluating models in different preprocessing pipelines (Figure 4). 1) The model rankings change considerably, as indicated by the relatively low Spearman coefficients between the standardized preprocessing pipeline and the other pipelines. 2) The performance gap between all models diminishes when considering expert preprocessing. On average, all models benefit from feature engineering, and multiple models can reach top performance. While all models benefit from TTA, the performance increase varies. 3) The superiority of CatBoost vanishes when considering dataset-specific preprocessing. The reason is that CatBoost already incorporates specific feature engineering steps in its algorithm for which other models need manual engineering, as we will further elaborate in Subsection 4.3."}, {"title": "Measurable Progress Through Recent Efforts", "content": "Figure 5 shows the model ranking on the private Kaggle leaderboard when trained after stan-dardized preprocessing. CatBoost achieves top ranks in three competitions (MBGM, BPCCM, HQC) where a high manual effort in feature engineering was previously necessary. Similar to Erickson et al. [25], AutoGluon achieves top ranks in two of these (BPCCM, HQC) and one additional competition (OGPCC). Regarding neural networks, novel architectures outper-form the ResNet baseline on nine datasets and even outperform tree-based solutions for three datasets (SVPC, SCS, SCTP). All neural net-works originally used in the competitions were custom-designed for the particular competition. Hence, our analysis confirms that meaningful progress has been made in developing general-purpose architectures for tabular data. Although the progress in the tabular data field is clearly visible, top performance cannot be reached with-out human effort for six datasets."}, {"title": "Feature Engineering is Still the Most Important Factor for Top Performance", "content": "The most remarkable performance gains are achieved through feature engineering. Figure 6 shows that expert feature engineering is the most important modeling component on average. This holds true for all models, indicating that un-like for modalities like imaging, neural networks do not automate feature engineering for tabular data. When comparing the performance of dif-ferent models in the standardized preprocessing pipeline (blue/orange/green bars in Figure 3), we can observe that using any other model than CatBoost rarely brings large gains. Only for the SCS dataset does FTTransformer clearly outper-form all other models. For all other datasets, the average performance gains achievable solely with model selection are small. Hence, our re-sults confirm the findings of McElfresh et al. [63] that model selection is less important than HPO on a strong tree-based baseline for most datasets. Furthermore, we extend this finding by quantifying the even more important aspect of dataset-specific feature engineering.\nFeature engineering is responsible for the high performance of CatBoost. Our analysis of different preprocessing pipelines reveals that CatBoost benefits much less from feature engineering than other models. The reason is that CatBoost incorporates explicit feature engineering techniques in its learning procedure. In particular, counts and target-based statistics are used to generate encodings for categorical features, and combinatorial encoding methods capture categorical feature interactions [70]. When considering the same feature engineering techniques for the other models, the gap to CatBoost drastically shrinks for most models, and XGBoost performs similarly to CatBoost on average. Hence, CatBoost's success in recent benchmarking studies [63, 28] can, at least to some extent, be attributed to feature engineering."}, {"title": "The Importance of Test-Time Adaptation and Temporal Characteristics", "content": "Test-time feature engineering con-sistently improves the performance of single models.Table 3 shows that test-time feature engineering leads to performance gains over solely using the train data for feature engineering for all datasets. From the task per-spective, the feature engineering used for AEAC and OGPCC only leads to performance gains when used as a test-time adaptation method. This shows that some of the feature engineering techniques used in Kaggle competitions actually serve the purpose of test-time adaptation. For three datasets, ranking among the top 1% on the leaderboard was not achieved without test-time adaptation. Our results indicate that simply comparing approaches to the Kaggle leaderboard, as done in previous studies [25, 87], is insufficient. Techniques like test-time adaptation are frequently used in Kaggle competitions and limit comparability to approaches that don't use the test data. Hence, a fair model comparison to expert solutions using the Kaggle leaderboard can only be ensured under controlled conditions through implemented expert solutions such as our pipelines.\nModels in real-world applications are often applied to non-i.i.d. tabular data. By definition, TTA should only be effective if the data violates the i.i.d. assumption and contains distribution shifts to adapt to. Indeed, the data collection process likely happened over time for most of the datasets used in our framework. However, timestamps were not always provided as the competitions were conceptualized as static tabular data tasks. Therefore, most of the datasets were also used in at least one comparative study for tabular data, although non-i.i.d. was a criterion for exclusion (e.g., SVPC, AEAC, and PSSDP in [30], SCTP and OGPCC in [47], or MBGM in [35]). Our results show, that despite treating datasets as static, the samples remain non-i.i.d. and approaches like test-time adaptation can improve performance. Furthermore, there is evidence that other datasets treated as i.i.d. in related work actually have a temporal nature. I.e., Kohli et al. [47] found that the most frequently used dataset in tabular data research is the forest cover type dataset [12]. At the same time, this dataset is used as a benchmark in online learning to measure the ability of models to adapt to concept drifts [24, 73]. As models for tabular data assume the data to be i.i.d., most benchmarks for evaluating tabular general-purpose models either directly name the data being non-i.i.d. as an exclusion criterion [35, 29] or exclude data that requires special CV procedures [11], which leads to the same results. In contrast, our analysis of Kaggle competitions revealed that most tabular data competitions have temporal characteristics and that the best solutions for such datasets typically engineer time-invariant features and utilize tabular data models assuming the data to be i.i.d (i.e. [3]). We conclude that there might be a mismatch between current evaluation frameworks for tabular data in academia and the tabular data tasks practitioners were interested in getting solved through ML competitions on Kaggle."}, {"title": "Implications for Future Work", "content": "We challenged the prevalent model-centric evaluation setups in tabular data research by comparing evaluations with standardized preprocessing pipelines to evaluations with expert preprocessing pipelines. We have shown that current research is overly model-centric, while tabular datasets often require dataset-specific feature engineering or violate the i.i.d. assumption the models are based on. This reveals important insights and directions for future work in Machine Learning for tabular data.\nDinstinguish between AutoML and model comparisons. Our findings highlight that standardized evaluation setups do not necessarily ensure fair model comparisons. In standardized preprocessing setups, models are treated as AutoML solutions, whereas in real-world applications, they are compo-nents of highly dataset-specific pipelines. Comparative studies should differentiate between datasets that benefit from known feature engineering steps and those that are self-sufficient. One approach could be to separate raw data benchmarks from fully preprocessed benchmarks, as done in our study. Standardized setups are more suitable for benchmarking AutoML solutions, while feature-engineered setups might be better for benchmarking models. Future research could emphasize incorporating dataset-specific (expert) preprocessing pipelines into benchmarks. However, gathering high-quality expert solutions at a large scale is tedious and may require a community effort.\nNeed for external performance references. Our analysis shows that evaluations without consider-ing the highest achievable performance on a task don't actually measure the state-of-the-art. Despite numerous benchmarks, there is no established standard to measure progress. A benchmark with a public leaderboard and a dynamic collection of meaningful and unsolved real-world datasets could facilitate progress.\nImprove feature engineering capabilities of models for tabular data. Researchers developing general-purpose models should recognize the impact of feature engineering on model performance. CatBoost has advanced the field by automating feature engineering on categorical data. However, significant feature engineering effort is still necessary for datasets where this is not the only challenge. Future work should take a data-centric perspective and focus on automating successful feature engineering techniques through novel architecture components. Our expert feature engineering pipelines can serve as a starting point for evaluating and developing new methods. Furthermore, unlike previously claimed [35], categorical features can indeed be an important challenge for deep learning models. Hence, future work could focus on improvements over classic embeddings for categorical features in general-purpose deep learning architectures.\nMethods for tabular data with temporal characteristics. Our analysis highlights the importance of distribution shifts in real-world tabular data, even when treating a task as static. Future work could investigate test-time adaptation methods specifically for tabular data, using our datasets and the identified test-time feature engineering techniques as baselines. Furthermore, our findings indicate that the current research focus on static i.i.d. data might hinder the development of techniques to handle weak temporal correlations in tabular data. Future work should focus on developing models with inductive biases for tabular data with temporal characteristics.\nAlign tabular benchmarks with practitioners needs. We have shown that models developed for tabular data are often applied to datasets with temporal characteristics, while existing tabular data benchmarks are overly focused on i.i.d. data. General-purpose tabular benchmarks should consider including tabular datasets with temporal characteristics instead of excluding them. Furthermore, a benchmark solely for tabular datasets with temporal characteristics could significantly advance model development for this relevant data problem."}, {"title": "Datasets and Expert Solutions", "content": "In this Section, we provide more details on the dataset/competition selection process and the expert solutions implemented in our framework. Table 4 shows the name of all Kaggle competitions included in our framework as well as a hyperlink for the implemented expert feature engineering."}, {"title": "Dataset Selection", "content": "The main paper already provides an overview of the main characteristics of the selected datasets and our main selection criteria. In this Subsection we further explain the selection criteria and summarize the excluded datasets."}, {"title": "Implemented Components of Expert Solutions", "content": "In this Subsection, we document the components of our framework that were directly derived from expert solutions.\nTask conceptualization in data loading. For all datasets, the data loading includes merging tables, defining the target, and defining categorical features. For some datasets, we incorporated parts of expert solutions into the task conceptualization as a part of the data-loading function:\n\u2022 mercedes-benz-greener-manufacturing: The index is used as a numeric feature as it was necessary to score top leaderboard ranks.\n\u2022 santander-value-prediction-challenge: 1) The target is marked as heavy-tailed to be trans-formed in the standardized preprocessing pipeline. 2) There was a data leak allowing to derive the test targets for some samples. The top expert solutions used these samples for data augmentation. Hence, we also moved these samples from the test to the training dataset, s.t. this leak is not an issue for any of our pipelines.\n\u2022 homesite-quote-conversion: Extract weekday from datetime feature.\n\u2022 porto-seguro-safe-driver-prediction: Replace -1 with nan.\nFeature Engineering Pipelines. For each expert solution, we extract the data preparation, which mostly consisted of feature engineering. The expert solutions of the datasets contained the following feature engineering operations:\n\u2022 mercedes-benz-greener-manufacturing: Addition of binary features, logical_and of binary features, sum of multiple binary features, feature selection\n\u2022 santander-value-prediction-challenge: {max, mean, min, median, first nonzero, last nonzero, no. of nans, no. of unique values} of groups of multiple features. The groups mostly consisted either of 40 or 99 features. Three groups were formed with 4991, 991, and 4000 features. The groups were previously determined based on expert knowledge. However, all operations to obtain the new features could theoretically be learned solely from the train data, and no timestamps are given explicitly.\n\u2022 amazon-employee-access-challenge: (normalized) groupby interactions, 2- and 3-order cate-gorical interactions, (normalized) frequency encoding, frequency encoding of interactions, log of frequency features, drop constant features\n\u2022 otto-group-product-classification-challenge: tSNE features, PCA features, KMeans centroid features\n\u2022 santander-customer-satisfaction: a few data cleaning steps, Remove highly correlated and constant features, remove features with <4 target=1 instances, count of value 0/3/6/9 in a row, percentile rank of feature A within feature B (considered a special kind of groupby interaction), ratios, (X mod 3) == 0, KMeans features with 2-11 clusters, binary feature separating population based on different other feature values\n\u2022 bnp-paribas-cardif-claims-management: 2- and 3-order categorical interactions, Convert numerical to categorical by rounding, 2-order Arithmetic combinations, 11-order categorical interaction, out-of-fold target encoding\n\u2022 santander-customer-transaction-prediction: replacing values that are unique in train data (added test for test-time adaptation) with the mean of the feature, Extract categorical features from numeric. Features have four (five if test data used) categories: 1) value appears at least another time in data with target==1 and no 0, 2) value appears at least another time in data with target==0 and no 1, 3) value appears at least two more time in data with target==0 & 1, 4) value is unique in data (if test-time adaptation: 5) value is unique in data + test)\n\u2022 homesite-quote-conversion: sum NAs in a row, sum of zeros in a row, two-order categorical interaction\n\u2022 ieee-fraud-detection: feature selection, normalize \"time deltas\" from some point in the past (Feature 1 (F1)-Feature 2 (F2)/(24*60*60)), frequency encoding (train & test), label encode categoricals, groupby interactions (mean, std, count), 2-way categorical interactions, (F1 - floor(F2), F1(cat) + ascat(floor(F2)-F3) - is not used directly, but for more aggregations), abs(F1-F2)>3, use cat features as numeric\n\u2022 porto-seguro-safe-driver-prediction: Feature selection, sum of missing values, frequency encoding of high-order interaction of categorical features, only for tree-based models: one-hot-encoding of categorical features, only for neural networks: train XGBoost models with one group of features as input and another feature as output - use the out-of-fold predictions as features\nFor the ieee-fraud-detection competition the winning solution found that once one transaction of a customer is a fraud in the train dataset - all are. They deal with that by implementing a postprocessing function labeling all customers as a fraud whenever one of the transactions is a fraud. As this pattern could also be learned by models, we decided to treat this aspect as part of the expert preprocessing pipeline.\nThe treatment of categorical features is left to the models whenever possible. The utilized encoding is listed as part of the expert feature engineering pipeline for datasets where the categorical data treatment was crucial for high performance. Operations that add new features based on existing categorical features (e.g., frequency encoding) are always considered part of the expert preprocessing pipeline, even though models like CatBoost use this information natively. Similarly, we remove the treatment of missing values from the expert pipelines and leave that to the respective models whenever possible.\nThe following feature engineering techniques were applied most frequently over all datasets: groupby interactions (4), two-order categorical interactions (3), feature selection (3), categorical frequency encoding (3), dimensionality reduction (2), three-order categorical interaction (2), 2-order arithmetic interactions (2), sum of missing values in a row (2), and sum of zeros in a row (2). Details on all implemented techniques for the datasets can be found in our code.\nFeature Engineering Techniques used for test-time adaptation. Of the abovementioned feature engineering techniques, the following were utilized as test-time feature engineering techniques:\n\u2022 Counts of categorical features (AEAC, IFD, PSSDP)\n\u2022 Dimensionality reduction (OGPCC, SCS)\n\u2022 Groupby interactions (SCS, IFD)\n\u2022 Occurrence of numeric values from train data in the test data (SCTP)\n\u2022 Model-based Denoising/Smoothing by training an XGBoost model to predict features and using out-of-fold predictions as features (PSSDP)\nCross-validation procedures. We used the same CV split type for each dataset as the expert solutions but unified the number of folds across all our datasets. We used 10 folds for most datasets, as this worked well for all datasets. We used 10-fold cross-validation for all but one dataset. For classification tasks, the folds were stratified across the target. For the IFD dataset, we split the data based on the month the data was collected, which resulted in six folds. For faster training, fewer folds could be used for large datasets with similar results. For large datasets, most expert solutions used fewer folds, e.g., for the PSSDP competition."}, {"title": "Discussion on Test-Time Feature Engineering", "content": "To deal with distribution shifts, test-time adaptation is a conceptual framework where the model parameters are allowed to depend on the test sample x but not on its unknown label y. This matches the common ML competition setup, where test samples are given, but the target is hidden. We found that successful participants in Kaggle competitions often use the test data for feature engineering. Hence, we established that using test data for feature engineering in Kaggle competitions can be considered a special kind of test-time adaptation. This subsection discusses when this practice can be considered for real-world applications and when it is an unfair and unrealistic setup for a task.\nWe argue that the common ML setup allowing for test-time adaptation corresponds to a frequent real-world application scenario where 1) The data to predict arrives in batches, 2) No real-time predictions are required, and 3) Retraining the model at test time is feasible. The first criterion is important as the employed test-time feature engineering techniques (e.g. dimensionality reduction and frequency encoding) often required the presence of many test samples at once. It is unclear whether this kind of domain adaptation would work per sample. The second and third criteria are necessary as test-time feature engineering always requires retraining the model, which is infeasible in online applications. In contrast, test-time feature engineering is not applicable in applications where online predictions are required, the number of test samples is small, or not retrainable (e.g., large-scale) models are used. Importantly, while test-time feature engineering might be infeasible in such applications, other test-time adaptation techniques might still apply. One example of such a task conceptualization amenable to test-time feature engineering is product return prediction, where samples are collected over a day, and a (lightweight) model can be retrained daily. In this scenario, using the test data in an unsupervised fashion for better adaptation to possible distribution shifts is feasible. After examining the application scenarios of the tasks in our framework, we found that most of them, like customer transaction prediction (SCTP) and customer satisfaction prediction (SCS), would allow such a setup, although likely with smaller amounts of test data than used for the competitions. Furthermore, our discussion in Subsection 4.4 reveals that many tabular datasets not in our scope have temporal components and thus may be amenable to test-time feature engineering."}, {"title": "Experimental Details", "content": "In this Section, we discuss all aspects of our experiments that are not a part of our proposed evaluation framework but rather are design choices we made for our experiments."}, {"title": "Software and Hardware", "content": "The deep learning models, CatBoost, and XGBoost, were trained using one or more of the following GPU hardware, depending on the availability: NVIDIA H100, NVIDIA A100, NVIDIA RTX A6000, or NVIDIA A40. LightGBM and AutoGluon were trained using the following CPU hardware: Intel(R) Xeon(R) CPU E2640v2 @ 2,00 GHz; Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz."}, {"title": "Model-Specific Preprocessing", "content": "For the tree-based models, model-specific preprocessing only included the correct assignment of datatypes to categorical features. For the deep learning models, the preprocessing was defined in line with the related work [34, 35]. For regression, the target is normalized to zero mean and unit variance. For numeric features, missing values are replaced with the mean, and the features are normalized using ScikitLearn's QuantileTransformer [68]. Categorical features are ordinally encoded as ResNet, FTTransformer, and MLP-PLR use embeddings for categorical features. The GRANDE library includes its own preprocessing, which contains the same steps as for the other deep learning models but uses leave-one-out-encoding for categorical features. For AutoGluon, all the preprocessing is left to the AutoML framework."}, {"title": "Model Training and Hyperparameter Optimization", "content": "We use the optuna library [4] for hyperparameter optimization. Each model is optimized for 100 trials with the first 20 trials being random search trials and the remaining 80 using the multivariate Tree-structured Parzen Estimator algorithm [10", "best_quality": "reset configuration and a time limit of 10 hours. Everything else is left to the AutoML library itself. We try to use default hyperparameters and tuning ranges that have been shown to```json\n{"}, {"title": "A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data", "authors": ["Andrej Tschalzev", "Sascha Marton", "Stefan L\u00fcdtke", "Christian Bartelt", "Heiner Stuckenschmidt"], "abstract": "Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing the performance of models typically consist of model-centric evaluation setups with overly standardized data preprocessing. This paper demonstrates that such model-centric evaluations are biased, as real-world modeling pipelines often require dataset-specific preprocessing and feature engineering. Therefore, we propose a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings are: 1. After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2. Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3. While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics.", "sections": [{"title": "Introduction", "content": "Since ancient times, tables have been used as a data structure, i.e., to record astronomical observations [82] or financial transactions [14]. Many traditional machine learning (ML) methods, like logistic regression or the first artificial neural networks, were initially developed for tabular data [22, 62, 72]. Even nowadays, in the age of AI, tabular data is the most prevalent modality in real-world applications, including medicine [41], finance [16], manufacturing [88], retail [57], and many others [75, 13]. Several novel deep learning architectures have been contributed in recent years to improve supervised machine learning for tabular data [69, 92, 61, 40, 7, 15, 34, 77, 19, 48, 33].\nTo evaluate existing approaches, various comparative studies were conducted in recent years [11, 31, 29, 34, 75, 13, 63]. While motivated by different goals, they all have one in common: The focus is on evaluating models on tabular datasets using predefined cross-validation splits and one standardized preprocessing for all datasets. In this paper, we challenge such model-centric evaluation setups by highlighting two major limitations (Section 2): 1) The evaluation setups are overly standardized and do not reflect the actual routine of practitioners, which typically includes dataset-specific feature engineering [81]. 2) There is no external reference for the highest possible performance on a task beyond a study's own reporting, which limits its reliability.\nTo address these issues, we advocate for shifting the research perspective in the tabular data field from model-centric to data-centric. Therefore, our main contribution is an evaluation framework that includes a collection of ten relevant real-world datasets, dataset-specific expert-level preprocessing pipelines, and an external measure of top performance for each dataset (Section 3). The datasets were carefully selected by screening Kaggle competitions involving tabular data, and, to our knowledge, our contribution represents the largest existing collection of implemented expert-level solutions for tabular datasets. To assess the potential bias from the first limitation, we investigate how the model comparison changes when considering dataset-specific preprocessing instead of standardized evaluation setups (Subsection 4.1). To address the second limitation, we use the leaderboard from Kaggle competitions as an external performance reference and reassess what is possible with modern methods that were not available when the Kaggle competitions took place (Subsection 4.2). We find that when considering dataset-specific expert preprocessing, performance differences between the best models shrink, and the importance of selecting the 'right' model diminishes. In addition, we dissect expert solutions for tabular data competitions and quantify the importance of different modeling components (Subsection 4.3). We find that measurable progress has been made in automating human effort, but feature engineering is still the most important aspect of many tabular data problems. No model fully automates this aspect and comparisons that don't consider feature engineering merely scratch the surface of the potential performance achievable on many datasets. This paper focuses on independent and identically distributed (i.i.d.) tabular data in line with related work. However, our analysis of Kaggle competitions shows strong evidence that this focus in the research community might not align with practitioners' needs. In particular, we find that many tabular data competitions on Kaggle have temporal characteristics (i.e., timestamp features) and we identify test-time adaptation (TTA) as an overlooked but important part of some supposedly static competitions (Subsection 4.4).\nOur findings indicate that current academic evaluation setups and benchmarks for tabular data are biased due to their overly model-centric focus. We conclude by discussing possible directions to improve machine learning for tabular data from a data-centric perspective (Section 5)."}, {"title": "Related Work", "content": "Machine Learning for tabular data. Unlike domains like computer vision and natural language processing, an established state-of-the-art neural network architecture does not exist for tabular data [75, 13]. Therefore, recent research has primarily concentrated on developing general-purpose deep learning models often inspired by architectures from other domains [69, 40, 52, 44, 7, 48, 92, 77, 37, 17, 83, 79, 90, 38, 65, 54, 85, 18, 19, 61, 33]. Despite these efforts, Gradient Boosted Decision Trees (GBDTs) remain the state-of-the-art, outperforming even the novel neural models in many studies [13, 35, 63]. This paper aims to motivate more research inspired by tabular data-specific techniques like feature engineering instead of architectures established in other domains.\nLimitations of current evaluation frameworks. Several benchmarks exist for evaluating tabular machine learning models, focusing on general model comparisons [11, 31, 29, 35, 63] and specific sub-problems [46, 28, 21, 74, 27]. However, these benchmarks do not provide preprocessing settings for the included datasets. Consequently, most studies adopt a fixed, standardized preprocessing for all datasets to concentrate on model comparisons [75, 34, 63, 35, 47]. While this model-centric approach is suitable for AutoML, it limits the real-world transferability of model comparisons, as models in practical applications typically follow dataset-specific preprocessing and feature engineering [81, 87, 39]. Our evaluation framework is the first to explicitly incorporate a more detailed distinction through diverse preprocessing pipelines. Furthermore, existing benchmarks lack an external reference (e.g., a leaderboard) for the current best task performance, hindering comparability across different studies. In contrast, we leverage datasets from ML competitions as an external benchmark for high performance on tasks. Many existing evaluation frameworks prioritize usability at the expense of representativeness by limiting sample sizes and removing high-cardinality categorical features, thus evaluating models on artificially constrained dataset versions [11, 35]. Our evaluation framework solely consists of tasks meaningful to the real world without imposing artificial restrictions on datasets. Finally, most evaluation frameworks concentrate on tasks where samples"}, {"title": "A Data-Centric Evaluation Framework for Tabular Machine Learning", "content": "We propose an evaluation framework built upon three crucial aspects that are often overlooked in tabular data research: 1) Evaluation on realistic datasets without removing frequently occurring challenging aspects like high cardinality categorical features, 2) Dataset-specific expert preprocessing pipelines, and 3) Evaluation against human expert performance on hidden test sets. Figure 1 depicts an overview of our framework. Our design choices are additionally justified by the fact that for each dataset, at least one model in our evaluation ranks among the top 1% of all competition participants."}, {"title": "Collection of Relevant and Challenging Datasets", "content": "We rely on the Kaggle community and competitions hosted by companies and institutions to select datasets with expert solutions. Figure 2 illustrates our dataset selection process, and Table 1 summa-rizes the main properties of the included datasets. Using data from Kaggle competitions has various benefits: 1) The selected tasks are challenging and meaningful to the real world, as companies and institutions only spend money on hosting competitions from which they benefit. 2) Each competition has a clear evaluation setup, including metrics selected to reflect the practitioners' needs. 3) Each competition has a large hidden test set, which has been shown to reduce the risk of adaptive overfitting [71]. 4) The competition leaderboard serves as an external reference for truly high performance, as many expert teams participated in the competitions. Furthermore, our framework ensures a fair comparison by including a data loading function for each dataset that removes potential side issues, like data leakage or faulty data. This distinguishes our framework from related work that compares approaches to Kaggle solutions [25, 87]. An important insight from screening the competitions is that most tabular datasets had temporal characteristics \u2013 i.e., datasets with weak temporal correlations that benefit from time-sensitive feature engineering but not from models with temporal inductive biases (i.e., [3]). This finding will be further discussed in Subsection 4.4."}, {"title": "Expert Solutions and Preprocessing Pipelines", "content": "Our proposed evaluation framework includes three preprocessing pipelines. One closely resembles the pipelines researchers currently use for model evaluation, and the other two are dataset-specific and directly derived from expert solutions. All preprocessing pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model and are explained in the Appendix.\nStandardized Preprocessing The main purpose of this pipeline in our framework is to evaluate single models in a scenario with minimal dataset-specific human effort invested. Continuous missing values are replaced with the mean, and missing categorical feature values are treated as a new category. Furthermore, constant columns are removed, and heavy-tailed targets are log-transformed for regression tasks. As these preprocessing steps are almost universally applied across related work [34, 35, 63], this pipeline represents current evaluation setups in academia well.\nExpert Feature Engineering We select one high-performance expert solution from Kaggle for each dataset. The solution was chosen based on the private leaderboard rank and the descriptions' quality and sufficiency. For each solution, we separate the data preparation from the remaining parts of the solution. For most datasets, this pipeline solely consists of feature engineering techniques. Besides a few distinctions between tree-based and deep learning models, the pipelines are model-agnostic. Model-specific preprocessing steps are considered part of the model in our framework and are explained in the Appendix. This paper focuses on a pipeline perspective and does not discuss single feature engineering steps further. Implementation details and feature engineering techniques"}, {"title": "Modeling and Evaluation Framework", "content": "Modeling Pipeline and Models We implement a unified modeling pipeline for all datasets with a dataset-specific cross-validation (CV) ensembling procedure. The validation sets are used for early stopping and determining the best hyperparameters. The final test data predictions are an ensemble of averaging the test predictions of each fold. Our experiments compare 7 models and one AutoML solution for all datasets and preprocessing pipelines. We use three gradient-boosted tree libraries (XGBoost [20], LightGBM [43], and CatBoost [70]) because each was used in at least one of the expert solutions. Each expert solution that used neural networks developed a highly customized network for the particular competition. We want to assess whether recently developed general-purpose architectures can replace the high effort of building custom networks. Hence, we chose ResNet and FTTransformer [34] because they have been frequently used in recent benchmark comparisons and have shown strong performance [35, 63]. Because the Resnet essentially is an MLP with skip connections, it serves as a baseline representing what was already possible before the recent developments in DL for tabular data. In addition, we use two more recent approaches: MLP-PLR [32], which can help learn high-frequency functions, mitigating a major weakness of deep learning for tabular data [35]; and GRANDE [61], a recent representative of hybrid neural-tree models. We are aware that even more recent architectures exist. However, our focus is not on particular models but on datasets and preprocessing. To assess how well fully automated solutions perform without any preprocessing, we additionally evaluate AutoGluon, which has been shown to be the current best AutoML solution [30].\nHyperparameter Optimization Hyperparameter optimization is done per fold to obtain a diverse CV ensemble. Each model is evaluated in three HPO regimes: 1) Default: Either library default or hyperparameters suggested in related work, 2) Light HPO: 20 random search iterations. 3) Extensive HPO: 20 random search warmup iterations + 80 iterations of the tree-structured Parzen estimator algorithm [4]. More details on the hyperparameter optimization can be seen in the Appendix.\nEvaluation We use the Kaggle API to automatically submit predictions and retrieve performance results after evaluating against the hidden targets. Each dataset is evaluated on the metric specified by the competition host. Instead of reporting this metric directly, we report the solution's private leaderboard position as the percentile. This has the benefit that although different metrics are used to evaluate the model, comparisons across datasets are possible. In the Appendix, we additionally report performances on the actual metrics for each dataset. Throughout the paper, higher values represent a better performance (leaderboard position)."}, {"title": "Experimental Evaluation", "content": "Our framework allows us to assess the dataset-specific individual performance impact of model selection, hyperparameter optimization, feature engineering, and test-time adaptation. As a general overview, Figure 3 shows how each of the analyzed modeling components improves over the default baseline for each model and dataset. The results demonstrate the importance of an external performance reference: If we only considered the standardized evaluation setup (blue/orange/green bars), we would only be scratching the surface of achievable task performance for many data sets."}, {"title": "How Model Comparisons Change When Considering Dataset-specific Preprocessing", "content": "Through the implemented modeling pipelines, it becomes possible to evaluate how model com-parisons change when evaluating inside the typically used standardized pipelines vs. expert pipelines with and without test-time adaptation. Three observations stand out when evaluating models in different preprocessing pipelines (Figure 4). 1) The model rankings change considerably, as indicated by the relatively low Spearman coefficients between the standardized preprocessing pipeline and the other pipelines. 2) The performance gap between all models diminishes when considering expert preprocessing. On average, all models benefit from feature engineering, and multiple models can reach top performance. While all models benefit from TTA, the performance increase varies. 3) The superiority of CatBoost vanishes when considering dataset-specific preprocessing. The reason is that CatBoost already incorporates specific feature engineering steps in its algorithm for which other models need manual engineering, as we will further elaborate in Subsection 4.3."}, {"title": "Measurable Progress Through Recent Efforts", "content": "Figure 5 shows the model ranking on the private Kaggle leaderboard when trained after stan-dardized preprocessing. CatBoost achieves top ranks in three competitions (MBGM, BPCCM, HQC) where a high manual effort in feature engineering was previously necessary. Similar to Erickson et al. [25], AutoGluon achieves top ranks in two of these (BPCCM, HQC) and one additional competition (OGPCC). Regarding neural networks, novel architectures outper-form the ResNet baseline on nine datasets and even outperform tree-based solutions for three datasets (SVPC, SCS, SCTP). All neural net-works originally used in the competitions were custom-designed for the particular competition. Hence, our analysis confirms that meaningful progress has been made in developing general-purpose architectures for tabular data. Although the progress in the tabular data field is clearly visible, top performance cannot be reached with-out human effort for six datasets."}, {"title": "Feature Engineering is Still the Most Important Factor for Top Performance", "content": "The most remarkable performance gains are achieved through feature engineering. Figure 6 shows that expert feature engineering is the most important modeling component on average. This holds true for all models, indicating that un-like for modalities like imaging, neural networks do not automate feature engineering for tabular data. When comparing the performance of dif-ferent models in the standardized preprocessing pipeline (blue/orange/green bars in Figure 3), we can observe that using any other model than CatBoost rarely brings large gains. Only for the SCS dataset does FTTransformer clearly outper-form all other models. For all other datasets, the average performance gains achievable solely with model selection are small. Hence, our re-sults confirm the findings of McElfresh et al. [63] that model selection is less important than HPO on a strong tree-based baseline for most datasets. Furthermore, we extend this finding by quantifying the even more important aspect of dataset-specific feature engineering.\nFeature engineering is responsible for the high performance of CatBoost. Our analysis of different preprocessing pipelines reveals that CatBoost benefits much less from feature engineering than other models. The reason is that CatBoost incorporates explicit feature engineering techniques in its learning procedure. In particular, counts and target-based statistics are used to generate encodings for categorical features, and combinatorial encoding methods capture categorical feature interactions [70]. When considering the same feature engineering techniques for the other models, the gap to CatBoost drastically shrinks for most models, and XGBoost performs similarly to CatBoost on average. Hence, CatBoost's success in recent benchmarking studies [63, 28] can, at least to some extent, be attributed to feature engineering."}, {"title": "The Importance of Test-Time Adaptation and Temporal Characteristics", "content": "Test-time feature engineering con-sistently improves the performance of single models.Table 3 shows that test-time feature engineering leads to performance gains over solely using the train data for feature engineering for all datasets. From the task per-spective, the feature engineering used for AEAC and OGPCC only leads to performance gains when used as a test-time adaptation method. This shows that some of the feature engineering techniques used in Kaggle competitions actually serve the purpose of test-time adaptation. For three datasets, ranking among the top 1% on the leaderboard was not achieved without test-time adaptation. Our results indicate that simply comparing approaches to the Kaggle leaderboard, as done in previous studies [25, 87], is insufficient. Techniques like test-time adaptation are frequently used in Kaggle competitions and limit comparability to approaches that don't use the test data. Hence, a fair model comparison to expert solutions using the Kaggle leaderboard can only be ensured under controlled conditions through implemented expert solutions such as our pipelines.\nModels in real-world applications are often applied to non-i.i.d. tabular data. By definition, TTA should only be effective if the data violates the i.i.d. assumption and contains distribution shifts to adapt to. Indeed, the data collection process likely happened over time for most of the datasets used in our framework. However, timestamps were not always provided as the competitions were conceptualized as static tabular data tasks. Therefore, most of the datasets were also used in at least one comparative study for tabular data, although non-i.i.d. was a criterion for exclusion (e.g., SVPC, AEAC, and PSSDP in [30], SCTP and OGPCC in [47], or MBGM in [35]). Our results show, that despite treating datasets as static, the samples remain non-i.i.d. and approaches like test-time adaptation can improve performance. Furthermore, there is evidence that other datasets treated as i.i.d. in related work actually have a temporal nature. I.e., Kohli et al. [47] found that the most frequently used dataset in tabular data research is the forest cover type dataset [12]. At the same time, this dataset is used as a benchmark in online learning to measure the ability of models to adapt to concept drifts [24, 73]. As models for tabular data assume the data to be i.i.d., most benchmarks for evaluating tabular general-purpose models either directly name the data being non-i.i.d. as an exclusion criterion [35, 29] or exclude data that requires special CV procedures [11], which leads to the same results. In contrast, our analysis of Kaggle competitions revealed that most tabular data competitions have temporal characteristics and that the best solutions for such datasets typically engineer time-invariant features and utilize tabular data models assuming the data to be i.i.d (i.e. [3]). We conclude that there might be a mismatch between current evaluation frameworks for tabular data in academia and the tabular data tasks practitioners were interested in getting solved through ML competitions on Kaggle."}, {"title": "Implications for Future Work", "content": "We challenged the prevalent model-centric evaluation setups in tabular data research by comparing evaluations with standardized preprocessing pipelines to evaluations with expert preprocessing pipelines. We have shown that current research is overly model-centric, while tabular datasets often require dataset-specific feature engineering or violate the i.i.d. assumption the models are based on. This reveals important insights and directions for future work in Machine Learning for tabular data.\nDinstinguish between AutoML and model comparisons. Our findings highlight that standardized evaluation setups do not necessarily ensure fair model comparisons. In standardized preprocessing setups, models are treated as AutoML solutions, whereas in real-world applications, they are compo-nents of highly dataset-specific pipelines. Comparative studies should differentiate between datasets that benefit from known feature engineering steps and those that are self-sufficient. One approach could be to separate raw data benchmarks from fully preprocessed benchmarks, as done in our study. Standardized setups are more suitable for benchmarking AutoML solutions, while feature-engineered setups might be better for benchmarking models. Future research could emphasize incorporating dataset-specific (expert) preprocessing pipelines into benchmarks. However, gathering high-quality expert solutions at a large scale is tedious and may require a community effort.\nNeed for external performance references. Our analysis shows that evaluations without consider-ing the highest achievable performance on a task don't actually measure the state-of-the-art. Despite numerous benchmarks, there is no established standard to measure progress. A benchmark with a public leaderboard and a dynamic collection of meaningful and unsolved real-world datasets could facilitate progress.\nImprove feature engineering capabilities of models for tabular data. Researchers developing general-purpose models should recognize the impact of feature engineering on model performance. CatBoost has advanced the field by automating feature engineering on categorical data. However, significant feature engineering effort is still necessary for datasets where this is not the only challenge. Future work should take a data-centric perspective and focus on automating successful feature engineering techniques through novel architecture components. Our expert feature engineering pipelines can serve as a starting point for evaluating and developing new methods. Furthermore, unlike previously claimed [35], categorical features can indeed be an important challenge for deep learning models. Hence, future work could focus on improvements over classic embeddings for categorical features in general-purpose deep learning architectures.\nMethods for tabular data with temporal characteristics. Our analysis highlights the importance of distribution shifts in real-world tabular data, even when treating a task as static. Future work could investigate test-time adaptation methods specifically for tabular data, using our datasets and the identified test-time feature engineering techniques as baselines. Furthermore, our findings indicate that the current research focus on static i.i.d. data might hinder the development of techniques to handle weak temporal correlations in tabular data. Future work should focus on developing models with inductive biases for tabular data with temporal characteristics.\nAlign tabular benchmarks with practitioners needs. We have shown that models developed for tabular data are often applied to datasets with temporal characteristics, while existing tabular data benchmarks are overly focused on i.i.d. data. General-purpose tabular benchmarks should consider including tabular datasets with temporal characteristics instead of excluding them. Furthermore, a benchmark solely for tabular datasets with temporal characteristics could significantly advance model development for this relevant data problem."}, {"title": "Datasets and Expert Solutions", "content": "In this Section, we provide more details on the dataset/competition selection process and the expert solutions implemented in our framework. Table 4 shows the name of all Kaggle competitions included in our framework as well as a hyperlink for the implemented expert feature engineering."}, {"title": "Dataset Selection", "content": "The main paper already provides an overview of the main characteristics of the selected datasets and our main selection criteria. In this Subsection we further explain the selection criteria and summarize the excluded datasets."}, {"title": "Implemented Components of Expert Solutions", "content": "In this Subsection, we document the components of our framework that were directly derived from expert solutions.\nTask conceptualization in data loading. For all datasets, the data loading includes merging tables, defining the target, and defining categorical features. For some datasets, we incorporated parts of expert solutions into the task conceptualization as a part of the data-loading function:\n\u2022 mercedes-benz-greener-manufacturing: The index is used as a numeric feature as it was necessary to score top leaderboard ranks.\n\u2022 santander-value-prediction-challenge: 1) The target is marked as heavy-tailed to be trans-formed in the standardized preprocessing pipeline. 2) There was a data leak allowing to derive the test targets for some samples. The top expert solutions used these samples for data augmentation. Hence, we also moved these samples from the test to the training dataset, s.t. this leak is not an issue for any of our pipelines.\n\u2022 homesite-quote-conversion: Extract weekday from datetime feature.\n\u2022 porto-seguro-safe-driver-prediction: Replace -1 with nan.\nFeature Engineering Pipelines. For each expert solution, we extract the data preparation, which mostly consisted of feature engineering. The expert solutions of the datasets contained the following feature engineering operations:\n\u2022 mercedes-benz-greener-manufacturing: Addition of binary features, logical_and of binary features, sum of multiple binary features, feature selection\n\u2022 santander-value-prediction-challenge: {max, mean, min, median, first nonzero, last nonzero, no. of nans, no. of unique values} of groups of multiple features. The groups mostly consisted either of 40 or 99 features. Three groups were formed with 4991, 991, and 4000 features. The groups were previously determined based on expert knowledge. However, all operations to obtain the new features could theoretically be learned solely from the train data, and no timestamps are given explicitly.\n\u2022 amazon-employee-access-challenge: (normalized) groupby interactions, 2- and 3-order cate-gorical interactions, (normalized) frequency encoding, frequency encoding of interactions, log of frequency features, drop constant features\n\u2022 otto-group-product-classification-challenge: tSNE features, PCA features, KMeans centroid features\n\u2022 santander-customer-satisfaction: a few data cleaning steps, Remove highly correlated and constant features, remove features with <4 target=1 instances, count of value 0/3/6/9 in a row, percentile rank of feature A within feature B (considered a special kind of groupby interaction), ratios, $(X \\text{mod} 3) == 0$, KMeans features with 2-11 clusters, binary feature separating population based on different other feature values\n\u2022 bnp-paribas-cardif-claims-management: 2- and 3-order categorical interactions, Convert numerical to categorical by rounding, 2-order Arithmetic combinations, 11-order categorical interaction, out-of-fold target encoding\n\u2022 santander-customer-transaction-prediction: replacing values that are unique in train data (added test for test-time adaptation) with the mean of the feature, Extract categorical features from numeric. Features have four (five if test data used) categories: 1) value appears at least another time in data with target==1 and no 0, 2) value appears at least another time in data with target==0 and no 1, 3) value appears at least two more time in data with target==0 & 1, 4) value is unique in data (if test-time adaptation: 5) value is unique in data + test)\n\u2022 homesite-quote-conversion: sum NAs in a row, sum of zeros in a row, two-order categorical interaction\n\u2022 ieee-fraud-detection: feature selection, normalize \"time deltas\" from some point in the past (Feature 1 (F1)-Feature 2 (F2)/(24*60*60)), frequency encoding (train & test), label encode categoricals, groupby interactions (mean, std, count), 2-way categorical interactions, (F1 - floor(F2), F1(cat) + ascat(floor(F2)-F3) - is not used directly, but for more aggregations), abs(F1-F2)>3, use cat features as numeric\n\u2022 porto-seguro-safe-driver-prediction: Feature selection, sum of missing values, frequency encoding of high-order interaction of categorical features, only for tree-based models: one-hot-encoding of categorical features, only for neural networks: train XGBoost models with one group of features as input and another feature as output - use the out-of-fold predictions as features\nFor the ieee-fraud-detection competition the winning solution found that once one transaction of a customer is a fraud in the train dataset - all are. They deal with that by implementing a postprocessing function labeling all customers as a fraud whenever one of the transactions is a fraud. As this pattern could also be learned by models, we decided to treat this aspect as part of the expert preprocessing pipeline.\nThe treatment of categorical features is left to the models whenever possible. The utilized encoding is listed as part of the expert feature engineering pipeline for datasets where the categorical data treatment was crucial for high performance. Operations that add new features based on existing categorical features (e.g., frequency encoding) are always considered part of the expert preprocessing pipeline, even though models like CatBoost use this information natively. Similarly, we remove"}, {"title": "Discussion on Test-Time Feature Engineering", "content": "To deal with distribution shifts, test-time adaptation is a conceptual framework where the model parameters are allowed to depend on the test sample x but not on its unknown label y. This matches the common ML competition setup, where test samples are given, but the target is hidden. We found that successful participants in Kaggle competitions often use the test data for feature engineering. Hence, we established that using test data for feature engineering in Kaggle competitions can be considered a special kind of test-time adaptation. This subsection discusses when this practice can be considered for real-world applications and when it is an unfair and unrealistic setup for a task.\nWe argue that the common ML setup allowing for test-time adaptation corresponds to a frequent real-world application scenario where 1) The data to predict arrives in batches, 2) No real-time predictions are required, and 3) Retraining the model at test time is feasible. The first criterion is important as the employed test-time feature engineering techniques (e.g. dimensionality reduction and frequency encoding) often required the presence of many test samples at once. It is unclear whether this kind of domain adaptation would work per sample. The second and third criteria are necessary as test-time feature engineering always requires retraining the model, which is infeasible in online applications. In contrast, test-time feature engineering is not applicable in applications where online predictions are required, the number of test samples is small, or not retrainable (e.g., large-scale) models are used. Importantly, while test-time feature engineering might be infeasible in such applications, other test-time adaptation techniques might still apply. One example of such a task conceptualization amenable to test-time feature engineering is product return prediction, where samples are collected over a day, and a (lightweight) model can be retrained daily. In this scenario, using the test data in an unsupervised fashion for better adaptation to possible distribution shifts is feasible. After examining the application scenarios of the tasks in our framework, we found that most of them, like customer transaction prediction (SCTP) and customer satisfaction prediction (SCS), would allow such a setup, although likely with smaller amounts of test data than used for the competitions. Furthermore, our discussion in Subsection 4.4 reveals that many tabular datasets not in our scope have temporal components and thus may be amenable to test-time feature engineering."}, {"title": "Experimental Details", "content": "In this Section, we discuss all aspects of our experiments that are not a part of our proposed evaluation framework but rather are design choices we made for our experiments."}, {"title": "Software and Hardware", "content": "The deep learning models, CatBoost, and XGBoost, were trained using one or more of the following GPU hardware, depending on the availability: NVIDIA H100, NVIDIA A100, NVIDIA RTX A6000, or NVIDIA A40. LightGBM and AutoGluon were trained using the following CPU hardware: Intel(R) Xeon(R) CPU E2640v2 @ 2,00 GHz; Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz."}, {"title": "Model-Specific Preprocessing", "content": "For the tree-based models, model-specific preprocessing only included the correct assignment of datatypes to categorical features. For the deep learning models, the preprocessing was defined in line with the related work [34, 35]. For regression, the target is normalized to zero mean and unit variance. For numeric features, missing values are replaced with the mean, and the features are normalized using ScikitLearn's QuantileTransformer [68]. Categorical features are ordinally encoded as ResNet, FTTransformer, and MLP-PLR use embeddings for categorical features. The GRANDE library includes its own preprocessing, which contains the same steps as for the other deep learning models but uses leave-one-out-encoding for categorical features. For AutoGluon, all the preprocessing is left to the AutoML framework."}, {"title": "Model Training and Hyperparameter Optimization", "content": "We use the optuna library [4", "26": ".", "56": ".", "best_quality": "reset configuration and a time limit of 10 hours. Everything else is left to the AutoML library itself. We try to use default hyperparameters and tuning ranges that have been shown to"}]}]}