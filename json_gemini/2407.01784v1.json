{"title": "Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language Models with Paraphrase Enrichment", "authors": ["Kota Shamanth Ramanath Nayak", "Leila Kosseim"], "abstract": "This paper describes our approach to hierarchical multi-label detection of persuasion techniques in meme texts. Our model, developed as a part of the recent SemEval task, is based on fine-tuning individual language models (BERT, XLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition to dataset augmentation through paraphrase generation from ChatGPT. The scope of the study encompasses enhancing model performance through innovative training techniques and data augmentation strategies. The problem addressed is the effective identification and classification of multiple persuasive techniques in meme texts, a task complicated by the diversity and complexity of such content. The objective of the paper is to improve detection accuracy by refining model training methods and examining the impact of balanced versus unbalanced training datasets. Novelty in the results and discussion lies in the finding that training with paraphrases enhances model performance, yet a balanced training set proves more advantageous than a larger unbalanced one. Additionally, the analysis reveals the potential pitfalls of indiscriminate incorporation of paraphrases from diverse distributions, which can introduce substantial noise. Results with the SemEval 2024 data confirm these insights, demonstrating improved model efficacy with the proposed methods.", "sections": [{"title": "1 Introduction", "content": "The recent SemEval-2024 shared task 4 [1] proposed three distinct subtasks dedicated to identifying persuasion techniques conveyed by memes. The primary aim was to unravel how memes, integral to disinformation campaigns, employ various techniques to shape user perspectives. Subtask 1 focused on the analysis of textual content alone and mandated the detection of 20 persuasion techniques structured hierarchically within the textual content of memes. On the other hand, subtasks 2 and 3 involved the analysis of multimodal memes that considered both textual and visual elements.\nThis paper describes the approach we used for our participation to subtask 1 and further analysis with the dataset after the shared task. The task provided a train-ing dataset in English, but in addition to being tested on English, the task also"}, {"title": "2 Previous Work", "content": "Several studies have explored multi-label classification of textual content. In 2019, [4] showed that Bi-GRUs with label-wise attention led to good performance, and the inclusion of domain-specific Word2vec and context-sensitive ELMo embeddings fur-ther boosted the performance on the EURLEX57K dataset that contained 57k En-glish EU legislative documents. [5] introduced five innovative contrastive losses for multi-label text classification using the dataset from the SemEval 2018 Multi-label Emotion Classification (MEC) task [6] in English, Arabic, and Spanish that con-tained 8640 instances. All five contrastive learning methods notably enhanced the performance of the previous top-performing model, SpanEmO [7], for the MEC task. Among these approaches, the Jaccard Similarity Probability Contrastive Loss demonstrated the highest effectiveness on the English dataset, achieving FMacro and FMicro scores of 57.68 and 71.01, respectively.\nIn hierarchical multi-label classification (HMC), samples are assigned to one or more class labels within a structured hierarchy. Approaches to HMC can be divided"}, {"title": "3 Labelling Persuasion Techniques", "content": "The goal of our model was to categorize the textual content of memes into one or several persuasion techniques. For example, given the training instance shown in Figure 1, the model needs to learn that the text Don't expect a broken government to fix itself should be labelled with the three techniques provided in the labels field.\nPersuasion Techniques: The SemEval organizers provided an inventory of 20 per-suasion techniques to be used as labels (eg: Loaded Language, Slogans, Name call-ing/Labelling) and were structured hierarchically as shown in Figure 2. This ren-dered the task a hierarchical multi-label classification problem and was therefore evaluated using hierarchical precision, recall and F measures.\nDatasets: The dataset provided contained memes collected from online public groups discussing a variety of topics such as politics, vaccines, COVID-19, gen-der equality, and the Russo-Ukrainian War. For our task, only the text extracted"}, {"title": "4 Proposed Approach", "content": "Figure 5 shows an overview of the classification pipeline we employed for this task. As shown in Figure 5, our methodology is based on fine-tuning three distinct pre-trained language models: BERT [13], XLM-RoBERTa [14], and mBERT [13] on augmented datasets."}, {"title": "4.1 Multi-label Classification", "content": "As Figure 5 shows, the data is first preprocessed using standard tokenization. Then we proceeded to fine-tune three distinct models: bert-base-uncased, xlm-roberta-base, and bert-base-multilingual-uncased which returned a probability distribution over the 20 techniques. These three model predictions were then pooled via aver-aging.\nDespite the hierarchical organization of the persuasion techniques, we opted to predicting solely the technique names (leaf nodes in Figure 2) and not their ancestor nodes. However, to address the multi-label classification, we implemented"}, {"title": "4.2 Data Augmentation", "content": "To mitigate the lack of data we took advantage of two data augmentation strategies: an external dataset and automatically generated paraphrases.\nExternal Dataset (Comb-14k dataset): The Technique Classification (TC) sub-task from the SemEval 2020 Task 11 [11] provided a dataset of 7k instances from the news domain annotated with similar guidelines as this year's. In contrast to\nthe 2020 task, this year's dataset covered a different domain and used a revised set of persuasion techniques compared to the 2020 inventory. Indeed, in the 2020 TC dataset, a few techniques were merged into a single category due to lack of data, resulting in a list of 14 techniques. In the current year, an expanded inventory of 20 techniques was employed. To ensure consistency between the two sets, we pre-processed the 2020 TC dataset by splitting techniques that had previously been merged. For example, we singled out Bandwagon and Reductio ad Hitlerum, which had been merged into a single technique in the SemEval 2020 TC dataset.\nWe considered two approaches to leverage the modified 2020 TC dataset. The ini-tial option involved pre-training models on this dataset, followed by fine-tuning on the 2024 training data-an approach implemented by [3]. Another approach en-tailed combining both datasets and fine-tuning models on this combined dataset. We chose the latter method because the two datasets covered different genres and a joint training approach would likely enable the model to better adapt and grasp nuanced linguistic patterns across both. For easy reference in the rest of the pa-per, we call the combined dataset Comb-14k. Figure 7(a) (gray + pink) shows the resulting distribution of the persuasion techniques in this combined dataset.\nParaphrasing (Para-* datasets) Despite having almost doubled each class with the use of the 2020 TC dataset, some classes were still severely underrepresented; see Figure 7(a) (gray + pink). To address this, we took advantage of an LLM to gen-"}, {"title": "5 Experimental Setup", "content": "5.1 System Pipeline and Training Details\nThe system pipeline code was implemented in PyTorch. The pre-trained models BERT [bert-base-uncased], XLM-ROBERTa [xlm-roberta-base], and mBERT [bert-base-multilingual-uncased] and their tokenizers were sourced from Hug-ging Face. All models were trained for 10 epochs using the Adam optimizer with a learning rate of 2e-5. Batch sizes varied with BERT utilizing 128, and XLM-ROBERTa and mBERT using 64. A final feedforward layer with 20 logits (equal to the number of persuasion techniques) was added to each model. The Binary Cross Entropy with logits served as the loss function, with one-hot encoding applied to"}, {"title": "6 Results and Analysis", "content": "For our official submission to SemEval, the Para-Bal dataset had not been created yet; hence our official results are based on the ensemble model trained on the union of Para-n3 and the development set (1k samples), for a total of 53k samples. The three surprise languages were Bulgarian, North Macedonian and Arabic. The test set contained 1500 samples for English, 426 samples for Bulgarian, 259 samples for North Macedonian and 100 samples for Arabic. The official results of our system are shown in Figure 8, along with a baseline score that assigns the most frequent persuasion technique to all instances, and the score obtained by the best performing systems for each language [16,17]. As Figure 8 shows, although our ensemble model did not reach the top performance for English (0.57 versus 0.75), it performed better than the baseline in all languages except Arabic, where the improvement was not significant.\nUsing the same testing protocol, we reproduced the results using the model trained with the balanced training dataset (Para-Bal). The results displayed in Figure 8 indicate an improvement in score with the English test set (0.62 versus 0.57). This again confirms the importance of a balanced dataset, and paraphrases"}, {"title": "7 Conclusion and Future Work", "content": "This paper described our approach to hierarchical multi-label detection of persua-sion techniques in meme texts. We used an ensemble model with three fine-tuned language models and incorporated data augmentation through paraphrasing from ChatGPT. We tested our approach through the SemEval 2024 Task 4 subtask 1 [1]. During testing, our system outperformed the baseline in all languages. Analysis of the results show the importance of dataset balancing and paraphrasing techniques in enhancing model performance. Despite having a smaller number of instances, the balanced dataset consistently outperforms its unbalanced counterparts, demon-strating the efficacy of balancing methods. Moreover, data augmentation improves model performance, as indicated by the under-performance of models trained on the original dataset (7k instances). Additionally, the results underscore the potential drawbacks of including paraphrases from diverse distributions, which may intro-duce significant noise into the system, potentially compromising overall effective-ness. This study prompts further inquiry into the specific drivers of performance improvement, whether it be dataset balancing or the inclusion of external data. Although our zero-shot approach exhibits limitations, it underscores the positive correlation between data volume and model performance, as illustrated by the supe-"}]}