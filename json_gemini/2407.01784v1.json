{"title": "Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language Models with Paraphrase Enrichment", "authors": ["Kota Shamanth Ramanath Nayak", "Leila Kosseim"], "abstract": "This paper describes our approach to hierarchical multi-label detection of persuasion techniques in meme texts. Our model, developed as a part of the recent SemEval task, is based on fine-tuning individual language models (BERT, XLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition to dataset augmentation through paraphrase generation from ChatGPT. The scope of the study encompasses enhancing model performance through innovative training techniques and data augmentation strategies. The problem addressed is the effective identification and classification of multiple persuasive techniques in meme texts, a task complicated by the diversity and complexity of such content. The objective of the paper is to improve detection accuracy by refining model training methods and examining the impact of balanced versus unbalanced training datasets. Novelty in the results and discussion lies in the finding that training with paraphrases enhances model performance, yet a balanced training set proves more advantageous than a larger unbalanced one. Additionally, the analysis reveals the potential pitfalls of indiscriminate incorporation of paraphrases from diverse distributions, which can introduce substantial noise. Results with the SemEval 2024 data confirm these insights, demonstrating improved model efficacy with the proposed methods.", "sections": [{"title": "Introduction", "content": "The recent SemEval-2024 shared task 4 [1] proposed three distinct subtasks dedicated to identifying persuasion techniques conveyed by memes. The primary aim was to unravel how memes, integral to disinformation campaigns, employ various techniques to shape user perspectives. Subtask 1 focused on the analysis of textual content alone and mandated the detection of 20 persuasion techniques structured hierarchically within the textual content of memes. On the other hand, subtasks 2 and 3 involved the analysis of multimodal memes that considered both textual and visual elements.\nThis paper describes the approach we used for our participation to subtask 1 and further analysis with the dataset after the shared task. The task provided a training dataset in English, but in addition to being tested on English, the task also"}, {"title": "Previous Work", "content": "Several studies have explored multi-label classification of textual content. In 2019, [4] showed that Bi-GRUs with label-wise attention led to good performance, and the inclusion of domain-specific Word2vec and context-sensitive ELMo embeddings further boosted the performance on the EURLEX57K dataset that contained 57k English EU legislative documents. [5] introduced five innovative contrastive losses for multi-label text classification using the dataset from the SemEval 2018 Multi-label Emotion Classification (MEC) task [6] in English, Arabic, and Spanish that contained 8640 instances. All five contrastive learning methods notably enhanced the performance of the previous top-performing model, SpanEmO [7], for the MEC task. Among these approaches, the Jaccard Similarity Probability Contrastive Loss demonstrated the highest effectiveness on the English dataset, achieving $F_{Macro}$ and $F_{Micro}$ scores of 57.68 and 71.01, respectively.\nIn hierarchical multi-label classification (HMC), samples are assigned to one or more class labels within a structured hierarchy. Approaches to HMC can be divided"}, {"title": "Labelling Persuasion Techniques", "content": "The goal of our model was to categorize the textual content of memes into one or several persuasion techniques. For example, given the training instance shown in Figure 1, the model needs to learn that the text Don't expect a broken government to fix itself should be labelled with the three techniques provided in the labels field.\nPersuasion Techniques: The SemEval organizers provided an inventory of 20 persuasion techniques to be used as labels (eg: Loaded Language, Slogans, Name calling/Labelling) and were structured hierarchically as shown in Figure 2. This rendered the task a hierarchical multi-label classification problem and was therefore evaluated using hierarchical precision, recall and F measures.\nDatasets: The dataset provided contained memes collected from online public groups discussing a variety of topics such as politics, vaccines, COVID-19, gender equality, and the Russo-Ukrainian War. For our task, only the text extracted"}, {"title": "Proposed Approach", "content": "Figure 5 shows an overview of the classification pipeline we employed for this task. As shown in Figure 5, our methodology is based on fine-tuning three distinct pre-trained language models: BERT [13], XLM-RoBERTa [14], and mBERT [13] on augmented datasets."}, {"title": "Multi-label Classification", "content": "As Figure 5 shows, the data is first preprocessed using standard tokenization. Then we proceeded to fine-tune three distinct models: bert-base-uncased, xlm-roberta-base, and bert-base-multilingual-uncased which returned a probability distribution over the 20 techniques. These three model predictions were then pooled via averaging.\nDespite the hierarchical organization of the persuasion techniques, we opted to predicting solely the technique names (leaf nodes in Figure 2) and not their ancestor nodes. However, to address the multi-label classification, we implemented"}, {"title": "Data Augmentation", "content": "To mitigate the lack of data we took advantage of two data augmentation strategies: an external dataset and automatically generated paraphrases.\nExternal Dataset (Comb-14k dataset): The Technique Classification (TC) sub-task from the SemEval 2020 Task 11 [11] provided a dataset of 7k instances from the news domain annotated with similar guidelines as this year's. In contrast to\nthe 2020 task, this year's dataset covered a different domain and used a revised set of persuasion techniques compared to the 2020 inventory. Indeed, in the 2020 TC dataset, a few techniques were merged into a single category due to lack of data, resulting in a list of 14 techniques. In the current year, an expanded inventory of 20 techniques was employed. To ensure consistency between the two sets, we pre-processed the 2020 TC dataset by splitting techniques that had previously been merged. For example, we singled out Bandwagon and Reductio ad Hitlerum, which had been merged into a single technique in the SemEval 2020 TC dataset.\nWe considered two approaches to leverage the modified 2020 TC dataset. The initial option involved pre-training models on this dataset, followed by fine-tuning on the 2024 training data an approach implemented by [3]. Another approach entailed combining both datasets and fine-tuning models on this combined dataset. We chose the latter method because the two datasets covered different genres and a joint training approach would likely enable the model to better adapt and grasp nuanced linguistic patterns across both. For easy reference in the rest of the paper, we call the combined dataset Comb-14k.\nParaphrasing (Para-* datasets) Despite having almost doubled each class with the use of the 2020 TC dataset, some classes were still severely underrepresented; see Figure 7(a) (gray + pink). To address this, we took advantage of an LLM to gen-\nerate paraphrases for each training instance, then labeled these paraphrases with the same set of labels as the original instance. Our intuition was twofold. First, generating paraphrases would expose the model to a more extensive set of samples for each class, potentially improving its ability to discern subtle nuances within the data. Second, paraphrasing sentences could unveil hidden semantics, providing the model with a tool to identify propaganda techniques reliant on nuanced linguistic choices or phrasing. To generate paraphrases, we leveraged ChatGPT-3.5 turbo, setting the temperature to 0.7. This value aimed to introduce diversity in the paraphrases while maintaining relevance to the original instances. Several datasets were created using this method:\nPara-n1 and Para-n3: For each instance in Comb-14k, we generated n paraphrases. We experimented with n=1 and n=3 leading to datasets of 28k and 52k respectively, which we call Para-n1 and Para-n3 respectively.\nThe overall hierarchical F-score with the validation set given (500 instances) showed an increase when training with these datasets and n = 3 seemed to perform better than n = 1.\nHowever, a per-class analysis showed that not all classes benefited from the increase in support. For example, the persuasion technique Bandwagon increased its F1 from 0.17 to 0.29; whereas Repetition decreased its F1 from 0.56 to 0.31. We therefore identified the classes with improvement in F-score greater than 0.03 when using the Para-n3 dataset compared to the Comb-14k dataset. These 8 techniques along with their increase in F-scores are shown in Figure 6. This set of techniques formed the basis for our subsequent strategy.\nPara-Benef: Since only 8 techniques seemed to benefit from the use of paraphrases, we created a new augmented dataset by increasing the number of paraphrases\nonly for these techniques. Specifically, let B be the set of 8 techniques that benefited from paraphrases (see Figure 6), for all data instances d in Comb-14k labeled with techniques T = {$t_1$, $t_2$,...$t_n$} (where n \u2264 20), for each $t_i$ \u2208 B, we generated 10 paraphrases of d and labeled them with all techniques from T \\ B. This newly created dataset called Para-Benef, contained 54k instances.\nOur next dataset therefore tried to address this issue.\nPara-Bal: Our last dataset used our paraphrase generation strategy to address the dataset imbalance. We rectified the underrepresented classes in the initial training dataset by augmenting them with paraphrases. The most frequent three techniques Smears, Name-calling/Labelling, and Loaded Language had 1990, 1750, 1518 samples respectively. We thus aimed at reaching similar number of instances for the other techniques. We balanced the dataset by generating batches of 5 paraphrases for each other technique to reach around 1500 instances. This newly created dataset called Para-Bal contained 49k instances (see Figure 7(b))."}, {"title": "Experimental Setup", "content": "The system pipeline code was implemented in PyTorch. The pre-trained models BERT [bert-base-uncased], XLM-ROBERTa [xlm-roberta-base], and mBERT [bert-base-multilingual-uncased] and their tokenizers were sourced from Hugging Face. All models were trained for 10 epochs using the Adam optimizer with a learning rate of 2e-5. Batch sizes varied with BERT utilizing 128, and XLM-ROBERTa and mBERT using 64. A final feedforward layer with 20 logits (equal to the number of persuasion techniques) was added to each model. The Binary Cross Entropy with logits served as the loss function, with one-hot encoding applied to"}, {"title": "Results and Analysis", "content": "For our official submission to SemEval, the Para-Bal dataset had not been created yet; hence our official results are based on the ensemble model trained on the union of Para-n3 and the development set (1k samples), for a total of 53k samples. The three surprise languages were Bulgarian, North Macedonian and Arabic. The test set contained 1500 samples for English, 426 samples for Bulgarian, 259 samples for North Macedonian and 100 samples for Arabic. The official results of our system are shown in Figure 8, along with a baseline score that assigns the most frequent persuasion technique to all instances, and the score obtained by the best performing systems for each language [16,17]. As Figure 8 shows, although our ensemble model did not reach the top performance for English (0.57 versus 0.75), it performed better than the baseline in all languages except Arabic, where the improvement was not significant.\nUsing the same testing protocol, we reproduced the results using the model trained with the balanced training dataset (Para-Bal). The results displayed in Figure 8 indicate an improvement in score with the English test set (0.62 versus 0.57). This again confirms the importance of a balanced dataset, and paraphrases"}, {"title": "Conclusion and Future Work", "content": "This paper described our approach to hierarchical multi-label detection of persuasion techniques in meme texts. We used an ensemble model with three fine-tuned language models and incorporated data augmentation through paraphrasing from ChatGPT. We tested our approach through the SemEval 2024 Task 4 subtask 1 [1]. During testing, our system outperformed the baseline in all languages. Analysis of the results show the importance of dataset balancing and paraphrasing techniques in enhancing model performance. Despite having a smaller number of instances, the balanced dataset consistently outperforms its unbalanced counterparts, demonstrating the efficacy of balancing methods. Moreover, data augmentation improves model performance, as indicated by the under-performance of models trained on the original dataset (7k instances). Additionally, the results underscore the potential drawbacks of including paraphrases from diverse distributions, which may introduce significant noise into the system, potentially compromising overall effectiveness. This study prompts further inquiry into the specific drivers of performance improvement, whether it be dataset balancing or the inclusion of external data. Although our zero-shot approach exhibits limitations, it underscores the positive correlation between data volume and model performance, as illustrated by the superior performance of models trained on larger paraphrased datasets, such as Para-n3 with 56k instances compared to Para-Bal with 49k instances\nMoving forward, to boost performance, it would be interesting to measure the influence of the quality and similarity of the paraphrases on the performance. Moreover, incorporating hierarchical predictions, possibly at a second-level node, could improve scores further. Exploring the utilization of larger multilingual models alongside language-specific datasets and experimenting with various ensemble methods could be fruitful. Finally, considering the integration of adversarial training or self-supervised learning techniques might offer valuable avenues for improvement."}]}