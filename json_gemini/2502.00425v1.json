{"title": "MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization", "authors": ["JiangYong Yu", "Sifan Zhou", "Dawei Yang", "Shuo Wang", "Shuoyu Li", "Xing Hu", "Chen Xu", "Zukang Xu", "Changyong Shu", "Zhihang Yuan"], "abstract": "Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application. While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles with MLLMs because of (a) high inference latency from large visual token counts, (b) distributional disparities between visual and textual tokens, and (c) extreme outliers introduced by Hadamard-based transformations. To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs. textual tokens; \u2022 Attention-Invariant Flexible Switching (AIFS), reordering tokens to preserve casual attention while eliminating expensive token-wise scale computations; \u2022 Rotation Magnitude Suppression (RMS), mitigating weight outliers arising from online Hadamard rotations. On five mainstream MLLMs (including Qwen-VL, MiniCPM-V, CogVLM2), MQuant under W4A8 achieves near-floating-point accuracy (<1% degradation) while reducing inference latency by up to 30%, significantly outperforming existing PTQ baselines. Our MQuant effectively bridges the gap for efficient and accurate MLLMs inference in resource-constrained devices. code will be released.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a;b; Dubey et al., 2024) have led to remarkable performance on a wide range of natural language processing tasks. However, these models often struggle when dealing with non-textual data such as images or videos. Multimodal large language models (MLLMs) (Reid et al., 2024; Achiam et al., 2023b; Wang et al., 2023) address this limitation by integrating visual and textual modalities, thereby enabling more comprehensive understanding and reasoning. Despite these benefits, their large parameter sizes, coupled with substantial computational demands, poses a major challenge for real-world deployment, particularly in resource-constrained or privacysensitive environments."}, {"title": "Challenges in MLLM Quantization", "content": "Quantization has proven an effective strategy for reducing memory usage and inference costs in LLMs (Yuan et al., 2023a; Shao et al., 2023), by converting high-precision parameters (e.g., FP32) into lower-bit representations (e.g., INT8). Yet, the transition from LLMs to MLLMs brings three unique difficulties:\n1. Time-to-First-Token (TTFT) Explosion. MLLMs often generate large numbers of visual tokens (e.g., patch embeddings or region proposals) with the resolution and aspect ratio of input images or videos. As shown in Fig 1(a), in models such as Qwen2-VL (Wang et al., 2024), the number of prefill visual tokens grows as image resolution increases (detailed figure in Fig. 7). As image or video resolution increases, the prefill visual tokens can escalate dramatically-leading to high TTFT and negatively impacting latency-sensitive tasks. Moreover, in higher-demand scenarios such as video-based tasks and multi-image dialogues, the accumulation of visual tokens becomes even more pronounced, further exacerbating the increase in TTFT. Per-token dynamic quantization, though flexible, exacerbates this overhead by computing scale factors token-by-token.\n2. Disparate Modality Distributions. As shown in Fig 1 (b), the activation distributions between visual and textual tokens reveal substantial numerical discrepancies, where visual token activations can span a significantly"}, {"title": "Our Approach", "content": "To tackle these challenges, we propose MQuant, an accurate and efficient post-training quantization (PTQ) framework specifically tailored to MLLMs. Our contributions center around:\n1. Modality-Specific Static Quantization (MSQ) and Attention-Invariant Flexible Switching (AIFS), which apply distinct per-tensor scales for visual vs. textual tokens and reorder multimodal inputs to avoid repeated, token-wise scale computation. By doing so, MQuant slashes TTFT and preserves accuracy.\n2. Rotation Magnitude Suppression (RMS) for mitigating fresh outliers introduced by online Hadamard rotations. Our theoretical analysis reveals the emergence of large-mean channels in MLLM weights, and RMS effectively reduces these outliers with minimal overhead.\nExperimental Results and Impact. We evaluate MQuant on five mainstream MLLMs-InternVL (Chen et al., 2024a), Qwen-VL (Bai et al., 2023b), MiniCPM-V (Yao"}, {"title": "2. Preliminaries", "content": ""}, {"title": "2.1. MLLMs Architecture and Models", "content": "As shown in the Figure, the existing MLLMs framework (Bai et al., 2023b; Chen et al., 2024b) mainly consists of three modules: a vision encoder E for processing visual inputs, a visual-language projector P that serves as a bridge to align the two modalities and a large language model (LLM) that handles the multimodal tokens and performs reasoning.\nVision Encoder. Taking the input image or video X as input and compressing the original vision information into more compact patch features Fr. This process typically utilizes a Vision Transformer (ViT) (Dosovitskiy, 2021), such as CLIP (Radford et al., 2021) and OpenCLIP (Ilharco"}, {"title": "2.2. Post Training Quantization (PTQ)", "content": "PTQ serves as a potent strategy for model compression. By converting the high-precision variables of pre-trained models into low-bit integers, it achieves a reduction in memory usage and an acceleration of inference speed. For uniform quantization, given a floating-point (FP) tensor x (weights or activations), it can be uniformly quantized to b-bits in signed quantization as follows:\n$x = Q_u(x, b) = (clamp(\\frac{x}{s} + z, q_{min}, q_{max}) - z) \\cdot s$ (2)\nwhere $s = \\frac{max(|x|)}{[I_{min}, q_{max}]}$ is the scale factor, [\u00b7] is the rounding-to-nearest operator, and the function clamp(\u00b7) clips values outside the integer range [qmin, qmax]. z is zero-point. s denotes the quantization scale factor, which reflects the proportional relationship between FP values and integers. [Imin, qmax] is the quantization range determined by the bit-width b. Generally, when we quantize the network's weight with 4-bit and activations with 8-bit, called it as W4A8. We can calculate s offline using the activations from calibration samples, known as static quantization. We can also use the runtime statistics of activations to get s, referred to as dynamic quantization. More details are in Appendix A.12.\nExisting post-training quantization (PTQ) methods for LLMs are categorized into weight-only and weightactivation quantization (Zhao et al., 2023; Yuan et al., 2024; Shang et al., 2023; Yue et al., 2024; Hu et al., 2024b; 2025). Weight-only methods like GPTQ (Frantar et al., 2022), QuIP (Chee et al., 2024), and AWQ (Lin et al., 2023) achieve high compression rates but offer limited inference acceleration. In contrast, weight-activation quantization methods (Xiao et al., 2022; Wei et al., 2022; Yuan et al., 2023b; Zhang et al., 2024) quantize both weights and activations, improving memory usage and latency. The main challenge is activation outliers causing quantization errors. Techniques like SmoothQuant (Xiao et al., 2022) shift quantization difficulty from activations to weights, while OmniQuant (Shao et al., 2023) optimizes performance by training quantization parameters. SliceGPT (Ashkboos et al., 2024a) reduces memory demands by designing a Pre-LN + Rotate Scheme for LLMs sparsification based on computational invariance. They achieve this by adding a linear layer in the residual connection (see Appendix A.14). Unlike SliceGPT, we further develop a Post-LN + Rotate scheme to accommodate more vision encoder and extends its applicability to various MLLMs. This enhancement broadens the the LayerNorm + Rotate approach, making it suitable for both Pre-LN and Post-LN configurations across various MLLMs. Recently, Quarot (Ashkboos et al., 2024b) introduces rotations to eliminate outliers; however, this solution is not applicable to MLLMs due to inherent modality differences."}, {"title": "3. Method", "content": "In this section, we present MQuant, a post-training quantization solution specifically designed for MLLMs. In Sec. 3.1, we describe modality-specific static quantization (MSQ) and attention-invariant flexible switching (AIFS). In Sec. 3.2, we identify the weight outliers caused by the online Hadamard rotations and state Rotation Magnitude Suppression (RMS). We provide the detailed MQuant algorithm for FP MLLMs in Appendix A.1 Algorithm 1."}, {"title": "3.1. Modality-Specific Static Quantization and Attention-Invariant Flexible Switching", "content": "Motivation and Overview. Multi-modal Large Language Models (MLLMs) often generate a large number of visual tokens. As discussed in Section 1, the number of these tokens increases rapidly with higher image or video resolution (see Fig. 1(a)). This growth causes high inference latency when using per-token dynamic quantization, due to expensive token-wise scale computations. Such computations are not ideal for mobile or embedded devices. In contrast, static per-tensor quantization bypasses the need for repeated scale updates and offers much lower overhead. However, na\u00efvely applying per-tensor quantization to both text and visual tokens can degrade accuracy, because their activation distributions differ significantly (Fig. 1(b)).\nWe address this issue with a two-part method: (1) ModalitySpecific Static Quantization (MSQ) and (2) AttentionInvariant Flexible Switching (AIFS). MSQ applies different static scaling factors for visual and textual tokens, while AIFS reorders their positions to avoid irregular data slicing. Our method retains accuracy while benefiting from the efficiency of per-tensor quantization, especially for highresolution inputs.\nModality-Specific Static Quantization (MSQ). Let E be an input sequence of length L that intermixes textual and visual tokens. Denote E = {e1,...,em-1, em,..., en, en+1,...,eL}, where m, n specify the visual token segment. We observe that visual tokens often have larger activation magnitudes, which can overshadow textual features. To handle these differences, we apply two distinct sets of static per-tensor quantization"}, {"title": "3.2. FHT-Based Weight Outlier Mitigation: Rotation Magnitude Suppression", "content": "Background and Motivation. Chee et al al. (Chee et al., 2024) and Tseng et al. (Tseng et al., 2024) introduced incoherence to measure the difficulty of quantization. A lower incoherence indicates a simpler quantization scenario. Concretely, let $W \u2208 R^{m\u00d7n}$ be a weight matrix with singular vectors $e_i$ and $e_j$. They define\n$\\left|w_{i j}\\right|=\\left|e_{i}^{\\top} W e_{j}\\right| \\leq \\mu \\frac{\\|W\\|_{F}}{\\sqrt{m n}}$ (6)\nwhere $\\mu$ is the incoherence coefficient. The smaller $\\mu$ is, the easier it is to quantize $W$. They also showed that applying a Hadamard transform to both weights and activations can effectively reduce $\\mu$. Quarot (Ashkboos et al., 2024b) further applies offline Hadamard transforms and a partially online Hadamard transform, as shown in Figure 3(a), to achieve state-of-the-art quantization on LLMs.\nHowever, Multi-modal LLMs (MLLMs) combine an LLM component with a visual encoder, which often relies on LayerNorm. Quarot's partial online transform cannot be applied directly to all such norms. Inspired by SliceGPT (Ashkboos et al., 2024a), we convert visual encoders' LayerNorms into RMSNorms. More details can be found in the Appendix A.14. This change makes Quarot-like Hadamard transforms applicable to MLLMs. Yet, as Table 2 demonstrates, Quarot still underperforms on many MLLM tasks. Tseng et al. (Tseng et al., 2024) also proved that random"}, {"title": "Our Approach: Rotation Magnitude Suppression (RMS)", "content": "We propose Rotation Magnitude Suppression (RMS) to handle these new outliers with minimal overhead. We first identify whether a channel meets (Eq. 8). If it does, we:\n1. Split the problematic channel from the main GEMM kernel and process it using a separate GEMV.\n2. Zero out that row in $HWe_2$ so that the main kernel does not double-count it.\n3. Add the partial output (from the split GEMV) back to the main path before the activation.\nFigure 4 shows the workflow. This targeted split ensures large-mean channels do not trigger extreme first-row values during the per-forward-pass FHT. The added cost is small, since only channels meeting Eq. 8 require this procedure.\nWhy Not Subtract Channel Means Directly? A straightforward idea is to separate out each channel's mean before applying H and then re-inject it afterward. However, this leads to two major issues: 1. The separated means are still subject to Hadamard rotation, creating a new linear transformation where the first channel again becomes large. 2. Splitting and then re-injecting the means effectively doubles the linear operations, significantly increasing the computational cost. In contrast, our RMS approach modifies only the row triggering the outlier condition and does not require additional linear layers, thereby offering a more efficient and effective resolution.\nSummary. Algorithm 2 outlines how RMS integrates with Quarot's FHT or other partially online Hadamard transforms. In Table 12, we show that RMS substantially curbs weight outliers in MLLMs under W4A8 quantization, raising accuracy with minimal overhead. Hence, RMS addresses a key"}, {"title": "4. Experiments", "content": "Models and Datasets. We evaluate our MQUANT on five MLLMs: InternVL2-8B (Chen et al., 2024a), Qwen-VLChat-9.6B (Bai et al., 2023b), MiniCPM-V 2.6-8B (Yao et al., 2024), Qwen2-VL-7B (Wang et al., 2024), and GLM4V-9B (Hong et al., 2024). Evaluations are conducted on four benchmarks covering OCR and general question answering: TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2023), and MME (Fu et al., 2023), which assesses perception and cognition across 14 subtasks. These MLLMs' details are in Appendix A.11.\nBaselines and Implementation Details. We test W8A8 and W4A8 quantization settings for both visual encoders and LLMs, comparing RTN, SmoothQuant (Xiao et al., 2022), and Quarot (Ashkboos et al., 2024b). Notably, we"}, {"title": "4.1. Overall Results", "content": "Weight-activation quantization results of various MLLMs. MQuant can be applied to the quantization of various MLLMs. As shown in Table 2, our MQuant demonstrates significant improvements over several representative quantization methods. In W8A8 setting, MQuant achieves performance nearly equivalent to that of FP models across all evaluation datasets. Notably, even in the more challenging W4A8 setting, MQuant maintains comparable performance with FP models, while other advanced quantization methods exhibit significant performance degradation. These results indicate that our MQuant provide a general and effective PTQ solution with strong compatibility for maintaining high accuracy in MLLMs under various bits settings."}, {"title": "4.2. Latency and Memory Analysis", "content": "We measure the efficiency of MQuant in terms of speedup and memory usage under varying image resolutions, as well as the decode-stage acceleration introduced by AIFS. Unless otherwise noted, we adopt the common \"text-image-text\" input format (Duan et al., 2024) with 15 textual tokens, while the image size changes from 280 \u00d7 280 to 5600\u00d75600.\nSpeedup and Memory Savings. Overall Speedup As shown in Table 3, MQuant surpasses PyTorch BF16 and AWQ (W4-only) across all image resolutions, achieving up to 24.76% speedup over PyTorch at 840 \u00d7 840. Even at higher resolutions (e.g., 56002), MQuant maintains a"}, {"title": "4.3. Ablation Study", "content": "In this section, we select Qwen2-VL-7B (Wang et al., 2024), currently one of the most powerful open-source MLLMs, to ablate the effectiveness of our proposed designs.\nAccuracy & Speedup via AIFS. While MSQ addresses the modality discrepancy, AIFS further reorders visual and textual tokens into a unified sequence to enable efficient per-tensor static quantization of activations. Table 7 shows that MSQ+AIFS not only achieves the same speedup as na\u00efve per-tensor static quantization but also maintains nearfloating-point accuracy across linear layers. Figure 5 further illustrates that AIFS yields speedups of 20%-80% as resolution increases, corroborating that rearranging tokens avoids the high overhead of dynamic per-token quantization."}, {"title": "5. Conclusion", "content": "In this paper, we propose MQuant, an accurate and efficient post-training quantization (PTQ) framework specifically designed for MLLMs. Our approach addresses the unique chal"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. MQuant Algorithm", "content": "Here, we present our MQuant algorithm for MLLMs in Algorithm 1."}, {"title": "A.2. Rotary Position Embedding for Attention-Invariant Flexible Switching", "content": "Many modern LLMs (Touvron et al., 2023a;b; Dubey et al., 2024) use rotary position embedding (RoPE) (Su et al., 2021) to encode information about the order of tokens in the input sequence. Rotary position embeddings are linear transformations applied to keys and queries defined as:\n$R_{\\Theta}^{d i m}=\\left(\\begin{array}{ccccccc}\\cos i \\theta_{1} & -sin i \\theta_{1} & 0 & 0 & 0 & 0\\\\sin i \\theta_{1} & \\cos i \\theta_{1} & 0 & 0 & 0 & 0\\\\0 & 0 & \\cos i \\theta_{2} & -sin i \\theta_{2} & 0 & 0\\\\0 & 0 & sin i \\theta_{2} & \\cos i \\theta_{2} & 0 & 0\\\\: & : & : & : & : & :\\\\0 & 0 & 0 & 0 & \\cos i \\theta_{d n / 2} & -sin i \\theta_{d n / 2}\\\\0 & 0 & 0 & 0 & sin i \\theta_{d n / 2} & \\cos i \\theta_{d n / 2}\\end{array}\\right)$ (9)\nwhere i \u2208 [1, L] is the token index, $\u0398 = {\u03b8i = 10000^{-2(i-1)/D}}, i \u2208 [1, 2, ..., D/2]$, and $\u03b8_i, i \u2208 1..D/2$ are predefined constants.\nIn the proposed Attention-Invariant Flexible Switching (AIFS) mechanism, we also apply the rearrangement for position embedding to maintain the computation equivalence. For a mixed input token $E = {e1, ..., em, ..., en, ..., eL} \u2208 (Ev, Et)$ (in Eq 3), where m and n denote the start and end indices of the visual tokens. Specifically, after AIFS, the unified token can formulated as: $E = {em, ..., en, e1, ..., em\u22121, ..., eL} \u2208 (Ev, Et)$. Therefore, the unified token indices after AIFS can be represented as:\n$(m, ..., n, 1, ..., m - 1, n + 1, ..., L) = AIFS(1, ..., m, ..., n, ..., L)$ (10)\nDue to we are aware of the final indices for input token after AIFS, than we can utilize the reorder token indices for Eq 9 to get the corresponding position embedding."}, {"title": "A.3. Weights Outliers after Online Hadamard Transformation", "content": "Following (Tseng et al., 2024; Ashkboos et al., 2024b) we make use of fast Hadamard transformation where convenient. Hadamard matrix is an orthogonal matrix with entries proportional to {+1, -1}. A Walsh-Hadamard matrix is a square matrix of size $2^n$ with For a Hadamard matrix:\n$H_{2}=\\left[\\begin{array}{cc}1 & 1\\\\1 & -1\\end{array}\\right]$ and $H_{2 n}=H_{2} \\otimes H_{2^{n-1}}$. (11)\n$(A B)_{i j}=\\sum_{r=1}^{n} a_{i r} b_{r j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\\ldots+a_{i n} b_{n j}$ (12)\nThereby the $HWe_2$ can be formulated as:\n$\\left(H W_{l 2}\\right)_{i j}=\\sum_{r=1}^{n} h_{i r} W_{r j}=h_{i 1} W_{1 j}+h_{i 2} W_{2 j}+\\ldots+h_{i n} W_{n j}$ (13)\nwhere $H \u2208 R^{din\u00d7din}$ and $W_{l2} \u2208 R^{din\u00d7dout}$, din and dout is the dimension of input and output of weight $W_{l2}$. Due to the properties of the Hadamard matrix H, whose first row consists entirely of 1, for the first row in $(HW_{l2})$, $(HW_{l2})_{0j} = \u03a3_{r=1}^{n} W_{rj}$, due to the property of Hadamard matrix H. So, the values in $W_{l2}$ are subject to continuous accumulation and summation, resulting in the exists of outliers in the first row of the output matrix $HW_{\u21132}$. Notably, the matrix $HW_{\u21132}Q$ still has the same problem, for simplicity, we omit the matrix Q in the main paper."}, {"title": "A.4. Attention Mask in AIFS when Multi-batch Inference", "content": "During multi-batch inference, we first identify the longest token length within the batch. Subsequently, we left-pad the shorter sequences with padtokenid to align all batches to this maximum length. By applying left padding, the padding tokens are associated with the image modality. Additionally, the padded regions are assigned a mask value of 0, ensuring that they do not interfere with attention computations and thereby do not affect the final results. For clarity, we also plot an illustration of causal mask when batch size >1."}, {"title": "A.5. Effectiveness of Rotational Magnitude Suppression for Weight Outliers in LLMs", "content": "For LLMs, compared to the original Quarot, integrating RMS with Quarot leads to performance improvements across LLaMA2 models with 7B, 13B, and 70B parameters, as detailed in Table 10. For MLLMs, ablation studies presented in Table 8 demonstrate that the RMS method significantly enhances quantization performance."}, {"title": "A.6. Image Tokens in Various MLLMs", "content": "As shown in Fig 7, for different MLLMs (Bai et al., 2023b; Wang et al., 2024; Yao et al., 2024; Chen et al., 2024b), the number of prefill visual tokens grows"}, {"title": "A.7. RMS Algorithm", "content": "Here, we present the algorithm of our RMS design with Quarot in Algorithm 2."}, {"title": "A.8. Speedup and Memory Savings with Scaling Image Resolution", "content": "We fixed the input sequence as \"text-image-text\" with 15 textual tokens and presente the detailed changes of speed and memory, varying the image resolution from 280 \u00d7 280 to 5600 \u00d7 5600. Notably, the \"text-image-text\" sequence setting is not arbitrarily chosen; instead, it is a common setting in existing evaluation datasets (Duan et al., 2024). We evaluate"}, {"title": "A.9. Comparison of Online vs. Without Online Hadamard Transform in Quarot", "content": "In this section, we evaluate how online Hadamard transformations affect perplexity (PPL) in Quarot for different LLaMA2 models (7B and 13B) under two bit settings, W4A8KV4 and W4A4KV4. As shown in Table 12, enabling online Hadamard transforms yields consistent improvements, especially under more aggressive quantization (e.g., W4A4KV4).\nWe observe that online Hadamard transforms provide substantial gains when using W4A4KV4 for both 7B and 13B models,"}, {"title": "A.10. Advantage of Proposed MSQ and AIFS", "content": "In per-tensor static quantization, the quantization parameters (i.e., scale and zero-point) are precomputed for an entire tensor (e.g., weights or activations) and remain fixed throughout inference. While efficient, this approach often leads to large and unacceptable accuracy loss in MLLMs due to their diverse activation distributions across varying inputs.\nIn contrast, per-token dynamic quantization computes quantization parameters on-the-fly for each input token during inference. This approach incurs significantly higher computational overhead, as the quantization parameters must be recalculated for every input token, along with multiple additional memory traversals. Such requirements make per-token dynamic quantization unfriendly or impractical for edge devices and some AI accelerators, which struggle with finegrained dynamic operations (Tan et al., 2024). This issue is especially severe in MLLMs, where the token count increases significantly with higher image resolution or more video frames. The Modality-Specific Static Quantization (MSQ) in MQuant is a novel per-modality quantization approach specifically designed to address the unique challenges of MLLMs quantization.\nFurthermore, MSQ can be naturally applied to the unified modality-decoupled tokens generated by AIFS. By integrating MSQ and AIFS, our designs yields three key advantages: (1) Computational Equivalence and Strong Compatibility: The unified causal mask and token index introduced by AIFS preserves the inherent causal relationships among tokens, ensuring numerical equivalence during attention computations. Moreover, since AIFS requires only a one-time rearrangement of the input data (adjust causal mask and token index in offline), it does not alter the overall computation graph. This characteristic allows for seamless integration with other LLM inference acceleration methods, such as FlashAttention (Dao et al., 2022), ensuring both computational equivalence and strong compatibility. As shown in Table 2, MQuant achieves SOTA quantization performance across 5 mainstream MLLMs. (2) Reduced Inference Latency: MSQ not only addresses the substantial distributional differences between modalities but also mitigates the significant computational overhead and increased inference latency caused by the surge in token counts from higher image and video resolutions. As shown in Table 7, MSQ+AIFS significantly reduces latency from 2.057s to 1.1017s, closely matching the speed of the per-tensor static setting while maintaining near-lossless accuracy comparable to the original Float model. (3) Enhanced Memory and Computational Efficiency: By combining MSQ and AIFS, we convert mixed input tokens into unified, modalitydecoupled tokens, eliminating the irregular memory operations (e.g., slice, concat, pad) introduced by directly applying MSQ. This transformation reduces memory consumption and improves efficiency of GEMM kernel, which would otherwise be compromised by the interleaved and non-fixed positions of visual and textual tokens. As shown in Table 11, MQuant can achieve up to 24.7% speedup and 152.9% memory savings."}, {"title": "A.11. Comparison of Different MLLMs: Input Pre-Process, LayerNorm Architecture in Vision Encoder, Model Parameters and Flops", "content": "In this section, we compare the visual input pre-process methods, LayerNorm structures in MLLM vision encoder, model parameters, and Flops across five mainstream MLLMs: InternVL2-8B (Chen et al., 2024a), Qwen-VL-Chat-9.6B (Bai et al.,"}, {"title": "A.12. Quantization Granularity", "content": "Furthermore, as mentioned in SoomthQuant (Xiao et al., 2022), there are different different granularity levels. The per-tensor static quantization uses a single step size for the entire matrix. Per-token dynamic quantization employs different s for the activations associated with each token, being a common granularity for activations quantization of existing LLMs. For weights, per-channel quantization applies distinct s for each output channel of the weights, while group-wise quantization utilizes a coarse-grained s for different channel groups. Notably, group-wise quantization is a prevalent granularity for weight quantization in LLMs (Frantar et al., 2022; Yao et al., 2022). Please refer to appendix for more quantization basics."}, {"title": "A.13. LayerNorm, RMSNorm and Computational Invariance", "content": "We introduce LayerNorm, RMSNorm, the computational Invariance, and their usage in Transformers.\nLayer Normalization (LayerNorm, LN) (Ba, 2016) is a technique to normalize the activations of intermediate layers of neural networks. Given a vector $x \u2208 R^d$, LayerNorm normalizes it to obtain a zero-mean unit-variance vector,\n$LayerNorm(x) = \\frac{x - \u03bc(x)1}{\\sqrt{\\|x\\|_2^2/d - \u03bc^2(x) + \u03f5}}$, where $\u03bc(x) = \\frac{1^Tx}{d}$, $\u03f5 > 0$. (14)\nLayerNorm recenters and rescales the activations and gradients in the forward and backward computations, which enables fast and robust training of neural networks.\nRoot Mean Square Normalization (RMSNorm) (Zhang et al., 2019) is another technique used for normalizing the activations. It is similar to LayerNorm in that it aims to accelerate and stabilize the training but uses a different normalization approach. Instead of normalizing the inputs based on their mean and variance, RMSNorm normalizes them based on their root mean square (RMS) value. It is defined in the following equation,\n$RMSNorm(x) = \\frac{x}{\\sqrt{\\|x\\|_2^2/d + \u03f5}}$, where $\u03f5 > 0$. (15)\nRMSNorm only rescales the input vector and the corresponding gradients, discarding the recentering process. As shown in their definitions, RMSNorm is computationally simpler and more efficient than LayerNorm. It is reported that replacing LayerNorm with RMSNorm can achieve comparable performance and save training and inference time by 7% - 64% (Zhang et al., 2019).\nGiven a zero-mean vector x, these two kinds of normalization are equivalent. Formally, if $\u03bc(x) = 0$, then $LayerNorm(x) = RMSNorm(x)$. We may optionally introduce learnable parameters and apply an element-wise affine transformation on the output of LayerNorm and RMSNorm.\nLayerNorm (LN) and RMSNorm Given the input concated token X after embeddings with the shape L \u00d7 D, the X is passed through a LayerNorm (Ba, 2016) operation, which subtracts the mean from each row of the matrix, divides the"}, {"title": "A.14. LayerNorm to RMSNorm Transformation", "content": "Therefore, as shown in Figure 9 (b), we can replace LN as RMSNorm layer through two modifications: \u25cf recenter the input $X_k$ to $X_k \u2013 \u03bc(X_k)1$, ensuring that the input to norm layer maintain a zero mean. adjust the weights $A_2$ and bias $b_2$ of the the linear $l_2$ to $A_2 = A_2 - \\frac{1}{D}11^TA_2$, $b_2 = b_2 \u2212 \u03bc(b_2)1$. Consequently, the LN can be replaced with an RMSNorm layer with the same arithmetic functionality. The first operation is to recenter $X_k$, while the second operation is to recenter the output of main branches. Notably, since $X_{k+1} = X_k+l_2(g(l_1(LN(X_k))))$, after applying 1 and 2, the input and the output of main branch are re-centered with zero-mean, while the input of residual branches also maintain a zero mean. Therefore, the output after current blocks, $X_{k+1}$ (which serves as the input for next block), still maintain zero-mean. A detailed proof is provided in the Appendix. Ultimately, we establish the equivalence of Pre-LN and Pre-RMSNorm Transformers. Now that every LayerNorm in the transformer has been converted to RMSNorm in MLLMs, we can use any orthogonal matrices Q to the model. Therefore, the visual encoder and LLMs are in a rotation-friendly RMSNorm-only transformer architecture."}]}