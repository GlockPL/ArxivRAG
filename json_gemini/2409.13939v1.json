{"title": "Simple Unsupervised Knowledge Distillation With Space Similarity", "authors": ["Aditya Singh", "Haohan Wang"], "abstract": "As per recent studies, Self-supervised learning (SSL) does not readily extend to smaller architectures. One direction to mitigate this shortcoming while simultaneously training a smaller network without labels is to adopt unsupervised knowledge distillation (UKD). Existing UKD approaches handcraft preservation worthy inter/intra sample relationships between the teacher and its student. However, this may overlook/ignore other key relationships present in the mapping of a teacher. In this paper, instead of heuristically constructing preservation worthy relationships between samples, we directly motivate the student to model the teacher's embedding manifold. If the mapped manifold is similar, all inter/intra sample relationships are indirectly conserved. We first demonstrate that prior methods cannot preserve teacher's latent manifold due to their sole reliance on $L_2$ normalised embedding features. Subsequently, we propose a simple objective to capture the lost information due to normalisation. Our proposed loss component, termed space similarity, motivates each dimension of a student's feature space to be similar to the corresponding dimension of its teacher. We perform extensive experiments demonstrating strong performance of our proposed approach on various benchmarks.", "sections": [{"title": "1 Introduction", "content": "In recent years, the development of self-supervised learning (SSL) has allowed networks to be trained on larger datasets without labels, leading to generic representations that are task agnostic and achieve superior downstream performances once fine-tuned [14, 35, 44]. As a result, SSL is an active area of reasearch. For real-time inference, such as in the domain of autonomous driving, industrial automation etc. often small sized networks are deployed. However, these networks do not readily benefit from SSL due to their smaller number of parameters, which can hinder their ability to learn underlying discriminative representations effectively [18].\nTo address this issue, Fang et al. [18] propose an unsupervised knowledge distillation (UKD) framework called SEED that allows smaller networks to take"}, {"title": "2 Related Work", "content": "Feature based distillation approaches leverage the internal representations produced by the teachers and the students.\nMany novel approaches have incorporated internal layers, attention mechanisms etc. to match various novel objectives between student and the teacher [1, 24, 32, 36, 38, 58, 61, 73, 77]. However, as most of these approaches require careful selection of an appropriate statistic, it can be a drawback in practice for defining the distillation procedure for newer architectures. Many existing approaches"}, {"title": "2.1 Logit Based Distillation", "content": "Early solutions for distillation have been designed for a fully supervised teacher and leverage the output space of the teacher model. Soft-label distillation [29] is amongst the first work towards training smaller networks with guidance from larger teacher networks. Apart from supervised loss from the ground-truth labels, it minimises cross-entropy between the teacher's and the student's output logit distribution. Many methods since then have utilised the output logits to improve the knowledge distillation performance [10, 34, 43, 71, 78].\nHuang et al. [31] propose a correlation based formulation which is very similar to ours. The key difference apart from the teacher (supervised vs. unsupervised) is that they normalise the logits (via. softmax) and then compute inter and intra-class similarities. Whereas, we independently normalize spatial and feature dimensions of the embedding features. From the perspective of computing intra-class similarity, it is logical to apply the softmax beforehand for generating class-wise scores, however, when operating on the embedding space, any normalisation on the features alters the space information as well."}, {"title": "2.2 Feature Based Distillation", "content": "Feature based distillation approaches leverage the internal representations produced by the teachers and the students.\nMany novel approaches have incorporated internal layers, attention mechanisms etc. to match various novel objectives between student and the teacher [1, 24, 32, 36, 38, 58, 61, 73, 77]. However, as most of these approaches require careful selection of an appropriate statistic, it can be a drawback in practice for defining the distillation procedure for newer architectures. Many existing approaches"}, {"title": "2.3 Key Differences", "content": "Our proposed method is designed for unsupervised distillation, but we believe it stands out from existing methods in both supervised and unsupervised regimes, despite its simplicity. In particular, our approach focuses on directly motivating the student to learn its teacher's latent manifold. As a quantitative summary, COSS differs from many existing UKD methods in the absence of (i) feature queues, (ii) contrastive objectives, (iii) and, heavy augmentations."}, {"title": "3 Motivation", "content": "In deep learning, it is generally accepted that neural networks learn a mapping from high dimensional space to a low dimensional manifold. Moreover, assumption of a locally euclidean manifold in unsupervised learning is fundamental but rarely articulated.\nFor example, many unsupervised learning methods employ manifold learning based data visualisations[47, 79]. These manifold learning approaches assume the embedding manifold to be locally eucldiean[4, 62]. This assumption of the manifold being locally euclidean, allows us to treat the embedded manifold as a topological manifold. Here, we present a simple argument to show that methods solely relying on $L_2$ normalized features cannot learn the teacher's embedding structure reliably."}, {"title": "Definition 1", "content": "Two topological spaces $X, Y$ are homeomorphic if there exists a mapping $f : X \\rightarrow Y$ s.t. $f$ is continuous, bijective and its inverse function $f^{-1}$ is also continuous.\nHomeomorphism[54] defines the concept of similarity (equivalence) between the two topological spaces. For methods which only rely on normalized cosine similarity, the student's normalized manifold and the original teacher's manifold are not homeomorphic. This is because the operation of $L_2$ normalisation is not a homeomorphism. It is not continuous, bijective, and lacks a continuous inverse. A straight forward example to support it is that, after normalization, all points lying on the ray starting from the origin will be mapped onto the same point on a hypersphere. Hence, minimisation of an objective operating on the normalized space will not preserve the original un-normalized structure.\nExisting UKD methods of SEED, DisCo, and BINGO leverage the normalized features during their intermediate steps while SMD, SimReg and AttnDistill perform regression on the normalized feature as an additional training objective. Since, the normalisation erases the original structure, an alternative strategy to retain the spatial information is by preserving similarity along the spatial dimensions."}, {"title": "4 Method", "content": "Our motivation centers around the idea of imposing homeomorphism between the manifolds of a teacher and student. In order to achieve this, we propose a two step training method. In the first (offline) step, we compute k-nearest neighbours for the training samples. In the second stage, we perform distillation."}, {"title": "4.1 Offline Pre-processing", "content": "Typically, during training, a mini-batch is often composed of randomly selected samples without replacement from the training set $X = {X_1,X_2,...,X_N}$. The"}, {"title": "4.2 Training Objectives", "content": "Similar to the teacher, we define the student neural network as $f_s$ with the output dimension as $d_s$. If $d_s \\neq d_t$, we can use a small projection head for the student which can then be discarded after the distillation process [18, 20, 70]. We thus replace $d_t$ and $d_s$ by $d$.\nFirstly, for a mini-batch $B$, we append additional samples to it from $\\Omega_i^k$ $\\forall i = 1,2,......|B|$ and denote the updated mini-batch as $\\hat{B}$. We denote the embedding representations of all inputs in $\\hat{B}$ generated by the teacher and student as matrices $A_t$ and $A_s \\in R^{bkxd}$ respectively. Here, $A_i$ is the embedding output $f_.(x_i)$. We then compose a matrix of normalized feature vectors $A. = [\\hat{A}_0,\\hat{A}_1,...\\hat{A}_{bk}]^T$. The widely known normalized cosine similarity loss on features is defined as:\n$L_{co} = -\\frac{1}{bk} \\sum_{i=0}^{bk} cosine(\\hat{A}_i^t, \\hat{A}_i^s)$       (2)\nThe loss computes the cosine similarity between corresponding embeddings of the teacher and student thus, performing a pairwise alignment of features in the normalized embedding manifold. Due to its simplicity and effectiveness $L_{co}$ is employed by a number of existing UKD methods [42, 45, 64]."}, {"title": "", "content": "To define space similarity, we first construct the transpose matrix of features $Z. = A^T$ and its normalized version $\\hat{Z}.$. The space similarity loss is\n$L_{ss} = - \\frac{1}{d} \\sum_{i=0}^{d} cosine(\\hat{Z}_i^t, \\hat{Z}_i^s)$       (3)\nNote, minimizing the loss along the spatial dimension indeed imposes homeomorphism as the normalization here scales all data points identically. In case of minimum space alignment loss, $f_s(x_i) = \\frac{\\beta}{\\alpha} f_t(x_i)$ where $\u03b1$ and $\u03b2$ are scaling (normalization) vectors for the teacher's and student's dimensions respectively. The scaling components for dimensions are not required to be identical, hence, the alignment is similar for each dimension up to a scale. As $\u03b1 > 0$ and $\u03b2 > 0$, the mapping imposed by $L_{ss}$ between the corresponding projected data points is continuous, bijective and invertible.\nThe loss is also very simple to implement and requires one to transpose the feature matrices prior to the normalization. Our final objective is composed of weighted combination of Cosine similarity and Space Similarity:\n$L_{COSS} = L_{co} + \\lambda L_{ss}$        (4)"}, {"title": "5 Experiments", "content": "In this experiment, we investigate the efficacy of knowledge distillation models in the context of image retrieval. Oxford-5k is a widely used dataset for image retrieval. It consists of 55 query images and \u2248 5000 database images categorized into medium and hard based on the perceived difficulty of the task. FORB is a recently proposed dataset for image retrieval in the flat object setting. It consists of 8 categories with a total of 14,000 query images and 54,000 database images. The queries are labelled as easy, medium and hard based on the difficulty level. For Oxford-5k, we report the mean average precision for medium and hard instances. Whereas, for FORB, we report the overall mean average precision across easy, medium and hard categories. We use the ResNet-18 student distilled from Moco-v2 ResNet-50 for this evaluation."}, {"title": "5.1 Settings", "content": "Dataset: Following SEED, BINGO and others, we report top-1/5 and knn-10 classification accuracy on the ImageNet [13] dataset. For transfer learning, we utilise CIFARs[39], STL-10 [12], Caltech-101 [19], Oxford-IIIT Pets [50], Flowers [46] and DTD [11]. For dense prediction tasks, we employ the PASCAL VOC (trainval2007, trainval2012, test2007) [17] and MS-COCO (train2017, val2017) [41] datasets. For image retrieval experiments, we employ the Oxford-5k [53] and FORB [66] datasets. We utilize various ImageNet variants namely, ImageNet-v2 [55], ImageNet-Sketch [63] and ImageNet-C [28] to understand robustness of distilled networks.\nTeachers: Following SEED, we use the ResNet-50 model pretrained on ImageNet using Moco-v2 [7]. For dense prediction, following PCD, we use the ResNet-50 pretrained on ImageNet using Moco-v3 [8].\nStudents: We use ResNet-18, ResNet-34, and EfficientNet-B0 as the student architectures. They consist of 10.7M, 20.4M and 4M parameters respectively.\nDistillation: We discard the projection head of the teacher and work directly with the 2048 dimensional features from ResNet-50 and add a projection head on top of the students to match the final embedding dimensions. Using"}, {"title": "5.2 Supervised Classification", "content": "We evaluate the goodness of a UKD method by performing classification leveraging the embedding features from the trained student network. For the ResNet baselines of BINGO, SEED, and DisCo, we present findings as reported in their corresponding publications. Since BINGO does not provide results for Efficient-Net, we report the reproduced values for BINGO from the official implementation using recommended hyper-parameters. Regarding SMD, their evaluation involves a distinct teacher-student pairing; hence, we employ their official implementation and recommended hyper-parameters to conduct distillation using our selected teacher-student pair. As SimReg is distillation with $L_{co}$ objective without nearest neighbour sampling, we defer the comparison to the adblation sstudies in Sec. 5.8. We do not compare our work with AttnDistill as it is a solely vision transformer based approach which utilises extended training (up to 800 epochs).\nLinear Evaluation: Following the precedent set by SEED, we freeze the backbone and only learn the final linear classification layer. We use a batch size of 256 and an initial learning rates of 10 and 3 for ResNets and EfficientNet respectively. We perform this fine-tuning for 100 epochs with learning rate reduced by a factor of 10 at 60 and 90 epochs.\nAs per the results in Tab. 1, CoSS achieves significant improvements over the Moco-v2 trained (Student) baseline both for top-1 and top-5 classification accuracies. Compared to the Moco-v2 student, for top-1 accuracy, our method provides an improvement of 10%, 7% and 15% for ResNet-18, ResNet-34 and"}, {"title": "5.3 Transfer Learning", "content": "We employ the transfer learning benchmark [16] to evaluate the transferability of learned representations. In this evaluation, we keep the student backbone frozen and exclusively train the final classification layer. Ensuring fairness in evaluation, we conduct a thorough hyper-parameter sweep across various learning configurations to pinpoint the optimal set of values for each model across each dataset.\nAs depicted in Tab. 2, CoSS students exhibit superior transfer learning accuracies across a wide array of datasets for both ResNet-18 and EfficientNet-b0 architectures. It yields the best top-1 accuracies for 6 and 7 datasets for ResNet-18 and Efficient-B0 students respectively. On STL-10, it provides competitive performance to the baselines."}, {"title": "5.4 Dense Predictions", "content": "As previously introduced, PCD is a UKD method aimed at learning a student model focused on dense prediction tasks of object detection and image segmentation. For this evaluation, we follow the protocols as outlined by PCD. We utilize the student ResNet-18 distilled from a Moco-v3 ResNet-50 teacher [8]. The hyper-parameters for distillation remain consistent with those mentioned earlier (see Sec. 5.1). We use the Detectron2 [67] package for training and evaluation. The baseline results are directly sourced from PCD.\nOn VOC2007+2012 [17] we train C4 backbone with Faster RCNN [57] detector whereas for MS-COCO [41], we train C4 with a Mask R-CNN [27]. For"}, {"title": "5.5 Image Retrieval", "content": "In this experiment, we investigate the efficacy of knowledge distillation models in the context of image retrieval. Oxford-5k is a widely used dataset for image retrieval. It consists of 55 query images and \u2248 5000 database images categorized into medium and hard based on the perceived difficulty of the task. FORB is a recently proposed dataset for image retrieval in the flat object setting. It consists of 8 categories with a total of 14,000 query images and 54,000 database images. The queries are labelled as easy, medium and hard based on the difficulty level. For Oxford-5k, we report the mean average precision for medium and hard instances. Whereas, for FORB, we report the overall mean average precision across easy, medium and hard categories. We use the ResNet-18 student distilled from Moco-v2 ResNet-50 for this evaluation.\nTable 4 presents the results of the retrieval task. According to these results, COSS demonstrates improved mAP performance compared to the baseline models. For the few instances where CoSS is not the top performing model, it achieves a second best rating. These results suggest the potential effectiveness of the la-"}, {"title": "5.6 Out-of-Distribution Robustness", "content": "Various studies have indicated that deep learning systems often break down when encountered with data outside the training distribution[21, 28, 63]. Due to the wide spread applicability of deep learning systems in the real world, it becomes important to ensure a high degree of robustness of such systems. In this experiment we explore the robustness of trained students under different kinds of shifts in the input data. ImageNet-v2 is a natural image dataset which closely resembles the sampling process of the original ImageNet dataset. It has been employed by various studies to understand the robustness of models under natural data shift [48, 59]. ImageNet-S consists of sketch images for ImageNet classes. It is a stronger deviation from the types of images present in the training set. ImageNet-C is a synthetically generated dataset for evaluating a model's performance under various forms of corruptions. We utilise corruption=1 for our evaluation.\nResults reported in Tab. 5 demonstrate that CoSS is robust across various input distribution shifts. It even outperforms BINGO which employs stronger data augmentations in the form of CutMix [76]. Strong data augmentations such as CutMix have been shown to improve model robustness."}, {"title": "5.7 Qualitative Evaluation", "content": "In this experiment, we qualitatively demonstrate the lack of manifold modelling capability of SEED, DisCo and SimReg($L_{co}$) with a set of toy experiments. We"}, {"title": "5.8 Ablation Study", "content": "Importance of Loss Components: We compare the results of distillation with individual components with the combined objective of CoSS. We report results with distillation of a Moco-v2 ResNet-50 to a ResNet-18 and EfficientNet-b0 student. The hyper-parameters are those provided in Section 5.2. Results in"}, {"title": "6 Discussion", "content": "In this work, we address the challenging and critical problem of unsupervised knowledge distillation. While prior studies have predominantly focused on establishing distillation-worthy relationships among samples, our approach takes a distinct perspective by directly modelling the teacher's manifold. By doing so, we aim to indirectly preserve relationships between the samples.\nCentral to our approach is the utilization of space similarity to establish a form of homeomorphism between the projections of the student and teacher models. This homeomorphism ensures that the fundamental topological properties of the learned representations are preserved during the distillation process which otherwise are lost due to sole reliance on $L_2$ normalized embedding features. Homeomorphism, however, only ensures that the alignment of individual spatial dimensions is upto a scale. Moving forward, we aim to explore even stronger constraints on learning student topologies, which could potentially improve the effectiveness and robustness of our approach.\nOur experimental results demonstrate the effectiveness of CoSS in training student models that not only perform well on the training distribution but also deliver competitive performance on various downstream tasks. For instance, CoSS outperforms PCD on the COCO dataset for dense prediction tasks. Furthermore, we believe that integrating CoSS into existing frameworks, such as PCD, could further enhance the outcomes of dense prediction tasks, opening up new avenues for research and application.\nIn this study, our primary focus has been on the domain of computer vision. However, the rapid development of unsupervised large models in natural language processing presents an intriguing opportunity [22, 23, 56, 65]. Exploring the potential transference of our proposed method to this domain is a direction we leave for future exploration."}, {"title": "7 Conclusion", "content": "In this paper, we are inspired by the necessity to distill large models trained with self-supervised learning into smaller models. However, since the labels used to train these large models are typically unavailable, we investigate knowledge distillation in a purely unsupervised setting. In this setting, we demonstrate that unsupervised feature distillation can be achieved without the need to store a feature queue, and directly modelling the teacher's manifold."}, {"title": "A Adapting Supervised Methods", "content": "As many distillation approaches operate on the features of the students and teachers, they can be applied to an unsupervised setting. However, to the best of our knowledge, such a study has not yet been performed. In this experiment, we make slight adjustments to the baselines implemented by CRD [60]. We remove the final-classification layer of the teacher and the student to simulate an unsupervised setting. Moreover, we set the contributions due to ground-truth and soft-labels to 0. For a fair comparison, for the baselines, we report the best top-1 achieved from hyper-parameter sweep by scaling loss {0.5, 1, 2, 4, 8, 16, 32}x.\nIn Tab. 11, we report the results of baselines and CoSS. We observe that baseline methods don't readily work in an unsupervised setting. In the future, it will be interesting to delve deeper into understanding why such a wide gap in performance exists for distillation methods when applied to an unsupervised setting."}, {"title": "D Batch Normalisation For Space Similarity", "content": "Batch Normalisation (BN) was proposed to reduce the covariate shift which occurs during the mapping of inputs from one layer to another [33]. Since its introduction, BN has found place in numerous deep learning architectures[25, 26, 68]. Here, we show how one can directly aim the distillation process to match student and network's embeddings and subsequently compare its performance with our formulation.\nBN operates on a batch of input data, $X \\in R^{bxd}$ where batch size is $b$ and $d$ is the feature dimension. It first performs standardisation $\\hat{X}_{:,i} = \\frac{X_{:,i}-\\mu_i}{\\sigma_i}$ where, : denotes all the entries in the batch dimension and $\\mu_i$, $\\sigma_i$ are the mean and variances respectively for the ith feature dimension. The normalized values are then scaled by trainable parameters $\\gamma_i$ and $\\beta_i$ as:\n$\\hat{Z}_{:,i} = \\gamma \\hat{X}_{:,i} + \\beta$        (5)\nhere, $\\gamma_i$ and $\\beta_i$ can be interpreted as affine transformations which operate independently for different spatial dimensions. We can utilise it to map the standardised student embeddings to the teacher's unnormalized embedding space. The corresponding loss can be defined as follows:\n$L_{coss} = \\frac{1}{i<b} \\sum_{i=0} D(X_i^t, \\hat{X_i^s})$\nwhere, $X_i^t$ is the teacher's embedding for the $i^{th}$ sample and $\\hat{Z_i^s}$ corresponds to the BatchNormalized student's embeddings. In table 14, we report the results using this approach on CIFAR-100 distillation task. We utilised mean-squared-error for the metric D."}, {"title": "B Ablation on CIFAR-100", "content": "Following Tian et al. [60], we utilise the teacher models which were trained using supervision. We remove the final classification layer of the teacher to simulate an unsupervised setting. In Tab. 12, we report the results of distillation on CIFAR-100 dataset. We can observe that combining feature and space similarity yields the best performing student."}, {"title": "B.1 Importance of loss components", "content": "Following Tian et al. [60], we utilise the teacher models which were trained using supervision. We remove the final classification layer of the teacher to simulate an unsupervised setting. In Tab. 12, we report the results of distillation on CIFAR-100 dataset. We can observe that combining feature and space similarity yields the best performing student."}, {"title": "B.2 Contribution of A", "content": "For our main experiments, we used \u03bb = 1.0. Here, we evaluate the distillation performance with different weights assigned to the space similarity component. From the reported results in Tab. 13, we note that while combining space similarity and feature similarity yields the best-performing student, the optimal results are achieved with varying degrees of space similarity contribution. Different architectures generally demonstrate varied behaviors in response to distillation. It is not yet known why some architectures perform better than others, even when they have similar capacities. We believe that space similarity similarly impacts different architectures to a varying degree."}, {"title": "C SEED + Space Similarity", "content": "We train the SEED objective with our proposed space similarity objective (k = 0). We observe that the SEED + SS student achieves a top-1 of 58.30% compared to the baseline SEED model's 57.60. As we did not employ the nearest neighbour sampling, we expect the performance to further increase upon doing so."}]}