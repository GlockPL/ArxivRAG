{"title": "KLOTSKI: Efficient Mixture-of-Expert Inference via Expert-Aware Multi-Batch Pipeline", "authors": ["Zhiyuan Fang", "Yuegui Huang", "Zicong Hong", "Yufeng Lyu", "Wuhui Chen", "Yue Yu*", "Fan Yu", "Zibin Zheng"], "abstract": "Mixture of Experts (MoE), with its distinctive sparse structure, enables the scaling of language models up to trillions of parameters without significantly increasing computational costs. However, the substantial parameter size presents a challenge for inference, as the expansion in GPU memory cannot keep pace with the growth in parameters. Although offloading techniques utilise memory from the CPU and disk and parallelise the I/O and computation for efficiency, the computation for each expert in MoE models is often less than the I/O, resulting in numerous bubbles in the pipeline.\nTherefore, we propose KLOTSKI, an efficient MoE inference engine that significantly reduces pipeline bubbles through a novel expert-aware multi-batch pipeline paradigm. The proposed paradigm uses batch processing to extend the computation time of the current layer to overlap with the loading time of the next layer. Although this idea has been effectively applied to dense models, more batches may activate more experts in the MoE, leading to longer loading times and more bubbles. Thus, unlike traditional approaches, we balance computation and I/O time and minimise bubbles by orchestrating their inference orders based on their heterogeneous computation and I/O requirements and activation patterns under different batch numbers. Moreover, to adapt to different hardware environments and models, we design a constraint-sensitive I/O-compute planner and a correlation-aware expert prefetcher for a schedule that minimises pipeline bubbles. Experimental results demonstrate that KLOTSKI achieves a superior throughput-latency trade-off compared to state-of-the-art techniques, with throughput improvements of up to 85.12\u00d7.", "sections": [{"title": "1 Introduction", "content": "Owing to the rapid advancement of deep learning, large language models (LLMs) have demonstrated remarkable efficacy across various domains [4, 7, 46]. To facilitate model scalability without escalating the costs associated with training and inference, recent research has introduced sparsely activated Mixture-of-Experts (MoE) models [10, 33]. MoE models typically replace the Feed-Forward Network (FFN) layers with MoE layers. For each input, only a subset of the parameters (i.e., experts) are sparsely activated for computation, rather than all parameters, significantly reducing computational cost. Current research has demonstrated the superiority of the MoE architecture through extensive experiments [15, 30]. However, due to the skew between model parameter sizes and advances in hardware, MoE-based models, with their massive parameter counts, face more severe memory bottlenecks during inference than other LLMs. For example, DeepSeek-V2 [6], with 236 billion parameters, requires at least seven state-of-the-art (SOTA) GPUs (H100, with 80GB of memory each) for inference. Furthermore, the high cost of memory often makes it difficult to use such large models in more common environments such as personal computers and small servers, limiting the wider adoption of large models [8, 25, 40]. This raises the question of how to deploy MoE models in resource-constrained environments where there is a significant gap between available GPU memory and model parameter sizes.\nOffloading is one of the current mainstream solutions for addressing memory optimization during the inference of LLMs [9, 20, 32, 34]. It significantly reduces GPU memory requirements for LLM inference by offloading tensors not"}, {"title": "2 Background and Related Work", "content": "2.1 MoE Architecture & Inference\nSince GShard [22] introduced MoE structure into Transformer models, its potential to enhance the performance of large language models has been evident. MoE has gradually become one of the mainstream structures of large language models. Prominent models like GPT-4 [1], Gemini 1.5 [35], and Mixtral-8\u00d77B all incorporate the MoE structure.\nThe MoE architecture primarily consists of multiple MoE blocks, each containing an attention layer, an MoE layer, and two normalization layers, as illustrated in Figure 2. The MoE layer comprises a gating network and multiple experts. The gating network is the key feature of the MoE architecture. It uses a softmax function to calculate the routing weights of each expert, then activates the top-k experts. Existing research [23, 24, 43, 44] indicates that the expert activation path of each token can reveal its characteristics, facilitating the prediction of future expert selections. Each expert is an FFN, and experts are sparsely activated, making MoE a feasible approach to training larger models. For each token, the final output of the MoE layer is a weighted sum of the outputs from the selected experts.\nThe MoE inference process, like other LLMs, follows an autoregressive approach [36], generating each new token based on the previous ones, as illustrated in Figure 2. This process comprises two stages: prefill and decoding. In the prefill stage, the model processes the entire prompt simultaneously, which often leads to the activation of multiple experts. During the decoding stage, the model uses the previous token generated as input, iteratively generating new tokens until it generates the end-of-sequence (<EOS>) token or the maximum output length limit is reached."}, {"title": "2.2 Offloading in LLM Inference", "content": "LLMs often have a large number of parameters, causing severe GPU memory bottlenecks during inference. Common memory optimisation techniques include quantization [12, 21, 38], pruning [11, 26], sparse attention [39, 47], etc. Among these, offloading is a particularly effective strategy in resource-constrained environments. As shown in Figure 3, DRAM and disk often have at least dozens of times more memory than VRAM. When it is difficult to store all model parameters in VRAM (as in the red line), offloading strategies offload tensors not currently involved in computation to DRAM or disk, freeing up a significant amount of VRAM (as in the black lines). Consequently, offloading strategies allow LLM inference to be performed with extremely small memory footprints. However, because the I/O speed between VRAM and DRAM is slower than the GPU's computing speed, frequent I/O will cause large delays in inference.\nEarly works [17, 29] proposed leveraging swapping during the training of Deep Neural Networks (DNNs) to reduce GPU memory demands. ZeRO-Offload [32] applied the offloading to the training of Transformer-based LLMs. ZeRO-Infinity [31] extended this approach by incorporating disk as an additional offloading destination. DeepSpeed-Inference [2], which includes the ZeRO-Inference component, applies offloading techniques to the inference, enabling LLM inference in resource-constrained environments. Flex-Gen [34] significantly improves inference throughput by"}, {"title": "3 Motivation", "content": "3.1 Shortcomings of Existing Work\nWhile MoE brings numerous advantages, it also faces significant challenges related to GPU memory usage due to the large number of experts. According to existing work, the percentage of experts' parameters in Switch Transformers can reach up to 99% [8]. At the same time, these experts are sparsely activated. There is no need to keep them resident in expensive GPU memory. Therefore, the sparse activation feature of MoE makes offloading a highly suitable strategy to address its memory challenges. However, offloading is not a comprehensive solution, and will introduce new issues.\nMany existing approaches for MoE focus on improving the accuracy of expert prefetching [8, 9, 18, 20]. However,"}, {"title": "3.2 A Strawman Offloading Strategy for MoE Models", "content": "To better adapt the aforementioned I/O overlapping strategy for MoE inference, we propose a strawman offloading strategy, the outline of whose construction process is shown in Figure 4. Two normalization layers are incorporated into the attention and MoE layers, respectively. Then we explain it incrementally from a simple offloading strategy as follows.\nFirstly, as shown in Figure 4(a), a simple offloading strategy is executing computations sequentially following the architecture of the MoE model, prefetching parameters of the next layer while computing the current layer. However, due to the slower I/O speed compared to computation speed,"}, {"title": "4 Overview", "content": "To solve the above challenges, we propose KLOTSKI, an inference engine designed for MoE that enables high-throughput MoE inference in resource-constrained environments. We show the system overview of KLOTSKI in Figure 6.\nFirstly, during the offline phase, we aggregate heterogeneous memory from GPU, CPU, and disk for model deployment. We adaptively sense the memory limits in the current environment and allocate the MoE model tensors across the heterogeneous memory (0) accordingly. In the online phase, when request batches are inputted (2), the constraint-sensitive I/O-compute planner formulates a pipeline plan based on the current hardware constraints (6). If the MoE model is performed for the first time, the correlation-aware expert prefetcher generates an expert correlation table during the warm-up process to guide expert prefetching."}, {"title": "5 Expert-aware Multi-batch Pipeline Paradigm", "content": "We aim to develop a pipeline that minimizes all bubbles to maximize GPU utilization. To achieve this, we propose an expert-aware multi-batch pipeline paradigm, which is designed based on zig-zag block schedule [34]. By considering the computations of multiple batches simultaneously, this paradigm enables weight sharing and orchestrates the multi-batch computational graph around the experts to reduce bubbles. A partial computational graph is illustrated in Figure 7, where each row corresponds to the computations of one batch, and the multiple batches are considered together as a batch group. Ultimately, this results in a nearly bubble-free pipeline, as shown in Figure 9. In the following, we will detail this paradigm from two perspectives: minimizing inter-layer bubbles and minimizing intra-layer bubbles.\nFirst, minimizing inter-layer bubbles. Inter-layer bubbles primarily occur between the attention layer and the MoE layer. During the computations of multiple batches in the attention layer, KLOTSKI prefetches only the weights of the gate and the hot experts, rather than the entire MoE layer. Because overlapping the I/O for the entire MoE layer is challenging, and Equation 1 must be satisfied.\n$n * t_{c\\_A} \\geq t_{I/O\\_MoE}$ (1)\nHere, n represents the number of batches in a batch group, $t_{c\\_A}$ denotes the computation time of an attention layer for a batch, and $t_{1/O\\_MOE}$ is the time required to transfer the entire MoE layer. Equation 1 clearly necessitates a large n to hold true, which would introduce a significant amount of KV cache. What's more, due to the nature of sparse activation, some experts may not be activated, even when multiple batches are being processed at the same time. Loading them all into VRAM not only wastes resources but also increases latency. In contrast, only overlapping the I/O for the gate and hot experts is easier and more effective, which just needs to satisfy Equation 2.\n$n * t_{c\\_A} \\geq t_{I/O\\_G} + K * t_{I/O\\_E}$ (2)\nHere, $t_{1/0\\_G}$ and $t_{1/O\\_E}$ represent the transfer times for the gate and a single expert, respectively. K equals k, the number of experts selected by the top-k gate, usually 1 or 2. Hot experts are chosen because they are likely engaged in most of the computations (see Figure 5), which provides an opportunity to minimize intra-layer bubbles subsequently. Additionally, during the computations of the gate, no prefetching is done. Instead, it is determined whether each gate-selected expert is a hot expert or one that has already been transferred. If not, the transfer of that expert is initiated immediately.\nSecond, minimizing intra-layer bubbles. As illustrated in the left panel of Figure 7(a), the sequence of experts shows that hot experts 2 and 4 have already been prefetched, while experts 5 and 3 are still undergoing transfer. Thus, the sequence of computations [2523424...] would result in the GPU stalling at positions 5 and 3, due to the incomplete transfer of data at these locations. However, computations involving experts 2 and 4 could proceed immediately. To reduce such unnecessary delays, we further adjust the order of expert computations across multiple batches, allowing computations involving the same experts to run continuously and prioritizing computations of hot experts. Since hot experts are transferred to GPU memory first and engaged in more computations, this adjustment allows more time for the transfer of experts still being loaded. After the computations for hot experts, the remaining experts compute in the order they are transferred. Additionally, experts that have completed all computations are offloaded immediately, rather than waiting for the entire layer's computations to finish, to reduce peak GPU memory usage.\nFinally, KLOTSKI executes computations according to the computational graph shown in Figure 7(b), sharing the loaded weights across multiple batches. This approach not only reduces the number of I/O operations to approximately 1/n of the original but also overlaps the time for each I/O, resulting in an almost bubble-free pipeline as illustrated in Figure 9"}, {"title": "6 Tensor Management", "content": "6.1 Adaptive Tensor Placement\nKLOTSKI constructs a multi-level heterogeneous memory space consisting of VRAM, DRAM, and disk to meet the storage demands of MoE models in resource-constrained environments. Then, we propose an adaptive tensor placement, which intelligently allocates tensors based on the available memory resources in the current environment, thereby enhancing the utilization of existing resources.\nFirstly, the GPU memory is primarily used to store necessary tensors required for current computations and prefetched tensors. When there is ample free GPU memory available, it can be further utilized to reduce some I/O operations. Specifically, we can choose storage locations for different types of tensors such as expert, gate, attention, KV cache, and activation. Furthermore, support is provided for layer granularity distribution. For example, placing the experts of the first three layers in VRAM, the experts of the next twenty layers in DRAM, and the remaining in disk.\nSecondly, inactive tensors can be offloaded to either CPU memory or disk. We prioritize allocating CPU memory to experts. This is because the MoE layer faces the challenge that the experts requested by the gating function cannot be accurately predicted in advance. Therefore, when handling tasks with large batch sizes, it is highly likely that immediate transfers of experts will be needed, necessitating the rapid transfer of the required expert to GPU memory. Considering the faster transfer bandwidth of CPU memory, which provides quicker response times, we prioritize placing expert parts in CPU memory.\nAdditionally, when sufficient CPU memory is available, we use pin_memory to achieve faster CPU-GPU communication. When CPU memory is insufficient and disk usage is necessary, to reduce the GPU getting tensors from disk, which is slow, we dynamically maintain tensors in the CPU"}, {"title": "6.2 Correlation-aware Expert Prefetcher", "content": "For dense models, the offloading strategies can directly prefetch the next layer. However, it is different for MoE models. Only after completing the computation of the gate can the activated experts be determined, making it challenging to design a unified prefetching strategy.\nTo address this, KLOTSKI design a correlation-aware expert prefetcher. In \u00a7 5, the prefetched experts need to engage in most computations across multiple batches to reduce intra-layer bubbles effectively. As illustrated in Figure 5, there are hot experts in the inference of MoE, where a few experts cover the majority of computations. Therefore, the prefetching targets for the MoE layer are the gate and hot experts. Since MoE is data-sensitive and hot experts may vary with different inputs, we establish a data-aware expert correlation table to identify the hot experts that tokens in the current multi-batch tend to select. Specifically, we record the correlations (i.e., frequency relationships) between experts activated by tokens at different layers through pre-run, resulting in a table. During inference, we use this table to determine each token's expert tendency in the current layer based on its selections in the previous l layers. The larger the value of l, the more accurate the prefetching. This process is illustrated in Figure 8, where each layer has four experts, the gate selects the top-1 expert, and l = 1. For the expert activation path of each token in the multi-batch, we look up the table"}, {"title": "7 Constraint-Sensitive I/O-Compute Planner", "content": "Planning Goal: To minimize the total time T required to complete tasks under existing resource constraints, achieving an almost bubble-free pipeline as illustrated in Figure 9.\n$\\\\min T = T_c + T_b$ (3)\ns.t.\n$M_{usage} < M_{GPU} + M_{CPU} + M_{disk}$,\n$M_{peak\\_GPU} < M_{GPU}$\nThe total time T is primarily composed of two parts: $T_c$ and $T_b$, representing the total computation time and the total time occupied by bubbles, respectively. $T_c$ mainly depends on hardware conditions. Our objective is to minimize $T_b$ under the constraints of available memory, making it approach zero, as shown in Equation 3. In our system, the reduction of $T_b$ is primarily influenced by two factors: (1) the placement of the tensors and (2) the batch size and the number of batches included in the batch group, denoted as n. Effective model placement can maximize the utilization of existing storage resources, thereby reducing some of the I/O demands, as considered in \u00a7 6.1. The batch size is typically a multiple of 4, leaving limited options for selection. However, determining the value of n is crucial. If n is too large, it will introduce a significant KV cache. Conversely, if n is too small, the total computation time for n batches may not overlap effectively with the I/O time of the next layer.\nTo investigate the value of n, our primary focus lies on the inter-layer overlap and intra-layer overlap in each MoE block. In Figure 9, we have inserted several arrows indicating the points where a specific tensor needs to start computing. These are interpreted as follows: (I) indicates the point where gate computations will begin, (II) marks the start of the computations for hot experts, (III) signifies the beginning of the computations for cold experts, and (IV) denotes the initiation"}, {"title": "8 Implementation", "content": "We implement KLOTSKI on top of PyTorch [28] and Hugging Face Transformers [37] with over 3k LOC of Python. Expert-aware multi-batch pipeline paradigm is implemented on top of FlexGen [34].\nExpert Correlation Table. We acquire input data by randomly sampling from wikitext-2 [27]. Subsequently, we conduct inference with a batch size of 8 and a sequence length of 512. Expert selections during the inference are recorded and tabulated in JSON format. The choice of small batches is deliberate to avoid excessively large statistical values, which would render updates to the expert correlation table meaningless. We set the activation path length 1 = 1 because we do not heavily rely on the accuracy of expert prefetching. A larger number of batches in a batch group already allows us to overlap communication and computation. Increasing l would add dimension to path recording, which increases the complexity of the table lookup and memory occupation.\nOverlapping Computation and I/O. KLOTSKI achieves I/O-computation overlap by orchestrating four CUDA streams: one for prefetching weights, another for transferring expert weights based on gating network results, a third for prefetching KV cache, and the last for storing new KV cache. Each stream operates asynchronously, executing its designated task independently. When certain data is needed, the corresponding stream will be synchronized."}, {"title": "9 Evaluation", "content": "9.1 Experimental Setup\nHardware. We evaluate KLOTSKI in two different environments, as shown in the Table 2. We don't care about the speed of disk reading in environment 2, because there is enough CPU memory.\nModels and Datasets. We evaluate KLOTSKI using the open-source MoE models: Mixtral-8\u00d77B and Mixtral-8\u00d722B. They have 46.7B and 141B parameters in bfloat16 precision respectively. We use Mixtral-8\u00d77B and Mixtral-8\u00d722B in environment 1 and use Mixtral-8\u00d722B in environment 2 only. This is because Environment 2 is not considered a resource-constrained environment for Mixtral-8\u00d77B. The inputs are randomly sampled from wikitext-103 [27], which has rich text from various fields. We use batch sizes from 4 to 64, with a sequence input length of 512 and an output sequence length"}, {"title": "9.2 End-to-End Throughput", "content": "We first evaluate the end-to-end throughput of Klotski and compare it with the baselines, as shown in Figure 10. We use the maximum n (= 15) from Figure 14 to show a better result than the default computed n. And we use n = 10 for Mixtral-8\u00d722B in Environment 1 because the computed n is large, which causes out-of-memory (OOM). We set FlexGen to use the same n as us. Across various scenarios, KLOTSKI consistently outperforms other methods in enhancing MoE inference throughput. Compared to Accelerate, Fast-Gen, FlexGen, MoE-Infinity, and Fiddler, KLOTSKI improves the inference throughput by up to 85.12x, 15.45\u00d7, 2.23x, 19.06x, and 9.53\u00d7, respectively.\nOn Mixtral-8\u00d77B, as the batch size increases, the time difference between computation and I/O gradually narrows, allowing Accelerate and FastGen to achieve decent performance. However, when applied to the larger Mixtral-8\u00d722B, the significantly increased weight transfer leads to a larger"}, {"title": "9.3 Throughput-Latency Trade-off", "content": "We plotted Figure 11 based on the throughput-related experimental results. It demonstrates that KLOTSKI offers a better throughput-latency trade-off for completing the same workload. Under the same time budget constraint, KLOTSKI can achieve more than three times the throughput of Flex-Gen and outperforms Accelerate, FastGen, MoE-Infinity, and Fiddler.\nFurthermore, as observed in both Figure 10 and Figure 11, quantization has a minimal impact on maximum throughput. However, it enables a more optimal throughput-latency trade-off curve. This improvement is due to the reduced data required for transfer between heterogeneous memory after tensor quantization, resulting in shorter I/O times. Consequently, a smaller n can achieve full overlap between computation and I/O. This also prevents the dramatic increase in KV cache size as n grows, especially when the batch size is large."}, {"title": "9.4 Memory Usage", "content": "In Figure 12, we illustrate the GPU memory usage of KLOTSKI during the prefill. The red line represents the minimum memory required for inference, while the orange line indicates the current GPU memory limitation. The blue line shows the memory usage after offloading all tensors, demonstrating that KLOTSKI requires minimal GPU memory to perform MoE inference, reducing memory usage by over 94.1%. However, there is still a significant amount of expensive GPU memory left unused. Thus, we can further utilize these memory resources, achieving a memory reduction of 74.5% while maintaining a throughput of approximately 40 tokens/s for Mixtral-8\u00d722B on a single H800. The changes during the decoding phase are essentially a repetition of the prefill phase, so for clarity, we only depict the prefill phase in the figure."}, {"title": "9.5 Ablation Study", "content": "We use that prefetching the entire MoE layer while computing the current layer in the single batch pipeline as a simple pipeline. Building upon this, we achieve our methodology in"}, {"title": "9.6 Prefetch Accuracy", "content": "To evaluate the effectiveness of the correlation-aware expert prefetcher, we calculated the accuracy of the prefetched hot experts at each layer, as shown in Figure 13. The green line shows the percentage of prefetched hot experts at each layer that were actually involved in the computations. This result remained consistently at 100%, demonstrating that KLOTSKI does not transfer experts who are not involved in the computations, thus avoiding unnecessary I/O. In contrast, we also evaluated the average accuracy of prefetching experts for a single sequence, which was found to be 42.24%. This comparison shows that processing multiple batches simultaneously can effectively reduce I/O waste. In addition, the blue line represents the accuracy of the selected hot experts, which varies with the data, giving an average accuracy of 58.89%. This suggests that we can accurately predict hot experts in most cases. Furthermore, one of KLOTSKI'Ss advantages is that KLOTSKI does not rely solely on the accuracy of expert prefetching to overlap I/O. Specifically, KLOTSKI takes a more fine-grained approach to overlap computation and I/O between experts, ensuring that even if a prediction is incorrect, it won't have a big impact."}, {"title": "9.7 Impacts of n and Batch Size", "content": "We present detailed end-to-end throughput data, as shown in Figure 14, to simulate different scenarios and analyze the impacts of n and batch size on throughput. Due to the large number of GPU hours required to complete all n \u00d7 bs combinations , we have not included it here.\nFrom Figure 14, we observed that when n is small, the throughput is low because the I/O time is much longer than the computation time, resulting in a long latency. In this stage, increasing n primarily achieves more overlap between computation and I/O. As n increases, larger batch sizes lead to a faster increase in throughput because each additional batch brings multiple sequences into the computations, further facilitating the overlap of computation and I/O. When n reaches a sufficiently large value, the slope of the corresponding point on the curve gradually approaches zero, indicating that most of the inter- and intra-layer bubbles have been eliminated. At this stage, increasing n mainly serves to share the weights among more batches to reduce the number of I/O operations further."}, {"title": "9.8 Bubble Reduction", "content": "As shown in Figure 15, we proportionally make a detailed inference pipeline of an MoE block, based on the data obtained from using the profiler tool. Figure 15(a) presents the inference pipeline of a single batch using methods designed for dense models. These methods load the entire MoE layer, resulting in significant inter-layer bubbles. In addition, the number of active experts often falls below eight, resulting in unnecessary I/O overhead for loading inactive experts. In contrast, as shown in Figure 15(b), KLOTSKI eliminates inter-layer gaps between the attention and MoE layers. After the gate computation is finished, the hot expert computation starts immediately without waiting. However, due to the significant gap between computation and I/O, it remains challenging to eliminate intra-expert bubbles, even at n = 10. By orchestrating expert computations, KLOTSKI overlaps the computation and the I/O between experts, effectively reducing latency. For an identical workload KLOTSKI completes inference in about 215 ms, compared to approximately 2367 ms using a simple overlap method.\nBy further increasing n, KLOTSKI can achieve the elimination of intra-expert bubbles within the MoE layer, such as n = 15 in \u00a7 9.2. However, the massively growing KV cache introduces additional load costs, resulting in new bubbles within multi-batch attention layer computations. We aim to address this in future work by developing a generalized and efficient sparse KV cache strategy for KLOTSKI, which will further improve efficiency and achieve a bubble-free multi-batch inference pipeline."}, {"title": "10 Conclusion", "content": "We present KLOTSKI, an inference engine designed for MoE models that can perform high-throughput inference in resource-constrained environments. Leveraging the proposed expert-aware multi-batch pipeline paradigm, KLOTSKI can significantly reduce the bubbles in the inference pipeline. Extensive experiments demonstrate that KLOTSKI Offers a superior"}]}