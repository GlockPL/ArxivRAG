{"title": "Revisiting the Solution of Meta KDD Cup 2024: CRAG", "authors": ["Jie Ouyang", "Yucong Luo", "Mingyue Cheng", "Daoyu Wang", "Shuo Yu", "Qi Liu", "Enhong Chen"], "abstract": "This paper presents the solution of our team APEX in the Meta KDD\nCUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The\nCRAG benchmark addresses the limitations of existing QA bench-\nmarks in evaluating the diverse and dynamic challenges faced by\nRetrieval-Augmented Generation (RAG) systems. It provides a more\ncomprehensive assessment of RAG performance and contributes\nto advancing research in this field. We propose a routing-based do-\nmain and dynamic adaptive RAG pipeline, which performs specific\nprocessing for the diverse and dynamic nature of the question in all\nthree stages: retrieval, augmentation, and generation. Our method\nachieved superior performance on CRAG and ranked 2nd for Task\n2&3 on the final competition leaderboard. Our implementation is\navailable at this link: https://github.com/USTCAGI/CRAG-in-KDD-\nCup2024.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the landscape\nof Natural Language Processing (NLP) tasks [5, 8, 10], particularly in\nquestion answering (QA). Despite advances in LLMs, hallucination\nremains a significant challenge, particularly for dynamic facts and\ninformation about less prominent entities.\nRetrieval-Augmented Generation (RAG) [9] has recently emerged\nas a promising solution to mitigate LLMs' knowledge deficiencies."}, {"title": "1.1 Dataset Description", "content": "The CRAG contains two parts of data: the QA pairs and the content\nfor retrieval.\nQA pairs. The CRAG dataset contains a rich set of 4,409 QA\npairs covering five domains: finance, sports, music, movie, and\nopen domain, and eight types of questions. For the KDD CUP 2024\nChallenge, the benchmark data were splited into three sets with\nsimilar distributions: validation, public test, and private test at 30%,\n30%, and 40%, respectively. In total, 2,706 examples from validation\nand public test sets were shared.\nThe dataset also reflects varied entity popularity from popular\nto long-tail entities, and temporal spans ranging from seconds to\nyears. Given the temporal nature of many questions, each question-\nanswer pair is accompanied by an additional field denoted as \"query\ntime.\" This temporal marker ensures the reliability and uniqueness\nof the answers within their specific temporal context.\nContent for retrieval. The CRAG dataset incorporates two\ntypes of content for retrieval to simulate a practical scenario for\nRAG: web search and knowledge graph (KG) search. This encom-\npasses up to 50 full HTML pages for each question, retrieved from"}, {"title": "1.2 Task Desription", "content": "This challenge comprises three tasks designed to improve question-\nanswering (QA) systems.\nTASK 1: The organizers provide 5 web pages per question, poten-\ntially containing relevant information. The objective is to measure\nthe systems' capability to identify and condense this information\ninto accurate answers.\nTASK 2: This task introduces mock APIs to access information\nfrom underlying mock Knowledge Graphs (KGs), with structured\ndata possibly related to the questions. Participants use mock APIs,\ninputting parameters derived from the questions, to retrieve rele-\nvant data for answer formulation. The evaluation focuses on the\nsystems' ability to query structured data and integrate information\nfrom various sources into comprehensive answers.\nTASK 3: The third task increases complexity by providing 50\nweb pages and mock API access for each question, encountering\nboth relevant information and noise. It assesses the systems' skill\nin selecting the most important data from a larger set, reflecting\nthe challenges of real-world information retrieval and integration.\nEach task builds upon the previous, steering participants toward\ndeveloping sophisticated end-to-end RAG systems. This challenge\nshowcases the potential of RAG technology in navigating and mak-\ning sense of extensive information repositories, setting the stage\nfor future Al research and development breakthroughs."}, {"title": "2 Methodlogy", "content": "Our approach, akin to most Retrieval-Augmented Generation (RAG)\nsystems, comprises three primary phases: retrieval, augmentation\nand generation. In all phases, we implement routing mechanisms\nto address diverse query types. Figure 1 illustrates the pipeline of\nour solution, while the remainder of this section details the three\nmain phases and the routers employed in this challenge."}, {"title": "2.1 Router", "content": "Routing is a crucial component of RAG systems, especially in real-\nworld QA scenarios. In practical applications, RAG systems fre-\nquently incorporate multiple data sources. In the CRAG Challenge,\nwe have three distinct data sources: Web Pages, Mock KGs, and\nMock APIs. The diversity of questions requires routing queries to\ndifferent data sources, individually or in combination. Even within\na single data source, such as Mock APIs, the question-specific se-\nlection of appropriate APIs is crucial. Furthermore, we can tailor"}, {"title": "Domain Router.", "content": "Domain router is fundamentally a classifier,\nmore specifically, a sentence classifier. Given a query, the domain\nrouter assigns a specific domain from a predefined set: finance,\nsports, music, movie, and open. Based on the assigned domain, the\nworkflow is then routed to the corresponding path.\nWe utilize Llama3-8B-Instruct [2] as our base model and enhance\nit with a classification head (Multilayer Perceptron, MLP) for do-\nmain classification. The 8B model inherently demonstrates a robust\ncapability to comprehend the domain of queries. We randomly split\nthe CRAG dataset into training, validation, and test sets with a\nratio of 8:1:1. Based on this split, we performed a simple LORA\n(Low-Rank Adaptation) [7] fine-tuning to adapt to the distribution\nof the CRAG dataset. This approach facilitates the development of\na high-quality classifier with minimal additional training.\nThe trained Domain router is employed at multiple stages within\nthe system. During the retrieval phase, the Domain router is pri-\nmarily used to select appropriate APIs. Following the retrieval of\nWeb Pages and APIs, it is further applied for selective fusion of the\nretrieved knowledge. In the generation phase, we first customize\nthe prompt templates based on the domain. Subsequently, after\nthe model completes its generation, the Domain is also utilized for\ncorresponding post-processing."}, {"title": "Dynamic Router.", "content": "Analogous to the Domain router, the Dy-\nnamism router is also a sentence classifier. Given a question, the\nDynamism router assigns a specific Dynamism, specifically one\nof: static, slow-changing, fast-changing, or real-time. The specific\ntraining methodology for the Dynamism router is congruent with\nthat of the Domain Router, and thus will not be recapitulated here.\nDue to the inherent limitation of Large Language Models in\nupdating their internal knowledge, they are prone to providing\noutdated answers to dynamic questions. Even when employing\nexternal knowledge through RAG, LLMs can readily generate hal-\nlucinations as most data sources are static, unless real-time APIs\nare utilized. In the absence of real-time APIs, the more rapidly\na question changes over time, the more susceptible LLMs are to\nhallucinations.\nTo attenuate hallucinations arising from dynamic questions, we\nimplemented the Dynamism router for post-processing. In scenarios\nwhere real-time APIs are inaccessible, we excluded certain real-time\nquestions and those that change rapidly over time."}, {"title": "2.2 Retrieval", "content": "As mentioned above, the CRAG dataset encompasses three types\nof content for retrieval: Web Pages, Mock KGs, and Mock APIs. For\nour final solution, we utilize two of these content types: Web Pages\nand Mock APIs."}, {"title": "2.2.1 Web Pages.", "content": "Web Pages are available for all 3 tasks of the\nchallenge. For Task 1&2, 5 Web Pages per question are provided,\neach potentially containing relevant information. Task 3 increases"}, {"title": "Therefore, we initially filter out an appropriate amount of\nrelevant text before ranking.", "content": "Specifically, we segment all the\ntext from the Web Pages into chunks of 1024 tokens (cal-\nculated based on tokens rather than characters). For these\nsegmented text chunks, we use BM25 [11] to select the top\n50 most relevant text blocks."}, {"title": "2.2.2 Mock APIs.", "content": "A total of 38 Mock APIs were provided for tasks\n2&3. As mentioned above, these Mock APIs can be categorized\ninto five distinct domains, with no overlap between the APIs of\ndifferent domains. Naturally, we designed separate workflows for\neach domain using a Domain Router. However, the overarching\nprocess flow of the workflows across all domains remains consistent,\nas shown in Figure 3:"}, {"title": "2.3 Augmentation", "content": "We employ input-layer integration for generation augmentation,\nwhich combines retrieved information/documents with the original\ninput/query and jointly passes them to the generator. In contrast\nto common input-layer integration, we do not utilize all retrieved\ndocuments. For different domains, we select specific data sources\nand integrate them to construct the final reference.\nFor the open domain, since we did not employ a Mock API, we\nexclusively utilized web search results. For the movie and music\ndomains, where most queries are relatively static or evolve slowly,\nresults retrieved from both web pages and mock APIs remain rele-\nvant. Therefore, we chose to integrate these two sources. For the\nsports and finance domains, which involve numerous real-time and\nfast-changing queries, we exclusively used Mock APIs to ensure\nthe timeliness and relevance of the retrieved information."}, {"title": "2.4 Generation", "content": "In the generation phase, we employed two widely-used methodolo-\ngies: Chain-of-Thought (CoT) reasoning and In-context Learning.\nAfter generation, we performed a simple post-processing proce-\ndure on the generated results based on the Domain and Dynamism\nRouters."}, {"title": "2.4.1 Chain of Thought.", "content": "Chain-of-Thought (CoT) [12] enhances\nthe reasoning process of language models by prompting them to\narticulate intermediate steps in problem-solving. This approach not\nonly enhances the model's ability to handle complex tasks but also\nsignificantly reduces hallucinations."}, {"title": "2.4.2 In-context Learning.", "content": "We improve the model's ability to\nrecognize invalid questions, particularly those based on false premises,\nthrough In-context Learning. We develop adaptive few-shot exam-\nples [3], selecting two of the most representative invalid question\nsamples for each domain and elucidating the reasons for their inva-\nlidity. Using the sports domain as an example, our few-shot samples\nare as follows:"}, {"title": "2.4.3 Post-processing.", "content": "Before finalizing the results, we imple-\nment basic post-processing strategies. Based on the question do-\nmain and volatility, we assign \"I don't know\" responses to queries\nsusceptible to hallucination. For domains lacking real-time API ac-\ncess, specifically open, movie, and music categories, we designated\n\"I don't know\" answers to fast-changing and real-time questions.\nFurthermore, due to the model's limited mathematical computation"}, {"title": "3 Experiments", "content": "In this section, we present our main results and ablation studies for\nsome crucial components.\nWe did not employ a strategy of fine-tuning the LLM; instead,\nwe used the LLM in a zero-shot setting. According to the rules set\nby the organizers, we used Llama3-70B-Instruct [2] for all our LLMs.\nFor the embedding model, we used BAAI/bge-m3, and for the rerank\nmodel, we used BAAI/bge-m3-v2-reranker. In the 1371 public test\ncases officially released for this round, we compared the following\nbaselines: LLM only (using the LLM without retrieving references)\nand straightforward RAG (the baseline provided by the organizers,\nusing straightforward RAG solutions)."}, {"title": "3.1 Metrics and Evaluation", "content": "In line with CRAG Benchmark, we conduct a model-based auto-\nmatic evaluation for our experiment. Automatic evaluation employs\nrule-based matching and GPT-4 [1] assessment to check answer\ncorrectness. It will assign three scores: correct (1 point), missing (0\npoints), and incorrect (-1 point). The final score for a given RAG\nsystem is computed as the average score across all examples in the\nevaluation set."}, {"title": "3.2 Overall Performance", "content": "Comparing our solutions to the RAG Baseline, we observe signifi-\ncant advantages in performance across all three tasks. In Table 1,\nour approach showcases notable improvements in accuracy and\ninformation retention. Specifically, when contrasted with the RAG\nBaseline, our solutions demonstrate superior results with reduced\nhallucination rates and enhanced information completeness. Task\n2 and Task 3, in particular, exhibit substantial enhancements in\naccuracy and reduced hallucination percentages, highlighting the\neffectiveness of our proposed methodologies in addressing these\nkey metrics."}, {"title": "3.3 Ablation Study", "content": "Table 2 presents the ablation study for major strategies employed\nin our solution.\nDuring the retrieval phase, we implemented several strategies,\nincluding pre-ranking, re-ranking, Entity Match, and Time Infor-\nmation Extraction. Ablation studies revealed that pre-ranking and\nre-ranking marginally reduce performance, while Entity Match and\nTime Information Extraction significantly decrease performance.\nBoth pre-ranking and re-ranking significantly contribute to the\nimprovement of retrieval quality. Pre-ranking enhances retrieval\nperformance by proactively filtering out a significant amount of\nnoise, while re-ranking ensures the accuracy of retrieval results\nthrough more refined and granular sorting. The enhancement in\nretrieval quality ultimately translates into an increase in answer\naccuracy. The absence of pre-ranking and re-ranking demonstrably\nleads to a substantial decrease in the accuracy of the final answers.\nFurthermore, pre-ranking significantly enhances retrieval effiency\nand reduces the retrieval time. Entity Matching and Time Informa-\ntion Extraction form the basis of using MOCK APIs. They ensure\nthe accuracy of API call parameters, which is crucially linked to the\noverall performance. The absence of either component can result\nin a significant performance decline."}, {"title": "4 Perspectives", "content": "Our method presents a robust and versatile framework for address-\ning a wide range of dynamic and complex real-world problems.\nThis approach, however, also opens up several avenues for further\ninvestigation."}, {"title": "API Integration and Scalability.", "content": "In real-world scenarios,\nwhere extensive API usage is common, our manually de-\nsigned matching rules are likely to prove inadequate. The\ndevelopment of a more universal method for selecting and\ncalling APIs, as well as processing the returned results, rep-\nresents a promising avenue for future research."}, {"title": "Handling Dynamic Information.", "content": "For questions that in-volve information that changes dynamically over time, sim-ply refusing to answer is merely a basic solution. Future\nresearch should focus on exploring methods to acquire\nthe most up-to-date knowledge and determine the timeliness of\ninformation. This is crucial to avoid hallucinations caused\nby outdated knowledge and to ensure the system provides\naccurate, current information. Developing techniques for\nreal-time information retrieval and verification, as well as\nimplementing mechanisms to assess the reliability and cur-\nrency of data sources, are key areas for investigation."}, {"title": "5 Conclusion", "content": "In this paper, we introduce our solution for the Meta KDD CUP\n2024: CRAG Comprehensive RAG Benchmark. We adopt a classic\nRAG framework with two specific routers. In the retrieval phase,\nwe demonstrated the process of obtaining high-quality information\nfrom various data sources and utilizing the Domain Router for in-\nformation filtering. In the augmentation phase, we employed the\nDomain Router in a similar manner for information aggregation\nbased on domain characteristics. Finally, in the generation phase,\nwe implemented two methods to significantly improve the model's\naccuracy and reduce hallucinations, while further mitigating hallu-\ncinations through post-processing based on the question's domain\nand dynamic nature.\nOur approach offers a viable pathway for addressing the diverse\nand dynamic challenges encountered in real-world scenarios. Nev-ertheless, our method has certain limitations. We have identified\nseveral inherent issues in the current methodology and provided\nour insights and reflections on the specific problems related to our\napproach. Ultimately, we anticipate that this study will make a\nmodest contribution to the broader RAG and LLM communities."}, {"title": "Model Cognitive Ability Assessment", "content": "Most conventional\nQA evaluation methods primarily focus on accuracy, neglect-ing the impact of hallucinations. Models should be aware of\ntheir knowledge boundaries, discerning what they should\nand should not answer. CRAG incorporates hallucinations\ninto evaluation metrics, but its settings lack sufficient jus-tification. Responding \"I don't know\" to all questions can\nyield a satisfactory score. Exploring the assessment of mod-els' cognitive abilities using methodologies for evaluating\nhuman cognition is a promising research direction."}]}