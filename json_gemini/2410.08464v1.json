{"title": "ARCap: Collecting High-quality Human Demonstrations for Robot Learning with Augmented Reality Feedback", "authors": ["Sirui Chen", "Chen Wang", "Kaden Nguyen", "Li Fei-Fei", "C. Karen Liu"], "abstract": "Recent progress in imitation learning from human demonstrations has shown promising results in teaching robots manipulation skills. To further scale up training datasets, recent works start to use portable data collection devices without the need for physical robot hardware. However, due to the absence of on-robot feedback during data collection, the data quality depends heavily on user expertise, and many devices are limited to specific robot embodiments. We propose ARCap, a portable data collection system that provides visual feedback through augmented reality (AR) and haptic warnings to guide users in collecting high-quality demonstrations. Through extensive user studies, we show that ARCap enables novice users to collect robot-executable data that matches robot kinematics and avoids collisions with the scenes. With data collected from ARCap, robots can perform challenging tasks, such as manipulation in cluttered environments and long-horizon cross-embodiment manipulation. ARCap is fully open-source and easy to calibrate; all components are built from off-the-shelf products. More details and results can be found on our website: stanford-tml.github.io/ARCap", "sections": [{"title": "I. INTRODUCTION", "content": "Developing robots to assist with domestic tasks has the potential to enhance human quality of life and augment human capabilities. To achieve this, robots must be able to manipulate everyday objects in unstructured and often cluttered environments. Imitation learning using human demonstrations has made significant progress in recent years. Demonstration data collected via teleoperated robotic systems provide precise, in-domain observation and action pairs, enabling effective robot policy learning through supervised learning [46]. However, the requirement for a robotic system and a skilled human operator upfront significantly limits the accessibility and scalability of data collection.\nAlternatively, human demonstrations can be collected using portable systems without the need for physical robot hardware [35, 6, 38]. These systems leverage human dexterity and adaptability to directly manipulate objects in-the-wild, facilitating the creation of large-scale, diverse human demonstration datasets. However, due to the absence of robot hardware, whether the collected demonstrations are useful for training robot policies is not immediately apparent without going through a multi-step process. First, the differences in embodiment between humans and robots require data retargeting. Second, the retargeted data must be validated by replaying the motion on the actual robot interacting with real objects. Finally, the robot policy must be trained using validated data. The success of demonstrations critically depends on the demonstrator's experience and awareness of"}, {"title": "II. RELATED WORK", "content": "Learning from Demonstrations. Imitation Learning (IL) has proven effective in enabling robots to perform various manipulation tasks [4, 20, 34, 23, 11, 13, 2, 1]. While traditional IL methods like DMP and PrMP [33, 24, 29, 30] are highly sample-efficient, they face challenges in handling high-dimensional observation spaces. In contrast, recent IL approaches leveraging deep neural networks can learn policies directly from raw image inputs [27, 14, 49], even for complex robotic systems with bimanual manipulators [47, 17, 42]. Although these methods are effective, scaling the amount of training data remains a significant hurdle. Teleoperation, a commonly used method for data collection in recent studies [44, 14, 26, 21, 39, 48, 3, 41, 16, 25, 46, 15, 5, 31, 19, 9, 32, 18]. Many low-cost teleoperation systems built upon VR controller or hand tracking[22, 5, 9, 18] and master-slave joint mapping[46, 15, 45, 42, 37, 12] were widely used. However, despite the low-cost nature of these action input devices, collecting data using teleoperation still requires the presence of a actual robot, which makes them expensive to distribute on a large scale. In contrast, our approach follows the recent fashion of collecting robot data without robot hardware [10, 40, 7, 38, 35], allowing us to scale up the training data more efficiently.\nData Collection System without Robots. Collecting data in the wild without the presence of a robot and training robots with that data has become an attractive direction to lower the total cost of the system. Prior works such as [7, 38, 35] proposed low-cost, in-the-wild data collection systems. Compared to directly using human video for training[36], these systems capture more fine-grained human movement and have helped robots to achieve complex tasks such as tea preparation [38], plate wiping[7, 38] and using air fryer[35]. Our ARCap system is another portable, in-the-wild data collection system; compared to existing systems, it provides visual, haptic feedback, which helps users without any data collection experience be aware of the embodiment gap between robots and humans. The most related work to ARCap is AR2-D2 [10, 40]. ARCap, however, focuses on providing real-time visual feedback and onboard collision checking using the reconstructed scene map. Additionaly, ARCap helps users collect data for different robot embodiments such"}, {"title": "III. METHOD", "content": "ARCap is an AR-based data collection interface and policy learning framework designed to transfer human hand motion capture data to robot control policies. The main features of ARCap's system design are:\nReal-time feedback. AR provides real-time visual-ization of the robot states, guiding users to collect high-quality and robot-reproducible demonstration data without physical robots.\nCross-embodiment. AR visualization supports both parallel-jaw grippers and multifinger dexterous hands, allowing users to collect data for different types of robot hardware using the same system.\nPortability. With a self-contained power supply, stor-age, and wireless tracking, the system enables data collection in-the-wild.\nIn this section, we first describe the system design that enables these features, followed by the training policies for controlling real robots."}, {"title": "A. ARCap System Design", "content": "Recent advancements in portable robot data collection interfaces [38, 7, 35] have made it possible to scale up robot data collection without needing a physical robot. However, since there is no real-time feedback from a robot during the data collection process, there is no guarantee that the collected data will be reproducible on an actual robot. Several failure modes have been observed: (1) Humans move too quickly for the robot to replicate; (2) Size differences between humans and robots cause the robot to collide with the environment, even when humans do not; (3) One data collection system is designed for one robot embodiment, requiring redesigns for different robot end-effectors. These observations begs the question: How can we alert humans about these issues during data acquisition and guide them to collect robot-reproducible data?\nInformative AR Feedbacks. In ARCap, we implement both visual and haptic feedback to inform users about camera visibility, robot kinematics, joint speed limits, and potential collisions between the robot and the environment.\na) Real-time visibility checking: One common failure mode for imitation learning is that the scene of manipulation is not always visible. This issue occurs frequently because RGB-D cameras used by robots usually have a narrower field of view compared to the cameras used for data collection\u2014 in our case, the passthrough cameras in Quest 3. To help the demonstrator always keep the manipulation scene within the field of view of the depth camera during data collection, we render a rectangular frame to visualize the actual field of view of the RGB-D camera, as shown in Fig.2. When collecting data, users needs to actively keep the scene inside the frame to ensure visual data is being recorded properly.\nb) Real-time retargeting: When collecting data for a particular robot, the robot may have significantly different kinematics compared to the human arm and hand. To remind users about the kinematic limit, we rendered a virtual robot in AR and retargeted it to the user's hand. Different end-effectors may have different retargeting methods, as we will discuss in the next section. Before data collection, the user will place the virtual robot at a fixed location in the world frame. During data collection, the end-effector of the virtual robot will track the user's hand; whenever the user uses their hand to interact with an object in the scene, they need to consider whether the virtual robot could perform such an action. For example, for a virtual robot equipped with a parallel jaw gripper, if the user tries to reorient an object using finger gaiting, the action performed by the virtual robot will appear to be invalid, as shown in the attached video. As each joint of the robot arm has its speed limit, the virtual robot also implements such limits and won't exceed the speed limit to track the user's input. If the user moves their hand too fast, there will be a significant visual mismatch between the user's hand and the robot end-effector; the rectangular frame will also blink yellow to remind the user the robot has its speed limit.\nc) Real-time collision checking: To remind users about the potential collisions between the robot and the environment, we also check the collision between the actual scene and the virtual robot. We found it is hard for humans to"}, {"title": "Cross-Embodiement with One System", "content": "ARCap can vi-sualize various end-effectors retargeted to the user's hand, enabling the collection of data for different robot embodi-ments without requiring hardware modifications. For any new robot embodiment, ARCap can be used for data collection as long as a retargeting process is in place that allows the robot to repeat human demonstrations. We present two real-time retargeting processes for different end-effectors attached to the Franka Panda arm: (1) Leap Hand, a fully actuated, four-finger dexterous hand, and (2) the Fin-ray gripper, a compliant parallel jaw gripper.\nDexterous hand. Similar to [38], we match the finger-tips of a dexterous hand to the fingertips of a human in the world frame using inverse kinematics. The inverse kinematics problem is solved in two steps. It first solves the leap hand wrist pose to match the human wrist pose provided by the quest controller and then solves the robot fingertip positions to match human fingertip positions tracked by the Rokoko data glove. As each finger of the leap hand has one redundant degree of freedom, we need to add null space regulation to encourage a natural hand posture and avoid self-"}, {"title": "Portable and Reproducible Design", "content": "ARCap is designed to be a low-cost, portable system that is easy to reproduce and calibrate, while accurately capturing detailed hand mo-tions. It also ensures user comfort during various tasks with minimal obstruction. To achieve these goals, ARCap is built around the Meta Quest 3 VR headset, as shown in Fig.4. The headset serves as both a display for feedback and a sensor hub, providing spatial tracking for itself and both controllers. A RealSense D435 camera is mounted on top of the headset using a 3D-printed bracket to capture 3D visual information, which is stored as point clouds. Since accessing the internal Quest 3 camera is difficult, future versions of ARCap could leverage the built-in RGB-D camera of an AR headset.\nFor wrist and hand motion capture, Quest 3 controllers are attached to the top of Rokoko data gloves. The controllers track wrist position and orientation relative to the headset, while the data gloves capture fingertip positions relative to the wrist. Using the headset's built-in SLAM function, we can access both visual and motion data within a world frame. Calibrating the system can be time-consuming due to the"}, {"title": "B. Imitation learning", "content": "1) Data processing: ARCap records the following data:\nColored point cloud in the camera frame\nJoint angle for the virtual robot solved by IK\nHeadset pose in the world frame\nVirtual robot pose in the world frame\nThe collected data can be used for imitation learning with a simple post-processing procedure. We first transform every data into the world frame. For point clouds, we further crop them in the world frame to remove background objects and the desktop. In the collected data, the hand and arm of a human user are visible. To reduce the visual gap, we superimpose a point cloud of the virtual robot visible by the depth camera in our point cloud dataset. After processing, all data for a single task will be stored in one hdf5 file.\n2) Training and testing: With processed data, we use diffusion policy for imitation learning. For encoding 3D point cloud, similar to [38, 43], a simple point net is used to compress colored point clouds into a latent vector. After that, the latent vector is concatenated with the current joint angle of the robot arm and hand as observation $o$. The generated action $a$ consists of the target joint angles of both robot arm and hand; for dex hand, $a$ consists of the target joint angles of each finger; for parallel jaw gripper, $a$ include a binary open and close command. Our training and testing pipeline is built upon robomimic[28], a unified framework for robot imitation learning. When testing trained policy, we can utilize the ARCap system to simplify the hand-eye calibration process. As shown in Fig.5, to compute the camera pose related to the robot base, we align the base of the virtual robot to the base of the actual robot in the ARCap application."}, {"title": "IV. EXPERIMENTS", "content": "We design experiments to answer the following questions:\nQ1 Does ARCap enable general users to collect higher-quality data\nQ2 Can data collected by ARCap help robots to manipulate under a cluttered environment?\nQ3 Can data collected by ARCap work on the robots with significantly different embodiments?\nQ4 Does data from ARCap good enough for achieving long-horizon manipulation?"}, {"title": "A. Experiment setup", "content": "We used two Franka Panda arms in our experiment, one attached with a Leap hand and another one attached with a Fin-ray gripper. Two robots shared the same workspace. For data collection, a Quest 3 headset runs a Unity application for visualization and data streaming, and a Windows laptop with an i5-13200H CPU is used for solving IK and storing data. For training and testing autonomous policy, we use a workstation with a single RTX3090 GPU and i7-13700 CPU. When testing, we calibrate the camera using the above-mentioned process and put the headset on a dummy head to serve as an RGB-D camera, as shown in Fig.7."}, {"title": "B. User study", "content": "To answer the Q1, we conduct a user study and invite 20 participants to use our new system ARCap with visual haptic feedback and the previous system DexCap, which has no feedback. Users have different exposure to VR/AR devices, and half of them have no data collection or robot learning experience; Fig.8.(c,e) shows the demographic of our participants. Moreover, none of the participants used either ARCap or DexCap before participating in this study. Test participants are asked to collect data for two tasks as shown in Fig.6: (1). Picking and placing a tennis ball"}, {"title": "C. Manipulation in cluttered environment", "content": "To verify whether data collected by ARCap can actually help robot imitation learning to achieve manipulation in a cluttered environment. We collected two 30-minute datasets using both systems and trained two diffusion policies on each of them. These two datasets are collected by the authors of this paper, who are familiar enough with both ARCap and DexCap. After training, we evaluate the policy using 20 trials with different initialization. Shown in Tab.I, ARCap can achieve a 35% higher success rate compared with DexCap, and no collision ever happens when testing ARCap policy. We also merge 30-minute data crowd-sourced from multiple first-time users during user study and train an autonomous policy from them. ARCap policy can achieve a 60% success rate across 3 designated initial states, while DexCap policy failed everytime across different trials, shown in Tab.I"}, {"title": "D. Long horizon manipulation with different embodiments", "content": "To answer Q3 and Q4, we also show that ARCap can collect high-quality data with embodiment significantly differ"}, {"title": "V. CONCLUSIONS AND FUTURE WORK", "content": "We propose ARCap, a portable data collection system that allows users without prior experience to collect high quality data across different embodiments via visual, haptic feedback. Using ARCap, we can teach robot manipulation in cluttered environments and achieve horizon cross embodiment manipulation with imitation learning. In the future, with additional design in feedback and retargeting process, ARCap also record human torso movement to collect data for mobile robots or humanoids. Currently, user improves their data collection strategies passively from feedback; with VLM, ARCap could also provide instruction for users to actively improve their data collection strategies and efficiency."}]}