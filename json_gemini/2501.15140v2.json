{"title": "ANALYZING AND BOOSTING THE POWER OF FINE-GRAINED VISUAL RECOGNITION FOR MULTI-MODAL LARGE LANGUAGE MODELS", "authors": ["Hulingxiao He", "Geng Li", "Zijun Geng", "Jinglin Xu", "Yuxin Peng"], "abstract": "Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced capabilities of MLLMs, such as object-centric visual question answering and reasoning. In our study, we revisit three quintessential capabilities of MLLMs for FGVR, including object information extraction, category knowledge reserve, object-category alignment, and position of the root cause as a misalignment problem. To address this issue, we present Finedefics, an MLLM that enhances the model's FGVR capability by incorporating informative attribute descriptions of objects into the training phase. We employ contrastive learning on object-attribute pairs and attribute-category pairs simultaneously and use examples from similar but incorrect categories as hard negatives, naturally bringing representations of visual objects and category names closer. Extensive evaluations across multiple popular FGVR datasets demonstrate that Finedefics outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy. The code is available at https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-modal Large Language Models (MLLMs) (Bai et al., 2023; Chen et al., 2023; Zhang et al., 2023b; Zhu et al., 2023; Dong et al., 2024; Liu et al., 2024b;a; Lauren\u00e7on et al., 2024a;b) have achieved remarkable advancements in understanding visual data, showcasing potential in advancing general artificial intelligence. These models enable users to interact with images as inputs, fostering seamless communication grounded in visual information. The impressive capabilities allow MLLMS to excel in various vision tasks while adeptly handling complex content comprehension and generation. However, despite their versatility and linguistic proficiency, MLLMs still face challenges in a fundamental task of machine vision: fine-grained visual recognition (FGVR) (Zhang et al., 2024b; Geigle et al., 2024), which aims at identifying subordinate-level categories, such as specific species of animals or plants (Wei et al., 2021). Poor FGVR performance of MLLMs hinders them from performing more advanced tasks like object-centric visual question answering and reasoning (Zhang et al., 2024b). For example, in smart agriculture, poor FGVR performance of pests may lead to incorrect treatment strategies and large-scale reduction in food production.\nEarly works have investigated the phenomenon (Zhang et al., 2024b) and attempted to improve the FGVR performance of MLLMs by integrating open-set classification data into pre-training or fine-tuning stage (Geigle et al., 2024; Zhang et al., 2024b). However, fine-tuning solely on the classification task harms the general capability of the instruction following, while purely integrating classification-focused data into the instruction tuning data brings limited improvement (Geigle et al., 2024), making their direct utilization impractical. To understand why MLLMs underperform in"}, {"title": "2 WHY DO MULTI-MODAL LARGE LANGUAGE MODELS UNDERPERFORM IN FINE-GRAINED VISUAL RECOGNITION?", "content": "This section scrutinizes the root causes of underperformance in FGVR via comprehensive empirical analyses on three quintessential capabilities of MLLMs: object information extraction, category knowledge reserve, and object-category alignment by comparing the representation space with corresponding VLMs (Idefics2 (Lauren\u00e7on et al., 2024a) and SigLIP (Zhai et al.) in our experiments).\nAssuming an image \\(I_i\\) containing an object \\(O_i\\) is processed by the vision encoder \\(V_a\\) and learnable modality connector \\(F_\\beta\\) to be transformed into a visual object token sequence of length \\(m\\): \\(S = [o_1, o_2, ..., o_m]\\). Input category name in textual modality \\(C_i\\) is passed through an embedding layer \\(E_f\\) of the LLM to obtain the category embedding sequence of length \\(n\\): \\(S = [c_1, c_2, ..., c_n]\\). Subsequently, the object embedding sequence \\(S\\) and category embedding sequence \\(S\\) are individually passed through the LLM layers \\(L_\\theta\\) to obtain the output from the last layer:\n\\[H = L_\\theta(S), \\quad \\quad (1a)\\]\n\\[H = L_\\theta(S), \\quad \\quad (1b)\\]\nwhere \\(H = [\\hat{o}_1,\\hat{o}_2, ..., \\hat{o}_m]\\), and \\(H_c = [\\hat{c}_1,\\hat{c}_2, ..., \\hat{c}_n]\\). Afterward, we select two ways to represent the global semantics of output sequence following (Zhang et al., 2024b): 1) last token embedding \\(\\hat{o}_m, \\hat{c}_n\\), and (b) average of the token embedding sequence \\(\\overline{o} = (\\sum_{j=1}^m \\hat{o}_j)/m\\), \\(\\overline{c} = (\\sum_{k=1}^n \\hat{c}_k)/n\\). For VLMs, the projected [CLS] embedding outputs from last layer of vision encoder \\(V_a\\) and text encoder \\(T_\\gamma\\) are taken to represent the global semantics of \\(O_i\\) and \\(C_i\\), denoted as \\(\\hat{O}_{CLS}\\) and \\(\\hat{C}_{CLS}\\), respectively."}, {"title": "2.1 OBJECT INFORMATION EXTRACTION", "content": "In the task of FGVR, a model that excels at object information extraction is required to have discriminative representations, i.e., large inter-class distance and small intra-class variance. To com-"}, {"title": "2.2 CATEGORY KNOWLEDGE RESERVE", "content": "Trained on enormous internet-scale corpora, LLMs are known for encoding the expert knowledge for general categories in their weights, but we ask ourselves, is the expert knowledge quintessential for FGVR already contained in MLLMs? We hypothesize that MLLMs' underperformance in FGVR tasks stems from the inadequate knowledge of subordinate-level categories. To test the hypothesis, we investigate whether LLMs utilized in MLLMs can distinguish different categories by generating discriminative descriptions. Specifically, we probe the knowledge in Idefics2 via using the prompt [\"Give a brief description of distinguishing features of\n{CLASS NAME}\"]. For each subordinate-level category in Oxford-IIIT Pet-37 (Parkhi et al., 2012), we set the number of return descriptions to 200 and then equally divided them into the train set and test set. The class names in returned descriptions are replaced with demonstrative pronouns to avoid leakage of classification labels. Similarly, we conduct linear probing experiments on top of \\(\\hat{c}_n\\) and \\(\\overline{c}\\). As shown in Table 1b, Idefics2 exhibits better classification performance than the text encoder of SigLIP, demonstrating its superiority in reserving category knowledge. Despite the rich semantics of the generated category description, the category names have lower discriminability in the representation space of Idefics2 than the text model of SigLIP, illustrated in Figure 2b and 2e."}, {"title": "2.3 OBJECT-CATEGORY ALIGNMENT", "content": "Since our empirical study shows that Idefics2 has an acceptable capability of object information extraction and adequate knowledge of subordinate-level categories, we hypothesize that the misalignment between the visual object and category name is the root cause. We randomly sample 120 object-category pairs from Stanford Dog-120 (Krause et al., 2013), and visualize the distributions of the last token embedding of the object \\(\\hat{o}_m\\) and the category name \\(\\hat{c}_n\\) in the same representation space. As shown in Figures 2c and 2f, object and category representations have significant semantic gaps. Since category names may not fully represent the semantics of the visual data (Lyu et al., 2024), the object cannot match the ground-truth category in the representation space and thus fails to decode into the correct category name."}, {"title": "3 METHOD", "content": "After thoroughly investigating the root cause of the underperformance in FGVR, this section formally introduces Finedefics, which enhances the model's FGVR performance by better aligning visual objects and category names. The framework to build Finedefics is illustrated in Figure 3, composed of two key components: (1) Attribute Description Construction for extracting useful attribute information that can distinguish different categories. (2) Attribute Augmented Alignment dedicated to using constructed attribute descriptions as the intermediate point to bind visual objects and category names in the representation space of LLMs, thus boosting the subsequent Classification-Centered Instruction Tuning."}, {"title": "3.1 ATTRIBUTE DESCRIPTION CONSTRUCTION", "content": "Although it has been demonstrated in (Liu et al., 2022; El Banani et al., 2023) that language is a powerful tool for capturing semantic relationships, dependency on category names exclusively to align with extracted visual embeddings is unreliable. As discussed in Section 2.2, leveraging adequate knowledge of subordinate-level categories, LLMs can describe distinguishing features that better capture category semantics than category names. Inspired by (Liu et al., 2024c) that exploits a cascade of foundation models to translate useful visual information from visual to textual modality, we propose constructing sample-wise attribute descriptions for each FGVR training set.\nSpecifically, the construction comprises three steps: 1) Useful Attribute Discovery by LLMs, such as GPT-4 (Achiam et al., 2023) and LLaMA (Touvron et al., 2023). These attribute names are employed as keys to instruct Visual Question Answering (VQA) models (such as BLIP-2 (Li et al., 2023) and LLaVA (Liu et al., 2024b)) for extracting useful attribute values. 2) Visual Attribute Extraction by VQA models. These attribute values enrich the information for distinguishing subordinate-level categories. 3) Attribute Description Summarization by LLMs. These descriptions help alleviate the gaps between visual objects and category names in the training phase of Finedefics."}, {"title": "Userful Attribute Discovery", "content": "Given the super-category of each FGVR dataset, such as aircraft for FGVC-Aircraft (Maji et al., 2013), we first identify a useful set of attributes that set apart the subordinate-level categories. For example, the wing shape attribute can help distinguish various aircraft models. To discover such key visual cues, we tap into the expert knowledge of LLMs, which is otherwise only restricted to experts. Specifically, we ask LLMs: [\"Your task is to tell me what are the useful attributes for distinguishing\n{SUPERCLASS} {CLASSUNIT} in a photo of a {SUPERCLASS}\"]. Formally, LLM takes a super-category \\(C_{sup}\\) as input and outputs a list of useful attributes:\n\\[N^{C_{sup}} = L_\\theta (P_{dis}(C_{sup})), \\quad \\quad (2)\\]\nwhere \\(N^{C_{sup}} = \\{N^{C_{sup}}_1, ..., N^{C_{sup}}_K\\}\\) are the generated attribute keys for the category \\(C_{sup}\\), \\(L_\\theta\\) are LLM layers, and \\(P_{dis}\\) is the How-to LLM-prompt in (Liu et al., 2024c)."}, {"title": "Visual Attribute Extraction", "content": "With the discovered attribute names \\(N^{C_{sup}}\\, we leverage VQA models that excel at identifying general visual attributes (e.g., shape, color) of objects to extract each attribute value per sample. For example, if an attribute is wing shape, VQA models are prompted to give a brief description of the wing shape, which is a much easier task than recognizing many subordinate-level categories. Following (Liu et al., 2024c), we add a general attribute name \\(N^{C_{sup}}_{gen}\\) = [\"General description of the image\"] and its prompt\n\\(P_{ext}\\) = [\"Questions: Describe this image in details. Answer:\"]. Formally, VQA model takes as input an image \\(I_i\\), its super-category \\(C_{sup}\\) and the attribute names \\(N^{C_{sup}}\\, the output visual attributes are given as:\n\\[V_i = Q_{\\theta} (I_i, C_{sup}, P_{ext}(N^{C_{sup}})), \\quad \\quad (3)\\]\nwhere \\(V_i = \\{V_1, ..., V_j\\}\\) denotes the extracted set of visual attributes for image \\(I_i\\), \\(Q_{\\theta}\\) is the VQA model, and \\(P_{ext}\\) is the Identify VQA-prompt in (Liu et al., 2024c)."}, {"title": "Attribute Description Construction", "content": "After obtaining the structured set of attribute key-value pairs, we further ask LLMs: [\"Summarize the information you get about the {SUPERCLASS} from the general description and attribute description with five sentences.\"]. The summarized attribute description contains richer semantics of subordinate-level categories, making it much easier for LLM to understand. Formally, given the set of attribute names \\(N^{C_{sup}}\\, and attribute values \\(V_i\\), LLM outputs a summarized attribute description for image \\(I_i\\):\n\\[A_i = L_{\\theta} (P_{con} (N^{C_{sup}}, V_i)), \\quad \\quad (4)\\]\nwhere \\(A_i\\) is the attribute description constructed for image \\(I_i\\), and \\(P_{con}\\) is the revised Reason LLM-prompt in (Liu et al., 2024c) for summarization task only. Expanding upon our newly built attribute descriptions, we transfer traditional (object, category) pairs in FGVR datasets to (object, attribute, category) triples. Without specification, the category refers to the subordinate-level category instead of the super-category in subsequent sections."}, {"title": "3.2 ATTRIBUTE AUGMENTED ALIGNMENT", "content": "With the constructed informative attribute descriptions, we introduce a new training paradigm named Attribute Augmented Alignment to build our Finedefics. It comprises two stages: (I) Attribute Augmented Contrastive Learning for aligning visual objects and category names in the representation space of LLMs. (II) Classification-Centered Instruction Tuning for enhancing the model's ability to follow the FGVR task instruction."}, {"title": "Stage I: Attribute Augmented Contrastive Learning", "content": "For each object-attribute-category triple \\((O_i, A_i, C_i)\\), we utilize the vision encoder \\(V_a\\) and the learnable modality connector \\(F_\\beta\\) to transfer \\(O_i\\) into an object embedding sequence of length \\(S^o = [o_1, o_2, ..., o_m]\\) with length \\(m\\). To better capture the global representations, we follow (Jiang et al., 2024) to pass an [EOS] token through an embedding layer \\(E_f\\) of LLM to obtain the vector representation and append it to the visual embedding sequence \\(S^o\\). Therefore, we obtain the newly built object embedding sequence \\(S^o_i = [o_1, o_2, ..., o_m, o_{EOS}]\\). Similarity, we obtain the attribute embedding sequence \\(S^a_i = [a_1, a_2, ..., a_p, a_{EOS}]\\) with length \\((p+1)\\), and category embedding sequence \\(S^c_i = [c_1, c_2, ..., c_n, c_{EOS}]\\) with length \\((n + 1)\\). Then, \\(S^o_i, S^a_i, S^c_i\\) are individually fed into LLM layers \\(L_\\theta\\), and the embeddings of the last predicted token \\(o_{EOS}, a_{EOS}, c_{EOS}\\) are utilized as the global representations of \\(O_i, A_i, C_i\\), respectively. Without specified, we use \\(\\hat{o}^o_i = o_{EOS}, \\hat{a}^a_i = a_{EOS}, \\hat{c}^c_i = c_{EOS}\\) for simplicity.\nTo improve the effectiveness of contrastive learning, we then mine difficult incorrect category names for each example object \\(O_i\\) used in the FGVR dataset. To do this, we use a CLIP model (Radford et al., 2021) for mining hard negative samples: for every example image, we select three images along with their attribute descriptions from the three most similar but incorrect categories. Attribute descriptions and category names from these hard negative samples are subsequently treated as additional negatives. Thus, the formulation of Object-Attribute Contrastive (OAC) loss with the inclusion of hard negatives can be described as follows:\n\\[L^{chn}_{OA} = \\sum_{(\\hat{o}^o_i, \\hat{a}^a_i,\\hat{c}^c_i) \\in B} - log \\frac{expSim(\\hat{o}^o_i, \\hat{a}^a_i)}{\\sum_{\\hat{a}^a \\in B} expSim(\\hat{o}^o_i, \\hat{a}^a) + \\sum_{\\hat{a}^w \\in Ann} expSim(\\hat{o}^o_i, \\hat{a}^w)}, \\quad \\quad (5a)\\]\n\\[L_{AO} = \\sum_{(\\hat{o}^o_i, \\hat{a}^a_i,\\hat{c}^c_i) \\in B} - log \\frac{expSim(\\hat{o}^o_i,\\hat{a}^a)}{\\sum_{\\hat{o}^k \\in B} expSim(\\hat{o}^k,\\hat{a}^a)}, \\quad \\quad (5b)\\]\n\\[L^{chn}_{OAC} = (L^{chn}_{OA} + L_{AO})/2, \\quad \\quad (5c)\\]\nwhere \\(Ann\\) denotes the attribute representation set of hard negatives for the object \\(O_i\\), \\(Sim(\\cdot,\\cdot)\\) measures the cosine similarity in a semantic space.\nSimilarly, Attribute-Category Contrastive (ACC) loss with the inclusion of hard negatives is formulated as follows:\n\\[L_{AC} = \\sum_{(\\hat{o}^o_i, \\hat{a}^a_i,\\hat{c}^c_i) \\in B} - log \\frac{expSim(\\hat{a}^a_i, \\hat{c}^c_i)}{\\sum_{\\hat{c}^c \\in B} expSim(\\hat{a}^a, \\hat{c}^c) + \\sum_{\\hat{c}^w \\in Cin} expSim(\\hat{a}^a_i, \\hat{c}^w)}, \\quad \\quad (6a)\\]\n\\[L^{ca}_{CA} = \\sum_{(\\hat{o}^o_i, \\hat{a}^a_i,\\hat{c}^c_i) \\in B} - log \\frac{expSim(\\hat{a}^a,\\hat{c}^c)}{\\sum_{\\hat{a}^a \\in B} expSim(\\hat{a}^a_i,\\hat{c}^c) + \\sum_{\\hat{a}^w \\in Ann} expSim(\\hat{a}^w,\\hat{c}^c)}, \\quad \\quad (6b)\\]\n\\[L^{chn}_{ACC} = (L_{AC} + L^{ca}_{CA})/2, \\quad \\quad (6c)\\]\nwhere \\(Cin\\) denotes the category representation set of hard negatives for the object \\(O_i\\).\nAs discussed in Section 2.2, it is hard to differentiate between category names in the representation space of LLMs. Inspired by the intra-modal contrastive loss to promote the model's ability to differentiate between hard nagative captions (Zhang et al., 2024a), we additionally define Category-Category Contrastive (CCC) loss as follows:\n\\[L_{CCC} = \\sum_{(\\hat{o}^o_i, \\hat{a}^a_i, \\hat{c}^c_i) \\in B} - log \\frac{1}{\\sum_{\\hat{c}^k \\in Chn} expSim(\\hat{c}^c_i,\\hat{c}^k)}. \\quad \\quad (7)\\]\nTo maintain the generative power of the model, we use the attribute descriptions as LLM-augmented captiona to formulate the attribute description generation task. Therefore, the optimization object of the first stage can be defined as follows:\n\\[\\beta,\\theta = arg \\min_{\\beta,\\theta} L^{att} + (L^{chn}_{OAC} + L^{chn}_{ACC} + L_{CCC})/2, \\quad \\quad (8)\\]\nwhere \\(L^{att}\\) denotes the attribute description generation loss."}, {"title": "Stage II: Classification-Centered Instruction Tuning", "content": "In the second stage, we formulate the FGVR dataset as two kinds of instruction tuning data: open-set QA data and closed-set multiple-choice data. Then we fine-tune the model using this classification-centered instruction tuning data. Consequently, the optimization object of the second stage can be formulated as:\n\\[\\beta,\\theta = arg \\min_{\\beta,\\theta} L^{cls}, \\quad \\quad (9)\\]\nwhere \\(L^{cls}\\) denotes the generation loss of classification-centered instruction tuning data."}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate the performance of Finedefics aiming to answer the following questions: (1) Can Finedefics effectively improve FGVR accuracy in MLLMs? (2) Does each core design of Finedefics benefit the accuracy improvement? (3) Is Finedefics effective in aligning visual objects and category names in the representation space of LLMs?"}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "Datasets. We conduct experiments on several popular FGVR datasets that include CaltechUCSD Bird-200 (Wah et al., 2011), Stanford Car-196 (Krause et al., 2013), Stanford Dog-120 (Krause et al., 2013), Flower-102 (Nilsback & Zisserman, 2008), Oxford-IIIT Pet-37 (Parkhi et al., 2012), and FGVC-Aircraft (Maji et al., 2013). Following (Geigle et al., 2024), we leverage the test sets as resources for annotated data and frame FGVR as a multiple-choice task with well-defined answer candidates. To facilitate Finedefics, we select the training sets of them to construct attribute description, build open-set QA and closed-set multiple-choice instructing tuning data, ensuring that these images are different from the ones used in testing.\nEvaluated MLLMs. We build Finedefics upon Idefics2 (Lauren\u00e7on et al., 2024b) for its open-source accessibility and leading zero-shot performance. Several recent MLLMs of comparable parameter sizes are evaluated, including LLaVA 1.5 (Liu et al., 2024b), LLaVA-Next (Liu et al., 2024a), MobileVLM v2 (Chu et al., 2024), InstructBLIP Vicuna (Dai et al., 2024), InstructBLIP Flan-T5-XL (Dai et al., 2024), Phi-3-Vision (Abdin et al., 2024), BLIP2 Flan-T5-XL (Li et al., 2023), InternLM XComposer 2 (Dong et al., 2024), Pali-Gemma 1, Idefics1 (Lauren\u00e7on et al., 2024a), Idefics2 (Lauren\u00e7on et al., 2024b), and Qwen-VL-Chat (Bai et al., 2023).\nTraining Settings. All seeds are fixed across the training procedures for fairness. We train Finedefics using the QLoRa technique (Dettmers et al., 2024), updating adapters in the LLM and modality connector including perceiver resampler with 8 NVIDIA A6000 GPUs with 48G of memory. We use 4-bit quantization, with y = 8 and a = 8 for LoRa, and a learning rate of 2e-4. For both stages, the model is trained for one epoch with the warming steps of 60. The accumulated batch size is set to 64 and 128 for stage I and stage II, respectively."}, {"title": "4.2 MAIN RESULTS", "content": "In Table 2, we compare Finedefics with previous leading approaches on six popular FGVR datasets. Finedefics exhibits a significantly enhanced FGVR capability compared to a wide range of MLLMs. Notably, Finedefics shows superior performance than Idefics2 by an average of +10.89% and Qwen-VL-Chat of +9.43% across all datasets. Note that Finedefics is built upon Idefics2 (Lauren\u00e7on et al., 2024b), a high-performing model in various vision-language and vision-centric tasks, and the"}, {"title": "4.3 ANALYSIS OF FINEDEFICS", "content": "Does Fine-tuning Solely on Additional Open-set FGVR Data Bring Performance Gains? In Table 3a, we fine-tuning Idefics2 solely on additional open-set FGVR data. We observe that it deteriorates the instruction following capability for answering multiple-choice questions in our test settings. Finedefics outperforms the fine-tuned model, which indicates that Finedefics effectively boosts FGVR accuracy by integrating attribute augmented alignment into the training paradigm rather than solely fine-tuning on additional data.\nDoes Attribute Descriptions Contribute Performance Gains of Contrastive Learning? To demonstrate the impact of augmenting contrastive learning with attribute descriptions, we conduct experiments and report the results in Table 3b. In the ablation experiments, we employ contrastive learning on object-category pairs without utilizing attribute descriptions. The results show that the attribute description benefits the alignment between visual objects and category names.\nIs Training in Two Stages Necessary for Building Finedefics? We further analyze the necessity of training in two stages, i.e., representation alignment before instruction tuning. Specifically, we fine-tune Idefics2 with a combined loss of classification-centered instruction tuning and attribute augmented contrastive learning. The results are reported in Table 3c. We observe that fine-tuning in one stage is prone to struggle with optimization and leads to degraded performance, which indicates the effectiveness of training in two stages.\nVisualization - Does Finedefics Effectively Align Visual Objects and Category Names? To substantiate our objective of enhancing the alignment between visual objects and category names with the auxiliary visual attributes, we randomly selected 100 data from Oxford-IIIT Pet-37 (Parkhi et al., 2012) for visualization. As illustrated in Figure 4a, a substantial gap between the object and category is observable in the data distribution when fine-tuning without contrastive learning. In Figure 4b, contrastive learning on object-category pairs without attribute descriptions involved fails to decrease the gap. In Figure 4c, with the usage of contrastive learning on object-attribute-category triples, the gap decreases significantly, thus boosting FGVR accuracy."}, {"title": "5 RELATED WORK", "content": "Multi-modal Large Language Models. Multimodal Large Language Models (MLLMs) aim to enhance machines' ability to understand and process complex information by integrating multiple data modalities such as vision, text, and audio. In recent years, MLLMs have achieved significant progress in three key areas. First was large-scale pre-training and fine-tuning, as seen in models like BLIP-2 (Li et al., 2023), LLaVA (Liu et al., 2024b), MiniGPT-4 (Zhu et al., 2023), PaLM-E (Driess et al., 2023), Kosmos-2 (Peng et al., 2023) and Visual ChatGPT (Wu et al., 2023), which used pre-training on vast multimodal datasets and were then fine-tuned for specific tasks, greatly improving the models' generalization ability and task performance. The second area was cross-modal consistency, focusing on ensuring information consistency across different modalities through techniques like contrastive learning. Models such as Shikra (Chen et al., 2023), FROMAGE (Koh et al., 2023), DLP (Jian et al., 2024), BuboGPT (Zhao et al., 2023b), ChatSpot (Zhao et al., 2023a), and Qwen-VL (Bai et al., 2023) enhanced performance in multimodal tasks by strengthening the alignment between modalities. The third area was interpretability and transparency. Models like ViperGPT (Sur\u00eds et al., 2023), GPT-4 (Achiam et al., 2023), PandaGPT (Su et al., 2023), Video-LLaMA (Zhang et al., 2023a), and Video-ChatGPT (Maaz et al., 2023) enhanced the explainability of model decision-making by incorporating attention mechanisms and natural language feedback, enabling users to understand better and trust the model's output. Despite these achievements, MLLMs still face challenges, such as the inability to extract informative visual features, insufficient understanding of subordinate-level categories, and misalignment between visual objects and category names.\nFine-Grained Visual Recognition. FGVR (Welinder et al., 2010; Maji et al., 2013; Wei et al., 2021) aims to classify visually similar subordinate categories under a broader super-category, often requiring expert-provided auxiliary annotations (Krause et al., 2013; Zhang et al., 2014; Vedaldi et al., 2014; He & Peng, 2017) due to the subtle differences between objects. FGVR methods can be divided into three types: (i) attention-based methods enhance the model's ability to recognize subtle differences by focusing on the most critical areas of the image. (ii) hierarchical representation methods effectively handle the subtle differences between categories by constructing hierarchical feature representations that allow the model to refine image recognition progressively. (iii) metric learning methods improve the model's discriminative power in fine-grained classification tasks by learning a metric space where samples of the same class are closer and those of different classes are further apart. Moreover, TransHP (Wang et al., 2023) integrated vision-language models, making FGVR less reliant on annotations and adaptable across various tasks. HI2R (Chen et al.) introduced a hypergraph-guided approach that captures intra-class and inter-class relationships, enhancing the model's ability to discern subtle distinctions between fine-grained categories. CLEVER (Choudhury et al., 2024) extracted non-expert descriptions from images and trained a fine-grained textual similarity model to match image descriptions with Wikipedia document sentences accurately. Recent advancements like FineR (Liu et al., 2024c) employed large language models to translate visual attributes into text, enabling category identification without expert-defined labels."}, {"title": "6 CONCLUSION", "content": "In this paper, our objective is to analyze and boost the power of FGVR for MLLMs. We investigate the root cause of underperformance from three quintessential capabilities: object information extraction, category knowledge reserve, object-category alignment, we position the problem as the misalignment between visual objects and category names. To address the challenge, we propose Attribute Augmented Alignment, designed to use attribute descriptions as an intermediate point to bind them. Based on the aligned representation space, we build Finedefics, a new MLLM adept at identifying the subordinate-level category of the visual object. Our experiments, conducted on six popular FGVR datasets, demonstrate the remarkable performance of Finedefics. The validity of our methodology is substantiated through rigorous empirical studies.\nFuture Works. While Finedefics attains remarkable results across various FGVR datasets, it would encounter challenges in effectively learning new subordinate-level categories, and thus developing fine-tuning methods that can boost the continual FGVR capability for MLLMs is a promising future research direction."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 MORE ABLATION STUDIES", "content": "Attribute augmented alignment on other MLLMs. We build Finedefics upon Idefics2 (Lauren\u00e7on et al., 2024b). To confirm the general applicability of Finedefics, we conduct attribute augmented alignment (A\u00b3) on another typical MLLM: LLaVA 1.5 (Liu et al., 2024b). As shown in Table 4, after employing our proposed method, LLaVA 1.5 gains an accuracy improvement by 13.97% on average, demonstrating the effectiveness and generalizability."}, {"title": "Effects of attribute types", "content": "We analyze the effects of specific attribute types in FGVR tasks. Specifically, we selectively remove typical attribute types from [color, shape, texture, size] to evaluate the contribution to performance improvement. As shown in Table 5, all four types of attributes play a crucial role in distinguishing subordinate-level categories, but the contribution varies with the dataset. For example, color and texture are more critical for specific datasets, like flowers and birds."}, {"title": "Effects of hard negatives", "content": "We compare using hard negatives and simple negatives for contrastive learning. Specifically, we replace hard negatives with randomly sampled simple negatives, meaning that the negatives used for contrastive learning are less visually similar to positives and easier to distinguish from them. As illustrated in Table 6a, after applying contrastive learning with simple negatives, the improvement is limited. With the utilization of hard negatives, the modality gap decreases further, and the model harvests a significant accuracy improvement."}, {"title": "Effects of two-stage training stages", "content": "We analyze the effects of two-stage training by evaluating Finedefics by selectively removing specific training processes within each stage. As shown in Table 6b, pretraining solely fails to follow the task instruction, while instruction tuning (I.T.) solely has a limited performance gain. Instead, pretraining and instruction tuning are complementary to further boost the accuracy, confirming the effectiveness of our two-stage training paradigm."}, {"title": "Effects of description quality", "content": "We first design an empirical study to evaluate the description quality, i.e, how reliable the attribute"}]}