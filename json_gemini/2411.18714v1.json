{"title": "Explainable deep learning improves human\nmental models of self-driving cars", "authors": ["Eoin M. Kenny", "Akshay Dharmavaram", "Sang Uk Lee", "Tung Phan-Minh", "Shreyas Rajesh", "Yunqing Hu", "Laura Major", "Momchil S. Tomov", "Julie A. Shah"], "abstract": "Self-driving cars increasingly rely on deep neural networks to achieve human-like\ndriving [1, 2]. However, the opacity of such black-box motion planners makes\nit challenging for the human behind the wheel to accurately anticipate when\nthey will fail [3-5], with potentially catastrophic consequences [6-8]. Here, we\nintroduce concept-wrapper network (i.e., CW-Net), a method for explaining the\nbehavior of black-box motion planners by grounding their reasoning in human-\ninterpretable concepts. We deploy CW-Net on a real self-driving car and show\nthat the resulting explanations refine the human driver's mental model of the\ncar, allowing them to better predict its behavior and adjust their own behav-\nior accordingly. Unlike previous work using toy domains or simulations [9-11],\nour study presents the first real-world demonstration of how to build authentic\nautonomous vehicles (AVs) that give interpretable, causally faithful explanations\nfor their decisions, without sacrificing performance. We anticipate our method\ncould be applied to other safety-critical systems with a human in the loop, such", "sections": [{"title": "1 Introduction", "content": "There are hundreds of companies developing autonomous vehicle (AV) technology\nglobally [12], promising to revolutionize transportation for everyone. However, the\ncomplexity of fully driverless autonomy has prompted an industry shift towards\nadvanced driver-assistance systems, which require successful communication between\nthe AV and human driver. This is made increasingly difficult by the adoption of deep\nneural networks in AVs for planning and decision making, the core cognitive func-\ntions that determine driving behavior [11]. Deep learning allows motion planners to\nlearn the nuances of human driving behavior from data, but the implicit nature of\nthe learned driving policies makes it challenging to understand the causes of their\ndecisions and to predict their behavior.\nA lack of effective communication between the AV and the human driver has\ncontributed to multiple high-profile incidents, some resulting in fatalities [6-8], high-\nlighting the urgent need to make deep motion planners interpretable [11]. Previous\nstudies have sought to address this need using surveys and simulated scenarios [13-20],\na human driver emulating the AV [10, 21], or large language models providing post-\nhoc explanations [2]. However, these studies were theoretical, did not provide faithful\nexplanations of the AV's reasoning process, or were only evaluated in simulation. This\nleaves open the question of how to provide understandable and useful explanations for\nthe decisions of deep motion planners deployed in real self-driving cars.\nTo answer this question, we scale up our recent work on interpretable-by-design\ndeep reinforcement learning with prototype-wrapper networks (PW-Nets) [9], using\nmotifs from the literature on concept-bottleneck models [22]. Our key proposal is to\nground the reasoning of black-box motion planners in human-interpretable concepts,\nsuch as \"Approaching stopped vehicle\u201d or \u201cClose to cyclist\". This method is rooted in\ncase-based reasoning, a classical artificial intelligence (AI) approach [23-25] inspired\nby cognitive models of human reasoning and memory [26]. It results in causal explana-\ntions, such as \u201cI chose to stop based on recognizing that we are approaching a stopped\nvehicle.\u201d. In our tests, these explanations help align a safety driver's mental model\nof the AV with its actual internal decision-making process, increasing transparency\nand predictability. Importantly, this approach can be applied to arbitrary pre-trained\ndeep neural networks, does not require retraining from scratch, and does not degrade\nperformance of the original black-box planner.\nWe apply our proposed method, CW-Net (short for concept-wrapper network), to\na deep motion planner trained to imitate human driving behavior using inverse rein-\nforcement learning [1]. We replace the final (reward) layer of the pretrained deep neural\nnetwork with a concept classifier, followed by a new reward layer. We then jointly train"}, {"title": "2 Black-box motion planner", "content": "We focus on the motion planning module of the AV stack (Figure 1a), which takes\nas input a scene context s and outputs a trajectory \u00ee. s is a symbolic object-oriented\nrepresentation of the scene computed by the perception module, while is the tra-\njectory that the subsequent controller module should follow. We use a deep neural\nnetwork architecture consisting of a scene encoder $H(s) \\rightarrow h$ and a trajectory gen-\nerator $G(s) \\rightarrow {T\u2081 ... Tk}$, followed by a scene-trajectory encoder $E(h, Ti) \\rightarrow zi$ and\na final reward layer $R(zi) \\rightarrow ri$ (Figure 1b). \u0397 computes a scene embedding h and\nG computes a set of k candidate trajectories {T1... Tk}. Those are combined in E to\ncompute an embedding zi for each trajectory Ti. Finally, R computes an estimated\nreward ri for each trajectory Ti, quantifying how human-like it is. In other words, ri\nis higher when T\u2081 is more similar to how a human would drive in this situation.\nDuring inference, the trajectory with highest reward is selected on each iteration:\n$\\uparrow = T_{i} \\text{ such that } i = \\underset{i \\in 1,...,k}{\\text{arg max}} r_i, r_i = R(E(h, T_i))$\nWe train the planner on 80 hours of human expert driving using inverse\nreinforcement learning [1]."}, {"title": "3 Planning over human-friendly concepts", "content": "To make the planner more interpretable, we replace R with a concept classifier\n$C(zi) \\rightarrow ci$, followed by a new reward layer $R'(ci) \\rightarrow r$ (Figure 1c). The concept clas-\nsifier C computes a logit vector ci which is passed through a softmax and/or sigmoid\nlayer which assigns probabilities to different human-interpretable concepts. Since R'\ncomputes trajectory rewards from ci, the final decisions are based solely on these con-\ncept assignments and hence they constitute a causally faithful explanation. The rest\nof the network remains the same.\nSimilarly to the black-box planner, trajectories are selected according to:\n$\\uparrow = T_{i} \\text{ such that } i = \\underset{i \\in 1,...,k}{\\text{arg max}} r\\', r\\'= R'(C(E(h, \\tau_i)))$"}, {"title": "4 Evaluation in simulation", "content": "As a baseline, we first evaluated the black-box planner (without CW-Net) using closed-\nloop simulations on the nuPlan dataset [27] (Table S1). Overall, the results were\ncompetitive with the top submissions to the nuPlan challenge, although performance\nwas slightly lacking when starting from a stop (see Section 8.5). This suggests that\nthere is room for improvement and, importantly, opportunities to study explanations\nof undesirable behavior.\nWe then evaluated the driving performance of CW-Net wrapped around the black-\nbox planner (Table S1). The results were equivalent, with less than 1% difference across\nall metrics, confirming our method did not degrade driving performance. We also\nevaluated concept classification on held-out datasets (Table S2/S3). Mean accuracy\nwas 54%, with 23% precision, 77% recall, and an F1 score of 0.31 (see Section 8.3).\nOverall, these results indicate that CW-Net can be used to ground the decision making\nof high-performance deep motion planners in human-interpretable concepts without\nsacrificing driving performance."}, {"title": "5 Explanations in real-world deployment", "content": "To evaluate the usefulness of the explanations in naturalistic settings [29], we deployed\nCW-Net on a real AV using the Lab2Car wrapper [28]. All tests were performed on\na closed course or private lot with an experienced safety driver. Figure 2 shows the\nexperimental setup inside the AV. We next detail three notable situations in which\nthe explanations proved beneficial."}, {"title": "5.1 Unexpected stopping for nearby vehicles", "content": "We observed that the AV repeatedly came to a stop shortly before a pedestrian\npickup/drop-off zone (Figure 3a). The driver's intuition was that the car stopped\nbecause of the pickup/drop-off zone, but the explanations indicated that the planner"}, {"title": "5.2 Hallucinating a stopped vehicle ahead", "content": "At another location, the AV would reliably come to a stop next to a traffic cone\n(Figure 3b). The driver's initial mental model was that the cone was responsible for\nthe phantom brake. However, the \u201cApproaching stopped vehicle\" (ASV) concept peaked\nshortly before the car stopped. This suggested an alternative hypothesis, that the\nplanner matched the current situation with training scenarios labeled ASV, which in\nturn promotes stopping behavior associated with such scenarios. As a counterfactual\ntest, the cone was removed. The AV exhibited the same stopping behavior at the\nsame location, along with similar ASV probability and speed profiles (L2 similarities\nof 1.12 and 1.6 between the respective time-warped profiles, compared to an average\nL2 > 200 for random events), thus confirming the alternative hypothesis. Note that\nalthough there was no vehicle in front of the AV, the explanation is causally faithful\nto the underlying motion planner and explains why it stopped (namely, because it\nincorrectly detected a stopped vehicle). Figure 3b illustrates a global analysis of ASV,\nshowing it to be a powerful predictor of braking."}, {"title": "5.3 Reacting safely to cyclist", "content": "Finally, we tested the ability of the AV to stop safely for cyclists (Figure 3c). For each\ntest, the driver engaged self-driving mode while approaching a cyclist. The driver was\ninstructed to engage self-driving from a speed at which they felt safe, since this deter-\nmines the subsequent speed of the AV. During the initial tests, the AV reliably stopped\nfor the cyclist. However, the BIKE concept maintained a low probability throughout\neach test (<1%), indicating that CW-Net was failing to detect the cyclist. Over time,\nthe driver became aware of the concept reading and gradually increased their cau-\ntion by initiating self-driving from slower speeds. A post-hoc analysis revealed that,\nalthough the perception system detected the cyclist, the motion planner failed to take\nit into account due to a lack of appropriate input features for cyclists. As a result, it\nchose unsafe trajectories which would have collided with the cyclist. In reality, it was\nthe built-in rule-based systems of the AV that overruled the motion planner and forced\nthe AV to stop. This indicates that the increased caution dictated by the driver's\nupdated mental model was warranted."}, {"title": "6 Confirmation study", "content": "We conducted an online study (N=120) to confirm the statistical significance of our\nresults. Following standard procedures in deployment-based research [21, 31], we sim-\nulated the sequence of key events from the three situations described previously. We\ndesigned a between-subjects study to assess the effect of explanations on the mental\nmodel of subjects in the driver's position. For each situation, subjects in the exper-\nimental group received the CW-Net explanation, while subjects in the control group\nreceived a generic explanation to balance cognitive load (see Figure S4 and Section 8.5\nfor details). Subjects then rated to what extent they agreed with the driver's incorrect\ninitial mental model. Across all situations, the experimental group gave significantly\nlower ratings than the control group (Figure 3, right column; mean result: 3.37\u00b11.63 v."}, {"title": "7 Conclusion", "content": "Our work shows, for the first time, how explainable deep learning can provide useful\nexplanations for the decisions of self-driving cars in the real world. CW-Net achieves\nthis by grounding the reasoning of a pretrained black-box motion planner in human-\ninterpretable concepts corresponding to types of scenario. By revealing otherwise\ninaccessible information about the decision-making process of the motion planner in\nreal time, CW-Net helps align the mental model of the human driver with the machine\ndriver. This allows the human driver to better anticipate and account for mistakes of\nthe AV, ultimately resulting in safer driving. Mental model alignment could addition-\nally build trust and understanding with passengers of driverless AVs by helping them\nanticipate the AV's decisions. This could be particularly beneficial when AV behavior\ndeviates from typical human behavior, such as when driving conservatively or getting\nstuck. Additionally, the explanations provided by CW-Net can help test engineers pro-\nvide more precise feedback to the research scientists and engineers working on model\nimprovement. This would be especially relevant for experimental motion planners that\nare still under development, such as the one used in our study.\nCW-Net extends the original PW-Net [9], which reasons over specific scenario\nprototypes, to general scenario types. In addition to increasing robustness, reasoning\nover types has the added benefit of highlighting which parts of the training distribution\ninfluence behavior at each point in time. This information can be used by researchers\nfor model improvement. For example, the inability of CW-Net to detect the BIKE\nconcept suggests there may not be enough training scenarios with cyclists, or that the\nfocal loss used was ineffective [32], leading to poor performance around cyclists. At\nthe same time, relying on types forgoes some of the benefits of using prototypes. For\nexample, CW-Net can explain that it is stopping because the current scene is similar\nto ASV scenarios in the training data, but it cannot explain why it believes so [33, 34].\nThis suggests a promising avenue of future research: much like humans, who rely on\nboth exemplar-based and rule-based reasoning [35, 36], machines that reason over\nboth prototypes and types could combine the strengths of both approaches, enhancing\ninterpretability while maintaining flexibility in decision making.\nMany safety-critical systems involving human-robot interaction require real-time\nexplanations, including AI wingmen, drone navigation systems, and robotic surgeons.\nSimilarly to AVs, many of these applications increasingly rely on deep learning and\nhave a long tail of failure cases, with potentially catastrophic outcomes. This creates\nan ethical imperative to better understand how they work, so the humans in the\nloop can intervene when necessary. The success of CW-Net suggests that explainable\ndeep learning may prove essential for meeting the safety and regulatory standards for\ndeplying sophisticated safety-critical agents in the real world."}, {"title": "8 Methods", "content": "8.1 Architecture\nThe black-box planner uses a modified version of the DriveIRL architecture\n(Figure 1b) [1]. For the trajectory generator G, we use a heuristic generator that\nproduces 143 jerk-optimal trajectories to anchor waypoints along the route. For the\nscene encoder H, we use the hierarchical vector transformer (HiVT) [37] pretrained for\nmulti-agent motion prediction. In addition to the scene embedding h, this produces\nan additional 3 trajectories for the AV, for a total of k = 146 candidate trajectories. In\nthe scene-trajectory encoder E, trajectories are encoded using a recurrent neural net-\nwork (RNN) and then fed jointly with the scene embedding into a transformer layer\nwhich produces the scene-trajectory embeddings zi. The reward model R is a multi-\nlayer perceptron (MLP). In CW-Net (Figure 1c), the classifier C and the new reward\nmodel R' are MLPs.\n8.2 Training\nIn all tests, we use one of two datasets, either with 500,000 (and 8 concept labels),\nor 3 million data (with 10 concept labels), for a full list of the concept labels and\ntheir meaning see Section 8.5. Each datum was associated with an additional 141\ntrajectories, thus giving between 70.5-423 million training data, each with multiple\nconcept labels.\nOur algorithm assumes access to the original dataset used to train the black-box\nplanner, along with annotated human-understandable concept labels for each of these\ndata points. The annotations can be multi-label, meaning that one datum can be\nassociated with as many concepts as desired or useful.\nDuring training, the parameters of the trajectory generator G, the scene encoder\nH, and the scene-trajectory encoder E are frozen, and only the concept classifier C and\nthe new reward model R' are trainable. Two separate losses are trained simultaneously.\nFirst, a loss is used to train C to predict the correct concept label(s). In our setting, this\nloss combines cross-entropy with binary cross-entropy for different concepts, depending\non the semantics of the corresponding scenario types. For example, in Dataset 1, we use\ncross-entropy to model the steering concepts of the car (LEFT, RIGHT, and STRAIGHT),\nand the speed concepts (STOPPED, SLOW), while also using binary cross-entropy to\npredict the presence of other concepts such as ASV, INTERSECTION, and CLOSE. These\nlosses are then averaged into one:\n$L_{\\text{concept}} = \\frac{1}{2k} \\left( \\frac{1}{M_{CCE}} \\sum_{j=1}^{M_{CCE}} L_{CCE}(C_{i,j}, \\hat{c}_{i,j}) + \\frac{1}{M_{BCE}} \\sum_{l=1}^{M_{BCE}} L_{BCE} (C_{i,l}, \\hat{c}_{i,l}) \\right)$\nwhere MCCE is the number of concepts modeled using categorical cross-entropy (e.g.,\nsteering and speed), and MBCE is the number of concepts modeled using binary cross-\nentropy (e.g., presence of features like ASV, INTERSECTION, and CLOSE). $C_{i,j}$ and $\\hat{c}_{i,j}$\nrepresent the true and predicted labels for the j-th concept under CCE for the i-\nth data point, while $c_{i,l}$ and $\\hat{c}_{i,l}$ represent the true and predicted labels for the l-th"}, {"title": "8.3 Concept separation", "content": "When adding interpretability modules post hoc as we have, there is the possibility\nthat the network will not have learned to separate the concepts of interest, and thus\nfail to be able to predict them accurately [38]. In fact, we observed this in Motional's\nexperimental prototype which we tested (see Table S3), when certain concepts such as\nCLOSE and PEDESTRIAN had poor precision and high recall, relatively speaking. There\nare two important points to note here. First, the better trained and more sophisticated\nan architecture is, the more it naturally learns to separate an impressive number of\nconcepts in an unsupervised manner [39, 40] (often in the millions), so this is unlikely\nto be an issue for most companies with the flagship models in the future. Secondly, even\nif the car has not learned to separate the concepts of e.g. red traffic lights compared to\ngreen ones, this would likely highlight the reason why the car would fail to stop (or go)\nin such a situation, so from an explainability point of view, it would never be an issue,\nin fact it is potentially very useful information, which we showed in the main paper.\n8.4 Alternative architecture\nAlongside our primary causal architecture illustrated in Figure 1, we also developed\nan alternative which gave post-hoc justifications for the car's actions (Figure S1).\nSpecifically, we simply left the black-box planner to drive the car as normal. However,\nwe trained a concept classifier to work in parallel it to the black-box planner, which\nclassified the scene-trajectory embeddings zi, and displayed these predictions while\nthe car drove, similarly to the causal architecture. This approach is beneficial because\nof its relative simplicity and accessibility, although the drawback is that it may be less\nfaithful to the model's reasoning process, as the concept classifications are not directly\nused by the model to rank state-trajectory pairs. However, there is ample evidence\nthat such explanations are often faithful and capable [41, 42], so we include both as\nan option and demonstrate the utility of both.\n8.5 Concept details\nDataset 1 concepts were as follows:"}, {"title": "Steering", "content": "Steering: A classification of either left/right/straight concepts, trained with cross\nentropy loss. The concept of e.g. left represents training data where the car was\nturning left.\n\u2022 Speed: A classification of either slow/stopped concepts, trained with cross entropy\nloss. The concept of e.g. stopped represents training data where the car was stopped.\n\u2022 ASV (Approaching stopped vehicle): Trained with binary cross entropy. The concept\nrepresents data in which the car was approaching stopped vehicles.\n\u2022 Intersection: Trained with binary cross entropy. The concept represents data in\nwhich the car was at an intersection.\n\u2022 Close: Trained with binary cross entropy. The concept represents data in which the\ncar was within 3 Meters of another vehicle."}, {"title": "Dataset", "content": "Dataset 2 had the following concepts:\n\u2022 Slow: Trained with binary cross entropy. The concept represents data in which the\ncar was driving 1-2 meters per second.\n\u2022 Stopped: Trained with binary cross entropy. The concept represents data in which\nthe car was stationary.\n\u2022 Fast: Trained with binary cross entropy. The concept represents data in which the\ncar was driving faster than 2 meters per second.\n\u2022 Stop Sign: Trained with binary cross entropy. The concept represents data in which\nthe car was close to a stop sign.\n\u2022 Traffic Light: Trained with binary cross entropy. The concept represents data in\nwhich the car was close to a traffic light.\n\u2022 Intersection: Trained with binary cross entropy. The concept represents data in\nwhich the car was at an intersection.\n\u2022 Pedestrian: Trained with binary cross entropy. The concept represents data in which\nthe car was close to a pedestrian.\n\u2022 Following: Trained with binary cross entropy. The concept represents data in which\nthe car was following another vehicle.\n\u2022 Bike: Trained with binary cross entropy. The concept represents data in which the\ncar was close to a cyclist.\n\u2022 PUDO (Pedestrian Pickup-Drop-off): Trained with binary cross entropy. The\nconcept represents data in which the car was in a pedestrian pickup drop-off zone."}, {"title": "Evaluation", "content": "In this section we give much greater detail about various aspects related to the eval-\nuation in the main paper. In our tests we deployed a highly experimental AV from\nMotional, partly because these datasets had the necessary annotations, but also to\nmaximize the number of potentially surprising events which would require explanation\nduring the deployment. Our evaluation encompassed (1) a simulation phase with con-\ncept accuracy verification, (2) deployment of the AV itself, and (3) a final confirmation\nstudy. All experiments involving users obtained IRB approval."}, {"title": "Simulation Results", "content": "We tested our CW-Net model across the entire nuPlan validation dataset to see how\nits performance compared to the original black-box algorithm it was trained from. The\ndataset represents the world's first large-scale planning benchmark for autonomous\ndriving, and measures how close a trained AV is to a human expert in L2 distance.\nIn the black-box model, when following the lane or decelerating from high speed,\nthe planner was able to make progress along the route (> 93% of human driving\ndistance), while avoiding collisions (> 90% collision-free) and staying close to the\nground-truth human expert trajectory (<1 m displacement at 5 s). Performance was\nworse when starting from a stop, with less progress (74% of human driving distance),\nmore collisions (81% collision-free), and greater deviation from the human expert (1.2\nm displacement at 5 s). Overall, the results showed our variation of the AV architecture\nhad less than 0.01 L2 difference to the original black-box agent on average across all\nmeasurements, and not meaningfully different, showing that it is possible to train our\nmore interpretable model in Figure 1 without sacrificing performance. The full results\nare in Table S1. For the concept accuracy verification, we used 5% holdout data from\nour training datasets, the results are given in Table S2 and Table S3. Across both\ndatasets, the mean accuracy was 0.54, precision 0.23, recall 0.77, and F1 Score 0.31.\nOverall, the results suggested that the prototype AV did not separate all concepts\nequally well, which suits our purposes as the explanations will highlight when and\nhow this happens, and how it relates to driving performance, thus helping with mental\nmodel refinement (see Section 8.3). Notable results include an F1 score of 0.82 for\ndetecting the SLOW concept, and < 0.00 for detecting the BIKE concept, showing the\nlatter is perhaps not well encoded or understood by the car."}, {"title": "Distribution Comparison", "content": "In this section we demonstrate how the distribution of concept activations differs based\non the deployment environment. The data here focuses on two deployments of the\nsame model in (1) a large carpark with many tight lanes and obstacles, and (2) a large\nopen court test track with the opportunity to drive long, straight distances at a higher\nspeed. This serves somewhat as validation for the concept accuracy in a deployment\nsetting. The data shown in Figure S2 is the full concept activation explanations across\nthe entire deployments when the AV was in self-driving mode only (i.e., all data when\nthe safety driver was in control in manual mode was deleted for this analysis). The\ndifference between deployments is perhaps best highlighted with how the concepts for\nRIGHT and LEFT have generally higher activations in the parking lot compared to the\nlarge track, which involved less turning in general. Other notable differences can be\nseen in SLOW and the AV's speed, in which the parking lot had generally lower values\nin both. Moreover, the STRAIGHT concept has a higher mass on the large track, again\nreflecting the actual environment around it. Lastly, ASV was also higher on the large\ntrack, which was caused by issues with the trajectory generator (see Section 8.5).\nOverall, relative to each other, the classification of concepts reasonably represent the\nenvironment around them and give evidence our system performs fittingly in various\nenvironments."}, {"title": "User Study", "content": "This section serves to give much greater detail about our user study in the main paper.\nWe crowd sourced responses (N=120) simulating the events in the car to see if they\ncorrelated with the driver's mental model during the events, which would allow us to\nfurther extrapolate the results. Note, this is the same principle used in the U.S. Air\nForce called \"spot checking\u201d, and similar research in academia [21, 31]. The point is\nto acquire additional evidence that results would generalize to a larger population,\nwithout the expense of repeating our tests in an expensive deployment environment\n(which was infeasible). Hence, we designed a between subjects study (N=120) to test\nfor the effect of concept-based explanations on the accuracy of a human's mental\nmodel of the car. Both groups were shown the real videos of the three events, and\nasked to rate on a Likert Scale (1-7) how much they agreed with the driver's initial\nmental model of the event. Ideally, after viewing the explanation, they should begin\nto disagree with the initial mental model (which was proven wrong in our deployment\nstudy) and move towards the more correct one based on our causal architecture. We\navoided asking how much they agreed with the driver's new mental model, because\n(1) it is best practice to minimize the number of metrics in a user study to avoid p-\nhacking, and (2) the explanations which essentially state this new mental model may\nlead users to simply agree with such a metric (e.g., one question states that the AV\ndetects a stopped vehicle ahead, so asking people how much they agreed that the AV\nstopped due it detecting a stopped vehicle ahead was judged to be too leading).\nInitially, subjects were given a disclaimer, introduction to the task, and a simple\npractice question before the main study. As an attention check we presented a video of\nthe car driving straight, and asked the question \"I think the car drove straight because\nthe detected the LEFT concept\". As a second attention check we also measured how\nlong users spent on each question, if they took less than 10 seconds, they would be\nexcluded. A final survey was also given to subjects which used questions extrapolated\nfrom our post-hoc interviews with 4 safety-drivers and 1 engineer, but they are not\nrelevant to this paper and not reported."}, {"title": "Materials", "content": "Materials\nIn total there were five videos shown to users in the main materials, three situations\nof the car acting in unusual ways (see Figure S4), and one attention check (a final\nquestion was removed post hoc, see discussion later). One group was given the car's\nparsed explanation as outlined in the main paper, and the control group a replacement"}]}