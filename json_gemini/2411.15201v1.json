{"title": "Beyond Visual Understanding\nIntroducing PARROT-360V for Vision Language Model Benchmarking", "authors": ["Harsha Vardhan Khurdula", "Janit Anjaria", "Basem Rizk", "Aviral Srivastava", "Indus Khaitan", "Rajvardhan Khaitan"], "abstract": "Current benchmarks for evaluating Vision Lan-\nguage Models (VLMs) often fall short in thor-\noughly assessing these models' abilities to un-\nderstand and process complex visual and tex-\ntual content. They typically focus on sim-\nple tasks that do not require deep reasoning\nor the integration of multiple data modali-\nties to solve an original problem. To ad-\ndress this gap, we introduce the PARROT-\n360V Benchmark, a novel and comprehensive\nbenchmark featuring 2487 challenging visual\npuzzles designed to test VLMs on complex vi-\nsual reasoning tasks. We evaluated leading\nmodels-GPT-40, Claude-3.5-Sonnet, and\nGemini-1.5-Pro\u2014using PARROT-360V to as-\nsess their capabilities in combining visual clues\nwith language skills to solve tasks in a manner\nakin to human problem-solving. Our findings\nreveal a notable performance gap: state-of-the-\nart models scored between 28 to 56% on our\nbenchmark, significantly lower than their per-\nformance on popular benchmarks. This under-\nscores the limitations of current VLMs in han-\ndling complex, multi-step reasoning tasks and\nhighlights the need for more robust evaluation\nframeworks to advance the field.", "sections": [{"title": "1 Introduction", "content": "Vision Language Models (VLMs) have shown re-\nmarkable capabilities in integrating visual and tex-\ntual data, excelling in tasks like image captioning\nand object recognition (Wang et al., 2024).\nThe aspiration to create artificial intelligence that\ncan seamlessly integrate into daily life\u2014solving\nproblems, performing tasks, and providing expert\nknowledge-has long been a driving force in tech-\nnological advancement (Mintz and Brodie, 2019).\nRecent developments in VLMs have brought us\ncloser to this vision, showcasing impressive abili-\nties in understanding and generating both textual\nand visual data (Yang et al., 2024a). Models like\nGPT-40, Claude-3.5-Sonnet, and Gemini-1.5-Pro\nhave set new standards in the field, while show-\ncasing high performance for vision related bench-\nmarks.\nThe rapid evolution of these models has sparked\nconcerns. There is a growing fear that AI could\nreplace human labor (Eloundou et al., 2024). These\nfears are often based on hypothetical scenarios\nrather than current capabilities. Despite these ap-\nprehensions, it's crucial to critically assess whether\nthese models truly perform at the levels claimed,\nespecially in complex tasks that mirror real-world\nchallenges.\nOur benchmark, PARROT-360V, contributes\nto the evaluation of leading VLMs by focusing\non step-by-step visual reasoning tasks. We aim\nto identify gaps between reported capabilities and\nactual performance, offering insights into specific\nareas where these models may underperform."}, {"title": "2 Why A New Benchmark?", "content": "Many commonly used benchmarks such as\nMMMU by Yue et al. (2024), ChartQA by Masry\net al. (2022), and AI2D by Kembhavi et al. (2016)\nhave been designed to evaluate VLMs on tasks\nthat are limited in scope, such as basic image-text\nalignment or single-step reasoning. These bench-\nmarks are typically straight forward, and models\ncan at times overfit to the datasets, resulting in mis-\nleadingly high performance scores (Samuel et al.,\n2024). We aim to adequately test models on puz-\nzles that require the skills of image-text alignment,\nmulti-step reasoning and sequential logic handling.\nIn particular order, they correspond to sub-tasks\nthat are critical for real-world decision-making (Tu\net al., 2024) and the analysis steps required."}, {"title": "2.1 Challenges In Reproducibility For VLMS", "content": "Reproducibility is a significant challenge in evalu-\nating vision based tasks, especially when dealing\nwith VLMs (Yang et al., 2024b). Unlike purely\ntextual models, vision models rely on visual input,\nwhich can be subject to variability in data prepro-\ncessing, annotation, and context (Wu et al., 2024).\nThis makes it difficult to replicate the exact condi-\ntions under which a model achieves specific results\nfor other benchmarks.\nA lack of standardization in how input images\nare processed or prompts are structured can lead\nto discrepancies in model outputs when evaluated\nacross different platforms or test environments\n(Anagnostidis and Bulian, 2024). Moreover, exist-\ning benchmarks, which often focus on answering a\nquestion posed at text within an image or on top of\nthe content depicted in it (a graph), not adequately\ncapturing the abilities of the model. The output for\na question is heavily skewed by the size of data it\nwas trained on. Rather benchmarking for VLMs\nshould evaluate perception, and ability to use that\ninformation. And move away from the emphasis\nof answering multiple choice questions from an\nimage, which could have easily been represented\nas a question in text (Yue et al., 2024)."}, {"title": "2.2 A Fairer Evaluation Paradigm", "content": "To ensure fairer comparisons, a benchmark should\nnot just test how much knowledge a model has\nabsorbed but should also evaluate how well it can\nperceive and follow instructions based on the visual\ninputs provided. This is where our PARROT-360V\nshines, as it requires models to integrate visual per-\nception with textual reasoning, testing their ability\nto interpret, reason, and solve complex problems\nstep by step, rather than regurgitating memorized\nknowledge (Duan et al., 2023).\nThis shift in focus is crucial for evaluating VLMs\nin a way that reflects their actual capabilities in real-\nworld scenarios, where their performance must rely\non accurate perception and decision-making rather\nthan simply having been trained on vast quantities\nof data and determine its strengths towards being\nemployed for automation involving visual tasks\n(Schwartz et al., 2023).\nThe PARROT-360V benchmark represents a\nstep forward in addressing key challenges in re-\nproducibility, data bias, and unfair comparisons in\nthe field of VLMs. It provides a rigorous and fair\nbenchmark for evaluating vision models based on\ntheir perceptual and reasoning abilities by employ-\ning Chain-of-Thought (CoT) (Wei et al., 2022) to\nplan how the model is going to solve the current\npuzzle, offering a clearer picture of how these mod-\nels would perform in real-world applications (Wei\net al., 2023)."}, {"title": "3 PARROT-360V Dataset", "content": "The PARROT-360V Benchmark dataset was care-\nfully curated by scraping Jumble puzzles from the\ninternet to challenge VLMs in solving complex\njumbles (Redblock.ai, 2024). Each scraped puz-\nzle combines various elements representing an in-\nstance of gameplay, as shown in Figure 3, which\nserves as the input puzzle to the VLM. Furthermore,\nwe extract additional features from the solved puz-\nzle to obtain the ground truth labels, as shown in\nFigure 4.\nThe dataset corresponds to an established col-\nlection of puzzles, all of which are of the same\nformat, with each containing a different set of\nclues/jumbled words, and a bonus clue with a visual\ncomponent (associated with a cartoon) as shown\nin figure 1. The dataset is intended to evaluate not\nonly language understanding but also visual per-\nception and reasoning, making it a more rigorous\ntest for these models."}, {"title": "3.1 Data Structure", "content": "Each puzzle in the PARROT-360V dataset consists\nof the following features:\n\u2022 Regular Clues: These are scrambled words\ngiven as clues within the puzzle image, which\nthe models must unscramble to find the correct\nwords, and extract characters circled charac-\nters to form a bonus clue.\n\u2022 Visual Clue: A cartoon or image that contains\na visual hint. The models must interpret this\nimage or other relevant information to form a\nbonus answer.\n\u2022 Answer Constraints: The models are re-\nquired to piece together specific letters (often\ncircled in the image) from the unscrambled\nwords to form the bonus answer.\nThe dataset contains 2487 samples, with 14 dis-\ntinct features from the release date of the puzzle to\nthe annotated answers as text within the dataset, as\ndescribed in table 1."}, {"title": "4 PARROT-360V Benchmarking Setup", "content": "We applied our benchmark on three state-of-the-\nart VLMs-GPT-40, Claude-3.5-Sonnet, and\nGemini-1.5-Pro. Requiring the models to inte-\ngrate both visual and textual information to arrive\nat correct answers, the evaluation environment is\ndesigned to simulate a realistic puzzle solving en-\nvironment:\n\u2022 Input: Images of jumbled word puzzles, in-\ncluding visual clues (circled letters, and car-\ntoon characters).\n\u2022 Task: The models are required to solve the\nscrambled words, interpret the visual clues,\nand synthesize the final solution by forming\nbonus answers from the circled letters in the\npuzzle. While framing context from the posed\nquestion in relation to the cartoon (as shown\nin the algorithm 1).\n\u2022 Metrics: We measured the correctness of\ntheir responses and the proportion of hallu-\ncinations, where the model incorrectly used\ncharacters that were not part of the given\nclues. Additionally, we calculated each\nmodel's overall performance across multi-\nple dimensions-accuracy, sequential perfor-\nmance (evaluation of intermediate steps in-\nvolved), and hallucination rate.\nTo mitigate the issue of potential data contam-\nination, we ensured that the setup used was en-\ntirely novel and tests the models for real world\ntask, that cannot be reproduced by simply using its\nknowledge base. Rather we prompt the VLM to\nexplicitly/step-by-step address the required tasks:\nidentify characters, plan on how it is going to han-\ndle the bonus clue, and solve with the entire puzzle.\nOne of the core challenges presented by the\nPARROT-360V benchmark is for VLMs to iden-\ntify the circled letters within the image clues as\nshown in 1 for a model. These letters are crucial\nfor solving the bonus clue, and failure to correctly\nidentify these letters often results in hallucinations\nor incorrect answers from the models.\nIn addition to recognizing circled characters,\nmodels are expected to interpret visual information\nfrom the cartoon or accompanying image (Figure"}, {"title": "4.1 Metrics", "content": "To quantify the models' performance on PAR-\nROT360V, we developed a scoring system that as-\nsigns weights to each component of the puzzle.\nThe scoring system is designed to reflect the im-\nportance of each task and to penalize omissions or\nincorrect answers appropriately.\nScoring components each puzzle consists of:\n\u2022 Four Scrambled Words: Each worth 10\npoints. (There are four scrambled words/clues,\nwithin each puzzle thus a candidate can score\n40 points at the most in this section.)\n\u2022 Synthesizing Answers to Extract Key Char-\nacters: Worth 10 points. (Each unscrambled\nclue serves as a distinct answer. Certain char-\nacters are circled within these answers; con-\ncatenating these circled characters provides\nthe final bonus clue.)\n\u2022 Solution to the Puzzle: Worth 20 points. (Us-\ning the extracted characters and interpreting\nthe cartoon, VLMs gather and synthesize in-\nformation to solve the puzzle.)"}, {"title": "4.2 Scoring Methodology", "content": "For each VLM's attempt at solving a puzzle, we\napplied the following evaluation criteria:\n\u2022 Correct Answer: If the model's answer\nmatches the labeled correct answer exactly\n(case-insensitive), it receives full points for\nthat component*.\n\u2022 No Answer or Incorrect Answer: If the model\nprovides no answer or an incorrect answer, it\nreceives a penalty of -5 points for that compo-\nnent.\n\u2022 Negative Score Adjustment: If the total points\nearned for a puzzle are negative due to penal-\nties, the score is clipped to zero.\n\u2022 Normalization: The total points earned are\ndivided by the total possible points (70) to\nobtain a performance score between 0 and 1,\nrounded to two decimal places.\nLet:\n\u2022 $W_i$ be the weight for component i.\n\u2022 $A$ be the model's answer for component i.\n\u2022 $C_i$ be the correct answer for component i.\n\u2022 T be the total possible points (70).\n*Clue or One of the sequential tasks involved in solving\nthe given puzzle."}, {"title": "5 Results", "content": "The evaluation of GPT-40, Claude-3.5-Sonnet,\nand Gemini-1.5-Pro on our benchmark highlights\nthe significant limitations of existing benchmarks\nin capturing true multimodal reasoning abilities.\nUnlike tasks found in common benchmarks like\nMMMU, MathVista, or ChartQA (as shown in\nthe table 2), PARROT-360V places special empha-\nsis on complex, multi-step reasoning involving vi-\nsual puzzles. This difference is reflected in the\nsharp decline in performance when these models\nare tested on our benchmarking dataset.\nOn benchmarks such as MMMU and Math-\nVista, GPT-40 and Claude-3.5-Sonnet achieved\nhigh scores of 0.69 and 0.72, respectively (Table 2),\nmainly because these tasks focus on simple image-\ntext alignment or basic reasoning. In these tests,\nthe image often serves merely as a backdrop to\na question that could just as easily be presented\nas pure text, reducing the need for genuine visual\nunderstanding.\nPARROT-360V, by contrast, involves complex\ntasks like word unscrambling, bonus clue extrac-\ntion, and interpreting visual elements, all requiring\ndeep integration of visual and textual information.\nGPT-40's accuracy dropped significantly to 0.57,\nClaude-3.5-Sonnet's to 0.50, and Gemini-1.5-Pro's\nto 0.28 (Figure 2) in this more challenging setup,\ndemonstrating how existing benchmarks fail to re-\nflect the complexity required for real-world tasks."}, {"title": "5.1 Visual Perception Failures", "content": "In tasks such as identifying circled letters with-in\nthe puzzle which is an image (Figure 1), all three\ncandidate models struggled. For instance, Gemini-\n1.5-Pro exhibited a high hallucination rate of 72%,\nlargely due to its inability to accurately recognize\nand use visual inputs. This stands in contrast to\nsimpler benchmarks like AI2D, where models face\nstraightforward visual questions with limited need\nfor complex image interpretation (Kembhavi et al.,\n2016). Often the models also failed to synthesize\nvisual scrambled text effectively. The challenge\nof extracting circled letters and forming a correct\nbonus clue required higher-order detail to reason-"}, {"title": "5.2 Hallucination Issues", "content": "All three VLMs exhibited frequent hallucinations\nduring evaluation, particularly when trying to de-\nrive answers from visual cues. While tasks in\nChartQA or MathVista are often solved by ap-\nplying memorized data or pattern recognition, Our\nbenchmark exposed the models' limitations in han-\ndling dynamic, real-time visual information. GPT-\n4o had a hallucination rate of 43%, Claude-3.5-\nSonnet with 50%, and Gemini-1.5-Pro with 72%\nas shown in the figure 2, thus proving how heavily\nmodels depend on structured data rather than real\nreasoning from raw inputs."}, {"title": "6 Discussion", "content": "When we compared the results of our proposed\nbenchmark to benchmarks like MMMU, ChartQA,\nMathVista, and AI2D, it became clear that these\nexisting benchmarks don't truly test a model's abil-\nity to reason through complex, real-world visual\nproblems. In benchmarks like MMMU, models of-\nten achieve high scores (e.g., GPT-40 scoring 0.69\nas shown in table 2) because the tasks typically in-\nvolving static image-text alignment or basic pattern\nrecognition. These benchmarks don't require the\nmodels to think, but rather to retrieve memorized in-\nformation or recognize patterns from training data.\nIn essence, they reduce the challenge to answering\nquestions that could just as easily be text-based.\nIn contrast, our framework challenges the mod-\nels with multi-step reasoning and visual puzzles\nthat demand a higher level of understanding. Mod-\nels can't just rely on large datasets or pre-learned\npatterns; they need to synthesize information from\nboth text and images. For example, tasks like iden-\ntifying circled letters in images and using them\nto solve a bonus clue are far more reflective of\nreal-world complexity than simple image-caption\nmatching. When we saw models like Gemini-1.5-\nPro struggle with hallucinations (72% rate as seen\nin figure 2), it was a clear indication that they're not\ntruly equipped for these kinds of tasks-yet these\nare the tasks that matter when it comes to applying\nAI in fields like healthcare or automation.\nOne of the biggest takeaways from PARROT-\n360V was that the models performed signifi-\ncantly worse on our benchmark-with perfor-\nmance scores as low as 28% for Gemini-1.5-\nPro-compared to traditional benchmarks (Figure\n2). In short, current benchmarks are giving us an in-\ncomplete and often inflated view of what these mod-\nels can really do. The models' performance drop\non PARROT-360V proves that while they might\nexcel at answering questions from pre-learned data,\nthey struggle when it comes to reasoning through\ncomplex, multi-step visual tasks. To move forward,\nwe need benchmarks like our PARROT-360V that\nchallenge these models to think and reason, not just\nrecognize or recall."}, {"title": "7 Conclusion", "content": "With PARROT-360V, we aim to push the bound-\naries of how we evaluate VLMs by focusing on\nreal-world tasks that demand visual perception,\nmulti-step reasoning, and instruction-following.\nWe saw that traditional benchmarks are not enough.\nThey tend to focus on simpler tasks like image-\ntext alignment and QA, which doesn't truly chal-\nlenge the models' ability to understand and process\nboth visual and textual data together. In contrast,\nPARROT-360V makes models tackle tasks that re-\nquire actual reasoning and visual integration, such\nas solving word puzzles with visual clues.\nOur findings reveal that GPT-40 (56%), Claude-\n3.5-Sonnet (50%), and Gemini-1.5-Pro (28%)\nstruggle on our benchmark when handling com-\nplex, real-world tasks. This performance gap un-\nderscores the need for a more reliable benchmark,\nwhich PARROT-360V aims to provide."}, {"title": "8 Limitations", "content": "While PARROT-360V introduces a fresh approach\nto evaluating VLMs, we recognize that there are\nareas where further refinement can enhance its ro-\nbustness. One aspect we've observed is the task\ncomplexity. Puzzles within the proposed bench-\nmarking dataset are intentionally challenging, de-\nsigned to test multi-step reasoning and visual per-\nception. However, there are instances where the\ncomplexity may obscure whether a model's failure\nis due to genuine reasoning difficulties or simply\nthe task's intricacy. As we move forward, main-\ntaining the right balance in task difficulty will help\nensure we are accurately measuring a model's rea-\nsoning capabilities.\nAnother focus is visual perception, as models\nmust interpret visual clues and recognize circled\nletters. We aim to separate perception from rea-\nsoning to ensure fair evaluation. Lastly, to address"}]}