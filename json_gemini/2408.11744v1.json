{"title": "JieHua Paintings Style Feature Extracting Model using Stable Diffusion with ControlNet", "authors": ["Yujia Gu", "Li Haofeng", "Xinyu Fang", "Zihan Peng", "Yinan Peng"], "abstract": "This study proposes a novel approach to extract stylistic features of Jiehua: the utilization of the Fine-tuned Stable Diffusion Model with ControlNet (FSDMC) to refine depiction techniques from artists' Jiehua. The training data for FSDMC is based on the open-source Jiehua artist's work collected from the Internet, which were subsequently manually constructed in the format of (Original Image, Canny Edge Features, Text Prompt). By employing the optimal hyperparameters identified in this paper, it was observed FSDMC outperforms CycleGAN, another mainstream style transfer model. FSDMC achieves FID of 3.27 on the dataset and also surpasses CycleGAN in terms of expert evaluation. This not only demonstrates the model's high effectiveness in extracting Jiehua's style features, but also preserves the original pre-trained semantic information. The findings of this study suggest that the application of FSDMC with appropriate hyperparameters can enhance the efficacy of the Stable Diffusion Model in the field of traditional art style migration tasks, particularly within the context of Jiehua.", "sections": [{"title": "1 INTRODUCTION", "content": "Style transfer has become a popular research topic in the field of computer vision and image processing, with a particular focus on the style transfer of Chinese landscape painting using stable diffusion techniques. Several recent studies have explored different methods and models to achieve high-quality style transfer results, especially in the context of Chinese ancient painting styles. Wang et al. [1] introduced the GLStyleNet model, which combines global and local pyramid features to achieve exquisite style transfer with superior quality. Their approach has been demonstrated to be effective, particularly in tasks related to Chinese ancient painting style transfer. This highlights the importance of incorporating both global and local features in the style transfer process to enhance the overall quality of the results. On the other hand, Lee [2] proposed the Stable Style Transformer, which follows a delete and generate approach with an encoder-decoder architecture for text style transfer. While this work focuses on text style transfer, the two-stage approach could potentially be adapted for image style transfer tasks, including Chinese landscape painting styles. Gao et al. [3] shifted the focus from transferring styles of artistic paintings to the automatic generation of realistic paintings using generative adversarial networks (GANs). By leveraging advanced methods such as Neural Style Transfer (NST) and unsupervised cross-domain image translation, this study explores the generation of realistic paintings, including landscapes and portraits, through machine learning techniques. Liu [4] addressed the challenges in training algorithms for image oil painting style transfer by proposing an improved generative adversarial network based on gradient penalty. By incorporating total variance loss constraints, the study aimed to enhance the edge and texture details in the migration process of image oil painting styles. This highlights the importance of stability and convergence in the training process for achieving high-quality style transfer results. Peng et al. [5] introduced the Contour-enhanced CycleGAN Framework for style transfer from scenery photos to Chinese landscape paintings. By enhancing the contours in the style transfer process, the framework aimed to improve the fidelity and realism of the transferred images, particularly in the context of Chinese landscape painting styles. Moreover, Way et al. [6] proposed the TwinGAN model for imitating multiple styles of Chinese landscape paintings. By leveraging deep learning techniques, this study aimed to provide a comprehensive analysis of different Chinese landscape painting styles and compare them with existing works in the field of style transfer. In conclusion, the literature on style transfer of Chinese landscape painting using stable diffusion techniques showcases a diverse range of approaches and models aimed at achieving high-quality and realistic style transfer results. By incorporating global and local features, leveraging generative adversarial networks, and enhancing contours in the transfer process, researchers have made significant strides in advancing the field of style transfer for Chinese landscape painting styles. Further research in this area could focus on refining existing models, exploring new techniques, and expanding the application of style transfer to other artistic domains. These methods help to extract the feature embedding of Chinese landscape paintings more efficiently and completely, paving the way for further innovation in this field. Based on the previous research, this paper will aim to find out the generative model for extracting the features of Jiehua [7], which is a type of Chinese landscape paintings. Main contributions of this research include:\n1) We collected datasets of ink paintings, oil paintings, and Jiehua on the Internet, including open source works by masters such as Zhang Xiao Gang, Yu Zhi Ding, and Yao Wen Han.\n2) We used CycleGAN Models to Learn jiehua and Evaluate Its Style Transfer Ability\n3) We leveraged Stable Diffusion Model with ControlNet to Learn Jiehua and Evaluate its Style Transfer Capability\n4) Comparison of the Style Transfer Capabilities of the two models was carried out by both machine and manual evaluation."}, {"title": "2 RELATED WORK", "content": "CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks has been a groundbreaking work in the field of image-to-image translation. Introduced by Zhu et al. [8], CycleGAN is capable of transforming images from one domain to another without the need for paired examples. This is achieved through the innovative use of cycle consistency loss, which ensures that the forward and reverse translations are consistent, thus preserving the original image content.\nThe key features of CycleGAN include its ability to handle unpaired data and its robust performance in various image translation tasks. It has been successfully applied in a wide range of applications, such as artistic style transfer, data augmentation for medical imaging, and cross-domain adaptation in computer vision tasks.\nCompared to other GAN-based approaches, CycleGAN stands out for its flexibility and effectiveness in scenarios where paired training data is scarce or non-existent. However, it is important to note that CycleGAN may still face"}, {"title": "3 METHODOLOGY", "content": "CycleGAN: CycleGANs are a special variant of traditional GANs. They can also create new data samples, but do so by transforming the input samples instead of creating them from scratch. In other words, they learn to transform data from two data sources. These data can be selected by the scientist or developer who provided the dataset for this algorithm. In the case where the two data sources were images of dogs and images of cats, the algorithm was able to effectively be able to convert images of cats to images of dogs and vice versa.\nCycleGAN is a neural network that learns two data conversion functions between two domains. One of them is G(x). It converts a given sample x \u2208 X into an element of domain Y. The second is F (y), which converts a sample element y \u2208 Y into an element of domain X.\n\n$G: X \u2192 Y$\n$F = Y \u2192 X$\nAs shown above the entire architecture has two GANs, which form a CycleGAN. To learn F and G, two conventional GANs are used. each GAN has a network of generators inside that learns how to transform the data as needed. the first generator of the GAN learns to compute F, and the second generator of the GAN learns to compute G. The first generator of the GAN learns to compute F, and the second generator of the GAN learns to compute G.\n$G: Generates \u0177 from sample x$\n$F: Generates \u00ee from sample y$\nBelow are the definitions of the generator functions G and F:\n$D_x: Discrinminates y from G(x)$\n$D_y: Discrinminates from F(y)$\nFurthermore, each generator is associated with a discriminator that learns to distinguish the actual data y from the synthetic data G(x).\nThus, CycleGAN consists of two generators and two discriminators that learn the transformation functions F and G. This structure is shown in the following figure 1:\nFor each GAN network loss function, each GAN generator will learn its corresponding transform function (F or G) by minimising the loss. The generator loss is calculated by measuring the difference between the generated data and the target data (e.g., the difference between a generated image of a cat compared to an image of a real cat). The greater the difference, the higher the penalty the generator will be subjected to.\nThe discriminator loss is also used to train the discriminator to be good at distinguishing real data from synthetic data.\nWhen these two are set together, they will improve each other. The generator is trained to spoof the discriminator, and the discriminator will be trained to better distinguish real data from synthetic data. As a result, the generator will be very good at creating/transforming the required data (learning the required transformations, e.g. F). Overall, GAN losses look like:\n$Loss_{gan} (G, D_x) = E[log(D_x(x))] + E[log(1 \u2212 D_x(G(x)))]$\nAs shown in the formula, the former $E [log (D_x(x))]$ is the loss for Discriminator loss, while the latter is the loss for Generator loss.\nFor the second generator-discriminator pair, a similar loss can be written:\n$LOSS_{GAN} (F, D_y) = E [log (D_y (y))] + E [log (1 - D_y(F(y)))]$\nTraining CycleGAN using only GAN losses is not guaranteed to maintain cycle consistency. Therefore, an additional cyclic consistency loss is used to enforce this property. Define this loss as the difference between the input value x and the forward prediction F (G(x)) and the input value y and the forward prediction G (F(y)). The larger the difference, the further away the prediction is from the original input. Therefore, the whole loss formula is like that:\n$Loss = Loss_{GAN} (F, D_y) + [E|G(F(x)) \u2013 x| + E|F(G(y)) \u2013 y|]$\nStable Diffusion with ControlNet\nIn a traditional diffusion model without ControlNet, where the original neural network F input x obtains y, the parameters are denoted by \u0398. And the mapping relationship is given in the following equation:\n$y = F(x; \u0398)$\nIn ControlNet, it is the original neural network F of the model that is locked, set as locked copy.\nThen a copy of the original network is made, called a trainable copy, on which operations are performed to impose control conditions. The results of the imposed control conditions are then added to the results of the original model to obtain the final output.\nAfter all these operations, after applying the control conditions, the output of the original network is finally modified to:\n$y_c = F(x; \u0398) + Z(F(x + Z(c; \u0398_{z1}); c); \u0398_{z2})$\nWhere zero convolution, or zero convolution layer Z is initialized with weight and bias of 0. The parameters of the two zero convolution layers are {\u0398z1, \u0398z2}."}, {"title": "EXPERIMENT", "content": "There are 112 images in the dataset used in this paper, with 44 Jiehuas and the other traditional paintings draw by Chinese artists. And all images are in the shape of (512,512). Then it is to construct the dataset for fine-tuning Stable Diffusion Model with ControlNet, here firstly the name of each artist is used as the text prompt for two kinds of Jiehua. with the prompt data, canny edge extraction is performed for each original image, and the result is used as the null condition image. Finally the (original image, control condition, text prompt) format is formed to fine tune the model.\nFot the Stable Diffusion Model with ControlNet, the pre-trained Text-to-Image model used in this paper is fine-tuned based on 'Stable-Diffusion-v1-5', which checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 checkpoint.\nImportant hyperparameters during fine-tuning are plotted below:"}, {"title": "4 CONCLUSION", "content": "In this paper, we investigate the performance of Fine-tuned Stable Diffusion Model with ControlNet (FSDMC) and CycleGAN, both mainstream models for Style Transfer tasks, in getting the style features of Jiehua painting. Firstly, in the introduction section, we outline some of the Generative models and its application in Style Transfer task. Subsequently, in the Related Work section, we introduce CycleGAN and Stable Diffusion Model with ControlNet and use them as comparative models in the Experimental section. In the methodology section, we present the essential principles of two completely distinct models. In the experimental part, we compare the 2 models with FID evaluation metric and experts rating, which conclude that the FSDMC has a very powerful performance on Jiehua Paintings."}]}