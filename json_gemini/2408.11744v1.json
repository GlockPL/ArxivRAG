{"title": "JieHua Paintings Style Feature Extracting Model using Stable Diffusion with ControlNet", "authors": ["Yujia Gu", "Li Haofeng", "Xinyu Fang", "Zihan Peng", "Yinan Peng"], "abstract": "This study proposes a novel approach to extract stylistic features of Jiehua: the utilization of the Fine-tuned Stable Diffusion Model\nwith ControlNet (FSDMC) to refine depiction techniques from artists' Jiehua. The training data for FSDMC is based on the open-\nsource Jiehua artist's work collected from the Internet, which were subsequently manually constructed in the format of (Original\nImage, Canny Edge Features, Text Prompt). By employing the optimal hyperparameters identified in this paper, it was observed\nFSDMC outperforms CycleGAN, another mainstream style transfer model. FSDMC achieves FID of 3.27 on the dataset and also\nsurpasses CycleGAN in terms of expert evaluation. This not only demonstrates the model's high effectiveness in extracting Jiehua's\nstyle features, but also preserves the original pre-trained semantic information. The findings of this study suggest that the\napplication of FSDMC with appropriate hyperparameters can enhance the efficacy of the Stable Diffusion Model in the field of\ntraditional art style migration tasks, particularly within the context of Jiehua.", "sections": [{"title": "1 INTRODUCTION", "content": "Style transfer has become a popular research topic in the field of computer vision and image processing, with a\nparticular focus on the style transfer of Chinese landscape painting using stable diffusion techniques. Several recent\nstudies have explored different methods and models to achieve high-quality style transfer results, especially in the\ncontext of Chinese ancient painting styles. Wang et al. [1] introduced the GLStyleNet model, which combines global\nand local pyramid features to achieve exquisite style transfer with superior quality. Their approach has been\ndemonstrated to be effective, particularly in tasks related to Chinese ancient painting style transfer. This highlights\nthe importance of incorporating both global and local features in the style transfer process to enhance the overall\nquality of the results. On the other hand, Lee [2] proposed the Stable Style Transformer, which follows a delete and\ngenerate approach with an encoder-decoder architecture for text style transfer. While this work focuses on text\nstyle transfer, the two-stage approach could potentially be adapted for image style transfer tasks, including Chinese\nlandscape painting styles. Gao et al. [3] shifted the focus from transferring styles of artistic paintings to the\nautomatic generation of realistic paintings using generative adversarial networks (GANs). By leveraging advanced\nmethods such as Neural Style Transfer (NST) and unsupervised cross-domain image translation, this study explores\nthe generation of realistic paintings, including landscapes and portraits, through machine learning techniques. Liu\n[4] addressed the challenges in training algorithms for image oil painting style transfer by proposing an improved\ngenerative adversarial network based on gradient penalty. By incorporating total variance loss constraints, the\nstudy aimed to enhance the edge and texture details in the migration process of image oil painting styles. This\nhighlights the importance of stability and convergence in the training process for achieving high-quality style\ntransfer results. Peng et al. [5] introduced the Contour-enhanced CycleGAN Framework for style transfer from\nscenery photos to Chinese landscape paintings. By enhancing the contours in the style transfer process, the\nframework aimed to improve the fidelity and realism of the transferred images, particularly in the context of\nChinese landscape painting styles. Moreover, Way et al. [6] proposed the TwinGAN model for imitating multiple\nstyles of Chinese landscape paintings. By leveraging deep learning techniques, this study aimed to provide a\ncomprehensive analysis of different Chinese landscape painting styles and compare them with existing works in\nthe field of style transfer. In conclusion, the literature on style transfer of Chinese landscape painting using stable\ndiffusion techniques showcases a diverse range of approaches and models aimed at achieving high-quality and\nrealistic style transfer results. By incorporating global and local features, leveraging generative adversarial\nnetworks, and enhancing contours in the transfer process, researchers have made significant strides in advancing\nthe field of style transfer for Chinese landscape painting styles. Further research in this area could focus on refining\nexisting models, exploring new techniques, and expanding the application of style transfer to other artistic domains.\nThese methods help to extract the feature embedding of Chinese landscape paintings more efficiently and\ncompletely, paving the way for further innovation in this field. Based on the previous research, this paper will aim\nto find out the generative model for extracting the features of Jiehua [7], which is a type of Chinese landscape\npaintings. Main contributions of this research include:\n1) We collected datasets of ink paintings, oil paintings, and Jiehua on the Internet, including open source works by\nmasters such as Zhang Xiao Gang, Yu Zhi Ding, and Yao Wen Han.\n2) We used CycleGAN Models to Learn jiehua and Evaluate Its Style Transfer Ability\n3) We leveraged Stable Diffusion Model with ControlNet to Learn Jiehua and Evaluate its Style Transfer Capability\n4) Comparison of the Style Transfer Capabilities of the two models was carried out by both machine and manual\nevaluation."}, {"title": "2 RELATED WORK", "content": "CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks has been a\ngroundbreaking work in the field of image-to-image translation. Introduced by Zhu et al. [8], CycleGAN is capable\nof transforming images from one domain to another without the need for paired examples. This is achieved through\nthe innovative use of cycle consistency loss, which ensures that the forward and reverse translations are consistent,\nthus preserving the original image content.\nThe key features of CycleGAN include its ability to handle unpaired data and its robust performance in various\nimage translation tasks. It has been successfully applied in a wide range of applications, such as artistic style transfer,\ndata augmentation for medical imaging, and cross-domain adaptation in computer vision tasks.\nCompared to other GAN-based approaches, CycleGAN stands out for its flexibility and effectiveness in scenarios\nwhere paired training data is scarce or non-existent. However, it is important to note that CycleGAN may still face\nchallenges in certain cases, such as handling fine-grained details or maintaining color consistency across\ntranslations.\nIn the context of related works, it is essential to acknowledge the contributions of other researchers who have\nbuilt upon or extended the ideas of CycleGAN.\nStable Diffusion Model: The advent of generative models in the realm of deep learning has opened new avenues\nfor creating realistic and diverse data samples. Among these, diffusion models have emerged as a powerful\nframework for generating high-quality images and other data types. This section reviews the evolution from latent\ndiffusion models to the more refined Stable Diffusion Model.\nLatent Diffusion Models are a class of generative models that operate on a latent space representation of data.\nThey have been pivotal in advancing the field of image synthesis, offering a probabilistic approach to model the data\ndistribution. The foundational work in this area, such as that by Rombach et al. [9], laid the groundwork for\nsubsequent developments in the field.\nBuilding upon the principles of latent diffusion, the Stable Diffusion Model introduces a series of innovations\nthat enhance the stability and efficiency of the diffusion process. Unlike its predecessors, the Stable Diffusion Model,\nincorporates advanced techniques to stabilize the diffusion process, leading to faster convergence and improved\nsample quality.\nThe Stable Diffusion Model has found applications in a variety of domains, including but not limited to image\ngeneration, where it has demonstrated its ability to produce high-fidelity images with diverse content, and style\ntransfer, where it can adapt the aesthetic of an image while preserving its semantic content.\nControlNet: In the realm of text-to-image synthesis, the advent of diffusion models has marked a significant leap\nforward, enabling the creation of visually striking images from textual descriptions However, these models often\nstruggle with precise spatial control, which limits their ability to generate images that accurately reflect complex\ncompositions and forms as envisioned by the user. To address this limitation, Zhang et al. introduce ControlNet, a\nneural network architecture designed to integrate spatial conditioning controls into large, pretrained text-to-image\ndiffusion models [10].\nThe paper demonstrates the effectiveness of ControlNet in conditioning image generation using various inputs\nsuch as Canny edges, human poses, and segmentation maps. Notably, the training of ControlNets is shown to be\nrobust across datasets of varying sizes, from less than 50k to over 1 million images, suggesting its versatility and\nresilience against overfitting and catastrophic forgetting\ncommon challenges in finetuning large models with\nlimited data [10]."}, {"title": "3 METHODOLOGY", "content": "CycleGAN: CycleGANs are a special variant of traditional GANs. They can also create new data samples, but do\nso by transforming the input samples instead of creating them from scratch. In other words, they learn to transform\ndata from two data sources. These data can be selected by the scientist or developer who provided the dataset for\nthis algorithm. In the case where the two data sources were images of dogs and images of cats, the algorithm was\nable to effectively be able to convert images of cats to images of dogs and vice versa.\nCycleGAN is a neural network that learns two data conversion functions between two domains. One of them is\nG(x). It converts a given sample $x \\in X$ into an element of domain Y. The second is F (y), which converts a sample\nelement $y \\in Y$ into an element of domain X.\nG: X \u2192 Y\nF = Y X\nAs shown above the entire architecture has two GANs, which form a CycleGAN. To learn F and G, two\nconventional GANs are used. each GAN has a network of generators inside that learns how to transform the data as\nneeded. the first generator of the GAN learns to compute F, and the second generator of the GAN learns to compute\nG. The first generator of the GAN learns to compute F, and the second generator of the GAN learns to compute G.\nG: Generates \u0177 from sample x\nF: Generates \u00ee from sample y\nBelow are the definitions of the generator functions G and F:\n$D_x$: Discrinminates y from G(x)\n$D_y$: Discrinminates from F(y)\nFurthermore, each generator is associated with a discriminator that learns to distinguish the actual data y from\nthe synthetic data G(x).\nThus, CycleGAN consists of two generators and two discriminators that learn the transformation functions F and\nG. This structure is shown in the following figure 1:\nFor each GAN network loss function, each GAN generator will learn its corresponding transform function (F or\nG) by minimising the loss. The generator loss is calculated by measuring the difference between the generated data\nand the target data (e.g., the difference between a generated image of a cat compared to an image of a real cat). The\ngreater the difference, the higher the penalty the generator will be subjected to.\nThe discriminator loss is also used to train the discriminator to be good at distinguishing real data from\nsynthetic data.\nWhen these two are set together, they will improve each other. The generator is trained to spoof the\ndiscriminator, and the discriminator will be trained to better distinguish real data from synthetic data. As a result,\nthe generator will be very good at creating/transforming the required data (learning the required transformations,\ne.g. F). Overall, GAN losses look like:\n$Loss_{gan} (G, D_x) = E[log(D_x(x))] + E[log(1 \u2212 D_x(G(x)))]$\nAs shown in the formula, the former $E [log (D_x(x))]$ is the loss for Discriminator loss, while the latter is the loss\nfor Generator loss.\nFor the second generator-discriminator pair, a similar loss can be written:\n$LOSS_{GAN} (F, D_y) = E [log (D_y (y))] + E [log (1 - D_y(F(y)))]$\nTraining CycleGAN using only GAN losses is not guaranteed to maintain cycle consistency. Therefore, an\nadditional cyclic consistency loss is used to enforce this property. Define this loss as the difference between the\ninput value x and the forward prediction F (G(x)) and the input value y and the forward prediction G (F(y)). The\nlarger the difference, the further away the prediction is from the original input. Therefore, the whole loss formula\nis like that:\nLoss = LossGAN (F, Dy) + [E|G(F(x)) \u2013 x| + E|F(G(y)) \u2013 y|]\nStable Diffusion with ControlNet\nIn a traditional diffusion model without ControlNet, where the original neural network F input x obtains y, the\nparameters are denoted by O. And the mapping relationship is given in the following equation:\ny = F(x; 0)\nIn ControlNet, it is the original neural network F of the model that is locked, set as locked copy.\nThen a copy of the original network is made, called a trainable copy, on which operations are performed to\nimpose control conditions. The results of the imposed control conditions are then added to the results of the original\nmodel to obtain the final output.\nAfter all these operations, after applying the control conditions, the output of the original network is finally\nmodified to:\n$y_c = F(x; 0) + Z(F(x + Z(c; \u03b8_{z1}); c); \u03b8_{z2})$\nWhere zero convolution, or zero convolution layer Z is initialized with weight and bias of 0. The parameters of\nthe two zero convolution layers are {$\u03b8_{z1}$, $\u03b8_{z2}$}.\nThe structure of the model is shown in the figure above, where the control conditions are added to the original\ninputs after being zero-convolutionally added, and then added to the replicated neural network block of ControlNet\n(Trainable copy), where the output of the network is added to the output of the original network after doing zero-\nconvolution once more.\nThe untrained ControlNet parameters after initialization should look like this:\n$\\begin{cases}\nZ(c; \u03b8_{z1}) = 0 \\\\\nF(x + Z(c; \u03b8_{z1}); \u03b8_c) = F(x; \u03b8_c) = F(x; \u03b8) \\\\\nZ(F(x + Z(c; \u03b8_{z1}); c); \u03b8_{z2}) = Z(F(x; \u03b8_c); \u03b8_{z2}) = 0\n\\end{cases}$\nThis means that when ControlNet is untrained, the output is 0, and the number added to the original network is\nalso 0. This has no effect on the original network and ensures that the performance of the original network is\npreserved. Afterwards, ControlNet training is only optimized on the original network, which can be considered the\nsame as fine-tuning the network.\nThen, for the loss function during training, the Stable Diffusion with ControlNet uses the following functions:\n$L = E_{z_0,t,c_t,c_f,\u03f5\u223cN(0,1)} [|| \u03f5 \u2013 \u03f5_\u03b8 (z_t, t, c_t, c_f)) ||]$\nWhere the latent variables obtained from the sampled $z_t$ after denoising using the network $\u03f5_\u03b8$ and the original\nimage through the network e were used to calculate L2loss to see how well they were reconstructed.\nThe latent variable is obtained from the original image after e, and the graph after reconstruction by the network\n$\u03f5_\u03b8$ is calculated as L2loss. The original Stable Diffusion decoder must deal with sample $z_t$ and time step t, where\ntwo control conditions have been added: the text prompt $C_t$ and the task-related prompt $C_f$.\nDuring training 50% of the textual cues $C_t$ were randomly replaced with empty strings. This facilitates the\nidentification of semantic content from control conditions by the ControlNet network. The purpose of this is that\nwhen Stable Diffusion does not have a prompt, the encoder is able to get more semantics from the input control\ncondition in place of the prompt, which is also known as classifier-free guidance."}, {"title": "EXPERIMENT", "content": "There are 112 images in the dataset used in this paper, with 44 Jiehuas and the other traditional paintings draw\nby Chinese artists. And all images are in the shape of (512,512). Then it is to construct the dataset for fine-tuning\nStable Diffusion Model with ControlNet, here firstly the name of each artist is used as the text prompt for two kinds\nof Jiehua. with the prompt data, canny edge extraction is performed for each original image, and the result is used\nas the null condition image. Finally the (original image, control condition, text prompt) format is formed to fine tune\nthe model.\nFot the Stable Diffusion Model with ControlNet, the pre-trained Text-to-Image model used in this paper is fine-\ntuned based on 'Stable-Diffusion-v1-5', which checkpoint was initialized with the weights of the Stable-Diffusion-\nv1-2 checkpoint.\nImportant hyperparameters during fine-tuning are plotted below:\nAnd for the CycleGAN, we just set the default hyperparameters during training except for batch_size and\nlearning_rate. Because of CycleGAN specifically designed to deal with small dataset size, this paper did not use data\naugmentation.\nEvaluation Metric\nThe present study employs the FID score as a means of conducting an objective evaluation of the results.\nFurthermore, three experts in the field of Jiehua were invited to provide an evaluation.\nThe FID score represents an improvement on the IS. It is generated by comparing the generated image with the\nreal image and calculating the 'distance value', which is used to generate the evaluation score. A smaller indicator\nvalue indicates a better result. The FID score is generated by comparing the generated image with the real image\nand calculating the 'distance value', which is generated by comparing the generated image with the real image. A\nsmaller value of the metric indicates a better result.\nExperiment result\nIn the experimental setup, the fine-tuned Stable Diffusion Model with ControlNet is used to generate an\nimage based on the prompt 'Green grasslands, Grey sky, People in bright colours, {artist's name} style',\nand then a random picture in the training set is used for FID calculation. This process is repeated 10 times\nto get the average FID result. And for the CycleGAN after full training, this paper chose to put other non-\nJiehua images in the dataset inside the model for inference. And then we used those predicted images to\ncalculate the FID value with the same strategy as the former. The results are as follows:\nFrom the above table, it can be clearly seen that the FID of CycleGAN is seriously higher than Fine-tuned Stable\nDiffusion with ControlNet. this means that CycleGAN is less capable of extracting the features of Jiehua. And\nswitching to other style migration images, such as Zebra to Horse, re-trained CycleGAN again shows amazing\nperformance. Therefore, in addition to the reason of data volume, CycleGAN is worth exploring in Jiehua feature\nlifting ability.\nSimilarly, Fine-tuned Stable Diffusion with ControlNet is also ahead of CycleGAN in the expert scoring session.\nThe following figure 3 is the inference result based on Fine-tuned Stable Diffusion with ControlNet, and it can be\nclearly seen that the ability of the model to extract Jiehua after fine-tuning is very strong."}, {"title": "4 CONCLUSION", "content": "In this paper, we investigate the performance of Fine-tuned Stable Diffusion Model with ControlNet (FSDMC)\nand CycleGAN, both mainstream models for Style Transfer tasks, in getting the style features of Jiehua painting.\nFirstly, in the introduction section, we outline some of the Generative models and its application in Style Transfer\ntask. Subsequently, in the Related Work section, we introduce CycleGAN and Stable Diffusion Model with ControlNet\nand use them as comparative models in the Experimental section. In the methodology section, we present the\nessential principles of two completely distinct models. In the experimental part, we compare the 2 models with FID\nevaluation metric and experts rating, which conclude that the FSDMC has a very powerful performance on Jiehua\nPaintings."}]}