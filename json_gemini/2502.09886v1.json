{"title": "Video2Policy: Scaling up Manipulation Tasks in Simulation through Internet Videos", "authors": ["Weirui Ye", "Fangchen Liu", "Zheng Ding", "Yang Gao", "Oleh Rybkin", "Pieter Abbeel"], "abstract": "Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.", "sections": [{"title": "1. Introduction", "content": "Training generalist policies requires collecting large quan-tities of diverse robotic expert data. However, collecting data through teleoperation is constrained by high operation costs, while collecting data from autonomous policies can be unsafe, or result in low-quality data. Simulation offers an appealing alternative to real-world data that does not suffer from these challenges, and can be used to train general and robust policies (Hwangbo et al., 2019; Andrychowicz et al., 2020). Recent work has explored automatically generating diverse and relevant tasks in simulation as a way to create a scalable pipeline for generating robotics data (Deitke et al., 2022; Wang et al., 2023b;c; Makatura et al., 2023).\nHowever, existing methods primarily rely on text-only task specification using Large Language Models (LLMs), which do not have grounded robotics knowledge. They often produce tasks that are not diverse, have uninteresting behavior, or use uninteresting object assets and are thus less useful for training generalist policies. To better capture the real-world distributions of task behaviors and objects, we propose leveraging RGB videos from the internet to create corresponding tasks. Unlike Real2Sim approaches that construct digital twins (Hsu et al., 2023; Torne et al., 2024) for a single scene, we want to train a generalist policy for multiple scenes and therefore we do not require perfect reconstructions. Instead, we leverage large amounts of internet videos to capture task-relevant information such as object assets and scene layouts. We then generate simulated tasks using a Vision-Language Model (VLM) that can take the video, video captions, object meshes, sizes, and 6D poses, and produce corresponding task codes, which can be executed to generate scenes.\nBeyond task proposals, we require an efficient and automatic way to solve tasks. Naively applying reinforcement or imitation learning is challenging as it requires manual human effort for each task to create demonstrations or reward functions. Inspired by the recent advancements of LLMs in code generation for various tasks (Achiam et al., 2023; Roziere et al., 2023), some researchers have proposed automating policy learning or deployment through using an LLM to produce policy code directly (Huang et al., 2023b; Liang et al., 2023; Wang et al., 2023b), or to produce reward function code (Ma et al., 2023; Wang et al., 2023c). Gensim (Wang et al., 2023b) leverages this idea for unsupervised task generation. However, predicting goals restricts it to simple tasks as it does not account for dynamic tasks or tasks that involve complex object interactions.\nIn contrast, reinforcement learning (RL) is effective at solving complex tasks (Schulman et al., 2017; Ye et al., 2021; Hafner et al., 2023; Wang et al., 2024; Springenberg et al., 2024). RoboGen (Wang et al., 2023c) leverages LLM-generated reward functions for RL. However, it is hard to scale as it requires manual success functions. However, since we leverage both text prompts and explicit visual prior knowledge from human videos, we can leverage that information for better success functions.\nWe propose Video2Policy, a framework to leverage internet RGB videos in an automated pipeline to create simulated data to train generalist police, as shown in Fig. 1. It produces code for task generation in simulation using VLMs and learns policies for those tasks via RL. This autonomous approach allows us to easily scale up data generation by internet videos to produce visually grounded tasks. Our framework leverages both behavioral and object diversity from the videos, enabling generalization both at the object level as well as task level. Specifically, our framework consists of two phases: (1) We reconstruct the object meshes involved in the tasks from videos as task assets, and extract the 6D pose of each object; (2) We leverage the VLM to write the task code based on visual information and prompts, and learn RL policies by the iterative generated reward functions. Ultimately, we obtain the learned policy model that demonstrates behavior similar to the input video. And we can transfer the learned policy back to the real world.\nFor experiments, we focus on table-top manipulation tasks with a single robot arm in IsaacGym simulator(Makoviychuk et al., 2021). We conduct experiments on the Something-Something V2 (SSv2) video dataset (Goyal et al., 2017), consist of human daily behaviors with diverse scenes and language instructions. We reconstruct over 100 videos on 9 distinct tasks from the dataset. To evaluate more complex behaviors, we also utilize three self-recorded videos. Significantly, the results indicate the learned policies significantly outperform the baselines from previous LLM-driven methods. We achieve 88% success rates on average. To demonstrate the efficacy of our framework, we (1) train a general policy by imitation learning from the simulation data collected from the learned policies for generalization capabilities, and (2) apply the learned general policy to the real robot in a sim2real manner. We find that it can achieve 75% success rates in simulation from the 10 unseen videos with the same behavior while achieving 47% success rates on real robot after deployment."}, {"title": "2. Related Work", "content": "Real2Sim Scene Generation for Robotics Generating real-istic and diverse scenes for robotics has recently emerged as a significant challenge aimed at addressing data issues through simulation. Some researchers have developed Real2Sim pipelines for image-to-scene or video-to-scene generation. Certain studies (Hsu et al., 2023; Torne et al., 2024) focus on constructing digital twins that facilitate the transition from the real world to simulation; however, these approaches often depend on specific real-world scans or extensive human assistance. (Dai et al.) introduces the concept of digital cousins, while (Chen et al., 2024) employs inverse graphics to enhance data diversity. Nevertheless, their approach to diversity primarily involves replacing various assets. The lack of task-level diversity hinders the ability to capture the distribution of real-world tasks, and the constraints of specific data formats complicate the scalability of robotics data. Although Real2Code (Mandi et al., 2024) aims to build simulation scenes from images, it focuses on articulation parts and requires in-domain code data.\nScaling up Simulation Tasks Previously, researchers aimed to build simulation benchmarks to facilitate scalable skill learning and standardized workflows (Li et al., 2023; Gu et al., 2023; Srivastava et al., 2022; Nasiriany et al., 2024). Most of these benchmarks were constructed manually, making them difficult to scale. Recently, some researchers have focused on text-to-scene generation, emphasizing the creation of diverse scenes. Works such as (Deitke et al., 2022; Makatura et al., 2023; Liang et al.; Chen et al., 2023) utilize procedural asset generation or domain randomization, while (Jun & Nichol, 2023; Yu et al., 2023a; Poole et al., 2022) engage in text-to-3D asset generation. Although these approaches can achieve asset-level or scene-level diversity in robotic tasks, they fall short in delivering task-level diversity. Gensim (Wang et al., 2023b) attempts to generate rich simulation environments and expert demonstrations using large language models (LLMs) to achieve task-level diversity. However, text-based task generation tends to be arbitrary regarding object selection and their relationships, limiting its ability to represent the true distribution of tasks in the real world. In contrast, our work leverages real-world RGB videos to create corresponding simulation tasks that better reflect the real-world distributions of tasks and objects, facilitating easier scalability due to the abundance and accessibility of internet video data.\nPolicy Learning via LLMs To enable automatic policy learning with high quality, researchers are increasingly turning to large language models (LLMs) for assistance. Some studies (Liang et al., 2023; Huang et al., 2023a; Lin et al., 2023; Wang et al., 2023a) propose generating structured code outputs for decision-making problems, most of which rely on predefined primitives. Other works (Yu et al., 2023b; Ma et al., 2023; Wang et al., 2023c) generate reward functions using LLMs for reinforcement learning. Nevertheless, Eureka (Ma et al., 2023) requires predefined success functions for iterative updates of reward functions, while RoboGen (Wang et al., 2023c) selects the highest reward as the initial state for the next sub-task, which introduces noise due to the variability in the generated reward functions. In contrast, our work generates success functions by leveraging visual prior knowledge from the provided videos and updates the reward functions iteratively using a Chain-of-Thought (CoT) approach (Wei et al., 2022)."}, {"title": "3. Method", "content": "The proposed framework, Video2Policy, steps further for task proposal and policy learning through internet videos, to provide diverse and realistic tasks as well as the corresponding learned policies. It consists of two phases: task scene generation and policy learning. In Sec. 3.1, we introduce the pipeline for reconstructing scenes from RGB videos. Subsequently, in Sec. 3.2, we demonstrate how to generate the code for the task and learn policies to solve it. Finally, we provide an example of training a generalist policy for real-world deployment within our framework in Sec. 3.3.\n3.1. Scene Reconstruction from Videos\nSince the goal of this work is to learn policies from videos rather than automatically retarget trajectories, our scene reconstruction phase focuses on reconstructing the manipulated objects along with their relative relationships. To master the skill demonstrated in the video, we allow for random positions and orientations of each object in the initial states. The steps are shown in Fig. 1: (1) detect and segment the manipulated objects in the video using text captions; (2) reconstruct object meshes from videos and estimate actual sizes of the meshes; (3) perform 6D position tracking for each object in the video. Afterward, we obtain a JSON file that includes the video and object information.\nObject Grounding We first use Grounding DINO (Liu et al., 2023) to detect the manipulated objects in the first frame of the video. Since the SSv2 dataset (Goyal et al., 2017) provides both video captions and object labels, we use these as text prompts for detection. For self-collected videos of more challenging behaviors, we provide the video captions and object names manually. Afterward, we perform video segmentation using the SAM-2 model (Ravi et al., 2024). Specifically, we segment the objects in the first frame using bounding boxes obtained during detection and select five positive pixel points in each object mask for video segmentation. This process yields segmented videos that contain the masks of the manipulated objects.\nObject Reconstruction With the segmentation masks of each object for each frame, we perform mesh reconstruction based on these images. Since most internet videos are recorded from a single viewpoint, we leverage the InstantMesh model (Xu et al., 2024) to reconstruct the 3D meshes, which supports mesh generation from a single im-age. Typically, we choose the first frame to reconstruct the meshes; however, for those with objects that are significantly occluded in the first frame, we utilize the last frame instead. To establish a more realistic size relationship between objects, we propose a simple and efficient size estimation method. We predict the camera intrinsic matrix $K$ and the depth $d_{i,j}$ of the image $I_{i,j}$ using UniDepth (Piccinelli et al., 2024), where $i, j$ are the pixel coordinates. Given the masked region $M$ of the object, we can calculate the maximum distance for the masked region in re-ality $D_{image}$: $D_{image} = \\max_{(i_1, j_1), (i_2, j_2) \\in M} ||p(i_1, j_1) - p(i_2, j_2)||$, where $p(i, j) = K^{-1} \\cdot [x, y, 1]^T \\cdot d_{i,j}$. Here p is the 3D position of each masked pixel in the camera co-ordinate system. We then calculate the maximum distance of vertices in the mesh object, denoted as $D_{mesh}$. The scale ratio $\\rho$ for the mesh object is defined as $\\rho = \\frac{D_{image}}{D_{mesh}}$. The absolute sizes may exhibit some noise due to errors in depth estimation, intrinsic estimation, and object occlusion. However, the relative size of each object is mostly accurate, as $D_{image}$ and $D_{image}$ are calculated within the same camera coordinate system.\n6D Position Tracking of Objects After reconstructing the objects, we predict the 6D position of each object throughout"}, {"title": "3.2. Task Code Generation and Reinforcement Learning", "content": "After extracting the visual information from the video into a task JSON file, we can build the task scene in simulation and learn policies based on GPT-40. This process occurs in two stages. First, we generate the complete task code, which can be executed directly in the Isaac Gym (Makoviychuk et al., 2021) environment. Second, inspired by recent work on LLM-driven reward function generation, we iteratively fine-tune the generated reward function using in-context reward reflection (Shinn et al., 2023; Ma et al., 2023; Wang et al., 2023a). In contrast to the previous work Eureka (Ma et al., 2023), which is the most similar to ours, we generate the task codes, including the reward functions, from scratch, rather than relying on pre-existing task codes and starting reward reflection from manually defined success functions.\nTask Code Generation Inspired by previous work (Ma et al., 2023; Wang et al., 2023b;c), we systematically introduce the pipeline for general task code generation, which helps to infer codes by prompting in a well-organized way. Notably, the task code consists of six parts: scene information, reset function, success function, observation function, observation space function, and reward function. (1) The scene information refers to the task scene JSON file created from the videos. It contains the task title, video file path, video description, and object information, including sizes, URDF paths, and tracked 6D position lists. (2) The reset function is responsible for positioning the objects according to specific spatial relationships in the beginning. (3) The success function determines the success state. Notably, both the reset and success functions are generated by GPT-40 based on the task description, the provided 6D position list, and Chain-of-Thoughts examples (Wei et al., 2022). (4) Furthermore, we have access to the states of the objects in simulation. Thus, we query GPT-40 to determine whether additional observations are necessary. Interestingly, we find that it can include observations such as the distance between the object and the gripper or the normalized velocity toward the target object. (5) Simultaneously, it calculates the correct observation shape for building the neural networks. (6) Regarding the reward function, we follow the instructions from (Ma et al., 2023) with CoT examples. We write a template for task code generation, allowing us to query the VLM just once to generate the executable code encompassing all six parts. We generate eight example codes and select one by re-checking the correctness, reasonability and efficiency from GPT-40, which is the base code candidate for the subsequent in-context reward reflection stage.\nReward function iteration Given the generated task code, we train policies through reinforcement learning under the reward function R and success function R0|1. Notably, we assign a high trade-off to the success rewards, formulating the training reward function as $R + \\lambda R_{0|1}, \\lambda = 100$. Moreover, following the approach in Eureka (Ma et al., 2023), we apply the in-context reward reflection to design the reward functions iteratively using GPT-40. Each time, we sample N = 8 different reward functions for training policies and collect the training and evaluation logs. We then select the best function from the previous iteration and generate new reward functions based on these logs, along with specific instructions and CoT examples. For example, in addition to providing good examples from previous tasks, we prioritize training outputs where the accumulated rewards for successful trajectories exceed those for failed ones."}, {"title": "3.3. Sim2Real Policy Learning", "content": "As witnessed by the recent success of policy learning from large-scale datasets with certain formats (Padalkar et al., 2023; Reed et al., 2022; Team et al., 2024; Brohan et al., 2023), we want to investigate how to learn a general policy from the internet videos, which directly outputs the executable actions of the robot rather than the un-executable future videos (Du et al., 2024; Qin et al., 2023) or language tokens (Liang et al., 2023; Brohan et al., 2023). We consider our Video2Policy a data engine to generate successful policies from internet videos. Then we can acquire expert trajectories in simulation, which match the video behavior. Notably, those expert trajectories can be any format we want, such as states, 2D images, or 3D images. In this work, we choose RGB image observation. We train RL policies from the videos and collect the successful trajectories from the learned policies. Afterward, we use imitation learning to learn the general policy from the collected dataset by behavior cloning. Finally, we transfer the learned policy to the real world. To bridge the gap between simulation and real, we apply some domain randomization in data collection and take the segmentation map as the input for deployment."}, {"title": "4. Experiments", "content": "In this section, we present detailed evaluations of the pro-posed Video2Policy framework on internet video datasets about manipulations. Specifically, the experiments are de-signed to answer the following questions: (a) How does the generated scene from the video look like, including the objects and visual informations, in Sec. 4.1. b) How does the policy trained under our framework perform compared to the videos and the baselines, in Sec. 4.1. (c) What is the performance of the general policy learned from diverse internet videos, and can it generalize to novel scenes, in Sec. 4.2. (d) How does the general policy perform by sim2real back, in Sec. 4.3. (e) What affects the proposed Video2Policy framework most for policy learning, in Sec. 4.4.\nExperimental Setup We use the Issac Gym (Makoviychuk et al., 2021) as the simulation engine for all the experiments, which is commonly used in robotics tasks due to the ad-vantages of efficient computing and highly realistic physics. We focus on table manipulation tasks, and the objects will randomly reset on the table in the beginning. The horizon of the tasks is set to 300 and the parallel environments are 8192, equally. For each task, we average success rates over 10 evaluation episodes across 3 runs with different seeds.\nVideo Data Source To reconstruct scenes from internet RGB videos, we choose the Something Something V2 (SSv2) dataset (Goyal et al., 2017), a common and diverse video dataset for the robotics community. It includes diverse behaviors concerning manipulating something with something by human hands. To further investigate the ability of our framework on more complex objects or behaviors, we record three in-the-wild videos of different behaviors by ourselves. Notably, all the videos we use in the experiment are 3 channels with RGB only, with the highest accessibility. For the video quality, the small motion of the camera is tolerated, and we scale the resolution to 1024.\nScene Generation As mentioned in Sec. 3.1, we do 6D position tracking for all the objects. Considering that we randomize the initial states of all the objects, we only feed the 6D pos from the first frame and the final frame into the prompts. It is reasonable in most tasks because those two frames are significant to infer the relationship among objects. Even if this simplification will miss the motion information for some behavior, e.g. throwing, we also provide the task description to design the reward function so that the LLM will generate the velocity reward components. Moreover, we explicitly calculate the difference between the 6D pos and feed the information into the LLM to think about the success function. For most of the SSv2 videos of a single object, there are severe occlusions, making it difficult to reconstruct the mesh asset. We manually choose the first frame or the last frame to reconstruct the mesh and predict the 6D position in the same pipeline. We generate the task code in a curriculum manner (Ma et al., 2023; Wang et al., 2023b) after obtaining the visual information. From the beginning, we provide the example code of reach a block and grasp a block. Then we will add the successfully generated task examples into the task pool for the next one. Finally, it can even learn to use the direction velocity reward for dynamic tasks and resolve them. Some demos for the generated tasks are in Fig. 2.\nReinforcement Learning For the policy learning, we choose the PPO (Schulman et al., 2017) algorithm in a well-tuned implementation codebase (Makoviichuk & Makoviychuk, 2021; Ma et al., 2023). We share the same parameters recommended in the codebase across all tasks and all baseliens. As for the evaluation metric, we write the ground-truth success function for each generated task. Unlike Eureka (Ma et al., 2023), we do not allow access to the evaluation metric during training, and we manually evaluate the results for the final models. For SSv2 dataset, we make"}, {"title": "4.1. Policy Learning from Videos", "content": "Policy Learning Baselines Since there is few works trans-forming the internet RGB videos into policies for the task, we focus on the policy learning part in the experiments. We benchmark our method against the following baselines. (1) Code-as-Policy (CoP) (Liang et al., 2023), which queries the LLM with all the states in the environment to write the executable code for the robot. To ensure better performance of CoP, we use the close-loop control and regenerate code policies every 50 steps. (2) RoboGen, which does not require a success function and learns without reward reflection iteration. (3) Eureka, which generates code for both the reward and the success function using an LLM and does not use video information. To make fair comparisons, we use the same object meshes and task codes generated from the videos for all the baselines.\nPerformance Analysis of Learned Policies We compare the performance of our method with the above baselines on 9 tasks generated from SSv2 dataset and 3 harder tasks collected by ourselves, as shown in Tab. 1. We find that the proposed Video2Policy method outperforms the baseline in most tasks. RoboGen (Wang et al., 2023c) and Eureka (Ma et al., 2023) achieve comparable results to ours in the videos of a single object. However, for multiple objects, the performance of RoboGen drops a lot, while the Eureka has much larger variances during training. Moreover, for harder tasks from self-collected videos, including the non-convex object manipulation and dynamic tasks, the three baselines perform much worse, especially for the CoP and RoboGen. CoP fails in all cases because the script policy receives feedback from the environment less frequently and cannot control the speed of the object."}, {"title": "4.2. Policy Generalization Analysis from diverse videos", "content": "To further demonstrate the scalability of our framework, we target training a general policy from diverse videos. As mentioned in Sec. 3.3, we regard the Video2Policy as a data engine to generate expert trajectories in simulation. To validate the generalization ability across unseen videos, we"}, {"title": "4.3. Sim2real for the learned general policy", "content": "We also conduct sim2real experiments. Specifically, fol-lowing Sec. 4.2, we collect 200 trajectories from each re-constructed scene in simulation for 100 lifting tasks. Then we train a policy model via imitation learning and deploy the policy in the real world. Moreover, the object's position varies within a 10 cm range. The image input had a resolution of 256x256. In terms of the setup, we use Franka robotic arms, Robotiq grippers, and Stereolabs cameras. We evaluate the performance of the policy towards lifting a mouse, a cup, and a piece of bread. Notably, while there are some mouse and bottle objects in the simulation, the bread is absent in the collected simulation dataset and is soft. To alleviate the sim-to-real gap, we employ two strategies:\nInput Representation and Network Architecture. We take as input the segmentation masks of the image, the robot's end-effector (EEF) state, and the gripper state. SAM-2 is adopted for segmentation, where the pixel position of the target object is provided manually as input in the first frame, shown in Fig. 3. We stack 2 frames and add an additional multi-layer perceptron (MLP) layer to map the robot state into a 256-dimensional feature vector. Furthermore, the rotation component of the action is scaled by a factor of 0.2.\nDomain Randomization. During data collection in the simulation, randomization is applied to the actions with noise levels of 0.02 and a random delay of 0.01-0.02 seconds. Moreover, the physical properties of the object, such as size and weight, are also randomized. We ensure consistency between the camera poses in sim and real.\nHere the general lifting policy achieves a success rate of 72% across 10 novel objects in simulation. The sim2real results are as shown in Tab. 2. Compared to the 72% success rate in simulation, it achieves 47% success rate in the real world. It proves the efficacy of our pipeline, which builds the pipeline of internet videos to policies. We notice that gripping slippery objects, such as a cup, pose challenges for the robot, resulting in a relatively low success rate. For the bread, the task was relatively easier despite the object being unseen in the simulation. This can be attributed to the segmentation mask observation and the bread's relatively large surface area, which facilitates successful manipulation.\nOverall, these experiments demonstrate that the general policy trained in simulation possesses effective sim-to-real transfer capabilities. Additionally, the results highlight the potential of the proposed Video2Policy pipeline, underscor-"}, {"title": "4.4. Ablation Study", "content": "Compared to the previous works on LLM-driven RL (Ma et al., 2023; Wang et al., 2023c), we apply the visual information and more reasoning from VLMs.\nAblation of Code Generation Components Here, we ab-late the results of removing each part in the code generation as follows: (1) V2P w.o. visual information, where only the video caption is provided to generate the codes; (2) V2P w.o. success picking, where we do not pick the best success function by sampling 8 different success functions; (3) V2P w.o. iterative reward designs, where we do not apply the iterative reward reflection to fine-tuning the reward functions; (4) V2P w.o. multiple reward samples, where we only train 1 example of the reward function instead of 8 samples. The results are on the Tab. 3. It will encounter a performance drop without any part. The most significant one can be the iterative reward reflection component, which finetunes the reward function based on the previous training results. Additionally, w.o. the multiple reward samples and w.o. success picking have larger variances than the others. With those techniques, we achieve better performance than the previous LLM-driven methods (Ma et al., 2023; Wang et al., 2023c). We also make ablations about the generated success function, reset function, tracking prompts and hallucination issues in App. \u0410.3.\nMoreover, we conduct experiments concerning the robust-ness analysis for each component and performance analysis for iterative generation in App. A.2."}, {"title": "5. Discussion", "content": "We have proposed Video2Policy, a pipeline for generating simulated tasks from human videos. We show that our design enables us to effectively learn from human videos and generate high quality data. As such, it is bottlenecked by the quality of these models, particularly mesh reconstruction and reward code generation. However, as these foundation"}, {"title": "A. Appendix", "content": "A.1. Implementation Details of Code Generation"}]}