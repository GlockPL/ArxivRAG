{"title": "Performance and Power: Systematic Evaluation of AI Workloads on Accelerators with CARAML", "authors": ["Chelsea Maria John", "Stepan Nassyr", "Carolin Penke", "Andreas Herten"], "abstract": "The rapid advancement of machine learning (ML) technologies has driven the development of specialized hardware accelerators designed to facilitate more efficient model training. This paper introduces the CARAML benchmark suite, which is employed to assess performance and energy consumption during the training of transformer-based large language models and computer vision models on a range of hardware accelerators, including systems from NVIDIA, AMD, and Graphcore. CARAML provides a compact, automated, extensible, and reproducible framework for assessing the performance and energy of ML workloads across various novel hardware architectures. The design and implementation of CARAML, along with a custom power measurement tool called jpwr, are discussed in detail.", "sections": [{"title": "I. INTRODUCTION", "content": "Fueled by the growing interest in training ever larger deep neu- ral networks, such as large language models and other foundation models, the demands for hardware specialized on these workloads have grown immensely. Graphics processing units (GPUs) have evolved from their origins in computer graphics to become the primary computational engines of the AI revolution. While the central processing unit (CPU) controls a program's execution flow, it offloads compute-intensive highly-parallel tasks to the GPU (the accelerator). Evolving from a pioneering company, NVIDIA has emerged as the dominant player in the market as of 2024, spearheading current hardware developments. Other vendors, such as AMD and Intel, also provide GPUs aiming to accelerate model training and inference.\nAnother promising class of AI accelerators is based on the idea of distributed local per-compute-unit memory together with on- chip message passing, in contrast to a shared memory hierarchy, typical to classical CPUs and GPUs. Vendors following this dataflow approach include Graphcore, Cerebras, Groq, Sam- baNova, and Tenstorrent.\nPerformance characteristics not only vary between generations and vendors, but depend on the node or cluster configuration in which the accelerator is embedded, including CPU, memory, and interconnect. When evaluating and comparing these hetero- geneous hardware options, e.g. for purchase decisions in an aca- demic or industrial setting, it is not sufficient to compare hardware characteristics such as number of cores, thermal design power (TDP), theoretic bandwidth, or peak performance in FLOP/s. Their effect on workload performance is not straightforward, and the accelerator architectures might barely be comparable. Performance data reflecting the actual intended workloads, collected on various competing systems independently of vendor interests, offer highly valuable information. Power consumption is one such important metric in this regard.\nIn particular within the field of machine learning, having a structured, automatic benchmarking tool to investigate the effect of hyperparameters, such as learning rate and batch size, and to identify optimal settings is important. When training large models across multiple cluster nodes, additional hyperparameters are necessary to define the parallelization layout, leveraging various forms of parallelism. Moreover, hardware configurations, such as those related to processor affinity or network communication, must be systematically explored.\nAs a framework to collect this data, we present the CARAML benchmark suite, a Compact, Automated, Reproducible Assessment of Machine-Learning workloads on novel accelerators. Further, we contribute performance data on various accelerators, collected with CARAML, including energy measurements, as well as first-hand experiences with encountered challenges. The machine learning workloads include the training of a Generative Pretrained Transformer (GPT)-based large language model using PyTorch, as well as training of a ResNet50 model implemented in TensorFlow. Energy measurements are facilitated by our jpwr tool, which we also contribute with this paper.\nThe remaining paper is structured as follows: in section II we provide background information on the state-of-the-art AI workloads included in CARAML in the fields of natural language processing and image recognition. More details on the investigated hardware configurations. More details on the implementation of the benchmark, including a description of used tools, require- ments, and execution instructions, are presented in section III. Performance results, in terms of throughput (images/s, tokens/s) for various batch sizes, are reported in section IV. Additional energy metrics such as images/Wh and tokens/Wh provide another layer of insight. In section V, we briefly discuss the difficulties encountered in the benchmark development in terms of hardware and software compatibility and comparability, and how they are resolved. Final conclusions, take-away messages, and future plans are given in section VI."}, {"title": "II. BACKGROUND", "content": "Deep learning constitutes an important workload of high perfor- mance computing (HPC) clusters, only increasing in significance in recent years. A neural network aims to generalize an input- output relation observed in its training data. A chain of matrix products and activation functions, reflecting a neural network architecture, is applied to the batched input data in a forward pass. The backward pass, also called back-propagation, updates the matrix elements (the \u201cweights\u201d) using further matrix products, This operation reflects a step of (stochastic) gradient descent to minimize the loss function representing the difference between computed and true output of the sample.\nAlthough specialized network architectures like transformers and convolutional neural networks have been developed for specific tasks, they rely on the fundamental building block of matrix multiplication. Since matrix multiplications are inherently parallel, many-core hardware architectures, such as GPUs and other accelerators optimized for this process, are crucial."}, {"title": "A. Natural Language Processing", "content": "The advent of large language models based on transformer architectures [1] has revolutionized the field of natural lan- guage processing (NLP). These models have enabled significant advancements across a wide range of tasks, including speech recognition, text classification, natural language understanding, and generation. By leveraging shared foundational models, these tasks can now be effectively addressed with additional fine-tuning or in-context learning, allowing for greater flexibility and efficiency in various domains.\nLanguage models use large text corpora as training data to predict the next piece of text (a token) given the preceding context. The original transformer architecture consists of an encoder and a decoder connected via a cross-attention mechanism. Attention is a key operation in this architecture, characterized by its quadratic complexity in the sequence length. It involves matrix-matrix products of learned token representations, allowing the model to capture relationships between tokens while accounting for their relative positions.\nScaling up transformer models is achieved by stacking multiple transformer layers, each built around an attention mechanism, along with feed-forward layers, residual connections, and nor- malization. This increases the number of learnable parameters, making larger networks more capable, especially when trained on sufficiently large datasets. To handle the computational demands of such large models, parallelization techniques are essential. State-of-the-art methods include 3D parallelism [2], [3] (combin- ing data, tensor, and pipeline parallelism), sequence parallelism [4], activation recomputation [4], and optimizations like flash attention [5].\nThe NLP benchmark task in CARAML utilizes the Megatron- LM framework [2], [6], a robust, research-focused software platform developed by NVIDIA using PyTorch. Megatron-LM has been instrumental in advancing large-scale language model training, incorporating and pioneering the previously mentioned features. The BigCode Project fork of Megatron-LM with ROCm adaptations is used for AMD and forked version of Graphcore application examples is used on Graphcore."}, {"title": "B. Computer Vision", "content": "Computer vision (CV), particularly its core task of image classification, has been a significant driver of deep learning advancements due to its numerous real-world applications. The architecture of convolutional neural networks (CNNs) effectively addresses challenges such as vanishing and exploding gradients by employing successive layers of convolutional filters, defined by learned parameters. It became evident that the massively parallel architecture of GPUs is well-suited for the computational de- mands of CNNs, leading to substantial improvements in training efficiency and model performance [7].\nResidual connections were introduced by the ResNet model family [8] and solved the problem of degradation, i.e. the counterintuitive observation of higher training errors in deeper networks with more parameters.\nWhile the transformer architecture has also found its way into image recognition [9], convolutional neural networks are extremely mature and widely used in production environments, and represent an important benchmark case. Together with trans- formers, they cover a wide portion of currently relevant deep learning paradigms.\nIn CARAML, a benchmark to train a ResNet50 model, i.e. a ResNet model with 50 convolutional layers from scratch, is curated from a forked version of the official TensorFlow CNN benchmark. The benchmark employs data-parallelism with Horovod to use multiple GPUs. In data-parallelism, each device holds a model copy but performs a backpropagation for a different batch of input data, combining the gradients after each step using an all-reduce collective operation."}, {"title": "C. Accelerators", "content": "GPUs have become the standard hardware for accelerating neural network training. However, while processing power has advanced rapidly in recent years, memory bandwidth has not kept pace, leading to potential bottlenecks. Accelerators based on data-flow architectures, such as Graphcore IPUs [10], offer a promising alternative by addressing these limitations. Unlike traditional architectures that rely on a shared memory hierarchy, IPUs leverage distributed per-core memory, which allows for faster data loading due to the physical proximity of memory to each core. This design enables all cores to operate independently, making the processing of irregular or sparse neural network architectures more efficient. In terms of Flynn's taxonomy [11], this can be considered a MIMD (multiple instruction streams, multiple data streams) architecture, while GPUs follow a SIMD (single instruction stream, multiple data streams) approach."}, {"title": "IV. RESULTS", "content": "In the following, we report throughput measurements obtained from the hardware systems described in Table I alongside the corresponding energy consumption data collected during the exe- cution of the CARAML benchmarks. The benchmarks were con- ducted with careful consideration of CPU binding, MPI threading, and GPU affinity to ensure optimal conditions on the examined machines."}, {"title": "A. LLM Training", "content": "Figure 2 provides throughput results in tokens/second per GPU for NVIDIA systems of various generations and the AMD MI250 GPU, for global batch sizes from 16 to 4096. All experiments train a decoder-only transformer model with 800M parameters using a subset of the OSCAR dataset preprocessed using GPT-2 tokenizers. Since the 800M model fits on a single device, data parallelism can be employed to scale the model across multiple accelerators within a node. The model was trained on an entire node for each system, utilizing data parallelism and micro-batch-size of 4, when multiple accelerators were available. All systems contained 4 GPUs, except the GH200 node in JURECA which has only one (see Table I).\nFor the AMD node, we report two sets of results to draw a complete picture and make a nuanced comparison possible in the context of Multi-Chip Modules (see details in Table I). The first set of results (AMD MI250:GCD) uses 4 GCDs (2 GPUs) with data parallelism of 4, while the second set (AMD MI250:GPU) uses all 8 available GCDs (4 GPUs) with data parallelism of 8. When using data parallelism of 8 the global batch size of 16 is not possible since it is not divisible by micro-batch-size times data parallel. In each case, the data is normalized per data parallel (i.e. by 4 and 8, respectively). Additionally, we present the average total energy consumed per GPU during one hour of model training, along with an energy efficiency metric, calculated as the number of tokens processed per unit of energy consumed.\nIn general, one can see the performance improvements in more recent GPU hardware generations, with GH200 nodes yielding a throughput of up to 47505 Tokens/s/GPU, 2.45\u00d7 higher than throughput achieved on A100 GPU nodes. This can be alluded to having more cores and SMs, faster CPU-to-GPU-NVLink connection, TDP, and fast memory. It is evident, that choosing a larger batch size is beneficial for throughput. However, when training a neural network in a production setting, this increased GPU utilization must be balanced against the potential drawback of slower convergence, which could impact the overall training efficiency of the neural network.\nDifferent variants of accelerators were examined, namely the H100 incorporated in the JURECA evaluation platform (referred to here as JRDC) and the H100 in the WestAI cluster, as well as the GH200 in JEDI and the GH200 in JRDC (see Table I), and different results can be inferred.\nWhen comparing the two GH200 configurations, we see that a device on a single-accelerator node (GH200 (JRDC)) yields a 20% higher performance than a device on a multi-accelerator node (GH200 JEDI)), accompanied by a 20% higher energy consumption. Hence, the tokens/energy efficiency per device is similar; even slightly better for the less performant JEDI case. On JEDI, all devices engage in data-parallel model training, and the additional communication overhead, together with the lower amount of CPU memory available per device, could be the reason for the lower performance per device.\nAnother large throughput difference can be observed between the two H100 systems, with the WestAI variant processing 1.3\u00d7 as many tokens as the JRDC variant. This could be due to the higher available bandwidth from NVLink connections between GPUs and the SXM GPU form factor, which comes with a higher power envelope (TDP).\nFor AMD, using 4 GCDs (2 GPUs) performs slightly better per device than using 8 GCDs (4 GPUs), again representing the overhead of higher parallelization. This overhead leads to a higher energy consumption per device and lower energy efficiency when using 8 GCDs (4 GPUs).\nIn terms of energy efficiency, the results indicate that the H100- PCIe (JRDC) outperforms all other devices by up to 25%, even against the newer technology of GH200 chips, which provide a throughput twice as high. This is likely related to the limited power budget of the PCIe card, moving its operation mode to a more power-efficient spot. Another factor could be that the other H100 variants, especially the GH200s, are not yet completely saturated in the examined benchmark scenario, as they have higher SM counts.\nTable II provides performance and energy efficiency results for the Graphcore machine. Here, the vendor benchmark specifies IPU POD16 as the minimum requirement for GPT-2 PyTorch model training [37]. As we only have access to an IPU POD4, a smaller GPT model with 117M parameters is used to benchmark the hardware with energy measurements for global batch sizes from 64 to 16384. The larger batch sizes may not be practical for model convergence, but were investigated to understand the limitations of the system. The model layers are split across 4 IPUs and trained for one epoch (global batch size samples) using synthetic data. We see in Table II that the throughput (tokens/second) increases with the batch sizes, saturating the accelerator, and uses a maximum of 33 Wh. The performance is very low compared to GPUs but can partially be explained by the required pipeline parallelism. This form of parallelism introduces a pipeline bubble [2] and is not as efficient as data parallelism."}, {"title": "B. ResNet50 Training", "content": "The ResNet50 training benchmark was performed on all avail- able systems with global batch sizes 16 to 2048.\nFigure 3 reports the throughput of the ResNet50 training process in images per second on a single device on various systems (see Table I), as well as consumed energy for the whole epoch (processing all images of the input dataset once), and energy efficiency in images/Wh. ImageNet data was used as input, containing 1281167 images.\nAs expected, the performance increases from older to newer GPU generations. Similar to the LLM training benchmark (see Figure 2), we see the WestAI H100 node, with higher-TDP SXM form factor, outperforming the PCIe variant when considering performance per node, with similar energy footprints. Again, GH200 (JRDC) performs better than GH200 (JEDI). This holds true especially for larger batch sizes, which can likely benefit from 4x as much available CPU memory per GPU, allowing for faster data loading. As seen for ResNet50 before, the PCIe- variant of the H100 (JRDC), appears to have the best energy efficiency amongst the NVIDIA GPUs, closely followed by the GH200 (JRDC). The reasons seem to stem from a combination of TDP, memory capacity, and bandwidth.\nTo provide data for more nuanced comparisons in the context of MCMs, as already explained for the LLM results, also for ResNet50, two benchmark runs on the AMD MI250 node were conducted. One run (AMD MI250:GPU) utilized 1 GPU (2 GCDs), requiring data parallelism of 2, and another run (AMD MI250:GCD) utilized only 1 GCD, without parallelism. Using 2 GCDs naturally leads to a higher throughput, and the device is used more efficiently. This leads to slightly lower amounts of energy needed to process the whole dataset, and a slightly higher energy efficiency.\nThe AMD MI250 gives the best efficiency in terms of images per unit of energy for higher batch sizes, while for smaller batches the H100 and GH200 (JRDC) devices are more energy efficient.\nIn the case of Graphcore, the vendor-based TensorFlow ResNet50 model training benchmark contains optimizations catering to the IPU execution strategy. When running the bench- mark for a single epoch, the IPU first compiles an optimized model graph, which takes close to an hour. It is excluded from the timings presented here. The compiled model graph upon execution is able to complete an epoch with 1281167 samples in 10 to 15 minutes.\nTable III provides results on throughput, energy consumption, and energy efficiency for the ResNet50 benchmark on a Graph- core GC200 IPU. The model performance does not scale on increasing the global batch size. This is likely related to the limitation of not being able to process a micro-batch-size of more than 16 due to limited on-chip RAM (SRAM) and having to execute multiple sequential calls to fetch data from the chip-external memory (DRAM). The energy efficiency compared to classical GPUs looks very promising."}, {"title": "V. TECHNICAL CHALLENGES", "content": "In CARAML, benchmarks are curated to compare the accel- erators with minimal parameter adjustments using open source codebases. For this purpose, NVIDIA and AMD GPUs are tested using the same baseline code. The distinct execution strategy of Graphcore IPUs necessitates a separate codebase, posing a challenge to ensure comparable hyperparameters between IPUs and GPUs.\nThe NVIDIA Megatron-LM code base incorporates hardware specific optimizations, with the current version focusing on the Hopper architecture, for example making use of its Transformer Engine. To ensure compatibility with the Ampere architecture, CARAML rolls back to a Megatron-LM commit that can be executed on all devices without losing other performance opti- mizations.\nTypically, new optimizations, such as flash attention, are first made available on NVIDIA hardware, with AMD accelerators receiving support later. Currently, the flash-attention2 implementation in ROCm is still under development and supports head dimensions only up to 128, whereas the CUDA implemen- tation on NVIDIA GPUs already supports head dimensions up to 256 and flash-attention3."}, {"title": "B. Containers", "content": "Using containers for reproducible workloads promises to sim- plify the creation of reproducible environments, but setting them up in performance-sensitive HPC environments can be chal- lenging. Issues include locating vendor-supported containers for specific software versions and managing conflicting package dependencies within the container, often with limited permissions.\nSolving these challenges required multiple iterations of con- tainer testing, finally leading to the creation of custom con- tainers, inheriting from vendor-provided containers. To man- age the installation of additional packages within the con- tainer, we utilize pip with the --prefix, --no-deps, and --ignore-installed options, and manually adjust the PYTHONPATH. The container's isolation from the system envi- ronment necessitates defining custom bind paths and the devel- opment of wrapper scripts to export environment variables.\nUtilizing the shared resources of HPC systems requires us- age of job schedulers (Slurm) and message transport libraries (MPI, NCCL). As the employed containers need to bring their own MPI installation, some effort needs to be taken to align the out-of-container distribution setup with the in-container installation. For our setup, the involved PMIx con- figurations need to be explicitly made compatible by manually setting PMIX_SECURITY_MODE=native out-of-container, but within a Slurm-distributed jobs."}, {"title": "C. System", "content": "In order to ensure a smooth user experience, some non- trivial, HPC-related technical fixes have to be implemented. We document them here to highlight on system-specific issues, which can be helpful when adding more system support for future benchmarks.\nAs with many HPC systems, the systems at J\u00fclich Super- computing Centre feature a high-speed interconnect (InfiniBand) between the nodes. IP connectivity is only available using In- finiBand devices (IP over Infiniband, IPoIB). PyTorch needs to be made aware of the different format of the hostname, which contains an appended i to the MASTER_ADDR variable. Further, a fixed torchrun.py script is required for execution on such a system."}, {"title": "VI. CONCLUSIONS", "content": "As AI continues to experience rapid growth and the market is seeing a growing influx of AI accelerators, evaluating accelerator performance using real world applications is crucial. In this paper, we introduced CARAML, a benchmark suite designed to assess Al workloads on accelerators with energy measurements. CARAML uses the JUBE framework to create compact, auto- mated benchmarks for both LLM and Computer Vision training. The benchmarks incorporate the modular jpwr tool to measure energy consumption. CARAML is further capable to perform ablation studies to identify hardware and model configurations for optimal performance. The details of the framework and results obtained using CARAML on seven different accelerators systems from NVIDIA, AMD and Graphcore that differ either in generation or configuration were discussed in detail.\nThe results confirm that the latest accelerator generations yield a better performance, but the energy efficiency is influenced by more factors in the hardware and network configuration. The GH200 generally gives the best performance, related to the CPU- to-GPU-NVLink connection, TDP, and fast memory. The PCIe- flavor of the H100 usually gives the best energy-efficiency, a result of operation at an efficient power operating point.\nWhile the surveyed Graphcore accelerator system could not yield a competitive performance to classical GPUs, the results on energy efficiency are very promising, outperforming GPUs in this regard for some benchmarks. This relies on code that is optimized for the execution on an IPU's data-flow architecture, which can yield performance improvements.\nSeveral technical challenges were encountered while automat- ing the CARAML benchmark setup. Solutions required a deep understanding of networking specifics, AI framework backends, and how containers interact with their environment.\nAs future work, we plan to further develop CARAML by incor- porating continuous benchmarking capabilities and enhancing its usability. We also aim to expand the suite by including additional AI training and inference benchmarks."}, {"title": "APPENDIX", "content": "To execute the CARAML benchmark, clone the CARAML repository and use the corresponding JUBE script and a tag to identify the target architecture. The available system tags are listed in the overview of systems in Table I"}]}