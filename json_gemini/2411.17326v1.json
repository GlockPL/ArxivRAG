{"title": "Towards Intention Recognition for Robotic Assistants Through Online POMDP Planning", "authors": ["Juan Carlos Sabor\u00edo", "Joachim Hertzberg"], "abstract": "Intention recognition, or the ability to anticipate\nthe actions of another agent, plays a vital role\nin the design and development of automated as-\nsistants that can support humans in their daily\ntasks. In particular, industrial settings pose inter-\nesting challenges that include potential distractions\nfor a decision-maker as well as noisy or incom-\nplete observations. In such a setting, a robotic assis-\ntant tasked with helping and supporting a human\nworker must interleave information gathering ac-\ntions with proactive tasks of its own, an approach\nthat has been referred to as active goal recognition.\nIn this paper we describe a partially observable\nmodel for online intention recognition, show some\npreliminary experimental results and discuss some\nof the challenges present in this family of problems.", "sections": [{"title": "1 Introduction", "content": "Imagine a busy factory setting where workers and\nrobots collaborate towards shared goals. For exam-\nple, if a worker is required to inspect a machine,\nthey will initiate a series of steps a plan - to sys-\ntematically determine which part or which compo-\nnent is faulty and whether it needs to be replaced. A\nrobotic assistant might support this worker and at-\ntempt to provide tools and parts as needed, but the\nrobot and the worker need a shared understanding\nof the problem and the robot must be able to assess\nthe worker's needs. Another worker may be tasked\nwith assembling a series of objects by picking parts\nfrom nearby containers, and another robotic assis-\ntant may be able to help by observing the worker's\nactivities, estimating whether some containers need\nto be restocked and bringing tools the worker may\nneed soon. In both cases the worker and the robot\nhave shared goals and a shared understanding of the\nenvironment and the problem, but the worker may\nfocus exclusively on their chores while the robot\nperforms support tasks to avoid or minimize issues.\nSuch robots must be able to receive and assess infor-\nmation about the worker's activities and the envi-\nronment in order to act appropriately, and consider\nthat this information may not be accurate or even\ncomplete.\nThese scenarios raise the following question:\nhow can an automated assistant support a human\nworker in a collaborative environment, without ex-\nplicit instructions, as well as plan and act with noisy\nand incomplete observations? At least some of these\ncomponents have been addressed in the past, and\nwe have learned lessons from automated assistance\nin e.g., nursing homes [14, 9]. Further improving\nrecognition, autonomy and resilience in complex\ndomains leads to a series of important, interrelated\nchallenges collectively addressed by the \"Plan, Ac-\ntivity and Intent Recognition\" (PAIR) community.\nAlthough the different types of recognition in\nPAIR share principles and methods, such as some\ntype of knowledge base (e.g. a plan library) and\nsome way to generate hypotheses over prospective\nanswers, distinctions exist in that activities com-\nmonly refer to singular actions, plans (or goals) are\nthe best explanation for a sequence of observed ac-\ntions and an intent may be seen as a motivation, or\nrather subsets of goal conditions or possible future\nactions [6]. From this point on we'll refer to the\nagent performing the PAIR tasks as the observer\nand to the agent being observed as the target (the\nrobot and the worker respectively in our motivating\nexamples).\nPlan recognition can also be seen as a reverse\nplanning problem, where the input is a sequence of\nactions and the output is a goal that explains this\nsequence [17, 18, 25]. Intent and goal recognition\nmay also be addressed using similar approaches,\nbut instead focusing on the goal conditions being\npursued by the target. Many such techniques, how-\never, only observe the target's behavior in order to\napproximate the likelihood of a plan or goal hypoth-"}, {"title": "2 Related Work", "content": "A substantial amount of prior work in plan recog-\nnition adopts a passive perspective in which the\nbehavior of a target agent is observed, but the ob-\nserver does not directly interact with the environ-\nment [17, 18, 7, 28]. Plan libraries are a popular\napproach that contain either precomputed plans,\nor a grammar that can construct possible plans\n[8, 7, 16]. The challenge then is estimating the\nprobability of some given plan in the library be-\ning the current one, given observations about the\ntarget's behavior or their activities. Alternatively,\nthe planning approach to plan recognition attempts\nto identify plans, goals, or intentions by observing\nthe target's actions and generating, as well as eval-\nuating, the space of possible target plans [17, 25, 5].\nWhile there is a strong reliance on classical plan-\nners that assume determinism and full observabil-\nity, some work has also been done to address miss-\ning information [25] and to extend goal recognition\nto probabilistic and partially observable domains\n[18].\nAs our motivating examples suggest, we are in-\nterested in problems that fall within AGR. Building\non previous work in partially observable domains, a\ndetailed POMDP representation for states, actions,\ntransitions, rewards, and observations has been pro-\nposed that combines the observer and target vari-\nables in factored structures [1]. Our approach also\nuses shared POMDP states with target and ob-\nserver variables but no explicit distiction is made,\nand instead we adopt a flat representation (with-\nout factoring) and model all action effects as part\nof the POMDP dynamics (or state transitions). We\nargue this achieves a similar level of expressiveness\nby allowing the target to behave independently of\nthe observer, and restricting the observer's ability\nto obtain perfect information, while also maintain-\ning a combination of fully and partially observed\nvariables. With additional effort perhaps our model\ncould be converted to the factored AGR POMDP,\nbut we think a flat representation is more straight-\nforward. The authors used the point-based planner\nSARSOP to solve the AGR problems, but point\nout the benefits of using online POMDP planning.\nPoint-based planners expand and evaluate large\nsections of the belief space and are restricted to\nfairly small domains, even if they are neatly fac-\ntored. Additional work that directly addresses AGR\nincludes a mixed-observability approach [11], but\nagain the authors recommend considering an online\napproach to deal with limitations including large\nstate spaces and combinatorial explosion.\nPOMDP planning on its own has a vast amount\nof literature that can perhaps be quickly summa-\nrized as a series of substantial improvements in\napproximation, using different techniques ranging\nfrom direct value iteration to grid-based and com-\npression methods [23, 3, 4, 2, 15, 30]. Many com-\nmonly used planners follow point-based methods\n[24, 12, 13, 27, 10], but these tend to be limited\nto small problems, often in factored form.\nOnline POMDP planning algorithms scale much\nbetter by focusing on the current belief state and\ncomputing value-updates often using sampling or\ngenerative approaches [22, 26, 21, 29]. POMCP is a\nwell-known POMDP planner based on Monte-Carlo\nTree Search (MCTS) [22], that continues to obtain\ngood performance even in larger problems, with\nthe limitation that it may produce low-performing\nworst-case results. These algorithms may also ex-\nploit prior knowledge to improve performance, but\nthis is often in the form of extensive domain knowl-\nedge, or highly detailed a-priori action preferences.\nAn approach called relevance-based planning builds\non the structure of POMCP, but improves per-\nformance by introducing online methods that 1)\ngenerate action selection preferences autonomously,\nand 2) reduce dimensionality through observation-\nbased criteria [20, 19]. We use this planner to\nobtain preliminary experimental results and com-\npare its performance with standard POMCP, which\nalso helps illustrate the connection between im-\nproved agent autonomy and performance in inten-\ntion recognition tasks."}, {"title": "3 Brief Background on POMDPs", "content": "Let S and A be finite sets of states and actions,\nT(s,a, s') = p(s'|s, a) the transition probability to\nstate s' (with s \u2208 S, a \u2208 A) and R a set of real-\nvalued rewards. The tuple (S, A, T, R) defines a\nfully observable MDP and 0 \u2264 y \u2264 1 its discount\nfactor. In partially observable domains the agent\nmaintains an internal belief state b \u2208 B, where b(s)\nis the probability of s being the current state. This\nprobability is updated from observations \u03c9 \u0395 \u03a9,\nreceived with probability O(s', a,w) = p(w|s', a). A\nPOMDP is defined as the tuple (S, A, T, R, \u03a9,\u039f),\nand the sequence ht = (a0,W1, ..., At\u22121,Wt) is the\nhistory at time t. A full POMDP policy maps be-\nliefs to optimal actions, but when planning and act-\ning online only the current belief and the next best\naction are considered.\nAn action for a belief is found by approximating\nthe optimal state-value function:\n\u03c5* (b)\n= max \u03a3b(s)R(s, a, s') +\na\ns\n\u03a3 \u03b3\u03a3O(s', a, w)\u03c5* (b')\nS\nw\n(1)\nwhich reflects the expected return (sum of dis-\ncounted rewards) when starting in b and pursu-\ning the optimal policy. The optimal action-value\nfunction q* (b, a) represents the value of executing\na in b and then pursuing the optimal policy. In\nour approach both the observer and the target are\nrepresented in the POMDP states (and variations\nthereof in the belief state), so states where the tar-\nget achieves goal conditions are the result of transi-\ntions where the observer acted correcly in advance.\nSuch transitions yield positive rewards which, suc-\ncintly put, transform intent recognition into an\naction-value maximization problem.", "equations": ["\u03c5* (b)\n= max \u03a3b(s)R(s, a, s') +\na\ns\n\u03a3 \u03b3\u03a3O(s', a, w)\u03c5* (b')", "s'\nw"]}, {"title": "4 Model Description", "content": "Similar to previous work in AGR, c.f. [1], we\npropose a POMDP model where both the agent\nand the observer share a state representation (i.e.\nthrough state variables) but the target consists of a\nsmall stochastic model that simulates target actions\nindependently from the observer, and is capable of\ngenerating reward signals during state transitions.\nThis independence as well as the generative aspect\nof target simulation make it easy to represent active\nintention recognition tasks as unfactored POMDPs\nthat can be solved online, in a fairly straightforward\nmanner.\nMore formally, let So be the set of state variables\nthat represent the observer and ST the set of state\nvariables that describe the target.\nThe target is described by a policy \u03c0\u03c4, which\nis a solution to a stochastic process (S, \u03a9\u03c4, T, R\u0442)\ndefined by:\n\u2022\n\u2022\nST is a finite set of target states\n\u03a9\u03c4 is a finite set of target actions\n\u2022 T(st, wt, s'y) is the transition probability from\nsy to s'y after executing action \u03c9\u03c4 \u2208 \u03a9\u03c4\n\u2022 RT(st,wt, s'y) is a set of real-valued rewards\nIn other words, \u03c0\u03c4 is a target simulator that can\ntake a target state as an input and generate the\nstate resulting from its transition model. We then\nincorporate this into a POMDP, redefined as:\n\u2022\n\u2022\n\u2022\nS = So \u00d7 ST the state space described by all\npossible observer and target variables. States\nare the result of So UST but not explicitly\nfactored.\nA the set of observer actions.\n\u03a9 = \u03a9\u03bf\u03c5 \u03a9\u03c4 is the set of observations that\ncombines non-target observations No and tar-\nget activities \u03a9\u03c4. Again, not explicitly fac-\ntored.\nThe transition model T = T(s, a, s')\u03c0\u03c4(st),\nsuch that:\nTarget variables s'\u2081 = s' \u2229 ST are deter-\nmined by \u3160\u315c(ST)\nR = R(s, a, s') + R(st, a, s'y) is a real-valued\nreward function that includes rewards from the\nobserver's actions and rewards from the tar-\nget's actions.\nStructural changes include the addition of \u03c0\u03c4, a\npriori non-deterministic policy that simulates the\ntarget's behavior and constitutes a generative re-\nplacement for a plan library, as well as the tar-\nget's rewards when transitioning to states that meet\ngoal conditions. This approach to target behavior\nlends itself well to online, generative POMDP plan-\nning approaches such as those based on MCTS. The\ntarget's behavior is perceived through observations\nwt \u2208 \u03a9\u03c4 \u2286 \u03a9, which are target activities. For the\nobserver (the planning agent), no explicit distinc-\ntion is made between actions that modify the do-\nmain and those that gather information; during and\nafter planning the agent simply follows a converg-\ning action-selection function (such as UCB1 which\nminimizes regret).\nThis creates a more or less standard, unfactored\nPOMDP with the added complexity of a \"target\"\nelement, represented by additional state attributes\nthat are beyond the planning agent's control and"}, {"title": "4.1 Relevance Estimation and Intention Recognition", "content": "Relevance-based planning is an approach to im-\nprove the performance of planning under uncer-\ntainty where agents (such as robots) must com-\nplete tasks in complex domains that 1) Have in-\ncomplete and noisy information, 2) Provide many\ninteraction opportunities, 3) Require quickly identi-\nfying suitable goals and subgoals to pursue [20, 19].\nThe methods are presented as improvements to\nPOMDP planning, particularly with respect to gen-\nerative algorithms such as MCTS.\nFunctionally, it consists of \"Partial Goal Satisfac-\ntion\" (PGS) and \"Incremental Refinement\" (IRE).\nPGS is a method that estimates goal proximity by\nawarding positive or negative points when goal con-\nditions are met or broken, respectively, upon state\ntransitions, leading to an improved rollout policy\nand a reward bonus during simulation. IRE is a\ndimensionality reduction method that estimates a\nrelevance value, as a function of \"features\" (the el-\nements of the domain that provide actions to the\nagent). The contribution or reward from the actions\nin each feature are aggregated, weighed and scaled\nresulting in a value that represents their impact in\nproblem solving, enabling the agent to focus on rele-\nvant actions and avoid those with poor or non-goal-\nrelated outcomes. An example in the AGR setting\ncould be a robot preferring to gather information\nabout tools or parts that a worker might need soon,\nand avoiding those that appear to play no role in\nthe perceived plan.\nTo the best of our knowledge, existing POMDP-\nplanning based approaches for active intention\nrecognition (or similar) use out-of-the-box planners\nthat do not exploit the structure of such problems,\nor use point-based planners limited to very small\nproblems. The appeal of the relevance-based ap-\nproach is that it may already improve performance\nwith little more than the information already avail-\nable when specifying an active intention recognition\nproblem (as per def. 1).\nWe incorporated the PGS component into the in-\ntention recognition framework by assigning points\nto the goal conditions in Gr, and negative points\nfor conditions that violate Gr. The total score is\ncomputed by the sum of these points in some given\nstate or POMDP history, which results in the re-\nward shaping function:\nF(ht, ht+1) = \u03b3\u00a2(ht+1) \u2013 \u03c6(ht)", "equations": ["F(ht, ht+1) = \u03b3\u00a2(ht+1) \u2013 \u03c6(ht)"]}, {"title": "4.2 Challenges in Active Intention Recognition", "content": "As previously stated, the passive approach to in-\ntention recognition is primarily based on observing\nand classifying the target's behavior, and may ac-\ncount for missing observations and perception un-\ncertainty. When the observer is allowed to actively\nparticipate in the problem, however, it can receive a\nmuch broader variety of observations coming from\nits extended action set, many of which may not\nyield information about the target. Obtaining suf-\nficient information becomes part of the deliberation\nprocess and we argue that a successful model should\naccount for this by providing a suitable array of in-\nformation gathering actions. In other words, active\nintention recognition may in fact require active or\ndirected observation.\nParticularly when using POMDPs, informative\nobservations are crucial to correctly transform the\nbelief state and approximate the true, \"real-world\"\nstate. This task however may become challenging\nif an active observer is engaged in tasks that limit\nor prohibit observing key predictive behaviors, and\noutright insurmountable if such information be-\ncomes inaccessible. For example, in our object as-\nsembly example, two different objects that require\ndifferent tools may consist of almost entirely the\nsame parts, except for one which the robot may\nnot observe when performing independent work. In\nthis case, the ability to directly observe properties\nof the domain, such as the object being assembled,\nmay be necessary.\nAn additional challenge is how the observer, or\nplanning agent, perceives rewards for useful actions\nover a potentially long horizon and the immediate\nreward for most actions might be 0, or even neg-\native. This appears to be a characteristic of active\nintention recognition because, by definition, the ob-\nserver acts in anticipation of the target's actions\nand only later are reward-generating conditions sat-\nisfied. For example, in the assembly task the robot's\ngoal is not simply to restock all parts, but to re-\nstock parts that will be needed by the worker to\nassemble objects. The robot can bring more parts\nat any time (at a cost) even if they are not needed,\nor perhaps all necessary parts are already present\nand no further action is required. The worker, on\nthe other hand, generates positive rewards when\nan object is assembled successfully (a condition in\nGT) which may be received by the observer several\nsteps after the relevant action was performed (e.g.,\nrestocking the correct part). This situation makes\nplanning over large spaces particularly difficult, as\nany given observer action may have many different\noutcomes across over some finite horizon that make\nit difficult estimate its true action value. This prob-\nlem is inherent to POMDP planning but possibly\namplified with such problem dynamics.\nRecognizing behavior and intentions from both\ndirect and indirect observations as well as acting\nwith delayed rewards constitute significant hurdles\nfor active intention recognition. In summary, our\nproposal is to utilize a model that is suitable for\nonline, generative planning and that integrates well\nwith existing contributions that improve the ob-\nserver's ability to generate context-driven prefer-\nences, such as the relevance-based methods."}, {"title": "5 Preliminary Results", "content": "We modeled the two domains introduced in the be-\nginning of the paper using the online, active in-\ntention recognition approach. The resulting prob-\nlems were solved with two MCTS-style POMDP\nplanners: RAGE (which implements the relevance-\nestimation approach) and POMCP, a uniformly\nrandom planner commonly used as a state-of-the-\nart baseline. Although much more work is pending,\ncurrent results show promise in solving intention\nrecognition tasks and their resulting, large, unfac-\ntored POMDPs in an online manner.\nIn both problems we adopted a triple of the form\n(a, o, r) for target observations, where a \u2208 A is a\ntarget action, \u03bf \u2208 O is an object and r\u2208 Ris\nthe result of the target performing a on o. The set\nof target activities in \u03c0\u03c4 is therefore \u03a9\u03c4 = A \u00d7\nO XR, and the state space the cartesian product\nof all possible activities and state variables of all\nobjects. Internally such structure is irrelevant for\nthe POMDP planner, but it makes the problems\neasier to model, understand and discuss and may in\nfact transfer to other intention recognition domains."}, {"title": "5.1 Maintenance scenario", "content": "In the maintenance problem, a worker must inspect\na machine consisting of a compartment and a cir-\ncuit board. The worker may visually inspect the\ncompartment to determine if it is loose, and if it is\nproceed to tighten it with a screwdriver. The cir-\ncuit board has a relay, which may or may not be\ndamaged. To inspect it the worker needs a multime-\nter that shows the relay status, and if damaged the\nworker proceeds to replace it. The worker actions\nare A = {none, inspect, replace, screw}, the ob-\njects O = {board, compartment} and the possible"}, {"title": "5.2 Assembly scenario", "content": "The maintenance scenario requires the worker to as-\nsemble two toy trucks (one \"red\" and one \"blue\"),\neach of which is composed of parts such as a cabin,\nchassis and wheels. A part is taken from a set of\nstorage containers within reach of the worker; each\ncontainer has only one type of part but there may\nbe several containers. Once all parts are in place,\na truck needs one of two types of glue to be fully\nassembled, and afterwards the worker moves on to\nassemble the next truck. The problem is finished\nwhen all trucks are successfully assembled. In the\nlayout we used, the worker actions are A = {none,\nassemble, wait}, objects are O = {chassis, wheels,\nblue cabin, yellow cabin, red cabin, container} and\nthe results R = {OK, FAIL}. Same as before,\nFAIL means a worker action could not be completed\n(missing parts, missing glue). If the worker needs a\npart from an empty container, the container be-\ncomes \"needed\".\nThe worker simulation \u3160\u315c cycles the worker in a\nloop: in a = none, try to assemble the next part of\nthe current truck. If OK, mark as assembled and\nrepeat, otherwise wait and go to a = none. If all\nparts are assembled, attempt to glue and if OK,\nmove to the next truck and repeat. This means that\nthe robot must continually monitor both the envi-\nronment and the worker activities to minimize the\namount of failures and avoid forcing the worker to\nwait. The worker on the other hand may be unable\nto complete their task without the robot's help. In\nthis setting \u03c0\u03c4 generates rewards of -5 when a part\nor the correct glue is missing, -2 every time they\nwait and 5 when a truck is successfully assembled.\nThe robot has the following actions and observa-\ntions:\n\u2022\n\"Perceive worker\", which generates an (a, o, r)\nobservation\n\u2022 \"Inspect\u201d the current object/truck as well as\neach individual container (a total of 6). With a\nprobability proportional to its sensor accuracy,\nthe robot perceives the correct status (empty,\nnot empty) or truck type and otherwise, it re-\nceives a noisy reading.\n\u2022\n\u2022\n\"Restock\" any container (out of 6)\nThe reward distribution is:\n\u2022\n-0.5 perceive and inspect\n-2 restock any part, bring any glue"}, {"title": "5.3 Performance in Active Intention Recognition", "content": "The following plots show the discounted returns col-\nlected by the observer in the two active intention\nrecognition problems, as a function of an increasing\namount of MCTS simulations per step for a maxi-"}]}