{"title": "Identifying Intensity of the Structure and Content in Tweets and the Discriminative Power of Attributes in Context with Referential Translation Machines", "authors": ["Ergun Bi\u00e7ici"], "abstract": "We use referential translation machines (RTMs) to identify the similarity between an attribute and two words in English by casting the task as machine translation performance prediction (MTPP) between the words and the attribute word and the distance between their similarities for Task 10 with stacked RTM models. RTMs are also used to predict the intensity of the structure and content in tweets in English, Arabic, and Spanish in Task 1 where MTPP is between the tweets and the set of words for the emotion selected from WordNet affect emotion lists. Stacked RTM models obtain encouraging results in both.", "sections": [{"title": "1 Introduction", "content": "SemEval-2018 (Apidianaki et al., 2018) contained two prediction tasks that referential translation machine (RTM) models (Bi\u00e7ici, 2017a,b; Bi\u00e7ici and Way, 2015) can be applied to enable new modeling capabilities and provide new results for comparison. The trace of the shared information with tweets or social media to the audiences' world can be approached by the intensity of the content and structure used in text with Task 1 (Mohammad et al., 2018). The effectiveness of attributes to semantically separate two words from each other is the goal in Task 10 (Paperno et al., 2018), which can be used to improve natural language understanding systems. Table 1 lists the number of instances in the provided datasets. We use RTMs to model both tasks.\nRTMs use parfda (Bi\u00e7ici, 2016a) to select both parallel and monolingual interpretants, data close to the task instances selected specifically for the task, to derive features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets using machine translation performance prediction system (MTPPS) (Bi\u00e7ici et al., 2013; Bi\u00e7ici, 2022) to build prediction models. Interpretants provide context and text for feature derivation to link translation source and target and training and test sets. RTMs are applicable in different domains and tasks and in both monolingual and bilingual settings. Figure 1 explains RTMs' model building process."}, {"title": "2 Predicting the Discriminative Power of Attributes", "content": "Capturing discriminative attributes task (Task 10) (Paperno et al., 2018) is looking at whether an attribute (e.g. red) can be used to discriminate between two other words (e.g. apple and banana) to complement semantic similarity efforts. The answer to whether the attribute can be used to discriminate the two words can be useful for semantic similarity with contextual dependency. The task is posed as a binary classification task:  f(w\u2081, w\u2082, a) \u2192 0 ? 1 . The target value to predict is either 0 or 1 and shows whether the attribute can be used for discrimination for the given context. Evaluation metric is F\u2081. RTMs are used by casting the task as MTPP between the words and the attribute and building predictors that use the distance between the predictions. We assume that the discriminative power increase when the attribute is similar to words with significant difference.\nThe stacked RTM model with combined prediction step (Figure 2) use the same model to predict the MTPP similarity of  w\u2081 \u2192 a and of  w\u2082 \u2192 a where the data is collected such that the first row for each attribute is for  w\u2081 \u2192 a and the second is for  w\u2082 \u2192 a . Stacking is used to build higher level models using predictions from base prediction models where they can also use the probability associated with the predictions (Ting and Witten, 1999). The combined model in Figure 2 is adding the predictions as additional feature. We obtain an RTM representation vector for each of the instances by using the derived features and the combination of the prediction scores along with 5 additional features:\n\u0177\u2081 \u0177\u2082 |\u0177\u2081 - \u0177\u2082|(\u0177\u2081 + \u0177\u2082)/2 \u221a\u0177\u2081 * \u0177\u2082 (1)\nAfter this filtering step, we run another learning and prediction on the concatenation of the features from both rows and the additional features.\nThe stacked RTM model with separate prediction steps use separated feature sets with an initial prediction with each (Figure 3). Using separate predictors for each word need not achieve better performance than using a single predictor since the rows of words we compare need not belong to different groups due to their ordering in the dataset and the words may be only randomly positioned. However, both predictions can be useful as coming from two weaker predictors since half of the training instances were used to train each. The benefit of using separate feature sets is further specialization of  w\u2081 \u2192 a and  w\u2082 \u2192 a models with feature selection and partial-least squares from Section 4. As we see in Table 4, separated feature sets with two separate learning and prediction steps improve the performance.\nThe architectures of Figure 2 and Figure 3 are general enough to be useful for Task 1 as well and when we want to make use of the differences between the predictions for the same instances."}, {"title": "3 Predicting the Intensity of the Structure and Content in Tweets", "content": "Affect in tweets \u00b9 task (Task 1) (Mohammad et al., 2018) is about predicting the intensity of the emotion expressed for tweets within sadness, joy, fear, or anger emotion categorizations or the valance (sentiment). The emotion within tweets is about how the tweeter wrote and valence or sentiment is about what the tweeter wrote. Intensity scores are obtained with best-worst scaling (bws) (Mohammad and Kiritchenko, 2018), which counts only the number of times a tweet is labeled as best or worst among 4 tweets where each is annotated by multiple workers. The scores are obtained with the percentage of counts scaled to [-1,1], which is later scaled to [0, 1] for the task. bws can decrease the annotation effort to obtain the set of binary comparisons used to obtain reliably agreed labels.\u00b2\nWe model the task as MTPP of the tweets to the emotions to answer questions like \u201cto what degree is this tweet showing the emotion of\u201d. Since a single emotion word need not provide enough context for semantic discrimination, we use sets of words for each emotion that express the same meaning using a subset of the WordNet affect emotion lists (Strapparava and Valitutti, 2004). The lexicon used for English is in Appendix A. We obtained their translations to Arabic and Spanish using web translation sites \u00b3 to obtain the corresponding lexicon. We use the whole set of words corresponding to the emotion instead of the emotion word to translate to. For valence intensity tasks, we used both of the sets of words from emotions joy and sadness as a single sentence to translate to and we also used them as separate rows using Figure 2 and Figure 3 where the tweet's MTPP is predicted to the sets of words of either joy or sadness (Table 10). We participate in the regression tasks of Task 1 to predict the emotion and valence intensity in Arabic, English, and Spanish tweets. The offi-"}, {"title": "4 RTM Models and Results", "content": "The RTM models used predictions from machine learning models including ridge regression (RR), k-nearest neighors (KNN), support vector regression (SVR), AdaBoost (Freund and Schapire, 1997), and extremely randomized trees (TREE) (Geurts et al., 2006) in combination with feature selection (FS) (Guyon et al., 2002) and partial least squares (PLS) (Wold et al., 1984). We use averaging of scores from different models for robustness (Bi\u00e7ici, 2017b). Model implementations use scikit-learn. \u2075 We optimize \u03bb for RR, k for KNN, \u03b3, C, and \u03f5 for SVR, minimum number of samples for leaf nodes and for splitting an internal node for TREE, the number of features for FS, and the number of dimensions for PLS. We use 500 estimators in the TREE model and also for AdaBoost along with exponential loss. We use grid search for SVR. We evaluate with Pearson's correlation (r), mean absolute error (MAE), relative absolute error (RAE), MAER (mean absolute error relative), and MRAER (mean relative absolute error relative) (Bi\u00e7ici and Way, 2015). We use 7-fold cross-validation performance on the training set to rank models.\nInterpretants are selected from the corpora distributed by the translation task of WMT17 (WMT, 2017) and they consist of monolingual sentences used to build the LM and parallel sentence pair instances used by MTPPS to derive the features. We built RTM models using:\n\u2022 300 thousand sentences for training data\n\u2022 5 million sentences for LM\nRTMs generate features for the training and the test set to map both to the same space where the total number of features in Task 1 becomes 492 and Task 10 becomes 117. The difference is due to the smaller context the attribute word provides and most of the sentence-level features become not useful including the sentence structure parsing features or word alignment features."}, {"title": "5 Experiments After the Challenge", "content": "We recalculated our results using 200 thousand training instances for Task 1, which also helps comparison with previous results (Bi\u00e7ici and Way, 2015) and an advanced IBM2 alignment model after the challenge. Table 4 lists the new results obtained after the challenge. The number of results with MRAER larger than 1 decreased to 5. The MRAER obtained by RTMs in STS in 2016 is 0.73 (Bi\u00e7ici, 2016b) and in quality estimation task for English to German in 2017 is 0.8 (Bi\u00e7ici, 2017a). Translation similarity distances alone can be useful to identify the discriminative power of attributes Table 9. The predictions for Task 10 were transformed to binary classes by thresholding with optimized thresholds on the training set. Prediction differences are used from the combined model or the separate model. Table 10 show that valence can be better predicted with MTPP towards the translation of the vocabulary of sadness rather than using the union of contrastive vocabulary sets of joy and sadness. Separate learning also improve the performance for English and Spanish valence prediction. Translation similarity distances are also used for Task 1 in Table 10 where instead of learning a threshold to optimize for F\u2081 score, we fit a linear model,  ax + b , to the distance between predictions."}, {"title": "5.1 Symbolic Grounding of Score Thresholds", "content": "We symbolically ground the score thresholds with respect to the statistics of the training set scores and use the corresponding linear mathematical equation in the form of  \u03b1\u03bc + bo + c on the test set statistics to obtain a relative test set threshold. We observe up to 1% improvements in the prediction performance in Task 10 (Section 5)."}, {"title": "5.2 Symbolic Grounding of Prediction Score Statistics", "content": "We symbolically ground the predictions with respect to the statistics of the training set scores and use the corresponding linear mathematical equation on the test set statistics to obtain relative predictions on the test set. We observe up to % improvements in the prediction performance (Section 5)."}, {"title": "5.3 Ranking Results", "content": "We would like to obtain robust sortings on the training set so that the sortings are reflected on the performance sorting on the test set. We compare the ranking differences that incur when we choose a metric to rank both the training and test set results. Section 5.2 compares the performance metrics we use using  rs , Spearman's correlation, and ranking errors, which compute the squared error of normalized ranking differences (Equation (2)).\nrankError = \u03a3(rank(i, train)/n-rank(i, test)/n)\u00b2 (2)"}, {"title": "6 Conclusion", "content": "Referential translation machines obtain automatic prediction of semantic similarity using MTPP. We presented encouraging results with stacked RTM models for predicting the intensity of the structure and content in text with our novel MTPP modeling for translation to WordNet emotion lists and the discriminative power of attributes using stacked RTM models. Our results also enable comparisons of prediction results of RTMs in different natural language processing tasks (Bi\u00e7ici, 2017c)."}, {"title": "Task 1", "content": "Yi - Yi\nYie\n(7)"}, {"title": "", "content": "Yi - Yi\nYie\n(8)"}, {"title": "", "content": "1 Yi - Yi (\u0177i \u2013 \u0177)(Yi \u2013 \u1ef3)\n(9)\nYi Yie V \u03a3o2 2"}, {"title": "", "content": "x if x \u2265 0\n-2x if x < 0\n(10)"}]}