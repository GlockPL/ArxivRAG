{"title": "Building a Rich Dataset to Empower the Persian Question Answering Systems", "authors": ["Mohsen Yazdinejad", "Marjan Kaedi"], "abstract": "Question answering systems provide short, precise, and specific answers to questions. So far, many robust question answering systems have been developed for English, while some languages with fewer resources, like Persian, have few numbers of standard dataset. In this study, a comprehensive open-domain dataset is presented for Persian. This dataset is called NextQuAD and has 7,515 contexts, including 23,918 questions and answers. Then, a BERT-based question answering model has been applied to this dataset using two pre-trained language models, including ParsBERT and XLM-ROBERTa. The results of these two models have been ensembled using mean logits. Evaluation on the development set shows 0.95 Exact Match (EM) and 0.97 Fl_score. Also, to compare the NextQuAD with other Persian datasets, our trained model on the NextQuAD, is evaluated on two other datasets named PersianQA and ParSQuAD. Comparisons show that the proposed model increased EM by 0.39 and 0.14 respectively in PersianQA and ParSQuAD- manual, while a slight EM decline of 0.007 happened in ParSQuAD-automatic.", "sections": [{"title": "1. Introduction", "content": "The increase in the number of documents and text corpus, makes information retrieval (IR) a substantial issue [1]. IR systems extract users' desired information from documents using Natural Language Processing (NLP) [2]. One of the main tasks of NLP is Machine Reading Comprehension (MRC), which is capable of comprehending a text and helping Artificial Intelligence (AI) compete with humans. Developing such a system that understands textual data will be valuable because, in many tasks, people need to extract information from text [3]. Although, search engine is the application of IR techniques working based on keywords, users prefer to ask questions using natural language, and like to receive short and precise answers instead of long text. Therefore, researchers are trying to find another option for IR to take a step toward the smartification of current search engines, so that users can interact with computers by natural language. In this regard, Question Answering (QA) systems are considered an advanced form of IR [4] [5], which are a growing research field in the world [6], and demand for such systems is increasing day by day because they represent short, precise, and specific answers [7]. Search engines give some related and unrelated pages to users based on given keywords, while QA systems take natural language questions and retrieve proper answers automatically [8] [9] [10] [11]. A reasonable answer is short, understandable, and accurate, which may be a word, sentence, paragraph, or complete document [12]. Generally, QA systems are classified into two major groups: open-domain and closed-domain. Suppose a QA system answers questions in a specific domain; for example, in health, it is called a closed domain. Closed-domain QA systems need knowledge in a particular field. Although a closed-domain QA system can give rapid, precise, and up-to-date answers in its specific domain, it can't be used in other areas [4]. Open-domain QA system is another kind of QA system used in different fields [4]. An open- domain QA system aims to extract answers from an extensive dataset. Existing datasets for reading comprehension mostly have one of the two following shortcomings:\n\u2022\tHigh-quality datasets [13] [14], are too small to train complex models.\n\u2022\tLarge datasets [15] [16], are semi-artificial with low quality.\nTherefore, to respond to the needs, a prominent and high-quality reading comprehension dataset, Stanford Question Answering datasets (SQuAD v1.0), has been proposed [17], which is freely available and consists of some Wikipedia articles. In this dataset, a part of the text is selected as the answer. SQUAD consists of 785,107 question-answer pairs in 536 articles and is almost two times bigger than previous reading comprehension datasets with manual labels like MCTest [13]. Some other large-scale annotated datasets are MCTest [18], Book-Test [19], SearchQA [20], NewsQA [21], ReCoRD [22], and ReCO [23], which have been recently represented. So many robust QA systems have been specifically developed for English. Unfortunately, the primary QA datasets are in English, while languages with fewer resources need more work [3]. Considering Persian, a few pieces of research have been done, including PersianQA [24], ParSQUAD [25] and Rasayel&massayel [26]. ParSQuAD is a translation of SQUAD [17] in Persian using google-translate, and PersianQA is a small QA dataset including 10000 questions. Despite the accessibility of different resources and datasets to answer retrieval, answering questions is still a challenging problem due to difficulty in comprehending questions and extracting answers [27]. As most existing systems support limited types of questions, to improve the performance of these systems, more efforts are required. Since there is a limited number of datasets in Persian, this study has gathered a comprehensive and authoritative open-domain dataset for this language. Also, a Persian NLP module has been proposed to answer questions. The suggested method is an open-domain QA that can be used in all domains, such as medicine and sports. The rest of this paper has been organized as follows: part 2, the related literature; part 3, the process of data collection and the methods used to make sure of a high-quality dataset; part 4, modeling; part 5, the evaluation of the gathered dataset, and part 6 conclusion and suggestions for future works."}, {"title": "2. Related Literature", "content": "Existing challenges in English and non-English QA models mostly rooted in quality and quantity of datasets. Therefore, studies about QA are divided into two parts: English and non-English datasets, and in this study, the focus in non-English is on Persian."}, {"title": "2.1. English Datasets", "content": "The increasing attention of researchers to MRC caused a growth in the significance of its datasets. Despite the fact that, there are many MRC datasets [18] [19] [17] [20] [21] [22] [23], there are still lots of features that can be considered to collect new datasets for MRC tasks. MCTest [18] is one of the first and the smallest MRC datasets, including imaginary stories, and multiple choices questions. MCTest is too small for training big models. SQUAD 1.1 [17] is another extensive dataset. It consists of a list of questions on a set of Wikipedia articles. The answer to each question is a part of the text (span), from the corresponding article. SQUAD 2.0 [28] is generated using SQuAD 1.1, in addition to more than 50000 unanswered questions and some other answered questions. The presence of unanswered questions turns SQUAD 2.0 into a more challenging dataset. Existing studies on SQUAD 2.0 showed that MRC models performed better than humans. SearchQA [20] is generated based on question-answer pairs from the popular television show Jeopardy. Each question has been googled, and retrieved text has been used as context. SearchQA consists of about 140000 question-answer pairs, and each has been associated with around 50 pieces of text. ReCoRD [22] is automatically gathered from news articles. The discriminating characteristic of ReCoRD is that despite other MRC datasets like SQuAD, it needs common sense reasoning in several sentences. This article discusses the limitations of existing MRC models, which primarily rely on paraphrasing questions to find answers. To address this, the ReCoRD dataset was designed to minimize queries that can be answered merely by paraphrasing. Instead, ReCoRD focuses on requiring commonsense reasoning, distinguishing it from other datasets. TWEETQA [29] is another detest in the question answering field, which has been produced from tweets that famous journalists have published. This dataset is suitable for question answering models that are looking to extract the answer for a question from an extensive text corpus."}, {"title": "2.2. Non-English Datasets", "content": "In the last few years, a high motivation is formed among researchers to collect low-resource languages datasets. Most of these datasets either use Wikipedia as their main resource, like SQUAD, or utilize the translation of the SQuAD, such as ReCo [23] in Japanese, ParSQUAD [25] in Persian, ARCD and Arabic-SQuAD [30] in Arabic, KorQuAD1.0 [31] in Korean, Spanish translation of SQuAD [32], FQuAD [33] in French, and a multilingual dataset [34]. In this study, the Persian MRC datasets are being elaborated more. Razzagh Nouri et al. [35], represented a solution for question classification using machine learning approaches. There are three feature extraction methods illustrated in this study. In the first method, words are divided into clusters, and then feature vectors of questions are generated based on clustering information. The second method extracts features using a neural network. Each question turns into a vector, which is achieved using word2vec and is weighed by tf-idf coefficients. In the third approach, not only does it retain the novelty of the first approach, but also it takes into account data types that are sequence-based. In this paper, they introduced the UTQD.2016, collected from some Jeopardy games. The described feature vectors' efficacy is proofed by SVM and neural network results."}, {"title": "3. The Proposed Dataset Gathering", "content": "Having a robust QA system, a complete and standard dataset is required. Considering shortcomings in Persian datasets, a complete and comprehensive dataset called NextQuAD is introduced in this study. Figure 1 shows the proposed approach for gathering NextQuAD. Data is collected with three methods:\n\u2022\tTranslation of SQUAD 2.0\n\u2022\t\"Quiz of Kings\u201d question-answer pairs\n\u2022\tManual data gathering from some well-known Persian websites by searching on Google."}, {"title": "3.1. SQUAD 2.0 Translation", "content": "In the first step of gathering the NextQuAD, SQUAD 2.0 is translated into Persian. To translate the dataset, Google Translation API [45] is used in python which leads in having more than 20 thousand interpreted records. From this, after undergoing a manual checking process, only 845 items have suitable translations or are useful with a bit of alternation. As this process is expensive in terms of human force and time, and the quality of most of the translated records is low, the process is stopped and we leave translation of the remained records of SQUAD 2.0."}, {"title": "3.2. \u201cQuiz of Kings\u201d question-answer pairs", "content": "In this step, like in SearchQA [18], a Persian game application called \u201cQuiz of Kings\", in which some questions are asked of users, and their knowledge is challenged, was used. This part of our dataset is produced using 17,000 questions and their answers on this application, by applying a web scraping technique through the following process:\n\u2022\tConcatenating questions and answers, and searching them in Google.\n\u2022\tChoosing the first founded result and opening the related webpage.\n\u2022\tCalculating the similarity between paragraphs of this webpage and the searched text, using BERT [46].\n\u2022\tSelecting the most similar paragraph as the context of that question. If this paragraph has a similarity under 0.8, this question-answer pair is ignored. This threshold has been determined through a process of trial and error.\n\u2022\tFinding the answer in the context and using its index for start and end.\nQuestions and answers have existed before, and the above process has been only used to find context and span. All this process resulted in 1,915 records.\""}, {"title": "3.3. Manual Data Gathering", "content": "Using the previous two steps, less than 3,000 accurate and high-quality records were collected. Next, seven crowdworkers helped to produce new question-answer pairs in different fields to complete the dataset, which are named in Table 1. Exploration was conducted through one of the following valid websites: Persian Wikipedia\u00b9, Chetor\u00b2, Namnak\u00b3, Digiato4, Zoomit, and some other news websites. This process leaded to production of more than 21,000 records in 4 months."}, {"title": "3.4. Final Dataset Preparation", "content": "After merging the aforementioned datasets, 7,515 contexts and 23,918 pairs of question and answer were achieved."}, {"title": "3.5. Inter Annotator Agreement", "content": "As in this dataset there are some cases with the same context assigning to multiple question and answer pairs, to prevent data leakage, we ensure to have each context with this feature entirely whether in train set or in development set. For each question, several answers were considered in the development set (like SQUAD 2.0). For example, if a text is about Ali Daei, and the question is \u201cHow many national goals does Ali Daei have?\", the answers could be \u201c109\u201d, \u201c109 goals\u201d, or \u201c109 national goals\u201d. Therefore, it is necessary to have two or three answers for some questions in the development set. \""}, {"title": "3.6. Dataset Texts Normalization", "content": "While there are some characters in Persian and some in Arabic which can be employed interchangeably, they have different unicodes. It makes words with such letters to be treated as two different words. So, standard character forms must be used in datasets [49]. In this step, all Arabic characters in the dataset are substituted with their Persian forms."}, {"title": "4. Modeling", "content": "The NextQuAD was evaluated using two pre-trained transformer-based language models, ParsBERT and XLM-ROBERTa. As we had limitations in GPU memory, the highest length for each context was considered 310 words, and K-fold cross-validation with five folds was employed to evaluate the modeling results on the training dataset. There are some basic models (trained in each fold) that outputs of their softmax output layer are two vectors with the length of 310, which estimate the place of start and end. Thus, according to [50] and [51], the results of basic models were ensembled to improve their performance. Mean logits was used as an ensemble method. The result of the ensemble method for the start point is the average of start vectors achieved by basic models, and precisely similar to that, the result for the end point is the average of end vectors performed by basic models."}, {"title": "5. Experiments", "content": "As there are five folds, and in each of them, an independent model is trained, at the end of each fold, its model is saved and evaluated on the development set. As it can be perceived in table 5, XLM-ROBERTa has outperformed ParsBERT."}, {"title": "6. Conclusion", "content": "QA system is a growing research field, and demand for this kind of system has been increasing. QA systems have been well developed in different languages, especially English. Nonetheless, as there are rare rich datasets for Persian, there would be a blockage in developing a robust QA system for this language. So, this study gathered and designed a comprehensive and authoritative open- domain dataset for the Persian called NextQuAD. This dataset was collected through three approaches: 1- translation of SQuAD 2.0, 2- \u201cquiz of kings\u201d question-answer pairs, and 3- manually gathered data from valid Persian websites. 23,918 questions and answers were gathered in 7,515 contexts, 10% of which were kept for evaluation in the development set, and the rest were used in the training set. The model trained by NextQuAD is applied to the development parts of ParSQuAD and PersianQA to compare the quality of NextQuAD and the other two datasets. The results showed that our proposed model performs better than the two other models. Comparing NextQuAD with ParSQuAD shows that a large dataset doesn't necessarily improve the quality of the model, and the dataset quality is more important. Developing and completing NextQuAD, both in the case of subjects and questions diversity, can improve QA models in Persian. Also, considering the particular form of QA models' outputs, it seems that there are lots of innovative ensemble techniques that can boost the results."}, {"title": "Availability of supporting data", "content": "All materials of this research are available in github:\nhttps://github.com/mosiomohsen/NextQuAD\nThe dataset will be added after our research is published. If that's required for reviewers, we could send it."}]}