{"title": "High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA Fine-Tuning with Multimodal LLaMA 3.2", "authors": ["Nandakishor M", "Anjali M"], "abstract": "Electrocardiogram (ECG) interpretation is a cornerstone of cardiac diagnostics. This paper explores a practical approach to enhance ECG image interpretation using the multimodal LLaMA 3.2 model. We used parameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA), specifically designed to boost the model's ability to understand ECG images and achieve better outcomes across a wide range of cardiac conditions. Our method is tailored for ECG analysis and leverages ECGInstruct [1], a large-scale instruction dataset with 1 Million samples. This dataset is a rich collection of synthesized ECG images, generated from raw ECG data from trusted open-source repositories like MIMIC-IV ECG [27] and PTB-XL [33]. Each ECG image in ECGInstruct comes with expert-written questions and detailed answers, covering diverse ECG interpretation scenarios, including complex cardiac conditions like Myocardial Infarction and Conduction Disturbances. Our fine-tuning approach efficiently adapts the LLaMA 3.2 model [15] (built upon LLAMA 3 [17]) by integrating low-rank adaptation techniques, focusing on efficiency by updating only a small set of parameters, excluding the 'lm_head' and 'embed_tokens' layers. This paper details the model setup, our efficient fine-tuning method, and implementation specifics. We provide a thorough evaluation through extensive experiments, demonstrating the effectiveness of our method across various ECG interpretation tasks. The results convincingly show that our parameter-efficient LoRA fine-tuning achieves excellent performance in ECG image interpretation, significantly outperforming baseline models and reaching accuracy comparable to or exceeding traditional CNN-based methods in identifying a wide range of cardiac abnormalities, including over 70 conditions from the PTB-XL dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "Interpreting electrocardiograms (ECGs) is a vital skill in diagnosing heart conditions, providing key information about the heart's electrical activity. While ECG analysis has traditionally focused on raw signals, there's an increasing need for reliable methods to interpret ECGs directly from images. Cardiologists, general medicine practitioners and other experts are more skilled towards in indentifying subtle variations in ecgs like arrhythmias, myocardial infarctions, atrial fibrillation, ventricular tachycardia, bundle branch blocks, ischemic heart disease, left ventricular hypertrophy, ST elevation myocardial infarction, Non-ST elevation myocardial infarction. But when patients comes to the emergency department with chest pain or discomfort, they are mainly diagnosed by inexperienced doctors, and may lead to misdiagnosis and deaths [3].\nThe rise of multimodal large language models (MLLMs) offers exciting possibilities for understanding information from different sources, potentially transforming ECG image interpretation [2], [19]. However, using MLLMs for ECG analysis has its challenges, as many of the generative AI models like GPT-4 series or their reasoning models from OpenAI prohibited to use in medical environment [16]. Also there are limited availability of large, instruction-tuning datasets for ECGs and the need for efficient fine-tuning methods. These methods must allow models to learn ECG complexities without losing their general knowledge. Traditional methods, often using Convolutional Neural Networks (CNNs), haven't always achieved the good accuracy and comprehensive interpretation that expert cardiologists can get from raw signals.\nThis paper tackles these challenges by using the multimodal LLaMA 3.2 model [15] with a parameter-efficient Low-Rank Adaptation (LoRA) fine-tuning strategy by ignoring certain layers from the model. This approach efficiently adapts the model by updating only a small set of parameters \u2013 specifically using LORA to deeply learn the ECG domain. This leads to significantly improved accuracy in ECG image interpretation. To make this possible, we leverage ECGInstruct [1], a carefully curated, large instruction dataset for ECG image interpretation. This paper details the model setup, the efficient fine-tuning approach, implementation, and presents extensive experimental results. These results show that our parameter-efficient LoRA fine-tuning achieves excellent accuracy in ECG image interpretation, clearly outperforming baseline models and reaching performance levels needed for clinical use. This includes accurately identifying over 70 different cardiac conditions.\nOur main contributions are:\n1) We demonstrate a high-accuracy ECG image interpretation method using parameter-efficient LoRA fine-tuning on a multimodal model, achieving state-of-the-art performance in this area. We selectively excluded certain layers from the model to improve the accuracy of the model, this was done by extensive trial and error.\n2) We leverage ECGInstruct [1], a large instruction dataset, expertly curated using public ECG data, which is a signal"}, {"title": "II. RELATED WORK", "content": "Automated ECG analysis has been around for a while, starting with a focus on raw time-series signals. Early systems, like those from Moody et al. [20], used rule-based methods to extract features. Deep learning, especially CNNs, brought major advancements to time-series ECG analysis [9], [11], [26]. Hannun et al. [9] showed that CNNs could classify arrhythmias as well as cardiologists using raw ECG signals. Ribeiro et al. [26] used CNNs on raw signals to classify six cardiac abnormalities. Hughes et al. [11] further confirmed the reliability of CNNs for time-series ECG data. These methods are powerful for real-time analysis, but they depend on raw ECG signals, which limits their use for image-based ECGs.\nImage-based ECG analysis is becoming more popular to overcome data availability issues [28]. Sangha et al. [28] showed that ECG images can be used to detect cardiac abnormalities and for multi-label diagnosis. However, current image-based methods often don't have the comprehensive diagnostic range and accuracy of expert human interpretation, especially when dealing with a wide variety of cardiac conditions.\nGeneral medical MLLMs like LLaVA-Med [13], MedPaLM [5], and Med-Gemini [14] show promise in medical cross-modal reasoning, but often lack specific fine-tuning for tasks like ECG analysis. They may not have the specialized knowledge needed for highly accurate and comprehensive ECG interpretation across many different cardiac conditions.\nFine-tuning is essential for adapting large models to specific tasks. Traditional methods often fine-tune models partially, updating only projection or cross-attention layers. However, this limited fine-tuning might restrict knowledge transfer and adaptation to specialized data like ECG images, potentially preventing the achievement of expert-level accuracy.\nIn contrast to previous work, our method uses parameter-efficient LoRA fine-tuning on multimodal LLaMA 3.2 [15], leveraging ECGInstruct, a large and diverse multi-task instruction dataset for ECGs by only ignoring language model head and embedding layers. This approach sets a new standard for high-accuracy and comprehensive ECG image interpretation, addressing the limitations of existing methods. It paves the way for better applications of MLLMs in cardiology, achieving accuracy levels necessary for reliable clinical decision support across a broad range of cardiac conditions."}, {"title": "III. MODEL ARCHITECTURE", "content": "Our model is built on the strong architecture of the multimodal LLaMA 3.2 model [15], with specific adjustments carefully made for the complex task of ECG image interpretation and achieving high diagnostic accuracy. LLaMA 3.2, a leading large language model from Meta, naturally combines vision and language processing. The model's design has three main parts: a sophisticated two-stage vision encoder, a powerful LLaMA 3.1-based language model, and a carefully designed integration mechanism. This integration effectively bridges the gap between visual features from ECG images and the language model's text understanding, enabling the cross-modal reasoning needed for accurate interpretation. Let's look at each component in detail.\nA. Vision Encoder\nThe vision encoder is the crucial front part of our model. Its job is to extract meaningful and needed features directly from ECG images. These visual features are the foundation for all further analysis and interpretation by the language model, directly affecting the overall diagnostic accuracy. Before an ECG image is processed by the vision encoder, it first goes through preprocessing to ensure consistency and optimize feature extraction. This includes resizing all input images to a standard 448x448 pixel resolution. After resizing, each image is divided into 14x14 pixel non-overlapping patches, a common technique in vision transformers [6]. This patching turns the image into a 32x32 grid of smaller image patches, which are then fed into the carefully designed two-stage vision encoder for feature extraction.\nVision encoder uses a two-stage approach to ensure enhanced and multi-scale feature extraction. This design is important for capturing both fine local details, like subtle wave shapes, and broader global context, such as overall rhythm patterns in the ECG. This multi-scale approach is key to comprehensive ECG interpretation and high diagnostic accuracy.\n1) 32-Layer Local Encoder: The first stage is a deep 32-layer Transformer architecture [32]. This 'local' encoder processes all image patches in a global context, allowing it to understand relationships between different parts of the ECG image. Each patch is processed through multiple self-attention operations in each of the 32 layers, helping the model understand how different parts of the ECG image relate to each other. Importantly, intermediate output representations are taken from specific layers within the local encoder \u2013 specifically, layers 3, 7, 15, 23, and 30. These intermediate outputs, along with the final output from layer 32, are saved for later use in the global encoder. This multi-layer output strategy is designed to capture visual information at different levels of detail. Earlier layers tend to capture basic visual features like lines and curves (representing individual ECG waves), while later layers capture more complex, high-level features such as overall wave shapes and rhythm patterns."}, {"title": null, "content": "This hierarchical feature extraction is crucial for detailed ECG analysis and high diagnostic accuracy.\n2) 8-Layer Global Encoder: The second stage is an 8-layer Transformer, which we call the 'global encoder'. This stage takes the final output from the 32-layer local encoder as input, and also includes the intermediate outputs saved from layers 3, 7, 15, 23, and 30 of the local encoder. A key feature of the global encoder is the use of a gated attention mechanism. This allows the model to adaptively weigh the importance of different visual features extracted by the local encoder. By selectively emphasizing important visual features and reducing the importance of less relevant ones, the gated attention mechanism improves the model's ability to focus on the most diagnostically important aspects of the ECG image, directly contributing to improved accuracy. For example, in an ECG showing ST-segment elevation, the gated attention might focus more on the ST-segment shape while paying less attention to baseline noise.\nThe final visual representation, Z, is created by combining the output of the 8-layer global encoder with the intermediate outputs saved from the 32-layer local encoder. This combination creates a comprehensive multi-scale representation that effectively encodes the rich visual information in the input ECG image, which is essential for accurate interpretation. The resulting feature vector Z has a dimension of 7680. This high-dimensional visual feature vector is then passed to the integration mechanism, where it will be aligned with the language model's text embedding space, preparing it for cross-modal interaction and high-accuracy ECG analysis.\nMathematically, consider an input ECG image $X \\in \\mathbb{R}^{H \\times W \\times C}$, where H, W, and C are the height, width, and number of color channels (usually 3 for RGB or 1 for grayscale), respectively. The patching process extracts patches of size P x P, where P = 14 in our case. Each patch can be represented as $X_p \\in \\mathbb{R}^{P \\times P \\times C}$. The 32-layer local encoder transforms the input patches into a sequence of patch embeddings $Z_i \\in \\mathbb{R}^{N \\times d}$, where N is the number of patches (N = (H/P) x (W/P) = 32 x 32 in our setup) and d is the embedding dimension. The intermediate representations extracted from the local encoder are denoted by $Z_i$. The 8-layer global encoder processes the final output of the local encoder, $Z_1$, and the intermediate representations $Z_i$ to produce the global visual representation $Z_g$. The final visual representation Z is then obtained by combining these components:\n$Z = Concat(Z_g, Z_i)$ (1)\nIn our implementation, the dimension of the final visual representation $Z$ is d' = 7680.\nB. Language Model\nThe language model at the core of our architecture is based on LLaMA 3.1, a highly capable pre-trained language model known for its excellent performance in many natural language processing tasks [17]. LLaMA 3.1 has 40 Transformer layers [32], each using a self-attention mechanism. This self-"}, {"title": null, "content": "attention is fundamental to the language model's ability to process text input in context, and it's crucial for generating accurate and clinically relevant interpretations. It allows the model to understand the relationships between words in a sentence and produce coherent, contextually relevant text. Think of it as the model's way of 'reading' and understanding text, similar to how a person would.\nTo enable effective interaction and integration with the visual information from the vision encoder, the language model architecture strategically includes cross-attention layers at specific points within its Transformer structure. These cross-attention layers act as bridges between the visual and text processing paths, helping the model achieve better accuracy by considering both visual and textual information.\n1) Self-Attention Layers: The language model is built on standard Transformer layers that use self-attention [32]. These layers are responsible for processing the initial text input, understanding its structure and meaning, and creating token embeddings. These embeddings are the basis for all further language processing within the model, ultimately contributing to the accuracy of the final interpretation. Self-attention allows the model to dynamically assess the importance of different words in the input text when building its internal representations. For example, in the phrase \"ECG shows ST-elevation myocardial infarction\", self-attention helps the model understand that \"ST-elevation\" and \"myocardial infarction\" are closely related and crucial for the overall meaning, leading to a more accurate diagnosis.\n2) Cross-Attention Layers: To seamlessly integrate visual information from the ECG image, cross-attention layers are strategically placed in the language model architecture at regular intervals, specifically every 5 layers, starting at layer 3 and then at layers 8, 13, 18, 23, 28, 33, and 38. The cross-attention mechanism allows the language model to pay attention to and incorporate relevant visual features from the ECG image during the text generation process, which is key to achieving good accuracy in ECG interpretation. Essentially, it lets the model 'look' at the ECG image while it's forming its text response. This enables the model to base its text output on the visual content of the ECG, facilitating true multimodal understanding and generation. It ensures that the generated text is supported by the visual evidence in the ECG. For example, if the visual encoder detects ST-segment elevation in the ECG image, the cross-attention layers will allow the language model to include this visual finding in its generated text. It might mention \"ST-segment elevation present\" in a report or answer a question about ST-segment changes, thus ensuring accuracy and clinical relevance.\nThis carefully designed architecture enables effective multimodal learning by allowing the model to understand both text prompts (like questions about the ECG) and visual ECG information in a unified way. This is crucial for achieving"}, {"title": null, "content": "accurate and reliable ECG interpretation. The hidden states of the language model, which represent the text context it has processed, are combined with the visual features extracted by the vision encoder through these strategically placed cross-attention layers. This fusion of visual and textual information is key to the model's ability to perform complex ECG interpretation tasks\nThe language model takes a sequence of input tokens $T \\in \\mathbb{R}^{L \\times v}$, where L is the sequence length (number of tokens in the input text) and v is the size of the vocabulary used by the model. These input tokens are first converted into continuous vector embeddings $E_t$. The LLaMA model then processes these text embeddings through its series of self-attention layers, generating a sequence of hidden states $H_t$. The cross-attention mechanism is then used to combine these hidden states from the text $H_t$ with the visual features extracted from the ECG image. The cross-attention calculation can be represented mathematically as:\n$H_c = CrossAttention(Q_t, K_v, V_v)$ (2)\nHere, $Q_t$ represents the query vectors, derived from the language model's hidden states $H_t$. $K$ and $V$ represent the key and value vectors, respectively, derived from the projected visual feature representation. The CrossAttention function calculates attention weights based on the similarity between queries and keys, and then uses these weights to combine the value vectors. This results in context-aware hidden states $H_c$ that now include both text and visual information. This approach allows the model to seamlessly integrate visual understanding into the text generation process. It can accurately answer questions about the ECG image, generate comprehensive reports with coherence.\nC. Integration Mechanism\nThe integration mechanism is a vital part that acts as a bridge, connecting the visual feature space with the language model's text embedding space. It ensures effective cross-modal interaction and high-accuracy ECG interpretation. Visual and text information are inherently represented and processed differently. The integration mechanism is specifically designed to transform the high-dimensional visual feature representations, extracted by the vision encoder, into a format that is semantically compatible with the language model. This crucial step ensures effective cross-modal reasoning and seamless information fusion. It allows the model to understand and relate visual and textual cues together, which is essential for achieving accurate diagnoses. In our architecture, we use a simple yet effective linear projection layer for the integration mechanism. The simplicity of linear projection has proven surprisingly powerful in similar multimodal models [2]. It offers a good balance between efficiency and performance without compromising the accuracy of the integrated features.\nThe linear projection can be mathematically described as:\n$F = W_pZ_v + b_p$ (3)\nWhere $W_p$ is the weight matrix of the projection layer, and $b_p$ is the bias vector. The visual representation $Z_v$, with a dimension of 7680, is projected down to a lower-dimensional feature space F with a dimension of 4096. This dimension reduction is done using the learned weight matrix $W_p$ and bias vector $b_p$. The target dimension of 4096 is carefully chosen to match the hidden dimension of the LLaMA 3.1 language model. This dimensional alignment is crucial because it ensures compatibility for the subsequent cross-attention operations within the language model, facilitating accurate and seamless information integration. This linear transformation effectively maps the visual features into the same semantic space as the language model's text embeddings, creating a shared space where visual and textual information can interact meaningfully. This contributes to robust and accurate ECG interpretation. This facilitates robust and coherent multimodal understanding and interaction, ultimately leading to the generation of relevant outputs with high precision. The resulting multimodal representation F is then used by the cross-attention layers within the language model. These layers use F to condition text generation on the visual content of the ECG image. This ensures that both text and visual features are not just present, but are aligned in a common semantic space, enabling robust and coherent multimodal reasoning and generation of better outputs with the desired accuracy."}, {"title": "IV. FINE-TUNING METHODOLOGY", "content": "A. Dataset: ECGInstruct\nTo effectively train and thoroughly evaluate the performance and accuracy of our multimodal LLaMA 3.2 model for ECG image interpretation, we leveraged a large dataset specifically for instruction-following tasks using ECG images, ECGInstruct. This dataset contains 1 Million instruction-following samples, a large amount designed to ensure robust training and achieve high generalization accuracy. ECGInstruct includes both carefully synthesized ECG images, generated from raw ECG signal data, and real-world ECG images, reflecting the variety of ECG data seen in clinical practice. This ensures the model's reliability in real-world situations. Each image in the dataset is paired with carefully written question-and-answer pairs, created and annotated by experienced medical professionals specializing in cardiology. This expert annotation is critical for ensuring the clinical accuracy and relevance of the dataset and for training the model to achieve expert-level performance. The curation of ECGInstruct was a multi-stage process, with key steps to ensure its clinical utility, and effectiveness in training a high-accuracy ECG interpretation model:\n1) Image Synthesis from Raw Signals: A large part of ECGInstruct [1] uses real ECG signal data from several public datasets. These include MIMIC-IV ECG [27] and PTB-XL [33], both well-regarded sources of clinical ECG data known for their diversity. These raw ECG signals are transformed into visual ECG images using advanced signal processing and rendering techniques."}, {"title": null, "content": "This synthesis process is essential for several reasons. First, it allows the dataset much larger than what's available in image-only ECG datasets, providing enough data to train a high-capacity model to achieve greater accuracy. Second, it introduces valuable diversity in ECG signal characteristics, reflecting the wide range of cardiac conditions and patient differences, which improves the model's ability to generalize. Third, it allows the quality control and characteristics of the generated ECG images, ensuring a consistent and better training dataset.\n2) Diverse Data Sources: To maximize the dataset's representativeness, generalizability, and its capacity to train a high-accuracy model, ECGInstruct, the dataset authors includes data from various public ECG datasets. Beyond PTB-XL and MIMIC-IV ECG, it also includes data from CODE-15\n3) Realistic Data Augmentation: To further improve the model's robustness, its ability to generalize to real clinical settings, and to improve its accuracy when dealing with common ECG image imperfections, the authors from ECGInstruct added various data augmentation techniques. These augmentations are carefully designed to realistically simulate common visual artifacts and variations often seen in clinical ECG images. These are not just random image manipulations; they are targeted augmentations that mimic real-world imperfections and challenge the model to maintain good precision even with noise and distortions.\n\u2022 Paper Creases and Folds: Simulating the visual distortions that can be caused by creases and folds in printed ECG reports. This is important because many older ECGs are only available as scanned images of printed reports, which often have imperfections. The model needs to be robust to these distortions to maintain accuracy.\n\u2022 Noise Injection: Adding different types of noise (e.g., Gaussian noise, salt-and-pepper noise) to mimic signal degradation or imperfections in image acquisition and digitization. Real-world ECG images are often not perfectly clean and may contain noise from various sources. The model must be resilient to noise to ensure accurate interpretation.\n\u2022 Varied Layouts and Orientations: Introducing variations in the layout of the ECG grid, lead placement, and image orientation to simulate the diversity of ECG report formats seen in clinical practice. ECG reports are not always presented in a perfectly standard format; variations in layout and orientation are common. The model should be unaffected by these variations to ensure consistent and accurate performance. We adapted common image augmentation techniques, similar to those described in [30], specifically for ECG images. This ensures that the augmentations are relevant and preserve the essential ECG information while challenging the model to achieve good results in diverse and"}, {"title": null, "content": "imperfect conditions.\nThis comprehensive and careful dataset curation process ensures that ECGInstruct is a diverse dataset. It's specifically designed to facilitate effective fine-tuning of multimodal models for robust and generalizable ECG image interpretation in real-world clinical settings.\nB. Parameter-Efficient LoRA Fine-Tuning\nOur fine-tuning approach uses parameter-efficient Low-Rank Adaptation (LoRA) [10] to efficiently adapt the multimodal LLaMA 3.2 model for high-accuracy ECG image interpretation. LoRA focuses on efficient adaptation by updating only a small set of newly introduced parameters while keeping the original large model parameters frozen. This method is particularly effective for large models, enabling efficient training without sacrificing performance. We apply LoRA to adapt LLaMA 3.2 for ECG analysis, excluding the 'lm_head' and 'embed_tokens' layers from the adaptation process to maintain generation quality and token embedding stability.\nThe parameter-efficient LoRA fine-tuning method works as follows:\n1) The llama 3.2 vision model is integrated with pre-trained Llama 3.1 language model, so the base LLM is based on LlaMA 3.1 architecture. The combined vision adapter model and LLM model in this paper is mentioned as LLaMA 3.2. For specific weight matrices W within the LLaMA 3.2 model (excluding 'lm_head' and 'embed_tokens'), we introduce a low-rank update AW. This update is not directly learned as a dense matrix, but is decomposed into the product of two smaller matrices, B and A, such that AW = BA. The matrices B and A are designed to have a significantly lower rank r compared to the original weight matrix W. This low-rank decomposition is the core of LoRA's efficiency and stability, contributing to a more robust and accurate fine-tuning process.\n$W' = W + \\Delta W = W + BA$ (4)\nHere, W' represents the effective weight matrix used during fine-tuning, W is the original pre-trained weight matrix from LLaMA 3.2 (which remains frozen), and AW = BA is the low-rank update we are learning. Only the parameters in matrices B and A are trained, while W stays fixed.\n2) A key advantage of LoRA is its parameter efficiency. By only training the low-rank matrices B and A, the number of trainable parameters is significantly reduced compared to full fine-tuning. This makes the fine-tuning process much more computationally efficient and reduces memory requirements, which is crucial when working with large models like LLaMA 3 series. The original weights W of the pre-trained LLaMA 3 series models are kept frozen during the fine-tuning process. This ensures that the model retains its general language understanding capabilities and avoids catastrophic forgetting, while still learning to effectively interpret ECG images."}, {"title": null, "content": "3) We specifically exclude the 'lm_head' (language model\nhead, responsible for token prediction) and 'em- bed_tokens' (token embedding layer) from LoRA adaptation. This exclusion is a deliberate choice to preserve the pre-trained language generation capabilities of LLaMA 3.2 and the stability of its token embeddings. Adapting these layers can sometimes lead to instability in language generation or degradation of the model's general language understanding. By focusing LoRA on other weight matrices within the model, we ensure that the fine-tuning process primarily adapts the model for ECG-specific tasks while maintaining its core language abilities.\nThis parameter-efficient strategy effectively adapts the model to the new ECG image interpretation task by learning task-specific low-rank updates, while preserving the valuable pre-trained knowledge embedded in LLaMA 3.2. This approach also helps to mitigate the risk of overfitting, which can be a concern when fine-tuning large models, and it maintains a balance between adaptation and generalization. This balance is particularly beneficial for complex tasks like medical image interpretation, where both nuanced feature extraction and robust generalization to new clinical cases are essential for achieving high diagnostic accuracy.\nC. Implementation Details\nThe fine-tuning process for our multimodal LLaMA 3.2 model was done using a distributed training setup for efficient and scalable training to achieve better results. We used a deepspeed training script [24], leveraging ZeRO-2 optimization [23]. ZeRO-2 is a powerful optimization technique designed for training very large models like LLaMA 3.2 to achieve high performance. It enables efficient parallel training across multiple GPUs by intelligently dividing model states (parameters, gradients, and optimizer states) across the devices. This significantly reduces the memory needed on each GPU, making it possible to train models that would otherwise be too large to fit into GPU memory. ZeRO-2 was crucial for training LLaMA 3.2, a large model, on our GPU cluster and for achieving the desired level of accuracy. The specific hyperparameter values we used during fine-tuning were carefully chosen and optimized for performance and accuracy on ECG interpretation. These key implementation details are outlined below:\n\u2022 LORA Configuration: For our parameter-efficient LoRA fine-tuning, we carefully configured the LoRA parameters to balance adaptation and stability while maximizing accuracy. We used a LoRA rank (r) of 64, a LoRA alpha (a) of 128, and a LoRA dropout rate of 0.05 [10]. The LORA rank of 64 determines the dimension of the low-rank matrices B and A, effectively controlling the capacity of the adaptation \u2013 a higher rank allows for more complex adaptations and potentially higher accuracy. The LORA alpha of 128 is a scaling factor applied to the low-rank updates; it influences the magnitude of the LORA adjustments and was tuned for optimal performance. A dropout rate of 0.05 was applied to the LoRA updates"}, {"title": null, "content": "\u2022 as a regularization technique. Dropout randomly sets a fraction of the LoRA update values to zero during training, which helps to further enhance regularization and prevent overfitting, improving the model's generalization to unseen ECG images and contributing to robust accuracy.\n\u2022 Training Hyperparameters: The training process was set up with a learning rate of 2e-4. This learning rate was empirically determined to be optimal for convergence and accuracy on the ECGInstruct dataset after testing different learning rates. We used a cosine learning rate scheduler [18] for a total of 3 training epochs. The cosine scheduler is a learning rate decay strategy that gradually reduces the learning rate following a cosine function over the training epochs. This gradual decay promotes smoother convergence during training and often leads to better generalization performance and higher accuracy compared to step-wise decay schedules. We used a batch size of 4 per GPU, and implemented gradient accumulation for 1 step. Gradient accumulation is a technique that effectively increases the batch size without increasing GPU memory requirements. By accumulating gradients over multiple mini-batches before performing a weight update, it simulates training with a larger batch size, which can improve training stability, convergence speed, and overall performance and accuracy.\n\u2022 Hardware and Training Duration: The fine-tuning experiments were done on a cluster of 4 NVIDIA A100 GPUs. Each A100 GPU has 80 GB of VRAM, providing substantial memory capacity for training large models like LLaMA 3.2 and achieving better accuracy. The total training time for the parameter-efficient LoRA fine-tuning process was approximately 2880 GPU hours. This represents the total computational resources used for fine-tuning and achieving the reported performance.\n\u2022 Optimization Techniques: To further optimize memory usage and speed up training without compromising accuracy, we used several advanced optimization techniques. We enabled gradient checkpointing [4]. Gradient checkpointing is a memory optimization technique that significantly reduces memory use during backpropagation. It does this by selectively discarding intermediate activations during the forward pass and then recomputing them on-the-fly during the backward pass. This trade-off between computation and memory is crucial for training very deep models on limited GPU memory and allows for training larger, more accurate models. Additionally, we enabled bfloat16 mixed-precision training to accelerate the training process. Bfloat16 is a reduced-precision floating-point format (16-bit) that offers significant speedups in computation, especially on modern GPUs like the A100, which are optimized for bfloat16 operations [12]. While using lower precision, bfloat16 maintains acceptable numerical stability for most deep learning tasks, making it an effective way to speed up training without losing model accuracy."}, {"title": "V. RESULTS AND DISCUSSION", "content": "A. Evaluation Metrics\nTo thoroughly assess the performance, capabilities of our fine-tuned model, we used a comprehensive and varied set of evaluation metrics. These metrics were carefully chosen to capture different important aspects of ECG image interpretation, ensuring a complete and detailed understanding of the model's strengths and weaknesses across various ECG analysis tasks. This provides a thorough evaluation of its diagnostic accuracy. We aimed to evaluate not just general accuracy, but also clinical relevance and practical utility, ensuring the metrics reflected real-world clinical needs.\n1) Abnormality Detection Performance: For evaluating the model's basic ability to accurately detect and classify different cardiac abnormalities from ECG images, which is most important for clinical use, we used a suite of standard classification metrics:\n\u2022 Area Under the Curve (AUC): AUC is a widely recognized and reliable metric for evaluating the performance of binary and multi-class classification models [7], especially in medical diagnosis. It measures the model's overall ability to discriminate - its ability to distinguish between different classes, such as ECGs with and without specific cardiac abnormalities. AUC is calculated as the area under the Receiver Operating Characteristic (ROC) curve. The ROC curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) at different classification thresholds. An AUC score ranges from 0 to 1, with a higher AUC indicating better discrimination ability. An AUC of 0.5 means performance is no better than random chance, while an AUC of 1.0 represents perfect classification. In ECG interpretation, a high AUC is crucial as it reflects the model's ability to accurately differentiate between normal and abnormal ECGs, a fundamental requirement for a valid diagnostic tool.\n\u2022 Macro F1 Score: The Macro F1 score is used to evaluate the balanced accuracy of the model in multi-class classification scenarios. It is calculated as the average of F1 scores for each class, without weighting. The F1 score itself is the harmonic mean of precision and recall, providing a balanced measure of a model's accuracy, especially when dealing with datasets where some classes are more frequent than others (imbalanced datasets). Macro F1 is particularly valuable in medical diagnosis, where class imbalances are common (some conditions are rarer than others). By averaging the F1 scores across all classes, Macro"}, {"title": null, "content": "F1 gives equal importance to each class, regardless of how often it appears in the dataset. This ensures that the model's performance on less frequent, but potentially critical, abnormalities is properly evaluated, and that the overall accuracy is not skewed by the more common classes. For ECG interpretation, a high Macro F1 score indicates that the model is not only generally accurate, but also performs well across a diverse range of cardiac conditions, including rare but significant ones.\n\u2022 Hamming Loss: Hamming loss is a metric specifically designed for multi-label classification tasks, which is relevant for ECG analysis because an ECG can often show multiple abnormalities at the same time. Hamming loss quantifies the fraction of labels that are incorrectly predicted. For each sample, it compares the predicted labels against the true labels and calculates the proportion of mismatches. A Hamming loss of 0 indicates perfect multi-label classification, while a higher Hamming loss means more incorrect label predictions. Therefore, a lower Hamming loss is better, indicating more accurate multi-label prediction performance. In ECG diagnosis, where multiple abnormalities can co-exist, a low Hamming loss is essential to ensure the model accurately identifies all relevant conditions present in the ECG, contributing to a comprehensive and accurate diagnosis.\nTogether, these three metrics provide a robust and multi-faceted evaluation of the model's abnormality detection abilities, focusing on different aspects of accuracy and clinical relevance. AUC assesses overall discrimination, Macro F1 assesses balanced accuracy across classes, and Hamming loss assesses the overall correctness of multi-label predictions. Together, they give a complete picture of the model's accuracy in identifying cardiac abnormalities from ECG images.\n2) Report Generation Quality: Evaluating the quality of generated ECG reports requires more than just standard metrics. We aimed to assess not only the accuracy but also the clinical utility and coherence of the reports produced by our model, ensuring the reports are valuable for clinical practice. To do this, we used a sophisticated evaluation approach using GPT-40 [21], a leading large language model, as an automated evaluator. GPT-40's advanced natural language understanding and generation capabilities make it well-suited for evaluating the quality of text-based outputs like medical reports, assessing aspects beyond simple factual accuracy. The evaluation process was as follows: we provided GPT-40 with the ECG reports generated by our fine-tuned model and baseline model(original llama 3.2 vision model) along with the corresponding ECG images and expert-annotated ground truth reports (gold standard reports created by cardiologists). We then carefully created prompts for GPT-40, instructing it to assess the generated reports"}, {"title": null, "content": "based on several important criteria that are important from a clinical perspective and contribute to the overall clinical accuracy and utility of the reports:\n\u2022 Medical Accuracy: This criterion assesses the factual correctness of the medical information in the generated report. Does the report accurately reflect established cardiology knowledge? Are the reported findings consistent with known ECG patterns for various conditions? GPT-40 was instructed to check if the medical statements in the generated report were medically sound and factually accurate", "Image": "This evaluates how well the generated report matches the visual information directly seen"}]}