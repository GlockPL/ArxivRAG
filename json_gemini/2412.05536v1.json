{"title": "Comprehensive Evaluation of Multimodal AI Models in Medical Imaging Diagnosis: From Data Augmentation to Preference-Based Comparison", "authors": ["Cailian Ruan", "Chengyue Huang", "Yahe Yang"], "abstract": "This study introduces an evaluation framework for multimodal models in medical imaging diagnostics. We developed a pipeline incorporating data preprocessing, model inference, and preference-based evaluation, expanding an initial set of 500 clinical cases to 3,000 through controlled augmentation. Our method combined medical images with clinical observations to generate assessments, using Claude 3.5 Sonnet for independent evaluation against physician-authored diagnoses. The results indicated varying performance across models, with Llama 3.2-90B outperforming human diagnoses in 85.27% of cases. In contrast, specialized vision models like BLIP2 and Llava showed preferences in 41.36% and 46.77% of cases, respectively. This framework highlights the potential of large multimodal models to outperform human diagnostics in certain tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid advancement of deep learning technologies\u2014particularly the innovative application of large multimodal models in medical image analysis\u2014AI-assisted diagnosis is reshaping traditional medical practice. This study introduces a novel evaluation framework to assess the diagnostic capabilities of the latest generation of multimodal AI models in interpreting complex abdominal CT images, focusing on cirrhosis and its complications, liver tumors, and multi-system lesions.\nA core challenge in medical imaging AI is accurately interpreting and integrating multi-dimensional clinical information. In our collected clinical data, a comprehensive abdominal CT diagnosis often requires simultaneous evaluation of multiple dimensions: liver parenchymal changes (such as cirrhosis and multiple nodules), vascular system abnormalities (like portal hypertension and portal vein cavernous transformation), secondary changes (including splenomegaly and ascites), and related complications (such as esophageal varices). Traditional computer vision models often struggle with such complex medical scenarios, prompting us to explore the potential of new-generation multimodal AI technology.\nWe developed a systematic evaluation framework to assess various multimodal models' capabilities in medical image interpretation. Our comprehensive pipeline incorporates data preprocessing, standardized model evaluation, and preference-based assessment. Starting with 500 clinical cases, each containing 4 sequential CT images paired with detailed diagnostic reports, we employed controlled augmentation techniques to expand the dataset to 3,000 samples while preserving critical diagnostic features. Our methodology utilizes standardized inputs combining medical images and detailed observations to generate diagnostic assessments, enabling direct comparison between different models and human expertise.\nThe results demonstrate remarkable capabilities across various multimodal AI systems, particularly in general-purpose models. Llama 3.2-90B achieved superior performance in 85.27% of cases compared to human diagnoses, with only 1.39% rated as equivalent. Similar strong performance was observed in other general-purpose models, with GPT-4, GPT-40, and Gemini-1.5 showing AI superiority in 83.08%, 81.72%, and 79.35% of cases respectively. This advantage manifests in their ability to simultaneously evaluate multiple anatomical structures, track disease progression, and integrate clinical information for comprehensive diagnosis. In contrast, specialized vision models BLIP2 and Llava demonstrated more modest results, with AI superiority in 41.36% and 46.77% of cases respectively, highlighting the challenges faced by vision-specific approaches in complex diagnostic scenarios.\nOur evaluation framework employed Claude 3.5 Sonnet as an independent assessor, implementing a three-way preference classification (AI superior, physician superior, or equivalent quality) to systematically compare model-generated and physician-authored diagnoses. This approach provides valuable insights into the current capabilities of different multimodal architectures in medical diagnostics, suggesting that general-purpose large multimodal models may significantly outperform both specialized vision models and human physicians in certain diagnostic tasks.\nThis study not only advances our understanding of AI capabilities in medical diagnosis but also introduces an efficient"}, {"title": "II. RELATED WORK", "content": "The integration of multimodal AI systems in medical\ndiagnostics has demonstrated significant advancements over\ntraditional single-modality approaches. Early work introduced\nframeworks that combined radiology images with electronic\nhealth records (EHRs), enhancing diagnostic accuracy by\nleveraging heterogeneous data sources [1], [2]. The idea\nwas further extended by integrating chest X-ray imaging with\npatient demographic data, which improved the detection of\nacute respiratory failure [3], [4].\nIn liver disease diagnosis, traditional computer vision mod\nels like CNNs have primarily focused on tasks such as tumor\ndetection [5]\u2013[9]. The efficacy of CNNs in detecting diabetic\nretinopathy was demonstrated by Ghosh et al. [10], laying\nthe groundwork for their application in liver imaging analysis.\nHowever, these models are often limited in their ability to\nsynthesize multi-dimensional clinical data, prompting the need\nfor more advanced multimodal approaches.\nRecent advancements in large language models (LLMs)\nsuch as GPT-4 have added new dimensions to multimodal\ndiagnostics. Studies showcased LLMs' capabilities in process-\ning and synthesizing medical language tasks, forming a basis\nfor their integration into multimodal frameworks [11]\u2013[13].\nSimilarly, it was demonstrated that combining imaging data\nand text with multimodal AI yielded superior performance in\nbreast cancer diagnostics [14]."}, {"title": "B. Specialized Vision Models and Evaluation Frameworks", "content": "Vision-specific models, such as BLIP2 [15] and Llava [16],\nhave been widely used for focused medical imaging tasks\n[17]\u2013[19]. While effective in detecting individual patholo-\ngies, these models often underperform in complex, multi-\ndimensional diagnostic scenarios compared to general-purpose\nmultimodal systems. Lee et al. [18] evaluated vision-specific\nmodels for diagnosing chest pathologies, finding them profi-\ncient in single-dimension tasks but limited in handling broader\ndiagnostic contexts.\nThe incorporation of standardized evaluation frameworks\nhas also been pivotal in advancing medical AI systems. The\nuse of independent assessors, such as Claude 3.5 Sonnet in\nthis study, represents a novel approach to comparing AI and\nhuman diagnoses. This aligns with the recommendations by\nCrossnohere et al. [20], who emphasized the critical need for\nstandardized protocols to benchmark AI systems in healthcare.\nBy systematically integrating independent assessments with\npreference-based evaluation, this study builds upon these prior\nworks, providing a structured methodology for evaluating the\ncapabilities of multimodal Al systems in complex medical\nscenarios."}, {"title": "III. METHODOLOGY", "content": "Our preprocessing pipeline consists of three main components: data de-identification, anomaly handling, and data augmentation, specifically designed to process paired CT image sequences and their corresponding diagnostic reports.\nThe de-identification process was implemented to ensure patient privacy while preserving clinically relevant information. For CT images, we developed an automated system to remove burned-in patient identifiers and replace DICOM header information with anonymized identifiers. The corresponding diagnostic reports underwent a similar process where personal identifiers, hospital names, and specific dates were systematically replaced with standardized codes while maintaining the temporal relationships between examinations. This process preserved the diagnostic value of both images and reports while ensuring compliance with privacy regulations.\nAnomaly handling addressed both image and text irregularities. For CT images, we implemented automated detection and correction of common artifacts, including beam hardening, motion artifacts, and metal artifacts. Image quality metrics were established to identify scans with suboptimal contrast enhancement or incomplete anatomical coverage. The text processing pipeline identified and corrected common reporting inconsistencies, standardized medical terminology, and ensured proper formatting of measurements and anatomical descriptions. Cases with severe anomalies that could not be automatically corrected were flagged for expert review.\nData augmentation strategies were carefully designed to maintain the paired relationship between image sequences and reports. For images, spatial transformations included minor rotations (\u00b110\u00b0), translations (within 10% of image boundaries), and subtle elastic deformations (controlled within 5% to preserve anatomical relationships). Intensity-based augmentations comprised contrast adjustments (\u00b110%), brightness variations (\u00b15%), and minimal Gaussian noise injection (\u03c3 = 0.01) to simulate imaging system variations. For the corresponding reports, we employed text augmentation techniques including synonym substitution for anatomical terms and standardized rephrasing of pathological findings. Each augmented case maintained the original format of four sequential CT images paired with one comprehensive diagnostic report, ensuring the preservation of the temporal and spatial relationships within the image series and their corresponding textual descriptions. This process generated ten augmented samples for each original case, with synchronized modifications in both the image sequences and their reports.\nThe effectiveness of our preprocessing pipeline was validated through both automated quality metrics and expert"}, {"title": "B. Workflow Design", "content": "The evaluation pipeline begins with\ncarefully curated CT image sequences, consisting of multiple\ncross-sectional views of the abdominal region. These images\nundergo standardized preprocessing to ensure consistent input\nquality across all models. Each case is paired with a detailed\nimage overview capturing essential anatomical and patholog\nical observations, providing standardized context for both AI\nmodels and human diagnosticians to ensure fair comparison.\nMulti-model Analysis We evaluate various state-of-the-art\nmultimodal models for their medical diagnostic capabilities.\nThe input for each model consists of paired CT image se-\nquences and corresponding text descriptions, where the text\nprovides detailed anatomical and pathological observations\nvisible in the images. The general-purpose multimodal models\n(Llama 3.2-90B, GPT-4, and GPT-40) process the combined\nimage-text pairs to leverage both visual features and contextual\ninformation in generating diagnostic assessments. Similarly,\nspecialized vision models (BLIP2 and Llava) analyze the CT\nimage sequences while incorporating the textual descriptions\nto provide comprehensive diagnostic interpretations. Each\nmodel generates independent diagnostic reports based on the\nsame standardized input, enabling direct comparison of their\ncapabilities in integrating visual and textual information for\nmedical diagnosis.\nEach model generates structured\ndiagnostic outputs encompassing primary findings, secondary\nobservations, and clinical recommendations. The standardized\noutput format allows direct comparison of diagnostic compre-\nhensiveness and accuracy across different models and human\nexperts. This structured approach ensures consistent evaluation\ncriteria while maintaining the unique analytical capabilities of\neach model.\nWe implement an innovative\npreference-based evaluation approach using Claude 3.5 Sonnet\nas an independent assessor. Through carefully crafted prompt-\ning strategies, we enable automated comparison between AI-\ngenerated and physician-authored diagnoses without requiring\nextensive manual review. The evaluation framework employs\na three-way classification system (AI Superior, Physician\nSuperior, or Equivalent), considering factors such as diag-\nnostic accuracy, comprehensiveness, and clinical relevance.\nThis prompt-based approach significantly reduces the need\nfor human evaluation resources while maintaining objective\nassessment standards.\nThe framework incorporates systematic\nquality monitoring through automated metrics and selective"}, {"title": "IV. EXPERIMENTS & RESULTS", "content": "We conduct comprehensive experiments to evaluate the\ndiagnostic capabilities of different multimodal models in\nmedical image interpretation. The experiments are structured\nin two main aspects: comparative analysis of model perfor-\nmance against human expertise and systematic evaluation of\ndiagnostic accuracy across different pathological conditions.\nOur evaluation framework employs a dataset of CT image\nsequences with corresponding clinical assessments, enabling\ndetailed comparison of AI and human diagnostic capabilities."}, {"title": "A. Experiments Setup", "content": "Our experimental dataset consists of 500 original clinical\ncases, each comprising a sequence of 4 cross-sectional CT\nimages accompanied by a comprehensive diagnostic report.\nThrough our data augmentation pipeline, we expanded this\ndataset to 3,000 samples while preserving the critical diag-\nnostic features and relationships between images and reports.\nThe augmentation process included controlled spatial trans-\nformations of images (\u00b110\u00b0 rotations, within 10% boundary translations), intensity adjustments (\u00b110% contrast, \u00b15%"}, {"title": "B. Experiments Evaluation", "content": "The evaluation of model performance was conducted focusing on quantitative metrics to ensure thorough evaluation of model capabilities. For quantitative assessment, we analyzed the preference-based evaluation results using a standardized scoring system. The three-way classification (AI Superior, Physician Superior, or Equivalent) was applied consistently across all 3,000 cases, with each case receiving independent assessment through Claude 3.5 Sonnet. The evaluation criteria emphasized accurate identification of primary pathologies, recognition of secondary complications, and proper integration of multi-dimensional clinical information. Statistical analysis of the results employed chi-square tests to determine significance in performance differences between models, with p-values adjusted for multiple comparisons using the Bonferroni correction. Additionally, we calculated confidence intervals for preference ratios to ensure robust interpretation of model performance differences."}, {"title": "C. Results and Discussion", "content": "Our experimental evaluation reveals significant variations in diagnostic capabilities across different multimodal AI architectures, as detailed in Table I. The results demonstrate a clear performance distinction between general-purpose models and specialized vision models in medical diagnostic tasks. General-purpose models consistently demonstrated superior performance, with Llama 3.2-90B achieving the highest preference rate of 85.27% over human diagnoses. This exceptional performance was particularly evident in complex cases involving multiple pathologies and cross-system interactions. The other general-purpose models (GPT-4, GPT-40, and Gemini-1.5) showed similarly strong results, all achieving preference rates above 79%. The consistently low equivalence rates (around 1.39% for most models) suggest clear differentiation in diagnostic capabilities rather than ambiguous comparisons. In contrast, specialized vision models BLIP2 and Llava demonstrated more modest performance levels, with preference rates of 41.36% and 46.77% respectively. The higher physician superiority rates for these models (53.25% and 48.84%) indicate that while they possess competence in specific pathology detection, they may struggle with comprehensive diagnostic assessment requiring integration of multiple clinical indicators. Statistical analysis confirms the significance of these performance differences (p < 0.001 for general-purpose models), suggesting that this superiority is not due to chance. The performance gap was most pronounced in cases requiring integration of multiple anatomical observations and clinical findings, where general-purpose models exhibited superior capability in synthesizing complex clinical information. Several factors may contribute to the superior performance of general-purpose models. First, their architecture enables better integration of visual and textual information, allowing for more comprehensive interpretation of clinical data. Second, their broader training potentially enables better understanding of complex medical relationships and dependencies. Third, their ability to process and synthesize multiple types of information simultaneously appears to more closely mirror the cognitive processes involved in medical diagnosis. These findings suggest important implications for"}, {"title": "V. CONCLUSION", "content": "This study introduces a novel evaluation framework for assessing multimodal AI models in medical imaging diagnosis, showcasing the significant potential of general-purpose models in handling complex diagnostic tasks. The superior performance of models such as Llama 3.2-90B and GPT-4 highlights a paradigm shift in medical imaging, where Al systems can surpass human experts in certain diagnostic scenarios.\nOur findings demonstrate that general-purpose multimodal models exhibit remarkable capabilities in synthesizing complex medical information, achieving preference rates exceeding 80% compared to human diagnoses. This exceptional performance is particularly evident in cases requiring the integration of multiple clinical indicators and cross-system analyses.\nHowever, these results should be considered within the context of certain limitations. While our evaluation framework provides a reliable methodology, future studies should explore its applicability across diverse clinical contexts and varied healthcare settings. Moreover, human expertise remains indispensable for complex decision-making and patient care.\nThe implications of this research extend beyond performance metrics. For clinical decision support systems, the proposed framework enhances clinical decision-making by integrating AI diagnostics with human expertise. Regarding quality assurance in diagnostic processes, it establishes a reliable methodology to improve consistency and reduce diagnostic errors. For medical education and training, the framework offers a valuable tool for educating healthcare professionals through AI-assisted diagnostics. Finally, this research contributes to the standardization of diagnostic procedures, aiding the development of uniform processes for integrating AI into clinical workflows.\nFuture research should expand the evaluation framework to include other medical imaging modalities and explore its integration into clinical workflows. Furthermore, developing hybrid approaches that combine the strengths of AI and human expertise will be essential for maximizing the potential of these technologies.\nThe proposed workflow for comparing human and AI-generated diagnostic results can also be integrated into clinical decision-making processes. This system is designed to process diagnoses from both human physicians and AI models, with a dedicated committee making the final diagnosis and treatment plan. Such a design leverages the strengths of both human expertise and AI, while the committee ensures the accuracy and reliability of the proposed treatments. By incorporating this integrated information system, medical diagnostics could achieve enhanced accuracy and efficiency, ultimately improving patient care outcomes.\nThis study significantly advances our understanding of AI capabilities in medical diagnosis and establishes a robust framework for evaluating future developments in this rapidly evolving field. The findings underscore the promising potential of multimodal AI systems to enhance diagnostic accuracy and efficiency through careful integration into clinical practice."}]}