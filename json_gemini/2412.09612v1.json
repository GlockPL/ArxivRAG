{"title": "Olympus: A Universal Task Router for Computer Vision Tasks", "authors": ["Yuanze Lin", "Yunsheng Li", "Dongdong Chen", "Weijian Xu", "Ronald Clark", "Philip H. S. Torr"], "abstract": "We introduce Olympus, a new approach that transforms Multimodal Large Language Models (MLLMs) into a unified framework capable of handling a wide array of computer vision tasks. Utilizing a controller MLLM, Olympus delegates over 20 specialized tasks across images, videos, and 3D objects to dedicated modules. This instruction-based routing enables complex workflows through chained actions without the need for training heavy generative models. Olympus easily integrates with existing MLLMs, expanding their capabilities with comparable performance. Experimental results demonstrate that Olympus achieves an average routing accuracy of 94.75% across 20 tasks and precision of 91.82% in chained action scenarios, showcasing its effectiveness as a universal task router that can solve a diverse range of computer vision tasks.", "sections": [{"title": "1. Introduction", "content": "\"If I have seen further it is by standing on the shoulders of Giants.\" - Isaac Newton\n\nMultimodal Large Language Models (MLLMs) have made significant strides in advancing understanding, generation and reasoning across diverse domains. For example, in understanding, MLLMs like LLaVA [47] excel in visual question-answering (VQA) [4], effectively integrating visual and textual data. In generation, diffusion models [26, 56, 58] have achieved exceptional results in text-to-image, text-to-video and text-to-3D generation [9, 19, 27, 42, 43, 58, 60, 64, 77].\n\nBuilding on these advancements, recent studies [17, 21, 41, 70, 78, 81, 87] have aimed to develop unified architectures capable of performing tasks across various domains. Notably, Emu3 [74] and Omni-Gen [79] introduce all-in-one models designed to handle both generation and understanding tasks. However, the integration of distinct domains within a single model continues to present significant challenges. In particular, variability within domains often leads to compromised performance on individual tasks due to conflicts between task objectives, such as those between text and image generation tasks [96]. These conflicts hinder the models' effectiveness and limit their utility in real-world applications.\n\nAnother key limitation of all-in-one models lies in their constrained ability to handle a broad spectrum of vision-language tasks across different domains due to differing input and output formats. This restriction presents a substantial bottleneck to scalability, particularly as the range of tasks continues to grow across images, videos, and the emerging 3D domain. Furthermore, extending these models to accommodate new tasks is inherently challenging. Training such comprehensive models with increasing model sizes demands substantial computational resources, and highly complex training methodologies. For instance, Omni-Gen [79] necessitates 104\u00d7A800 GPUs and five distinct training stages. These issues underscore the pressing need for modular or task-adaptive frameworks to enhance scalability and efficiency in addressing the increasingly diverse demands of vision tasks. Additionally, all-in-one models often struggle to integrate meticulously designed, task-specific components effectively, reducing their overall efficiency and performance in specialized applications.\n\nThis prompts us to explore another alternative approach for seamlessly unifying vision tasks within a single framework. Inspired by HuggingGPT [62] and the advanced contextual understanding of MLLMs, we propose leveraging MLLMs to handle vision-language comprehension internally while delegating other tasks externally. Although technically straightforward, it represents a foundational and significant step toward advancing unified frameworks for computer vision tasks. Specifically, the MLLM can function as a task router, coordinating with specialized external models to address various tasks and overcome individual model limitations. However, it still faces significant challenges, primarily due to the variability in user prompts across a wide range of tasks and the lack of comprehensive, task-specific instruction datasets essential for effective training and evaluation.\n\nIn this paper, we introduce Olympus, a unified framework that leverages MLLMs to handle a diverse array of computer vision tasks. Our Olympus differs from existing methods [71, 74, 81, 96] which focus on presenting all-in-one models to solve diverse tasks. To accomplish this, we collected 446.3K high-quality training instructions and 49.6K evaluation instructions from GPT-40 [29] named as OlympusInstruct and OlympusBench respectively, spanning 20 different vision tasks. Furthermore, we designed specific routing tokens tailored to delegate individual tasks. Finally, leveraging these routing tokens and OlympusInstruct, our model can even perform a chain of tasks within a single user instruction if needed.\n\nIn our experiments, Olympus achieves comparable performance to the leading MLLMs on standard multimodal benchmarks [48]. Additionally, it supports over 20 distinct tasks across the domains of image, video, and 3D, as shown in Figure 1. We further investigate the effectiveness of decomposing user instructions to interface with suitable external models, Olympus achieves an impressive average routing accuracy of 94.75% across 20 individual tasks. In chain-of-action scenarios, which involves performing multiple tasks to complete an instruction, our model attains 91.82% precision. These results highlight the potential of Olympus. In summary, our contributions can be included as:\n\n\u2022 We introduce Olympus, an innovative framework that leverages Multimodal Large Language Models (MLLMs) to perform contextual understanding tasks through their inherent capabilities, while addressing other tasks via allocating external models.\n\u2022 We develop task-specific routing tokens and enhance MLLMs with chain-of-action capabilities. Our model achieves comparable performances with leading MLLMs on multimodal benchmarks, Olympus achieves 94.75% routing accuracy in single-task scenarios, 91.82% precision in chain-of-action settings, and solves up to 5 tasks within a single instruction.\n\u2022 We have curated high-quality instruction datasets named OlympusInstruct and OlympusBench across 20 computer vision tasks, comprising 446.3K and 49.6K samples for training and evaluation respectively. These datasets provide a solid foundation for further exploration and advancement in this domain."}, {"title": "2. Related Work", "content": "2.1. Vision-Language Understanding\n\nRecent advancements in large language models (LLMs) [7, 73] have catalyzed the development of multimodal large language models (MLLMs) [2, 3, 5, 11, 16, 34, 35, 37, 44\u201346, 48, 54, 57, 63, 72, 82]. Pioneering multimodal large language models (MLLMs), such as MiniGPT-4 [97], have demonstrated impressive capabilities in processing and integrating multiple modalities. Models like Kosmos-2 [57], LLaVA [48] and LLaVA-OneVision [34] have further enhanced the visual cognitive abilities of MLLMs. Additionally, approaches including LLaVA-Phi [100], MobileVLM [12], and Mipha [99] focus on refining training methodologies and architectural frameworks to develop more efficient and lightweight MLLMs. Although these models excel in visual perception and multimodal understanding, they are predominantly limited to generating text-based outputs, which restricts their effectiveness across a broader range of vision tasks involving images, videos, and 3D content generation. In this work, we adopt a multimodal model structure following Mipha [99].\n\n2.2. Unified Vision-Language Foundation Model\n\nExtensive research [1, 17, 21, 66, 70, 74, 78, 81, 84, 89, 93, 96] has focused on developing unified multimodal language models proficient in both understanding and generating content. Approaches such as [21, 98] integrate continuous embeddings with textual tokens within autoregressive frameworks for image generation. Emu2 [66] combines CLIP ViT [18] image embeddings with text tokens for autoregressive modeling, while Chameleon [71] employs a transformer across diverse modalities with autogressive modeling. Show-o [81] and TransFusion [96] incorporate autoregressive and diffusion modeling within a single transformer. Omni-Gen [79] utilizes a VAE [32] encoder-decoder alongside a transformer to process free-form prompts. Recently, Emu3 [74] trained a unified transformer with next-token prediction across video, image, and text datasets, achieving superior performance on multimodal benchmarks and generation tasks. However, current unified multimodal foundation models predominantly support a narrow range of generative tasks, such as image and video creation or editing, and face significant scalability challenges for broader AI applications. Additionally, their training requires substantial computational resources. To overcome these limitations, we further enhance MLLMs by enabling the seamless integration of domain-specialized models tailored for diverse applications.\n\n2.3. LLM-Based Tools\n\nLarge language models (LLMs) [73], trained on extensive datasets, demonstrate exceptional proficiency in zero-shot and few-shot settings, as well as in complex tasks such as mathematical problem-solving and commonsense reasoning. To extend their capabilities beyond text generation, recent research [38, 40, 59, 61, 62, 62, 68, 75] has focused on integrating external tools and models into LLM architectures. Toolformer [61] pioneered this approach by embedding API calls within textual sequences, thereby enabling LLMs to utilize external tools effectively. Building upon this foundation, subsequent studies have incorporated visual modalities: Visual ChatGPT [75] integrates LLMs with visual models such as BLIP [35], Visual Programming [23] and ViperGPT [68] translate visual queries into executable Python code, facilitating the processing of visual data by LLMs. Additionally, HuggingGPT [62] enhances large language models (LLMs) by utilizing them as controllers that direct user requests to specialized expert models, thereby integrating language comprehension with domain-specific expertise.\n\nAlthough HuggingGPT assigns specific AI models to perform various tasks, it primarily relies on prompt engineering to leverage ChatGPT as an interface for connecting diverse external models without training. In contrast, our approach involves training multimodal large language models (MLLMs) from the ground up, enabling them to internally handle vision-language understanding tasks while designate specialized models to address a wide range of AI tasks."}, {"title": "3. Olympus", "content": "Olympus leverages MLLMs as the foundation for various computer vision tasks. For vision-language tasks like visual question answering (VQA), MLLMs utilize their inherent capabilities. For other vision tasks, such as generative tasks (e.g., image, video, and 3D generation) and classic vision tasks (e.g., image super-resolution and depth estimation), MLLMs act as intermediaries, routing user instructions to specialized models. As shown in Figure 2, upon receiving a request, the MLLM can autonomously orchestrate the workflow, coordinating expert models to achieve the objective. The following subsections outline the details of Olympus.\n\n3.1. Instruction Dataset Collection\n\nIn order to accurately assign user instructions to the appropriate model, we constructed a high-quality and diverse dataset of user instruction-response pairs using GPT-40. This dataset comprises 446.3K training samples, designated as OlympusInstruct, and 49.6K evaluation samples, designated as OlympusBench, encompassing 20 distinct tasks. For each task, a specialized prompt was developed to align with the specific context of the task. This involved crafting detailed directives that enable GPT-40 to generate coherent and contextually relevant user requests and responses. An example of the image editing prompt used by GPT-40 to collect user instruction-response pairs is provided in Figure 4.\n\nTo ensure diversity in user instructions, we incorporated various prefixes and phrases that introduce different language styles, tones, and structures. Additionally, we categorized instruction complexities into three levels: short, moderate, and extended. This stratification allows GPT-40 to produce instructions that vary in length and complexity. Furthermore, we prompted GPT-40 to generate responses that are both practical and direct, thereby enhancing the applicability and clarity of the interactions. We also performed thorough data cleaning by removing duplicate entries and utilizing GPT-40 to remove entries that were contextually or grammatically inappropriate. This purification process ensures the integrity and quality of the dataset. Figure 4 presents examples for image editing task, demonstrating how the task-specific prompts enable GPT-40 to generate instructions with diverse levels of complexity and varied language styles.\n\nFigure 5 illustrates the statistical characteristics of our training and evaluation datasets. Specifically, the training set comprises 381.5K single-task instruction-response pairs and 64.8K chain-of-action instruction-response pairs. Similarly, the evaluation set consists of 49.6K single-task pairs and 7.2K chain-of-action pairs. The maximum word length across all instructions is 372 words, with an average instruction length of 20.2 words. Responses have an average length of 10.7 words. The 20 covered tasks are categorized into three groups: (1) Image domains: image generation/editing, deblurring, deraining, super-resolution, denoising, pose/normal/canny/depth estimation, controllable image generation across six conditions (canny, pose, segmentation, depth, normal, scribble), object detection/segmentation, and visual grounding; (2) Video domains: video generation/editing, controllable video generation across the same six conditions, and referring video object segmentation; and (3) 3D domains: image-to-3D and text-to-3D generation.\n\n3.2. Task-Specific Routing Tokens\n\nAs shown in Figure 3, Olympus directs user requests to dedicated models using task-specific routing tokens (e.g., image editing). To facilitate MLLMs in predicting appropriate models aligned with users' goals, we design a set of routing tokens specific to individual tasks. For instance, in the domains of image and video generation, we use the routing tokens <image_gen></image_gen> and <video_gen>\u2026\u2026</video_gen>, respectively. Given a user instruction such as \"Please craft an image displaying a chihuahua dog dressed in a vibrant, multicolored costume.\", the corresponding response can be <image_gen>a chihuahua dog dressed in a vibrant, multicolored costume.</image_gen>, which is designed to effectively address the user's request. Thus, user instructions and the responses form input-answer pairs for training. Detailed information on designed routing tokens and corresponding specialist models are explained in the Appendix.\n\nChain-of-Action. By introducing domain-specific routing tokens, Olympus enables chain-of-action capabilities, allowing to handle multiple tasks within a single instruction. For instance, consider a user prompt combining pose-based image generation and image editing: \"In homage to the pose imagery and the prompt 'a majestic castle', generate an image. In the following step, please refine the image by adding green trees.\" The predicted response using the routing tokens is: <pose_to_image>a majestic castle</pose_to_image><image_edit>adding green trees</image_edit>. Therefore, Olympus can sequentially route user instructions to the appropriate modules for pose-conditioned image generation and image editing, in alignment with the task-oriented routing tokens. Moreover, Olympus supports up to five consecutive tasks within a single prompt and is capable of scaling to accommodate an even larger number of tasks, thereby demonstrating its flexibility and scalability.\n\n3.3. Training\n\nSince the goal is to generate task-specific response together with its routing tokens conditioning on the user instructions, we can train MLLMs with next-token prediction paradigm using the cross-entropy loss:\n\n$L$\n\n$P(Y_a|F_v, F_t) = \\prod_{i=1}^{l} P_\\theta(y_i|F_v, F_t, Y_{a,<i}).$\n\nHere, L represents the sequence length of the response Ya, Fv denotes the visual embedding which is adopted for those multimodal instructions, and 0 means the trainable parameters of MLLMs. The notation Ya,<i denotes all tokens preceding the current token yi, and Ft represents the input instruction embeddings.\n\n3.4. Inference\n\nAs displayed in Figure 3, upon receiving a prompt, Olympus generates a response with task-customized routing tokens. These tokens invoke the appropriate AI models to handle various tasks, and their predictions are aggregated into a final response. For tasks solvable by MLLMs alone, responses are generated directly, bypassing routing tokens."}, {"title": "4. Experiment", "content": "4.1. Experimental Setup\n\nMLLM Model. Our model follows the setting of Mipha [99] with its vision and language encoders, i.e., SigLIP-384 [92] and Phi-2 [55]. For the multimodal projector, same as LLaVA [48] and Mipha [99], we adopt a two-layer MLP.\n\nTraining Setting. We initialize the weights from Mipha-3B [99] and fine-tune the model on the LLaVA-Mix665K dataset [48] and OlympusInstruct for 2 epochs, using a learning rate of 5e-5 and a batch size of 256 on 64 V100 32GB GPUs. The whole training process takes approximately 24.8 hours. All model components, including the vision encoder, language encoder, and MLP, are fully fine-tuned during the training process.\n\nEvaluation Details. We compare our method with a bunch of state-of-the-art multimodal large language models (MLLMs) across 11 popular benchmarks, as shown"}, {"title": "4.2. Quantitative Evaluation", "content": "Task Routing Performance. To demonstrate our routing effectiveness, we compare our results with HuggingGPT on OlympusBench using GPT-40 mini and GPT-40 models for their strong predictive capabilities in Table 2 and 3. For a fair comparison, we included prompts covering all task types supported by Olympus and excluded prompts for irrelevant tasks. In Table 2, under the single-task setting, our method achieves notable improvements of 13.4%, 10.26%, 13.2%, and 12.21% in accuracy, precision, recall, and F1 score, even against the strong GPT-40 model. In the chain-of-action setting, Olympus demonstrates further gains of 0.17, 16.79%, 32.52%, and 30.73% for edit distance, precision, recall, and F1 score, respectively.\n\nAdditionally, we collected 200 diverse human-generated instructions to evaluate Olympus's real-life performance against HuggingGPT, using success rate as the metric. The success rate reflects whether specialized models generate outputs that fully satisfy user requests, requiring both accurate task planning and effective task-tailored prompt generation.\n\nAs shown in Table 4, Olympus outperforms HuggingGPT with an 11.3% higher success rate using GPT-40. These results highlight the significant potentials of Olympus and"}, {"title": "4.3. Ablation Study", "content": "The ablation study exploring the impact of varying the number of training tasks is presented in Tables 5, 6, 7, and illustrated in Figure 7. Table 5 demonstrates that the number of tasks has a limited influence on overall performance across multimodal benchmarks. Notably, the 10-task setting achieves the best results on MME-P [20], while the 20-task setting performs optimally on VizWiz [24]. The 20-task configuration is selected for its robustness and generality across a diverse range of tasks. Tables 6 and 7 illustrate a slight performance degradation in both single-task and chain-of-action settings as the number of tasks increases. This degradation is reasonably attributed to the increased prediction complexity associated with handling a larger number of tasks.\n\nFigure 7 highlights the training cost, which increases from 1286.4 GPU hours without utilizing OlympusInstruct to 1589.5 GPU hours with it, representing a modest 23.6% increase in time. This relatively low cost increase is attributed to the avoidance of training complex generative models. Further experimental details are provided in the Appendix.\n\n4.4. Visualization\n\nFigure 6 illustrates the versatility of Olympus across various tasks. The first two columns present single-turn examples, including visual grounding, depth estimation, controllable image generation, and image super-resolution. The third column illustrates Olympus's proficiency in executing a variety of tasks, such as image editing, visual question answering (VQA), and canny edge detection, within the context of multi-turn conversations. This is particularly noteworthy given that OlympusInstruct does not include any multi-turn conversation data, underscoring our model's impressive capacity for generalization. The final column showcases its chain-of-action capability to conduct text-to-image generation, object segmentation and image-to-3D generation within one instruction. These examples clearly show that Olympus can handle diverse prompts for multiple tasks and generate comprehensive responses for users."}, {"title": "5. Limitation", "content": "Since Olympus is trained on the dataset collected through GPT-40, it still has some limitations, e.g., the quality and diversity of the samples collected directly impact the performance of the generated responses. The inherent biases and inaccuracies in GPT-40's responses propagate into MLLMs, potentially leading to suboptimal or biased outputs."}, {"title": "6. Conclusion", "content": "We present Olympus, a universal task router designed to address diverse computer vision tasks by integrating MLLM's internal abilities with task-specific routing to expert models. To achieve this, we introduce OlympusInstruct and OlympusBench, datasets collected from GPT-40 covering 20 distinct tasks. With the presented routing tokens, Olympus can handle multiple tasks within a single prompt, highlighting its potential as a robust foundation for unifying a wide range of computer vision tasks."}, {"title": "A. Appendix", "content": "In the supplementary materials, we provide the following sections:\n\n\u2022 Training details in Section B.\n\u2022 Task-specific routing tokens in Section C.\n\u2022 Adopted specialist models in Section D.\n\u2022 More dataset statistic in Section E.\n\u2022 Ablation study experiments in Section F.\n\u2022 More results in Section G.\n\nB. Training Details\n\nWe train our models using 64 \u00d7 V100 GPUs, each equipped with 32GB of memory. The Adam optimizer [31] is employed, combined with a cosine learning rate scheduler, aligning with the configuration utilized in LLaVA [48]. For fine-tuning, we set a learning rate of 5e-5, which is optimized for stability and convergence, and adopt a batch size of 256 to accommodate the large-scale data and distributed training setup. The training process spans two epochs over the combined fine-tuning datasets of LLaVA-Instruct-158K [45] and OlympusInstruct, ensuring that the models are effectively exposed to both general-purpose and task-specific instructions.\n\nAdditional training configurations include a warmup ratio of 0.03, which helps in stabilizing the initial training phase, and gradient accumulation steps set to 4 to balance memory efficiency with gradient updates. To handle varying image sizes in the datasets, we employ a padding-based image aspect ratio strategy. Moreover, numerical precision is set to float16, enabling faster computation and reduced memory usage while maintaining sufficient numerical accuracy. These hyperparameters, summarized in Table 8, were meticulously selected to optimize training performance and ensure scalability across diverse tasks.\n\nC. Task-specific Routing Tokens\n\nAs illustrated in Figure 8, we present the task-specific routing tokens for 20 distinct computer vision tasks, spanning image, video, and 3D domains. These routing tokens play a crucial role during the training of MLLMs on OlympusInstruct, acting as explicit indicators to guide task-specific responses. For instance, when handling a text-to-3D generation task, a sample instruction such as: I'd appreciate it if you could design a 3D representation of an ancient library, which is a repository of books and scrolls from ancient times.", "response": "ancient library, a repository of books and scrolls from ancient times.", "as": ". Such augmentation ensures that the model learns to associate specific tasks with their respective routing tokens.\n\nBy incorporating these routing tokens into the training process, the MLLMs are endowed with the ability to predict and append the appropriate tokens based on diverse user instructions during inference. This mechanism enables the invocation of the most relevant specialist models for a given task, such as  for image editing or  for referring video object segmentation. The framework's modularity not only enhances task alignment but also ensures adaptability across evolving domains, facilitating the seamless integration of new tasks and specialist models in the future. This mechanism underscores the scalability and versatility of the Olympus framework in handling complex AI tasks.\n\nD. Adopted Specialist Models\n\nIn Table 9, we present the specialist models selected for 20 distinct computer vision and multimodal tasks, illustrating the flexibility and adaptability of our Olympus framework. Notably, for tasks like canny estimation, we utilize the highly efficient and widely recognized Canny operator from the OpenCV library. Similarly, for other tasks such as image generation, image editing, and text-to-3D generation, we incorporate state-of-the-art models like Stable Diffusion XL [58], InstructPix2Pix [6], and LGM [69], respectively. By leveraging these specialized, task-specific models, Olympus circumvents the need for training excessively large and cumbersome multimodal all-in-one models, instead opting for a modular and scalable approach.\n\nA key strength of the Olympus framework lies in its ability to seamlessly integrate superior models as they become available. For instance, advanced models like Ground-", "equations": ["P(Y_a|F_v, F_t) = \\prod_{i=1}^{l} P_\\theta(y_i|F_v, F_t, Y_{a,<i})"]}]}