{"title": "Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference", "authors": ["Shengyuan Ye", "Jiangsu Du", "Liekang Zeng", "Wenzhong Ou", "Xiaowen Chu", "Yutong Lu", "Xu Chen"], "abstract": "Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users' privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity- aware parallelism planning for fully exploiting the resource potential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of communication and computation to mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments. Extensive evaluation based on prototype implementation demonstrates that Galaxy remarkably outperforms state-of-the-art approaches under var- ious edge environment setups, achieving up to 2.5\u00d7 end-to-end latency reduction.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer-based models [1], [2] have achieved superior performance in the field of Natural Language Processing (NLP) and driven increasing intelligent applications at the network edge. In edge intelligent applications, such as Al assistants in smart homes [3] and voice-controlled robots in smart factories [4], single-shot inference (referring to single- command requests) tasks are prevalent, necessitating efficient and low-latency inference for seamless user interactions. Cur- rently, most Transformer-based intelligent applications heavily depend on cloud services, with the actual inference of large- scale Transformer-based models taking place in the cloud [5], [6]. At the edge, only a proxy daemon is deployed to forward user requests [3]. However, the cloud-assisted approaches suffer from following issues: (1) Quality-of-Service may suffer due to unreliable and delay-prone wide-area network (WAN) connections between edge devices and remote clouds [7]. (2) Inference requests from numerous edge clients can impose significant pressure on both the backbone network and data-centers. (3) The sensory data in smart applications can contain highly sensitive or private information. Transferring these data to the remote cloud owned by commercial companies inevitably raises users' privacy concerns [8].\nTo address that, in-situ inference [9], [10] on edge devices without remote assistance, which keeps data locally and avoids network transmission, has been recognized as a promising paradigm for intelligent applications at the edge. However, the computation-intensive and resource-hungry nature of Trans- former inference presents significant challenges for resource- constrained edge devices [11]. As we will show in \u00a7II-B, inference on the Bert-L model [12] in an off-the-shelf edge device imposes a minimum available memory space of almost 700MB, while taking 121\u00d7 longer latency than that in a datacenter GPU. These results demonstrate the fundamental contradiction between intensive Transformer inference work- load and constrained onboard resources. To tackle these chal- lenges, existing arts explore to design sophisticated scheduling mechanisms to leverage the resource potential of edge devices [9], [13]\u2013[15], but are still bottlenecked by the limited onboard resource of a single device.\nAlternatively, we observe that prevalent edge environments like smart homes usually comprise a rich set of trusted idle devices in physical proximity [10], [16]. This motivates us to regard vicinal available edge devices as a resource aug- mentation and collaborate with them in a distributed manner to render expedited Transformer inference at the edge. As illustrated in Fig. 1, we can utilize the distributed computing resources in a smart home (with tablet, smart speaker, and television) to accelerate the Transformer-based (such as Bert [1] and GPT [12]) voice assistant. Nevertheless, this paradigm brings several key challenges: (1) how to parallelize the"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Current language-related applications trend towards using Transformer-based models, which are composed of stacks of Transformer layers, due to their superior accuracy. The original Transformer formulation [17] comprises both an Encoder and a Decoder. In this paper, we focus on the recent language models like Bert [1] and GPT-2 [12], which use only the Encoder or Decoder components. Fig. 2 shows the model architecture of the Transformer layer we consider in this paper.\nIn a Transformer layer, the primary components are the Multi-head Attention (MHA) block and the Multilayer Per- ception (MLP) block. These components are connected by element-wise operations such as Dropout, Residual Addition, and Layer Norm. In MHA block, the first linear layer generates query (Q), key (K), and value (V) matrices for each attention head. Each head conducts self-attention independently, and their outputs are concatenated and further processed through a final linear layer to obtain the output. MLP block involves two linear operations which increase the hidden size from h to 4h and then reduce it back to h."}, {"title": "B. Transformer Inference on Resource-Limited Edge Devices", "content": "In-situ inference can leverage idle resources in edge environ- ments while fully preserving users' data privacy, making it a widely utilized paradigm in privacy-sensitive edge applications [10], [18]. However, the resource-intensive nature of Trans- former inference presents significant challenges for resource- limited edge devices [19], [20]. We conduct experiments to analyze how limited computation resources affect on-device Transformer inference. The experimental setup is described in \u00a7IV-A, and the results are presented in Table I. Specifically, we perform on-device inference for five typical Transformer- based models on off-the-shelf edge devices and the Nvidia GPU platform using an input sequence of length 30. We observe that the inference latency exhibits a huge gap between A100 and Nano-M, e.g., 121\u00d7 slowdown for Jetson Nano when comparing with A100 on Bert-L. Memory budget is another critical factor in Transformer inference. GPT2-L in"}, {"title": "C. Collaborative Transformer Inference with Multiple Devices", "content": "In collaborative Transformers inference across edge devices, the key question is the choice of parallelism strategy. We illustrate different parallelism plans in Fig. 3.\n1) Data and Pipeline Parallelism: Data Parallelism (DP) and Pipeline Parallelism (PP) are the common way to execute Transformer-based model in parallel [21]\u2013[23]. DP partitions workloads along the sample dimension, allowing each device to perform inferences independently. In edge intelligence services, where single-shot inference requests are frequently raised (e.g., sending a single piece of voice command to a smart assistant), DP is not applicable due to the absence of data batches. PP horizontally partitions the model into consecutive stages along layer dimension, with each stage mapped to a distinct device. However, in the case of single-shot inference, PP still falls short in leveraging multiple edge devices concurrently, as the inter-stage data dependencies force each device to await completion of the preceding one.\n2) Model Parallelism: Model Parallelism (MP) is a parallel computing paradigm that horizontally partitions the opera- tions within a model layer, facilitating concurrent execution of single-shot inference. The most common techniques of model parallelism applied to Transformer models are Tensor Parallelism (TP) [19], [24] and Sequence Parallelism (SP) [25]. TP partitions model weights across devices, each hosting a subset of parameters, yet it fails to parallelize some element- wise operations between MHA and MLP block. In contrast, SP segments the input along the sequence dimension, facilitating parallelism for all operations, but requires each device to store the entire model parameters. Due to intra-layer data dependen- cies, synchronization points are inserted during MP to ensure consistency between collaborative and local inference results. However, these synchronization points introduce significant communication latency, potentially becoming a bottleneck in inference performance, especially in bandwidth-limited edge environments."}, {"title": "III. GALAXY DESIGN", "content": "Our system design aims to concurrently utilize multiple heterogeneous edge devices to achieve low-latency in-situ Transformer inference. Fig. 4 illustrates the workflow of our proposed Galaxy, which features three primary phases: Preprocessing Phase, Parallelism Planning Phase and Exe- cution Phase. Preprocessing Phase is an offline procedure that runs once before deployment. Galaxy Profiler performs an inference process using calibration data as input on the physical edge devices to record the run-time traces necessary for parallelism planning (step 1). In parallelism planning phase, Galaxy adopts a novel hybrid model parallelism (HMP) architecture that incorporates both TP and SP to orchestrate distributed edge devices (step 2). Galaxy Planner takes profiling results from Galaxy Profiler as input to generate a parallelism planning configuration (step 3). This configura- tion comprehensively considers both resource heterogeneity and memory budget, and is subsequently applied to target models and edge devices in Execution Phase for efficient edge collaborative inference (step 4). Distributed inference inevitably involves tensor synchronization operations. Galaxy incorporates a tile-based fine-grained communication opti- mization to mitigate the performance degradation brought by additional communication overhead (step 5). With the above modules, Galaxy focuses on the following design goals:"}, {"title": "B. Hybrid Model Parallelism", "content": "Galaxy incorporates an innovative HMP architecture that facilitates efficient parallel Transformer inference within edge environments. In this section, we will elaborate on our HMP architecture, using an example of collaborative inference con- ducted across two edge devices. As illustrated in Fig. 5, TP and SP alternate throughout a Transformer layer. Specifically, TP is applied to the MHA block and MLP block while SP is applied to operations connecting the MHA and the MLP blocks, namely connective blocks."}, {"title": "1) Tensor Parallelism on MHA Block", "content": "The aim of designing an efficient TP approach is to reduce the data dependencies among operators split across various devices, thereby reduc- ing the frequency of tensor synchronization [24], [26]. As illustrated in Fig. 5, the first block applied with TP is the MHA block. We exploit the inherent parallelism advantage of MHA: the computation of multiple attention heads is entirely independent. This head-level dependency allows us to split the operations of each attention head across edge devices without any tensor synchronization during the execution of Multi Self- Attention operations. With this in mind, we partition the weight matrices associated with key (WK), query (WQ), and value (WV) along their head dimension. The initial General Matrix Multiply (GEMM) is distributed to distinct devices and parallelized along head dimension (1). Subsequently, Self-Attention corresponding to each attention head is carried out locally on each respective device (2). The final GEMM from the output linear layer is parallelized along its row dimension, ensuring alignment with the initial GEMM's head- wise partition (3). The operations on device $i$ ($i\\in \\{0, 1\\}$) can be formulated as follows ([..]is the concat operation):\n$\\begin{aligned} &{[Q_{i}K_{i}V_{i}] = [W^Q | W^K | W^V] \\cdot A, \\\\ &B_{i} = Self\\text{-}Attention(Q_{i}, K_{i}, V_{i}),\\\\ &C_{i} = W^O B_{i}.\\end{aligned}$"}, {"title": "2) Tensor Parallelism on MLP Block", "content": "As illustrated in Fig. 5, the second block applied with TP is the MLP block, which comprises two consecutive GEMMs. To obviate tensor synchronization between the first and second GEMM oper- ations, we leverage the concept of matrix tiling to remove data dependencies. We partition the weight matrix of the first GEMM along its column dimension (4), and partition the second GEMM along its row to align with the column-wise partition of the first GEMM (5). The second GEMM can directly take the output of the first GEMM as input without a synchronization point. The operations on device $i$ ($i \\in \\{0,1\\}$) can be formulated as follows:\n$\\begin{aligned}E_{i} &= GELU(W_{i}^{D}),\\\\F_{i} &= W^{E}_{i}.\\end{aligned}$"}, {"title": "3) Sequence Parallelism on Connective Block", "content": "TP expe- dites the most computationally intensive parts of each Trans- former layer while leaving the Dropout, Residual Addition and Layer Norm connecting the MHA block and the MLP block untouched (6). Although these operations are element-wise and entail no intensive matrix multiplication, they require a considerable amount of memory access, thus also yielding a non-negligible execution latency. We notice that these element-wise operations are independent along the sequence dimension which allows us to parallelize them by partitioning the input sequence. The operations on device $i$ ($i \\in \\{0,1\\}$) can be formulated as follows:\n$H_{i} = LayerNorm(ResidualAdd(Dropout(G_{i}))).$"}, {"title": "4) Tensor Synchronization Points", "content": "To ensure that the in- ference results from our HMP align with the local inference results, a synchronization point is required at the end of each TP and SP block, as illustrated in Fig. 5.\nTowards the completion of TP blocks, a ReduceSum opera- tion is required to aggregate the computation results across multiple devices ($G \\leftarrow C_{0} + C_{1}$ and $G \\leftarrow F_{0} + F_{1}$). Subsequently, the aggregated results are partitioned along the sequence dimension and scattered across various edge devices for SP ($[G_{0} | G_{1}] \\leftarrow G$). These two operations can be efficiently combined and implemented using a single ReduceScatter oper- ation (7). Towards the completion of SP blocks, each device retains only a segment of the input sequences. It is essential to gather all these fragments, concatenate them, and distribute them across all devices for subsequent TP ($A \\leftarrow [H_{0}|H_{1}]$ and $D \\leftarrow [H_{0}|H_{1}]$). Consequently, we perform an AllGather communication primitive at the end of each SP block (8)."}, {"title": "5) Merits of Hybrid Model Parallelism Architecture", "content": "Em- ploying the HMP architecture presents numerous advantages over straight TP or SP architecture. Compared to TP: (1) the HMP architecture eliminates redundant computations in the connective blocks, which fully exploits the parallel potential of Transformer layers. (2) HMP does not introduce additional communication overhead. At first glance, state-of-the-art TP [24] requires two AllReduce, while the HMP requires two"}, {"title": "C. Heterogeneity and Memory Aware Workload Planning", "content": "Fig. 5 shows that a synchronization point is required after each TP or SP block completion. The initiation of these synchronization points is bound by the completion time of the slowest device (straggler). Such straggler can starve other faster devices, resulting in resource under-utilization. Given the inherent heterogeneity in computing capacities of de- vices, particularly notable in edge environments, adopting a heterogeneity-aware workload planning is essential to dis- tribute the workload in a balanced manner. Furthermore, infer- ence on Transformer-based models necessitates considerable memory. In practical deployment, an out-of-memory (OOM) issue is a game-stopper for inference, which poses substantial challenges for edge devices that usually operate within tight memory limitations. Consequently, our workload planning should also comprehensively consider each device's memory budget to prevent overconsumption of available memory."}, {"title": "1) Optimization Target Formulation", "content": "As elaborated in \u00a7III-B, our HMP architecture allocates workload by partition- ing along three distinct dimensions: the head dimension for the MHA block, the row dimension of the weight matrix for the MLP block, and the sequence dimension of the input tensor for the connective block. Our workload planning focuses on de- termining the partition configuration for each of these blocks, namely: the MHA blocks partition $A = \\{a_{0}, a_{1},...,a_{p-1}\\}$, the MLP blocks partition $B = \\{b_{0}, b_{1}, ..., b_{p-1}\\}$, and the connective blocks partition $S = \\{s_{0}, s_{1}, ..., s_{D-1}\\}$, where D is the number of edge devices. We introduce the notation $L(MHA, A_{d}, d)$, $L(MLP, B_{d}, d)$, and $L(CON, S_{d}, d)$ to repre- sent the execution latency of the MHA block, the MLP block, and the connective block on device d, respectively, each given their partition configurations $A_{d}$, $B_{d}$, and $S_{d}$. The execution time L for each TP or SP block is determined by the straggler:\n$\\begin{aligned}L(MHA, A) &= \\underset{d\\in \\{0,1,...,D-1\\}}{max} L(MHA, A_{d}, d), \\\\L(MLP, B) &= \\underset{d\\in \\{0,1,...,D-1\\}}{max} L(MLP, B_{d}, d),\\\\L(CON, S) &= \\underset{d\\in \\{0,1,...,D-1\\}}{max} L(CON, S_{d}, d).\\end{aligned}$"}, {"title": "2) Workload Planning Algorithm", "content": "A straw-man approach to address the above constrained optimization problem would involve an exhaustive search of all potential partitioning combinations, subsequently selecting the optimal solution that satisfies the memory constraints. However, this method suffers from an exponential complexity, rendering it infeasible for large-scale Transformer models.\nThe connective block's execution time hinges primarily on memory access volume rather than the SoC's computing capabilities, where we adopt a strategy of equal partition for SP planning. Equal partition preserves uniform communication volume across all devices during tensor synchronizations, lay- ing a conducive foundation for our tile-based communication overlapping in \u00a7III-D. Towards TP, we can achieve optimal partitioning of blocks with workload distribution proportional to each device's computing capacity, disregarding the memory budget. This proportional partition ensures that all devices complete their tasks almost simultaneously, effectively miti- gating potential delays that might lead to suboptimal resource utilization. With these insights, we devise a two-step heuristic algorithm, outlined in Algorithm 1. In the first step, the algorithm disregards the memory constraints of the devices and distributes the workload commensurate with their computing capacities, thereby ensuring a balanced workload (lines 1-8). Subsequently, building on this initial distribution, the second step fine-tunes the workload allocation. It redistributes excess workloads from devices that surpass their memory budgets to those with spare memory capacity. (lines 9-19). Considering that the granularity of partitioning for MHA block (head"}, {"title": "D. Tile-based Communication Optimization", "content": "In contrast to stable, high-bandwidth networks in datacen- ters, edge environments frequently grapple with inconsistent, bandwidth-limited connections. This amplifies synchronization latency during the collaborative inference, serving as a signif- icant bottleneck of global system performance. Overlapping communication and computation is an effective optimization strategy. However, its implementation becomes intricate in the Transformer inference due to the strict data dependencies between communication and computation. To address this, Galaxy introduces a tile-based approach to effectively decou- ples their dependency to achieve a fine-grained overlapping.\nWe observe from Fig. 5 that each TP block starts and ends with GEMM operations. We design to overlap these GEMM operations with the AllGather and ReduceScatter operations when entering and exiting the TP blocks. To illustrate this, the following section provides an example of collaborative inference across three devices, demonstrating how to overlap GEMMs with synchronization points before and after the MLP blocks (also applicable to the MHA blocks)."}, {"title": "1) AllGather Overlapping", "content": "As illustrated in Fig. 5, a strict data dependency exists between the AllGather and the ini- tial matrix multiply (GEMM1) in MLP block. Specifically, GEMM1 on device i (i \u2208 {0,1,2}) can only commence after the AllGather has finished aggregating all sub-sequences:\n$D = AllGather(H_{0}, H_{1}, H_{2}), E_{i} = GEMM1(D, W^{D}).$\nTo decouple the strict dependency between AllGather and GEMM1, we leverage matrix tiling to decompose GEMM1. We discover that the direct calculation of GEMM1 can be equivalently achieved by segmenting matrix D horizontally into tiles, executing the GEMM1 independently on each tile, and subsequently concatenating the results.\n$E_{i} = \\begin{bmatrix} H_{0}^{WP} \\\\ H_{1}^{WP} \\\\ H_{2}^{WP} \\end{bmatrix} W^{D} = D \\cdot W^{D}.$"}, {"title": "2) ReduceScatter Overlapping", "content": "As illustrated in Fig. 5, a strict data dependency exists between the final matrix multi- plication (GEMM2) in the MLP block and the ReduceScatter operation (i \u2208 {0,1,2}):\n$F_{i} = GEMM2(E_{i}, W^{E}), G_{i} = ReduceScatter(F_{0}, F_{1}, F_{2}).$\nTo decouple the strict dependency between ReduceScatter and GEMM2, we mirroring the tiling approach used with the AllGather. We split the matrix $E_{i}$ into three equally-sized tiles $E_{i,r}$ ($r \\in \\{0,1,2\\}$) along the row dimension (aligns with the partition configuration of connective block) and compute GEMM2 independently for each tile (Eq.10). To obtain the final result $G_{r}$, an additional ReduceSum operation across all devices is necessary (Eq.11).\n$\\begin{bmatrix} O_{i,0} \\\\ O_{i,1} \\\\ O_{i,2} \\end{bmatrix} = \\begin{bmatrix} E_{i,0}W^{E} \\\\ E_{i,1} W^{E} \\\\ E_{i,2} W^{E} \\end{bmatrix} = E_{i} \\cdot W^{E},$\n$G_{r} = \\sum_{i} O_{i,r}.$"}, {"title": "IV. IMPLEMENTATION AND EVALUATION", "content": "We have fully implemented the prototype system of Galaxy and baselines with ~1500 LoC in Python and C/C++ atop Pytorch [28]. Galaxy's idea is also portable and can work well with other lightweight ML frameworks such as MNN [29] and TF-Lite [30]. In this section, we evaluate the performance of Galaxy prototype for five different sizes of Transformer-based models on physical testbeds."}, {"title": "A. Experimental Setup", "content": "We evaluate Galaxy across a diverse range of realistic edge environments, incorporating both homogeneous and heterogeneous configurations of off- the-shelf edge devices (Jetson Nano [32]), as detailed in Table II and III. In homogeneous environments, the memory budget for Nano-M is set at 1.5GB. In the heterogeneous environments, the memory budgets are set at 1.5GB for Nano- L, 1.2GB for Nano-M, and 0.7GB for Nano-S, respectively. We limit usage to the onboard CPU to simulate resource- constrained edge scenarios. We will also demonstrate the effectiveness of Galaxy in GPU environments in \u00a7IV-E. We adjust the D2D bandwidth to simulate the diverse network conditions within realistic edge environments."}, {"title": "Baseline Methods", "content": "We compare Galaxy with both single- device method and state-of-the-art parallel methods:"}, {"title": "B. Comparison to Baselines", "content": "Table IV summarizes the general performance results com- paring Galaxy with state-of-the-art methods M-LM and SP. We conduct experiments on three different homogeneous edge environments with 125Mbps intra-cluster bandwidth. We employ the average end-to-end inference latency as our performance metric. The results indicate that owing to our HMP architecture and tile-based communication optimization, Galaxy outperforms baselines across various models and edge environments. Specifically, when comparing to M-LM, Galaxy achieves up to 1.46\u00d7 higher performance. With the increase in model size, the communication-to-computation ratio declines. This narrows the room for our communication optimization, correspondingly leading to a decrease in the speedup ratio. Within a specific model, an increase in the number of partici- pating devices raises the communication-to-computation ratio, thus magnifying the benefits of our communication optimiza- tion. When compared to SP, Galaxy achieves up to 1.11\u00d7 performance enhancement. SP requires less synchronous com- munication than both Galaxy and M-LM, resulting in a smaller speedup ratio. However, as SP applies partitioning along the sequence dimension, it necessitates that each device retains a full set of model weights. This requirement is particularly memory-intensive and thus unfriendly to resource-constrained edge devices, as evidenced by frequent OOM issues.\nWe further compare Galaxy's performance with baselines under varied network conditions. Using the switcher's traffic control, we simulate five D2D bandwidths to mimic various network conditions at edge. Evaluation results are shown in Fig. 8. We observe that in varying network bandwidth conditions, Galaxy consistently exhibits superior performance over baselines, achieving an inference latency reduction of 1.04x-1.45x across diverse models and edge environments."}, {"title": "C. Evaluate with Heterogeneous Edge Environments", "content": "We conducted comparisons between Galaxy and baselines within various edge environments (125Mbps), each compris- ing devices with different computing capacities and memory budgets. The results are demonstrated in Fig. 9. We observe that Galaxy consistently and remarkably outperforms other state-of-the-art parallelism methods in various heterogeneous edge environments, yielding a substantial inference latency reduction in the range of 1.3\u00d7 to 2.5\u00d7. Galaxy's superior performance in heterogeneous edge environments derives from its consideration of device heterogeneity, a factor overlooked by M-LM and SP, both tailored for datacenters equipped with homogeneous accelerators. In addition to device heterogene- ity, Galaxy workload planning comprehensively considers the memory budget of edge devices, enabling them to collabo- ratively accommodate the target model. In contrast, M-LM and SP overlook the memory constraints during parallelism planning, resulting in OOM errors."}, {"title": "D. Scalability Analysis", "content": "To explore the scalability of Galaxy, we set up both weak and strong scaling experiments in edge environment C (1000Mbps). To obviate the impact of OOM errors on our experimental observations, we load and repeatedly perform inference on one single layer, rather than loading entire model."}, {"title": "1) Weak Scaling", "content": "In a weak scaling setup, the global workload increases proportionally with the number of devices. We set a weak scaling with a fixed sequence length of 96 per device (e.g. sequence length is equal to 384 for 4 Jetson Nano- M). The overall system's floating-point operations per second (FLOPS) are then evaluated. As depicted in Fig.10, we observe excellent scaling performance in both GPT2-L and OPT-XL. Specifically, the GPT2-L case with 4-way (four Jetson Nano- M) HMP can achieve 81% of linear scaling while the OPT-XL case with 4-way can achieve 86% of linear scaling."}, {"title": "2) Strong Scaling", "content": "In a strong scaling setup, the global workload is independent of the number of participating de- vices. We fix the sequence length to a constant value of 384. As depicted in Fig. 11, we measure the average inference latency per Transformer layer for a varying number of edge devices. Galaxy also demonstrates superior scalability under a strong scaling setup. Specifically, Galaxy achieves 3.05\u00d7 inference latency reduction compared to Local Inference in GPT2-L case, while achieving 3.24\u00d7 inference latency reduc- tion compare to Local Inference in OPT-XL case."}, {"title": "E. GPU Support", "content": "We further evaluate Galaxy's performance in mobile GPUs environments and compare it against baselines. The GPU environment is set up using two Jetson Nanos' onboard GPUs, operating at a locked frequency of 460MHz. The experiments encompass all five Transformer-based models with edge environment A (500Mbps), as shown in Table V. We observe Galaxy outperforming baselines, achieving an inference latency reduction of 1.12x-1.67\u00d7 under the GPU environment. Despite the potential underutilization of GPUs for small models like DistilBERT due to Galaxy's communication optimization with matrix tiling, Galaxy still achieves accelerations up to 1.36\u00d7 compared to baselines."}, {"title": "V. RELATED WORK", "content": "Data Parallelism [21], [35] is the most extensively used distributed train-"}, {"title": "VI. CONCLUSION", "content": "This paper introduces Galaxy, an innovative collaborative in-situ Transformer inference system featuring a hybrid model parallelism architecture, a heterogeneity and memory-budget aware planning algorithm, and a tile-based communication op- timization. Our extensive evaluation demonstrates that Galaxy achieves up to 2.5\u00d7 performance enhancement compare to state-of-the-art approaches."}]}