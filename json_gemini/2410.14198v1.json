{"title": "SUPERVISED CHAIN OF THOUGHT", "authors": ["Xiang Zhang", "Dujian Ding"], "abstract": "Large Language Models (LLMs) have revolutionized natural language processing and hold immense potential for advancing Artificial Intelligence. However, the core architecture of most mainstream LLMs-the Transformer-has inherent limitations in computational depth, rendering them theoretically incapable of solving many reasoning tasks that demand increasingly deep computations. Chain of Thought (CoT) prompting has emerged as a technique to address these architectural limitations, as evidenced by several theoretical studies. It offers a promising approach to solving complex reasoning tasks that were previously beyond the capabilities of these models. Despite its successes, CoT and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a \u201cone-prompt-for-all\" approach, using a single prompt structure (e.g., \"think step by step\") for a wide range of tasks-from counting and sorting to solving mathematical and algorithmic problems. This approach poses significant challenges for models to generate the correct reasoning steps, as the model must navigate through a vast prompt template space to find the appropriate template for each task. In this work, we build upon previous theoretical analyses of CoT to demonstrate how the one-prompt-for-all approach can negatively affect the computability of LLMs. We partition the solution process into two spaces: the prompt space and the answer space. Our findings show that task-specific supervision is essential for navigating the prompt space accurately and achieving optimal performance. Through experiments with state-of-the-art LLMs, we reveal a gap in reasoning performance when supervision is applied versus when it is not. Our goal is to provide deeper insights into the mechanisms underlying CoT, offering guidance for the effective design of CoT variants. Additionally, we underscore the limitations of traditional \u201cunsupervised\u201d prompting methods, arguing that users of CoT cannot simply \"sit back\" and rely entirely on the model. Instead, we advocate for task-specific \"supervised\" CoT, enriched with human knowledge, to enable more effective reasoning in LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "The advent of large language models (LLMs) (Achiam et al., 2023) has ushered in a new era for natural language processing and artificial intelligence (Kojima et al., 2022; Zhao et al., 2023). These models exhibit remarkable capabilities across various domains (Thirunavukarasu et al., 2023; Wei et al., 2022; Valmeekam et al., 2023; Zhang et al., 2023a), achieving near-human performance in tasks such as knowledge retrieval and articulation (Chang et al., 2024). However, concerns have been raised regarding their reasoning abilities (Valmeekam et al., 2022; Zhang et al., 2024). These tasks range from fundamental operations like counting, sorting, and multiplication (Dziri et al., 2024), to more complex challenges such as mathematical problem-solving, algorithm design, and coding (Xu et al., 2022; Thirunavukarasu et al., 2023). Previous research has explored several factors contributing to these reasoning deficiencies, including training optimizations (Thorburn & Kruger, 2022), tokenization methods (Singh & Strouse, 2024), and dataset choices (Ye et al., 2024). Among these, the architecture of the model plays a pivotal role in determining its reasoning capabilities (Raghu et al., 2017; Zhang et al., 2024; Del\u00e9tang et al., 2022). The backbone architecture of most mainstream LLMs-the Transformer (with finite precision) (Vaswani, 2017)\u2014has intrinsic limitations related to computational depth (Li et al., 2024). Specifically, the attention mechanism within Transformers can perform only a fixed number of sequential computational steps (Li et al.,"}, {"title": "2 DEMYSTIFYING COT: A STRAIGHTFORWARD UNDERSTANDING", "content": "In this section, we summarize key findings from previous theoretical analyses (Li et al., 2024; Zhang et al., 2024; Feng et al., 2024) of CoT prompting, presenting them in a unified and accessible manner. The conclusions drawn here will serve as a foundation for our subsequent analysis of supervised CoT."}, {"title": "2.1 LIMITATIONS OF TRANSFORMER ARCHITECTURE", "content": "Transformers, unlike recurrent networks, are not designed to perform reasoning over an arbitrary number of sequential steps (depth) internally. Specifically, in a Transformer model, the hidden state $h_{t-1}$ at time step $t-1$ is not reused when calculating $h_t$ (Figure 2.b), as it would be in recurrent networks like RNN (Figure 2.a). Instead, the hidden state $h_t$ is passed forward only through the layers of the Transformer (Dehghani et al., 2018) (Figure 1.c), not through time, which means that the number of sequential steps is fixed and limited for any given Transformer architecture (Li et al., 2024; Zhang et al., 2024; Elbayad et al., 2019). In contrast, Recurrent Neural Networks (RNNS) (Grossberg, 2013) allow the hidden state h to be passed through time steps via recurrent"}, {"title": "2.2 NATURE OF REASONING", "content": "Reasoning inherently requires sequential depth. For tasks with input of length n, reasoning is typically performed step by step to arrive at the final result. Examples include counting (incrementing a counter iteratively), playing chess (updating the board state iteratively), and searching (marking visited nodes iteratively). To solve a given task, there is a theoretical lower bound on the required depth of computation (Sanford et al., 2024). Since models like Transformers can only perform a constant number of sequential reasoning steps over the hidden state h, they are unable to solve reasoning tasks where the depth requirement increases with the length of the input.\nConsider chess as an example. For a sequence of chess moves, $x_n = (x_1, x_2, ..., x_n)$, to validate the n-th move, the n-th board state $h_n$ must be calculated. This requires n sequential computations, as the n-th board state depends not only on the sequence of moves x but also on the previous board state $h_{n-1}$. While a neural network could memorize the mapping from $x_n$ to the correct $h_n$ (Arpit et al., 2017), bypassing the need for sequential computation, memorization is much more resource-intensive than reasoning. This is because memorization would require storing all possible permutations of $x_n$ and their corresponding resulted board states, an exponential challenge that eventually demands infinite memory to store instances of arbitrary length.\nThus, in the example of simulating a chess game, the model's internal representation h, which encodes the board state, must be sequentially computed n times to simulate the game. Transformers, which lack the infinite precision needed for memorization, cannot perform such tasks, as their hidden states h are computed a fixed number of times, regardless of the input length."}, {"title": "2.3 COT + AUTOREGRESSIVE = RECURRENT", "content": "As previous studies have shown Li et al. (2024); Zhang et al. (2024); Feng et al. (2024), Chain of Thought (CoT) effectively bridges the gap between autoregressive Liang et al. (2022); Liu et al. (2022) models and recurrent structures Zhang et al. (2024) within large language models (LLMs). Instead of merely outputting tokens to answer questions, CoT also generates intermediate steps which are not part of the answers. These intermediate steps, represented as a sequence of natural language tokens $(o_1, o_2, ..., o_k)$, act as a discretization of the latent information $h_n$ (Figure 2.c). Given that natural language is a powerful medium for encoding nearly any type of information, $h_n$ is effectively transformed into a token sequence o, which is then converted back into a vector h via the embedding layer. In this way, computational information is preserved through a process of discretization followed by vectorization, represented as: $h_t \\xrightarrow{\\text{discritization}} (o_1, o_2, ..., o_k) \\xrightarrow{\\text{vectorization}} h_{t+1}$ (Figure 2.c). This approach, effectively achieve the same effect as $h_t \\Rightarrow h_{t+1}$ in the RNN-like recurrent network, allowing h to be recurrently updated by the network.\nIn the earlier chess example, the LLM generates intermediate reasoning steps as natural language strings during the CoT process. Specifically, it produces a sequence of tokens (e.g., in English) to describe the board state $h_k$ after the first k moves, detailing the positions of pieces such as the bishop and the king. In the subsequent computation, the LLM reads this board description up to move k and uses it to calculate the k+1-th board state, thereby avoiding the need to re-compute the reasoning from scratch-something Transformers cannot do internally due to their non-recurrent architecture."}, {"title": "3 COT SEARCH SPACE = PROMPT SPACE + ANSWER SPACE", "content": "While theory suggests CoT-augmented LLMs can solve any problem Li et al. (2024), finding solutions in practice is much harder. CoT is limited by a finite number of steps, and the conversion from latent states h to token sequences o is imperfect. Consequently, only partial information is extracted at each step, making it crucial to identify the right data to continue the correct computation. We decompose the CoT reasoning into two components: template search within the prompt space and answer search within the answer space. We show how effective navigation of the prompt space can simplify answer space complexity and reveal limitations of unsupervised \u201cX-of-thought\" methods."}, {"title": "3.1 PROMPT SPACE", "content": "The latent vector h contains rich intermediate information when processing a task, including counters, sums, flags for binary indicators, and more. When LLMs are prompted to \"think step by step\" along with the task instance, they generate a step template, specifying which information from h to extract and discretize into tokens $(o_1, o_2, ..., o_k)$. Ideally, as $k\\rightarrow \\infty$\u2500meaning the length of the CoT is arbitrarily long-all vectorized information in h can be fully textualized, achieving true recurrence through autoregression. However, with limited k, only partial information is discretized.\nIf we define the amount of information stored in h as m bits, and each CoT step extracts up to s bits of information into o, each unique step template specifies a way to extract s bits from the full m-bit space. Thus, the total number of potential step templates is $C(m, s) = \\frac{m!}{s!(m-s)!}$, which estimates the number of ways information can be extracted via CoT at each step. Each template defines an extraction of unique s bits of information.\nFor example, in the chess simulation case, h encodes details such as the <current board layout>, <the next player>, <board status>, <number of pieces taken by each player> and so on. When given the instruction to \"think step by step\", the model decides which information to extract based on the step template it generates. Extracting the wrong"}, {"title": "3.2 ANSWER SPACE", "content": "Once the model \u201cdecides\u201d on the steps to follow during CoT, it performs reasoning accordingly. With a specific step (prompt) template $p_i$ chosen from the prompt space P, CoT iteratively executes $(o_1, o_2, ..., o_k) \\Rightarrow h_{t+1}$ to update h and calculate the next state, continuing this process until reaching the final state (solution). The complexity of finding solutions in the answer space depends on both the choice of $p_i$ and the nature of the task itself.\nEach task embeds a different level of complexity in its answer space. For instance, in the chess simulation task of <finding a set of actions leading to game end>, the answer space S = ($S_1, S_2, ..., S_\\infty$) contains all possible combinations of action sequences s. The solution set $C_R \\subset S$ includes all valid action sequences that lead to the end of the game, being a subset of the entire answer space S. Solving the problem requires identifying one single correct action sequence $S_{correct} = (y_1, y_2, ..., y_l) \\in C_R$.\nIf a fixed step (prompt) template for this task, such as $p_0$ = , is used, the CoT process iteratively extracts the current board description and use it for calculating next board state in h to identify the valid next move $y_1$, eventually forming the correct answer $S_{correct} = (y_1, y_2,..., y_l)$. The complexity of navigating the answer space can be roughly measured by:\n$\\frac{\\text{len}(C_R)}{\\text{len}(S)}|P$\nThis ratio measures the proportion of the solution space CR relative to the entire answer space S, given a specific template p. If the chosen template p extracts irrelevant information\u2014such as determining which player is next at each step\u2014the ratio simplifies to $\\frac{\\text{len}(C_R)}{\\text{len}(S)}$. In this case, each $y_i$ would be generated randomly, as h can not be computed iteratively over useful information needed for extracting correct $y_1$, making the correct answer only discoverable by chance.\nCorrectly identifying the step template p is crucial for reducing the complexity of $\\frac{\\text{len}(C_R)}{\\text{len}(S)}|P$, as p dictates what information is recurrently overlayed in the process $h_t \\Rightarrow h_{t+1}$ and in turn what can be calculated, essentially acting as the \"algorithm\" for solving tasks in the CoT process. In the chess example, the optimal template would be , allowing the model to reason over the board state iteratively, i.e., $h_t \\Rightarrow h_{t+1}$. With the correct board state computed recurrently, the valid next move $y_i$ can be effortlessly derived from h (Figure 3 right). However, using a less relevant template, such as , would expand the search space nearly to $\\frac{\\text{len}(C_R)}{\\text{len}(S)}$, as the number of pieces doesn't provide useful information for determining the next valid move. Consequently, the model would have to recalculate the board state at each step from previously generated moves $y_i$, which requires O(n) depth-"}, {"title": "3.3 COT AS AN UNSUPERVISED TASK SOLVER", "content": "CoT operates in an unsupervised manner for any given task, relying on a single universal prompt, Think Step by Step, and leaving it to the model to generate its own step template $p \\in P$ for extracting information at each step. Since humans do not supervise step completion, the generation of steps-i.e., determining which information to extract from h and compute recurrently-comes primarily from the model's heuristics. For example, in counting tasks, LLMs use learned heuristics to extract a Counter value from h and perform recurrent updates. However, these unsupervised, heuristic-driven templates are often unreliable, as the model lacks the knowledge to identify key components for some computation or tasks with complicated descriptions, as demonstrated in previous work Valmeekam et al. (2022) and our experiments."}, {"title": "3.4 COT VARIANTS AS UNSUPERVISED HELPERS FOR NAVIGATING ANSWER SPACE", "content": "In practice, the answer space S can be large and complex, and even with the optimal step (prompt) template p, CoT can make errors. Various CoT variants, such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT), have been proposed to mitigate these mistakes in solution searching. While these \"X-of-thought\" approaches don't dictate which specific information to extract at each step like p does, they improve solution finding by exploring multiple paths and self-verifying. For instance, ToT explores multiple instances in the answer space simultaneously under some given template p, unlike the single-path exploration of CoT. Specifically, information extracted from the current hidden state $h_t$ using p is used to generate q possible answers for the next step, denoted as $(y_1, y_1, ..., y_1)$. Each answer leads to a different next state $h_{t+1}$. In the example of , the board state at step t is extracted into descriptions using the correct template p and to form $h_{t+1}$, and instead of producing a single next move $y_{t+1}$ from h, multiple actions are derived. Each derived action along with previous actions forms a unique path that leads to a potential solution in S. Since some paths may fail (e.g., leading to a non-ending game), exploring multiple paths simultaneously increases the efficiency of searching the answer space.\nSimilarly, GoT improves search accuracy by iteratively revisiting previously generated partial answers. However, none of these approaches are supervised, as the model is not informed of the correct step template p and generates it on its own, extracting information at each step accordingly. X-of-Thought still relies on a \u201cone-prompt-for-all\" approach and only aids in finding answers after $p \\in P$ is fixed. As we have shown, this can lead to poor outcomes, since p directly influences the complexity of the answer space, and X-of-Thought may be too late to correct errors in some cases."}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct experiments to demonstrate the importance of supervision in the CoT process. Specifically, we design scenarios where the correct step template is provided through supervision, and compare them to cases where incorrect steps are simulated by the model. Our results show significant performance degradation when the step templates are incorrectly derived, highlighting the need for human supervision to ensure reliable task performance with LLMs."}, {"title": "4.1 EXPERIMENTS DESIGNS", "content": "Although we used chess simulation as an example of reasoning with CoT due to its resemblance to real-life complex reasoning tasks, tasks involving chess boards and actions can be difficult to implement and evaluate. Instead, we follow previous work Zhang et al. (2024); Del\u00e9tang et al. (2022) by focusing on more fundamental reasoning tasks for LLMs. Specifically, we evaluate tasks at three levels of computability: Regular (R), Context-Free (CF), and Context-Sensitive (CS), each corresponding to tasks solvable by different levels of computational power, from deterministic automata all the way to linear bounded automata (restricted Turing machines). These tasks involve operations such as counting, sorting, and number addition-basic operations that are required by more complex algorithmic problems (like NP problems). Each task has a strong dependency on identifying the correct step template, thus allowing us to clearly observe the impact of selection on step template on CoT performance.\nAll of these tasks require a level of computability beyond the capabilities of the Transformer's internal architecture Del\u00e9tang et al. (2022). Specifically, they demand a minimum computational depth that scales linearly with input length, surpassing the constant depth inherent to Transformer models. Thus, solving these tasks necessitates the use of CoT, and correctly identifying the information to extract during CoT is crucial for resuming computation and building the necessary depth.\nWe use GPT-4-o classic, a version that eliminates the use of external tools (e.g., calculators or programs) and functions solely based on the model itself. We test each task using instances sampled according to previous work (Zhang et al., 2024). To ensure that factors such as long-context information retrieval and tokenization do not affect the results, we follow the setup from prior research and conduct controlled experiments. Details of our experimental design, including length sampling, task specifications, format adjustments, and prompt usage, are provided in the Appendix.\nWe extend the previous findings on expert models Del\u00e9tang et al. (2022), which are specifically trained for particular tasks, to our experiments with LLMs. Due to differences in experimental settings, the results from expert models are presented for reference rather than direct comparison. Unlike prior research, which reports the best performance out of N trials Del\u00e9tang et al. (2022); Zhang et al. (2024) for each task instance, we report the average one-trail performance across all tested instances. Our focus is on practical usability beyond the theoretical upper-bound computability analysis in previous work. The final results are shown in Table 1."}, {"title": "4.2 MAIN RESULT", "content": "Recurrence is key for reasoning. As demonstrated in both expert models (RNN, Tape-RNN, and Transformers) and LLMs, recurrence is the determining factor for solving tasks in each category. Specifically, expert models like RNN and Tape-RNN show the ability to solve tasks across various categories with over 90% accuracy, depending on their memory architecture. Transformers, however, are limited by their shallow depth of reasoning, as shown earlier, and fail to solve any tasks. Similarly, LLMs without CoT, relying solely on internal Transformer reasoning, achieved 0% performance on most tasks, with low performance on others likely due to guessing. When CoT augments LLMs with recurrent computational power, accuracy improves significantly. These comparisons highlight the critical role of recurrence in a model's computability, reinforcing the analysis we previously discussed.\nRole of Step Template in Reasoning Performance: Supervision Is Essential. We provide human supervision for all tasks, and we observed that, due to the relatively simple nature of the tasks, the model makes mistakes in finding the optimal step template less frequently. As a result, it is difficult to clearly observe the performance gap between optimal and non-optimal step templates. To address this, we introduce two types of supervision for each task: Correct Supervision (CR Supervised), where the model is guided with optimal steps to demonstrate the best possible performance, and Incorrect Supervision (IN Supervised), which simulates scenarios where the model derives incorrect"}, {"title": "COT Variants are Useful in Navigating Answer Space.", "content": "We compare the results of different CoT variants for the same tasks. As shown in Table 2, both ToT and GoT improve performance over naive CoT. However, this improvement is due to correcting \u201cincorrect calculations\" during computation, not from improvements in step-template selection. ToT provides little benefit, as the tasks typically have only one path to the solution. In contrast, GoT shows greater accuracy gains, thanks to its self-revisiting mechanism,"}, {"title": "Prompt Space Analysis.", "content": "We further analyzed the model's performance in navigating the prompt space, i.e., finding the correct (optimal) step template for each task. As shown in Figure 5, all tasks involve relatively simple calculations, and the model exhibits a high average success rate in identifying the correct template. Specifically, the success rate for R-type tasks exceeds 90%. As task complexity increases, we observe a slight decline, with CS tasks showing an 84% success rate in extracting the correct information during CoT. We further include case studies showcasing how \u201csub-optimal\" steps are derived from unsupervised CoT process, which are shown in Figure 1 and Appendix Figure 6, 7 and 8.\nLastly, we showcase how incorrect navigation in the prompt space leads to uncorrectable results. As shown in Appendix Figures 9 and 10, the incorrect step template results in incorrect information extraction, leading to a wrongly computed next state and ultimately increasing the difficulty of searching the answer space."}, {"title": "5 SUPERVISED COT: USERS' PERSPECTIVE", "content": null}, {"title": "5.1 HOW TO SUPERVISE?", "content": "As we've demonstrated, providing correct supervision is crucial for helping the model achieve accurate results. A natural question arises: how can effective supervision be derived? The key to good supervision lies in understanding CoT's underlying mechanism, which essentially involves relaying information through the text space. For tasks requiring multiple steps, users need to identify what each step is and what key information should be extracted at each step.\nWhile this might seem straightforward in the basic reasoning tasks used in our experiments, it becomes more complex for challenging tasks, where correctly identifying the information requires careful task analysis. Therefore, human knowledge is critical for enhancing the model's computational abilities and can directly influence task success. However, this supervision adds a substantial workload, as each task demands a unique understanding of its computational structure.\nAgain, Supervised CoT requires clearly stating what should be outputted as text at each step, as this information will be used to construct the next h, which we have shown before. Users need to provide as specific instructions as possible to detail what intermediate steps need to be outputted at each \"think-step-by-step\" step."}, {"title": "5.2 WHEN TO SUPERVISE?", "content": "As we've observed, using an incorrect step template-whether model-derived or human-injected-can result in significant performance degradation. Based on this, it's important to avoid providing supervision unless you are reasonably confident that the steps will not hinder the reasoning process. In cases of uncertainty, it may be better to rely on the model's own heuristics."}, {"title": "6 CONCLUSIONS", "content": "Our work offers a unique perspective on the mechanics of Chain of Thought (CoT) prompting and its role in enhancing model reasoning. Through theoretical analysis and practical insights, we show how CoT transforms latent information into text space, enabling iterative and resumable reasoning steps that expand a model's computational depth. We further connect the model's problem-solving capabilities with the complexities of finding solutions. Our analysis of prompt space and answer space underscores the importance of identifying the correct step template to simplify navigation\u2014an often overlooked aspect in prompt-related research. The success of CoT hinges not only on generating steps but on extracting the right information at each stage. Our experiments demonstrate that incorrect step templates can severely impact reasoning, reinforcing the importance of supervision. Even small errors in template selection can lead to significant failures. Our findings combine theoretical analysis and experimental evidence, offering valuable insights into CoT's limitations and potential for improving reasoning tasks in large language models."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 EXPERIMENTAL DESIGN", "content": "Our experimental setup carefully addresses potential pitfalls that could influence the model's performance, specifically focusing on tokenization and context length. Tokenization issues can significantly affect how models handle specific tasks, often leading to failures not tied to the model's reasoning ability. To counter this, we reformatted task instances to eliminate tokenization biases. Moreover, LLMs often struggle with retrieving information from long contexts, leading to hallucinations or forgotten data during extended reasoning processes. This tends to degrade accuracy, as models fail to maintain accurate references to the initial task elements throughout longer sequences. While these challenges are important in real-world applications of LLMs, they are outside the scope of our investigation, which prioritizes analyzing the effect of using different step template. To maintain a controlled environment, we restrict task lengths from 10 to 20 elements. This threshold was determined from preliminary analysis, where longer task sequences often introduced issues not related to reasoning but to the model's internal optimization process. When the task sequence exceeds 25 steps, models can divide output over multiple contexts, which distorts accurate information retrieval. By maintaining a manageable length, we isolate and evaluate the differences between reasoning with and without CoT, avoiding disruptions caused by excessive context length. For each task, we generate 50 instances using a pre-written script and the results are examined by humans."}, {"title": "TASK DESIGN", "content": "Each task involves simple rule-based iterations, emphasizing memory access and iterative processes. The challenge for the model lies in its ability to execute these tasks within the constraints of its architecture and memory systems. Below, we describe each task in detail, including sample inputs and outputs. For the Regular (R) class tasks, we include the following:\n1. Modular Arithmetic: Given a sequence of n numbers and basic operations (+, -), compute the result modulo 5. For example, the input 4 + 2 - 3 should yield 3.\n2. Parity Check: Determine if the word \"banana\" appears an even number of times in a list containing the words \"apple\" and \"banana.\" For example, the input (\"banana\", \"apple\", \"banana\") yields True.\n3. Cycle Navigation: Based on a sequence of actions (\"forward,\" \"backward,\" \"stay\"), determine the final position in a 5-state cycle starting from state 1. For example, (\"forward\", \"stay\", \"backward\") will return state 1.\nFor the Context-Free (CF) class tasks, we use the following:\n1. Stack Manipulation: Given a list of fruit names representing a stack and a sequence of stack operations, compute the final stack. For example, applying (pop \"banana\", push \"orange\") to (\"apple\", \"banana\", \"grape\") results in (\"apple\", \"orange\", \"grape\")."}, {"title": null, "content": "2. Reverse List: Reverse a list of vegetable names. For example, (\"carrot\", \"potato\", \"onion\") becomes (\"onion\", \"potato\", \"carrot\").\n3. Modular Arithmetic (Complex): Compute the result of an arithmetic expression with n operations modulo 5. For instance, ((2+4) \u00d7 (3 \u2013 1)) mod 5 yields 0.\nFor the Context-Sensitive (CS) class tasks, we evaluate the following:\n1. Odd First: Extract all items at odd positions from a list of animal names, followed by those at even positions. For example, (\"dog\", \"cat\", \"elephant\", \"tiger\") yields (\"dog\", \"elephant\", \"cat\", \"tiger\").\n2. Addition: Given two large numbers with n digits, calculate their sum. For instance, the input 123456 + 987654 yields 1,111,110.\n3. Multiplication: Multiply two large numbers with n digits. For instance, the input 345 \u00d7 567 yields 195,615.\n4. Sorting: Sort a list of integers using the insertion sort algorithm. For example, the input (8, 3, 5, 1) would result in (1, 3, 5, 8)."}]}