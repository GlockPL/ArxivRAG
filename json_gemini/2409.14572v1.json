{"title": "Evaluating the Performance and Robustness of LLMs in Materials Science Q&A and Property Predictions", "authors": ["Hongchen Wang", "Kangming Li", "Scott Ramsay", "Yao Fehlis", "Edward Kim", "Jason Hattrick-Simpers"], "abstract": "Large Language Models (LLMs) have the potential to revolutionize scientific research, yet their robustness and reliability in domain-specific applications remain insufficiently explored. This study conducts a comprehensive evaluation and robustness analysis of LLMs within the field of materials science, focusing on domain-specific question answering and materials property prediction. Three distinct datasets are used in this study: 1) a set of multiple-choice questions from undergraduate-level materials science courses, 2) a dataset including various steel compositions and yield strengths, and 3) a band gap dataset, containing textual descriptions of material crystal structures and band gap values. The performance of LLMs is assessed using various prompting strategies, including zero-shot chain-of-thought, expert prompting, and few-shot in-context learning. The robustness of these models is tested against various forms of 'noise', ranging from realistic disturbances to intentionally adversarial manipulations, to evaluate their resilience and reliability under real-world conditions. Additionally, the study uncovers unique phenomena of LLMs during predictive tasks, such as mode collapse behavior when the proximity of prompt examples is altered and performance enhancement from train/test mismatch. The findings aim to provide informed skepticism for the broad use of LLMs in materials science and to inspire advancements that enhance their robustness and reliability for practical applications.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent a significant advancement in the field of artificial intelligence and have been rapidly adopted for application in various scientific disciplines\u00b9. With their ability to process and generate natural language, LLMs are potent tools for tasks like information retrieval, question and answering (Q&A), and property predictions2\u20134. Similar to traditional ML models, LLMs can require extensive data processing, large volumes of data, and massive compute resources to trains. Despite these limitations, pretrained LLMs can be adapted to new tasks with few-shot examples via in-context learning (ICL), making them both cost-effective and rapid to deploy6,7. In the context of materials science, where data acquisition can often be costly and time-consuming, leveraging ICL enables LLMs to efficiently prototype and generate predictive insights even in low-data settings8\u201310. Recent work has demonstrated that LLMs are capable of domain-specific Q&A11\u201313, materials property predictions14\u201316, and information extraction from complex datasets4,17. These studies demonstrate LLMs' potential to serve as flexible and powerful analytical tools.\nHowever, the robustness of LLMs is crucial in real-world scenarios, but has not been sufficiently investigated in the literature. One aspect of the robustness of LLMs is their sensitivity to prompt changes either due to innocuous or adversarial reasons18,19. Variations in how a query or instruction is formulated may cause a response to factually change18. As an example, 0.1 nm and 1 Angstrom are equivalent but switching them in a prompt could result in different LLM predictions for the same task. Alternatively, the response of the LLM can be deliberately altered through intentional misinformation or misleading inputs19. These attributes are not only theoretical concerns but are critical for the reliable usage of LLMs as they become integrated into the materials science research and development pipeline. Given that LLMs generate outputs with indifference to truth20, thoroughly probing LLM prompt sensitivity would allow us to critically evaluate model performance in practical situations; providing informed skepticism for the broad use of LLMs in materials science.\nIn this work, we conducted a holistic robustness analysis of commercial and open-source LLMs for materials science. We systematically investigated how textual perturbations like text ordering within a prompt, and common unintentional and intentional prompt variations influence the inference of LLMs. Three distinct datasets of domain-specific Q&A and materials property prediction were selected and first benchmarked via prompt engineering to identify baseline and optimal"}, {"title": "Results", "content": "An overview of the framework is illustrated in Figure 1. For the performance evaluation and robustness analysis of materials science Q&A, we use the MSE-MCQs dataset, which has 113 multiple-choice questions from a first-year introductory materials science and engineering course at the University of Toronto. The questions are meant to test students' understanding of materials science knowledge, including material mechanics, thermodynamics, crystal structures, materials properties, etc. The questions are categorized into easy (number of questions, n=39), medium (n=40), and hard levels (n=34), based on the conceptual difficulty, calculation complexity, and the depth of reasoning required. For the performance evaluation of property prediction, we use matbench_steels, a subcategory of the Matbench test set originally proposed for benchmarking traditional machine learning (ML) models for materials property predictions21. The matbench_steels dataset has 312 pairs of material compositions (as chemical formula) and yield strength (in MPa). For the robustness analysis of property prediction, we use a band gap dataset, which comprises 10,047 descriptions of material crystal structures generated via Robocrystallographer, along with their corresponding band gap values from the Materials Project database14."}, {"title": "Performance Evaluation of Materials Science Q&A", "content": "The results of the performance evaluation of LLMs on the MSE-MCQs dataset are shown in Figure 2. For each model and setting, three trials were conducted, and the error bars represent the inherent non-determinism in LLMs even at the lowest temperature settings. This non-determinism may affect the reliability and reproducibility of the models22 and is likely a result of the stochastic sampling during text generation23\u201325. The result shows that gpt-4-0613 scored the highest in all categories, followed by gpt-3.5-turbo-0613 and llama2-70b. The remaining models scored around or slightly below the baseline score of 0.246, which is equivalent to random guessing for the provided MCQs.\nUpon implementing the expert prompt, we observed a consistent improvement in model performance across almost all the models and question types. For gpt-3.5-turbo-0613, the significant variation in the data indicated by the large error bar prevents\na definitive conclusion about the expert prompt improving performance on easy and medium-level questions. However, the accuracy of hard-level questions nearly doubled.\nWe investigated why the smaller Llama models (llama-13b and llama-7b) scored lower than the baseline without the expert prompt. Beyond the limited skill levels, these models even failed to understand the intent when instructions were not provided. Instead of answering, they often attempted to \"complete\" the questions rather than answering them. However, once the expert prompt was implemented, these smaller models could follow the instructions and attempt to solve the questions, in which case the performance improved to around and sometimes above the baseline scores.\nThe performance trends observed when evaluating large language models on the MSE-MCQs dataset were expected. Specifically, larger models like gpt-4-0613 significantly outperformed smaller ones, with model performance generally improving across the board when an expert prompt was used, especially in handling more complex questions."}, {"title": "Performance Evaluation of Materials Property Prediction", "content": "Here we investigate LLM materials property predictions, utilizing the matbench_steels dataset. Figure 3 presents a comparative analysis between gpt-3.5-turbo-0613, employing few-shot ICL, and a traditional random forest regressor (RFR) model, in predicting the yield strength of steels. It is important to note that gpt-3.5-turbo-0613 was not subject to an explicit training phase for this task but processed the data as few-shot examples. In contrast, the RFR was trained explicitly using MAGPIE features from the same data points26.\ncan be observed that gpt-3.5-turbo-0613, utilizing its ICL capabilities, performs comparably to the trained RFR model. When given only 5 data points as few-shot examples, the median MAE of gpt-3.5-turbo-0613 begins at a slightly higher value of 249 MPa but improves significantly to 171 MPa when increased to 25 data points. Meanwhile, the RFR model, starting with a slightly lower median MAE of 231 MPa with a large error bar, shows a more modest improvement to 194 MPa at 25 training points. The declining MAE trend suggests that both models benefit from more data, yet the LLM's rate of improvement is more significant.\nThis analysis reveals a notable adaptability of pretrained LLMs to new predictive challenges using ICL, particularly when data availability is constrained. Such adaptability makes LLMs potentially valuable in early-stage research or exploratory studies in materials science, where data may be scarce or costly to obtain. However, as the number of data points increases, most LLMs suffer from limited prompt windows, which make such applications computationally expensive or impossible, in which case fine-tuned LLMs and traditional machine learning models with dedicated training may be more effective.\nTo better understand how the selection of highly relevant training data can be used to enhance LLM performance we conducted a systematic study using nearest-neighbor-boosted ICL. The method developed here involves deliberately selecting"}, {"title": "Robustness Analysis of Materials Science Q&A", "content": "In Figure 6, we present the outcomes of the robustness assessment of gpt-3.5-turbo-0613 when confronted with different types of textual modifications to the MSE-MCQs, to evaluate its stability to various types of \"noise\". The gpt-3.5-turbo-0613 was chosen because of its moderate performance, availability, and cost-effectiveness.\nRanking by the degradation severity on the easy-level questions, sentence reordering has the least performance drop, followed by synonym replacement, distractive info, unit mixing, and superfluous info. The performance on the hard-level questions is close to the baseline score, indicating the model's struggle with complex queries regardless of textual modifications, and will not be discussed in detail. The larger error bars in medium and hard questions suggest that LLMs tend to generate\nmore varied responses to complex and lengthy queries.\nSentence reordering has a minimal impact on gpt-3.5-turbo-0613's performance, demonstrating the model's robust parsing capabilities. The model maintains comprehension and extracts the necessary information to answer correctly, despite the alteration in the natural flow of questions. This suggests an advanced level of syntactic flexibility and the ability to contextualize information effectively.\nThe performance drop with synonym replacement indicates that the LLMs can generate different or faulty responses when presented with different terminologies of the same meaning. This reveals LLMs' reliance on specific terminologies for recognition and comprehension in materials science. In contrast to humans, who can grasp the conceptual continuity behind varied expressions for flexible cognition, LLMs' struggles with synonym replacement emphasize the need for advanced training that prioritizes semantic networks over mere word recognition29.\nIntroducing distractive information mimics a real-world scenario where irrelevant data often accompanies critical information, requiring sharp focus and analytical precision. Improving LLMs' ability to filter out irrelevant information is crucial for more effective information retrieval, problem-solving, and data interpretation30. The results indicate that LLMs struggle to filter distractions in easy-level questions, where mostly simple conceptual knowledge is tested. The added non-essential details seemed to divert the LLMs' \"attention\", leading to fewer correct answers. The degradation is less severe in the medium and hard-level questions, where greater complexities and lengths may make the distractive information more distinguishable.\nMixing and converting the units tests LLMs' abilities to perform numerical reasoning and apply mathematical concepts within a linguistic context. The added complexity introduced by unit mixing degraded the performance of the gpt-3.5-turbo-0613 model on easy and medium-level questions. Although some state-of-the-art LLMs support multi-modal applications and function calls to perform calculations31, the ability to correctly identify and convert units from a large text can still benefit use cases such as information retrieval and data interpretation.\nSuperfluous information differs from distractive information in that it is more relevant to the questions themselves. For LLMs, distinguishing the necessary information from merely relevant but non-essential details is a more challenging cognitive process, mirroring the nuances of advanced human problem-solving. It requires an understanding of the problem's objective, prioritizing information based on the question, and applying only the information that will lead to the correct conclusion. The noted decrease in accuracy shows a current limitation in the LLM's ability for critical information assessment. The variation in performance degradation is influenced by the type and relevancy of the superfluous information provided."}, {"title": "Robustness Analysis of Materials Property Prediction", "content": "Table 1 shows the result of the degradation study on the LLM-Prop model, demonstrating how the LLM's performance on the band gap prediction is affected by various modifications to the textual descriptions of material crystal structures.\nAfter adding distractive information to the material descriptions, the LLM-Prop model showed negligible degradation, indicating this application-specific model can effectively differentiate relevant from irrelevant information. This resilience, likely due to the targeted training and fine-tuning on domain-specific texts, enables it to focus on key features for band gap prediction. This showcases the potential noise-filtering capabilities of the trained and fine-tuned transformer models, which traditional ML models may suffer from.\nThe impact of sentence reordering increased the MAE by 12.9%, suggesting the model's reliance on the structured descriptions for accurate predictions. From the previous study on MSE-MCQs degradation, the effect of sentence reordering was less significant, indicating that larger general LLMs, which are trained with more various texts, can exhibit better contextual understanding and are less prone to order changes. The presence of superfluous information, particularly an additional sentence from another material's description, leads to a 39% increase in MAE. This substantial degradation indicates that while the model can filter out irrelevant distractive noise, it struggles considerably when faced with data that is contextually relevant to the specific prediction task.\nTo further assess the model's robustness and determine which description elements are essential for prediction accuracy, we conducted an ablation study that involves altering the structures and lengths of the input description. As shown in Figure 7, the description length is expressed as percent sentence inclusion, ranging from 10% to 100% and MAE is used as a measure of prediction accuracy.\nWhen the number of description sentences is incrementally increased, the MAE rapidly decreases as more information is provided, and optimized at 100% sentence inclusion. Interestingly, in the other configurations, the initial MAEs with just 10% sentence inclusion are notably lower and some can double the performance of the original setting. This indicates that the initial sentences may not contain the most useful information for prediction. The MAEs begin to converge around 50% sentence inclusion, beyond which differences become statistically insignificant. Notably, in the random order setting, there is virtually no variation in the MAE when incorporating three different sentence shuffles. This suggests that LLM-Prop can effectively extract key information and deliver consistent predictions, despite variations in the sentence order.\nBy 40% sentence inclusion, the reversed order yields the lowest MAE, indicating that sentences at the end of descriptions contain crucial information. However, by 50% sentence inclusion, the performance of the reversed order begins to align with that of the random order. This alignment suggests that central information may not be as crucial for prediction accuracy, especially as the random order likely incorporates more of the initial sentences than the reversed order at this inclusion level.\nBased on these insights, we developed the sides-to-middle approach, aiming to prioritize information at the beginning and the end. The approach shows the most significant improvement, achieving the lowest MAE between 40% and 70% sentence inclusion among all. The error continues to decrease and is optimized at full sentence inclusion being only 5.8% higher than the original setting in MAE. This implies that the contextual framing provided by the beginning and end of the description holds considerable importance for the model's predictive accuracy, yet the model is still best optimized for the original order.\nThis ablation study showcases that the fine-tuned model can perform effectively even when provided with significantly reduced prompts. We found that diverging from the training setup (i.e., changing the textual order of the prompt) can sometimes result in improved performance at ablated data volumes. This counterintuitive result suggests that highly templated training or fine-tuning data can lead to unexpected effects. Consequently, this implies two key considerations: 1) training templates should be diverse to prevent models from overfitting to unimportant patterns, and 2) when using a fine-tuned model trained on a specific template, it may not always be optimal to match the template during inference, particularly under ablated data conditions. These insights highlight the potential for optimizing training costs while maintaining performance."}, {"title": "Discussion", "content": "The findings of this study offer crucial insights into the behaviors and limitations of LLMs in materials science domain-specific Q&A and materials property prediction. While the robustness analysis indicates that LLMs can manage certain types of noise with resilience, their performance is significantly challenged under more complex and deceptive conditions, such as when superfluous information is introduced. Prompting techniques (e.g. expert prompting, zero-shot chain-of-thought prompting) can sometimes improve the model performance, not by unlocking new capabilities, but by increasing the probability of following an expected format. During property prediction, in-context learning enables pretrained LLMs to achieve relatively high accuracy in low-data settings when provided with few-shot examples with high proximity to the target. However, the observed mode collapse behavior, where the model generates repeated outputs despite varying inputs, showcases that providing ineffective few-shot examples can cause the model to default to a memorized response rather than conditioning its output on the provided prompt. This study also highlights that fine-tuned models can exhibit enhanced performance under ablated data conditions, when diverging from the training setup, such as altering the textual order. This underscores the risks associated with fixed templating during fine-tuning and suggests that users should be cautious about strictly matching the training templates during inference. This study highlights the challenges and limitations of using LLMs in materials science, aiming to motivate future improvements that enhance their reliability and effectiveness as research tools."}, {"title": "Methods", "content": "The methodology is divided into four subsections that cover the performance evaluation and robustness analysis of LLMs in materials science Q&A and property predictions. In each subsection, we will introduce the models, datasets used, prompting techniques, and evaluation criteria chosen for the specific study."}, {"title": "Performance Evaluation of Materials Science Q&A", "content": "Using MSE-MCQs, we benchmarked a range of both commercial and open-source LLMs, including OpenAI's gpt-3.5-turbo-061332 and gpt-4-061333, alongside Meta AI's Llama variants - llama2-70b, llama2-13b, and llama2-7b34. The suffixes in the Llama model names denote the number of model parameters where, e.g., llama-70b contains 70 billion parameters. This study primarily utilized fixed-version and open-source models to enhance the reproducibility of the findings.\nThe prompting strategies used in this evaluation are zero-shot chain-of-thought (CoT) and expert prompting. Zero-shot CoT prompting uses an instructional prompt to enable the model to \"think aloud\" and step through a reasoning process that leads to an answer, potentially boosting the accuracy in problem-solving tasks35. Expert prompting, i.e. instructing an LLM to act as a domain expert, has been shown to influence the model's responses to be more aligned with expert knowledge and reasoning36. The specific implementations of each for their respective tasks are defined below.\nIn the Q&A evaluation, the expert prompt includes instructions to define the domain of study, introduces the settings of the questions, and emphasizes step-by-step reasoning and calculations. The goal is to improve the LLMs' ability to retrieve domain-specific knowledge, follow the instructions, and correctly perform reasoning and calculations. The expert prompt is shown below:\nThe questions were fed to LLMs in two settings: 1) as is, to benchmark the LLM's level of understanding of materials science domain-specific knowledge and problem-solving; 2) to evaluate the aforementioned prompting strategies to enhance the LLM performance. Given the lengthy reasoning in the answers and the potential for errors in manual verification, we used the gpt-4-0613 API in a separate client to extract and assess responses automatically. This system compared answers to the provided correct choices, generating a simple binary score (1 for correct, 0 for incorrect) without evaluating the reasoning steps. Finally, the average accuracy of each category was calculated and reported. When selectively compared to manual checks (>200 answers), the method was found to be reliable, consistently identifying correct answers with over 95% accuracy."}, {"title": "Performance Evaluation of Materials Property Prediction", "content": "For this study, we used the matbench_steels dataset to evaluate the predictive capabilities of pretrained LLMs, using their ICL capability with few-shot examples. We benchmarked llama2-7b, llama2-13b, mistral-7b37, gpt-3.5-turbo-0613, and gpt-4-0613 on their abilities to predict the yield strengths given the steel compositions. For performance comparison, a KNN and an RFR"}, {"title": "Robustness Analysis of Materials Science Q&A", "content": "To evaluate the robustness of LLMs for materials science Q&A, we continued using the MSE-MCQs dataset and tested the gpt-3.5-turbo-0613 model to observe how textual perturbations impacted performance. In this study, no prompting strategy was implemented, and the questions were directly used as the user prompts. We identified different types of \"noise\" that can be introduced to the MCQs to evaluate the robustness of LLMs. As shown in Table 2, the textual inputs were modified systematically in five different ways, i.e. 1) unit mixing, 2) sentence reordering, 3) synonym replacement, 4) distractive info, and 5) superfluous info."}, {"title": "Robustness Analysis of Materials Property Prediction", "content": "In materials property prediction, we selected the LLM-Prop model along with its associated band gap dataset. LLM-Prop is a fine-tuned T5 model, topped with a linear layer, designed to predict materials properties from crystal structure descriptions generated using Robocrystallographer14,39,40.\nThe material descriptions underwent systematic modifications mirroring those applied in the Q&A evaluations, except for unit mixing and synonym replacement. During data preprocessing for LLM-Prop, all numerical values and units, such as bonding distances and angles, are replaced with a [NUM] token, to emphasize the model's focus on text-based understanding14. Unit mixing might disrupt the preprocessing algorithm, and thus was excluded from the analysis. Synonym replacement was excluded because the original terminology was already highly specific and lacked equivalent synonyms. Furthermore, we conducted an ablation study of textual degradation to examine the model's resilience against structural and length variations in the input data, as well as to explore which aspects of the descriptions the model relies on for predictions. We manipulated the order and fraction of sentences included, testing configurations including 1) original order, which prioritizes the initial information in a description, 2) reverse order, which prioritizes the sentences from the end of a description, 3) random order, shuffling the information, and 4) sides-to-middle, which deprioritized central information. The impact of these textual degradations was quantitatively assessed by measuring the resultant prediction error in MAE."}]}