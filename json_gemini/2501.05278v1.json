{"title": "Off-Policy Evaluation and Counterfactual Methods in Dynamic Auction Environments", "authors": ["Ritam Guha", "Nilavra Pathak"], "abstract": "Counterfactual estimators are critical for learning and refining policies using logged data, a process known as Off-Policy Evaluation (OPE). OPE allows researchers to assess new policies without costly experiments, speeding up the evaluation process. Online experimental methods, such as A/B tests, are effective but often slow, thus delaying the policy selection and optimization process. In this work, we explore the application of OPE methods in the context of resource allocation in dynamic auction environments. Given the competitive nature of environments where rapid decision-making is crucial for gaining a competitive edge, the ability to quickly and accurately assess algorithmic performance is essential. By utilizing counterfactual estimators as a preliminary step before conducting A/B tests, we aim to streamline the evaluation process, reduce the time and resources required for experimentation, and enhance confidence in the chosen policies. Our investigation focuses on the feasibility and effectiveness of using these estimators to predict the outcomes of potential resource allocation strategies, evaluate their performance, and facilitate more informed decision-making in policy selection. Motivated by the outcomes of our initial study, we envision an advanced analytics system designed to seamlessly and dynamically assess new resource allocation strategies and policies.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern-day marketplaces are increasingly designed as two-sided platforms. In such marketplaces, the goal is to maximize the utility on both the buyers and the sellers. This in turn enhances the overall revenue generated within the marketplace, and is critical in ensuring the long-term viability and competitiveness of the platform. Marketplaces typically implement complex auction mechanisms to achieve these goals. Unlike traditional auctions, that focus solely on price-based allocation, modern auctions consider a variety of factors. It includes the strategic behaviors of both buyers and sellers, the dynamic nature of their payment policies, and the overall impact on marketplace revenue [1], [2]. Key industrial applications of these advanced auction mechanisms include Energy Markets, Telecommunications, Digital Marketing and many others.\nGiven the complexity of these auctions, payment policies have also become increasingly dynamic [2]. Validating a new payment policy in the real-world settings presents significant challenges. A/B testing is commonly used for online evaluation of the new policies, which is both expensive and time-consuming. Moreover, in user-facing commercial applications, A/B tests can risk exposing users to poor-performing policies, which can directly harm the application's revenue. As an alternative, simulation-based learning has been proposed as a new direction for evaluation [3]. However, these systems are still in their infancy, often relying on a deep understanding of the auction environment or on assumptions that may not hold true in real-world scenarios. While offline validation has its drawbacks, particularly the necessity of logged data, it has demonstrated significant advantages in applications such as recommendation and ranking. This method, known as Off-Policy Evaluation (OPE) [4], is employed in reinforcement learning and decision-making contexts to estimate the performance of a new policy using historical data collected under a different policy. Essentially, OPE predicts how well a new or hypothetical strategy would perform based on data generated by an existing strategy. In industrial settings, particularly in recommendation and ranking systems, OPE provides a practical and efficient alternative to traditional online evaluation methods.\nIn this paper, we explore the limitations and potential of using OPE in a dynamic auction environment. Our goal is to develop a measurement system capable of evaluating the payment policies using existing data alone, thereby enabling us to assess the feasibility of improving performance by testing new policies. To evaluate the viability of this approach, our study addresses the following key research questions:\n\u2022\tConduct a comprehensive analysis of discrete versus continuous policy evaluation methods for dynamic competitive auctions.\n\u2022\tAssess the feasibility of identifying the best-performing policy among three options based on the outcomes of two prior tests.\n\u2022\tLearning optimal policies through optimization of policy estimators and evaluating their effectiveness via simulation."}, {"title": "II. OVERVIEW OF THE APPROACH", "content": "In this section we outline the approach for using the proposed off-policy evaluation method. \n\n**A. Problem Description**\n\nIn this dynamic auction environment, resources are considered to be theoretically infinite, and an indeterminate number of agents compete to acquire them. Each agent assigns a personal value to the resources and aims to maximize their utility through participation. Agents operate according to a payment policy (\u03c0), which influences their willingness to pay to acquire the resources for which they are competing.\nThe determination of the winning agent involves considering both the payment amount and the potential revenue generated from the allocation [2]. Winning the auction incurs costs for the agent, related to the resources expended. The effectiveness of the allocation is evaluated through metrics such as reach, resources, and returns Reach measures the impact or extent of the allocation, this is analogous to views in marketing. Resources denote the quantity of units allocated after conversion, such as purchases, download, etc. Returns reflect the overall utility of winning the allocation.\nAgents have the flexibility to design their payment policies to optimize their performance based on these metrics. This allows them to balance the costs incurred with the expected benefits derived from the allocated resources.\n\n**B. Evaluation Pipeline**\n\nWe adopt our pipeline based on the Open-Bandit Modules [4] but adapt it for our use-cases. The only controllable units in this case are the payment policies. An overview of the pipeline is provided in the Figure 1. The pipeline consists of the following components:\n\u2022\tAction Space: In our problem, the action space consists of payment policies, which are continuous variables. For non-continuous evaluators, we discretize them by grouping them into bins.\n\u2022\tPolicy Modeling: Instead of directly using the original payment policy, we identified proxies by training Random Forest Classifiers (for discrete action space) / Regressors (for continuous action space) on the collected data. This approach is both generalizable and data-driven, making it applicable to any test scenario, regardless of the underlying model.\n\u2022\tReward Modeling: A reward model is required for some of the metrics outlined in Section II-C. In these cases, we employed a Random Forest Regressor to model the reward.\n\u2022\tPolicy Evaluation: Finally, we compare the rewards predicted by the OPE methods with the actual rewards observed during the A/B tests to assess the accuracy of the estimators. Our experiments involve comparing a wide range of discrete and continuous evaluators. The evaluators used in these comparisons are listed in Section II-C. The OPEs consist of some tunable hyperparameters which are learned to correct the distribution mismatch between the pre-tested policy and the new candidate policy. The data collected during the past A/B tests is used for this task.\n\u2022\tOff-Policy Learning: Off-policy learning is the approach where an agent learns about an optimal policy based on the off-policy evaluator. Due to the dynamic nature of the policies and the complex underlying probability associated with them, we learn proxy payment models for simulation. We elaborate more about the process in Section II-D.\n\n**C. Off-Policy Evaluation**\n\nThe main idea behind using OPE in dynamic environments is to hypothetically test the performance of different payment policies offline before selecting one for the final A/B test. This additional evaluation allows us to identify the most promising payment policies from various options, thereby reducing the need for multiple"}, {"title": "III. EXPERIMENTAL EVALUATION", "content": "In our initial study, we pose two different research questions:\n\u2022\tGiven two separate A/B Tests, conducted under different conditions, can we determine the best policy?\nWe consider two prior A/B tests as the basis of our evaluation which we call Test-1 and Test-2. The results, as evaluated in the two tests, are provided in Fig 3. At first glance, it seems that the Policy Z had the better lift, but Test-1 and Test-2 are conducted under separate conditions. Thus it is difficult to say whether Z is truly better than Y. Thus our initial goal is to simulate under similar conditions which one is better in a simulated scenario of Y vs Z.\n\u2022\tCan we learn an optimal policy from the data itself ?\nAfter assessing the suitability of OPE, we sought to explore counterfactual scenarios to determine what the optimal payment policy might have been in the past. Based on this analysis, we aim to learn an optimal policy by optimizing over the evaluation metrics.\n\n**A. Comparing Discrete vs Continuous case for payment**\n\nWe need to compare the discretized action space and continuous action space to check how much accuracy we are gaining in terms of different metrics for moving to the continuous version. Both approaches are compared with respect to the Mean Absolute Percentage Error (MAPE) [14] for different metrics in the Test-1 and Test-2 experiments. The MAPEs aggregated over the OPE estimators for both tests are shown in Figure 4.\nObservation- Continuous OPE tends to outperform discretized OPE. This is intuitive because continuous OPE can capture the smooth, nuanced dependencies of the key metrics across small changes in payment values, providing a more detailed and accurate evaluation of policies. In contrast, discretized OPE may miss these\n\n**B. Comparison of Continuous Version of the Estimators**\n\nAfter verifying that continuous evaluators work better than the discrete evaluators, we evaluate the continuous version of the estimators mentioned in Section II-C. In Figure 5, we show the results of the top four evaluators for both tests.\nObservation- SNDR works the best in our case. The optimal kernel and bandwidth for the estimator can be determined through a separate hyperparameter tuning experiment. As more data is aggregated and used to predict overall rewards, the uncertainty in these predictions diminishes. For instance, predicting the total number of resources acquired over a week is more reliable than predicting at a daily grain. This reduction in uncertainty with larger aggregated data highlights the importance of selecting appropriate hyperparameters to acquire accurate predictions.\n\n**C. Counterfactual Test of Policies: Y vs Z**\n\n1) Use Case I: Assessing the Accuracy of OPE: To assess the accuracy of the learning proxies and the OPE, we conducted an evaluation the Test-3, comparing policies Y and Z. Following this, we constructed a Proxy Policy Y based on the past data collected for Policy Y. We then replaced Proxy Policy Y with Policy X in Test-2 which we refer to as Counterfactual Test-2 and compared the outcomes with the original Test-3 results, as illustrated in Figure 6.\nTakeaways - Directional Lifts are Correlated: For the mentioned metrics we were able to achieve similar directional lifts which shows that the OPEs were able to estimate the proper directional lifts for Policy Y through Proxy Policy Y. Improving the learning algorithms can potentially lead to accurate reward prediction and thus effective policy evaluation.\n\n2) Use Case II: Estimating the Impact of Policy Z in Test-1 in Place of Policy X, and Comparison with Counterfactual Test-2: After verifying that the OPEs are able to accurately predict the directional lifts for a counterfactual test, our next objective is to utilize the counterfactual evaluation process to compare different payment policies in different contexts without doing actual A/B tests. In the Test-1, Policy Y showed negative lifts in reach, resources, and returns, while in the Test-2, Policy Z demonstrated significant lifts in resources acquired and the subsequent returns. These results suggest that Policy Z is likely to outperform Policy Y in a direct comparison.\nTo create a hypothetical evaluation, named as Counterfactual Test-1, we simulated a scenario where a proxy version of Policy Z (Z) replaced Policy X as the control in the Test-1. The treatment was set to Policy Y. The resulting lifts for Counterfactual Test-1 and Counterfactual Test-2 scenarios are compared in Fig 7. In Counterfactual Test-1, the lifts indicate that Proxy Policy Z outperforms Policy Y across all KPIs. In Counterfactual Test-2, although Policy Z shows a slight negative lift in Reach, this is offset by substantial positive lifts in Resources and Returns.\nInsights - Robustness of Policy Z: The consistency in performance of Policy Z across both tests suggests that it is generally a more robust choice for improving key metrics.\n\n**D. New Policy Learning through OPE Optimization**\n\nFinally, we conducted new model learning to identify the optimal policy in retrospect. For our use case, we selected profit maximization as the objective function. As a standard practice in machine learning, the negative of the profit maximization is used as the loss function, defined as:\n\n$min\\ f = (Returns_{OPE}(P) - Cost_{OPE}(P))$,\nwhere $p = OptPaL(C)$"}, {"title": "IV. CONCLUSION", "content": "In this study, we have demonstrated the effectiveness of off-policy evaluation (OPE) techniques in refining payment strategies by leveraging counterfactual scenarios. Our results underscore the advantages of continuous OPE methods over traditional discretized approaches, providing more precise and nuanced insights into policy performance. Through counterfactual policy evaluation, we are able to estimate directional lifts for new policies, offering a preliminary assessment of their utility prior to conducting A/B tests. Furthermore, we developed optimal policies by optimizing the continuous estima-"}, {"title": "V. FUTURE VISION", "content": "The future of policy evaluation envisions a fully automated system designed to seamlessly and dynamically assess new payment strategies and policies. This system will integrate scalable engineering and advanced methodologies to deliver continuous, accurate, and actionable insights. We provide a description of the system design of the Advanced Analytics Platform in the Figure 9.\nThe foundational components of the system will include:\n\u2022\tRetrospective Policy Evaluation: This component focuses on learning the optimal policy in hindsight, allowing us to determine the best course of action based on past data. It involves simulating outcomes of new algorithmic policies, offering foresight into potential impacts and enabling the evaluation of policies before deployment. This subsystem will help decide whether a new proposed policy should undergo A/B testing.\n\u2022\tOptimal Policy Learning: Aims to generate an optimal policy from historical data. This will help us compare and analyze the regret of past actions.\n\u2022\tComprehensive Monitoring and Insights: The system will provide intuitive and actionable insights into policy performance, trends, and recommendations, enabling decision-makers to understand and respond effectively. Integrating Large-Language Models will further refine the learning process and enhance actionable insights.\nOur next research direction will focus on enhancing proxy policy learning and exploring Structural Causal Model-Based Counterfactual Evaluation [15] to improve the accuracy of policy assessments. Additionally, we will consider large dimensional action spaces [16] to better account for complex auction environments."}, {"title": "APPENDIX", "content": "A. Evaluators Detailed Description\n1) Inverse Probability Weighting (IPW): IPW estimates the policy value of evaluation policy(\u03c0\u03b5) [5], [6], [7]:\n\n$V_{IPW}(\\pi_e; D) := E_n[w(x_i, a_i) \\cdot r_i]$ (1)\n\nwhere $D = \\{(x_i,a_i, r_i)\\}_{i=1}^n$ is logged bandit data with n observations collected by behavior policy \u03c0b; $w(x,a) := \\pi_e(a|x)/\\pi_b(a|x)$ is the importance weight given x and a. $E_n[]$ is the empirical average over n observations in D. When the clipping is applied, a large importance weight is clipped as $w(x,a) := min\\{\\lambda, w(x,a)\\}$ where \u03bb(> 0) is a hyperparameter to specify a maximum allowed importance weight. IPW re-weights the rewards by the ratio of the evaluation policy and behavior policy (importance weight). When the behavior policy is known, IPW is unbiased and consistent for the true policy value. However, it can have a large variance, especially when the evaluation policy significantly deviates from the behavior policy.\n2) Self-Normalized Inverse Probability Weighting (SNIPW): SNIPW estimates the policy value of evaluation policy (\u03c0\u03b5) [8], [9] as\n\n$V_{SNIPW}(\\pi_e; D) := \\frac{E_n [w(x_i, a_i) r_i]}{E_n [w(x_i, a_i)]}$ (2)\n\nSNIPW normalizes the observed rewards by the self-normalized importance weight. This estimator is not unbiased even when the behavior policy is known. However, it is still consistent for the true policy value and gains some stability in OPE.\n3) Direct Method (DM): DM [6], [10] first trains a supervised ML model, such as ridge regression and gradient boosting, to estimate the reward function $q(x, a) = E[r|x, a]$. It then uses the estimated rewards to estimate the policy value as follows.\n\n$V_{DM}(\\pi_e; D, \\hat{q}) := E_n[ \\sum_{a \\in A} \\hat{q}(x_i, a) \\pi_e(a|x_i)] = E_n [\\hat{q}(x_i, \\pi_e)] $ (3)\n\n$\\hat{q}(x, a)$ is the estimated expected reward given x and a. $\\hat{q}(x_i, \\pi_e) := E_{a \\sim \\pi_e(a|x)} [\\hat{q}(x,a)]$ is the expectation of the estimated reward function over \u03c0. If the regression model (q) is a good approximation to the true mean reward function, this estimator accurately estimates the policy value of the evaluation policy. If the regression function fails to approximate the reward function well, however, the final estimator is no longer consistent.\n4) Doubly Robust (DR): Similar to DM, DR [6], [11], [12], [7], estimates the reward function ( $\\hat{q}(x,a) = E[r|x, a]$ ). It then uses the estimated rewards to estimate the policy value as follows.\n\n$V_{DR}(\\pi_e; D, \\hat{q}) := E_n[\\hat{q}(x_i, \\pi_e) + w(x_i, a_i)(r_i - \\hat{q}(x_i, a_i)] $ (4)\n\nWhen the clipping is applied, a large importance weight is clipped as $\\hat{w}(x, a) := min\\{\\lambda, w(x,a)\\}$ where \u03bb(> 0) is a hyperparameter to specify a maximum allowed importance weight. DR mimics IPW to use a weighted version of rewards, but DR also uses the estimated mean reward function (the regression model) as a control variate to decrease the variance. It preserves the consistency of IPW if either the importance weight or the mean reward estimator is accurate (a property called double robustness). Moreover, DR is semi-parametric efficient when the mean reward estimator is correctly specified.\n5) Self-Normalized Doubly Robust (SNDR): Similar to SNIPW, the SNDR estimator applies the self-normalized importance weighting technique to gain some stability. The SNDR estimator computes the policy value of the evaluation policy \u03c0e as\n\n$V_{SNDR}(\\pi_e; D, \\hat{q}) := E_n[\\hat{q}(x_i, \\pi_e) +  \\frac{w(x_i, a_i)(r_i - \\hat{q}(x_i, a_i))]}{E_n [w(x_i, a_i)]}$ (5)\n\n6) Continuous Evaluators: In traditional discretized off-policy evaluators, significant drawbacks arise due to the loss of information when continuous values are binned together. This binning process can obscure subtle differences within the data, making it challenging to evaluate small changes, as these nuances are often lost within the bins. Furthermore, discretized approaches rely on ad-hoc modeling of the policies, which may not accurately reflect the underlying continuous nature of the data.\nIn contrast, continuous off-policy evaluation (OPE) is more appropriate for contexts with continuous treatment spaces. To address these challenges, we employ Multivariate Kernel Density Estimation (KDE) to model the conditional probability density of the continuous variables (e.g., payment, denoted as b) with respect to the context (denoted as x):\n\n$P(T = b|X = x) = \\frac{P(T = b, X = x)}{P(X = x)}$\nOnce we model the probability densities, different kernels are applied to map the differences between"}]}