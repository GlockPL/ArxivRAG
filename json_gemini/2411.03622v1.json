{"title": "Fully Hyperbolic Rotation for Knowledge Graph Embedding", "authors": ["Qiuyu Liang", "Weihua Wang", "Feilong Bao", "Guanglai Gao"], "abstract": "Hyperbolic rotation is commonly used to effectively model knowledge graphs and their inherent hierarchies. However, existing hyperbolic rotation models rely on logarithmic and exponential mappings for feature transformation. These models only project data features into hyperbolic space for rotation, limiting their ability to fully exploit the hyperbolic space. To address this problem, we propose a novel fully hyperbolic model designed for knowledge graph embedding. Instead of feature mappings, we define the model directly in hyperbolic space with the Lorentz model. Our model considers each relation in knowledge graphs as a Lorentz rotation from the head entity to the tail entity. We adopt the Lorentzian version distance as the scoring function for measuring the plausibility of triplets. Extensive results on standard knowledge graph completion benchmarks demonstrated that our model achieves competitive results with fewer parameters. In addition, our model get the state-of-the-art performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and challenging than before. Our code is available at https://github.com/llqy123/FHRE.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) store a vast amount of facts in the form of triplets, which provide a rich external semantic knowledge base for many artificial intelligence tasks, such as word sense disambiguation [1], question answering [9] and recommendation systems [11]. Due to the diverse range of applications, representation learning for knowledge graphs has attracted significant interest from researchers.\nThe construction of knowledge graphs typically involves automatically extracting triplets from multiple sources of knowledge to form a graphical representation. They usually comprise numerous entities, relations and an extensive collection of triplets. However, since these KGs are automatically extracted from the knowledge sources, they are incomplete [18]. In the context of large-scale knowledge graphs, it is impractical to complete missing facts manually. To address this challenge, researchers have proposed several algorithms for automatic filling, Knowledge Graph Completion (KGC), also known as link prediction. Its aim is to the prediction of missing facts using known triples. In KGC, a common approach to predict missing facts in KGs is to embed them into vector spaces, which is called Knowledge Graph Embedding (KGE). The KGE model calculates the valid facts with higher scores through their scoring function."}, {"title": "2 Related Work", "content": "The embedding space of shallow network-based models can be classified into three main categories: Euclidean, Complex and Hyperbolic.\nEuclidean-based embeddings. TransE [5] treated the relation vectors of the knowledge graph as translations from head entities to tail entities. Although TransE is fairly simple and has few parameters, it cannot model knowledge graphs with one-to-many, many-to-one and many-to-many relationships. To overcome these shortcomings, a series of translation models (e.g., TransH [32] and TransR [20]) were proposed. While these models perform excellently, they are unable to model the rich logical patterns (e.g., symmetry, inversion and composition) in knowledge graph. To solve these problems, rotation models based on Euclidean space have been proposed, such as RotE [7] and recent CompoundE [14] model. However, using Euclidean space with a constant curvature of 0, which is a flat space, is insufficient to accurately model the hierarchical structure of a knowledge graph. Thus resulting in distorted data.\nComplex-based embeddings. In order to improve the representation of space, researchers have tried to model knowledge graphs in complex spaces. Entities and relations in complex space consist of real and imaginary parts. For example, RotatE [26] defined each relation in complex space as a rotation from the head entity to the tail entity. QuatE [37] explored embeddings in hypercomplex spaces and used Hamilton products for rotation operations. Rotate4D [18] decomposed the relation into unit vectors to perform 4D rotations in a hypercomplex space. Although these models achieve better results, one drawback of these embeddings is that they often require high-dimensional spaces and then increased memory costs.\nHyperbolic-based embeddings. In recent years, due to the development of hyperbolic geometry in the field of artificial intelligence, more and more researchers use the properties of hyperbolic geometry to model the knowledge graph. For example, MuRP [3] was the first to focus on the hierarchical structure of KGs and transformed entity embeddings by learning relation-specific parameters via Mobius matrix vector multiplication and Mobius addition. RotH [7] performed rotation modelling knowledge mapping in hyperbolic space. HBE [24] used the extended Poincar\u00e9 ball and polar coordinate system to capture hierarchies. FFTRotH [33] further enhanced the RotH model by a Fast Fourier Transform. The latest model COPE [35] learned embeddings using the Poincar\u00e9 ball of hyperbolic geometry to preserve the hierarchy between entities. Although these models address the issue of hierarchical structure, they only project data features into hyperbolic space, limiting their ability to fully leverage the potential of hyperbolic space. Thus resulting in a lack of performance in knowledge graph embedding."}, {"title": "3 Problem Formulation and Preliminaries", "content": "In this section, we first introduce the definition of knowledge graph completion task, and then provides a brief background of the background knowledge of hyperbolic geometry. Table 1 provides a summary of the main mathematical symbols utilized throughout the paper."}, {"title": "3.1 Knowledge graph completion", "content": "For a knowledge graph G, we represent each piece of data in G in the form of a triplet (h, r, t). In the task of knowledge graph completion, there are three main categories: head entity completion (?, r, t), tail entity completion (h, r, ?) and relation completion (h, ?, t). Our paper will focus on head or tail completion, as is the case in most previous work. This is because models require relational information to be trained."}, {"title": "3.2 Hyperbolic geometry", "content": "Hyperbolic geometry is a non-Euclidean geometry with constant negative curvature c. A lower value of c typically indicates a more curved surface. Previous research has demonstrated the effectiveness of hyperbolic geometries, such as the Poincar\u00e9 ball [12] and the Lorentz model [23], in the field of natural language processing. Our model is based on the Lorentz model due to its simplicity and numerical stability [21].\nLorenz model. The Lorentz model of n-dimensional hyperbolic space is defined as the Riemannian manifold $L = (H^n, g_e)$, where $g_e = diag(-1, 1, . . ., 1)$ is the Riemannian metric tensor and manifold $H^n$ satisfies:\n$x \\in \\mathbb{R}^{n+1} : \\langle x, x \\rangle = -1,$\nwhere $\\langle , \\rangle$ is Lorentzian scalar product. Given $x, y \\in \\mathbb{R}^{n+1}$, where n is the space dimension. Formally, the Lorentzian scalar product is"}, {"title": "4 Methodology", "content": "In this section, we will introduce the detail of our model. Our model is comprises of three key components:\n\u2022 Initialization: Our model initializes entity and relation embeddings in hyperbolic space.\n\u2022 Lorentz rotation: Our model treats each relation as a rotation that from the head entity to the corresponding candidate tail entity. The rotation guided the head entities to stay closer with tail entities. The rotation operations are performed fully in a hyperbolic space.\n\u2022 Scoring function: The prediction step calculates the scoring function to measure the plausibility of a triplet. We propose the Lorentzian distance as our scoring function."}, {"title": "4.1 Initialization", "content": "To obtain the embedding in the hyperbolic space, we randomly initialize the head and tail entity embedding parameters in tangent space, and initialize parameterized relation embedding as $\\theta_r$. Then we map these entity embeddings to hyperbolic space using exponential mapping (Equation 4) and get their embedding of the head entities $v_h$ and tail entities $v_t$ in the hyperbolic space. It is noting that our model only requires once spatial transformation during the initialization process and does not depend on mapping with other parts of the process."}, {"title": "4.2 Lorentz, rotation", "content": "The use of rotational transformation has been proven to efficiently encode complex logical structures in knowledge graphs, including symmetric, anti-symmetric, inverse and compositional relations [7, 13, 26, 33].\nAs an example, in Figure 2, we show rotational transformations based on different spaces. Figure 2(a) shows the rotation in Euclidean space. The rotational transformation of an entity does not depend on hyperbolic space. Figure 2(b) illustrates the rotation in hyperbolic space. To perform a rotational transformation in hyperbolic space, an entity in tangent space (Euclidean space) must be mapped into hyperbolic space by exponential mapping. This is followed by a mapping of the transformed entities back into tangent space by logarithmic mapping for training purposes. Please note that the same conversion process must be applied to each batch of data.\nFigure 2(c) illustrates the rotation in full hyperbolic space, which is the method we propose. The initialized data is projected into hyperbolic space by an exponential mapping function, without relying"}, {"title": "4.3 Scoring function", "content": "Similar to previous studies [3, 7, 33], we adopt the distance function as our scoring function to measure the plausibility of triplets. The score for each triplet (h, r, t) is defined as:\n$s(h, r, t) = d(v_h', v_t) + b_h + b_t,$\nwhere $d( , )$ is the Lorentzian distance (Equation 6), $v_h'$ is the head entity after Lorentz rotation (Equation 10), $v_t$ is the hyperbolic embedding of the tail entity, $b_h$ and $b_t$ are entity biases which act as margins in the scoring function [7, 27]."}, {"title": "4.4 Loss function", "content": "Following with previous work [3, 7, 8], for each triplet, we randomly corrupt its head or tail entity with k entities and compute the probability of the triplet as $p = \\sigma(s(h, r, t))$, where $\\sigma$ is the sigmoid function. Finally, we train our model by minimizing the binary cross entropy loss:\n$loss = -\\frac{1}{N} \\sum_{i=1}^{N} (log(p^{(i)}) + \\sum_{j=1}^{k} log(1 - p^{(i,j)})),$\nwhere N is the number of training set triplets, $p^{(i)}$ and $p^{(i,j)}$ are the probabilities of correct and incorrect triplets, respectively."}, {"title": "5 Experiments", "content": "5.1 Experimental settings\nDatasets. To evaluate the effectiveness of our model, we used two frequently benchmark datasets, FB15k-237 [29] and WN18RR [10]. The FB15k-237 and WN18RR datasets are subsets of the FB15k [5] and WN18 [10] datasets, respectively. They were created to address the issue of reversible relations, and enabling more realistic predictions.\nFurthermore, we also validated the robustness of our model on CoDEx-s and CoDEx-m [25]. The CoDEx-s and CoDE-m datasets were proposed in [25] to enlarge the KG scope and improve the level of KG difficulty. This dataset includes three knowledge graphs with different sizes and structures. Importantly, it contains thousands of hard negative triples that are plausible but verified to be false. These two datasets are more diverse and interpretable benchmarks. Therefore, the CoDEx dataset is a more difficult link prediction benchmark than FB15k-237.\nFinally, we also evaluated our model on Nations [15] dataset. In this dataset, the number of entities is smaller than the number of relation. Table 2 provides statistics for all datasets. For a fair comparison, we used the same partition of train, valid and test with other work.\nEvaluation Protocols. Similar to previous work [7, 8, 17], we augment all datasets by adding inverse relations to each triplet. In other words, we add an additional triplet (t, r\u207b\u00b9, h) for every (h, r, t). We adopt the Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3 or 10) as evaluation metrics. Higher MRR and Hits@k values on the valid set indicate better model performance. The final scores on the test set are obtained from the best validation model, which achieved the highest MRR on the validation set.\nBaselines. In the low-dimensional experiments, we compared with 10 baseline models. These models can be classified into three categories depending on the embedding space:\n\u2022 Euclidean space: MuRE [3] transformed entity embeddings by learning relation-specific parameters in Euclidean space. RotE [7] performed rotation operations in Euclidean space. Rot2L [31] modelled knowledge graphs in Euclidean space using a double-layer superposition transformation. SAttLE-Tucker [2] utilized a large number of self-attention heads as the key to applying query-dependent projections to capture mutual information between entities and relations.\n\u2022 Complex space: RotatE [26] and ComplEx-N3 [17] modelled knowledge graphs in complex space to improve their expression ability.\n\u2022 Hyperbolic space: MuRP [3] transformed entity embeddings by learning relation-specific parameters in Poincar\u00e9 ball. RotH [7] performed rotation operations in hyperbolic space. FFTRotH [33] improved RotH model with Fast Fourier Transform. HYBONET [8] performed a Lorentz linear transformation on each triplet in hyperbolic space. UltraE [34] presented an ultra-hyperbolic KG embedding method that interleaves hyperbolic and spherical manifolds. Note that MuRP, RotH, FFTRotH and UltraE models rely on spatial mappings between hyperbolic space and their tangent space."}, {"title": "5.2 Results", "content": "The spatial accommodation capacity of hyperbolic spaces grows exponentially, which means that models based on hyperbolic spaces"}, {"title": "5.2.1 Low-dimensional embedding experiments", "content": "Table 3 demonstrates that our model outperforms all other competitors on the FB15k-237 and WN18RR datasets. In the same dimensions (d = 32), our FHRE outperforms the Euclidean space-based model Rot2L by an average of 6.1% and 3.7% on the FB15k-237 and WN18RR datasets, respectively. We argue that although Rot2L also uses rotations to model knowledge graphs, our FHRE is superior to it. Our rotation was operated in hyperbolic space, which allows FHRE to capture hierarchical structures more efficiently.\nIn comparison to models based on hyperbolic spaces, both RotH and FFTRotH utilize hyperbolic rotations to model the knowledge graph. However, these models project data features into hyperbolic space for rotational transformation during training, which limits their ability to fully exploit the hyperbolic space and results in poor performance. Our model is based on a fully hyperbolic space and does not rely on frequent spatial transformations and thus shows excellent performance on both datasets. We even outperform the UltraE model since it learned the embeddings with an ultrahyperbolic manifold.\nAlthough HYBONET [8] performs a Lorentzian linear transformation in hyperbolic space, the linear transformation introduces additional parameters. Our analysis is as follows: given the same embedding dimension d, HYBONET has ($|V| \u00d7 d + |R| \u00d7 d \u00d7 d$) parameters, while our model has ($|V| \u00d7 d + |R| \u00d7 d$) parameters, where V and R are the set of entities and relations, respectively. Therefore, according to the linear transformation time complexity calculation method, the time complexity of our model is O(n), while the time complexity of HYBONET is O(n\u00b2). Moreover, our experimental observations have confirmed that rotational transformations are more expressive than linear transformations for modelling knowledge graphs. We also observed that increasing the number of parameters does not necessarily result in a corresponding performance improvement."}, {"title": "5.2.2 High-dimensional embedding experiments", "content": "Table 5 illustrates that our model achieve superior performance compared to other models with high embedding dimension, particularly on the FB15k-237 dataset. Although the HAKE utilized polar coordinates to model the hierarchical structure of the knowledge graph, it is modelled in Euclidean space and cannot fully capture the hierarchical structure. Our model even exceed in the mixed space model GIE. GIE combined Euclidean, hyperbolic and spherical spaces to form a new interaction space when modeling knowledge graphs. However, GIE encountered spatial transformation issues.\nWhile our metrics for the WN18RR model are not optimal in terms of H@1 and H@10, our model has fewer parameters compared to Rotate4D and HYBONET. Table 7 illustrates the number of parameters. For a fair comparison, we fixed the embedding dimension at 500. Table 7 clearly demonstrates that our model's parameters are 21.1% and 74.9% lower than those of the Rotate4D and HYBONET models, respectively."}, {"title": "5.2.3 Robustness and generalization experiments", "content": "To validate the robustness and generalization of our model, we performed experiments on more diverse and challenging datasets of different scales. The table 6 shows that our model achieves better results compared to other models. Compared to CoPE model, our model improved the MRR, H@1 and H@10 metrics by 34.0%, 46.5%, 21.2%, 19.9%, 25.8%, 15.0%, 5.9% and 11.1% on the CoDEx-s, CoDEx-s and Nations datasets. Although CoPE employed the Poincar\u00e9 ball model of hyperbolic space to preserve the hierarchies between entities, the CoPE encountered frequent spatial mapping problems.\nOur model outperforms deep network models such as NOGE, achieving better results. We believe that NoGE, based on deep neural networks, is susceptible to over-fitting issues. Furthermore, as shown in Table 6, the experiments demonstrate that our model maintains excellent performance in the face of more diverse and challenging datasets, which verifies the robustness of our model and its ability to perform well in a variety of situations."}, {"title": "5.3 Exploring the multi-relations", "content": "To assess the effectiveness of our model when handling with multi-relation(1-to-N, N-to-1 and N-to-N) between entities. According to the calculation rules in [5], we divided the test set of FB15k-237 into four categories: 1-to-1, 1-to-N, N-to-1 and N-to-N. \"1-to-N\" means that a head entity can form a fact triplet with multiple tail entities. \"N-to-1\" means that a tail entity can form a fact triplet with multiple head entities. The division results are shown in Table 8, where nh and nt represent the average degree of head and tail entities, respectively.\nWe compared our model with HYBONET[8] model when fixed the embedding dimension for entities and relations at 32. For a fair comparison, we reproduced the HYBONET with the optimal hyper-parameter settings from their paper, and remained other setting same with ours. The experiment results are shown in Table 9."}, {"title": "5.4 Visualizations of Rotation in hyperbolic space", "content": "To verify the impact of rotations entirely in hyperbolic space, we visualized the same embedding after rotation under performed our model and RotH model on the WN18RR dataset. The dimensions of embeddings are 32. We selected the first 1000 triplet instances associated with the relation \"_derivationally_related_form\" and used the t-SNE method to reduce the dimension of head and tail entity embeddings. We then projected them onto a polar coordinate system for visualization. With this relation, entities expects to be closer and together.\nFigure 3 shows the embedded visualisations. In Figure 3(a), RotH performs rotation in hyperbolic space. In Figure 3(b), our model performs rotation in fully hyperbolic space. It is evident that the entity in our model is closely associated with its associated entities. However, in the RotH, the entity is relatively isolated from other entities. This discrepancy can be attributed to the frequent spatial transformations of the data features when training. The spatial transformations between different representation space could lead to lose the semantic and structure information of original KGs during the transformation process. Our model is rotation entirely in a hyperbolic space without spatial transformation, which compensates for this limitation."}, {"title": "6 Conclusion", "content": "In this paper, we propose FHRE, a fully hyperbolic rotation model for knowledge graph embedding. In contrast to previous hyperbolic KGE models, our model does not employ exponential and logarithmic mappings to transform data features during the training phase. Instead, it is defined directly on the Lorenz model. We conducted extensive experiments on five datasets. In the standard benchmarks WN18RR and FB15k-237, our model has demonstrated competitive results in both low and high dimensions. We conducted an analysis of the time complexity of the model, which indicated that our model could be faster. Furthermore, we have validated the effectiveness and generalization ability of our approach on more diverse and challenging benchmark datasets. The experiment demonstrates that our model get the state-of-the-art performance compared to the latest model. Finally, visualization results show that FHRE can encourage entities with similar semantics to have similar embeddings, which is beneficial to the prediction of unknown triplets."}]}