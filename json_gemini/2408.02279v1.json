{"title": "DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting", "authors": ["Ruixin Ding", "Yuqi Chen", "Yu-Ting Lan", "Wei Zhang"], "abstract": "Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution features. Additionally, we introduce a group-aware rotary position encoding technique to enhance intra- and inter-group position awareness among representations across different temporal scales. Our proposed model, named DRFormer, is evaluated on various real-world datasets, and experimental results demonstrate its superiority compared to existing methods. Our code is available at: https://github.com/ruixindingECNU/DRFormer.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting is crucial in various domains such as fi-nance [10, 13], traffic prediction [5, 16, 44, 47], etc. The ability to accurately predict future values in time series data has significant implications for decision-making and planning [4, 40, 49]. The rapid advancement of deep learning has fueled remarkable progress in time series forecasting [1, 12, 17]. Among various deep learning approaches, Transformer [25, 40, 46, 50] and MLP-based [43, 48] models have demonstrated superior performance due to their ability to capture long-term dependency. Furthermore, recent works have witnessed a significant breakthrough in patch-based transform-ers [6, 25, 29] for the long-term time series forecasting (LTSF) task. These approaches divide time-series data into sub-level patches and utilize Transformer models [37] to generate meaningful input features. However, existing methods are mostly designed to break the time series into patches of a fixed length [6, 25] or with a set of predetermined patch lengths [29]. This static patching with fixed patch length requires expert knowledge and poses challenges for extracting temporal features and dependencies from various scales of temporal intervals.\nTo illustrate these challenges more comprehensively, it is essen-tial to consider the following aspects: (i) The optimal sizes for patch division are influenced by the complex inherent characteristics and dynamic patterns of time series data, such as periodicity and trends. These intricate temporal patterns involve diverse variations and fluctuations across different temporal scales [2]. Currently, no es-tablished rules exist that can be validated either experimentally or theoretically to determine the optimal patch length. (ii) Real-world time series usually present multi-periodicity, such as daily, weekly, and monthly variations for traffic conditions [15, 21], or weekly and quarterly variations for electricity consumption [39]. These short-term and long-term recurring patterns contribute to the complexity of the forecasting task. (iii) The overall trend across the entire period and the specific time points of the learned sparse"}, {"title": "2 Related Work", "content": "In this section, we discuss the related studies from the following aspects: transformer for long-term time series forecasting, CNNs for time-series forecasting, and relative position embedding."}, {"title": "2.1 Transformer for Long-term Time Series Forecasting", "content": "The adoption of Transformer-based models has emerged as a promis-ing approach for long-term time series forecasting [20, 22, 24, 40, 45, 49, 50]. Among these models, Reformer [20] proposes locality-sensitive hashing attention for efficient and scalable sequence mod-eling. Informer [49] employs ProbSparse self-attention to extract important keys efficiently. Autoformer [40] introduces a novel decomposition framework, along with an auto-correlation atten-tion mechanism. FEDformer [50] utilizes Fourier transformation to model temporal characteristics and dynamics. Patch-based trans-formers [6, 25, 29], dividing time-series data into sub-level patches, have yielded significant enhancements in forecasting accuracy and complexity reduction. However, existing methods mainly model time series within limited or fixed patch lengths, which necessitate expert knowledge to select the optimal patch lengths and pose chal-lenges in capturing diverse characteristics across varying scales. Very recently, [2] developed a multi-scale Transformer that divides the time series into different temporal resolutions. However, it fails to learn a wide range of receptive fields given the selection of multiple patch lengths and static patching. In contrast, our pro-posed DRFormer can adeptly learn from a wide range of receptive fields, capture both overarching trends and nuanced variations, and extract the inherent multi-resolution properties of the data."}, {"title": "2.2 CNNs for Time-Series Forecasting", "content": "In addition to Transformers, convolutional neural networks (CNNs) are highly regarded by researchers in the time-series community [8, 11, 14, 32]. To enhance the generalization capabilities of time-series tasks and encompass diverse receptive fields, [26] proposed the use of dilated convolution kernels as a structure-based low bandpass filter. Moreover, OS-CNN [34] introduced the Omni-Scale block (OS-block) for 1D-CNNs, enabling the model to learn a range of diverse receptive fields. Additionally, DSN [41] presented a dynamic sparse network that can adaptively cover various receptive fields without the need for extensive hyperparameter tuning. Drawing inspiration from CNNs, we integrate dynamic sparse networks and multi-scale modeling into the Transformer structure, enabling the model to leverage the advantages of CNNs."}, {"title": "2.3 Relative Position Embedding", "content": "In the realm of natural language processing, several approaches for relative position embedding (RPE) have been proposed [7, 18, 28, 30, 33]. Among these, ROPE [33] is a representative approach that encodes relative position by multiplying the context representations with a rotation matrix. Additionally, the adoption of ROPE has been widespread among large language models as a means to extend the context windows [3, 35]. In this paper, we apply RoPE to enhance position awareness in the LTSF task. Besides, we propose group-aware rotary position embedding to encode intra- and inter-group relative position information into the attention mechanism, which is better suited for extracting multi-scale characteristics in time series data."}, {"title": "3 Methodology", "content": "In this section, we detailedly describe our method, DRFormer, which captures multi-scale characteristics with diverse receptive fields and multi-resolution representations for LTSF, as shown in Figure 1. We briefly introduce the intuition of our DRFormer. As aforemen-tioned, time series data is characterized by multi-scale properties and identifying critical timestamps provides crucial insights for pre-diction. To model such inductive bias, we propose a novel dynamic patching strategy coupled with a multi-scale Transformer to inject such characteristics priors into the forecasting pipelines. As illus-trated in Figure 2, DRFormer first incorporates a dynamic sparse network within the tokenizer, which simultaneously learns adaptive receptive fields and sparse patterns of the data. Next, we propose to transform time-series sequences into multi-scale sequences, al-lowing each token to represent features at multiple granularities. Finally, to capture cross-group interactions, we introduce a group-aware rotary position encoding technique for learning intra- and inter-group relative position embeddings.\nIn the following, we first formulate the problem and give an overview of our method. Then, we delve into the details of the"}, {"title": "3.1 Problem Formulation", "content": "The task of time series forecasting involves predicting a future series of length-O with the highest probability, based on a given past series of length-I, denoted as input-I-predict-O. In the context of long-term forecasting, the objective is to predict the future over a longer time horizon, specifically with a larger value of O. Given a multivariate time series, i.e., $X_{1:I} \\in \\mathbb{R}^{I\\times C}$, where I denotes the length of the time series, and C denotes the number of variates. The general objective of this research is to predict $X_{I+1:I+O} \\in \\mathbb{R}^{O\\times C}$ with $X_{1:I}$ as input. Note that our model is conducted on each variate of time series, i.e., channel independence. Thus, we denote $x_i$ as the time series for the i-th variate and omit the variate of time series for simplicity."}, {"title": "3.2 Dynamic Tokenizer", "content": "In this section, we describe how we discover and exploit various receptive fields adaptively with the dynamic sparse network. As aforementioned, static patching requires expertise to determine the length of temporal patch, in which complex inherent characteris-tics and dynamic patterns of time series data should be considered. Moreover, integrating fine-grained and coarse-grained features is crucial to model diverse variations and fluctuations, which is chal-lenging for the pre-defined static model. To address this, we involve a novel dynamic tokenizer to dynamically capture the optimal scale features through a sparse learning strategy."}, {"title": "3.2.1 Data Normalization", "content": "To migrate the distribution shift be-tween the training and testing data [19, 25, 36], we employ instance normalization on the input data. Specifically, we normalize each variable $x_i$ by subtracting its mean and dividing by its standard deviation before applying patching. After the output prediction, we add back the normalized values to restore the original distribution with the mean and standard deviation."}, {"title": "3.2.2 Static Patching", "content": "The proposed dynamic tokenizer, as depicted in Figure 2, first adopts a static patching operation to transform the input time series into sub-level patches [25]. Next, a dynamic linear transformation is applied to obtain dynamic representations, which serve as the input token embeddings. Specifically, we denote the patch length as P and the stride as S. Each input univariate time series $x_i$ is first divided into patches $p_i \\in \\mathbb{R}^{P\\times N}$ where N is the number of patches, $N = \\frac{(I-P)}{S} + 2$."}, {"title": "3.2.3 Dynamic Linear Transformation", "content": "Previous works adopt linear transformation to obtain input tokens [25]. Assume that $w_E \\in \\mathbb{R}^{P\\times D}$ and $b_E \\in \\mathbb{R}^{D}$, where D is the number of hidden dimensions of the model. The embeddings are obtained by $e_i = p_i w_E + b_E$. However, these embeddings are limited by a fixed receptive field, i.e., all dimensions of each token have the same receptive field size of P. To address the limitation, we introduce a learnable sparse mask, i.e.,\n$f_i = p (w_E I(w_E)) + b_E$,\nwhere $I(): \\mathbb{R}^{P\\times D} \\rightarrow \\{0,1\\}^{P\\times D}$ denotes an indicator function [41], $\\odot$ denotes the element-wise product. A dynamic linear layer with sparse ratio SR satisfies that $||I(w_E)||_0 \\le (1-SR)\\times P\\times D$. In Figure 2, a dynamic linear layer is depicted, showcasing the first, second, and last groups. The first dimension of each group learns receptive fields of sizes 3, 7, and 14, respectively. By definition of the token receptive field (tRF, as defined below), a dynamic linear layer is inherently designed to capture a comprehensive set of receptive fields, denoted as $RF = \\{0, 1, ..., P\\}$.\nRemark: Token Receptive Field (tRF) for dynamic linear layer.\nThe receptive field (RF) in CNN layers is defined as the region in the input that the feature is looking at. In the context of the dynamic linear layer, tRF is defined as the region in the input that a token is looking at. Mathematically, assume that the indicator function of the weight vector w is defined as $Ind = I(w) \\in \\{0, 1\\}^P$. Let S be the set of indices where $Ind_j = 1$, i.e., $S = \\{Ind_j = 1|1 \\le j \\le P\\}$, tRF is calculated as\n$tRF = \\begin{cases} max(S) \u2013 min(S) + 1, & \\text{if } Ind \\neq 0 \\\\ 0, & \\text{otherwise} \\end{cases}$"}, {"title": "3.2.4 Group Partition", "content": "By design, during the training phase, the total number of activated weights must not exceed $(1 - SR) \\times P\\times D$. However, a larger token receptive field occupies the majority of the"}, {"title": "3.2.5 Training the Indicator", "content": "Updating the indicator directly through backpropagation is a non-differentiable operation. We adopt a heuristic algorithm to explore and update the weights [41]. In the selection of the masking strategy, various possibilities were ex-plored. We ultimately determine masking out weights with small magnitudes as the masking strategy since it is intuitive and has been experimentally proven to be the most effective (more details in Section 4.4.3). The whole algorithm is listed in Algorithm 1.\nSpecifically, assume we have a total of T training iterations. For every $A_t$ iteration, we perform one step of update. At iteration t, since weights with smaller magnitudes contribute insignificantly or negligibly to the overall computation, we select n weights with the smallest absolute values from the candidate set and set these weights to 0, effectively deactivating them. To ensure recoverabil-ity from pruning, we randomly reintroduce n weights, matching the number of pruned weights, to facilitate better exploration of activated weights. This dynamic and plastic weight exploration approach allows for adaptive exploration during the training pro-cess. The value of n is controlled by the annealing function, which adjusts the pruning rate over time:\n$n=\\alpha \\frac{\\left(1+\\cos \\left(\\frac{t}{T} \\pi\\right)\\right)}{2} \\times||I(w_E)||_0$,\nwhere $\\alpha$ is a hyper-parameter to control the learning rate."}, {"title": "3.3 Multi-Scale Sequence Extraction", "content": "Time series data is characterized by both fine-grained local details and coarse-grained global composition, and capturing both aspects is crucial for comprehensive modeling. To address this, we propose a multi-scale approach that utilizes multi-group representations through hierarchical max-pooling on patched tokens. Specifically, we denote $f_i \\in \\mathbb{R}^{D\\times N}$ as a latent representation of patching of the dynamic tokenizer. The hierarchical max-pooling strategy involves the application of max-pooling from fine-grained to coarse-grained with non-overlapping windows of diverse on consecutive patches to generate multi-resolution representations as follows:\n$F = \\{f_1, f_2,..., f_{K_k}\\}$\nwhere k denotes the number of multi-scale sequences and $f_i$ is the original sequence from the dynamic tokenizer. Here, $f_i^j \\in \\mathbb{R}^{D\\times \\lceil\\frac{N}{K_j}\\rceil}$, $j \\in \\{1,2,..., k\\}$ denotes the representation after max pooling operation, i.e.,\n$f_{i}^j = MaxPooling(f_{i,p}, f_{i,p+1}, ..., f_{i,p+K_j-1})$,\nwhere $p + K_j - 1 \\le N$ and $f_{i,p}$ denotes the p-th token in $f_i$. Besides, we denote $S_k = \\{K_1, ..., K_k\\}$ as the set of different kernels. As is shown in Figure 1, we design the $S_k$ as a set of power two, i.e., $S_k =$"}, {"title": "3.4 Multi-Scale Transformers", "content": "In this section, we formulate our multi-scale transformer. To over-come the limitations of position awareness of multi-scale repre-sentations for the transformer model, we propose a group-aware relative position encoding technique, which empowers our model to effectively capture intricate dependencies and interactions among different groups of representations, resulting in enhanced forecast-ing performance."}, {"title": "3.4.1 Group-Aware Rotary Position Encoding", "content": "Instead of using tra-ditional absolute or relative position encoding, which ignores the inductive bias of intra and inter-group relations and treats differ-ent group embedding equally, we propose a novel group-aware rotary position encoding technique to capture intricate dependen-cies and interactions among different representation groups. We follow Roformer [33] and formulate the position encoding as the rotary matrix with pre-defined angle parameters. We derive the group-aware rotary position encoding for $f_{i,m}^{K_j} \\in \\mathbb{R}^{D\\times \\lceil\\frac{N}{K_j}\\rceil}$. Let $f_{i,m}^{K_j}$ be the m-th embedding for i-th group and the intra-group rotary position encoding for $f_{i,m}^{K_j}$ can be formulated as:\n$\\Theta = \\{ \\theta_i = \\frac{1}{10000^{-2(i-1)/d}}, i \\in [1,2,...,d/2] \\}$\nwhere $m = m/\\lceil\\frac{N}{K_j}\\rceil$, indicates the relative position of m within the sequence, and pre-defined parameters:\nSince $R^{d,intra}$ ignores the group information, we define another inter-group rotary position encoding as:\nHere, intra- and inter-group rotary position encoding share the same parameters."}, {"title": "3.4.2 Transformer Backbone", "content": "The Transformer model is widely recognized for its effectiveness in sequence modeling tasks. How-ever, to further improve its capacity to capture both inter-group and intra-group correlations, we introduce a novel group-aware rotary position encoding technique. Specifically, given the multi-scale inputs F and corresponding inter- and intra-group rotary"}, {"title": "3.4.3 Representation Fusion with Deconvolution", "content": "One possible ap-proach is to use these embeddings for prediction directly. However, to achieve predictions that incorporate both fine-grained local de-tails and coarse-grained global composition, we propose a fusion technique that combines these representations using deconvolution operations. Specifically, the output from the Transformer backbone is first split into multi-scale sequences:\nWe then perform a deconvolution operation [42], which is a tech-nique that upsamples features:\nFinally, the output is obtained by:"}, {"title": "3.5 Loss Function", "content": "We adopt the Mean Squared Error (MSE) loss to measure the dis-crepancy between the forecasting results and the ground truth observations. Let $\\hat{X}_{I+1:I+O}$ and ${X}_{I+1:I+O}$ be the predictions and real observations from time I + 1 to I + O. We denote $\\hat{x}_{I+1:I+O}^i$ and ${x}_{I+1:I+O}^i$ be the predictions and real observations from the i-th variate. The training loss is defined as:"}, {"title": "4 Experiments", "content": "In this section, we study the sensitivity of DRFormer to its hyper-parameters and masking strategy."}, {"title": "4.3 Ablation Study", "content": "In this section, we delve into a comprehensive analysis of DRFormer to showcase the effectiveness of each component of the model."}, {"title": "4.3.1 The effectiveness of dynamic modeling", "content": "We employ a dynamic tokenizer technique to capture fine-grained features within the patch size, which brings about diverse receptive fields. We demon-strate the effectiveness of the dynamic tokenizer using different Transformer architectures. As shown in Table 3, the dynamic to-kenizer can consistently decrease the prediction error, indicating the robustness of the dynamic tokenizer across various types of Transformer models."}, {"title": "4.3.2 The effectiveness of multi-scale Transformer", "content": "The considera-tion of multi-scale properties is a crucial aspect of time-series fore-casting. To tackle this issue, we propose a hierarchical pooling strat-egy and a group-aware multi-scale Transformer model. By compar-ing the results of Transformer+ROPE and Transformer+MS+gROPE, as listed in Table 3, we consider the design of multi-scale features effective when combined with group-aware RoPE. The comparison results between Transformer+ROPE and Transformer+MS+ROPE, with a few cases where performance decreases (MSE w/o DT), are reasonable. Transformer+MS+ROPE faces challenges in effectively aligning spatially close patches across varying scales and captur-ing intricate dependencies among different representation groups. In summary, the synergistic use of a multi-scale Transformer and gROPE emerges as a requisite for optimal performance."}, {"title": "4.3.3 The effectiveness of relative position embedding", "content": "Incorporat-ing relative position information in input sequences is crucial for Transformer-based models to overcome the weak sensitivity to the ordering of time series [43]. To tackle the issue, we first apply ROPE on the Transformer model. By comparing the results of the Transformer and Transformer+ROPE, as listed in Table 3, we can observe that RoPE improves the forecasting performance.\nFurthermore, to overcome the limitations of position awareness of multi-scale representations for the transformer model, we pro-pose a novel group-aware ROPE (gRoPE). By comparing the results of Transformer+MS+ROPE and Transformer+MS+gRoPE in Table 3, we can observe that multi-scale Transformer models with gROPE perform better than those with ROPE."}, {"title": "4.4 Sensitivity Analysis", "content": "In this section, we study the sensitivity of DRFormer to its hyper-parameters and masking strategy."}, {"title": "4.4.1 The Influence of Multi-Scale Sequences", "content": "Using multi-scale se-quences allows us to extract features at multiple scales by transform-ing original resolution time series into multi-scale representations. To examine the impact of parameter k on forecasting results, we varied k within the range {1, 2, 3, 4} and evaluated the performance of DRFormer in terms of mean squared error (MSE) on the ETTh1 and ETTm1 datasets. The results, depicted in Figure 3, demonstrate relatively stable and consistent trends across four different predic-tion horizons {96, 192, 336, 720}. Notably, increasing the value of k leads to a significant reduction in MSE errors on both datasets, as long as k remains below 3. This improvement can be attributed to the incorporation of features at more diverse scales through an in-creased number of multi-scale sequences. However, it is important to note that the length of the resized sequence decreases rapidly with larger kernel sizes, which ultimately limits the potential for further enhancement in forecasting performance."}, {"title": "4.4.2 The Influence of Patch Length", "content": "To analyze the impact of patch length on ETTh2 and ETTm1 datasets, we select patch length from {8, 16, 24, 32, 40, 48}. Results from Figure 4 indicate that DRFormer exhibits significant insensitivity to changes in patch length com-pared to patch-based Transformers without dynamic tokenizer and multi-scale sequences. The accuracy of DRFormer on the test set remains consistently high across a wide range of patch-length"}, {"title": "4.4.3 The Influence of Masking Strategy", "content": "We explored various mask-ing approaches, including masking out weights based on their mag-nitudes, both small and large, as well as masking weights according to the product of their magnitudes and gradients [31]. We ultimately chose masking out weights with small magnitudes as it is intuitive and has been experimentally proven to be the most effective as shown in Figure 6. It is widely recognized that the contribution of weights with smaller magnitudes is insignificant or even negligible."}, {"title": "4.5 Model Complexity Analysis", "content": "We conducted experiments to assess the complexity of DRFormer, focusing on two key metrics: parameters and training time. To en-sure fairness, we maintained the same batch size for all models. As depicted in Figure 7, DRFromer demonstrates significant advantages in both metrics, trailing only PatchTST. This can be attributed to the implementation of a multi-scale Transformer, which increases the total number of tokens by adding coarse-grained tokens via hierarchical max pooling. However, the additional resource require-ments are deemed acceptable. In comparison to models Autoformer, Informer, and Reformer, DRFormer exhibits lower complexity."}, {"title": "4.6 Visualization", "content": "We select one test example from the Traffic dataset for case visual-ization. The ground truth and the predictions from DRFormer and other baselines, i.e., PatchTST, and DLinear, are shown in Figure 5, where DRFormer provides the best forecasting. Specifically, we observe that DRFormer, less affected by low amplitude at the end of the input sequence, relies on long-term trends to align accurately with corresponding segments in the historical sequence. Compared with PatchTST and DLinear, the diverse receptive fields in Figure 5a enable DRFormer to learn multi-scale temporal patterns, improving its ability to predict periodicity and long-term variation without sacrificing compromising the accuracy of details."}, {"title": "5 Conclusion", "content": "In this paper, we propose a multi-scale Transformer model coupled with a dynamic tokenizer, named DRFormer, for long-term time series forecasting. DRFormer is a patch-based Transformer with a dynamic tokenizer and multi-resolution representations. Addition-ally, we present a novel group-aware ROPE method, named gROPE to enhance intra- and inter-group position awareness among repre-sentations with different temporal scales. Extensive experimental results on both multivariate and univariate time series forecasting demonstrate that DRFormer outperforms the previous state-of-the-art approaches. Dynamic tokenizer and multi-scale Transformer can be transferred easily to other patch-based models.\nLimitations: DRFormer is designed under a channel-independent setting and it can be further explored to incorporate the correlation between different channels."}]}