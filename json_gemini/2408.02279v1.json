{"title": "DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for Long Time-Series Forecasting", "authors": ["Ruixin Ding", "Yuqi Chen", "Yu-Ting Lan", "Wei Zhang"], "abstract": "Long-term time series forecasting (LTSF) has been widely applied in finance, traffic prediction, and other domains. Recently, patch-based transformers have emerged as a promising approach, segmenting data into sub-level patches that serve as input tokens. However, existing methods mostly rely on predetermined patch lengths, necessitating expert knowledge and posing challenges in capturing diverse characteristics across various scales. Moreover, time series data exhibit diverse variations and fluctuations across different temporal scales, which traditional approaches struggle to model effectively. In this paper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm to capture diverse receptive fields and sparse patterns of time series data. In order to build hierarchical receptive fields, we develop a multi-scale Transformer model, coupled with multi-scale sequence extraction, capable of capturing multi-resolution features. Additionally, we introduce a group-aware rotary position encoding technique to enhance intra- and inter-group position awareness among representations across different temporal scales. Our proposed model, named DRFormer, is evaluated on various real-world datasets, and experimental results demonstrate its superiority compared to existing methods. Our code is available at: https://github.com/ruixindingECNU/DRFormer.", "sections": [{"title": "1 Introduction", "content": "Time series forecasting is crucial in various domains such as finance [10, 13], traffic prediction [5, 16, 44, 47], etc. The ability to accurately predict future values in time series data has significant implications for decision-making and planning [4, 40, 49]. The rapid advancement of deep learning has fueled remarkable progress in time series forecasting [1, 12, 17]. Among various deep learning approaches, Transformer [25, 40, 46, 50] and MLP-based [43, 48] models have demonstrated superior performance due to their ability to capture long-term dependency. Furthermore, recent works have witnessed a significant breakthrough in patch-based transformers [6, 25, 29] for the long-term time series forecasting (LTSF) task. These approaches divide time-series data into sub-level patches and utilize Transformer models [37] to generate meaningful input features. However, existing methods are mostly designed to break the time series into patches of a fixed length [6, 25] or with a set of predetermined patch lengths [29]. This static patching with fixed patch length requires expert knowledge and poses challenges for extracting temporal features and dependencies from various scales of temporal intervals.\nTo illustrate these challenges more comprehensively, it is essential to consider the following aspects: (i) The optimal sizes for patch division are influenced by the complex inherent characteristics and dynamic patterns of time series data, such as periodicity and trends. These intricate temporal patterns involve diverse variations and fluctuations across different temporal scales [2]. Currently, no established rules exist that can be validated either experimentally or theoretically to determine the optimal patch length. (ii) Real-world time series usually present multi-periodicity, such as daily, weekly, and monthly variations for traffic conditions [15, 21], or weekly and quarterly variations for electricity consumption [39]. These short-term and long-term recurring patterns contribute to the complexity of the forecasting task. (iii) The overall trend across the entire period and the specific time points of the learned sparse patterns are significant for the LTSF task. The morning and evening peaks typically offer crucial information for traffic prediction. These characteristics require careful model design to introduce proper inductive bias.\nTo address these challenges, we propose a novel dynamic patching strategy coupled with a group-aware Roformer [33] network for LTSF. The proposed dynamic patching approach incorporates a dynamic sparse learning algorithm [41], which overcomes the need for expert knowledge by learning diverse receptive fields and extracts sparse patterns to identify critical points, thereby making it more applicable to real-world scenarios. To capture the inherent multi-resolution features, we introduce a Transformer model that enables multiple scales of temporal modeling. Additionally, we present a novel group-aware ROPE [33] method, named gRoPE, to enhance intra- and inter-group position awareness among representations with different temporal scales. By incorporating group awareness, DRFormer can effectively capture complex dependencies and interactions among different groups of representations, leading to improved forecasting performance. The contributions of the paper are as below:\n\u2022 We propose a multi-scale Transformer model, named DRFormer, which employs a dynamic tokenizer to learn diverse receptive fields and utilizes multi-scale sequence extraction to capture inherent multi-resolution features.\n\u2022 We introduce a group-aware rotary position encoding technique for learning intra- and inter-group relative position embedding. With such a design, DRFormer excels at capturing intricate dependencies among representations with distinct temporal scales.\n\u2022 We conduct extensive experiments to demonstrate the superiority of DRFormer over various baseline models in diverse real-world scenarios."}, {"title": "2 Related Work", "content": "In this section, we discuss the related studies from the following aspects: transformer for long-term time series forecasting, CNNs for time-series forecasting, and relative position embedding."}, {"title": "2.1 Transformer for Long-term Time Series Forecasting", "content": "The adoption of Transformer-based models has emerged as a promising approach for long-term time series forecasting [20, 22, 24, 40, 45, 49, 50]. Among these models, Reformer [20] proposes locality-sensitive hashing attention for efficient and scalable sequence modeling. Informer [49] employs ProbSparse self-attention to extract important keys efficiently. Autoformer [40] introduces a novel decomposition framework, along with an auto-correlation attention mechanism. FEDformer [50] utilizes Fourier transformation to model temporal characteristics and dynamics. Patch-based transformers [6, 25, 29], dividing time-series data into sub-level patches, have yielded significant enhancements in forecasting accuracy and complexity reduction. However, existing methods mainly model time series within limited or fixed patch lengths, which necessitate expert knowledge to select the optimal patch lengths and pose challenges in capturing diverse characteristics across varying scales. Very recently, [2] developed a multi-scale Transformer that divides the time series into different temporal resolutions. However, it fails to learn a wide range of receptive fields given the selection of multiple patch lengths and static patching. In contrast, our proposed DRFormer can adeptly learn from a wide range of receptive fields, capture both overarching trends and nuanced variations, and extract the inherent multi-resolution properties of the data."}, {"title": "2.2 CNNs for Time-Series Forecasting", "content": "In addition to Transformers, convolutional neural networks (CNNs) are highly regarded by researchers in the time-series community [8, 11, 14, 32]. To enhance the generalization capabilities of time-series tasks and encompass diverse receptive fields, [26] proposed the use of dilated convolution kernels as a structure-based low bandpass filter. Moreover, OS-CNN [34] introduced the Omni-Scale block (OS-block) for 1D-CNNs, enabling the model to learn a range of diverse receptive fields. Additionally, DSN [41] presented a dynamic sparse network that can adaptively cover various receptive fields without the need for extensive hyperparameter tuning. Drawing inspiration from CNNs, we integrate dynamic sparse networks and multi-scale modeling into the Transformer structure, enabling the model to leverage the advantages of CNNs."}, {"title": "2.3 Relative Position Embedding", "content": "In the realm of natural language processing, several approaches for relative position embedding (RPE) have been proposed [7, 18, 28, 30, 33]. Among these, ROPE [33] is a representative approach that encodes relative position by multiplying the context representations with a rotation matrix. Additionally, the adoption of ROPE has been widespread among large language models as a means to extend the context windows [3, 35]. In this paper, we apply RoPE to enhance position awareness in the LTSF task. Besides, we propose group-aware rotary position embedding to encode intra- and inter-group relative position information into the attention mechanism, which is better suited for extracting multi-scale characteristics in time series data."}, {"title": "3 Methodology", "content": "In this section, we detailedly describe our method, DRFormer, which captures multi-scale characteristics with diverse receptive fields and multi-resolution representations for LTSF, as shown in Figure 1. We briefly introduce the intuition of our DRFormer. As aforementioned, time series data is characterized by multi-scale properties and identifying critical timestamps provides crucial insights for prediction. To model such inductive bias, we propose a novel dynamic patching strategy coupled with a multi-scale Transformer to inject such characteristics priors into the forecasting pipelines. As illustrated in Figure 2, DRFormer first incorporates a dynamic sparse network within the tokenizer, which simultaneously learns adaptive receptive fields and sparse patterns of the data. Next, we propose to transform time-series sequences into multi-scale sequences, allowing each token to represent features at multiple granularities. Finally, to capture cross-group interactions, we introduce a group-aware rotary position encoding technique for learning intra- and inter-group relative position embeddings.\nIn the following, we first formulate the problem and give an overview of our method. Then, we delve into the details of the dynamic tokenizer, multi-scale sequence extraction and the multi-scale Transformer with group-aware rotary position embedding."}, {"title": "3.1 Problem Formulation", "content": "The task of time series forecasting involves predicting a future series of length-O with the highest probability, based on a given past series of length-I, denoted as input-I-predict-O. In the context of long-term forecasting, the objective is to predict the future over a longer time horizon, specifically with a larger value of O. Given a multivariate time series, i.e., $X_{1:I} \\in \\mathbb{R}^{I\\times C}$, where I denotes the length of the time series, and C denotes the number of variates. The general objective of this research is to predict $X_{I+1:I+O} \\in \\mathbb{R}^{O\\times C}$ with $X_{1:I}$ as input. Note that our model is conducted on each variate of time series, i.e., channel independence. Thus, we denote $x_i$ as the time series for the i-th variate and omit the variate of time series for simplicity."}, {"title": "3.2 Dynamic Tokenizer", "content": "In this section, we describe how we discover and exploit various receptive fields adaptively with the dynamic sparse network. As aforementioned, static patching requires expertise to determine the length of temporal patch, in which complex inherent characteristics and dynamic patterns of time series data should be considered. Moreover, integrating fine-grained and coarse-grained features is crucial to model diverse variations and fluctuations, which is challenging for the pre-defined static model. To address this, we involve a novel dynamic tokenizer to dynamically capture the optimal scale features through a sparse learning strategy.\n3.2.1 Data Normalization. To migrate the distribution shift between the training and testing data [19, 25, 36], we employ instance normalization on the input data. Specifically, we normalize each variable $x_i$ by subtracting its mean and dividing by its standard deviation before applying patching. After the output prediction, we add back the normalized values to restore the original distribution with the mean and standard deviation.\n3.2.2 Static Patching. The proposed dynamic tokenizer, as depicted in Figure 2, first adopts a static patching operation to transform the input time series into sub-level patches [25]. Next, a dynamic linear transformation is applied to obtain dynamic representations, which serve as the input token embeddings. Specifically, we denote the patch length as P and the stride as S. Each input univariate time series $x_i$ is first divided into patches $p_i \\in \\mathbb{R}^{P\\times N}$ where N is the number of patches, $N = \\frac{(I-P)}{S} + 2$.\n3.2.3 Dynamic Linear Transformation. Previous works adopt linear transformation to obtain input tokens [25]. Assume that $w_E \\in \\mathbb{R}^{P\\times D}$ and $b_E \\in \\mathbb{R}^{D}$, where D is the number of hidden dimensions of the model. The embeddings are obtained by $e_i = p_iw_E + b_E$. However, these embeddings are limited by a fixed receptive field, i.e., all dimensions of each token have the same receptive field size of P. To address the limitation, we introduce a learnable sparse mask, i.e.,\n$f_i = p \\odot (w_EI(w_E)) + b_E$,\nwhere $I(): \\mathbb{R}^{P\\times D} \\rightarrow \\{0,1\\}^{P\\times D}$ denotes an indicator function [41], $\\odot$ denotes the element-wise product. A dynamic linear layer with sparse ratio SR satisfies that $||I(w_E)||_0 \\le (1-SR)\\times P\\times D$. In Figure 2, a dynamic linear layer is depicted, showcasing the first, second, and last groups. The first dimension of each group learns receptive fields of sizes 3, 7, and 14, respectively. By definition of the token receptive field (tRF, as defined below), a dynamic linear layer is inherently designed to capture a comprehensive set of receptive fields, denoted as $RF = \\{0, 1, ..., P\\}$.\nRemark: Token Receptive Field (tRF) for dynamic linear layer. The receptive field (RF) in CNN layers is defined as the region in the input that the feature is looking at. In the context of the dynamic linear layer, tRF is defined as the region in the input that a token is looking at. Mathematically, assume that the indicator function of the weight vector w is defined as $Ind = I(w) \\in \\{0, 1\\}^{P}$. Let S be the set of indices where $Ind_j = 1$, i.e., $S = \\{Ind_j = 1|1 \\le j \\le P\\}$, tRF is calculated as\n$tRF = \\begin{cases} \\text{max}(S) - \\text{min}(S) + 1, & \\text{if } Ind \\ne 0 \\\\ 0, & \\text{otherwise} \\end{cases}$\n3.2.4 Group Partition. By design, during the training phase, the total number of activated weights must not exceed $(1 - SR) \\times P\\times D$. However, a larger token receptive field occupies the majority of the tokens, especially as the sparsity ratio SR decreases [41], leading to a leak of local patterns being captured. To address this problem, we utilize a group partition strategy. In this approach, the dynamic linear layer is divided into several groups, whose corresponding exploration regions are of different sizes. Specifically, the weights $w_E \\in \\mathbb{R}^{P\\times D}$ are split into G groups along the output channel, that is, $w_E^1,..., w_E^G \\in \\mathbb{R}^{P\\times \\frac{D}{G}}$. For the i-th group, the exploration region comprises the last $\\lfloor \\frac{P}{G} \\rfloor$ positions, thereby ensuring that activated weights only appear within these positions and the number of activated weights must not exceed $(1-SR)\\times \\lfloor \\frac{P}{G} \\rfloor \\times \\frac{D}{G}$. Additionally, we define the candidate set C as the set of weights that can be activated.\n3.2.5 Training the Indicator. Updating the indicator directly through backpropagation is a non-differentiable operation. We adopt a heuristic algorithm to explore and update the weights [41]. In the selection of the masking strategy, various possibilities were explored. We ultimately determine masking out weights with small magnitudes as the masking strategy since it is intuitive and has been experimentally proven to be the most effective.\nSpecifically, assume we have a total of T training iterations. For every $A_t$ iteration, we perform one step of update. At iteration t, since weights with smaller magnitudes contribute insignificantly or negligibly to the overall computation, we select n weights with the smallest absolute values from the candidate set and set these weights to 0, effectively deactivating them. To ensure recoverability from pruning, we randomly reintroduce n weights, matching the number of pruned weights, to facilitate better exploration of activated weights. This dynamic and plastic weight exploration approach allows for adaptive exploration during the training process. The value of n is controlled by the annealing function, which adjusts the pruning rate over time:\n$n = \\alpha (1 + cos(\\frac{t\\pi}{T})) \\times ||I(w_E)||_0$,\nwhere \u03b1 is a hyper-parameter to control the learning rate."}, {"title": "3.3 Multi-Scale Sequence Extraction", "content": "Time series data is characterized by both fine-grained local details and coarse-grained global composition, and capturing both aspects is crucial for comprehensive modeling. To address this, we propose a multi-scale approach that utilizes multi-group representations through hierarchical max-pooling on patched tokens. Specifically, we denote $f_i \\in \\mathbb{R}^{D\\times N}$ as a latent representation of patching of the dynamic tokenizer. The hierarchical max-pooling strategy involves the application of max-pooling from fine-grained to coarse-grained with non-overlapping windows of diverse on consecutive patches to generate multi-resolution representations as follows:\n$F = \\{f_i^1, f_i^2,..., f_i^{K_k}\\}$\nwhere k denotes the number of multi-scale sequences and $f_i^1$ is the original sequence from the dynamic tokenizer. Here, $f_i^j \\in \\mathbb{R}^{D\\times[\\frac{N}{K_j}]}$, $j \\in \\{1,2,..., k\\}$ denotes the representation after max pooling operation, i.e.,\n$f_i^j = MaxPooling(f_{i,p}, f_{i,p+1}, ..., f_{i,p+K_j-1})$,\nwhere $p + K_j - 1 \\le N$ and $f_{i,p}$ denotes the p-th token in $f_i$. Besides, we denote $S_k = \\{K_1, ..., K_k\\}$ as the set of different kernels. As is shown in Figure 1, we design the $S_k$ as a set of power two, i.e., $S_k = \\{1, 2, ..., 2^{k-1}\\}$, to empower the model with multi-scale ability and obtain a comprehensive representation from fine-grained to coarse-grained temporal information. With such a design, the tokens can capture a more comprehensive set of receptive fields of:\n$RF = \\{0, 1, ..., P + (2^{k-1} - 1) \\cdot S\\}$ ."}, {"title": "3.4 Multi-Scale Transformers", "content": "In this section, we formulate our multi-scale transformer. To overcome the limitations of position awareness of multi-scale representations for the transformer model, we propose a group-aware relative position encoding technique, which empowers our model to effectively capture intricate dependencies and interactions among different groups of representations, resulting in enhanced forecasting performance.\n3.4.1 Group-Aware Rotary Position Encoding. Instead of using traditional absolute or relative position encoding, which ignores the inductive bias of intra and inter-group relations and treats different group embedding equally, we propose a novel group-aware rotary position encoding technique to capture intricate dependencies and interactions among different representation groups. We follow Roformer [33] and formulate the position encoding as the rotary matrix with pre-defined angle parameters. We derive the group-aware rotary position encoding for $f_{i,m}^{K_j} \\in \\mathbb{R}^{D\\times[\\frac{N}{K_j}]}$. Let $f_{i,m}^{K_j}$ be the m-th embedding for i-th group and the intra-group rotary position encoding for $f_{i,m}^{K_j}$ can be formulated as:\n$\\Theta = \\{ \\theta_i = \\frac{1}{10000^{-2(i-1)/d}}, i \\in [1,2,...,d/2] \\}.$\nSince $R^{d, intra}$ ignores the group information, we define another inter-group rotary position encoding as:\nHere, intra- and inter-group rotary position encoding share the same parameters.\n3.4.2 Transformer Backbone. The Transformer model is widely recognized for its effectiveness in sequence modeling tasks. However, to further improve its capacity to capture both inter-group and intra-group correlations, we introduce a novel group-aware rotary position encoding technique. Specifically, given the multi-scale inputs F and corresponding inter- and intra-group rotary position encoding, i.e., $R^{d, inter}, R^{d, inter}, R^{d, intra}, R^{d, intra}$. We first calculate keys and queries, i.e.,\n$\\begin{cases} \\{Q, K\\}_{inter} = FW_{\\{Q,K\\}} \\odot R^{d, \\{inter, intra\\}} \\\\ \\{Q, K\\}_{intra} = FW_{\\{Q,K\\}} \\odot R^{d, \\{inter, intra\\}} \\end{cases}$\nwhere $W_{\\{Q,K\\}}$ represents the transformation matrices for queries and keys, respectively. Next, we define the group-aware attention:\n$Attn(F) = softmax(\\frac{(Q_{inter}K_{inter})^T + (Q_{intra}K_{intra})^T}{\\sqrt{d_k}})FW_v$\nThe multi-scale Transformer is a highly efficient model that effectively extracts multi-scale information from time series data while also capturing group awareness. Additionally, each Transformer layer incorporates a feed-forward network and layer normalization [37]. The mathematical formulations are as follows:\n$F^{l,1} = F^{l-1} + LN(Attn(F^{l-1}))$,\n$F^l = F^{L,1} + LN(FFN(F^{l,1}))$,\nwhere $F^l$ indicates the output for the l-th Transformer layer with the input $F^0$ defined in Eq. (4), LN and FFN represent the layer normalization operation and the feed-forward network, respectively [37].\n3.4.3 Representation Fusion with Deconvolution. One possible approach is to use these embeddings for prediction directly. However, to achieve predictions that incorporate both fine-grained local details and coarse-grained global composition, we propose a fusion technique that combines these representations using deconvolution operations. Specifically, the output from the Transformer backbone is first split into multi-scale sequences:\n$o_m = \\{ f_i^{K_1}, f_i^{K_2},..., f_i^{K_k} \\}.$\nWe then perform a deconvolution operation [42], which is a technique that upsamples features:\n$de_i = Deconv(o_i, K_j)$,\nwhere $de_i \\in \\mathbb{R}^{N \\times d}$. Finally, the output is obtained by:\n$\\hat{O}_i = \\sum_{j=1}^k \\frac{de_i}{K_j}.$\n3.5 Loss Function\nWe adopt the Mean Squared Error (MSE) loss to measure the discrepancy between the forecasting results and the ground truth observations. Let $\\hat{X}_{I+1:I+O}$ and $X_{I+1:I+O}$ be the predictions and real observations from time I + 1 to I + O. We denote $\\hat{x}_{I+1:I+O}^i$ and $x_{I+1:I+O}^i$ be the predictions and real observations from the i-th variate. The training loss is defined as:\n$\\mathcal{L} = E_{x \\sim D} [\\frac{1}{C} \\sum_{i=1}^C ||\\hat{x}_{I+1:I+O}^i - x_{I+1:I+O}^i||_2^2 ]$"}, {"title": "4 Experiments", "content": "4.1 Experimental Setting\n4.1.1 Dataset Description. We conducted extensive experiments on time-series benchmark datasets. These datasets cover a variety of applications, including ETT and Electricity for electricity prediction, Exchange for financial applications, ILI for disease prediction, and Traffic for traffic prediction.\n4.1.2 Baselines. We compare DRFormer with several Transformer-based models, including\n\u2022 Reformer [20], which proposes a locality-sensitive hashing mechanism to reduce the time cost of self-attention calculation.\n\u2022 Informer [49], which proposes a ProbSparse self-attention with distilling techniques to extract the most important keys.\n\u2022 Autoformer [40], which proposes an auto-correlation attention mechanism and a novel decomposition architecture.\n\u2022 FEDformer [50], which proposes to combine Fourier analysis with the Transformer-based method.\n\u2022 ETSformer [38], which exploits the principle of exponential smoothing and performs a layer-wise level, growth, and seasonal decomposition.\n\u2022 PatchTST [25], which divides the time-series data into sub-level patches to generate meaningful input features inspired by a patch-based Transformer on images in [9].\nWe also consider several non-Transformer models, including\n\u2022 DLinear [43], a simple linear model that only adopts a one-layer MLP model on the temporal dimension."}, {"title": "4.2 Model Comparisons", "content": "4.2.1 Multivariate Forecasting Results. The multivariate forecasting results are shown in Table 1, indicating DRFormer achieves the state-of-the-art results on 6 datasets. Specifically, as compared to the best baselines, DRFormer reduces the MSE by 8.4% (0.476 \u2192 0.439) on the Traffic dataset, and overall 2.7% (0.380 \u2192 0.370) reduction on four subsets of the ETT dataset, and 9.2% (2.139 \u2192 1.959) reduction on the ILI dataset. On average, DRFormer achieves a 6.20% (0.617 \u2192 0.581) reduction on the MSE metric compared\n4.2.2 Univariate Forecasting Results. We show the univariate forecasting results [40] in Table 2. As shown in the table, DRFormer achieves state-of-the-art results on four ETT datasets. Additionally, DRFormer achieves the second-best results for the MAE metric on the ILI dataset. Specifically, DRFormer achieves a 5.33% reduction on the ETTh1 dataset under the MSE metric, a 3.85% reduction on the ETTm1 dataset, a 2.63% reduction on the ETTh2 dataset and a 2.54% reduction on the ETTm2 dataset respectively."}, {"title": "4.3 Ablation Study", "content": "In this section, we delve into a comprehensive analysis of DRFormer to showcase the effectiveness of each component of the model.\n4.3.1 The effectiveness of dynamic modeling. We employ a dynamic tokenizer technique to capture fine-grained features within the patch size, which brings about diverse receptive fields. We demonstrate the effectiveness of the dynamic tokenizer using different Transformer architectures. As shown in Table 3, the dynamic tokenizer can consistently decrease the prediction error, indicating the robustness of the dynamic tokenizer across various types of Transformer models.\n4.3.2 The effectiveness of multi-scale Transformer. The consideration of multi-scale properties is a crucial aspect of time-series forecasting. To tackle this issue, we propose a hierarchical pooling strategy and a group-aware multi-scale Transformer model. By comparing the results of Transformer+ROPE and Transformer+MS+gROPE, as listed in Table 3, we consider the design of multi-scale features effective when combined with group-aware ROPE. The comparison results between Transformer+ROPE and Transformer+MS+ROPE, with a few cases where performance decreases (MSE w/o DT), are reasonable. Transformer+MS+ROPE faces challenges in effectively aligning spatially close patches across varying scales and capturing intricate dependencies among different representation groups. In summary, the synergistic use of a multi-scale Transformer and gROPE emerges as a requisite for optimal performance.\n4.3.3 The effectiveness of relative position embedding. Incorporating relative position information in input sequences is crucial for Transformer-based models to overcome the weak sensitivity to the ordering of time series [43]. To tackle the issue, we first apply ROPE on the Transformer model. By comparing the results of the Transformer and Transformer+ROPE, as listed in Table 3, we can observe that RoPE improves the forecasting performance.\nFurthermore, to overcome the limitations of position awareness of multi-scale representations for the transformer model, we propose a novel group-aware ROPE (gRoPE). By comparing the results of Transformer+MS+ROPE and Transformer+MS+gROPE in Table 3, we can observe that multi-scale Transformer models with gROPE perform better than those with ROPE."}, {"title": "4.4 Sensitivity Analysis", "content": "In this section, we study the sensitivity of DRFormer to its hyperparameters and masking strategy.\n4.4.1 The Influence of Multi-Scale Sequences. Using multi-scale sequences allows us to extract features at multiple scales by transforming original resolution time series into multi-scale representations. To examine the impact of parameter k on forecasting results, we varied k within the range {1, 2, 3, 4} and evaluated the performance of DRFormer in terms of mean squared error (MSE) on the ETTh1 and ETTm1 datasets. The results demonstrate relatively stable and consistent trends across four different prediction horizons {96, 192, 336, 720}. Notably, increasing the value of k leads to a significant reduction in MSE errors on both datasets, as long as k remains below 3. This improvement can be attributed to the incorporation of features at more diverse scales through an increased number of multi-scale sequences. However, it is important to note that the length of the resized sequence decreases rapidly with larger kernel sizes, which ultimately limits the potential for further enhancement in forecasting performance.\n4.4.2 The Influence of Patch Length. To analyze the impact of patch length on ETTh2 and ETTm1 datasets, we select patch length from {8, 16, 24, 32, 40, 48}. Results indicate that DRFormer exhibits significant insensitivity to changes in patch length compared to patch-based Transformers without dynamic tokenizer and multi-scale sequences. The accuracy of DRFormer on the test set remains consistently high across a wide range of patch-length configurations, highlighting the advantage of capturing a set of receptive fields with a predetermined patch length.\n4.4.3 The Influence of Masking Strategy. We explored various masking approaches, including masking out weights based on their magnitudes, both small and large, as well as masking weights according to the product of their magnitudes and gradients [31]. We ultimately chose masking out weights with small magnitudes as it is intuitive and has been experimentally proven to be the most effective as shown in Figure 6. It is widely recognized that the contribution of weights with smaller magnitudes is insignificant or even negligible."}, {"title": "4.5 Model Complexity Analysis", "content": "We conducted experiments to assess the complexity of DRFormer, focusing on two key metrics: parameters and training time. To ensure fairness, we maintained the same batch size for all models. As depicted in Figure 7, DRFromer demonstrates significant advantages in both metrics, trailing only PatchTST. This can be attributed to the implementation of a multi-scale Transformer, which increases the total number of tokens by adding coarse-grained tokens via hierarchical max pooling. However, the additional resource requirements are deemed acceptable. In comparison to models Autoformer, Informer, and Reformer, DRFormer exhibits lower complexity."}, {"title": "4.6 Visualization", "content": "We select one test example from the Traffic dataset for case visualization. The ground truth and the predictions from DRFormer and other baselines, i.e., PatchTST, and DLinear, are shown in Figure 5, where DRFormer provides the best forecasting. Specifically, we observe that DRFormer, less affected by low amplitude at the end of the input sequence, relies on long-term trends to align accurately with corresponding segments in the historical sequence. Compared with PatchTST and DLinear, the diverse receptive fields enable DRFormer to learn multi-scale temporal patterns, improving its ability to predict periodicity and long-term variation without sacrificing compromising the accuracy of details."}, {"title": "5 Conclusion", "content": "In this paper, we propose a multi-scale Transformer model coupled with a dynamic tokenizer, named DRFormer, for long-term time series forecasting. DRFormer is a patch-based Transformer with a dynamic tokenizer and multi-resolution representations. Additionally, we present a novel group-aware ROPE method, named gROPE to enhance intra- and inter-group position awareness among representations with different temporal scales. Extensive experimental results on both multivariate and univariate time series forecasting demonstrate that DRFormer outperforms the previous state-of-the-art approaches. Dynamic tokenizer and multi-scale Transformer can be transferred easily to other patch-based models.\nLimitations: DRFormer is designed under a channel-independent setting and it can be further explored to incorporate the correlation between different channels."}, {"title": "6 Acknowledgments", "content": "The authors would like to extend their heartfelt thanks to Zanwei Zhou and Jiajun Cui for their insightful feedback and valuable suggestions on paper writing. This work was supported in part by the National Key Research and Development Program of China (2021ZD0111000, 2019YFB2102600) and Key Laboratory of Advanced Theory and Application in Statistics and Data Science, Ministry of Education."}]}