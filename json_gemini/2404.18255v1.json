{"title": "PatentGPT: A Large Language Model for Intellectual Property", "authors": ["Zilong Bai", "Ruiji Zhang", "Linqing Chen", "Qijun Cai", "Yuan Zhong", "Cong Wang", "Yan Fang", "Jie Fang", "Jing Sun", "Weikuan Wang", "Lizhi Zhou", "Haoran Hua", "Tian Qiu", "Chaochao Wang", "Cheng Sun", "Jianping Lu", "Yixin Wang", "Yubin Xia", "Meng Hu", "Haowen Liu", "Peng Xu", "Licong Xu", "Fu Bian", "Xiaolong Gu", "Lisha Zhang", "Weilei Wang", "Changyang Tu"], "abstract": "In recent years, large language models (LLMs) have attracted significant attention due to their exceptional performance across a multitude of natural language process tasks, and have been widely applied in various fields. However, the application of large language models in the Intellectual Property (IP) domain is challenging due to the strong need for specialized knowledge, privacy protection, processing of extremely long text in this field. In this technical report, we present for the first time a low-cost, standardized procedure for training IP-oriented LLMs, meeting the unique requirements of the IP domain. Using this standard process, we have trained the PatentGPT series models based on open-source pretrained models. By evaluating them on the open-source IP-oriented benchmark MOZIP, our domain-specific LLMs outperforms GPT-4, indicating the effectiveness of the proposed training procedure and the expertise of the PatentGPT models in the IP demain. Remarkably, our model surpassed GPT-4 on the 2019 China Patent Agent Qualification Examination, scoring 65 and matching human expert levels. Additionally, the PatentGPT model, which utilizes the SMoE architecture, achieves performance comparable to that of GPT-4 in the IP domain and demonstrates a better cost-performance ratio on long-text tasks, potentially serving as an alternative to GPT-4 within the IP domain.", "sections": [{"title": "Introduction", "content": "Over the past few years, Large Language Models (LLMs) based on decoder-only transformers have received substantial attention, due to their excellent alignment with human preference and remarkable performance across various NLP tasks, including reading comprehension, novel creation, text summarization, code completion, and document drafting. Recent research suggests that LLMs are not merely reproducing surface statistics, but are instead learning a meaningful world model. [1-3] opening up the imaginative space for the application of LLMs. The performance of LLMs follows the scaling law that models with more pretraining data and parameters achieve better performance [4-7], indicating that the costs to train and apply a high-performance LLM are significant.\nTo reduce the pretraining and deployment costs of LLMs, more and more pretrained models are being published in open-source communities. These LLMs, including LLaMA [8, 9], Bloom [10], and ChatGLM [11], have parameters ranging from several billions to tens of billions, and they were pre-trained on over a trillion tokens, showing exceptional performance in base capabilities. Developers can adapt these general models into domain-specific models by continuous training or fine-tuning without conducting an extensive pretraining process. We have selected LLama as the base model for further"}, {"title": "Pretraining", "content": "We trained PatentGPT models with the base models of LLaMA2 (13B), LLaMA2 (70B) [9], and Mixtral 8 \u00d7 7B [27], and we named the series of LLMs PatentGPT-0.5, PatentGPT-1.0-Dense and PatentGPT-1.0-MoE respectively."}, {"title": "Pretraining Data", "content": "To create an effective pre-training dataset, it is crucial to ensure that the pretraining data are diverse and cover a wide range of types, domains, and tasks. In the IP domain, it involves legal knowledge as well as engineering and technical knowledge from various disciplines, and requires a strong ability to contextualize when writing patent specifications and comparing distinctions between two patents. Aiming to meet these requirements, the pretraining dataset was made up of publicly available internet resources, such as websites, wikis, books, exam databases, code repositories, and news articles, as well as non-public internal sources, including patents, file wrappers [32], annotated datasets, and litigation records. Additionally, we incorporated data from third parties like research papers, books, and research reports, along with internally generated data. We trained PatentGPT models using a two-stage pretraining process over 240 billion tokens of data in both English and Chinese. The proportion of Chinese pretraining data is 37%. The composition of the training data is shown in Figure 1. To obtain a high-quality dataset for pretraining, we employed several data preprocessing procedures including filtering, deduplication rewriting, and synthesis."}, {"title": "Training Details", "content": "We began with the pretraining approach described in Touvron et al. [9], using an optimized auto-regressive transformer. However, we made several changes to pretrain IP-oriented LLMs. These changes include multilingual pretraining and a carefully constructed, IP-oriented pretrain dataset as described in Section 2.1. Additionally, we conducted a two-stage pretraining process to inject IP-oriented knowledge as well as develop abilities for recognizing, drafting, and comparing patents. All"}, {"title": "Alignment", "content": "LLMs have demonstrated impressive performance in various NLP tasks. However, their implementation presents limitations, as pretrained LLMs frequently misconstrue human intentions or generate harmful"}, {"title": "Supervised Finetuning", "content": "Data: In total, we used 30,000 general instructions (\\(D_{gen}\\)) and 13,000 IP-specific instructions (\\(D_{exp}\\)) for conducting SFT on the pretrained PatentGPT models. \\(D_{gen}\\) is composed of sampled data from several public datasets, including FLAN datasets [44], COIG datasets [45], Firefly [46], BELLE [47], MOSS [48], and Ultrachat [49]. \\(D_{exp}\\) consists of IP-specific instructions annotated by patent examiners and other experts in technics. \\(D_{exp}\\) not only contains Q&A data in the IP domain and technical domains, but also includes supervised data specially designed to support retrieval augmented generation (RAG), and data supporting several IP application scenarios, including technical means summaries,"}, {"title": "Reinforcement Learning from Human Feedback", "content": "RLHF is a training procedure applied to a fine-tuned language model to further align model behav-iors with human preference and instruction following. We applied RLHF to the PatentGPT models, following the approaches of Bai et al. [51]."}, {"title": "Reward model (RM)", "content": "We collected 100k human preference data for reward modeling, including 20k expert annotations and 80k AI-generated annotations. Each annotation consists of a prompt followed by several responses generated by our PatentGPT models of different sizes, and commercial LLMs such as GPT-4 and ChatGPT-3.5, to enhance response diversity. For an expert annotation, an annotator ranked the responses from best to worst following standard annotation guidelines, and constructed comparison pairs based on this ranking. In the AI-generated annotation process, responses were ranked by GPT-4 in accordance with the RLAIF approach proposed by Lee et al. [52]."}, {"title": "Reinforcement learning (RL)", "content": "To stabilize RL training, we used proximal policy optimization (PPO) [54] with a reward signal provided by the RM score [9]. During the RLHF training process, both the actor model and the reference model were initialized with the SFT models described in Section 3.1, while the critic model was initialized with RM. In each step, the actor model was required to generate four responses from a given prompt, and the response with the highest reward signal, provided by the RM scores, was selected to optimize the actor model."}, {"title": "Evaluation Results", "content": "In order to evaluate the general capabilities of LLMs objectively and effectively, benchmarks such as Truthful QA [55], LongBench [56], and MMLU [57] have been proposed in previous works. On the other hand, considering the diverse demands of various industries, domain-specific benchmarks such as CoderEval [58], PubMedQA [59], and FinBench [60] also have been introduced as evaluation standards for domain-specific LLMs. Recently, Ni et al. [61] introduced an IP-oriented benchmark called MoZIP, which utilizes IP-related open question answering and multiple-choice questions to evaluate models' performance in IP-oriented knowledge and their proficiency in matching patent texts. However, MOZIP does not involve tasks such as patent specification drafting, patent classification, and summarizing key technical information from patent specifications, which are very important for the application of LLMs in the IP domain.\nIn this section, we first introduce Patent Bench, a comprehensive benchmark focusing on the IP domain. Then, we present the evaluation results of the PatentGPT models, as well as those of ChatGPT-3.5-turbo and GPT-4 on PatentBench, MoZIP, MMLU, and C-Eval, respectively."}, {"title": "Patent Bench", "content": "To compensate for the lack of specialization in the IP domain of open-source benchmarks, we proposed PatentBench which for the first time incorporates tasks related to patent agency and examination pro-cess such as patent specification drafting, patent classification, and summarizing key technical points of patents into a benchmark for evaluating IP-specific LLMs. We plan to open-source Patent Bench in the fourth quarter of this year, contributing to the development of the open source communities and the IP industry. The main evaluation tasks of PatentBench are as follows:\nPatent_QA: We constructed the Patent_QA dataset to evaluate if an LLM understand IP-related concepts and regulations. The instructions were collected from product experts, top sellers of Patsnap and FAQs from the official websites of major national IP offices and the leading IP education websites. Each instance consists of a prompt with one or two questions and the corresponding reference answers as the output. The total number of questions is 250 for Chinese and 250 for English.\nPatent_Writing: Patent_Writing is a dataset designed to evaluate the ability of LLMs to draft patent texts. It involves drafting patent specifications based on technical disclosure documents, rewriting and refining various sections within the patent descriptions, as well as supplementing the claims. All instances in this dataset were manually constructed by annotators using tools assisted by LLMs based on patent specifications in either English or Chinese. The total number of instances in this dataset is 300.\nPatent_Classification: According to the classification rules of the International Patent Classification (IPC), patents can be divided into 118 categories based on the Section and Class of the IPC number. We have uniformly selected 1,180 patents from the Chinese and United States patent databases across these 118 categories. The instructions for this dataset require the LLMs to generate the correct cate-gories based on the abstract and claims of each patent.\nPatent_Summary: This dataset comprises four sub-tasks: technical effects, technical problems, technical means, and patent abstracts. Each sub-task contains 300 instances. First, we trained an extraction model to accurately extract paragraphs involving technical effects, technical problems, and technical means. Then we inputted the extracted paragraphs along with carefully designed prompts into GPT-4, requesting GPT-4 to summarize the technical effects, technical problems, technical means,"}, {"title": "Results on Patent Bench", "content": "We first evaluated zero-shot performance of our PatentGPT models on the PatentBench using both GPT-4 and the metrics widely used in NLP. To assess the models' summarization, writing, and con-versational abilities, we employed GPT-4 as a judge by inputting a prompt that required it to compare the outputs from two assistants (GPT-3.5-turbo and one of our PatentGPT models) and to generate scores based on well-designed guidelines. Each pair of outputs from different models was evaluated twice with their positions in the prompt reversed, and the final scores were calculated by averaging, aiming to eliminate influences associated with position bias. Finally, we statisticed on the number of wins, loses, and ties across the two models, and the results is shown in Figure 4. Our PatentGPT models show significant improvements over ChatGPT-3.5-turbo in terms of drafting, and IP-oriented open question answering, indicating that our PatentGPT models have the potential to serve as patent assistants, thus helping humans in drafting patent specifications, reading patents, and understanding patent laws and regulations. In summarization tasks, the proportion of ties is very high, suggesting that the gap in summarization ability between ChatGPT-3.5-turbo and our PatentGPT models is difficult to be evaluated by GPT-4.\nWe subsequently evaluated classification, examination, translation, text correction, and reasoning abil-ities based on different metrics. The classification capability of the models was measured by the F1 scores, the examination ability by accuracy, the translation quality by BLEU scores, and the reasoning and text correction abilities by EM scores [62]. Besides PatentGPT-0.5, our PatentGPT models have outperformed ChatGPT-3.5-turbo in capabilities other than reasoning. These results demonstrate the effectiveness of our pre-training and the advanced nature of our PatentGPT models in the IP domain.\nTo further demonstrate the expertise of the PatentGPT models in the IP domain, we tested Patent GPT models using the 2019 China Patent Agent Qualification Examination. For convenience in statistics, we simplified the score calculation rules for the patent agent exam. We determined that for the 200 multiple choice questions in patent law and related laws, each correct answer will be awarded 0.5 points, achieving a perfect score of 100 if all questions are answered correctly. Figure 5 depicts the exam scores of ChatGPT-3.5-turbo, GPT-4-1106-preview and the PatentGPT models on the patent agent exam. Results show that all commercial general LLMs failed to pass the cutoff of 60 points for the patent agent exam, whereas PatentGPT-1.0-Dense and PatentGPT-1.0-MoE scored 65 points and 60 points respectively, approaching the level of an IP expert. This result reveals the shortcomings of"}, {"title": "Results on MoZIP Benchmark", "content": "In addition to evaluating our PatentGPT models on the Patent Bench, we also assessed them on MOZIP [61], which is a public benchmark specifically designed for the IP industry. MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). GPT-3.5-turbo and GPT-4-1106-preview were used as baselines to evaluate the Patent GPT models. The 5-shot results on IPQuiz are shown in Table 2. Patent GPT-1.0-Dense and Patent GPT-1.0-MoE outperform GPT-3.5-turbo, indicating that these PatentGPT models demonstrate a better understanding of the fundamental concepts and regulations of IP. The PatentGP-1.0 model achieved a score of 79.9, which is 2.1 points higher than the GPT-4-1106-preview, and once again performs the best among all the models, which is consistent with the results of the China Patent Agent Qualification Examination shown in Figure 5. This result suggests a significant advantage of PatentGPT-1.0-Dense over generic models in the IP domain. We also notice that, in this experiment,"}, {"title": "Results on MMLU and C-Eval", "content": "In the process of a patent specification being drafting or reviewed, common sense knowledge in areas such as physics, chemistry, and engineering are involved, along with patent laws. Therefore, we evaluated the scientific and technological common sense of our PatentGPT models using MMLU and C-Eval, which are widely used to evaluate performance of LLMs. We assessed the performance of the PatentGPT models and ChatGPT-3.5-turbo on the test set of MMLU and the validation set of"}, {"title": "Results on inference resources consumption", "content": "The evaluation results presented in Sections 4.1-4.3 demonstrate that PatentGPT-1.0-MoE slightly underperforms PatentGPT-1.0-Dense but achieves performance comparable to that of GPT-4. This indicates that both PatentGPT-1.0-Dense and PatentGPT-1.0-MoE have the potential to serve as alternatives to GPT-4 in the IP industry. In addition to performance, the response latency and op-erational costs of the models are critical for their commercial viability. Therefore, we performed the 4-bit quantization cross all PatentGPT models and employed Text-Generation-Inference(TGI)-1.4 to assess their resource consumption when they output the first token. All experiments were conducted on a server equipped with a NVIDIA A100 80GB GPU."}, {"title": "Conclusions and Future Works", "content": "In this report, we introduced methodologies for training IP-oriented LLMs and establish the most comprehensive benchmark in the IP domain, the PatentBench. The main conclusions of this report are summarized as follows:\nFirst, we proposed a standard training procedure for LLMs in the IP domain, including data pre-processing, pretraining, alignment, and evaluation. Under the guidance of this procedure, we trained the PatentGPT models. The results of the experiments indicate that our PatentGPT models demon-strated overall performance that surpasses that of ChatGPT-3.5-turbo in the IP domain. Moreover, PatentGPT-1.0-Dense, with 70 billion parameters, even exhibits superior performance compared to GPT-4 in this field, indicating the effectiveness of the methodology we employed.\nSecondly, we compared the performance of PatentGPT models that were pre-trained using open-source base models with various parameter scales and across different frameworks. After a comprehensive evaluation of these models' performance on IP-oriented tasks and their computational resource con-sumption, we found that IP-oriented LLMs with 70 billion parameters tended to perform better on tasks in the IP domain, while the SMoE model with 47 billion parameters exhibited a better cost-performance ratio on long-text tasks, achieving a better tradeoff between model quality and inference efficiency.\nFinally, we provided a benchmark closer to the use cases of LLMs in the IP domain, called Patent-Bench, which serves a reference for comprehensive evaluation of LLMs in the IP domain.\nFuture work will focus on enhancing long-context support, aiming to achieve the ability to support up to 128k tokens to meet more diverse IP-oriented scenarios. Additionally, we will prioritize the accu-mulation of English pretraining corpora and SFT data to further improve our models' performance in English."}, {"title": "Examples of Synthetic Data", "content": "Combination of Patent Text and File Wrapper:\nIn this section, we introduce the method for constructing synthetic data using structured data from various sources along with the corresponding attribute labels.\nTaking the generation of synthetic data with File Wrappers and patents as an example:\nWe first extracted the patent numbers of the Patent under Examination and the Public Patent used as comparisons from the corresponding File Wrapper using named entity recognition (NER) to construct attribute labels.\nThen, utilizing these attribute labels, we located the claims of the Patent under Examination and the claims and description of the Public Patent from the patent database. These elements were con-catenated with the sections of the File Wrapper that expound the examination comments, forming synthetic data for pretraining. Table 5 and Figure 6 illuminate the English and Chinese examples of the mentioned process, respectively.\nExtraction of Similar Corpus Pairs in the File Wrapper:\nIn addition to combining patent texts and File Wrapper into a training corpus, we can also extract X-file text pairs (text pairs extracted from the patent under examnation and the public patent, respec-tively, representing the existence of technological connections between the two patents) from the File Wrapper, and the X-file text pairs can also be combined with other patent texts to form training data or instruction data. Table 7 shows the logic of generating English X-file text pairs, Table 8 shows the logic of generating Chinese X-file text pairs, and Table 9 shows the samples of X-file text combined with other patent text to form training data.\nFirstly, we extracted the description text for patent comparison by the examiner from the File Wrap-per; secondly, we extracted the original text (at the level of large paragraphs) for patent comparison mentioned in the File Wrapper from the patent under examnation and the public patent (the patent used for comparison); finally, we extracted the original sentence from the two patents according to the description text for patent comparison mentioned in the File Wrapper and the original text (at the level of large paragraphs) of the patent.\nWith training the similar corpus pairs (X-file pairs) that showed in Table 9, PatentGPT can be real-ized to capture the sentence pairs with the closest technical expression in patent comparisons, which meanwhile that the later application of the two patents may be at risk of infringement. This capability can greatly improve the efficiency and accuracy of IP practitioners in designing the scope of patent protection."}]}