{"title": "A Critical Assessment of Modern Generative Models' Ability to Replicate Artistic Styles", "authors": ["Andrea Asperti", "Franky George", "Tiberio Marras", "Razvan Ciprian Stricescu", "Fabio Zanotti"], "abstract": "In recent years, advancements in generative artificial intelligence have led to the development of sophisticated tools capable of mimicking diverse artistic styles, opening new possibilities for digital creativity and artistic expression. This paper presents a critical assessment of the style replication capabilities of contemporary generative models, evaluating their strengths and limitations across multiple dimensions. We examine how effectively these models reproduce traditional artistic styles while maintaining structural integrity and compositional balance in the generated images. The analysis is based on a new large dataset of AI-generated works imitating artistic styles of the past, holding potential for a wide range of applications: the \"AI-pastiche\" dataset. The study is supported by extensive user surveys, collecting diverse opinions on the dataset and investigation both technical and aesthetic challenges, including the ability to generate outputs that are realistic and visually convincing, the versatility of models in handling a wide range of artistic styles, and the extent to which they adhere to the content and stylistic specifications outlined in prompts.", "sections": [{"title": "1 Introduction", "content": "Generative AI has rapidly expanded into creative fields, transforming how visual art is produced, modified, and experienced. Early breakthroughs, such as StyleGAN [1, 2], laid the foundation for high-quality image synthesis, but the field has since been revolutionized by the rapid rise of diffusion-based models [3-5]. These newer techniques have significantly enhanced the ability to generate realistic images, mimic artistic styles [6, 7], and even create entirely new visual compositions [8, 9], establishing diffusion models as the dominant paradigm in generative artistry. Among these capabilities, style replication has emerged as a key area of interest, allowing users to apply diverse historical and modern artistic styles to AI-generated images [10-12]. This technology enables greater artistic expression and personalization, bridging the gap between computational creativity and traditional artistry and empowering artists, designers, and hobbyists to explore and reinterpret visual styles in ways that were previously highly specialized or time-intensive [13-16].\nThe purpose of this study is to provide a critical assessment of the capabilities and limitations of current generative tools in effectively replicating styles. By examining both the technical performance and aesthetic outcomes of these tools, the study aims to highlight their strengths, identify areas where they fall short, and offer insights into the potential improvements needed to enhance their application in creative fields.\nSpecifically, we compared twelve modern generative models comprising DallE3, StableDiffusion1.5, StableDiffusion3.5large, Flux1.1Pro, Flux1Schnell, Omnigen, Ideogram, Kolors1.5, FireflyImage3, LeonardoPheonix, Midjourney V6.1 and Auto-Aestheticsv1.\nThe models were compared using 73 uniform prompts that span a broad range of painting styles from the past five centuries. This resulted in the creation of a large supervised dataset of AI-generated artworks: the AI-pastiche dataset. This dataset not only provides labeled examples of AI-generated images but also offers a valuable resource for advancing research in areas such as deepfake detection, digital forensics, and the ethical study of AI-generated content. By supplying a controlled, high-quality set of deepfakes, the dataset aids in training and testing models for improved detection accuracy, robustness"}, {"title": "2 Related works", "content": "AI-driven artistic style transfer has grown significantly in recent years, driven by advances in deep learning and generative models. Several works have explored the capabilities, limitations, and applications of AI-generated imagery. Our work contributes with a comprehensive evaluation of multiple generative models, emphasizing their adherence to artistic style and prompt fidelity.\nEarly works such as Gatys et al. [21] laid the foundation for neural style transfer, introducing methods that blend content and style representations from convolutional neural networks. Subsequent research expanded on these concepts, improving efficiency and control over style application [22]. More recently diffusion-based models have demonstrated superior results in high-fidelity artistic synthesis, allowing for more nuanced style adaptation. Our study build upon these advancements but diverges in its focus on evaluating multiple state-of-the-art models across diverse artistic styles and historical periods. This allows for a broader assessment of model performance.\nOne major area of focus has been figuring out how to evaluate and detect AI-generated images. For instance, studies like CIFAKE by Bird and Lotfi [23] and GenImage by Zhu et al. [24] have worked on measuring how realistic synthetic images are and developing techniques to tell them apart form human-made art. Similarly, Li et al. [25] explored the world of adversarial AI-generated art, shedding light on the challenges of authentication and detection. These efforts are vital for assessing the authenticity of generated works, particularly in contexts where human perception plays a critical role.\nTo support this kind of research, several large scale datasets have been created.\n\\u2022 Artifact Dataset:[26] This is a diverse mix of real and synthetic images, covering everything from human faces to animals to landscapes, vehicles and artworks. It includes images synthesized by 25 different methods, including 13 GAN-based models and 7 diffusion models.\n\\u2022 WildFake Dataset: [27] A dataset designed to assess the generalizability of AI-generated image detection models. It contains fake images sourced from the open-source community, covering various styles and synthesis methods.\n\\u2022 TWIGMA Dataset: [28] A large-scale collection of AI-generated images scraped from Twitter, from 2021 to 2023, including metadata such as tweet text, engagement metrics and associated hashtags.\nWhile these studies focus on detecting AI-generated images we focus on examining how convincingly these images replicate human-created art. Through"}, {"title": "3 Methodology", "content": "In this section we outline our methodology for the creation of the dataset, the selection of models, and their evaluation."}, {"title": "3.1 Creation of the dataset, aims, methodology used for data acquisition", "content": "The most delicate point in the creation of the dataset was the definition of the prompts. The importance of providing well-structured prompts for style-transfer operations is well known, due to their direct impact on the quality and relevance of the generated outputs [31-33]. A clear and well-defined prompt eliminates ambiguity, ensuring that the model has a precise understanding of the desired style and content. Without this clarity, models can produce inconsistent or irrelevant results, making it difficult to achieve the intended artistic effect.\nIn our case, prompts were generated with the assistance of ChatGPT, iteratively fine-tuning its output until acceptable results were obtained across different generators. A sample was deemed \u201cacceptable\u201d if ChatGPT could recognize the required style in the generated image based on the prompt. Once finalized, the same prompt was passed to all models, and the results were compiled into a database accompanied by a rich set of metadata (see Section 4).\nAll prompts followed a common structure. They typically began with an indication of the style and historical period to imitate, sometimes reinforced by referencing a specific painter. This was followed by a detailed description of the subject, including suggestions for colors and tones. Finally, each prompt concluded with a hint about the overall sentiment or emotion the artwork was intended to convey.\nHere are a couple of examples:\n\\u2022 \"Generate a detailed winter landscape painting in the Flemish renaissance style of the second half of the XVI century. Depict a snow-covered village with small, rustic houses nestled into a hilly landscape. Include bare, slender trees in the foreground with hunters walking through the snow, accompanied by dogs. The scene should feature frozen lakes or ponds in the background, where villagers are skating and engaging in winter activities. The sky is a muted, wintry blue-gray, and the overall tone of the painting should evoke a peaceful, yet somewhat melancholic atmosphere, with intricate details showing rural life during winter.\"\n\\u2022 \"Generate a view of Venice in the vedutism style of the first half of the XVIII century, focusing on a scene along the Grand Canal. The composition features detailed classical architecture with grand domes and facades, and gondolas moving along the canal. Add soft clouds to the sky and ensure there is little fading in the horizon, providing clear visibility of distant buildings. The color palette should include very soft blues and warm earth tones, avoiding saturated colors. The atmosphere remains calm and luminous, with minimal light-and-shadow effects, capturing the beauty and grandeur of Venice from a broad perspective.\""}, {"title": "3.2 Models", "content": "Image generative models are a class of machine learning algorithms designed to synthesize novel images by learning the underlying patterns in existing data. By approximating the underlying distribution of visual data, these models generate outputs that form the foundation of various creative AI applications.\nWithin the domain of image generation, models are broadly categorized into Text-to-Image (Text2Img) and Image-to-Image (Img2Img) frameworks [34, 35], although hybrid and specialized approaches also exist. Text2Img models generate entirely new images based on textual descriptions, effectively translating linguistic cues into visual representations. In contrast, Img2Img models modify or enhance existing images by leveraging an input image as a reference while applying stylistic or contextual transformations. This study primarily focuses on Text2Img models due to their ability to create images purely from descriptive text prompts, making them particularly suited for analysing artistic style recreation.\nTo systematically evaluate the artistic fidelity and limitations of state-of-the-art (SOTA) commercial generative models, we selected 12 diffusion-based models, which are among the most widely used and highly regarded in the field. These models were identified based on their popularity and performance, as detailed in the Introduction. The selection was motivated by three key considerations:\n1. Benchmarking Established Models: Using well-established models enables the creation of a high-quality AI-generated art dataset, which could serve as a valuable resource for future research.\n2. Avoiding Training and Fine-Tuning Biases: Training a model from scratch or fine-tuning an existing open-source model would not provide a fair assessment of the out-of-the-box capabilities of these models. Our goal was to evaluate their pre-trained performance rather than their adaptability to new training objectives.\n3. Computational Constraints: Training or fine-tuning diffusion models is highly resource-intensive. Proprietary models, in particular, are trained on vast datasets with ongoing refinements by dedicated research teams, making them the most suitable candidates for assessing the current peak capabilities of image generative AI.\nInitially, 15 diffusion models were considered. Each model was tested using three standardized prompts to evaluate its ability to generate visually coherent and stylistically accurate images. Five researchers independently assessed the outputs based on realism, artifact minimization, and adherence to the prompt. A model was discarded if all five unanimously agreed it failed to meet these criteria. For example, DeepFloyd IF [36] was among the initial 15 models considered but was excluded from further experimentation. Its generated outputs frequently failed to align with the described artistic movements, particularly struggling with facial features and even simple object shapes (e.g., dogs and other animals)."}, {"title": "3.3 Evaluation", "content": "Models are evaluated based on two distinct and orthogonal criteria, each addressing a crucial aspect of their performance.\n\\u2022 Authenticity. The first criterion evaluates the model's ability to generate samples that are sufficiently realistic and convincing, such that they could be mistaken for artifacts created by a human. This involves assessing the quality of the generated output in terms of visual coherence, attention to detail, and overall believability. A high score in this area indicates that the model produces outputs that closely mimic human creativity and craftsmanship.\n\\u2022 Adherence to Prompt Instructions. The second criterion focuses on the model's capacity to accurately follow the detailed instructions specified in the prompt. This involves assessing how well the generated outputs align with the intended artistic style, thematic elements, or any specific requirements outlined. Success in this area demonstrates the model's ability to interpret and faithfully execute complex and nuanced instructions."}, {"title": "4 The AI-pastiche Dataset", "content": "AI-Pastiche is a carefully curated dataset comprising 953 AI-generated paintings in famous artistic styles. These images were produced using manually crafted text prompts and include comprehensive metadata describing their generation details. The dataset was created using 73 carefully crafted prompts, with over 20 images generated per prompt across the selected generative models (Section 3.2). From this pool, the highest-quality images were manually selected, ensuring fidelity to artistic styles and overall visual appeal."}, {"title": "4.1 Dataset Objectives", "content": "The two primary purposes of the dataset are:\n1. Analyzing the capabilities and limitations of SOTA generative models in accurately recreating well-known painting styles.\n2. Providing a high-quality AI-generated painting dataset for the research community, facilitating future studies on generative AI in artistic domains.\nWhile the current dataset consists of 953 carefully selected images, we plan to expand it in future iterations, incorporating additional artistic styles and more diverse prompts to further evaluate model performance and limitations."}, {"title": "4.2 Metadata and Composition", "content": "The AI-Pastiche dataset includes detailed metadata for each generated painting, summarized in Table 2. It is important to note that attributes such as subject, style, and period correspond to the intended description in the prompt rather than a direct analysis of the generated image itself.\nAt present, the dataset exhibits some stylistic imbalances, particularly in terms of artistic periods and movements. As shown in Table 3, the majority of paintings emulate XIX-th and the XX-th century styles, with Renaissance, Impressionism, Romanticism, and Baroque being the most represented artistic movements. In future expansions, we aim to mitigate these imbalances by incorporating a broader range of historical styles and more diverse prompts.\nUpon publication of this study, the AI-Pastiche dataset will be made publicly available on Kaggle to facilitate open research and further exploration of generative AI in artistic applications."}, {"title": "5 The surveys", "content": "In order to evaluate the performance of the models along the criteria discussed in the methodology section 3. we implemented and collected data from two distinct surveys."}, {"title": "5.1 Authenticity", "content": "With authenticity, we refer to the extent to which a model generates outputs that convincingly resemble human-made creations.\nThe evaluation was conducted using a survey-based approach, where participants were asked to classify images as either AI-generated or human-made. For the human-made paintings, we used a subset of open-access images from the National_Gallery_of_Art in Washington. Participants were shown a set of"}, {"title": "5.1.1 Adherence to Prompt Instructions", "content": "The purpose of this evaluation is to assess each generated image based on its alignment with the requirements specified in the given prompt.\nThis classification task is significantly more complex than the previous one, as it requires a careful reading and thorough understanding of the prompt, as well as a comparative evaluation of outputs from different models. For this reason, we decided to limit participation to a selected number of members, comprising people of our research group, colleagues of the department of fine arts, and some of their students. While the collective number of participants was sensibly smaller than for the first survey, each person evaluated multiple prompts, resulting in 5706 entries with an average of about 475 assessments for each model.\nWe also considered the possibility of conducting a fully automated evaluation using techniques like CLIP [47] or similar models. However, the ability of such embeddings to accurately capture nuanced factors such as artistic style, historical period, or other subtle attributes\u2014remains uncertain. The data collected through this survey can also serve to clarify this issue. This relates to our broader research program focused on investigating the aesthetic capabilities of Large Language Models, and we plan to address CLIP, among other models, in our future research.\nOur evaluation metric is based on the subjective assessment of how well each image reflects the requirements of the prompt. While it is theoretically possible to rank the generated images along a continuous scale, the inherent complexity of the task and the subjective nature of the evaluations led us to"}, {"title": "6 Results", "content": "In this section, we report and analyze the results of our survey. It is important to emphasize that our goal is not to compare the performance of different models, but rather to provide a clearer understanding of the current state of the field. Our focus is on identifying the persistent challenges faced by generative models, highlighting specific problem areas, and discussing potential directions for improvement. By examining these limitations, we aim to contribute to the broader discourse on how these models can be refined and enhanced for more reliable and aesthetically convincing outputs."}, {"title": "6.1 Authenticity", "content": "In Figure 1, we show the confusion matrix resulting from the survey: overall, around 28% of AI-generated images were mistakenly attributed to Humans. Interestingly enough, a slighly lower but still relevant number of Human-generated images were attributed to AI: in this case, the misclassification percentage is around 20%."}, {"title": "6.1.1 Distinction of results for cultural background", "content": "As mentioned in the introduction, we asked participants to disclose their cultural background to assess its potential impact on the perception of European paintings.\nIn this regard, the collected data are highly unbalanced, with European participants outnumbering non-European participants by approximately six to one. As a result, any analysis of this factor must be approached with caution, as the sample distribution may limit the reliability of our findings.\nThe only interesting result is relative to the misclassification rate for different historical periods, shown in Figure 6."}, {"title": "6.1.2 Influence of the subject", "content": "Our final investigation examines the influence of subject matter on the model's ability to generate artifacts that can be mistaken for human-made creations. For this analysis, we use the tags described in Section 4.2. Specifically, each prompt is represented as a multilabel binarization over its associated set of tags. We then perform a linear regression to predict the average degree of \u201cauthenticity\u201d, as determined by the survey, for all entries associated with those tags. The analysis is restricted to tags occurring in at least two different prompts.\nNaturally, we do not expect to obtain a highly accurate estimation, as additional such as the required style and historical period also play a role. However, our focus is not on the predicted output itself but rather on the weights assigned by the model to different tags, particularly negative tags, which may indicate categories that present challenges for the models.\nAfter normalizing the output using Gaussian normalization, we obtain a prediction error of approximately 0.4 (compared to the unit standard deviation). As expected, the prediction accuracy is not particularly high, but it is sufficient to demonstrate a correlation between tags and perceived authenticity.\nIn Figure 7 we show the weights associated with the different tags. We do the investigation for all models (blue) and for a restricted subset of models comprising Ideogram, Midjourney, Stable-Diffusion-3.5-large and Dall.E, obtaining high scores both in authenticity and prompt adherence.\nLooking at the negative scores, a notable group is composed by tags related to humans: \"crowd\u201d, \u201cperson\u201d, \u201cpersons\u201d, \u201cchild\u201d, and \u201cportrait\u201d. This provides strong evidence that generative models still struggle to represent humans convincingly when mimicking artistic painting. In addition, portraits of women tend to present more challenges compared to those of men.\nThis difficulty may arise from several, sometimes contrasting, factors. For example, generative models may fail to achieve realism in highly complex and dynamic scenes involving multiple people or crowds, while at the same time, they may adopt an exaggerated hyperrealism in portraits. We discuss these issues in more detail in Section 7.\nFrom the naturalistic point of view, \u201cclouds\u201d, \u201cflowers\u201d and \u201cwater\u201d seem to have a negative impact. The tag \"flower\" contrasts with \u201cstill_life\", which, by comparison, has a significantly more positive score. In our dataset, the negative perception associated with flowers seems to be primarily linked to paintings in na\u00eff style one of the styles where generative models, as observed in the previous section, tend to perform the worst. The negative scores for \"clouds\u201d and \u201cwater\u201d appear to stem from the inherent complexity of rendering\""}, {"title": "6.2 Adherence to Prompt Instructions", "content": "This survey measured user satisfaction based on the alignment of generated images with the requirements specified in the given prompt, considering both content and style. Users rated their evaluations in three categories: \u201cGood\u201d \u201cMedium\u201d, and \u201cLow.\u201d The evaluation was not intended to be absolute, but rather comparative, assessing how each model's output performed relative to others.\nFor instance, if a particular image was unanimously classified as \"Good,\u201d this does not necessarily imply that it was a highly satisfactory interpretation of the prompt. Rather, it simply indicates that, in the collective judgment of the reviewers, it outperformed the outputs of other models.\nReviewers were encouraged to give a balanced repartition in the three categories, to reduce the impact of the prompt complexity, and inherent variability within each batch of images.\nTo derive a summary score, we computed a weighted average, assigning a value of 1 to \u201cGood\u201d, 0 to \u201cMedium\u201d, and -1 to \u201cLow\u201d .\nThe results are summarized in Figure 8 and Table 8. Again, in the Table we only list the most performant models, according to out investigation."}, {"title": "7 Critical aspects of artificial generation", "content": "In this section, we highlight some of the most common and critical challenges observed in the generative models under consideration. These insights stem both from our direct experience in dataset creation and from the results of our surveys.\nWe structure the discussion around three major problem areas: Artifacting and Distortion (Section 7.1), Hyperrealism (Section 7.2) and Anachronisms (Section 7.3)."}, {"title": "7.1 Artifacting and Distorsion", "content": "One of the most evident problems is artifacting and distortion, where models fail to maintain anatomical coherence or structural integrity. A few major instances are discussed below."}, {"title": "7.1.1 Fingers, Hands and Limbs", "content": "Correctly rendering hands and fingers remains one of the major challenges in generative image synthesis. The problem is common to most of the models: some examples are given in Figure 9\nThe problem is well known and stems from several factors. The primary challenge is that hands frequently interact with objects or other parts of the body, leading to complex occlusions and overlapping regions. This poses difficulties both during training, where the model must abstract hands and fingers from their specific context, and during generation, where the model must realistically render them within the context of these interactions."}, {"title": "7.1.2 Distorsions in complex scenarios", "content": "Distortions become more pronounced in complex scenarios, such as groups of people, highly dynamic scenes, or intricate architectural compositions. In these cases, maintaining a natural balance between stylistic accuracy and structural coherence remains a challenging task for most models. A few typical examples are shown in Figure 11, but nearly all models struggled with these specific prompts: (a) a traditional rural festival in the 19th-century realism style, (b) a music lesson in the Rococo style, and (c) a battle between knights in the early Renaissance style.\nOne frustrating limitation of current generative models is their inability to dynamically adjust generation time based on the complexity of the task. Unlike human artists, who naturally dedicate more time to intricate compositions while completing simpler ones more quickly, these models follow a fixed computational budget, regardless of the difficulty of the image being generated.\nFor instance, diffusion-based models operate within a predefined number of denoising steps, meaning they do not inherently \u201crealize\u201d when an image requires additional refinement to resolve ambiguities in structure, perspective, or stylistic details. Whether generating a minimalist still life or a highly detailed historical battle scene, the model performs the same number of steps, often leading to overprocessing in simple cases and underdeveloped details in complex ones."}, {"title": "7.2 Hyperrealism", "content": "Most generative models, often optimized for photorealism, struggle to reproduce the unique nuances and distinctive qualities characteristic of artistic styles from the past. We shall discuss the issues in three paradigmatic cases: portraits, still lifes, and landscapes."}, {"title": "7.2.1 Portraits", "content": "Modern generative models often exhibit a hyperrealistic tendency when replicating facial details, often in contrast with the historical artistic style they were supposed to mimic according to the prompt. This excessive sharpness and detail can create a fundamental mismatch between the expected stylistic conventions and the generated output, leading to images that feel anachronistic or unconvincing. A few examples are given in Figure 12\nThe problem becomes even more apparent when considering historical limitations in artistic materials and techniques. Painters working with oil or tempera could not achieve the pore-level skin textures or ultra-sharp reflections that modern models tend to generate by default. When a generative model introduces such hyperrealistic details into a Rinascimental painting or a 17th-century Dutch portrait, the output no longer aligns with the stylistic expectations of that period."}, {"title": "7.2.2 Still lifes", "content": "Another common subject where generative models struggle to restrain their tendency toward excessive realism is still life painting. While still lifes often contain highly detailed depictions of objects, traditional artistic styles especially those from historical periods-frequently employ soft lighting, controlled textures, and a painterly touch that distinguishes them from hyperrealistic renderings.\nGenerative models, however, tend to overemphasize surface details, reflections, and textures, producing results that lean toward photographic realism rather than adhering to the stylistic characteristics of classical still life compositions. This issue becomes particularly noticeable in flower arrangements, fruit compositions, and table settings, where the AI-generated images may include overly sharp edges, unnatural glossiness, or exaggerated depth-of-field effects that are inconsistent with traditional oil painting techniques.\nA few typical examples are shown in Figure 13."}, {"title": "7.2.3 Landscapes and cityscapes", "content": "As evidenced by the tag analysis in Section 6.1.2, the most challenging elements in the representation of naturalistic scenes are clouds and water, particularly when combined with specific times of the day such as dawn or sunset-or when the prompt demands highly dramatic atmospheric effects. These conditions require a delicate interplay of light, color gradients, and reflections, which can be difficult for generative models to reproduce in a way that remains both visually coherent and stylistically faithful.\nSuccessfully depicting clouds and water often requires a nuanced understanding of texture, movement, and atmospheric perspective. While most"}, {"title": "7.3 Anachronisms", "content": "Not infrequently, models may add anachronistic elements in the painting, completely disrupting the historical setting, and often creating unintentionally humorous effects. A typical example is the van in the middle of the scene of Figure 16.a, inspired to the style of Pieter Bruegel the Elder.\nThe artwork of Figure 16 (b) was supposed to be a watercolor inspired by the Realism Art of the Industrial Revolution period, depicting an open-cut mine with industrial activity. Almost all models filled the scene with modern crane-like machines and trucks."}, {"title": "8 Conclusions", "content": "In this work, we have explored the capabilities and limitations of modern generative models in replicating historical artistic styles. Our analysis is structured around two main contributions: (1) the creation of a large, supervised dataset of AI-generated artworks- the AI-Pastiche dataset- and (2) a comprehensive evaluation of generative models through user surveys assessing perceptual authenticity and prompt adherence.\nThe AI-Pastiche dataset is a richly annotated collection of AI-generated images, categorized by model, style, period, and subject matter. It serves as a valuable resource for analyzing the strengths and weaknesses of different generative approaches, holding potential for a wide range of applications and providing a benchmark for future research on AI-driven artistic replication.\nUsing the AI-Pastiche dataset, we conducted a systematic evaluation of generative models based on extensive user surveys. We separately assessed perceptual authenticity-how convincingly an artwork mimics human-created paintings and prompt adherence\u2014how faithfully the output aligns with the given instructions. The results reveal a key trade-off: some models prioritize aesthetic quality over strict adherence to the prompt, while others sacrifice visual refinement for greater accuracy. This discrepancy underscores the challenges in balancing creative flexibility and control in generative image synthesis."}, {"title": "Declarations", "content": "Funding. Research partially supported by the Future AI Research (FAIR) project of the National Recovery and Resilience Plan (NRRP), Mission 4 Component 2 Investment 1.3 funded from the European Union - NextGenerationEU.\nConflict of Interest. On behalf of all authors, the corresponding author states that there is no conflict of interest."}]}