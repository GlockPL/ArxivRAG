{"title": "Leveraging Mixture of Experts for Improved Speech Deepfake Detection", "authors": ["Viola Negroni", "Davide Salvi", "Alessandro Ilic Mezza", "Paolo Bestagini", "Stefano Tubaro"], "abstract": "Speech deepfakes pose a significant threat to personal security and content authenticity. Several detectors have been proposed in the literature, and one of the primary challenges these systems have to face is the generalization over unseen data to identify fake signals across a wide range of datasets. In this paper, we introduce a novel approach for enhancing speech deepfake detection performance using a Mixture of Experts architecture. The Mixture of Experts framework is well-suited for the speech deepfake detection task due to its ability to specialize in different input types and handle data variability efficiently. This approach offers superior generalization and adaptability to unseen data compared to traditional single models or ensemble methods. Additionally, its modular structure supports scalable updates, making it more flexible in managing the evolving complexity of deepfake techniques while maintaining high detection accuracy. We propose an efficient, lightweight gating mechanism to dynamically assign expert weights for each input, optimizing detection performance. Experimental results across multiple datasets demonstrate the effectiveness and potential of our proposed approach.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, methods for generating or modifying synthetic media have become increasingly widespread and accessible. In the audio domain, the use of deepfakes involves the synthesis of speech using the voice of a target speaker, making them say arbitrary utterances. This kind of AI-generated media pose a synthetic threat to the integrity of digital content, especially as the techniques used to generate them evolve at an alarming pace [1], [2]. To counter this threat, researchers have proposed various speech deepfake detection methods, employing a range of model architectures, processing techniques, and strategies [3], [4]. Likewise, extensive research has been devoted to assessing the robustness of the developed systems to assess their applicability in real-world conditions [5]-[7]. Although these systems have achieved remarkable performance in controlled scenarios [8]-[11], the challenge of effectively generalizing their capabilities to unseen data and increasingly sophisticated speech deepfakes remains critical. This issue is further complicated by the unique characteristics of the state-of-the-art speech deepfake datasets, which include diverse generation methods and multilingual data, making it reasonable to consider them as distinct domains. For these reasons, developing flexible tools capable of dealing with increasingly variable scenarios is becoming crucial.\nTo tackle this challenge, we propose a novel method based on a Mixture of Experts (MoE) framework. MoEs are hierarchical models comprising an ensemble of N experts governed by a probabilistic gating function [12]\u2013[15]. While all experts work toward the same goal, such as classification or regression, the gating function dynamically determines the contribution of each expert in inferring the final output. In particular, the expert weights are typically calculated by applying a softmax function to the logits produced by the gating network so that each weight can be interpreted as the percentage of involvement of each expert in the decision-making process relative to the given input sample.\nIn the original formulation by [12], both the experts and the gating functions were implemented using feedforward neural networks and jointly trained to minimize a single objective function in an end-to-end fashion. This way, the gating network proportionally distributes the gradient flow across all experts, effectively rewarding the experts with the best performance. In turn, this encourages the subdivision of the problem space into homogeneous regions [16] and localizes each expert onto a specific distribution on the input feature space via an underlying competitive learning process. Indeed, this family of MoEs, also known as mixtures of implicitly localized experts [14], are rooted in the idea of partitioning a complex task into sub-problems easier for each expert to learn and solve, fundamentally amounting to a divide and conquer strategy.\nWe adapt the MoE framework to the speech deepfake detection task by leveraging expert specialization across different domains, i.e., state-of-the-art datasets. To the best of our knowledge, this work constitutes the first application of the MoE architecture in speech deepfake detection. Our findings indicate that our approach shows promise in enhancing generalization and adaptability to a range of deepfake techniques, making it a valuable contribution to the speech deepfake detection domain."}, {"title": "II. PROPOSED METHOD", "content": "The speech deepfake detection problem is formally defined as follows. Let us consider a discrete-time input speech signal x sampled at a frequency $f_s$ and associated with a class $y \\in \\{0,1\\}$, where 0 denotes that the signal is authentic and 1 indicates that it has been synthetically generated. The goal of this task is to develop a detector D that estimates the class of the signal x as $\\hat{y} \\in [0, 1]$, where $\\hat{y}$ is the likelihood of the signal x being fake.\nIn the proposed approach, we design the detector D as a Mixture of Experts (MoE) specifically tailored for the speech deepfake detection task. Our MoE system is composed of multiple detectors, each pre-trained on a different speech deepfake dataset, thus becoming an \u201cexpert\" in that domain. We denote the i-th expert as $E_i$, where $i \\in \\{1,2,...,N\\}$, and N represents the total number of considered domains. While all experts are fed with the same audio signal x, the input of the gating network G varies based on the chosen MoE implementation.\nWe consider two different architectures for the MoE framework, which we will later refer to as standard and enhanced, respectively. These are both dense MoEs [15], meaning that every expert $E_i$ is queried for each input x.\nThe standard model is a classic MoE inspired by [12], where both G and the experts $E_i$ directly receive the input audio excerpt x. Each expert returns a vector of logits as\n$z_i = E_i(x),$"}, {"title": "A. Problem Formulation"}, {"title": "B. Proposed Systems", "content": "In the proposed approach, we design the detector D as a Mixture of Experts (MoE) specifically tailored for the speech deepfake detection task. Our MoE system is composed of multiple detectors, each pre-trained on a different speech deepfake dataset, thus becoming an \u201cexpert\" in that domain. We denote the i-th expert as $E_i$, where $i \\in \\{1,2,...,N\\}$, and N represents the total number of considered domains. While all experts are fed with the same audio signal x, the input of the gating network G varies based on the chosen MoE implementation.\nWe consider two different architectures for the MoE framework, which we will later refer to as standard and enhanced, respectively. These are both dense MoEs [15], meaning that every expert $E_i$ is queried for each input x.\nStandard MoE. The standard model is a classic MoE inspired by [12], where both G and the experts $E_i$ directly receive the input audio excerpt x. Each expert returns a vector of logits as\n$z_i = E_i(x),$\nwhich could in principle be used for classification already. The output of the gating network G is\n$a = g(x),$\nwhere $a \\in \\mathbb{R}^N$. The i-th element of a is denoted $a_i$ and acts as a weight for the output of the i-th expert. The final MoE output is obtained as\n$z = \\sum_{i=1}^{N} a_i z_i$\nThe final prediction score $\\hat{y}$ of the complete speech deepfake detector D is obtained by applying a softmax function to the weighted logits vector z.\nThe enhanced MoE architecture differs from the standard MoE described above in that the gating network, instead of being fed the raw audio signal x, relies on the internal representations produced by the N experts. In this design, the gating network uses the first few layers of the experts as embedding extractors and takes as input a concatenation of the embeddings from each expert, along with a combined embedding."}, {"title": "Enhanced MoE"}, {"title": null, "content": "Formally, each expert embedding $e_i \\in \\mathbb{R}^d$ is extracted from the last batch normalization layer of $E_i$. The combined embedding $w \\in \\mathbb{R}^d$, in turn, is created through a learnable weighted element-wise multiplication of $e_1, ..., e_N$. Considering N = 4 and d = 64, the combined embedding is calculated as\n$w = (e_1 \\odot e_2 \\odot e_3 \\odot e_4) \\odot p,$\nwhere $\\odot$ denotes element-wise multiplication, and $p \\in \\mathbb{R}^d$ is a learnable parameter. The final concatenated embedding vector $e_{input}$ is formed by\n$e_{input} = [e_1^T, e_2^T, e_3^T, e_4^T, w^T]^T.$\nThis specific design is intended to fully leverage the domain-specific knowledge acquired during pre-training to enhance the overall performance. We draw inspiration for this approach from [17]. The output of the gating network is\n$a = G(e_{input}),$\nwhere $a \\in \\mathbb{R}^N$. The final MoE output is obtained as in (3) and the final prediction score $\\hat{y}$ is obtained likewise."}, {"title": "III. EXPERIMENTAL SETUP", "content": "Our proposed method utilizes a MoE framework with four experts, each trained on a specific speech deepfake dataset. By integrating these experts, we aim to leverage their specialized knowledge to develop a speech deepfake detector with enhanced detection capabilities for both seen and unseen deepfake datasets.\nTo ensure consistency across all our experiments, we used a single speech deepfake detection model, a LCNN model [18], as both a baseline and expert within the MoE framework. Despite its simplicity, this lightweight CNN-based model is renowned for its competitive and reliable performance, even when compared to more complex systems. For this reason, it has been considered as a baseline in the ASVspoof 2021 challenge [19]. Our version of the LCNN processes mel-spectrograms from the input audio. We trained four separate LCNN models on distinct datasets, with each model serving as an expert in our MoE framework. Additionally, we trained a single LCNN model on a combination of all the training datasets. We consider this model as a first baseline, as it benefits from exposure to all training datasets simultaneously, allowing it to better acquire knowledge from and generalize across diverse data distributions. As a second baseline, we consider the average ensemble as a simple, input-independent approach for combining scores from multiple experts [20], making it a particularly suitable baseline to compare with the proposed MoE systems.\nEach LCNN model processes 3 seconds of audio, corresponding to 48000 audio samples. Inputs shorter than this are padded with repeated data to meet the required length, while the longer ones are truncated. We trained each individual expert for 100 epochs, monitoring the validation loss computed using the Cross-Entropy function. We employed early stopping with a patience of 20 epochs, used a batch size of 128, and applied the AdamW optimizer with an initial learning rate of 10-4, adjusted according to a cosine annealing schedule. Each batch was balanced to include an equal number of samples from both the real and fake classes.\nIn both the standard MoE and enhanced MoE, the gating network architecture comprises a single fully-connected layer followed by dropout, batch normalization, and a Leaky ReLU activation function. At training time, all experts and the gating network were trained jointly, the former starting from the pre-trained weights, while the latter is randomly initialized. To do so, we used the same setup as for individual experts, except the batch size was reduced to 64. The code used to train and test all the models is available online.\u00b9"}, {"title": "A. Speech Deepfake Detectors"}, {"title": "B. Datasets", "content": "During our experimental campaign, we employed six different speech deepfake detection datasets D. All audio data were sampled at a frequency of $f_s$ = 16 kHz.\nReleased for the homonymous challenge, this dataset was designed to develop effective ASV models. It includes real speech from the VCTK corpus [22] and synthetic speech fabricated by 19 different synthetic speech generators. Data are distributed unevenly across the dataset partitions to enable open-set evaluation. We employ the Logical Access partition of this corpus.\nThis dataset contains over 198000 utterances of both real and synthetic speech. The synthetic data were generated using various Text to Speech (TTS) systems, including both open-source and commercial tools. The real data are sourced from a diverse range of open-source speech datasets and additional sources, (TED Talks, YouTube, etc.).\nThis is a synthetic speech dataset released for the homonymous challenge. It contains real and synthetic speech data simulating realistic scenarios, such as background noises and disturbances. All the tracks contained in this set come from Chinese speakers. We include it in our experiments to introduce linguistic diversity, as all other datasets we consider consist of English speech.\nThis dataset aims to evaluate speech deepfake detectors in realistic environments by providing in-the-wild speech data. It consists of nearly 38 hours of audio, evenly split between fake and real speech. The fake clips were created by segmenting publicly accessible video and audio files, while the real clips come from publicly available material featuring the same speakers. We divided the dataset into three partitions: 60% for training, 20% for development, and 20% for evaluation, ensuring a balanced distribution of real and fake samples in each split.\nThis corpus includes 25000 synthetic speech tracks generated by five advanced diffusion model-based voice cloning methods: ProDiff, DiffGAN-TTS, ElevenLabs, UnitSpeech, and XTTS. The real speech data were sourced from LJSpeech [26] and LibriSpeech [27].\nThis is a speech dataset that includes only fake audio samples, generated from 12 different TTS methods. It is created based on the VidTIMIT corpus [29] by generating a synthetic copy of its 430 tracks using all the considered synthetic speech generators. We consider the clean partition of this corpus.\nWe used four of these datasets (DASV, DFOR, DADD, Ditw) for both training and testing the proposed models, while the remaining two (DPUR, DTIM) were employed only in the testing phase, to evaluate the detection capabilities of the considered systems on unseen data."}, {"title": null, "content": "Released for the homonymous challenge, this dataset was designed to develop effective ASV models. It includes real speech from the VCTK corpus [22] and synthetic speech fabricated by 19 different synthetic speech generators. Data are distributed unevenly across the dataset partitions to enable open-set evaluation. We employ the Logical Access partition of this corpus."}, {"title": null, "content": "This dataset contains over 198000 utterances of both real and synthetic speech. The synthetic data were generated using various Text to Speech (TTS) systems, including both open-source and commercial tools. The real data are sourced from a diverse range of open-source speech datasets and additional sources, (TED Talks, YouTube, etc.)."}, {"title": null, "content": "This is a synthetic speech dataset released for the homonymous challenge. It contains real and synthetic speech data simulating realistic scenarios, such as background noises and disturbances. All the tracks contained in this set come from Chinese speakers. We include it in our experiments to introduce linguistic diversity, as all other datasets we consider consist of English speech."}, {"title": null, "content": "This dataset aims to evaluate speech deepfake detectors in realistic environments by providing in-the-wild speech data. It consists of nearly 38 hours of audio, evenly split between fake and real speech. The fake clips were created by segmenting publicly accessible video and audio files, while the real clips come from publicly available material featuring the same speakers. We divided the dataset into three partitions: 60% for training, 20% for development, and 20% for evaluation, ensuring a balanced distribution of real and fake samples in each split."}, {"title": null, "content": "This corpus includes 25000 synthetic speech tracks generated by five advanced diffusion model-based voice cloning methods: ProDiff, DiffGAN-TTS, ElevenLabs, UnitSpeech, and XTTS. The real speech data were sourced from LJSpeech [26] and LibriSpeech [27]."}, {"title": null, "content": "This is a speech dataset that includes only fake audio samples, generated from 12 different TTS methods. It is created based on the VidTIMIT corpus [29] by generating a synthetic copy of its 430 tracks using all the considered synthetic speech generators. We consider the clean partition of this corpus."}, {"title": null, "content": "We used four of these datasets (DASV, DFOR, DADD, Ditw) for both training and testing the proposed models, while the remaining two (DPUR, DTIM) were employed only in the testing phase, to evaluate the detection capabilities of the considered systems on unseen data."}, {"title": "IV. RESULTS", "content": "As a first experiment, we evaluate the effectiveness of the proposed approach on the speech deepfake detection task. We refer to the classic MoE as MoE (standard) and to our customized MoE as MoE (enhanced), which utilizes the hidden representations of the experts. We evaluate these systems against the performances of the four individual experts (denoted as EASV, EFoR, EADD and Eitw), an averaging ensemble of such experts, and the LCNN model trained jointly at once on all the four training datasets.\nThe results are presented in Table I, in terms of Equal Error Rate (EER) and Area Under the Curve (AUC). The \u201cKnown\" column indicates the average performance on the evaluation partitions of the datasets DASV, DFOR, DADD and Ditw, which were used during training, while the \"All\" column reflects the average performance across all the considered datasets, including DPUR and DTIM.\nThe results reveal that the MoE models, particularly the MoE (enhanced), outperform other approaches in both EER and AUC, demonstrating their effectiveness in handling known and unseen datasets. Specifically, the MoE (enhanced) achieves the lowest EER, with scores of 10.90% on known datasets and 8.85% across all domains. The joint training strategy and the MoE (standard) also perform well but are slightly less effective. They achieve EER scores of 11.23% and 11.05% on known datasets, and 10.10% and 9.35% on all sets, respectively. The ensemble method delivers moderate performance, as it outperforms all the individual experts, but it is surpassed by both the joint training and MoE approaches. On the other hand, the individual expert models (Easy, EFOR, EADD, and Eitw) show varied performance. Some experts excel on specific datasets but underperform on others. Notably, the EADD expert exhibits inconsistent behavior, performing exceptionally well on DFOR, but worse on DADD, its own training dataset. This counterintuitive result is likely due to the dataset's construction, which involves disjoint deepfake algorithms between the training and testing partitions, presenting a significant challenge for the detectors. In this regard, we observe that no system excels on DADD, all of them showing relatively poor results. This leads to an unusual situation where the average performance on the \"known\" datasets is often worse than on all datasets, as DPUR and DTIM seem not to be as challenging as DADD."}, {"title": "A. Detection Performance"}, {"title": "B. Gating Network Analysis", "content": "We now perform a further analysis of the gating network in the best-performing system, i.e., MoE (enhanced). By examining the average gating weights across different datasets, we aim to evaluate several aspects: (1) whether the gate selects the expert for which pre-training and test dataset coincide, (2) the selectiveness of the gating weights' softmax distribution, (3) the relative contribution of each expert to predictions on speech samples from unknown datasets.\nFigure 1 shows the results of this analysis for each individual dataset. For the datasets DASV, DFOR, and Ditw, the expert with the highest average weight is the one specifically pre-trained on that dataset. This was largely expected since the corresponding experts showcase high performance on the respective datasets. Conversely, for DADD, the experts EADD and Etw show nearly equal importance. This observation aligns with the results in Table I, where EADD performs better than other experts within its own domain but does not excel to the same extent as they do within theirs. This likely explains why the gating network assigns substantial weight to Eitw, which performed second-best on DADD. The analysis results for the two unknown datasets, DPUR and DTIM, provide intriguing insights. For DPUR, which includes fake clips generated by advanced diffusion models, the expert trained on DADD emerges as the most relevant. In contrast, for DTIM, all experts contribute almost equally to the final predictions. These results underscore the effectiveness of a MoE-based approach and a gating network system in effectively integrating multiple experts for the task at hand. Moreover, it is noteworthy how the gating network can also serve as a valuable analysis tool, offering insights into the similarity between different domains, enabling us to better understand the relationship between them."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we present a novel speech deepfake detection method based on the MoE framework. Our approach leverages the divide-and-conquer strategy inherent to MoEs, aiming to address the generalization challenges faced by current deepfake detectors. Extensive evaluations across multiple state-of-the-art datasets demonstrate the effectiveness of our approach in managing data from diverse domains. As this is an initial exploration of applying MoEs to speech deepfake detection, future work will focus on experimenting with new MoE architectures and learning strategies for the gating network. Additionally, we plan to assess the scalability of our model by increasing the number of experts."}]}