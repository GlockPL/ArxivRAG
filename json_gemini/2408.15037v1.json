{"title": "Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering", "authors": ["Haowei Du", "Huishuai Zhang", "Dongyan Zhao"], "abstract": "To address the hallucination in generative question answering (GQA) where the answer can not be derived from the document, we propose a novel evidence-enhanced triplet generation framework, EATQA, encouraging the model to predict all the combinations of Question, Evidence, Answer) triplet by flipping the source pair and the target label to understand their logical relationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a QE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap to distill the knowledge from evidence in inference stage. Our framework ensures the model to learn the logical relation between query, evidence and answer, which simultaneously improves the evidence generation and query answering. In this paper, we apply EATQA to LLama and it outperforms other LLMs-based methods and hallucination mitigation approaches on two challenging GQA benchmarks. Further analysis shows that our method not only keeps prior knowledge within LLM, but also mitigates hallucination and generates faithful answers.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) represent a significant milestone in the development of general artificial intelligence (Brown et al., 2020; Touvron et al., 2023; Chowdhery et al., 2023). While these models have demonstrated unprecedented performance across various general tasks, they still face a series of challenges, including issues such as hallucination (Tonmoy et al., 2024) and handling long contexts (Jin et al., 2024). In document-based generative question answering (GQA) (Lewis and Fan, 2018), model may generate answers which are inconsistent with the document or mismatch the query, well known as hallucinations (Gunjal et al., 2024; Liu et al., 2024). Recent works utilize an external model to retrieve relevant information and agument the factuality of generation. However, the intrinsic discrepancy between retriever and LLM may introduce information which is only surface relevant but not helpful to answer the question (Salemi and Zamani, 2024).\nTo enhance the logical reasoning and avoid the misleading information, we highlight the supporting evidence of the answer in document-based QA. Instead of the retrieve-then-read pipeline, we let the LLM to tackle the generation of evidence and answer in a unified triplet generation framework, where each two of serve as the input in the respective instruction to generate the other. The evidence is utilized to reconstruct the question to ensure the model captures its logical relations to question and answer instead of superficial relevance.\nWe take an example in MultiRC (Khashabi et al., 2018) dataset in Figure 1. The question is \"After the Osprey resumed flights how long did it take for the Air Force to begin using the aircraft? \" and the answer can not be extracted from a single sentence in the document. To answer this question, the model needs to find the multiple evidence \"Osprey resumed flights in 2002\u201d and \u201cAir Force began using Ospreys in 2008 after testing the aircraft in 2006\u201d, and decide the answer is \"4 years\". If the model is distracted by the incorrect evidence \"Marines develop the aircraft in Iraq in 2007\", it will derive the"}, {"title": "2 Related Work", "content": "Generative question answering (GQA) aims to generate an abstractive answer rather than extract an answer to a given question from provided passages (Fan et al., 2019; Li et al., 2021). Early works on GQA mostly tried to improve the faithfulness of the answer by investigating reliable external knowledge sources or incorporating multiple information sources. Yin et al. (2015) propose Neural Generative Question Answering, an end-to-end model that generates answers to simple factoid questions based on the knowledge base, while Bi et al. (2019) propose the Knowledge-Enriched Answer Generator (KEAG) to generate a natural answer by integrating facts from four different information sources, namely, questions, passages, vocabulary, and knowledge.\nRecent works focus more on the conditional generation model. Li et al. (2021) propose Rationale-Enriched Answer Generator (REAG), in which they add an extraction task to obtain the rationale for an answer at the encoding stage, and the decoder is expected to generate the answer based on both the extracted rationale and original input. Su et al. (2022) propose a framework named RBG (read before generate), to jointly models answer generation with machine reading. They augment the generation model with fine-grained, answer-related salient information predicted by the MRC module, to enhance answer faithfulness. Such methods can exploit and utilize the information in the original input better, while they require the extra effort of building models to extract that information. CAD (Shi et al., 2023) follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. RHO (Ji et al., 2023) introduce"}, {"title": "3 Methodology", "content": "In this part, first we introduce the document-based GQA task and the model architecture of Triplet-QA. Then we present the unified triplet generation framework, which predicts all the combinations of (Question, Evidence, Answer) triplet by flipping the source pair and the target label to learn their logical relationships. Specifically, our method contains 3 subtasks, answer-aware evidence retrieval, evidence-aware query answering and evidence-aware query restoration, which is shown in Figure 2 from upper to down.\nThe motivation of triplet generation framework comes from that the posterior probability of question answering is positively proportional to the probability of evidence generation and question recovery by Bayesian formulation:\n$\\PM(alq, e, d) = \\frac{PM(a, q, e, d)}{PM(q, e, d)} = \\frac{PM(q, e, d)}{\u0420\u041c(\u0430, q)PM(ela, d)PM(q|e, a, d)}$\n(1)\nwhere d, q, e, a denote the document, question, evidence and answer. It shows the accuracy of QA should be proportional to the accuracy of evidence extraction and question restoration. We assume the evidence sentences contain the sufficient information to reconstruct the question, i.e. $PM(q|e, a) = PM(qle, a, d)$.\nAs the basis of our framework, we explain the feasibility of our method. Take query restoration as an example: in Figure 1, only from the answer \"4 years\", the model is hard to restore the query because there may be multiple sentences involving \"4 years\" in the document. However, given the evidence sentences which point to the key events \u201cOsprey resumed flights\u201d and \u201cAir Force begin using the aircraft\", the model has derived the basic part of the query and it enables our query restoration module reasonable, which improves the model ability to arrange the information to restore the query.\""}, {"title": "3.1 Preliminary", "content": "Document-base GQA task aims to generate the answer of the natural text question based on the document including multiple sentences:\nD = [$1,$2,\uff65\uff65\uff65,Sn]\n(2)\nwhere D denotes the document, si denotes the i-th sentence in the document and n denotes the number of sentences. The model can be formulated as a function of\nn\nfM(a) = [P(ai|a1, a2,\u2026, ai\u22121, q, D)\ni=1\n(3)\nwhere n denotes the answer length, q denotes the query and ao denotes the begin-of-speech (BOS) token. Generally the answer has flexible forms which can not be directly extracted from the document."}, {"title": "3.2 Model Architecture", "content": "EATQA is built on the widespread LLM, LLama (Touvron et al., 2023) with a few additional learnable parameters. we additionally adopt several trainable adapter tokens p = [P1,P2,\u2026,PNp] which are prepended to the key and value of each self-attention layer, where Np is the number of adapter tokens. So the number of trainable parameters of EATQA 7B is 4.5M, only 0.06% of total parameters of LLama 7B. With such a few trainable parameters, EATQA effectively preserves LLMs' prior knowledge and the casual reasoning"}, {"title": "3.2.1 Answer-Aware Evidence Generation\n(QA->E)", "content": "In this part, we model the probability of supporting evidence extraction for the query-answer pair PM(ela, d). We design the instruction and ask the LLM to generate the evidence which supports the query and the corresponding answers. Therefore, the input to model is the instruction, source document, the query and the corresponding answer. The output of model is the supporting evidence. The specific instruction is \u201cgenerate the relevant evidence from the document to answer the following question\" and we insert the document, question and answers into the template in Figure 5.\nAs for the loss function, by Bayesian Formula (Mises, 1942) we derive\nlog(P(e, q)) = log P(e, q, a)da\n=log\\frac{q(ale, q)}{\\int \\frac{P(e, q, a)}{ q(ale, q)} da}\n\u2265q(ale, q)log(\\frac{P(e, q,a)}{q(ale, q)}) da\n= Eq(ale,q) log(\\frac{P(e, q, a)}{q(ale, q)})\n= Eq(ale,q) log(\\frac{P(a, q)P(ela, q)}{q(ale, q)})\n= Eq(ale,g) log(P(ela, q)) + Eq(ale,g) log(P(a, q))\n= Eq(ale,q) log(P(e|a, q)) \u2013 KL(P(a,q)||qa|e, q))\n(4)\nwhere q(ale, q) denotes the probability of answer a to the question q holds based on the evidence e, which is produced by the same backbone in our method with specific prompt, KL denotes Kullback-Leibler divergence (Van Erven and Harremos, 2014). To maximize the evidence extraction probability, we should maximize the probability of evidence supporting the question-answer pair P(ela, q) and minimize the distribution distance between question answering with or without evidence KL(P(a,q)||q(a|e, q)). Considering we can not get access to the golden evidence during inference stage, the second term KL(P(a, q)||qa|e,q)), named as \u201cdistribution"}, {"title": "3.2.2 Evidence-Enhanced Question\nAnswering (QE->A)", "content": "In this part, we ask the LLM to generate the answers based on the corresponding question and the relevant evidence. The instruction is \u201cgenerate the correct answers for the following question based on the document and the evidence support the answers to the question.\", and we insert the instruction, document, question and evidence into the template in Figure 5, as inputs into the LLM. The objective function formulated as:\nLseq = - log P(a | D, q, e)\n=$\\sum_{t=0}^{Na-1} \\log P (at+1 | D, q, e, a\u2264t)$\n(6)\nwhere Na denotes the length of the answers, P (a1 | D, q, e, a<0) := P(a1|D, q, e). This task can be seen as the main task of EATQA and enables the model to derive the answers based on the question and evidence. On the other hand, to narrow the gap between training and inference, we minimize second term of Eq.4: KL(P(a, q)||qa|e, q)). When the evidence provided is incomplete or has misleading information, the model learns to resort to the original document for the answer, which improves the robustness of training stage. Therefore, the loss function of this part is:\nLQEA = LSeq + \u03b1\u03ba\u03b9\u00b7 KL(P(a, q)||qa|e, q))\n(7)\nwhere akl denotes the hyper-parameter to tune.\""}, {"title": "3.2.3 Evidence-Aware Question Restoration\n(EA->Q)", "content": "In this part, we aim to model the probability of PM(qle, a) and ask the LLM to recover the question based on the evidence-answer pair. The prompt"}, {"title": "3.3 Training and Inference", "content": "With the unified optimization of all three EATQA objectives, our model captures the logical relations between question, evidence and answers. The overall objective is the weighted accumulation:\nLTriplet = Q1LQAE + Q2LQEA + Q3LEAQ\n(9)\nwhere a1, a2 and 03 are tuneable hyper-parameters."}, {"title": "4 Experiments", "content": "We evaluate on two widespread benchmark GQA datasets, MultiRC (Khashabi et al., 2018) and QASPER (Dasigi et al., 2021), across different domains. MultiRC creates multi-domain multi-hop questions, where documents across various domains are selected from multiple datasets. Each instance consists of a document including about 15 sentences. All instances were constructed such that it is not possible to answer a question correctly"}, {"title": "5 Analysis", "content": "In this part, we investigate the effectiveness of different modules in our method, including QA->E, EA->Q and the distribution bridging.\nDoes question restoration matter? In this ablation, we remove the module of question restora-"}, {"title": "5.2 Different document lengths and sentence number", "content": "In this part, we investigate our performance on cases with different document lengths and sentence numbers comparing with the backbone. We classify the MultiRC development set into 4 groups based on the document length and sentence number respectively and apply F1 to evaluate the performance of different models."}, {"title": "5.3 Performance on Evidence Generation", "content": "Not only deriving effectiveness on GQA, our method also shows improvement on evidence generation. In Table 7, comparing with sequentially generating evidence and answer, our method outperforms by 3.1 on 7B and 2.5 F1 on 13B. It shows our model captures the logical relation of query, evidence and answer in the unified generation framework."}, {"title": "5.4 Hallucination Mitigation", "content": "Considering the prior knowledge within LLM, we observe for some \u201calready-known\" questions, the model can generate the correct answer without the document, such as \"What is gravity's role in space?\". We utilize $P(YA|Q = \u0176)$ to evaluate the internal knowledge of model. When the model can not generate the correct answer without the document, the model resorts to the document rather than internal knowledge. We utilize the probability of generating faithful answer based on the document (without hallucinations) $P(YA\\Q,D = \u0176|YA|Q \u2260 \u0176)$ to evaluate the hallucination mitigation of the model. In Table 4, our model significantly mitigates the hallucination while keeping prior knowledge to solve the \u201calready-known\u201d questions."}, {"title": "5.5 Correlation between Different Modules", "content": "In this part, we investigate the correlation of model performance in query answering (QEA), evidence generation (QAE) and query restoration (EAQ) on data samples. To mitigate the bias of extreme sample, we classify the samples in development set into 50 groups with same size based on the QEA F1. We take the average F1 score of all samples in the group as its overall F1 score. We respectively draw the scatter plot of each pair of QEA, QAE, EAQ score versus the other and fit with linear function. In Figure 3. we find the QAE score and EAQ score are directly proportional to QEA score. In our triplet generation framework, with better performance in evidence generation and query restoration, the model derives better performance in query answering. This shows the effectiveness of our EATQA, which enhances the understanding of LLM about logical relations between query, evidence and answer."}, {"title": "5.6 Attention Weights", "content": "In this part, we compute the average attention weights about query to document and evidence in the query answering task in respective layers of the transformer block. We conduct statistics on the development set of the MultiRC dataset with 13B model. In evidence-aware query answering, the model assigns about 2 times attention weights to evidence token than context token. It shows the evidence contains denser information to derive the answer. Our introduction of distribution bridging distills the abundant information in evidence to evidence-absent query answering in inference phrase. In the EA->Q, the token-average attention weights of generated query paid to evidence are comparable to answer texts. Considering the evidence contains more tokens than the answer, It shows the evidence sentences are crucial for the feasibility of EA->Q task."}, {"title": "6 Conclusion", "content": "In this paper, we propose the unified triplet generation framework including three instruction tuning tasks to improve the logical reasoning ability of LLM for GQA task. We conduct experiments on two widespread document-based QA datasets MultiRC and QASPER with different sizes of LLM, and achieve the new state-of-the-art on both datasets."}, {"title": "Limitations", "content": "In this paper, we propose the unified triplet generation framework to improve the logical reasoning ability of LLM for GQA task. We conduct experiments with different sizes of LLama and achieve the new SOTA. We will conduct experiments on other LLMs with different architectures in further research."}]}