{"title": "Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering", "authors": ["Haowei Du", "Huishuai Zhang", "Dongyan Zhao"], "abstract": "To address the hallucination in generative question answering (GQA) where the answer can not be derived from the document, we propose a novel evidence-enhanced triplet generation framework, EATQA, encouraging the model to predict all the combinations of (Question, Evidence, Answer) triplet by flipping the source pair and the target label to understand their logical relationships, i.e., predict Answer(A), Question(Q), and Evidence(E) given a QE, EA, and QA pairs, respectively. Furthermore, we bridge the distribution gap to distill the knowledge from evidence in inference stage. Our framework ensures the model to learn the logical relation between query, evidence and answer, which simultaneously improves the evidence generation and query answering. In this paper, we apply EATQA to LLama and it outperforms other LLMs-based methods and hallucination mitigation approaches on two challenging GQA benchmarks. Further analysis shows that our method not only keeps prior knowledge within LLM, but also mitigates hallucination and generates faithful answers.", "sections": [{"title": "Introduction", "content": "Large language models (LLM) represent a significant milestone in the development of general artificial intelligence (Brown et al., 2020; Touvron et al., 2023; Chowdhery et al., 2023). While these models have demonstrated unprecedented performance across various general tasks, they still face a series of challenges, including issues such as hallucination (Tonmoy et al., 2024) and handling long contexts (Jin et al., 2024). In document-based generative question answering (GQA) (Lewis and Fan, 2018), model may generate answers which are inconsistent with the document or mismatch the query, well known as hallucinations (Gunjal et al., 2024; Liu et al., 2024). Recent works utilize an external model to retrieve relevant information and agument the factuality of generation. However, the intrinsic discrepancy between retriever and LLM may introduce information which is only surface relevant but not helpful to answer the question (Salemi and Zamani, 2024).\nTo enhance the logical reasoning and avoid the misleading information, we highlight the supporting evidence of the answer in document-based QA. Instead of the retrieve-then-read pipeline, we let the LLM to tackle the generation of evidence and answer in a unified triplet generation framework, where each two of <question, evidence, answer> serve as the input in the respective instruction to generate the other. The evidence is utilized to reconstruct the question to ensure the model captures its logical relations to question and answer instead of superficial relevance.\nWe take an example in MultiRC (Khashabi et al., 2018) dataset in Figure 1. The question is \"After the Osprey resumed flights how long did it take for the Air Force to begin using the aircraft? \u201d and the answer can not be extracted from a single sentence in the document. To answer this question, the model needs to find the multiple evidence \"Osprey resumed flights in 2002\u201d and \u201cAir Force began using Ospreys in 2008 after testing the aircraft in 2006\u201d, and decide the answer is \"4 years\". If the model is distracted by the incorrect evidence \"Marines develop the aircraft in Iraq in 2007\", it will derive the wrong answer \u201c5 years\u201d. On the other hand, based on the correct evidence, the model can reconstruct the original question because the correct evidence contains the enough information. However, with the incorrect evidence, the reconstructed question is \"How long did it take for the Marines to begin using the aircraft \" which is not consistent with the original question. It shows the correct evidence is crucial for the question answering and the reconstruction of the question based on evidence and answer can reflect the correctness of the evidence.\nTo alleviate the hallucination and enhance the logical reasoning between the question, evidence and answers, we propose our Evidence enhanced Triplet generation framework (EATQA), which includes three instruction tuning tasks to predict all the combinations of Question, Evidence, Answer) triplet by flipping the source pair and the target label to understand their logical relationships, i.e., predict A(Answer), Q(Question), and E(Evidence) given a QE, EA, and QA pairs, respectively. We close the QA distribution distance between evidence-aware and evidence-absent settings (distribution bridging) to distill the knowledge from evidence to mitigate the gap in inference stage when the evidence sentences can not be derived.\nWe conduct experiments in two widespread document-based GQA datasets with diverse answer types, MultiRC and QASPER, based on different sizes of LLMs. Compared with different sizes of the backbone model, our unified triplet generation framework shows significant improvement on the two datasets, becoming the new state-of-the-art. Further analysis demonstrates the ability of our approach to tackle longer document with more sentences. Moreover, we observe the positive correlation among the performance of 3 subtasks in the triplet generation framework, which shows the effectiveness of unifying the generation of the three parts with one LLM in the triplet generation framework.\nWe conclude our contributions as follows: 1. We highlight the evidence retrieval to alleviate hallucinations of LLM in GQA task. Instead of utilizing another LM as the retriever, which may introduce misleading information, we propose the unified triplet generation framework including three instruction tuning tasks to improve the logical reasoning ability of LLM for GQA task. Specifically, we train the LLM to generate the Answer(A), Question(Q), and Evidence(E) based on the information of QE, EA, and QA pairs respectively. The distribution bridging is utilized to distill knowledge from evidence for inference stage. 2. We conduct experiments on two widespread document-based GQA datasets MultiRC and QASPER with different sizes of LLM, and achieve the new state-of-the-art on both datasets. 3. Further experiments show the effectiveness of unified triplet generation framework on both evidence retrieval and question answering. Moreover, our method not only keeps the prior knowledge within LLM, but also mitigates the hallucination for questions beyond the internal knowledge."}, {"title": "Related Work", "content": "Generative question answering (GQA) aims to generate an abstractive answer rather than extract an answer to a given question from provided passages (Fan et al., 2019; Li et al., 2021). Early works on GQA mostly tried to improve the faithfulness of the answer by investigating reliable external knowledge sources or incorporating multiple information sources. Yin et al. (2015) propose Neural Generative Question Answering, an end-to-end model that generates answers to simple factoid questions based on the knowledge base, while Bi et al. (2019) propose the Knowledge-Enriched Answer Generator (KEAG) to generate a natural answer by integrating facts from four different information sources, namely, questions, passages, vocabulary, and knowledge.\nRecent works focus more on the conditional generation model. Li et al. (2021) propose Rationale-Enriched Answer Generator (REAG), in which they add an extraction task to obtain the rationale for an answer at the encoding stage, and the decoder is expected to generate the answer based on both the extracted rationale and original input. Su et al. (2022) propose a framework named RBG (read before generate), to jointly models answer generation with machine reading. They augment the generation model with fine-grained, answer-related salient information predicted by the MRC module, to enhance answer faithfulness. Such methods can exploit and utilize the information in the original input better, while they require the extra effort of building models to extract that information. CAD (Shi et al., 2023) follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. RHO (Ji et al., 2023) introduce local and global knowledge-grounding techniques into dialogue generation and further utilize a conversational reasoning model to re-rank the generated responses."}, {"title": "Methodology", "content": "In this part, first we introduce the document-based GQA task and the model architecture of Triplet-QA. Then we present the unified triplet generation framework, which predicts all the combinations of (Question, Evidence, Answer) triplet by flipping the source pair and the target label to learn their logical relationships. Specifically, our method contains 3 subtasks, answer-aware evidence retrieval, evidence-aware query answering and evidence-aware query restoration.\nThe motivation of triplet generation framework comes from that the posterior probability of question answering is positively proportional to the probability of evidence generation and question recovery by Bayesian formulation:\n\\(P_M(a|q, e, d) = \\frac{P_M(a, q, e, d)}{P_M(q, e, d)}\\)\n\\(=\\frac{P_M(q, e, d)}{P_M(q, d)}P_M(a|q)P_M(e|a, q)P_M(q|e, a, d)\\)        (1)\nwhere d, q, e, a denote the document, question, evidence and answer. It shows the accuracy of QA should be proportional to the accuracy of evidence extraction and question restoration. We assume the evidence sentences contain the sufficient information to reconstruct the question, i.e. \\(P_M(q|e, a) = P_M(q|e, a, d)\\).\nAs the basis of our framework, we explain the feasibility of our method. Take query restoration as an example: in Figure 1, only from the answer \"4 years\", the model is hard to restore the query because there may be multiple sentences involving \"4 years\" in the document. However, given the evidence sentences which point to the key events \u201cOsprey resumed flights\u201d and \u201cAir Force begin using the aircraft\", the model has derived the basic part of the query and it enables our query restoration module reasonable, which improves the model ability to arrange the information to restore the query.\""}, {"title": "Preliminary", "content": "Document-base GQA task aims to generate the answer of the natural text question based on the document including multiple sentences:\n\\(D = [s_1, s_2, \\cdots, s_n]\\)   (2)\nwhere D denotes the document, \\(s_i\\) denotes the i-th sentence in the document and n denotes the number of sentences. The model can be formulated as a function of\n\\(f_M(a) = \\prod_{i=1}^n P(a_i|a_1, a_2,\\dots, a_{i-1}, q, D)\\)     (3)\nwhere n denotes the answer length, q denotes the query and \\(a_0\\) denotes the begin-of-speech (BOS) token. Generally the answer has flexible forms which can not be directly extracted from the document."}, {"title": "Model Architecture", "content": "EATQA is built on the widespread LLM, LLama (Touvron et al., 2023) with a few additional learnable parameters. we additionally adopt several trainable adapter tokens \\(p = [p_1, p_2, \\dots, p_{N_p}]\\) which are prepended to the key and value of each self-attention layer, where \\(N_p\\) is the number of adapter tokens. So the number of trainable parameters of EATQA 7B is 4.5M, only 0.06% of total parameters of LLama 7B. With such a few trainable parameters, EATQA effectively preserves LLMs' prior knowledge and the casual reasoning ability to understand the logical relations between the question, evidence and answer. EATQA consists of three objectives: answer-aware evidence generation, evidence-enhanced query answering and evidence-aware query restoration."}, {"title": "Answer-Aware Evidence Generation (QA->E)", "content": "In this part, we model the probability of supporting evidence extraction for the query-answer pair \\(P_M(e|a, d)\\). We design the instruction and ask the LLM to generate the evidence which supports the query and the corresponding answers. Therefore, the input to model is the instruction, source document, the query and the corresponding answer. The output of model is the supporting evidence. The specific instruction is \u201cgenerate the relevant evidence from the document to answer the following question\" and we insert the document, question and answers into the template in Figure 5.\nAs for the loss function, by Bayesian Formula (Mises, 1942) we derive\n\\(log(P(e, q)) = log \\int P(e, q, a) da\\)\n\\(= log \\int  \\frac{P(e, q, a)}{q(a|e, q)} q(a|e, q)da\\)\n\\(\\geq \\int q(a|e, q) log(\\frac{P(e, q, a)}{q(a|e, q)}) da\\)\n\\(= \\int q(a|e, q) log(P(e, q, a)) da - \\int q(a|e, q) log(q(a|e, q)) da\\)\n\\(= E_{q(a|e,q)} log(\\frac{P(e, q, a)}{q(a|e, q)})\\)\n\\(= E_{q(a|e,q)} log(\\frac{P(a, q)P(e|a, q)}{q(a|e, q)})\\)\n\\(= E_{q(a|e,g)} log(P(e|a, q)) + E_{q(a|e,g)} log(\\frac{P(a, q)}{q(a|e, q)})\\)\n\\(= E_{q(a|e,q)} log(P(e|a, q)) - KL(P(a,q)||q(a|e, q))\\)             (4)\nwhere \\(q(a|e, q)\\) denotes the probability of answer a to the question q holds based on the evidence e, which is produced by the same backbone in our method with specific prompt, KL denotes Kullback-Leibler divergence (Van Erven and Harremos, 2014). To maximize the evidence extraction probability, we should maximize the probability of evidence supporting the question-answer pair \\(P(e|a, q)\\) and minimize the distribution distance between question answering with or without evidence \\(KL(P(a,q)||q(a|e, q))\\). Considering we can not get access to the golden evidence during inference stage, the second term \\(KL(P(a, q)||q(a|e,q))\\), named as \u201cdistribution bridging\", narrows down the gap between training and inference for answer generation. we utilize cross-entropy loss function to optimize the probability \\(P(e|a, q)\\):\n\\(L_{QAE} = - log P(e | D, q, a)\\)\n\\(= -\\sum_{t=0}^{N_e-1} log P (e_{t+1} | D, q, a, e_{<t})\\)                   (5)\nwhere D denotes the document, \\(N_e\\) denotes the length of the evidence, \\(P (e_1 | D, q, a, e_{<0}) := P (e_1 | D, q, a)\\).\""}, {"title": "Evidence-Enhanced Question Answering (QE->A)", "content": "In this part, we ask the LLM to generate the answers based on the corresponding question and the relevant evidence. The instruction is \u201cgenerate the correct answers for the following question based on the document and the evidence support the answers to the question.\", and we insert the instruction, document, question and evidence into the template in Figure 5, as inputs into the LLM. The objective function formulated as:\n\\(L_{seq} = - log P(a | D, q, e)\\)\n\\(= -\\sum_{t=0}^{N_a-1} log P (a_{t+1} | D, q, e, a_{\\leq t})\\)                     (6)\nwhere \\(N_a\\) denotes the length of the answers, \\(P (a_1 | D, q, e, a_{<0}) := P(a_1|D, q, e)\\). This task can be seen as the main task of EATQA and enables the model to derive the answers based on the question and evidence. On the other hand, to narrow the gap between training and inference, we minimize second term of Eq.4: \\(KL(P(a, q)||q(a|e, q))\\). When the evidence provided is incomplete or has misleading information, the model learns to resort to the original document for the answer, which improves the robustness of training stage. Therefore, the loss function of this part is:\n\\(L_{QEA} = L_{Seq} + \\alpha_{kl}\\cdot KL(P(a, q)||q(a|e, q))\\)                  (7)\nwhere \\(\\alpha_{kl}\\) denotes the hyper-parameter to tune.\""}, {"title": "Evidence-Aware Question Restoration (EA->Q)", "content": "In this part, we aim to model the probability of \\(P_M(q|e, a)\\) and ask the LLM to recover the question based on the evidence-answer pair. The prompt is \"reconstruct the question based on the answers and corresponding supporting evidence\u201d, and we insert the prompt, document, evidence and answers into the template in Figure 5. The objective function is formulated as:\n\\(L_{EAQ} = - log P(q | D, e, a)\\)\n\\(= -\\sum_{t=0}^{N_q-1} log P (q_{t+1} | D, e, a, q_{<t})\\)                  (8)\nwhere \\(N_q\\) denotes the length of the question, \\(P (q_1 | D, e, a, q_{<0}) := P (q_1 | D, e, a)\\). Considering the incorrect evidence does not contain the full information of the original question, this objective helps to enhance the casual relations between evidence and answers."}, {"title": "Training and Inference", "content": "With the unified optimization of all three EATQA objectives, our model captures the logical relations between question, evidence and answers. The overall objective is the weighted accumulation:\n\\(L_{Triplet} = \\alpha_1L_{QAE} + \\alpha_2L_{QEA} + \\alpha_3L_{EAQ}\\)                   (9)\nwhere \\(\\alpha_1\\), \\(\\alpha_2\\) and \\(\\alpha_3\\) are tuneable hyper-parameters."}, {"title": "Experiments", "content": "We evaluate on two widespread benchmark GQA datasets, MultiRC (Khashabi et al., 2018) and QASPER (Dasigi et al., 2021), across different domains. MultiRC creates multi-domain multi-hop questions, where documents across various domains are selected from multiple datasets. Each instance consists of a document including about 15 sentences. All instances were constructed such that it is not possible to answer a question correctly without gathering information from multiple sentences. Qasper includes 5049 questions over 1585 Natural Language Processing papers in the academic research domain focusing on entire papers, which is designed to facilitate document-grounded, information-seeking QA. Qasper contains a variety of answer types, including extractive, abstractive, yes/no, and unanswerable questions.\nWe utilize Exact Match (EM) and F1 scores (Opitz and Burst, 2019) to evaluate our method. The F1 score measures the overlap of answer tokens between the predicted and ground-truth answer. EM is more strict which awards point if any of the annotated answers is generated exactly."}, {"title": "Implementation Details", "content": "We conduct experiments with LLama2 (Touvron et al., 2023) from 7B to 13B as the LLM. To reduce computation cost and keep prior knowledge in LLM, we use LoRA (Hu et al., 2021), which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the LLM. The parameters \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\alpha_3\\) in Eq. 9 are tuned from [0.1, 0.3, 0.5, 0.7, 1.0], and set to 0.3, 1.0 and 0.3 in our method. We use AdamW as optimizer and the initial learning rate is set to 3e-5. Because the maximum input length of LLama2 is 4096 and the average context length of QASPER is about 16K, we utilize position interpolation (Chen et al., 2023) to extend the context length to 32K."}, {"title": "Baselines", "content": "We compare our method with existing widespread LLMs including T5-11B (Raffel et al., 2020), Flan-137B (Wei et al., 2021), Vega2-6B (Zhong et al., 2022), GPT-3 (few shot) (Brown et al., 2020), LoRAMOE (Dou et al., 2023), PaLM 540B (Anil et al., 2023) for MultiRC. For Qasper, we compare our method with LLM-based long context methods, AttenWalker (Nie et al., 2023), ChatGLM3-6B-32k (Du et al., 2021), SE-Mistral-7B (Jin et al., 2024), VCC-3B (Zeng et al., 2024) and TOVA-7B (Oren et al., 2024) For hallucination mitigation methods, we compare our approach against RAG with Dense Passage Retriever (DPR) (Karpukhin et al., 2020), CAD (Shi et al., 2023), RHO (Ji et al., 2023) using the same backbone."}, {"title": "Results", "content": "From Table 1, compared with the backbone, our method improves by 3.7 EM and 2.0 F1 on 7B-scale model as well as 2.3 EM and 1.5 F1 on 13B-scale model. It demonstrates the effectiveness of our evidence enhanced triplet generation framework on document based GQA. Moreover, our method with 13B parameters outperforms the 540B PaLM finetuning by 0.7 EM and 1.0 F1, becoming the new state-of-the-art. Our method with 7B-scale model has achieved the comparable performance on F1 with larger models like T5-xxl and ERNIE-3.0.\nFrom Table 2 compared with the backbone, our method improves by 2.7 F1 on 7B-scale model. Qasper contains more rigorous samples and existing hallucination mitigation methods struggle to improve the performance. It demonstrates the effectiveness of our method on challenging long document QA."}, {"title": "Analysis", "content": "In this part, we investigate the effectiveness of different modules in our method, including QA->E, EA->Q and the distribution bridging."}, {"title": "Ablation", "content": "In this part, we investigate the effectiveness of different modules in our method, including QA->E, EA->Q and the distribution bridging."}, {"title": "Does question restoration matter?", "content": "In this ablation, we remove the module of question restoration and investigate its effect on question answering. In table 3, removing question restoration will drop 1.7 EM and 1.0 F1 with 7B model, as well as 1.3 EM and 1.1 F1 with 13B model. Considering the context is not inputted into model in the query restoration module, the model has to utilize the information in evidence to recover the question. This module enhances the ability to arrange multiple pieces of information in evidence sentences, and understand logical relation between query, answer and evidence for LLM, which shows the effectiveness for GQA."}, {"title": "Does evidence generation matter?", "content": "In this ablation, we remove the module of evidence generation and investigate its effect on GQA. In Table 3, removing evidence restoration will drop 2.1 EM and 0.4 F1 with 7B model, as well as 1.0 EM and 1.2 F1 with 13B model. Evidence extraction encourages the model to reason for the supporting facts that entail the question-answer pair, which enhances the understanding of logical relation among query, answer and evidence. Removing evidence generation decreases the attention of model pays to the important facts in the document."}, {"title": "Should we narrow down the distance between \\(P(a,q)\\) and \\(q(a|e,q)\\)?", "content": "In this ablation, we remove the KL-divergence loss in Eq.4 in training. In inference stage, we input the predicted evidence and the query to derive the answer. In Table 3, removing KL loss will drop 0.9 EM and 0.5 F1 with 7B model, as well as 0.9 EM and 0.7 F1 with 13B model. Though keeping effective performance, the distribution bridging distills the knowledge of evidence and narrows down the gap between training and inference, avoiding first retrieving the evidence and then inputting the evidence alongside the query into model to reason for the answer."}, {"title": "Different document lengths and sentence number", "content": "In this part, we investigate our performance on cases with different document lengths and sentence numbers comparing with the backbone. We classify the MultiRC development set into 4 groups based on the document length and sentence number respectively and apply F1 to evaluate the performance of different models.\nGenerally, our model derives significant improvement over LLama2-13B in groups with different document lengths and sentence numbers. It demonstrates the effectiveness of our evidence enhanced triplet generation framework on document-based GQA. In Table 5, EATQA outperforms LLama2 by 3.5 and 1.5 F1 in groups 3 and 4, as well as 1.8 and 1.2 F1 in groups 1 and 2. In Table 6, EATQA outperforms LLama2 by 3.4 and 2.7 F1 in groups 3 and 4. Longer context brings the difficulty for model to capture important information about the query and derive the correct answer. Our method enhances the logical relation between evidence, query and answer, which mitigates the hallucination about distracting information in the document."}, {"title": "Performance on Evidence Generation", "content": "Not only deriving effectiveness on GQA, our method also shows improvement on evidence generation. In Table 7, comparing with sequentially generating evidence and answer, our method outperforms by 3.1 on 7B and 2.5 F1 on 13B. It shows our model captures the logical relation of query, evidence and answer in the unified generation framework."}, {"title": "Hallucination Mitigation", "content": "Considering the prior knowledge within LLM, we observe for some \u201calready-known\" questions, the model can generate the correct answer without the document, such as \"What is gravity's role in space?\". We utilize \\(P(Y_{A|Q} = \\hat{Y})\\) to evaluate the internal knowledge of model. When the model can not generate the correct answer without the document, the model resorts to the document rather than internal knowledge. We utilize the probability of generating faithful answer based on the document (without hallucinations) \\(P(Y_{A|Q,D} = \\hat{Y}|Y_{A|Q} \\ne \\hat{Y})\\) to evaluate the hallucination mitigation of the model. In Table 4, our model significantly mitigates the hallucination while keeping prior knowledge to solve the \u201calready-known\u201d questions."}, {"title": "Correlation between Different Modules", "content": "In this part, we investigate the correlation of model performance in query answering (QEA), evidence generation (QAE) and query restoration (EAQ) on data samples. To mitigate the bias of extreme sample, we classify the samples in development set into 50 groups with same size based on the QEA F1. We take the average F1 score of all samples in the group as its overall F1 score. We respectively draw the scatter plot of each pair of QEA, QAE, EAQ score versus the other and fit with linear function. In Figure 3. we find the QAE score and EAQ score are directly proportional to QEA score. In our triplet generation framework, with better performance in evidence generation and query restoration, the model derives better performance in query answering. This shows the effectiveness of our EATQA, which enhances the understanding of LLM about logical relations between query, evidence and answer."}, {"title": "Attention Weights", "content": "In this part, we compute the average attention weights about query to document and evidence in the query answering task in respective layers of the transformer block. We conduct statistics on the development set of the MultiRC dataset with 13B model. In evidence-aware query answering, the model assigns about 2 times attention weights to evidence token than context token. It shows the evidence contains denser information to derive the answer. Our introduction of distribution bridging distills the abundant information in evidence to evidence-absent query answering in inference phrase. In the EA->Q, the token-average attention weights of generated query paid to evidence are comparable to answer texts. Considering the evidence contains more tokens than the answer, It shows the evidence sentences are crucial for the feasibility of EA->Q task."}, {"title": "Conclusion", "content": "In this paper, we propose the unified triplet generation framework including three instruction tuning tasks to improve the logical reasoning ability of LLM for GQA task. We conduct experiments on two widespread document-based QA datasets MultiRC and QASPER with different sizes of LLM, and achieve the new state-of-the-art on both datasets."}, {"title": "Limitations", "content": "In this paper, we propose the unified triplet generation framework to improve the logical reasoning ability of LLM for GQA task. We conduct experiments with different sizes of LLama and achieve the new SOTA. We will conduct experiments on other LLMs with different architectures in further research."}, {"title": "Appendix", "content": "Input templates of different modules in EATQA."}]}