{"title": "FVEval: Understanding Language Model Capabilities in Formal Verification of Digital Hardware", "authors": ["Minwoo Kang", "Mingjie Liu", "Ghaith Bany Hamad", "Syed Suhaib", "Haoxing Ren"], "abstract": "The remarkable reasoning and code generation capabilities of large language models (LLMs) have spurred significant interest in applying LLMs to enable task automation in digital chip design. In particular, recent work has investigated early ideas of applying these models to formal verification (FV), an approach to verifying hardware implementations that can provide strong guarantees of confidence but demands significant amounts of human effort. While the value of LLM-driven automation is evident, our understanding of model performance, however, has been hindered by the lack of holistic evaluation. In response, we present FVEval, the first comprehensive benchmark and evaluation framework for characterizing LLM performance in tasks pertaining to FV. The benchmark consists of three sub-tasks that measure LLM capabilities at different levels\u2014from the generation of SystemVerilog assertions (SVAs) given natural language descriptions to reasoning about the design RTL and suggesting assertions directly without additional human input. As test instances, we present both collections of expert-written verification collateral and methodologies to scalably generate synthetic examples aligned with industrial FV workflows. A wide range of existing LLMs, both proprietary and open-source, are evaluated against FVEval, based on which we investigate where today's LLMs stand and how we might further enable their application toward improving productivity in digital FV. Our benchmark and evaluation code is available at https://github.com/NVlabs/FVEval.", "sections": [{"title": "1 Introduction", "content": "Hardware verification is a critical stage in digital VLSI development that is not only challenging but also indispensable to successful chip manufacturing. To ensure that design implementations are fully verified against expected functional behaviors, industrial teams are increasingly adopting formal verification (FV), an approach that formulates specifications of the design as temporal logical properties and applies formal methods to mathematically prove that the said properties hold across all expected input stimuli. While FV offers distinct advantages, such as the ability to detect bugs that are elusive or hard-to-reach with simulation-based approaches, it also bears limitations: FV workflows suffer from steep engineering costs of manually crafting formal testbenches and collateral by human experts. As a result, scaling the adoption of FV and extending its use to cover a larger number of sub-systems remains a challenge."}, {"title": "2 Preliminaries", "content": "Hardware verification is a crucial stage in the chip design process, necessary to make sure the end product functions according to the specification. However, even though on average more resources are now allocated to verification than hardware design, recent industry data shows that an increasing percentage of projects see critical bugs escape verification [12] that must be fixed by an expensive re-spin. There are two widely accepted approaches to hardware verification: traditional simulation-based verification and formal verification (FV), which has been gaining momentum in the recent decade.\nUnlike simulation-based verification, FV aims to formally prove that a digital circuit design meets a given specification [23]. The specification in FV is represented as a set of properties that either constrain the input stimuli, as assumptions; or define the expected behavior of the design to be verified, as assertions. Digital circuits can naturally be expressed as state transition systems, and the properties in temporal logic [9, 8]. This representation allows model checkers to traverse through the state transition graph to search for any states where the properties do not hold [7]. If the model checker can find any such state, it returns a counterexample or trace for which that property fails. In contrast, simulation-based verification would only check the properties for a subset of states based on the defined input stimuli. If a model checker can provide a formal guarantee that a property is indeed true in every state\u2014it has found a formal proof the property always holds for the design."}, {"title": "3 Overview of the FVEval Benchmark Framework", "content": "This section presents an overview of the proposed FVEval\u2014a collection of three sub-benchmarks that assess language models on varying tasks related to hardware FV. Here, we first describe the motivation for formulating these sub-benchmarks and the capabilities of LLMs that we seek to measure. Then we describe the details, including the dataset collection or generation process, definitions of model inputs and outputs, and evaluation metrics for each sub-benchmark."}, {"title": "3.1 Motivation", "content": "In this work, we focus on assessing the viability of LLMs to be applied towards industrial hardware FV based on SystemVerilog Assertions (SVAs). In performing functional verification, FV engineers go through the process of: (1) analyzing the design-under-test (DUT) RTL and the specification doc- umentation describing the expected functional behaviors of the design; and (2) writing testbenches that consist of assertions checking the design against the specification. LLMs could potentially improve FV productivity by automating the implementation of such SVA assertions from natural language specifications.\nIn this context, we curate the first sub-benchmark, NL2SVA-Human, through which we raise the question: Do LLMs have the capability to generate SVA assertions, given real-world, human-written testbenches and high-level specifications of design functionality? Even though SVA assertions can be relatively short pieces of code, the outputs must accurately reflect the specification and do so in the context of the provided testbench, which entails grounding the implementation of the assertion on existing modeling code provided in the testbench. In curating the evaluation dataset, we focus on a representative set of testbenches that target common unit-level modules that engineers repeatedly"}, {"title": "3.2 NL2SVA-Human: Assertion Generation with Real-World Testbenches", "content": "Dataset. Our collection of test instances cover basic units of FV such as first-in-first-out (FIFO) queues, arbiters, hardware counters, random-access memory (RAM) units, and finite-state machines. Each testbench comes with a set of assertions and accompanying natural language description written by formal experts that address various design functionalities, with the benchmark holding a total of 79 assertions. Figure 2 summarizes the statistics about the test instances, in terms of the distribution of lengths of natural language specifications and reference SVA assertion solutions. We measure length as the number of tokens based on the tokenizer used in the Llama3 models [29]. We see a wide range in the distribution reflecting the variety of formal properties in the benchmark."}, {"title": "3.3 NL2SVA-Machine: Synthetic Benchmark for Stress-Testing Formal Assertion Gen- eration", "content": "Dataset. To create diverse test cases of NL description and SVA assertion pairs, we follow the process of: (1) random SVA assertion generation, based on random sampling of SVA operators and symbolic signal names; (2) LLM generation of NL descriptions for each random assertion; (3) LLM as a critic to assess whether the NL description accurately reflects the temporal logic of the assertion\u2014if this fails, re-try description generation; and (4) Human inspection to finalize the"}, {"title": "3.4 Design2SVA: Direct Generation of Formal Assertions from Design RTL Alone", "content": "Dataset. Our objective in formulating the Design2SVA benchmark is to present a set of test cases consisting of design RTL that are: (1) sufficiently correct, as in there exist formal properties that can be proven to be true; (2) relevant to real-world FV use-cases; (3) suitable as test instances for language models; and (4) varied in terms of complexity. While it is ideal to collect and curate existing RTL examples, we find that openly available SystemVerilog/Verilog repositories contain limited numbers of verified RTL designs. Of the cases that satisfy the first and second criteria, such as module instances from OpenTitan [33], these cases fail to be suitable for language model evaluation, as each module is a part of a large System-on-a-Chip (SoC) system and the context needed to resolve all sub-module dependency information is prohibitively large. Instead, we propose a methodology to scalably generate complex and parameterized synthetic test instances that are derived from common design patterns encountered in industrial FV workflows. As shown in Figure 4, the two categories of RTL designs we generate are (1) arithmetic pipelines that resemble scenarios where we check for data integrity and forward propagation across datapaths; and (2) finite-state machines (FSMs) that commonly appear in control logic implementations, such as cache controllers, memory interfaces, etc. Each category of our designs has randomized sub-components\u2014 the arithmetic execution units in the pipeline designs and the FSM graph and state transition logic in the FSM design\u2014and each generated RTL is parameterized such that controlled generation of test instances is possible. While a virtually limitless number of designs can be generated in this manner, we compose the Design2SVA benchmark to contain 96 test instances for each design category (pipeline and FSM) based on a controlled sweep of generator parameters.\nEvaluation For each generated RTL test instance, we also generate an accompanying formal testbench, based on which language models generate suggestions of assertions. The FVEval eval- uation flow then re-formats the testbench with the model-generated assertions and supplies the SystemVerilog to the commercial tool backend for evaluation. The metrics of evaluation are similar to the NL2SVA benchmarks: (1) Syntax\u2014we measure the LLM generated assertion is first syntac- tically correct; (2) Functionality\u2014we use the results of formal proofs, i.e. whether the assertions are proven with model checkers and other formal engines in industrial tools, as an indication of functional correctness.\nUnlike in NL2SVA-Human and NLSVA-Machine, the Design2SVA benchmark subjects LLMs to generate relevant and correct SVA assertions given a design under verification, and as such, there"}, {"title": "4 Results and Analysis", "content": "In this section, we report the results of evaluating a suite of current state-of-the-art language models, trained on considerable sizes of text corpora pertaining to diverse programming languages."}, {"title": "4.1 Experiment Setup", "content": "A wide range of LLMs, both proprietary and open-source (i.e. models with weights publicly available) are considered. We consider the most recent OpenAI model gpt-40-0513 [1] and Google Gemini models [14] gemini-1.5-pro-001 and gemini-1.5-flash-001. All results of the proprietary models are based on accessing their respective endpoint services in May 2024. For open-source models, we evaluate the models from the Llama 3 [29] and Llama 3.1 family [30] with 8B and 70B parameters; we also consider mixtral-8x22b-instruct-v1, a sparse Mixture-of-Experts model with approximately 39B active parameters [19]. Note that all models are instruction fine-tuned models, as our tasks heavily require instruction-following capabilities. Evaluation of open-source models was performed via vLLM [24], a high-throughput LLM inference framework."}, {"title": "4.2 NL2SVA-Human Results", "content": "We consider zero-shot model inference where (1) the problem testbench and (2) NL descriptions of the intended SystemVerilog assertion is given as model input. Evaluation results from running model inference with greedy decoding are summarized in Table 1. We observe that the models reported with highest general capabilities in code generation and logical reasoning, such as gpt-40 and gemini models, show the best performance across all metrics."}, {"title": "4.3 NL2SVA-Machine Results", "content": "A further task to quantify model capability in handling diverse SystemVerilog operator syntax and temporal logic reasoning based on such grammar, NL2SVA-Machine evaluation examines whether LLMs can accurately reason about the temporal logic expressed in natural language and translate into SVA syntax. Table 3 summarizes model performances in zero-shot and with 3-shot inference settings, where the hand-crafted in-context examples are fixed across test questions."}, {"title": "4.4 Design2SVA Results", "content": "Finally, Design2SVA tests if the models can produce plausible assertions directly from design RTL. Besides the logical reasoning capabilities required in the NL2SVA-Human and NL2SVA-Machine tasks, Design2SVA further requires understanding the semantics of the given RTL implementation and also what concrete implementation of a formal property would be relevant for the design. The synthetic RTL test cases (pipeline and FSM examples) are generated from simple more complex examples, with the most complex instances taking greater than 16K tokens when provided as context to all of the LLMs considered. As such, we do not consider the Llama-3 family models and older OpenAI models with less than a 32K context window for this task."}, {"title": "5 Related Work", "content": "LLMs for Chip Design and Formal Verification. Researchers have applied language models, and more broadly deep learning to various problems in chip design [37, 4, 13, 54, 22]. Recent work fine-tune open-source language models such as CodeGen [31] and LLaMA2 [45] on chip design related data and show that fine-tuned models out-perform both open-source and proprietary models in Verilog code generation [43, 28] and EDA tool script generation [17]. Prior work on applying LLMs to hardware formal verification can be broadly categorized into two groups: one line of work takes micro-architectural descriptions of the design-under-test (DUT) and prompts LLMs to generate a plausible formal testbench [34, 41, 21, 16]. Another line of work instead have LLMs consume architectural specification documents, describing high-level specifications that have not yet been formulated as RTL implementations, and have the models create an entire testbench [11]. However, existing evaluations of LLMs on FV [34, 11, 27, 41] are limited to less than ~20 cases of test instances and have considered a limited variety of task settings. In contrast, FVEval expands both the scope and scale of test instances, combining collected human-written examples from industrial experts as well as verified synthetic instances generated from parameterized templates, and considers three sub-tasks each exercising varying aspects of LLM capabilities for real-world applications in FV.\nBenchmarks for Code Generation and Formal Logical Reasoning. Several benchmarks exist to measure language model capabilities on code generation, mostly targeting code synthesis in software programming languages (Python, C/C++, Java, etc.) [5, 3, 6]. Recent iterations have con- sidered extensions that explore multi-lingual [2] contexts, are geared towards particular applications such as data science [25], and consider repository-level editing to reflect scenarios that are closer to real-world software engineering tasks [53, 20, 10]. Most related to our work, VerilogEval [28] pro-"}, {"title": "6 Limitations and Future Work", "content": "The current state of FVEval is only the first step towards understanding and thereby improving model capabilities for hardware FV. We expect future work to encompass further variety in the designs and testbenches, such as considering synthetic data generation with different styles of de- sign modules besides the arithmetic pipeline and FSMs considered in this work. We also anticipate methodologies that will improve LLM performance on our task, e.g. carefully chosen prompting schemes [47] and other structural inference methods [51, 50], as well as ideas to incorporate tool- feedback or external symbolic reasoning tools as part of a LLM-agentic framework. Evaluations made in this work have not considered such variety of techniques and we invite future work to explore novel avenues based on FVEval."}, {"title": "7 Conclusion", "content": "In this work, we present FVEval\u2014a first comprehensive benchmark for evaluating language models in the context of hardware formal verification. We demonstrate three sub-tasks that are derived from industrial FV use cases and each examine different aspects of LLM capabilities necessary for their application in this domain. FVEval defines evaluation metrics for each task with end-to-end automated evaluation made possible with a industry-standard formal verification tool. In partic- ular, the custom-implemented formal equivalence checking between model generated assertions against ground-truth solution offers, for the first time, a concrete metric for evaluating LLMs in tasks generating formal properties in SVA syntax."}, {"title": "A Additional Details on FVEval: NL2SVA-Human", "content": "In this section, we provide additional details and test instance examples for NL2SVA-Human."}, {"title": "A.1 NLSVA-Human Benchmark Statistics and Examples", "content": "Detailed statistics on the composition of formal testbenches consisting of the NL2SVA-Human benchmark are shown in Table 6.\nNext, we show an example testbench in NL2SVA-Human and its associated assertions and NL specifications:"}, {"title": "A.2 NL2SVA-Human Evaluation: Model Prompt", "content": "The following figure describes the prompt provided to language models during evaluation."}, {"title": "A.3 NL2SVA-Human Example Model Responses", "content": "Here, we show sample LLM responses to the example test instance (testbench and NL specification of assertion) referred to in Section A.1 and A.2."}, {"title": "B Additional Details on FVEval: NL2SVA-Machine", "content": "In this section, we provide additional details and test instance examples for NL2SVA-Machine."}, {"title": "B.1 NL2SVA-Machine Evaluation: Model Prompt", "content": "The following figure describes the prompt provided to language models during evaluation. The prompt shown is for zero-shot evaluation; for k = 3-shot, we include the ICL examples listed in Figure 15 as part of the user prompt, before the indicated question."}, {"title": "B.2 NL2SVA-Machine Evaluation: In-Context Examples Used", "content": "The following lists the in-context examples used in NL2SVA-Machine:"}, {"title": "B.3 NL2SVA-Machine Example Model Responses", "content": "Here, we show sample LLM responses to the example test instance referred to in Section B.1."}, {"title": "C Additional Details on FVEval: Design2SVA", "content": "In this section, we provide additional details and test instance examples for Design2SVA."}, {"title": "C.1 Examples of Design2SVA RTL Test Cases", "content": "The following demonstrates an example design RTL generated for the Design2SVA benchmark. As noted, we generate synthetic test cases representing arithmetic pipelines and finite state ma- chines (FSMs). Here we show the simplest examples from each category. Each generated design is accompanied with a testbench harness, of which the module definition and input/output port information is provided as context to LLMs, as described in Section C.2. Synthetic pipeline design RTL and testbench header example:"}, {"title": "C.2 Design2SVA Evaluation: Model Prompt", "content": "The following details the prompt provided to language models during evaluation of Design2SVA."}, {"title": "C.3 Design2SVA Example Model Responses", "content": "Here, we show sample LLM responses to the example test instance referred to in Section C.2. Again, we list examples for both the pipeline RTL example and the FSM RTL example."}]}