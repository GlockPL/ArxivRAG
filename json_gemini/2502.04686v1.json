{"title": "Learning Strategic Language Agents in the Werewolf Game with Iterative Latent Space Policy Optimization", "authors": ["Zelai Xu", "Wanjun Gu", "Chao Yu", "Yi Wu", "Yu Wang"], "abstract": "Large language model (LLM)-based agents have recently shown impressive progress in a variety of domains, including open-ended conversation and multi-step decision-making. However, applying these agents to social deduction games such as Werewolf, which requires both strategic decision-making and free-form language interaction, remains non-trivial. Traditional methods based on Counterfactual Regret Minimization (CFR) or reinforcement learning (RL) typically depend on a predefined action space, making them unsuitable for language games with unconstrained text action space. Meanwhile, pure LLM-based agents often suffer from intrinsic biases and require prohibitively large datasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO), an iterative framework that addresses these challenges by first mapping free-form text to a discrete latent space, where methods like CFR and RL can learn strategic policy more effectively. We then translate the learned policy back into natural language dialogues, which are used to fine-tune an LLM via Direct Preference Optimization (DPO). By iteratively alternating between these stages, our LSPO agent progressively enhances both strategic reasoning and language communication. Experiment results on the Werewolf game show that our method improves the agent's performance in each iteration and outperforms existing Werewolf agents, underscoring its promise for free-form language decision-making.", "sections": [{"title": "1. Introduction", "content": "Developing intelligent agents that can reason rationally, make strategic decisions, and interact with humans has been a long-term goal in artificial intelligence (AI) research (Wooldridge & Jennings, 1995; Russell & Norvig, 2016). In recent years, large language model (LLM)-based agents have made significant strides towards this goal by exhibiting strong performance in open-ended conversation and multi-step decision-making (Brown et al., 2020; Ouyang et al., 2022). Trained on massive text corpora, LLM-based agents have demonstrated remarkable versatility across various domains, ranging from web navigation (Nakano et al., 2021; Yao et al., 2022b) and code generation (Chen et al., 2021; Yang et al., 2024) to video game environment (Wang et al., 2023a) and real-world scenarios (Ahn et al., 2022; Brohan et al., 2023). Beyond single-agent tasks, LLM-based agents have also shown potential in multi-agent interactions including collaborative teamwork (Li et al., 2023), adversarial gameplay (Meta et al., 2022), and human-AI interation (Park et al., 2023; Liu et al., 2023).\nAmong these interactive domains, social deduction games such as Werewolf present unique challenges because they require both high-level strategic decision-making and free-form conversational abilities. Unlike classic games with predefined and limited actions such as board games (Silver et al., 2016; 2018), card games (Morav\u010d\u00edk et al., 2017; Brown & Sandholm, 2018), and video games (Mnih, 2013; Vinyals et al., 2019), Werewolf relies heavily on free-form conversation to achieve agreements and perform strategic deceptions. Players must communicate, bluff, and infer hidden roles through unrestricted, natural language interactions. This free-form language space expands the strategic possibilities and introduces additional complexity unmatched by more rigidly defined domains. As a result, Werewolf can serve as an ideal testbed for developing strategic agents with language-grounded decision-making capabilities.\nHowever, designing an effective AI agent that can interact with human players in Werewolf or similar free-form language environments is still challenging. On the one hand, classic game-solving approaches like Counterfactual Regret Minimization (CFR) and reinforcement learning (RL) have proven successful in games like Go and Poker, thanks to their ability to handle finite action spaces. However, Werewolf has a free-form action space, making direct application of these methods computationally infeasible. Mapping ev-"}, {"title": "2. The Werewolf Game", "content": "Werewolf is a popular social deduction game where players with hidden roles cooperate and compete with others in natural languages. The Werewolf side needs to conceal their identities and eliminate the other players, while the Village side needs to identify their teammates and vote out the Werewolves. Players are required to have both language proficiency for communication and strategic ability for decision-making to achieve strong performance in the Werewolf game. We consider a seven-player game with two Werewolves being the Werewolf side and one Seer, one Doctor, and three Villagers being the Village side. Detailed descriptions of the game's rule, observation space, and reward function can be found in Appendix A."}, {"title": "2.1. Game Environment", "content": "We consider a text-based seven-player Werewolf game that proceeds through natural languages. We exclude other information like the speaking tone, facial expression, and body language (Lai et al., 2022). This pure text-based environment is a common setup in the literature (Xu et al., 2023a;c; Wu et al., 2024; Bailis et al., 2024).\nRoles and Objectives. At the beginning of each game, the seven players are randomly partitioned into two sides. The Werewolf side has two Werewolf players who know each other's role and aim to eliminate the other players while avoiding being discovered. The Village side has one Seer who can check the role of one player each night, one Doctor who can protect one player each night, and three Villagers without any ability. The players in the Village side only know their own role and need to share information to identify the Werewolves and vote them out.\nGame Progression. The game proceeds by alternating between night round and day round. In the night round, players can perform secret actions that are only observable by themselves. More specifically, the two Werewolves can choose a target player to eliminate, the Seer can choose a target player to investigate whether the player's role is Werewolf, and the Doctor can choose a target player to protect the player from being eliminated. The Doctor does not know the target player chosen by the Werewolves. If the Doctor chooses the same target player as the Werewolves, then no player is eliminated in this night round, otherwise, the Doctor fails to protect any player, and the target chosen by the Werewolves is eliminated.\nObservations and Actions. The language observation of each agent is a list of natural languages that log the game history to the current step. This list include both private"}, {"title": "2.2. Challenges for Language Agents", "content": "Unlike board, card, or video games with a finite set of actions, Werewolf has a free-form language action space. The vast space of natural language actions poses two key challenges for language agents to achieve strong performance in the Werewolf game.\nIntrinsic Bias in Action Generation. As observed in simple games like Rock-Paper-Scissor (Xu et al., 2023c), pure LLM-based agents tend to exhibit intrinsic bias in their action generation, which is inherited from the model's pre-training data. This issue is more pronounced in complex language games like Werewolf, where the opponents can exploit these predictable biases to counteract the agent's move. Therefore, mitigating intrinsic bias is essential for language agents to reduce exploitation and achieve strong performance.\nCoverage of Unbounded Action Space. Due to the immense combinatorial space induced by free-form text, it is impractical to map every possible utterance to an action in the language space. On the other hand, manually engineering or prompting an LLM to produce a limited set of actions may fail to capture the full strategic landscape. Even if an agent optimally masters the action distribution within a limited subset, it could be easily exploited by out-of-distribution utterance. Consequently, inadequate coverage of the action space could result in suboptimal performance in free-form language games like Werewolf."}, {"title": "3. Latent Space Policy Optimization", "content": "To tackle the intrinsic bias and the coverage issue, we propose an iterative Latent Space Policy Optimization (LSPO) framework. Our method combines game-theoretic optimization with LLM fine-tuning and operates on an expanding latent strategy space to iteratively improve the agent's decision-making ability and action coverage. As shown in"}, {"title": "3.1. Latent Space Construction", "content": "One of the key challenges in free-form language games like Werewolf is achieving broad coverage of the unbounded text space while maintaining a computationally tractable action representation for game-theoretic methods. To strike a balance between coverage and tractability, we propose to abstract the vast language action space into a finite set of latent strategies, which we then expand over iterations for better coverage. Specifically, our latent space construction in each iteration involves two steps including latent strategy generation and clustering.\nLatent Strategy Generation. In our setting, secret actions and voting actions are already discrete and therefore do not require further abstraction. We focus instead on the free-form discussion actions, which we aim to capture as latent strategies. We assume that each role in the game has the same set of latent strategies across all discussion rounds and collect the latent strategies for each role by letting the current LLM agent self-play as different roles for multiple trajectories. To further improve the coverage of latent strategies, we prompt the LLM to generate N strategically distinct discussion candidates and randomly choose one to execute in the game. This process encourages diversity in the collected discussion actions and generate a set of latent strategies in natural language for each role.\nLatent Strategy Clustering. Although we generate a set of latent strategies for each role, they are still in the form of natural language. To transform them into a discrete latent strategy space, we embed each discussion action into a vector representation using an embedding model such as \"text-embedding-3-small\" that captures its semantic and contextual information. We then apply a simple k-means clustering algorithm to partition the embedded utterances into k clusters, where each cluster represents a distinct latent strategy. Clustering reduces the infinite free-form text space to a finite set of abstract strategies, paving the way for subsequent game-theoretic optimization. By interpreting each cluster as a latent action, we can more efficiently search for and optimize strategic policies with minimal sacrifice of coverage of language space."}, {"title": "3.2. Policy Optimization in Latent Space", "content": "Another challenge in free-form language games is to address the intrinsic bias in the agent's action distribution. After constructing a discrete latent strategy space, we can reformulate the original game with unbounded language space as an abstracted game with a finite latent strategy space. This reformulation allows us to apply standard game-solving techniques such as Counterfactual Regret Minimization (CFR) or reinforcement learning (RL) methods to learn near-optimal strategies that overcome the intrinsic bias. In our implementation, we employ CFR as the game solver.\nAbstracted Game Formulation. To represent the game in a compact, finite form, we replace the free-form discussion actions with the discrete latent strategies from latent space construction. Specifically, in the abstracted game, the secret action and voting action remain the same, and the discussion action is replaced by the latent strategy. The state in the abstracted game is a vector including information like the player's role, secret action, etc., and history of past latent strategies. The transition dynamics and payoff function remain unchanged in the abstracted game. This abstracted representation retains the key strategic elements of the original game while reducing the complexity of the action space, making large-scale game-solving computationally tractable.\nOptimal Policy Learning. Once the game is represented in this discrete form, we apply CFR to learn a policy and solve the abstracted game. Classical CFR (Zinkevich et al., 2007) iteratively improves policies by minimizing counterfactual regret $R$ for each information set. For each iteration $t$, the regret for each action $a$ in the latent space is updated by:\n$R_t(a) = R_{t-1}(a) + (u(\\sigma_{t|a}), \\sigma_t) - u(\\sigma_t)$,\nwhere $(\\sigma_t, \\sigma_{t|a})$ is the utility of taking action $a$ under the current strategy profile $\\sigma_t$, and $u(\\sigma_t)$ is the utility under the full strategy profile. We use neural networks to approximate regret value to scale CFR to more complex games and learn a policy for each different role in the Werewolf game. By repeatedly simulating self-play among agents employing Deep CFR in the abstracted game, each role's policy converges to a near-optimal strategy profile. The resulting latent space policies address the intrinsic bias in action distribution and achieve strong strategic play in the abstracted game."}, {"title": "3.3. Latent Space Expansion", "content": "To further improve the agent's performance in free-form language games, the latent space must remain sufficiently expressive to cover novel strategies and resist exploitation by out-of-distribution actions. We achieve this by fine-tuning the LLM to align with the learned policy in the abstracted game and then re-generating and expanding the latent strategy space using the fine-tuned LLM. This iterative process progressively increases coverage of the action space, enabling stronger and more robust decision-making.\nAlignment to Latent Space Policy. We employ Direct Preference Optimization (DPO) (Rafailov et al., 2024) to fine-tune the LLM so that its open-ended language outputs align with the near-optimal strategies derived from the abstracted game. To construct the preference dataset required"}, {"title": "4. Experiments", "content": "We conduct extensive experiments in the Werewolf game to evaluate the effectiveness of our LSPO framework. We use \"Llama-3-8B-Instruct\" as the base model in our experiments. We first visualize how the latent strategy space evolves to show that our agents progressively acquire more complex strategic behaviors. We then quantitatively evaluate the performance of our LSPO agent using prediction accuracy and win rate to show the improving performance over iterations. We also compare the LSPO agent with four state-of-the-art agents, showing that our agents achieve the highest win rate as both the Werewolf side and the Village side. We further perform ablation studies to assess the effectiveness of specific designs in our framework. More implementation details can be found in Appendix C."}, {"title": "4.1. Latent Space Visualization", "content": "To gain insight into how LSPO organizes free-form language actions into discrete latent strategies, we first visualize the latent strategy space constructed at different training iterations. Specifically, for each role in the Werewolf game, we gather the utterances generated by the LSPO agent in 100 games, embed them with the sentence encoder, and apply dimensionality reduction for projection. The visualization of latent spaces for the Werewolf and the Seer in different iterations is shown in Figure 2. Earlier iterations yield relatively indistinct clusters, reflecting a lack of strategic diversity. Over successive iterations, clearer and more refined clusters emerge, indicating that the LSPO agent evolves toward an increasingly structured latent space and learn to express different strategic intentions such as accusing specific roles, defending teammates, and bluffing.\nWerewolf's Latent Space. In the first iteration, the latent space of the Werewolf is dominated by three main clusters. The blue cluster corresponds to a simple strategy of concealing its role or pretending to be a villager, while the smaller orange cluster reflects strategies like pretending to be a Seer or a Doctor. There is even a green cluster corresponding to unintentionally revealing the true role of a Werewolf, which is obviously a flawed strategy. As training proceeds, we see more sophisticated patterns emerge. The flawed strategy of disclosing one's Werewolf role disappears, and the agent begins to incorporate deliberate bluffs and misdirections instead. For example, the red cluster features the agent pretending to be a Seer and providing fabricated investigative results to sow confusion, and the purple cluster centers on defending the teammate and redirecting suspicion onto other players, leveraging more nuanced language and reasoning to guide the conversation toward scapegoats. This refined partitioning demonstrates that the Werewolf agent progressively covers an increasing number of latent strategies.\nSeer's Latent Space. In the first iteration, the Seer's latent space is relatively coarse, containing primarily two strategies including staying silent about its true role or revealing its role and sharing information. This shows a limited range of strategic diversity in the early stage. As training proceeds"}, {"title": "4.2. Iterative Performance Evaluation", "content": "We then evaluate how the performance of our LSPO agent progresses with more iterations, demonstrating that our framework produces increasingly stronger strategic language agents over time. We focus on two key metrics including prediction accuracy and win rate.\nPrediction Accuracy. Accurate role identification is a critical aspect of Werewolf, as it underpins effective decision-making and voting. Therefore, we measure the agent's ability to predict the roles of other players with an additional prediction phase before each voting phase in a Werewolf game. Specifically, we use the final-iteration LSPO agent as the fixed opponent and let LSPO agents at different iterations play against this opponent for 100 games. For the Werewolf side, a higher prediction accuracy of crucial roles like Seer and Doctor allows them to eliminate these threats earlier. Conversely, for the Village side, a higher prediction accuracy of Werewolves improves their chance to vote out the Werewolf and win the game.\nWin Rate. While prediction accuracy serves as an intermediate metric to evaluate the agents' reasoning and decision-making ability, we also use the win rate as a direct measure of the performance of our agents. Similar to the evaluation of prediction accuracy, we use the final-iteration LSPO agent as the fixed opponent and let our agents at different iterations play 100 games against the opponent. A higher win rate indicates a stronger performance in the game.\nAs shown in Table 1, both prediction accuracy and win rate exhibit a clear growing trend as the iteration increases, indicating that our iterative LSPO framework steadily strengthens the agents' reasoning and decision-making capabilities. From the Werewolf side, the identification rate for the Seer starts off relatively high but has only modest improvement. This is because the Seer often reveals its roles to share information, making it easier for the Werewolf side to identify. By contrast, the Werewolf's prediction accuracy of the Doctor shows more significant gains, reflecting the strategic importance of eliminating the Doctor who can save potential victims. On the Village side, identifying the Werewolf and the Seer benefits most from iterative learning, since confirming these central roles is crucial for coordinated voting and elimination of Werewolves. Overall, these results confirm that our framework consistently improves the strategic language abilities of the LSPO agent, enabling it to adapt and excel in complex social deduction scenarios with each additional iteration."}, {"title": "4.3. Comparison with State-of-the-Art Agents", "content": "We compare the performance of the LSPO agent in the Werewolf game with four state-of-the-art agents including Reason and Act (ReAct) (Yao et al., 2022b), Recursive Contemplation (ReCon) (Wang et al., 2023b), a Cicero-like agent (Meta et al., 2022), and Strategic Language Agent (SLA) (Xu et al., 2023c). As some of these methods were not initially developed for Werewolf, we make minor modifications to ensure compatibility with our experimental setting while preserving each approach's core design.\nReAct. ReAct is a classic prompt-based method that synergizes reasoning and acting for agent tasks. We implement ReAct for the Werewolf game by providing the LLM with raw game observations to generate both intermediate reasoning and final actions within a single prompt.\nReCon. ReCon is another prompt-based method designed for Avalon agents. The ReCon agent is prompted to first think from its own perspective and then think from its op-"}, {"title": "4.4. Ablation Studies", "content": "To show the effectiveness of our design, we compare the LSPO agent with an ablated version of itself. This ablated agent only performs latent space construction and policy optimization in latent space, without LLM fine-tuning and latent space expansion. To generate discussion action in gameplay, this agent first uses the latent space policy to sample a latent strategy, then the previously collected discussions corresponding to the latent strategy are used as few-shot examples to prompt the LLM for the discussion action. We compare this agent with the LSPO agent trained for one iteration and the result is shown in Table 3. The LSPO agent trained for one iteration achieves higher win rates than the ablated agent as both the Village side and the Werewolf side. This result indicates that fine-tuning the LLM to align with the latent space policy can help the LLM better generalize to new language actions beyond the collected samples and expand the latent strategy space."}, {"title": "5. Related Work", "content": "Large Language Model-Based Agents.\nRecent advancements in large language models (LLMs) have led to the development of agents capable of performing complex tasks across various domains, such as web interactions (Nakano et al., 2021; Yao et al., 2022a; Deng et al., 2023), code generation (Chen et al., 2021; Yang et al., 2024), gaming environments (Huang et al., 2022a; Wang et al., 2023c;a; Ma et al., 2023), real-world robotics (Ahn et al., 2022; Huang et al., 2022b; Vemprala et al., 2023), and multi-agent systems (Park et al., 2023; Li et al., 2023; Chen et al., 2023b). A common approach in these works is to exploit"}, {"title": "6. Conclusion", "content": "In this work, we presented Latent Space Policy Optimization (LSPO), an iterative framework that combines structured game-solving techniques with the expressive power of large language models to build strategic language agents in free-form social deduction games. By abstracting unconstrained language action space into a discrete latent strategy space, our approach enables efficient CFR in the latent space to overcome intrinsic bias and learn strong strategies. We then perform iterative fine-tuning via DPO to align the LLM's language generation with the evolving strategy and expand the latent strategy space to address the action coverage issue. Our extensive evaluation in the Werewolf game demonstrates that LSPO not only addresses intrinsic biases and action coverage issues inherent in prompt-based agents, but also achieves increasing performance with respect to iterations and outperforms four state-of-the-art baseline agents. Looking ahead, we envision LSPO's synergy of latent-space abstraction and preference-based language alignment can be extended to a variety of other complex decision-making tasks with free-form language actions."}, {"title": "Impact Statement", "content": "Our research advances the capabilities of LLM-based agents in a purely text-based Werewolf environment. While this setting allows the agents to develop robust decision-making and deception-detection skills, it also underscores the potential for misuse if similar techniques were to be adapted to real-world scenarios involving manipulation or misinformation. To mitigate these risks, our implementation remains strictly focused on text-based simulation and does not directly transfer to broader applications without additional safeguards. At the same time, our experiment results indicate that our agent could be used to identify potential deceptive and manipulative content. We envision that any future extensions of this work will require careful consideration of ethical guidelines and responsible deployment strategies to ensure that such language agent systems serve society constructively."}, {"title": "A. Werewolf Game Implementation Details", "content": "A. Werewolf Game Implementation Details"}, {"title": "A.1. Game Rules", "content": "Setup. Each game begins by randomly assigning seven roles\u2014two Werewolves, one Seer, one Doctor, and three Villagers-to seven different players labeled \"player_0,\u201d \u201cplayer_1,\u201d ..., \u201cplayer_6.\" The two Werewolves are aware of each other's identities, while the Seer, Doctor, and Villagers only know their own roles.\nNight Round. During the Night round, only the surviving Werewolves, Seer, and Doctor take secret actions that are disclosed only to the relevant parties.\n\u2022 Werewolf: The living Werewolves collectively decide on a target to kill, but they follow a specific order when there are two of them. First, the Werewolf with the smaller ID proposes a target; the other Werewolf then makes the final decision. For instance, if \"player_0\u201d and \u201cplayer_2\" are Werewolves, \u201cplayer_0\" proposes \u201cplayer_i,\u201d and \u201cplayer_2\" chooses the ultimate kill target \"player_j.\" If only one Werewolf is alive, that Werewolf's decision stands. Werewolves cannot kill a dead player, themselves, or their teammate.\n\u2022 Seer: The Seer selects a living player to investigate, revealing whether that player is a Werewolf. The Seer may not investigate a dead player or themselves, although they are allowed to investigate the same player on different nights (albeit a less effective strategy).\n\u2022 Doctor: The Doctor selects a player to protect, without knowledge of the Werewolves' choice. The Doctor cannot save someone who is already dead but can choose to save themselves.\nDay Round. The day round proceeds with three phase including announcement, discussion, and voting.\n\u2022 Announcement: at the start of the Day round, the events of the previous night are made public to all players still in the game. Anyone killed during the Night round is immediately removed and cannot reveal their role or participate in discussions. Two scenarios determine the announcement: if the Werewolves targeted \"player_i\" and the Doctor either saved a different \u201cplayer_j\" or was no longer alive, \u201cplayer_i\" is killed, and the announcement states: \u201cplayer_i was killed last night.\" If the Doctor saved exactly the same person the Werewolves intended to kill (\"player_i\"), then no one is killed, and the announcement is: \"no player was killed last night.\"\n\u2022 Discussion: all surviving players join an open discussion in a set speaking order, each speaking exactly once. If, for example, the remaining players are \u201cplayer_0,\u201d \u201cplayer_2,\u201d and \u201cplayer_5,\u201d then \u201cplayer_0\" speaks first, followed by \"player_2,\" and concluding with \"player_5.\"\n\u2022 Voting: after the discussion, all surviving players simultaneously vote to eliminate one other player or choose to abstain. They are not allowed to vote for a dead player or for themselves. The individual who receives the most votes is eliminated without role disclosure. In the event of a tie, one of the tied players is randomly chosen to be eliminated. Everyone knows the final voting tally.\nWinning. The Werewolves win if, at any point, the number of living Werewolves is equal to that of all other remaining players. They do not need to eliminate every non-Werewolf to claim victory. Conversely, the Villagers (including the Seer and Doctor) win once both Werewolves have been eliminated.\""}, {"title": "A.2. Observation Space", "content": "Language Observation. Each agent's language observation is represented as a list of natural language statements that log the game's history up to the current step. This list comprises both private information, which is accessible only to the current player, and public information, which is shared among all players. The private information includes the player's role, secret actions taken during the night phase by the Werewolf, Seer, and Doctor, as well as the Werewolf's teammate. On the other hand, the public information consists of the player's ID, the eliminated player in each night and day phase, discussions, and voting outcomes from each day phase. An example of the language observation is as follow."}, {"title": "A.3. Reward Functions", "content": "The reward functions are defined as follows:\n\u2022 Winning Reward: all winning players receive +300, and all losing players receive -300.\n\u2022 Surviving Reward: +5 for all surviving players in each round.\n\u2022 Voting Reward (Village side only): +20 for correct votes, -20 for incorrect votes.\n\u2022 Voting Result Reward: -10 for the player that is eliminated. +5 when an opponents is eliminated, -5 when a teammate is being eliminated."}, {"title": "B. Detailed Prompt", "content": "B. Detailed Prompt"}, {"title": "B.1. System Prompt", "content": "You are an expert in playing the social deduction game named Werewolf. The game has seven roles including two Werewolves, one Seer, one Doctor, and three Villagers. There are seven players including player_0, player_1, player_2, player_3, player_4, player_5, and player_6.\nAt the beginning of the game, each player is assigned a hidden role which divides them into the Werewolves and the Villagers (Seer, Doctor, Villagers). Then the game alternates between the night round and the day round until one side wins the game.\nIn the night round: the Werewolves choose one player to kill; the Seer chooses one player to see if they are a Werewolf; the Doctor chooses one player including themselves to save without knowing who is chosen by the Werewolves; the Villagers do nothing.\nIn the day round: three phases including an announcement phase, a discussion phase, and a voting phase are performed in order.\nIn the announcement phase, an announcement of last night's result is made to all players. If player_i was killed and not saved last night, the announcement will be \"player_i was killed\"; if a player was killed and saved last night, the announcement will be \"no player was killed\"\nIn the discussion phase, each remaining player speaks only once in order from player_0 to player_6 to discuss who might be the Werewolves.\nIn the voting phase, each player votes for one player or choose not to vote. The player with the most votes is eliminated and the game continues to the next night round.\nThe Werewolves win the game if the number of remaining Werewolves is equal to the number of remaining Seer, Doctor, and Villagers. The Seer, Doctor, and Villagers win the game if all Werewolves are eliminated."}, {"title": "B.2. Prompt for Secret Actions", "content": "Now it is night <n_round> round, you (and your teammate) should choose one player to kill/see/save. As player_<id> and a <role>, you should first reason about the current situation, then choose from the following actions: <action_0>, <action_1>, ....\nYou should only respond in JSON format as described below."}, {"title": "B.3. Prompt for Discussion Actions", "content": "Now it is day <n_round> discussion phase and it is your turn to speak. As player_<id> and a <role>, before speaking to the other players, you should first reason the current situation only to yourself, and then speak to all other players. You should only respond in JSON format as described below."}, {"title": "B.4. Prompt for Voting Actions", "content": "Now it is day <n_round> voting phase, you should vote for one player or do not vote to maximize the Werewolves' benefit (for the Werewolves) / you should vote for one player that is most likely to be a Werewolf or do not vote (for the Villagers). As player_<id> and a <role>, you should first reason about the current situation, and then choose from the following actions: do no vote, <action_0>, <action_1>, ....\nYou should only respond in JSON format as described below."}, {"title": "B.5. Prompt for Diverse Action Generation", "content": "For the discussion actions, we iteratively ask the LLMs to produce one new action at a time by adding the following prompt in the action prompt: \u201cconsider a new action that is strategically different from existing ones.\u201d"}, {"title": "C. Implementation Detail", "content": "C. Implementation Detail"}, {"title": "C.1. Hyperparameters", "content": "For latent space construction, we let the LLM agent play 1000 games to collect all discussion actions generated by each role in these games. For diverse action generation, we prompt the LLM to generate 3 action candidates and randomly select one to execute in the game. We pair the language observation with the 3 action candidates to use for preference-based fine-tuning in the following components. For sentence embedding, we use OpenAI's \u201ctext-embedding-3-small\" embedding API to embed the sentence to a vector of 1536 dimensions. Then we apply standard k-means clustering to cluster the embedding and get the discrete latent strategy space. The number of clusters k in the first iteration is 3 for the Werewolf and 2 for the Seer, Doctor, and Villagers. In each iteration, we add 1 cluster to the existing clusters. That is, if the first iteration has k clusters, then the i-th iteration has k + i - 1 clusters."}, {"title": "C.2. Counterfactual Regret Minimization", "content": "Counterfactual Regret Minimization (CFR) ((Zinkevich et al., 2007)) is a self-play algorithm, and each player continuously updates their strategies according to regret matching to achieve a Nash equilibrium.\nWe use the following notation. Z is the set of all the end states z. $h \\sqsubseteq z$ means state h is a prefix of state z, that is, z can be achieved from h. $\\pi_p^{\\sigma}$ is the probability contribution of the player p, and $\\pi^{\\sigma} = \\prod_p \\pi_p^{\\sigma}$. $\\pi_{-p}^{\\sigma}$ is the probability contribution of all players except player p. $u_p(z)$ is the utility function for the player p in the state z.\nCounterfactual value for a state h and a player p according to startegy o is defined as:\n$v_p^{\\sigma}(h) = \\sum_{z \\in Z, h \\sqsubseteq z} \\pi_{-p}^{\\sigma}(h) \\pi^{\\sigma}(z|h) u_p(z)$.\nThe regret for a action a in state h for player p is defined as: $v_p^{\\sigma_{h \\rightarrow a}}(h) - v_p^{\\sigma}(h)$, where $\\sigma_{h \\rightarrow a}$ is same to $\\sigma$ except in state h the player will choose action a. The regret matching is choosing the strategy according to sum of previous regret values defined as $R(h, a)$, then the new strategy $\\sigma(h, a) = \\frac{R(h, a)^+}{\\sum_{a'} R(h, a')^+}$, $R(h, a)^+ = max(0, R(h, a))$. If $\\sum_{a'} R(h, a')^+ = 0$, just set o to be uniform random.\nBecause the game tree is very big, it is impossible to traverse the entire tree, our implementation is based on deep CFR ((Brown et al., 2019)). We use a neural network to fit observation to regret value. The amount of computation required to search for only one player is also unacceptable, so a restriction is added based on deep CFR. If the number of layers currently searched is too large, the previous strategy is directly used to sample the actions of all players until the end of the game and return the utility for each player in that state. The complete process can be seen as running some complete game trajectories, and then starting from each intermediate node, searching a few layers to do CFR."}, {"title": "C.3. Baseline Implementation", "content": "ReAct, ReCon, and SLA are implemented following the original paper. The Cicero-like agent predefines a set of high-level atomic actions and trains an RL policy with this fixed action space. The RL policy takes the embeddings of the information record and deduction result as input and selects the atomic action based on this input. Then the natural language actions used in gameplay are generated by prompting the LLM to follow the selected atomic actions. In our case, the atomic action set consists of 13 actions including \u201cidle\", \"target player_0\", \"target player_1\", \"target player_2\", \"target player_3\", \"target player_4\""}]}