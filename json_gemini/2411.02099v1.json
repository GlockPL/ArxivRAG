{"title": "Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition", "authors": ["Idris Zakariyya", "Linda Tran", "Kaushik Bhargav Sivangi", "Paul Henderson", "Fani Deligianni"], "abstract": "Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical con-tact and they can integrate with pre-existing Wi-Fi net-works. They are also seen as less privacy-invasive com-pared to camera-based systems. However, recent research has shown high accuracy in recognizing subjects or gen-der from radar gait patterns, raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by at-tributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various lev-els of attacker-accessible information. We extensively eval-uated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously as-sessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings, particu-larly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step to-wards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.", "sections": [{"title": "1. Introduction", "content": "Demographic projections indicate that the worldwide population of individuals aged 60 and above will surge to approximately 1.4 billion people by 2030 [65], leading to an increased reliance on advanced healthcare monitor-ing technologies in patients' homes [23]. Human motion analysis shows significant potential for healthcare monitor-ing and early disease detection [13, 14]. Deep Neural Net-works (DNNs) have exhibited remarkable efficacy in Hu-man Activity Recognition (HAR) for monitoring patients and detecting abnormalities [1, 24]. Especially, in home settings these technologies can play a key role in preventive and proactive healthcare strategies by enabling personalised care systems [23,84].\nHowever, while DNNs excel at encoding input fea-tures, this capability also renders them vulnerable to privacy breaches [39, 53]. For instance, Membership Inference At-tacks (MIA) [35, 69, 74] and Model Inversion (MI) attacks [27] can disclose private information about the patients and their training data by accessing only the pre-trained model outputs. To mitigate such risks, privacy-preserving tech-niques needs to be incorporated into HAR systems to safe-guard sensitive user information. Differential Privacy (DP) is a commonly used technique to enhance privacy in deep learning models [8, 22]. DP quantifies the risk of an in-dividual's information being disclosed by ensuring that the model's output is not significantly affected by the inclusion or exclusion of any single individual's data. The implemen-tation of DP involves introducing controlled perturbations to the data.\nRecently, human motion sensing with radar emerges as a prominent sensing technology for continuous monitoring thanks to its non-intrusive nature [25]. This makes it suit-able for privacy-sensitive environments such as assisted-living facilities, hospitals and homes. Nevertheless, recent studies have revealed high accuracy in subject recognition from radar human gait patterns, challenging the perception of privacy in radar-based systems [50,51]. This under-scores the need for implementing privacy safeguards in hu-man motion sensing systems, regardless of whether the out-put is visually identifiable by humans.\nPrevious research in human motion analysis has primar-ily focused on privacy preservation of Red Green and Blue (RGB) videos on a frame by frame basis [21]. Some meth-ods use anonymisation techniques by replacing the face or whole body with synthetic data [33], while other ap-proaches use obfuscation of sensitive attributes [82]. In most cases, the utility of the data is severely compromised, hindering the application of these methods in healthcare."}, {"title": "2. Background and Related Work", "content": "More recently, it has been recognised that other human mo-tion tracking modalities such as accelerometer, gyroscope sensor data and radar data also record sensitive information that can help identify users and track them during their daily activities [22,37,50,51]. However, to our knowledge, there is no systematic work modeling threats and evaluating the robustness of privacy preservation techniques under sophis-ticated machine learning (ML)-driven attacks in radar-based systems.\nHere we firstly define black-box MIA that are relevant to HAR setting with radar data. We assume that the adver-sary has access to the logit space of the model and it is also possible to gain partial access to the training data or their underlying distribution. In these ways, the adversary can orchestrate attacks to identify individuals. Subsequently, we propose a novel method, named IDG-DP, based on DP and the Integrated Decision Gradient (IDG) attribution al-gorithm [80]. The IDG was selected for its ability to com-pute attributions precisely at the model's decision points, making it a superior attribution algorithm with enhanced performance [80]. IDG-DP injects more noise in input fea-tures that contribute more towards the subject identification than to the activity recognition and thus it preserves privacy, while it maintains high performance in HAR.\nOur paper presents the following contributions:\n1. To our knowledge, we are the first to systematically\ninvestigate state-of-the-art threat models that are rele-vant to HAR in a home setting with radar technology. This enables the identification of sensitive information leakage that is not perceptible to a human observer.\n2. We introduce a novel methodology that drives DP\nby identifying the model's highest attributions during training. In this way, we achieve a better balance be-tween data utility and privacy preservation.\n3. We devise a rigorous evaluation strategy of the mit-igation capabilities of the proposed approach against black-box MIA attacks.\nWe exploit a publicly available dataset on HAR with radar data [20] to assess the effectiveness of the proposed IDG-DP privacy method under various black-box MIA attacks. We demonstrate promising results in balancing data utility and privacy in the data."}, {"title": "2.1. Human Activity Recognition with Radar Technology", "content": "Human motion analysis involves examining human movement patterns, employing diverse technologies that in-volve multiple [54] and single camera setups [81]. Ad-vancements in pose extraction, human tracking and human activity recognition (HAR) have made the technology ac-cessible to patients at home and suitable for continuous monitoring [36,58,79]. Nevertheless, applying these tech-niques broadly in healthcare is met with significant hur-dles, including ethical considerations, safety, and privacy issues, all of which must be thoroughly considered and re-solved prior to widespread adoption [28]. Other technolo-gies, such as wearables have been suggested for the analy-sis of body motion patterns but they require complex setup that exclude their use in patients with cognitive impairments [61].\nRadar sensors can detect human motion signatures through the analysis of frequency modulations in the radio-frequency spectrum, which are induced by the reflection of signals from wifi/radar devices off moving targets [88]. Fur-thermore, radar sensing is not affected by variable light-ing conditions and eliminates the need for individuals to wear any sensors. Radar sensing has attracted attention be-cause it can promote acceptance of monitoring technolo-gies in healthcare applications, since it does not produce images that allow for the immediate recognition of individ-uals and their settings by human observers [48]. Within radar systems, human motion is identified by analyzing micro-Doppler frequency shifts caused by vibrations, trans-lational motions, and rotations of body parts [15]. Var-ious DNNs architectures including Convolutional Neural Network (CNN) [7, 16,43, 52], Recurrent Neural Network (RNN) [44, 75] and auto-encoders [73] have been em-ployed for radar-based HAR [48]."}, {"title": "2.2. Privacy in Human Motion Analysis", "content": "Radar technology enables innovative home monitoring ap-plications, but raises privacy concerns due to continuous surveillance. Beyond data theft, real-time tracking of in-dividuals at home poses greater risks than traditional com-puter vision applications used for diagnostics. Whereas video data can reveal subject's identity directly, privacy preservation against threats that expose subject identity via her/his body movements are less studied. Individuals ex-hibit distinct movement patterns that can be used as bio-metrics [57]. These identity signatures are also present in radar technologies [10,71] and even wearables [3,68,92]. Leveraging this knowledge enables effective human recog-nition [2, 11, 60] but it also creates concerns about users privacy under sophisticated ML driven attacks. The major-ity of subject recognition methods focus on analyzing the gait motion, with walking or running consistently yielding the most successful outcomes [89].\nAlthough, manually extracted features have demon-strated effectiveness [46], deep CNN models have devel-oped to automatically extract features from micro-Doppler signatures for subject recognition [49,89]. Transfer learn-ing, employing pre-trained AlexNet on ImageNet dataset, has proven to be effective in subject recognition based on"}, {"title": "2.2.1 Biometrics in Human Motion", "content": null}, {"title": "2.2.2 Privacy Threats Against DNN Models in HAR", "content": "DNNs occasionally assign high likelihood to some input samples, which reflects memorisation of the training data and it can lead to inadvertently disclose sensitive informa-tion without user consent [37]. An adversary can have ac-cess to the model's architecture and weights and/or full or partial access to the training data. Sophisticated attacks can be orchestrated even when the attacker does not have ac-cess to the model's weights and parameters [59]. For exam-ple, in MIA an adversary with access to the model's logit space might attempt to infer whether a particular data record belongs to a subject whose data has been used for train-ing [35, 69]. In HAR, this might have severe implications, resulting in identifying people and tracking them while they perform different activities at home. Despite their apparent simplicity, MIA form the basis for more robust extraction attacks [69]. The accurate identification of users within sensitive datasets constitutes a serious privacy breach."}, {"title": "2.2.3 Defense methods against MIA", "content": "Defenses against MIA [32] fall into four categories: Reg-ularization, Transfer Learning, Generative Models-based, and Information Perturbation. Regularization techniques reduce overfitting and improve generalization, making models less vulnerable to MIA by adjusting internal pa-rameters during training (e.g., L2 norm, dropout, and early stopping) or data augmentation and label smoothing [42] [91] [83]. Generative models like VAES, GANs, and EBMS produce alternative datasets that mimic the original data dis-tribution, reducing membership information leakage by de-coupling the original data from the model [5] [31] while maintaining overall data characteristics for effective train-ing. Transfer learning protects against MIA [66] by lever-aging knowledge from different domains, reducing the need for target data access. While these methods offer some protection, they lack theoretical guarantees for the privacy of underlying data or models. Consequently, information perturbation techniques have gained popularity due to their stronger theoretical foundations. These approaches defend againt MIA by adding noise and include differential privacy [35], output perturbation (adjusting confidence scores) [41], and data perturbation (hiding member information) [32] to obscure sensitive data while balancing utility."}, {"title": "2.2.4 Privacy-Preservation techniques in HAR", "content": "Privacy preservation technologies in human motion analy-sis have mainly tackled video cameras due to their inherent nature to directly expose the subject identities, ie. facial features and surrounding space [34, 85]. Preservation of privacy for these video cameras has been implemented via traditional techniques that involve blurring, pixelation and distortion. More recently more advanced \u2018anonymisation' techniques have emerged with learnable optics and gener-ative AI [55]. Learnable optics anonymises the data by removing privacy-breaching attributes while retaining the system's main capability [12, 29, 30]. The concept involves optimizing the lens to effectively remove sensitive attributes or degrade video quality. For instance, authors in [30] op-timizes an optical hardware encoder with a software de-coder CNN to degrade private attributes of people filmed on videos while maintaining important features to perform human pose estimation. Although, these approaches offer some protection, since images cannot be identified by hu-man observers, there is no evidence that they can withstand more sophisticated attacks that are eminent in healthcare ap-plications.\nDP has been originally proposed by [17] and it pro-vides a strong theoretical privacy guarantee while analyz-ing datasets. Normally, it involves injecting calibrated noise (typically quantified by privacy budget \u20ac) into the input fea-tures, the gradients computation and/or the model param-eters. Although, the risk of identifying or learning spe-cific information about any individual sample is minimised, data utility might be severely compromised [86]. Fur-thermore, most techniques focus on frame by frame anal-ysis and there is less work on the privacy-preservation of the spatio-temporal dynamics present in whole video se-quences [47]."}, {"title": "3. Methods", "content": "The method section begins by detailing the underlying threat models that our method addresses (Section 3.1). This provides the context for understanding the privacy risks in radar-based HAR systems. We then describe the HAR model used in our study (Section 3.2. This section explains the architecture and functionality of the model that forms the basis of our privacy-preserving approach. Finally, we introduce our novel IDG-DP technique (Section 3.3). This section elaborates on how we combine IDG with DP to miti-gate the identified privacy risks while maintaining the utility of the HAR model."}, {"title": "3.1. Problem definition and Threat Model", "content": "We exploit black-box MIA attacks to understand and tackle system's vulnerabilities in DNNs used in computer vision for HAR, because they can be executed with differ-"}, {"title": "Definition 1:", "content": "Consider an adversary A, who is provided with a specific data point z, the distribution D from which a training set S has been drawn, and access to a model M trained on S. The objective of A is to ascertain whether data point z was included in the dataset S used to train model M. This process of determining the membership of z in S constitutes MIA. [4, 90]."}, {"title": "Definition 2:", "content": "Let \u0454 > 0, a randomized algorithm M guar-antees e-differential privacy if for all neighboring input datasets D\u2081 and D2 differing on at most one element and VS \u2286 Range(M), we have [18]:\nPr[M(D\u2081) \u2208 S] < $e^{\u0454}$Pr[M(D2) \u2208 S]\nThe DP techniques often come at the cost of reducing data quality and utility [87]. Hence, drawing inspiration from [22] we propose to add stronger noise only to features that are important for subject recognition and less noise to other features. To identify which features contribute mostly to the output decision, we adapt the IDG approach [80]. IDG builds on Integrated Gradients (IG), which fulfil strong axiomatic properties [78]. However, the IDG assigns larger weights to path integrals from regions where the gradients do not suffer form saturation effects and thus reflect stronger impact on the model's decision.\nThe procedure for obtaining IDG-DP is detailed in Algo-rithm 1. The Function DP in Algorithm 1, require a dataset and an instance of the multi-task HAR model with an e and an attribution threshold that control the addition of noise. Initially, the attribution maps for both the subjects and ac-tivity recognition tasks are estimated using the IDG [80] technique. Then, lines 4 and 5 of Algorithm 1 estimated the average attributions of activities and subjects to discern those features and inject noise based on their importance."}, {"title": "3.2. Multi-Task Network", "content": "As demonstrated in [62], transfer-learned ResNet [26] models are successful at gait-based person classification. We adapt this architecture to create a multi-task network which is capable of identifying both activities and person-nel from the Micro-Doppler signatures. The Micro-Doppler signatures were created based on the Fast Fourier Transform (FFT) of overlapping time windows. This multi-task model functions as an evaluation baseline to the efficacy of the proposed privacy preservation technique."}, {"title": "3.3. Integrated Decision Gradients Differential Privacy (IDG-DP)", "content": "We develop a novel privacy preservation approach, namely IDG-DP, that builds on both IDG and Pure-DP method proposed in [18]. IDG leverages that moving across a gradient path from the decision regions would rapidly affect the output logit [80]. Its robustness stems from its strong theoretical guarantees of sensitivity, imple-mentation invariance and completeness.\nPure-DP method provides a way to control the impact of an individual's data on the outcomes of computations of ML models by intentionally adding noise into the data. Specif-ically, we use e-DP (Definition 2) with a Laplace mecha-nism [19] as the randomised algorithm. The Laplace DP noise mechanism adds noise sampled from the Laplace dis-tribution to the data. This method is known for providing a good balance between privacy protection and data util-ity, approaching optimal utility in many DP applications [18, 19] than the Gaussian distribution which only satisfies the (\u20ac, \u03b4) DP with \u20ac < 1 [18]. The chosen Laplace distribu-"}, {"title": "4. Evaluation", "content": "The dataset used in this study is the publicly available Radar dataset collected in 2019 at the Age UK West Cum-bria centre [20]. The radar used was a Frequency Modu-lated Continuous Wave (FMCW) radar operating at 5.8 GHz with 400 MHz bandwidth and 1ms chirp duration [20]. The raw-data are temporal series organised into a matrix according to the Pulse Repetition Frequency (PRF). Data are converted to micro-doppler spectrograms normalised in log scale by estimating the FFT over overlapping windows.\nFigure 2 shows the micro-doppler signatures used for classi-fication whilst considering three activities namely: walking, picking up an object and drinking from a cup. We consider ten subjects that have repeated all the activities more than once. This provided with the total number of 90 sessions, where 60 samples are used for model training and the re-maining 30 data samples are used for testing."}, {"title": "4.1. Dataset and preprocessing", "content": null}, {"title": "4.2. Experimental Setup", "content": "Experiments were run on a NVIDIA RTX A5000 GPU. All models were developed using Pytorch library [67] whilst the Adversarial Robustness Toolbox (ART) [63] was utilized in generating the attacks defined in Section 3.1."}, {"title": "4.3. Multi-task HAR DP Noise Models Implementation Procedure", "content": "Section 3.2 provides details on the HAR model input, architecture and Secton C in Appendix described the train-ing processes. To determine the effective e and attribution threshold for the proposed IDG-DP method, an initial in-vestigation was conducted. Figure 3 (a) shows the impact of various e and attribution threshold values on the per-formance of user and activity recognition using the multi-task HAR model. Figure 3(b) illustrates the recorded per-formance across various e values ranging from 2.5 to 0.05 with 15 steps. Lower epsilon values (indicating stronger privacy), make it less likely for a user to be identified. Conversely, a higher HAR accuracy indicates better per-"}, {"title": "4.4. Attack procedures", "content": "To create MIA black-box attack [74], we create an attack model using an ART [63]. The attack model was trained us-ing a subset of the training and testing data from the radar dataset [20]. Specifically, the attack model was fitted using 25 training samples and a subset of 25 unseen testing sam-ples. The trained attack model was then used to infer data features to test their resilience to the black-box MIA. The prediction of the attack model based on the inferred fea-tures determine the membership status of a data point. For the rule-based black-box attack, the attacker did not fit the attack model but instead used predetermined rules to check the membership of a data point. To ensure a fair compari-son, the same size of data used in the black-box MIA was used in the rule-based black-box attack.\nIn creating the label-only [9], we employed the Hop-SkipJump [6] adversarial technique, a decision-based at-tack. The maximum query limit was set to two for calibrat-ing the distance threshold during the generation of adversar-ial attacks. We began by creating an ART baseline model using the same architecture as the multi-task HAR model. This model was trained and tested with 20 samples over 3 epochs using a batch size of 128. The loss function and op-timiser parameters were the same to those used in the base-line multi-task model. The ART model used 20 subset sam-ples each for the training and testing to generate label-only attacks. Furthermore, a subset of 10 unseen samples were used to investigate the success rate of the generated attacks on the target HAR models (see Figure 4(a)). Furthermore, to investigate the success rate of label-only attacks [9], a larger number of samples were considered for the training and testing sizes in Figure 4(b). The attack training size was increased to 40, the testing size to 30, and the evalua-tion size to 20. The baseline ART model used in generating attacks in this scenario was trained at 10 epochs.\nFurthermore three shadow models were initialized based on the multi-task HAR target model architecture. The dataset used to train and test the shadow models consisted of a subset of 25 data samples, attack training and test-ing size 25 each (out of 90 total samples available). Sam-ples were drawn randomly out of the 25 samples for testing MIA black-box attack [74]. The learning rate and optimiza-tion parameters for each shadow model were kept consistent with those of the baseline multi-task HAR model, which were then used to create samples for testing the MIA black-box attack [74]. To assess the success rate, a black-box attack was conducted to ascertain the membership status of the generated shadow data samples."}, {"title": "5. Results", "content": null}, {"title": "5.1. HAR Performance of Attribution Based DP Models", "content": "Table 1 presents the performance comparison of HAR model across various DP-based attribution methods tested on the Radar dataset [20]. The comparison includes a base-line multi-task model without DP, a baseline learnable op-tics and other attribution methods with DP integration men-tioned above, all using an equal value of e = 1.20 for consistency. Notably, adding noise while considering rel-evant attributions does not reduce model accuracy. In fact, performance accuracy improves across all attribution meth-ods compared to the baseline model. IDG-DP achieves the best testing performance, followed by IG-DP, ISG-DP, and IIG-DP, respectively. The learnable optics method provides the lowest accuracy, likely due to the masking effect during model development. Since robustness is our primary con-cern, testing accuracy alone is insufficient to determine a method's effectiveness. It is also crucial to consider how the model behaves under MIA."}, {"title": "5.1.1 Models Performance Against Black-box and Black-box Rule Based Attacks", "content": "Table 2 describes the performance of various models against black-box MIA [74] on the Radar dataset [20]. The metrics of total attack accuracy, precision, and recall were based on the chosen attack training and testing ratios. The test accu-racy serves as the baseline clean accuracy target classifier, which is crucial for assessing model performance in the tar-get estimator role.\nThe IDG-DP clean model accuracy of HAR is better as tested using the selected samples. The total attack recall for IGD-DP is lower than the baseline, indicating enhanced ro-bustness, the GradC-DP, IIG-DP, and Sal-DP models show lower total attack recall. However, these models also ex-hibit reduced accuracy with the selected samples. Consid-ering the IDG-DP clean model's performance with the HAR target model, it appears to be the preferable option as it ef-fectively balances accuracy and attack resilience. This pref-erence is further reinforced when evaluating against more robust adversarial black-box attacks designed to induce sig-nificant misclassification in the model. In summary, the IDG-DP model stands out as a promising approach, offer-ing improved accuracy while maintaining strong defense against various attack strategies."}, {"title": "5.2. Models Performance Against Label-Only Adversarial Attacks", "content": "To evaluate the robustness of the proposed IDG-DP method, we subjected it to more sophisticated, adversar-ial MIA. Specifically, the Label-Only [9] attack was em-ployed, using the Radar [20] dataset and the Hop-Skip-Jump [6] adversarial perturbation technique to generate robust adversarial samples. The objective was to evaluate each method's resistance to this attack by observing their accuracy in detecting adversarial samples."}, {"title": "6. Conclusion", "content": "Our study breaks new ground in addressing privacy preservation for radar-based human motion analysis mod-els. We introduce a novel IDG-DP method, which exploits the solid theoretical guarantees of Differential Privacy (DP) with a principle way driven by the Integrated Decision Gra-dients (IDG) of a multi-task model to preserve the utility of the dataset. We assess IDG-DP's robustness against vari-ous black-box membership inference attacks (MIA), includ-ing sophisticated label-only attacks. Our findings demon-strate that DP, when applied with carefully selected attribu-tions effectively mitigates MIA risks. IDG-DP stands out for its ability to maintain strong HAR performance, while effectively countering MIA attacks. In the attack target HAR model, IDG-DP's accuracy surpasses that of the tested benchmarks, making it an optimal choice for balancing pri-vacy protection and utility in activity recognition. We plan to test our methods on more extensive and diverse datasets to confirm their scalability, resistance and generalizability. This research provides a solid foundation for developing privacy-preserving techniques in radar-based human motion analysis, paving the way for more secure and ethical appli-cations in healthcare monitoring and activity recognition."}]}