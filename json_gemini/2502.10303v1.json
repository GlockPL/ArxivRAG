{"title": "Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMind's Innovations", "authors": ["Abdelrhman Shaheen", "Anas Badr", "Ali Abohendy", "Hatem Alsaadawy", "Nadine Alsayad"], "abstract": "Reinforcement Learning (RI) has been widely used in many applications, one of these applications is the field of gaming, which is considered a very good training ground for AI models. From the innovations of Google DeepMind in this field using the reinforcement learning algorithms, includ- ing model-based, model-free, and deep Q-network approaches, AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning, reinforcement learning to achieve master in the game of Go, surpassing the performance of professional human players. AlphaGo Zero refines this approach by eliminating the dependency on human gameplay data, instead employing self-play to enhance learning efficiency and model performance. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across many games, including complex Atari games. In this paper, we reviewed the importance of studying the applications of reinforcement Learning in Atari and strategy-based games, by discussing these three models, the key innovations of each model,and how the training process was done; then showing the challenges that every model faced, how they encounterd them, and how they improved the performance of the model. We also highlighted the advancements in the field of gaming, including the advancment in the three models, like the MiniZero and multi-agent models, showing the future direction for these advancements, and new models from Google DeepMind.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) has revolutionized the gaming industry, both as a tool for creating intelligent in-game oppo- nents and as a testing environment for advancing AI research. Games serve as an ideal environment for training and eval- uating Al systems because they provide well-defined rules, measurable objectives, and diverse challenges. From simple puzzles to complex strategy games, AI research in gaming has pushed the boundaries of machine learning and reinforcement learning. Also The benfits from such employment helped game developers to realize the power of AI methods to analyze large volumes of player data and optimize game designs. [1] Atari games, in particular, with their retro visuals and straight- forward mechanics, offer a challenging yet accessible bench- mark for testing AI algorithms. The simplicity of Atari games hides complexity that they require strategies that involve planning, adaptability, and fast decision-making, making them also a good testing environment for evaluating Al's ability to"}, {"title": "II. RELATED WORK", "content": "There are a lot of related work that reviewed the reinforce- ment learning in strategy-based and atari games. Arulkumaran et al [9] this paper serves as a foundational reference that out- lines the evolution and state-of-the-art developments in DRL up to its publication. It also offers insights into how combining deep learning with reinforcement learning has led to significant advancements in areas such as game playing, robotics, and autonomous decision-making systems. Zhao et al. [10] surveys how DRL combines capabilities of deep learning with the decision-making processes of reinforcement learning, enabling systems to make control decisions directly from input images. It also analysis the development of AlphaGo, and examines the algorithms and techniques that contributed to AlphaGo's success, providing insights into the integration of DRL in com- plex decision-making tasks. Tang et al. [11] also surveys how AlphaGo marked a significant milestone by defeating human champions in the game of Go, and its architecture and training process; then delves into AlphaGo Zero. Shao et al. [12] categorize DRL methods into three primary approaches: value- based, policy gradient, and model-based algorithms, offering a comparative analysis of their techniques and properties. The survey delves into the implementation of DRL across various video game types, ranging from classic arcade games to complex real-time strategy games. It highlights how DRL agents, equipped with deep neural network-based policies, process high-dimensional inputs to make decisions that maxi- mize returns in an end-to-end learning framework. this review also discusses the achievement of superhuman performance by DRL agents in several games, underscoring the significant progress in this field. However, it also addresses ongoing challenges such as exploration-exploitation balance, sample efficiency, generalization and transfer learning, multi-agent coordination, handling imperfect information, and managing delayed sparse rewards.\nOur paper is similar to Shao et al. [12], as we discussed the developments that Google DeepMind made in developing AI models for games and the advancments that they made over the last years to develop the models and the future directions of implementating the DRL in games; how this implementation helps in developing real life applications. The main contribution in our paper is the comprehensive details of the three models AlphaGo, AlphaGo Zero, and MuZero, focusing on the key Innovations for each model, how the training process was done, challenges that each model faced and the improvements that were made, and the preformance benchmarks. Studying each on of these models in details helps in understanding how RL was developed in games reaching to the current state, by which it is now used in real life applications. Also we discussed the advancments in these three AI models, reaching to the future directions."}, {"title": "III. BACKGROUND", "content": "Reinforcement Learning (RL) is a key area of machine learning that focuses on learning through interaction with the environment. In RL, an agent takes actions (A) in specific states (S) with the goal of maximizing the rewards (R) received from the environment. The foundations of RL can be traced back to 1911, when Thorndike introduced the Law of Effect, suggesting that actions leading to favorable outcomes are more likely to be repeated, while those causing discomfort are less likely to recur [13].\nRL emulates the human learning process of trial and error. The agent receives positive rewards for beneficial actions and negative rewards for detrimental ones, enabling it to refine its policy function\u2014a strategy that dictates the best action to take in each state. That's said, for a give agent in state u, if it takes action u, then the immediate reward r can be modeled as r(x, u) = E[rt | x = Xt\u22121, u = ut\u22121].\nSo for a full episode of T steps, the cumulative reward R can be modeled as R = =1 t\n### A. Markov Decision Process (MDP)\nIn reinforcement learning, the environment is often modeled as a Markov Decision Process (MDP), which is defined as a tuple (S, A, P, R, \u03b3), where:\n\u2022\nS is the set of states,\n\u2022\nA is the set of actions,\n\u2022\nP is the transition probability function,\n\u2022\nR is the reward function, and\n\u2022\ny is the discount factor.\nThe MDP framework is grounded in sequential decision- making, where the agent makes decisions at each time step based on its current state. This process adheres to the Markov property, which asserts that the future state and reward depend only on the present state and action, not on the history of past states and actions.\nFormally, the Markov property is represented by:\nP(s' | s, a) = P[St+1 = s' | St = s, at = a] (1)\nwhich denotes the probability of transitioning from state s to state s' when action a is taken.\nThe reward function R is similarly defined as:\nR(s, a) = E[rt | St\u22121 = 8, at\u22121 = a] (2)\nwhich represents the expected reward received after taking action a in state S.\n### B. Policy and Value Functions\nIn reinforcement learning, an agent's goal is to find the optimal policy that the agent should follow to maximize cumulative rewards over time. To facilitate this process, we need to quantify the desirability of a given state, which is done through the value function V(s). Value function estimates the expected cumulative reward an agent will receive starting from"}, {"title": "C. Reinforcement Learning Algorithms", "content": "There are multiple reinforcement learning algorithms that have been developed that falls under a lot of categories. But, for the sake of this review, we will focus on the following algorithms that have been used by the Google DeepMind team in their reinforcement learning models.\n1) Model-Based Algorithms: Dynamic Programming\nDynamic programming (DP) algorithms can be applied when we have a perfect model of the environment, represented by the transition probability function P(s',r | s,a). These algorithms rely on solving the Bellman equations (recursive form of equations 3 and 4) iteratively to compute optimal policies. The process alternates between two key steps: policy evaluation and policy improvement."}, {"title": "1.1 Policy Evaluation", "content": "Policy evaluation involves computing the value function V\" (s) under a given policy \u03c0. This is achieved iteratively by updating the value of each state based on the Bellman equation:\nV* (s) = \u03a3\u03c0(\u03b1 | s) P(s', r | s,a) [r + V (s')]. (6)\n\u03b1\u0395\u0391\ns',r\nStarting with an arbitrary initial value V\u2122(s), the updates are repeated for all states until the value function converges to a stable estimate."}, {"title": "1.2 Policy Improvement", "content": "Once the value function V\u2122(s) has been computed, the policy is improved by choosing the action a that maximizes the expected return for each state:\n\u03c0'(s) = arg max \u2211P(s',r | s,a) [r + yV\" (s')]. (7)\na\ns',r\nThis step ensures that the updated policy \u03c0' is better than or equal to the previous policy \u03c0. The process alternates between policy evaluation and improvement until the policy converges to the optimal policy \u03c0*, where no further improvement is possible. It can be visualized as:"}, {"title": "1.3 Value Iteration", "content": "Value iteration simplifies the DP process by combining policy evaluation and policy improvement into a single step. Instead of evaluating a policy completely, it directly updates the value function using:\nV*(s) = max \u2211 P(s', r | s, a) [r + yV*(s')] . (8)\na\ns',r\nThis method iteratively updates the value of each state until convergence and implicitly determines the optimal policy. Then the optimal policy can be obtained by selecting the action that maximizes the value function at each state, as\n\u03c0*(s) = arg max \u2211 P(s', r | s,a) [r + yV*(s')]. (9)\na\ns',r\nDynamic Programming's systematic approach to policy evaluation and improvement forms the foundation for the techniques that have been cruical in training systems like AlphaGo Zero and MuZero."}, {"title": "2) Model-Free Algorithms", "content": "### 2.1 Monte Carlo Algorithm (MC)\nThe Monte Carlo (MC) algorithm is a model-free reinforce- ment learning method that estimates the value of states or state-action pairs under a given policy by averaging the returns of multiple episodes. Unlike DP, MC does not require a perfect model of the environment and instead learns from sampled experiences.\nThe key steps in MC include:\n\u2022 Policy Evaluation: Estimate the value of a state or state- action pair Q(s, a) by averaging the returns observed in multiple episodes."}, {"title": "Policy Improvement:", "content": "Update the policy to choose actions that maximize the estimated value Q(s, a).\nMC algorithms operate on complete episodes, requiring the agent to explore all state-action pairs sufficiently to ensure accurate value estimates. The updated policy is given by:\n\u03c0(s) = arg max Q(s, a). (10)\na\nWhile both MC and DP alternate between policy evalua- tion and policy improvement, MC works with sampled data, making it suitable for environments where the dynamics are unknown or difficult to model.\nThis algorithm is particularly well-suited for environments that are episodic, where each episode ends in a terminal state after a finite number of steps.\nMonte Carlo's reliance on episodic sampling and policy re- finement has directly influenced the development of search- based methods like Monte Carlo Tree Search (MCTS), which was crucial in AlphaGo for evaluating potential moves during gameplay. The algorithm's adaptability to model-free settings has made it a cornerstone of modern reinforcement learning strategies."}, {"title": "2.2 Temporal Diffrence (TD)", "content": "Temporal Diffrence is another model free algorithm that's very similar To Monte Carlo, but instead of waiting for termination of the episode to give the return, it estimates the return based on the next state. The key idea behind TD is to update the value function based on the difference between the current estimate and the estimate of the next state. The TD return is then given by:\nGt = rt+1 + V(St+1) (11)\nthat's the target (return value estimation) of state s at time t is the immediate reward r plus the discounted value of the next state St+1.\nThis here is called the TD(0) algorithm, which is the simplest form of TD that take only one future step into account. The update rule for TD(0) is:\nV(st) = V(st) + a[rt+1+yV(St+1) - V(st)] (12)\nThere are other temporal difference algorithms that works exactly like TD(0), but with more future steps, like TD(1).\nAnother important variant of TD is the Q-learning algo- rithm, which is an off-policy TD algorithm that estimates the optimal action-value function Q* by updating the current action value based on the optimal action value of the next state. The update rule for Q-learning is:\nQ(st, at) = Q(st, at)+a[rt+1+ymaxQ(st+1, a)-Q(st,at)].\na\n(13)\nand after the algorithm converges, the optimal policy can be obtained by selecting the action that maximizes the action- value function at each state, as \u03c0*(s) = arg maxa Q* (s, a).\nTemporal Difference methods, including Q-learning, play a crucial role in modern reinforcement learning by enabling"}, {"title": "3) Deep RL: Deep Q-Network (DQN)", "content": "Deep Q-Networks (DQN) represent a significant leap for- ward in the integration of deep learning with reinforcement learning. DQN extends the traditional Q-learning algorithm by using a deep neural network to approximate the Q-value function, which is essential in environments with large state spaces where traditional tabular methods like Q-learning be- come infeasible.\nIn standard Q-learning, the action-value function Q(s,a) is learned iteratively based on the Bellman equation, which updates the Q-values according to the reward received and the value of the next state. However, when dealing with complex, high-dimensional inputs such as images or unstructured data, a direct tabular representation of the Q-values is not practical. This is where DQN comes in: it uses a neural network, typ- ically a convolutional neural network (CNN), to approximate Q(s, a; 0), where @ represents the parameters of the network. The core ideas behind DQN are similar to those of tradi- tional Q-learning, but with a few key innovations that address issues such as instability and high variance during training. The DQN algorithm introduces the following components:\n\u2022 Experience Replay: To improve the stability of training and to break the correlation between consecutive expe- riences, DQN stores the agent's experiences in a replay buffer. Mini-batches of experiences are randomly sampled from this buffer to update the network, which helps in better generalization.\n\u2022 Target Network: DQN uses two networks: the primary Q-network and a target Q-network. The target network is updated less frequently than the primary network and is used to calculate the target value in the Bellman update. This reduces the risk of oscillations and divergence during training.\nThe update rule for DQN is based on the Bellman equation for Q-learning, but with the neural network approximation:\nQ(st, at; 0) = Q(st, at;0)+a [rt+1+ ymax Q(st+1, a'; 0\u00af) \u2013 Q(st, at; 0)] (14)\nwhere 0 represents the parameters of the target network. By training the network to minimize the difference between the predicted Q-values and the target Q-values, the agent learns an optimal policy over time. [16]\nThe DQN algorithm revolutionized reinforcement learning, especially in applications requiring decision-making in high- dimensional spaces. One of the most notable achievements of DQN was its success in mastering a variety of Atari 2600 games directly from raw pixel input, achieving human- level performance across multiple games. This breakthrough demonstrated the power of combining deep learning with reinforcement learning to solve complex, high-dimensional problems.\nIn subsequent improvements, such as Double DQN, Dueling DQN, and Prioritized Experience Replay, enhancements were made to further stabilize training and improve performance. However, the foundational concepts of using deep neural networks to approximate Q-values and leveraging experience replay and target networks remain core to the DQN frame- work."}, {"title": "IV. ALPHAGO", "content": "### A. Introduction\nAlphaGo is a groundbreaking AI model that utilizes neural networks and tree search to play the game of Go, which is thought to be one of the most challenging classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves [18].\nAlphaGo uses value networks for position evaluation and policy networks for taking actions, that combined with Monte Carlo simulation achieved a 99.8% winning rate, and beating the European human Go champion in 5 out 5 games.\n### B. Key Innovations\nIntegration of Policy and Value Networks with MCTS\nAlphaGo combines policy and value networks in an MCTS framework to efficiently explore and evaluate the game tree. Each edge (s, a) in the search tree stores:\n\u2022 Action value Q(s,a): The average reward for taking action a from state s.\n\u2022 Visit count N(s, a): The number of times this action has been explored.\n\u2022 Prior probability P(s, a): The probability of selecting action a, provided by the policy network.\nDuring the selection phase, actions are chosen to maximize:\nat = arg max (Q(s, a) + u(s,a)) (15)\na\nwhere the exploration bonus u(s, a) is defined as:\nu(s, a) x (16)\nP(s, a)\n1+ N(s, a)\nWhen a simulation reaches a leaf node, its value is evaluated in two ways: 1. Value Network Evaluation: A forward pass through the value network predicts ve(s), the likelihood of winning. 2. Rollout Evaluation: A lightweight policy simulates the game to its conclusion, and the terminal result z is recorded.\nThese evaluations are combined using a mixing parameter \u03bb:\nV(SL) = \u03bb\u03c5\u03b8(SL) + (1 \u2212 1)ZL (17)\nThe back propagation step updates the statistics of all nodes along the path from the root to the leaf.\nIt's also worth noting that the SL policy network performed better than the RL policy network and that's probably because"}, {"title": "C. Training Process", "content": "1) Supervised Learning for Policy Networks\nThe policy network was initially trained using supervised learning on human expert games. The training data consisted of 30 million board positions sampled from professional games on the KGS Go Server. The goal was to maximize the likelihood of selecting the human move for each position:\n\u0394\u03c3\u03b1 Vlog po(as) (18)\nwhere po (als) is the probability of selecting action a given state s.\nThis supervised learning approach achieved a move pre- diction accuracy of 57.0% on the test set, significantly outper- forming prior methods. This stage provided a solid foundation for replicating human expertise.\n2) Reinforcement Learning for Policy Networks\nThe supervised learning network was further refined through reinforcement learning (RL). The weights of the RL policy network were initialized from the SL network. AlphaGo then engaged in self-play, where the RL policy network played against earlier versions of itself to iteratively improve."}, {"title": "3) Value Network Training", "content": "The value network was designed to evaluate board positions by predicting the likelihood of winning from a given state. Unlike the policy network, it outputs a single scalar value \u0e02\u0e2d(s) between \u22121 (loss) and +1 (win).\nTraining the value network on full games led to overfitting due to the strong correlation between successive positions in the same game. To mitigate this, a new dataset of 30 million distinct board positions was generated through self- play, ensuring that positions came from diverse contexts. The value network was trained by minimizing the mean squared error (MSE) between its predictions ve(s) and the actual game outcomes z:\nL(0) = E(s,z)~D [(Ue(s) \u2013 z)2] (21)"}, {"title": "D. Challenges and Solutions", "content": "AlphaGo overcame several challenges:\n\u2022 Overfitting: Training the value network on full games led to memorization. This was mitigated by generating a diverse self-play dataset.\n\u2022 Scalability: Combining neural networks with MCTS required significant computational resources, addressed through parallel processing on GPUs and CPUs.\n\u2022 Exploration vs. Exploitation: Balancing these in MCTS was achieved using the exploration bonus u(s, a) and the policy network priors.\n### E. Performance Benchmarks\nAlphaGo achieved the following milestones:\n\u2022 85% win rate against Pachi without using MCTS.\n\u2022 99.8% win rate against other Go programs in a tourna- ment held to evaluate the performance of AlphaGo.\n\u2022 Won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi, respectively.\n\u2022 Victory against professional human players such as Fan Hui (5-0) and Lee Sedol (4-1), marking a significant breakthrough in AI."}, {"title": "V. ALPHAGO ZERO", "content": "### A. Introduction\nAlphaGo Zero represents a groundbreaking advancement in artificial intelligence and reinforcement learning. Unlike its predecessor, AlphaGo, which relied on human gameplay data for training, AlphaGo Zero learns entirely from self-play, employing deep neural networks and Monte Carlo Tree Search (MCTS). [19]\nBy starting with only the rules of the game and leveraging reinforcement learning, AlphaGo Zero achieved superhuman performance in Go, defeating the previous version of AlphaGo in a 100-0 match.\n### B. Key Innovations\nAlphaGo Zero introduced several groundbreaking advance- ments over its predecessor, AlphaGo, streamlining and enhanc- ing its architecture and training process:\n1) Unified Neural Network fo: AlphaGo Zero replaced AlphaGo's dual-network setup-separate networks for policy and value\u2014with a single neural network fe. This network outputs both the policy vector p and the value scalar v for a given game state, reprsented as\nfo(s) = (p, v) (22)\nThis unified architecture simplifies the model and im- proves training efficiency.\n2) Self-Play Training: Unlike AlphaGo, which relied on hu- man games as training data, AlphaGo Zero was trained entirely through self-play. Starting from random moves, it learned by iteratively playing against itself, generating data and refining fe without any prior knowledge of hu- man strategies. This removed biases inherent in human gameplay and allowed AlphaGo Zero to discover novel and highly effective strategies.\n3) Removal of Rollouts: AlphaGo Zero eliminated the need for rollouts, which were computationally expensive simulations to the end of the game used by AlphaGo's MCTS. Instead, fo directly predicts the value v of a state, providing a more efficient and accurate estimation.\n4) Superior Performance: By integrating these advance- ments, AlphaGo Zero defeated AlphaGo 100-0 in direct matches, demonstrating the superiority of its self-play training, unified architecture, and reliance on raw rules over pre-trained human data.\n### C. Training Process\n1) Monte Carlo Tree Search (MCTS) as policy evaluation operator\nIntially the neural network fe is not very accurate in predicting the best move, as it is intiallised with random weights at first. To overcome this, AlphaGo Zero uses MCTS to explore the game tree and improve the policy.\nAt a given state S, MCTS expands simualtions of the best moved that are most likely to generate a win based on the initial policy P(s, a) and the value V. MCTS iteratively selects moves that maximize the upper confidence bound (UCB) of the action value. UCB is designed to balanced exploration and exploitation. and it is defined as\nUCB = Q(s, a) + U(s, a) (23)\nwhere\nU(s, a) x (24)\np(s, a)\n1+ N(s, a)\nMCTS at the end of the search returns the policy vector \u03c0 which is used to update the neural network fe by minimizing the cross-entropy loss between the predicted policy by fe and the MCTS policy.\n2) Policy Iteration and self play\nThe agent plays games against itself using the predicted policy P(s, a). The agent uses the MCTS to select the best move at each state and the game is played till the end in a process called self play. The agent then uses the outcome of the game, z game winner and \u3160 to update the neural network. This process is repeated for a large number of iterations.\n3) Network Training Process\nThe neural network is updated after each self-play game by using the data collected during the game. This process involves several key steps:\n1) Intilisation of the network: The neural network starts with random weights 00, as there is no prior knowledge about the game.\n2) Generating Self-play Games: For each iteration i \u2265 1 self-play games are generated. During the game, the neural network uses its current parameters \u03b8i-1 to run MCTS and generate search probabilities \u03c0t for each move at time step t.\n3) Game Termination and scoring: A game ends when either both players pass, a resignation threshold is met, or the game exceeds a maximum length. The winner of the game is determined, and the final result zt is recorded, providing feedback to the model.\n4) Data Colletion: for each time step t, the training data (St, t, 2t) is stored, where st is the game state, \u03c0t is the search probabilities, and zt is the game outcome.\n5) Network training process: after collecing data from self- play, The neural network fe is adjusted to minimize the error between the predicted value v and the self-play winnder z, and to maximize the similarity between the search probabilities P and the MCTS probabilities. This is done by using a loss function that combines the mean- squared error and the cross entropy losses repsectibly. The loss function is defined as\nL = (z \u2212 v)2 \u2013 \u03c0\u00b9 log p + c||0||2\nwhere c is the L2 regularization term."}, {"title": "D. Challenges and Solutions", "content": "Alpha Go Zero overcame several challanges:\n1) Human knowledge Dependency: AlphaGo Zero elimi- nated the need for human gameplay data, relying solely on self-play to learn the game of Go. This allowed it to discover novel strategies that surpassed human expertise.\n2) Compelxity of the dual network approach in alpha go: AlphaGo utilized separate neural networks for policy prediction p and value estimation V, increasing the com- putational complexity. AlphaGo Zero unified these into a single network that outputs both p and V, simplifying the architecture and improving training efficiency.\n3) The need of handcrafted features: AlphaGo relied on handcrafted features, such as board symmetry and pre- defined game heuristics, for feature extraction. AlphaGo Zero eliminated the need for feature engineering by us- ing raw board states as input, learning representations directly from the data."}, {"title": "E. Performance Benchmarks", "content": "AlphaGo Zero introduced a significant improvement in neural network architecture by employing a unified residual network (ResNet) design for its fe model. This replaced the separate CNN-based architectures previously used in AlphaGo, which consisted of distinct networks for policy prediction and value estimation.\nThe superiority of this approach is evident in the Elo rating comparison shown in fig.4. The \"dual-res\" architecture, utilized in AlphaGo Zero, achieved the highest Elo rating, significantly outperforming other architectures like \"dual- conv\" and \"sep-conv\" used in earlier versions of AlphaGo."}, {"title": "VI. MUZERO", "content": "### A. Introduction\nThrough the development of AlphaZero, a general model for board games with superhuman ability has been achieved in three games: Go, chess, and Shogi. It could achieve these results without the need for human gameplay data or history, instead using self-play in an enclosed environment. However, the model still relied on a simulator that could perfectly replicate the behavior, which might not translate well to real- world applications, where modeling the system might not be feasible. MuZero was developed to address this challenge by developing a model-based RL approach that could learn without explicitly modeling the real environment. This allowed"}, {"title": "C. Loss function and learning equations", "content": "80 = ho(01,..., Ot) (25)\nTk, Sk = 90(Sk\u22121,ak) (26)\nPk, Uk = fo(sk) (27)\nPk\nUk\n= \u03bc\u03bf(01,..., Ot, A1, ..., ak) (28)\nTk\n\u03bd\u03c4, \u03c0\u2081 = MCTS(so, \u03bc\u03bf) (29)\nAt ~ t (30)\n\u03a1\u03b5\u03c5\u03ba\u03b1 = \u03bc\u03bf(01,..., Ot, At+1,...,at+k) (31)\nZt = \n\u0438\u0442,\nfor games\nUt+1 + yut+2+... +yn-1ut+n+yVt+n, for general MDPs"}, {"title": "D. MCTS", "content": "MuZero uses the same approach developed in AlphaZero to find the optimum action given an internal state. MCTS is used where states are the nodes, and the edges store visit count, mean value, policy, and reward. The search is done in a three- phase setup: selection, expansion, and backup. The simulation starts with a root state, and an action is chosen based on the state-transition reward table. Then, after the end of the tree, a new node is created using the output of the dynamics function as a value, and the data from the prediction function is stored in the edge connecting it to the previous state. Finally, the simulation ends, and the updated trajectory is added to the state-transition reward table. In two-player zero-sum games, board games, for example, the value function is bounded between 0 and 1, which is helpful to use value estimation and probability using the pUCT rule. However, many other environments have unbounded values, so MuZero rescales the value to the maximum value observed by the model up to this training step, ensuring no environment-specific data is needed. [20]"}, {"title": "AlphaZero", "content": "While AlphaGo Zero was an impressive feat, designed specifically to master the ancient game of Go through self-play, AlphaZero developes it by generalizing its learning framework to include multiple complex games: chess, shogi (Japanese chess), and Go. The key advancement is in its ability to apply the same algorithm across different games without requiring game-specific adjustments. AlphaZero's neural network is trained through self-play, predicting the move probabilities and game outcomes for various positions. This prediction is then used to guide the MCTS, which explores potential future moves and outcomes to determine the best action. Through iterative self-play and continuous refinement of the neural network, AlphaZero efficiently learns and improves its strategies across different games [21]. Another significant improvement is in AlphaZero's generalized algorithm, is that it does not need to be fine-tuned for each specific game. This was a departure from AlphaGo Zero's Go-specific architecture, making AlphaZero a more versatile AI system.\nAlphaZero's architecture integrates a single neural network that evaluates both the best moves and the likelihood of win- ning from any given position, streamlining the learning process by eliminating the need for separate policy and value networks used in earlier systems. This innovation not only enhances computational efficiency but also enables AlphaZero to adopt unconventional and creative playing styles that diverge from established human strategies."}, {"title": "MiniZero", "content": "MiniZero is a a zero-knowledge learning framework that supports four state-of-the-art algorithms, including AlphaZero, MuZero, Gumbel AlphaZero, and Gumbel MuZero [22]. Gumbel AlphaZero and Gumbel MuZero are variants of the AlphaZero and MuZero algorithms that incorporate Gumbel noise into their decision-making process to improve explo- ration and planning efficiency in reinforcement learning tasks."}, {"title": "Multi-agent models", "content": "Multi-agent models in reinforcement learning (MARL) rep- resent an extension of traditional single-agent reinforcement learning. In these models, multiple agents are simultaneously interacting, either competitively or cooperatively, making de- cisions that impact both their own outcomes and those of other agents. The complexity in multi-agent systems arises from the dynamic nature of the environment, where the actions of each agent can alter the environment and the states of other agents. Unlike in single-agent environments, where the agent learns by interacting with a static world, multi-agent systems require agents to learn not only from their direct experiences but also from the behaviors of other agents, leading to a more complex learning process. Agents must adapt their strategies based on what they perceive other agents are doing, and this leads to problems such as strategic coordination, deception, negotiation, and competitive dynamics. In competitive sce- narios, agents might attempt to outwit one another, while in cooperative scenarios, they must synchronize their actions to achieve a common goal [23].\nAlphaGo and AlphaGo Zero are not designed to handle multi- agent environments. The core reason lies in their foundational design, which assumes a single agent interacting with a static environment. AlphaGo and AlphaGo Zero both rely on model- based reinforcement learning and self-play, where a single agent learns by interacting with itself or a fixed opponent, refining its strategy over time. However, these models are not built to adapt to the dynamic nature of multi-agent environ- ments, where the state of the world constantly changes due to the actions of other agents. In AlphaGo and AlphaGo Zero, the environment is well-defined, and the agent's objective is to optimize its moves based on a fixed set of rules. The agents in these models do not need to account for the actions of other agents in real-time or consider competing strategies, which are essential in multi-agent systems. Additionally, AlphaGo and AlphaGo Zero are not designed to handle cooperation or ne- gotiation, which are key aspects of multi-agent environments. On the other hand, MuZero offers a more flexible framework that can be adapted to multi-agent environments. Unlike Al- phaGo and AlphaGo Zero, MuZero operates by learning the dynamics of the environment through its interactions, rather than relying on a fixed model of the world. This approach allows MuZero to adapt to various types of environments, whether single-agent or multi-agent, by learning to predict the consequences of actions without needing explicit knowledge of the environment's rules. The key advantage of MuZero in multi-agent settings is its ability to plan and make decisions without needing to model the entire system upfront. In multi- agent environments, this ability becomes essential, as MuZero can dynamically adjust its strategy based on the observed behavior of other agents. By learning not just the immediate outcomes but also the strategic implications of others' actions, MuZero can navigate both competitive and cooperative set- tings."}, {"title": "VIII. FUTURE DIRECTIONS", "content": "As mentioned earlier in the paper, The development of the Al models and systems in the field of gaming represent a good training set for the models to study the environment, address the challenges, modify the models, and achieve good results in this field, to judge whether this model is able to be implemented in real world, and how it can be implemented. The main purpose from such models, Google DeepMind, through the previous years, had been training the models to play games, and the main goal was to implement the models of reinforcement learning in real life, and benefit from"}]}