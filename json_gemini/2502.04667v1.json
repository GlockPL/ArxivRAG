{"title": "Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization", "authors": ["Xinhao Yao", "Ruifeng Ren", "Yun Liao", "Yong Liu"], "abstract": "Training large language models (LLMs) with high-quality Chain-of-Thought (CoT) annotations has become a widely adopted strategy due to its significant enhancement of reasoning capabilities. To fully comprehend this approach, two questions naturally arise: (Q1) What advantages does training with CoT offer compared to training without CoT? (Q2) If there are advantages, what are the underlying mechanisms of explicit CoT training? Analyzing the advantages and mechanisms of CoT training is challenging due to the many factors involved. To address this, we conduct a detailed analysis using clear and controllable data distributions and, for the first time, reveal that CoT training offers the following advantages: (1) Training with CoT markedly improves reasoning generalization, extending it from in-distribution (ID) to both ID and out-of-distribution (OOD) scenarios, while also speeding up convergence; (2) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization. We further explore the underlying mechanisms from a circuit perspective: (1) The data distribution (e.g., ratio \u5165 and pattern) plays a crucial role in influencing the model's systematic generalization; (2) CoT training (with two-hop facts) internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Our findings elucidate the mechanisms underlying explicit CoT training and offer critical insights into tuning strategies for LLMs to achieve robust generalization\u00b9.", "sections": [{"title": "1. Introduction", "content": "Training large language models (LLMs) to generate solutions step by step during the training phase has gained significant attention and is progressively becoming a widely adopted practice in the industry (Yue et al., 2023; Yu et al., 2024b; Wang et al., 2024b; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a; Kim et al., 2023; Hsieh et al., 2023; Ho et al., 2023; OpenAI, 2024a; DeepSeek-AI et al., 2025). For instance, OpenAI has advanced AI customization with the introduction of reinforcement fine-tuning (RFT) for its Ol models, unveiled on the second day of the \"12 Days of OpenAI\" livestream series (OpenAI, 2024a). A key component of RFT/ReFT (Trung et al., 2024) is conducting supervised fine-tuning (SFT) using Chain-of-Thought (CoT) annotations (Wei et al., 2022). In the DeepSeek-R1 model (DeepSeek-AI et al., 2025), a small amount of long CoT cold-start data is incorporated to tune the model as the initial reinforcement learning (RL) actor. However, to gain a comprehensive understanding of the strategy of training with CoT, two key questions need to be addressed:\n\u2022 Q1: What advantages does training with CoT offer compared to training without CoT?\n\u2022 Q2: If there are advantages, what are the underlying mechanisms of explicit CoT training?"}, {"title": "Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization", "content": "Given the numerous factors involved in the actual training process, analyzing the advantages and underlying mechanisms of explicit CoT training presents significant challenges. To address this, we perform a detailed analysis using clear and controllable data distributions and uncover the following intriguing phenomena:\n\u2022 The Advantages of Training with CoT. (i) Compared to training without CoT, training with CoT significantly enhances reasoning generalization, extending it from in-distribution (ID)-only to both ID and (out-of-distribution) OOD scenarios (indicating systematic generalization), while also accelerating the convergence speed (Figure 1, Section 4). (ii) Even when training with CoT includes a certain range of erroneous reasoning steps, it still enables the model to learn reasoning patterns, leading to systematic generalization (Figures 4 and 5, Section 6.1). This highlights that data quality is more important than the method itself. The primary bottleneck in training lies in collecting complex long CoT solutions, with some errors being acceptable.\n\u2022 The Internal Mechanisms of Training with CoT. (i) The crucial factors of data distribution (e.g., ratio \u5165 and pattern) in characterizing the model's systematic generalization (Section 4.4). In other words, a model that has only encountered two-hop data during CoT training cannot directly generalize to three-hop cases, it needs to have encountered the relevant patterns. (ii) Through Logit Lens and Causal Tracing experiments, we find that CoT training (with two-hop facts) internalizes reasoning steps into the model, forming a two-stage generalizing circuit. The number of reasoning circuit stages matches the number of explicit reasoning steps during the training process (Section 5).\nA Summary of Our Analyses. In Section 3, we define multi-step reasoning, training data, CoT methods, and generalization evaluation using symbols. Our analyses in Sections 4 and 5 correspond to advantages and mechanisms, respectively. We further extend our analyses to distributions with errors present in the reasoning process (Section 6.1) and verify the insights remain valid for more complex architectures on real-world data (Section 6.2). As far as we are aware, our work is the first to explore the benefits of CoT training in controlled experiments and provide a circuit-based explanation of the mechanisms underlying CoT training. These findings offer valuable insights into CoT and tuning strategies for LLMs to achieve robust generalization."}, {"title": "2. Related Work", "content": "Chain-of-Thought Reasoning and Training. Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers (Vaswani et al., 2017) with advanced generation capabilities. This includes prompting LLMs (Wei et al., 2022; Zhou et al., 2023b; Khot et al., 2023), or training LLMs to generate reasoning chains, either through supervised fine-tuning (Yue et al., 2023; Yu et al., 2024b) or reinforcement learning (Wang et al., 2024b; Havrilla et al., 2024; Shao et al., 2024; Yu et al., 2024a). However, the specific advantages of CoT training remain an open question, which is the focus of our study. There are also theoretical analyses that have shown the usefulness of CoT from the perspective of model capabilities (e.g., expressivity) (Feng et al., 2023; Merrill & Sabharwal, 2024; Li et al., 2024c; Prabhakar et al., 2024; Yin et al., 2025). For instance, by employing CoT, the effective depth of the transformer increases as the generated outputs are fed back into the input (Feng et al., 2023). These analyses, along with the practice effectiveness of CoT, motivate our exploration of the core mechanisms underlying explicit CoT training. Our findings suggest that a two-layer transformer may be sufficient for learning generalizing circuits through CoT training, which may explain the origin of CoT's expressivity during the training phase.\nLatent Reasoning in Language Models. The investigation of latent multi-hop abilities of LLMs could also have significant implications for areas such as generalization (Lake & Baroni, 2018b; Onoe et al., 2023) and model editing (Zhong et al., 2023; Cohen et al., 2024). To enhance the latent reasoning capabilities of LLMs, it is crucial to first understand the internal mechanisms by which LLMs effectively handle two-hop facts using latent multi-hop reasoning, which is also our focus. While a precise latent reasoning pathway has been found in controlled experiments (Stolfo et al., 2023; Nanda et al., 2023; Conmy et al., 2023; Brinkmann et al., 2024; Li et al., 2024b; Rai & Yao, 2024; Yao et al., 2024; Wang et al., 2024a), it has not been thoroughly investigated in large pre-trained models. To fill this gap, Yang et al. (2024) construct a dataset of two-hop reasoning problems and discovered that it is possible to recover the intermediate variable from the hidden representations. Biran et al. (2024) identify a sequential latent reasoning pathway in LLMs, where the first-hop query is initially resolved into the bridge entity, which is then used to answer the second hop, they further propose to intervene the latent reasoning by \"back-patching\u201d the hidden representation. Nevertheless, these studies focus only on CoT prompting and do not consider CoT training. Recently, it has also been found that one can \"internalize\" the CoT reasoning into latent reasoning in transformers with knowledge distillation (Deng et al., 2023) or a special training curriculum that gradually shortens CoT (Deng et al., 2024). Loop transformers (Giannou et al., 2023; Cabannes et al., 2024; Fan et al., 2024) have been proposed to solve algorithmic tasks. However, these works focus more on innovations in training methods and algorithms, without fully analyzing the underlying mechanisms of CoT training. This is precisely what we aim to uncover."}, {"title": "3. Preliminaries and General Setup", "content": "Atomic and Multi-Hop Facts. The atomic (one-hop) fact describes two entities and the relationship between them, which can be represented as a triplet $(e_1, r_1, e_2)$. For example, \"The United States' capital is Washington, D.C.\" More specifically, $e_1$ refers to the head entity (e.g., the United States), $r$ is the relation (e.g., Capital), and $e_2$ refers to the tail entity (e.g., Washington, D.C.). Based on atomic facts, a two-hop fact can be derived by combining two atomic facts, such as $(e_1, r_1, e_2) \\oplus (e_2, r_2, e_3) \\Rightarrow (e_1, r_1, r_2, e_3)$, where $e_2$ serves as a bridge entity. Similarly and recursively, a multi-hop fact can be constructed from more atomic facts, resulting in $(e_1, r_1, e_2) \\oplus (e_2, r_2, e_3) \\oplus ... \\oplus (e_n, r_n, e_{n+1}) \\Rightarrow (e_1, r_1, r_2, ..., r_n, e_{n+1})$, where n is the number of steps. In real-world scenarios, $e_2$ corresponds to the intermediate inference in a two-hop fact, while $e_2, e_3, ..., e_n$ are intermediate results in more complex multi-step reasoning.\nTraining Data. Firstly, following the work of Wang et al. (2024a), we define the sets of entities $\\mathcal{E}$ and relations $\\mathcal{R}$, from which the set of atomic facts is constructed as $S = \\{(e_1, r_1, e_2)|e_1, e_2 \\in \\mathcal{E}, r_1 \\in \\mathcal{R}\\}$. Our training set of atomic facts, denoted as $S$, is sampled from $\\mathcal{S}$ (i.e., $S \\subset \\mathcal{\\check{S}}$). Then, we partition $S$ into two subsets, $S_{ID}$ and $S_{OOD}$, which are used to form two sets of two-hop facts, $S_{ID}^{(2)}$ and $S_{OOD}^{(2)}$, where $S_{ID}^{(2)} = \\{(e_1, r_1, r_2, e_3)|(e_1, r_1, e_2), (e_2, r_2, e_3) \\in S_{ID}, e_1 \\neq e_3\\}$, and $S_{OOD}^{(2)}$ is formed in a similar manner. To evaluate the model's in-distribution (ID) and out-of-distribution (OOD) generalization ability, the training dataset $T$ excludes $S_{IDtest}^{(2)}$, $S_{OOD}^{(2)}$, that is, $T \\subseteq S \\cup S_{IDtrain}^{(2)} \\cup S_{OOD}^{(2)}$, where $S_{IDtrain}^{(2)} = |S| / |S_{ID}|$, where $|set|$ denotes the number of samples in the set. Notice that $S_{ID} \\cap S_{OOD} = \\emptyset$, so $S_{ID}^{(2)} \\cap S_{OOD}^{(2)} = \\emptyset$, and the relation ($r_1, r_2$ in $S$) will not appear in $S_{OOD}$.\nTraining without/with CoT. For atomic facts ($S = S_{ID} \\cup S_{OOD}$), training and evaluation are performed by having the model predict the final tail entity ($e_1, r_1 \\rightarrow e_2$, $(e_1, r_1, e_2) \\in S$, $e_2$ is the prediction of $e_2$, input $\\xrightarrow[]{predict}$ output). As for two-hop facts, we consider whether to use CoT annotations during training ($(e_1, r_1, r_2, e_3) \\in S_{ID}^{(2)}$). (1) Training without CoT: $e_1, r_1, r_2 \\xrightarrow[]{predict} e_3$, only predict the final tail entity $e_3$. (2) Training with CoT: $e_1, r_1 \\xrightarrow[]{predict} e_2$ and $e_1, r_1, r_2, e_2 \\xrightarrow[]{predict} e_3$, predict both the bridge entity $e_2$ and the final tail entiy $e_3$.\nID/OOD Evaluation. To better evaluate the generalization capacity of the model, we assess its performance on both ID and OOD data. (1) ID generalization aims to determine whether the model has correctly learned the latent patterns by evaluating its ability to complete previously unseen two-hop facts $S_{IDtest}^{(2)}$. (2) OOD generalization aims to"}, {"title": "4. Systematic Composition Generalization", "content": "Our investigations focus on composition, where a model needs to \"chain\" different pieces of facts, as stated in Section 3. Although explicit verbalizations of reasoning steps (e.g., chain-of-thought rationales) can enhance task performance (Lake & Baroni, 2018a; Wei et al., 2022; Wang et al., 2022; Zelikman et al., 2022; Liu et al., 2023), they are not available during large-scale (pre-)training, which is the stage in which the model's core capabilities are developed (Li et al., 2020; Zhou et al., 2023a). Prior work has extensively studied whether transformer-based language models can perform implicit composition, with consistently negative results reported (Press et al., 2023; Yang et al., 2024). Specifically, there exists a \u201ccompositionality gap\u201d (Press et al., 2023), i.e., the frequency at which the model knows all the underlying basic facts but fails to compose them, which is considerable across different LLMs and does not decrease as models scale. To be more precise, Wang et al. (2024a) reveal that transformers are capable of learning to perform implicit reasoning in ID generalization, but not in OOD generalization (same in Figure 1(left)). It naturally raises the question: How would the generalization ability be affected if we use explicit reasoning steps in models during training? (answer to Q1: What advantages does training with CoT offer compared to training without CoT?)\n4.1. Setup\nThe model we employ is a standard decoder-only transformer as in GPT-2 (Radford et al., 2019), with a configuration of 8 layers, 768 hidden dimensions, and 12 attention heads, and the tokenization is done by having a unique token for each entity/relation for convenience2.\nMoreover, in order to control the data distribution, we utilize the data-generation process introduced in Section 3. Specifically, $S$ is constructed with $|E| = 2000$ entities and $|R| = 200$ relations. One-hop (atomic) facts correspond to the $(e_1, r_1, e_2)$ triplets in $S$."}, {"title": "4.2. CoT Training Boosts Reasoning Generalization", "content": "As shown in Figure 1, we demonstrate the model's accuracy on the training and testing two-hop facts throughout optimization, with $\\lambda$ = 7.2.\n(1) Training without CoT (Figure 1 (left)). We observe the same phenomenon (known as grokking (Power et al., 2022)) as Wang et al. (2024a), namely that the model can generalize well to ID testing examples $S_{IDtest}^{(2)}$, but high performance is only achieved after extensive training, far beyond the point of overfitting. Furthermore, even after training for millions of optimization steps, there is still no sign of OOD generalization ($S_{OOD}^{(2)}$), indicating that it is a case of delayed generalization lacking systematicity. The model may have memorized or learned patterns present in the training data.\n(2) Training with CoT (Figure 1 (right)). With CoT annotations, the convergence speed on the training set is accelerated, and the model achieves high test performance much earlier in the training process, particularly for ID testing examples. The accuracy on the ID test dataset $S_{IDtest}^{(2)}$ reaches near-perfect levels after approximately 4,000 optimization steps, indicating a significant improvement in generalization compared to training without CoT. The OOD generalization ($S_{OOD}^{(2)}$) also shows notable improvement, highlighting that CoT prompting training plays a crucial role in enhancing generalization not only within the distribution but also out-of-distribution, albeit with varying degrees of effectiveness. Therefore, by incorporating explicit reasoning steps during training, the model's generalization ability is enhanced, evolving from nonsystematic to systematic generalization. The model has learned the underlying patterns."}, {"title": "Insight 1.", "content": "Compared to training without CoT, training with CoT significantly boosts reasoning generalization, expanding it from ID-only to both ID and OOD scenarios, while also accelerating convergence speed."}, {"title": "4.3. Exploration of Influencing Components", "content": "We further conduct an ablation study to assess the impact of the following components on CoT during training. (1) Ratio \u03bb between two-hop facts and one-hop facts in training set. (2) Model scales of the from-scratch trained model. (3) The training set size, which scales linearly with |E|. Notice that what we present here is the accuracy on the OOD test dataset ($S_{OOD}^{(2)}$)."}, {"title": "4.4. Two-Hop to Muti-Hop", "content": "In Sections 4.1 to 4.3, we primarily focus on two-hop facts. In this subsection, we shift our focus to multi-hop scenarios: Can a model that has only encountered two-hop facts during the CoT training phase generalize to three-hop facts\u00b3?\nExperiment Setup. The rule of (three-hop) composition is $(h, r_1, b_1) \\oplus (b_1, r_2, b_2) \\oplus (b_2, r_3, t) \\rightarrow (h, r_1, r_2, r_3, t)$,"}, {"title": "Insight 2.", "content": "When the intermediate reasoning results (data patterns) from explicit CoT training are highly aligned with or closely match the intermediate reasoning required for final inference during testing, the model's generalization ability is significantly enhanced. This could explain why the researchers behind DeepSeek-R1 (DeepSeek-AI et al., 2025) construct and collect a small amount of long CoT data to fine-tune the model in the Cold Start phase."}, {"title": "Summary.", "content": "Up to this point, we have demonstrated that incorporating explicit CoT training in controlled experiments significantly enhances the reasoning generalization ability, shifting it from ID-only generalization to encompassing both ID and OOD generalization. The crucial factors of data distribution (e.g., ratio and pattern) in characterizing the model's systematic generalization. However, the internal mechanisms driving these improvements remain unclear, which we will further investigate in the next section (anwser to Q2: If there are advantages, what are the underlying mechanisms of explicit CoT training?)."}, {"title": "5. Two-Stage Generalizing Circuit", "content": "We analyze the inner workings of the model throughout generalization via two prevalent approaches: logit lens (Nostalgebraist, 2020) and causal tracing (Pearl, 2009). We apply our analysis to the setting in Section 4.1, and we also show the layer-wise gradients change in Appendix D.4.\nLogit Lens & Causal Tracing. Logit lens is a widely used for inspecting hidden states of LLMs (Dar et al., 2023;"}, {"title": "Two-Stage Generalizing Circuit.", "content": "We perform a set of logit lens and causal tracing experiments after training with CoT. Figure 3 illustrates the discovered generalizing circuit, which represents the causal computational pathways after the 8-layer model achieves two-hop out-of-distribution (OOD) generalization. We also plot the 2-layer model circuit in Appendix D.1, demonstrating that a two-layer transformer is sufficient to learn generalizing circuits from CoT training4. Specifically, we identify a highly interpretable causal graph consisting of states in layers 0, l, and 8, where weak nodes and connections have been pruned. The circuit exhibits two"}, {"title": "Insight 3.", "content": "CoT training internalizes reasoning steps, with the number of reasoning circuit stages matching the number of explicit reasoning steps during training."}, {"title": "6. More General Experience", "content": "At a high level, our study so far paves the way for a deeper understanding and enhancement of the transformer's generalization abilities through CoT training on controlled data distributions. However, real-world training data distributions are often more complex. In this section, we extend our"}, {"title": "6.1. CoT Training with Noise", "content": "Method. We aim to analyze the robustness of the systematic generalization gained through CoT training under noisy training data. We choose the same datasets in Section 4.2, and a 2-layer model stated in Section 4.1. We introduce noise to the $S_{ID}^{(2)}$ by randomly selecting a valid entity (the gold training target is h, r_1, r_2, b, t, similar in Section 5):\n(1) Only the second hop is noisy, h, r_1, r_2, b, $t_{noise}$; (2) Both hops are noisy, h, r_1, r_2, $b_{noise}$, $t_{noise}$. Note that the noise"}, {"title": "Insight 4.", "content": "CoT training still enables systematic generalization with noisy data, highlighting that data quality outweighs the method itself. The training bottleneck lies in collecting or synthesizing complex long CoT solutions, with some errors being acceptable."}, {"title": "6.2. Realistic Data Verification", "content": "Experiment Setup. We extend our experiments to Llama2-7B (Touvron et al., 2023) to verify whether the insights (that the model can achieve systematic composition generalization through CoT training) hold true for real-world datasets. We use the dataset provided by (Biran et al., 2024), which contains 82,020 two-hop queries based on data from Wikidata (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014). To exclude the influence of the model's inherent knowledge, we filter the data and split the training and testing set.\n(1) The training set is made up of 6,442 cases where the model correctly answers both the two-hop query and the first hop.\n(2) The testing set includes 2,320 cases where the model"}, {"title": "7. Discussion", "content": "Conclusion. This paper reveals the core mechanisms underlying Chain-of-Thought (CoT) training by illustrating how systematic composition generalization emerges in transformers through explicit CoT training in controlled and interpretable settings. Specifically:\n(1) Compared to training without CoT, CoT training significantly enhances reasoning generalization, expanding it from in-distribution (ID)-only generalization to encompass both ID and out-of-distribution (OOD) scenarios.\n(2) Through Logit Lens and Causal Tracing experiments, we find that CoT training (with two-hop facts) internalizes reasoning steps into the transformers, forming a two-stage generalization circuit. However, the model's reasoning ability is constrained by the complexity of the training data, as it struggles to generalize from two-hop to three-hop cases. This suggests that CoT primarily reproduces reasoning patterns present in the training set.\n(3) We further extend our analysis to distributions with errors present in the reasoning process, demonstrating that CoT training can still enable the model to achieve systematic generalization when the noise remains within a certain range. The structure of such noisy data can contribute to the generalization circuits.\nInterestingly, our work also highlights the bottleneck of training with CoT: training data distribution (ratio \u03bb and"}, {"title": "Limitation and Future Work.", "content": "(1) Although our bottom-up work provides valuable insights for practical applications, a key limitation of our work is that the experiments and analyses are based on synthetic data, which may not fully capture the complexities of real-world datasets and tasks. While some of our conclusions have also been validated in models like Llama2-7B (Touvron et al., 2023), further verification on a wider range of models is necessary to bridge the gap between our theoretical understanding and practical application.\n(2) Our analysis is currently limited to the use of natural language. In the future, we aim to explore the potential of LLM reasoning in an unrestricted latent space, specifically through approaches such as training large language models to reason in a continuous latent space (Hao et al., 2024b).\n(3) Common interpretability methods project weights and hidden states obtained from the forward pass onto the models' vocabularies (Nostalgebraist, 2020; Geva et al., 2021; 2022; Ma et al., 2023; Katz & Belinkov, 2023a), helping to uncover how information flows within Transformer-based LMs, such as the \u201cLogit Lens\u201d we use. Back propagation has been playing a major role in interpreting deep learning models and multiple lines of study aggregate the gradients to provide explainability (Sanyal & Ren, 2021; Chefer et al., 2022; Miglani et al., 2023). A recent method, \u201cBackward Lens\u201d (Katz et al., 2024), projects LM gradients onto the vocabulary space to capture the backward information flow. This provides us with a new perspective to analyze the mechanisms underlying CoT training."}, {"title": "Impact Statement", "content": "This study presents the advantages gained from training with CoT and sheds light on the mechanisms underlying explicit CoT training. Our goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. However, future research could use the insights we provided to design effective strategies for fine-tuning LMs. We hope such cases will be used to develop better and safer models."}, {"title": "A. Example with Distribution Differences", "content": "The key distinction between ID and OOD data lies in whether their distributions are the same. We provide a more prominent example with distribution differences here. The training set is composed of $S_{ID}$, $S_{OOD}$ and $S_{IDtrain}^{(2)}$:\n$S_{ID}$:\n(Paris, CapitalOf, France), (France, LocatedIn, Europe), (Berlin, CapitalOf, Germany), (Germany, LocatedIn, Europe).\n$S_{OOD}$:\n(Lima, CapitalOf, Peru), (Peru, HasNaturalFeature, Andes Mountains).\n$S_{IDtrain}^{(2)}$, a uniformly random subset of the inferred facts derived from $S_{ID}$:\n(Paris, CapitalOfCountryLocatedIn, Europe).\n$S_{IDtest}^{(2)}$, previously unseen inferred facts derived from $S_{ID}$ (ID generalization):\n(Berlin, CapitalOfCountryLocatedIn, Europe).\n$S_{OOD}^{(2)}$, previously unseen inferred facts derived from $S_{OOD}$ (OOD generalization):\n(Lima, CapitalOfCountryWithNaturalFeature, Andes Mountains).\nKey Changes. This OOD data setup requires the model to tackle greater knowledge distribution differences and reasoning challenges during evaluation.\nChange in Relation Types: New relation types, such as \"HasNaturalFeature,\" are introduced in the OOD data.\nComplexity of Reasoning Paths: Reasoning paths involving natural features are added, requiring the model not only to reason but also to generalize to new types of knowledge."}, {"title": "B. Training Details", "content": "Model. The model we employ is a standard decoder-only transformer as in GPT-2 (Radford et al., 2019). We use 8-layer for Figure 1 and Figure 2 (left), while 2,4,8-layer for Figure 2 (right) and 2-layer for Figure 4 and 5. The other hyperparameters are consistent with those described in Section 4.1.\nTokenization. In our main content (Section 4), we assign a unique token to each relation/entity by default. This is because Wang et al. (2024a) find that different tokenizations affect the results in rather expected ways, and do not influence the reasoning generalization findings. We also validate our findings in the real-world entity-tokenization in Section 6.2.\nOptimization. Optimization is done by AdamW (Loshchilov & Hutter, 2019) with learning rate $10^{-4}$, batch size 512, weight decay 0.1 and 2000 warm-up steps. Notably, models are trained for a large number of epochs/steps beyond the point where training performance saturates."}, {"title": "C. More Method Details", "content": "C.1. Causality Analysis\nWe follow the recent practice (Wang et al., 2023b), where the causal tracing process consists of three steps (for every stage in Section 5):\n(1) In the normal run, we capture the model's hidden state activations for a regular input (h, r1, r2)/(h, r1, r2, b). Notably, as the model maintains perfect training accuracy throughout the CoT training process, the final prediction invariably aligns with the ground truth7 bridge entity b for the first-hop stage and tail entity t for the second-hop stage.\n(2) During the perturbed run, the model is given a slightly modified input that alters its prediction, and the hidden state activations are recorded once again. Specifically, for the hidden state of interest, we modify the input token at the"}, {"title": "C.2. Realistic Data Filtering Process", "content": "Referring to (Biran et al., 2024), we aim to filter out cases where no latent reasoning occurs. Given a two hop query ((h, r\u2081, b), (b, r2, t) we test two prompts constructed to detect and filter out cases where the model performs reasoning shortcuts (Xu et al., 2022; Wang et al., 2023a; Ju et al., 2024). (1) The first prompt is ((, r\u2081, b), (b, r2, t) (i.e., the query without h), aimed at filtering out cases where the model predicts generally popular entities. (2) The second prompt is ((h,, b), (b, r2, t) (i.e., the query without r\u2081), aimed at filtering out cases where the model predicts correctly due to a high correlation between h and t. We perform this filtering for the model (Llama2-7B(-chat)) using greedy decoding creating a subset of the dataset.\nWe are specifically interested in the model's reasoning performance, therefore, we further generate two dataset subsets: (1) The first subset is made up of 6,442 cases where the model correctly answers both the two-hop query and the first hop. (2) The second subset includes 2,320 cases where the model correctly answers both the first and second hop in isolation, but fails to answer the full two-hop query. Notice that we only use the first subset for CoT training and the second subset for generalization evaluation."}, {"title": "D. More Experience Results", "content": "D.1. Two-Layer Model Circuit"}, {"title": "D.4. Layer-Wise Gradients Change", "content": "Nuclear Norm: The nuclear norm of gradient (G) is defined as the l\u2081 norm of the singular values, which reflects the sparsity of the spectrum and serves as a convex surrogate of the matrix rank. Hence, it does not only quantify the gradient magnitude but also the concentration of the spectrum on its top few singular values, which is vital to understand the gradient patterns in each layer, i.e., $||G||_{nuclear} = \\sum_{j=1}^{min(m,n)} |\\sigma_j|$, $\\sigma_j(j = 1, ...,)$ are singular values of G.\nD.5. Data Generation Process\nSpecifically, for one-hop facts, a random knowledge graph G is constructed with |E| entities and |R| = 200 relations. Each entity, acting as the head (h), is linked to 20 unique relations, with each relation connecting to another randomly selected entity serving as the tail (t). One-hop facts correspond to the (h, r,t) triplets in G. S C G C \u0160. These triplets are partitioned into two disjoint sets: SID (95%) and SOOD (5%), which are used to deduce the ID/OOD two-hop facts (\u2200h, b, t \u2208 E, \u2200r1, r2 \u2208 R): (h, r1, b) \u2295 (b, r2, t) \u2192 (h, r1, r2, t)."}, {"title": "E. More Related Work", "content": "Chain-of-Thought Reasoning. Research has demonstrated that incorporating an intermediate reasoning process in language before producing the final output significantly enhances performance, especially for transformers (Vaswani et al., 2017) with advanced generation capabilities. This includes prompting LLMs (Wei et al., 2022; Zhou et al., 2023b; Khot et al., 2023), or training LLMs to generate reasoning chains, either with supervised fine-tuning ("}]}