{"title": "How Good Are LLMs for Literary Translation, Really? Literary Translation Evaluation with Humans and LLMs", "authors": ["Ran Zhang", "Wei Zhao", "Steffen Eger"], "abstract": "Recent research has focused on literary machine translation (MT) as a new challenge in MT. However, the evaluation of literary MT remains an open problem. We contribute to this ongoing discussion by introducing LITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified human translations and outputs from 9 MT systems, which totals over 2k paragraphs and includes 13k annotated sentences across four language pairs, costing 4.5k\u20ac. This corpus enables us to (i) examine the consistency and adequacy of multiple annotation schemes, (ii) compare evaluations by students and professionals, and (iii) assess the effectiveness of LLM-based metrics. We find that Multidimensional Quality Metrics (MQM), as the de facto standard in non-literary human MT evaluation, is inadequate for literary translation: While Best-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with professional translators prefer human translations at rates of ~82% and ~94%, respectively, MQM with student annotators prefers human professional translations over the translations of the best-performing LLMs in only ~42% of cases. While automatic metrics generally show a moderate correlation with human MQM and SQM, they struggle to accurately identify human translations, with rates of at most ~20%. Our overall evaluation indicates that human professional translations consistently outperform LLM translations, where even the most recent LLMs tend to produce more literal and less diverse translations compared to human translations. However, newer LLMs such as GPT-40 perform substantially better than older ones.", "sections": [{"title": "1 Introduction", "content": "With the advent of Large Language Models (LLMs), literary translation, once the exclusive domain of human translators now"}, {"title": "2 Related work", "content": "Human evaluation for literary texts Literary MT evaluation primarily relies on human assessment. Error-span-based MQM, widely used in the renowned MT venue WMT , has proven reliable for non-literary texts but its suitability for literary translation is uncertain. BWS is employed for literary texts, including poetry , but has limitations in comparing multiple systems simultaneously . SQM, a Likert scale rating, is used in non-literary domains. To our best knowledge, a comprehensive comparison of these schemes for literary MT is lacking, with different studies employing various methods ad libitum. Our work provides the first systematic comparison of these evaluation methods for literary texts.\nHuman evaluation of NLP models has been the subject of much recent debate . Human evaluation is typi-"}, {"title": "3 LITEVAL-CORPUS", "content": "We detail our LITEVAL-CORPUS in this section. We first discuss the data construction process and then statistics."}, {"title": "3.1 Dataset construction", "content": "Our data construction process consists of three steps: (1) preselecting and constructing paragraph-level parallel corpus; (2) selecting MT systems and collecting translation hypotheses; (3) assessing translation quality.\nConstructing parallel corpora We consider three languages: English, Chinese, and German, which encompass translations between four different language pairs: English-Chinese (En-Zh), German-English (De-En)/English-German (En-De), and German-Chinese (De-Zh). We begin by collecting paragraph-level parallel corpora for each language pair, including both classic and contemporary works. Classics often present more challenges due to their complex syntax, language change, and cultural references that may no longer be familiar to modern audiences. Notably, older translations are especially valuable, as they were created before the widespread use of MT and are thus free from concerns of being post-edited MT outputs. Additionally, including contemporary works helps mitigate potential data contamination in LLMs, as these models are more likely to have been pre-trained on widely available texts, including classic literature.\nTo ensure the quality of our dataset, we derive both source and target paragraphs from verified publications, including books from Project Gutenberg , reading samples from published works, and purchased materials. For classic works, the target paragraphs include at least two human translations, while contemporary works have at least one. For classic De-En works, we utilize PAR3 samples . For contemporary De-En works and the other three language pairs (De-Zh, En-Zh, and En-De), we build the corpus entirely from scratch."}, {"title": "Selecting MT systems and collecting translation hypotheses", "content": "We explore various-sized models, including cost-efficient alternatives to larger models:\n\u2022 Commercial systems like Google Translate and DeepL. DeepL is particularly noteworthy for translators due to its widespread use within the translation community.\n\u2022 Previous state-of-the-art (SOTA) transformer models optimized for sentence-level translation, including NLLB-3.3b and M2M_100-1.3b .\n\u2022 Closed-source GPT-40 and four series of open-source LLMs of smaller sizes: (1) Llama 3 , one of the strongest open-source models; (2) Qwen 2 , specialized in Chinese; (3) Gemma 1.1, developed with the same technology as the closed-source Gemini model ; and (4) TowerInstruct , trained for translation-related tasks. We use identical prompts across all LLMs when requesting literary translations: \"Please translate the following literary texts from [source language] to [target language]. The texts are as follows: [texts]\u201d."}, {"title": "3.2 Assessing translation quality", "content": "Our study examines three annotation schemes, i.e., MQM, SQM, and BWS, using both student annotators and professional translators. For MQM (Error annotation), evaluators read the source/candidate translation pair, identify error spans, and categorize them based on the annotation guideline with six major categories: (1) Terminology; (2) Accuracy;(3) Fluency; (4) Style; (5) Locale-convention; and (6) Non-translation. Each error is labeled as major or minor. The MQM score is thus computed with the following formula:\n$(\\frac{C_{Non-translation} \\times 25 + C_{major} \\times 5 + C_{minor} \\times 1}{number \\ of \\ sentences \\ per \\ paragraph})$\nC represents the count of errors. In contrast, MQM (Free annotation) does not follow any guidelines. The evaluators highlight spans indicating errors or good translations and then comment on the reasons"}, {"title": "3.3 LITEVAL-CORPUS statistics", "content": "Table 1 summarizes the statistics of LITEVAL-CORPUS. The dataset contains 227 paragraphs, encompassing 2,184 translation segments at paragraph level including over 13k sentences. On average, source paragraphs contain 5.7 sentences, while target paragraphs have 6.1 sentences. To our knowledge, LITEVAL-CORPUS is the most extensive dataset for literary translation evaluation, offering comprehensive information on both source and target translations. It uniquely features paragraph-level translations across 10 generation systems."}, {"title": "4 Human evaluation", "content": "We use LITEVAL-CORPUS to examine the consistency and adequacy of MQM, SQM, and BWS."}, {"title": "4.1 Consistency", "content": "Student vs. student Table 2 (a) shows the annotation agreement between pairs of student annotators across three language pairs: De-En, En-De, and En-Zh. We assess MQM and SQM using Kendall's Tau (Puka, 2011). For annotated error spans, we employ Cohen's kappa (McHugh, 2012), where span match indicates agreement on error spans regardless of labels, and label match considers agreement on both spans and labels. We calculate the accuracy for BWS agreement.\nThe results show moderate to high agreement for both MQM and SQM. MQM agreement ranges from 0.434 for En-De to 0.582 for En-Zh, while SQM scores the highest agreement for En-Zh at 0.656 and the lowest for En-De at 0.350. On average, MQM yields an agreement of 0.493, similar to SQM of 0.487.\nIn terms of error span agreement, En-Zh achieves the highest span match at 0.452, while En-De shows the highest label match at 0.283. Although error span agreement is generally lower than MQM and SQM, the results remain competitive with previous sentence-level multi-domain findings .\nHowever, a direct comparison of agreement on paragraph-level annotations in literary domain has yet to be explored, underscoring the value of this study as a baseline for future research in literary"}, {"title": "Professional translators vs. student annotators", "content": "Table 2 (b) compares the annotation agreement between student annotators and professional translators (S-P). For comparability, we also report the agreement between student annotators (S-S) computed with the same segments. For SQM, on average, S-P agreement is slightly higher than S-S agreement. For En-Zh, the agreement is 0.359 (S-S) versus 0.517 (S-P). One exception is the De-En pair, where S-S agreement is 0.528, while S-P agreement drops to 0.363. The agreement for En-De is low for both groups. For error span match, the agreements between S-S and S-P both decline. Overall, S-S shows better agreement than S-P. This is understandable as student annotators are presented with an annotation guideline, while professional translators perform free annotation without guidelines. The results indicate that both student annotators and professionals focus on joint error spans to some degree. While there is consistency between the two groups, particularly in certain language pairs, discrepancies also exist. We discuss these in detail in Section 4.2."}, {"title": "Consistency in system ranking", "content": "The annotation results in also indicate strong consistency in system rankings between student and professional annotators for both MQM and SQM. Human translation consistently ranks first in both scores when averaged across all segments in the four languages studied. This top ranking holds for both student and professional annotators, emphasizing human translation's superiority. GPT-40 ranks second, followed by Google Translate and DeepL or Qwen. Smaller models such as M2M and NLLB consistently rank the lowest. Additionally, according to student annotators, there is a clear performance gap between the top 4 systems and the rest, i.e., 5.5 points drop from the fourth to fifth model in MQM and 1.1 points by SQM. Notably, professional translators rate human translations substantially higher than the second-place"}, {"title": "4.2 Adequacy", "content": "We assess the adequacy of translation evaluation schemes by evaluating their ability to differentiate high-quality human translations from machine outputs. Our analysis assumes experienced human translators outperform machine systems, based on recent studies and the inherent advantages of professional translators: access to full-text context, professional education, and ability to research cultural nuances. These factors enable translators to develop a deeper understanding of the source material, intuitively resulting in more accurate and creative translations.\nTable 3 shows the percentage of segments where human translations are preferred over other systems. The table presents two scenarios: (1) human translations compared to top-performing models like GPT-40, DeepL, Google Translate, and Qwen, and (2) human translations compared to other MT systems, excluding these top-performing models. While student MQM demonstrates good performance in scenario (2), the bad performance in scenario (1) indicates that MQM alone may not be adequate for evaluating translation quality effectively, especially for top systems. This is further evidenced by the error distribution shown in which illustrates that the challenge lies in evaluating cases involving top models, as their error distribution closely resembles that of human translation. For instance, 16.6% of GPT-40 outputs are \u201cerror-free\u201d but may still fall short in wording or overall style compared to human translations. Additionally, our professional translators express concerns about the \u201cerror-counting\u201d schemes, particularly when assessing high-quality literary translations, as they sometimes make intentional choices, such as omitting certain elements or slightly altering meanings, to enhance the expression in the target language.\nSQM performs similarly with students but substantially better with professionals, who demonstrate a high preference for human translation (100% in De-En and De-Zh). However, student annotators show more mixed results, indicating that SQM's effectiveness heavily depends on the annotator's experience and expertise. Professional SQM outperforms student SQM by a large margin, as students\u2014lacking sufficient competence"}, {"title": "Summary:", "content": "Student annotators and professional translators can consistently evaluate literary translations using MQM and SQM at a moderate level with BWS showing slightly better agreement. Student MQM is not adequate to distinguish human translations from top LLM translations. Using it may yield false conclusions regarding the quality of LLMs for literary translation. The effectiveness of SQM heavily depends on the expertise of the evaluator. BWS excels at distinguishing human from machine translations even among less-experienced annotators for top-performing systems, though this scheme offers less detailed feedback for further improvements such as post-editing."}, {"title": "5 Automatic metrics", "content": "We use LITEVAL-CORPUS to assess recent automatic metrics. While human MQM and SQM may not fully capture the nuances of literary translation quality, as discussed in the previous section, it is still valuable to examine how consistently current metrics align with human MQM/SQM evaluations. This is particularly relevant given that the recent top-performing metrics are built on similar principles. We focus on reference-free metrics due to the scarcity of reference translations for the majority of literary works and evaluate metric performance by computing segment-level correlation between automatic metrics and human annotation.\nWe examine four SOTA metrics that rely on LLMs: Prometheus 2 (7 billion parameters) , an open-source model trained to evaluate other language models, which is queried using a tailored prompt (shown in in the appendix) for SQM evaluation of literary translations; XCOMET-XL/XCOMET-XXL (3.5 and 10 billion parameters) , one of the strongest open-source MT metrics for standard MT, which is fine-tuned to assess quality, generating scores and error spans with severity labels; GEMBA-MQM , a top-performing prompting-based metric, which is designed to detect translation quality errors based on the MQM framework using LLMs. We modify GEMBA-MQM with the same error categories we use for student annotators. This allows us to view it as a potential substitute for a human annotator. We implement GEMBA-MQM (Original) utilizing the original prompt template and GEMBA-MQM (Literary), which is adapted with domain knowledge and literary translation examples shown in ."}, {"title": "The best metric (GEMBA-MQM) correlates moderately with human MQM but it cannot distinguish human translation from LLM output", "content": "Figure 2 (a) illustrates the segment-level correlation between human MQM and evaluation metrics. GEMBA-MQM, particularly its Original version, consistently outperforms other metrics. GEMBA-MQM (Literary) performs similarly to GEMBA-MQM (Original), though slightly worse across the board. XCOMET-XL and XCOMET-XXL models rank second overall, demonstrating poor to moderate correlation with human annotations. Their performance notably declines for more distant language pairs, such as De-Zh, possibly due to differences or limitations in their training data for this language direction. Moreover, we notice that XCOMET-XL and XCOMET-XXL are negatively correlated with the length of the target translation at -0.420 and -0.393, respectively. In contrast, the correlations between the length of the target and other metrics are all below 0.05 in absolute value. Additionally, Prometheus performs the worst, assigning a score of 4 (equivalent to \u201cgood\u201d translation) to 54.9% of segments. Furthermore, its correlation to human SQM also ranks lowest among all metrics, as shown in \nEven the best metric is not adequate for literary MT evaluation. GEMBA-MQM, despite moderate correlation with human SQM and MQM, faces adequacy issues in literary translation evaluation. shows the percentage of segments where GEMBA-MQM versions prefer human translations over MT. The results show GEMBA-MQM severely struggles to distinguish human translations from top LLM systems. For scenario (1), human translator vs. top systems, GEMBA-MQM (Literary)'s highest percentage (9.6%) is still 32.9 points lower than the worst human per-"}, {"title": "Additionally, our analysis reveals that GEMBA- MQM's performance is primarily driven by Accu- racy aspects, with less reliability in evaluating Flu- ency, Style, and Terminology", "content": "Accuracy shows the strongest correlations across all language pairs and Accuracy for De-En and De- Zh even surpasses the full correlation. Meanwhile, Fluency and Style demonstrate poor to fair corre- lations. Notably, Terminology consistently shows near-zero correlations, which could be problem- atic for distant language pairs with more culture- specific terms, e.g., En-Zh with Terminology as the second most frequent error category. These findings suggest that future metric development should focus on improving the assessment of Flu- ency, Style, and Terminology.\nHuman MQM vs. GEMBA-MQM: LLMs demonstrate a tendency towards more literal and less diverse translations. We further compare human MQM with GEMBA-MQM of system rankings, considering the characteristics of the translation systems. To assess a translation's literalness, we analyze the syntactic similarity be- tween the translated texts from each system and"}, {"title": "Chinese characters in De-En and En-De outputs, which may result in low syntactic similarity but still higher than human.", "content": "Summary: GEMBA-MQM outperforms other current automatic metrics, particularly for distant language pairs. However, all metrics lack adequacy in distinguishing human translations from LLM outputs and GEMBA-MQM struggles with non- Accuracy aspects. Our findings also reveal that LLMs tend to produce and favor more literal and less diverse translations. LLM-generated outputs show higher syntactic similarity to the source text and greater lexical overlap with other systems compared to human translations. Furthermore, LLMs as evaluators tend to prefer more literal translations over human translations and exhibit a bias towards their own outputs."}, {"title": "6 Conclusion", "content": "This paper introduces LITEVAL-CORPUS, offering a comprehensive study on literary translation evaluation that encompasses human and automatic metrics evaluations.\nOur annotated corpus reveals the limitations of MQM, particularly when comparing top- performing systems. This may be attributed to two factors: first, errors in literary translation might be intentional to achieve certain equivalence in the target language, which the current MQM framework fails to account for; second, today's leading systems produce fewer errors than previous MT systems, rendering MQM insufficient. Our findings suggest that using BWS or experienced profession-"}, {"title": "als yields more plausible assessments", "content": "However, BWS is limited by its lack of utility for further improvements, such as post-editing, compared to MQM. These results have significant implications, particularly for determining human parity\u2014a crucial issue in today's LLM landscape. They are especially relevant for professional translators, as naive translation companies might eliminate human translators due to misjudgments based on MQM or SQM evaluations by inexperienced practitioners. While we highlight MQM's limitations in literary MT evaluation, our current study does not propose specific improvements to the existing MQM guidelines. Future work can explore these aspects in depth.\nFurthermore, our results indicate that automatic metrics development should focus on aspects such as Style and Terminology. LLMs still need to address the issue of overly literal outputs, and a substantial gap remains between LLM and human quality in literary translation, despite the clear advancements of recent models."}, {"title": "Limitations", "content": "Several limitations of our current work warrant consideration:\nFirstly, our study primarily focuses on prompting-based techniques with identical basic prompts across LLMs. While our initial exploration with GPT-40 showed no substantial improvement with complex prompts, we acknowledge that LLM-specific prompt optimization could potentially yield better results in literary translation. This aligns with the current understanding that prompting is a delicate process .\nAdditionally, other techniques such as fine- tuning with domain knowledge may further enhance LLM performance in this context. Conversely, data contamination may inadvertently improve translation quality for classic works already in the model's training data. However, classics unseen by the model or yet to be translated might exhibit lower translation quality.\nSecondly, our focus on paragraph-level translation still does not fully capture the nuances of literary texts. The absence of annotation on (consecutive) chapters limits our ability to comprehensively evaluate translation quality within a broader context. Although we conducted limited experiments with multi-paragraph samples, resource constraints"}, {"title": "Ethical Considerations", "content": "LLMs in literary translation have sparked widespread public interest and concern. This topic has become a focal point for researchers, translators, and publishers, generating intense debate at various forums including workshops, interviews, and book fairs. The heightened engagement underscores the need for responsible handling of MT outputs in general.\nLow-quality translations can misrepresent or negatively affect an author's original work without their awareness. LLMs may introduce or exacerbate issues of bias and toxicity in translated literature, impacting the audience, particularly children. The growing capabilities of LLMs in translation also threaten professional translators, raising concerns about job displacement and the devaluation of human expertise due to misconceptions about"}]}