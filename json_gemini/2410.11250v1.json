{"title": "Learning Agents With Prioritization and\nParameter Noise in Continuous State and\nAction Space", "authors": ["Rajesh Mangannavar", "Gopalakrishnan Srinivasaraghavan"], "abstract": "Among the many variants of RL, an important class of prob-\nlems is where the state and action spaces are continuous autonomous\nrobots, autonomous vehicles, optimal control are all examples of such\nproblems that can lend themselves naturally to reinforcement based al-\ngorithms, and have continuous state and action spaces. In this paper,\nwe introduce a prioritized form of a combination of state-of-the-art ap-\nproaches such as Deep Q-learning (DQN) and Deep Deterministic Policy\nGradient (DDPG) to outperform the earlier results for continuous state\nand action space problems. Our experiments also involve the use of pa-\nrameter noise during training resulting in more robust deep RL models\noutperforming the earlier results significantly. We believe these results\nare a valuable addition for continuous state and action space problems.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is about an agent learning an optimal way to con-\ntrol and/or navigate through an environment that requires sequential decision\nmaking by the agent. The agent does this by trying to maximize a numerical\nperformance measure that expresses a long-term objective, by trial-and-error.\nRL arises naturally in a wide range of domains including autonomous control,\ngaming, natural language processing & dialogue management, etc. [1]."}, {"title": "RL in Continuous State and Action Space", "content": "Many interesting real-world control tasks, such as driving a car or riding a snow-\nboard, require smooth continuous actions taken in response to high-dimensional,\nreal-valued sensory inputs. In applying RL to continuous problems, the predom-\ninant approach in the past involved discretizing the state and action spaces and\nthen applying an RL algorithm for a discrete stochastic system [2]. However\nthe drawbacks of discretization are, they do not scale, do not allow fine-grained"}, {"title": "Deep Reinforcement Learning", "content": "The advent of deep learning has had a significant impact in many areas in ma-\nchine learning, dramatically improving the state-of-the-art tasks such as object\ndetection, speech recognition and language translation [3]. Deep Neural Networks\nare very powerful function approximators and can be trained to automatically\nfind low-dimensional representations of high-dimensional data. This enables deep\nlearning to scale to problems which were previously intractable including rein-\nforcement learning problems with high dimensional, continuous state and action\nspaces [4]."}, {"title": null, "content": "Few of the current state of the art methods in the area of Deep RL are:\nDeep Q-learning Networks (DQN) - introduced novel concepts which helped\nin using neural networks as function approximators for reinforcement learn-\ning algorithms (for continuous state space) [5].\nPrioritized Experience Replay (PER) - builds upon DQN with some newer\napproaches to outperform DQN (for continuous state space) [6].\nDeep Deterministic Policy Gradients (DDPG) - follows a different paradigm\nas compared to the above methods. It uses DQN as the function approxima-\ntor while building on the seminal work of [7] (for both continuous state and\naction space) on deterministic policy gradients."}, {"title": "Prioritized Experience Replay in DDPG ( PDDPG)", "content": "We propose a new algorithm, Prioritized DDPG using the ideas proposed in\nDQN, prioritized experience replay and DDPG such that it outperforms the\nDDPG in the continuous state and action space. Prioritized DDPG uses the\nconcept of prioritized sampling in the function approximator of DDPG. Our\nresults show that prioritized DDPG outperforms DDPG in a majority of the\ncontinuous action space environments. We then use the concept of parameter\nspace noise for exploration and show that this further improves the rewards\nachieved."}, {"title": "Previous work", "content": null}, {"title": "Critic Methods of RL", "content": "Critic methods rely exclusively on a value or Q-function approximation and aim\nat learning a \"good\" approximation of the value/Q-function[8]. We survey a few\nof the recent best-known critic methods in RL."}, {"title": "Deep Q-learning Networks", "content": "As in any value function-based approach, DQN\nmethod tries to find the optimal value function for any given state / state-\naction-pair. The novelty of their approach is that they efficiently use a non-\nlinear function approximator to learn the function. Prior to their work, though\nthe potential for using non-linear function approximators was recognized, there\nwas no tangible demonstration of their use in practice in an efficient manner.\nThe challenge in using function approximators was that the Monte-Carlo\nsampling typically used for collecting experience in the form of state-action-\nrewards by generating episodes did not guarantee i.i.d data that most machine\nlearning algorithms assume for training the models from. To overcome this draw-\nback, the novel ideas that were proposed which made it possible to use non-linear\nfunction approximators for reinforcement learning are the following:\nExperience Replay Buffer: In this technique, the neural network is trained\nfrom random samples from a large buffer of stored observations.\nPeriodic Target Network Updates: Two sets of parameters are maintained\nfor the same neural network one for generating behaviour from (possi-\nbly using an e-greedy strategy) and the other (the target network) for the\nvalue function estimation. The target network parameters are used in the\nloss computation and are updated by the behaviour network parameters\nperiodically."}, {"title": "Prioritized Experience Replay", "content": "The prioritized experience replay algorithm\nis a further improvement on the deep Q-learning methods and can be applied\nto both DQN and Double DQN. The idea proposed by the authors is as follows:\ninstead of selecting the observations at random from the replay buffer, they can\nbe chosen based on some criteria which will help in making the learning faster.\nIntuitively the selection criterion from the replay buffer is biased towards the\n'more useful observations' and less on the 'stale' ones. To select these more useful\nobservations, the criteria they use is the error of that particular observation.\nThis criterion helps select those observations which provide the highest learn-\ning opportunity for the agent. The problem with this approach is that greedy\nprioritization focuses on a small subset of the experience and this lack of diver-\nsity may lead to over-fitting. Hence, the authors introduce a stochastic sampling\nmethod that interpolates between pure greedy and uniform random prioritiza-\ntion. Hence, the new probability of sampling a transition i is\n$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$"}, {"title": null, "content": "where $p_i$ is the priority of transition i and $\\alpha$ determines how much priori-\ntization is used. The approach, while it improves the results has a problem of\nchanging the distribution of the expectation. This is resolved by the authors by\nusing Importance Sampling (IS) weights\n$W_i = (\\frac{1}{NP(i)})^\\beta$"}, {"title": "Actor Methods in RL", "content": "Actor methods work with a parameterized family of policies. The gradient of\nthe performance, with respect to the actor parameters, is directly estimated by\nsimulation, and the parameters are updated in the direction of improvement\n([8])."}, {"title": "Deterministic Policy Gradients (DPG)", "content": "The most popular policy gradient\nmethod is the deterministic policy gradient(DPG) method and in this approach,\ninstead of having a stochastic policy, the authors make the policy deterministic\nand then determine the policy gradient.\nThe deterministic policy gradient is the expected gradient of the action-value\nfunction, which integrates over the state space, whereas in the stochastic case,\nthe policy gradient integrates over both state and action spaces. What this leads\nto is that the deterministic policy gradient can be estimated more efficiently\nthan the stochastic policy gradient.\nThe DPG algorithm, presented by [7] maintains a parameterized actor func-\ntion \u03bc(s|04) which is the current deterministic policy that maps a state to a\nparticular action. The authors use the Bellman equation to update the critic\nQ(s, a). They then go on to prove that the derivative expected return with re-\nspect to actor parameters is the gradient of the policy's performance."}, {"title": "Actor-Critic Methods", "content": "Actor-critic models (ACM) are a class of RL models that separate the policy\nfrom the value approximation process by parameterizing the policy separately.\nThe parameterization of the value function is called the critic and the param-\neterization of the policy is called the actor. The actor is updated based on the\ncritic which can be done in many ways, while the critic is update based on the\ncurrent policy provided by the actor ([8] [9])."}, {"title": "Deep Deterministic Policy Gradients (DDPG)", "content": "The DDPG algorithm\ntries to solve the reinforcement learning problem in continuous action and state\nspace setting. The authors of this approach extend the idea of deterministic\npolicy gradients. What they add to the DPG approach is the use of a non-linear\nfunction approximator ([10]).\nWhile using a deterministic policy, the action value function reduces from\n$Q^\u03c0 (s_t, a_t) = E_{r_t,s_{t+1}\u223cE} [r(s_t, a_t) + \u03b3E_{a_{t+1}\u223c\u03c0} [Q^\u03c0 (s_{t+1}, a_{t+1})]]$"}, {"title": null, "content": "$Q^\u03bc (s_t, a_t) = E_{r_t,s_{t+1}\u223cE} [r(s_t, a_t) + \u03b3Q^\u03bc (s_{t+1}, \u03bc(s_{t+1}))]$ as the inner expectation is no longer required. Here, \u03b3\u2208 [0,1] is the discounting\nfactor. What this also tells us is that the expectation depends only on the envi-\nronment and nothing else. Hence, we can learn off-policy, that is, we can train\nour reinforcement learning agent by using the observations made by some other\nagent. The authors use this as well as the novel concepts used in DQN to con-\nstruct their function approximator. These concepts cannot be applied directly\nto continuous action space, as there is an optimization over the action space at\nevery step which is in-feasible when there is a continuous action space [10].\nOnce we have both the actor and the critic networks with their respective\ngradients, we can then use the DQN concepts - replay buffer and target networks\nto train these two networks. They apply the replay buffer directly without any\nmodifications but make small changes in the way target networks are used. In-\nstead of directly copying the values from the temporary network to the target\nnetwork, they use soft updates to the target networks."}, {"title": "Parameter Space Noise for Exploration", "content": "There is no best exploration strat-\negy in RL. For some algorithms, random exploration works better and for some\ngreedy exploration. But whichever strategy is used, the important requirement\nis that the agent has explored enough about the environment and learns the best\npolicy. Plappert et. al. [11] in their paper explore the idea of adding noise to the\nagent's parameters instead of adding noise in the action space. In their paper\nParameter Space Noise For Exploration, they explore and compare the effects\nof four different kinds of noises\nUncorrelated additive action space noise\nCorrelated additive Gaussian action space noise\nAdaptive-param space noise\nNo noise\nThey show that adding parameter noise vastly outperforms existing algo-\nrithms or at least is just as good on a majority of the environments for DDPG\nas well as other popular algorithms such as Trust Region Policy Optimization\n([12])."}, {"title": "Prioritized Deep Deterministic Policy Gradients", "content": "The proposed algorithm is primarily an adaptation of DQN and DDPG with\nideas from the work of Schaul et. al. [6] on continuous control with deep re-\ninforcement learning to design an RL scheme that improves on DDPG signifi-\ncantly. The intuition behind the idea is as follows: The DDPG algorithm uses the\nDQN method as a sub-algorithm and any improvement over the DQN algorithm\nshould ideally result in the improvement of the DDPG algorithm. But from the\nabove-described methods, not all algorithms which improve DQN can be used"}, {"title": "Prioritized DDPG Algorithm", "content": "Now, the improvement to the DQN algorithm, the prioritized action replay\nmethod can be integrated into the DDPG algorithm in a very simple way. In-\nstead of using just DQN as the function approximator, we can use DQN with\nprioritized action replay. That is, in the DDPG algorithm, instead of selecting\nobservations randomly, we select the observations using the stochastic sampling\nmethod as defined in equation 1. The pseudo-code for the prioritized action re-\nplay is given in Algorithm 3.1. The algorithm runs in a for loop M times where\nM is the number of episodes we want to train the agent for.\nThis algorithm is quite similar to the original DDPG algorithm with the only\nchanges being the way the observations are selected for training and the transi-\ntion probabilities are being updated. The first change ensures we are selecting\nthe better set of observations which help in learning faster and the second change\nhelps in avoiding over-fitting as it ensures all the observations have a non-zero\nprobability of being selected to train the network and only a few high error\nobservations are not used multiple times to train the network."}, {"title": "PDDPG With Parameter Noise", "content": "As introducing parameter noise improved the results obtained by DDPG, we in-\ntroduce the parameter noise with PDDPG in the same way. The noise is added\nsuch we can achieve structured exploration by applying the noises to the param-\neter of the current policy. Also, the policy on which we have applied our noise\nis sampled at the beginning of each episode.\nThe noise we add to the parameter are the ones discussed before -\nUncorrelated additive action space noise\nCorrelated additive Gaussian action space noise\nAdaptive-param space noise\nWe also have a result without adding any noise (original PDDPG) for com-\nparison."}, {"title": "Results", "content": "The proposed, prioritized DDPG algorithm was tested on many of the standard\nRL simulation environments that have been used in the past for benchmarking\nthe earlier algorithms. The environments are available as part of the Mujoco\nplatform ([13])."}, {"title": "Mujoco Platform", "content": "Mujoco is a physics environment which was created to facilitate research and\ndevelopment in robotics and similar areas, where fast simulation is an important\ncomponent.\nThis set of environments provide a varied set of challenges for the agent as\nenvironments have continuous action as well as state space. All the environments\ncontain stick figures with some joints trying to perform some basic task by\nperforming actions like moving a joint in a particular direction or applying some\nforce using one of the joints."}, {"title": "Empirical Evaluation", "content": "The implementation used for making the comparison was the implementation\nof DDPG in baselines ([14]). The prioritized DDPG algorithm was implemented\nby extending the existing code in baselines."}, {"title": "Results for PDDPG", "content": "The following are the results of the prioritized DDPG\nagent as compared DDPG agent ([10]). The overall reward - that is the average\nof the reward across all epochs until that point and reward history - the average\nof the last 100 epochs on four environments are plotted. The y-axis represents\nthe reward the agent has received from the environment and the x-axis is the\nnumber of epochs with each epoch corresponding to 2000 time steps."}, {"title": null, "content": "As seen in Figure 1, the Prioritized DDPG algorithm reaches the reward of\nthe DDPG algorithm in less than 300 epochs for the HalfCheetah environment.\nThis shows that the prioritized DDPG algorithm is much faster in learning.\nThe same trend can be observed in Figure 1 for HumanoidStandup, Hopper\nand Ant environments. That is, the prioritized DDPG agent learns and gets the\nsame reward as DDPG much faster. This helps is in reducing overall training\ntime. Prioritized DDPG algorithm can also help in achieving results which might\nnot be achieved by DDPG even after a large number of epochs. This can be seen\nin the case of the Ant environment. Figure 1 shows that DDPG rewards are\nactually declining with more training. On the other hand, Prioritized DDPG\nhas already achieved a reward much higher and is more stable."}, {"title": null, "content": "One a few environments such as the Reacher, InvertedDoublePendulum and\nWalker2d, it can be seen from Figure 1 the prioritized DDPG only, marginally\noutperforms the DDPG algorithm."}, {"title": "Results for PDDPG with parameter noise", "content": "The PDDPG algorithm with\nparameter noise was run on the same set of environments as the PDDPG algo-\nrithm - the Mujoco environments. The empirical results are as follows."}, {"title": null, "content": "We can see from figure 2 that noise improves the overall reward obtained\nonly some of the environment. This is because the idea behind adding noise\nis for better exploration and PDDPG explores and learns much faster as seen\nin 1. In environments such as Inverted Pendulum or inverted double pendulum\nwhere lesser exploration is required, the addition of noise does not improve the\nrewards achieved, but in environments such as Humanoid-Stand up or Walker-\n2d, where a lot of exploration is required, the noise does improve the overall\nreward achieved.\nOverall, with the proposed changes, prioritization with the addition of noise,\nthe proposed algorithm outperforms DDPG on eight of the ten environments as\nseen in Figure 3 and does reasonably well in the others."}, {"title": "Conclusions", "content": "To summarize, this paper discusses the state of the art methods in reinforcement\nlearning with our improvements that have led to RL algorithms in continuous\nstate and action spaces that outperform the existing ones."}]}