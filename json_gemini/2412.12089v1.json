{"title": "STABILIZING REINFORCEMENT LEARNING IN DIFFERENTIABLE MULTIPHYSICS SIMULATION", "authors": ["Eliot Xing", "Vernon Luk", "Jean Oh"], "abstract": "Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables.", "sections": [{"title": "INTRODUCTION", "content": "Progress in deep reinforcement learning (RL) has produced policies capable of impressive behavior, from playing games with superhuman performance (Silver et al., 2016; Vinyals et al., 2019) to controlling robots for assembly (Tang et al., 2023), dexterous manipulation (Andrychowicz et al., 2020; Akkaya et al., 2019), navigation (Wijmans et al., 2020; Kaufmann et al., 2023), and locomotion (Rudin et al., 2021; Radosavovic et al., 2024). However, standard model-free RL algorithms are extremely sample inefficient. Thus, the main practical bottleneck when using RL is the cost of acquiring large amounts of training data.\nTo scale data collection for online RL, prior works developed distributed RL frameworks (Nair et al., 2015; Horgan et al., 2018; Espeholt et al., 2018) that run many processes across a large compute cluster, which is inaccessible to most researchers and practitioners. More recently, GPU-based parallel environments (Dalton et al., 2020; Freeman et al., 2021; Liang et al., 2018; Makoviychuk et al., 2021; Mittal et al., 2023; Gu et al., 2023) have enabled training RL at scale on a single consumer GPU.\nHowever, such successes of scaling RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics (Makoviychuk et al., 2021), while physics-based simulation techniques for soft bodies are comparatively several orders of magnitude slower. Consequently for tasks involving deformable objects, such as robotic manipulation of rope (Nair et al., 2017; Chi et al., 2022), cloth (Ha & Song, 2022; Lin et al., 2022), elastics (Shen et al., 2022), liquids (Ichnowski et al., 2022; Zhou et al., 2023), dough (Shi et al., 2022; 2023; Lin et al., 2023), or granular piles (Wang et al., 2023; Xue et al., 2023), approaches based on motion planning, trajectory optimization, or model predictive control have been preferred over and outperform RL (Huang et al., 2020; Chen et al., 2022).\nHow can we overcome this data bottleneck to scaling RL on tasks involving deformables? Model-based reinforcement learning (MBRL) has shown promise at reducing sample complexity, by leveraging some known model or learning a world model to predict environment dynamics and rewards (Moerland et al., 2023). In contrast to rigid bodies however, soft bodies have more complex dynamics"}, {"title": "RELATED WORK", "content": "We refer the reader to (Newbury et al., 2024) for an overview of differentiable simulation. We cover non-parallel differentiable simulation and model-based RL in Appendix A."}, {"title": "SOFT ANALYTIC POLICY OPTIMIZATION (SAPO)", "content": "Empirically we observe that SHAC, a state-of-the-art FO-MBRL algorithm, is still prone to suboptimal convergence to local minima in the reward landscape (Appendix, Figure 5). We hypothesize that entropy regularization can stabilize policy optimization over analytic gradients from differentiable simulation, such as by smoothing the optimization landscape (Ahmed et al., 2019) or providing robustness under perturbations (Eysenbach & Levine, 2022).\nWe draw on the maximum entropy RL framework (Kappen, 2005; Todorov, 2006; Ziebart et al., 2008; Toussaint, 2009; Theodorou et al., 2010; Haarnoja et al., 2017) to formulate Soft Analytic Policy Optimization (SAPO), a maximum entropy FO-MBRL algorithm (Section 4.1). To implement SAPO, we make several design choices, including modifications building on SHAC (Section 4.2). In Appendix B.1, we describe how we use visual encoders to learn policies from high-dimensional visual observations in differentiable simulation. Pseudocode for SAPO is shown in Appendix B.2, and the computational graph of SAPO is illustrated in Appendix Figure 4."}, {"title": "MAXIMUM ENTROPY RL IN DIFFERENTIABLE SIMULATION", "content": "Maximum entropy RL (Ziebart et al., 2008; Ziebart, 2010) augments the standard (undiscounted) return maximization objective with the expected entropy of the policy over \u03c1\u03c0(st):\n$J(\u03c0) = \\mathbb{E}_{(s_t,a_t)\\sim \\rho_{\\pi}}[\\sum_{t=0}^{\\infty} r_t +\\alpha\\mathcal{H}_{\\pi}[a_t|s_t]]$,\nwhere $\\mathcal{H}[a_t|s_t] = - \\int_{\\mathcal{A}} \\pi(a_t|s_t) \\log \\pi(a_t|s_t) da_t$ is the continuous Shannon entropy of the action distribution, and the temperature \u03b1 balances the entropy term versus the reward."}, {"title": "DESIGN CHOICES", "content": "I. Entropy adjustment. In practice, we apply automatic temperature tuning (Haarnoja et al., 2018b) to match a target entropy $\\mathcal{H}$ via an additional Lagrange dual optimization step :\n$\\min_{\\alpha_t\\ge 0} \\mathbb{E}_{(s_t,a_t)\\sim \\rho_{\\pi}} [\\alpha_t(\\mathcal{H}_{\\pi}[a_t|s_t] - \\mathcal{H})]$.\nWe use $\\mathcal{H} = -dim(\\mathcal{A})/2$ following (Ball et al., 2023).\nII. Target entropy normalization. To mitigate non-stationarity in target values (Yu et al., 2022) and improve robustness across tasks with varying reward scales and action dimensions, we normalize entropy estimates. The continuous Shannon entropy is not scale invariant (Marsh, 2013). In particular, we offset (Han & Sung, 2021) and scale entropy by $\\mathcal{H}$ to be approximately contained within [0, +1].\nIII. Stochastic policy parameterization. We use state-dependent variance, with squashed Normal distribution $\u03c0_\u03b8 = tanh(N(\u03bc_\u03b8(s), \u03a3_\u03b8(s)))$, which aligns with SAC (Haarnoja et al., 2018b). This enables policy entropy adjustment and captures aleatoric uncertainty in the environment (Kendall & Gal, 2017; Chua et al., 2018). In contrast, SHAC uses state-independent variance, similar to the original PPO implementation (Schulman et al., 2017).\nIV. Critic ensemble, no target networks. We use the clipped double critic trick (Fujimoto et al., 2018) and also remove the critic target network in SHAC, similar to (Georgiev et al., 2024). However when updating the actor, we instead compute the average over the two value estimates to include in the return (Eq. 19), while using the minimum to estimate target values in standard fashion, following (Ball et al., 2023). While originally intended to mitigate overestimation bias in Q-learning (due to function approximation and stochastic optimization (Thrun & Schwartz, 2014)), prior work has shown that the value lower bound obtained by clipping can be overly conservative and cause the policy to pessimistically underexplore (Ciosek et al., 2019; Moskovitz et al., 2021).\nTarget networks (Mnih et al., 2015) are widely used (Lillicrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018b) to stabilize temporal difference (TD) learning, at the cost of slower training. Efforts have been made to eliminate target networks (Kim et al., 2019; Yang et al., 2021; Shao et al., 2022; Gallici et al., 2024), and recently CrossQ (Bhatt et al., 2024) has shown that careful use of normalization layers can stabilize off-policy model-free RL to enable removing target networks for improved sample efficiency. CrossQ also reduces Adam \u03b2\u2081 momentum from 0.9 to 0.5, while keeping the default \u03b22 = 0.999. In comparison, SHAC uses \u03b2\u2081 = 0.7 and \u03b22 = 0.95. Using smaller momentum parameters decreases exponential decay (for the moving average estimates of the 1st and 2nd moments of the gradient) and effectively gives higher weight to more recent gradients, with less smoothing by past gradient history (Kingma & Ba, 2015).\nV. Architecture and optimization. We use SiLU (Elfwing et al., 2018) instead of ELU for the activation function. We also switch the optimizer from Adam to AdamW (Loshchilov & Hutter, 2017), and lower gradient norm clipping from 1.0 to 0.5. Note that SHAC already uses LayerNorm (Ba et al., 2016), which has been shown to stabilize TD learning when not using target networks or replay buffers (Bhatt et al., 2024; Gallici et al., 2024)."}, {"title": "REWARPED: PARALLEL DIFFERENTIABLE MULTIPHYSICS SIMULATION", "content": "We aim to evaluate our approach on more challenging manipulation and locomotion tasks that involve interaction between articulated rigid bodies and deformables. To this end, we introduce Rewarped, our parallel differentiable multiphysics simulation platform that provides GPU-accelerated parallel environments for RL and enables computing batched simulation gradients efficiently. We build Rewarped on NVIDIA Warp (Macklin, 2022), the successor to DFlex (Xu et al., 2021; Murthy et al., 2021; Turpin et al., 2022; Heiden et al., 2023).\nWe proceed to discuss high-level implementation details and optimization tricks to enable efficient parallel differentiable simulation. We develop a parallelized implementation of Material Point Method (MPM) which supports simulating parallel environments of complex deformable materials, building on the MLS-MPM implementation by (Ma et al., 2023) used for non-parallel simulation. Furthermore, we support one-way coupling from kinematic articulated rigid bodies to MPM particles, based on the (non-parallel) MPM-based simulation from (Huang et al., 2020; Li et al., 2023a)."}, {"title": "PARALLEL DIFFERENTIABLE SIMULATION", "content": "We implement all simulation code in NVIDIA Warp (Macklin, 2022), a library for differentiable programming that converts Python code into CUDA kernels by runtime JIT compilation. Warp implements reverse-mode auto-differentiation through the discrete adjoint method, using a tape to record kernel calls for the computation graph, and generates kernel adjoints to compute the backward pass. Warp uses source-code transformation (Griewank & Walther, 2008; Hu et al., 2020) to automatically generate kernel adjoints.\nWe use gradient checkpointing (Griewank & Walther, 2000; Qiao et al., 2021b) to reduce memory requirements. During backpropogation, we run the simulation forward pass again to recompute intermediate values, instead of saving them during the initial forward pass. This is implemented by capturing and replaying CUDA graphs, for both the forward pass and the backward pass of the simulator. Gradient checkpointing by CUDA graphs enables us to compute batched simulation gradients over multiple time steps efficiently, when using more simulation substeps for simulation stability. We use a custom PyTorch autograd function to interface simulation data and model parameters between Warp and PyTorch while maintaining auto-differentiation functionality."}, {"title": "EXPERIMENTS", "content": "We evaluate our proposed maximum entropy FO-MBRL algorithm, Soft Analytic Policy Optimization (SAPO, Section 4), against baselines on a range of locomotion and manipulation tasks involving rigid and soft bodies. We implement these tasks in Rewarped (Section 5), our parallel differentiable multiphysics simulation platform. We also compare algorithms on DFlex rigid-body locomotion tasks introduced in (Xu et al., 2021) in Appendix F.2.\nBaselines. We compare to vanilla model-free RL algorithms: Proximal Policy Optimization (PPO, Schulman et al. (2017)), an on-policy actor-critic algorithm; Soft Actor-Critic (SAC, Haarnoja et al. (2018b)) an off-policy maximum entropy actor-critic algorithm. We use the implementations and hyperparameters from (Li et al., 2023b) for both, which have been validated to scale well with parallel simulation. Implementation details (network architecture, common hyperparameters, etc.) are standardized between methods for fair comparison, see Appendix C. We also compare against Analytic Policy Gradient (APG, Freeman et al. (2021)) and Short-Horizon Actor-Critic (SHAC, Xu et al. (2021)), both of which are state-of-the-art FO-MBRL algorithms that leverage first-order analytic gradients from differentiable simulation for policy learning. Finally, we include gradient-based trajectory optimization (TrajOpt) as a baseline, which uses differentiable simulation gradients to optimize for an open-loop action sequence that maximizes total rewards across environments.\nTasks. Using Rewarped, we re-implement a range of challenging manipulation and locomotion tasks involving rigid and soft bodies that have appeared in prior works. Rewarped enables training algorithms on parallel environments, and differentiable simulation to compute analytic simulation gradients through environment dynamics and rewards. We visualize these tasks in Figure 1. To simulate deformables, we use \u223c 2500 particles per env. See Appendix E for more details.\nAntRun \u2013 Ant locomotion task from DFlex (Xu et al., 2021), where the objective is to maximize the forward velocity of a four-legged ant rigid-body articulation.\nHandReorient \u2013 Allegro hand manipulation task from Isaac Gym (Makoviychuk et al., 2021), where the objective is to perform in-hand dexterous manipulation to rotate a rigid cube given a target pose. We replace non-differentiable terms of the reward function (ie. boolean comparisons) with differentiable alternatives to enable computing analytic gradients.\nRollingFlat - Rolling pin manipulation task from PlasticineLab (Huang et al., 2020), where the objective is to flatten a rectangular piece of dough using a cylindrical rolling pin.\nSoftJumper - Soft jumping locomotion task, inspired by GradSim (Murthy et al., 2021) and DiffTaichi (Hu et al., 2020), where the objective is to maximize the forward velocity and height of a high-dimensional actuated soft elastic quadruped.\nHandFlip - Shadow hand flip task from DexDeform (Li et al., 2023a), where the objective is to flip a cylindrical piece of dough in half within the palm of a dexterous robot hand.\nFluidMove \u2013 Fluid transport task from SoftGym (Lin et al., 2021), where the objective is to move a container filled with fluid to a given target position, without spilling fluid out of the container."}, {"title": "RESULTS ON REWARPED TASKS", "content": "We compare SAPO, our proposed maximum entropy FO-MBRL algorithm, against baselines on a range of challenging manipulation and locomotion tasks that involve rigid and soft bodies, re-implemented in Rewarped, our parallel differentiable multiphysics simulation platform. In Figure 2, we visualize training curves to compare algorithms. SAPO shows better training stability across different random seeds, against existing FO-MBRL algorithms APG and SHAC. In Table 2, we report evaluation performance for final policies after training. SAPO outperforms all baselines across all tasks we evaluated, given the same budget of total number of environment steps. We also find that on tasks involving deformables, APG outperforms SHAC, which is consistent with results in DaXBench (Chen et al., 2022) on their set of soft-body manipulation tasks. However, SHAC outperforms APG on the articulated rigid-body tasks, which agrees with the rigid-body locomotion results in DFlex (Xu et al., 2021) that we also reproduce ourselves in Appendix F.2.\nIn Appendix Figure 11, we visualize different trajectories produced by SAPO policies after training. We observe that SAPO learns to perform tasks with deformables that we evaluate on. For RollingFlat, SAPO controls the rolling pin to flatten the dough and spread it across the ground. For SoftJumper, SAPO learns a locomotion policy that controls a soft elastic quadruped to jump forwards. For HandFlip, SAPO is capable of controlling a high degree-of-freedom dexterous robot hand, to flip a"}, {"title": "SAPO ABLATIONS", "content": "We investigate which components of SAPO yield performance gains over SHAC, on the HandFlip task. We conduct several ablations on SAPO: (a) w/o Vsoft, where instead the critic is trained in standard fashion without entropy in target values; (b) w/o \u0397\u03c0, where we do not use entropy-augmented returns and instead train the actor to maximize expected returns only; (c) w/o H and Vsoft, which corresponds to modifying SHAC by applying design choices {III, IV, V} described in Section 4.2.\nWe visualize training curves in Figure 3, and in Table 3 we report final evaluation performance as well as percentage change from SHAC's performance as the baseline. From ablation (b), we find that using analytic gradients to train a policy to maximize both expected return and entropy is critical to the performance of SAPO, compared to ablation (a) which only replaces the soft value function.\nAdditionally, we observe that ablation (c), where we apply design choices {III, IV, V} onto SHAC, result in approximately half of the performance improvement of SAPO over SHAC on the HandFlip task. We also conducted this ablation on the DFlex rigid-body locomotion tasks however, and found these modifications to SHAC to have minimal impact in those settings, shown in Appendix F.3. We also conduct individual ablations for these three design choices in Appendix F.4."}, {"title": "CONCLUSION", "content": "Due to high sample complexity requirements and slower runtimes for soft-body simulation, RL has had limited success on tasks involving deformables. To address this, we introduce Soft Analytic Policy Optimization (SAPO), a first-order model-based actor-critic RL algorithm based on the maximum entropy RL framework, which leverages first-order analytic gradients from differentiable simulation to achieve higher sample efficiency. Alongside this approach, we present Rewarped, a scalable and easy-to-use platform which enables parallelizing RL environments of GPU-based differentiable multiphysics simulation. We re-implement challenging locomotion and manipulation tasks involving rigid bodies, articulations, and deformables using Rewarped. On these tasks, we demonstrate that SAPO outperforms baselines in terms of sample efficiency as well as task performance given the same budget for total environment steps.\nLimitations. SAPO relies on end-to-end learning using first-order analytic gradients from differentiable simulation. Currently, we use (non-occluded) subsampled particle states from simulation as observations to policies, which is infeasible to obtain in real-world settings. Future work may use differentiable rendering to provide more realistic visual observations for policies while maintaining differentiability, towards sim2real transfer of policies learned using SAPO. Another promising direction to consider is applications between differentiable simulation and learned world models."}, {"title": "REWARPED PHYSICS", "content": "We review the simulation techniques used to simulate various rigid and soft bodies in Rewarped. Our discussion is based on (Xu et al., 2021; Heiden et al., 2023; Murthy et al., 2021; Macklin, 2022), as well as (Hu et al., 2019b; Huang et al., 2020; Ma et al., 2023) for MPM.\nTo backpropagate analytic policy gradients, we need to compute simulation gradients. Following reverse order, observe :\n$\\nabla J(\u03c0) = \\nabla_\u03b8 \\sum_{t=0}^{T-1} \u03b3^t r_t$\n$\\frac{dr_t}{d\u03b8} = \\frac{dr_t}{da_t} \\frac{da_t}{d\u03b8} + \\frac{dr_t}{ds_t} \\frac{ds_t}{d\u03b8}$\n$\\frac{ds_{t+1}}{d\u03b8} = \\frac{df(s_t, a_t)}{ds_t} \\frac{ds_t}{d\u03b8} + \\frac{df(s_t, a_t)}{da_t} \\frac{da_t}{d\u03b8}$\nwith $a_t \\sim \u03c0_\u03b8(s_t)$, $s_{t+1} = f(s_t, a_t)$.\nWe use the reparameterization trick to compute gradients $\\frac{da_t}{d\u03b8} = \\nabla_\u03b8 \u03c0_\u03b8(s_t)$ for stochastic policy $\u03c0_\u03b8$ to obtain $\\frac{da_t}{d\u03b8}$. Note that the simulation gradient is $\\frac{df(s_t, a_t)}{ds_t}$, which we compute by the adjoint method through auto-differentiation (AD).\nWhile we could estimate the simulation gradient with finite differences (FD) through forms of $f'(x) = \\frac{f(x+h)-f(x)}{h}$, FD has two major limitations. Consider $f : \u211d^N \u2192 \u211d^M$, where N is the input dimension, M is the output dimension, and L is the cost of evaluating f. Then FD has time complexity $O((N + 1) \u00b7 M \u00b7 L)$. In comparison, reverse-mode AD has time complexity O(M \u00b7 L), and also benefits from GPU-acceleration. Furthermore, FD only computes numerical approximations of the gradient with precision depending on the value of h, while AD yields analytic gradients. For a simulation platform built on a general differentiable programming framework, computing gradients for different simulation dynamics f is straightforward using AD. While alternatives to compute analytic simulation gradients through implicit differentiation have been proposed (see Appendix A), they are less amenable to batched gradient computation for parallel simulation."}, {"title": "COMPOSITE RIGID BODY ALGORITHM (CRBA)", "content": "Consider the following rigid body forward dynamics equation to solve :\n$M\\ddot{q} = J^T F(q, \\dot{q}) + c(q, \\dot{q}) + \u03c4(q, \\dot{q}, \u03b1)$,\nwhere $q, \\dot{q}, \\ddot{q}$ are joint coordinates, velocities, accelerations, F are external forces, c includes Coriolis forces, \u03c4 are joint-space actuations, M is the mass matrix, and J is the Jacobian. Featherstone's composite rigid body algorithm (CRBA) is employed to solve for articulation dynamics. After obtaining joint accelerations $\\ddot{q}$, a semi-implicit Euler integration step is performed to update the system state $s = (q, \\dot{q})$. We use the same softened contact model as (Xu et al., 2021)."}, {"title": "FINITE ELEMENT METHOD (FEM)", "content": "To simulate dynamics, a finite element model (FEM) is employed based on tetrahedral discretization of the solid's mesh. A stable neo-Hookean constitutive model (Smith et al., 2018) is used to model elastic deformable solids with per-element actuation :\n$\u03a8(q, t) = \\frac{\u03bc}{2} (I_c - 3) + \\frac{\u03bb}{2} (J - a)^2 - \u03bc \\log(\\frac{I_c}{J} + 1)$,\nwhere (\u03bb, \u03bc) are the Lam\u00e9 parameters which control each tetrehedral element's resistance to shear and strain, a is a constant, J = det(F) is the relative volume change, $I_c = tr(F^T F)$, and F is the deformation gradient. Integrating \u03a8 over each tetrahedral element yields the total elastic potential energy, to then compute $F_{elastic}$ from the energy gradient, and finally update the system state using a semi-implicit Euler integration step. We use the same approach as (Murthy et al., 2021)."}, {"title": "MATERIAL POINT METHOD (MPM)", "content": "The moving least squares material point method (MLS-MPM) (Hu et al., 2018) can efficiently simulate a variety of complex deformables, such as elastoplastics and liquids. Consider the two equations for conservation of momentum and mass :\n$\u03c1 \\dot{v} = \u2207 \u00b7 P + \u03c1b$\n$\\frac{D\u03c1}{Dt} = \u2212\u03c1\u2207 \u00b7 v,$\nwhere v is the velocity, $\\dot{v}$ is the acceleration, \u03c1 is the density, and b is the body force. For this system to be well-defined, constitutive laws must define P, see (Ma et al., 2023). We simulate elastoplastics using an elastic constitutive law with von Mises yield criterion :\n$P(F) = U (2\u03bc\u03b5 + \u03bb tr(e))U^T$\n$\u03b4_\u03b3 = \\frac{||\u03c4||}{2\u03bc}$\nP(F) = $\\begin{cases}F & \u03b4_\u03b3 \u2264 0 \\ \\frac{\u03c4}{||\u03c4||} U \\exp(e - \\frac{\u03b4_\u03b3}{\u03c4})V^T & \u03b4_\u03b3 > 0, \\end{cases}$$\nwhere P(F) is a projection back into the elastic region for stress that violates the yield criterion, and $F = U\u03a3V^T$ is the singular value decomposition (SVD). We simulate fluids as weakly compressible, using a plastic constitutive law with the fixed corotated elastic model :\n$P(F) = AJ(J \u2212 1)F^{-T}$\n$P(F) = JI,$\nwhere P(F) is a projection back into the plastic region and J = det(F).\nWe use the same softened contact model from PlasticineLab (Huang et al., 2020), with one-way coupling between rigid bodies to MPM particles. For any grid point with signed distance d to the nearest rigid body, we compute a smooth collision strength s = min(exp(\u2212ad), 1). The grid point velocity before and after collision projection is linearly interpolated using s. Coupling is implemented by the Compatible Particle-In-Cell (CPIC) algorithm, see steps 2\u20134 of (Hu et al., 2018)."}, {"title": "REWARPED TASKS", "content": "We visualize each task in Figure 1, and high-level task descriptions are provided in Section 6."}, {"title": "TASK DEFINITIONS", "content": "For each task, we define the observation space O, action space A, reward function R, termination d, episode length T, initial state distribution \u03c1o, and simulation method used for transition function P. We denote the goal state distribution p\u2217 if the task defines it. We also report important physics hyperparameters, including both shared ones and those specific to the simulation method used.\nWe use i, j, k for standard unit vectors and $proj_u v = \\frac{u.v}{||v||^2} v$ for the projection of u onto v. We use ux to denote the x-coordinate of vector u.\nWe use 1 for the indicator function and $U(b) = U(\u2212b, b)$ for the uniform distribution. We denote an additive offset +\u03b4 or multiplicative scaling \u00d7\u03b4, from some prior value.\nFor notation, we use z-axis up. Let q, $\\dot{q}$, $\\ddot{q}$ be joint coordinates, velocities, accelerations. Let (p, \u03b8) be world positions and orientations, (v, \u03c9) be linear and angular velocities, (a, \u03b1) be linear and angular accelerations, derived for root links such as the center of mass (CoM).\nAll physical quantities are reported in SI units. We use h for the frame time, and each frame is computed using S substeps, so the physics time step is \u0394t = h/S. The gravitational acceleration is g.\nFEM. We use x' to denote a subsampled set of FEM particle positions x, where x' is the CoM of the subsampled particles (i.e. average particle position for uniform density). Similarly for velocities v. We also report the following quantities: number of particles Nx, number of tetrahedron Ntet, particles' density \u03c1, Lam\u00e9 parameters (\u03bb, \u03bc) and damping stiffness kdamp.\nMPM. We use x' to denote a subsampled set of MPM particle positions x, where x' is the CoM of the subsampled particles (i.e. average particle position for uniform density). We also report the following quantities: friction coefficient for rigid bodies \u03bcR, grid size Ng, number of particles Nx, particles' density \u03c1, Young's modulus E, Poisson's ratio \u03bd, yield stress \u03c3y."}, {"title": "ANTRUN", "content": "O \u211d37 : [pz, \u03b8, vx, \u03c9, q, $\\dot{q}$, uup, uheading, at\u22121]\nA \u211d8 : absolute joint torques \u03c4\nR $v_x$ + (0.1Rup + Rheading + Rheight)\nd $1{pz < hterm}$\nT 1000\n\u03c1o +\u03b4\u03c1 \u223c U(0.1), +\u03b4q \u223c U(0.1\u03c0)\n+$\\dot{q}$ \u223c U(0.2), q \u223c U(0.25)\nP CRBA\nh 1/60\nS 16\nThe derived quantities correspond to the ant's CoM. We use termination height hterm = 0.27. We denote uup = proj\u0302z p and uheading = projx\u0302 p. The reward function maximizes the forward velocity vx, with auxiliary reward terms Rup = uup to encourage vertical stability, Rheading = uheading to encourage running straight, and Rheight = pz \u2212 hterm to discourage falling. Initial joint angles, joint velocities, and CoM transform of the ant are randomized."}, {"title": "HANDREORIENT", "content": "O \u211d72 : \"full\", see (Makoviychuk et al., 2021)\nA \u211d16 : absolute joint torques \u03c4\nR see (Makoviychuk et al., 2021)\nd see (Makoviychuk et al., 2021)\nT 600\n\u03c1o see (Makoviychuk et al., 2021)\nP CRBA\nh 1/120\nS 32\nSee (Makoviychuk et al., 2021). Initial joint angles of the hand, CoM transform of the cube, and target orientations for the cube are randomized."}, {"title": "ROLLINGFLAT", "content": "O $[\u211d^{250\u00d73}, \u211d^3, \u211d^3]$ : [x', $\\dot{x}$', (px, pz, \u03b8z)]\nA \u211d3 : relative positions px, pz and orientation \u03b8z\nR Rd + Rflat\nT 300\n\u03c1o +\u03b4\u03c3 \u223c U(0.9, 1.1)\n+\u03b4xx,y \u223c U(0.1), +\u03b4xz \u223c U(0, 0.05)\nP MPM\nh $5 \u00b7 10^{\u22125}$\nS 40\n\u03bc\u03b5 0.9\nNg 483\nNx 2592\n\u03c1 1.0\nE 5000.\n\u03bd 0.2\n\u03c3y 50.\nThe derived quantities correspond to the rolling pin's CoM. We use target flatten height hflat = 0.125. For the reward function, the first term Rd minimizes the difference between the particles' CoM and hflat, while the second term maximizes the smoothness of the particles Rflat:\nd $\\frac{x'_z}{h_{flat}}$\n$R_d = \\frac{1}{1 + d}. \n R_{flat} = \\frac{1}{\\frac{1}{N}  \\sum_i [x']_i^2}[1{\\{d > 0.33\\}} \u00b7 1 + 1{\\{d \u2264 0.33\\}} \u00b7 2]$\nInitial volume and CoM transform of the particles are randomized."}, {"title": "SOFTJUMPER", "content": "O $[\u211d^{204\u00d73}, \u211d^3, \u211d^3, [\u211d^{222}]] : [x', $\\dot{x}$', v', at\u22121]\nA \u211d222 : absolute tetrahedral volumetric activations (subsampled)\nR $v'_x$ + $(3.0 R_{up} \u2212 0.0001 \\sum_i ||a||)$ \nT 300\n\u03c1o +\u03b4\u03c1 \u223c U(0.9, 1.1)\n+\u03b4xx,y \u223c U(0.8), +\u03b4xz \u223c U(0, 0.4)\nP FEM\nh 1/60\nS 80\nNx 204\nNtet 444\n\u03c1 1.0\n\u03bb 1000.\n\u03bc 1000.\nKdamp 1.0\nDenote the default initial height h\u2081 = xz = 0.2. The reward function maximizes the forward velocity v'x of the quadruped's CoM, with two auxiliary reward terms Rup = x'z \u2212 h\u2080 to encourage jumping and the other to encourage energy-efficient policies by minimizing action norms. For the action space, we downsample by a factor of 2 and upsample actions to obtain the full set of activations. Initial volume and CoM transform of the quadruped are randomized."}, {"title": "HANDFLIP", "content": "O $[\u211d^{250\u00d73"}, "\u211d^3, \u211d^{24}"], "\u211d24": "relative joint angles\nR $"}