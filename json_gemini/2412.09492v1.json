{"title": "Video Seal: Open and Efficient Video Watermarking", "authors": ["Pierre Fernandez", "Hady Elsahar", "I. Zeki Yalniz", "Alexandre Mourachko"], "abstract": "The proliferation of AI-generated content and sophisticated video editing tools has made it both important and challenging to moderate digital platforms. Video watermarking addresses these challenges by embedding imperceptible signals into videos, allowing for identification. However, the rare open tools and methods often fall short on efficiency, robustness, and flexibility. To reduce these gaps, this paper introduces Video Seal, a comprehensive framework for neural video watermarking and a competitive open-sourced model. Our approach jointly trains an embedder and an extractor, while ensuring the watermark robustness by applying transformations in-between, e.g., video codecs. This training is multistage and includes image pre-training, hybrid post-training and extractor fine-tuning. We also introduce temporal watermark propagation, a technique to convert any image watermarking model to an efficient video watermarking model without the need to watermark every high-resolution frame. We present experimental results demonstrating the effectiveness of the approach in terms of speed, imperceptibility, and robustness. Video Seal achieves higher robustness compared to strong baselines especially under challenging distortions combining geometric transformations and video compression. Additionally, we provide new insights such as the impact of video compression during training, and how to compare methods operating on different payloads. Contributions in this work including the codebase, models, and a public demo are open-sourced under permissive licenses to foster further research and development in the field.", "sections": [{"title": "1 Introduction", "content": "Within digital media, video watermarking has always been a very active field of research. The film industry, including Hollywood studios and streaming websites, has been particularly invested in developing robust video watermarking techniques to fight against piracy. However, with the rapid advancement of technology, new challenges and applications have emerged. For instance, the development of generative AI models for images, like DALLE  or Stable Diffusion  , and videos like Sora  or MovieGen , raises concerns about the spread of misinformation and general misuse of such technology. Regulators  are now pushing generative model providers to embed watermarks into the generated content to ease detection and attribution of said content. Additionally, they also encourage hardware providers to watermark real data at the physical device level , which requires fast embedding and detection. All this requires the development of robust and efficient video watermarking techniques that can keep pace with the rapidly evolving landscape of digital media and AI-generated content.\nIt may seem logical to simply decompose videos into their constituent frames and leverage well-established image watermarking techniques to embed watermarks into each frame separately. This approach, however, is hindered by two significant limitations. Firstly, the computational load of watermarking every frame is prohibitively high, particularly for high-resolution videos with high frame rates. Processing videos as clips (chunks of frames) for embedding or extraction can help with parallelization, but large clips exceed memory limits, while smaller clips introduce synchronization issues, complicating watermark extraction. Secondly, the widespread use of video compression codecs such as AV1  and H.264  along with the ease of access to free video editing software and social media filters poses a significant challenge to video watermarking. Whenever a video is downloaded, or shared on social media platforms, these codecs are often automatically applied, storing videos as keyframes, intraframes, and optical flows that enable frame decoding through interpolation. This process substantially reduces redundancy in videos, resulting in a strong decrease in the watermark signal. Consequently, even when computational efficiency is no longer a concern, image watermarking models may still struggle to remain effective in the face of these codecs and video editing tools, underscoring the need for video-specific watermarking solutions.\nThere have been some works on neural video watermarking addressing the aforementioned challenges. For instance, DVMark  employ a compression network to simulate video compression, while VHNet  leverages a similar trick for steganography applications. ItoV  adapts architectures from image models to video watermarking by merging the temporal dimension of the videos with the channel dimension, enabling deep neural networks to treat videos as images. It also employs a straight-through estimator to allow for gradient flow on compression augmentation. However, despite these efforts, several limitations persist. Notably, most existing models are restricted to low-resolution videos (e.g., 128\u00d7128) or short clips (e.g., 64 frames), rendering them impractical for real-world applications.\nMost importantly, there is a lack of reproducibility in existing research on video watermarking. To our knowledge, none of the existing video watermarking models have been publicly released, hindering fair comparisons and reproducibility. This omission not only undermines the validity of the reported results but also stifles progress in the field.\nIn this paper, we introduce Video Seal, a state-of-the-art video watermarking model that sets a new standard for efficiency and robustness. By leveraging temporal watermark propagation, a novel technique that converts any image watermarking model into an efficient video watermarking model, Video Seal eliminates the need to watermark every frame in a video. Video Seal also employs a multistage training that includes image pre-training, hybrid post-training, and extractor fine-tuning. This training regimen is supplemented with a range of differentiable augmentations, including the popular H.264 codec, allowing Video Seal to withstand common video transformations and high compression rates.\nDue to the scarcity of reproducible baselines for video watermarking, we adapt state-of-the-art image watermarking models to create strong baselines using the temporal watermark propagation technique. This adaptation is a significant contribution of this paper, as it provides a much-needed foundation for evaluating and comparing video watermarking techniques. Video Seal outperforms strong image baselines, including MBRS , Trust Mark and WAM , in terms of robustness under basic geometric transformations such as cropping, small rotations, and perspective changes. Although MBRS and TrustMark offer higher message capacities (256 and 100 bits, respectively), their design and training limitations make them vulnerable to degradation under these common transformations, which limits their real-world applicability."}, {"title": "2 Method", "content": "We adopt the embedder/extractor framework originally developed for image watermarking by Zhu et al. (2018) and extend it to videos in a similar way as Ye et al. (2023). We focus on speed and practicality. Our approach operates in 2D to ensure streamability, simplify extraction, and maintain flexibility. This design also enables a unified embedder-extractor mechanism for both images and videos. Our models are based on state-of-the-art architectures trained on longer schedules with a comprehensive set of augmentations that include video codecs. They are effective at any resolution and for videos of any length."}, {"title": "2.1 Embedder & extractor architectures", "content": "Our architectures are kept voluntarily small and efficient to facilitate inference and to possibly run on mobile devices. The embedder is based on an efficient U-Net architecture with 16M parameters in total, while the extractor is based on a vision transformer with 24M parameters. The number of bits $nbits$ is set to 96."}, {"title": "2.1.1 Embedder", "content": "The embedder takes as input a frame $x \\in R^{3\\times256\\times256}$ and a binary message $m \\in \\{0,1\\}^{nbits}$, and outputs a watermarked frame $xw \\in R^{3\\times256\\times256}$ that slightly differs from the original. Its architecture is detailed in Tab. 1. It is based on a shrunk U-Net architecture , with modifications taken from the \"Efficient U-Net\" of Imagen . The message embedding happens in the bottleneck which operates at a lower resolution. It is done through a binary message lookup table T structured to facilitate the embedding of binary messages into the latent representation of the frame, as previously presented by San Roman et al. (2024); Sander et al. (2024).\nThe U-Net consists of an encoder-decoder structure with skip connections, allowing to preserve the image information throughout the network, while doing most of the operations at a lower resolution. The encoder path begins with an initial residual block \"ResNetBlock\u201d that processes the input image of shape $3 \u00d7 256 \u00d7 256$ into a feature map of shape $d\u2082/8 \u00d7 256 \u00d7 256$. This is followed by a series of downsampling blocks \"DBlocks\u201d,"}, {"title": "2.1.2 Extractor", "content": "The extractor takes as input a frame $x \\in R^{3\\times256\\times256}$ and outputs a \"soft\" message $m \\in R^{nbits}$ which can be thresholded to recover a \"hard\" binary message $m \\in \\{0,1\\}^{nbits}$ (soft because continuous, hard because binary). Its architecture is detailed in Tab. 2. It is based on a vision transformer (ViT)  followed by a patch decoder and an average pooling layer that maps to a $nbits$ dimensional vector.\nThe ViT consists of a series of attention blocks to process image patches into a high-dimensional feature space. We use the ViT-Small architecture  (22M parameters), with patch size 16, with $d = d' = 384$. The patch embeddings are processed by a residual block, which is made of a Conv2D with kernel size of 3 and stride of 1, a LayerNorm, and a GELU activation, and with the number of channels equals to the one of input channels. We obtain a latent map of shape $(d', 256, 256)$, which is average-pooled and mapped to $nbits$-dimensional pixel features by a linear layer. Finally, a Sigmoid layer scales the outputs to [0, 1] (this is in fact only done at inference time, since the training objective implicitly applies it in PyTorch)."}, {"title": "2.2 Video inference", "content": "Our embedder and extractor are designed to work on individual frames of fixed resolution (256x256). \u03a4\u03bf operate in an efficient manner on videos, we use a few tricks to speed up the embedding and extraction processes. Namely, we downscale frames to the fixed resolution, embed the watermark every k frames, upscale the watermark to the original resolution, and propagate the watermark signal to the k - 1 neighboring frames. This is illustrated in Fig. 2 and detailed in the following paragraphs."}, {"title": "2.2.1 High-resolution and scaling factor", "content": "Our embedder and extractor are trained at a fixed resolution of 256 \u00d7 256. To extend it to higher resolution, we use the same trick as presented by Bui et al. (2023); Sander et al. (2024).\nGiven a frame x of size H \u00d7 W, we first downscale it to 256 \u00d7 256 using bilinear interpolation. The embedder takes the downsampled frame and the message as input and produces the watermark distortion w. We then upscale w to the original resolution - again using bilinear interpolation - and add it to the original frame to obtain the watermarked frame:\n$Xw = x + aw.resize(w), w = Embedder(resize(x), m)$.\n$aw$ is called the scaling factor and controls the strength of the watermark. It may be adjusted at inference time to trade quality for robustness. In the following sections of the paper, we say that $aw$ is \"nominal\" at inference when it is set to the same value as during training.\nWe proceed similarly for extraction and we resize all frames to 256 \u00d7 256 before giving them to the extractor."}, {"title": "2.2.2 Temporal watermark propagation", "content": "Watermarking each frame of a video can be computationally costly. To mitigate this, a trick suggested in the codebase by Xian et al. (2024) is to watermark every k frames instead. However, this approach complicates the extraction process. Indeed, leaving some frames unwatermarked can compromise the robustness of the watermark under temporal editing and video compression algorithms. Even without any video edition, the extractor signal will be mixed with a lot of signal coming from unwatermarked frames, which will reduce the accuracy of the extraction.\nIn our approach, called temporal watermark propagation, the video is divided into segments of k frames, the first frame of each segment is passed through the embedder to generate a watermark distortion which is then copied to the k \u2212 1 subsequent frames within the segment. More rigorously, let $x_i \u2208 R^{3\u00d7256\u00d7256}$ denote the ith frame of the video, and $w_i \u2208 R^{3\u00d7256\u00d7256}$ denote the watermark distortion of $x_i$. Let $m \u2208 \\{0,1\\}^{nbits}$ denote the binary message to be embedded. Temporal watermark propagation can be formulated as follows:\n$Wi = \\{\nEmbedder (x,m), if i mod k = 0,\\\\\nWi-1, otherwise.\n$\nIn practice, if k is set to 1, the watermark is applied to every frame of the video, and temporal watermark propagation is equivalent to watermarking each frame independently. When k increases the efficiency of the embedding increases. At the same time, it introduces some noise in the extraction process because we approximate the watermark signal in the unwatermarked frames. It may also introduce \"shadow\" artifacts if the video contains a lot of motion as the distortion often follows the image content. In practice k is set small enough for these two reasons, k = 4 in this work. Note that this operation is fully differentiable, allowing for the optimization of both imperceptibility and robustness during training."}, {"title": "2.2.3 Extraction", "content": "The watermark extraction processes each frame x, independently before aggregating the soft messages m over the entire video. For aggregation, we simply average the soft messages across all frames, and threshold the average to obtain the hard message contained in the video m:\n$mk = \\{\n1 if (\u03a3mi,k) > 0 ,\\\\\n0 otherwise\n$\\with mk the bit at position k.\nwhere T is the number of frames on which the extraction is done. In particular, one may choose to extract the watermark on certain frames for instance the first ones only or the whole video to increase robustness or to speed up the extraction process. This aggregation is chosen for simplicity and speed, but more advanced aggregation methods could be used, as studied in Sec. 5.3."}, {"title": "2.3 Training pipeline", "content": "In this section, we describe our method in detail, including image pre-training, mixed training with videos and images, and embedder freezing. Our training pipeline follows the traditional embedder/extractor approach , illustrated in Fig. 3. The embedder takes as input a batch of images or video frames and a binary message and produces watermarked images or frames. The extractor then attempts to recover the original message from them. We adopt a multistage training strategy that combines the benefits of image and video training. The following paragraphs detail these stages."}, {"title": "2.3.1 Training objectives", "content": "The training process involves minimizing a combination of perceptual losses and an extraction loss. The perceptual losses ensure that the watermark is imperceptible, while the extraction losses ensure that the extractor's output is close to the original message. The optimizer minimizes the following objective function:\n$L = AdiscLdisc + AiLi + AwLw$,\nwhere Adisc, i, and Aw are the weights of the discriminative loss, the image perceptual loss, and the watermark extraction loss, defined in the following paragraphs.\nExtraction loss. The watermark extraction loss ensures that the extracted message m is as close as possible to the original message m. We use the average binary cross-entropy (BCE) loss:\n$Lw = \\frac{1}{Nbits} \u03a3 BCE(mk, mk), with BCE(mk, mk) = mk log(mk) + (1 \u2212 mk) log(1 \u2013 mk).$"}, {"title": "2.3.3 Transformations", "content": "We use a comprehensive set of transformations during training, which are detailed in Tab. 3. Most of them are applied at the frame level. We categorize them into two main groups: valuemetric, which change the pixel values; geometric, which modify the image's geometry - and are unfortunately absent from many recent works on both image and video watermarking  .\nFrame transformations. We include crop, resize, rotation, perspective, brightness, contrast, hue, saturation, Gaussian blur, median filter, JPEG compression. The strengths of these transformations are randomly sampled from a predefined range during training, and applied the same way to all images of the mini-batch. For crop and resize, each new edge size is selected independently, which means that the aspect ratio can change (because the extractor resizes the image). Moreover, an edge size ratio of 0.33 means that the new area of the image is 0.332 \u2248 10% times the original area. For brightness, contrast, saturation, and sharpness, the parameter is the default factor used in the PIL and Torchvision  libraries. For JPEG, we use the Pillow library.\nVideo transformations. When applied on videos, frame transformations are applied to their whole content. Additionally, we train and evaluate on common video codecs (e.g., H.264, H.265), as implemented in the PyAV wrapper around FFmpeg.\nAbout non-differentiable transformations. Non-differentiability or lack of backpropagatable implementations in PyTorch prevents us from backpropagating through video codecs. This poses a challenge since the gradients of the objective function cannot be backpropagated through the compression back to the embedder. One common solution is to use a differentiable approximation of the augmentation instead of the real one. For instance, Zhu et al. (2018); Zhang et al. (2023) use a differentiable JPEG compression and Luo et al. (2023); Shen et al. (2023) use a neural network trained to mimick video codec artifacts. We choose a second option for its ease of implementation and its popularity (Zhang et al., 2021; Ye et al., 2023; Sander et al., 2024). It involves using a straight-through estimator that approximates the gradient of the non-differentiable operation with the identity function:\nXaug = Xw + nograd (T(xw) - Xw),\nwhere nograd does not propagate gradients and T is the non-differentiable transformation."}, {"title": "3 Experimental Setup and Implementation Details", "content": null}, {"title": "3.1 Metrics", "content": "Watermarking is subject to a trade-off between imperceptibility, i.e., how much the watermarking degrades the video, and robustness, i.e., how much image or video transformations affect the recovery of the input binary message. We therefore use two main categories of evaluation metrics.\nMetrics for image and video quality. We evaluate the quality of the watermarked videos using per-pixel and perceptual metrics. The PSNR (peak-signal-to-noise ratio) measures the difference between the original and watermarked videos in terms of mean squared error (MSE), and is defined as PSNR = 10 log10 (2552/MSE). SSIM  (structural similarity index measure) measures the similarity between the original and watermarked videos in terms of luminance, contrast, and structure. LPIPS  is better at evaluating how humans perceive similarity. It is calculated by comparing the features extracted from the two frames using a pre-trained neural network. On videos, SSIM and LPIPS metrics are computed frame-wise and averaged over the entire video.\nThe above metrics do not take into account the temporal consistency of the video. VMAF (Netflix, 2016) (video multi-method assessment fusion) is, on the contrary, designed specifically for video quality assessment. It uses a neural network to predict the subjective quality of a video based on various objective metrics such as PSNR, SSIM, and motion vectors.\nMetrics for robustness of extraction. The main metric to evaluate the robustness of the watermarking in a multi-bit setting is the bit accuracy. Given an input message $m\u2208 \\{0,1\\}^{nbits}$ and an output message m, the bit accuracy is defined as the percentage of bits that are correctly decoded, i.e.,"}, {"title": "3.2 Datasets", "content": "We use two main datasets for training and evaluation across video and image domains. For image training, we use the SA-1B dataset , from which we randomly select 500k images resized to 256 \u00d7 256. For evaluation we use 1k random images at their original image resolution (with an average resolution of 1500 \u00d7 2250). To keep a fair comparison with existing image watermarking models, we also evaluate on 1k images from the COCO validation dataset , which are of slightly lower resolution.\nFor video training we use the SA-V dataset  which comprises 51k diverse videos captured across multiple countries, with resolutions ranging from 240p to 4K and an average duration of 14 seconds at 24 fps. We randomly select 1.3-second clips (32 frames) from each video resized to a resolution of 256 \u00d7 256, while evaluation uses the first 5 seconds at the original resolution, unless stated otherwise."}, {"title": "3.3 Training", "content": "We first train the model on 16 GPUs, using the AdamW optimizer . For the first 800 epochs, we only use images from the SA-1b dataset (see Sec. 3.2 for details on datasets), with a batch size of 16 per GPU, with 1500 steps per epoch. The learning rate is linearly increased from 10-6 to 10-5 over the first 50 epochs, and then follows a cosine schedule  down to 10-7 until epoch 800. For the last 300 epochs, we also use the SA-V dataset, with 200 steps per epoch. We only forward one 32-frame clip per GPU, randomly chosen at every step. The learning rate is linearly increased from 10-7 to 10-6 over the first 10 epochs, and then follows a cosine schedule down to 10-8 until the last epoch. At epoch 250, we freeze the embedder and only optimize the extractor (see Sec. 2.3.2). The objectives are weighted with Aw = 1.0, \u03bb\u2081 = 0.5, Adisc = 0.1."}, {"title": "3.4 Baselines", "content": "In the absence of an established open-source video watermarking baselines, we leverage state-of-the-art image watermarking models as foundational baselines for video watermarking. HiDDeN  is one of the earliest deep-learning watermarking methods. We trained it on 48 bits with the same augmentations for fairer comparison. MBRS  is based on the same architecture, but embeds 256-bit watermarks, with a training using mini-batches of real and simulated JPEG compression. CIN  combines invertible and non-invertible mechanisms to embed 30-bit watermarks. Trust Mark  also uses a U-Net architecture trained similarly to HiDDeN, embedding 100 bits. Finally, WAM  embeds 32 bits (in addition to one bit of detection which we do not use in this study), and offers robustness to splicing and inpainting. We use the original open weights for all baselines, except for HiDDeN, for which the authors do not provide weights. Video Seal operates with nbits = 96, with \u03b1\u03c9 = 2.0, unless stated otherwise.\nInference. All methods operate at resolution 256 \u00d7 256, except CIN, which is at 128 \u00d7 128. We extend them to arbitrary resolutions as presented in Sec. 2.2.1 (when the networks directly predict an image xw and not a watermark distortion w, we retrieve it by doing w = xw - x). By default, we use the original watermark strength aw of Eq. 1 (1.0 for most methods), except in Sec. 4.4 where we study the imperceptibility/robustness trade-off. When evaluating the baselines on videos, we apply the image watermarking model with the same inference strategy as our models, i.e., get the watermark distortion every k = 4 frames, and propagate the watermark to the other 3 frames as described in Sec. 2.2.2. For watermark extraction, we aggregate the soft bit predictions across the frames, and average the outputs to retrieve the global message (see Sec. 2.2.3)."}, {"title": "3.5 Evaluated transformations", "content": "We evaluate the robustness of our method to many transformations for different strengths. For simplicity, we aggregate the transformations into five categories: no transformation, geometric, valuemetric, compression, and combined transformations. For instance, geometric transformations include rotations, crops and perspective, while valuemetric transformations include brightness, contrast, and saturation changes, all with different ranges. The combined augmentations are realistic augmentations applied sequentially, e.g., an H.264 compression followed by a crop and a brightness change. We show some examples of these transformations in Fig. 4. Full results and details on which transformations constitute each group are given in App. \u0412.2."}, {"title": "4 Results", "content": null}, {"title": "4.1 Robustness", "content": "We report in Tab. 4 the robustness of watermark extraction across many transformations and for various models, on the SA-1b and the SA-V datasets. Full results, detailed by transformation type and strength, are available in App. B.2. We also report results on the COCO dataset, to test the generalization of the models to unseen distributions.\nWe first observe that many of the image models are already strong baselines for video watermarking (as suggested by Ye et al. (2023), although this seems to be even more the case when working on high resolution videos). Most of them achieve high bit accuracy both for image and video transformations, even against video codecs."}, {"title": "4.2 Imperceptibility", "content": "We first show some examples of watermarked images in Fig. 5, and of video frames in App. B.1. We observe that the watermarks are imperceptible at first glance, but most are visible under close inspection, especially in flat areas, like the skies in both images. Different methods, which employ various perceptual losses and architectures, result in watermarks of distinct characteristics. For instance, MBRS and CIN tend to create grid-like patterns, while TrustMark and Video Seal tend to create wavier patterns.\nWe also quantitatively evaluate the imperceptibility of the watermarking models on the image datasets COCO and SA-1b and the video dataset SA-V, and report results in Tab. 5. For every baseline, we use their nominal strength (most of the time aw = 1 in Eq. 1), although they could be adapted to control the imperceptibility/robustness trade-off as done in Sec. 4.4. We report the PSNR, SSIM, and LPIPS between the watermarked and original images of the SA-1b dataset, as well as the same metrics for videos of the SA-V dataset (cut to 5s), with the addition of VMAF for videos (note that the PSNR is computed on the whole video, and not as an average of the frames as for SSIM and LPIPS). We observe that Video Seal achieves the highest PSNR and SSIM scores, while MBRS achieves better VMAF and TrusMark achieves better LPIPS, closely followed by Video Seal.\nIt is important to note that video imperceptibility is not fully captured in these examples and in these metrics. In practice, a watermark that is imperceptible in an image may not necessarily be imperceptible in a video, particularly when the watermark lacks consistency across frames. For instance, we found that TrustMark can produce shadowy artifacts as the watermark tracks the motion of the video, making it more visible. This is less pronounced for Video Seal, which tends to produce blobs that do not follow objects. However, clear metrics to evaluate this are still lacking, and would require a more comprehensive study on the perception of watermarks in videos. Notably, we observe that even at very high PSNR, SSIM or VMAF, the artifacts produced by Video Seal may be annoying to the human eye and highly depend on the cover videos."}, {"title": "4.3 Latency", "content": "We evaluate the latency of Video Seal compared to the image watermarking models repurposed for video watermarking. We use the video inference framework introduced in Sec. 2.2, with the downscale/upscale of the watermark signal and temporal watermark propagation with k = 4 - to ensure a fair comparison across all models and see if the inference efficiency generalizes the same way across all models. Each model was compiled using TorchScript to optimize performance. Experiments are conducted on video clips from the SA-V dataset (full length, with a duration ranging from 10 to 24 seconds), with 2 Intel(R) Xeon(R) 6230 @ 2.10GHz and 480GB of RAM as CPU, and (optionally) a Tesla V100-SXM2-16GB as GPU. We evaluate the time needed for embedding and extraction in two scenarios: using only the CPU and using both the CPU and GPU (we do not consider video loading and saving times in the following).\nWe report the GFlops and time in seconds for both CPU and GPU configurations in Tab. 6. The GFlops required for embedding are consistent across models within a range of 10 to 43, while the GFlops required for extraction vary more widely from 3 to 69. In terms of GPU time, WAM is the slowest at embedding because it uses a heatmap to attenuate the watermark, which is computationally expensive at high resolution (high resolution images are never sent to the GPU to reduce memory constraints, so the compute of the heatmap is done on the CPU). The other models are much faster (around 0.5-2 seconds on CPU), but quite similar to each other. On GPU in particular, the transfer time from CPU to GPU and the CPU operations on high-resolution videos seem to be the bottleneck. For extraction, all the models are in the same ballpark."}, {"title": "4.4 Imperceptibility/Robustness trade-off", "content": "We previously reported the robustness and imperceptibility of the watermarking models at their nominal strength. In practice, one may want to adapt the strength aw to control the imperceptibility/robustness trade-off. We investigate this trade-off by varying the strength of the watermark for each model. We report in Fig. 6 the bit accuracy and log10(p) for various models, under different transformations, against the VMAF between the watermarked and the original videos. This is done on 3-seconds clips from SA-V. We observe that MBRS and TrustMark obtain higher values for - log10(p) for a good range of VMAF since they hide more bits (256 and 100 respectively). However, these methods fall short on more challenging transformations, especially when combining geometric transformations and video compression where Video Seal achieves higher robustness, in particular at very high PSNR (> 50 dB) or VMAF (> 94)."}, {"title": "5 Ablation Studies", "content": null}, {"title": "5.1 Video training", "content": "In this section, we investigate whether training a video watermarking model with frame propagation and differentiable video compression is beneficial, or if applying an image watermarking model to videos during inference is sufficient. We also investigate if it is beneficial to pre-train on images and then to continue training on a mix of images and videos. This could potentially leverage the faster training times of image-based models while adapting to video-specific transformations.\nTo test this we design three main scenarios:\n1. Image-only training, where the model is trained solely on images;\n2. Video-only training, where the model is trained exclusively on videos;\n3. Mixed training, where the model is first pre-trained on images and then further trained on a mix of images and videos using a scheduled approach.\nWhen video training is activated, we further explore two sub-cases:\nA. With all augmentations, including video compression,\nB. Without video compression augmentations.\nThis allows us to isolate the impact of video compression on the training process, as opposed to relying solely on differentiable frame propagation of the watermark. We report the mean bit accuracy over different compressions and the PSNR during training, across multiple seeds for each experiment. In this experiment, Nbits = 16 to facilitate training and focus on the impact of video training. During the video training phase, we employ a balanced schedule, alternating between images and videos with a 1:1 ratio (i.e., one epoch for images followed by one epoch for videos) from our experiments we found that this helps stabilizing the training.\nThe results, as shown in Fig. 7, reveal that the most effective combination involves pre-training on images, followed by video training with compression augmentation. This approach yields significant improvements in bit accuracy, particularly at higher compression rates. Notably, when video training commences (epoch 100 or 200) after image pre-training, the bit accuracy increases rapidly, especially for stronger compressions (CRF 40 and 50). This suggests that the incorporation of differentiable compression augmentation provides a robust optimization signal to the model. Furthermore, this improvement in robustness does not come at the cost of lower PSNR values compared to other ablations, underscoring the effectiveness of the proposed approach.\nIn contrast, video training alone without image pre-training proves ineffective, resulting in a very low bit accuracy. This highlights the importance of the mixed approach, which leverages image pre-training to initialize the network before training on videos. The scheduled training strategy employed in this study demonstrates the benefits of combining the efficiency of image-based models with the adaptability to video-specific transformations afforded by video training."}, {"title": "5.2 Extractor fine-tuning", "content": "In this section, we investigate the impact of fine-tuning the extractor of the watermark while freezing the generator as a method to break free from the trade-off between imperceptibility and robustness. We expect fine-tuning to provide additional gains in bit accuracy for some models, particularly towards augmentations that have not been seen enough during training or models that haven't achieved full convergence. To investigate this, we train multiple models with varying parameters including numbers of bits (64 and 128) and video training start epoch (200, 500, and 1000). We train all models to convergence for 1000 epochs, then freeze the generator and fine-tune the extractor for an additional 200 epochs. We then compare two scenarios:\n1. Training and fine-tuning with compression augmentations, where the models are trained on lightweight augmentations and leaving robustness to compressions to the end.\n2. Training on all augmentations, with compression augmentations left to fine-tuning time.\nThe rationale behind scenario 2. is that compression augmentations introduce instabilities in training due to the slow compression times and the small batch size needed to fit in memory. Therefore, we investigate the benefits of leaving the compression augmentations only when the embedder is frozen."}, {"title": "5.3 Video inference parameters", "content": "Step-size at embedding time. To efficiently embed the watermark in videos, we use temporal propagation presented in Sec. 2.2.2. It involves embedding the watermark every k frames, where k is the step-size, and copying the watermark distortion onto the next frames. We investigate the impact of the step-size on the watermark robustness and the speed of the embedding. We report the bit accuracy on the same combined augmentation as in Fig. 6, i.e., for an H.264 compression with CRF=30, a crop removing half of the video, and a brightness change, as well as the time taken to embed the watermark on both CPU and GPU. We observe that the step-size k does not significantly impact the watermark robustness, while greatly increasing the speed of the embedding. However, it empirically introduces shadowy or blinkering artifacts in the video. Therefore, the step-size"}]}