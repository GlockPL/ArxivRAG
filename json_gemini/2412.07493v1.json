{"title": "Ontology-driven Prompt Tuning for LLM-based Task and Motion Planning", "authors": ["Muhayy Ud Din", "Jan Rosell", "Waseem Akram", "Isiah Zaplana", "Maximo A Roa", "Lakmal Seneviratne", "Irfan Hussain"], "abstract": "Performing complex manipulation tasks in dynamic environments requires efficient Task and Motion Planning (TAMP) approaches, which combine high-level symbolic plan with low-level motion planning. Advances in Large Language Models (LLMs), such as GPT-4, are transforming task planning by offering natural language as an intuitive and flexible way to describe tasks, generate symbolic plans, and reason. However, the effectiveness of LLM-based TAMP approaches is limited due to static and template-based prompting, which struggles in adapting to dynamic environments and complex task contexts. To address these limitations, this work proposes a novel ontology-driven prompt-tuning framework that employs knowledge-based reasoning to refine and expand user prompts with task contextual reasoning and knowledge-based environment state descriptions. Integrating domain-specific knowledge into the prompt ensures semantically accurate and context-aware task plans. The proposed framework demonstrates its effectiveness by resolving semantic errors in symbolic plan generation, such as maintaining logical temporal goal ordering in scenarios involving hierarchical object placement. The proposed framework is validated through both simulation and real-world scenarios, demonstrating significant improvements over the baseline approach in terms of adaptability to dynamic environments, and the generation of semantically correct task plans.", "sections": [{"title": "I. INTRODUCTION", "content": "Task and motion planning (TAMP) is an essential component in sequential manipulation to perform tasks in complex environments. TAMP involves two stages: high-level symbolic plan generation, which develops a long-term abstract action sequence, and low-level motion planning, which determines the trajectories with geometric constraints required to execute the computed symbolic plan. The classical solution to TAMP problems consists in describing the task in a specific planning representation, such as the Planning Domain Description Language (PDDL) [1], and using a task planner, such as GraphPlan [2] or FastForward [3], to compute a symbolic plan, whose execution requires the call to a motion planner for all actions involving movements. A custom-designed communication layer is usually used to enable interaction between the task and motion planning modules.\nDescribing the task specification using PDDL for TAMP has proved very successful. However, PDDL has some drawbacks; it requires manual coding of all real-world descriptions and task constraints, which is impractical in a dynamic environment, and in addition, its syntax may be difficult for new users to understand [4]. On the other hand, compared to PDDL, natural language offers a more intuitive and user-friendly interface for task descriptions [5]. In this line, recent advances in pre-trained large language models (LLMs), such as GPT4 [6], have shown good performance in various natural language tasks, and this has accelerated research on the use of LLMs in task planning [7] and TAMP [8] [9].\nLLM-based methods for TAMP typically use pre-trained LLM models, such as GPT4, that receive task descriptions as input [9]. The LLM module is responsible for computing symbolic plans and reasoning for failures, showing significant capabilities when in-context prompting is provided [10]. However, a weak prompt could drastically reduce planning and reasoning capabilities, and hence several recent studies proposed different ways of prompt engineering for better task elaboration. For example, a template-based prompt is proposed in [9], where fixed templates with variables are used to describe spatial and geometric relations, or the proposal in [8], where a template is used to describe the scene and another to textualize the state of the environment. And to better exploit natural language, an interactive task planning using LLM was proposed in [11], where the user input was integrated with stored task guidelines to generate the prompt.\nMost of the above-stated prompt generation methods rely on fixed templates. This can result in static and repetitive prompts that may not effectively capture the dynamic nature of the environment, i.e., these approaches may not generalize well to new or unexpected situations, thus leading to suboptimal prompts when the task context changes. In addition, fixed templates may not capture the detailed context of a task, as they often oversimplify the complex states of the environment. This might result in incomplete or misleading prompts, which may lead to misunderstandings in task planning. Therefore, since effective prompting methods are crucial to improve LLM performance [12], robust and refined prompt engineering techniques are needed to support the elaboration of accurate task plans.\nTo address the above-stated challenges, this study proposes an ontology-driven prompt-tuning approach that utilizes knowledge-based reasoning to refine and expand the user's prompt within the task context. Building on the baseline LLM3 TAMP framework [9], we extend it with our novel prompt-tuning method. Some other approaches, such as [13], have also proposed to integrate ontological knowledge with LLM-based task planning, but their primary focus is on objects properties to define optimal grasping strategies (e.g., glass \u2192 pick gently, wood \u2192 pick stably). On the contrary, we employ knowledge-based reasoning for prompt tuning to refine the action sequence for correct temporal goal ordering, integrating it with the user's prompt input for the LLM. This ensures that the generated plan is not only correct, but also semantically accurate. For example, consider the scenario depicted in Fig. 1, where the user instructs to \"put the banana, apple, and bowl in the plate\". In most cases, the symbolic baseline planner generates a sequence like \"pick banana and apple, and place in the plate\" followed by \"pick bowl and place in the plate\". Although it is syntactically correct, it is semantically flawed, as it could result in placing the bowl on top of the apple and banana. Our proposed prompt-tuning approach addresses this by incorporating domain-specific knowledge. It integrates a guiding note into the prompt, specifying that \"put bowl before food items because crockery has priority over food items. Put banana and apple after crockery because food items have less priority.\" This extended prompt will help the LLM generate a semantically correct and consistent plan.\nThe core contribution of this work is a novel ontology-driven prompt-tuning framework for task and motion planning that helps LLM to compute semantically accurate plans. Building upon this core framework, the following contributions are highlighted:"}, {"title": "Automated prompt-tuning with task contextual reasoning:", "content": "This approach extracts task-related common sense and procedural knowledge from the ontology to guide the large language model (LLM) to generate a semantically accurate and logically ordered plan."}, {"title": "Knowledge-based environment state description:", "content": "We utilize ontological knowledge to construct detailed, context-aware descriptions of the environment, enhancing the prompt tuning process for more effective task planning."}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Task and Motion Planning", "content": "TAMP uses a structured approach where task planning and motion planning are handled separately but in a coordinated manner. Task planning typically uses symbolic AI techniques, such as hierarchical planning, to generate a sequence of discrete actions based on predefined rules or logic. Motion planning uses geometric algorithms, such as sampling-based planners, to compute feasible paths [14] [15]. A declarative language, such as PDDL, is used for TAMP. However, its use is limited due to the requirement of a large number of parameters to define the problem, actions, and effects [16]. Moreover, as the action space expands, finding geometrically feasible symbolic action sequences becomes computationally challenging without effective heuristics [17]. Recent studies have explored data-driven heuristics to enhance TAMP efficiency [18] [19]. Other approaches use knowledge-based reasoning to reduce the action space, using domain-specific ontological knowledge, and then apply heuristics for efficient symbolic plan generation [20] [21] [22]. The approach proposed here is a general manipulation framework to work across various domains, with the specific application ontology providing the specialized knowledge needed to correctly solve the task."}, {"title": "B. Manipulation planning with LLMs", "content": "Large Language Models (LLMs) are increasingly being explored to enhance TAMP by using their ability to understand and generate human-like instructions, descriptions, and planning sequences, i.e., LLMs can be used to translate natural language task descriptions into formal task representations without the need of manually representing the problem domain [23], [24]. Due to their extensive language understanding and contextual reasoning capabilities, LLMs are also used to generate high-level task plans and their translation into feasible motion sequences with minimal additional training [25], [26]. For instance, LLM-GROP [8] utilizes a pre-trained LLM to define symbolic goals for object placement and rearrangement, integrating these with traditional TAMP methods, and Auto-TAMP [4] employs a pre-trained LLM to convert user inputs into formal language, which is then used in a TAMP algorithm.\nIn all the approaches mentioned above, effective prompting techniques are crucial to optimize LLM performance [12]. In addition, [27] identified key challenges in prompting large language models (LLMs) for robotic manipulation, including the need for detailed and accurate problem descriptions, as well as managing biases that shape response structures. This study tackles the issue of effective prompting for task and motion planning by utilizing ontological knowledge to enhance task elaboration for LLM."}, {"title": "III. PROBLEM FORMULATION", "content": "This section formally defines the task and motion planning problem and how we model the problem for ontology-driven LLM-TAMP."}, {"title": "A. Task and Motion Planning", "content": "A task and motion planning problem is typically described as a tuple X = (O, S, A, T, so, g), where:\n\u2022 O denotes the set of objects in the environment.\n\u2022 S represents the state space of all possible configurations of objects within the environment, where st \u2208 S signifies the state of the objects at time t.\n\u2022 A is the set of primitive actions, each one requiring the call to the motion planner for its execution. A primitive action a \u2208 A is defined by a set of objects o \u2208 O and a set of continuous parameters y, e.g., Place (cup, [x, y, z, 0]). The parameters y serve as goals for the motion planner, and a is considered feasible if the motion planner finds a solution, i.e., a collision-free trajectory \u0442. We denote such a feasible action as a(\u03c4).\n\u2022 T is the state transition function that generates the subsequent state st+1 after executing an action at(Tt) at state st, formally represented as st+1 = T(st, at(Tt)). This transition function can be evaluated with a black-box simulator.\n\u2022 so is the initial state, with so \u2208 S.\n\u2022 g is the goal function, defined as g(s) : S \u2192 {0,1}, which checks if the task goal has been met at a given state s.\nThe objective in Task and Motion Planning is to derive a sequence of feasible actions \u03b1\u03bf(\u03c4\u03bf), \u03b11(T1), ..., \u0430\u0442(\u0442\u0442), such that g(sT+1) = 1 and St+1 = T(St, at(Tt)), for each t = 0,1,..., \u03a4."}, {"title": "B. Problem Modeling", "content": "The ontology-driven LLM-TAMP framework is defined as a tuple (P, K, \u03be, X, L), where each component plays a crucial role in structuring a feasible action plan:\n\u2022 P represents the prompt, containing essential information such as the task description, environment state, and details about the objects involved.\n\u2022 K denotes the ontological knowledge base, which provides domain-specific information and task-related rules.\n\u2022 \u03be is the knowledge reasoner, responsible for enriching the prompt with task-specific correctness knowledge:\n$\\xi :P \\times K\\rightarrow \\epsilon$  (1)\nwhere \u20ac represents additional guidance relevant to the task.\n\u2022 X is the set of task parameters.\n\u2022 L is the LLM model, such as GPT-4, that operates as a black-box component, defined by:\n$L: P \\times \\epsilon \\times X \\rightarrow a$ (2)\nThis model receives the prompt P, enhanced task knowledge \u20ac, and TAMP parameters X, and it outputs a set of feasible actions a CA. The generated action sequence is expressed as \u03b1 = {\u03b1\u03bf(\u03c4\u03bf), \u03b11(T1),...,\u0430\u0442(\u315c\u315c)}, where each action at with trajectory Tt enables the robot to transition from the initial environment state to the desired goal state.\nThe effectiveness of L depends significantly on the quality of the prompt, since an elaborate and semantically accurate prompt improves the probability of generating a valid sequence of actions. The primary objective of this work is to explore how ontological knowledge and semantic reasoning can further refine and enrich prompts, thereby enhancing the accuracy of semantically correct action sequence generation for consistent task execution."}, {"title": "IV. PROPOSED APPROACH", "content": "This section explains how domain-specific ontological knowledge is used to generate suggestions for the LLM, ensuring semantically correct symbolic plan generation with proper temporal goal ordering. The architecture of our prompt-tuning framework is illustrated in Fig. 2. It is composed of four core modules: the Prompt Construction Layer, Ontological Prompt Tuning Module, Perception Module, and LLM-TAMP Module.\nThe Prompt Construction Layer comprises three submod-ules: 1) the User Input which describes the task to be performed, 2) the Prompt Template which contains the static sections of the prompt, provides general task descriptions and structure the overall prompt\u00b9, and 3) the Environmental State Descriptor that translates the environmental state into a text format, describing the spatial layout and types of objects in the environment. The subsections below will explain the framework's module in detail."}, {"title": "A. Application ontology: classes, properties and rules", "content": "We developed a simple kitchen domain ontology to provide semantic reasoning for performing tasks in a kitchen environment. This domain-specific ontology enhances reasoning about task constraints, priority rules and object relations, and for this, three main classes have been defined: Task, ActionPriority and Object. The Object class is further subdivided into six subclasses describing different kitchen objects types: FoodItems, BoxedFood, Crockery, Utensil, KitchenItems, and Container. The hierarchy of classes, object properties, data properties, and some example individuals are shown in Fig. 3.\nThe data properties of the ontology describe the essential attributes of different objects, including action priority values and specific descriptions with temporal constraints, such as pick after FoodItem or place Crockery before FoodItem. These descriptions are utilized by the reasoner for prompt tuning, enabling it to guide action sequencing. Other data properties capture object-specific attributes like position, orientation, bounding box, and additional task-related properties. Object properties, on the other hand, define relationships between objects and actions. For instance, has object type specifies the classification of an object (e.g., FoodItem or Crockery), while has action associates relevant actions, such as \u201cput,\u201d \u201cclean\u201d or \u201carrange\u201d with each object. These properties are essential for knowledge-based reasoning about task execution.\nWe define a set of rules\u00b2 that specify the correct order of action sequences when manipulating objects in the kitchen environment. These rules are implemented as instances of ActionPriority, where each rule associates a task (such as put or clean) with an object type (such as FoodItem or Crockery) and assigns a priority level. Through these rules, the reasoning mechanism infers a semantically correct action sequence for task execution. For example, Eq. (3) and (4) show the description logic representation for two rules, named Rule 1 and Rule 2, that state the priorities for the put task: Rule 1 states that the crockery has higher priority and should be placed first, and Rule 2 states that food items have lower priority and should be placed after the crockery.\nRule1 Action Priority hasAction.Put\nhasObjectType.Crockery\nhasPriority.1  (3)\nRule2 Action Priority \u2203hasAction.Put\n\u2203hasObjectType.FoodItem\u2203hasPriority.2 (4)"}, {"title": "B. Semantic Tagging", "content": "The semantic tagging process and the identification of tasks and objects from the user input are key components of the proposed system. We use the SpaCy library (https://spacy.io/), which provides advanced capabilities for part-of-speech (POS) tagging and dependency parsing. It takes user input in the form of natural language and extracts the key tasks (verbs) and objects (nouns) by analyzing the grammatical structure of the sentence.\nThe user input string is first tokenized into individual words using SpaCy's tokenization. Each token is annotated with a POS tag that identifies its grammatical role, such as a verb, noun, or preposition. The system iterates over the tokens and identifies the verbs using their POS tag. The task is then extracted by selecting verbs that correspond to a predefined set of valid tasks. We assume that the set of tasks that a robot can perform are: tasks = {\"clean\", \"arrange\", \"put\", \"serve\", \"stack\"}.\nFormally, the input sentence can be represented as a sequence of tokens, i.e., {t1, t2, ..., tn }. Each token t\u1d62 is assigned a part-of-speech tag p\u1d62, where p\u1d62 \u2208 {VERB, NOUN, ADJ, ...}. The system identifies a task a such that: a = t\u1d62 if P\u1d62 = VERB and t\u1d62 \u2208tasks.\nOnce the task is identified, the next step is to extract the objects of the task. We use SpaCy's dependency parsing to identify the direct objects (dobj), prepositional objects (pobj), and conjunct objects (conj). These dependencies are used to capture the relationships between the verbs and their corresponding objects. If tj represents a token with POS tag NOUN, then tj is extracted as an object if it satisfies the condition: Dependency(tj) \u2208 {dobj,pobj,conj}\nIn addition to processing single-word objects, the system is designed to handle compound nouns connected by underscores (e.g., green_cup) or adjectives. In the case of compound nouns, the dependency tag compound is used to combine modifiers with the noun. The system ensures that compound words are grouped together, using an underscore (_) to create a single object label (e.g., green_cup). Formally, if a noun tk has a compound modifier tm, then the object O is given by: O = {tm_tk}, where Dependency(tm) = compound."}, {"title": "C. Contextual Inference Module", "content": "Once the task and the list of objects are identified, as explained in Sec. IV-B, the inference module applies a set of SPARQL queries to the ontological knowledge explained in Sec. IV-A. These queries are designed to generate textual descriptions that facilitate the LLM in computing a semantically correct task plan. The ontology is represented in RDF (Resource Description Framework) format, which provides a standard way of encoding information about objects, actions, and relationships. Using the rdflib library(https://rdflib.readthedocs.io/en/stable/), we load the ontology as RDF graphs, which allows us to perform SPARQL queries to access and extract specific knowledge that is essential for the task-planning process. These queries retrieve information about objects, their classifications, and prioritized actions. By executing these queries, the system formulates natural language prompts and then passes to the LLM alongside user input to ensure that the robot executes actions in a logically and semantically accurate sequence.\nThe first set of SPARQL queries identifies the types of objects referenced in a user command. This classification is essential, as different objects have specific priorities depending on their type and the task at hand. For example, the query below identifies whether an object, such as an \"apple\" or \"plate,\" is categorized as a FoodItem, Crockery, or Container.\nPREFIX ex:\nhttp://www.example.org/kitchen_ontology#\nSELECT ?type WHERE {\n?obj a ?type ;\nrdfs:label ?label .\nFILTER (lcase(str(?label))\n=\n\"object_name.lower()\")\nFILTER (?type !=\nowl:NamedIndividual) }\nThis query is executed by iterating over items identified in the user's input, enabling the system to classify each item and prepare for a task-specific prioritization. The returned object types inform subsequent queries about priorities and relationships, ensuring that the instructions provided to the LLM are contextually accurate.\nOnce object types are identified, the inference module executes another set of queries to retrieve predefined task descriptions and priorities from the ontology. These descriptions specify the correct order for performing actions based on the object type and task type (e.g., \u201cput,\u201d \u201cclean,\u201d or \u201carrange\u201d). For example, a query to retrieve the priority for any particular action is as follows:\nPREFIX ex:\nhttp://www.example.org/kitchen_ontology#\nSELECT ?priority ?description WHERE {\n?rule rdf:type ex:ActionPriority;\nex:hasAction \"action\" ;\nex:hasObjectType \"object_type\";\nex:hasPriority ?priority ;\nex:hasDescription ? description\n}\nThis query fetches the action priority and a natural language description for each object, allowing the reasoner to compile a comprehensive instruction set. For example, if a Crockery item like a \"plate\" has a higher priority than a FoodItem like an \u201capple\u201d for the \u201cput\u201d action, the system will generate a prompt indicating that the plate should be placed before the apple. This description is adapted to align with the task type, enhancing the LLM's ability to generate a structured and semantically correct plan."}, {"title": "D. Perception and Environmental State State Descriptor", "content": "The perception module and the environment state descriptor are responsible for detecting and identifying objects in the environment and generating a textual description of the scene based on the types of objects and their spatial relationships. The perception module uses a YOLO-based object detection model and NVIDIA's FoundationPose library, for object recognition and precise pose estimation, respectively. The environment state descriptor applies an ontology-based semantic classification, that allows the system to produce structured, detailed descriptions of the environment, which is useful for understanding the environment state.\nThe perception module begins with YOLO, which detects objects in the input image. Each detected object is labeled with a class name, such as \u201capple\u201d or \u201ccup\", based on the YOLO that is fine-tuned on YCB dataset, allowing the system to associate each detected object with its specific label. Once objects are detected and labeled, the same image along with point cloud data is processed by the FoundationPose library to compute the precise 3D pose of each object. The environment state descriptor uses the object names and spatial attributes determined by the perception module and applies a set of SPARQL queries on the ontological knowledge to classify each detected object (such as apple and plate as a FoodItem and Crockery) and retrieve relevant geometric information such as object bounding box, object length, and width.\nAll extracted information is compiled into a textual description that combines both the object's class type and its computed spatial data. For instance, after processing the object \"apple\" the system generates a description indicating the \"apple\" is a \"FoodItem\" located at position [x,y,z] and orientation [yaw] with a bounding box spanning from [xmin, ymin] to [xmax, ymax] and dimensions of l meters in length and w meters in width. This is done in a similar way as well for all the other detected objects. This textualized environment state description is then fed to the prompt generator that develops the final prompt for the LLM task planner."}, {"title": "E. LLM Task and Motion Planner", "content": "The LLM-Task planner takes a generated prompt as input and formulates a symbolic plan to be executed by the robotic system. The symbolic plan generation closely follows the methodology used in the baseline approach [9]. For instance, in a user input prompt such as \"put the apple and plate on the tray,\u201d the actual final output prompt feed to the LLM-Task Planner after tuning will be the one shown in Fig. 2 inside the Prompt Generator box, where the yellow text comes from the Contextual Inference Engine and the blue text form the Environment State Descriptor. This prompt will be interpreted by the LLM that generates the following symbolic plan:\nFull Plan ="}, {"title": "V. RESULTS AND DISCUSSION", "content": "The proposed ontology-driven LLM-TAMP framework is validated through simulations as shown in Fig. 4 and real-world experiments depicted in Fig 5, ensuring its applicability and robustness in different task scenarios. We utilized a PyBullet-based simulation environment to test the scenarios. In the first scenario (Fig. 4-A), the objective was to organize items by placing an apple, a banana, and a bowl onto a plate. In the second scenario (Fig. 4-B), the task involved cleaning a table by moving objects. Crockery items such as a cup and a plate are to be moved to the left_table, while boxed food items, including a cracker_box, sugar_box, and tomato_can, to the right_table. These scenarios were designed to evaluate the system's ability to handle complex object arrangements where the correct temporal order of the subtasks is crucial for performing the task semantically correctly.\nWe conducted a comprehensive evaluation to compare the proposed framework with the baseline approach. The results presented in Table I highlight the importance of the integration of ontology-based reasoning into LLM-TAMP systems. The parameters we compare are task planning success rate (TPSR), execution success rate (EXESR), and the number of LLM calls (# CALLs). The ontology-driven approach consistently outperforms the baseline LLM-TAMP, particularly in complex scenarios that require refined reasoning and object categorization.\nFor simple tasks where the task is clearly represented in a proper order of objects, such as Put bowl, banana, and apple in plate, the ontology-driven LLM-TAMP and the traditional LLM-TAMP achieve the same TPSR and EXESR of 100%, requiring only a few LLM calls on average. In the same task, if the order of the objects is inverted, the performance of the traditional LLM-TAMP is reduced with the TPSR and EXESR of 60% and a high number of LLM calls, 8.6 on average. Whereas ontology-driven LLM-TAMP maintains the same TPSR, EXESR, and lower call to LLM, 2.6 on average. The reason for better results is that the ontology-driven approach uses knowledge-based reasoning to explicitly specify the correct sequence of object manipulation for LLM.\nIn contrast, the traditional LLM-TAMP approach leaves it to LLM to decide which considers the similar order to manipulate objects as described in the input prompt, which is semantically incorrect, as illustrated in Fig. 6 (put apple and banana first, then place bowl on them). These results confirm the framework's capability to enhance task and motion planning using knowledge-based reasoning for improved prompt generation and execution accuracy.\nIn more complex scenarios, but with proper instruction and proper order of objects, such as Clean table, move sugar_box, tomato_can, and cracker_box to the left table, move plate and cup to the right table, the LLM-TAMP and the ontology-driven approach maintain a TPSR of 100% and EXESR of 90% while requiring similar number of calls to LLM. However, in the task if the prompt is given as Clean table, move plate and cup to the right table, move sugar_box, tomato_can, and cracker_box to the left table, the performance of LLM-TAMP is significantly reduced. At the same time, the ontology-driven LLM-TAMP maintains a performance similar to that of the first case. If we provide the prompt in a more generic way, such as Clean table, move crockery items to the correct table, move the boxed food items to the left table the LLM-TAMP fails (0% TPSR and EXESR). The ontology-driven system achieves modest success (50% TPSR and 50% EXESR). The reason for the ontology- driven LLM-TAMP to success is the environment state description using ontological knowledge in which the type of each object is specified by querying on the ontological knowledge (such as cracker_box is a boxed food and plate is a crockery item). This reflects that even in the case of under-specified or ambiguous prompts, where additional context is required, the ontology-driven LLM-TAMP is still able to achieve some success.\nIn general, ontology-driven enhancements demonstrate improved planning success rates and efficiency across all but the most ambiguous scenarios. Using structured domain knowledge, the system reduces reliance on LLM calls and achieves more consistent and interpretable results, leading to more reliable task planning and execution in complex environments.\nQuerying the ontology for prompt tuning required additional time to construct the final prompt for the LLM. To evaluate computational efficiency, we compared the average time taken by LLM-TAMP and ontology-driven LLM-TAMP for prompt construction (Fig. 7). In addition, we analyzed their corresponding task planning success rates. As illustrated in Fig. 7, although ontology-driven LLM-TAMP requires more time for prompt generation compared to standard LLM-TAMP, it consistently outperforms the latter in generating semantically correct symbolic plans. This shows that the improved quality of planning justifies the additional computational overhead in most cases."}, {"title": "VI. CONCLUSION", "content": "This work introduced a novel ontology-driven prompt-tuning framework to enhance task and motion planning with large language models. Integrating knowledge-based reasoning and ontology-driven environment state descriptions, the framework dynamically refines user prompts to generate semantically accurate and context-aware symbolic plans. Unlike static, template-based approaches, it addresses flaws such as incorrect temporal goal ordering, and adapts effectively to dynamic environments. Validation in both simulation and real-world scenarios demonstrated significant improvements in planning accuracy and adaptability, highlighting the potential of combining LLMs with ontological reasoning for advanced robotic planning in complex tasks and dynamic contexts."}]}