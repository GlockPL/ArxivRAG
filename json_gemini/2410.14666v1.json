{"title": "DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph", "authors": ["Maitreya Prafulla Chitale", "Uday Bindal", "Rajakrishnan Rajkumar", "Rahul Mishra"], "abstract": "Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the \"lost in the middle\" issue. To address these challenges, we introduce DiscoGraMS, a novel resource that represents movie scripts as a movie character-aware discourse graph (CaD Graph). This approach is well-suited for various downstream tasks, such as summarization, question-answering, and salience detection. The model aims to preserve all salient information, offering a more comprehensive and faithful representation of the screenplay's content. We further explore a baseline method that combines the CaD Graph with the corresponding movie script through a late fusion of graph and text modalities, and we present very initial promising results.", "sections": [{"title": "1 Introduction", "content": "Text summarization has been extensively studied within the NLP community (Nallapati et al., 2016, 2017; Zheng and Lapata, 2019; Urlana et al., 2024). Recently, large language models (LLMs) have demonstrated human-level performance in this area (Liu et al., 2023; Zhang et al., 2024). However, summarizing long documents remains a challenge for even the most advanced LLMs, as their effectiveness can be influenced by the location of salient information within the text (Liu et al., 2024). For language models to effectively utilize information within very long input documents, their performance should exhibit minimal sensitivity to the positional placement of relevant information within the input (Liu et al., 2024). Movie script or screenplay summarization (Papalampidi et al., 2020; Saxena and Keller, 2024) is a relatively hard task compared to standard document summarization due multitude of reasons. Movie scripts are typically very long documents characterized by intricate narratives, numerous subplots, and substantial dialogue, which pose significant challenges for summarizing the content without losing the core elements of the story. Many of the movie scripts have non-linear flow of events such as flashbacks, flash-forwards, and parallel plot lines, making the summary to retain the coherence and original flow.\nTo address this, we present DiscoGraMS, an innovative resource that represents movie scripts as a character-aware discourse graph (CaD Graph). This graph captures the core essence of the movie plot by modeling latent relationships among key elements, including characters, the scenes they participate in, and the dialogues they deliver, thereby highlighting all possible semantically important aspects of the narrative. The CaD Graph captures intricate"}, {"title": "2 Related Work", "content": "Since the origin of modern graph theory in 1736 with Euler's proof the Seven Bridges of K\u00f6nigsberg problem i.e. traversing a city crossing 7 bridges exactly once (Harary, 1960), graph representations have been used to model data in diverse fields like chemistry, biology and computer science. Linguistic data has also been represented as graph structures like dependency representations (Tesni\u00e8re, 1959) and successfully deployed in NLP applications. The idea of representing entire texts as graphs was proposed in seminal work by Mihalcea and Tarau (2004). They created graphs comprising of nodes which keywords connected to other words located within a window of 2 to 10 words. This approach was extremely effective for the task of extractive summarization. More recently, Wang et al. (2022) show the efficacy of this technique for abstractive summarization of scientific articles. Here, entities in the text served as nodes (with co-referential entity clusters represented as a single node) connected to one another via labelled edges depicting relationships (like hyponymy) between nodes. There have been no significant efforts to employ graphs for movie script summarization. Only recently, (Saxena and Keller, 2024) adapted TextRank (Zheng and Lapata, 2019), a sentence centrality-based graph approach, for movie scripts. However, this approach was outperformed by the simpler Longformer Encoder-Decoder (LED) model (Beltagy et al., 2020) by large margin."}, {"title": "3 Dataset", "content": "We use the MovieSum (Saxena and Keller, 2024) dataset, a comprehensive resource for movie summarization, containing 2,200 movie screenplays along with metadata and plot summaries, including movies up to 2023. The plot summaries are sourced from IMDb and Wikipedia, ensuring a diverse range of writing styles and perspectives. The summaries were generated through a combination of automatic extraction and manual curation by trained annotators. The scripts are in XML format, preserving key elements such as scene descriptions, dialogues, and character names for efficient analysis. The dataset is split into training (1,800 movies), validation (200 movies), and test (200 movies) sets, with average screenplay lengths of 29k words and summaries of 717 words. The summaries, sourced from IMDb and Wikipedia, blend automatic extraction and manual curation. Analysis reveals a high level of abstractiveness in the summaries, indicated by novel 3-grams and 4-grams not found in the original scripts."}, {"title": "4 Methodology", "content": "In this section, we describe the process of constructing the character-aware discourse graph (CaD Graph) from movie scripts. We then present a baseline method that leverages both the CaD Graph and the textual content of the scripts, using a late modality fusion approach to generate movie script summaries."}, {"title": "4.1 Graph Construction and Encoding:", "content": "The first step involves constructing a graph representation of the movie script. In this representation, nodes are created for key elements, which are scenes, characters, and their dialogues.\nThe constructed graph can be described as a heterogeneous graph $G = (V, E)$, where V is the set of nodes, and E is the set of edges. There are three types of nodes, scenes ($V_s$), dialogues ($V_d$), and characters ($V_c$). The edges represent different relationships, $E_{ss} \\subseteq V_s \\times V_s$: Edges between consecutive scenes, $E_{sd} \\subseteq V_s \\times V_d$:\nEdges between scenes and dialogues occurring in those scenes, $E_{sc} \\subseteq V_s \\times V_c$: Edges between scenes and characters appearing in those scenes,\n$E_{cd} \\subseteq V_c \\times V_d$: Edges between characters and\ndialogues spoken by those characters. Formally, the graph construction is written as follows:\n$G = (V_s \\cup V_d \\cup V_c, E_{ss} \\cup E_{sd} \\cup E_{sc} \\cup E_{cd})$\nScene Nodes: $V_s = \\{ s_i | s_i \\text{ is a scene} \\}$\nEach scene node $s_i$ has an associated embedding $e(s_i)$ representing the scene description text, derived from the sentence embedding model (SE) (Reimers and Gurevych, 2019):\n$e(s_i) = SE(\\text{Scene Description}(s_i))$ The scenes\nlist is ordered according to the order in which the scenes occur in the movie.\nDialogue Nodes: $V_d = \\{ d_j | d_j \\text{ is a dialogue} \\}$\nEach dialogue node $d_j$ has an associated embedding $e(d_j)$, representing the dialogue text:\n$e(d_j) = SE(\\text{Dialogue Text}(d_j))$\nCharacter Nodes: $V_c = \\{ C_k | C_k \\text{ is a character} \\}$\nThe characters are initialised with zero embedding whose dimension matches with the embedding dimension of the sentence encoder.\nEdges: The edges between the scenes\nand other entities are defined as follows:\nthe scene-to-scene edges are given by\n$E_{ss} = \\{ (s_i, s_{i+1}) | s_i, s_{i+1} \\in V_s \\}$; the\nscene-to-dialogue edges are defined as $E_{sd} =$\n$\\{ (s_i, d_j) | d_j \\in V_d, s_i \\in V_s, d_j \\text{ occurs in scene } s_i \\}$;\nthe scene-to-character edges are defined as $E_{sc} =$\n$\\{ (s_i, C_k) | C_k \\in V_c, s_i \\in V_s, C_k \\text{ appears in scene } s_i \\}$;\nand finally, the character-to-dialogue\nedges are given by $E_{cd} =$\n$\\{ (C_k, d_j) | C_k \\in V_c, d_j \\in V_d, d_j \\text{ is spoken by } C_k \\}$.\nA movie's CaD Graph consists of intricate connections that represent the three-way relationships between scenes, characters, and dialogues, as illustrated in Figure 1. Adding sequential links between scenes helps the model capture the movie's overall flow. The connections from scenes to characters and dialogues to characters enable the model to differentiate between characters and understand their roles. We hypothesize that this structure also helps the model infer a character's significance within the movie, making our graphs, DiscoGraMS, character-aware."}, {"title": "4.2 The Proposed Model LGAT", "content": "We propose a novel late fusion-based model, LGAT, which integrates the CaD Graph and the textual content of movie scripts through a Graph Neural Network (GNN) using graph attention with convolutions and a Longformer Encoder-Decoder (LED) (Beltagy et al., 2020) text encoder, as illustrated in Fig 2. This combination generates the script's encoding, followed by a decoder that produces the"}, {"title": "5 Results", "content": "We select the models LongT5 (Guo et al., 2022), PEGASUS-X (Phang et al., 2023), and the Longformer Encoder-Decoder (LED) model (Beltagy et al., 2020), (See Table 2) as the baselines (inspiration for baselines are drawm from (Saxena and Keller, 2024)) to compare with our proposed model.\nAs presented in Table 2, our proposed model, LGAT, significantly outperforms all baseline models on both ROUGE and BERT score metrics. This improvement can be attributed to the cues and patterns provided by the CaD Graph, which capture the overall essence of the movie plot. However, we observe that for the ROUGE-L metric, LGAT does not surpass the LED baseline, likely due to the smaller context window used in our encoder (4K vs. 16K)."}, {"title": "5.1 Ablation Studies", "content": "The LE architecture, along with GATConv, has proven to be suited for processing long sequences. Following this, we run ablation studies on LGAT to prove the effectiveness of our proposed architecture of combining GATConv and LE. Specifically, we train both the encoders decoupled and test them on the test set. We compare the results against the full model (LGAT) to prove the effectiveness of the individual parts of the architecture, and hence show how they individually contribute towards the final result. We observe that GNN-based CAD graph encoding is very useful andcontributing more than LED-based textual encoder."}, {"title": "6 Discussion and Conclusions", "content": "Our experiments on abstractive summarization of movie screenplays (i.e., the process of generating a plot summary given a screenplay) show that representing screenplays as graphs consisting of scenes, dialogues and characters hold a lot of promise for movie summarization. Our approach outperforms quantitative results (except R-L) reported in prior work on movie summarization using the same dataset (Saxena and Keller, 2024). We attribute the better performance of our system to the presence of richer graphs, and encoding schemes. Specifically, we attribute the phenomenal improvement in BERT Score to the introduction of an attention layer to combine the encodings of the chunks as discussed in Section A.1 and the novel CaD Graph which enables the model to easily retain salient information which is validated by the high BERT Scores. We suspect that the low scores obtained in R-L are mainly due to the lower context size model (LED 4K) due to a restriction on the available compute resources. The model's (LED) low performance in isolation validates our believes. Our results indicate that knowledge-based representations of the text and plot structure help deep learning algorithms.\nWe expect our approach to have implications for other NLP problems like Question-Answering, Genre Identification, and Saliency Detection. (Xu et al., 2024) propose a system to represent narrative text consisting of passages as nodes connected by edges encoding cognitive relations between them. In addition to mainstream engineering applications, our graph representations can be deployed in scientific studies of inferencing processes in narrative comprehension by humans."}, {"title": "Limitations", "content": "Our graphs are devoid of co-reference resolution strategies which can take insights from the referred characters and add crucial information about the movie plot. In addition to this, we were inhibited by our lack of compute resources, due to which we were not able to load the LED 16K model to encode movie scripts. This lack of compute resources also limited our choice of architecture_dim which is capped at 4K. This constraint potentially impacts the Rouge-L scores, resulting in lower performance. We were unable to conduct graph ablations (specifically, the removal of character and dialogue nodes) to evaluate their individual contributions to the model's performance. In future work, we plan to address these."}, {"title": "Ethics Statement", "content": "Dataset: Even though metadata and summaries of each movie are sourced from public domains (wikipedia, imdb), privacy and copyright considerations have been respected. Care has been taken so no sensitive or personally identifiable information is included. The movie scripts may reflect bias to particular genres or cultural context which may affect the behavior of the model.\nLanguage Models: The paper includes the usage of pre-trained language models for the task of generating embeddings (section 4). These models are susceptible to biases inherent in their training data. As a result, any summaries produced from our model should be subject to manual review before being released."}, {"title": "A Details of the Proposed Model", "content": "The constructed CAD graph is subsequently encoded using a Graph Attention Network (GATConv in PyTorch Geometric) (Veli\u010dkovi\u0107 et al., 2018). This encoding process helps in capturing complex relationships and contextual information inherent in the graph structure. The resulting graph embeddings provide a rich representation of not only the interconnections among scenes, characters, and dialogues, but also the information contained within the scenes, and dialogues.\nThe choice of a GATConv was made by keeping in mind that not all scenes, dialogues, or characters, are equally important and should be included in the summary. Thus, a convolution method which attends differently to different nodes was an ideal choice for this."}, {"title": "A.1 Movie Script Encoding:", "content": "We employ the longformer encoder to generate embeddings for the textual content of the movie script.\nFirst, the entire script is divided into chunks, with each chunk sized according to the maximum input length the encoder can process.\nEach chunk is then passed through the encoder, producing an encoding of shape [chunk_size, max_tokens, encoding_dim], where encoding_dim refers to the dimensionality of the encoder.\nFinally, these embeddings are transformed into a single embedding of shape [1, architecture_dim] via a multi-headed self-attention layer (Vaswani et al., 2023). Here, architecture_dim is a hyper-parameter, as described in Appendix C, it also represents the final embedding dimension for the movie.\nWe hypothesize that by applying multi-headed self-attention, the resulting compressed embedding will effectively capture the most relevant parts of the movie for the purpose of summarization."}, {"title": "A.2 Encoding Integration:", "content": "After obtaining the encodings from both the Graph Encoder Model and the Text Encoder Model, we perform a concatenation of these representations and then pass it through another multi-headed self-attention layer. This integration facilitates"}, {"title": "A.3 Decoding", "content": "We use the standard Transformer Decoder architecture described in (Vaswani et al., 2023) as the decoding architecture to facilitate the generation of movie summaries from the learned embeddings. The details of implementation of this decoder can be found in the Appendix C."}, {"title": "B Results and Findings", "content": "In this section, we provide the detailed results obtained during our experiments with DiscoGraMS."}, {"title": "B.1 Evaluation Metrics", "content": "To assess the performance of our proposed models in generating summaries, we employ two widely recognized evaluation metrics: ROUGE and BERT Scores. These metrics provide valuable insights into the quality and effectiveness of the generated summaries in comparison to the reference (gold) summaries. More details about the evaluation metrics can be found in Appendix E"}, {"title": "C Implementation Details", "content": "We used a single NVIDIA RTX 6000 with 50 GB VRAM to train and test our model. The VRAM of the GPU was not enough to load models with higher context size than 4K. 20 Epochs on the train set take 42 hours to complete, while testing on all 20 epochs takes another 4 hours. The hyperparameters used while training are as follows:\nNumber of Epochs: 20\nLearning Rate: 0.00001\nArchitecture Dimension: 4096\nSentence Encoder (SE) Dimension: 768\nLongformer Encoder (LE) Dimension: 1024\nDropout in Attention Layer of Encoder: 0.15\nNumber of heads in Encoder side Attention: 8\nDropout in Attention of Encoding Integration: 0.15\nNumber of heads in Attention of Encoding Integration: 8\nDecoder Number of Heads: 8\nDecoder Heads: 6\nInternal Dimension of Decoder: 8192\nMax Sequence Length of the Decoder: 2284"}, {"title": "D Example of a CaD Graphs from the Dataset.", "content": "In this section, we provide real graphs that we obtain from the dataset used. We visualise these graphs with the help of gephi . Through these examples, we aim to demonstrate our effective character-aware graph construction method and how it helps the model identify the salient characters in the network and the roles that they play. This can be observed by the high density of edges around pivotal characters in the movie. Naturally (or by design), the model will tend to give more importance to these nodes and their connected nodes, deeming them to salient.\nExample graph of the movie 8MM from 1999 can be seen in Figure 3\nExample graph of the movie The Iron Lady from 2011 can be seen in Figure 4\nExample graph of the movie Adventureland from 2009 can be seen in Figure 5"}, {"title": "E Evaluation Metrics", "content": "E.1 ROUGE Scores\nROUGE (Recall-Oriented Understudy for Gisting Evaluation) Scores (Lin, 2004) are a set of metrics used to evaluate automatic summarization and machine translation by comparing the overlap of n-grams between the generated summaries and the reference summaries. We utilize three variants of ROUGE scores:\nROUGE-N: This measures the overlap of n-grams (where n can be 1, 2, or higher) between the generated summary and the reference summaries. Specifically, ROUGE-1 (Referred to as R-1 Later) calculates the overlap of uni-grams, while ROUGE-2 (Referred to as R-2 Later) evaluates the overlap of bi-grams."}, {"title": "E.2 BERT Scores", "content": "BERT Scores (Zhang* et al., 2020) leverage contextual embeddings derived from the BERT model (Devlin et al., 2019) to evaluate the quality of generated summaries. Unlike traditional n-gram-based methods, BERT scores take into account the semantic similarity between the generated and reference summaries. BERT Scores are usually reported as:\nBERT Score Precision (BSp): It focuses on the accuracy of the generated content.\nBERT Score Recall (BSr): It emphasizes completeness in capturing relevant content.\nBERT Score F1 Score (BSf1): It combines both metrics to provide a balanced assessment of summary quality\nBy utilizing both ROUGE and BERT scores, we can gain a well-rounded understanding of how our proposed models perform in terms of both surface-level text overlap and deeper semantic alignment with gold summaries. This dual approach allows for a more robust evaluation of the generated summaries, ensuring that they not only contain relevant information but also maintain coherence and fluency."}]}