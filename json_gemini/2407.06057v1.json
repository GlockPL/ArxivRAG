{"title": "VARIATIONAL BEST-OF-N ALIGNMENT", "authors": ["Afra Amini", "Tim Vieira", "Ryan Cotterell"], "abstract": "Best-of-N (BON) is a popular and effective algorithm for aligning language models\nto human preferences. The algorithm works as follows: at inference time, N sam-\nples are drawn from the language model, and the sample with the highest reward,\nas judged by a reward model, is returned as the output. Despite its effectiveness,\nBoN is computationally expensive; it reduces sampling throughput by a factor of\nN. To make BoN more efficient at inference time, one strategy is to fine-tune the\nlanguage model to mimic what BoN does during inference. To achieve this, we\nderive the distribution induced by the BoN algorithm. We then propose to fine-tune\nthe language model to minimize backward KL divergence to the BoN distribution.\nOur approach is analogous to mean-field variational inference and, thus, we term it\nvariational BoN (vBoN). To the extent this fine-tuning is successful and we end\nup with a good approximation, we have reduced the inference cost by a factor of\nN. Our experiments on a controlled generation task suggest that while variational\nBoN is not as effective as BoN in aligning language models, it is close to BoN\nperformance as vBoN appears more often on the Pareto frontier of reward and KL\ndivergence compared to models trained with KL-constrained RL objective.", "sections": [{"title": "1 INTRODUCTION", "content": "Language models are pre-trained on large corpora to model a distribution over natural language\ntext. Beyond their initial pre-training, they are often additionally fine-tuned on domain-specific data\nthrough a process called supervised fine-tuning (SFT). The goal of SFT is to enable the model to\nbetter perform various downstream tasks of interest. While the fine-tuned model, called the reference\nmodel in our paper, is indeed typically much better at performing the downstream task of interest,\ne.g., dialogue generation or summarization, it may still generate undesirable content, e.g., harmful\nor offensive text. To mitigate this issue, aligning the reference model to human preferences has\nbecome a fundamental step in the development of modern large language models (Touvron et al.,\n2023; OpenAI et al., 2023; Gemini et al., 2024).\nThe degree to which text is aligned with human preferences is typically operationalized using a\nreal-valued reward function. Rather than constructing a reward function by hand, it is typically\nestimated from a dataset of human preferences. And, after estimation, we expect the reward function\nto return higher values for text that is more likely to be preferred by humans, and lower values for\ntext that is more likely to be dispreferred. Then, given an estimated reward function, an alignment\nalgorithm further alters the reference models in a manner such that it places the highest probability on\nthat text that is high reward under the reward model and high probability under the reference model.\nAlignment algorithms can be taxonomized into two groups: (i) alignment via fine-tuning, where\nwe change the language model's parameters to achieve alignment (Christiano et al., 2017; Rafailov\net al., 2023), and (ii) alignment through inference (Nakano et al., 2022; Mudgal et al., 2024). A\ncommon alignment-via-fine-tuning method is reinforcement learning from human feedback (RLHF;\nChristiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022). RLHF typically consists of\nfurther fine-tuning the language model under a KL-constrained RL objective, which is made up\nof two terms: a term that encourages the model to maximize the reward, and a term that discourages"}, {"title": "2 BACKGROUND: REINFORCEMENT LEARNING FROM HUMAN FEEDBACK", "content": "Let $\\Sigma$ be an alphabet, a finite, non-empty set of symbols. A string is a finite sequence of symbols\ndrawn from $\\Sigma$. A language model is a distribution over strings $y \\in \\Sigma^*$, where $\\Sigma^*$ is the set of all\nstrings over the alphabet $\\Sigma$. In this paper, we consider language models, e.g., those based on neural\nnetworks, that are parameterized by a real vector $\\theta \\in \\mathbb{R}^d$, denoted as $\\pi_\\theta$. Furthermore, we restrict\nourselves to language models that are differentiable functions of $\\theta$. In conditional generation tasks,\ne.g., summarization or dialogue generation, it is desirable to prompt the language model with a string\n$x \\in \\Sigma^*$. Consequently, we consider prompted language models, i.e., those that give a conditional\ndistribution over response strings $y$, given a prompt string $x$, as $\\pi_\\theta(y \\mid x)$. However, for notational\nconvenience, we will drop the explicit conditioning on the prompt $x$ and simply write $\\pi_\\theta(y)$.\nAlgorithms for RLHF fine-tune the language model to increase the expected reward of the strings\nsampled from it while not diverging too far from the reference model. RLHF consists of three steps.\nFirst, the language model is fine-tuned on a task-specific dataset using the maximum-likelihood\nobjective. Recall we term the language model after this step the reference model and show that with\n$\\pi_{\\text{ref}}$. Next, a reward model $r : \\Sigma^* \\to \\mathbb{R}$ is trained to capture human preferences; the reward of a\nstring is high if it is preferred by humans. Finally, the reference model is fine-tuned to maximize the"}, {"title": "3 DERIVING THE BEST-OF-N OBJECTIVE", "content": "Best-of-N algorithm is a simple alignment-via-inference algorithm. The algorithm works as follows.\nLet $\\mathcal{Y}_N = \\{y^{(n)}\\}_{n=1}^{N}$ be the multi-set containing N i.i.d samples from $\\pi_{\\text{ref}}$. Then, BoN algorithm\nreturns $y^*$, where\n$$y^* = \\underset{y^{(n)} \\in \\mathcal{Y}_N}{\\operatorname{argmax}} r(y^{(n)}).$$   (3)\nWe show the probability distribution induced from BoN sampling algorithm with $\\pi_{\\text{bon}}$. Importantly,\n$\\pi_{\\text{bon}}$ is not the optimal distribution under Eq. (1), the KL-constrained RL objective. Nevertheless,\nthe BoN algorithm often performs well---even compared to RLHF-based methods. This raises the\nquestion: under what optimization objective is $\\pi_{\\text{bon}}$ the optimal distribution? To derive such an\nobjective, we begin by computing the probability of strings under $\\pi_{\\text{bon}}$.\nProposition 1. Suppose $r: \\Sigma^* \\to \\mathbb{R}$ is a one-to-one mapping. Then, the probability that a string\n$y_{\\text{bon}}$ is given by\n$$\\pi_{\\text{bon}}(y) = \\sum_{i=1}^N {N \\choose i} \\, F(r(y))^{N-i} \\, \\pi_{\\text{ref}}(y)^{i},$$ (4)\nwhere we define $F(R)$ as\n$$F(R) \\triangleq \\mathbb{P} (r(y) < R) .$$ (5)\nWe now describe how to fine-tune the language model to approximate $\\pi_{\\text{bon}}$ by minimizing the reverse\nKL divergence between $\\pi_\\theta$ and $\\pi_{\\text{bon}}$. Concretely, we maximize the following objective:\n$$\\begin{aligned}\n\\mathcal{J}_{\\text{VBON}}(\\theta) &= - D_{\\text{KL}} \\left( \\pi_\\theta \\mid \\pi_{\\text{bon}} \\right) = \\mathbb{E}_{y \\sim \\pi_\\theta} \\left[ \\log \\pi_{\\text{bon}}(y) - \\log \\pi_{\\theta}(y) \\right] \\\\\n&= \\mathbb{E}_{y \\sim \\pi_\\theta} \\left[ \\log \\pi_{\\text{bon}}(y) \\right] + H(\\pi_\\theta) \\\\\n&= \\mathbb{E}_{y \\sim \\pi_\\theta} \\left[ \\log \\sum_{i=1}^N {N \\choose i} F(r(y))^{N-i} \\pi_{\\text{ref}}(y)^{i} \\right] + H(\\pi_\\theta).    (6)\n\\end{aligned}$$\nThis is an entropy-regularized objective, where we use the probability of the string under the BoN\ndistribution as the reward and discourage the model from having low entropy."}, {"title": "4 COMPARING BON AND RL OBJECTIVES", "content": "To explore the connection between the vBoN objective and the KL-regularized RL objective, we\nderive two lower bounds for $\\mathcal{J}_{\\text{VBON}}$. Through these lower bounds, we can get more insights on how\nthe reward function is used in the variational BoN objective, and why this objective discourages high\nKL divergence from the reference model.\nTo derive the first lower bound, we substitute the BoN distribution in Eq. (4) into the vBoN objective\nin Eq. (6). We then use Jensen's inequality to bound this objective, as explained in the following\ntheorem.\nTheorem 2. Let $\\alpha = \\frac{(N+2)(N-1)}{2}$, $\\beta = \\frac{N(N+1)}{2}$, and $\\gamma = \\frac{N(N-1)}{2}$. Then, we have $\\mathcal{J}_{\\text{VBON}}(\\theta) \\geq L_1(\\theta)$,\nwhere we further define\n$$L_1(\\theta) \\triangleq \\mathbb{E}_{y \\sim \\pi_\\theta} [\\log F (r(y))] - \\alpha H(\\pi_\\theta) - \\beta D_{\\text{KL}} (\\pi_\\theta \\mid \\pi_{\\text{ref}}).$$  (9)\nWe can already see the connection between $L_1(\\theta)$ and the KL-regularized RL objective, Eq. (1). They\nboth encourage maximizing a function of reward values. In the BoN objective this function is $F(\\cdot)$,\nwhile in the KL-regularized RL objective, it is an identity function. Furthermore, both objectives\ninclude a negative KL-divergence term between the language model and the reference model. $L_1(\\theta)$\nfurther encourages the model to have low entropy. In fact, we can show that Thm. 2 also holds for\n$\\alpha = 0$, $\\beta = 1$, $\\gamma = N - 1$. This further simplifies $L_1$ to arrive at a looser lower bound, which we call\n$L_2$, that is even more similar to the KL-regularized RL objective.\n$$\\log F (R) = \\log \\mathbb{E}_{Y \\sim \\pi_{\\text{ref}}} [1 \\{r(y) < R\\}]$$\n$$> \\mathbb{E}_{y^{(1)},...,y^{(M)} \\sim \\pi_{\\text{ref}}} \\bigg[ \\log \\bigg[ \\frac{1}{M} \\sum_{m=1}^{M} 1 \\{r(y^{(m)}) < R\\} \\bigg] \\bigg],$$ (8a)\n$$\\approx \\mathbb{E}_{y^{(1)},...,y^{(M)}} \\bigg[ \\frac{1}{M} \\sum_{m=1}^{M} \\log(1 \\{r(y^{(m)}) < R\\}) \\bigg],$$\n(8b)\nwhere Jensen's inequality is applicable because log is concave. Consistency can be shown with an application of\nthe delta method (\u00a75.5.4; Casella & Berger, 2001)."}, {"title": "5 EXPERIMENTS", "content": "We now employ the variational BoN objective, Eq. (6), to fine-tune language models. We perform an\nopen-ended text generation task where the goal is to generate movie reviews with positive sentiment.\nThe reference model, $\\pi_{\\text{ref}}$, is GPT-IMDB6, a version of GPT-2 (Radford et al., 2019) fine-tuned on\nIMDB corpus (Maas et al., 2011). We use a binary sentiment classifier, $r$, denoted as $p$, with two\nclasses $\\{POS, NEG\\}$ as the reward model, and define $r(y) \\triangleq p(POS \\mid y)$. Following Rafailov et al.\n(2023), we sample 5000 movie reviews from the training set of IMDB dataset and for each sample,\nwe randomly choose a prefix length between 2 \u2013 8 and take that prefix as the prompt. We further\ngenerate 512 prompts in the same way from the test set of IMDB that we use to evaluate our models."}, {"title": "6 CONCLUSION", "content": "Motivated by the effectiveness of the BoN algorithm, we formally derive a variational approximation\nto the distribution induced by BoN algorithm via fine-tuning language models. Our analysis highlights\nthe similarities and distinctions between the variational BoN objective and the KL-constrained RL\nobjectives. Our empirical findings reveal that models fine-tuned using the variational approximation\nto BoN not only attain high reward values but also maintain proximity to the reference models.\nCrucially, inference on the fine-tuned models with the vBoN objective remains as cost-effective as\ninference on the original reference model."}, {"title": "A PROOF OF PROP. 1", "content": "Proposition 1. Suppose $r: \\Sigma^* \\to \\mathbb{R}$ is a one-to-one mapping. Then, the probability that a string\n$y \\sim \\pi_{\\text{bon}}$ is given by\n$$\\pi_{\\text{bon}}(y) = \\sum_{i=1}^N {N \\choose i} \\, F(r(y))^{N-i} \\, \\pi_{\\text{ref}}(y)^{i},$$ (4)\nwhere we define $F(R)$ as\n$$F(R) \\triangleq \\mathbb{P} (r(y) < R) .$$ (5)\nThe proof follows Casella & Berger (2001, Theorem 5.4.3). To compute $\\pi_{\\text{bon}}(y)$, we first\ndefine two events: (i) the event that all N samples have rewards less than or equal to $r(y)$, and (ii) the\nevent that all N samples have rewards less than $r(y)$. The probability of those events is as follows:\n$$P_1(y) \\triangleq \\mathbb{P}(\\text{all N samples have rewards} \\leq r(y)) = (F(r(y)) + \\pi_{\\text{ref}}(y))^{N}$$ (12a)\n$$P_2(y) \\triangleq \\mathbb{P}(\\text{all N samples have rewards} < r(y)) = F(r(y))^{N}.$$  (12b)\nNote that for Eq. (15a) to hold, we need the assumption that the reward function is a one-to-one\nmapping. Furthermore, given this assumption, $\\pi_{\\text{bon}}(y)$ is the probability that at least one of\nthe sampled strings out of N samples have the reward exactly equal to $r(y)$ and the rest of the\nsamples have rewards less than or equal to $r(y)$. Given how we defined $p_1$ and $p_2$, we have\n$\\pi_{\\text{bon}}(y) = p_1(y) - p_2(y)$.\n$$\\pi_{\\text{bon}}(y) = (F(r(y)) + \\pi_{\\text{ref}}(y))^{N} - F(r(y))^{N} = \\sum_{i=1}^{N} {N \\choose i} F(r(y))^{N-i} \\pi_{\\text{ref}}(y)^{i}. $$ (13)"}, {"title": "B STRATEGIES FOR NON-INJECTIVE REWARD FUNCTIONS", "content": "If the reward function is not a one-to-one mapping, we need a tie-breaking strategy for BoN algorithm.\nWe formalize this as defining a total order $\\prec$ on $\\Sigma^*$ as follows: for any two strings $y_1$ and $y_2$, if\n$r(y_1) < r(y_2)$ then we have $y_1 \\prec_r y_2$. If $r(y_1) = r(y_2)$ then $y_1 \\prec_r y_2$ only if $y_1 \\prec y_2$, where $\\prec$ is\nsome arbitrary but fixed total order, e.g., lexicographic order. Therefore, we define $F(y)$ as\n$$F(y) \\triangleq \\mathbb{P}(y' \\prec_r y).$$ (14)\nWe then need to define the two events and their probabilities, $p_1$ and $p_2$, given this total order on\nstrings, as follows:\n$$P_1(y) \\triangleq \\mathbb{P}(\\text{all N samples are } \\preceq_r y) = (F(y) + \\pi_{\\text{ref}}(y))^{N}$$ (15a)\n$$P_2(y) \\triangleq \\mathbb{P}(\\text{all N samples are } \\prec_r y) = F(y)^{N}$$ (15b)\nThe rest of the proof is the same as with the one-to-one reward functions."}, {"title": "C PROOF OF THM. 2", "content": "Theorem 2. Let $\\alpha = \\frac{(N+2)(N-1)}{2}$, $\\beta = \\frac{N(N+1)}{2}$, and $\\gamma = \\frac{N(N-1)}{2}$. Then, we have $\\mathcal{J}_{\\text{VBON}}(\\theta) \\geq$ L_1(\\theta), where we further define\n$$L_1(\\theta) \\triangleq \\mathbb{E}_{y \\sim \\pi_\\theta} [\\log F (r(y))] - \\alpha H(\\pi_\\theta) - \\beta D_{\\text{KL}} (\\pi_\\theta \\mid \\pi_{\\text{ref}}).$$ (9)"}, {"title": "D PROOF OF THM. 3", "content": "Theorem 3. We have $\\mathcal{J}_{\\text{VBON}}(\\theta) \\geq L_1(\\theta) \\geq L_2(\\theta)$, where\n$$L_2(\\theta) \\triangleq (N-1) \\mathbb{E}_{y \\sim \\pi_\\theta} [\\log F (r(y))] - D_{\\text{KL}} (\\pi_\\theta \\mid \\pi_{\\text{ref}}).$$ (10)"}, {"title": "E HYPERPARAMETERS", "content": "To visualize the trade-off between the expected rewards and KL divergence, we vary the degree of\nthe visualization using the following hyperparameters for each method:\n*   **BoN-SFT:** $N \\in$ [10, 50, 90, 130, 170, 210, 250, 290, 330, 370, 410, 450, 490, 530, 570, 600]\n    with 2 different seeds, resulting in 32 runs.\n*   **PPO:** $\\beta \\in$ [0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1., 2., 3., 4., 5.] with 2\n    different seeds, resulting in 32 runs.\n*   **DPO:** $\\beta \\in$ [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1., 2., 3., 4., 5.] with 3 different seeds, resulting in\n    33 runs.\n*   **BONBON and vBoN:** $N \\in$ [1, 2, 3, 4, 8, 16, 32, 64, 128, 256, 512] with 3 different seeds,\n    resulting in 33 runs.\n*   **vBoN with $L_2$ bound:** $\\beta \\triangleq \\frac{1}{N} \\in$ [0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2,\n    0.3, 0.4, 0.5, 1., 2., 3., 4., 5.] with 2 different seeds, resulting in 32 runs. Note that\n    comparing Eq. (6) and Eq. (1), we have $N = \\frac{1}{\\beta} + 1$."}, {"title": "F COMPARING THE VBON OBJECTIVE AND $L_2$ LOWER BOUND", "content": "We compare the performance of models fine-tuned with the vBoN objective and its lower bound ($L_2$)\nin Fig. 3. We observe that the performance of the models is very close to each other."}, {"title": "G ADDITIONAL EXPERIMENTS WITH BON-SFT", "content": "We further experiment with training with maximum likelihood objective on BoN generations when\nvarying N. The results are depicted in Fig. 4. We observe that BoN diverges too much from the\nreference model compared to other fine-tuning methods for alignment."}]}