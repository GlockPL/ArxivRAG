{"title": "A Feature-Level Ensemble Model for COVID-19 Identification in\nCXR Images using Choquet Integral and Differential Evolution\nOptimization", "authors": ["Amir Reza Takhsha", "Maryam Rastgarpour", "Mozhgan Naderi"], "abstract": "The COVID-19 pandemic has profoundly impacted billions globally. It challenges public health and healthcare\nsystems due to its rapid spread and severe respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While RT-PCR is considered the gold\nstandard for diagnosing COVID-19, it has some limitations such as the risk of false negatives. To address this\nproblem, this paper introduces a novel Deep Learning Diagnosis System that integrates pre-trained Deep\nConvolutional Neural Networks (DCNNs) within an ensemble learning framework to achieve precise identification\nof COVID-19 cases from Chest X-ray (CXR) images. We combine feature vectors from the final hidden layers of\npre-trained DCNNs using the Choquet integral to capture interactions between different DCNNs that a linear\napproach cannot. We employed Sugeno-\u03bb measure theory to derive fuzzy measures for subsets of networks to enable\naggregation. We utilized Differential Evolution to estimate fuzzy densities. We developed a TensorFlow-based layer\nfor Choquet operation to facilitate efficient aggregation, due to the intricacies involved in aggregating feature\nvectors. Experimental results on the COVIDx dataset show that our ensemble model achieved 98% accuracy in\nthree-class classification and 99.50% in binary classification, outperforming its components\u2014DenseNet-201 (97%\nfor three-class, 98.75% for binary), Inception-v3 (96.25% for three-class, 98.50% for binary), and Xception\n(94.50% for three-class, 98% for binary)\u2014and surpassing many previous methods.\nKeywords: COVID-19, Computer-aided diagnosis, CXR Images, Deep Learning, Differential Evolution,\nEnsemble Learning, Fuzzy Choquet integral.", "sections": [{"title": "1. Introduction", "content": "COVID-19, caused by the SARS-CoV-2 virus, was first found in Wuhan in December 2019 [1]. It was declared a\nglobal pandemic in March 2020 [2] and still remains a problem, even though the WHO no longer calls it a public\nhealth emergency [3]. Its ability to spread easily, even among people without symptoms, makes it hard to control\n[4]. By March 2024, there were over 704 million reported cases worldwide [5]. Effective vaccines play a key role\nin ending the pandemic [6]; however, despite their availability, achieving complete eradication of COVID-19\nremains impossible [7]. Since the pandemic began, variants like Alpha, Beta, Delta, and Omicron have emerged\n[8]. Omicron, being more transmissible, poses challenges by reducing vaccine efficacy and speeding up immunity\nloss [7]. Even after four years, COVID-19 continues to spread globally, driven by fast-spreading Omicron\nsubvariants [9].\nMoreover, there is concern about a new variant that could avoid the protection of current COVID-19 vaccines\n[10]. A stronger variant would put more strain on health systems, showing we are not fully prepared [11]. Because\nof this, we must focus on being ready to deal with the changing COVID-19 situation.\nAn effective COVID-19 pandemic response strategy includes integrating testing to detect COVID-19 patients,\nisolation, and contact tracing [12]. The Reverse Transcriptase-Polymerase Chain Reaction (RT-PCR) test is widely\nrecognized as the gold standard for COVID-19 diagnosis. However, the RT-PCR testing method for COVID-19\ndiagnosis can be time-consuming [13] and prone to generating false negative results due to sampling and testing\nerrors [14].\nComputer-Aided Detection and Diagnosis (CAD) systems can reduce false negative rates and address errors in\nperception [15]. CAD systems are divided into two types: Computer-Aided Detection (CADe), which identifies\nabnormalities in medical images, and Computer-Aided Diagnosis (CADx), which characterizes lesions [16]. These\nsystems are designed to provide diagnostic results without human subjectivity [17]. They also help alleviate the\nburden on medical care systems [18]. As a result, CAD systems can address challenges such as the strain on the"}, {"title": "2. Related Work", "content": "Amid the challenges presented by COVID-19, recent advancements in technology, particularly in areas like deep\nlearning, offer significant promise for improving global healthcare. With CXR imaging playing a vital role in cost-\neffective and prompt COVID-19 detection, this section underscores the latest developments in employing deep learning\nmodels to identify the virus from CXR images.\nWang et al. [24] introduced COVID-Net, a customized DCNN model designed through a human-machine\ncollaborative strategy to detect COVID-19 from CXR images. Using a machine-driven approach guided by expert\ninput, they optimized macro and micro architecture designs, including layer arrangements and activation functions.\nThis led to a lightweight PEPx design pattern with two 1\u00d71 convolutions, a 3\u00d73 depth-wise convolution, and two\nmore 1\u00d71 convolutions. COVID-Net includes 16 PEPx blocks and uses MLP for classification. After being trained\non the COVIDx dataset, the model achieved 93.3% accuracy in a three-class classification task (COVID-19,\nPneumonia, Normal).\nUcar and Korkmaz [36] introduced COVIDdiagnosis-Net, a compact deep neural network designed for rapid\nCOVID-19 detection. Built on Deep Bayes-SqueezeNet, a fusion of SqueezeNet and Bayesian optimization, this\nmodel benefits from SqueezeNet's smaller size, enabling efficient hardware deployment critical for combating\nCOVID-19. To address the imbalanced COVIDx dataset, Offline Augmentation was used, boosting accuracy to\n98.83% in three-class classification. Without data augmentation, the model's accuracy was 76.37%.\nHiedari et al. [37] utilized the VGG-16 model to detect COVID-19 in CXR images. They applied transfer\nlearning to acquire the model's weights, mitigating the risk of overfitting linked to training on a dataset with limited\nCOVID-19 cases. After fine-tuning, the model achieved an accuracy of 94.5% for three-class classification\n(Normal, Pneumonia, and COVID-19) and 98.1% for two-class classification (COVID-19 and non-COVID-19).\nSimilarly, Minae et al. [25] and Hemdan et al. [38] employed various pre-trained models for the task of COVID-\n19 detection. Minae et al. [33] employed pre-trained ResNet-18, ResNet-50, SqueezeNet, and DenseNet-169, and\nfine-tuned the last layer of the pre-trained versions of these DCNNs for COVID-19 and non-COVID-19\nclassification. The best performer, SqueezeNet, achieved a sensitivity rate of 98% and a specificity rate of 92.9%.\nHemdan et al. [38] utilized Inception-v3, VGG-19, DenseNet-201, Xception, MobileNetv2, and Inception-ResNet-\nV2 for two-class classification (COVID-19 and Normal). Among these models, VGG-19 and DenseNet-201 proved\nmost successful, both achieving a precision of 83% for the COVID-19 class.\nTurkoglo [39] used the pre-trained AlexNet to extract features from CXR images for COVID-19 diagnosis. The\nRelief algorithm, based on KNN principles, identified and prioritized key features across all layers. These selected\nfeatures were input into an SVM classifier, achieving 99.2% accuracy in a three-class task (Normal, COVID-19,\nPneumonia) using 1500 deep features.\nChandra et al. [40] introduced the Automatic COVID Screening (ACoS) system, which uses tissue descriptors\nfrom CXR images to distinguish between normal, abnormal, COVID-19, and Pneumonia cases. The system extracts\nfeature vectors such as first-order statistics, gray-level co-occurrence matrix, and histograms of oriented gradients.\nThe Binary Grey Wolf Optimization (BGWO) algorithm selects relevant features for training five supervised\nmodels: Decision Tree (DT), SVM, KNN, Na\u00efve Bayes (NB), and Artificial Neural Network (ANN), chosen for\ntheir ability to perform well with small datasets. An ensemble majority voting strategy is used to reduce\nmisclassification, particularly false negatives. ACoS achieved 91.329% accuracy in classifying COVID-19 from\nPneumonia CXR images.\nBhowal et al. [23] proposed a fuzzy Choquet ensemble model using pre-trained VGG-16, Xception, and\nInception-V3 models for feature extraction, followed by input into an MLP component. Fuzzy measures were\ncomputed using coalition game theory, information theory, and Sugeno-\u03bb measures, with the Shapley value used\nto evaluate individual classifier contributions. Three weighting schemes based on validation accuracies were\napplied, leading to the calculation of three fuzzy measures. These measures were then used to compute Choquet\nintegrals, which were aggregated through majority voting. The model achieved 93.81% accuracy in a three-class\nclassification on the COVIDX dataset."}, {"title": "3. Proposed Method", "content": "3.1. Method Overview\nWe developed an ensemble model using pre-trained DCNNs, specifically DenseNet-201 [49], Inception-v3 [50], and\nXception [51], as base models. Each DCNN was modified with an MLP classification section for both three-class and\ntwo-class classification tasks. The MLP includes a flattening layer, a dense layer with 100 ReLU-activated neurons, a\ndropout layer for regularization, another dense layer with 256 ReLU-activated neurons, and a prediction layer with either\nthree or two neurons depending on the classification task. The softmax function is used in the prediction layer to generate\na probability distribution for each CXR image.\nInstead of using vote aggregation, we integrate the outputs of the dense layers (256 ReLU-activated neurons)"}, {"title": "3.2. Data Preprocessing", "content": "In the preprocessing stage, images from both the training and testing datasets of the COVIDx dataset were read and\ndecoded into pixel data, which represents color information in the RGB (Red, Green, Blue) color space. Each image\nwas then converted into a three-dimensional tensor, where the first dimension corresponded to the image's height,\nthe second to its width, and the third to the color channels (Red, Green, Blue). To meet the input requirements of\npre-trained DCNNs, all tensors were resized to a standardized dimension of 224\u00d7224\u00d73. This resizing was\naccomplished using the Bilinear Interpolation algorithm. The algorithm efficiently computes new pixel values. It\ndoes this by averaging neighboring pixels from the original image. This process is efficient and not computationally\nintensive. To facilitate batch mode training and testing of DCNNs with four-dimensional input layers, the three-\ndimensional tensors are stacked together, resulting in the creation of a four-dimensional tensor array. To ensure\nconsistency and optimal readiness for subsequent adjustments of the DCNNs, pixel values were rescaled to a\nuniform range of [0, 1]. This was achieved by applying simplified min-max normalization, in which the values of the\nfour-dimensional tensor were divided by 255. The rescaling step plays a crucial role in ensuring data uniformity\nand enhancing the readiness of the data for subsequent model refinement procedures."}, {"title": "3.3. Transfer Learning", "content": "We employed the transfer learning technique to counter the limited COVID-19 images in the COVIDx dataset and\nalleviate overfitting risks. Pre-trained weights from models trained on the ImageNet dataset [52], which includes\ndiverse images such as Zucchini flower, Ostrich, Lemur, Quokka, Gharial, Mandrill, and Okapi, were loaded into\nthree DCNN models. Although there are differences between ImageNet and medical datasets like COVIDx,\ntransferring knowledge from ImageNet helps DCNNs capture low-level features. This transfer is effective, even\nthough the object classes differ. In our transfer learning approach, we employed a fine-tuning technique to optimize\nthe weights of layers in the DCNNs specifically for the CXR dataset."}, {"title": "3.4. DCNNs and MLP Blocks", "content": "The effectiveness of ensemble models comes from combining networks with different approaches to feature\nextraction. DenseNet-201, Inception-V3, and Xception, each built on distinct architectures, introduce valuable\ndiversity into the ensemble. DenseNet-201 offers densely connected feature maps, Inception-V3 focuses on multi-\nscale extraction, and Xception uses depthwise-separable convolutions for efficient representation. Incorporating\nthis diversity and combining these diverse architectures enriches feature representation and boosts performance. In\nthis section, we briefly describe the different and diverse architectures of the three DCNNs, followed by the\nconfigurations of their MLP blocks."}, {"title": "3.4.1. DenseNet-201", "content": "DenseNet-201, based on the DenseNet architecture introduced by Huang et al. [49], is known for its dense blocks\nand connectivity. Each Dense Block consists of multiple layers, where each layer performs batch normalization,\nReLU activation, and a 3\u00d73 convolution. Dense connectivity ensures direct connections from each layer to all\nsubsequent layers, improving information flow, feature reuse, and gradient flow, which enhances model\nperformance.\nFurthermore, DenseNet incorporates a bottleneck structure by utilizing a 1\u00d71 convolution before each 3\u00d73\nconvolution. In each dense layer that uses a bottleneck structure, the process starts with a batch normalization\noperation, followed by a ReLU activation. This is succeeded by a 1\u00d71 Convolution operation. Subsequently,\nanother round of batch normalization operation and ReLU activation layer follows, leading to a final 3\u00d73\nconvolution operation. The bottleneck layers compress feature maps and decrease the number of input channels\nbefore transitioning into more computationally intensive layers.\nTo manage feature map dimensions and the number of channels as the network progresses, DenseNet employs\ntransition layers between dense blocks. Each transition layer starts with a batch normalization layer. Subsequently,\na 1\u00d71 convolutional operation is applied, followed by a 2\u00d72 average pooling layer to downsample the feature maps.\nDenseNet consists of three models: DenseNet-201, DenseNet-169, and DenseNet-121, each named for the total\nnumber of layers they contain. For instance, in DenseNet-201, the number 201 indicates the total number of layers."}, {"title": "3.4.2. Inception-V3", "content": "Inception-V2 and Inception-V3, developed by Szegedy et al. [50], build upon the Inception-V1 architecture, which\nwas designed to address the challenge of handling images with features of varying sizes. The core innovation of\nInception-V1 was the \"Inception module,\" which uses convolutional layers with different filter sizes (1\u00d71, 3\u00d73,\nand 5\u00d75) alongside a pooling layer. These layers are combined through concatenation to improve efficiency. To\noptimize computation, 1\u00d71 convolutional layers are placed before and after the larger convolution layers and\npooling layers to reduce input channels. Additionally, Inception-V1 introduced auxiliary classifiers to improve\ngradient flow in deep networks, helping mitigate issues during backpropagation.\nInception-V2 enhances the basic Inception module by factorizing the 5\u00d75 convolution into two consecutive 3\u00d73\nconvolutions, improving computational efficiency. This is because two 3\u00d73 convolutions require fewer\nmultiplications than a single 5\u00d75 convolution. Additionally, Inception-V2 further optimizes convolutions by\nbreaking down n\u00d7n convolutions into 1\u00d7n and n\u00d71 convolutions within each Inception module. Rather than\nincreasing the depth of the Inception module, Inception-V2 expands the filter bank, using filters of various sizes\n(both width and height) in parallel convolution layers. This strategy alleviates representational bottlenecks,\nenabling the network to learn a broader range of features while reducing computational costs and model complexity.\nTo enhance the Inception-V2 architecture without significant modifications to its modules, the authors\nintroduced Inception-V3. This version incorporates several improvements, including the utilization of the\nRMSProp optimizer and Factorized 7\u00d77 convolutions. Additionally, batch normalization is applied in the auxiliary\nclassifiers, and label smoothing regularization is implemented to prevent the model from becoming excessively\nconfident in its predictions."}, {"title": "3.4.3. Xception", "content": "Xception, proposed by Chollet et al. [51], introduces a DCNN architecture that relies heavily on Depthwise\nseparable convolution layers. The key idea behind Xception is to separate channel-wise and spatial dependencies\nwithin feature maps. This concept builds upon the Inception architecture. Depthwise separable convolutions\nachieve this separation through two steps: Depthwise convolution and pointwise convolution. In Depthwise\nconvolution, each input channel is convolved with a separate filter, unlike traditional convolutions that apply a\nsingle filter to all channels. This allows the network to efficiently capture spatial information while reducing\ncomputational costs. The pointwise convolution (1\u00d71) follows, transforming the output channels from the\nDepthwise convolution into a different dimensionality."}, {"title": "3.4.4. MLP Blocks", "content": "The MLP block at the top of each DCNN uses a feedforward design. Data flows from the flattened input layer"}, {"title": "3.5. Training Parameters", "content": "We use Stochastic Gradient Descent (SGD) to fine-tune all DCNNs with the same settings: an initial learning rate\nof 0.001, a momentum of 0.9, and a clip-norm of 1.0. Categorical Cross Entropy serves as the loss function, and\nwe monitor validation loss after each epoch. If the validation loss plateaus or increases after the 14th epoch, we\nreduce the learning rate by 40% to improve convergence. We save the model weights from the epoch with the\nhighest validation accuracy as \".h5\" files. After fine-tuning, we reload these weights into their respective models.\nEach DCNN undergoes 50 epochs of fine-tuning with a batch size of 32."}, {"title": "3.6. Proposed Fuzzy Choquet Ensemble Model", "content": "Given our preference for feature combination over vote aggregation, selecting an efficient aggregation method is\ncrucial. Dense layers with 256 neurons have higher dimensionality than the prediction layers of individual DCNNs.\nThis makes it essential to merge feature vectors rather than probabilities, especially when using functions like the\nChoquet integral. To achieve this, we use TensorFlow and Keras to implement a custom Choquet integral layer.\nThis custom layer optimizes computation for both CPU and GPU platforms. It leverages TensorFlow's parallelism\nand computational efficiency. Using TensorFlow ensures compatibility with existing deep learning workflows. It\nalso simplifies experimentation and deployment while reducing resource usage. With this setup, we route the\noutputs of the dense layers to the Choquet layer. The output of the Choquet layer is then fed directly into DenseNet-\n201's prediction layer.\nTo effectively utilize the Choquet integral for fusing information vectors, it's imperative to take into account\nfuzzy measures for subsets of DCNNs and fuzzy density associated with each DCNN. The coordination of fuzzy\nmeasures effectively manages interactions among Criteria (DCNNs) and leads to an enhanced scrutiny of\nrelationships. However, obtaining all possible fuzzy measures for a given universal set requires human-collected\nevaluations for each subset, with the exclusion of the empty and universal sets. For additional clarity, our universal\nset consists of {DenseNet-201, Inception-V3, Xception}, thus necessitating the determination of optimal fuzzy\nmeasures for {Inception-V3}, {DenseNet-201},{Xception}, {Inception-V3, Xception}, {DenseNet-201,\nInception-V3}, and {DenseNet-201, Xception}, which is indeed a complex task. To streamline this, we employ\nSugeno-\u03bb measures proposed by Sugeno [53]. However, it remains essential to compute fuzzy measures for\nsingletons (fuzzy densities) such as {Inception-V3}, {DenseNet-201}, and {Xception}. In this study, near-optimal\nfuzzy densities are calculated using the Differential Evolution algorithm, which explores a fuzzy space where the\nmembership value of each element of the ensemble set ranges from 0 to 1."}, {"title": "3.6.1 Fuzzy Measures Calculation using Sugeno-\u03bb Measures Theory", "content": "A fuzzy measurement assigns a degree of membership to subsets of a given set. According to [53], a measurement\n$\\Omega$ in a finite set $X$ is considered a fuzzy measurement if its measurement function $g_{\\Omega}: 2^x \\rightarrow [0,\\infty)$ satisfies the\nfollowing conditions:\nNullity at the empty set and unity at the entire set: $g_{\\Omega}(\\Phi) = 0$ and $g_{\\Omega}(X) = 1$.\nMonotonicity: If $A \\subseteq B$, then $g(A) \\le g(B)$.\nLimit preservation: If $\\{A_i\\}_{i=1}^{\\infty}$ is an ascending measurable sequence then $g_{\\Omega}(A_i) = g_{\\Omega}(\\lim(A_i))$.\nAccording to [53], the Sugeno- $\\lambda$ measures can be expressed as follows:\nIf $A, B \\subseteq X$ and $A \\cap B = \\O$ then:\n$g_{\\Omega}(A \\cup B) = g_{\\Omega}(A) + g_{\\Omega}(B) + \\lambda g_{\\Omega}(A) g_{\\Omega}(B)$ for some $\\lambda > -1$.  (1)\nThe equation necessary to calculate the value of $\\lambda$, as outlined in [54], can be derived using the following\nexpression:\n$1 + \\lambda = \\prod_{i=1}^N(1 + \\lambda g_{\\Omega}^i)$  (2)\nHere, $g_{\\Omega}^1, g_{\\Omega}^2$, and $g_{\\Omega}^N$ represent singletons or fuzzy densities, which in this study are fuzzy measures of three\nDCNNs. By knowing the fuzzy densities, we can determine fuzzy measures for all combinations."}, {"title": "3.6.2 Finding Fuzzy Densities", "content": "To calculate fuzzy densities and enable aggregation, this study uses the Differential Evolution algorithm. This"}, {"title": "3.6.2.1 Population", "content": "In the Differential Evolution, the population represents a set of candidate solutions utilized to explore and search\nthe solution space. The population size, designated by NP and set to 15 in this study, defines the number of\nindividuals in each generation, denoted as G. In this context $S_G = \\{x_{j,G} \\mid j = 1, 2, 3 ... NP \\}$ represents the\ncollection of individual solutions and $X_{j,G} = \\{X_{1,j,G}, X_{2,j,G} ... X_{D,j,G} \\}$ signifies a D-dimensional vector, denoting an\nindividual solution. Each element of $S_G$ represents a set of candidate fuzzy densities for DCNNs, and each element\nwithin $x_{j,G}$ represents a candidate fuzzy density for a unique DCNN. Since we aim to find near-optimal fuzzy\ndensities for three DCNNs, our problem is three-dimensional, with D equaling 3. Initially, without any near-optimal\nfuzzy densities to consider as solutions, candidate solutions are formulated. These solutions adhere to the\nboundaries specified by the lower and upper limits of the solution search space during the initialization phase. At\nthe start of the Differential Evolution process, the generation of $x_{j,G}$ occurs as follows:\n$x_{jG} = x_{lower} + (x_{upper} - x_{lower}) \\times rand[0,1]$  (3)\nHere, $x_{lower}$ and $x_{upper}$ represent the lower and upper bounds of the search space, respectively. $rand[0,1]$ is a\nprobability distribution function that generates values within the specified range [0,1] with equal probability."}, {"title": "3.6.2.2 Mutation", "content": "Mutation explores new regions of the solution space beyond the current population. It promotes diversity by\nintroducing random perturbations. This helps in finding better solutions during the optimization process. It\ngenerates a mutation vector by randomly combining different vectors in accordance with following equation:\n$V_{Tj,G} = X_{BestG} + F \\times (X_{r_1G} - X_{r_2G})$  (4)\nWhere $V_{Tj,G}$ is mutant vector, and $X_{BestG}$ signifies the best solution found up to generation G. Additionally, $X_{r_1G}$\nand $X_{r_2G}$ are random base vectors (where $r_1\\neq r_2\\neq Best$, and $Best_g, r_1, r_2 \\in \\{1, 2, ..., NP\\}$). F, serving as the\nscaling factor, regulates the mutation process, with its value constrained within the range of [0, 1]."}, {"title": "3.6.2.3 Crossover", "content": "In the crossover stage the mutated_vector $V_{Tj,G} = \\{VT_{1,j,G}, VT_{2,j,G} ..., VT_{D,j,G}\\}$ and the target vector $x_{j,G} =$\n$\\{X_{1,j,G}, X_{2,j,G} ... X_{D,j,G} \\}$ are compared component-wise using a Crossover Probability or Crossover Rate (CPr), which\nranges between 0 and 1, to determine the selection of components for the resulting vector in the crossover stage. If\na component of the mutant vector is selected, it is included in the crossover result vector, referred to as the trial\nvector denoted as $XX_{j,i,G}$. Otherwise, the corresponding component from the target vector is chosen. This process\ncan be represented as follows:\n$XX_{j,i,G} = \\begin{cases}\nV_{Ti,j,G} \\quad \\text{if } j\\in CrossPoints \\\\\nX_{i,j,G} \\quad \\text{otherwise}\n\\end{cases}$ (5)\nIn equation 5, CrossPoints signifies a collection of crossover points established through consideration of both the\nCPr value and the selected crossover method, which, in our study, is Binomial. To ensure that CrossPoints is non-\nempty, a random member $j_{rand}\\in\\{1,2,...,D\\}$ is chosen. Subsequently, a new member, denoted as $m\\in \\{1,2,...,D\\}$, is\nincluded in CrossPoints under the following condition:\nCrossPoints = CrossPoints $\\cup \\{m\\} \\times \\delta\\{ rand[0,1] < CPr \\land m\\neq j_{rand}\\}$, for $j\\in \\{1,2,...,D\\}$  (6)\nHere, $\\delta$ is a Kronecker delta function used to determine whether the addition of the new element m should occur."}, {"title": "3.6.2.4 Selection", "content": "In the selection stage, the fitness values of the target vector $x_{jG}$ and the crossover result vector $XX_{jG}$ are compared.\nAfter comparing $x_{jG}$ and $XX_{jG}$, the vector with lower fitness value is chosen to be promoted to the next generation.\nThis process can be illustrated in the following manner:\n$X_{j,G+1} = \\begin{cases}\nXX_{jG} \\quad \\text{if } f(XX_{jG}) \\le f(x_{jG}) \\\\\nx_{jG} \\quad \\text{otherwise}\n\\end{cases}$  (7)\nIn equation 7, $f(XX_{jG})$ denotes the process of evaluating the objective function for the crossover result vector,\nwhile $f(x_j)$ represents the process of evaluating the objective function for the target vector. The comparison\n$f(XX_{jG}) \\le f(x_{jG})$ then represents the evaluation of whether the objective function value for the crossover result\nvector is less than or equal to the objective function value for the target vector. The result of this comparison\ndetermines which vector should be included in the next generation's population."}, {"title": "3.6.3 Fuzzy Choquet Integral Layer", "content": "The aggregating process using the Choquet function involves considering a real-valued function, $h:X\\rightarrow R$, which\nrepresents the evidence or support for a hypothesis (such as features or output votes from DCNNs). The specific\nrelation for obtaining the aggregated feature vector using the Choquet operator can be represented by the following\nequation:\n$C_g(h, g_{\\Omega}) = \\sum_{i=1}^N(h(x_{\\pi(i)})( g_{\\Omega}(A_i) \u2013 g_{\\Omega}(A_{i-1}))$  (8)\nThe function \"h\" maps real values to the elements within set X. Here, these values are specifically obtained from\nthe outputs of dense layers, each containing 256 neurons activated by ReLU activation function, in the three DCNNS\n(Criteria). Given that \"h\" produces real numbers, the outputs from the three Dense layers can act as supporting\nhypotheses for each input image class. These supporting hypotheses are consolidated using the Choquet operator,\nresulting in an aggregated vector. This aggregated vector is then used to assign each input image to its respective\nclass, with the Choquet output directed to the prediction layer of DenseNet-201.\nIn equation 8, Each $\\pi$ signifies a permutation of X, where function \"h\" applied to permuted entries satisfies\n$h(x_{\\pi(1)}) > h (x_{\\pi(2)}) > \\ldots > h (x_{\\pi(N)})$. The notation $g_{\\Omega}(A_i)$, with $A_i = \\{x_{\\Pi(1)}, x_{\\Pi(2)}, ..., x_{\\Pi(N)}\\}$, represents fuzzy\nmeasures associated with these permutations. In this study, N, which is 3, corresponds to the number of DCNNs.\nThe individual elements from the outputs of the three dense layers undergo pairwise permutation. Elements at\ncorresponding positions across the vectors are compared and arranged in descending order. We have provided the\nPseudo-Code of our proposed Choquet layer in Fig. 2 further portrays the operation of our proposed Choquet layer\napplied to three feature vectors originating from three DCNNs, resulting in an aggregated vector. In Fig. 2, the\nChoquet layer manages three vectors: FD representing the feature vector of DenseNet-201, FI representing the\nfeature vector of Inception-V3, and Fx representing the feature vector of Xception. The resulting vector of\naggression is denoted as FCH, representing the feature vector of the Choquet layer."}, {"title": "4. Experimental Results", "content": "4.1. Dataset Description\nWe used the COVIDx benchmark dataset [24] for both training and testing phases. We opted for COVIDx versions\n8A/B instead of 9A/B, given our resource constraints and reliance on transfer learning with DCNNs pre-trained on\nImageNet. This choice reflects the suitability of these versions for our limited computational resources. The\nCOVIDx dataset has two variations. COVIDxA includes three categories: Normal, COVID-19, and Pneumonia.\nCOVIDxB is binary, containing COVID-19 and non-COVID-19 cases. Researchers can compare their model\nperformance with ours using either version\u2014COVIDx8 or COVIDx9\u2014since both share similar test samples.\nWe have implemented the train-test split according to the authors' provided labels, ensuring that the sizes of\nour training and testing sets, as well as the images in both the training and testing sets, match those available in the\nCOVIDx8 train set and test set. Moreover, we created a validation dataset by applying a 90/10 split rate for\ntraining/validation from the original training set, and the test set was not split, as the COVIDx repository provides\nits own test set.\nCOVIDx8A includes 1215 COVID-19, 7966 Normal, and 5475 Pneumonia images. The test set consists of 200\nCOVID-19 images and 100 images each for Normal and Pneumonia classes. Similarly, COVIDx8B contains an\nequivalent number of images as COVIDx8A. In COVIDx8B, the none-COVID19 category comprises images\nobtained by combining those from the Pneumonia and Normal classes, while the COVID-19 category in\nCOVIDx8B aligns exactly with the COVID-19 images in COVIDx8A."}, {"title": "4.2. Experimental Setup", "content": "To implement our proposed approach, we selected Python as our programming language and utilized TensorFlow\nalong with its Keras package. Our code was executed in the Google Colab environment, which provided the\nfollowing hardware resources at no cost: Nvidia Tesla T4 GPU with 15.6 GB memory, 12.6 GB RAM, Intel Xeon\nCPU @ 2.20 GHz, and 107.7 GB HDD."}, {"title": "4.3. Evaluation Metrics", "content": "Before defining the evaluation metrics used in this study", "class": "TP\nsignifies individuals within the COVID-19 class", "follows": "n$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$  (9)\n$Precision = \\frac{TP}{TP+FP}$  ("}]}