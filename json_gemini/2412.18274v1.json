{"title": "GenAI Content Detection Task 2:\nAI vs. Human \u2013 Academic Essay Authenticity Challenge", "authors": ["Shammur Absar Chowdhury", "Hind Almerekhi", "Mucahid Kutlu", "Kaan Efe Kele\u015f", "Fatema Ahmad", "Tasnim Mohiuddin", "George Mikros", "Firoj Alam"], "abstract": "This paper presents a comprehensive overview\nof the first edition of the Academic Essay Au-\nthenticity Challenge, organized as part of the\nGenAI Content Detection shared tasks collo-\ncated with COLING 2025. This challenge\nfocuses on detecting machine-generated vs\nhuman-authored essays for academic purposes.\nThe task is defined as follows: \u201cGiven an essay,\nidentify whether it is generated by a machine or\nauthored by a human.\u201d The challenge involves\ntwo languages: English and Arabic. During\nthe evaluation phase, 25 teams submitted sys-\ntems for English and 21 teams for Arabic, re-\nflecting substantial interest in the task. Finally,\nseven teams submitted system description pa-\npers. The majority of submissions utilized fine-\ntuned transformer-based models, with one team\nemploying Large Language Models (LLMs)\nsuch as Llama 2 and Llama 3. This paper out-\nlines the task formulation, details the dataset\nconstruction process, and explains the evalu-\nation framework. Additionally, we present a\nsummary of the approaches adopted by partici-\npating teams. Nearly all submitted systems out-\nperformed the n-gram-based baseline, with the\ntop-performing systems achieving F1 scores\nexceeding 0.98 for both languages, indicating\nsignificant progress in the detection of machine-\ngenerated text.", "sections": [{"title": "1 Introduction", "content": "The rapid progress in Artificial Intelligence (AI)\nand the proliferation of generative content pro-\nduced by LLMs have introduced transformative op-\nportunities across various domains yet they also\npose profound challenges (Wu et al., 2023). One\nsuch challenge lies in the detection and prevention\nof misuse of LLMs in contexts such as fake news,\nmisinformation, disinformation, and academic dis-\nhonesty (Tang et al., 2024). For instance, the\nvolume of AI-generated news on misinformation-\nprone websites surged by 457% between January\n1, 2022, and May 1, 2023, with a corresponding\nincrease of 57.3% on mainstream platforms (Han-\nley and Durumeric, 2024). These issues pose sub-\nstantial barriers to the broader adoption of LLMs,\nthereby limiting their potential across various ap-\nplications. Effectively detecting LLM-generated\ncontent is crucial for leveraging the capabilities of\nthese models while mitigating associated risks.\nResearchers have responded to these challenges\nthrough a variety of approaches. Previous methods\ninclude classification algorithms designed to distin-\nguish between AI-generated and human-authored\ntext (Guo et al., 2023), as well as watermarking\ntechniques (Szyller et al., 2021; He et al., 2022;\nKirchenbauer et al., 2023). These watermarking\napproaches strategically embed imperceptible sig-\nnatures within generated texts, enabling model-\nspecific identification while maintaining human-\nindistinguishable quality. Other recent efforts have\nfocused on the creation of question-answering\ndatasets such as M4 (Wang et al., 2024b), gen-\nerated by humans and ChatGPT in both English\nand Chinese and the associated shared task (Wang\net al., 2024a).\nWithin academic settings, concerns surrounding\nthe potential misuse of LLMs have intensified, par-\nticularly regarding academic dishonesty involving\nAI-assisted essay writing and problem-solving. Re-\ncent research has made considerable progress in\nthe development of datasets and benchmarking ef-\nforts to address these issues. For instance, Yu et al.\n(2023) introduced the CHEAT dataset, which fo-\ncuses on abstracts from IEEE Xplore, while Wang\net al. (2024b) developed a comprehensive multi-\nlingual dataset. Additionally, Dugan et al. (2024)\npresented a robust dataset designed to address the\nchallenge of detecting machine-generated text.\nDespite these efforts, large-scale initiatives in\nacademic contexts remain limited. Hence, this\nshared task aims to bridge this gap by tackling the\ntask of distinguishing AI-generated essays from"}, {"title": "2 Related Work", "content": "The detection of AI-generated text relies on an-\nalyzing statistical patterns and linguistic features\nthat distinguish human and machine writing styles.\nZaitsu and Jin (2023) highlight that AI-generated\ntext tends to use repetitive sentence patterns and\na limited vocabulary, prioritizing clarity over the\nnuanced variations of human writing. Similarly,\nWeber-Wulff et al. (2023) report that such texts of-\nten exhibit lower syntactic complexity and reduced\nlexical diversity, making them identifiable through\nthese markers. Additionally, Gall\u00e9 et al. (2021)\nreport that higher predictability in word n-gram is\na key indicator of machine generated text.\nMachine learning approaches have become cen-\ntral to AI-generated text detection. Darda et al.\n(2023) explored traditional classification algo-\nrithms such as Support Vector Machines (SVM)\nand Random Forest. Vora et al. (2023) propose a\nmultimodal approach that uses BERT to analyze\nsyntactic and semantic features of text and CNN\narchitectures for image. Mikros et al. (2023) inves-\ntigated using stylometric features and transformer-\nbased models. Their findings showed that ensemble\ntechniques, particularly those employing majority\nvoting, outperformed individual classifiers.\nThere has also been effort to combine different\nmachine learning approaches. For instance, deep"}, {"title": "3 Task and Dataset", "content": "The main objective of the task is to detect whether\nthe given candidate essay is AI-generated or\nhuman-written. Given the input essay e, the task is\nto design a text detector D(e), such that the model\noutputs label indicating AI-generated or Human-\nauthored content. For this edition, we designed the\ntask as binary classification problem."}, {"title": "3.1 Task Definition", "content": ""}, {"title": "3.2 Datasets", "content": "The task aims to develop a system specifically de-\nsigned for detecting AI generated text in academic\nessays. The dataset comprises essays authored by\nboth native and non-native speakers, alongside AI-\ngenerated content. A significant challenge in this\ntask was collecting authentic human-authored aca-\ndemic essays while addressing the following con-\nsiderations:\n\u2022 Ensuring author privacy, obtaining informed\nconsent, and ethically sourcing the content.\n\u2022 Verifying that the collected essays were gen-\nuinely authored by humans, free from any AI\ninterference or plagiarism.\n\u2022 Acquiring a diverse set of essays representing\ndifferent academic levels and cultural back-\ngrounds to ensure inclusivity in the dataset.\nFor the task, we focused on two languages: En-\nglish and Arabic. For each language, we provided\ntraining, validation, dev-test, and the final test sets,\nwhich included human-authored and AI-generated\ntexts. We released these data splits in two phases -\n(i) Development phase \u2013 we released the training,\nvalidation, and mock test data (dev-test); (ii) Eval-\nuation phase \u2013 we released the final test set which\nis used to rank the submitted system. Below, we\ndiscuss the dataset design for the development and\nfinal evaluation phases, respectively."}, {"title": "3.3 Development Phase", "content": "During the development phase we have released\ntraining, validation, and dev-test. For this phase,\nwe first collected human-authored essays and es-\nsay topics. To create the data splits, we carefully\ndesigned each set to ensure unique essay topics,\navoiding overlap between training, validation, and\ndev-test datasets.\nFurthermore, within each split, we manually cat-\negorized the essay topics based on their thematic\nsimilarity. This classification is used to assign\ntopics for generating essays using LLMs, and the\nrest is reserved exclusively for selecting human-\nauthored essays from various existing datasets men-\ntioned below. The final statistics of the dataset\nreleased in this phases are presented in Table 5.\nThe human-authored\ndata was sourced from different language assess-\nment datasets, including examinations like IELTS,\nand TOEFL among others. To ensure the authen-\nticity of human-authored content, we selected es-\nsays that were either handwritten or composed in\na supervised classroom setting, explicitly to make\nHuman-authored Essay"}, {"title": "AI-generated Essay", "content": "The generated essays, for\nboth languages, utilized seven state-of-the art\nLLMs including: GPT-3.5-Turbo (2023-03-15-\npreview), GPT-40 (2024-08-06), GPT-40-mini\n(2024-07-18) (OpenAI, 2024), Gemini-1.5 (Team,\n2024), phi3.5,7 Llama-3.1 (8B) (Abdin et al., 2024),\nand Claude-3.5.8 To produce these essays, we de-\nsigned the prompts by utilizing a selected subset of\nessay statements from the aforementioned datasets.\nThe designed prompts included detailed instruc-\ntions to emulate human writing styles, specify es-\nsay length requirements, and incorporate prede-\nfined personas reflecting various factors such as\nnativity and/or language proficiency, following the\nmetadata and statistics obtained from the human-\nauthored essay collections. This approach ensured\nthe generation of essays that closely resemble real-\nworld human writing in both style and content. An\nexample of such a prompt is shown in Table 1."}, {"title": "3.4 Evaluation Phase", "content": "For the evaluation, we designed and developed a\nnovel dataset, the Generated and Real Academic\nCorpus for Evaluation (GRACE), which includes\nboth human-authored and AI-generated essays in\nEnglish and Arabic."}, {"title": "3.4.1 Data Collection", "content": "For designing the human-authored portion of the\ndataset, we began by carefully designing test set\nessay statements aligned with those used in devel-\nopment phase topics. We selected five different\nessay types, and under each type, we created sev-\neral essay statements (see Table 4 for examples).\nThe topics include social influence & technology,\nlifestyle choices & preferences, cultural & global\nperspective, environmental & societal responsibil-\nity, and personal growth & experience."}, {"title": "3.4.2 Data Generation", "content": "For the AI-generated essays, we followed two dis-\ntinct methodologies:\n\u2022 Freehand Generation: An instruct-tuned\nLLM, namely gpt-40, independently gener-\nated essays using the Freehand Generation\nPrompt shown in Table 2. The prompt was de-"}, {"title": "3.5 Baseline and Evaluation Setup", "content": ""}, {"title": "3.5.1 Baseline", "content": "For all languages, we train an n-gram (unigram,\nn = 1) based baseline model. We transformed the\ntexual content of the essays into a TF-IDF (Term\nFrequency-Inverse Document Frequency) represen-\ntation with a maximum of 10k features. A Support\nVector Machine (SVM) classifier is then trained\non this feature representation to evaluate its perfor-\nmance."}, {"title": "3.5.2 Evaluation Setup", "content": "The task was organized into two phases, corre-\nsponding to the previously described dataset de-\nvelopment process:\n\u2022 Development phase: We released the train\nand validation subsets, and participants sub-\nmitted runs on the dev-test set through a com-\npetition on Codalab.10\n\u2022 Evaluation phase: We released the official\ntest subset - GRACE, and the participants\nwere given four days to submit their final pre-\ndictions through the same Codalab competi-\ntion URL. Only the latest submission from\neach team was considered official and was\nused for the final team ranking."}, {"title": "3.5.3 Evaluation Measure:", "content": "We measure the performance of the participating\nsystems using accuracy, macro- precision, recall\nand F1 measure. However, official ranking was\nbased on macro-F1."}, {"title": "4 Results and Overview of the Systems", "content": "In Table 7, we present the results of participants'\nsystems for both Arabic and English including base-\nline. For Arabic, all systems outperformed the n-\ngram baseline, whereas, for English, three teams\nperformed below the baseline. The task generated\nsignificant interest, with 56 teams registering to\nparticipate. However, the number of system sub-\nmissions was nearly halved, and ultimately, only\nfive teams submitted system description papers. In\nTable 8, we provide an overview of the partici-\npating systems for which a description paper was\nsubmitted. For Arabic top team, IntegrityAI (AL-\nSmadi, 2025), fine-tuned Electra model. For En-\nglish top team, CMI-AIGCX (Kaijie et al., 2025),\nused LLMs (Llama 2 and 3) and also fine-tuned\nXLM-roberta model."}, {"title": "5 Conclusion and Future Work", "content": "We presented an overview of the shared task on\nthe Academic Essay Challenge. The task attracted\nsignificant attention, with a total of 56 teams regis-\ntering to participate in the development and evalua-\ntion phases. Of these, 21 teams submitted official\nresults on the test set for Arabic, and 25 teams\ndid so for English. Finally, seven teams submitted\ntask description papers. Most systems fine-tuned\ntransformer-based language models; however, sev-\neral teams also incorporated additional features,\nsuch as style, language complexity, bias, subjec-\ntivity, and emotion. For both languages, the top-\nperforming teams achieved F1 scores above 0.98."}, {"title": "Limitations", "content": "A major limitation of the dataset is its small size,\nparticularly for Arabic, which restricts the devel-\nopment of more robust models. The challenging\nnature of academic essay collection is reflected in\nthe limited dataset size. Future studies could focus\non curating larger datasets to enable the creation\nof more challenging tasks and the development of\nmore robust models."}, {"title": "Ethical Considerations", "content": "The datasets used in the shared task may reflect\nsubjective biases or perspectives of the essay au-\nthors, even though they followed the provided in-\nstructions. Importantly, the datasets do not include\nany personal information, and no such informa-\ntion was collected during the data curation process.\nTherefore, we do not anticipate any ethical con-\ncerns related to privacy. Furthermore, the dataset\nwas shared only with participants who signed an\nagreement, ensuring responsible use of the dataset."}]}