{"title": "Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning", "authors": ["Bingchen Yan"], "abstract": "Few-shot image classification is a challenging task in the field of machine learning, involving the identification of new categories using a limited number of labeled samples. In recent years, methods based on local descriptors have made significant progress in this area. However, the key to improving classification accuracy lies in effectively filtering background noise and accurately selecting critical local descriptors highly relevant to image category information.\nTo address this challenge, we propose an innovative weighted adaptive threshold filtering (WATF) strategy for local descriptors. This strategy can dynamically adjust based on the current task and image context, thereby selecting local descriptors most relevant to the image category. This enables the model to better focus on category-related information while effectively mitigating interference from irrelevant background regions.\nTo evaluate the effectiveness of our method, we adopted the N-way K-shot experimental framework. Experimental results show that our method not only improves the clustering effect of selected local descriptors but also significantly enhances the discriminative ability between image categories. Notably, our method maintains a simple and lightweight design philosophy without introducing additional learnable parameters. This feature ensures consistency in filtering capability during both training and testing phases, further enhancing the reliability and practicality of the method.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning Deep learning models have achieved re-markable success across various computer vision do-mains when trained on large-scale manually annotated datasets [1]\u2013[5]. However, these models continue to face significant challenges when dealing with novel classes containing only a few labeled samples, often resulting in overfitting or con-vergence failure. In contrast, humans can effortlessly recog-nize new classes from a limited number of labeled samples by leveraging prior knowledge. Few-shot learning aims to bridge this gap by generalizing knowledge acquired from base classes (with abundant labeled samples) to novel classes (with limited labeled samples), thus garnering increasing attention [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15].\nThe field has witnessed the emergence of various exemplary few-shot learning methods, broadly categorized into three types: metric learning-based [2, 3, 4, 5, 11, 12, 13, 14, 15], meta-learning-based [8, 9, 10], and transfer-based [16, 17,\n18, 19, 20, 21, 22] approaches. Notably, metric learning-based methods have achieved significant success due to their simplicity and efficacy. This paper primarily focuses on this approach. The typical pipeline of metric learning-based few-shot learning methods encompasses three steps: 1) Feature extraction from all query and support images; 2) Distance computation between the query image and each support image, prototype, or class center using a specific metric; 3) Label assignment to query images through nearest neighbor search.\nDespite the impressive performance of metric learning-based few-shot learning methods, they are persistently plagued by noisy local regions irrelevant to image category information, as the semantics of local regions within images can vary significantly [14, 23]. As illustrated in Figure 1, some regions contain critical semantics consistent with image category in-formation, i.e., category-relevant information (e.g., the \"dog\" area in a \"dog\" image, or the \"bird\" area in a \"bird\" image). Conversely, other regions may contain semantics irrelevant to image category information, i.e., category-irrelevant informa-tion (e.g., the \"sky\" area in a \"dog\" image, or the \"grass\" area in a \"bird\" image).\nTo address this issue, GLIML[7] and KLSANet [5]employ a dual-branch architecture to simultaneously learn global and local features of images, selecting local features by measur-ing their similarity to the image's global features. Although this approach has yielded significant results, it substantially increases model complexity and computational time. BDLA [3]proposed computing bidirectional distances between local features of query and support samples to enhance the effective alignment of contextual semantic information in image local features.\nOur method builds upon previous work [1, 2, 3, 4] utilizing local descriptor-level image features. Aiming to maximize the elimination of noisy local regions irrelevant to image category information, we ingeniously propose a weighted dynamic filtering method for local descriptors in few-shot learning. Specifically, we innovatively introduce the concept of category-relevant weights for local descriptors. Through vi-sualization experiments, we demonstrate that these weights conform to a normal distribution in statistical terms. Based on this observation, we design an adaptive threshold strategy for weights, dynamically filtering out local regions with the highest relevance to category information.\nIn summary, our work makes the following key contribu-tions:\n\u2022 We propose a novel weighted dynamic filtering method for local descriptors in few-shot learning, which ef-fectively addresses the challenge of noisy, category-irrelevant regions in images.\n\u2022 We introduce the concept of category-relevant weights for local descriptors and empirically demonstrate their conformity to a normal distribution through visualization experiments.\n\u2022 Based on this statistical insight, we develop an adap-tive threshold strategy that dynamically selects the most category-relevant local regions, significantly enhancing the model's focus on pertinent information.\n\u2022 Our method achieves state-of-the-art performance on three widely-used few-shot learning classification datasets, surpassing existing metric learning-based approaches.\nOur method outperforms current state-of-the-art approaches on three commonly used few-shot learning classification datasets. More surprisingly, the experimental results on the CUB-200 dataset even surpass several recent transfer learning-based few-shot learning methods. We believe our method holds significant reference value for subsequent research in few-shot learning."}, {"title": "II. RELATED WORKS", "content": "Few-shot learning algorithms can be broadly categorized into three main classes: initialization-based methods, methods rooted in transfer learning and metric-based methods."}, {"title": "A. Initialization-based methods", "content": "Initialization-based methods [8, 9, 10, 24, 25, 26] utilize gradient updates to achieve effective initialization. MAML [8] introduced a powerful initialization technique that significantly enhances performance with just a few gradient steps, em-ploying a bi-level optimization strategy where the outer loop learns to generalize across tasks and the inner loop adapts to specific tasks. LEO [24] extends MAML by operating in a low-dimensional space to improve generalization in FSL tasks. Proto-MAML [25] combines the strong inductive bias of ProtoNet [11] with the flexible adaptation mechanism of MAML [8]. However, the MAML family typically uses a simple cross-entropy function for inner loop optimization, which can result in limited generalization performance. To address this, Baik et al. (2021) [27] proposed a task-specific loss function to update meta-learner parameters during the meta-training process. Wang et al. (2022) [26]provided a theoretical analysis of how MAML with deep neural networks converges to the global optimum and developed a specialized neural architecture search algorithm for FSL."}, {"title": "B. Methods rooted in transfer learning", "content": "Methods rooted in transfer learning frameworks have demonstrated competitive performance in the realm of few-shot learning, often rivaling meta-learning techniques. The general methodology of these approaches follows a distinct pattern:\nInitially, a classification model is trained on the entire available training dataset. Subsequently, the classification layer is discarded, preserving only the feature extraction compo-nent. Finally, utilizing the support set from the test data, a new classifier is developed and trained. This strategy has proven effective, with several notable implementations gaining traction in the field. Among these, Dynamic Classifier [28], Baseline++ [17], and RFS [30] stand out as particularly influential contributions."}, {"title": "C. Metric-Based Methods", "content": "Metric-based methods [2, 3, 4, 5, 11, 12, 13, 14, 15] aim to learn a universal metric space to measure the relationship be-tween query images and support sets, thereby quantifying their similarity. Matching Networks [12] determine the similarity between each support set sample and a query sample, predict-ing the query sample's label by computing a weighted sum of these similarities. Prototypical Networks innovatively average the support set features to form class prototypes and evaluate the Euclidean distance between the query and class prototypes in the embedding space [11]. Relation Networks compare the relation between images by learning a deep nonlinear metric. TADAM [28]enhances few-shot learning (FSL) by learning a task-dependent metric space through metric scaling.\nDespite their potential, current methods largely depend on image-level global features, assuming their transferability across seen and unseen classes, which is often unrealistic. In contrast, low-level features like local descriptors and local features are more likely to be shared among different classes and are expected to transfer better to unseen classes have demonstrated the superiority of local descriptors over global representations in few-shot image classification.\nFor instance, LMP-Net [13]leverages local descriptor-level features rather than global features in Prototypical Networks, learning multiple class prototypes for each class to capture the complex distribution of the class more comprehensively. DN4 [4] employs deep local descriptor representation and explicitly uses local descriptors through k-nearest neighbors (k-NN), while the Relational Network [15] implicitly mea-sures distances between query and support samples using local descriptors. However, local descriptors often contain redundant information from spatially adjacent areas, and the semantic local descriptors commonly shared by all classes are not crucial for recognizing new instances [23]. To address the limitations of local descriptor-based methods, ATL-Net [23]designs an episodic attention mechanism that can select and weight key local descriptors without overemphasizing the common parts across the entire task. BDLA [3] introduces the calculation of bidirectional distance between local descriptors of query samples and support samples to enhance the effec-tive alignment of contextual semantic information. KLSANet [5]utilizes randomly cropped local features instead of local descriptors, selecting key query local features by measuring their relationship to the image semantics to reduce the impact of irrelevant query parts on image semantics. However, ex-tracting both local features and global feature representations for each image significantly increases computational overhead and model complexity."}, {"title": "III. \u041c\u0415\u041d\u0422OD", "content": "Few-shot learning aims to develop models that excel with minimal data while maintaining robust generalization. We tackle the N-way K-shot challenge, where N represents class count and K denotes samples per class, typically a small number like 1 or 5.\nOur goal is to train model parameters 0, for swift adaptation to unseen data using episodic training. Each episode in both training and test datasets contains a support set S (N classes, K labeled images each) and a query set Q for evaluation.\nThe data is split into non-overlapping training, validation, and testing sets, each containing more classes and samples than N and K. These sets are then further divided into episodes with distinct support and query sets sharing the same label space.\nTo simulate real-world scenarios, all phases employ this episodic mechanism. For example, during training, random episodes are selected for parameter updates until convergence. In validation and testing, the model classifies the query set based on the support set."}, {"title": "B. Overview", "content": "As illustrated in Figure [X], our proposed approach com-prises three principal components: the Embedding Feature Extraction Module (EFEM), the Weighted Adaptive Threshold Filtering Module (WATFM), and the Key Local Descriptors Classification Module (KLDCM).\nInitially, we employ an embedding network constructed on the episodic learning mechanism to extract local descriptor-level embedding features from both the support set and query set images. Subsequently, the WATFM computes weight infor-mation for each local descriptor of the images in the support and query sets. This process enables the identification and se-lection of key local descriptors while eliminating background noise, thereby enhancing few-shot classification performance. In the final stage, we input the filtered key local descriptors from both the support and query set images into a k-Nearest Neighbors (k-NN) classifier, a commonly used technique in previous works. This classifier then generates the predicted class labels for the query set images."}, {"title": "C. EFEM", "content": "We utilize a widely-used neural network, typically a Convo-lutional Neural Network (CNN) or ResNet, following previous work, to serve as a local descriptor feature extractor. This local descriptor feature extractor can be implemented by removing the last pooling layer or the fully connected layer of the neural network. To illustrate with a CNN as an example:\nEach image X is passed through the CNN to obtain a three-dimensional (3D) tensor $F_{\\theta}(X) \\in \\mathbb{R}^{C\\times H \\times W}$. This tensor represents the image, where F(X) is the hypothesized function learned by the CNN, $\\theta$ stands for the parameters of the CNN, and C, H, and W denote the channel, height, and width of the 3D tensor, respectively. This can be expressed as:\n$F_{\\theta}(X) = [x_1,...,x_M]\\in \\mathbb{R}^{C\\times M}$  (1)\nHere, M = H \u00d7 W, maps all images to a representational space. Each 3D tensor contains M units of C dimensions, with each unit representing a local descriptor of the image."}, {"title": "D. WATFM", "content": "Due to the large intra-class variation and background clutter, the measurement of using all local descriptors directly for few-shot image classification is far from satisfactory. Therefore, it is more reasonable to filter out the local descriptors most rele-vant to the category and then carry out subsequent operations.\nOur local descriptor filtering strategy is based on the fol-lowing premise: As shown in Figure X, in a typical few-shot task, the support set usually consists of five categories, with N typically set to 5. For K support set images of a category, if a local descriptor in one of the K support set images is category-relevant (containing exact representative features of that category), then similar local descriptors should exist in the other (K- 1) support set images. Conversely, if a local descriptor comes from a background area irrelevant to the category of the support set image, the likelihood of similar local descriptors appearing in the other (K - 1) support set images of the same category is low, and they may even appear in support set images of other categories.\nFollowing the approach of ProtoNet [11], we calculate the category prototype for each support set category by averag-ing, which possesses more comprehensive and representative information related to the support set category, used for key local descriptor filtering. The filtering process includes two main steps. First, we compute the similarity between each candidate local descriptor of the support sample and its support set category prototype. In the feature embedding space, we denote the prototype representation of the nth category as Cn, where n \u2208 [1, \u039d].\nFor a support set image, we obtain the local descriptor representation as follows:\n$F_{\\theta}(X_s) = [x_1^s,...,x_M^s]\\in \\mathbb{R}^{C\\times M}$ (2)\nwhere $x_i^s$, i \u2208 [1, M] represents the ith local descriptor extracted by EFEM belonging to the support set image, and M represents the number of local descriptors.\nWe then calculate the similarity between each category pro-totype and each local descriptor using the following formula: The importance $w_{in}$ of the local descriptor $x_i^s$ for the n-th class can be estimated by the normalized cosine similarity be-tween the local descriptor $x_i^s$ and the prototype representation $c_n$, i.e.,\n$w_{in} = \\frac{e^{cos(x_i^s,c_n)}}{\\sum_{i=1}^M e^{cos(x_i^s,c_n)}}$ (3)\nAs seen from Equation 3, local descriptors containing category-relevant information for the kth class will have higher importance weights, while those containing category-irrelevant information will have lower weights.\nBased on the calculated weights, we select local descriptors with high weights for subsequent processing while ignoring those with low weights. We accomplish this by setting an adaptive threshold that automatically adjusts according to the weight distribution, retaining only local descriptors with weights above this threshold. Our threshold filtering strategy adapts to the number of key local descriptors, dynamically changing according to the current task and different image local contexts.\nSpecifically, through Equation (3), we calculate a weight matrix W with shape [L, N, M], where L represents the number of support set or query set samples, N represents the number of categories, and M represents the number of local features per sample.\nWeight Aggregation and Expansion: In our method, the weight of each local descriptor is five weights for five categories, each weight corresponding to a specific category. To derive the importance of each local descriptor across all five categories, we need to average the weight values of the five categories, thus obtaining the importance of this local descriptor for the five categories, i.e., whether this local descriptor is important for the main subjects of images across all five categories.\nThe formula is as follows:\n$W_i = \\frac{1}{N} \\sum_{n=1}^N w_{in}$ (4)\nwhere $W_i$ represents the average weight of the ith local descriptor, N represents the number of categories, $w_{in}$ repre-sents the weight of the ith local descriptor for the nth category, where n \u2208 [1, N], i \u2208 [1, \u039c].\nThreshold Calculation: To determine the adaptive threshold, we first calculate the mean and standard deviation of the average weights of all local descriptors:\n$\\mu = \\frac{1}{L \\times M} \\sum_{j=1}^L \\sum_{i=1}^M W_{i,j}$ (5)\n$\\sigma = \\sqrt{\\frac{1}{LXM} \\sum_{j=1}^L \\sum_{i=1}^M (W_{ij} - \\mu)^2}$ (6)\nwhere $\\mu$ represents the mean of the average weights of all local descriptors, $\\sigma$ represents the standard deviation of the average weights of all local descriptors, and $W_{i,j}$ represents the average weight of the ith local descriptor of the jth support or query sample. In statistics, the 68-95-99.7 rule is\n$\\tau= \\mu - \\sigma$ (7)\nThis corresponds to the part in the normal distribution that is greater than one standard deviation from the mean, which accounts for approximately 15.87% ((100% - 68.27%) / 2) of the total. Therefore, we actually retain about 84.14% of the local descriptors, whose weights are higher than or equal to the mean and can be considered more important parts for the main subjects of images across the five categories.\nFiltering Strategy: Based on the calculated threshold $\\tau$, we retain all local descriptors with weights higher than $\\tau$:\n$F_{o iltered}(X) = {X | W_{i,j} > \\tau}$ (8)\nwhere $F_{o iltered}(X)$ represents the set of filtered local descrip-tors, $x_i^j$ represents the ith filtered local descriptor of the jth support or query sample. Figure 3 shows the visualization results of local descriptors for four randomly sampled 5-way 1-shot classification tasks, comparing the cases with and without our WATF module. As shown in the Figure 5, when using WATF, the selected local descriptors within each cluster exhibit a more compact arrangement, indicating that it is easier to distinguish local descriptors between different image categories.\nAfter filtering the local descriptors of the support set through the above steps, we recalculate the category prototypes and repeat the above local descriptor filtering on the query set using the updated category prototypes. The algorithm flow is shown in Algorithm Pseudocode X.\nThrough our WATF module, the neural network can focus attention on the category-relevant key information of the im-age, improving the representation of support set and query set images, mitigating the negative impact of category-irrelevant non-target areas. Moreover, our filtering method maintains simplicity and lightweight design without introducing addi-tional learnable parameters, ensuring consistency in filtering capability during both training and testing phases."}, {"title": "E. KLDCM", "content": "To predict the category of a query image, we extend the concept of image-to-class measure, utilizing the selected local descriptors for classification. Specifically,\nThe key local descriptors of a given query image q selected after WATFM filtering are represented as:\n$F_{o iltered}(X_q) = [x_1^q,...,x_H^q] \\in \\mathbb{R}^{C\\times H}$ (9)\nwhere H < M. After WATFM filtering, each category in the support set can be represented as class i (i = 1,2,3,...,5). For each filtered key local descriptor $x_h^q$ of q, where h\u2208 [1,H], we find its k nearest neighbors denoted as $n_1,\\ldots, n_k$ in each filtered support set local descriptor and compute the corre-sponding cosine similarities as $cos(x_h^q, n_1),\\ldots, cos(x_h^q, n_k)$. The similarity score between image q and class i is defined as:\n$Score(q, class i) = \\sum_{h=1}^H \\sum_{j=1}^k cos(x_h^q, n_j)$ (10)\nThen, we use softmax to obtain the probability that the category $y_q$ of q is class i:\n$p(y_q = i | q) = \\frac{exp (score (q, class i))}{\\sum_{i=1}^5 exp (score (q, class i))}$"}, {"title": "IV. EXPERIMENT", "content": "CUB-200 is a fine-grained bird image classification dataset involving 200 different bird species. The number of images per category varies, with 130 categories used for training, 20 for validation, and the remaining 50 for testing.\nThe Stanford Dogs dataset focuses on fine-grained dog image classification, comprising 20,580 photographs of 120 different dog breeds. 70 dog breeds are used for training, 20 for validation, and the remaining 30 for testing.\nThe Stanford Cars dataset is designed for fine-grained car image classification, containing 16,185 images of 196 different car categories, defined by make, model, and year of manufacture. 130 categories are used for training, 17 for validation, and the remaining 49 for testing."}, {"title": "B. Implementation Details", "content": "In our experiments, we primarily focus on 5-way 1-shot and 5-shot classification tasks. To ensure fair comparison with other methods, we employ two commonly used backbone network structures in few-shot learning: Conv4 and ResNet-12, following the implementation details outlined in DN4 [4]and CovaMNet [29].\nDuring the training phase, we use the Adam optimization algorithm (Kingma & Ba, 2014) with an initial learning rate of 0.001, which is halved every 100,000 episodes.\nIn the testing phase, to ensure the reliability of the experi-mental results, we randomly construct 600 episodes from the test set of each dataset to evaluate the model's performance. We select the best model based on the accuracy on the validation set and then evaluate it on the test set, which contains new classes. Each randomly sampled new task from the test set is similar to the training tasks, containing 5 classes, with K (1 or 5) support samples per class and 15 query samples per class. The test results are reported as the mean accuracy over 600 new tasks with a 95% confidence interval. It is worth noting that our model is trained end-to-end from scratch, with no fine-tuning performed during the testing phase."}, {"title": "C. Experimental Results", "content": "General few-shot classification: To validate the effec-tiveness and superiority of our proposed WATFM method, we compare our approach with 14 state-of-the-art few-shot classi-fication methods on three fine-grained datasets, as summarized in Table I.\nIt can be observed that WATF with ResNet-12 backbone significantly outperforms all comparison methods on most settings across the three datasets. Benefiting from less noisy local features, it can more accurately depict discriminative regions, showing significant improvements compared to other methods. In 1-shot and 5-shot settings, even using the same four-layer convolution as a local feature extractor, WATF im-proves accuracy by an average of 9.27% and 2.75% compared to the DN4 method that does not process local descriptors. This reveals to some extent how poor local descriptor represen-tations can degrade classification performance in fine-grained image classification scenarios.\nNotably, existing metric-based few-shot image classification methods can be divided into three categories based on feature level: global feature-based methods (e.g., ProtoNets [11], GNN [30], and QPN [31]), local descriptor-based methods (e.g., DN4 [4], DN4-DA [4], RelationNet [15], MADN4 [2], TDSNet [14], LMPNet [13], ATL-Net [23], CovaMNet [29], and BDLA [3]), local random crop feature-based methods (e.g., KLSANet [5]), and methods combining global features and local descriptors (e.g., GLCL[6], GLIML [7]). The recent excellent performance of local random crop feature-based methods once raised doubts about whether local descriptors were too detailed, losing crucial image local semantic infor-mation. Our method's outstanding performance reaffirms the superior position of local descriptor-level features in few-shot image classification.\nSpecifically, in 1-shot and 5-shot settings, WATF improves by an average of 10.05% and 4.91% compared to the best global representation-based method QPN, and by an average of 8.77% and 4.71% across the three datasets compared to the local feature-based method KLSANet. Comparing the experimental results with methods like BDLA and Hao, which also focus on improving local descriptor semantic alignment [BDLA, Hao], demonstrates the superiority of our method that does not introduce additional learnable parameters. Convergence Analysis. To analyze the convergence of WATF, we present its training loss, validation loss, and test loss curves under the 5-way 1-shot setting across three datasets in Figure 4. Across all three datasets, we observe that around the 50th epoch, the test loss stops decreasing, indicating model convergence. This demonstrates that our model is optimally trained and able to converge rapidly. Furthermore, we employ validation accuracy for model weight file selection in experi-ments to avoid overfitting."}, {"title": "2) Cross-domain Few-Shot Classification", "content": "To evaluate the cross-domain generalization of WATF, we conducted experi-ments in the miniImageNet\u2192CUB setting (see Table II) and compared it with state-of-the-art methods. The model was trained on 64 base classes from miniImageNet and perfor-mance was evaluated on 50 novel classes in the CUB test set. WATF demonstrated significant advantages in this cross-domain scenario, achieving an accuracy of 48.39% in the 5-way 1-shot setting and 68.92% in the 5-way 5-shot setting.\nIt also outperformed classic few-shot methods such as MatchingNet, ProtoNet, RelationNet, and GNN, for example, surpassing ProtoNet by 3.08% and 6.27% in 1-shot and 5-shot settings, respectively. Notably, compared to methods tailored for cross-domain scenarios (such as Finetuning, LRP, MN+AFA, baseline, baseline++, GNN+FT, and FDMixup), WATF maintained a lead. For instance, in 1-shot and 5-shot settings, it outperformed the FDMixup method (which advocates using limited labeled target data to guide cross-domain learning) by 2.01% and 3.58%, respectively."}, {"title": "D. Ablation Studies", "content": "Impact of WATFM: This paper proposes WATF, which innovatively introduces a weighted local descriptor adaptive threshold filtering strategy to improve classification perfor-mance. This section investigates the effectiveness of our method in eliminating class-irrelevant noise information by comparing the experimental accuracy using our WATF strategy against that without any processing of local descriptors.\nAs shown in Table III, where \"w/\" and \"w/o\" denote the use and non-use of the WATF strategy respectively, the results demonstrate that our WATF strategy can eliminate class-irrelevant noise information. Furthermore, since our WATF module achieves the elimination of noisy local descriptor features, it should produce more effective representations. To better understand the changes in local descriptor feature distribution before and after WATF, we visualize their repre-sentations in two-dimensional space using t-SNE technology. Figure 5 shows the distribution of support set local descrip-tor features before and after applying WATF on three fine-grained classification datasets. From the visualization, it can be observed that after applying WATF in each class, the local descriptor features become more tightly clustered together, and the class boundaries become clearer. The extensive experi-mental evaluations in this study confirm that the enhanced inter-class separability contributes to the subsequent k-NN classifier's improvement in classification, enhancing feature stability.\n2) Impact of Different k Values in k-NN Classifier on Experimental Results: Following the work of DN4 [4], BDLA [3], and DLDA [32], we employ the k-nearest neighbors (k-NN) model as the classifier to align the similar semantic information between local descriptor features of images. To investigate the impact of different k values on the results of the FAFD-LDWR method, we conducted k-NN parameter analysis"}, {"title": "V. CONCLUSION", "content": "In this study, we propose a effective WATF method to enhance the performance of few-shot learning."}]}