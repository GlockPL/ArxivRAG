{"title": "Learning to Correction: Explainable Feedback Generation for Visual Commonsense Reasoning Distractor", "authors": ["Jiali Chen", "Xusen Hei", "Yuqi Xue", "Yuancheng Wei", "Jiayuan Xie", "Qing Li", "Yi Cai"], "abstract": "Large multimodal models (LMMs) have shown remarkable performance in the visual commonsense reasoning (VCR) task, which aims to answer a multiple-choice question based on visual commonsense within an image. However, the ability of LMMs to correct potential visual commonsense errors in the distractor upon their occurrence is yet under-explored. Drawing inspiration from how a human teacher crafts challenging distractors to test students' comprehension of the concepts or skills and assists them in identifying and correcting errors toward the answer, we are the pioneering research for LMMs to simulate this error correction process. To this end, we employ GPT-4 as a \u201cteacher\u201d to collect the explainable feedback dataset VCR-DF for error correction, which serves as a benchmark to evaluate the ability of LMMs to identify misconceptions and clarify reasons behind the error in VCR distractors toward final answers. In addition, we propose an LMM-based Pedagogical Expert Instructed Feedback Generation (PEIFG) model to incorporate the learnable expert prompts and multimodal instruction as guidance for feedback generation. Experimental results show that our PEIFG significantly outperforms existing LMMs. We believe that our benchmark provides a new direction for evaluating the capabilities of LMMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual commonsense reasoning (VCR) task aims to predict the answer to the multiple-choice question and provide a convincing rationale [11, 24, 44, 45, 49] about the image. In recent years, it has gained considerable attention from computer vision (CV) and natural language processing (NLP) communities due to the advancement of large multimodal models (LMMs) [3, 14, 24, 41, 46, 51]. Specifically, inferring a reliable answer in VCR requires LMMs to not only recognize objects and scenes but also deeply understand the underlying visual commonsense (e.g., likely intents, goals, and social dynamics of people) in the image.\nScrutinizing existing LMMs [3, 14, 24, 51], we identify their common paradigm as two stages: pre-training and instruction tuning with a large language model. Specifically, they are first pre-trained on large-scale image-text pairs for modality alignment and then construct instruction data that combines both visual and language features, which is fed into the large language model to infer the answer. By this training paradigm, they often demonstrate strong"}, {"title": "2 RELATED WORK", "content": "Equipping machines with multimodal reasoning ability is a long-standing goal of artificial intelligence (AI) systems [28, 48, 53]. This form of reasoning empowers machines to emulate human-like cognition and commonsense understanding of the world. Recent progress in multimodal learning has been driven by incorporating visual features with pre-trained large language models as large multimodal models (LMMs) [8, 17, 27, 54]. The current paradigm for training LMMs primarily comprises two stages (image-text pre-training and instruction tuning). Specifically, during the image-text pre-training stage, these LMMs are initially trained on a large scale of image-text pairs for cross-modal alignment [1, 21, 22, 47]. This process ensures that both visual and textual input information are effectively mapped into a unified semantic space. For example, BLIP-2 [22] design a Q-Former to align visual and textual features in the pre-trained stage. During instruction tuning, LMMs fine-tune on multimodal instruction datasets to enhance their instruction-following ability and tackle more complex multimodal tasks. These instruction datasets originate from manually annotated data [5, 9, 15, 30, 38] and data generated by GPT-4 [4, 23, 26, 27, 47]. InstructBLIP [8] builds upon the BLIP-2 framework by fine-tuning Q-Former with instruction data. MiniGPT-4 [54] directly employs a linear layer to project visual features into the semantic space of language, leveraging instruction data for this process. LLaVA [27] is the first LMM to fine-tune through self-instruction, which employs language-only GPT-4 to generate instruction-following data with some manual samples for fine-tuning. Hong et al. [17] propose an 18B LMM CogAgent for graphical user interfaces (GUI) under-standing and navigation, which utilizes both low-resolution and high-resolution image encoders. Meanwhile, it achieves the state-of-the-art on various reasoning benchmarks. Moreover, some other works [6, 16, 29, 39] use the large language models as the controller, which aims to control various visual modules with code generation for reasoning. Specifically, VisProg [16] and ViperGPT [39] utilize predefined APIs to access available modules, and compose them by generating Python code for execution without any task-specific training. Existing LMMs primarily focus on forward reasoning ca-pabilities in multimodal tasks. However, their ability to analyze the causes of errors and rectify them is yet under-explored. To the best of our knowledge, we are the first to investigate LMMs for error correction in the visual commonsense reasoning task."}, {"title": "3 DATASET CONSTRUCTION", "content": "Existing datasets for visual commonsense reasoning (VCR) [11, 49] are mainly structured as multiple-choice questions. However, they are inadequate for comprehensively assessing models' visual com-monsense reasoning capability and their proficiency in error correc-tion. The reasons are as follows: i) The original distractors within these datasets contain inherent bias, where excessive overlap be-tween the entities of the image and answer, and the distractors often lack relevance to the question and image. It results in a defi-ciency of visual commonsense errors within these distractors. ii) They are deficient in feedback mechanisms to identify and address errors within distractors, which is essential for timely correction. Inspired by cognitive-developmental theory in the field of peda-gogy [31], teachers often employ Bloom's taxonomy [2] to identify questions at various levels and then craft corresponding distracors, aiming to probe students' potential cognitive challenges. Moreover, they provide feedback to assist students in understanding and cor-recting their errors after failing to solve a problem. Therefore, we construct the VCR-DF dataset as a benchmark for evaluating the error correction ability of large multimodal models (LMMs) in vi-sual commonsense reasoning. In total, the VCR-DF dataset contains 22,401 data samples and splits 20,163 and 2,238 samples for training and testing respectively.\nSpecifically, our VCR-DF dataset is derived from the original VCR dataset [49], which leverages GPT-4 [32] for the new distractors and feedback data collection. For the input image I, question Q, and correct answer A, we prompt GPT-4 to generate multiple distractors {Di}N1 and corresponding feedback {Fi}N1. The prompts for GPT-4 are shown in the Supplementary. In the following subsection, we will detail the procedure of data collection."}, {"title": "3.1 Distractor and Feedback Collection", "content": "To avoid the laborious demand of human annotation, we design a data reformation pipeline assisted by language-only GPT-4 and a manual filtering process for distractor and feedback collection. Specifically, the source images are from the original VCR dataset [49]. We also preserve questions, correct answers and object boxes pro-vided in VCR."}, {"title": "3.1.1 Distractor Collection", "content": "For distractor data collection, we guide GPT-4 to identify the Bloom's taxonomy level of the ques-tion with the given answer, and generate five distractors with the corresponding Bloom's taxonomy level for each QA pair based on the given inputs (i.e., manually annotated image events and places, questions, answers, and object boxes).\nConsidering the potential mistakes in annotations with GPT-4, we develop a web page for manual distractor filtering to remove those of lower quality. We first ask a group of trained annotators to assess whether each distractor is related to the question and image. Upon confirming relevance, they need to review the distractor against the image, answer, and question for inaccuracies, where distractors without errors are discarded. Finally, annotators rank the distractors and select the top-3 of them."}, {"title": "3.1.2 Feedback Collection", "content": "Given the above input information and each filtered distractor, we instruct GPT-4 to identify the mis-conception and explain the error in the distractor, serving as the"}, {"title": "4 METHODOLOGY", "content": "Our goal is to develop a large multimodal model (LMM) for explain-able feedback generation, which simulates the teaching process of educators. Specifically, given the input image I, question Q, correct answer A, and N distractors {Di}Ni=1, we identify the correspond-ing Bloom's taxonomy level of the question, and then generate the misconception and explanation as feedback {Fi}Ni=1. The overall architecture of our proposed Pedagogical Expert Instructed Feed-back Generation (PEIFG) model is shown in Fig. 2, which consists of three components: (i) visual feature extractor (VFE), which adds the object box information as visual markers into the image and obtains the contextually enriched visual features. (ii) expert prompt selector (EPS), which incorporates language instructions and vi-sual features to select the most relevant expert prompts as expert knowledge from a learnable prompt pool. (iii) text generator, which constructs a multimodal instruction for the large language model (LLM), integrating the visual features, expert prompts and language instruction to generate the feedback. The details of each component are shown in the following subsections."}, {"title": "4.1 Visual Feature Extractor", "content": "Different from previous visual question answering datasets (e.g., VQA v2.0, OKVQA and so on), where the input is an image and ques-tion. The VCR dataset provides the model with additional object boxes including multiple instances of the same object category (e.g., person0 and person1). Consequently, we first automatically anno-tate the objects within the image with the provided boxes as visual markers, as shown in Fig. 2. Then, we introduce a visual marker perceiver (VMP) to comprehend the visual markers information (i.e., object boxes and object text annotation) in the image by stage 1 training, as shown in Fig. 2. The region-level features from the VMP and global-level features from CLIP [34] image encoder are concatenated as final visual features."}, {"title": "4.1.1 Visual Marker Perceiver", "content": "Given the image with visual markers, we enable the visual marker perceiver (VMP) to under-stand the object with markers in stage 1 training. Specifically, we initially reshape the image into high resolution (i.e., 1024 \u00d7 1024) and use the SAM-base [20] backbone with two convolution layers as VMP to obtain the region-level visual features.\nFollowing [43], we fed the region-level features and detection in-struction into a language model (i.e., OPT-350M) [50] to effectively enhance the VMP capability in discerning object spatial information through visual markers. Technically, we employ a multilayer per-ception (MLP) to map region-level visual features into the semantic"}, {"title": "4.1.2 Visual Features Extraction", "content": "After stage 1 training, we employ the trained VMP and CLIP image encoder to obtain region-level and global-level features, respectively. Specifically, given the high-resolution image with visual markers Ih, VMP extracts the region-level visual features. Meanwhile, we utilize the CLIP image encoder [34] to encode the low-resolution image Ir \u2208 \\mathbb{R}^{224\u00d7224}, and then obtain the global-level visual features. Subsequently, we map both types of visual features into the semantic space of language, enhancing their compatibility with the input format of the large language model for further feedback generation, denoted as vr and vg respectively. The process of visual feature extraction can be formulated as:\n$v_r = MLP_r(f_r(I_h))$,\n$v_g = MLP_g(f_g(I_r))$,\nwhere fr and fg represent the VMP module and CLIP image en-coder. MLPr and MLPg are multilayer perception (MLP) layers. vr \u2208 \\mathbb{R}^{L_d\u00d7d} and vg \u2208 \\mathbb{R}^{L_d\u00d7d}. Ld is the length of image patches and the dimension of features d is 1024. Finally, we concatenate vr and ug as the integrated visual features, denoted by v = [vr; vg], where v \u2208 \\mathbb{R}^{L_d\u00d72d} and [;] is the concatenation operation."}, {"title": "4.2 Expert Prompt Selector", "content": "The introduction of the expert prompt selector (EPS) is motivated by the desire to enable our model to emulate the diverse exper-tise of teachers as expert knowledge in generating feedback. Thus, we maintain a learnable prompt pool representing diverse expert knowledge and employ instruction-aware visual features (i.e., inte-grating both visual and language instruction) to select the relevant expert prompts from the pool."}, {"title": "4.2.1 Instruction-aware Visual Feature", "content": "We design manually crafted natural language instructions to align with the integrated visual features for further expert prompt selection. Additionally, we supplement these instructions with the corresponding question and answer as auxiliary information. Following [8], we use the Query Transformer (Q-Former) [22] module to extract the instruction-aware visual features. Technically, within the Q-Former, learnable query tokens align with the language instructions by self-attention mechanisms and align with the integrated visual features by cross-attention mechanisms. Next, the output sequence of Q-Former is fed into an average-pooling layer to obtain instruction-aware visual features us. The computation of this process can be expressed as:\n$\u03c5_s = Avg(Q-Former(X_q, X_n, v))$,\nwhere us \u2208 \\mathbb{R}^{768}. Xq \u2208 \\mathbb{R}^{L_q\u00d7768} is the learnable query tokens and Xn \u2208 \\mathbb{R}^{L_n\u00d7768} denotes embeddings of the manually crafted"}, {"title": "4.2.2 Expert Prompt Selection", "content": "To select the expert prompts rel-evant to the current input information (i.e., the image, question and answer), we use the instruction-aware visual features as guidance for selection. We first build a learnable prompt pool to maintain diverse expert knowledge which can be formulated as:\n$P = {P_1, P_2, ..., P_S}$,\nwhere Pi \u2208 \\mathbb{R}^{L_p\u00d7768} and Lp is the length of each expert prompt. S denotes the size of the prompt pool. Given the necessity for the pool to encompass a wide array of expert prompts, we consider that each prompt should retains its distinct expertise. Each prompt should be jointly independent in the sense that each of them contains unique information. To this end, we design the expert correlation loss to reduce the correlation among the expert prompts in the pool by minimizing their inner product:\n$L_{cor} = ||PP^T - diag (PP^T)||_F$,\nwhere diag(\u00b7) only preserves diagonal entries and || \u00b7 ||F is the square of the Frobenius norm. Subsequently, we employ the query and match key-value based strategy for expert prompt selection. Concretely, we utilize the average pooling result of each expert prompt as its corresponding key, denoted as {k1, k2, ..., kS}, where ki \u2208 \\mathbb{R}^{768}. Importantly, the keys are updated corresponding to the expert prompts during training. Given the input information, we aim to find out the top-K keys {km1, km2, ..., kmK}, making them closer to the input sample, where {mj}Ki=1 denotes a subset of K (1 \u2264 K \u2264 S) indices of keys in the pool. Specifically, we calculate the cosine similarity sim(\u00b7) between keys and the instruction-aware vi-sual features us to select top-K keys and calculate the key matching loss for selection:\n$\\hat{k} = \\underset{\\{m_j\\}_1^K\\subset[1,S]}{argmin} \\sum_{i=1}^K sim (u_s, k_{m_i}), $\n$L_{se} = - \\sum sim(k_{m_i}, U_s),$\ni\nk\nwhere k is the set of top-K keys. Finally, we obtain the top-K expert prompts from the pool corresponding to the selected keys:\n$\\hat{P} = [P_{m1}; P_{m2}; ...; P_{mK}],$\nwhere P \u2208 \\mathbb{R}^{(K\u00d7L_p)\u00d7768}, mi denotes indices of selected prompts P and [;] is the concatenation operation."}, {"title": "4.3 Text Generator", "content": "Upon acquiring the integrated visual features and top-K expert prompts, we fuse them into the LLM-based text generator (i.e., QWen1.5) for feedback generation. Specifically, we adopt the widely used instruction tuning method to incorporate the language prompt, integrated visual features, and expert prompts into multimodal in-struction. We first utilize a multilayer perception (MLP) to project the expert prompts into a 2d dimensional space, meeting the input requirement of the LLM. The multimodal instruction is defined as: \"Image: <img>. Expert: <expert>. Please generate the feedback based on the question: {Question}, answer: {Answer}, distractor: {Distractor}\". The special tokens \"<img>\u201d and \u201c<expert>\" are re-placed by integrated visual features and selected expert prompts, respectively. \"{Question}\", \"{Answer}\" and \"{Distractor}\" are the input question, answer and distractor of a specific sample. The mul-timodal instruction is directly fed into the frozen large language"}, {"title": "4.3.1 Refinement", "content": "To ensure the logical coherence between the feedback and input information, we leverage the trained PEIFG model to generate pseudo training feedback data, which is then used to refine the model performance with the assistance of GPT-4. Specifically, we randomly sample from the VCR-DF training data and employ the top-p sampling method to generate M feedback instances for each sample, denoted as {F1, F2, ..., FM}. Given the question, ground truth distractor, feedback, and the generated feed-back, we formulate five diagnostic questions for GPT-4 to ascertain whether the generated feedback meets the specified criteria. For each diagnostic question, feedback that meets the criteria is awarded 1 point, otherwise 0 point. Therefore, the final diagnostic score sd for each generated feedback ranges from 0 to 5. Finally, we rank the generated feedback as pairs based on the diagnostic score and utilize the direct preference optimization (DPO) [36] as reinforcement learning algorithm to further fine-tune the LLM. More details about the refinement process are shown in the Supplementary."}, {"title": "4.4 Training Objective", "content": "In our setting, we treat the generation of feedback and distractor as two distinct tasks. Specifically, we generate feedback for error correction for each given distractor within the VCR-DF dataset. The objective of our PEIFG model is to minimize the total loss, i.e., key matching loss in Eq. 6, correlation loss in Eq. 4 and language modeling loss in Eq. 8. The definition of total loss is:\n$L = \\frac{1}{D} \\sum_{t=1}^{Ma} (L_{llan} + \u03bb_1L_{cor} + \u03bb_2L_{se}),$\nwhere D is the total number of training samples, \u03bb1 and \u03bb2 stand for hyperparameters. It is worth noting that the refinement loss Lre is applied to optimize the model's parameters after the PEIFG has been trained."}, {"title": "5 EXPERIMENT", "content": "5.1 Implementation Details\nWe implement our PEIFG model with Pytorch and train it on two RTX 3090 cards. For the visual feature extraction, we employ the SAM-base [20] backbone and two convolution layers as the visual marker perceiver (VMP). Furthermore, in this stage 1 training for the VMP module, we use AdanW [19] optimizer with an initial learning rate of 5e-5. The ViT-L/14 [12] pre-trained in CLIP [34] is used for the image encoder, where ViT-L/14 denotes ViT-Large model with the patch size 14 \u00d7 14. Thus, the length of image tokens Ld = 256 and the dimension of features d is 1024. When selecting"}, {"title": "5.2 Baseline and Ablation Models", "content": "5.2.1 Baseline Models. In this paper, we evaluate our proposed PEIFG by comparing it with two types of baseline models.\n\u2022 Explanation-Enhanced visual question answering models in-cluding NLX-GPT [37] and KICNLE [42]. Specifically, NLX-GPT adopts the pre-trained CLIP encoder and GPT-2 [35] language model for feedback and distractor generation. KICNLE incorpo-rates external knowledge and designs a multi-iteration generative approach to ensure logical consistency between the generated distractor and feedback.\n\u2022 Multimodal large language models (LMMs) with different pa-rameter scales from 3B to 18B, including BLIP-2 [22], Instruct-BLIP [8], VisualGLM [13], LLaVA-v1.5 [27] and CogAgent [17]. Specifically, we integrate the LoRA layers into the self-attention mechanisms of LLMs. Furthermore, we randomly select 200 sam-ples for comparison between our model and the GPT-4 with vision (GPT-4V) [32], which boasts parameter scales above 175B and has the best performance on various multimodal tasks.\nMore implementation details of baseline models are provided in the Supplementary."}, {"title": "5.2.2 Ablation Models", "content": "To investigate the performance effect of each module in PEIFG, we compare the following variants of our method on VCR-DF dataset. We independently conduct corre-sponding ablation experiments for each module.\n\u2022 PEIFG w/o stage 1: PEIFG without stage 1 training for visual marker percevier.\n\u2022 PEIFG w/o VMP: PEIFG without visual marker percevier for visual feature extraction.\n\u2022 PEIFG w/o CLIP: PEIFG without CLIP image encoder for visual feature extraction.\n\u2022 PEIFG w/o EPS: PEIFG without expert prompt selector, which removes expert prompts from the multimodal instruction.\n\u2022 PEIFG w/o Ref: PEIFG without refinement for generation."}, {"title": "5.3 Evaluation Metric", "content": "5.3.1 Automatic Evaluation Metrics. We evaluate the perfor-mance with eight standard metrics, including BLEU-(1 to 4) [33], ROUGEL [25], METEOR [10], CIDEr [40], and BERTScore [52]. These metrics are commonly used for evaluating text generation and we compute metric values using the publicly available code\u00b2."}, {"title": "5.4 Results and Analysis", "content": "5.4.1 Performance Comparison. Table 1 and 2 show the auto-matic evaluation results of baselines and our model on feedback and distractor generation. We find that: i) Experimental results in Table 1 provide evidence that our PEIFG also surpasses the open-source baselines on feedback generation task. Specifically, compared with two explanation-enhanced visual question answering models (i.e., NLX-GPT and KICNLE) trained with full fine-tuning, the feedback generated by them often fails to produce lengthy textual content of the feedback, while merely providing the correct answer. It suggests"}, {"title": "5.4.2 Ablation Study", "content": "We conduct ablation experiments to ver-ify the effectiveness of different components in our PEIFG model. Experimental results are also shown in Table 1. We observed that: i)"}, {"title": "A BENCHMARK CONSTRUCTION", "content": "In this section, we will describe the prompts used to generate dis-tractors and feedback from GPT-4."}, {"title": "A.1 Prompt for GPT-4", "content": "Given the question, answer, place, object boxes, and events depicted in the image, we utilize a language-only GPT-4 to generate distrac-tors and feedback. Specifically, we first utilize GPT-4 to identify the educational level of the given question based on Bloom's taxonomy, as shown in Table 8. The prompts used for distractors and feedback generation are detailed in Table 9 and 10, respectively."}, {"title": "B MORE EXPERIMENTAL DETAILS", "content": "In this section, we provide more implementation details and exper-imental results of our PEIFG model and the baseline models."}, {"title": "B.1 Instruction for Q-Former", "content": "Fig. 4 shows the diverse instructions for Q-Former, which are used for alignment between visual features and learnable query tokens. The diversity of instructions demonstrates the same meaning with"}, {"title": "B.2 Refinement Details", "content": "For the refinement step, we randomly sample 800 instances from the training set and employ a top-p sampling strategy, where the temperature is set to 0.8 and p = 0.95 to generate four candidate gen-erated feedback for each input sample. Given the question, ground truth distractor, feedback, and the generated feedback, we design five diagnostic questions for GPT-4 to ascertain whether the gen-erated feedback meets the specified criteria. These questions and a diagnostic example are shown in Table 11. Specifically, for each diagnostic question, feedback that meets the criteria is awarded 1 point, otherwise 0 point. Therefore, the final diagnostic score for each generated feedback ranges from 0 to 5. We rank the generated feedback as pairs based on the diagnostic score and utilize the direct preference optimization loss to further fine-tune the LLM."}, {"title": "B.3 Experiment", "content": "We conduct experiments to investigate the sensitivity of the hy-perparameters, i.e., pool size S, number of selected expert prompts K, \u03bb1 and \u03bb2 of the loss function. Table 4, 3 and 5 demonstrate the experimental results. Moreover, we also investigate the influence of the expert correlation loss Lcor, which aims to reduce the corre-lation among the expert prompts in the pool and enforce each of the expert prompt to maintain its special feature. The experiment results are shown in Table 6."}, {"title": "B.4 Implementation of Baseline Models", "content": "For NLX-GPT and KICNLE, we employ full fine-tuning approach on the VCR-DF dataset. We optimize them with an initial learning rate of 2e-5. For the large multimodal models (i.e., BLIP-2, InstructBLIP, VisualGLM, LLaVA-v1.5 and CogAgent), we utilize the LoRA layers into the self-attention mechanisms of their LLMs and the lora rank of BLIP-2, InstructBLIP, VisualGLM and LLaVA-v1.5 is set to 16, while it is set to 32 for CogAgent. During training, we use Adam optimizer with an initial learning rate of 8e-5 and cosine scheduler."}, {"title": "B.5 Human Evaluation", "content": "We randomly select 200 samples from the VCR-DF test set and 5 participants with good English education are asked to evaluate these samples based on the following criteria: Fluency (Flu) mainly reflects the grammatical correctness and fluency of the generated sentence. Helpfulness (Help) measures whether the generated feed-back aids human in more effectively reaching to the correct answer. Logical consistency (Loc) refers to the logical coherence between feedback and input information. Diversity (Div) represents the variety of content within the three distractors generated for each sample. Relevance (Rel) evaluates whether the distractor is related to the image, question and educational level. The Flu and Div are scored on a range from 0 to 2 (higher values indicate greater flu-ency, diversity and fewer grammatical errors), while others are binary values. Table 7 shows the human evaluation results with five metrics. We conclude that: i) For fluency Flu, the results il-lustrate that our PEIFG model achieves competitive results with"}]}