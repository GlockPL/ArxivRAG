{"title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning", "authors": ["Dong Li", "Aijia Zhang", "Junqi Gao", "Biqing Qi"], "abstract": "Incremental graph learning has gained significant attention for its ability to address\nthe catastrophic forgetting problem in graph representation learning. However,\ntraditional methods often rely on a large number of labels for node classification,\nwhich is impractical in real-world applications. This makes few-shot incremental\nlearning on graphs a pressing need. Current methods typically require extensive\ntraining samples from meta-learning to build memory and perform intensive fine-\ntuning of GNN parameters, leading to high memory consumption and potential\nloss of previously learned knowledge. To tackle these challenges, we introduce\nMecoin, an efficient method for building and maintaining memory. Mecoin em-\nploys Structured Memory Units to cache prototypes of learned categories, as well\nas Memory Construction Modules to update these prototypes for new categories\nthrough interactions between the nodes and the cached prototypes. Additionally,\nwe have designed a Memory Representation Adaptation Module to store prob-\nabilities associated with each class prototype, reducing the need for parameter\nfine-tuning and lowering the forgetting rate. When a sample matches its corre-\nsponding class prototype, the relevant probabilities are retrieved from the MRaM.\nKnowledge is then distilled back into the GNN through a Graph Knowledge Dis-\ntillation Module, preserving the model's memory. We analyze the effectiveness of\nMecoin in terms of generalization error and explore the impact of different distil-\nlation strategies on model performance through experiments and VC-dimension\nanalysis. Compared to other related works, Mecoin shows superior performance\nin accuracy and forgetting rate. Our code is publicly available on the Mecoin-\nGFSCIL.", "sections": [{"title": "Introduction", "content": "In the field of graph learning, conventional methods often assume that graphs are static[1]. However,\nin the real world, graphs tend to grow over time, with new nodes and edges gradually emerging. For\nexample, in citation networks, new papers are published and cited; in e-commerce, new products\nare introduced and updated; and in social networks, new social groups form as users join. In these\ndynamic contexts, simply updating graph representation learning methods with new data often leads\nto catastrophic forgetting of previously acquired knowledge.\nDespite numerous methods proposed to mitigate the catastrophic forgetting problem in Graph Neu-\nral Networks(GNNs)[2, 3, 4], a critical and frequently neglected challenge is the scarcity of labels\nfor newly introduced nodes. Most current graph incremental learning methods [5, 6] combat catas-\ntrophic forgetting by retaining a substantial number of nodes from previous graphs to preserve prior\nknowledge. However, these methods become impractical in graph few-shot class-incremental learn-"}, {"title": "Notation", "content": "Let $G_0 = (V_0, E_0)$ be the base graph with node set $V_0$ and edge set $E_0$. We consider $T$ temporal\nsnapshots of $G_0$, each corresponding to a GFSCIL task or session. Denote $S = {S_0, S_1, ..., S_T}$ as\nthe set of sessions including the pre-training session $S_0$, and $C = {C_0, C_1, ..., C_T}$ as the family\nof class sets within each session. The graph under session $S_i (i = 1, 2, ..., T)$ is denoted as $G_i =$\n$(V_i, E_i)$, with node feature matrix and adjacency matrix represented by $X_i = (x_1,...,x_{|V_i|}) \\in$\n$\\mathbb{R}^{|V_i| \\times d}$ and $A_i \\in \\mathbb{R}^{d \\times d}$ respectively. For session $S_i$, let $X_{tr} \\in \\mathbb{R}^{K|C| \\times d}$ and $Y_i^{tr}$ be the features\nand corresponding labels of the training nodes respectively, where $K$ is the sample size of each class\nin $C_i$, thus defining a $C_i$-way $K$-shot GFSCIL task. Let $Y_i$ be the label space of session $S_i$, and we\nassume that the label spaces of different sessions are disjoint, i.e., $V_i \\cap V_j = \\emptyset$ if $i \\neq j$. Our goal\nis to learn a model $f_\\theta$ across successive sessions that maintains strong performance in the current\nsession and also retains memory of the past sessions."}, {"title": "Efficient Memory Construction and Interaction Module", "content": "In this section, we present a comprehensive overview of our proposed framework, Mecoin. Unlike\nprevious methods that are hampered by inefficient memory construction, low computational effi-\nciency, and extensive parameter tuning\u2014which often lead to the loss of prior knowledge-Mecoin\nenhances the learning of representative class prototypes. It achieves this by facilitating interaction\nbetween input nodes and seen class prototypes stored in the SMU, while integrating local graph\nstructure information of the input nodes. Moreover, Mecoin decouples the learning of class proto-\ntypes from their corresponding probability distributions, thereby mitigating the loss of prior knowl-\nedge during both the prototype learning and classification processes. Fig. 1 illustrates the architec-\nture of Mecoin."}, {"title": "Structured Memory Unit", "content": "To acquire and store representative class prototypes, we develop SMU within Mecoin. Let $M$ be\nthe set of class prototypes ${m_0, m_1, ..., m_{n-1}}$, where $n = |C_{T-1}|$ refers to the total number of\nclasses learned from the past $T - 1$ sessions, with each $m_i \\in \\mathbb{R}^k (\\forall i \\in [n])$. For current session $S_T$,"}, {"title": "Memory Representation Adaptive Module", "content": "In the traditional GFSCIL paradigm, adapting to evolving graph structures requires continuous learn-\ning and updating of class prototypes based on the current task's graph structure, alongside classifier\nretraining. This process, which involves adjusting parameters during class prototype learning, can\nlead the classifier to forget information about past categories, exacerbating the model's catastrophic\nforgetting. To address this, we introduce the MRaM within Mecoin. MRaM tackles this challenge\nby decoupling class prototype learning from class representation learning, caching probability dis-\ntributions of seen categories. This separation ensures that class prototype updates don't affect the\nmodel's memory of probability distributions for nodes in seen categories, thus enhancing the stabil-\nity of prior knowledge retention [14, 15, 16].\nTo maintain the model's memory of the prior knowledge, we introduce GKIM into MRaM for infor-\nmation exchange between Mecoin and the model. Specifically, let $P = {P_0, P_1, ..., P_{n-1}}$ denote\nthe class representations learned from the GNN and stored in MRaM, where each $p_i$ corresponds to\nits respective class prototype $m_i$. For a node feature $x$, from the seen classes with its class repre-\nsentation $p_x$, let $P_{MLP}$ be the category predicted probabilities learned from GNN and MLP, then\nGKIM transfers the prior knowledge stored in Mecoin to the model through distillation that based\non the memory loss function:\n\n$\\mathcal{L}_{memory} = \\frac{1}{N_s} \\sum_{i=1}^{N_s} KL(p_i || p_{MLP}) = \\frac{1}{N_s} \\sum_{i=1}^{N_s} p_i log \\frac{p_i}{p_{MLP}}$,\n                                              (8)"}, {"title": "Theoretical Analysis", "content": "In this section, we analyze the advantages of Mecoin and how it improves models generalization\nfrom the perspective of generalization error. Besides, we also provide insights from the viewpoint of\nVC-dimension by comparing GKIM with non-parametric methods and MLPs in classifying category\nrepresentations stored in MRaM, and distilling Mecoin as a teacher model with GNN model.\nWhat are the advantages of Mecoin over other models? In few-shot learning, the limited training\ndata and the overall samples in current session $S_T$ often have different distributions, which leads to\noverfitting. However, Mecoin can mitigate overfitting since it has a lower bound of generalization\nerror than other corresponding models (Thm.1). Before introducing our theoretical result, we first\nsupplement some notations. For the current session $S_T$, let $X_T$ and $Y_T$ be the sample space and label\nspace of $X$ respectively, and $T_r = {(x, y)}$ be the training samples. Let $F$ be a hypothesis\nclass, $f_\\theta$ and $f \\in F$ be Mecoin and other corresponding models trained on $T_r$, and assume that\nthe inputs $x \\in X_T$ undergo distributional shifts through any function $g_\\theta$. Then by comparing the\ngeneralization error bounds of $f_\\theta$ with $f$, we demonstrate in Thm.1 (proof in Appendix B.1) that\nMecoin excels in distributional shifts, indicating its stronger generalization capability. The result is\ngiven in Thm.1, which is derived from theorem 3.1 in [12].\nTheorem 1: For any model $f \\in {f, f}$ trained on $T_r$, denote $R$ as its generalization error,\nthen there exists a constant $c$ such that for any $\\delta > 0$, the following holds with probability at least\n$1 - \\delta$:\n\n$R < R_{\\xi} + B_{\\hat{f}} \\mathbb{I}{f = \\hat{f}} + c \\sqrt{\\frac{2ln(e/\\delta)}{N}}$,\n                            (11)\n\nwhere $R_{\\xi} = \\frac{1}{N} \\sum_{y_T \\in Y_T} \\sum_{x \\in I^\\mathbb{T}} |E_z[E_\\theta[l(f(g_\\theta(x_\\mathbb{T})), y_T)] - l(f(x_{\\mathbb{T}}), y_T)]|z \\in C_\\mathbb{T}]$, $B_{\\hat{f}} =$\n$Sup_{f\\in F} \\frac{1}{2N} [\\sum_{z\\in \\mathbb{T}} [\\sum_{j=1}^{m} (\\sigma_j*E[l(f_{\\hat{f}}(g_\\theta(x)),y)]) \\in C_m, y = y_{\\mathbb{T}}] + c\\sqrt{\\frac{ln(2e/\\delta)}{2N}}$ where\n${\\xi_i}_i$ are i.i.d. random variables uniformly taking values in ${-1, 1}$, and $l$ is the loss function\n$\\mathcal{L}_{Mecoin}$, $z = (X_\\mathbb{T}, y_\\mathbb{T})$, $C = {(x,y) \\in X_\\mathbb{T} \\times Y_\\mathbb{T} | y = y_{\\mathbb{T}}, m = argmin_{i\\in[N]}d(k_\\theta(x), m_i)}$,\n$I = {i \\in [N] | x_{\\mathbb{T}}^* \\in C_\\mathbb{T}, y_{\\mathbb{T}} = y_{\\mathbb{T}}}$, $C_m = {x \\in X_{\\mathbb{T}} | m = argmin_{i\\in[|M|]}d(k_\\theta(x, m_i)}$,\n$I_X = m \\in [|M|| | |I_\\mathbb{T} | \\geq 1}$ and $k_\\theta$ is the $MeC_s$ operation.\nWhy use GKIM to interact with GNN models? Unlike traditional knowledge distillation techniques\nthat rely on high-capacity teacher models, GKIM uses probability distributions stored in MRaM\nto preserve node of seen class distributions. This prevents knowledge loss in GNN during teacher\nmodel training. For unseen classes, the classifier learns and stores their probability distributions\nin MRaM. Notably, updating unseen class distributions and extracting seen class distributions can\nalso use non-parametric methods [12]. Thus, we must examine GKIM's advantages over conven-\ntional distillation and non-parametric methods. We analyze the VC dimension when category repre-\nsentations in MRaM are distilled into models, comparing scenarios where MRaM, non-parametric\nmethods, and multi-layer perceptrons (MLPs) act as teacher models."}, {"title": "Experiments", "content": "In this section, we will evaluate Mecoin through experiments and address the following research\nquestions: Q1). Does Mecoin have advantages in the scenarios of graph few-shot continual learning?\nQ2). How does MeCs improve the representativeness of class prototypes? Q3). What are the\nadvantages of GKIM over other distillation methods?"}, {"title": "Graph Few-Shot Continual Learning (Q1)", "content": "Experimental setting. We assess Mecoin's performance on three real-world graph datasets: Cora-\nFull, CS, and Computers, comprising two citation networks and one product network. Datasets are\nsplit into a Base set for GNN pretraining and a Novel set for incremental learning. Tab.1 provides\nthe statistics and partitions of the datasets. In our experiments, we freeze the pretrained GNN pa-\nrameters and utilize them as encoders for subsequent few-shot class-incremental learning on graphs.\nFor CoraFull, the Novel set is divided into 10 sessions, each with two classes, using a 2-way 5-shot\nGFSCIL setup. CS's Novel set is split into 10 sessions, each with one class, adopting a 1-way 5-shot\nGFSCIL configuration. Computers' Novel set is segmented into 5 sessions, each with one class, also\nusing a 1-way 5-shot GFSCIL setup. During the training process, the training samples for session\n0 are randomly selected from each class of the pre-trained testing set, with 5 samples chosen as the\ntraining set and the remaining testing samples used as the test set for training. It is worth noting that\nfor each session, during the testing phase, we construct a new test set using all the testing samples of\nseen classes to evaluate the model's memory of all prior knowledge after training on the new graph\ndata. Our GNN and GAT models feature two hidden layers. The GNN has a consistent dimension\nof 128, while GAT varies with 64 for CoraFull and 16 for CS and Computers. Training parameters\nare set at 2000 epochs and a learning rate of 0.005.\nBaselines. 1) Elastic Weight Consolidation\n(EWC) [17]-imposes a quadratic penalty on\nweights to preserve performance on prior tasks.\n2) Learning without Forgetting (LwF) [18]-\nretains previous knowledge by minimizing the\ndiscrepancy between old and new model out-\nputs. 3) Topology-aware Weight Preserving\n(TWP) [7]-identifies and regularizes parame-\nters critical for graph topology to maintain\ntask performance. 4) Gradient Episodic Mem-\nory (GEM) [19]-employs an episodic memory\nto adjust gradients during learning, prevent-\ning loss increase from prior tasks. 5) Experi-\nence Replay GNN (ER-GNN) [3]- incorporates"}, {"title": "MeCs for Memory (Q2)", "content": "We conduct relevant ablation experiments on MeCs across the CoraFull, CS, and Computers\ndatasets:1) Comparing the impact of MeCs usage on model performance and memory retention;\n2) Analyzing the effect of feature of node interaction with class prototypes stored in SMU on model\nperformance and memory retention;3)Assessing the impact of using graphInfo on model perfor-\nmance and memory retention. We conduct a randomized selection of ten classes from the test set,\nextracting 100 nodes per class to form clusters with their corresponding class prototypes within the\nSMU. Subsequently, we assess the efficacy of MeCs and its constituent elements on model perfor-\nmance and the rate of forgetting. In cases where the MeCs was not utilized, we employ the k-means\nmethod to compute class prototypes. In experiment, we use GCN as backbone. The experimental\nparameters and configurations adhered to the standards established in prior studies.\nExperimental Results. The results of the ablation experiments on CoraFull and CS are shown in\nFig.3 and results on other datasets are shown in Appendix.A. We deduce several key findings from\nthe figure:1) The class prototypes learned by MeCs assist the model in learning more representative\nprototype representations, demonstrating the effectiveness of the MeCs method. 2) The difference\nin accuracy between scenarios where graphInfo is not used and where there is no interaction with\nclass prototypes is negligible. However, the scenario without graphInfo exhibits a higher forgetting\nrate, indicating that local graph structural information provides greater assistance to model memory.\nThis may be because local structural information reduces the intra-class distance of nodes, thereby\nhelping the model learn more discriminative prototype representations."}, {"title": "GKIM for Memory (Q3)", "content": "In our experimental investigation across three distinct datasets, we scrutinized the influence of vary-\ning interaction modalities between MRaM and GNN models on model performance and the propen-\nsity for forgetting. We delineate three distinct scenarios for analysis: 1) Class representations stored\nin MRaM are classified using non-parametric methods and used as the teacher model to interact\nwith the GNN. 2) Class representations stored in MRaM are classified using MLP and Mecoin is\nemployed as the teacher model to transfer knowledge to the GNN model. 3) GNN models are de-\nployed in isolation for classification tasks, without any interaction with Mecoin. In this experiment,\nwe use GCN as backbone. Throughout the experimental process, model parameters were held con-\nstant, and the experimental configurations were aligned with those of preceding studies. Due to\nspace constraints, we include the results on the Computers dataset in the Appendix.A.\nExperimental Results. It is worth noting that due to the one-to-one matching between MRaM\nand SMU via non-parametric indexing, there is no gradient back-propagation between these two\ncomponents. This implies that updates to MRaM during training do not affect the matching between\nnode features and class prototypes. Our experimental findings are depicted in Fig.4, and they yield\nseveral key insights: 1) GKIM outperforms all other interaction methods, thereby substantiating\nits efficacy. 2) The second interaction mode exhibits superior performance compared to the first and\nthird methods. This is attributed to the MLP's higher VC-dimension compared to the non-parametric\nmethod, which gives it greater expressive power to handle more complex sample representations.\nHowever, the use of MLP results in a higher forgetting rate compared to the non-parametric method.\nThis is because when encountering samples from new classes, the parameters of MLP undergo\nchanges, leading to the loss of prior knowledge. For the third method, extensive parameter fine-\ntuning leads to significant forgetting of prior knowledge. Method one performs less effectively than"}, {"title": "Conclusion", "content": "Current GFSCIL methods typically require a large number of labeled samples and cache extensive\npast task data to maintain memory of prior knowledge. Alternatively, they may fine-tune model\nparameters at the expense of sacrificing the model's adaptability to current tasks. To address these\nchallenges, we propose the Mecoin for building and interacting with memory. Mecoin is made up\nof two main parts: the Structured Memory Unit (SMU), which learns and keeps class prototypes,\nand the Memory Representation Adaptive Module (MRaM), which helps the GNN preserve prior\nknowledge. To leverage the graph structure information for learning representative class prototypes,\nSMU leverages MeCs to integrate past graph structural information with interactions between sam-\nples and the class prototypes stored in SMU. Additionally, Mecoin introduces the MRaM, which\nseparates the learning of class prototypes and category representations to avoid excessive parameter\nfine-tuning during prototype updates, thereby preventing the loss of prior knowledge. Furthermore,\nMRaM injects knowledge stored in Mecoin into the GNN model through GKIM, preventing knowl-\nedge forgetting. We demonstrate our framework's superiority in graph few-shot continual learning\nwith respect to both generalization error and VC dimension, and we empirically show its advantages\nin accuracy and forgetting rate compared to other graph continual learning methods."}, {"title": "Acknowledgement", "content": "This work is supported by the National Science and Technology Major Project (2023ZD0121403).\nWe extend our gratitude to the anonymous reviewers for their insightful feedback, which has great-\nlycontributed to the improvement of this paper."}, {"title": "Ablation Experiments", "content": "In Section 4.2, we presented the relevant ablation experiments on the CoraFull and CS dataset.\nBelow are the experimental results of the model on the Computers datasets.\nThe experimental results in this section are similar to those on the CoraFull and CS dataset, and\nrelevant experimental analyses can be referred to in the main text. Additionally, we designed re-\nlated ablation experiments on the dimensions of GraphInfo and its integration position with node\nfeatures. Detailed results are shown in tab.5, 6. From these results, we can draw the following\nconclusions:1)It is evident from the tables that concatenating GraphInfo to the left of node features\nwith a dimension of 1 yields the best performance; 2)The model's performance decreases as the\ndimension of GraphInfo increases, while the forgetting rate generally exhibits a decreasing trend.\nThis indicates that local graph structural information provides some assistance to the model's mem-\nory. When the dimension of class prototypes is fixed, an increase in the dimension of GraphInfo\nimplies less graph structural information extracted from prototypes of seen classes in past sessions.\nConsequently, MeCs integrates less graph structural information, making it difficult for the model\nto learn representative class prototypes, thereby limiting the model's performance.\nFurthermore, for Section 4.3, the relevant ablation experiment results on the Computers dataset can\nbe found in Figure 8."}, {"title": "Proof of Theorems", "content": ""}, {"title": "Proof of Theorem 1", "content": "Proof. By instituting $C_\\mathbb{T}$ with $C_\\mathbb{T}^*$, $C_m$ with $C_k$, $I_\\mathbb{T}$ with $I_\\mathbb{T}^*$, $I_X$ with $I_m$, and $k_\\theta$ with\n$k_{\\theta^{zoa}}$, our proof is similar to the proof of theorem 3.1 in [12] (please refer to [12] for more details).\nFor $f \\in {f_M, f}$, through similar procedure in the proof of [12], we obtain the coarse upper bound"}, {"title": "Parameters and Devices", "content": "The relevant experimental parameters in this paper were determined through grid search. The pa-\nrameter settings for each dataset are shown in Table 7."}, {"title": "Limitations", "content": "While our method has shown promising performance in graph few-shot incremental learning, there\nare still some issues that need to be addressed: 1) Our approach assumes that the graph structures\nacross all sessions originate from the same graph. However, in real-world scenarios, data from\ndifferent graphs may be encountered, and we have not thoroughly explored this issue; 2) Although\nour method currently maintains model performance with only a small number of samples, further\nvalidation is needed to assess whether it can still achieve excellent performance under low-resource\nconditions where graph structural information is scarce."}]}