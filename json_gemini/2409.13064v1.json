{"title": "Fear and Loathing on the Frontline: Decoding the Language of Othering by Russia-Ukraine War Bloggers", "authors": ["Patrick Gerard", "William Theisen", "Tim Weninger", "Kristina Lerman"], "abstract": "Othering, the act of portraying outgroups as fundamentally different from the ingroup, often escalates into framing them as existential threats-fueling intergroup conflict and justifying exclusion and violence. These dynamics are alarmingly pervasive, spanning from the extreme historical examples of genocides against minorities in Germany and Rwanda to the ongoing violence and rhetoric targeting migrants in the US and Europe. While concepts like hate speech and fear speech have been explored in existing literature, they capture only part of this broader and more nuanced dynamic which can often be harder to detect, particularly in online speech and propaganda. To address this challenge, we introduce a novel computational framework that leverages large language models (LLMs) to quantify othering across diverse contexts, extending beyond traditional linguistic indicators of hostility. Applying the model to real-world data from Telegram war bloggers and political discussions on Gab reveals how othering escalates during conflicts, interacts with moral language, and garners significant attention, particularly during periods of crisis. Our framework, designed to offer deeper insights into othering dynamics, combines with a rapid adaptation process to provide essential tools for mitigating othering's adverse impacts on social cohesion.", "sections": [{"title": "Introduction", "content": "In times of crisis, people turn to social media for information to help them make sense of events. Consequently, social media platforms can significantly influence individual perceptions and understanding of reality. This dynamic creates a strong incentive for various actors to manipulate public perceptions through online messaging and propaganda. Tactics that frame certain groups as separate and inherently dangerous are particularly effective at evoking strong emotional responses (Saha et al. 2021), often contributing to radicalization and intergroup conflict (Cervone, Augoustinos, and Maass 2021). This rhetoric spans from explicit hate speech and dehumanization (Buyse 2014; Kennedy et al. 2023) to subtler forms of fear speech (Buyse 2014; Saha et al. 2021; Schulze, M\u00fcller, and Lenz 2023; Saha et al. 2023).\nWhile previous research studied some manifestations of this dynamic, like hate speech (Basile et al. 2019; Waseem and Hovy 2016; Kennedy et al. 2018) and fear speech (Saha et al. 2021, 2023), these represent just pieces of the broader social process of othering. Othering involves the systematic construction of an outgroup-the other-as fundamentally different and ultimately threatening to the ingroup. The goal of othering is to exclude and marginalize certain groups based on arbitrary characteristics like race, ethnicity or religion, ultimately establishing a self-sustaining group identity-based 'us-versus-them' mentality.\nHistorically, othering has been used to justify extreme measures and perpetuate cycles of violence and exclusion. Across various societies and periods, outgroups have been constructed as threats to the social, cultural, or political stability of the ingroup, legitimizing repression and violence. In Nazi Germany, discursive strategies mobilized hatred against Jews, Romani people, and homosexuals, reducing them to targets of mass extermination based solely on their group membership (Reicher, Haslam, and Rath 2008). Similarly, Stalinist terror was framed as a moral battle against an evil 'other', such as \u201cwreckers, diversionists, and spies,\" with repression framed as necessary for the preservation of the state (Gerwarth 2007). In India, Hindutva organizations invoked narratives of Hindu tolerance to justify violence against Muslims, blaming conflicts on supposed Muslim intolerance (Kaur 2005). In recent years, social media has increasingly been used to spread narratives depicting immigrants and ethnic minorities as threats to national and cultural values (Nortio et al. 2021). In one particularly egregious example, Facebook was used to incite deadly riots and genocidal behavior in Myanmar against the Rohingya minority (Yue 2019). Ultimately, this process-driven by persistent themes of intergroup conflict and perceived threats-endures across communities and time, adapting to the evolving needs of the ingroup, serving as a self-sustaining force of exclusion. As such, tracking and quantifying this social dynamic on online platforms is crucial. While existing research has extensively explored explicit manifestations of intergroup conflict like hate speech and fear speech (Cervone, Augoustinos, and Maass 2021; Saha et al. 2021), these expressions are only part of the broader process of othering. Our work explores the subtler, lesser-understood mechanisms that sustain these expressions, shedding light on their"}, {"title": "Related Work and Background", "content": "nature and their interactions with moral language and attention mechanisms.\nWe ground our analysis in sociological theory, defining 'othering' as the process of depicting a group as fundamentally different from one's own (Reicher, Haslam, and Rath 2008; Cikara 2015; Jetten, Spears, and Manstead 1997), this group self-talk is often characterized by the positive portrayal of one's own group (i.e., the ingroup) and the negative portrayal of the other group (i.e., the outgroup). This social dynamic marginalizes, excludes, or discriminates against the outgroup based on arbitrary or perceived differences, such as race, religion, or ethnicity, reinforcing social hierarchies and justifying unequal treatment (Reicher, Haslam, and Rath 2008; Duckitt 2003). We introduce a taxonomy of othering language and show that it subsumes and extends commonly used text indicators of intergroup conflict.\nAdditionally, we introduce an innovative computational framework that leverages large language models (LLMs) as classifiers and enables their rapid adaptation to new domains.After validating a model of othering, we use it to explore the language of intergroup conflict in real-world scenarios. We analyze a corpus of messages posted on Telegram by Russian and Ukrainian war bloggers during the ongoing war between Russia and Ukraine, as well as a corpus of messages posted on the social media platform Gab. We explore the following research questions:\n1. How does the use of othering by Russian and Ukrainian war bloggers on Telegram change over the course of the war?\n2. How do moral language and othering interact, especially in the expressions of intergroup conflict?\n3. How does constructing and reinforcing the image of a target group as the 'other' affect social attention?\n4. Does othering intensify during times of crisis, and in what ways are these behaviors more strongly rewarded?\nOur analysis reveals the amplification of othering language in polarized online environments and its tendency to attract attention, especially during crises. While we find that othering language is often moralized across groups, its asymmetrical use by Russian war bloggers highlights its distinct utility in propaganda. By exploring these dimensions, we demonstrate how othering underpins more overt expressions like fear speech, hate speech, and exclusionary practices, offering crucial insights for developing strategies to counteract othering and mitigate its impact on social cohesion.\nThe Language of Intergroup Conflict Intergroup conflict often drives violence by framing outgroups as existential threats to the ingroup. From Nazi Germany to modern online spaces, fear-mongering and hateful rhetoric radicalize populations by portraying outgroups as dangerous and immoral, justifying hostility and violence (Greipl, Rothut, and Schulze 2022; Reicher, Haslam, and Rath 2008).\nSuch conflict escalates during crises-pandemics, financial collapses, or political upheaval-when fears are externalized and outgroups are scapegoated for societal prob-"}, {"title": "Methods", "content": "lems. Economic hardship heightens competition and prejudice, as seen during the Great Depression. Similarly, the 2015 European Refugee Crisis saw the rise of exclusionary rhetoric, driven by political and social tensions (Pettersson and Sakki 2017). Misperceptions of outgroup hostility further exacerbate tensions, with groups mistakenly believing the other supports violence, as seen in the 2021 Israeli-Palestinian conflict. However, corrective interventions have shown promise in reducing these tensions (Nir et al. 2023).\nA key psychological mechanism driving intergroup conflict is the perception of outgroup threat the belief that outgroups endanger the ingroup's identity or very existence. This dynamic was particularly evident during the COVID-19 pandemic, where heightened awareness of mortality intensified xenophobia (Esses and Hamilton 2021).\nUltimately, these often-manufactured perceptions of threat can escalate intergroup conflict, leading to scapegoating that legitimizes violence and perpetuates hatred. This cycle frequently results in real-world violence, reinforcing instability and deepening social divisions (Fink 2018; Warofka 2018).\nComputational scientists often operationalize the language of intergroup conflict through two key concepts: hate speech and fear speech. Hate speech refers to expressions intended to insult, degrade, or incite hostility toward a group based on attributes like race, religion, or gender (Mathew et al. 2021). It promotes explicit hostility typically through vilification and dehumanization (Basile et al. 2019; Waseem and Hovy 2016; Kennedy et al. 2018). In contrast, fear speech invokes existential fear, portraying the target group as a fundamental threat to the ingroup's survival, culture, or identity (Buyse 2014; Saha et al. 2021, 2023). Both forms of speech reinforce an 'us-versus-them' mentality, either by inciting hostility or by instilling fear, emphasizing the danger the outgroup poses to the ingroup's way of life. Together, they contribute to the broader process of othering, where groups are marginalized or excluded based on perceived differences (Reicher, Haslam, and Rath 2008; Cikara 2015; Jetten, Spears, and Manstead 1997).\nSocial Mechanisms of Othering Othering language extends beyond specific expressions like hate speech and fear speech; it encompasses the broader sociological process of constructing an outgroup and excluding individuals based on race, religion, or ethnicity. This process has been observed throughout history, from the Holocaust to contemporary political discourse on immigration (Reicher, Haslam, and Rath 2008). Understanding these mechanisms is essential for mitigating their effects and preventing intergroup conflict.\nWe base our understanding of othering on Reicher's model (Reicher, Haslam, and Rath 2008), which conceptualizes hate as emerging from a continuous process of othering. As illustrated in Figure 1, extreme violence and genocide are driven by a distorted perception of group identity, where individuals are targeted solely for belonging to an outgroup (Reicher, Haslam, and Rath 2008; Kaur 2005). In this process, even disavowing one's group identity of-"}, {"title": "LLMs and In-Context Learning", "content": "Data\nRussia-Ukraine war bloggers We use posts collected from Russian-oriented and Ukrainian-oriented Telegram channels (Theisen et al. 2022), spanning from October 2015 to August 2023. This dataset, which was gathered using both an expert-curated list and snowballing methods, includes 989 channels and over 9.67 million posts, primarily in Ukrainian and Russian. This data is released alongside this paper to support our findings and compel further research.\nLabeling Telegram Channels: Telegram, a messaging app supporting private/public group interactions and one-way broadcasts via channels, has become a stronghold of a 'free' Internet in Russia. Evading bans affecting other major platforms like like Facebook and TikTok since 2020, Telegram has emerged as a key platform for military (war) bloggers and a primary source of news for both Ukrainians and Russians during the Russia-Ukraine war (Oleinik 2024). We construct a network of Telegram channels within the war bloggers corpus, where a directed link with weight w connects channel A to channel B if A references or forwards B's posts w times during the period. The network, Figure 11, shows the channels and connections between them. The global structure of information sharing shows roughly two clusters, as expected of groups in conflict.\nWe manually labeled 100 random channels as \"pro-Russia', 'pro-Ukraine,' or 'Other' based on their bios and recent posts, then used these as seeds for a label propagation algorithm (Garza and Schaeffer 2019) to categorize the network. To validate, we reviewed 100 randomly selected channels from each group. The final dataset includes 243 pro-Ukrainian channels ( 4.2 million posts) and 325 pro-Russian channels (4.4 million posts) from October 2015 to August 2023, though messages prior to late 2021 are sparse.\nGab Corpus Introduced in prior work by Saha et al. (Saha et al. 2023), this corpus contains 9,441 text posts from the popular 'alt-tech' social media platform Gab (Dehghan and Nagappa 2022). Gab, which is popular with political conservatives in US, hosts discussions often revolving around issues of race, immigration, and national identity. Despite the absence of direct physical conflict, the rhetoric on Gab is steeped in othering narratives, making it a platform of choice for studying hate speech and fear speech (Kennedy et al. 2018; Saha et al. 2023). Each post was manually classified by humans annotators into one or more categories: 'normal,' 'fear speech,' or 'hate speech,' with some posts receiving multiple labels. We detail the definitions used for fear speech and hate speech in a later section on othering language's relationship to expressions of intergroup conflict."}, {"title": "A Model of Othering", "content": "Ultimately, these often-manufactured perceptions of\nthreat can escalate intergroup conflict, leading to scapegoat-\ning that legitimizes violence and perpetuates hatred. This cy-\ncle frequently results in real-world violence, reinforcing in-\nstability and deepening social divisions (Fink 2018; Warofka\n2018).\nComputational scientists often operationalize the lan-\nguage of intergroup conflict through two key concepts: hate\nspeech and fear speech. Hate speech refers to expressions\nintended to insult, degrade, or incite hostility toward a group\nbased on attributes like race, religion, or gender (Mathew\net al. 2021). It promotes explicit hostility typically through\nvilification and dehumanization (Basile et al. 2019; Waseem\nand Hovy 2016; Kennedy et al. 2018). In contrast, fear\nspeech invokes existential fear, portraying the target group\nas a fundamental threat to the ingroup's survival, culture, or\nidentity (Buyse 2014; Saha et al. 2021, 2023). Both forms of\nspeech reinforce an 'us-versus-them' mentality, either by in-\nciting hostility or by instilling fear, emphasizing the danger\nthe outgroup poses to the ingroup's way of life. Together,\nthey contribute to the broader process of othering, where\ngroups are marginalized or excluded based on perceived dif-\nferences (Reicher, Haslam, and Rath 2008; Cikara 2015; Jet-\nten, Spears, and Manstead 1997).\nSocial Mechanisms of Othering Othering language ex-\ntends beyond specific expressions like hate speech and fear\nspeech; it encompasses the broader sociological process of\nconstructing an outgroup and excluding individuals based on\nrace, religion, or ethnicity. This process has been observed\nthroughout history, from the Holocaust to contemporary po-\nlitical discourse on immigration (Reicher, Haslam, and Rath\n2008). Understanding these mechanisms is essential for mit-\nigating their effects and preventing intergroup conflict.\nWe base our understanding of othering on Reicher's\nmodel (Reicher, Haslam, and Rath 2008), which conceptu-\nalizes hate as emerging from a continuous process of other-\ning. As illustrated in Figure 1, extreme violence and geno-\ncide are driven by a distorted perception of group identity,\nwhere individuals are targeted solely for belonging to an\noutgroup (Reicher, Haslam, and Rath 2008; Kaur 2005).\nIn this process, even disavowing one's group identity of-"}, {"content": "Overall, 44.8% of the posts are labeled as normal, 19.7% as fear speech, and 42.4% as hate speech\nWe develop a flexible, LLM-based model to recognize othering language in text and show how it can be rapidly adapted to new domains.\nOthering is a group self-talk process that helps shape group's conceptualization of itself as good and virtuous and the other group as inherently evil and dangerous. When talking about itself, i.e., the ingroup, the group uses fear-laden speech (Lerman et al. 2024), which serves to bind the group together, often in response to a perceived threat from the outgroup. When talking about the other, i.e., the outgroup, othering manifests itself through animosity and hostility (Stephan, Ybarra, and Rios 2015; Joffe 1999). To capture the various dimensions of othering, we define four categories of language linked to the othering process and provide translated examples from Russian war bloggers. The first two categories address perceived threats to the ingroup, while the latter two focus on the demonization of outgroups.\n arise when the outgroup is framed as a danger to the ingroup's cultural or social survival, challenging its values, language or traditions (Wohl, Branscombe, and Reysen 2010; Reicher, Haslam, and Rath 2008; Stephan, Ybarra, and Rios 2015; Joffe 1999). Example post: \"The erosion of the Russian language in Ukrainian schools: Ukrainian policymakers pushing to erase the Russian tongue risk severing the threads that weave together our history.\"\n involve portraying the outgroup as an existential menace to the ingroup's physical well-being, thereby justifying preemptive hostility (Wohl, Branscombe, and Reysen 2010; Reicher, Haslam, and Rath 2008; Stephan, Ybarra, and Rios 2015; Joffe 1999). Example post: \"Zelensky's regime has accumulated 30 tons of plutonium and 40 tons of enriched uranium at the Zaporizhia NPP [...] the regime really is on the verge of creating its own nuclear bomb! And hundreds of 'dirty' bombs can be made from such a quantity of radioactive material!\"\n casts the outgroup as inherently evil or immoral, which in turn validates resistance and aggression (Joffe 1999; Reicher, Haslam, and Rath 2008; Stephan, Ybarra, and Rios 2015). Example post: \"Because these Ukronazi girls can fight only by hiding behind hostages. All their courage went down the drain in chants and slogans like 'hang the Muscovite.' But when the Russians came, they shit themselves, just like their Bandera.\"\n represents the most extreme form of othering, where the outgroup is compared to animals, objects or spirits, paving the way for extreme violence (Joffe 1999; Reicher, Haslam, and Rath 2008; Stephan, Ybarra, and Rios 2015). Example post: \"These are zombies, who may have been brothers before, but"}, {"title": "Artificial Annotator Alignment Process", "content": "over the past 8 years, from the bite of Nazism and Banderization, they have turned into non-humans. That is why our army calls on all brothers to lay down their arms, so that we can distinguish a brother from an infected zombie, who can only bite and infect.\"\nThese classes are integral to understanding how outgroups are systematically constructed, legitimizing their exclusion and violence.\nWe train a classifier to recognize othering language, using an \u201cArtificial Annotator Alignment\" process inspired by knowledge distillation (Gou et al. 2021) to efficiently train the model and adapt it to new domains. Our alignment process, shown in Figure 2, essentially \u201ctrains\" LLMs as annotators by requiring them to be consistent with human annotators. Fist, human annotators label a small subset of the data. This subset is then annotated by a \u201chigh-quality\u201d (HQ) LLM like ChatGPT-4. If the HQ-LLM's annotations closely align with those of the human annotators, we proceed to have the HQ-LLM annotate more data to effectively train an open-source LLM (OS-LLM), e.g., Llama3, as an annotator. This strategy is driven by the goal of maximizing effectiveness and cost: utilizing a combination of an HQ-LLM and OS-LLM to avoid the prohibitive costs of large-scale annotation.\nThis approach is guided by two core principles: (1) The initial, high-quality (HQ) LLM must produce annotations that closely resemble those made by humans; (2) The open-source (OS) LLM trained on the HQ-LLM-annotated data must achieve performance on par with the HQ-LLM when evaluated against the human-annotated data (which is held-out throughout the process for evaluation). To adhere to the first principle, we assess the continuity between human annotations and the HQ-LLM using both standard machine learning metrics (accuracy, F1-score, etc.) and inter-annotator agreement metrics. This approach allows us to evaluate the HQ-LLM both as a classifier and as an artificial annotator, ensuring its consistency with human annotations. If both sets of evaluations yield optimal results, we consider the HQ-LLM a reliable proxy for human annotation. We then proceed to use the HQ-LLM to annotate a substantially larger portion of the dataset.\nTo adhere to the second principle, after training the target"}, {"title": "Rapid Domain Adaptation", "content": "OS-LLM on the data annotated by the HQ-LLM, we test its performance on the human-annotated dataset, once again using both sets of metrics to ensure that its classification metrics do not degrade from the HQ-LLM (ensuring inter-model continuity) and that its classifications are consistent with human annotations (ensuring inter-annotator continuity). This two-step validation ensures that the target model OS-LLM not only replicates the quality of the HQ-LLM's annotations but also aligns with human judgment, thereby confirming its effectiveness in the domain.\n Our analysis began with two datasets: messages from Russian war bloggers and from Ukrainian war bloggers. For each dataset, we used a combination of random post selection and keyword-based upsampling, targeting phrases and coded terms of denigration specific to each context (e.g., \"Ukronazis\" in the Russian data).\n The approach yielded 316 posts for the Russian data, which were labeled by six human annotators with overlapping annotations. A similar process was applied to the Ukrainian dataset, but with two annotators, based on the strong performance observed in the Russian workflow. Inter-annotator agreement, shown in Tables 6 and 9 for the Russian and Ukrainian data, is consistent with benchmarks in similar studies (Saha et al. 2021, 2023). Final classification counts for each domain were determined through majority vote, as summarized in Tables 5 and 9.\n Following independent human annotations, we used ChatGPT-40 (HQ-LLM) to annotate the same examples using prompts tailored to the specific context (prompts available in our GitHub repository), outputting a dictionary for each post: \u201c\u201cThreats to Culture or Identity': 1, 'Threats to Survival or Physical Security': 0, 'Vilification/Villainization': 1, 'Explicit Dehumanization': 0, 'None': 0\", along with an explanation. For example, \"The text describes local Nazis desecrating a historic Russian cemetery, in a way that represents a threat to cultural identity and vilifies the opposing group.\" These explanations were crucial for understanding the rationale behind each annotation and for testing our Rapid Domain Adaptation method later. The annotations were validated using metrics, such as Cohen's Kappa, treating ChatGPT-40 as an additional annotator. The results, detailed in tables 10, show that the ChatGPT-40 annotations were reliable and consistent across domains. In total, ChatGPT-4o annotated 20,000 posts (10,000 from each dataset) at a cost of approximately $70 USD, significantly lower than human annotation costs while remaining consistent with human annotators.\n Next, we trained three different models (OS-LLMs) on the ChatGPT-40-annotated (HQ-LLM) data: Mistral, LLaMA3-8b-Instruct, and LLaMA2. Each model was trained separately on three datasets: Russian-only, Ukrainian-only, and a combined Russian-and-Ukrainian dataset. To ensure a fair evaluation across different domains, we withheld common test and validation sets for each dataset, preventing data leakage that could give a model an unfair advantage. Each model was trained for 5 epochs on an NVIDIA Quadro RTX 8000 GPU using a learning rate of 1e-5 and followed a 0.7:0.1:0.2 split for training, validation,\""}, {"title": "RDA Evaluation", "content": "Adapting models to new domains presents a significant challenge in classification tasks. To study othering across multiple domains in a cost-effective manner, we test whether our models could effectively transfer knowledge from one domain to another. LLMs are well-suited for this task due to their (1) inherent complexity and (2) the pseudo-world mapping generated through extensive pretraining on vast corpora. To leverage this power, we employ two techniques that adapt the classifier to new, unseen data: system prompt steering and logit disambiguation. We name this approach Rapid Domain Adaptation (RDA).\n The first component of our RDA system involves manipulating the system prompt for the trained model. A system prompt provides the model with initial instructions, guiding how it processes and classifies incoming data. While in-context learning offers some benefits, simply appending context to new data only minimally improves performance. However, a well-crafted system prompt, designed to steer the model's reasoning, led to significant improvements in new domains by explicitly framing the new domain (this logic is illustrated in Figure\nOverall, these components \u2013 system prompt steering and logit disambiguation work together to enable rapid and reliable domain adaptation, leveraging modern LLMs to effectively handle drastic shifts in domain context.\nWe evaluated our RDA system across all cross-domain combinations (e.g., a model trained on Russian war blogger data performing on Ukrainian war blogger data), with detailed results available in our repository. We first compared system prompt steering to two baseline approaches: no added context and traditional in-context learning, where context is simply appended to the new prompting data. Table 13 shows the F1 scores for different domain transitions. Figure 3 visualizes the substantial performance gains across metrics when a model trained on Russian data applied to Gab, which contains messages posted on a different platform, in a different language (English), and in a different cultural and geopolitical context (representing the most substantial domain transition). These results demonstrate the effectiveness of our RDA system, especially in models that had no prior exposure to the new domain."}, {"title": "Relationship to Language of Intergroup Conflict", "content": "We examine the relationship between othering language and other widely studied expressions of intergroup conflict, such as fear speech and hate speech, using the annotated Gab corpus. As illustrated in Fig. 4, fear speech and hate speech reflect expressions of othering, but they do not fully encompass it. Instead, they function as partial components of the broader process. This underscores the asymmetry suggested by our model, which posits that othering language subsumes, but is not limited to, specific expressions like fear speech and hate speech. Additionally, we consider the practical implications for content moderation by evaluating how well current toxicity classifiers detect othering language alongside fear speech and hate speech. This analysis highlights the limitations of existing classification tools in addressing the full spectrum of othering language.\nFear speech, defined as expressions that instill existential fear of a target group (often based on attributes such as race, religion, or gender) (Buyse 2014), is significantly associated with othering language. Specifically, the probability that fear speech contains \u2018Vilification/Villainization' is 68.9%, and the probability of containing 'Threats to Culture or Identity' is 50.5%. It has a weaker association with 'Threats to Survival or Physical Security' at 20.3%. These connections reflect the nature of fear speech in both evoking existential fear and subtly vilifying the outgroup, such as in messages like 'They will destroy our way of life unless we stop them.' Moreover, fear speech shows an asymmetric relationship to othering: 88.9% of fear speech instances involve othering, but only 24.2% of othering messages are classified as fear speech.\nHate Speech Hate speech is language used to express hatred toward a targeted individual or group or is intended to be derogatory, to humiliate, or to insult the members of the group, based on attributes such as race, religion, or gender (Mathew et al. 2021), and is typically the most explicit form of othering. Our analysis shows strong associations with 'Vilification/Villainization' (74.1%), 'Explicit Dehumanization' (37.3%), and \u2018Threats to Culture or Identity' (32.3%). These connections underscore hate speech's dual role both denigrating the outgroup and framing it as a threat.\nHate speech also demonstrates an asymmetric relationship with othering language: approximately 87.4% of hate speech involves othering, but only 51.1% of messages containing othering language are classified as hate speech.\nFinally, we analyze the relationship to toxicity. Using the Detoxify classifier, which rates text on a scale from 0 to 1 (with scores above 0.5 considered toxic), we find the following average toxicity scores: fear speech averages 0.46, while hate speech scores higher at 0.65. For oth-"}, {"title": "Results", "content": "We label the corpus of messages posted on Telegram by Russian and Ukrainian war bloggers for othering language, focusing on the period from late 2021 to August of 2023. This period of war was characterized by high conflict and animosity between the groups. We also label a corpus of messages posted on Gab, which is often favored by far-right users within the US.\nFigure 5 shows that othering rhetoric fluctuated throughout the conflict, rising after key events. We define key events as significant events during the war that also became prominent discussion topics within their respective communities (i.e., Russian and Ukrainian war bloggers separately). Here, we utilize the events enumerated in Tables 15 and 16, which were compiled using domain knowledge in conjunction with methods from previous work (Gerard et al. 2024).\nThe types of events that tend to correlate with spikes in othering rhetoric differ significantly between these communities. For Russian bloggers, the most prominent driver was US military and security aid to Ukraine, which often led to a rise in rhetoric, as they framed themselves as victims of Western aggression. In contrast, for Ukrainian bloggers, increases in othering language were primarily driven by military gains and losses on the battlefield, reflecting a more direct response to the dynamics of the war itself. These findings are consistent with previous research showing that Russian bloggers tend to react more to international actions, while Ukrainian bloggers are more focused on military developments (Gerard et al. 2024). In future work, we plan to"}, {"title": "The Moral Language of Othering", "content": "automate event detection using change point analysis and further explore the causal relationship between these events and the prevalence of othering language.\nWe explore the interaction between othering and moral language on Telegram and Gab. For both platforms, we label moral values expressed in text using a model trained to recognize moral language (Trager et al. 2022). The model identifies the moral foundations of human intuitive ethics, such as valuing of purity, respect for authority, equality (fairness), group loyalty, and care or protection of the more vulnerable (Graham et al. 2013).\nWe begin by analyzing messages posted by war bloggers on Telegram, using a chi-squared test to explore the relationship between"}, {"title": "Ethical Considerations", "content": "moral language in messages containing othering language versus those without it. The chi-squared test helps determine whether the association between moral language and othering is statistically significant, rather than occurring by chance. The results indicate a strong, statistically significant connection (p < 0.001), demonstrating that moral language is more likely to co-occur with othering language than with non-othering language. This suggests that moral framing is frequently used to justify or intensify othering language, supporting the Moralized Threat Hypothesis (Hoover et al. 2021).\nNext, we analyzed the use of moral language by Russian and Ukrainian war bloggers across different othering categories to identify differences in their moral framing strategies. Figure 7 highlights the significant variations in the moral values expressed by each side when using othering language, based on log-odds ratios. We also examined the interaction between specific moral language categories and individual othering classes. As shown in Figure 14, the two groups also differ significantly in their use of moral language within othering rhetoric, with these differences confirmed by two-proportion z-tests (p < 0.001).\nExplicit forms of othering, such as Explicit Dehumanization, are the least associated with moral language in both groups, likely due to their overtly aggressive nature, which doesn't align well with broader moral frameworks (Saha et al. 2023). However, for Russian war bloggers, the strongest association between morality and othering is found in the purity moral frame within Explicit Dehumanization. This suggests that while Russians use moral language less frequently with dehumanization, when they do, it is often tied to purity, reflecting popular narratives portraying Ukrainians (and the West) as 'puppets' or 'zombies' corrupting Russian values. Meanwhile, for Ukrainian war bloggers, the most morally charged category is Threats to Survival or Physical Safety, most closely associated with care, which aligns with the context of Russia's invasion.\nOverall, both groups display similar trends, but the use of moral language reveals strategic differences. Russian bloggers emphasize purity and cultural threats, reinforcing existential threat and victimization (Geissler et al. 2023), while Ukrainian bloggers focus on care in response to physical threats. Our analysis shows that moral language and othering are deeply intertwined, with Russian bloggers heavily relying on moralized language to justify intergroup prejudice. This suggests moralized othering is an effective propaganda tool, though further research is needed to understand its role in polarized environments."}, {"title": "Othering and Attention", "content": "As in the previous case study, we first applied the chi-squared test to confirm a significant difference in the use of moral language between messages with and without othering language (p < 0.001). We then evaluated the log-odds ratios of moral language in othering messages and the interaction between specific moral categories and othering types, as shown in Figures8 and 15. The most morally loaded othering category is Threats to Survival and Physical Security, primarily associated with care, similar to Ukrainians. Threats to Culture or Identity is linked to equality and loyalty, similar to Russians. Explicit Dehumanization is tied to purity but not to other moral values, unlike in Russians, where it had (weak) links to all moral values.\nThese results not only support the hypothesis that moral language and othering are closely intertwined but also reaffirm the findings from the previous case study, highlighting how moral language shapes and sustains othering narratives.\nNext, we explore the relationship between online attention and the use of othering language in messages by Russian and Ukrainian war bloggers.\nWe measure channel centrality in the reference network of war blogger channels (see Methods) using degree centrality and eigenvector centrality. (For simplicity, we use an undirected version of this network.) Degree centrality reflects the influence distribution and communicative ability of nodes in the network and eigenvector centrality captures the positional importance of network nodes. We then calculate the Spearman correlation between channel centrality and its use of othering language (as a proportion of its messages).\nThe results, shown in Table 4, reveal statistically significant correlations for both degree centrality and eigenvector centrality and the use of othering language by both groups of war bloggers, with a stronger correlation among Russian war bloggers. This suggests that war bloggers who use othering language occupy more influential positions within the Telegram network, though"}]}