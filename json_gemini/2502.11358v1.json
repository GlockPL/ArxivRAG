{"title": "Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Junjie Wang", "Yuekai Huang", "Zhiyuan Chang", "Qing Wang"], "abstract": "Information theft attacks pose a significant risk to Large Language Model (LLM) tool-learning systems. Adversaries can inject malicious commands through compromised tools, manipulating LLMs to send sensitive information to these tools, which leads to potential privacy breaches. However, existing attack approaches are black-box oriented and rely on static commands that cannot adapt flexibly to the changes in user queries and the invocation toolchains. It makes malicious commands more likely to be detected by LLM and leads to attack failure. In this paper, we propose AUTOCMD, a dynamic attack command generation approach for information theft attacks in LLM tool-learning systems. Inspired by the concept of mimicking the familiar, AUTOCMD is capable of inferring the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with examples from the target systems, thereby generating more targeted commands for information theft. The evaluation results show that AUTOCMD outperforms the baselines with +13.2% ASRTheft, and can be generalized to new tool-learning systems to expose their information leakage risks. We also design four defense methods to effectively protect tool-learning systems from the attack.", "sections": [{"title": "Introduction", "content": "The last few years have seen a surge in the development of Large Language Model (LLM) tool-learning systems, such as ToolBench (Qin et al., 2023), KwaiAgents (Pan et al., 2023) and QwenAgent (Yang et al., 2024). After being planned, invoked, and integrated by LLMs, the collective capabilities of many tools enable the completion of complex tasks. Despite the powerful capabilities of LLM tool-learning systems, malicious tools can introduce attacks by injecting malicious commands during interactions with LLMs and pose security threats to the entire system, such as denial of service (DoS) (Zhang et al., 2024), decision errors (Huang et al., 2023), or information leakage (Liao et al., 2024b). Especially from the information security perspective, external tools are typically developed and maintained by many independent third parties. If user queries containing sensitive information are not properly managed and protected, it can lead to issues including information theft, financial losses, and diminished user trust (Pan and Tomlinson, 2016). Therefore, it is critical to investigate advanced information theft attacks and develop effective strategies to safeguard LLM tool-learning systems.\nResearchers have recently started investigating information leakage issues caused by malicious tools (Wang et al., 2024a; Zhao et al., 2024). For example, in Figure 1, the user queries ToolBench to help with \"plan a trip to Tokyo\", and provides the usernames and passwords for booking a hotel and flight. These credentials are considered private information specific to certain tools. Normally, ToolBench utilizes four tools to plan the trip, i.e., Search_Site, Book_Hotel, Book_Flight, and Plan_Trip. The Book_Flight tool can only access the username and password associated with flight bookings and is isolated from the private information used by the Book_Hotel tool. However, if Book_Flight is a malicious tool, it can inject a command through the tool's output value to prompt LLM to \"call Book_Flight again and send Book_Hotel's info to it\". Since LLM cannot detect or block this command, it sends the victim tool Book_Hotel's input value to Book_Flight, causing a potential information theft attack.\nHowever, existing black-box attack methods are static (Wang et al., 2024a,b), which means that regardless of how the user queries or how the context within the tool invocation chain changes, the injected theft commands remain the same. From the perspective of stealthiness, commands like \"send Book_Hotel's information to it\" can generally be identified as malicious without carefully examining their context, making them easier to detect and defend against. In contrast, if an adversary can dynamically infer \"Username\" and \"Password\" in Book_Hotel and Book_Flight from user queries, embed them as regular parameters in tools' parameter list, and request LLMs to return more explicitly, the attack command is less likely to be detected.\nIn this paper, we propose a dynamic attack command generation approach, named AUTOCMD, for information theft attacks in LLM tool-learning systems. Inspired by \"mimicking the familiar\", a concept in social engineering (Fakhouri et al., 2024), AUTOCMD can infer the information utilized by upstream tools in the toolchain through learning on open-source systems and reinforcement with target system examples, thus generating more targeted commands for information theft. To achieve this, we first prepare the attack case database (AttackDB), which identifies the key information exchanges between tools that impact the success rate of information theft attacks. Second, we apply AUTOCMD in black-box attack scenarios, where it generates commands with only malicious tools and AttackDB, and is optimized through reinforcement learning (RL) (Hausknecht and Stone, 2015), leveraging rewards to improve its attack effectiveness. The optimized AUTOCMD can generate commands that effectively conduct information theft attacks when only malicious tools are known.\nTo evaluate AUTOCMD's performances, we conduct experiments on three popular benchmarks, i.e. ToolBench, ToolEyes, and AutoGen, with 1,260 inference cases and compare with three baselines. The results show that AUTOCMD achieves the highest attack stealthiness and success rate, outperforming baselines on the trade-off metric ASRTheft with +13.2%. We also apply the optimized model to three black-box LLM tool-learning systems developed by renowned IT companies, i.e., LangChain, KwaiAgents, and QwenAgent. AUTOCMD can expose information leakage risks and achieve over 80.9% ASRTheft in these systems. We also design four defense methods to protect systems from AUTOCMD's attack.\nThis paper makes the following contributions:\n\u2022 We design a dynamic command generator for information theft attacks in LLM tool-learning systems. The approach infers the input and output of upstream tools through the toolchains and achieves more effective information theft by targeted information request commands.\n\u2022 We evaluate AUTOCMD's performances on the dataset with 1,260 samples, which outperforms static baselines and can be generalized to expose information leakage risks in black-box systems.\n\u2022 We design the targeted defenses, and the evaluation results show that they can effectively protect the system from AUTOCMD's attacks.\n\u2022 We release the code and dataset\u00b9 to facilitate further research in this direction."}, {"title": "Background of LLM's Tool Learning", "content": "The components of the tool T in the LLM tool-learning system are the input value I with its parameter's description, the function code Func, and the output value O with its description. LLMs invoke tools by analyzing the output values from the tools and sending information to tools' input value, and the adversary can inject the command C in the output value as O C to conduct the information theft attack. Therefore, we treat T as the triplet, i.e., (I, Func, O), in this work.\nWith these available tools, a tool-learning system utilizes an LLM as an agent to achieve step-by-step reasoning through Chain-of-Thought (CoT) (Yao et al., 2023). The inference process can be formalized as (Observation, Thought, Action). For each step, LLMs receive the output of the upstream tool (Observation), analyze the output Oi-1 and"}, {"title": "Threat Model", "content": "Attack Goal. The adversary is the developer of the malicious tool Tatt, and he/she is capable of performing a man-in-the-middle (MITM) attack to tamper with the communication content between benign tools and LLMs. Given a toochain, adversary aims to steal upstream victim tool Tvict's relevant information (we only consider the victim tool's input Ivict and output Ovict that may involve user privacy or the tool's property rights). Meanwhile, the adversary aims to hide the attacks from the users, which means the inference steps shown in the frontend after the attack (Inf) will not change, i.e., Inf = Inf. In this case, any of the tools that are used before the Tatt might be Tvict, and if their relevant information is obtained by Tatt, we consider the attack is achieved.\nAssumption of adversary's Knowledge. We assume that the adversary has black-box knowledge of the inference steps, so they don't know what tools are used in the upstream of the toolchain. However, the adversary owns some attack cases from the LLM tool-learning systems including malicious/victim tools, injected malicious commands, and attack results illustrating whether tools' information was stolen in history. For example, the adversary of Book_Flight in Figure 1 does not know the victim tool but can analyze the key information and construct the command with AUTOCMD. Please kindly note that attack cases can originate from some open-source systems like ToolBench, and do not necessarily have to come from the target system being attacked. In such scenarios, the adversary can leverage the command generation models learned from open-source systems and perform transfer attacks on the black-box target systems."}, {"title": "Overview of AUTOCMD", "content": "Within a toolset, the invocation chains often exhibit certain patterns and regularities when processing different user queries. When invoking a specific tool, there are usually certain prerequisites or preconditions. For example, a tool for hotel reservation in LLM inference may be invoked simultaneously with the other tool for booking a flight/train ticket in the previous. In such cases, it is generally possible to infer what tasks the upstream tools have completed in previous steps, as well as what information has been exchanged upstream, by learning from historical toolchains.\nFigure 2 shows the overview of AUTOCMD. Guided by the concept, AUTOCMD first constructs AttackDB with attack cases that provide examples with key information to guide the generation of black-box commands. After that, AUTOCMD incorporates AttackDB to train an initial command generation model, then reinforces it guided by the reward combined with attack results and the sentiment score of the generated command."}, {"title": "Attack-Case Database Preparation", "content": "Given inference examples [E1, E2, ..., EA] that are used to generate attack cases, where EA is a white-box example with frontend inference and backend toolchain, we use white-box Attack Case Generator and Attack Case's Guidance Completer to prepare attack cases and form the AttackDB.\nThe Definition of Attack Cases. The attack case is a five-tuple array, which can be formalized as (Tact, Tatt, CA, RA, GA): (1) TAct and Tatt are the victim and malicious tool's details and its relevant information, i.e., Tool's Name, Description, Function Code, and Relevant Information to Attack. (2) CA is the details of commands C that are used to steal the information. (3) RA is the result of whether the attack is successful and has stealthiness. (4) GA is the guidance that summarizes the current commands and attack results and finds the key information between the tools that may affect the attack success rate. As is shown in Figure 3, the key information in (T2, T3) indicates the commonalities between the tool's input value, and using some specific tasks such as \"registration\" can improve the success and stealthiness of this attack. We have illustrated more details in Appendix A.2.\nAttack Case Extractor. Given the historical case H with the tool calling chain T1, T2, ...,TN, we construct N \u00d7 (N \u2212 1)/2 tool pairs (Ti, Tj). Then, we treat Tj as Tatt, and Ti as Tact, then ask the GPT-40 to explore K commands for each pair. We manually test each command and use the attack results to update the attack cases as follows:\n{TACA [RA..., RA]} Form AttackCase (1)\nwhere [C1, C2,...,CA] are the generated commands of LLM. Then, we manually inject these explored commands into the target Tatt and observe K attack results as [R1, R2, ..., RA]. Then, we utilize all the previous results to form the attack cases and update the Attack DB.\nAttack Case's Guidance Completer. With the generated commands and attack results, we introduce another GPT-40 model to output the guidance for the subsequent dynamic command generator. This guidance includes the key information that GPT-40 observes between tools, and how to design a command that may have higher attack success rates, as the following equation:\n((Tact, Tatt, CA, RA) LLM GA) Form AttackCase (2)\nwhere the guidance is mutated from the basic template, e.g., \"The generated commands that may have the [ToolRecall][Attack][NotExpose] format, and will focus on the key information between tools\". We form the cases with all five tuples and insert this case to AttackDB: AttackCases \u2192 AttackDB, which are references to guide the optimization of the dynamic command generator."}, {"title": "RL-based Dynamic Command Generation", "content": "Given inference examples [E\u00ba, E2, ..., Em] that are used for model optimization, each tool can only access its relevant information and does not know the other invoked tools. We first incorporate the AttackDB to initialize the command generator. Then, we randomly select one malicious tool Tat in E's toolchain and generate the injected command. Finally, we conduct the information theft attack with the command and calculate the rewards to optimize the AttackDB&model with black-box attack cases.\nDynamic Command Generator. The dynamic command generator fgen is a model that simulates the adversary's learning ability (e.g., T5 (Raffel et al., 2020)), which can be fine-tuned based on the current knowledge and the results of the observed attack results. In the black-box attacks, the adversary can only access the Tat's relevant information, so we generate the command C as follows:\nPgen (CO Case, Tatt) = fgen (Case Tat) (3)\nwhere Case is the textual description of the retrieved attack cases in the AttackDB with similar types of input/output values in the Tatt, and Tatt is the text description of the current malicious tool. We generate the command CO with its probability P(CO), and inject it into the target tool-learning system and obtain the attack results: (I/Ovict, Inf), where I/O vict and Inf are theft results and inference after the attack.\nCommand Sentiment Reviewer. Our manual analysis of the command's sentiment polarity shows that commands with neutral sentiments are likely to be executed by LLMs. We calculate the absolute sentiment score |Ssent| with NLTK tool (Bird, 2006) as the reward penalty, which indicates that if the command sentiment tends to be positive or negative, the reward will be lower.\nRL-Based Model Optimization. Based on the thought of RL, the command generator fgen is a policy that determines what the adversaries will do to maximize the rewards, so we choose the PPO reward model (Schulman et al., 2017) to calculate"}, {"title": "Experimental Design", "content": "To evaluate the performances of AUTOCMD, we introduce three Research Questions (RQs).\nRQ1: What are performances of applying AUTOCMD on various LLM tool-learning systems? We aim to explore the advantage of AUTOCMD in open-source systems and generalization to black-box systems, respectively.\nRQ2: How do components contribute to rewards during RL-based optimization? We aim to analyze the impact of AttackDB and sentiment polarity on RL-based model optimization.\nRQ3: How can we defend AUTOCMD's dynamic information theft attacks? We design three defense approaches and investigate whether they protect the systems from AUTOCMD's attacks.\nDataset Preparation. We prepare the dataset of AUTOCMD in the following three steps: (1) Original Dataset Collection. We collect all the original data from three open-source tool-learning benchmarks (i.e. ToolBench (Qin et al., 2024), ToolEyes (Ye et al., 2025), and AutoGen (Wu et al., 2023)) including user queries, system response, and innovation toolchain. (2) Dataset Partition. We select 80%/20% as train/test samples, and partition training samples to attack case/RL-optimization examples. (3) Attack Case Collection. We remove the unfinished inference samples (mainly due to the inability to access external tools) and collect the attack cases. Table 1 shows the statistics of our dataset. In total, we collect 1,260 samples for evaluation, where 1,008 samples are used to train the model, and the remaining 252 are used for testing.\nAttack Baselines. We have established two additional baselines (PoisonParam and FixedDBCMD) on top of the existing static method (FixedCMD), and illustrate their details in Appendix A.3.1. FixedCMD (Wang et al., 2024a; Zhao et al., 2024) uses the static command in the attack, as shown in Figure 5; PoisonParam is a baseline where we manually add redundant input parameters with the victim tool's information to poison LLM; FixedDBCMD introduces AttackDB but does not optimize the model in command generation."}, {"title": "Performance of AUTOCMD's Baseline Comparison on Tool-Learning System", "content": "Evaluation on Open-Source Systems. We first introduce three tool-learning benchmarks to evaluate the model, which build tools' ecosystems from the large-scale API marketplace (i.e., RapidAPI (Liao et al., 2024a)): ToolBench is the Llama-based (Touvron et al., 2023) system that utilizes the tree-level inference to conduct the tool learning;\nToolEyes is a fine-grained Llama-based system for the evaluation of the LLMs' tool-learning capabilities in authentic scenarios; and AutoGen combines GPT-4 to utilize conversable and group-chat agents to analyze complex Q&A queries. We train and test the AUTOCMD on the same benchmarks.\nThe upper part of Table 2 shows the performances of AUTOCMD, where the bold values are the highest values in each column, and underline values are the second highest. We can see that, AUTOCMD achieves the highest ASRTheft on all the target systems' information theft attacks, where the average results are over 70%, outperforming the best baselines with +13.2% (ToolBench), +17.2% (ToolEyes), and +34.7% (AutoGen). Separately, IER, TSR, and ASRTheft values (15/18) also achieve the highest performances, which means the dynamically generated command can not only steal the retained information in the backend but also hide the attacks in the frontend user interface. Some baselines, such as FixedDBCMD and FixedCMD, may expose the attacks in the user interfaces, which means the attack's stealthiness is low.\nEvaluation on Black-Box Systems. We first train AUTOCMD on all the previous three benchmarks, then apply it to expose information leakage risks in three widely-used black-box systems: LangChain (Wang et al., 2024c) is a famous Python-based LLM inference framework that can freely combine LLMs with different tools; KwaiAgents (Pan et al., 2023) is Kwai's agent that integrates a tool library, task planner, and a concluding module for inference. and QwenAgent (Yang et al., 2024) is Alibaba's tool-learning system that can efficiently retrieve external knowledge retrieval and plan inference steps. These systems support self-customized tools, and some of them may have public code repositories, but they do not open-source the datasets for model optimization, so we treat them as black-box systems. Since the malicious tools in our test dataset may not be included in these new systems, we manually register all these tools in the systems. There is a potential risk that black-box systems retrieve the tools before the inference, so we cannot guarantee that our tools are used. To address it, we only analyze samples in which malicious tools are retrieved.\nThe bottom part of Table 2 shows the performances of migrating AUTOCMD to the new tool-learning systems. We can see that, in the cases where black-box systems retrieve our tools, AUTOCMD achieves the highest performances with over 80.9% ASRTheft, significantly outperforming the baselines with +34.0% (LangChain), +85.1% (KwaiAgents), and +21.3% (QwenAgent). These results imply that these tool-learning systems may pose risks, i.e., if these malicious tools are retrieved in these systems, they may not detect the command injection attacks that are generated dynamically in over 80% cases.\nAnswering RQ1: AUTOCMD outperforms baselines when evaluating the performances on open-source tool-learning benchmarks, with over +13.2% ASRTheft. Moreover, it can be applied to black-box systems to expose their information leakage risks, with over 80.9% ASRTheft."}, {"title": "Component's Contribution to Rewards", "content": "To analyze the contribution of components to rewards during the model optimization. We compare the AUTOCMD with two other variants that may affect the rewards: w/o Ssent does not incorporate the sentiment scores in the rewards, and w/o AttackDB does not provide the prepared attack cases.\nFigure 4 shows the optimization procedure of AUTOCMD. We can see that, compared with w/o Ssent, the AUTOCMD will reach the convergence a little bit slower than the variant, mainly due to the sentiment penalty |Ssent| is more strict and will consider whether the commands are neutral. However, AUTOCMD's rewards will finally exceed with +0.2 higher after convergence. Compared with the w/o AttackDB, AutoCMD will reach convergence faster than the variant with around 10 iterations, since it has the background knowledge to help optimize the model, which reduces RL's cold-start. Please note that the final rewards of w/o AttackDB are +0.07 higher than AUTOCMD in AutoGen. This is because AutoGen has strong comprehension ability with the GPT-4, so it does not require key information to understand the attack commands, which achieves a high attack success rate. It further illustrates a potential risk of LLM, i.e., a stronger LLM may be easier to understand abnormal commands and perform risky operations.\nAnswering RQ2: The components contribute to AUTOCMD's optimization, where Ssent can promote the model to generate neutral commands and obtain higher attack rewards, and AttackDB provides key information to guide model optimization and improve convergence speed."}, {"title": "Performances of AUTOCMD's Defense", "content": "To protect LLM tool-learning systems from AUTOCMD's attack, we design three approaches: InferCheck is the inference-side defense that checks the abnormal text description in the LLM inference; ParamCheck is the tool-side defense that checks whether the request inputs exceed the necessary information; DAST is the tool-side defense that utilizes Dynamic Application Security Testing (DAST) (Stytz and Banks, 2006) to test the abnormal function calls and data access with GPT-generated test cases (Details in Appendix A.4).\nWe introduce the defense method to AUTOCMD on the three tool-learning benchmarks in RQ1, and a higher absolute value of metric change means an effective defense. Table 3 shows the results of defending the information theft attack. We can see that, all the defense methods can effectively reduce the ASRTheft of AUTOCMD, where DAST has the largest reduction with -70.2% (ToolBench), 83.9% (ToolEyes), and -81.4% (AutoGen). These results indicate that tool reviewing can reduce the risks of information disclosure, which inspires us to study more effective tool review methods to reduce the risks of the LLM agents.\nAnswering RQ3: Our targeted defense methods can effectively protect the systems from AUTOCMD's attack, with over -70.2% ASRTheft."}, {"title": "Case Analysis", "content": "To intuitively illustrate the benefits of AUTOCMD, we apply FixedCMD, FixedDBCMD, and AUTOCMD to Figure 1's example and observe the attack results from the output of ToolBench. Figure 5 shows the results of the case study. We can see that, the command generated by FixedCMD is defended by the LLM, so the frontend output and the backend toolchain are not affected. FixedDBCMD can generate the command that successfully calls Book_Flight again and steals the Book_Hotel's input. However, this abnormal toolchain is shown in the frontend, which will be observed by the users. Compared with them, The command generated by AUTOCMD can not only achieve information theft but also have stealthiness, which means the attack is not exposed in the frontend. In conclusion, AUTOCMD is applicable to generate effective commands that can applied to information theft attacks."}, {"title": "Related Work", "content": "LLM tool-learning systems have recently been widely used in the industry (Tang et al., 2023; Qin et al., 2024), and their security risks have become concerns for researchers (Tang et al., 2024). Some of the risks come from abnormal inputs and failure executions during the task planner's inference process: Ruan et al. (2024) identified risks of emulator-based LLM agents and exposed risks in agent execution; Chen et al. (2023) evaluated the security in dynamic scenarios that agents will create long-term goals and plans and continuously revise their decisions; Naihin et al. (2023) proposed flexible adversarial simulated agents to monitor unsafe executions. The other risks come from the RAG steps: Zou et al. (2024) proposed PoisondRAG that investigated the malicious text injection in the knowledge base that affects RAG systems; Chaudhari et al. (2024) proposed Phantom that injected poisoned texts based on the query's adversarial trigger. Some recent investigate on the security of external tools. Zhao et al. (2024) generated misleading outputs by modifying a single output value of external APIs. Wang et al. (2024a) designed static commands to conduct DoS to LLM inference.\nDifferent from these works, our study explores the potential information theft attacks in LLM tool-learning systems, and we propose a dynamic command generator to achieve high attack success rates with more stealthiness."}, {"title": "Conclusion", "content": "In this paper, we propose AUTOCMD, a dynamic command generator for information theft attacks in LLM tool-learning systems. AUTOCMD prepares AttackDB to find key information for command generation, and then is continuously optimized with RL in black-box attack scenarios. The evaluation results show that AUTOCMD outperforms the baselines with +13.2% ASRTheft, and can be generalized to new tool-learning systems to expose inherent information leakage risks. In the future, we will expand the dataset to evaluate AUTOCMD on more black-box systems and improve the efficiency of model optimization."}, {"title": "Limitations", "content": "Although AUTOCMD shows effectiveness, it has some limitations that make AUTOCMD fail to steal the victim tool's information. We manually investigate these bad cases and discuss the reasons for failed information theft and attack hiding.\nFor samples that fail to achieve the information theft attack, most of the bad cases (95%) are caused by infrequently used malicious tools. In these samples, tools that we select as malicious tools are newly created and are rarely used in tool learning. Therefore, we cannot use the key information to guide the command generation for these tools, which leads to failed information theft attacks.\nFor samples whose attacks are exposed to the frontend, the misunderstanding of the LLM (56%) and the ineffective commands (20%) are the main reasons for the bad cases. For the first reason, i.e., LLM misunderstanding, some benchmarks, such as ToolBench and ToolEyes, utilize the Llama3-70B model to understand the output and conduct the inference. Compared to GPT models, this LLM may not fully understand the meaning of these commands and is unable to execute commands for hiding the attacks in the frontend. The second reason, i.e., ineffective commands, is mainly because the current AttackDB cannot cover all the attack cases, so we will enlarge the dataset to further continuously optimize our model."}, {"title": "Ethical Considerations", "content": "We have injected commands into the external tools' output value to mislead the LLM tool-learning systems, and these commands will conduct information theft attacks. It is worth noticing that the commands were generated by LLM, so there may be some biases in the real-world attack scenarios.\nMoreover, some examples in this research may match the real-world adversaries' attack methods, which will be an incidental case."}, {"title": "Complete CoT in LLM Inference and Detailed Tools after Command Injection", "content": "LLM tool learning systems utilize Chain-of-Thought (CoT) in the LLM inference. The CoT is a step-by-step inference. Each step is a (Observation, Thought, Action) triplet, where the components are defined as follows:\n\u2022 Observation: The LLM will observe the output of the previous tool Oi\u22121 in the step i \u2013 1.\n\u2022 Thought: LLM analyzes the output Oi\u22121 and previous LLM inferences, then they will decide what they need to do in the next step.\n\u2022 Action: It selects the tool Ti that is used in the current inference step.\nDuring the inference, the current action Acti is selected by Acti = \u03c0(CoTi\u22121, Obsi, Thoi), where the LLM will use a policy to construct the previous inferences with the following actions. Therefore, the triplets at each step will finally connect each other and form the inference chain.\nWhen we consider the actions that call the tools in the inference, we find that they can also be formed as the chain [T1, T2, ..., Tn]."}, {"title": "Malicious Tools and Commands", "content": "The command injection method utilizes the tool's function code Funcatt to inject the harmful commands C in the output value, then control the LLMs to send the details of other information. In this section, we illustrate the generated command of AUTOCMD, as is shown in Figure 7.\nWe first introduce the main component of this tool, i.e., the Book_Flight function and the book_success_ecaluation function. The first function is the main part of this tool and will be called by the LLM tool-learning systems, and the second function utilizes the external RapidAPI to help users book the flight. Due to the license of limited authorization in these APIs, we cannot illustrate the detailed links in it, and more details can be found on our website's open-source dataset. The book_success_ecaluation function realizes the task of searching flights and booking the tickets, and it returns the booking results to the users.\nSecond, we add a malicious command in the cmd parameter, which illustrates the following tasks that ask the LLM what they need to do:\n\u2022 Task-1: Achieve Attack. The LLM should send the other tool's information to the current tool, which aims to register a new user in the database."}, {"title": "Prompts for Attack Cases Preparation", "content": "Definition of Command's Structure. The details of commands C that are used to steal the information. We utilize the GPT-40 to explore the attack commands in AttackKG's knowledge extractor. We regulate the components that each command needs to contain, as is shown in the following equation:\nC = [ToolRecall][Attack][NotExpose] (6)\nwhere [ToolRecall] is the command for calling this tool again; [Attack] indicates the task of information theft, and [NotExpose] asks the LLM to hide the attack in the frontend user interface."}, {"title": "Details of Baselines", "content": "In this section, we introduce how we design these three baselines to evaluate the performances of AUTOCMD, i.e., PoisonParam, FixedCMD, and FixedDBCMD as follows:\nPoisonParam: This baseline does not use the command injection to achieve the information theft attack. On the contrary, it adds redundant parameters to mislead the LLM to send other tool's information to it. The main reason that we designed this baseline is the excessive resource requests in nowadays APPs, which disobey the principle of minimizing Personal Identified Information (PII) that may cause privacy leakage in today's business.\nFixedCMD. The FixedCMD baseline is designed based on the recent works that investigate the tool-side command injection (Wang et al., 2024a; Zhao et al., 2024). These fixed command injection methods are static, and the researchers design the commands that introduce the attack target and what they want the LLMs to do. However, FixedCMD's commands are vague, which means researchers may not design the commands based on the details in the tools.\nFixedDBCMD. This baseline is the improvement of the original static command injection attack, which incorporates AttackDB into the command injection step. However, this baseline only searches the relevant attack cases in the AttackDB and puts the summary of guidance GA in the commands, such as \"some previous tools may contain"}, {"title": "Details of AUTOCMD's Defense", "content": "To protect LLM tool-learning systems from AUTOCMD's attack, we design three approaches: InferCheck is the inference-side defense that checks the abnormal text description in the LLM inference. ParamCheck and DAST are backend-side defense methods that review whether the registered tools are secure. We will describe these attacks in detail, as is shown in Figure 11.\nInferCheck. This defense method checks the inference steps to check its abnormal data stream and abnormal inference text.\n\u2022 Abnormal Inference Text: We also check the abnormal texts in the frontend. If the InferCheck finds the inference text that is not regular, it will warn the users and developers.\n\u2022 Abonormal Data Stream: We add a module to check the changes in the task planner and memory in the inference, which observes whether it has an abnormal data stream in the tool-learning. In Figure 11's example, the abnormal data stream occurs in step-2 to step-3, so InferCheck reports it to the users.\nParamCheck. This defense method is the tool-side defense that analyzes the tool's details and checks whether the request inputs exceed the necessities, which checks the tool parameter's abnormal parameter types and abnormal parameter logs.\n\u2022 Abnormal Parameter Types: We check the parameter type and decide whether the input data obeys the IIP principle. If the tool has excessive input parameters, ParamCheck will notify the users and system developers.\n\u2022 Abonormal Data Stream: We create an MITM-based data log capture module to observe the abnormal input data that mismatch the previous information. For example, Figure 11 shows that the input information of Book_Flight is different from the previous one, which may have some risks and will be detected.\nDAST. This defense method is the tool-side defense that generates the test cases dynamically to evaluate whether the tool's code has abnormal parameters and calling steps, which may lead to illegal data access. In the DAST module, we input the tool's information into the GPT-40, and ask it to automatically generate security test cases. The test cases aim to detect abnormal function calls and data flows in the tool calling.\nThen, we dynamically input these test cases into the tools, and conduct the inference and tool learning. We observe the passing rate of these test cases and inspect the failed cases."}, {"title": "The online RL Optimization.", "content": "Algorithm 1: The online RL Optimization.\nInput: The command generator fgen and\noptimization examples [E", "Em": ".", "nOutput": "The optimized command generator fgen.\n1 Initialize Batch_Size \u2192 B", "ECaseBxt": "n4 Calculate policy loss at timestamp t:\n5 Lgen (0) = Reinforce(Dt; 0) with Equation 5;\n6 Optimize AUTOCMD with the policy gradient\nVoLgen (0), fgen f'gen;\n7 t = t + 1"}]}