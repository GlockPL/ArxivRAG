{"title": "Lagrangian Index Policy for Restless Bandits with Average Reward", "authors": ["Konstantin Avrachenkov", "Vivek S. Borkar", "Pratik Shah"], "abstract": "We study the Lagrangian Index Policy (LIP) for restless multi-armed bandits with long-run average reward. In particular, we compare the performance of LIP with the performance of the Whittle Index Policy (WIP), both heuristic policies known to be asymptotically optimal under certain natural conditions. Even though in most cases their performances are very similar, in the cases when WIP shows bad performance, LIP continues to perform very well. We then propose reinforcement learning algorithms, both tabular and NN-based, to obtain online learning schemes for LIP in the model-free setting. The proposed reinforcement learning schemes for LIP requires significantly less memory than the analogous scheme for WIP. We calculate analytically the Lagrangian index for the restart model, which describes the optimal web crawling and the minimization of the weighted age of information. We also give a new proof of asymptotic optimality in case of homogeneous bandits as the number of arms goes to infinity, based on exchangeability and de Finetti's theorem.", "sections": [{"title": "Introduction", "content": "We consider restless multi-armed bandits with long-run average reward criterion as in [44]. These are Markov Decision Processes (MDPs) with two actions (\u2018active' and 'passive') coupled only through the constraint on the number of active actions. This model has numerous applications in resource allocation [5, 26], queueing systems [1, 4, 18], web crawling and age of information [6, 23, 39], clinical trials and other health applications [12, 41, 42], just to name a few. See the books [22, 24, 37] and the recent survey [30] for thorough accounts of theory and applications of restless bandits."}, {"title": "Model Formulation and Index Policies", "content": "Consider $N > 1$ controlled discrete-time Markov chains (arms in bandit terminology) $X_n, n \\geq 0$, for $1 \\leq i \\leq N$, on finite state space $X$, controlled by the $U := \\{0,1\\}$-valued control sequences $U_n, n \\geq 0$. Given the control $U_n$, the $i$-th chain moves from the current state $X \\in X$ to the next state $y \\in X$ according to the transition probability kernel $p^i(y|X, U_n)$ and obtains the reward $r^i(X, U_n)$. We seek to solve the following long-run average reward MDP problem:\n(Po) Maximize the long-run average reward\n$\\lim_{n\\rightarrow\\infty} \\frac{1}{n} E\\left[\\sum_{m=0}^{n-1} \\sum_{i=1}^{N} r^i(X_m, U_m)\\right]$\\qquad(2.1)\nsubject to the constraint\n$\\sum_{i=1}^{N} U_n^i = M, \\qquad n \\geq 0,$\\qquad(2.2)\nfor a prescribed $1 < M < N$. The constraint (2.2) means that we need to set exactly $M$ arms active at each time step.\nWe shall consider the Whittle relaxation [44] of this problem, where we replace (2.2) by an average constraint. Thus the problem becomes\n(P1) Maximize the average reward\n$\\lim_{n\\rightarrow\\infty} \\frac{1}{n} E\\left[\\sum_{m=0}^{n-1} \\sum_{i=1}^{N} r^i(X_m, U_m)\\right]$\\qquad(2.3)"}, {"title": "Tabular Q-Learning for Lagrangian Indices", "content": "In many practical situations the model ($p^i(y|x, u)$ and $r^i(x, u)$) is not known exactly or known approximately. This motivates us to develop model-free learning algorithms for the Lagrangian index. We propose two reinforcement learning algorithms: the first algorithm learns the Lagrangian index and does not enforce the hard constraint (2.2). This algorithm can be used for instance when a simulator is available. The second algorithm respects the hard constraint (2.2) during the learning process. Both algorithms are based on two-time scale primal-dual iterations for the saddle point reformulation of the Lagrangian relaxation [3, 10]:\n$\\lambda^* = arg \\min_{\\lambda} \\max_{\\varphi} L(\\varphi, \\lambda),$ \\qquad(3.1)\nwhere\n$L(\\varphi, \\lambda) = \\lim_{n\\rightarrow\\infty} \\frac{1}{n} E\\left[\\sum_{m=0}^{n-1} \\sum_{i=1}^{N} (r^i(X_m, U_m) + \\lambda(I\\{U_m=1\\} - \\frac{M}{N}))\\right]$\\qquad(3.1)\nis the Lagrangian.\nThe primal iterates correspond to RVI Q-learning [2] and are carried out on the faster time scale. Namely, the Q-values are updated according to\n$Q_{n+1}^i(x, u) = Q_n^i(x, u) +\\alpha(\\nu(x, u, n, i))I\\{X_n = x, Z_n = u\\} \\times (u\\gamma(x, 1) +(1-u)(\\gamma_n + r(x, 0)) + max_vQ_n^i(X_{n+1}, v)- f(Q_n^i) - Q_n^i(x, u)),$ \\qquad(3.2)\nwhere $X_{n+1} \\sim p^i(\\cdot |x, u), \\nu(x, u, n, i)$ counts the number of times $Q^i(x, u)$ has been updated till time $n$, and where4\n$f(Q) = \\frac{1}{2} \\sum_{i \\in S} (Q(i,0) + Q(i,1).$ \\qquad(3.3)\nThe Lagrange multiplier is estimated by the dual iterates on the slower time scale:\n$\\lambda_{n+1} = \\lambda_n - \\beta(n) (\\sum_{i=1}^{N} U_n^i - M),$\\qquad(3.4)"}, {"title": "Deep Q-Learning for Lagrangian Indices", "content": "Till now we have used tabular Q-learning for learning Q-values and ultimately the Lagrangian indices. Now, let us use the Deep Q-Learning Network (DQN) [27, 28] for approximation of the Q values. This will especially be useful in cases where the state space is large. A complete scheme of how our DQN algorithm works can be found in Algorithm 3. We note that in comparison with the DQN schemes for WIP [31, 35, 36], the DQN for LIP has a much simpler architecture and as a result numerically more stable and simpler to tune. This is because the DQN algortihms for WIP [31, 35, 36] require extra input for the reference state.\nIn the case of homogeneous arms, one-dimensional input to NN suffices, which represents the state of the arm. In the case of heterogeneous arms, a two-dimensional input to NN is used. The first dimension is used to give information about the type of arm, and the second dimension specifies the state of the arm. The neural network outputs are the Q-values for both possible actions. The actions are taken in an $\\epsilon$-greedy fashion. Namely, the probability for exploration $\\epsilon$ is initialized as one and then, at the end of every iteration, is reduced by the exploration-decay factor. The min value of $\\epsilon$ is set to be 0.01.\nThe equation for the Q target used in training DQN is:\n$Q_{target}(s_n, a_n) \\leftarrow ((1 - \\alpha_n) (r_0(s_n) + \\lambda_n) + \\alpha_n r_1(s_n) + \\frac{1}{|S|} \\sum_{k \\in S} 2\\alpha_n (Q_n(k, 0) + Q_n(k, 1))$).\nNote that in DQN we employ a second neural network, called the target model, for the computation of $max_{v \\in \\{0,1\\}} Q_n (s_{n+1}, v)$. The reason for this decision is due to the maximisation bias of Q-learning: overestimating the Q-values results in the error increase over time, due to the target being $r + \\gamma max_a Q(s, a)$. The use of a second neural network for the target helps control this bias. This second neural network copies periodically the parameter values of the main network.\nWe store the $(s_n, a_n, r_n, s_{n+1}, \\lambda_n)$ tuples in an experience replay buffer, from which a batch of random samples is taken every time to train the main neural network. For each tuple $(s_n, a_n, r_n, s_{n+1}, \\lambda_n)$ we calculate Q target values using equation (4.1). Once we have calculated Q-values for all the examples in batch through Q(.), we compute the loss function as the mean square error between the two and train the main neural network, through a standard Adam optimiser."}, {"title": "Application to Restart Model", "content": "Let us consider the following MDP, which can describe both the optimal web crawling [6] and the minimization of the weighted age of information [23, 38].\nThere are $N$ information sources, which can be viewed as arms of RMAB. The central device can probe $M$ information sources at a time. If the central device probes source $i$, it succeeds with probability $p_i$ (the failure can be due to either the distant server timeout or due to the loss of the probing packet). The freshness of the information at source $i$ at time slot $n$ is modelled by $X^i_n$ with the following dynamics:\n$X^i_{n+1} = \\begin{cases}\nX^i, & \\text{if $U_n^i = 1$ and the probe is successful,}\nX^i+1, & \\text{otherwise.}\n\\end{cases}$ \\qquad(5.1)\nFor the relaxed problem formulation (P2), we consider the following immediate reward\n$r(x, u) = -w_i x + \\lambda u,$\\qquad(5.2)\nwhere $w_i$ gives the factor of importance of source $i$. We shall again use the saddle point reformulation (3.1) of the Lagrangian relaxation. Since we have moved the constraint to the Lagrangian, the optimal policy can be taken to be deterministic. In fact, it is proven in [23] (see also a similar development in [6]) that the optimal policy for the unconstrained MDP with the reward (5.2) is of threshold type5. Therefore, we can search for an optimal policy"}, {"title": "Asymptotic Optimality", "content": "In this section we provide an alternative proof of asymptotic optimality of the Lagrangian index for the case of homogeneous arms.\nSince in this case the arms are identical, by symmetry one could divide both the displays in the statement of (P1) by $N$ and reduce to a single constrained problem of maximizing the stationary expectation of $r(X_m, U_m)$ subject to the stationary expectation of $I\\{U_m = 1\\}$ being equal to $\\frac{M}{N}$. This symmetry implies in particular that $\\lambda^*$ is also the Lagrange multiplier for this single arm constrained MDP. This MDP, however, is a complete artifice, but it will play a role in our proof. We assume that these MDPs are irreducible under all stationary policies. From the theory of constrained MDPs [3, 11, 34], we have the following."}, {"title": "Numerical Examples", "content": "In this first numerical experiment, we consider the restart model as described in Section 5 and apply Algorithm 4. There are four different categories of arms:\n\\begin{itemize}\n    \\item Type 1: $p_1 = 0.95, w_1 = 0.9$ (reliable, important)\n    \\item Type 2: $p_2 = 0.95, w_2 = 0.2$ (reliable, not important)\n    \\item Type 3: $p_3 = 0.7, w_3 = 0.95$ (not very reliable, important)\n    \\item Type 4: $p_4 = 0.7, w_4 = 0.2$ (not very reliable, not important)\n\\end{itemize}\nA total of 100 arms are taken, with 25 belonging to each category and the budget is set as 16 arms. Therefore, in this example $N = 100$ and $M = 16$. Note that for all the 100 arms we still learn just one common Lagrangian multiplier.\nFigure 1(a) shows the convergence of the subsidy (Lagrange multiplier) to the value -11.6. In the right Figure 1(b) we plot the value of the optimal Lagrangian function (optimized with respect to the policy) at different values of the Lagrange multiplier and as expected we see that the Lagrangian function is piecewise linear and takes the minimum value around -11.6 (shown by the red line) same as the value of the Lagrange multiplier we obtain by our Algorithm 4."}, {"title": "Conclusion", "content": "We have proposed and analyzed the Lagrangian index for average cost restless bandits. It is computationally more tractable than Whittle index when the latter is not explicitly known and does not require Whittle indexability. We have calculated analytically the Lagrangian index for the restart model. Then, we have provided several numerical experiments. LIP is seen to perform comparably well on problems that are Whittle indexable and as well or better on problems that are not. We also prove its asymptotic optimality in the infinite arms limit under the popular \u2018global attractor hypothesis' using a novel argument based on exchangeability and de Finetti's theorem."}]}