{"title": "KAN See In the Dark", "authors": ["Aoxiang Ning", "Minglong Xue", "Jinhong He", "Chengyun Song"], "abstract": "Existing low-light image enhancement methods are difficult to fit the complex nonlinear relationship between normal and low-light images due to uneven illumination and noise effects. The recently proposed Kolmogorov-Arnold networks (KANs) feature spline-based convolutional layers and learnable activation functions, which can effectively capture nonlinear dependencies. In this paper, we design a KAN-Block based on KANs and innovatively apply it to low-light image enhancement. This method effectively alleviates the limitations of current methods constrained by linear network structures and lack of interpretability, further demonstrating the potential of KANs in low-level vision tasks. Given the poor perception of current low-light image enhancement methods and the stochastic nature of the inverse diffusion process, we further introduce frequency-domain perception for visually oriented enhancement. Extensive experiments demonstrate the competitive performance of our method on benchmark datasets. The code will be available at: https://github.com/AXNing/KSID.", "sections": [{"title": "I. INTRODUCTION", "content": "Low-light image enhancement (LLIE) is a critical task in computer vision and is essential for various applications ranging from surveillance to autonomous driving [12]. Images captured in low-light environments often suffer from low contrast and loss of detail, making downstream tasks such as object or text detection, semantic segmentation, and others highly challenging [22]. Therefore, to further enhance various visual applications in poor environments, low-light image enhancement tasks have received extensive attention from researchers [4], [25], [26].\nTraditional methods utilize retinex theory [3] and gamma correction [19] to correct image illumination. With the development of deep learning, some methods [1], [24], [31], [32] significantly improve the performance of low-light image enhancement by learning the mapping between low-light and normal images in a data-driven way. Recently, the diffusion model [5], [18] has received much attention for its remarkable performance in generative tasks. [2], [6] introduced the diffusion model into the low-light image enhancement task to improve the recovery of image details and textures in low-light conditions. However, there are nonlinear degradation factors in low-light enhancement tasks, such as uneven illumination and"}, {"title": "II. METHODS", "content": "The Kolmogorov-Arnold theorem states that any continuous function can be represented as a composition of a finite number of continuous univariate functions. Specifically, for any continuous function $f(x)$ defined in n-dimensional real space, where $x = (X1,X2,...,xn)$, it can be expressed as a composition of a univariate continuous function h and a series of continuous bivariate functions xi and $gq,i$. Specifically, the theorem shows that there exists such a representation:\n$$f(x_1, x_2,...,x_n) = \\sum_{q=1}^{2n+1}h(\\sum_{i=1}^{n}g_{q,i} (X_i))$$\nThis representation indicates that even complex functions in high-dimensional spaces can be reconstructed through a series of lower-dimensional function operations."}, {"title": "B. Overall Network Architecture", "content": "The structure of our proposed (KSID) is shown in Fig. 2 (a). Our training is divided into two phases: In the first phase, inspired by [6], we introduced uncertainty-guided regularization into the diffusion process to enhance the recovery performance in challenging areas; in the second phase, we froze the weights of the uncertainty network to guide the network's learning. In both phases, we utilized the KAN-Block to strengthen the learning of nonlinear dependencies, and in the second phase, we incorporated a frequency-domain perception module to achieve visually-guided enhancement."}, {"title": "C. KAN-Block", "content": "We aim to embed KANs into low-light image enhancement networks to enhance the interpretability of the model and its ability to learn from nonlinear dependencies. As shown in Fig. 2 (c), [16] proposed KANs based on the Kolmogorov-Arnold representation theorem, similar to multilayer perception (MLP), KANs with N KAN-Layers can be represented as:\n$$\u039a\u0391\u039d(Z) = (\u03a6_{N-1} \u25cb \u03a6_{N-2} \u25cb \u22ef \u25cb \u03a6_1 \u25cb \u03a6_0)$$\nwhere Z is the input feature; \u0424\u1d62 signifies the i-th layer of the entire KAN network. Each KAN-Layer, with $n_{in}$-dimensional input and $n_{out}$-dimensional output, I comprises $n_{in}$ \u00d7 $n_{out}$ learnable activation functions \u03a6:\n$$\u03a6 = {\\phi_{q,p}}, p = 1,2,..., N_{in}, q = 1,2,..., N_{out}$$\nwhere $\\phi_{q,p}$ is the parameter that can be learned. Unlike the traditional MLP, the weight of each connection in KAN is not a simple numerical value but is parameterized as a learnable spline function, which is used to parameterize these variable functions in the formula. The results of the computation of KANs from the l-th layer to the 1+1-th layer can be expressed in the form of a matrix:\n$$Z_{l+1} = \\begin{bmatrix}\n  \\phi_{l,1,1}(\\cdot) & \\phi_{l,2,1}(\\cdot) & \\dots & \\phi_{l,n_{l+1},1}(\\cdot) \\\\\n  \\phi_{l,1,2}(\\cdot) & \\phi_{l,2,2}(\\cdot) & \\dots & \\phi_{l,n_{l+1},2}(\\cdot) \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\phi_{l,1,n_l}(\\cdot) & \\phi_{l,2,n_l}(\\cdot) & \\dots & \\phi_{l,n_{l+1},n_l}(\\cdot)\n\\end{bmatrix} Z_l$$\nwhere \u03a6\u2081 is the function matrix corresponding to the l-th KAN-Layer.\nAs shown in Fig. 2(b), we designed KAN-Block to introduce KANs into the U-Net structure in the low-light enhancement task. The U-Net extracts high-level features through a stepwise downsampling operation and recovers low-level details using jump connections. To avoid interference from low-level information, we replace the middle layer in the U-Net with the KAN-Block, with no change in the sampling stage.\nSpecifically, our network first takes the image $X_i \u2208 R^{H\u00d7W}$ with added random noise $ \u2208 R^{HW}$ and extracts the high-frequency features through downsampling operations and residual learning, where the height and width become 1/8 of the original, and we change its shape and input it into the first KAN-Block. As shown in Fig. 2(b), KAN-Block consists of KAN-Layer and DwConv. We take the input features by combining multiple KAN-Layer and DwConv to learn the nonlinear dependencies and fine-grained information further. The operation is defined as follows:\n$$\u03a7_{l} = DwConv(\u039a\u0391\u039d(\u03a7_{l-1}))$$\nwhere X\u2081 is the feature map output from the l-th KAN-Layer. To learn global nonlinear dependencies, we designed a loop structure."}, {"title": "D. Frequency Domain Perception Module", "content": "Although the current low-light enhancement methods based on the diffusion model have made good progress, due to the stochastic nature of their inverse diffusion process and unsatisfactory visual effects, we have introduced frequency-domain perception to make the whole process more stable and achieve visually oriented enhancement.\nThe training of the diffusion denoising probabilistic model [18] (DDPM) starts with obtaining a closed form Xt at any time step t; in the subsequent process, the model uses a learnable function e\u0473(Y, Xt,\u0101t) to learn the underlying noise distribution [6]. Since the network can successfully learn this noise, we construct a learnable Xt-1 for frequency-domain perception, thereby constraining the entire training process to be more stable. It is defined as follows:\n$$Xt-1 = \\frac{1}{\\sqrt{(1 - a_t)}} (Xt - \\frac{\\beta_t}{\\sqrt{1-a_t}} \u0395\u03b8(\u03a5, \u03a7\u03c4, \u03ac\u03c4))$$\nwhere \u20ac\u03b8 \u2208 RH\u00d7W is the predicted noise distribution. We introduce a frequency domain perceptual loss to learn the spectrum of the normal image. Specifically, we perform the FFT of Xo and Xt\u22121, respectively, which can be expressed as follows:\n$$amphigh, phahigh = F(X0)$$\n$$amplow,phalow = F(Xt-1)$$\nwhere amp denotes amplitude and pha denotes phase. F stands for Fast Fourier Transform. To align Xt-1 with Xo"}, {"title": "III. EXPERIMENTS", "content": "We used four common low-light image enhancement benchmark datasets for evaluation: LOLv1, LOLv2, and LSRW. For evaluation metrics, we use the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) as two full-reference distortion metrics to evaluate the performance of the proposed method. In addition, we use Learned Perceptual Image Block Similarity (LPIPS) and Fr\u00e9chetInceptionDistance (FID) as two perceptual metrics to measure the visual quality of the enhanced results.\nWe implemented KSID on a NVIDIA RTX 3090 GPU with PyTorch, setting the batch size to 8 and patch size to 96x96. The learning rate was set to 1e-4, with Adam as the optimizer. The training process is divided into two phases: the first phase focuses on optimising the uncertainty network, with the number of epochs set to 1e6; the second phase extends the training by setting the number of epochs to 2e6."}, {"title": "B. Comparisons With State-of-the-Art Methods", "content": "We qualitatively and quantitatively compare the proposed KSID with state-of-the-art low-light image enhancement methods. We train on the LOLv1 dataset and test on all datasets.\nTable I presents the quantitative results of various LLIE methods, indicating that our approach is competitive across PSNR, SSIM, LPIPS, and FID metrics. Notably, our method attains the best SSIM scores on the LOLv1 dataset and remains competitive in other metrics. Furthermore, on the LOLv2-real dataset, our method excels in both SSIM and PSNR, which are full-reference metrics, as well as in FID and LPIPS, which are perceptual metrics. These results underscore the ability of our model to effectively learn and emulate complex nonlinear degradation processes in image restoration, while also adeptly recovering image details and markedly enhancing their perceptual quality. Additionally, our method achieves the best results on the large-scale LSRW dataset in SSIM, LPIPS, and FID metrics, further confirming its strong generalization and robustness."}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a novel low-light image enhancement method, KSID, which introduces KANs into the LLIE task for the first time, improves the model's ability to learn nonlinear dependencies and achieves high-quality mapping of the degradation parameters. In addition, we introduce the Frequency Domain Perception Module to refine the image details further and make the inverse diffusion process more stable. Extensive experiments validate the effectiveness and robustness of our method. Overall, we provide an initial exploration of the potential of KANs in the field of LLIE and argue that this non-traditional linear network structure is important for processing low-level visual tasks."}]}