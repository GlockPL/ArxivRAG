{"title": "Improving Predictor Reliability with Selective Recalibration", "authors": ["Thomas P. Zollo", "Zhun Deng", "Jake C. Snell", "Toniann Pitassi", "Richard Zemel"], "abstract": "A reliable deep learning system should be able to accurately express its confidence with respect to its predictions, a quality known as calibration. One of the most effective ways to produce reliable confidence estimates with a pre-trained model is by applying a post-hoc recalibration method. Popular recalibration methods like temperature scaling are typically fit on a small amount of data and work in the model's output space, as opposed to the more expressive feature embedding space, and thus usually have only one or a handful of parameters. However, the target distribution to which they are applied is often complex and difficult to fit well with such a function. To this end we propose selective recalibration, where a selection model learns to reject some user-chosen proportion of the data in order to allow the recalibrator to focus on regions of the input space that can be well-captured by such a model. We provide theoretical analysis to motivate our algorithm, and test our method through comprehensive experiments on difficult medical imaging and zero-shot classification tasks. Our results show that selective recalibration consistently leads to significantly lower calibration error than a wide range of selection and recalibration baselines.", "sections": [{"title": "1 Introduction", "content": "In order to build user trust in a machine learning system, it is important that the system can accurately express its confidence with respect to its own predictions. Under the notion of calibration common in deep learning (Guo et al., 2017; Minderer et al., 2021), a confidence estimate output by a model should be as close as possible to the expected accuracy of the prediction. For instance, a prediction assigned 30% confidence should be correct 30% of the time. This is especially important in risk-sensitive settings such as medical diagnosis, where binary predictions alone are not useful since a 30% chance of disease must be treated differently than a 1% chance. While advancements in neural network architecture and training have brought improvements in calibration as compared to previous methods (Minderer et al., 2021), neural networks still suffer from miscalibration, usually in the form of overconfidence (Guo et al., 2017; Wang et al., 2021). In addition, these models are often applied to complex data distributions, possibly including outliers, and may have different calibration error within and between different subsets in the data (Ovadia et al., 2019; Perez-Lebel et al., 2023). We illustrate this setting in Figure la with a Reliability Diagram, a tool for visualizing"}, {"title": "2 Related Work", "content": "Making well-calibrated predictions is a key feature of a reliable statistical model (Guo et al., 2017; Hebert-Johnson et al., 2018; Minderer et al., 2021; Fisch et al., 2022). Popular methods for improving calibration error given a pretrained model and labeled validation dataset include Platt (Platt, 1999) and temperature scaling (Guo et al., 2017), histogram binning (Zadrozny & Elkan, 2001), and Platt binning (Kumar et al., 2019) (as well as others like those in Naeini et al. (2015); Zhang et al. (2020)). Loss functions have also been proposed to improve the calibration error of a neural network in training, including Maximum Mean Calibration Error (MMCE) (Kumar et al., 2018), S-AvUC, and SB-ECE (Karandikar et al., 2021). Calibration error is typically measured using quantities such as Expected Calibration Error (Naeini et al., 2015; Guo et al., 2017), Maximum Calibration Error (Naeini et al., 2015; Guo et al., 2017), or Brier Score (Brier, 1950) that measure whether prediction confidence matches expected outcomes. Previous research (Roelofs et al., 2022) has demonstrated that calibration measures calculated using binning have lower bias when computed using equal-mass bins.\nAnother technique for improving ML system reliability is selective classification (Geifman & El-Yaniv, 2017; El-Yaniv & Wiener, 2010), wherein a model is given the option to abstain from making a prediction on certain examples (often based on confidence or out-of-distribution measures). Selective classification has been well-studied in the context of neural networks (Geifman & El-Yaniv, 2017; 2019; Madras et al., 2018). It has been shown to increase disparities in accuracy across groups (Jones et al., 2021), although work has been done to mitigate this effect in both classification (Jones et al., 2021) and regression (Shah et al., 2022) tasks by enforcing calibration across groups.\nRecent work by Fisch et al. (2022) introduces the setting of calibrated selective classification, in which predictions from a pre-trained model are rejected for the sake of improving selective calibration error. The authors propose a method for training a selective calibration model using an S-MMCE loss function derived from the work of Kumar et al. (2018). Our work differs from this and other previous work by considering the joint training and application of selection and recalibration models. While Fisch et al. (2022) apply selection directly to a frozen model's outputs, we contend that the value in our algorithm lies in this joint optimization. Also, instead of using S-MMCE, we propose a new loss function, S-TLBCE, which more closely aligns with the objective function for Platt and temperature scaling."}, {"title": "3 Background", "content": "Consider the multi-class classification setting with K classes and data instances $(x, y) \\sim D$, where x is the input and $y \\in \\{1,2,..., K\\}$ is the ground truth class label. For a black box predictor f, $f(x) \\in \\mathbb{R}^K$ is a"}, {"title": "3.1 Selective Classification", "content": "In selective classification, a selection model g produces binary outputs, where 0 indicates rejection and 1 indicates acceptance. A common goal is to decrease some error metric by rejecting no more than a 1 - \u03b2 proportion of the data for given target coverage level \u03b2. One popular choice for input to g is the feature embedding $f_e(x)$, although other representations may be used. Often, a soft selection model $\\hat{g} : \\mathbb{R}^d \\rightarrow [0,1]$ is trained and g is produced at inference time by choosing a threshold \u03c4 on $\\hat{g}$ to achieve coverage level \u03b2 (i.e., $\\mathbb{E}[1{\\{\\hat{g}(X) \\geq \\tau\\}}] = \\beta$)."}, {"title": "3.2 Calibration", "content": "The model f is said to be top-label calibrated if $\\mathbb{E}_p[y^\\circ |f(x) = p] = p$ for all $p \\in [0, 1]$ in the range of f(x). To measure deviation from this condition, we calculate expected calibration error (ECE):\n$ECE_q = \\bigg(\\mathbb{E}_{f(x)} \\bigg[ \\bigg( | \\mathbb{E}_{D}[y^\\circ| f(x)] - f(x) |\\bigg)^q \\bigg]\\bigg)^{\\frac{1}{q}}$ (1)\nwhere q is typically 1 or 2. A recalibrator model h can be applied to f to produce outputs in the interval [0, 1] such that $h(f(x)) \\in \\mathbb{R}^K$ is the recalibrated prediction confidence for input x and $\\hat{h}(f(x)) = \\max_k h(f(x))_k$. See Section 4.3 for details on some specific forms of h(\u00b7)."}, {"title": "3.3 Selective Calibration", "content": "Under the notion of calibrated selective classification introduced by Fisch et al. (2022), a predictor is selectively calibrated if $\\mathbb{E}_v [y^\\circ| f(x) = p, g(x) = 1] = p$ for all $p \\in [0, 1]$ in the range of f(x) where g(x) = 1. To interpret this statement, for the subset of examples that are accepted (i.e., g(x) = 1), for a given confidence level p the predicted label should be correct for p proportion of instances. Selective expected calibration error is then calculated as:\n$S-ECE_q = \\bigg(\\mathbb{E}_{f(x)} \\bigg[ \\bigg( | \\mathbb{E}_{D}[y^\\circ| f(x), g(x) = 1] - f(x) |\\bigg)^q \\bigg| g(x) = 1 \\bigg]\\bigg)^{\\frac{1}{q}}.$ (2)\nIt should be noted that selective calibration is a separate goal from selective accuracy, and enforcing it may in some cases decrease accuracy. For example, a system may reject datapoints with f(x) = 0.7 and $\\mathbb{P}(y = 1|x) = 0.99$ (which will be accurate 99% of the time) in order to retain datapoints with f(x) = 0.7 and $\\mathbb{P}(y = 1|x) = 0.7$ (which will be accurate 70% of the time). This will decrease accuracy, but the tradeoff would be acceptable in many applications where probabilistic estimates (as opposed to discrete labels) are the key decision making tool. See Section 5.2.1 for a more thorough discussion and empirical results regarding this potential trade-off. Here we are only concerned with calibration, and leave methods for exploring the Pareto front of selective calibration and accuracy to future work."}, {"title": "4 Selective Recalibration", "content": "In order to achieve lower calibration error than existing approaches, we propose jointly optimizing a selection model and a recalibration model. Expected calibration error under both selection and recalibration is equal to\n$SR-ECE_q = \\bigg(\\mathbb{E}_{h(f(x))} \\bigg[ \\bigg( | \\mathbb{E}_{D}[y^\\circ|h(f(x)), g(x) = 1] - h(f(x)) |\\bigg)^q \\bigg| g(x) = 1 \\bigg]\\bigg)^{\\frac{1}{q}}$ (3)"}, {"title": "4.1 Selection Loss", "content": "In selective recalibration, the selection loss term measures the calibration of selected instances. Its general form for a batch of data $D = \\{(x_i, y_i)\\}_{i=1}^n$ with n examples is\n$L_{sel} = \\frac{l(\\hat{g}, h; f, D)}{\\sum_{i=1}^n \\hat{g}(x_i)}$ (6)\nwhere l measures the loss on selected examples and the denominator scales the loss according to the proportion preserved. We consider 3 forms of l: S-MMCE of Fisch et al. (2022), a selective version of multi-class cross entropy, and our proposed selective top label cross entropy loss."}, {"title": "4.1.1 Maximum Mean Calibration Error", "content": "We apply the S-MMCE loss function proposed in Fisch et al. (2022) for training a selective calibration system. For a batch of training data, this loss function is defined as\n$l_{S-MMCE} (\\hat{g}, h; f, D) = \\frac{1}{\\binom{n}{2}} \\bigg[ \\sum_{i,j}^n \\bigg( |\\mathbb{E}_{y|x_i} [y_i] - h(f(x_i))||y_j - h(f(x_j))|^*\\hat{g}(x_i)\\hat{g}(x_j) \\phi[h(f(x_i)), h(f(x_j))] \\bigg)^{\\frac{1}{q}} \\bigg]$ (7)\nwhere \u03c6 is some similarity kernel, like Laplacian. On a high level, this loss penalizes pairs of instances that have similar confidence and both are far from the true label y (which denotes prediction correctness 0 or 1). Further details and motivation for such an objective can be found in Fisch et al. (2022)."}, {"title": "4.1.2 Top Label Binary Cross Entropy", "content": "Consider a selective version of a typical multi-class cross entropy loss:\n$l_{S-MCE} (\\hat{g}, h; f, D) = - \\frac{1}{n} \\sum_{i=1}^n \\hat{g}(x_i) \\log h(f(x_i))_{y_i}.$ (8)"}, {"title": "4.2 Coverage Loss", "content": "When the goal of selection is improving accuracy, there exists an ordering under $\\hat{g}$ that is optimal for any choice of \u03b2, namely that where $\\hat{g}$ is greater for all correctly labeled examples than it is for any incorrectly labeled example. Accordingly, coverage losses used to train these systems will often only enforce that no more than \u03b2 proportion is to be rejected. Unlike selective accuracy, selective calibration is not monotonic with respect to individual examples and a mismatch in coverage between training and deployment may hurt test performance. Thus in selective recalibration we assume the user aims to reject exactly \u03b2 proportion of the data, and employ a coverage loss that targets a specific \u03b2,\n$L_{cov} (\\beta) = \\bigg(\\beta - \\frac{1}{n} \\sum_{i=1}^n \\hat{g}(x_i)\\bigg)^2.$ (10)\nSuch a loss will be an asymptotically consistent estimator of $(\\beta - \\mathbb{E}[\\hat{g}(x)])^2$. Alternatively, Fisch et al. (2022) use a logarithmic regularization approach for enforcing the coverage constraint without a specific target \u03b2, computing cross entropy between the output of $\\hat{g}$ and a target vector of all ones. However, we found this approach to be unstable and sensitive to the choice of \u03bb in initial experiments, while our coverage loss enabled stable training at any sufficiently large choice of \u03bb, similar to the findings of Geifman & El-Yaniv (2019)."}, {"title": "4.3 Recalibration Models", "content": "We consider two differentiable and popular calibration models, Platt scaling and temperature scaling, both of which attempt to fit a function between model confidence and output correctness. The main difference between the models is that Platt scaling works in the output probability space, whereas temperature scaling is applied to model logits before a softmax is taken. A Platt recalibrator (Platt, 1999) produces output according to\n$h_{Platt} (f(x)) = \\frac{1}{1 + \\exp(w f(x) + b)}$ (11)\nwhere w, b are learnable scalar parameters. On the other hand, a temperature scaling model (Guo et al., 2017) produces output according to\n$h_{Temp} (f_s(x)) = \\text{softmax} \\bigg( \\frac{f_s(x)}{T} \\bigg)$ (12)\nwhere $f_s(x)$ is the vector of logits (unnormalized scores) produced by f and T is the single learned (scalar) parameter. Both models are typically trained with a binary cross-entropy loss, where the labels 0 and 1 denote whether an instance is correctly classified."}, {"title": "4.4 Inference", "content": "Once we have trained $\\hat{g}$ and h, we can flexibly account for \u03b2 by selecting a threshold \u03c4 on unlabeled test data (or some other representative tuning set) such that $\\mathbb{E}[1{\\{\\hat{g}(X) \\geq \\tau\\}}] = \\beta$. The model g is then simply $g(x) := 1{\\{\\hat{g}(x) \\geq \\tau\\}}$."}, {"title": "4.4.1 High Probability Coverage Guarantees", "content": "Since $1{\\{\\hat{g}(x) \\geq \\tau\\}}$ is a random variable with a Bernoulli distribution, we may also apply the Hoeffding bound (Hoeffding, 1963) to guarantee that with high probability empirical target coverage \u03b2 (the proportion of the target distribution where $\\hat{g}(x) \\geq \\tau$) will be in some range. Given a set V of $n_V$ i.i.d. unlabeled examples from the target distribution, we denote empirical coverage on V as $\\hat{\\beta}$. With probability at least 1 \u2013 \u03b4, $\\hat{\\beta}$ will be in the range $[\\beta - \\epsilon, \\beta + \\epsilon]$, where $\\epsilon = \\sqrt{\\frac{\\sigma^2}{2 n_V} \\log(\\frac{2}{\u03b4})}$. For some critical coverage level \u03b2, \u03c4 can be decreased until $\\hat{\\beta} - \\epsilon \\leq \\beta$."}, {"title": "5 Experiments", "content": "In this section we examine the performance of selective recalibration and baselines when applied to models pre-trained on real-world datasets and applied to a target distribution possibly shifted from the training distribution. In Section 5.1 we investigate whether, given a small validation set of labeled examples drawn i.i.d. from the target distribution, joint optimization consistently leads to a lower empirical selective calibration error than selection or recalibration alone or sequential optimization. Subsequently, in Section 5.2 we study multiple out-of-distribution prediction tasks and the ability of a single system to provide decreasing selective calibration error across a range of coverage levels when faced with a further distribution shift from validation data to test data. We also analyze the trade-off between selective calibration error and accuracy in this setting.\nSince we are introducing the objective of selective recalibration here, we focus on high-level design decisions, in particular the choice of selection and recalibration method, loss function, and optimization procedure (joint vs. sequential). For selective recalibration models, the input to g is the feature embedding. Temperature scaling is used for multi-class examples and Platt scaling is applied in the binary cases (following Guo et al. (2017) and initial results on validation data). Calibration error is measured using ECE\u2081 and ECE\u2082 with equal mass binning. For the selection loss, we use $l_{S-TLBCE}$ and $l_{S-MMCE}$ for binary tasks, and include a selective version of typical multi-class cross-entropy ($l_{S-MCE}$) for multi-class tasks. Pre-training is performed where h is optimized first in order to reasonably initialize the calibrator parameters before beginning to train g. Models are trained both with h fixed after this pre-training (denoted as \"sequential\" in results) and when it is jointly optimized throughout training (denoted as \u201cjoint\" in results).\nOur selection baselines include confidence-based rejection (\u201cConfidence\u201d) and multiple out-of-distribution (OOD) detection methods (\u201cIso. Forest\u201d, \u201cOne-class SVM\u201d), common techniques when rejecting to improve accuracy. The confidence baseline rejects examples with the smallest f(x) (or \u0125(f(x))), while the OOD methods are measured in the embedding space of the pre-trained model. All selection baselines are applied to the recalibrated model in order to make the strongest comparison. We make further comparisons to recalibration baselines, including the previously described temperature and Platt scaling as well as binning methods like histogram binning and Platt binning. See Appendix A for more experiment details including calibration error measurement and baseline implementations."}, {"title": "5.1 Selective Recalibration with i.i.d. Data", "content": "First, we test whether selective recalibration consistently produces low ECE in a setting where there is a validation set of labeled training data available from the same distribution as test data using outputs of pretrained models on the Camelyon17 and ImageNet datasets. Camelyon17 (Bandi et al., 2018) is a task where the input x is a 96x96 patch of a whole-slide image of a lymph node section from a patient with potentially metastatic breast cancer and the label y is whether the patch contains a tumor. Selection and recalibration models are trained with 1000 samples, and we apply a Platt scaling h since the task is binary. ImageNet is a well-known large scale image classification dataset, where we use 2000 samples from a supervised ResNet34 model for training selection and recalibration models, and temperature scaling h since the task is multi-class. Our soft selector \u011d is a shallow fully-connected network (2 hidden layers with dimension 128), and we report selective calibration error for coverage level \u03b2\u2208 {0.75,0.8,0.85,0.9}. Full experiment details, including model specifications and training parameters, can be found in Appendix A."}, {"title": "5.2 Selective Re-Calibration under Distribution Shift", "content": "In this experiment, we study the various methods applied to genetic perturbation classification with RxRx1, as well as zero-shot image classification with CLIP and CIFAR-100-C. RxRx1 (Taylor et al., 2019) is a task where the input x is a 3-channel image of cells obtained by fluorescent microscopy, the label y indicates which of 1,139 genetic treatments (including no treatment) the cells received, and there is a domain shift across the batch in which the imaging experiment was run. CIFAR-100 is a well-known image classification dataset, and we perform zero-shot image classification with CLIP. We follow the setting of Fisch et al. (2022) where the test data is drawn from a shifted distribution with respect to the validation set and the goal is not to target a specific \u03b2, but rather to train a selector that works across a range of coverage levels. In the case of RxRx1 the strong batch processing effect leads to a 9% difference in pretrained model accuracy between validation (18%) and test (27%) output, and we also apply a selective recalibration model trained on validation output from zero-shot CLIP and CIFAR-100 to test examples drawn from the OOD CIFAR-100-C test set. Our validation sets have 1000 (RxRx1) or 2000 (CIFAR-100) examples, \u011d is a small network with 1 hidden layer of dimension 64, and we set \u03b2 = 0.5 when training the models. For our results we report the area under the curve (AUC) for the coverage vs. error curve, a typical metric in selective classification (Geifman & El-Yaniv, 2017; Fisch et al., 2022) that reflects how a model can reduce the error on average at different levels of \u03b2. We measure AUC in the range \u03b2 = [0.5, 1.0], with measurements taken at intervals of 0.05 (i.e., \u03b2\u2208 [0.5, 0.55, 0.6, ..., 0.95, 1.0]). Additionally, to induce robustness to the distribution shift we noise the selector/recalibrator input. See Appendix A for full specifications."}, {"title": "5.2.1 Trade-offs Between Calibration Error and Accuracy", "content": "While accurate probabilistic output is the only concern in some domains and should be of at least some concern in most domains, discrete label accuracy can also be important in some circumstances. Table 1 shows accuracy results under selection, and Figure 3 shows the selective accuracy curve and confidence histogram for our selective recalibration model trained with S-TLBCE for RxRx1 and CIFAR-100 (and applied to shifted distributions). Together, these results illustrate that under different data and prediction distributions, selective recalibration may increase or decrease accuracy. For RxRx1, the model tends to reject examples with higher confidence, which also tend to be more accurate. Thus, while ECE@\u03b2 may improve with respect to the full dataset, selective accuracy at \u03b2 is worse. On the other hand, for CIFAR-100-C, the model tends to reject examples with lower confidence, which also tend to be less accurate. Accordingly, both ECE@\u03b2 and selective accuracy at \u03b2 improve with respect to the full dataset."}, {"title": "6 Theoretical Analysis", "content": "To build a deeper understanding of selective recalibration (and its alternatives), we consider a theoretical situation where a pre-trained model is applied to a target distribution different from the distribution on which it was trained, mirroring both our experimental setting and a common challenge in real-world deployments. We show that with either selection or recalibration alone there will still be calibration error, while selective recalibration can achieve ECE = 0. We also show that joint optimization of g and h, as opposed to sequentially optimizing each model, is necessary to achieve zero calibration error."}, {"title": "6.1 Setup", "content": "We consider a setting with two classes, and without loss of generality we set y \u2208 {-1,1}. We are given a classifier pre-trained on a mixture model, a typical way to view the distribution of objects in images (Zhu et al., 2014; Thulasidasan et al., 2019). The pre-trained classifier is then applied to a target distribution containing a portion of outliers from each class unseen during training. Our specific choices of classifier and training and target distributions are chosen for ease of interpretation and analysis; however, the intuitions built can be applied more generally, for example to neural network classifiers, which are too complex for such analysis but are often faced with outlier data on which calibration is poor (Ovadia et al., 2019)."}, {"title": "6.1.1 Data Generation Model", "content": "Definition 1 (Target Distribution) The target distribution is defined as a $(\\theta^*, \\sigma, \\alpha)$-perturbed mixture model over (x,y) \u2208 \u211dP \u00d7 {1,\u22121}: x | y,z \u223c zJ\u2081 + (1 \u2212 z)J\u2082, where y follows the Bernoulli distribution P(y = 1) = P(y = \u22121) = 1/2, z follows a Bernoulli distribution P(z = 1) = \u03b2, and z is independent of y.\nOur data model considers a mixture of two distributions with disjoint and bounded supports, J\u2081 and J\u2082, where J\u2082 is considered to be an outlier distribution. Specifically, for y \u2208 {\u22121,1}, J\u2081 is supported in the balls with centers $y \\theta^*$ and radius r1, J\u2082 is supported in the balls with centers $-y \\alpha \\theta^*$ and radius r2, and both J\u2081 and J\u2082 have standard deviation \u03c3. See Figure 4 for an illustration of our data models, and Appendix C.1 for a full definition of the distribution."}, {"title": "6.1.2 Classifier Algorithm", "content": "Recall that in our setting f is a pre-trained model, where the training distribution is unknown and we only have samples from some different target distribution. We follow this setting in our theory by considering a (possibly biased) estimator $\\hat{\\theta}$ of $\\theta^*$, which is the output of a training algorithm $A(S_{tr})$ that takes the i.i.d. training data set $S_{tr} = \\{(x_i^{tr}, y_i^{tr})\\}_{i=1}^m$ as input. The distribution from which $S_{tr}$ is drawn is different from the target distribution from which we have data to train the selection and recalibration models. We only impose one assumption on the model $\\hat{\\theta}$: that A outputs a $\\hat{\\theta}$ that will converge to $\\theta_0$ if training data is abundant enough and $\\theta_0$ should be aligned with $\\theta^*$ with respect to direction (see Assumption 3, Appendix C.2 for formal statement). For ease of analysis and explanation, we consider a simple classifier defined by $\\hat{\\theta} = \\frac{1}{m}\\sum x^t y^t /m$ when the training distribution is set to be an unperturbed Gaussian mixture $x^{tr}|y^{tr} \\sim N(y^{tr} \\cdot \\theta^*, \\sigma^2 I)$ and $y^{tr}$ follows a Bernoulli distribution P(ytr = 1) = 1/2.1 This form of $\\hat{\\theta}$ is closely related to Fisher's rule in linear discriminant analysis for Gaussian mixtures (see Appendix C.2.1 for further discussion).\nHaving obtained $\\hat{\\theta}$, our pretrained classifier aligns with the typical notion of a softmax response in neural networks. We first obtain the confidence vector f(x) = ($f_1(x), f_{-1}(x)$), where\n$f_1(x) = \\frac{e^{\\frac{2 \\hat{\\theta}^T x}{\\sigma^2}}}{e^{\\frac{2 \\hat{\\theta}^T x}{\\sigma^2}} + 1}, \\quad f_{-1}(x) = \\frac{1}{e^{\\frac{2 \\hat{\\theta}^T x}{\\sigma^2}} + 1}$ (13)\nand then output $\\hat{y} = arg \\max_{k \\in \\{-1,1\\}} f_k(x)$. For $k \\in \\{-1,1\\}$, the confidence score $f_k(x)$ represents an estimator of P(y = k|x) and the final classifier is equivalent to $\\hat{y} = sgn(\\hat{\\theta}^T x)$."}, {"title": "6.2 Main Theoretical Results", "content": "Having established our data and classification models, we now analyze why selective recalibration (i.e., joint training of g and h) can outperform recalibration and selection performed alone or sequentially. To measure calibration error, we consider ECEq with q = 1 (and drop the subscript q for notational simplicity below). For the clarity of theorem statements and proofs, we will restate definitions of calibration error to make them explicitly dependent on selection model g and temperature T and tailored for the binary case we are studying. We want to emphasize that we are not introducing new concepts, but instead offering different surface forms of the same quantities introduced earlier. First, we notice that under our data generating model and pretrained classifier, ECE can be expressed as\n$ECE = \\mathbb{E}_{ \\hat{\\theta}^T x} \\bigg| \\mathbb{P}[ y = 1 | \\hat{\\theta}^T x = v] - \\frac{1}{1 + e^{-\\frac{2 \\hat{\\theta}^T x}{\\sigma^2}}} \\bigg|$\nBy studying such population quantities, our analysis is not dependent on any binning-methods that are commonly used in empirically calculating expected calibration errors."}, {"title": "6.2.1 Selective Recalibration v.s. Recalibration or Selection", "content": "We study the following ECE quantities according to our data model for recalibration alone (R-ECE), selection alone (S-ECE), and selective recalibration (SR-ECE). For recalibration, we focus on studying the popular temperature scaling model, although the analysis is nearly identical for Platt scaling.\nR-ECE(T) := $E_{y=1} \\bigg| \\mathbb{P}[y=1| \\hat{\\theta}^T x] - \\frac{1}{1+e^{\\frac{-2 \\hat{\\theta}^T x}{\\sigma^2}}} \\bigg|$\nS-ECE(g) := $E_{\\hat{\\theta}^T x} \\bigg| \\mathbb{P}[y=1| \\hat{\\theta}^T x, g(x) = 1] - \\frac{1}{1+e^{\\frac{-2 \\hat{\\theta}^T x}{\\sigma^2}}} \\bigg| g(x)=1$\nSR-ECE(g,T) := $E_{\\hat{\\theta}^T x} \\bigg| \\mathbb{P}[y=1| \\hat{\\theta}^T x, g(x) = 1] - \\frac{1}{1+e^{\\frac{-2 \\hat{\\theta}^T x}{\\sigma^2}}} \\bigg| g(x)=1$\nOur first theorem proves that under our data generation model, S-ECE and R-ECE can never reach 0, but SR-ECE can reach 0 by choosing appropriate g and T.\nTheorem 1 Under Assumption 3, for any \u03b4\u2208 (0,1) and $\\hat{\\theta}$ output by $\\cal{A}$, there exist thresholds M \u2208 N+ and \u03c4 > 0 such that if $max\\{r1, r2, \u03c3, ||\u03b8\u2217||\\} < \u03c4$ and $m > M$, there exists a positive lower bound L, with probability at least 1 \u2013 \u03b4 over $S_{tr}$\n$\\mathop{\\min}_{g: E[g(x)]\u2265\u03b2} \\bigg\\{ \\mathop{\\min}_g S-ECE(g), \\mathop{\\min}_{T\u2208\\cal{R}} R-ECE(T)\\bigg\\} > L.$\nHowever, there exists T0 and g0 satisfying E[g0(x)] \u2265 \u03b2, such that SR-ECE(g0, T0) = 0.\nIntuition and interpretation. Here we give some intuition for understanding our results. Under our perturbed mixture model, R-ECE is calculated as\nR-ECE(T) = E$\\frac{ \\hat{\\theta}^T x}{\u03c3^2}\\\n(\\approx \u03b8\u2217)$\\bigg\\| \\frac{1\\{v \u2208 A\\}}{1 + \\exp(-2\\frac{ \\hat{\\theta}^T x}{\u03c3^2}))} + \\frac{1\\{v \u2208 B\\}}{1 + \\exp(2\u03b1\\frac{ \\hat{\\theta}^T x}{\u03c3^2}))} - \\frac{1}{e^{-2v/T} +1 }\\bigg\\|\nfor disjoint sets A and B, which correspond to points on the support of J\u2081 and J\u2082 respectively. In order to achieve zero R-ECE, when v \u2208 A, we need T = $\u03b8 \\theta^*/(\u03c3^2||\u03b8||^2)$. However, for v \u2208 B we need T = $\u2212\u03b1\u03b8 \\theta^*/(\u03c3^2||\u03b8||^2)$. These clearly cannot be achieved simultaneously. Thus the presence of the outlier data makes it impossible for the recalibration model to properly calibrate the confidence for the whole population. A similar expression can be obtained for S-ECE. As long as $\u03b8 \\theta^*/(\u03c3^2||\u03b8||^2)$ and $-\u03b1\u03b8 \\theta^*/(\u03c3^2||\u03b8||^2)$ are far from 1 (i.e., miscalibration exists), no choice of g can reach zero S-ECE. In other words, no selection rule alone can lead to calibrated predictions, since no subset of the data is calibrated under the pre-trained classifier. However, by setting g0(x) = 0 for all x \u2208 B and g0(x) = 1 otherwise, and choosing T0 = $\u03b8 \\theta^*/(\u03c3^2||\u03b8||^2)$, SR-ECE = 0. Thus we can conclude that achieving ECE = 0 on eligible predictions is only possible under selective recalibration, while selection or recalibration alone induce positive ECE. See Appendix C for more details and analysis."}]}