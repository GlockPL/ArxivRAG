{"title": "MSWA: Refining Local Attention with Multi-Scale Window Attention", "authors": ["Yixing Xu", "Shivank Nag", "Dong Li", "Lu Tian", "Emad Barsoum"], "abstract": "Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.", "sections": [{"title": "1 Introduction", "content": "The popularity of Transformer-based (Vaswani et al., 2017) large language models (LLMs) (Tou- vron et al., 2023; Achiam et al., 2023) has surged due to their remarkable performance on a wide range of applications, including NLP tasks like machine translation (Zhang et al., 2023a), text sum- marization (Zhang et al., 2024), as well as more complex uses such as coding assistance (Ross et al., 2023) and communicative agents (Li et al., 2023). However, the standard Transformer employs the self-attention mechanism, whose quadratic time complexity becomes a bottleneck for the model's computational efficiency. Moreover, the KV cache required by the self-attention mechanism during inference autoregressively increases GPU memory consumption, making the deployment of LLMs unfriendly.\nRecently, a lot of architectures have been pro- posed with the aim of being efficient foundations for LLMs (Gu and Dao, 2023; Peng et al., 2023; Poli et al., 2023). One strand of research focuses on improving the efficiency of attention mechanism, such as sparse attention (Child et al., 2019), slid- ing window attention (Beltagy et al., 2020; Zaheer et al., 2020), and linear attention (Choromanski et al., 2020; Yang et al., 2023). While most studies focus on capturing global information of text se- quence with linear computational time and limited memory, sliding window attention (SWA) offers a more intuitive approach. By focusing on local information, it serves as a valuable mechanism for building LLMs (Jiang et al., 2023) or creating novel architectures (De et al., 2024; Arora et al., 2024).\nThe key idea of SWA is to utilize the locality of reference (Zaheer et al., 2020) in NLP data, where most information about a token can be derived from its neighboring tokens. By allowing each token to attend to its neighbors within a fixed-size local win- dow, SWA ensures linear computational complex- ity and constant KV cache consumption. However, each head in every layer of the original SWA shares the same window size, ignoring the fact that the scale of contextual information can vary signifi- cantly. For instance, a news report can span up to 2000 tokens, while a keyword might consist of only 4 tokens. Setting the attention window to the same size might lead to sub-optimal adaptation to contexts of different scales. Additionally, different components of a Transformer model serve different roles. For example, shallower layers may exhibit more locality (Child et al., 2019). Restricting all components to the same receptive field can severely impair the model's representation capacity.\nTo address the aforementioned issues, we pro- pose a novel window attention variant called Multi- Scale Window Attention (MSWA), which intro- duces diverse window sizes across heads and layers and improves the performance of SWA while re-"}, {"title": "2 Related Works", "content": "In this section, we briefly introduce the studies of large language models and attention mechanisms."}, {"title": "2.1 Large Language Models (LLMs)", "content": "Language models (Bengio et al., 2000) have be- come a cornerstone of modern Natural Language Processing (NLP). Their primary purpose is to un- derstand and generate human language, making them crucial for applications ranging from machine translation (Zhang et al., 2023a) to communicative agents (Li et al., 2023). The advent of large-scale pre-trained models has significantly enhanced the performance of these applications.\nAmong them, Transformer-based models have revolutionized the field. Introduced by Vaswani et al. (2017), the Transformer architecture uses self- attention mechanism to process input sequences in a more parallelizable way. This innovation has led to the development of increasingly large and powerful models, such as GPT-4 (Achiam et al., 2023), Llama-3 (AI@Meta, 2024), and Claude-3 (Anthropic, 2024). These models, often termed \"large language models\", leverage vast amounts of data and computational resources to achieve state- of-the-art results on a wide array of NLP tasks."}, {"title": "2.2 Attention Mechanisms", "content": "The attention mechanism which enables the model to capture intricate dependencies across the entire sequence is at the heart of the Transformer's suc- cess. However, the standard self-attention mecha- nism has quadratic complexity with respect to the sequence length, which poses scalability challenges for longer sequences. To address this issue, many efficient attention variants have been proposed (Qiu et al., 2019; Wang et al., 2020; Peng et al., 2021; Hua et al., 2022). For example, sliding window"}, {"title": "3 Preliminaries", "content": "In this section, we briefly introduce the preliminar- ies about self-attention, sliding window attention and linear attention operations."}, {"title": "3.1 Self-Attention Mechanism", "content": "For the Transformer models, its main computa- tional and memory costs arise from the multi-head self-attention mechanism, focusing on two points: (1) quadratic time complexity over the input length, and (2) linearly increased size of the KV cache during inference. In the following, we analyze the attention mechanism based on the decoder form, as it is widely used in language models.\nGiven input vectors $\\{x_i\\}_{i=1}^n \\in \\mathbb{R}^D$, where each $x_i$ represents a single input token, n is the sequence length, and D is the dimension of the input vectors, a head in the self-attention layer first maps the input tokens into query vectors $\\{q_i\\}_{i=1}^n$, key vectors $\\{k_i\\}_{i=1}^n$, and value vectors $\\{v_i\\}_{i=1}^n$:\n$q_i = x_iW_q, k_i = x_iW_k, v_i = x_i W_v$. (1)\nwhere $W_q, W_k, W_v \\in \\mathbb{R}^{D\\times d}$ are the mapping matrices, and d is the dimension of each head. The output of this attention head is calculated by:\n$\\alpha_{ij} = \\frac{\\exp(q_i k_j^T/\\sqrt{d})}{\\sum_{k=0}^n \\exp(q_i k_k^T/\\sqrt{d})}$. (2)\n$o_i = \\sum_{j=0}^n \\alpha_{ij} v_j$. (3)\nPerforming the above computation for the entire sequence of length n requires a time complexity of O(dn\u00b2) and a space complexity of O(dn)."}, {"title": "3.2 Sliding Window Attention", "content": "Sliding Window Attention (SWA) is an efficient variant that restricts each token to attend to tokens within a local window of size w, as shown in Fig. 1.\nGiven the query, key and value vectors $q_i, k_i, v_i$, the output of the SWA head is defined as:\n$\\alpha_{ij} = \\frac{\\exp(q_i k_j^T/\\sqrt{d})}{\\sum_{k=max(0,i-w)}^i \\exp(q_i k_k^T/\\sqrt{d})}$. (4)\n$o_i = \\sum_{j=max(0,i-w)}^i \\alpha_{ij} v_j$. (5)\nBy using this method, the time and space complexity required for each head is reduced to O(dnw) and O(dw), respectively. Considering a Transformer with I layers, each equipped with h attention heads, the time and space complexity will become O(dn(whl)) and O(d(whl)) respectively. We can see that the computational and memory cost is proportional to whl, which is the summa- tion of the window sizes of all the sliding window attention operations from all heads in all layers."}, {"title": "3.3 Linear Attention Mechanism", "content": "Linear attention replaces the softmax operation in standard attention with a feature map-based dot product, eliminating the computation of exp(\u00b7) and exchanging the matrix multiplication order, thereby achieving the goal of acceleration and constant memory cost. Specifically, given a kernel function $\\phi(\\cdot)$ that maps the $q_i$ and $k_j$ vectors into features, linear attention approximates $\\exp(q_i k_j^T/\\sqrt{d})$ with the dot-product $\\phi(q_i)\\phi(k_j)^T$. Therefore, the out- put of an attention head is calculated by:\n$\\alpha_{ij} = \\frac{\\phi(q_i)\\phi(k_j)^T}{\\sum_{k=0}^i \\phi(q_i)\\phi(k_j)^T}$. (6)\n$o_i = \\sum_{j=0}^i \\alpha_{ij} v_j = \\frac{\\phi(q_i) \\sum_{j=0}^i \\phi(k_j) v_j}{\\phi(q_i) \\sum_{j=0}^i \\phi(k_j)^T}$. (7)\nBased on the exchange of multiplication order, the computational time complexity of linear at- tention is reduced to O(d\u00b2n). In addition, dur- ing the auto-regressive inference process, both $\\sum_{j=0}^i \\phi(k_j) v_j$ and $\\sum_{j=0}^i \\phi(k_j)$ can be written as a variable that is continuously accumulated, requir- ing only O(d\u00b2) space complexity."}, {"title": "4 Multi-Scale Window Attention", "content": "In this section, we present our proposed Multi- Scale Window Attention (MSWA) mechanism, which leverages diverse window sizes across differ- ent heads and layers in Transformer architecture,"}, {"title": "4.1 Diverse Window Across Heads", "content": "This section focuses on dynamically changing the window size for each attention head within a layer. We refer to this mechanism as MSWA-h, as shown in the bottom right part of Fig. 1.\nDifferent from SWA where all heads use the same window size $w_i$ in the i-th layer, in MSWA- h different heads have different scales of window sizes, and the summation of the total window sizes within a layer is less than that in the SWA, which is $w_i h$. Specifically, inspired by many hierarchical ar- chitecture designs in the CV field (Liu et al., 2021), we divide the attention heads into four groups and adjust the receptive field range with a 2\u00d7 change between each group, resulting in window sizes of $\\frac{w_i}{4}, \\frac{w_i}{2}, w_i, 2w_i$, respectively. Therefore, the sum- mation of the total window sizes is:\n$(\\frac{w_i}{4} + \\frac{w_i}{2} + w_i + 2w_i) \\times \\frac{h}{4} = \\frac{15}{16} w_i h$. (8)\nLeveraging diverse window sizes among the heads within a layer allows the Transformer model to capture the relevant context of different scales simultaneously. This is because the outputs of dif- ferent heads within an attention layer are concate- nated together and then mapped through a matrix to form the final output of the layer, which allows contextual information at different distances to be integrated together. Additionally, considering the allocation of attention resources: all heads will at- tend to tokens within a distance of $\\frac{w_i}{4}$ from the current token, while $\\frac{h}{4}$ of the heads will attend to tokens within the $\\frac{w_i}{4}$ to $\\frac{w_i}{2}$ range, and so on. This"}, {"title": "4.2 Diverse Window Across Layers", "content": "This section further introduces changing the allo- cation ratio of the attention window sizes between layers. We refer to this mechanism as MSWA-1, as illustrated in the upper right part of Fig. 1.\nTo explain more clearly, we still use SWA as a comparison. In SWA, each attention layer has a total window size allocation of hw, where h is the number of heads per layer, and w is the base window size, which means that for any layer index i, $w_i = w$. In MSWA-1, the window size allocation varies across layers. More specifically, we divide all attention layers into several groups, and from shallow to deep, we continuously increase the total window size allocated to the attention layers in each group. We adopt a similar setup to MSWA-h, with four groups having a 2\u00d7 change between each group, resulting in window size allocations of $\\frac{hw}{4}, \\frac{hw}{2}, hw$, and 2hw, respectively. The total window size resource allocated to all layers in MSWA-1 is:\n$(\\frac{hw}{4} + \\frac{hw}{2} + hw + 2hw) \\times \\frac{l}{4} = \\frac{15}{16} whl$. (9)\nFor an attention layer, a larger window size allo- cation means that the window sizes of the heads in that layer are generally larger, allowing for the per- ception of a broader range of context. Therefore, gradually increasing the window size allocation from shallow to deep layers enables the model to focus on building local fine-grained information in the initial stages and progressively enhance the capture of long-distance relationships in the later stages. Additionally, the gradually expanding at- tention window allocation enables the model to continuously integrate local information from pre- vious stages based on a larger receptive field."}, {"title": "4.3 Integrate Diversity of Heads and Layers", "content": "In this section, we aim to integrate the two strate- gies MSWA-h and MSWA-1 introduced earlier to construct the final MSWA mechanism.\nThe description of MSWA starts with a base win- dow size w. In SWA, w is the window size used for all heads across all layers. In contrast, in MSWA, w serves as the basis for window size variation. We denote the base size value of the i-th layer as $w_i$ and the actual window size of the j-th head in"}, {"title": "4.4 Combination with Linear Attention", "content": "This section further proposes combining MSWA with an efficient global attention mechanism, i.e., linear attention, as illustrated in Fig. 2.\nAs we introduced earlier, many efficient mecha- nisms focus on capturing global information with limited resources. Therefore, they often fail to allo- cate high importance to relevant local information. Linear attention is a typical example of this issue. As introduced in Sec. 3.3 it stores global sequence information in fixed-size variables $\\sum_{j=0}^i \\phi(k_j)Tv_j$"}, {"title": "5 Experiments", "content": "This section demonstrates the effectiveness of our MSWA mechanism. An overview of experiments, the main datasets and the baselines are described below. More experimental details, including the im- plementation dependencies and the detailed setup for each experiment, are shown in the Appendix B."}, {"title": "5.1 Language Modeling Evaluation", "content": "In this section, we evaluate the language model- ing capabilities of MSWA mechanism by training"}, {"title": "5.1.1 Direct Construction of Language Model", "content": "This section presents the results of directly using MSWA as the Transformer backbone, which is a straightforward way to validate its performance.\nAs shown in Tab. 2, we report perplexity (PPL) results on the Wikitext-103 test set and bits-per- character (bpc) results on the enwik8 test set. The experimental results demonstrate that: 1) MSWA achieves better language modeling performance compared to SWA with smaller computational and memory cost, reducing PPL by 1.14 on Wikitext- 103 and bpc by 0.11 on enwik8. 2) Dynamically ad- justing the window size from either the layer or the head perspective results in improved language mod- eling capability, and combining both approaches yields further enhancements. 3) Although there is still a performance gap between local attention and the standard self-attention, MSWA can achieve closer or similar results to standard attention. For example, on enwik8, MSWA obtains a bpc that is 0.01 lower compared to standard attention with a sequence length of 1,024 and 0.01 higher compared"}, {"title": "5.1.2 Combination with Linear Attention", "content": "This section demonstrates the combination of MSWA and Linear Attention mechanism, achiev- ing language modeling capabilities comparable to the standard Transformer in a more efficient way. We use the 2nd-order Taylor series feature map (Zhang et al., 2023b; Arora et al., 2024) as the kernel function for linear attention.\nThe experimental results are shown in Tab. 3. We can conclude that: 1) Combining MSWA with linear attention achieves comparable perfor- mance to the standard Transformer. Specifically, on Wikitext-103 the combined model achieves a PPL that is only 1.4 higher than the Transformer, while on enwik-8, it achieves a 0.2 lower bpc compared to the Transformer. 2) The performance of linear attention is greatly improved when combined with either SWA or MSWA. Among them, combining with MSWA yields better language modeling per- formance, providing direction for future researches. 3) Compared to directly using MSWA, combining MSWA with linear attention achieves a balance between performance and efficiency, enhancing ef- ficiency with minimal loss in performance."}, {"title": "5.2 Evaluation on Downstream Tasks", "content": "In this section, we evaluate the performance of Llama-7B after fine-tuning with local attention pat- terns and testing on downstream common-sense reasoning tasks. The purpose of this evaluation is to verify the compatibility of MSWA with the current pre-trained LLM and its effectiveness when scaled to a large number of model parameters.\nTab. 4 presents the accuracy results of the mod- els on various downstream benchmarks using 3- shot and 5-shot settings. The experimental results indicate that: 1) The MSWA mechanism demon- strates better common-sense reasoning ability com-"}, {"title": "5.3 Computational Efficiency Evaluation", "content": "This section further evaluates the actual computa- tional efficiency of our MSWA mechanism. Un- like the inferable size of the KV cache, the com- putational speed of Transformer models needs to be measured in practice to obtain realistic results. Therefore, we measure the time required to predict the next token during forward propagation in infer- ence process for various attention mechanisms. We utilize FlashAttention (Dao, 2023) for the compu- tation of various attention mechanisms.\nThe computational efficiency is shown in Fig. 3. From the experimental results, we can observe the following: 1) Both SWA and MSWA has a signifi- cant efficiency advantage compared to the standard self-attention mechanism. This advantage becomes more pronounced as the batch size increases. 2) Compared to the traditional SWA, MSWA adapts better to larger batch sizes, often achieving better performance as the batch size increases. 3) For larger base window sizes, the efficiency advantage of MSWA becomes even more apparent, making it well-suited for scaling up context lengths."}, {"title": "5.4 Ablation Study", "content": "We conduct a series of ablation studies in this sec- tion, mainly focusing on the impact of the base window size on the MSWA mechanism, as well as the effects of other window variation strategies on the MSWA mechanism. In these experiments, we use the same setup from Sec. 5.1.1, only alter the sizes of windows and the variation method."}, {"title": "5.4.1 Effect of Base Window Size", "content": "The impact of the base window size on the MSWA mechanism is shown in Tab. 5, where for each w, the window size variation method for MSWA is introduced as in Sec. 4.3. It can be observed that: 1) In each case from w = 64 to w = 512, MSWA achieves better results compared to traditional local attention SWA. 2) MSWA can achieve better per- formance than traditional local attention with less than half the resource consumption. For example, SWA with a window size of 512 achieves a PPL of 29.20 on Wikitext-103, while MSWA evolved from a base window size 256 achieves a PPL of 28.92."}, {"title": "5.4.2 Effect of Window Variation Strategy", "content": "The comparative results with other window varia- tion strategies are shown in Tab. 6. We consider two approaches. In the first approach, to demon- strate the effectiveness of our layer-wise alloca- tion, where lower layers model local information and higher layers capture long-range information, we reverse the original window size allocation be- tween layers, which means reducing the window size from shallow to deep layers. In the second approach we change the window size variation be- tween each group from multiplying by 2 each time to an arithmetic progression. For example, for the base window size of 128, we change the evolution of each group to {64, 96, 128, 160}. The experi- mental results demenstrate that the performance achieved by both variation strategies is slightly weaker than the method we introduced previously."}, {"title": "6 Conclusion", "content": "We propose a novel window attention variant called Multi-Scale Window Attention (MSWA), which leverages diverse window sizes for different heads in different layers. Compared to the traditional sliding window attention, which is inefficient in capturing context of varying scales, we enable the model to capture contextual information of varying lengths and distances with less computational re- sources and memory usage. Experimental results on lanaguage modeling and common-sense reason- ing tasks demonstrate that MSWA can outperform previous local attention mechanism, while obtain- ing better efficiency."}, {"title": "A Implementation of MSWA", "content": "Here, we provide the specific implementation of MSWA mechanism. Essentially, MSWA is a com- bination of two sub-mechanisms: MSWA-h and MSWA-1. Their implementations in the program are independent of each other."}, {"title": "A.1 Implementation of MSWA-h", "content": "As for the implementation of MSWA-h, the overall flow is consistent with the standard attention layer in Transformer, except for the grouped implemen- tation of the multi-head attention. Since different groups use different window sizes, we first use the reshape function from PyTorch (Paszke et al., 2019) to divide all head's q, k, and v vectors into different groups. For heads within the same group, we use the efficient methods of previous SWA im- plementations (e.g. FlashAttention (Dao, 2023), xFormers (Lefaudeux et al., 2022)) for parallel com- putation. Calculations between different groups are carried out separately. After completing the atten- tion calculation of all groups, we use cat function of PyTorch to concatenating the attention outputs of each group together in the group's dimension, and then project them onto the final output of this attention layer through a matrix. Therefore, we can implement the MSWA-h mechanism without much additional development."}, {"title": "A.2 Implementation of MSWA-I", "content": "Regarding the implementation of MSWA-1, it is simpler compared to MSWA-h because in the Transformer model, different layers are stacked, and the computations between different layers are sequential and completely independent. We only need to assign the window size allocation for each layer as a parameter and pass it to the initialization function of each layer object."}, {"title": "B Experimental Details", "content": "In this section, we introduce the setups and training details for each experiment."}, {"title": "B.1 Dependencies", "content": "For all the methods in our experiments, we imple- ment them using the PyTorch (Paszke et al., 2019) and FlashAttention (Dao, 2023) library. Addition- ally, the training and test process for the experi- ments in Sec. 5.1 is based on Fairseq (Ott et al., 2019), while for the experiments in Sec. 5.2, the fine-tuning process is based on DeepSpeed (Rasley et al., 2020) and the evaluation process is imple- mented using Im-evaluation-harness (Gao et al., 2023). The efficiency evaluation in Sec. 5.3 is per- formed on a NVIDIA A100 GPU."}, {"title": "B.2 Setups for Each Experiment", "content": "In this section, we introduce the setups and training details for each experiment."}, {"title": "B.2.1 Language Modeling Evaluation", "content": "Direct Construction of Language Model For all attention mechanisms, we apply them to a 12- layer standard Transformer model, with each layer having 8 attention heads. The model dimension and head dimension are 512 and 64, respectively. To better simulate the model's operation on the long- range sequence and reflect the memory overhead of various mechanisms, we introduce the cache mech- anism from Transformer-XL (Dai et al., 2019) and simultaneously adopt its relative position embed- ding. Each model is trained from scratch on two datasets based on the casual language modeling objective for 150,000 update steps. The number of tokens trained per step, which is the product of batch size and sequence length, is kept consis- tent (16,384 for Wikitext-103, 49,152 for enwik8). We use the AdamW (Loshchilov and Hutter, 2018) optimizer with beta values of (0.9, 0.98), set the learning rate to 2e-4 with 1,000 warm-up steps, and use a cosine learning rate scheduler.\nCombination with Linear Attention For the lin- ear attention mechanism, we use the 2nd-order Tay- lor series feature map (Zhang et al., 2023b; Arora et al., 2024) as the kernel function. Following the setup by Arora et al. (2024), we employ RoPE (Su et al., 2021) encoding for both linear attention and local attention, and use RMSNorm and SwiGLU mechanism. Each model consists of 12 layers. For combination of linear attention and local attention, each consecutive stack of three layers containing one linear attention layer and two local attention layers. The model dimension, head dimension, and feature dimension are set to 512, 64, and 16, respec- tively. The base window size for local attention is 128. During training and evaluation, the data is segmented into sequences containing 2,048 tokens without using the Transformer-XL style caching mechanism. The batch size for both datasets is 8. Other training settings are the same as in Sec. 5.1.1."}, {"title": "B.2.2 Evaluation on Downstream Tasks", "content": "For the fine-tuning process, the Llama-7B model is trained for 2,000 steps using each local attention"}, {"title": "B.2.3 Computational Efficiency Evaluation", "content": "For all attention mechanisms, their computation is based on the FlashAttention library, which is the current standard method for efficiently implement- ing attention operation. We apply them in a 32- layer Transformer model, with each layer contain- ing 16 attention heads. The model dimension and head dimension are 1,024 and 64, respectively. We use a sequence of 2,048 tokens for measurement and report the median value across the computation time at positions {500, 1,000, 1,500, 2,000} in the sequence. The batch size is set to {8, 16, 64, 128, 256, 512}, and we record the experimental results for each case."}]}