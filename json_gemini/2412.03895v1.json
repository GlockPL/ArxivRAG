{"title": "A Noise is Worth Diffusion Guidance", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Jaewon Min", "Minjae Kim", "Wooseok Jang", "Hyoungwon Cho", "Sayak Paul", "SeonHwa Kim", "Eunju Cha", "Kyong Hwan Jin", "Seungryong Kim"], "abstract": "Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to 'guidance-free noise', we uncover that small low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose NoiseRefine, a novel method that replaces guidance methods with a single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noise-space learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how refined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/.", "sections": [{"title": "1. Introduction", "content": "In recent years, Text-to-Image (T2I) diffusion models [3, 6, 32, 36, 39], which generate images conditioned on text prompts, have achieved remarkable advancements. However, their ability to produce high-quality samples largely relies on guidance techniques, such as classifier-free guidance (CFG) [13] and its variants [1, 5, 15, 16]. These methods significantly enhance image quality during inference but double the computational cost. Despite drawbacks such as increased batch size, high guidance scale requirements, oversaturation, and reduced diversity, the dramatic performance gains make CFG the de facto standard. Recent works [4, 37, 38] aim to mitigate these limitations, but the impact of CFG on image quality makes it indispensable in most diffusion pipelines.\nThis raises a fundamental question: Can we replace the effects of guidance techniques with minimal changes to the diffusion pipeline? While some works have proposed"}, {"title": "2. Related Work", "content": "Diffusion guidance. Classifier Guidance (CG) [27] enhances fidelity by leveraging trained classifier gradients, albeit at the cost of diversity. CFG [13] models an implicit classifier to achieve similar effects. Ahn et al. [1] and Karras et al. [21] further generalize those guidance methods by intentionally generating lower-quality samples to guide the process toward improved outputs and other guidance techniques [15, 16, 38] generate 'bad' samples in various ways. While effective, these methods double computational and memory costs by requiring degraded sample generation at each step, which is essential to their operation.\nDiffusion inversion. Denoising Diffusion Implicit Models (DDIM) [46] introduced deterministic sampling, enabling inversion from image to noise. This means that by starting the sampling process from the inverted noise, we can reconstruct the original images. Although DDIM Inversion [46] is the most commonly used inversion method for diffusion models, its reliance on linear approximation often leads to noticeable artifacts in reconstructed images. Several works [8, 29, 31] employ fixed-point iteration to reduce the approximation error. If guidance is used during inversion, then guidance must be applied during sampling to achieve the same generated image and the same holds in reverse.\nNoise optimization. Optimizing or selecting noises with certain objectives has been a key research focus in diffusion models [7, 28, 41]. ReNO [7] optimizes noises based on reward models and Samuel et al. [41] proposed a bootstrap-based method to optimize initial noises for rare concept generation. However, these optimization methods require a substantial number of iterations, which poses a challenge"}, {"title": "3. Method", "content": "In this section, we first identify characteristics of mapping from the Gaussian noise space to guidance-free noise space, a space of initial noises that can be denoised into high-quality images without guidance (Sec. 3.1). Next, we introduce a method for learning this mapping from arbitrary Gaussian noise (Sec. 3.2). Finally, we demonstrate that, with a carefully designed dataset construction and filtering process, predicting guidance-free noise using a simple single-step neural network can effectively replace traditional guidance techniques [1, 13], enabling efficient and high-quality image generation without guidance (Sec. 3.3)."}, {"title": "3.1. Guidance-Free Noise Space", "content": "To obtain the guidance-free noise space, we emphasize the capability of inversion methods [8, 29, 46] to precisely reconstruct the original image without guidance. In theory, inverting an infinite set of natural images would fully capture this space, but this is infeasible. Instead, we leverage the powerful generation capabilities of text-to-image diffusion models [36], which produce high-quality images with guidance [1, 13].\nSpecifically, Gaussian noise $x_T$ is sampled from standard Gaussian distribution $\\mathcal{N}(0, I)$ and denoised into a plausible image $x_{Guide}$, using text prompt or condition $c$ with CFG [13] or any other guidance method [1, 15]. Inverting the image with an inversion method [8, 29] gives us the noise $x_{Guide}$, defined as:\n$x_{T}^{Guide} := Inversion(Denoise_{Guide} (x_T, c))$,\nwhere $Inversion(\\cdot)$ and $Denoise_{Guide}(\\cdot)$ denote inversion [8, 29] and denoising with condition $c$ and guidance, respectively. A more detailed explanation of the notations can be found in supplementary material A.1. Note that the generated image $x_{Guide}$ and inversion noise $x_{Guide}$ are conditioned on context $c$ such as the text prompt, but we omit the notation $c$ in the paper for simplicity. Now, we can map $x_T$ into $x_{Guide}$. Ideally, if the mapping is consistent or generalizable, a neural network can learn to map initial noise to guidance-free noise. This concept is illustrated in Fig. 2.\nWe investigate the structure of this mapping by generating $\\{x_T, x_{Guide}\\}$ pairs via the aforementioned process with 10K randomly selected prompts from the MS-COCO dataset [25]. We employ Stable Diffusion 1.5 [36],"}, {"title": "3.2. Learning to Map Guidance-Free Noise Space", "content": "Mitigating inversion error. A straightforward approach for learning a mapping to guidance-free noise space would be to learn the inversion noise directly. Although possible enough, inversion methods [8, 29, 46] have inherent limitations. They rely on approximations, which means true inversion noise $x^{Guide}_T$ is not guaranteed. Thus, attempting to learn this approximated inversion noise which includes inversion error may limit the performance. Hence, we try to sidestep this issue, by learning directly in the im-"}, {"title": "5. Analysis and Discussion", "content": "We analyze what noise refining model learns and identify components in refined noise that contribute to guidance-free generation, discussing the advantages of working in this space."}, {"title": "5.1. What Does noise refining model Learn?", "content": "In Fig. 7, we show that our model refines the noise by adding mostly small, low-frequency components. The distribution of absolute norm and frequency of the added noise difference is similar to the noise difference between a Gaussian noise $x_T$ and the inversion noise $x_{Guide}$ shown in Fig. 3, without explicit constraints to achieve this.\nLow-frequency components aid denoising. Fig. 9 shows that the low-frequency components in the noise difference are dependent on the condition (e.g., text prompt) and act as an initial layout for the synthesized image. These low-frequency signals significantly help diffusion models in forming object shapes in the early steps of denoising. Fig. 10 (a) shows that starting from refined noise, the model quickly establishes plausible images at much earlier stages, allowing the model to focus on adding details within the given layout during denoising. In contrast, as shown in Fig. 10 (b), the diffusion model struggles to create a coherent layout in the early denoising steps, resulting in partial details filled in incorrect locations, often leaving ambiguous regions untouched throughout the denoising process.\nDiversity and generalizability. While these initial layouts might appear to limit diversity, Fig. 9 shows that the generated layouts vary significantly depending on the initial noise. We confirm this with diversity metrics of IS [40], demonstrating greater diversity than guidance-based methods. In addition, our model generalizes beyond the training data, performing well with unseen noise and prompts (Fig. 9, Table. 1), suggesting a generalizable mapping between initial and refined noise."}, {"title": "5.2. Why Learn Noise Mapping?", "content": "We consider the rise of prompt learning. Large-scale models like CLIP [35], trained on web-scale datasets often contain up to billions of parameters. Fine-tuning such models is impractical and risks disturbing well-learned representations [52]. Instead, tuning the input prompts of the model, a method known as prompt learning, has gained popularity as an effective approach [18, 45, 51, 52]."}, {"title": "6. Conclusion", "content": "In this work, we propose NoiseRefine, an efficient and effective approach to replacing guidance in diffusion sampling with a noise refinement by a single neural network forward pass. Our noise refining model functions as a plug-and-play module based on the original diffusion model and significantly improves image fidelity. Furthermore, our method is highly efficient, which can be trained using lightweight lora, requiring only a small set of model-generated images for training and remaining feasible on consumer-grade GPUs, thanks to our proposed MSD loss. Beyond its practicality, we believe this work serves as a stepping stone toward a deeper understanding of the role of guidance and noise in diffusion models."}, {"title": "A. Theoretical Background", "content": "In the supplementary material, we clarify the notations and formulations related to diffusion models used in the main paper\nand provide the proofs for our propositions (Section A), more ablation studies regarding noise refining model (Section B),\nadditional results including qualitative results, comparison with other methods, user study (Section C), implementation details\nand experimental settings (Section D) and further discussions (Section E).\nA. Theoretical Background"}, {"title": "A.1. Preliminaries", "content": "Denoising Diffusion Probabilistic Models (DDPM). DDPM [14] defines a forward process that derives $x_t$ by adding\nGaussian noise to the image $x_{t-1}$ according to the variance schedule, and a reverse process that samples $x_{t-1}$ from $x_t$, both\nas a Markovian chain. The forward process is defined as\n$q(x_t | x_{t-1}) = \\mathcal{N} \\left(x_t; \\sqrt{1 - \\frac{\\alpha_t}{\\alpha_{t-1}}} x_{t-1}, \\left(1 - \\frac{\\alpha_t}{\\alpha_{t-1}}\\right) I\\right)$,\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_0, (1 - \\alpha_t) I)$,\nwith noise rate at timestep $t$ as $1 - \\alpha_t/\\alpha_{t-1}$, where $\\alpha_t$ denotes noise scaling factors up to time step $t$. The reverse process is\ndefined below.\n$p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N} \\left(x_{t-1}; \\mu_{\\theta}(x_t), \\sigma_{\\theta}^2(x_t) \\mathbf{I}\\right)$.\nTo reparameterize the equation using\n$x_t = \\sqrt{\\alpha_t} x_0 + \\sqrt{1 - \\alpha_t} \\epsilon  \\text{ for } \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}),$\nand $\\epsilon_{\\theta}$, which is a function approximator for predicting $\\epsilon$ from $x_t$, the inference process becomes\n$x_{t-1} \\approx \\left(\\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}}\\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\alpha_t}} \\epsilon_{\\theta}(x_t)\\right)\\right) + \\sigma_t \\zeta,$\nWhere $\\zeta \\sim \\mathcal{N}(0, \\mathbf{I})$ and $\\sigma_t^2$ denotes the variance of Gaussian trainsitions .The objective of DDPM is defined as\n$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t, x_0, \\epsilon} [\\Vert \\epsilon - \\epsilon_{\\theta}(x_t)\\Vert^2],$\nwhere the L2 loss between the actual noise $\\epsilon$ added during training and the noise prediction $\\epsilon_{\\theta}(x_t, t)$ for uniformly sampled\n$t \\in \\{1, ..., T\\}.\nDenoising Diffusion Implicit Models (DDIM). DDIM [46] consider the following inference distributions:\n$q_\\theta(x_{1:T}|x_0) := q_\\theta (x_T|x_0)\\prod_{t=2}^T q_\\theta(x_{t-1}|x_t, x_0)$.\nwith a mean function as below.\n$q_0 \\left(x_{t-1} \\mid x_t, x_0\\right)=\\mathcal{N} \\left(x_{t-1} ; \\sqrt{\\alpha_{t-1}} x_0+\\sqrt{1-\\alpha_{t-1}-\\sigma_t^2}\\left(x_t-\\sqrt{\\alpha_t} x_0\\right) / \\sqrt{1-\\alpha_t}, \\sigma_t^2 \\mathbf{I}\\right)$.\nDistinctively from DDPM, the forward process is Non-Markovian since each $x_t$ could depend on both $x_{t-1}$ and $x_0$. Reparameterizing with $e_{\\theta}$, we can sample $x_{t\u22121}$ from $x_t$ through an equation:"}, {"title": "A.2. Derivations", "content": "Proposition 1. Let $x_T$ be an initial noise, and suppose that $x_0$ is the image obtained through denoising. Assuming Lipschitz\ncontinuity with distance metric $d$, for every $x_T$, there exists a constant $\u03ba > 0$ such that the following holds:\n$d(x_T, x_{Guide}^+) \\le kd(x_0, x_{Guide})$.\nproofs. The Lipschitz condition is expressed as follows:\n$d(\\epsilon_{\\theta}^{(t)}(x), \\epsilon_{\\theta}^{(t)}(y)) \\le L_td(x, y)$,\nwhere $L_t$ is constant dependent on $t$, $x$ and $y$ are arbitrary inputs to $\\epsilon_{\\theta}^{(t)}$. DDIM step in terms of $x_t$ can be expressed as\nfollows:\n$x_{t-1} \\approx \\left(\\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}}\\left(x_t + \\sqrt{1-\\alpha_t} \\epsilon_{\\theta}^{(t)}(x_t)\\right)\\right).$\nEq. (19) can be expressed in terms of $x_t^{Guide^+}$ which is denoised from $x_t^{Guide^+}$. With those equations, we can get the following\nequation,\nFinally, we have the derivation.\n$d(x_{t-1},x_{Guide}) \\le (\\sqrt{\\frac{\\alpha_{t-1}}{\\alpha_t}}(1+\\gamma_t \\epsilon_{\\theta}))$. Recursively organizing Eq. (20) for t = T, T \u2212 1, . . . , 1, it can be expressed as follows:\n$d(x_{T}, x_{Guide}) < (x)\\frac{\\alpha}{\\alpha_0}$."}, {"title": "B. More Ablation Studies", "content": "B. More Ablation Studies"}, {"title": "B.1. Diffusion Model Jacobian Approximation", "content": "In this section, we present experimental results demonstrating that the Jacobian of the diffusion model $e_\u03b8$ with respect to the\ninput $x_t$ can be approximated as proportional to the identity matrix. Fig. 12 illustrates the Jacobian of $e_\u03b8$. We observe that the\nJacobian of diffusion model behaves like the identity regardless of the timestep, except when $t$ is significantly small. In such\ncases, the deviation does not affect our primary analysis. According to the results of Proposition 2, the timestep-dependent\nconstant multiplied to each Jacobian term $\u03b7_t$ is expressed as follows:"}, {"title": "C. Additional Results", "content": "We present our additional qualitative results on Fig. 15, 16, 17 and 18. Results show that the performance of using refined\nnoise by noise refining model is comparable to that of using guidance on random Gaussian noise. All the results are selected\nfrom images used in Tab. 1 and 2.\nWe conducted a user study to evaluate prompt adherence and image quality by comparing images generated from random\nGaussian noise and our refined noise. The results are presented in Tab. 6. The study demonstrates that our method outper-\nformed the baseline in all human evaluation criteria. A total of 26 participants anonymously evaluated 20 pairs of images,\neach pair consisting of an image generated using initial Gaussian noise and our refined noise from noise refining model.\nThe percentage was calculated by dividing the total number of selections for each option by the total number of responses,\nfollowing the same methodology as in Tab. 4.\nParticipants were provided with the following instructions for each pair of images:"}, {"title": "E. Discussion", "content": "In this section, we compare the performance between training the noise refining model $g_{\\phi}$ and the denoising network $e_{\\theta}$ in the\ndenoising process without guidance (Sec. E.1). In addition, we present our hypothesis on why refined noise eliminates the\nneed for guidance methods, explaining it step by step (Sec. E.2). We further analyze the impact of initial noise and prompt\non the generated image (Sec. E.3)."}, {"title": "E.1. Effectiveness of Prompt Learning", "content": "As shown in the training framework of our method (Fig. 19), the noise refining model can be trained using the loss\n$d(x_{\\text{Guide}}, x_0)$, but the denoising network $e_{\\theta}$ within the Guidance-Free T2I pipeline can also be trained. Instead of directly\ntraining the model itself (e.g., fine-tuning models like CLIP [35]), our method demonstrates the efficiency of learning the\nnoise input to the Guidance-Free T2I pipeline, akin to prompt learning, which optimizes prompts instead of models. Specif-\nically, similar to conditional prompt learning such as CoCoOp [51], noise prompts are conditionally generated based on\ndifferent inputs (Gaussian noise $x_T$ and text prompt $c$).\nBy leveraging the knowledge of pretrained denoising networks, noise prompts can be generated efficiently, as verified\nin Sec. B.2. Here, we examine the case of training Guidance-Free T2I pipeline itself. Specifically, following the settings\nof ablation study on the number of denoising steps (Tab. 3) where we use filtered MS COCO [25] dataset, we compare the\nperformance of models trained using our learning method (Fig. 19) with models where $e_\\theta$ is fine-tuned on $x_T$ as input without\nusing the noise refining model. Instead of directly fine-tuning $e_{\\theta}$, we train a LoRA [17] module with the same rank and layers\nas $g_{\\phi}$ (used in the noise refining model)."}, {"title": "E.2. Why does refined noise help denoising?", "content": "To identify which refined noise components contribute to guidance-free generation, we first decompose the refined noise\ninto multiple frequency components. In this study, we utilize a two-dimensional Fourier transform to break down both the\nrefined noise and the initial noise into their respective frequency components. Each frequency component is represented by\na frequency band, denoted as $(a, b)$, which corresponds to the frequency range from $a$ to $b$. Note that although we explored\nother decomposition methods, such as dividing the noise into patches, they did not yield interpretable results."}, {"title": "E.3. Impact of Initial Noise and Prompt on Generated Image", "content": "We previously demonstrated how refined noise affects initial layouts and how guidance and refined noise contribute to\nforming these layouts effectively. In this section, we investigate how the 'layout' and the prompt influence the final generated\nimage during the denoising process. Specifically, we explore what happens when the prompt used to generate the initial\nlayout (P1, one of the inputs to the noise refining model g\u2084) differs from the prompt used during denoising (P2, one of the\ninputs to the denoising network ee in the Guidance-Free T2I Pipeline shown in Fig. 19). Does the model prioritize one prompt\nover the other? Or does it attempt to harmonize both? We investigate this question through the results shown in Fig. 26."}, {"title": "E.4. Comparison with a related work", "content": "A recent study [22] exists under the category of noise manipulation. To the best of our knowledge, this work is unique in its\nfocus on learning the noise space itself, rather than optimizing or selecting. Therefore, we compare our proposed approach"}, {"title": "E.5. Robustness to the number of denoising steps and schedulers", "content": "Since the noise refining model is trained with a fixed scheduler (DDIM [46]) and denoising steps (10), concerns arise regard- ing its performance when using different schedulers or denoising steps. To examine the impact of varying schedulers and denoising steps, we conduct experiments comparing qualitative results across diverse configurations. For comparison, we select DPM++ SDE [26], DPM++ 2M [26], and EDM [20], using the prompt \"a photo of a cat\". The results, presented in Fig. 29, show that our refined noise consistently produces reliable outputs regardless of the denoising timestep or scheduler. This demonstrates the robustness of the noise refining model across diverse schedulers and denoising step configurations."}]}