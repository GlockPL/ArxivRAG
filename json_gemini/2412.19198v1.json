{"title": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting", "authors": ["Ashutosh Baheti", "Debanjana Chakraborty", "Faeze Brahman", "Ronan Le Bras", "Ximing Lu", "Nouha Dziri", "Yejin Choi", "Mark Riedl", "Maarten Sap"], "abstract": "Obeying precise constraints on top of multiple external attributes is a common computational problem underlying seemingly different domains, from controlled text generation to protein engineering. Existing language model (LM) controllability methods for multi-attribute constraint satisfaction often rely on specialized architectures or gradient-based classifiers, limiting their flexibility to work with arbitrary black-box evaluators and pretrained models. Current general-purpose large language models, while capable, cannot achieve fine-grained multi-attribute control over external attributes. Thus, we create Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of finetuning language models on any sequential domain to satisfy user-specified constraints on multiple external real-value attributes. Our method trains LMs as editors by sampling diverse multi-attribute edit pairs from an initial set of paraphrased outputs. During inference, LM iteratively improves upon its previous solution to satisfy constraints for all attributes by leveraging our designed constraint satisfaction reward. We additionally experiment with reward-weighted behavior cloning to further improve the constraint satisfaction rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint Satisfaction (FINECS) benchmark, featuring two challenging tasks: (1) Text Style Transfer, where the goal is to simultaneously modify the sentiment and complexity of reviews, and (2) Protein Design, focusing on modulating fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical results show that MACS achieves the highest threshold satisfaction in both FINECS tasks, outperforming strong domain-specific baselines. Our work opens new avenues for generalized and real-value multi-attribute control, with implications for diverse applications spanning natural language processing and bioinformatics.", "sections": [{"title": "1 Introduction", "content": "Multi-attribute constraint satisfaction is a challenging problem that holds many useful applications in the domains of natural language processing (NLP), drug design, and protein engineering. In NLP, numerous classifiers and regressors exist for detecting individual linguistic attributes such as fluency, sentiment, formality, and complexity. Enabling fine-grained granular control over such attributes will allow users to personalize any text with their desired style (Kumar et al., 2021; 2022). In the realm of medicine and biotechnology, fine-grained control of multiple physicochemical properties opens avenues for engineering of novel drugs and proteins, for example, antibiotics with increased efficacy and reduced toxicity (Wong et al., 2023), and specialized proteins with manipulated attributes like fluorescence, binding affinity (Shen et al., 2014), and stability (Chan et al., 2021).\nConventional methods for multi-attribute control often rely on mechanisms such as class-conditioned LMs (Keskar et al., 2019; Lu et al., 2022; Hallinan et al., 2023) or latent attribute embeddings (He et al., 2020; Russo et al., 2020; Riley et al., 2021; Gu et al., 2022; Liu et al., 2022; Ding et al., 2023). However, these approaches"}, {"title": "2 Related Work", "content": "Precise Multi-Attribute Control While controlled text generation and style transfer have been widely studied problems in NLP literature, enabling fine-grained constraint satisfaction still proves to be quite challenging. A prominent approach is to incorporate attribute signals in gradients during decoding to allow satisfying multiple attribute constraints on them (Dathathri et al., 2020; Kumar et al., 2021; 2022; Qin et al., 2022; Li et al., 2022; Liu et al., 2023). However, there are three major limitations of these methods, (1) they require white-box access to evaluators for gradient computation, (2) their decoding speed is slow and memory intensive, and (3) their output length needs to be predefined to tractably compute the gradients. Other studies have proposed architecture augmentations and specialized loss functions (Russo et al., 2020; Riley et al., 2021; Gu et al., 2022; Liu et al., 2022; Ding et al., 2023; Hu et al., 2023) to perform multi-attribute control of language models. However, they typically cannot work with arbitrary external attribute evaluators and some also require expensive on-policy or off-policy samples during training. Recently, Mireshghallah et al. (2022) proposed probabilistic energy models to allow black-box attribute scorer constraints, but can only use masked language models for output sampling. In contrast to all the above methods, our framework leverages offline learning to offer the most flexibility in terms of external scorers, LM architecture choice, and training data sources.\nIterative Refinement via verbal feedback LLMs may not generate the best output on their first attempt. Therefore, many recent prompting methods have been introduced for LLMs to iteratively improve model outputs while incorporating internal and/or external LLM evaluators as verbal feedback (Shinn et al., 2023; Zhang et al., 2023; Madaan et al., 2023; Dhuliawala et al., 2023; Akyurek et al., 2023; Gou et al., 2024). However, these methods implicitly expect the availability of expert large language model (LLM) which may become costly during inference. Studies also find prompting LLMs with only scalar feedback is not as effective as using both scalar and verbal feedback (Peng et al., 2023). These methods are further limited by unrecognizable language or non-language sequential data sources (for example, DNA, protein, or chemical sequences) due to lack of domain knowledge (Ouyang et al., 2024), motivating the need for general-domain rewriting approach like MACS.\nIterative Refinement via fine-tuning To reduce inference costs, a few studies have demonstrated single attribute improvement across a diverse set of tasks via finetuning approaches for small LMs (Padmakumar et al., 2023; Welleck et al., 2023). Typically, a corrector a small LM-edits the previous response from itself or an external LLM to improve downstream task performance. These correctors are supervised finetuned on edit pairs obtained from off-policy sampling or paraphrasing techniques (mask and infill). We built upon these works to provide a unified framework for fine-grained control of multiple external attributes while only using offline data.\nData-driven approaches for Protein Engineering Designing proteins with desirable functionalities using limited data has been a longstanding challenge in biotechnology. Recent works have successfully leveraged machine learning and deep learning methods on assay-labeled data to find new protein sequences with enhanced properties such as fluorescence, binding affinity, stability, assembling, and net charge content (Hsu et al., 2022; Sinai et al., 2020; Ren et al., 2022; Padmakumar et al., 2023; Kirjner et al., 2024; Sternke & Karpiak, 2023). However, most of these approaches are limited to unidirectional optimization of only"}, {"title": "3 Multi-Attribute Constraint Satisfaction", "content": "3.1 Problem Definition\nWe aim to solve multi-attribute constraint satisfaction for any sequential data as a multi-step LM rewriting task. Formally, the language model is the actor in the Markov Decision Process (MDP), that learns to navigate across a multi-attribute space defined by a set of attribute evaluators $C = {C_1, C_2, ..., C_k}$ (which can be classifier probability, regressor, embedding similarity, protein attribute predictors, etc). All attribute evaluators convert sequential inputs into a scalar value within a finite range ($c_j(.) \\in [U_{j,min}, U_{j,max}]$). Each MDP episode begins with the initial state containing a context x (that can be empty), a starting sequence yo and its attribute location C(yo) and a set of threshold window constraints $T = {t_1, t_2, ..., t_k}$, where $t_j = (t_{j,start}, t_{j,end})$ is the threshold boundary for attribute cj. The rewriting language model M iteratively edits the previous sequence until it satisfies the given threshold constraints, i.e., $P_M(y_{i+1}|x, y_i, C(y_i), T)$.\nHere, each edit $y_i \\rightarrow y_{i+1}$ is considered an action, with a deterministic transition to the next state. During inference, the goal is to generate a series of consecutive edits starting from yo to yn, such that C(yn) \u2208 \u03a4.\n3.2 MACS Approach\nEdit Pairs Construction Even though the rewriting process is inherently multi-step during inference, we can isolate individual edits and train language model rewrite using offline pairs. For example, given any pair of similar sequences ya and yr which have distinct attribute locations C(ya) and C(yb), we can construct a training instance by asking the language model to edit ya\u2192 yb and artificially selecting threshold windows $T_{a\\rightarrow b}$\u00b3 that encourage M to move from C(ya) towards C(yb) Andrychowicz et al. (2017). We can similarly define another training instance going from yb\u2192 ya. Assuming m variations of a particular sequence are available ($y_1, y_2, \u2026\u2026y_m$), we can construct $P_m$ trainable edit pairs from them. In \u00a74 and \u00a75 we show how we create edit pairs for languages and proteins respectively.\nConstraint Satisfaction Reward We want to encourage the rewriter LM to make edits that move closer to the user-provided multi-attribute threshold boundary. If the initial sequence is already inside the target threshold boundaries, we expect the LM to paraphrase the sequence. Based on these two aspects, for each attribute ($c_j(.) \\in [U_{j,min}, U_{j,max}]$) and its corresponding threshold boundary ($t_j = (t_{j,start}, t_{j,end})$), we define its constraint satisfaction reward as the sum of the two components,\n$R(y_n, y_o, C_j(.), t_j) = f(c_j(y_n), t_j) + f(c_j(y_n), t_j) - f(c_j(y_o), t_j)$\nSatisfaction Score Change in Satisfaction Score \nHere yn and yo represent the new and the old sequence respectively, while f(.) \u2208 [0,1] is the threshold satisfaction scoring function that shows the deviation of the attribute score from its threshold boundary. We set the satisfaction score as 1 if its attribute location satisfies the threshold and linearly decreases to 0 as it moves towards the extreme ends,\n$f(c_j(y), t_j) = 1$                                                                                                                            if $t_{j,start} \\leq C_j(y) \\leq t_{j,end}$\nOtherwise"}, {"title": "4 FineCS - Text Style Transfer", "content": "While foundational large language models are capable of solving a variety of general language tasks via prompt engineering (Ouyang et al., 2022; OpenAI et al., 2024), they incur large computation overhead during inference and often underperform in directly incorporating external real-value feedback Peng et al. (2023).\nTo mitigate their limitations, we develop MACS to fine-tune small language models that enable constraint satisfaction on external signals via iterative refinement (Padmakumar et al., 2023; Welleck et al., 2023). To evaluate these methods, we create FINECS - Text Style Transfer task, where the goal is to precisely modify the sentiment and complexity of Yelp reviews while preserving fluency and content similarity.\nAttribute Evaluators To obtain the sentiment and complexity evaluators, we train RoBERTa-large (Liu et al., 2020) regressors on Yelp reviews (Zhang et al., 2015) and the SWIPE Wikipedia simplification dataset (Laban et al., 2023). The output range of the sentiment regressor is \u2208 [1,5], while the complexity regressor is within the range \u2208 [-2, 2]. Subsequently, we defined five threshold boundaries for sentiment as follows: (1,1.5) very negative, (1.5, 2.5) negative, (2.5, 3.5) neutral, (3.5, 4.5) positive, (4.5, 5) very positive and five threshold boundaries for complexity as follows, (-2, -1.5) very simple, (-1.5, -0.5) simple, (-0.5, 0.5) normal, (0.5, 1.5) complex, (1.5, 2) very complex. In total, these results in 25 different multi-attribute threshold combinations.\nWe further include two more evaluators to encourage fluency and content preservation: (1) fluency classifier probability (\u2208 [0, 1]) and cosine text embedding similarity score between the previous and the new output (\u2208 [0, 1]). Since we always want to maximize both properties, we add their scores directly in the constraint satisfaction reward function (eqn. 3) as two additional components.\nCreating Attributed Variations and Edit Pairs To synthetically obtain a diverse set of paraphrases previous studies have proposed various techniques such as mask-then-infill (Xu et al., 2018; Li et al., 2018; Ma et al., 2020; Padmakumar et al., 2023), back-translations (Prabhumoye et al., 2018; Zhang et al., 2018; Lample et al., 2019; Luo et al., 2019), paraphrasing models (Krishna et al., 2020) and generating multiple samples (Welleck et al., 2023). In our preliminary experiments, these methods did not yield many diverse attribute variations. Instead, we use few-shot prompted LLMs to generate alternate attributed variations of reviews. In particular, for both sentiment and complexity attributes, we first sample an equal number of reviews from each threshold boundary (1K from each label, 5K total for each attribute). Then, we construct few-shot"}, {"title": "4.1 Text Style Transfer Evaluation", "content": "For the Text Style Transfer task on Yelp Reviews, we design a fixed inference budget evaluation setup, i.e., each method will have a fixed number of allowed rewrites to satisfy all multi-attribute constraints. Subsequently, we construct a test set of 250 total reviews (10 from each of the 25 sentiment and complexity threshold combinations). The task is to generate 25 attributed paraphrases for every test review within 5 rewrites (250 \u00d7 25 \u00d7 5 \u2248 31.2K total inference budget). For every baseline and our models, we compare the constraint satisfaction success rate of multi-step inference strategies: best-of-N, naive rewriting, and reward-prioritized rewriting. We report the average satisfaction rate, fluency, and embedding similarity of paraphrases that satisfied the given constraints.\nBaselines and MACS models As a baseline, we use few-shot prompted Llama2-7B (Touvron et al., 2023) and Llama3-8B (AI@Meta, 2024) models as fine-grained editors for our Text Style Transfer task. For every transition from one threshold combination to another, we find 10 edit pairs as few-shot demonstrations (a total of 25 \u00d7 25 \u00d7 10 = 6250 edit pairs). For fine-tuning methods, we use a smaller TinyLlama (Zhang et al., 2024) 1.1B parameter model as the multi-attribute rewriter LM. Among finetuning baselines, we compare with Control Tokens (Keskar et al., 2019) that simply convert each threshold combination into style tokens. We allocate 10 total style tokens (5 for each attribute) and simply append the style tokens of the target threshold windows in the prompt along with the target response as follows: ya [Sentiment"}, {"title": "4.2 Text Style Transfer Results", "content": "We present the performance of all baselines and MACS models with the three inference types in Table 1. Among few-shot methods, the newer Llama3 model outperforms Llama2, however, both struggle to achieve very high satisfaction rates and show high variance across different threshold combinations. In comparison, the control tokens-based finetuning baseline works much better than few-shot prompting and gains a further boost in overall satisfaction rate when trained with our proposed k-NN edit pair sampling. Interestingly, naive rewriting is occasionally worse than best-of-N inference, indicating that models may not consistently move toward the threshold boundaries. The reward-prioritized rewriting improves over naive rewriting by leveraging the external scorers and our reward function to guide its search process.\nAmong our methods, the text-only finetuned model matches the performance of the control tokens baseline when trained with the proposed k-NN edit pair sampling. For models without anchor conditioning, we notice that multi-step rewriting can drift away from the original content indicated by a drop in embedding similarity when switching from best-of-N to rewriting. Anchor conditioning (\u571f) resolves the content drift problem and subsequently improves satisfaction rate and final embedding similarity when employing rewriting inference strategies. Finally, we notice that models trained with wBC outperform their counterpart SFT-only models"}, {"title": "5 FineCS - Protein Design", "content": "Unlike language, where text can be paraphrased in many different ways, fine-grained editing in protein space is challenging due to (a) the uneven distribution of assay-labeled data across multiple attributes and (b) the existence of limited potential solutions in nature for a given set of attribute constraints (Sternke & Karpiak, 2023). Moreover, for any given set of constraints, obtaining multiple novel and diverse candidates is important to maximize the chances of success in wet lab experiments (Jain et al., 2022). To evaluate fine-grained control of MACS framework in protein space, we create FINECS - Protein Design, where the task is to simultaneously modulate fluorescence and folding stability of Green Fluorescent Protein (GFP), (a protein widely investigated and used as biosensors in life sciences research).\nFluorescence and folding Stability Evaluators We obtain the dataset of \u2248 51.7K mutants of the GFP wild-type (i.e., the protein sequence that occurs in nature) (Sarkisyan et al., 2016; Gonzalez Somermeyer et al., 2022). The dataset contains fluorescence levels on logarithm 10 scale for every mutant sequence \u2208 [1.28,4.12]. Due to a lack of assay-labeled data for a second attribute, we calculate the theoretical folding stability values (AAG or ddG) of each mutant with respect to the wild-type structure using FoldX software(Schymkowitz et al., 2005). The wild-type ddG is 0 and any mutant with negative ddG is more stable than wild-type. The overall distribution of ddG for all the mutants is \u2208 [-5.66, 60.75]. We train ESM2-based regressors (Lin et al., 2023) as evaluators for both attributes using mean squared error loss. The test set correlation for fluorescence and ddG are 0.974 and 0.987 respectively.\nAttribute Distribution and Edit Pairs We plot the distribution of log fluorescence and ddG of all the GFP mutations in Figure 8 in the Appendix. Unlike language data, protein mutants are even more unevenly distributed across the multi-attribute landscape, with the bulk of the mutants clustered near the wild-type (WT) GFP sequence (which has \u2248 3.72 log fluorescence and 0 ddG). To effectively navigate this skewed distribution, we define four threshold boundaries in log fluorescence, (< 3.0) - very low, (3.0, 3.4) - low, (3.4, 3.7) - medium, (> 3.7) - bright and four threshold boundaries in ddG, (< 0) - more stable than WT, (0.0, 0.5) - as stable as WT, (0.5, 2.0) - slightly destabilized, (> 2.0) - highly destabilized (Dill et al., 2008).\nThe limited viable solutions in certain regions (< 10% of proteins have < 0 ddG) make the protein design task very challenging, especially when learning from an offline dataset of mutations. Here, all GFP mutants are considered paraphrases of each other, and thus, total possible edit pairs are \u2248 P51.7K. To train the LM rewriting models to edit in all possible directions, we employ the following edit pair sampling strategy: (1) pick two multi-attribute threshold boundaries, (2) sample a mutant at random from both of the selected threshold constraints and (3) construct an edit pair by treating the first as the source and the second as the target mutant."}, {"title": "5.1 Protein Design Evaluation", "content": "Unlike the Style Transfer task, where we only care about one solution for each constraint, the goal of the Protein Design task is to find the maximum number of new mutants in every multi-attribute constraint under a fixed inference budget. For each threshold constraint, we initiate multiple random walks of different lengths starting from wild-type GFP sequence (WT \u2192 Y1.\u2026\u2026 \u2192 Yn). We assign a total 3000 inference budget which results in (1) 3000 \u00d7 1-hops, (2) 1000 \u00d7 3-hops, and (3) 300 \u00d7 10-hops random walks. We expect duplicated predictions under specific constraints since certain regions will have naturally very few solutions. Among the 3000 predictions in each inference method, we calculate the total success rate: ratio of distinct mutants that satisfy the constraints according to our evaluators and unique success rate: ratio of unique successful mutants outside of the offline training data. We also compare with reward-prioritized walks from wild-type (\u00a73) where the LM generated intermediate edit Yi \u2192 Yi+1 is only retained if it moves closer to the threshold constraints, i.e. R(yi+1,WT,C,T) > R(yi, WT,C,T). We experiment with reward-prioritized walks in 1000 \u00d7 3-hops"}, {"title": "5.2 Protein Design Results", "content": "We report the evaluation results of baselines and different variants of MACS in Table 2. Random mutation shows the worst performance as expected, while Recombine is a strong baseline that finds more unique and successful mutants. With Unique Recombine, we establish an upper bound on the baseline's performance by only retaining unique sequences. However, the offline wBC model outperforms both the Recombine baseline and the SFT model across all inference strategies. When augmented with entropy penalty, we observe a boost in success rate for the wBC model and a larger spread of edit distances indicating more diverse"}, {"title": "6 Conclusion", "content": "We create Multi-Attribute Constraint Satisfaction (MACS) framework to cheaply train LMs as fine-grained editors by sampling edit pairs from offline sequential datasets. We also create a new Fine-grained Constraint Satisfaction (FINECS) benchmark to evaluate our method, comprising two challenging fine-grained controllability tasks. In the FINECS Text Style Transfer task, LM editors trained with weighted behavior cloning paired with proposed k-NN edit pair sampling, and multi-step reward-prioritized editing outperform their SFT counterparts and other inference methods. We boost its performance further with anchor conditioning and achieve the highest constraint satisfaction rates compared to previous fine-tuning and few-shot prompted baselines. Interestingly, in the FINECS Protein Design task, MACS can train protein language models to discover novel proteins outside the training data with high success rates while highlighting different mutational hotspots. Our study demonstrates the potential of LMs as fine-grained writing assistants and protein engineering models that can aid in the creation of novel proteins with fine-grained properties."}, {"title": "C Limitations and Future Work", "content": "MACS is an easy-to-implement framework to train domain-specific language models as fine-grained editors in an offline setting. However, there are a few limitations. Due to the offline nature of our method and our sampling strategy, it is unable to extrapolate well to regions within the multi-attribute space with low or no data points. To train good multi-attribute LM editors, MACS requires a good initial domain-specific pretrained language model. In our preliminary experiments with antibody generation task (Wong et al., 2023), a chemistry LM trained with MACS was not able to generate many novel candidates, likely due to its small size and poor data coverage.\nIn the future, we aim to extend our method such that it can use both offline and on-policy samples to improve its performance and diversity in the fine-grained control task. Further research is also needed to support categorical and lexical constraints in MACS."}]}