{"title": "Explore Activation Sparsity in Recurrent LLMs for\nEnergy-Efficient Neuromorphic Computing", "authors": ["Ivan Knunyants", "Maryam Tavakol", "Manolis Sifalakis", "Yingfu Xu", "Amirreza Yousefzadeh", "Guangzhi Tang"], "abstract": "The recent rise of Large Language Models (LLMs)\nhas revolutionized the deep learning field. However, the desire\nto deploy LLMs on edge devices introduces energy efficiency\nand latency challenges. Recurrent LLM (R-LLM) architectures\nhave proven effective in mitigating the quadratic complexity of\nself-attention, making them a potential paradigm for computing\non-edge neuromorphic processors. In this work, we propose a\nlow-cost, training-free algorithm to sparsify R-LLMs' activations\nto enhance energy efficiency on neuromorphic hardware. Our\napproach capitalizes on the inherent structure of these models,\nrendering them well-suited for energy-constrained environments.\nAlthough primarily designed for R-LLMs, this method can be\ngeneralized to other LLM architectures, such as transformers,\nas demonstrated on the OPT model, achieving comparable\nsparsity and efficiency improvements. Empirical studies illustrate\nthat our method significantly reduces computational demands\nwhile maintaining competitive accuracy across multiple zero-\nshot learning benchmarks. Additionally, hardware simulations\nwith the SENECA neuromorphic processor underscore notable\nenergy savings and latency improvements. These results pave\nthe way for low-power, real-time neuromorphic deployment of\nLLMs and demonstrate the feasibility of training-free on-chip\nadaptation using activation sparsity.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs), such as GPT-4 [1], and\nopen-source models, like LLaMA-3 [2], have become pivotal\nin advancing natural language processing. Their ability to\nprocess and generate high-quality text has led to widespread\nadoption across various industries. However, as LLMs and\ntraining datasets become larger, they demand more com-\nputational resources for training and inference. Specifically,\nthe self-attention mechanism of LLM architectures scales\nquadratically with the input length, while the computations\nin linear layers also increase quadratically with the growing\ndimension of tokens. These computational complexities are\neven more problematic when deploying LLMs on Edge AI de-\nvices with limited computing and memory resources. Besides,\ndata-specific optimizations are often too resource-intensive to\nexecute directly on the device, leading to privacy concerns\nwhen transferring data between devices and servers.\nRecurrent LLMs (R-LLMs), such as Transformer-RNN fu-\nsion models RWKV [3], RetNet [4], XLSTM [5], and Griffin\n[6], as well as state space models like S4 [7] and Mamba\n[8], [9], have emerged as lighter alternatives to self-attention\nLLMs. These methods combine the ability of recurrent in-\nference and operate with linear complexity, addressing the\nquadratic self-attention issue while still benefiting from the\nrapid parallel training. Additionally, the recurrent computa-\ntional paradigm is well-suited for neuromorphic processors,\nwhich are brain-inspired low-power AI-dedicated hardware\n[10]-[12]. These processors manage event-based data-flow\nprocessing, exploiting the activation sparsity in neural net-\nworks for energy-efficient and low-latency computation. To\nmaximize the benefits of neuromorphic computing, a high\nlevel of activation sparsity in the neural networks is crucial\n[13]. However, R-LLM models consist of dense linear layers,\nincluding high-dimensional upward and downward projec-\ntions, resulting in costly computation on neuromorphic proces-\nsors. Therefore, there is a need to explore activation sparsity\nin R-LLMs for energy-efficient neuromorphic computing.\nState-of-the-art methods for improving activation sparsity\nin LLMs apply the ReLU activation function before linear\nlayers in pre-trained models and perform fine-tuning on large\ndatasets to restore performance [14], [15]. Moreover, [15]\napplied activation regularization in loss functions to enable\nsparse-aware fine-tuning and further increased sparsity. How-\never, these approaches involve a training-based fine-tuning\nstage, which requires high computational costs due to the\nmodel size and training tokens. Privacy concerns over local\nfine-tuning data make on-device LLM adaptations preferable\n[16], but training computational costs render this impractical.\nFurthermore, current solutions are designed for transformer-\nbased models, and to our knowledge, no low-cost activation\nsparsification method exists for R-LLMs that are also well-\nsuited for on-device adaptation.\nThis paper proposes an R-LLM with activation sparsity\nfor energy-efficient neuromorphic computing and a training-\nfree threshold adaptation algorithm for improving activation\nsparsity. Our contributions are as follows:\nWe introduce an event-based R-LLM with thresholding\nfunctions. The resulting optimized R-LLM obtains an\naverage activation sparsity up to 63%, increasing 2.2\u00d7\ncompared to the original model with natural sparsity [3].\nWe propose a training-free algorithm to find thresholds\nusing local data adaptively. The algorithm can be de-\nployed on neuromorphic processors and is 30\u00d7 more\nefficient on GPU than the training-based method [14].\nWe demonstrate a 1.9\u00d7 energy and latency improvement\nof the sparse model via hardware simulation with the\nSENECA neuromorphic processor [10]."}, {"title": "II. NEUROMORPHIC RECURRENT LLMS", "content": "We present an efficient approach to sparsify the activa-\ntion of recurrent LLMs (R-LLMs), inspired by neuromorphic\ncomputing principles, that can be trained on a small dataset\nwithout requiring costly fine-tuning. The core idea is integrat-\ning activation sparsity before the linear layers, automatically\nzeroing out insignificant values during matrix multiplications.\nFollowing the concept of weight sparsity, we assume that\nactivations with small absolute values are unimportant, as they\nhave minimal impact on computations. In our approach, we\nuse a thresholding function with a parameter \u03bb, setting values\nwith an absolute magnitude lower than A to zero,\n\\(Threshold(x, \u03bb) =\\{\\begin{array}{ll}x & \\text { if } x \\geq \\lambda, \\\\0 & \\text { otherwise. }\\end{array}\\)\nTo this end, we iterate over all linear layers and initialize\nthe sparsifying function before each layer by performing a\ntraining-free threshold search using a small dataset. Given\nthe varying importance of different layers of the architecture,\ntheir respective activation functions are set to different sparsity\nlevels to optimize the overall average sparsity. As a result, we\nplace these sparsifying thresholding functions before all linear\nlayers of the targeted R-LLM for energy-efficient neuromor-\nphic processing.\nWe use RWKV [3] as the R-LLM representative to demon-\nstrate the effectiveness of our proposed method. The RWKV\nconsists of Time-Mix and Channel-Mix sub-blocks. A Time-\nMix sub-block with linear recurrent processing offers an alter-\nnative to the self-attention mechanism. This block applies three\nlinear projections: R, K and V, to the tokens and mixes them\nwith the previous tokens in the WKV recurrent mechanism.\nThe Out linear projection precedes the addition of the residual\nconnection. The Channel-Mix uses a receptance linear layer\n(R) while also using the classic LLM structure of upward\n(K) and downward (V) projections to/from 4\u00d7 dimension of\nembeddings. Inputs to the six linear layers are sparsified with\nthe proposed thresholding function (see Figure 1), while the\ndownward projection (V in Time-Mix) has a natural sparse\ninput due to the ReLU\u00b2 function before the linear layer in the\noriginal structure of RWKV."}, {"title": "B. Efficient Training-free Threshold Initialization Algorithm", "content": "We propose a training-free algorithm to efficiently initialize\nthe thresholding functions in pre-trained R-LLMs (see Figure\n2). Since each initialization step modifies the activation dis-\ntribution of subsequent layers, the algorithm iterates activa-\ntions/blocks sequentially from the first to the last layer/block.\nSince all blocks in the R-LLM share the same structure, the or-\nder of initializing the thresholding functions within each block\nremains consistent. Figure 1 exemplifies the initialization order\nwithin RWKV blocks, with components numbered.\nThe thresholding functions must be initialized with a cer-\ntain sparsity level, ensuring a controllable trade-off between\nsparsity and model performance. Before each threshold initial-\nization, we record the activation values on a small initialization\ndataset. We use these values to determine the thresholds\ncorresponding to the average sparsity percentages we want\nto consider. For example, we can calculate threshold values"}, {"title": "III. EMPIRICAL STUDY AND RESULTS", "content": "To iteratively compute the loss during the threshold ini-\ntialization algorithm, we utilize the Minipile dataset [18],\nwhich features diverse data intersecting with the RWKV's pre-\ntrained dataset. Minipile is a 6GB filtered and deduplicated\nsubset of the Pile dataset [19] for pre-training the RWKV.\nIn our experiments, we randomly sample 1k documents from\nMinipile's 1M training set as the initialization dataset and\nutilize a separate test dataset of 10k documents for final loss\nand sparsity calculation. We iterate over linearly increasing\nsparsity percentages \u2208 [10,20, ..., 80, 90] for thresholding\nfunctions, and use loss_inc = 1.0005."}, {"title": "B. Minipile Test Performance Results", "content": "We first evaluate the performance of our proposed approach\non three different-sized pre-trained RWKV models (with\n430M, 1.5B, and 3B parameters). We measure our models'\nsparsity and loss on the Minipile test dataset to obtain reliable\nvalues and compare against the original dense models with\nnatural sparsity (baseline) on Table I. On average, the original\nmodels show a low sparsity level of around 28% due to the\nnatural sparsity but exhibit \u2248 5% lower test loss compared to\nour models. For instance, for the smaller model with 430M\nparameters, the baseline shows a test loss of 2.2377, whereas\nthe proposed approach, with a significantly higher sparsity of\n57.03%, results in a slightly increased test loss of 2.3377,\nindicating a 4.47% increase in loss. This pattern holds across\nthe larger models as well. The 3B baseline model demonstrates\na test loss of 1.9297, whereas the proposed approach results\nin a higher test loss of 2.0510, corresponding to a 6.29%\nincrease while achieving a 2.2x increase in sparsity. Despite\nthe slight increase in the loss, the higher sparsity achieved by\nthe proposed approach leads to more efficient computation and"}, {"title": "C. Performance on Zero-Shot Learning Benchmarks", "content": "As the loss values do not accurately reflect the degradation\nin language modeling capabilities, we measure the perfor-\nmance of our models on multiple well-known LM benchmarks\nin the Language Model Evaluation Harness [20], PIQA, Wino-\ngrande, Hellaswag, ARC Challenge, ARC Easy, LAMBADA,\nOpenBookQA, and SciQ. Table II outlines the obtained results\nfrom this analysis.\nThe benchmark results demonstrate that our approach main-\ntains a robust overall performance across a wide range of\ntasks while significantly increasing sparsity. While there is a\nslight reduction in accuracy compared to the baseline models,\nthe drop is minimal, indicating that our approach effectively\nbalances sparsity and efficiency with only a minor impact\non performance. This consistent pattern across various bench-\nmarks highlights the robustness of our approach, making it a\ncompelling solution for optimizing resource usage while still\nachieving competitive results."}, {"title": "D. Effectiveness of Sparsity Percentage Search", "content": "In the threshold initialization algorithm, we use the fact that\nall R-LLM blocks share the same structure. We can anticipate\nsimilar behavior across corresponding thresholding function\npositions in each block. Therefore, we used a heuristic to start\nthe iteration process with the most commonly used percentage\nin the previous blocks at the same position. If we do not exploit\nthis heuristic, we will iterate over all sparsity percentages,\nstarting with the lowest. Therefore, without the heuristic, the\ninitialization algorithm will result in an increase in runtime\n(number of inferences for initialization) by around 3 times in\nour experiment."}, {"title": "IV. HARDWARE SIMULATION", "content": "To clearly understand the benefit of activation sparsity\nimprovements, we conduct an analytical hardware simulation\nstudy to evaluate the key hardware metrics: energy and latency.\nThe study is performed using the SENECA neuromorphic\nprocessor [10] and based on actual hardware measurements\nin [21]. Each input token processing in RWKV uses the same\nnumber of dense computations. Our energy and latency studies\nfocus on processing a single token within one RWKV block\nand use sparsity values averaged over RWKV blocks and\ntokens within the test dataset. The results can be multiplied by\nthe number of layers and tokens to estimate the performance"}, {"title": "B. Hardware Simulation Results", "content": "We present the results of two parts of the RWKV model:\nTime-Mix and Channel-Mix sub-blocks. The comparisons for\nenergy and latency are very similar, as both depend linearly\non the total operation count. We focus on two RWKV sub-\nblocks and omit LayerNorm due to its negligible impact. The\nhardware operations consist of memory operations (read/write)\nand computational operations (add, mult, div, etc.). Even\nthough the RWKV block uses functions like sigmoid, token\nshift, and others, around 97% of all operations are made by\nlinear layers. respectively illustrate the results\nfor energy and latency. We can observe that the influence of\nsparsification is considerable for Time-Mix by 2.3\u00d7 because of\nthe highly sparsified R, V, K, and Out linear layers. However,\nChannel-Mix is improved by 1.6\u00d7 as we counted the natural\nsparsity of the original RWKV model, and the only gain is in\nR and K linear layers. Nevertheless, both latency and energy\nmetrics in the entire model are improved by 1.9\u00d7, which is\na considerable change, while the sparsified model still shows\ncompetitive language modeling abilities."}, {"title": "V. GENERALIZATION TO SELF-ATTENTION LLM", "content": "Our approach can be extended to the standard transformer\narchitectures. and VI compare the application of\nour method on a 2.7B pre-trained OPT model [17] with the\nstate-of-the-art fine-tuning-based approach that requires costly\ntraining [14]. The results indicate that our approach performs\non par with [14] with similar activation sparsity and zero-\nshot benchmark accuracy. Moreover, we show results with"}, {"title": "VI. CONCLUSION", "content": "This paper introduced an efficient activation sparsification\ntechnique for recurrent LLMs (R-LLMs). Our method sig-\nnificantly reduces computation and energy demands without\nusing training-based fine-tuning, making it ideal for resource-\nconstrained applications. Experiments show that our approach\ndelivers competitive performance with high activation sparsity,\nwhile hardware simulations on the SENECA neuromorphic\nprocessor demonstrate substantial energy efficiency and la-\ntency gains. Consequently, this work advances the deployment\nof dedicated LLMs for neuromorphic computing, facilitating\nlow-power, real-time generative AI applications."}]}