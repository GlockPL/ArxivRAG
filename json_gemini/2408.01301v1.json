{"title": "A Decision-driven Methodology for Designing Uncertainty-aware AI Self-Assessment", "authors": ["Gregory Canal", "Vladimir Leung", "Philip Sage", "Eric Heim", "I-Jeng Wang"], "abstract": "Artificial intelligence (AI) has revolutionized decision-making processes and systems throughout society and, in particular, has emerged as a significant technology in high-impact scenarios of national interest. Yet, despite Al's impressive predictive capabilities in controlled settings, it still suffers from a range of practical setbacks preventing its widespread use in various critical scenarios. In particular, it is generally unclear if a given AI system's predictions can be trusted by decision-makers in downstream applications. To address the need for more transparent, robust, and trustworthy AI systems, a suite of tools has been developed to quantify the uncertainty of AI predictions and, more generally, enable AI to \"self-assess\u201d the reliability of its predictions. In this manuscript, we categorize methods for AI self-assessment along several key dimensions and provide guidelines for selecting and designing the appropriate method for a practitioner's needs. In particular, we focus on uncertainty estimation techniques that consider the impact of self-assessment on the choices made by downstream decision-makers and on the resulting costs and benefits of decision outcomes. To demonstrate the utility of our methodology for self-assessment design, we illustrate its use for two realistic national-interest scenarios. This manuscript is a practical guide for machine learning engineers and AI system users to select the ideal self-assessment techniques for each problem.", "sections": [{"title": "1 Introduction", "content": "In the past decade, the world has witnessed an explosion in the capabilities of artificial intelligence (AI) systems, and their use has proliferated throughout most corners of society. AI systems have been deployed in applications ranging from commercial and industrial uses, such as recommendation systems for social networks, advertising, and e-commerce, to platforms of national interest, including defense, healthcare, climate, and scientific sectors. While the power of generative and predictive AI systems to imitate, augment, and enhance human capabilities has become abundantly clear, arguably the most significant roadblock to fully deploying AI systems at scale in critical problem scenarios has been a lack of certainty about the reliability, robustness, and trustworthiness of their predictions and model outputs.\nIn general, AI systems can have their performance limited by a range of practical considerations, such as reproducing systematic bias present in their training data, failing to generalize to domains outside those encountered during training, overfitting to spurious correlations present in data, or lacking the ability to abstain from making unfounded predictions. In certain commercial applications such as online streaming, such predictive flaws are only moderately problematic since, in the worst case, a user may be provided with a suboptimal user experience (e.g., being recommended movies they would not typically enjoy) rather than suffering a catastrophic loss. However, in many problems of national interest, such shortcomings cannot"}, {"title": "1.1 Mathematical framework", "content": "It will sometimes be useful to describe techniques using a standardized notation and mathematical framework to discuss uncertainty-aware, decision-driven self-assessment in a unified manner. To this end, we build on notation and concepts from Kirchenbauer, Oaks, and Heim [7] and Zhao, Kim, Sahoo, et al. [9] to develop a mathematical, decision-driven framework for general self-assessment. We depict this framework in its entirety in Figure 1, and describe each core component below. Although this framework is a useful tool for relating self-assessment techniques to one another and understanding them in the broader context of AI learning and decision making, it is not necessary for a reader to understand this mathematical framework\nto make use of our methodology. Therefore, the mathematical framework below should be considered optional, and readers interested in higher-level self-assessment guidance can skip to Section 2.\nAI predictive model Let $X$ denote a data domain of interest (e.g., $X = \\mathbb{R}^d$) with individual examples denoted by $x$. Each example $x$ is associated with a label $y \\in Y$, such as $y = 1 ... k$ for classification over $k$ classes. Without loss of generality, we describe the AI model as a composition of two stages: a \"forward model\" containing the bulk of AI computation (i.e., forward pass in a neural network) and a \"prediction model\" converting the output of this computation into a specific prediction. Letting $Z$ denote an intermediate space to be defined shortly, we define the AI \"forward\" model as $f_\\theta: X \\rightarrow Z$, parameterized by the parameter set $\\theta$ in parameter space $\\Theta$. We define the secondary \u201cprediction\u201d function $h_w : Z \\rightarrow \\hat{Y}$, parameterized by $w$ in parameter space $W$, as a post-processing function mapping the intermediate representation $z \\in Z$ to a specific prediction $\\hat{y} \\in \\hat{Y}$, where $\\hat{Y}$ is a space of predicted labels. For a given example $x$, the AI model's prediction is then given by the composition $\\hat{y} = h_w(f_\\theta(x))$. We separately define the intermediate representation $z \\in Z$ and the specific model prediction $y \\in Y$ to make our framework as general as possible and allow for a distinction between intermediate variables produced by a model (which may be used in subsequent self-assessment) and the ultimate label prediction."}, {"title": "AI self-assessment", "content": "Beyond the specific predictions $\\hat{y}$ produced by an AI model, we define $S$ as the space of outputs produced by a corresponding self-assessment technique. For example, one popular approach to self-assessment is for the model to produce a scalar probability indicating its confidence in its prediction $\\hat{y}$. In this case, $S = [0, 1]$ is the unit interval on which this confidence value lies. To produce a self-assessment output, we define the mapping $g: X \\times Z \\times \\hat{Y} \\rightarrow S$, parameterized by $\\phi \\in \\Phi$, since in the most general scenario the self-assessment might depend on the input data $x$, forward representation $z = f_\\theta(x)$, and predicted output $\\hat{y} = h_w(f_\\theta(x))$, as $s = g(x, f_\\theta(x), h_w(f_\\theta(x)))$. As an example, to describe the scalar confidence self-assessment described above, suppose that the forward model outputs $p = f_\\theta(x)$ (where $Z = \\Delta^{k-1}$). Then, defining $g = \\max_i p_i$, we can write $s = g(f_\\theta(x))$, where in this case $g$ does not depend on $x$ or $y$ directly (but could in general) and does not have separate parameters $\\phi$ (so $\\Phi = \\emptyset$). Note that in some approaches where the output of $f_\\theta$ relies on stochastic elements (e.g., dropout), the self-assessment stage $g$ may require access to multiple forward passes to arrive at an uncertainty estimate (as in methods based on ensembles or Monte Carlo samples). Other self-assessment methods may rely only on knowledge of the forward model's parameters $\\theta$, without requiring additional uncertainty parameters. For clarity we do not indicate all possible dependencies in Figure 1 for every self-assessment scenario, and instead adopt a general framework that covers many use cases."}, {"title": "Downstream decision-making", "content": "Suppose that a decision-maker (e.g., human user, downstream software) observes the AI model's prediction $\\hat{y}$ and self-assessment $s$ to inform a downstream decision over a discrete set of possible actions $a \\in A$. We formally define a decision-making policy $\\delta: X \\times \\hat{Y} \\times S \\rightarrow \\Delta^{|A|-1}$ that takes as input the current example $x$, AI prediction $\\hat{y}$, and self-assessment $s$ and outputs a probability distribution over the set $A$ of possible actions. We assume without loss of generality that an action is then sampled from this probability distribution $\\delta(x, \\hat{y}, s)$. Since the decision-maker is typically unaware of the true label $y$, we assume that $a \\sim \\delta(x, \\hat{y}, s)$ is conditionally independent of $y$, given $x$.\nWe define a loss function $l : X \\times Y \\times A \\rightarrow \\mathbb{R}$ that measures the \"cost\" of each decision, as follows: given input example $x$ with ground-truth label $y$, if the decision-maker selects action $a$, then a cost of $l(x, y, a)$ is incurred; such a loss formulation is consistent with standard concepts in decision theory. Furthermore, this formulation generalizes common losses such as classification error, in which case $A = Y$ and $l(x, y, a) = \\mathbb{1}_{[a \\neq y]}$. In some frameworks such as Zhao, Kim, Sahoo, et al. [9], the loss function only depends on label $y$ and action $a$, but for generality, we include a dependence of $l$ on $x$. For a distribution $D_{X,Y}$ over example/label pairs $(x, y)$, we define the expected decision cost as $C = E_{x,y \\sim D_{X,Y}} E_{a \\sim \\delta(x,\\hat{y},s)} [l(x, y, a)]$. Generally speaking, for a given decision policy $\\delta$ and data drawn from distribution $D_{X,Y}$, a system designer should choose the AI forward model $f_\\theta$, predictive model $h_w$, and self-assessment $g_\\phi$ to jointly minimize the overall downstream decision cost $C$."}, {"title": "2 Self-assessment attributes and design guidelines", "content": "There is a wide spectrum of approaches and techniques developed for uncertainty quantification of AI predictions; see, for example, Abdar, Pourpanah, Hussain, et al. [4] and Gawlikowski, Tassi, Ali, et al. [5].\nTo categorize this varied range of techniques in a manner that aids practitioners, we organize\nmethods according to attributes that are important factors for designing and evaluating decision-\ndriven AI self-assessments. In the following, we identify these key attributes and provide examples of"}, {"title": "2.1 AI task", "content": "Uncertainty estimation is focused on characterizing the uncertainty of the AI model's output/prediction given an input. Hence, the relevant representation and estimation mechanisms for uncertainty will naturally depend on the underlying AI task. It is also important to consider the desired performance for the underlying AI tasks, as applying some uncertainty estimation techniques may impact the AI's performance for its given task. The majority of existing research on uncertainty estimation has been focused on classification and regression, but recent efforts have started examining a broader range of AI tasks:\n\u2022 Classification with performance measures based on the prediction confusion matrix, rather than average accuracy\n\u2022 Object detection\n\u2022 Regression\n\u2022 State estimation (tracking)\n\u2022 Segmentation\n\u2022 Reinforcement learning\n\u2022 Generative tasks\nAlthough uncertainty quantification techniques have been developed for several of these tasks (e.g., regression [10]\u2013[13], object detection [14]\u2013[17]), here we mostly focus on classification problems due to their ubiquity across a variety of practical AI problem settings. It would be straightforward to apply our framework more broadly to various other AI tasks, and we leave to future work an in-depth study of decision-driven self-assessment for non-classification AI tasks such as those listed here."}, {"title": "2.2 Uncertainty representation", "content": "An important feature of an uncertainty estimation technique is which notion of uncertainty is estimated and how it is represented. Different notions of uncertainty will naturally call for different representations and technical approaches. We identify the most common types of representations below:\n\u2022 Scalar confidence: often interpreted as an estimate of the posterior probability of the model's decision, given the observations.\n\u2022 Confidence interval/set: compact set in the output space with a probability bound (often referred to as the \"coverage\" for confidence intervals); could be an interval for continuous output or a discrete set for categorical output.\n\u2022 Parametric density/distribution: An approximation to the output distribution, often through approximation with a Gaussian distribution (first and second moments).\n\u2022 Out-of-distribution (OOD) score/probability: A measure of likelihood that an input to the model \"comes from\" a distribution distinct from the training data distribution. OOD is generally ill-defined without further assumptions on the underlying distributional shifts [18]\u2013[20]. Conceptually, a high OOD score for an input implies that the model trained and calibrated with training data provides limited information on the \"true\" prediction for the input."}, {"title": "2.3 Generic metrics", "content": "Given a representation of AI uncertainty, there are \"generic\" metrics that measure the quality of the underlying estimation problem. We refer to these as \"generic\" since they often do not consider specific impacts on downstream decisions. In selecting and tuning decision-driven self-assessment techniques, these generic metrics may not be the appropriate metric to optimize directly without further accounting for downstream decision-making. However, these metrics are often central to designing analogous metrics that directly take downstream decision costs into consideration.\n\u2022 Metrics for scalar confidence: Expected calibration error (ECE) [21], maximum calibration error (MCE) [21], negative log likelihood (NLL), Brier score, class-wise ECE, adaptive calibration error (ACE) [22]. Such metrics for calibration error have been generalized in a unified metric known as Generalized Expected Calibration Error (GECE) [7].\n\u2022 Metrics for confidence interval/set: Prediction Interval Coverage Probability (PICP) and Mean Prediction Interval Width (MPIW) are the standard metrics to measure the associated accuracy and \"tightness/specificity\" of confidence intervals. We can generalize their definitions for discrete confidence sets by defining an appropriate notion for the \"size\" of a set (e.g., by cardinality).\n\u2022 Metrics for parametric distribution: Several metrics can be used to measure the quality of the distributional approximation including negative log likelihood and an estimation of the Kullback-Leibler or related notions of divergence.\n\u2022 Metrics for OOD: The standard approach to evaluate OOD performance is to view the problem as a binary classification and resort to metrics such as the area under the receiver operating characteristic (AUROC) and the area under the precision-recall curve (AUPRC). It is important to characterize the anticipated distribution shifts (e.g., covariate versus concept shifts) OOD is intended to address to interpret these metrics accordingly (see [23])."}, {"title": "2.4 Estimation mechanisms", "content": "Uncertainty estimation can be broadly categorized into the following three mechanisms, differentiated by when they occur in training, and how much additional modeling is required beyond the native model $f_\\theta$:\n\u2022 Post-hoc techniques: These techniques are applied to a trained ML model without further tuning the forward model weights $\\theta$. Instead, post-hoc adjustments are made to the parameters $\\phi$ determining the behavior of the self-assessment stage $g_\\phi$ itself.\n\u2022 Integral to training: These techniques require access to the training process of the AI model, i.e., the training algorithm for $f_\\theta$. For instance, MC dropout [24] utilizes the same dropout mechanism used during training to sample from a posterior distribution of network weights, rather than requiring a separate re-training step. Similarly, ensemble methods [25] rely on training multiple candidate models, and the disagreement between such models captures a notion of uncertainty.\n\u2022 Intrinsic to model: rather than introducing self-assessment as an auxiliary modification for an AI model originally designed for prediction-only, there exists a class of self-assessment approaches where the elements of uncertainty quantification are intrinsic to the predictive model itself. For instance, Bayesian Neural Networks explicitly maintain a probability distribution over network weights. Typically, self-assessment techniques that are \"intrinsic to the model\" involve fundamental changes to the model itself, such as significant architectural modifications (e.g., outputting the mean and variance of a distribution rather than a pointwise prediction).\nIt is important to consider the underlying mechanism when considering an uncertainty estimation approach as the AI/ML model may have been trained a priori and prevent the applications of approaches requiring fundamental changes to training and/or model architecture selection. In addition, approaches intrinsic to the model will likely impact the performance of the underlying ML task."}, {"title": "2.5 Design parameters", "content": "Given the variety of metrics that might be used to assess the performance of any particular self-assessment technique, it is natural to utilize these metrics to guide the selection of which self-assessment technique to deploy and how to set any relevant SeA design parameters governing self-assessment behavior. As discussed in Section 1.1 and Figure 1, when possible we distinguish between the parameters $\\theta$ involved in the AI \"forward\" model making label predictions, and any SeA design parameters $\\phi$ involved in tuning the self-assessment model itself.\nWhile the relevant SeA design parameters depend on the specific technique being utilized, a few representative examples include:\n\u2022 Temperature $T > 0$ controlling the \u201csharpness\" of an estimated label distribution [26]. In the notation of our framework let $Z = \\mathbb{R}^k$, define $z = f_\\theta(x)$ as the \"logit\" vector output by a forward model, define $S = \\Delta^{k-1}$, $\\phi = \\{T\\}$, and let $\\sigma(\\cdot)$ notate the softmax operation. We can then \u201cscale\u201d an AI model's output label distribution by letting $g_\\phi(z) = \\sigma(z/T)$. Depending on the value of $T$, this operation softens or sharpens an estimated label distribution from the forward model to ensure that the scaled distribution is well-calibrated or satisfies other properties.\n\u2022 Error rate $0 < \\alpha < 1$ utilized in forming a conformal set [27]. In our framework, let $Z = \\mathbb{R}^k$ and $z = f_\\theta(x)$ denote a score vector over $k$ classes, $\\phi = \\{\\alpha\\}$, and let $S = \\mathcal{P}(\\hat{Y})$ be the power set (set of all subsets) of $\\hat{Y}$. Then $g_\\phi(x) = \\{y' : f_\\theta(x)[y'] \\ge r\\}$ where $f_\\theta(x)[y']$ is the entry of $f_\\theta(x)$ corresponding to label $y'$, and $r$ is computed such that on average over a held-out calibration set $g_\\phi(x)$ contains the true label $y$ with probability at least $1 - \\alpha$.\nIn certain settings, such SeA design parameters might be numerically optimized by an algorithm minimizing a relevant loss function (e.g., optimizing temperature $T$ to minimize negative log-likelihood [26]). In other cases, a practitioner might set such parameters manually by monitoring their effect on downstream evaluation metrics of self-assessment performance, or may select parameters based on desired SeA characteristics (e.g., error rate $\\alpha$ in conformal prediction). Here, we take a broad perspective and loosely refer to the process of either numerically adjusting or hand-tuning SeA design parameters as \"optimization.\" Furthermore, in typical machine learning nomenclature one would distinguish between parameters that directly tune SeA model behavior and any hyperparameters associated with SeA design, which can also be tuned or selected from to affect self-assessment performance. Examples of such hyperparameters might include:\n\u2022 Binning scheme (number of bins, bin edges) utilized in the histogram-based calibration of estimated label distributions [21].\n\u2022 Tuning regularization parameters in a loss function for training $g_\\phi$, including choosing a different loss function altogether. Deciding on a different loss function can affect, for instance, whether or not a self-assessment technique is risk-aware.\n\u2022 Costs/weights on different prediction errors or levels of self-assessment confidence (e.g., overconfidence can be costly since a human decision-maker may blindly trust the results [28]).\n\u2022 Selection of calibration set used for tuning self-assessment parameters.\n\u2022 Choice of models used in an ensemble and how uncertainty is estimated from this ensemble.\nIn a slight abuse of terminology, for conciseness we continue to refer to such hyparameters as simply being other SeA \"design parameters\" that a practitioner might tune or optimize. What all of the \"parameters\" described in this section have in common is that they are all intrinsic to the self-assessment technique, and need to be set by some means."}, {"title": "2.6 Downstream decisions", "content": "Even though not an attribute of an estimation technique, the downstream decision-maker that will \"consume\" the output from the AI model and its self-assessment shall dictate the applicable uncertainty representations. For example, a human user may not be able to interpret a complex label distribution to arrive at an informed decision, and it may be better to provide an uncalibrated yet interpretable distribution instead. Beyond compatibility with the decision-maker, the output of self-assessment mapping $g_\\phi$ may drastically affect which downstream decisions are made, even for non-human decision-makers. For example, a downstream Kalman-filter based tracking algorithm may assume a covariance matrix (to characterize observation uncertainty) associated with each detection to combine detections from multiple sensors. If this covariance matrix is provided by the self-assessment model $g_\\phi$, then $g_\\phi$, in effect, significantly impacts the downstream performance of the tracking algorithm.\nWhile many self-assessment techniques are designed to increase decision-maker trust in AI predictions and effectively convey uncertainty, it is not always the case that explicit effects on downstream decisions and associated costs are considered during self-assessment design and optimization. Many self-assessment techniques are derived, optimized, and evaluated based on a set of \"generic\" statistical metrics measuring various properties of their uncertainty outputs (e.g., distribution calibration, see Section 2.3), which are usually calculated independently from subsequent downstream decisions; here, we refer to such methods as being decision-agnostic. This stands in contrast to self-assessment techniques that explicitly consider self-assessment outputs' effects on downstream decisions, which we categorize as being decision-aware. For example, as described in Section 3.2, certain self-assessment approaches optimize SeA parameters to minimize the expected downstream decision cost explicitly."}, {"title": "2.7 Guided self-assessment design", "content": "The key attributes outlined above define the important \"dimensions\" to consider when selecting, tuning, and evaluating uncertainty estimation for AI self-assessment. As these dimensions are not necessarily orthogonal, it is unlikely that a monolithic decision process (i.e., a single decision tree) exists for all use cases to guide a user towards an optimal self-assessment technique. Here, we postulate a likely \u201cdecision flow\u201d (depicted in Figure 2) to illustrate a typical self-assessment design process:\n1. Identify candidate uncertainty estimation approaches based on the AI task at hand;\n(a) If a model has been trained, then only the post-hoc approaches are applicable.\n(b) Determine if an approach could have a significant negative impact on the associated task-specific performance measure.\n2. Determine the appropriate uncertainty representation taking into account the model (if already trained) and the downstream decision (what the downstream decision-maker can consume).\n3. Examine the published benchmark on the generic metrics associated with the uncertainty representation to develop a qualitative analysis of candidate approaches.\n4. Identify tunable optimization parameters for the candidate approaches and derive an optimization criterion following our decision-theoretic framework.\n5. Optimize uncertainty estimation against the optimization criterion and evaluate the performance based on the expected cost associated with the downstream decision."}, {"title": "3 Overview of self-assessment techniques", "content": "By following the guidelines presented in Section 2.7 and in Figure 2, a user can arrive at a method for self-assessment appropriate for their particular problem scenario. In this section, we provide an overview of relevant self-assessment techniques and describe them according to the attributes presented in Section 2. The set of techniques considered here should be considered a representative, but not necessarily exhaustive, suite of complementary approaches for self-assessment. As new approaches are developed, they can easily be categorized along the dimensions of Section 2 and added here. In Section 3.1, we detail various approaches to decision-agnostic self-assessment (summarized in Table 1), followed by a representative suite of decision-aware techniques in Section 3.2 (summarized in Table 2)."}, {"title": "3.1 Decision-agnostic self-assessment", "content": "In this section we present a representative overview of popular and distinctive methods for decision-agnostic self-assessment (summarized in Table 1). First, we outline approaches rooted in post-hoc adjustment of a model's class label distribution to minimize various notions of calibration error. Then, we pivot to discuss methods with uncertainty representations intrinsic to the model or integral to the model's training process. Finally, we briefly mention several techniques with noteworthy uncertainty representations distinct from direct estimates of label or model uncertainty.\nPost-hoc approaches One of the most common approaches to self-assessment is to output the estimated probability of a model's prediction as a scalar notion of \"confidence\" by letting $h(z) = \\arg \\max_{y'} z_{y'}$ and $g(z) = \\max_{y'} \\sigma(z)[y']$, where $z$ is a vector of logits output by an AI forward model, $\\sigma(z) = \\text{softmax}(z)$, and $\\sigma(z)[y']$ denotes the softmax probability corresponding to class $y'$. However, it has generally been established that such a confidence measure is not necessarily well-calibrated, especially for modern neural networks [26]. A simple and practical strategy to remedy this miscalibration is to apply post-hoc calibration by sharpening or smoothing the estimated confidence scores with a temperature parameter $T > 0$ by letting $g_T(z) = \\max_{y'} \\sigma(z/T) [y']$. For small values of $T$, the model places higher confidence in its prediction, and for larger temperature values, the output probabilities are \u201csoftened,\u201d resulting in higher model uncertainty. Such temperature scaling for uncertainty quantification was introduced in Guo, Pleiss, Sun, et al. [26] and extended to other scalar notions of confidence by Yona, Feder, and Laish [29].\nBuilding on this approach, Guo, Pleiss, Sun, et al. [26] introduced an extension to temperature scaling where a linear transformation $Wz+b$ is applied to the logits $z$ before the softmax operator in lieu of a simple scaling by $1/T$. Kull, Perello Nieto, K\u00e4ngsepp, et al. [30] build on this concept even further by applying such a transform to log probabilities before subsequently applying softmax, which was shown to be equivalent to learning a Dirichlet distribution over the class labels as a means of calibration. Rather than using a parametric transformation such as temperature or matrix scaling to calibrate an output distribution, Zadrozny and Elkan [31] introduced a non-parametric approach to post-hoc calibration known as histogram binning. In this approach (applicable to binary classification with $k = 2$), data points are first binned according to their sorted predicted probabilities, and then each point's positive class probability is estimated by computing the fraction of positive instances empirically observed in the bin (theoretical guarantees were shown for histogram binning in Gupta and Ramdas [32]). Pakdaman Naeini, Cooper, and Hauskrecht [21] extend this technique by simultaneously employing multiple binning schemes in a Bayesian model to address the limitations of histogram binning associated with having fixed bin edges. We note that principles from scaling-based methods and histogram binning have been combined in a scaling-binning calibrator that enjoys theoretical guarantees on calibration error [33].\nIntegral to training / intrinsic to model Beyond self-assessment methods that applying a post-hoc re-calibration method to model outputs, as discussed in Section 2 there exist other approaches tied to model training itself, which sometimes include uncertainty representation components intrinsic to the forward model. Such approaches vary in their degree of explicitness in how uncertainty is represented. At one extreme, deep ensembles [25] train separate networks with varying initializations, and average their output predictions to implicitly account for model uncertainty. Similarly in MC Dropout, uncertainty is represented implicitly by"}, {"title": "3.2 Decision-aware self-assessment", "content": "Qualitatively, the mere presence of AI self-assessment may encourage downstream decision-makers about the trustworthiness of the AI's predictions. However, to quantitatively benefit downstream decision processes, the self-assessment must directly improve metrics measuring downstream decision quality. In the context of our mathematical framework, this is achieved by a self-assessment strategy being designed and tuned to optimize the expected downstream decision cost $C$ with respect to a cost function $l(x, y, a)$ that is appropriate for a user's particular application (see Section 1 and Figure 1).\nChoosing self-assessment strategies that optimize or at least account for such downstream decision costs goes a step beyond optimizing \"generic metrics\" for self-assessment that may not result in optimal downstream decisions. For example, uncalibrated label distributions taking into account the nuances of human psychology can sometimes result in more optimal human decision-maker performance than by using calibrated distributions that don't explicitly take the decision-maker into account. In this vein, there is a significant body of human factors research analyzing the joint performance of human-AI decision-making systems under various types of self-assessment [28], [46], [47]. In recent years, a suite of self-assessment techniques has emerged that are explicitly designed to take these factors into account and optimize downstream decision costs; we highlight a representative set of such methods here (summarized in Table 2).\nMarx, Zalouk, and Ermon [48] develop a general, kernel-based calibration method that is integral to model training, where different selections of kernels result in certain types of calibration being optimized. Kumar, Sarawagi, and Jain [39] can be seen as a special case of this more general framework. As an example, a kernel based on downstream decision losses is presented as a special case of their more general framework, and evaluated on a threshold-based decision-making task. This approach strives to minimize Decision Calibration Error (DCE) which measures the difference between the expected decision cost as calculated from the self-assessment distribution, versus the true expected decision cost. By ensuring a low DCE, downstream-decision makers can trust that estimates of decision cost calculated from the self-assessment output are reliable estimates of the true cost for each candidate decision, to better inform decision-making.\nThis work builds on the earlier work in Zhao, Kim, Sahoo, et al. [9], which first introduced the notion of decision calibration and devised an algorithm to re-calibrate a given label distribution to minimize decision calibration error. However, this earlier work is calibrated to the set of decision losses over a fixed number of actions rather than being calibrated with respect to a specific decision loss $l$, and is only applicable to Bayes optimal decision makers $\\delta$, which may not be applicable in real-world scenarios. Sahoo, Zhao, Chen, et al. [49] address the setting of thresholded decisions on a regression model, and present an algorithm for re-calibrating a probability distribution over regression values to minimize the difference between estimated and true losses incurred under a threshold decision model.\nAlthough Zhao, Kim, Sahoo, et al. [9] and Marx, Zalouk, and Ermon [48] take into account downstream decision costs in their self-assessment tuning, they only allow for a Bayes optimal decision maker $\\delta$, which is likely to be unrealistic in practical settings. Instead, Vodrahalli, Gerstenberg, and Zou [50] address this design aspect explicitly by modeling the decision-maker $\\delta$ using a separate predictive model. Once $\\delta$ is estimated from human behavior training data, along the lines of temperature scaling the AI self-assessment (in this context, called the AI \u201cadvice\u201d) notated by A is re-calibrated with parameters $\\alpha, \\beta \\ge 0$ to arrive at a re-calibrated scalar confidence value $g(A) = 1/(1 + \\exp(-\\text{sign}(A)(\\alpha |A| + \\beta)))$. These constants are selected to minimize the binary cross-entropy loss of the human decision-maker's (modeled via $\\delta$) ultimate decision, after viewing the re-calibrated AI \"advice.\" While presenting a promising concept, the author's also note that this framework is \"not currently suitable for practical use.\"\nIn a similar vein, [51] uses an estimate of the decision-maker's confusion matrix over classes $\\hat{Y}$ with respect to the ground-truth class $y$ to choose an optimal coverage probability for a conformal set to minimize the decision-maker's ultimate probability of decision error. Kerrigan, Smyth, and Steyvers [52] similarly utilize confusion matrix estimates to combine human and AI decisions into joint decisions that outperform either the AI or human making decisions on their own. Bansal, Nushi, Kamar, et al. [53] expand the considered downstream loss function $l$ to account for unequal costs of correct or incorrect decisions. The authors utilize these cost parameters along with several decision-theoretic assumptions to derive a novel loss function equal to the expected utility, which is used directly for model training.\nIn contrast to many of the decision-agnostic methods described in the previous section, what unifies the decision-aware approaches presented here is their explicit consideration of the downstream decision maker"}, {"title": "4 Notional examples", "content": "In this section, we briefly illustrate how our self-assessment selection and design methodology might be applied to two realistic scenarios of national interest where AI transparency is crucial in informing downstream decision-making. To demonstrate the breadth of our approach to many problem settings, we discuss examples with human and autonomous decision-makers (i.e., $\\delta$), which observe self-assessment outputs to make downstream decisions. We first describe a disaster relief scenario with a human decision-maker, followed by a UAV ISR tracking application with an autonomous decision-maker."}, {"title": "4.1 Disaster relief triaging (human decision-maker)", "content": "Medical triaging during disaster relief scenarios is key to effectively applying limited medical resources to prioritize various casualty needs appropriately. As an example of a realistic disaster relief scenario and triaging effort, we consider a scenario inspired by the DARPA Triage Challenge (DTC), which breaks triaging into two stages of primary and secondary triaging:\n\u2022 Primary triaging consists of identifying the severity of a situation, the presence of casualties, injury severity levels, and who requires hands-on medical evaluation or interventions.\n\u2022 Secondary triaging involves placing non-invasive sensors on these casualties to determine critical vs. non-critical conditions."}, {"title": "4.1.1 Problem setup", "content": "In the notional disaster triaging scenario we consider, the primary mission is to maximize the number of saved lives by appropriately deploying resources to casualties present in the scene. We envision this task being performed by a human deciding where to deploy medical resources and personnel based on the outputs of an Object Detection and Classification model (stylization in Figure 3b). Specifically, we assume the model receives as input high-resolution UAV RGB imagery captured by an expert operator (each image instance denoted by $x$ in our framework in Figure 1). We assume that the images will cover the entire incident area, ensuring all viewable casualties are within at least one frame and no frames are overlapping.\nEach casualty is associated with a \u201cground-truth\" class label (y) given by one of four states, corresponding to various levels of injury severity: Healthy, Delayed, Immediate, or Expectant. These labels are a simplified mapping from recorded ground-truth labels, which include Trauma, Alertness, and Vitals presence/continuous values alongside the presence of critical factors, Severe Hemorrhaging and Respiratory distress. For this notional example, we assume there is a meaningful mapping from these health descriptors to injury severity. The Object Detection algorithm will determine each casualty location, while the Classifier predicts casualty state (y). We assume the forward model ($f_\\theta$) is a combined Object Detection and Classifier model (e.g., YOLO [54]) that outputs as $z$ bounding box coordinates and a probability vector $p(x)$ containing the confidence of each class. As feedback to the human decision-maker, we assume these outputs are visualized by superimposing the output bounding boxes with their corresponding predicted injury state. We also assume that the visualization allows for a single scalar confidence value to be presented with each prediction.\nBased on this visualization, the human decision-maker ($\\delta$) then takes one of three possible actions (a), per casualty bounding box: no action, deploy medical personnel, evacuate. Each of these actions requires"}]}