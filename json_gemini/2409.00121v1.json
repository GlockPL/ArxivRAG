{"title": "BELT-2: BOOTSTRAPPING EEG-TO-LANGUAGE REPRESENTATION ALIGNMENT FOR MULTI-TASK BRAIN DECODING", "authors": ["Jinzhao Zhou", "Yiqun Duan", "Fred Chang", "Thomas Do", "Yu-Kai Wang", "Chin-Teng Lin"], "abstract": "The remarkable success of large language models (LLMs) across various multi-modality applications is well established. However, integrating large language models with humans, or brain dynamics, remains relatively unexplored. In this paper, we introduce BELT-2, a pioneering multi-task model designed to enhance both encoding and decoding performance from EEG signals. To bolster the quality of the EEG encoder, BELT-2 is the first work to innovatively 1) adopt byte-pair encoding (BPE)-level EEG-language alignment and 2) integrate multi-task training and decoding in the EEG domain. Inspired by the idea of Bridging the Brain with GPT, we further connect the multi-task EEG encoder with LLMs by utilizing prefix-tuning on intermediary output from the EEG encoder. These innovative efforts make BELT-2 a pioneering breakthrough, making it the first work in the field capable of decoding coherent and readable sentences from non-invasive brain signals. Our experiments highlight significant advancements over prior techniques in both quantitative and qualitative measures, achieving a decoding performance with a BLEU-1 score of 52.2% on the ZuCo dataset. Furthermore, BELT-2 shows a remarkable improvement ranging from 31% to 162% on other translation benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, the emergence of large language models (LLMs) has spurred efforts to integrate them with various modalities, such as VisualLLMs (Liu et al., 2023b; Oquab et al., 2023), and Robotics (Driess et al., 2023). These methods achieved remarkable improvement in various task settings. Yet, an important topic, the direct combination of LLMs with human intention remains relatively unexplored. Nonetheless, the inherent subject-wise non-stationary characteristics of Electroencephalography (EEG) signals, coupled with rigorous experimental protocols, make the task of decoding words or sentences exceptionally challenging."}, {"title": "2 BELT-2", "content": "BELT-2 introduces the Q-Conformer which enhances both the capacity to encode EEG information and the extendibility to multi-task. To bridge the modality gap between EEG and language, we boost EEG-to-Language representation learning through two learning stages: (1) the EEG-to-language alignment learning stage for learning the Q-Conformer EEG encoder. (2) a prefix-tuning stage for bridging Q-Conformer with LLM."}, {"title": "2.1 Q-CONFORMER AS EEG ENCODER", "content": "The overall structure of the Q-Conformer is illustrated in Figure 5 which consists of a discrete conformer, a Context Transformer (C-Former), and a query prompt. The discrete conformer functions as a discrete EEG tokenizer that captures primitive patterns from the input EEG embeddings. The C-Former extracts mid-layer coding (MLC) that contains context information specific to a given task given by the learnable query prompt."}, {"title": "Discrete Conformer:", "content": "The discrete conformer consists of a conformer model and a vector quantizer. After preprocessing, the raw EEG waveform is segmented into windows using eye-tracking information. Then a frequency domain transform converts EEG segments into fix-size EEG embeddings \\(e \\in \\mathbb{R}^{L\\times N \\times D}\\). L is the maximum length of the embedding sequence, N denotes the number of EEG channels, and D denotes the embedding size. The conformer model consists of 2 conformer blocks which follow the structure manifested in (Gulati et al., 2020). The conformer model \\(E(\\cdot)\\) converts the EEG embeddings e into continuous EEG tokens \\(h \\in \\mathbb{R}^{L\\times N \\times d}\\), where d denotes the size of the continuous EEG tokens.\nWe then convert h to a set of discrete tokens b by a vector quantizer (VQ) that looks up the nearest discrete code \\(v_k, k = \\{0,1,\\ldots, K\\}\\) from the codebook V (Razavi et al., 2019). The quantization process \\(z_q(h)\\) can be written as Equation 1."}, {"title": "C-Former and Query Prompt", "content": "We create a set number of learnable query embeddings (query prompt) as input to the C-Former. The C-Former is composed of self-attention layers and cross-attention layers arranged in consecutive order. After feeding the query prompts and the discrete EEG tokens into the C-Former, the query prompts interact with each other through the self-attention layers and further interact with the discrete EEG tokens through the following cross-attention layer. A new query prompt will be initialized when training the Q-Conformer for a specific task. After training on a specific task, the query prompts learn to act as the instruction of the current task that guides the C-Former to extract MLC as the task-specific context from the EEG modality.\nThis querying mechanism enables a more flexible adaptation of the pretrained Q-Conformer to a new downstream task by adding a new set of query prompts. It also allows the reusing of knowledge learned from previous training tasks. In our experiment setup, we initialize the C-Former with the pre-trained weights of BARTlarge (Lewis et al., 2019). We employ a query prompt of 20 learnable tokens for a specific, with each query possessing a dimensionality of 1024."}, {"title": "2.2 EEG-TO-LANGUAGE ALIGNMENT LEARNING", "content": "In the EEG-to-language alignment learning stage, we train the Q-Conformer and align the encoded EEG tokens to the language modality. To achieve EEG-to-Language alignment, we combine two contrastive objectives and a pretraining objective to the VQ objective in Equation 2. The two contrastive objectives include (1) BPE-level contrastive learning (BPE-CL), and (2) Negative Contrastive learning (NCL). We further pretrain the Q-Conformer to achieve a task-specific query prompt by the EEG-to-Language matching (ELM) objective, which guides the C-Former to extract MLC that contains the most relevant EEG contexts in the specific task."}, {"title": "BPE-level contrastive learning (BPE-CL)", "content": "learns to align the discrete EEG tokens with BPE subword embeddings by maximizing their mutual information. Unlike the BELT-1 model (Zhou et al., 2023a) where contrastive learning is only performed at the word level, we perform EEG-language alignment in the BPE subword level to improve EEG-language alignment. Given the limited size of EEG-language pairs in the training set, this method enforces stronger semantic guidance to the EEG representation while enhancing the matching of subword units that are out of training vocabulary."}, {"title": "2.3 BRIDGING Q-CONFORMER WITH LLM", "content": "We propose to bridge the frozen Q-Conformer and a frozen LLM to leverage both models effectively for EEG-to-Language tasks by tuning a set of virtual prefixes added to the output embeddings of the Q-Conformer, in order to achieve stronger performance at a lower training cost."}, {"title": "Prefix-tuning", "content": "To achieve a proper prefix prompt that can steer the LLM to decode the MLC without changing the LLM's parameters, we adopt the prefix-tuning (Li & Liang, 2021) method to only train a set of virtual prefix tokens as prompts to the LLM. In particular, we concat the virtual prefix and the MLC from the Q-Conformer as input to the subsequence frozen LLM. Please refer to Appendix C.3 for more details on prefix-tuning."}, {"title": "Speculative Augmentation (SA)", "content": "Despite the use of the lightweight prefix-tuning method, the size and diversity of training samples are still lacking. This is because while the Q-Conformer learns to extract task-specific context, it also learns to ignore task-irrelevant information. This would be a well-anticipated perk for an EEG encoder if we choose to directly decode language output from the EEG encoder. However, it also significantly reduces the diversity of training samples, making the learning of a good prefix difficult.\nOur BELT-2 framework solves this issues by proposing the SA method to sample MLC from a total of K+1 Q-Conformer checkpoints to provide more diverse prefix-tuning samples. In particular, we randomly sample K model checkpoints other than the best-performing checkpoint to produce MLC for the prefix-tuning. During the forward process, a speculative ratio r is defined to determine whether to use best checkpoint or one of the K suboptimal checkpoints. To reduce the cost of memory, we cache the output MLCs of these K model checkpoints during the training of Q-Conformer to avoid actually loading the checkpoints in the prefix-tuning stage.\nIn our experiment, we set K = 15 for a balance of performance and training costs to achieve a 6\u00d7 larger and more diverse training sample set for the tuning of the LLM Decoder."}, {"title": "2.4 EXTENDING DECODING TO MULTI-TASK", "content": "Translation: Our definition of the EEG-to-Text translation task follows previous works on this topic (Wang & Ji, 2022). Given the word-level EEG embedding sequence and text sentence pair (E, S), we maximize the probability of the decoded sentence p(S|E) produced by our model. The training objective Ltr for the translation task could be written as follows:"}, {"title": "Summary:", "content": "We propose the first EEG-to-text summarization task by creating a summary dataset from the Zuco datasets. Human attention lingers around keywords and pivotal concepts during reading (Ding et al., 2022). Consequently, we hypothesize that the extraction of key concepts could be a more direct way to facilitate the transmission of neural information and the understanding of a person's intention. As such, our nuanced summarization task not only enhances our understanding of EEG data but also opens up exciting possibilities for advancing research in cognitive science."}, {"title": "Sentiment Classification:", "content": "We could further extend the Q-conformer to perform the sentiment classification task by adding another query prompt for the Q-Conformer and using the last output token from the Q-conformer as the CLS token. In particular, we use the EEG-sentiment label pair (E, c). Unlike Wang & Ji (2022), we don't need to use external sentiment classification datasets or learn an additional classifier. The training objective for sentiment classification is as follows:"}, {"title": "3 EXPERIMENT AND RESULTS", "content": "We use the ZuCo datasets (Hollenstein et al., 2018; 2019) for the training and evaluation of the proposed BELT-2 framework. The ZuCo datasets contain EEG data recorded during natural reading tasks with eye-tracking data for word-level EEG segmentation. Reading material is collected from movie reviews (Socher et al., 2013) and Wikipedia articles. We split the dataset into train, val, and test subsets (80%,10%,10%). In this cross-sentence setting, sentences will not overlap among any two subsets. In addition, cross-subject performance is also evaluated. We evaluate translation and summary performance using the BLEU scores (Papineni et al., 2002) and ROUGE-1 scores Lin (2004). We use P., R., F1, and Acc. to denote precision, recall, F1-score, and accuracy respectively."}, {"title": "3.2 IMPLEMENTATION DETAILS", "content": "The code could be assessed through an anonymous link 2. For the word-level EEG embeddings, the total length of an embedding sequence is L = 56 and the embedding size is d = 840. The discrete conformer has 8 attention heads with the feed-forward dimension size of 2048 and a discrete codebook with 1024 entries with a latent size of 1024. The number of querying tokens used for The Q-Conformer is 20. We train the Q-Conformer with a learning rate of 5e -06 for 60 epochs during EEG-to-language alignment learning using AdamW (Loshchilov & Hutter, 2017). For the bridging stage, we use 8 virtual prefix and set the speculative augmentation factor K to 15 with a speculative ratio of 0.3. We use pre-trained BART and T5 models from the huggingface platform to initialize the Q-conformer and the LLM decoder. We also conducted experiments of massive size LLAMA2 model \u00b3 in Section 3.5. Due to the limitation of space, refer to Appendix C for more details."}, {"title": "3.3 TRANSLATION PERFORMANCE", "content": "Quantitative Results We show quantitative results in Table 1. Compared to previous methods, e.g., EEG-to-Text (Wang & Ji, 2022), Dewave (Duan et al., 2023), and BELT-1 (Zhou et al., 2023a) When only using EEG Encoder, We observe that the introduction of BPE-level contrastive learning bootstrapped a significant improvement (row 4 compared to row 5), achieving the SOTA EEG decoding BLEU-{1, 2, 3, 4} scores of 43.06, 25.57, 15.16, and 9.17, which outperform DeWave by 1.71, 1.42, 1.24, and 0.95. By further connecting with the LLM decoder, BELT-2 further achieves the BLEU-{1, 2, 3, 4} scores of 52.59, 36.32, 25.21, and 17.85, which brings additional 9.66, 10.96, 10.16, and 8.76 BLEU score improvements. The increase of the metrics is more significant for longer phrases (+162% for 4-gram and +99% for 3-gram) compared to the baseline EEG-to-Text method. Additionally, we present ablation results that analyze the influence of VQ and the BPE-CL within our model, revealing that the utilization of BPE-CL significantly contributes to the enhancement of performance. However, multitask training did not bring a significant improvement to the translation result, which is elaborated in the Appendix F."}, {"title": "Qualitative Evaluation", "content": "We showcase the generated text alongside the established approach from Wang & Ji (2022) in Table 2. We observe that BELT-2 generates more fluent sentences with greater grammatical coherence. Notably, our model adeptly captures subject-predicate relationships while other methods miss the subject and predicate. This is demonstrated by the accurate decoding of phrases like \"He was\u201d vs. \u201cHe is\", \"It just doesn't work\" vs. \"It just doesn't have\". Furthermore, for sentence structures involving quoted dates, such as \u201c(January 15, 1875 - January 15, 1945)\u201d vs. \u201c(December 25, 1876 - June 9, 1959)\u201d, were also consistently deciphered."}, {"title": "3.4 MULTI-TASK PERFORMANCE", "content": "Sentiment Classification As shown in Table 4, previous works need to train an LLM classifier using an external Stanford Sentiment Treebank dataset (around 11,000 sentences) (Socher et al., 2013) and a new EEG encoder due to poor performance when training directly on the ZoCo dataset (Row 1-3). In contrast, an EEG encoder incorporating external classifiers (row 4-7) demonstrated improved performance (Wang & Ji, 2022). Our proposed Q-Conformer Encoder, achieve the state-of-the-art sentiment classification accuracy of 74.62% on the ZuCo dataset. We also observe that our method could effectively leverage pretrained knowledge from the translation task to improve performance (row 8-9).\nSummarization We compare the summarization performance of the BELT-2 model with the EEG-to-Text model as the baseline. As shown in Table 3, the EEG-to-Text struggles to generate summarization while the proposed BELT-2 model exhibited better generative capacity, especially in longer phrases. Compared to using a newly initialized encoder (row 2), our BELT-2 exhibits a remarkable capacity to utilize the pretrained knowledge to increase the performance for the summarization task (row 3). Generally, it attains the BLEU-{1, 2, 3, 4} scores of 31.17, 15.7, 8.91, 5.09, outperforming the baseline method."}, {"title": "3.5 ABLATION STUDY", "content": "Bridging Q-Confomer Encoder with different LLMs Table 1 shows the result of bridging our Q-Conformer encoder with the T5 (Raffel et al., 2020). In Table 5, we conduct a comprehensive investigation of bridging LLM decoders with the Q-Conformer model, including the LLAMA2, T5, and the PEGASUS (Zhang et al., 2020) models. Results show that T5 LLMs consistently outperform other variants and boost the decoding performance. We attribute this superiority to T5's denoising training objectives. However, the sheer scale of the LLM decoder does not necessarily lead to enhanced decoding performance. For example, PEGASUS and LLAMA2 did not yield much improvement in the translation performance."}, {"title": "4 CONCLUSION", "content": "This paper introduces BELT-2, a pioneering EEG-language learning framework for bridging brain signals to LLMs. Our framework achieves EEG-to-language alignment by incorporating the novel BPE-CL objective and proposed an effective method for bridging a frozen Q-Conformer EEG Encoder and a frozen LLM to leverage their generative capacity. The multi-task extendibility of the Q-Conformer also establishes BELT-2 as the first work to achieve a multi-task decoding model in EEG research. Extensive experiments were conducted to evaluate the performance of BELT-2 quantitatively and qualitatively. Especially, this work provides the first study investigating the feasibility of using frozen pretrained LLM to process EEG contexts exampled by a wide range of LLMs. Our experimental result shows that the BELT-2 framework represents a significant step forward in integrating human brain signals with LLMs, opening up exciting new avenues for research and development in cognitive neuroscience and brain-computer interfaces. We hope that this work will inspire further exploration and innovation in this exciting and rapidly evolving field."}, {"title": "A RELATED WORKS", "content": "EEG decoding Prior brain studies demonstrated the potential to decode speech (Anumanchipalli et al., 2019) and language signals (Anumanchipalli et al., 2019) from the human brain using invasive neuro-sensors, but the risks make it impractical for most people. More recently, a surge of efforts was made to extract rich information from noninvasive brain signals through advanced representation learning techniques, opening the door to a wide array of innovative tasks based on brain signals, such as image reconstruction (Singh et al., 2023) and movement prediction (Zhou et al., 2023b). Nonetheless, Many of these efforts have limitations, including vocabulary size and decoding performance, hindering their suitability for complex practical scenarios. Our work focuses on open-vocabulary sentence decoding from noninvasive brain signals with fluent decoding performance and versatile multi-task adaptability, making it a promising solution for a diverse range of applications.\nEEG-Language representation alignment A crucial step for most cross-modality tasks is the acquisition of aligned multi-modal representations (Liu et al., 2023a; Mokady et al., 2021; Rombach et al., 2022). Achieving this involves an alignment step following the acquisition of unimodality pretrained models (Li et al., 2023). Yet, the formidable challenge persists due to the limited scale and sparse EEG dataset annotations, as we strive to create a semantically coherent and universally adaptable EEG encoder, akin to visual counterparts (Dosovitskiy et al., 2020; Radford et al., 2021).\nDiverging from the conventional fully-supervised paradigm, infusing natural language supervision enriches non-language modalities representation with semantics and zero-shot generalization (Desai & Johnson, 2021). Previous studies in unimodal vision tasks show that a large vision encoder, trained directly with language supervision, can match performance compared to learning from massive datasets (Joulin et al., 2016). Recent works incorporating language-guided learning also support the value of additional semantics for non-language representation generalization (Wang et al., 2023; Elizalde et al., 2023). Inspired by their successes, our work endeavors to bootstrap the learning of an Encoder that aligns EEG and language representation through natural language supervision."}, {"title": "B MATHEMATICAL SYMBOLS USED IN THIS PAPER", "content": "In Table 6 we show a list of mathematical symbols used in this paper."}, {"title": "C IMPLEMENTATION DETAILS", "content": "The Q-Conformer is implemented using the configuration detailed in Table 7. The detailed structures for the convolution module are shown in Table 8. We use the same Conformer block for the encoder and decoder, each with 2 Conformer blocks. We trained All models are trained on Nvidia A40 GPUs."}, {"title": "C.2 TRAINING DETAILS FOR EEG-TO-LANGUAGE ALIGNMENT LEARNING", "content": "To train the Q-Conformer during the EEG-to-language alignment learning, we use a weighted summation of all the following loss terms:\n \\(\\mathcal{L} = \\lambda_1 \\mathcal{L}_{vq} + \\lambda_2 \\mathcal{L}_{bpe} + \\lambda_4 \\mathcal{L}_{elm} + \\lambda_3 \\mathcal{L}_{neg},\\)\n\\(\\lambda_1\\) to \\(\\lambda_4\\) are coefficients for each loss term. We set \\(\\lambda_1\\) to \\(\\lambda_4\\) as [1, 10, 10, 0.001]. The main reason for such a setting is the aim to prioritize the learning of achieving EEG-to-language alignment and the training of the query prompt specific to the ELM task. To avoid collapse in training, we implemented the gradient normalization method to normalize the scale of the loss function and stabilize the training process."}, {"title": "C.3 TRAINING VIRTUAL PREFIX FOR BRIDGING Q-CONFORMER AND LLM", "content": "The prefix-tuning method used in our paper closely follows the implementation in Li & Liang (2021), the objective function (\\(\\mathcal{L}_{bridge}\\)) is defined as a modified loss function tailored to guide the selective of continuous virtual prefix prompts. We use \\(\\Theta\\) to denote the matrix that stores the virtual prefix. Using the machine translation loss \\(\\mathcal{L}_{tr}\\) as an example, the objective function can be expressed as:\n\\(\\mathcal{L}(\\Theta_{bridge}) = \\mathcal{L}_{tr}(\\mathcal{\\hat{S}}, S)\\)\nIn this example, the prefix prompts to learn properly describe the EEG-to-Langugage translation task to the subsequence frozen LLM, utilizing the generation capacity of the LLM models to improve translation performance."}, {"title": "C.4 TRAINING DETAILS FOR MULTI-TASK LEARNING", "content": "To extend our model to multi-task decoding, we simultaneously train the model in three EEG decoding tasks including translation, summary, and sentiment classification task. We randomly sample a task for each batch during the training epochs. The loss function for translation task \\(\\mathcal{L}_{tr}\\) and sentiment classification tasks \\(\\mathcal{L}_{st}\\) are illustrated in Equation 6 and Equation 7 respectively.\nFor learning the summary task, the loss function could be written as follows:\n\\(\\mathcal{L}_{sum} = - \\sum_{i=1}^{\\mathcal{|\\hat{S}|}} log p(s_i \\in \\mathcal{S})\\)"}, {"title": "D IMPROVED Q-CONFORMER EEG ENCODER", "content": "We observed a noteworthy trend when utilizing a relatively larger learning rate of 1e - 4, as opposed to the optimal learning rate of 5e \u2013 6 for the top-performing Q-Conformer Encoder, as indicated in Figure 8. This variance in learning rates led to a remarkable performance by the Q-Conformer Encoder on the training dataset, resulting in notably high BLEU Scores. Specifically, the BLEU-1 and BLEU-4 scores soared to remarkable levels, reaching 93.03 and 92.69 respectively. In stark contrast, the EEG-to-Text baseline method significantly lagged behind, registering only BLEU-1, 4 scores of 38.98 and 6.82 during our replicated training, highlighting the superior EEG encoding capabilities of the Q-Conformer Encoder.\nIt's also worth noting that the BLEU-1 performance of the Q-Conformer encoder experienced a decline from 42.43 to 35.48 during the testing phase, we interpret this as a minor setback. Such a reduction in performance can often be attributed to the challenges of generalization, which frequently happen in the context of training on a relatively small dataset.\nFurthermore, it's worth highlighting that within this setting, the Q-Conformer still achieved a testing BLEU-4 score of 9.3, surpassing the baseline EEG-to-Text method's training set BLEU-4 score. This outcome serves as a compelling testament to the enhanced encoding capacity conferred by our Q-Conformer Encoder."}, {"title": "E COMPARISON WITH BELT-2 WITHOUT BPE-LEVEL CONTRASIVE LEARNING", "content": "In Figure 9(a) and Figure 9(b), we present a comprehensive comparison of the learning curves and BLEU-1 curve of the baseline EEG-to-Text model (Cruttenden, 2014), the Q-Conformer encoder without applying the BPE-level contrastive learning (BELT-2 w/o BPE-CT) and the Q-Conformer encoder with BPE-level contrastive learning (BELT-2 w/ BPE-CT)g. The visualized learning curves include the BLEU-1 score and loss values for 30 epochs on the test split. Comparing the EEG-to-Text model and the BELT-2 model, it's evident that BELT-2 offers a significant reduction in loss values with or without BPE-level contrastive learning, indicating the proposed model architecture is more efficient in capturing EEG patterns. However, a notable observation arises after epoch 8. Without the BPE-contrastive learning (orange curves), the BLEU-1 score fluctuates and drops significantly. On the contrary, the introduction of BPE-level loss helps stabilize the model's performance, particularly on unseen EEG data. This highlights the substantial enhancement brought about by our proposed BPE-contrastive learning framework."}, {"title": "F MULTI-TASK TRAINING RESULTS", "content": "We show the performance of translation, summary, and sentiment classification on the test set during the multitask training learning phase of BELT-2 in Table 10. In Table 10(a), we can observe that without the use of pretrained weights, all tasks are learned from scratch. In this case, the translation BLEU-1 score starts from 4.06 BLEU-1 score and rises to only reaches 41.47 and the summarization BLEU-1 score reaches 28.72. Also, the sentiment classification accuracy gradually increased to 59%. However, the use of Q-Conformer pretrained on translation tasks could improve the training stability and performance of both the sentiment classification task and the summarization task. Due to the pretrained weights, we observed that in Table 10(b), the BLEU-1 score of the summarization performance and sentiment achieved 23.0 BLEU-1 score after the first training epoch. Then continued to increase to 31.17. The accuracy for sentiment classification also reaches 79.86% at its peak and stabilizes at around 74%. However, the performance of the translation task slightly decreased. This is an expected phenomenon in multi-task training. Nonetheless, this ethernet still shows the multi-task learning capacity and extensibility of our BELT-2 framework."}, {"title": "G GENERATED SUMMARIZATION RESULTS", "content": "We created the summarization dataset with the prompt \"Rewrite the sentence by summarizing its main idea using 8 words from the sentence and keep the summarized sentence similar to the original sentence: {s}\" where and {s} is the original sentence from the dataset. Table 9 showcases summary and prediction samples generated by the BELT-2 model. We could see those summary ground truths cover the key ideas of the original sentence and are within the maximum summarization word limit. On the training set, our BELT-2 model could learn and precisely generate a summary of the EEG signal, such as \"film with twists\" vs. \"film with twists.\". However, this summarization capacity did not generalize well on unseen test and validation data. We consider the lack of training data as one of the major reasons for this problem. Another reason is that our current model lacks higher-level"}, {"title": "H ABLATION EXPERIMENTS ON HYPER-PARAMETERS", "content": "We conducted an ablation study on different hyper-parameters including the learning rate, batch size, frequency of the inserted cross-attention layer in the context layer of the Q-Conformer, and the number of querying prompts. The evaluation metrics can be found in Figure 11. We observe that the introduction of BPE-contrastive learning consistently improves training stability and model performance in different hyper-parameter settings. This result shows that the learning performance of BELT-2's EEG encoder is not easily affected by the change of training parameters and is relatively easy to reproduce."}, {"title": "I AUGMENTATION EFFECT OF SPECULATIVE AUGMENTATION", "content": "The limitation of unique sentence from the training dataset also limits the diversity of the MLC context outputed by the Q-Conformer. The training set we used in our cross-sentence setting contains only 790 unique sentences as target for prefix-tuning when bridging Q-Conformer and LLM. For the Q-Conformer, predicts around 900 uniques MLC throughout the training dataset. This lack of training inputs makes the training for a good virtual prefix difficult. To solve this problem, our speculative augmentation method reuse cached MLC from the training stage of Q-Coformer. When using MLC from K = 15 checkpoints, we achieve a total of 5107 samples for prefix-tuning."}, {"title": "J EXTENSIVE EXAMPLES OF GENERATED TRANSLATION OUTPUTS", "content": "We provide extensive translation outputs from our BELT-2 model compared with the baseline EEG-to-Text model and the ground truth in Table 10. It shows that for some samples, the BELT-2 model still has insufficient performance, which indicates room for future improvements."}]}