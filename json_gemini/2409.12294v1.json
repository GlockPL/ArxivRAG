{"title": "RAG-Modulo: Solving Sequential Tasks using Experience, Critics, and Language Models", "authors": ["Abhinav Jain", "Chris Jermaine", "Vaibhav Unhelkar"], "abstract": "Large language models (LLMs) have recently emerged as promising tools for solving challenging robotic tasks, even in the presence of action and observation uncertainties. Recent LLM-based decision-making methods (also referred to as LLM-based agents), when paired with appropriate critics, have demonstrated potential in solving complex, long-horizon tasks with relatively few interactions. However, most existing LLM-based agents lack the ability to retain and learn from past interactions an essential trait of learning-based robotic systems. We propose RAG-Modulo, a framework that enhances LLM-based agents with a memory of past interactions and incorporates critics to evaluate the agents' decisions. The memory component allows the agent to automatically retrieve and incorporate relevant past experiences as in-context examples, providing context-aware feedback for more informed decision-making. Further by updating its memory, the agent improves its performance over time, thereby exhibiting learning. Through experiments in the challenging BabyAI and AlfWorld domains, we demonstrate significant improvements in task success rates and efficiency, showing that the proposed RAG-Modulo framework outperforms state-of-the-art baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Solving goal-driven sequential tasks is a core problem in robotics, with a wide array of challenges [1], [2], [3], [4], [5], [6]. Due to imperfect actuation, real-world robots operate in stochastic environments. Their sensors often provide only a partial view of the surroundings, requiring decision-making under partial observability and limited knowledge of the world model. To reduce the programming burden for end-users, even complex, long-horizon tasks are frequently defined by sparse reward functions or natural language descriptions of the robot's goal.\nVarious paradigms and corresponding methods have been explored to address this fundamental challenge [7], [8], [9], [10], [11]. The planning paradigm assumes access to a task model, which is often unavailable in real-world applications. While reinforcement learning can operate without a task model, it typically requires a prohibitively large number of exploratory interactions and significant manual effort for reward design. This challenge is further compounded in partially observable environments, where sparse rewards and safety concerns limit the feasibility of extensive exploration.\nTo complement these long-standing paradigms, language models have recently emerged as promising tools for solving"}, {"title": "II. PROBLEM FORMULATION", "content": "In this section, we formally model the tasks of interest and define the problem, followed by an explanation of how language models can be prompted to function as agents."}, {"title": "A. Task Model", "content": "We focus on object-centric, goal-driven sequential robotic tasks that may involve uncertainties in both actions and observations [29]. More specifically, we denote So as the set of all possible objects in the robot's environment and Sp as the set of object properties. We formally define the task model with the tuple (S, G, A, O, T, Rg, h, \u03b3). Given So and Sp, a state s \u2208 S is defined as an assignment of object properties. A is the set of low-level physical actions and G is the set of all goals. A goal g \u2208 G is the natural language description of the goal state. O is the set of observations retrieved from states via an observation function O : (S \u00d7 A) \u2194 0, and T: (S\u00d7A) \u2192 S is the transition function. Rg is the goal-conditioned reward function, which = 1 when goal is achieved, else 0. Finally, \u03b3 denotes the discount factor and h represents the task horizon.\nFollowing prior work [2], [16], the agent is also equipped with a set of high-level text actions, denoted by C. In reinforcement learning (RL) literature, these can be interpreted as macro actions or options [30], [31]. Each action c\u2208 C is composed of a function and its corresponding set of arguments, i.e., c = FUNCTION(ARGUMENT), such as OPEN(TYPE.DOOR, COLOR.RED). We assume that the robot can execute this high-level action by breaking it down into a sequence of primitive actions (a1,a2,...), governed by its low-level policy \u03c0\u03b5, until a termination condition Be is met. For the remainder of the paper, we simply refer to high-level actions as actions."}, {"title": "B. Problem Statement", "content": "We can now formally define the problem statement. Given the initial state, so, generate the shortest sequence of actions (C1, C2, ..., Ct) to reach the goal state described as g."}, {"title": "C. Language Models as Agents", "content": "As shown in recent works [16], [17], large language models (LLM) can be prompted at each time step to generate a sequence of actions using the following prompt:\nPROMPTt = Penv; {gk, ok, ck}}k=1; 9; {01:t\u22121, C1:t\u22121}; Ot\nwhere, at time-step t, the prompt consists of (i) fixed prefix Penv describing the environment; (ii) K in-context examples comprised of goal-observation-action tuples, {gk, ok, ck}=1; (iii) goal description g; (iv) the history of actions for previously visited states, {01:t-1, C1:t-1}; (v) the current observation, Ot. The in-context examples demonstrate how to solve similar tasks and the history of past interactions provides the language model with context about how the agent has interacted with the environment so far."}, {"title": "III. RELATED WORK", "content": "In this section, we discuss related methods that utilize language models as agents, use memory components and incorporate retrieval-augmented generation (RAG)."}, {"title": "Language Models as Agents.", "content": "Recent works have explored using language models as agents for solving long-horizon tasks by generating plans [14], [16], [17], [19], [18]. Approaches like ProgPrompt [16], [32] generate static plans offline, which may fail when encountering unforeseen object interactions in a partially observable environment. LLM-Planner-like approaches [17], [19], [18] offer a more online approach, allowing for plan updates if an action fails, but it does not store past successes and failures to guide future decisions. The method in [19] involves a human in the loop to prompt and verify. [18] generates feasible plans but relies on precise model dynamics estimation to assess plan feasibility. More recently, [26] have shown that language models should be coupled with verifiers or critics to generate sound plans. These recent methods have informed our work; however, in contrast to these works, RAG-Modulo stores and retrieves past interactions from memory to inform and improve decision-making."}, {"title": "Learning with Experience.", "content": "Reinforcement learning agents typically use a replay buffer to store experiences for policy optimization. However, solving complex long-horizon tasks often demands millions of trajectories or environment in-teractions to learn effectively [1]. In contrast, our approach requires only a few hundred experiences to enable meaningful learning. Very recently, some LLM-based approaches have introduced memory modules that store past experiences and expand as the agent interacts with the environment [33], [34], [35], [36], [37]. These methods store experiences at the skill level, retrieving them when needed, but lack the ability to track past successes and failures at the interaction level. Moreover, they often require multiple LLMs to reason, relabel"}, {"title": "RAG systems for Robotics.", "content": "Retrieval Augmented Gener-ation (RAG) systems enhance language model predictions by retrieving relevant information from external databases [38], [39]. For example, [40] employs RAG to collect exemplars for solving sub-tasks with web agents, while [41] retrieves driving experiences from a database for autonomous vehicle planning. In robotics, [42] explores retrieval for deep RL agents, but it does not use LLMs, limiting its adaptability and scalability. [43] employs a policy retriever to extract robotic policies from a large-scale policy memory. In contrast, our approach integrates a RAG system within an LLM-Modulo framework, where past interactions and feedback from critics is stored and continuously expanded. This enables the retrieval of interaction-level experiences, including mistakes and corrections, providing more detailed and context-aware guidance for sequential decision-making."}, {"title": "IV. PROPOSED APPROACH", "content": "We now describe RAG-Modulo, summarized in Alg. 1, which is composed of an LLM, a bank of critics, and an"}, {"title": "Algorithm 1 RAG-Modulo", "content": "1: INPUT: (g, h, LLM, M)\n2: t \u2190 1 \n3: M\u2190 {}  \u25b7 Initialize the time-step\n4: while t < h or (g is satisfied) do\n5:  Ot\u2190 Observe the environment\n6:  (Ik, ck) \u2190 Retrieve interactions from memory (Eq. 2)\n7:  PROMPT \u2190 Construct the prompt (Eq. 1)\n8:  Ct\u2190 LLM(PROMPTt)  \u25b7 Predict action\n9:  ft \u2190 CHECKFEASIBILITY (Ot, Ct)\n10:  if ft is SUCCESS then  \u25b7 Keep track of interactions\n11:  M \u2190 M\u222a {I\u2081 = (g, Ct\u22121, ft\u22121, 0t), Ct}\n12:  end if\n13: end while\n14: if g is satisfied then\n15:  M \u2190 MMUM  \u25b7 Update memory\n16: end if\n17: return (C1:t, M)\ninteraction memory (M) coupled with mechanisms for storing and retrieving interaction experience. At each step t of a task specified by natural language goal g and horizon h, RAG-Modulo first retrieves interactions I from the memory that are relevant to the task and current observation ot, using them to guide the LLM's decision-making (line 6 in Alg. 1). The LLM selects action ct based on this context and receives feedback (lines 8 9) from a bank of critics (Alg. 2). If feasible, the interaction is stored (lines 10-12). Once the"}, {"title": "Algorithm 2 CHECKFEASIBILITY", "content": "1: INPUT: Ot, Ct\n2: ft \u2190 SUCCESS, REASON \u2190 NONE\n3: try:\n4:  Parse ct using syntax  \u25b7 Syntax Critic\n5:  Parse Ct using semantics  \u25b7 Semantics Critic\n6:  repeat\n7:  Execute ct  \u25b7 Low-level Policy Critic\n8:  until Be is True\n9: except Exception as REASON:\n10: ft \u2190 FAILURE(REASON)\n11: return ft\ngoal is achieved, the interaction memory is updated for future retrieval (lines 14 \u2013 16), enabling learning from experience."}, {"title": "A. Critics and Feedback", "content": "Informed by the [26], RAG-Modulo includes a bank of critics (Psyntax, Ysemantics, Ylow-level) who provide feed-back on actions selected by the LLM. F denotes the set of feedbacks described in natural language. The syntax parser syntax: C \u2194 F returns feedback based on syntactical correctness. It ensures that the LLM's response adheres to the grammar rules of the environment. The semantics parser (semantics : (C \u00d7 O) \u2194 F returns feedback based on semantic correctness. It verifies that the predicted action is meaningful and logically consistent with the current observation o, e.g., ensuring the agent has the correct key before opening a door. The low-level policy critic low-level: (CO) \u2194 F checks if c is executable from o. It runs the execution using \u03c0\u03b5(0) until \u03b2\u03b5(0) is satisfied. For example, while traversing a path it can determine if an obstacle is encountered. As summarized in Alg. 2, each critic, either returns SUCCESS or FAILURE along with the corresponding REASON. This mimics how programmers receive feedback from compilers during debugging. We now formally define the overall feasibility feedback f\u2208 F as a function of the feedback from all critics:\nf = { SUCCESS, ifsyntaxsemanticslow\u2212level FAILURE(REASON), otherwise"}, {"title": "B. Interaction Memory, Storage and Retrieval", "content": "RAG-Modulo considers a database of past interactions representing the agent's memory M of solving prior tasks and their outcomes. We represent such interaction with the tuple (I, c), where I = (g, c\u22121, f, 0). Formally, the memory includes a set of interactions M = {(I\u00b9, c\u00b9), . . ., (Im,cm)}, where m represents the memory size.\nRetrieval. At every decision-making step of a given task, RAG-Modulo retrieves from the memory the top-K most relevant interactions {Ik,ck}K_1 ,ck}K_1 that resemble the current task and situation and uses them as in-context examples as shown in Fig. 2. Formally, this is represented as:\nI1:K = argmax\u043a cos(e(It), e(I)) IEM\nargmaxk returns the top-K samples from the memory that have the highest cosine similarity with It. e(I) represents the fixed-size embedding of I generated by the encoder model e. As detailed in Sec. V, we use OpenAI's TEXT-EMBEDDING-3-LARGE [44] as the encoder model e for realizing RAG-Modulo in our experiments.\nStorage. For every successfully completed task, {9, C1:h, f1:h, 01:h, C1:h)}, RAG-Modulo fills the memory with its interactions (It, ct) for which the current option ct is always feasible (i.e. f(ct) = SUCCESS). Thus, every stored tuple is a successful interaction that includes rectifications when ft-1 = FAILURE, which can be used by the LLM when planning future actions."}, {"title": "V. EXPERIMENTAL SETUP", "content": "We evaluate the performance of RAG-Modulo in AlfWorld [2] and BabyAI [1], [15] benchmarks, depicted in Fig. 3. These benchmarks include several features representative of challenges in robot decision-making, thereby making them suitable benchmarks for evaluating RAG-Modulo. For instance, both benchmarks include a suite of sequential tasks that need to be performed by situated agents. The"}, {"title": "Baselines.", "content": "We consider the following baselines for compar-ison, each using language models as high-level planners: (i) ProgPrompt [16] is a powerful static planner for robotic tasks that generates a complete plan at the start of a task and uses assertion checks to ground the plan to the current state. It is representative of LLM-based agents that do not involve memory or learning from experience. (ii) LLM-Planner [17] is a method that employs grounded re-planning, dynamically updating the plan throughout the task. It is a representative approach of more recent LLM-based agents that also utilize retrieval-augmented generation; however, in a different manner than that of RAG-Modulo. For each environment, all baselines have access to 100 training tasks with expert-provided demonstrations. We initialize the memory in RAG-Modulo using these expert demonstrations. We refer to the initial memory as prior experience, which is updated online based on experience of solving new tasks."}, {"title": "Metrics.", "content": "To measure the decision-making performance, we consider three evaluation metrics. (i) Success Rate (SR) measures the fraction of tasks that the planner completed successfully. (ii) Average In-Executability (InExec) is the average number of selected actions that cannot be executed in the environment. (iii) Average Episode Length (Len) is the average number of planning actions that are required to complete a given task. As ProgPrompt is an offline approach, (InExec) and (Len) metrics are not applicable for it."}, {"title": "VI. RESULTS AND DISCUSSION", "content": "How does RAG-Modulo compare against other LLM as Agents baselines? In Table I, we report comparison with the baselines. RAG-Modulo demonstrates a higher success rate than ProgPrompt across both domains. This can be attributed to ProgPrompt's lack of memory and critics, which means it does not benefit from interactive learning. By interacting with the environment and retrieving relevant experience, RAG-Modulo enables more informed decision-making.\nRAG-Modulo also outperforms LLM-Planner in terms of success rate, in-executability, and average episode length. Notably, the success rate improvements range from +0.33"}, {"title": "What is the optimal number of interactions K to use as in-context examples?", "content": "We ablate the number of interactions retrieved from memory and evaluate performance on BabyAl environments. The results reported in Fig. 4 show that the success rate improves as K increases, peaking at K = 5 for BossLevel and K = 10 for SynthLevel, before beginning to decline. Similarly, in Fig. 5 we observed that in-executability and average episode length decrease initially but start to rise as K continues to grow. The initial boost in performance can be attributed to the inclusion of more informative interactions, enhancing the LLM's decision-making capabilities. The subsequent decline likely stems from the LLM's sensitivity to irrelevant or noisy context [46], [47]. As K increases, the chance of introducing less relevant or low-quality interactions also rises, which can distract the model and degrade its output quality [48]. These trends suggest retrieving a modest number of interactions (between 5 and 10) while solving tasks using the RAG-Modulo framework."}, {"title": "How does the choice of retrieval function affect performance?", "content": "We examine how retrieving interactions at different levels of granularity impacts performance. Specifically, we compare against an ablation of our approach that utilizes a trajectory-level retrieval function. This ablation first identifies the most relevant task in the memory by computing the cosine similarity between goals, and then extract top K interactions from that task's trajectory. We represent the performance of this variable in the second row of Table II. We observe that retrieving at the interaction level generally yields better results, with lower in-executability and shorter episode lengths, while maintaining similar or higher success rates across both BabyAI domains. This suggests that retrieving interactions from a diverse set of tasks provides"}, {"title": "How does prior experience affect performance?", "content": "Lastly, in the third row of Table II, we report results of RAG-Modulo when it is not seeded with any prior expert-generated experience. Unsurprisingly, we find that prior experience generally helps in sequential decision-making. Interestingly, even starting our approach with an empty memory (third row, Table II) still outperforms the variant that does not include a memory component (fourth row, Table II), as the agent can gradually collect experiences of successes and failures, allowing it to learn and improve its decision-making."}, {"title": "VII. CONCLUSION", "content": "This paper introduces RAG-Modulo, a framework for solv-ing sequential decision-making tasks by providing LLM-based agents memory of past interactions. Extending the recent LLM-Modulo framework, RAG-Modulo not only incorporates critic feedback regarding the feasibility of generated actions but also enables agents to remember successes and mistakes and learn from them. RAG-Modulo demonstrates superior performance on the challenging BabyAI and AlfWorld benchmarks, achieving higher success rates while requiring fewer actions to complete sequential tasks.\nIn future work, we plan to utilize RAG-Modulo to solve tasks in other environments involving physical robots, such as FurnitureBench with the Panda robot [6]. We also see potential in integrating RAG-Modulo with existing continual learning frameworks, such as BOSS and Voyager [33], [34], to enable learning from experience at multiple layers of abstractions: namely, skills and interactions. Another avenue is to explore tunable retrieval models that can anticipate future needs to further enhance the agent's performance [49], [50]. Finally, we are interested in studying how RAG-Modulo can enhance end-user programming of complex robot behaviors by leveraging user commands, experience and critiques."}]}