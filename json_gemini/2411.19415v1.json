{"title": "AMO Sampler: Enhancing Text Rendering with Overshooting", "authors": ["Xixi Hu", "Keyang Xu", "Bo Liu", "Qiang Liu", "Hongliang Fei"], "abstract": "Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pre-trained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively control the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost.", "sections": [{"title": "1. Introduction", "content": "Recent advances in diffusion models [18, 19, 31-33] have enabled high-quality image and video generation. Text-to-image generation, where neural networks create images from natural language prompts, has emerged as a transformative application of AI. Despite significant progress, a key challenge remains in precisely aligning generated images with given text instructions, especially in text rendering tasks, where models often struggle to display specific text accurately. This misalignment results in errors like misspellings or incorrect wording (see Figure 1 for examples), limiting the models' utility in fields like graphic design, advertising, and assistive technologies [7, 30].\nAlthough fine-tuning with curated text data can improve text rendering [4, 5], it requires additional data collection and computationally expensive retraining, making it impractical for many applications. Furthermore, such fine-tuning may inadvertently compromise the model's overall image-generation capabilities. In this work, we investigate methods to enhance text rendering quality. We focus on rectified flow (RF) [19] models, which have emerged as a compelling alternative to conventional diffusion models due to their conceptual simplicity, ease of implementation, and improved generation quality [7]. Specifically, we introduce a lightweight, training-free sampling approach that significantly improves text rendering accuracy in generated images.\nWe introduce a novel and straightforward stochastic sampling approach on top of RF models, named the Overshooting sampler, that iteratively adds noise to the Euler sampler while preserving the marginal distribution. In particular, the Overshooting sampler alternates between over-simulating the learned ordinary differential equation (ODE) and re-introducing the noise (See Section 3). As we will show in Section 3.1, Overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from the successive applications of the Euler sampler, therefore enhancing the text generation quality. The overshooting strength is controlled by a hyper-parameter $c > 0$, corresponding to the magnitude of the Langevin step. When c is large, the Langevin term becomes inaccurate and can introduce error itself. To mitigate this problem, we propose a targeted use of the Overshooting sampler for text rendering, by adaptively controlling its strength on different patches of the image according to their attention scores with the text content in the prompt. We name the combined approach as Attention Modulated Overshooting sampler (AMO).\nWe validate the AMO sampler on state-of-the-art RF-based text-to-image models, including SD3, Flux, and AuraFlow. Our experiments demonstrate a significant improvement in text rendering accuracy, with correct text generation rates increasing by 32.3% on SD3 and 35.9% on Flux, without compromising overall image quality."}, {"title": "2. Background on Rectified Flow", "content": "This section provides a brief introduction to Rectified Flow (RF) [19]. RF seeks to learn a mapping from an easy-to-sample initial distribution $X \\sim \\pi_0$, which we assume to be the standard Gaussian $\\mathcal{N}(0, I)$, to a target data distribution $X_1 \\sim \\pi_1$. This is achieved by learning a velocity field $v$ that minimizes the following objective:\n$\\min_{v} \\mathbb{E}_{(X_0, X_1)\\sim \\pi_0, X_1 \\sim \\pi_1} \\int_{0}^{1} ||v(x_t,t) - \\dot{X_t}||^2 dt$.\nIn RF, $X_t$ is defined as a time-differentiable interpolation between $X_0$ and $X_1$, i.e., $X_t = tX_1 + (1 - t) X_0$ and $\\dot{X_t} = X_1 - X_0$. Once $v$ is learned, it induces an ordinary differential equation (ODE):\n$\\frac{d}{dt} Z_t = v(Z_t,t), \\forall t \\in [0,1], Z_0 = X_0$.\nIt can be shown that $Z_t$ and $X_t$ share the same marginal law [19] if $v$ is learned well, and therefore simulating this ODE with $Z_0 = X_0$ results in $Z_1$ being samples from the target distribution $\\pi_1$. In practice, this ODE can be discretized using the Euler method by selecting $N$ time steps $t_0 = 0 < t_1 < \\dots < t_N = 1$, and iteratively updating:\n$Z_{t_{k+1}} = Z_{t_k} + (t_{k+1} - t_k)v(Z_{t_k},t_k), Z_0 \\sim \\pi_0$.\nWe use $\\tilde{Z}$ to differentiate the discretized ODE trajectory from its ideal continuous time limit $Z$. Note the entire process is deterministic once $Z_0$ is chosen."}, {"title": "3. Attention Modulated Overshooting Sampler", "content": "In this section, we derive the Overshooting sampler that adds stochastic noise to the Euler sampler while preserving the marginal distribution (Section 3.1). Then we illustrate this is equivalent to adding a Langevin dynamics term at each Euler step (Section 3.2). Importantly, the extra Langevin term can help correct the compounding error from successive Euler steps. When the overshooting strength is high, the stochastic sampler can introduce artifacts. To mitigate this problem, we propose a straightforward attention modulation method that adaptively controls the strength of the overshooting for each image patch based on its attention score with the text content (Section 3.3)."}, {"title": "3.1. Stochastic Sampling via Overshooting", "content": "This section provides a derivation of a stochastic sampling method, Overshooting sampler, for RF-trained models. The main idea is to overshoot the forward Euler step and subsequently compensate with backward noise injection. In the limit of small step sizes, this process converges to a stochastic differential equation (SDE), and we will show rigorously in the subsequent section that the resulting SDE ensures the marginal preserving property according to the Fokker-Planck equation.\nFollowing our notation in the previous section, let $\\tilde{Z_0} = X_0 \\sim \\pi_0$ be a sample from the initial noise distribution, and assume we have obtained $\\tilde{Z_t}$ at time $t$, and want to get a $\\tilde{Z_s}$ for the next time point $s = t + \\epsilon$, where $\\epsilon > 0$ is the step size when denoising. Note that for standard Euler sampler, $Z_s$ is obtained from $Z_t + \\epsilon v(\\tilde{Z_t}, t)$. In comparison, to introduce stochastic noise, we propose the overshooting sampler which consists of the following two steps (See Figure 2):\n1. ODE Overshooting First, we temporarily advance $\\tilde{Z_t}$ to $\\tilde{Z_{\\omicron}}$ where $\\omicron = s + c \\epsilon$ (with $c > 0$) denotes an overshooting time point that is larger than $s$. Specifically, we conduct the following the forward Euler step:\n$\\tilde{Z_{\\omicron}} = \\tilde{Z_t} + v(\\tilde{Z_t}, t)(\\omicron - t) = \\tilde{Z_t} + (1 + c)\\epsilon v(\\tilde{Z_t}, t)$.\nWe use $\\tilde{Z_{\\omicron}}$ to emphasize that it is reached via overshooting.\n2. Noise Compensation Next, we want to revert from time $\\omicron$ to time $s$ by noising $\\tilde{Z_{\\omicron}}$. Assume we achieve this by computing\n$\\tilde{Z_s} = a \\tilde{Z_{\\omicron}} + b \\xi, \\xi \\sim \\mathcal{N}(0, I)$.\nThen, the goal is to determine the coefficients $a$ and $b$ here. If we assume the overshooting step is accurate, then $\\tilde{Z_{\\omicron}} \\overset{\\text{Law}}{\\approx} \\omicron X_1 + (1 - \\omicron) X_0$, where $\\overset{\\text{Law}}{=}$ denotes equality in distribution. Therefore,\n$\\tilde{Z_s} \\overset{\\text{Law}}{\\approx} a(\\omicron X_1 + (1 - \\omicron) X_0) + b \\xi$\\\n        $= a \\omicron X_1 + \\sqrt{a^2(1 - \\omicron)^2 + b^2} \\xi'$,\nwhere $\\xi' \\sim \\mathcal{N}(0, I)$ and the last line is derived as both $\\xi$ and $X_0$ are i.i.d Gaussian noises. On the other hand, we know that $Z_s \\overset{\\text{Law}}{\\approx} s X_1 + (1 - s) X_s$. Hence, by matching the coefficients, we get\n$a = \\frac{s}{\\omicron}, b = \\sqrt{(1 - s)^2 - \\frac{s^2 (1 - \\omicron)^2}{\\omicron^2}}$.\nRecall that $s = t + \\epsilon$ and $\\omicron = t + (1 + c)\\epsilon$, as $\\epsilon \\to 0$, the above process (steps 1 and 2) will approach the following limiting stochastic different equation (SDE) (See Appendix A.1 for the derivation):\ndZt=((1+c)v(Zi,t)\u2212Zi)dt+2(1\u2212t)ctdWt,$\\mathrm{d} Z_{t}=\\left(\\frac{2(1-t) c}{t}\\right)^{\\frac{1}{2}} \\mathrm{d} W_{t}$,\nwhere $W_t$ denotes the Brownian motion."}, {"title": "3.2. Overshooting \u2248 Euler + Langevin Dynamics", "content": "Equation (5) can also be derived directly from the Fokker-Planck equation following similar ideas from [33]. Let $dZ_t = f_t(Z_t)dt + \\sigma_t dW_t$ be a SDE where $\\sigma_t \\geq 0$ is a diffusion coefficient independent of $X_t$. Denote by $p_t$ the density of $Z_t$. According to the Fokker-Planck equation:\n$\\dot{p_t} = -\\nabla \\cdot (f_t p_t) + \\frac{1}{2} \\nabla^2 (\\sigma_t^2 p_t)$,\n$\\dot{p_t} = -\\nabla \\cdot \\left(\\left(f_t - \\frac{1}{2} \\nabla \\log p_t\\right) p_t \\right)$.\nThis is true in almost all contemporary diffusion/flow models."}, {"title": "3.3. Attention Modulation", "content": "In practice, while increasing $c$ can enhance text rendering quality, it may also introduce artifacts (see Figure 6 for examples). This is because, with larger values of $c$, the single-step Langevin correction in Equation (8) becomes less accurate. To address this issue, we propose dynamically adjusting the overshooting strength for different image patches based on their attention scores to the text content in the prompt. Simply put, this approach increases overshooting for areas related to the text while applying less to the rest of the image.\nMore concretely, assume the image consists of $h \\times w$ patches, where $h$ and $w$ denote the height and width dimensions of the 2D image tokens (e.g., 64 \u00d7 64 in our experiment). Let $x_{\\text{image}}^{h,w}$ be the $(h, w)$-th image patch token. Let $\\{x_{\\text{text}}^i\\}_{i=1}^{n}$ denote the set of tokens within the language instruction prompt for generating text and $n$ is the total number of text-related tokens (e.g., $x_{\\text{text}}^i$ can be the 'Tokyo Halloween Night' in the prompt \u201cA poster design with a title text of 'Tokyo Halloween Night'). Then, we construct a mask $m \\in \\mathbb{R}^{h\\times w}$ in the following way:\n$m_{h,w} = \\frac{\\exp(Q(x_{\\text{text}}) \\top K(x_{\\text{image}}^{h,w}))}{\\sum_{h',w'} \\exp(Q(x_{\\text{text}}) \\top K(x_{\\text{image}}^{h',w'}))}$,\nwhere $Q$ and $K$ denote the query and key vectors in attention. We then average the attention map over different layers and heads and rescale its values between 0 and 1. After that, we apply the resulting attention map, $m$, to give different image patches different amounts of overshooting. Specifically, assume $\\omicron \\in [0, 1]^{h\\times w}$, where $\\omicron_{h,w} = s + c \\epsilon m_{h,w}$, we have\n$\\tilde{Z_{\\omicron}} = \\tilde{Z_t} + v(\\tilde{Z_t}, t) \\omicron (\\omicron - t) = \\tilde{Z_t} + \\epsilon (1 + c m) \\omicron v(\\tilde{Z_t}, t)$,"}, {"title": "4. Related Work", "content": "This section gives an overview of diffusion and flow-based generative models, followed by a discussion on deterministic and stochastic sampling techniques, and recent advances in enhancing text rendering in text-to-image generation.\nDiffusion and Flow Models. Diffusion models [12, 32, 33] have emerged as powerful generative frameworks capable of producing high-fidelity data, including images, videos, audio, and point clouds [2, 3, 6, 13, 26, 30]. These models add noise to data in a forward process, then learn to reverse this noise to generate new samples, thereby modeling the data distribution through a progressive denoising process. Recently, Rectified Flow (RF) [1, 10, 18, 21]-also known as Flow Matching, InterFlow, and IADB-has been proposed as a novel approach that leverages an ordinary differential equation (ODE) with deterministic sampling during inference. RF simplifies the diffusion and denoising process, offering computational efficiency while maintaining high-quality generation, and positioning itself as a compelling alternative to traditional diffusion models. The rectified flow model has proven successful in various applications, including image generation[7], sound generation [9, 16] and video generation [27].\nDeterministic and Stochastic Sampling Methods. Sampling strategies in generative modeling are crucial as they influence the quality, diversity, and efficiency of generated samples. Deterministic sampling methods, such as those based on ODE solvers [14, 23, 31, 33], provide computational efficiency and stability but may lead to poorer output quality [14]. On the other hand, stochastic sampling methods introduce randomness into the sampling process, offering an alternative approach with added variability in intermediate steps. Meng et al. [25] introduced an N-step stochastic sampling method for distilled diffusion models, where noise is added at intermediate steps to achieve efficient sampling with as few as 2-4 steps. This approach provides an alternative to deterministic sampling, allowing the model to produce high-quality samples. Karras et al. [14] proposed a hybrid stochastic sampling technique that combines deterministic ODE steps with noise injection. In their method, noise is temporarily added at each step to improve sampling quality, followed by an ODE backward step to maintain the correct distribution. This hybrid approach results in better output quality compared to purely deterministic sampling methods. However, these existing methods are specifically designed for diffusion models. In contrast, we propose a stochastic sampler for rectified flow, providing an alternative solution to the traditional Euler method.\nEnhancing Text Rendering in T2I Generation. Accurate text rendering in text-to-image (T2I) generation models remains a significant challenge, as models often struggle to produce text within images that precisely matches the prompts, leading to incoherent or incorrect textual content. Classifier-Free Guidance (CFG) [11] can alleviate this issue by adjusting the influence of the text prompt during sampling, effectively balancing prompt guidance and the diversity of the generated content. Scaling or enlarging the text encoder has been shown to benefit text rendering[26]. Additionally, using a T5 text encoder significantly improves text rendering performance [7, 30]. Specialized fine-tuning approaches have also been explored, where pretrained text-to-image models are adapted with architectures designed specifically for text rendering tasks [4, 5, 20, 22, 24, 29, 34, 35]. These methods enhance the model's ability to generate accurate textual content but typically require extensive retraining or fine-tuning, which can be computationally intensive. In contrast, our work introduces a training-free approach that enhances text rendering during inference. By incorporating stochastic sampling and an attention mechanism into the Rectified Flow framework, we improve text rendering quality without modifying the underlying model or incurring additional training costs."}, {"title": "5. Experiment", "content": "We conduct experiments with several open-source text-to-image models based on Rectified Flow, including Stable Diffusion 3 (medium) [7], Flux (dev) [15], and AuraFlow [8]. Image generation was performed using the NVIDIA A40 GPU during inference. To ensure high-quality visual assessment, all output images were generated at a resolution of 1024x1024 pixels. Detailed model configurations and hyperparameter settings can be found in the Appendix A.3.\nEvaluation Metrics. We use a combination of automated and human evaluations to assess the performance of our models. For automated evaluation, we adopt benchmarks including DrawTextCreative [20], ChineseDrawText [24] and TMDBEval500 [5], which comprises a total of 893 prompts drawn from various data sources. To assess the correctness of rendered text, we compute OCR Accuracy and OCR F-measure using a pre-trained Mask TextSpotter v3 model [17]. We evaluate the samples' visual-textual alignment using CLIP Score, specifically CLIP L/14 [28], and also compute FID for overall visual quality between the CLIP image features and validation set images."}, {"title": "5.1. Comparison with Euler Sampler", "content": "In this section, we compare AMO against the Euler sampler both quantitatively and qualitatively. We found that AMO significantly outperforms Euler on text rendering without sacrificing overall visual quality.\nQuantitative Results As shown in Table 2, AMO achieves 82.5% accuracy on Flux model in human evaluation, notably surpassing the 74% accuracy of the standard Euler sampler. In addition, AMO improves the OCR metrics across all three text-to-image models, demonstrating a substantial enhancement in the model's ability to render text accurately. Furthermore, compared to the standard Euler method, AMO yields better FID scores, indicating superior overall image quality. As shown in Figure 5, we evaluated the performance of AMO using 20, 50, and 100 steps. Our results demonstrate that AMO consistently outperforms the deterministic sampler across all step counts, with a better performance improvement in the low-step scenarios.\nQualitative Results. Figure 4 presents a visual comparison between AMO and the standard Euler sampling applied to Flux, Stable Diffusion 3, and AuraFlow. Images generated"}, {"title": "5.2. Ablation Studies", "content": "In this section, we conduct a detailed ablation study to analyze the impact of different components in AMO on text rendering accuracy and image quality using the Flux model.\nImpact of Components in AMO. The AMO sampler essentially consists of three parts: ODE overshooting (O), noise compensation (N), and attention modulation (A). We ablate their individual contribution, by comparing AMO without all of them (X, X, X) (i.e., the Euler sampler), with only overshooting (\u2713, X, X), without attention modulation (\u2713, \u2713, X) and the full AMO. Results are summarized in Table 3. It is observed that both overshooting and noise compensation are crucial for achieving accurate text rendering and high image quality. Notably, introducing overshooting alone results in a 0% correct rate, as the marginal law is not preserved. The full AMO results in a similar performance to the Overshooting sampler (AMO without attention modulation), we think this is because metrics like OCR-F do not capture the image generation quality well. Therefore, we provide a visualization of samples from the Overshooting sampler against those from AMO in Figure 6. We observe that Overshooting without attention modulation can result in an over-smoothing effect, making the generated samples lose high-frequency details (See the parrot feather and smoke). This confirms the necessity of attention modulation."}, {"title": "5.3. Comparison with Finetuned T2I models", "content": "In this section, we evaluate both text rendering capability and overall quality for two image generation model families for text rendering: 1) General-purpose T2I models trained on extensive image datasets using rectified flow, such as Stable Diffusion 3, Flux and AuraFlow; and 2) Task-specific T2I models explicitly trained on datasets of ground-truth written text, such as GlyphControl [22] and TextDiffuser [5].\nSpecifically, we conducted a human evaluation study to assess image generation and text rendering quality across"}, {"title": "6. Conclusion and Limitation", "content": "Accurate text rendering has been a critical challenge in text-to-image generation. Existing solutions, such as specialized fine-tuning or scaling up the text encoder, often require modifications to the training process, which can be computationally expensive and time-consuming. This work introduces a training-free method, the Overshooting sampler sampler, that enhances text rendering by strategically incorporating curated randomness into the sampling process. Importantly, Overshooting sampler significantly improves text rendering accuracy with almost no overhead compared to the vanilla Euler sampler. In particular, Overshooting sampler alternates between overshooting the learned ODE and re-introducing noise, while ensuring the marginal laws are well-preserved. In addition, we introduce an attention modulation that quantitatively controls the degree of overshooting, concentrating on text regions within the image while minimizing interference with other areas. We validate AMO on popular open-source text-to-image flow models, including Stable Diffusion 3, Flux, and AuraFlow. Results demonstrate that AMO consistently outperforms baseline methods in text rendering accuracy without degrading the image quality of the pretrained model.\nOne limitation of this work is the lack of systematic evaluation of overshooting's impact on specific aesthetics or its applicability beyond image generation. Future work could explore extending AMO to other domains."}, {"title": "A. Appendix", "content": "A.1. The SDE Limit of the Overshooting Sampler\nIn this section, we derive the asymptotic limit of the overshooting sampler's update as a stochastic differential equation (SDE) by considering the infinitesimal step size $\\epsilon \\to 0$ in the definitions of $s$ and $\\omicron$. Recall that\ns = t + \\epsilon, \\quad \\omicron = s + c \\epsilon = t + (1 + c)\\epsilon,\nwhere $c$ is a constant parameter. Combining the update equations (see Equation (1) and Equation (2)), we obtain\nZ_s = a \\tilde{Z_{\\omicron}} + b \\xi\n= a \\tilde{Z_t} + a(\\omicron - t)v(\\tilde{Z_t}, t) + b \\xi\n= \\tilde{Z_t} + (a - 1)\\tilde{Z_t} + a(\\omicron - t)v(\\tilde{Z_t}, t) + b \\xi,\nDrift\nDiffusion\nWe aim to express the update in the form\n\\tilde{Z_{t+\\epsilon}} \\approx \\tilde{Z_t} + v_{\\text{adj}} (\\tilde{Z_t}, t) \\epsilon + \\sigma_t \\sqrt{\\epsilon} \\xi_t,\nwhich corresponds to the Euler-Maruyama discretization of the SDE\ndZ_t = v_{\\text{adj}} (Z_t, t) dt + \\sigma_t dW_t,\nwith $W_t$ denoting a standard Wiener process. To this end, we perform a first-order Taylor expansion assuming $\\epsilon \\to 0$: First, we compute $a - 1$:\na - 1 = \\frac{s}{\\omicron} - 1 = \\frac{s - \\omicron}{\\omicron} = \\frac{-\\epsilon c}{\\omicron} \\approx \\frac{-\\epsilon c}{t},\nwhere we use the approximation $\\omicron \\approx t$ for small $\\epsilon$. Next, we compute $a(\\omicron - t)$:\na(\\omicron - t) = \\frac{s}{\\omicron} (\\omicron - t) = \\frac{t + \\epsilon}{t + (1 + c)\\epsilon} c \\epsilon \\approx (1 + c) \\epsilon,\nNow, we compute $b^2$:\nb^2 = (1 - s)^2 - \\frac{s^2}{\\omicron^2} (1 - \\omicron)^2\n= (1 - s)^2 \\left(1 - \\frac{s^2}{\\omicron^2} \\left(\\frac{1 - \\omicron}{1 - s}\\right)^2\\right)\n= s^2 (f(s) - f(\\omicron)),\nwhere $f(x) = \\left(\\frac{1 - x}{x}\\right)^2$. Using a first-order Taylor expansion of $f(x)$ around $x = s$, we have\nf(\\omicron) \\approx f(s) + f'(s)(\\omicron - s)\n= f(s) + f'(s) c \\epsilon\n= \\frac{1 - t}{t} c \\epsilon,\nCombining the above results, the update equation becomes\n\\tilde{Z_{t+\\epsilon}} \\approx \\tilde{Z_t} + v_{\\text{adj}} (\\tilde{Z_t}, t) \\epsilon + \\sigma_t \\sqrt{\\epsilon} \\xi_t,\nwhere the adjusted velocity is\nv_{\\text{adj}} (\\tilde{Z_t}, t) = \\frac{(a - 1)}{\\epsilon} \\tilde{Z_t} + \\frac{a(\\omicron - t)}{\\epsilon} v(\\tilde{Z_t}, t)\n= \\frac{-c}{t} \\tilde{Z_t} + (1 + c) v(\\tilde{Z_t}, t)."}, {"title": "A.2. Stochastic Sampler by Fokker Planck Equation", "content": "As mentioned in Section 3.2, according to the Fokker-Planck Equation, for an ODE dZt = v(Zt, t)dt, we can construct a family of SDEs that share the same marginal law as the ODE at all t:\n$\\mathrm{d} Z_{t}=\\left(v\\left(Z_{t}, t\\right)+\\frac{\\sigma_{t}^{2}}{2} \\nabla \\log p_{t}\\left(Z_{t}\\right)\\right) \\mathrm{d} t+\\sigma_{t} \\mathrm{d} W_{t}$.\nNow, we only need to figure out $\\nabla \\log p_t (Z_t)$ and then we can find the corresponding $\\sigma_t$ that matches with the limiting SDE of the overshooting algorithm. To this end, we present the next two lemmas before presenting the equivalence.\nLemma A.1. Assume random variables $X = Y + Z$, where $Y$ and $Z$ are independent, then\n$\\nabla_x \\log p_X(x) = \\mathbb{E}[\\nabla_y \\log p_Y(Y) \\mid X = x] = \\mathbb{E}[\\nabla_z \\log p_Z(Z) \\mid X = x]$,\nwhere $p_Z$ and $p_Y$ are the density functions of $Z$ and $Y$, respectively.\nProof.\n$\\nabla_x \\log p_X(x) = \\frac{\\nabla_x p_X(x)}{p_X(x)}$\n= \\frac{\\nabla_x \\int_z p_{X,Z}(x, z)dz}{p_X(x)}\n= \\frac{\\int_z p_Z(z) \\nabla_x p_Y(x - z) dz}{p_X(x)}\n= \\frac{\\int_z \\frac{p_Y(x - z)}{p_Y(x-z)} \\nabla_x p_Y(x - z) p_Z(z) dz}{p_X(x)} \\qquad \\text{// Y and Z are independent}\n= \\frac{\\int_z \\nabla_x \\log p_Y(x - z) p_Z(z) p_Y(x - z) dz}{p_X(x)}\n= \\mathbb{E}[\\nabla_x \\log p_Y(X - Z) \\mid X = x]\n= \\mathbb{E}[\\nabla_Y \\log p_Y(Y) \\mid X = x].\nLemma A.2. Given the linear interpolation in Rectified Flow $X_t = t X_1 + (1 - t) X_0$, where $X_0 \\sim \\mathcal{N}(0, I)$, we have\n$\\nabla_x \\log p_t(x) = \\frac{t v(x, t) - x}{1 - t}$.\nProof. As $X_0$ and $X_1$ are independent since $X_0$ is the standard multivariant Gaussian and $X_1$ is the data distribution, take $Y = t X_1$ and $Z = (1 - t) X_0$. According to Lemma A.1, we have\n$\\nabla_x \\log p_t(x) = \\mathbb{E}[\\nabla_Z \\log p_Z(Z) \\mid X_t = x]$\n= \\frac{1}{(1 - t)^2} (1 - t) \\mathbb{E}[Z \\mid X_t = x]\n// $Z \\sim \\mathcal{N}(0, (1 - t)^2 I)$\n= - \\frac{1}{1 - t} \\mathbb{E}[X_0 \\mid X_t = x]\n// Z = (1 - t) X_0\n= \\frac{1}{1 - t} [t(X_1 - X_0) - X_t]  // X_t = t X_1 + (1 - t) X_0\n= \\frac{1}{1 - t} \\frac{t(v(x, t) - (x - \\mathbb{E}[X_0 \\mid X = x])) \\mid X_t = x]}{1 - t} // \\mathbb{E}[X_1 - X_0 \\mid X = x] = v(x, t)"}]}