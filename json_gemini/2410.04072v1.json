{"title": "MROSS: Multi-Round Region-Based Optimization for Scene Sketching", "authors": ["Yiqi Liang", "Ying Liu", "Dandan Long", "Ruihui Li"], "abstract": "Scene sketching is to convert a scene into a simplified, abstract representation that captures the essential elements and composition of the original scene. It requires semantic understanding of the scene and consideration of different regions within the scene. Since scenes often contain diverse visual information across various regions, such as foreground objects, background elements, and spatial divisions, dealing with these different regions poses unique difficulties. In this paper, we define a sketch as some sets of B\u00e9zier curves. We optimize the different regions of input scene in multiple rounds. In each round of optimization, strokes sampled from the next region can seamlessly be integrated into the sketch generated in the previous round of optimization. We propose additional stroke initialization method to ensure the integrity of the scene and the convergence of optimization. A novel CLIP-Based Semantic loss and a VGG-Based Feature loss are utilized to guide our multi-round optimization. Extensive experimental results on the quality and quantity of the generated sketches confirm the effectiveness of our method.", "sections": [{"title": "Introduction", "content": "Scene sketching refers to the process of creating rough sketches or drawings to visually represent a scene or environment. It is commonly used in various fields, including art, design, architecture, film, and animation.\nScene sketching offers the advantage of conveying information quickly and efficiently. It provides a concise visual summary that allows viewers to grasp the overall layout, spatial relationships, and key features at a glance. Additionally, scene sketching facilitates the creative process by enabling artists and designers to visualize their ideas and concepts.\nHowever, generating a scene sketch is highly challenging, as it requires the ability to understand and depict the visual characteristics of the scene with complex subjects and interactions. (Azadi et al. 2018; Chen, Lai, and Liu 2018; Isola et al. 2017; Li and Wand 2016) often rely on explicit sketch datasets for training. The sketches of them are often simplified and abstract expressions of the original images, with a fixed style or preset. It is difficult to balance visual effect of sketches, producing visual appeal and aesthetics.\nBesides, different regions within a scene may have varying levels of importance or prominence. For example, foreground objects or focal points might require more attention to detail and precision in sketching, while background elements may be more loosely represented. Some works can achieve this with flexibility. However, these works focus specifically on the task of object sketching (Muhammad et al. 2018; Vinker et al. 2022b) or portrait sketching (Berger et al. 2013), and often simply use the number of strokes to define the effect of their sketches.\nTo address the above two issues, we introduce a scene sketching method based on regions with multi-round optimization. We utilize the black parametric B\u00e9zier curves as our fundamental shape primitive for strokes of a sketch and optimized them by a pre-trained CLIP-ViT model (Radford et al. 2021; Dosovitskiy et al. 2020) and VGG16 model (Simonyan and Zisserman 2014).\nUnlike (Frans, Soros, and Witkowski 2022; Schaldenbrand, Liu, and Oh 2022), optimization is performed for different regions in our method, which helps highlight regions of interest, achieving coarse-to-concrete sketches. An intuitive and succinct learning process can be seen in Figure 1). In each round of optimization, we are in pursuit of full content exploration rather than only the salient guidance. To achieve this, we present an edge-based stroke initialization and utilize a farthest point sampling (FPS) algorithm to uniformly sample strokes in the input image. Figure 5 shows that our sampling method has a more efficient effect and is able to generate sketches in a reasonable manner. To transform a detailed scene into a simplified sketch, one must condense intricate visual elements into fundamental lines, shapes, and tones, all while retaining the scene's identifiable characteristics. We utilize intermediate layer of CLIP-ViT (Dosovitskiy et al. 2020) to guide the optimization, where encourages the creation of looser sketches that emphasize the scene's semantics. We introduce a VGG16 (Simonyan and Zisserman 2014) model to promote the visual consistency and similarity between a sketch and an image.\nWe evaluate our proposed method for various photographs, including people, nature, indoor, animals et al., to showcase the effectiveness of our method. Our main contributions in this work can be summarized as follows:\n\u2022 We propose a novel method to convert a scene photograph into a sketch by optimizing it in a region-by-region"}, {"title": "Related Work", "content": "Photo-Sketch Synthesis. Traditional photo-sketch synthesis methods are mainly based on image processing techniques, such as convolution, filtering, and grayscale, etc., to obtain corresponding sketches style pictures. However, the generated sketch of these methods are of low quality. Researchers investigated approaches that related to the deep learning, considering photo-sketch as a cross domain image-to-image translation problem. (Muhammad et al. 2018) trained a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. (Li et al. 2019) proposed a learning-based method to resolve the diversity in the annotation of datasets. Pinz (Kampelmuhler and Pinz 2020) employed a fully convolutional encoder-decoder structure to accomplish a mapping from image space to sketch space.\nYet, recently, CLIPasso (Vinker et al. 2022b) proposed a novel object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. They defined a sketch as a set of B\u00e9zier curves and extracted the salient regions of the input image to define the initial locations of the strokes. In contrast to CLIPasso (Vinker et al. 2022b), our method is not restricted to objects and can handle the challenging task like scene sketching. Additionally, while they only utilized a single form of optimization, we disentangle optimization into multiple rounds controlling the fidelity of different region in the input image, which allows for a wider range of editing and manipulation.\nVector Graphics. Vector representations are widely used for a variety of sketching tasks and applications, combining with a number of deep learning models including RNN (Ha and Eck 2017), CNNs (Chen et al. 2017), BERT (Lin et al. 2020), Transformers (Bhunia et al. 2020; Ribeiro et al. 2020), GANs (Balasubramanian, Balasubramanian et al. 2019) and reinforcement learning algorithms (Ganin et al. 2018; Mellor et al. 2019; Zhou et al. 2018). While traditional image generation methods operating over vector images require a vector-based dataset, (Li et al. 2020; Mihai and Hare 2021) had shown its possible to manipulate or synthesize vector content by using raster-based loss functions bypassing this limitation (that is, the process of actually drawing the vectors into an image is not part of the learning machinery). Among them, we consider to use the method of (Li et al. 2020), as it can flexibly handle a wide range of curves and strokes, including B\u00e9zier curves.\nCLIP-Based Image Vectorization. (Radford et al. 2021) proposed CLIP, which is a neural network trained on 400 million image-text pairs collected from the internet with the objective of creating a joint latent space. Being trained on a wide variety of image domains along with lingual concepts, CLIP models are found to be very useful for a wide range of zero-shot tasks, and have enabled a number of successful methods for drawings, such as CLIPDraw (Frans, Soros, and Witkowski 2022), StyleCLIPDraw (Schaldenbrand, Liu, and Oh 2022), CLIP-CLOP (Mirowski et al. 2022), and Clipascene (Vinker et al. 2022a). (Tian and Ha 2022) employed evolutionary algorithms combined with CLIP, to produce"}, {"title": "Method", "content": "We present a new method to processively convert a given scene photograph into a sketch with multiple rounds of optimization. An overview of our method can be seen in Figure 2. Briefly, given an arbitrary image, our method can recursively learn its different regions by adding optimizable B\u00e9zier curves. We define our sketch as some sets of black B\u00e9zier curves from different regions placed on a white background. Firstly, we introduce a stroke allocation method to reasonably divide the total number of strokes into different regions. Then we use a proposed stroke sampling to determine stroke locations, which will be converted into our initial strokes (B\u00e9zier curves). To improve convergence, we define the order from the global region to other regions to optimize strokes successively.\nThere are several advantages behind our method. First, it considers the information from different regions of the input image separately, which can help ignore background interference (see Figure 7). Also, the sketch results at the same level of abstraction is greatly increased based on different choices of the regions (see Figure 8). Last, during our optimization process, we also can easily obtain sketches results at different levels of abstraction without changing the total number of strokes (see Figure 1)."}, {"title": "Stroke Initialization", "content": "Stroke Allocation. Our method is based on multiple rounds of stroke superposition. Thus we first should consider the allocation of strokes in different regions. As a case of fairness, we allocate strokes according to the edge points in each region, based on the information gathered from each region's edge contents. EdgeDetector represents the edge extractor to gain the number of edge points in the region Ri. r presents the number of regions. The stroke allocation ratio of region Ri is calculated as follows:\n$E_{i} = EdgeDetector(R_{i})$\n$NE_{i} = len(E_{i})$\n$Ratio_{i}=\\frac{NE_{i}}{\\sum NE_{i}}$\nNs presents the number of total strokes. We then compute the final stroke allocation of region i:\n$N_{i} = Ratio_{i} * Ns$\nStroke Sampling. As each stroke corresponds to a sample point, the distribution of those points determines the distribution of strokes. For uniform coverage of image information and to reduce the lack of information, we adopt a farthest point sampling (FPS) (the process shown in Figure 4). As a first step, we process the region to get the edges. Next, we use a farthest point sampling to sample strokes, selecting"}, {"title": "Loss Function", "content": "In previous works, some commonly used loss functions to minimize the error between images and results based on pixel-wise metrics. Although pixel-wise loss is simple yet intuitive, it is not sufficient to measure the distance between sketches and images as a sketch is highly sparse and abstract. To address this, we leverage a pre-trained CLIP model to guide the training process.\nCLIP-Based Semantic Loss. Due to the capabilities of encoding shared information from both sketches and images, we follow the work of (Vinker et al. 2022b) using CLIP model to compute the distance between the embeddings of the sketch CLIP(Sketch) and image CLIP(Image) as:\n$L_{CLIP1} = dist(CLIP(Sketch), CLIP(Image))$\nwhere $dist(x,y) = 1-\\frac{x y}{||x||\\cdot||Y||}$ is the cosine distance. However, while (Vinker et al. 2022b) utilize the ResNet-based (He et al. 2016) CLIP model for sketching, we find that the ViT-based (Dosovitskiy et al. 2020) CLIP model provides better coverage of global context. Therefore the loss function is defined as the L2 distance between the activation of ViT-B/32 CLIP model at layers:\n$L_{CLIP2} = ||CLIP{layers}(Sketch) \u2013 CLIP_{layers}(Image)||_{2}$\nwith $\u03bb = 0.1$, the CLIP-Based Semantic loss of the optimization is then defined as:\n$L_{CLIP} = L_{CLIP1} + \u03bbL_{CLIP2}$\nVGG-Based Feature Loss. To further improve the similarity between the image and the sketch, we compute the L2 distance between the feature of the sketch and the image based on VGG16 (Simonyan and Zisserman 2014):\n$L_{VGG} = ||VGG(Sketch) \u2013 VGG(Image) ||_{2}$\nFor more details of VGG model we used please refer to the supplementary material. The final objective of the optimization is then defined as:\n$L_{SUM} = L_{CLIP} + L_{VGG}$"}, {"title": "Optimization", "content": "For each optimization round, the parameters of stroke are superimposed with the previous round. All parameters are trained for 800 iterations. We define that the first round of optimization is from the global region, and we are able to gain a sketch at a low level of abstraction. Then the sketch will be refined based on the superimposition of strokes from other regions selected by the user. Since the strokes superimposed are the results of stroke sampling from the next region, the sketch in each optimization is flexible.\nIn Figure 3, we show the process of our optimization and sketch results at different levels of abstraction generated by different optimization round."}, {"title": "Experiments", "content": "In the following, we demonstrate the performances of our technique qualitatively and quantitatively, and provide comparisons to state-of-the-art sketching methods. Figure 8 illustrates the impact of the regions. When different regions"}, {"title": "Qualitative Evaluation", "content": "We compare our method with alternative sketching methods for different subjects, including XoG (Winnem\u00f6ller, Kyprianidis, and Olsen 2012), Photo-Sketching (Li et al. 2019), Chan et al. (Chan, Durand, and Isola 2022) and UPDG (Yi et al. 2020). Note that, none of these sketching approaches can produce sketches with varying abstraction levels, and none can produce sketches in vector format. Due to the fact that our method requires a predefined number of strokes as input, we present three sketches produced by our method depicting three representative levels of abstraction.\nThe sketches produced by UPDG (Yi et al. 2020) and Chan et al. (Chan, Durand, and Isola 2022) are detailed, closely similar to the edge maps of the input images (like results of XOG (Winnem\u00f6ller, Kyprianidis, and Olsen 2012))."}, {"title": "Quantitative Evaluation", "content": "LPIPS. LPIPS (Zhang et al. 2018) measures the perceptual similarity between two images and lower LPIPS scores indicate less perceptual difference. We calculate the scores between input images and sketches of different methods. Table 1 shows the average LPIPS scores for each methods. Our results under high level of abstraction are close to those of Chan et al. (Chan, Durand, and Isola 2022), which means our sketches follow the content of the input image. The results of XOG (Winnem\u00f6ller, Kyprianidis, and Olsen 2012) are not the best, it may because LPIPS does not only consider edge information.\nSSIM. To measure the fidelity level of sketches, we compute the SSIM (Wang et al. 2004) score between each input image and the corresponding sketch. In Table 1, we show the average resulting scores, where a higher score indicates a higher similarity. The sketches by XoG (Winnem\u00f6ller, Kyprianidis, and Olsen 2012) obtained high scores, which is consistent with our observation that SSIM highlight the edges of the image. Under different levels of abstraction, our method shows relatively good scores, indicating that we have a better effect on edge information preservation. This shows the effectiveness of our edge-based sampling method.\nUser Study. The user study examines how well the sketches depict the input scene. Since the absence of CLIPascene's (Vinker et al. 2022a) code, we use 30 scene images to compare our sketches with four methods: CLIPasso (Vinker et al. 2022b), Photo-Sketching (Li et al. 2019), Chan et al. (Chan, Durand, and Isola 2022) and UPDG (Yi et al. 2020).\nThe participants were presented with the input image along with two sketches, one produced by our method and the other by the alternative method. In order to make a fair"}, {"title": "Ablation Study", "content": "In Figure 9, we show the results of different loss function. As can be seen, in that case the VGG-Based Feature loss faithfully preserves geometric structure, while the CLIP-Based Semantic loss strongly conveys the semantic concept, however at the cost of structure. Using their combination can result in an ideal sketch, which is not only semantically accurate, but also retains geometric details."}, {"title": "Conclusion", "content": "We propose a multi-round optimization for scene sketching based on different regions. We progressively infers the sketch with the help of proposed stroke initialization and loss functions: a CLIP-Based Semantic loss and a VGG-Based Feature loss. We can investigate the coarse-to-concrete representation for complex images with different regions. Extensive experiments and human evaluation confirm the superior reconstruction performance of our method over the existing. In the emerging field of sketch generation, we hope our work will open the door for further research."}]}