{"title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation", "authors": ["Mohammed Khalil", "Mohammed Sabry"], "abstract": "Classical Arabic represents a significant era, encompassing the golden age of Arab culture, philosophy, and scientific literature. With a broad consensus on the importance of translating these literatures to enrich knowledge dissemination across communities, the advent of large language models (LLMs) and translation systems offers promising tools to facilitate this goal. However, we have identified a scarcity of translation datasets in Classical Arabic, which are often limited in scope and topics, hindering the development of high-quality translation systems. In response, we present the ATHAR dataset, comprising 66,000 high-quality Classical Arabic to English translation samples that cover a wide array of subjects including science, culture, and philosophy. Furthermore, we assess the performance of current state-of-the-art LLMs under various settings, concluding that there is a need for such datasets in current systems. Our findings highlight how models can benefit from fine-tuning or incorporating this dataset into their pretraining pipelines. The dataset is publicly available on the Hugging-Face Data Hub at https://huggingface.co/ datasets/mohamed-khalil/ATHAR.", "sections": [{"title": "1 Introduction", "content": "Classical Arabic is the foundation of Arabic linguistic theory and is well comprehended by educated Arabic readers. It significantly differs from Modern Standard Arabic (MSA) it is also called (Arangiyya), which is more simplified in terms of its vocabulary, syntax, morphology, phraseology, and semantics.\nClassical Arabic poses unique challenges for accurate translation into English. Unlike MSA, which dominates formal speeches, news channels, and modern literary works, and urban dialects prevalent on social media platforms, Classical Arabic is less commonly used today. Yet, it remains vital, present in many historical documents, books, and literary texts rich with knowledge from the Arab and Muslim golden ages, all awaiting translation and broader exposure.\nCurrent translation systems, including Google Translate and large language models like ChatGPT and Llama, struggle with Classical Arabic, often neglecting it in favour of MSA and urban dialects during dataset creation for machine translation.\nThis work introduces the ATHAR dataset, a translation resource from Classical Arabic to English. \u201cATHAR\u201d \"\u0623\u062b\u0631\" means \"legacy\" or \"ancient work.\" It represents the literary and cultural heritage and underscores the dataset's role in illuminating classical Arabic texts, emphasizing their importance in preserving and conveying this heritage. The ATHAR dataset aims to address the representativeness and quality limitations of previous datasets.\nThis work is organised as follows: Section 2 explores the challenges faced by previous researchers in translating Classical Arabic and details how the ATHAR dataset addresses these challenges. Section 3 elaborates on the methodologies used to create the ATHAR dataset, including steps for data collection, cleaning, and preprocessing to ensure the quality and reliability of the data. In Section 4 we conduct experiments to assess the performance of state-of-the-art LLMs on the ATHAR dataset across various settings such as zero-shot, few-shot, and fine-tuning scenarios. The paper concludes with Section 5, highlighting the importance of the ATHAR dataset in developing culturally and linguistically authentic Arabic language models and advancing Arabic natural language processing."}, {"title": "2 Related Work", "content": "The notable gap in datasets for Classical Arabic has led to several efforts to gather more resources for Arabic Natural Language Processing (NLP). Prominent among these are the Tanzil and Authentic Hadith datasets, which draw from religious texts. The Tanzil dataset offers translations of the Quran in over 40 languages, including Arabic to English, and is hosted on Tanzil.net and the OPUS database (Tiedemann, 2012). The Authentic Hadith dataset provides translations of the sayings and practices of the Prophet Muhammad, known for its authenticity and rigorous translation process (Altammami et al., 2020). While these datasets are rich, they mainly focus on religious content and don't fully represent the diverse genres of classical Arabic literature. Additionally, the Poem Comprehensive Dataset (PCD) (Yousef et al., 2019) provides a dataset focused on Classical Arabic poetry. While this dataset is a valuable resource, it encompasses a limited range of thematic areas.\nIn contrast, there are numerous datasets for Modern Arabic that include a rich and diverse context, such as the OPUS-100 dataset (Zhang et al., 2020), the MultiUN dataset (Eisele and Chen, 2010), and the IWSLT2017 dataset (Cettolo et al., 2017). However, Modern Arabic differs significantly from Classical Arabic in its vocabulary, syntax, and stylistic features, which are not well-represented in these contemporary datasets.\nAdditionally, significant efforts like those by Alrabiah et al. (2014) have focused on Arabic historical linguistics, producing datasets that explore the evolution and contexts of the Arabic language. Although these datasets are not directly applicable in practical translation tasks due to their lack of translations into other languages, they offer invaluable resources for pretraining LLMs with the knowledge necessary to distinguish between Classical and Modern Arabic. Moreover, the initiative by Aloui et al. (2024) introduced a corpus of 101 billion Arabic words, crucial for developing LLMs targeted at the Semitic Arabic language. This extensive corpus, predominantly in Modern Arabic with some Classical content, could help LLMs understand Classical Arabic, particularly when combined with smaller, specialized downstream translation datasets.\nATHAR dataset aims to address the representativeness issues in previous classical Arabic datasets by compiling sentences from various contexts and historical periods on topics like science, medicine, philosophy, and culture. This dataset will help fill the gaps in classical Arabic resources and provide a more comprehensive foundation for developing effective translation models."}, {"title": "3 ATHAR Dataset", "content": "This section outlines the development of the ATHAR dataset. We start by identifying the sources from which the data was collected. Subsequently, we detail the processing steps implemented to ensure the dataset's high quality. Additionally, we compare ATHAR to previous classical Arabic datasets and well-known modern Arabic datasets."}, {"title": "3.1 Data Collection", "content": "The ATHAR dataset comprises 66K sentences extracted from seminal Classical Arabic texts, expertly translated into English. These texts are cornerstone works spanning diverse genres and historical periods, providing profound insights into Islamic and world history, philosophy, science, medicine, and culture. This rich literary heritage is showcased in our dataset, sourced from the Rasaif website at https://rasaif.com/. The specific books included in our dataset are detailed below:\n\u2022 \u062a\u0627\u0631\u064a\u062e \u0627\u0644\u0637\u0628\u0631\u064a (History of al-Tabari): Authored by the Persian historian Muhammad ibn Jarir al-Tabari, this comprehensive history covers the world from creation to 915 AD and is a pivotal source for early Islamic history.\n\u2022 \u062a\u062d\u0641\u0629 \u0627\u0644\u0646\u0638\u0627\u0631 - \u0627\u0644\u0631\u062d\u0644\u0629 (The Travels of Ibn Battuta): This travelogue by Ibn Battuta documents his 14th-century travels across the Islamic world and beyond, offering insights into the era's geography, cultures, and politics.\n\u2022 \u0645\u0642\u062f\u0645\u0629 \u0627\u0628\u0646 \u062e\u0644\u062f\u0648\u0646 (The Muqaddimah of Ibn Khaldun): A pioneering work in historiography and sociology by Ibn Khaldun that analyzes the rise and fall of empires and the dynamics of society, completed in 1377 AD.\n\u2022 \u0627\u0644\u0627\u0645\u0648\u0627\u0644 (The Book of Revenue): Abu Ubayd al-Qasim ibn Sallam's exploration of Islamic economic principles, taxation, and public finance, written around 837 AD.\n\u2022 \u0627\u0644\u0639\u0642\u062f \u0627\u0644\u0641\u0631\u064a\u062f (The Unique Necklace): A collection by Ibn Abd Rabbih encompassing Arabic prose and poetry on literature, history, and moral teachings, finalized in the early 10th century.\n\u2022 \u0627\u0644\u0645\u0646\u0627\u0638\u0631 (The Optics): Ibn al-Haytham's foundational text in optics that discusses the nature of light and vision, pivotal to the development"}, {"title": "3.2 Preprocessing", "content": "To prepare the dataset for use in machine translation models, several preprocessing steps were undertaken:\nCleaning the Data: During the initial stages of the ATHAR dataset collection process, the primary challenge we encountered involved entries where Arabic and English texts were flipped within HTML class labels. For further details on this issue, see Appendix A. To address this, we implemented a simple rule-based technique that identifies the language of the text based on the predominance of characters from the respective language's alphabet. After collecting the data, we found the texts contained various types of noise such as empty entries, incorrect sentences, duplicate entries, entries consisting solely of numbers, and other unwanted characters. These issues were systematically identified and removed to enhance the dataset's quality. Additionally, unnecessary columns like \"book\" and \"author\" were deleted to focus exclusively on the translation pairs. We also removed religious Quranic verses from the dataset, as they were few in number and not dealt with correctly.\nAlignment Verification: As in the Rasaif websites-where we collected the translations from-the translations are created by human volunteers. Given the lack of detailed insights into their methods, and to ensure that each Arabic sentence was correctly aligned with its English translation, thereby maintaining the context and intended meaning, the authors manually verified the collected"}, {"title": "3.3 Comparative Analysis of ATHAR and Other Arabic Datasets", "content": "In this subsection, we analyze our dataset in comparison to existing classical and modern Arabic datasets, focusing on several linguistic measures: lexical diversity, stopword ratio, and the distribution of short versus long sentences, in addition to unique words count and dataset sizes.\nFor lexical diversity, we utilized the Measure of Textual Lexical Diversity (MTLD) index (McCarthy, 2005), which calculates the average length of text segments where the vocabulary diversity remains above a specified threshold\u20140.75 in our case\u2014indicating how consistently a text maintains its lexical richness. The stopword ratio was calculated by determining the occurrence of stopwords relative to the total word count in the datasets. Short sentences were defined as any sentence containing 10 or fewer words, while long sentences are those with 30 or more words.\nBefore conducting the analysis, all datasets were standardized by removing redundant diacritics and letters. As detailed in Table 1, the ATHAR dataset boasts one of the highest MTLD scores, suggesting that the text can sustain a high level of lexical diversity over a large number of words. This implies that the vocabulary is varied and the text does not quickly repeat words. Furthermore, our dataset maintains a balanced representation of both short and long sentences, providing a stark contrast to the variable sentence lengths found in other datasets."}, {"title": "4 Evaluating State-of-the-Art LLMs on the ATHAR Dataset", "content": "In this section, we aim to evaluate the performance of state-of-the-art language models on classical Arabic translations using the ATHAR dataset. We selected four leading models for this analysis: GPT-4o, Llama-3 70B, Llama-3 8B, and Llama-2 7B.\nInitially, we assessed the zero-shot capabilities of these models. Subsequently, we evaluated the Llama-3 8B and Llama-2 7B models under few-shot conditions. Finally, we focused on fine-tuning the Llama-3 8B model using two distinct methods: full fine-tuning, where all parameters of the model were adjusted, and LoRA (Hu et al., 2021) fine tuning, which only involved adjustments to a subset of newly added parameters.\nThe objective of these comprehensive experiments is to maximize the potential of these models, understand performance variations under different settings, and explore how the ATHAR dataset can bridge existing performance gaps.\nIn the following subsections, we will detail the hyperparameters and metrics used in our experiments and analyze the results."}, {"title": "4.1 Hyperparameters and Evaluation Metrics", "content": "Hyperparameters: During inference, the generation decoding strategy involved setting the maximum number of new tokens to 2048. Sampling strategies included Top-K and Top-P settings at 100 and 0.95, respectively, with a temperature parameter set at 0.3.\nFor the fine-tuned models, specifically Llama-3 8B with both full and LoRA tuning, training was implemented in an instruction-input/response format. The input consisted of Arabic text, and the models were trained to generate the corresponding English translation as the response. The training dataset included 65k samples. Models were trained using FP16 precision, with a learning rate of 5e \u2212 6, adjusted via a linear scheduler across three epochs. The batch size was set at 16k tokens, which was achieved by accumulating gradients of four samples twice, each sample having a maximum length of 2048 tokens. An AdamW optimizer was utilized, with beta values of 0.90 and 0.999 for the first and second moment estimates, respectively.\nRegarding the prompt structures used in our experiments, Table 3 details the specific prompt structures we utilized across zero-shot, few-shot, and fine-tuning settings.\nEvaluation Metrics: In assessing our models, we employed well-established metrics commonly used in translation evaluations: METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), and SacreBLEU (Post, 2018). These metrics are all scored on a scale where higher values indicate better performance, though each has a different range. METEOR focuses on the alignment between the translation output and reference translations, considering synonymy and stemming. ROUGE-L measures the longest common subsequence, which is useful for evaluating the fluency of the text. SacreBLEU provides a consistent and comparable score"}, {"title": "4.2 Results and Discussion", "content": "Results: The evaluation results, presented in Table 2, highlight significant variances in model performance across different settings. The GPT-4o model excelled in a zero-shot (ZS) setting, outperforming all other models with scores of 0.357 on METEOR, 0.441 on ROUGE-L, and 14.705 on SacreBLEU. In contrast, the Llama-3 70B Instruct model, also evaluated in a zero-shot setting, registered slightly lower scores of 0.342 on METEOR, 0.413 on ROUGE-L, and 13.011 on SacreBLEU. This disparity might reflect differences in training regimes or underlying model architectures.\nIn the same zero-shot context, both the Llama-3 8B Instruct and Llama-2 7B models displayed considerably lower performance, especially noted by sharp declines in their SacreBLEU scores to 0.339 and 0.310, respectively. These findings suggest inherent limitations in the zero-shot capabilities of these models for translation tasks.\nRemarkable improvements were evident when the Llama-3 8B model was tested in a few-shot (FS) setting using just three samples. This adjustment led to marked score increases to 0.174 on METEOR, 0.167 on ROUGE-L, and 0.971 on SacreBLEU, underscoring the potential of in-context learning to significantly boost model performance. In contrast, the few-shot enhancement for the Llama-2 7B model was minimal, which may indicate model-specific sensitivity to training data increments.\nThe Llama-3 8B model demonstrated further enhancements upon undergoing full fine-tuning, achieving a METEOR score of 0.275, a ROUGE-L score of 0.336, and a SacreBLEU score of 6.061. Additionally, the LoRA tuning method, which involves less extensive modifications, also yielded improved results, with scores reaching 0.279 on METEOR, 0.339 on ROUGE-L, and 8.792 on SacreBLEU.\nDiscussion: The results presented in Table 2 underscore the challenges faced by state-of-the-art LLMs when tasked with translating Classical Arabic to English. By providing state-of-the-art models with targeted training opportunities, the ATHAR dataset not only boosts model performance but also contributes significantly to the broader NLP community's understanding of and engagement with Classical Arabic. This dataset, therefore, holds substantial value, as it aids in developing more nuanced and capable translation systems."}, {"title": "5 Conclusion", "content": "To conclude, we introduce the ATHAR dataset, which enhances the existing corpus of Classical Arabic datasets by incorporating a broader range of topics. Our evaluation of the current status of LLMs underscores the critical need for the ATHAR dataset within the fine-tuning and training pipelines. More broadly, this need highlights the necessity for more comprehensive Classical Arabic datasets to improve the quality of translation systems in this domain. Future work will aim to expand the ATHAR dataset to include an even wider array of"}, {"title": "A Preprocessing: Flipped Cells in Data Collection", "content": "During the scraping process, we encountered difficulties in extracting the English and Arabic texts from the containers (cells) because the Arabic texts were sometimes labeled as \u201cflex-right\" and English texts as \"flex-left\u201d in many instances, with the positions reversed in other cases. To address this, we counted the number of Arabic and English characters in each label and assigned the language based on the predominance of characters from either alphabet. Examples of such inconsistencies are provided below, where the labels for \u201cflex-right\" and \"flex-left\" are swapped, complicating the identification process:\nExample.1 Arabic text on left and English text on right:\nExample.2 Arabic text on right and English text on left:"}]}