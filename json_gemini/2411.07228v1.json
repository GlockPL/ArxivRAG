{"title": "TOOLING OR NOT TOOLING?\nTHE IMPACT OF TOOLS ON LANGUAGE AGENTS FOR\nCHEMISTRY PROBLEM SOLVING", "authors": ["Botao Yutc", "Frazier N. Baker", "Ziru Chen", "Garrett Herb", "Boyu Gou", "Daniel Adu-Ampratwump", "Xia Ning", "Huan Sun"], "abstract": "To enhance large language models (LLMs) for chemistry problem solving, several\nLLM-based agents augmented with tools have been proposed, such as ChemCrow\nand Coscientist. However, their evaluations are narrow in scope, leaving a large\ngap in understanding the benefits of tools across diverse chemistry tasks. To bridge\nthis gap, we develop ChemAgent, an enhanced chemistry agent over ChemCrow,\nand conduct a comprehensive evaluation of its performance on both specialized\nchemistry tasks and general chemistry questions. Surprisingly, ChemAgent does\nnot consistently outperform its base LLMs without tools. Our error analysis with a\nchemistry expert suggests that: For specialized chemistry tasks, such as synthesis\nprediction, we should augment agents with specialized tools; however, for general\nchemistry questions like those in exams, agents' ability to reason correctly with\nchemistry knowledge matters more, and tool augmentation does not always help.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated impressive problem-solving capabilities in many\ndisciplines (Wang et al., 2024b; Yue et al., 2024; Grossmann et al., 2023). When it comes to chem-\nistry, LLMs still face significant challenges, such as incorrect calculation, lack of domain knowl-\nedge, or inability to perform certain tasks like reaction prediction (Guo et al., 2023; Mirza et al.,\n2024). To address these limitations, LLM-based agents integrated with tools have been proposed to\ntackle chemistry-specific problems (Wang et al., 2024a; Ramos et al., 2024). For example, Chem-\nCrow (M. Bran et al., 2024) expands LLMs' capabilities by incorporating 18 tools, ranging from\nweb search to chemical reaction prediction. Similarly, Coscientist (Boiko et al., 2023) integrates the\ncontrol of cloud labs to enable LLMs to automate wet lab experiments.\nDespite the promise of these tool-augmented agents, existing evaluations have been largely qualita-\ntive and limited in scope. For example, ChemCrow is assessed with only 14 individual tasks mainly\nfocusing on compound synthesis, and Coscientist's evaluation involves merely six specific tasks.\nThese narrow assessments leave a large gap in our understanding of how tool-augmented agents\nperform across diverse chemistry tasks in real-world applications.\nIn this work, we conduct a comprehensive evaluation of LLM-based agents on different chemistry\ntasks to grasp a deep understanding of their potential and limitations. To explore and enhance the\ncapabilities of agents in diverse and complex chemistry scenarios, we introduce ChemAgent, a new\nchemistry agent capable of handling a wide spectrum of tasks. It leverages the ReAct framework\n(Yao et al., 2023) and integrates 29 tools, such as a search tool for PubChem (Kim et al., 2019),\nseveral molecular property predictors, as well as many practical tools present in ChemCrow. Then,\nwe adapt two categories of real-world chemistry problems for systematic evaluation: specialized\ntasks and general questions. For specialized tasks, we use SMolInstruct (Yu et al., 2024), which\ncontains 14 types of specialized molecule- and reaction-centric tasks. For general questions, we"}, {"title": "CHEMAGENT", "content": "We introduce ChemAgent (Figure 1), a chemistry agent\nimproved over ChemCrow (M. Bran et al., 2024) and\nequipped with enhanced tools for a wider range of chem-\nistry tasks. It implements two essential cognitive abili-\nties (Sumers et al., 2024) required for chemistry problem-\nsolving: (1) Reasoning: This ability is required in the\nThought step for comprehending user queries and tool out-\nputs, assessing current status, and formulating subsequent\nsteps. (2) Grounding: Based on the reasoning result (i.e.,\nthe \"thought\"), this ability determines the appropriate tool\nto execute and its corresponding input.\nTo enhance ChemAgent's capabilities, we develop an ex-\ntensive set of 29 tools (Appendix A), categorized into\ngeneral, molecule, and reaction tools. General tools\nprovide the agent with common problem-solving abili-\nties, such as the execution of Python code for computa-\ntions and various operations via PythonREPL. Molecule\ntools specialize in the analysis, prediction, and con-\nversion of molecules and their properties.\nFor ex-\nample, FunctionalGroups can identifies functional\ngroups within a molecule, which is crucial for analyz-\ning molecular characteristics. Lastly, reaction tools are\ninstrumental in predicting chemical reaction outcomes\n(ForwardSynthesis) and suggesting synthesis paths\nfor desired products (Retrosynthesis), both of which\nare essential in applications like drug discovery (Berdi-\ngaliyev & Aljofan, 2020)."}, {"title": "EXPERIMENTS", "content": "Datasets. We use three well-established datasets (listed in Table 1) to thoroughly assess tool-\naugmented agents on two categories of chemistry problems: (1) Specialized chemistry tasks focus\non experiment-like problems involving molecular manipulations, predictions, and representations.\nThis category includes SMolInstruct (Yu et al., 2024), which contains 14 molecule- and reaction-\ncentric tasks and requires models to understand molecular representations like SMILES (Weininger,\n1988) and perform specific chemical operations, such as predicting synthesis paths and converting\nchemical names (Figure B.1). (2) General chemistry questions resemble questions appearing in\nexams at different levels and test a wide range of fundamental knowledge and general reasoning\nin chemistry. This category includes MMLU-Chemistry, a manually verified chemistry subset of\nthe MMLU benchmark (Hendrycks et al., 2021) that consists of questions at the high school and\ncollege level (Appendix B.2), and GPQA-Chemistry, the chemistry section of the GPQA-Diamond\nbenchmark (Rein et al., 2023) that consists of difficult graduate-level questions.\nLLMs and Agents. We compare our ChemAgent with two baselines: (1) State-of-the-art (SoTA)\nbase LLMs, including GPT-40 (OpenAI, 2024) and Claude-3.5-Sonnet (Anthropic, 2024), which\nhave shown superior capabilities in chemistry problem-solving among existing LLMs (Wang et al.,\n2024b). (2) ChemCrow (M. Bran et al., 2024), a pioneering chemistry-focused agent equipped with\n18 expert-designed tools. For ChemCrow and ChemAgent, we utilize GPT-40 or Claude-3.5-Sonnet\nas the backbone language models, and refer to them as GPT and Claude, respectively."}, {"title": "OVERALL PERFORMANCE", "content": "Specialized Chemistry Tasks. Models are evaluated on 50 randomly selected samples from the\ntest set of SMolInstruct for each task, and the results on four selected tasks are presented in Table 2\n(see Appendix B.1.2 for the full results). We can observe that: (1) ChemAgent exhibits substan-\ntial improvements over its base LLM counterparts, highlighting the critical role of domain-specific\ntools in augmenting LLMs' capabilities on the specialized tasks in SMolInstruct. (2) Compared to\nChemCrow, ChemAgent demonstrates superior performance. Our analysis suggests that the dis-\nparity is attributed to ChemCrow's limited tool set and the potential lack of robustness in its tool\nimplementations. For instance, ChemCrow's apparent deficiency in molecular property prediction\ntools and its limited web search capabilities seem to hinder its performance in property prediction\ntasks. In contrast, ChemAgent's tool set (Appendix A) is more comprehensive and robust for LLMs\nto leverage effectively.\nGeneral Chemistry Questions. As presented in Table 3, contrary to our expectations, the ChemA-\ngent variants underperform their base LLM counterparts. This trend persists across both datasets and"}, {"title": "ERROR ANALYSIS", "content": "To examine the errors made by ChemAgent, we use SMolInstruct and MMLU-Chemistry as repre-\nsentatives from their respective categories and conduct a manual error analysis. For all the samples\nwhere ChemAgent (GPT) fails in our experiments, we engage a chemistry expert to analyze the\nerrors, which are then classified into three types, namely reasoning error, grounding error, and\ntool error, based on the components (the cognitive abilities and the environment) responsible for\nthe errors. The definitions of the errors identified during our experiment are as follows:\nReasoning errors. Errors made by the \"reasoning\u201d ability, where the agent inaccurately assesses\nthe situation or devises an incorrect plan for subsequent steps, such as misinterpreting tool outputs\nor suggesting incorrect methodologies. Specifically, they include the following errors:\n\u2022 Wrong knowledge/reasoning: an error where agent makes a mistake in applying chemistry knowl-\nedge or makes a conclusion that does not logically follow from the previous information.\n\u2022 Wrong final answer: an error where the analysis process is correct but the final answer is wrong.\n\u2022 Information oversight: an error where the agent neglects to consider relevant information given in\nthe question or the previous steps.\n\u2022 Algebra error: an error in algebraic manipulation or simplification, such as the incorrect solving\nof equations or misapplication of algebraic axioms.\n\u2022 Incomplete reasoning: An error where the reasoning process is not fully developed, such as when\nsolving a problem but omitting necessary steps or details.\nGrounding errors. These occur during tool invocation, such as selecting an inappropriate tool,\nusing an incorrect input format, or providing erroneous inputs to a tool. Specifically:\n\u2022 Wrong input format: an error arising from data being provided in a format that the tool cannot\nprocess, resulting in failures or incorrect results.\nTool errors. These errors originate from the environment (i.e., the tools used in this study), where\nthe tools either fail to execute properly or return inaccurate information. Specifically:\n\u2022 Wrong tool output: an error occurring when a tool produces incorrect or unexpected results, lead-\ning to faulty conclusions or actions.\n\u2022 Inconsistent tool outputs: an error where multiple tools return inconsistent information, leading to\nfaulty conclusions or actions.\nAs illustrated in Figure 2, the error distributions are very different on the two datasets. On SMolIn-\nstruct (Figure 2a), tool errors account for over 99.0% of all errors. These errors mainly stem from\nthe neural networks-based tools (e.g., ForwardSynthesis, BBBPPredictor), which inherently possess\nimperfect accuracy. For these specialized tasks where dedicated tools exist, the agent can easily\npinpoint and correctly use the needed tools (Appendix D), resulting in limited or no reasoning and"}, {"title": "CONCLUSION", "content": "In this paper, we conducted a comprehensive evaluation of tool-augmented language agents for\nchemistry problem-solving. We introduce ChemAgent, an enhanced chemistry agent with an en-\nhanced tool set, and assess its performance across diverse chemistry problems, including special-\nized tasks and general questions. Our findings reveal that the impact of tool augmentation is highly\ndependent on task characteristics: While ChemAgent demonstrates significant improvements on\nspecialized tasks, it does not surpass the base LLMs without tools on general questions. The manual\nerror analysis highlights that tool errors predominate in specialized tasks, whereas reasoning errors\nare more frequent in general questions due to delicate mistakes in the problem solving process.\nTo minimize reasoning errors and enhance performance in general questions, future agent design\nshould focus on optimizing the cognitive load of LLMs and improving their ability to reason and\nverify information, especially when resolving inconsistencies from multiple sources."}, {"title": "LIMITATIONS", "content": "This study focuses on evaluating the performance of tool-augmented agents across various chem-\nistry tasks. While it presents the most comprehensive evaluation on this topic to the best of our\nknowledge, there are several limitations:\n\u2022 Our evaluation uses GPT-4o and Claude-3.5-Sonnet as the primary models for comparison and as\nbackbones for the agents. This selection does not encompass a broader range of potential LLMs,\nsuch as Llama-3.2 (Meta, 2024) or Qwen2 (Yang et al., 2024). Although these additional models\nmight exhibit different performance patterns, our choice of state-of-the-art (SoTA) models ensures\nstrong baselines without significant loss of generality.\n\u2022 While our study addresses both specialized tasks and general questions in the field of chemistry,\nit may not entirely represent the vast array of real-world chemistry problems. The scope might\noverlook certain nuanced challenges and scenarios encountered in practical applications.\n\u2022 The manual error analysis, while thorough, is conducted by a chemistry expert together with two\nPhD students with chemistry knowledge and is limited in scale. This constraint could introduce\npotential biases and may not capture all subtle variations in error types or frequencies."}, {"title": "TOOL SET", "content": "The tool set contains 29 tools ranging from general tools, molecule tools, to reaction tools. This\nsection introduces all the tools in detail.\nGeneral tools: Provide broad information retrieval, web searching, and computational.\n\u2022 AiExpert: A general-purpose LLM prompted to answer any questions when other tools cannot\nhandle. We use GPT-4o or Claude-3.5-Sonnet in our experiments, identical to the backbone mod-\nels of ChemAgent.\n\u2022 PythonREPL: Executes Python commands and allows for package installation.\n\u2022 WebSearch: Searches the internet for both general and domain-specific information, providing\nconcise summaries of relevant content. This involves an LLM-based search service\u00b9 that uses\nLLMs to summarize the search result, providing more straightforward and organized results.\n\u2022 WikipediaSearch: Searches Wikipedia and provides summaries of related content.\nMolecule tools: Offer various analyses, predictions, and conversions related to chemical compounds\nand their properties.\n\u2022 BBBPPredictor: Predicts the probability of a compound penetrating the blood-brain barrier using\nthe Uni-Mol model (Zhou et al., 2023).\n\u2022 CanonicalizeSMILES: Converts SMILES representation to its canonical form with RDKit (RD-\nKit, 2023).\n\u2022 CompareSMILES: Determines if two molecule SMILES representations are identical.\n\u2022 CountMolAtoms: Counts the number and types of atoms in a molecule.\n\u2022 FunctionalGroups: Identifies functional groups present in a molecule.\n\u2022 GetMoleculePrice: Retrieves the cheapest available price for a purchasable molecule.\n\u2022 HIVInhibitorPredictor: Predicts the probability of a compound inhibiting HIV replication using\nthe Uni-Mol model.\n\u2022 IUPAC2SMILES: Converts IUPAC names to SMILES representation by searching PubChem,\nChemSpace, or using the neural network based STOUT model (Rajan et al., 2021).\n\u2022 LogDPredictor: Predicts the octanol/water distribution coefficient (logD) at pH 7.4 using the Uni-\nMol model.\n\u2022 MolSimilarity: Computes the Tanimoto similarity between two molecules.\n\u2022 MoleculeCaptioner: Generates a textual description of a molecule using neural networks using the\nMolT5 model (Edwards et al., 2022).\n\u2022 MoleculeGenerator: Creates SMILES representations based on molecular descriptions using neu-\nral networks using the MolT5 model (Edwards et al., 2022).\n\u2022 Name2SMILES: Converts common molecule names to SMILES representation.\n\u2022 PatentCheck: Verifies if a molecule is patented.\n\u2022 PubchemSearchQA: Searches and retrieves molecule/compound information from PubChem, a\ncomprehensive database of chemical molecules and their activities. Given the information of\na molecule/compound (SMILES, IUPAC name, orcommon name) and a related question, it re-\ntrieves the corresponding document from PubChem, and applies an instructed LLM (GPT-40 in\nour experiments) to briefly answer the input questions. Instead of directly returning the whole\ndocument, which is typically very long, this QA design reduces the irrelevant information in the\ncontext, so as to avoid distractions and length limit violation for LLMs.\n\u2022 SELFIES2SMILES: Converts SELFIES (Krenn et al., 2019) to SMILES representation.\n\u2022 SMILES2Formula: Derives the molecular formula from SMILES representation using fixed algo-\nrithm implemented with RDKit.\n\u2022 SMILES2IUPAC: Converts SMILES representation to IUPAC name by searching PubChem,\nChemSpace, or using the neural network based STOUT model.\n\u2022 SMILES2SELFIES: Converts SMILES representation to SELFIES representation.\n\u2022 SMILES2Weight: Calculates the molecular weight from SMILES representation.\n\u2022 SideEffectPredictor: Predicts the probabilities of a compound causing various side effects across\n20 different categories using the Uni-Mol model.\n\u2022 SolubilityPredictor: Predicts the log solubility of a compound in mol/L using the Uni-Mol model.\n\u2022 ToxicityPredictor: Predicts the probability of a compound being toxic using the Uni-Mol model."}, {"title": "SMOLINSTRUCT", "content": "SMolInstruct (Yu et al., 2024) contains 14 molecule- and reaction-centric tasks, which, along with\nthe task name abbreviations and examples, are illustrated in Figure B.1.\nWe evaluate the models on 50 randomly selected samples from the test set for each task. For refer-\nence, in the following detailed results, we also include the SoTA non-LLM models used in Yu et al.\n(2024), and LlaSMol\u00b3, which is a Mistral model (Jiang et al., 2023) fine-tuned on SMolInstruct.\nFor SoTA non-LLM models and LlaSMol, we adopt their own formats of input and output. For\nother models, we prompt them to think step by step, i.e., using chain-of-thought (CoT) (Wei et al.,\n2022; Wang et al., 2022), and wrap their final answers with \u201c<ANSWER>\u201d and \u201c</ANSWER>\"\nto facilitate answer extraction. The evaluation metrics are adopted from Yu et al. (2024).\n\nDETAILED RESULTS\nThe detailed results on SMolInstruct are presented in Table B.1 and Table B.2. We can see that: (1)\nThe SOTA LLMS, GPT-40 and Claude-3.5-Sonnet, demonstrate relatively low performance across\nall evaluated tasks, which underscores the persistent challenges faced by general-purpose LLMs in\nspecialized chemistry domains, particularly in handling molecular representations such as SMILES"}, {"title": "MMLU-CHEMISTRY", "content": "To effectively and efficiently evaluate the models, we build MMLU-Chemistry, a subset of 70\nchemistry question samples derived from the widely-used MMLU dataset (Hendrycks et al., 2021).\nSpecifically, to increase the difficulty and differentiation of the questions, while avoiding erroneous\nsamples presented in the original MMLU, we select samples that appear in both MMLU-Pro (Wang\net al., 2024b) and MMLU-Redux (Gema et al., 2024). These two datasets are verified versions of\nMMLU, and MMLU-Pro has extended the answer options from 4 to 10 to introduce more chal-\nlenges. When the gold standard answers from both sources match, we utilize the 10 options from\nMMLU-Pro. In cases of discrepancies, we manually review and correct any potential issues. To\nreduce the cost of evaluation, we eliminated samples where all models performed correctly in our\npreliminary experiments. This results in a final set of 70 questions, divided evenly between 35 high\nschool-level and 35 college-level questions.\nIn our evaluation, all the models are prompted to generate a CoT solution and close the solution with\n\"the answer is ...\" to facilitate answer extraction. To mitigate randomness, we run each sample three\ntimes and report the average accuracy.\nIn addition, to understand the influence of in-context examples, in the following detailed results, we\nalso introduce a 5-shot setting in comparison with 0-shot for the base LLMs and ChemAgent. The\nquestions of the in-context examples are originally from MMLU's and MMLU-Pro's development"}, {"title": "GPQA-CHEMISTRY", "content": "We use GPQA-Chemistry, the 93 chemistry multi-choice questions from the expert-verified GPQA-\nDiamond subset of the GPQA dataset (Rein et al., 2023) to evaluate models' abilities in high-level\nchemistry knowledge and complex reasoning. All the evaluated models are prompted to generate\nCoT solutions and close their output with \"the answer is ...\" to facilitate answer extraction. We\nreport the average accuracy across 3 runs. The results are presented in Table 3."}, {"title": "ERROR CASES", "content": "This section presents a specific case of each error. For reasoning errors, the cases are shown in\nTable 4, Appendix C.1, Appendix C.2, and Appendix C.3. For grounding errors, the case is presented\nin Appendix C.4. For tool errors, the cases are presented in Appendix C.5 and Appendix C.6."}, {"title": "REASONING: WRONG FINAL ANSWER", "content": "This is an example of a reasoning error: wrong final answer. The question is from MMLU-Chemistry, and the\nagent answers it with the LLM's internal knowledge and no tool. Although it clearly rules out option (B) as\nhighlighted in its analysis, it still selects (B) as the final answer, which is wrong."}, {"title": "REASONING: INFORMATION OVERSIGHT", "content": "This is an example of a reasoning error: information oversight. The question is from MMLU-Chemistry,\nand the agent answers it by using PythonREPL to calculate the result. However, the model ignores \"13C\" in\nthe question, which indicates that the frequency should be approximately 1/4 of the frequency of the NMR\ninstrument. This oversight leads to the wrong final answer."}, {"title": "REASONING: ALGEBRA ERROR", "content": "This is an example of a reasoning error: algebra error. The question is from MMLU-Chemistry, and the agent\nanswers it by firstly using Name2SMILES to understand the molecular structures and then calculating the ratio\non its own. Everything is correct until the highlighted algebra calculation step fails to correctly cross-multiply\nto solve for the ratio, leading to the wrong final answer."}, {"title": "GROUNDING: WRONG INPUT FORMAT", "content": "This is an example of a grounding error: wrong input format. This question is from SMolInstruct, and the\nagent initially calls the dedicated tool, SideEffectPredictor, to predict the potential side effect. However, it\ninputs an extra \"SMILES:\" (highlighted), causing the tool to malfunction. The same issue occurs with the\nCanonicalizeSMILES tool."}, {"title": "TOOL: WRONG TOOL OUTPUT", "content": "This example shows the tool: wrong tool output error. The question is from MMLU-Chemistry, and the agent\nanswers it by searching the internet with the WebSearch tool. The tool provides an incorrect result (highlighted),\nwhich leads to the wrong final answer."}, {"title": "TOOL: INCONSISTENT TOOL OUTPUTS", "content": "This is an example of a tool error: inconsistent tool outputs. The question is from SMolInstruct, and the agent\nanswers it by predicting the answer with ToxicityPredictor and verifying the answer with PubchemSearchQA.\nHowever, the two tools give different answers and the agent chooses to believe the latter, which leads to the\nwrong final answer."}]}