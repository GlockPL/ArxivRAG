{"title": "Efficient and Interpretable Neural Networks Using Complex Lehmer Transform", "authors": ["Masoud Ataei", "Xiaogang Wang"], "abstract": "We propose an efficient and interpretable neural network with a novel activation function called the weighted Lehmer transform. This new activation function enables adaptive feature selection and extends to the complex domain, capturing phase-sensitive and hierarchical relationships within data. Notably, it provides greater interpretability and transparency compared to existing machine learning models, facilitating a deeper understanding of its functionality and decision-making processes. We analyze the mathematical properties of both real-valued and complex-valued Lehmer activation units and demonstrate their applications in modeling nonlinear interactions. Empirical evaluations demonstrate that our proposed neural network achieves competitive accuracy on benchmark datasets with significantly improved computational efficiency. A single layer of real-valued or complex-valued Lehmer activation units is shown to deliver state-of-the-art performance, balancing efficiency with interpretability.", "sections": [{"title": "1 Introduction", "content": "We might have already entered the era of superintelligence, in which machines achieve better cognitive capabilities than humans. The current machine learning methods have made many revolutionary advances that were not even imaginable just a few years ago. The current deep learning models, while effective in capturing intricate feature interactions, often rely on numerous layers to achieve high performance with millions or even billions of parameters to capture complicated patterns. The black-box nature of the current machine learning architect has always been prohibitive for human reasoning to truly understand the nature of those millions of operations. This results in increased computational cost, diminished transparency, and challenges in understanding the underlying mechanisms driving their predictions. More importantly, this brings significant unforeseeable risks and uncertainties if we blindly trust the outcomes of those large models. For human intelligence to remain relevant in building machine learning models and to greatly reduce computational costs, we would need to construct neural network architectures that are not only efficient but also interpretable.\nInspired by the Kolmogorov-Arnold representation theorem, a Kolmogorov-Arnold Network (KAN) has been previously proposed with a learnable node activation function [1]. This network serves as an alternative to the traditional multi-layer perceptron architecture by leveraging the theorem's elegant decomposition of high-dimensional functions. However, learning a high-dimensional function through a one-dimensional function can be challenging due to potential non-smoothness or fractal-like behavior in some mappings, which complicates optimization and interpretability. The KAN mitigates these issues by introducing a learnable activation function that adapts to the complexity of the target mappings. In this work, we take a different approach by employing a fixed nonlinear activation function within a single-layer architecture. This design not only simplifies the optimization process but also enhances interpretability and computational efficiency, offering a compelling alternative to KAN for applications requiring clarity and scalability.\nSpecifically, we propose an efficient and interpretable neural network using a novel activation function called the Lehmer transform. The Lehmer transform, a parameterized family of means, provides a robust framework for summarizing and transforming features [2]. Its ability to interpolate between different forms of mean behaviors allows it to emphasize small or large elements dynamically, making it a versatile tool for data representation. Building on its foundational mathematical properties, we extend the Lehmer transform into a weighted form, enabling adaptive feature importance through trainable weights. Additionally, we generalize the transform to the complex domain, introducing an oscillatory component that captures phase-sensitive dependencies and hierarchical relationships within data. The weighted form reflects the relative connection among inputs and also their absolute scale. It also enjoys perturbation and proportion invariance as well as smoothness and Schur convexity. The complex-valued weighted Lehmer transform introduces great flexibility by introducing an oscillatory component and enjoys differentiability in the complex domain. It also has a built-in smoothing or summarizing functionality for the inputs.\nThese extensions form the foundation of the Lehmer Neural Network (LNN), a novel architecture that leverages the unique properties of the Lehmer transform."}, {"title": "2 Mathematical Framework", "content": ""}, {"title": "2.1 Lehmer Transform", "content": "The Lehmer transform was originally introduced as a parameterized family of means, capturing a spectrum of aggregation behaviors [2]. Let $x = [x_1,x_2,...,x_n]^T \\in R_+$ denote n given input features. For the parameter $s \\in R$, the Lehmer transform is defined as follows:\n$L(s; x) = \\frac{\\sum_{i=1}^{n} x_i^s}{\\sum_{i=1}^{n} x_i^{s-1}}$                                   (1)\nThe parameter s, referred to as the suddency moment, governs the emphasis placed on small or large elements. When s = 0, the transform corresponds to the harmonic mean, prioritizing smaller elements. For s = 1, the Lehmer transform reduces to the arithmetic mean, and for s = 2, it emphasizes larger elements, aligning with the contra-harmonic mean. Furthermore, for the cases of $s \\to -\\infty$ and $s \\to \\infty$, the minimum and maximum of the sample are retrieved, respectively. This interpolation between extremes provides remarkable flexibility for modeling aggregation behaviors.\nA notable property of the Lehmer transform is its monotonicity with respect to the parameter s. For strictly positive inputs, the derivative of L(s) with respect to s"}, {"title": "2.2 Weighted Lehmer Transform", "content": "The weighted Lehmer transform introduces positive weights $w = [w_1,w_2,..., w_n]^T \\in R_+$ to modulate the contributions of each input as follows:\n$L(s) := L(s; x, w) = \\frac{\\sum_{i=1}^{n} w_ix_i^s}{\\sum_{i=1}^{n} w_ix_i^{s-1}}$                             (4)\nwhere the weights $w_i$ are trainable parameters. This adaptation allows for dynamic emphasis on certain elements based on their relative importance.\nOne of the key properties of the weighted Lehmer transform is its homogeneity of degree one. If the inputs $x_1, x_2,...,x_n$ are scaled by a positive constant $ \\lambda > 0$, the transform scales proportionally; i.e.,\n$L(s; \\lambda x, w) = \\lambda L(s; x, w)$.                             (5)\nThis property emphasizes that the transform reflects not only the relative relationships between inputs but also their absolute scale.\nThe weighted Lehmer transform is also invariant to proportional scaling of the weights $w_1,w_2,..., w_n$. If all weights are scaled by a positive constant $a > 0$, the transform remains unchanged; i.e.,\n$L(s; x, aw) = L(s; x, w)$.                             (6)\nThis highlights that only the relative proportions of the weights influence the transform."}, {"title": "2.3 Complex-valued Lehmer Transform", "content": "The Lehmer transform extends naturally into the complex domain, enabling enhanced analytical capabilities and a broader range of applications. In this extension, the suddency moment s is generalized to take complex values s = a + bi, where a, b \u2208 R."}, {"title": "3 Lehmer Activation Units", "content": "The Lehmer transform, with its parameterized flexibility and aggregation capabilities, serves as a robust foundation for designing activation mechanisms in neural networks. Building on this foundation, we introduce LAUs, which is a class of nonlinear trans-formations tailored for neural network architectures. Designed in both real-valued and complex-valued variants, these units dynamically adapt their behavior during training, offering a mathematically grounded alternative to traditional activation functions.\nThe real-valued LAU is a direct adaptation of the weighted Lehmer transform to the context of neural networks. For a given positive input vector x, positive weights w, and a trainable suddency moments \u2208 R, the LAU is defined as follows:\n$LAU(x, w, s) = \\frac{\\sum_{i=1}^{n} w_ix_i^s}{\\sum_{i=1}^{n} w_ix_i^{s-1}}$                               (15)\nThe parameters wi and s, optimized during training, enable the unit to interpolate between various means, such as weighted harmonic mean (s = 0), weighted arithmetic mean (s = 1), and weighted contra-harmonic mean (s = 2). This flexibility allows the LAU to adapt to hierarchical or attention-based patterns in data, capturing both dominant and subtle feature relationships."}, {"title": "4 Experiments and Discussions", "content": "The LAUs were evaluated on four diverse datasets: Iris, Wine, Wisconsin Breast Cancer (WBC), and MNIST. These datasets provided varying levels of complexity and feature structures, allowing a thorough assessment of the effectiveness of both real-valued and complex-valued LAUs. For the Iris, Wine, and WBC datasets, a 10-fold cross-validation strategy was employed to ensure robust evaluation and mitigate the effect of sample variability. In the case of the MNIST dataset, a standard train-test split was used, reflecting its larger size and high-dimensional feature space.\nFor all datasets, the architecture with real-valued or complex-valued LAUs employed a single dense layer with 3 neurons, followed by a softmax layer to pro-duce the final output. For the MNIST dataset, convolutional layers (Conv2D, Batch Normalization, and MaxPooling) were incorporated to extract spatial features, with LAUs serving as the dense layers. The streamlined design of using a single dense layer demonstrates the efficiency and adaptability of both real-valued and complex-valued LAUs in capturing diverse data characteristics.\nThe experimental results highlight the effectiveness of both real-valued and complex-valued LAUs across all datasets. Real-valued LAUS achieved strong performance, effectively leveraging their parameterized aggregation capabilities to handle hierarchical or heterogeneous features. Complex-valued LAUs demonstrated comparable or superior accuracy, particularly excelling on the MNIST dataset with a notable 98% accuracy. This performance advantage is attributed to their ability to model oscillatory and phase-sensitive dependencies, which are especially beneficial for high-dimensional and spatially structured data. The results underscore the adaptability and robustness of both variants of LAUs in diverse learning tasks.\nThe Iris dataset, characterized by its small size and structured nature, exhib-ited strong performance for both real-valued and complex-valued LAUs, achieving accuracies of 95% in both cases. This success is attributed to the dataset's inherent hierarchical relationships and distinct feature separability, particularly in the petal and sepal dimensions. Real-valued LAUs effectively incorporated their parameterized aggregation mechanisms to dynamically emphasize dominant features such as petal length and width. This flexibility allowed them to interpolate between different aggre-gation behaviors, capturing both global trends and finer details within the dataset. Complex-valued LAUs matched this performance, demonstrating their capability to adaptively model subtle feature interactions even in the absence of explicit oscillatory"}, {"title": "5 Conclusion", "content": "This paper introduced LAUs, a novel nonlinear activation mechanism grounded in the Lehmer transform. Real-valued LAUs adapt flexibly to hierarchical or heterogeneous data, while complex-valued LAUs extend this adaptability by incorporating oscillatory and phase-sensitive behaviors. Their ability to interpolate between aggregation forms enables dynamic adaptation across diverse datasets, offering a compelling alternative to traditional activation mechanisms.\nEmpirical evaluations demonstrated the versatility and efficacy of LAUs, achiev-ing competitive or superior performance with simpler architectures. Notably, a single layer of complex-valued LAUs achieved remarkable accuracy on benchmark datasets, underscoring their efficiency and representational power. These results highlight the potential of LAUs to advance neural network design and applications in machine learning.\nFuture research should explore LAUs in diverse applications such as time-series analysis, natural language processing, and bioinformatics, while also investigating their integration with advanced architectures like transformers. Further exploration of fully complex-valued neural networks may unlock their potential in specialized domains like signal processing and quantum machine learning."}]}