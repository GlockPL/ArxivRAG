{"title": "FreeCond: Free Lunch in the Input Conditions of Text-Guided Inpainting", "authors": ["Teng-Fang Hsiao", "Bo-Kai Ruan", "Sung-Lin Tsai", "Yi-Lun Wu", "Hong-Han Shuai"], "abstract": "In this study, we aim to determine and solve the deficiency\nof Stable Diffusion Inpainting (SDI) in following the in-\nstruction of both prompt and mask. Due to the training\nbias from masking, the inpainting quality is hindered when\nthe prompt instruction and image condition are not related.\nTherefore, we conduct a detailed analysis of the internal\nrepresentations learned by SDI, focusing on how the mask\ninput influences the cross-attention layer. We observe that\nadapting text key tokens toward the input mask enables the\nmodel to selectively paint within the given area. Lever-\naging these insights, we propose FreeCond, which adjusts\nonly the input mask condition and image condition. By in-\ncreasing the latent mask value and modifying the frequency\nof image condition, we align the cross-attention features\nwith the model's training bias to improve generation qual-\nity without additional computation, particularly when user\ninputs are complicated and deviate from the training setup.\nExtensive experiments demonstrate that FreeCond can en-\nhance any SDI-based model, e.g., yielding up to a 60% and\n58% improvement of SDI and SDXLI in the CLIP score.\nThe code and appendix are available in our repository at\nhttps://github.com/basiclab/FreeCond\u00b9.", "sections": [{"title": "1. Introduction", "content": "Text-to-image (T2I) inpainting seeks to fill specified\nmasked areas based on user-provided text prompts. Sta-\nble Diffusion Inpainting (SDI), a tailored adaptation of"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Image Inpainting", "content": "Image inpainting focuses on repainting specified regions\nwhile ensuring coherence with the surrounding image. Var-\nious non-text-guided inpainting methods have been devel-\noped to achieve this [4, 12, 18, 22, 36-38, 41], alongside the\nemergence of text-to-image inpainting methods [1, 21, 25,\n29, 33, 39]. SDI [26] pioneered the integration of a random\nmasking strategy into its training objective, producing har-\nmonized outputs. However, this approach often prioritizes\nimage conditioning over adherence to instructions. To im-\nprove this, recent methods [13, 20, 31, 42] have introduced\nsolutions focused primarily on enhancing mask-fitting. De-\nspite these advancements, these methods often lack prompt\n-adherence when handling complex instructions. In con-\ntrast, FreeCond leverages insights into the inpainting mech-\nanism to improve instruction-following across any SDI-\nbased inpainting model, achieving a balanced performance\nbetween mask-fitting and prompt-adherence."}, {"title": "2.2. Unveiling the Mechanism of T2I Models", "content": "T2I diffusion models possess powerful image-generation\ncapabilities. To fully harness this potential, recent works\nhave explored various training-free modifications based on\nin-depth analyses of different components [3, 5-8, 10, 11,\n17, 19, 34, 35]. Notably, FreeU [28] reveals that the skip-\nconnection primarily retains texture details, while the back-\nbone captures more semantic information. By adjusting the\nbalance between them, FreeU enhances semantic accuracy\nwith minimal impact on detail and computational costs.\nOur analysis reveals that the image and mask conditions\nplay similar roles: the image condition provides contextual\ndetails, while the mask condition regulates prompt influ-\nence. By modulating both conditions, we achieve improved\ninstruction-following without additional costs, providing a\n\"free lunch\" in performance enhancement."}, {"title": "3. Analysis of SDI model", "content": "In this section, we provide an in-depth analysis of the SDI\nmodel. Firstly, we identify the training bias of the SDI\nmodel in Sec. 3.2 that the model heavily relies on the im-\nage condition to generate the content. This mechanism can\ncause the generated output unrelated to the input prompt.\nSecondly, we demonstrate in Sec. 3.3 that increasing the\nsize of the mask provides SDI with more potential solutions\nto integrating prompt instructions into the image context,\nresulting in outputs that more closely follow the instruction.\nFinally, in Sec. 3.4, we test the hypotheses that interpret\nthe internal mechanics of the SDI model, with a particular\nfocus on the relationship between the inpainting condition-\ning and the cross-attention layer. The analysis of the SDI\nmodel helps us understand the factors leading to successful\ninstruction-following inpainting."}, {"title": "3.1. Stable Diffusion Inpainting Model (SDI)", "content": "The SDI model receives a prompt p, an image $I \\in$\n$]R^{H\\times W\\times 3}$, and a mask $M \\in R^{H\\times W}$ that specifies the in-\npainting area. To prevent the model from copying content\ndirectly from I, the masked area of I is set to zero, yielding\n$I^{o} = (1 \u2013 M) I$. Since the UNet operates in VAE [14]\nlatent space, $I^{o}$ is encoded as the image condition $z^{o} =$\n$E(I^{C}) \\in R^{(H/4)\\times(W/4)\\times 4}$ and E denotes the pretrained\nVAE encoder. To match the input size, the SDI model uses\nan interpolated mask condition $M^{o} \\in R^{(H/4)\\times(W/4)}$, cre-\nated by downsampling M with nearest-neighbor interpola-\ntion. The final inpainting noise prediction from the diffu-\nsion model is $\\epsilon_{\\theta}(z_{t}, z^{o}, M^{c},t,p)$, where $\\epsilon$ represents the\nSDI UNet model, $z_{t}$ is the noise latent at timestep t$\\in [0, T]$,\nand initial noise $z_{\\bar{1}}$ is sampled from $N (0, I)$. To control the\ninfluence of prompt p, we follow classifier-free guidance\n(CFG) [9], modifying the noise prediction with a scaling\nparameter w \u2208 R:\n$\\hat{\\epsilon}_{o} (z_{t}, z, M^{c}, t,p) = \\epsilon_{\\theta}(z_{t}, z^{o}, M^{o}, t, \\varnothing)$\n$+ w(\\epsilon_{\\theta} (z_{t}, z^{o}, M^{c},t,p) \u2013 \\epsilon_{\\theta}(z_{t}, z^{o}, M^{o},t,\\varnothing))$ (1)\nTo evaluate the inpainting results in our study, we\nuse a set of six metrics adopted from BrushBench [13].\nThese metrics cover three key areas: Image Quality, mea-\nsured by Image Reward (IR) [32], HPS [30], and Aes-\nthetic Score (AS) [27]; Background Preservation, as-\nsessed using PSNR and LPIPS[40]; and Instruction Fol-\nlowing, evaluated through CLIP [24]. Additionally, we\nintroduce a novel Intersection-over-Union (IoU) score to\nspecifically capture the mask-fitting quality, complement-\ning the CLIP Score by distinguishing mask accuracy from\nprompt-adherence. This score is computed by the IoU be-\ntween input mask and auto-labeled mask via SAM [15], as\ndetailed in Appendix."}, {"title": "3.2. Influence of Image Condition ze", "content": "The random masking strategy of SDI is via creating masked\ndata by randomly masking 25% of image areas in LAION-\n5B [27], aiming to enhance generalizability across vari-\nous inputs. To investigate the mask distribution under this\nrandom strategy, we define three mask placements: \u201cnot\nmasked,\" \"partially masked,\u201d and \u201cfully masked.\" Although\nthe exact SDI training mask distribution is not accessible,\nour analysis on the COCO dataset as a surrogate reveals\nthat, with a 25% mask coverage, over 80% of training data\nfalls under the \"not masked\" or \"partially masked\" cate-\ngories (see the Appendix for details). Thus, we hypothesize\nthat the random masking design optimizes SDI for main-\ntaining overall image harmony rather than strict prompt-\nadherence. For instance, in the second row of Fig. 3a, when"}, {"title": "3.3. Influence of Input Mask M", "content": "In the preceding analysis, we observe that the inpainting re-\nsult can be hugely guided by the image condition $z^{o}$, lead-\ning to its deficiency in the instruction following. Here,\nwe explore how to adjust the input mask M to promote\ninstruction-following outputs across varied inputs. Study-\ning the SDI model under complex scenarios such as mul-\ntiple or rough masks, unrelated prompt instruction p for\nreference image I-requires a more comprehensive bench-\nmark. However, since the COCO dataset includes only\nthe precise masks, and its prompts for generating are sim-\nple and highly related to the image context, we propose\nFCIBench, which incorporates rough masks, multi-mask,\nand complex prompts with unrelated contexts of image con-\ndition. FCIBench compensates the shortage of existing\nbenchmarks [13, 16], as shown in Appendix.\nBuilding on our observation in the first row of Fig. 3a that\nthe prompt \"zebra\" and ground-truth \"person\" coexist, we\nexplore which modifications to the input mask M can facil-\nitate this coexistence across different scenarios. Intuitively,\nwe hypothesize that increasing the mask size may provide\nSDI with more potential solutions to integrate prompt\ninstructions into the image context, rather than simply\ndisregarding the prompt. The results, illustrated in Fig. 4,\nreveal that in both scenarios, AS remains nearly constant,\nindicating that image quality is nearly invariant to mask\nsize. Additionally, LPIPS decrease as the non-masked area\nbecame smaller. Finally, as mask size increases, CLIP con-\nsistently improves, especially in Fig. 4b where the prompt\nwas unrelated to the background context. This finding sup-\nports our hypothesis that increasing the mask size can en-\nhance prompt-adherence in the model's output."}, {"title": "3.4. The Mechanism Behind Inpainting", "content": "In Sec. 3.2, we identify that SDI's deficiency in instruction-\nfollowing stems from its preference for maintaining har-"}, {"title": "4. Method", "content": "In Sec. 3.2, we identify that the image context provided\nby ze can impede instruction-following in the SDI model.\nIn Sec. 3.3 and Sec. 3.4, we observe that the $M^{o}$ plays\nan important role in the cross-attention layer, the inclusion\nof $M^{c}$ leading to prompt-adherence by shifting the cross-\nattention features. Building on these insights, we propose\nFreeCond to directly reduce the heavy reliance on the image\ncontext provided by $z^{o}$ and increase the feature shift led by\n$M^{c}$. With FreeCond, we can achieve improved instruction-\nfollowing for the SDI-based approach in a post-hoc manner\nwithout additional fine-tuning or computational costs."}, {"title": "4.1. FreeCond Image Condition", "content": "In light of the phenomenon observed in Sec. 3.2, where\ninpainting outputs can be significantly influenced or dom-\ninated by the image condition $z^{o}$, it appears reasonable to\nreduce the influence of $z^{o}$ to improve insturction following.\nHowever, since the inpainting model relies on za to preserve\nthe background, any adjustments to $z^{o}$ can compromise the\nmask preservation. Nonetheless, based on the nature of the\nT2I diffusion process, as described in [2, 28, 35], we note\nthat low-frequency components are formed in early steps\nwhile high-frequency details emerge in later steps. In other\nwords, we can still largely preserve the background in the\nfinal output by inputting only the low-frequency portion of\nze in the early step and then transitioning to the original ze.\nWe define the FreeCond image condition as:\n$z^{fc} =$\\begin{cases}\n$LPF(x, \\gamma)$, if t > $t^{fc}$\n$z^{o}$,\nift < $t^{fc}$\n\\end{cases} (3)\nwhere $LPF(x, \\gamma)$ is a low-pass filter that excludes high-\nfrequency components above the threshold y, and tfe is the\ntimestep control parameter, with lower values of tfe pro-\nducing a blurrier output. The effect of $z^{fc}$ is demonstrated\nin Fig. 8. By modifying zf in the early step, such as setting\n$t^{fc} \\in [0.5T, 0.9T]$, we can effectively improve instruction-\nfollowing with minimal impact on background preserva-\ntion. Since the LPF filters out high-frequency image infor-\nmation, the overall image context is disrupted, thus enhanc-\ning instruction-following by reducing interference from the\noriginal image context."}, {"title": "4.2. FreeCond Mask Condition", "content": "Building on our observations in Sec. 3.4, we find that the\nT2I inpainting effect of the SDI model is raised by the\nshifting in cross-attention features with non-zero mask in-\nput M. Further analysis in Appendix reveals that while\nboth z and Me affect the inpainting outcome, shifts in\ncross-attention features are primarily driven by $M^{c}$ values.\nBased on this, we explore the potential to enhance cross-\nattention feature shifts by manipulating the mask condition\nMc. Accordingly, we introduce a FreeCond mask con-\ndition, Mfc, which scales up the value of Me to induce\na stronger cross-attention feature shift, thereby improving\nprompt alignment. Another observation, shown in Fig. 7,\nis that the CI indicator in the (1 \u2013 M) region is subtly im-\npacted by the mask M. Thus, increasing mask values within\nthe (1 - M) region can amplify feature shifts within M. To\ntest this, we define the FreeCond mask condition:\n$M^{fc} = \\alpha \\cdot M^{o} + \\beta \\cdot (1 \u2013 M^{o})$ (4)\nwhere a and \u03b2 are scaling factors to control the influence\nof M\u00ba and (1 - M\u00ba). The impact of Mfc is illustrated\nin Fig. 9. Compared to the baseline results in Fig. 9a, the\noutput with a Mfc exhibits greater prompt-adherence. For\ninstance, in the \u201cprecise mask\u201d condition, instead of filling\nthe background element, the \u201cgolden retriever wearing as-\ntronaut gear\" appears. Additionally, in the \"large mask\"\nsetting, the golden retriever now includes the \"astronaut\ngear\". We also provide the response of the CI indicator\nfor Mfc in the right side of Fig. 9 to show that modifying\nthe latent mask Me can effectively enhance feature shifts\nwithin the cross-attention layer.\nWith our proposed alteration, the noise predic-\ntion function from Eq. (1) can be generalized as\n$\\hat{\\epsilon}_{o}(z_{t}, z^{fc}, M^{fc},t,p)$. As FreeCond only modifies the in-\nput, it is compatible with other SDI-based models, detailed\nin Appendix."}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Experiment Setting", "content": "We conduct experiments on three datasets: COCO [16],\nBrushBench [13], and our proposed FCIBench, each com-\nprising 600 instruction pairs (details in Appendix). To"}, {"title": "5.2. Experiment Results", "content": "Tab. 2 presents the quantitative improvements achieved with\nthe inclusion of FreeCond. We compare FreeCond with the\noriginal SDI [26] and its variants, including ControlNet In-\npainting (CNI) [39], HD-Painter (HDP) [20], PowerPaint\n(PP) [42], and BrushNet (BN) [13]. Additionally, we assess\nSDXL [23], a much larger model, as a reference to show-\ncase the zero-shot improvements by FreeCond, it is not di-\nrectly compared with other baselines.\nIn our proposed FCIBench, shown in the right section\nof each block. Designed as a more challenging benchmark,\nFreeCond demonstrates substantial gains, achieving a 60%\nincrease over the original SDI model and a 1% improvement\nover BrushNet, the existing SOTA. Additionally, FreeCond\nimproves metrics such as IR, HPS, and IoU across all base-\nlines. Notably, both BrushNet and PowerPaint benefit from\nFreeCond with modest increases in IoU yet larger gains in\nCLIP score, highlighting FreeCond's capability to further\nenhance prompt-adherence in SOTA methods that are op-\ntimized for mask-fitting. For the two widely used datasets,\nCOCO and BrushBench-represented in the left and middle\nsections of each block-current inpainting methods demon-\nstrate similar performance levels. FreeCond advances this\nupper limit, increasing BrushNet's CLIP score from 19.22\nto 19.27 on COCO and PowerPaint's CLIP score from 27.02\nto 27.05 on BrushBench. Overall, FreeCond enhances per-\nformance across both instruction following and image qual-\nity, with improvements in IR, HPS, and CLIP scores.\nNevertheless, modifying learned mechanism with"}, {"title": "5.3. Ablation Study", "content": "In Fig. 10 and Fig. 11, we examine the impact of adjust-\ning five components: (a) the classifier-free guidance (CFG)\nscale w [9], (b) the inner-mask scale a, (c) the outer-mask\nscale \u03b2, (d) the LPF threshold y with a fixed tfc = 25, and\n(e) the LPF timestep tfe with y = 0.75\u03c0. For each test,\nwe fix the parameters at (w, \u03b1, \u03b2, \u03b3, tfc) = (15,1,0, \u03c0,\u03a4)\n(the default configuration of original SDI) and vary only one\nparameter at a time. Based on quantitative and qualitative\noutcomes, we summarize our findings below.\nEffect of w. As discussed in Sec. 3.2, SDI's random mask-\ning strategy prioritizes generating objects within the mask\nrather than strict mask conformity. Therefore, increasing w\nin Fig. 11a primarily enhances prompt-related details with-\nout substantially increasing object size. This outcome is fur-\nther reflected in the lesser improvement of the IoU score\ncompared to the CLIP score in Fig. 10a.\nEffect of a. As explained in Sec. 4, increasing a inten-\nsifies the cross-attention response within the masked area,\nenhancing both prompt-adherence and mask-fitting, as\nillustrated in Fig. 11b and Fig. 9b. However, excessively\nhigh a disrupts the learned feature distribution, leading to\nover-saturated results and a drop in AS.\nEffect of B. Unlike other parameters, increasing Ben-\nhances both the CLIP score and AS, indicating a stronger\nself-attention interaction between M and 1 M, which re-\nsults in a more harmonious output. However, as shown in\nFig. 11c and Fig. 9c, higher \u1e9e values also increase back-\nground distortion, reflected by the LPIPS in Fig. 10c.\nEffect of zfc (controlled by \u03b3 and tfc). These parame-\nters control the frequency components of zfe, which play\na key role in reducing contextual influence and establishing\nprompt-related structures at early timesteps. As shown in\nFig. 10e, increasing tfe significantly improves both CLIP\nand IoU scores. This is reflected in Fig. 11d, where mask-\nfitting is improved while prompt-adherence is lacking (e.g.,\nthe \u201cmoss and flowers\u201d are not fully generated)."}, {"title": "6. Conclusion", "content": "In this work, we identify an instruction-following deficiency\nthat persists across current SDI-based inpainting methods,\nparticularly when complex prompts are provided along-\nside unrelated image conditions. Through an in-depth in-\nvestigation of the SDI mechanism, we discover that its\nselective inpainting capability within masked areas stems\nfrom a feature shift in the cross-attention layer. Based on\nthis insight, we propose FreeCond\u2014a training-free plug-in\nthat introduces no additional computation overhead. Un-\nlike classifier-free guidance, FreeCond enhances not only\nprompt-adherence but also mask-fitting and image quality.\nHowever, excessive parameter adjustments can degrade im-\nage quality, highlighting the need for careful tuning."}]}