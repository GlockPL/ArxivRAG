{"title": "Policy Abstraction and Nash Refinement in Tree-Exploiting PSRO", "authors": ["Christine Konicki", "Mithun Chakraborty", "Michael P. Wellman"], "abstract": "Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model in extensive form using data obtained from querying a simulator that represents a detailed description of the game. We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. First, we introduce a scalable representation for the empirical game tree where edges correspond to implicit policies learned through DRL. These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration. To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model.", "sections": [{"title": "1 INTRODUCTION", "content": "Empirical game-theoretic analysis (EGTA) [16, 18] reasons about complex game scenarios through empirical game models esti-mated from simulation data. A popular form of EGTA is the policy space response oracles (PSRO) framework [9] (Fig. 1), in which the empirical game is iteratively extended by adding best responses (BRs) derived from deep reinforcement learning (DRL). The vast majority of prior work on EGTA and PSRO [1, 19] represents theempirical game in normal form even though the real underlying game consists of agents' strategies interacting via sequential decisions under various unknowns. McAleer et al. [12] introduced XDO as an alternative to PSRO that maintains an empirical game in extensive form, to capture a combinatorial space of strategies with the choice of actions at each decision point in the game tree. We originally proposed and evaluated a tree-exploiting version of EGTA (TE-EGTA) that maintains an empirical game in an extensive form based on a coarsening of the underlying game [8]. This work demonstrated that a significant improvement in model accuracy and strategy exploration, compared to normal-form EGTA, can be achieved by using the tree structure to model even a little of the information-revelation and action-conditioning patterns of the underlying game.\nA key step in PSRO is augmenting the empirical game model with new BR results. This is straightforward for a normal-form model: add the new strategies to the game matrix, and estimate payoffs for new profiles using simulation. For PSRO using an extensive-form model, where we can no longer treat a BR as an atomic entity, we face new questions such as the following. In what precise sense does the empirical game tree coarsen or abstract the underlying multiagent scenario? Relatedly, how should we incorporate elements of the BRs (detailed policy specifications) at appropriate places in the empirical game tree? The way Konicki et al. [8] address these challenges in their approach towards tree-exploiting PSRO (TE-PSRO) is by systematically coarsening away select (non-strategic) stochastic events in the underlying game. This approach is not general or scalable in the sense that it relies on the use of stochastic events to model imperfect information in the underlying game. In this paper, we reformulate TE-PSRO by developing a method to abstract broad swaths of the state and observation spaces of the underlying game, providing a more implicit rendering of games with complex information structure, including high degrees of imperfect information.We also introduce other methodological advances that enhance the power of TE-PSRO in many directions."}, {"title": "2 TECHNICAL PRELIMINARIES", "content": "An extensive-form game (EFG) is a tuple\n$G = (N, H, V, \\{I_j\\}_{j=0}^n, \\{A_j\\}_{j=1}^n, X, P, u )$,\nwhere $N = \\{0,..., n\\}$ is the set of players or agents; $H$ is a finite tree of histories divided into subsets of terminal nodes or leaves $Z$ and decision nodes $D$; $V$ is a function assigning each decision node $h$ to an acting player; $A_j(\\cdot)$ is the set of actions available at each decision node; $u$ is a function mapping each $z \\in Z$ to a utility vector $\\{u_j(z)\\}_{j=1}^n$; in games of imperfect information, the set $I_j$ is a partition of $V^{-1}(j)$ where each $I \\in I_j$ is an information set (or infoset) of $j$. All nodes $h \\in I$ are indistinguishable to player $j$, meaning their action spaces are also indistinguishable and denoted $A_j(I)$. The directed edge connecting any $h \\in I$ to its child represents a transition resulting from $V(h)$'s move. We assume perfect recall [15, Definition 5.2.3]. A node $h$ where $V (h) = 0$ is called a chance node controlled by Nature, with a set of possible outcomes $X(h)$ and probability distribution $P(. | h)$ over $X(h)$.\nSince the underlying or true game corresponding to the simulator is too large to be represented directly with a tree, we instead express it in a state-action formulation. A play of the game is a sequence of actions taken by the players (including Nature), where each action leads to a world state $w \\in W$. The joint space of actions is given by $A = \\times_{j=1}^n A_j$, and the set of legal actions for agent $j$ at world state $w$ is given by $A_j(w) \\subseteq A_j$. The probability distribution of the world state $w'$ following joint action $a = (a_1,..., a_n) \\in A$ taken in world state $w$ is given by a transition function $T (w, a) \\in \\Delta W$. Upon transitioning to $w'$ from $w$ via $a$, agent $j$ makes a partial observation $o_j = O_j (w, a, w')$ instead of fully observing $w'$. A reward $R_j (w)$ is given to agent $j$ at"}, {"title": "3 DESCRIPTION OF GAMES STUDIED", "content": "We will now describe in detail the two games used in our experimen-tal assessment of TE-PSRO in \u00a76. The game analyst using TE-PSRO has no direct access to a game description at this level of detail, but can query a simulator based on such a description for data samples relevant to game histories induced by input strategy profiles."}, {"title": "3.1 BARGAIN", "content": "In this game, two players negotiate the division of $m$ discrete items of $t$ types. We represent the item pool by a vector $p$ where the $i$th entry $p_i$, $i \\in \\{1, ..., \\tau\\} = [\\tau]$, is the number of items of type $i$; $\\Sigma_{i=1}^\\tau p_i = m$. Each player $j \\in \\{1,2\\}$ has a private valuation over the items given by a vector $v_j$ of non-negative integers such that the $i$th entry $v_{j,i}$ is player $j$'s value for one item of type $i$. In each game instance, $(v_1, v_2)$ are sampled uniformly at random from the collection $V$ of all vector pairs satisfying three constraints. First, for both players, the total value of all items is a constant: $v_j. p = V$, $j \\in \\{1, 2\\}$. Second, each item type must have nonzero value for at least one player: $\\forall i \\in [\\tau]. v_{1,i} + v_{2,i} > 0$. Finally, some item type must have nonzero value for both players: $\\exists i \\in [\\tau].v_{1,i}v_{2,i} > 0$.\nAn additional feature of our game is that each player $j$ has a private outside offer in the form of a vector of items $o_j$, defining the fallback payoff the player obtains if no deal is reached. This offer is drawn from a distribution $P_j(.)$ at the start of each game instance. During negotiation, a player $j$ may choose to reveal coarsened information about its outside offer to the other player in the form of a binary signal which is L (resp. H) if the value of the offer $o_j v_j$ is at most (resp. greater than) a fixed threshold $v$ where $1 < v < V$. In each of a finite number $T > 0$ of negotiation rounds, the players take turns proposing a partition of the pool between themselves, with player 1 moving first in each round. In its turn, a player can accept the latest offer from the other player (DEAL), end negotiations (WALK), or make an offer-revelation combination of the form $(\\omega, R)$. Offer $\\omega \\in \\{(p_1, p_2) | p_1 + p_2 = p\\}$ is a proposed partition of the items, with $p_j$ a vector of $t$ non-negative integers representing player $j$'s share. Revelation $R \\in \\{\\text{TRUE, FALSE}\\}$ represents that player's decision to either disclose its signal (TRUE) in that turn or not (FALSE). We also include a discount factor $\\gamma \\in (0, 1]$ to capture preference for reaching deals sooner. Negotiation fails if a player chooses WALK in any round $p \\in \\{1, ..., T\\}$ or $T$ rounds pass without any player choosing DEAL. In case of failure in round $p$, each player $j$ receives a reward of $\\gamma^p o_j v_j$ from its outside offer. If a proposed partition $(p_1, p_2)$ is accepted in round $p$, then the reward to $j$ is $\\gamma^{p-1} p_j. v_j$."}, {"title": "3.2 GENGOOF", "content": "GENGOOF is parametrized by a positive integer $K$ which determines the number of game rounds ($K - 1$) as well as the size of each player's action space ($K$).\nFirst, a discrete stochastic event with $K$ outcomes occurs at the game root, that is, $|X(0)| = K$. Let $e_1$ denote the realized outcome"}, {"title": "4 TREE-EXPLOITING PSRO", "content": "Our domain of interest comprises game instances where expanding the full game out as an extensive-form tree for the purpose of game analysis is infeasible. TE-PSRO tackles this challenge by maintain-ing a coarsened and abstracted (yet extensive-form) version as its empirical game model. The full game is specified in the form of a gameplay simulator, which is formalized in terms of the world state framework (\u00a72). A key question for TE-PSRO (Fig. 1) is how to translate new best-response results into elements that can be sys-tematically incorporated into an abstracted empirical game model as part of the model augmentation operation. Our approach bridges the detailed state-space model of the simulator and the simplified game tree using the concept of abstract policies."}, {"title": "4.1 Abstract Policies", "content": "In the underlying game, the space of possible infostates of player $j$ is given by $I_j$, and a policy $r_j$ specifies $j$'s action $a \\in A_j (I)$ for each $I \\in I_j$. We represent such policies in our implementation by neural"}, {"title": "4.2 Best Response: Deep Reinforcement Learning", "content": "With the ability to simulate profiles over the empirical game strat-egy space, we can employ the simulator within a deep RL algorithm to derive best responses $\\pi$. Our implementation employs the DQN algorithm [13], which combines a feed-forward neural network parameterized by with temporal difference learning and a second target network to estimate Q-values over time given $I \\in I_j$.\nAs an illustration, we provide details of our deep Q-network for Bargain. The input to the neural network representing $\\pi$ for this game is an encoding of player $j$'s current information state $I$. $[\\log_2(V)]$ bits are allotted for player $j$'s valuation $v_{j [i]}$, for each $i \\in [\\tau]$. One bit is allotted for player $j$'s outside offer signal. For each player's turn in the game, $p[i] + 1$ bits are allotted per item type $i \\in [\\tau]$ to represent a partition of $p$, plus one bit for the decision to reveal the signal or not. Two bits are allotted to represent the other player's signal: 00 means no reveal so far, 01 means L, and 10 means H. One final bit is allotted to be set to 1 when negotiations are complete. The output of the network is an $|A_j|$-long vector containing the Q-values of each action in $A_j$ given the input infostate vector. Our parameter settings, optimized via hyperparameter tuning, are included in App. G. After successfully training player $j$'s DQN, the learned weights of & are saved and mapped to the abstract policy label \u201c\u03c0\u201d in G (\u00a74.1)."}, {"title": "4.3 Augmenting the Empirical Game Model", "content": "Given BRs \u03c0* computed from DRL, the next step is to augment the empirical game G (Fig. 1). Though an abstract policy is potentially applicable at any point in the game tree, adding \u03c0 to every I \u2208 \u00cej could lead to unsustainable growth in G.\nOur approach is to select a fixed number M of infosets to augment for each player. Our selection is based on an assessment of the gain \\varGamma_{I} of playing \u03c0 instead of \u1ee1* at candidate infosets I \u2208 \u00cej"}, {"title": "5 COMPUTING REFINED NASH EQUILIBRIA", "content": "Tree-based game models afford consideration of solution concepts specific to the extensive form. We specifically investigate the use of subgame perfect equilibrium (SPE), a refinement of NE that rules out solutions containing non-credible threats. To make use of SPE, we need a definition that applies to games of imperfect information, and an algorithm that computes such solutions.\nDefinition 5.1. A subgame of game $G$ is a directed rooted subtree given by $G' = (N, H', V', \\{I'_j\\}_{j=0}^n, \\{A'_j\\}_{j=1}^n, X', P', u')$ satisfying the following:\n\u2022 The root $h'$ of tree $H'$ must be the only node in its information set.\n\u2022 As a subtree of $H$, $H'$ must include all nodes in $H$ that succeed $h'$.\n\u2022 For any $j \\in N$ and for all $I \\in I_j$, if $I \\in I'_j$, then the nodes $h \\in I$ must all be part of $H'$; if $I \\notin I'_j$, then all its nodes must be part of $H \\setminus H'$.\n\u2022 $V', \\{I'_j\\}_{j=0}^n, \\{A'_j\\}_{j=1}^n, X', P'$, and $u'$ are restrictions to $H'$ of $V, \\{I_j\\}_{j=0}^n, \\{A_j\\}_{j=1}^n, X, P$, and $u$, respectively.\nDefinition 5.2 ([14]). A subgame perfect equilibrium (SPE) of game $G$ is an NE of $G$ that also induces NE play in each of $G$'s subgames.\nIn finite perfect-information EFGs, an SPE always exists in pure strategies and can be readily computed using the classic backward induction approach. With imperfect information, however, a sub-game may not admit a pure-strategy NE at all. Kaminski [7] proposed the generalized backward induction (GBI) approach for finding the set of SPE for a potentially infinite game of imper-fect information. A key feature of GBI is the re-expression of the game tree as a set of its proper subgames organized by their roots. Other crucial implementation details are not fully specified in the original article, in particular how to identify NE of subgames that include non-singleton information sets; a na\u00efve implementation using exhaustive enumeration of strategy profiles for combinations of subgames has a runtime that is exponential in game size.\nWe provide a practical, modular algorithm for finding an SPE via GBI in a finite, imperfect-information EFG. Our algorithm com-bines dynamic programming with a Nash solver subroutine, using Kaminski's [2019] idea of organizing the game into subgames. Alg. 1 presents our method. COMPUTESPE uses subroutines described here at a high level (see App. C for full pseudocode)."}, {"title": "6 EXPERIMENTS", "content": "We now report experiments that we conducted to evaluate our TE-PSRO approach by applying it to the two games described in \u00a73."}, {"title": "6.1 Parameter settings", "content": "For BARGAIN (\u00a73.1), we set \u03c4 = 3, V = 10, v = 5, y = 0.99, T = 5, and n \u2208 {5, 6, 7}. We generated five unique sets of the remaining parameters p, (v1, v2), P1, P2 uniformly at random from their respective supports in order to evaluate TE-PSRO's performance on a variety of game instances. For GENGOOF (\u00a73.2), we set K = 4, Umax = 10, calling this instance GENGOOF4. The simulator budget was 100 samples for BARGAIN and 200 samples for GENGOOF4.\nWe ran all experiments on our local computing cluster using a single core. Runtime and memory requirements depend on the choice of M \u2208 {1, 2, 4, 8, 16}, which determines the rate of growth of \u011c across TE-PSRO epochs (see App. E for details). Unless oth-erwise stated, every experiment was performed for five randomly seeded trials for each setting. Error bars in our plots correspond to a 95% confidence interval."}, {"title": "6.2 Results", "content": "Our first set of experiments assesses space requirements for the em-pirical game \u011c as a function of the number M of infosets augmented per epoch. At each epoch of TE-PSRO, we recorded the total number of information sets across players in G and the memory required by the emprical model. We report the average empirical game size for each of the two games studied in terms of the number of player information sets of all players and memory required in megabytes (MB) in Fig. 3 and Fig. 15 in App. F.1 respectively, for representative values of M; each curve for BARGAIN is averaged over 100 trials per value of M across all five sets of bargaining parameters. The broad takeaway from all plots in this set is the following. Although the rate of increase in the size of the \u011c steepens with M in the plots, its size is still manageable after many epochs of TE-PSRO. If we had added a new policy to all information sets rather than to only a subset of size M, \u011c would grow to as many as 3000 or 4000 information sets after only five epochs of TE-PSRO, which is an unsustainable trajectory. See App. F.1 for further insights on the difference between the two games.\nOur second set of experiments provides evidence for the effective-ness of our algorithm COMPUTESPE(\u00a75) as well as the non-triviality of obtaining an SPE of an imperfect-information extensive-form game. We ran two suites of TE-PSRO on each of BARGAIN and GEN-GOOF4: each suite consisted of 50 trials for each M setting, using NE as the MSS for 25 trials and SPE as the MSS for the remaining 25. In one suite, we evaluated intermediate and final \u011c (EVAL in Fig. 1) by NE computed using CFR, and the other by SPE using Alg. 1. We computed the regret of the respective solutions with respect to each subgame of G and reported the maximum over subgames, that is, the worst-case subgame regret; a solution with a lower value of this quantity is a better SPE approximation. Thus, Figs. 4b and 4d verify that Alg. 1 does indeed produce an approximate SPE Figs. 4a and 4c demonstrate that our regular Nash solver does not happen to stumble upon NE that are also subgame-perfect. Additionally, the worst-case subgame regret increases with the complexity of G, re-flected in both setting of M and epochs of TE-PSRO. Note that these experiments are not for gauging the quality of the models produced by TE-PSRO; instead, TE-PSRO is used to generate a sequence of empirical games of increasing size and complexity (in terms of the number of non-singleton information sets) that serve as more and more challenging test cases for our game-solving algorithms.\nOur final experiment set characterizes TE-PSRO performance in terms of the MSS choice (SPE vs. NE) and different values of M. To compare MSS choices, we computed the profile regret (\u00a72) with respect to the underlying game of the solution \u03c3* returned by EVAL in each epoch of TE-PSRO epoch; we used both NE and SPE as EVAL, giving us two sets of comparison metrics.\nFig. 5 depicts our average regret results for BARGAIN under var-ious settings. We ran TE-PSRO for 25 trials per value of M and four combinations of choices for EVAL and MSS. In each trial, TE-PSRO was allowed to run for at most 30 epochs, terminating early when the computed best responses did not yield an improvement greater than 0.1 over the current solution \u03c3*. Fig. 5a shows that TE-PSRO outperforms the normal-form version NF-PSRO, regard-less of EVAL/MSS choice, even for M = 1. Figs. 5b and 5c show that"}, {"title": "7 CONCLUSIONS", "content": "We introduced multiple extensions of Tree-Exploiting PSRO, enabling its application to complex games of imperfect information. Our main innovation is the treatment of best responses computed by DRL as abstract policies, incorporated as actions in the empir-ical game tree. To manage growth of the empirical game as BRs are generated over the course of TE-PSRO, we introduced a hy-perparameter M, which controls the number of infosets that can be expanded per epoch. Finally, we demonstrated that having an extensive-form empirical game model can be leveraged in the form of new meta-strategy solvers based on Nash refinements. Toward that end, we developed a modular algorithm for identifying SPE solutions in imperfect-information games. We demonstrated these methods on two carefully constructed complex games, featuring multiple rounds of offer/counteroffer with signaling options. We"}]}