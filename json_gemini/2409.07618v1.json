{"title": "Understanding Foundation Models:\nAre We Back in 1924?", "authors": ["Alan F. Smeaton"], "abstract": "This position paper explores the rapid development\nof Foundation Models (FMs) in AI and their implications for\nintelligence and reasoning. It examines the characteristics of\nFMs, including their training on vast datasets and use of\nembedding spaces to capture semantic relationships. The paper\ndiscusses recent advancements in FMs' reasoning abilities which\nwe argue cannot be attributed to increased model size but to\nnovel training techniques which yield learning phenomena like\ngrokking. It also addresses the challenges in benchmarking FMs\nand compares their structure to the human brain. We argue\nthat while FMs show promising developments in reasoning and\nknowledge representation, understanding their inner workings\nremains a significant challenge, similar to ongoing efforts in\nneuroscience to comprehend human brain function. Despite\nhaving some similarities, fundamental differences between FMs\nand the structure of human brain warn us against making direct\ncomparisons or expecting neuroscience to provide immediate\ninsights into FM function.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundation Models or Frontier Models (FMs) are having\nan impact across society which is comparable or perhaps\nexceeds any other technological development in AI since the\nearliest days of computation. To add even more significance\nand importance to this, the rate of development of FMs is at\na pace that is difficult for us to keep up with. This means that\nwe should try to identify the trends rather than get bogged\ndown in the minute details and incremental developments that\nnew releases of FMs bring.\nIt is natural for us to want to look beyond the short-term\nfuture and predict where these trends might take us, should\nthey continue in the same directions.\nIn this paper we identify and examine some of those trends\nand weave them together to identify emerging patterns in\nFoundation Model development, their implications for artificial\nintelligence capabilities, and potential future directions in this\nrapidly evolving field."}, {"title": "II. FOUNDATION MODELS", "content": "Here we provide a quick re-cap on the most important char-\nacteristics of Foundation Models, more recently sometimes\nreferred to as Frontier Models, that term being used to capture\nthe characteristics of the more advanced models being at the\nboundary of current performance levels.\nFoundation Models are large-scale AI models trained on\nextensive unannotated datasets which generate statistical rep-\nresentations of the distributions of any 1, 2, 3 or even multi-\ndimensional streams of data [29]. They create an embedding\nspace from their training data which is a parametric memory,\nso called because the models are just a set of weights or\nparameters. Where the training data is text, these embedding\nspaces allow words, phrases or sentences to be represented\nas vectors in that space and the vectors capture the semantic\nrelationships among tokens in the training data. In this way\nthe resulting models infer a higher level of representation of\ninformation than their original source materials.\nThe mapping from the training data to the Foundation\nModel is done using the Transformer Architecture, introduced\nin [23] and for the largest of the large language models\n(LLMs), which are a type of foundation model, the compu-\ntation time for this is very large. For example GPT-4 from\nOpenAI is believed to have about 13 trillion tokens as its\ntraining data which, if printed as a series of books and stacked\nlike on a shelf, round be more than 650km in length. Its\ncompute time is estimated at 2.15 \u00d7 10^{25} FLOPs which would\nbe more than 2.5 million years on an Apple MacBook with an\nM2 chip. The training of a foundation model has no natural\nendpoint and LLM training is usually stopped when there are\nnegligible performance improvements on validation sets which\nare a form of testing how well the model would perform if\ntraining was stopped, or when the target performance levels are\nreached or there are signs of overfitting, or when the model's\ntime-to-release considerations are taken into account.\nMost big tech companies now have their own Foundation\nModels or LLMs, some are proprietary while others are open,\nmost have a free and a subscription base for accessing them\nand many have multiple versions of their models which vary\nin model size and many of the companies compete with each\nother based on model size, which is reminiscent of the way"}, {"title": "III. FOUNDATION MODELS AND INTELLIGENCE", "content": "Consider the fragment of text shown in Figure 1. Can the\nreader decipher it correctly? Probably yes. Now consider the\nfragment of text in Figure 2, can the reader decipher it?\nProbably yes, but only because you deciphered the first one. If\nyou gave the text in Figure 1 to an early version of a LLM, say\nGPT3, then it would correctly decipher it and if you then gave\nthe model the text in Figure 2 then it would correctly decipher\nthat one too. However, if you reversed the order and gave\nFigure 2 followed by Figure 1 the model would not decipher\nthe text in Figure 2 but would decipher the text in Figure 1.\nThese texts are in a form of informal language known as leet\nspeak which originated in the 1980s and was used in online\ngaming and internet forums to bypass the very simple text\nfilters that were used then. It involves replacing letters with\nnumbers or characters that resemble the shapes of the original\nletters. For example \u201cE\u201d might be replaced with \u201c3\u201d and \u201cA\u201d\nwith \u201c4\u201d. The leet speak version of the quote that \u201cintelligence\nis the ability to adapt to change", "7H15 15 7H3 B357 P4P32 3V32\"\ndoes not appear on the internet at all according to Bing and\nGPT3 does not have the commonsense reasoning ability to\ncorrectly decrypt the text from Figure 2 without the context\nof having already decrypted text from Figure 1 (but the reader\nhopefully had !). If you give the text from Figure 2 to a more\nrecent model like Claude-3 or GPT-4 then it correctly decrypts\nit without needing the context of the text from Figure 1.\nSo what has happened in the last 18 months to allow\nFoundation Models to exhibit greater commonsense reason-\ning? Elsewhere in AI it is well-established that larger neural\nnetworks based on larger datasets used to train them, work\nbetter in applications like speech recognition [1] and machine\ntranslation [15] and they out-perform conventional, symbolic\nand rule-based approaches to AI. The early stage LLMs\nwere already good at one or two-step logical inferencing,\nsome forms of abductive reasoning, anaphora resolution and\npronoun matching, and even sentiment analysis, and some\nother generative natural language text applications. This is\nbecause neural networks, and Foundation Models, are a form\nof compression, compressing logic, reasoning and inference\ninto their structure and weights in ways we do not understand,\nbecause we do not yet understand how a neural network\noperates. So when the very recent giant Foundation Modes\ndemonstrate logic-based processing we assume this is by dint\nof the volume of their training data, but how is it achieved?\nGPT-4 was the first model to really show larger LLMs\ngetting better at reasoning and that was attributed to their\ngreater statistical basis. In 2023 Mistral-7B [11] was released\nby a French startup company and although only 7B parameters\nin size it beat LLaMA-1 on some inference tasks despite being\nonly 20% of its size. Mistral-7B runs on a laptop and is\ntrained using grouped-query attention (GQA) and a variable\nsize sliding window attention (SWA) so it prunes the model\nsize and is efficient. This was the first evidence that models\nwhich are smaller in size can not only be more efficient to\nuse, which is a target outcome, but counter-intuitively can also\nperform better at reasoning tasks.\nGrokking [14] is a phenomenon whereby large LLMs are\ntrained and trained, and may even exhibit overfitting, and then\"\n    },\n    {\n      \"title\": \"IV. BENCHMARKING FOUNDATION MODELS\",\n      \"content\": \"For evaluating Foundation Models' abilities to handle dif-\nferent types of common-sense reasoning, several benchmarks\nexist as outlined in [4], including the following:\nAI2 Reasoning Challenge (ARC): This test assesses\nknowledge and common-sense reasoning through grade-\nschool level multiple choice questions [7].\nHellaSwag: This test evaluates common-sense reasoning\nby requiring models to complete sentences based on\neveryday events, and in this way it evaluates natural\nlanguage inference [27].\nBoolQ: This benchmark consists of real yes/no questions\nfrom Google searches paired with Wikipedia passages. It\nchallenges models to infer answers from context that may\nbe implied but not stated [6].\nOpenBookQA: This question-answering dataset has been\nmodelled after open book exams used for assessing\nhuman understanding of various subjects [16] and is thus\nan evaluation of knowledge retrieval.\nPIQA (Physical Interaction Question Answering): This\nbenchmark evaluates a models' knowledge and under-\nstanding of the physical world by presenting hypothetical\nscenarios with specific goals and multiple choice solu-\ntions [3].\nMultitask Language Understanding (MMLU): This\nbenchmark measures LLM knowledge across multi-\nple different subject areas using multiple choice ques-\ntions [10].\nTruthfulQA [13] is designed to assess the truthfulness of a\nmodel's responses. It achieves this by querying a model's\nresponses on 817 questions of various styles across a\nrange of 38 diverse categories, intentionally constructed\nto challenge both comprehension and accuracy. The out-\nput generated by the model is then scrutinised for signs\nof misinformation.\nM-HALDetect [9] serves as a dataset specifically tailored\nfor evaluating a model's tendency for object hallucina-\ntions. This benchmark is important for identifying in-\nstances where the model may generate outputs containing\nfalse or misleading information.\nWhile this is a large range of benchmarks, almost all\nfocus on some single aspect of a model's response, like\ntruthfulness, knowledge or reasoning. Another weakness is that\nthe question of polluting the test data by the use of training\ndata in order to achieve high scores which undermines any\nsubsequent LLM/FM comparison. Benchmarking to assesses\noverall answer quality is challenging as systems struggle to\naccurately measure the value of free-form responses as such\ncomparisons cannot be based on pairwise comparisons since\nthere is no groundtruth against which to compare.\nA crowdsourced platform called Chatbot Arena [20] [5]\nrun by the Large Model Systems Organization (LMSYS Org)\ndoes address overall model quality by including human-in-\nthe-loop evaluation. A screenshot of Chatbot Arena is shown\nin Figure 3. It uses the Elo rating system [8] widely used in\nchess and other pairwise competitive games, and at the time\nof writing it ranks 114 LLMs (though some are just fine-tuned\nmajor LLMs) by pairing them against each other in \u201cbattles": "nBattles occur where a (real) user prompts the system, a random\nselection of two models respond, the user rates the responses\nand the user judgement is fed into the evaluation. As of June\n2024 more than 1.3 million battles have taken place and while\nthis is a significant amount of feedback what is evaluated by\nthe humans-in-the-loop is really LLM popularity rather than\nLLM quality.\nAnother third party platform for evaluating LLMs, includ-\ning multimodal (vision-language) models, is OpenCompass\n2.0 [19]. This benchmarks more than 100 LLMs, has more\nthan 100 datasets and can perform up to 29 core tasks on those\ndatasets via 400,000 questions. The tasks include information\nretrieval, intention recognition, sentiment analysis, summa-\nriization, critiquing, machine translation, traditional cultural\nunderstanding, Chinese semantic understanding, and multi-\nturn conversation. OpenCompass is a toolchain with a browser\ninterface to a collection of evaluation tools and it culminates\nin a set of leaderboards incorporating both open-source and\nproprietary benchmarks. OpenCompass is open-source and\nreproducible and its evaluation leader boards are based on\nperformance averaged across multiple datasets using multiple\nevaluation metrics.\nOne of the most recent uses of OpenCompass by the Sea-\nNEXT Joint Lab has been to track developments in the sizes\nof recently released Foundation Models vs. their scores on the\nOpenCompass benchmark. Figure 4 illustrates the performance\nof 24 LLMs released between October 2023 and June 2024,\nwith time represented on the x-axis [26]. This visualization\nis one of the outputs from the OpenCompass evaluation. The\ny-axis of Figure 4 is a non-linear depiction of model sizes\nwith the topmost models having unknown sizes. The colour\nof the dot for each model indicates its OpenCompass score.\nThe systems listed include some that voluntarily requested"}, {"title": "V. INTELLIGENCE AND THE HUMAN BRAIN", "content": "The human brain is the most complex system on the planet\nand has held a fascination for scientists for centuries. Neuro-\nscience, the scientific study of the brain and its behaviour, is a\nmultidisciplinary field which has been active for the last 100\nyears.\nEarly neuroscience used brain probes (EEGs) to measure\nelectrical activity in different parts of the human brain in order\nto try to \"map it\" to determine which areas performed which\nkinds of task. The first of these investigations was carried out\nby Hans Berger in July 1924 [18]. Fast forward 100 years and\ncurrent neuroscience still uses brain imaging (fMRI, PET, CT)\nand electrophysiology (EEG, MEG) as imaging techniques\nto observe neural behaviours as a subject performs certain\ntasks. As tasks are performed, neuroscientists use the outputs\nfrom the imaging to build an understanding of the brain's\nstructure, function, physiology and pathology [17]. Yet despite\n100 years of investigation we still don't understand, let alone\ncan cure, neurological impairments like motor neuron disease,\nParkinson's, epilepsy, or dementias, so neuroscience has a long\nway to go.\nIn the study of Foundation Models, Mechanistic Inter-\npretability [2] as mentioned earlier which involves understand-\ning a model from observations of its behaviour under different\ninput situations is equivalent to imaging and probing of the\n(human) brain's neural activation patterns. The most recent\nwork which discovered that combinations of active neurons in\nClaude are monosemantic [22] indicates that while this is a\nfirst step, Mechanistic Interpretability of Foundation Models\nhas a very long way to go, and there is almost no guidance\non a direction from 100 years of experimental research in\nneuroscience.\nOne consolation for us here is that there are major dif-\nferences between the human brain and a Foundation Model.\nThe human brain has about 86 Billion neurons and up to\n100 trillion synapses, which we could equate to parameters\nwhile the largest LLM has 0.3% of such connections. Brain\nsynapses are far more complex than the fixed links with\nweights which we have in Foundation Models, involving\nnumerous biochemical processes. Synapses continuously adapt\nthrough neural plasticity, while Foundation Model links are\nfixed post-training, except for when fine tuning occurs. Brain\nsynapses are energy-efficient compared to Foundation Models\nand neurons and synapses die and new ones generate as we\nage. Finally, the brain has homeostatic regulation in that it\nmaintains overall balance of excitation and activation levels\n(except during seizures) for energy management and it reg-\nlates circadian rhythm whereas Foundation Models, as we\nare just discovering, have irregular bursts of activation [22].\nSo with all these differences perhaps we should not turn to\nneuroscience research for guidance in trying to understand\nFoundation Models."}, {"title": "VI. CONCLUSIONS", "content": "This paper has examined the rapid evolution of Foundation\nModels (FMs) and their growing capabilities in reasoning\nand knowledge representation. We have identified several key\ntrends including:\n1) FMs are demonstrating increasingly sophisticated rea-\nsoning abilities, often surpassing earlier models in tasks\nrequiring common-sense understanding and logical in-\nference.\n2) The phenomenon of grokking, where models suddenly\nexhibit near-perfect performance after extended training,\nsuggests that FMs may be developing deeper, more\nabstract representations of knowledge.\n3) Contrary to initial assumptions, smaller models with\nefficient training techniques are showing competitive\nperformance against larger counterparts, indicating that\nmodel size alone does not determine capability.\n4) The discovery of monosemantic neuron combinations in\nFMs points to the emergence of structured knowledge\nrepresentation within these models, analogous to pattern\nrecognition in biological neural networks.\n5) Current benchmarking methods, while diverse, still\nstruggle to comprehensively evaluate the full spectrum\nof FM capabilities, particularly in assessing overall\nanswer quality and human-like reasoning.\n6) Finally, despite some similarities, fundamental differ-\nences between the structure of FMs and the human\nwarn us against making direct comparisons or expecting\nneuroscience to provide immediate insights into FM\nfunction.\nThese trends collectively suggest that FMs are progressing\ntowards more efficient, interpretable, and potentially more\n\"intelligent\" systems. However, our understanding of their"}]}