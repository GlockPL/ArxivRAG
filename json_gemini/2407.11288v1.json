{"title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems", "authors": ["Ya\u015far Utku Al\u00e7alar", "Mehmet Ak\u00e7akaya"], "abstract": "Diffusion models have emerged as powerful generative techniques for solving inverse problems. Despite their success in a variety of inverse problems in imaging, these models require many steps to converge, leading to slow inference time. Recently, there has been a trend in diffusion models for employing sophisticated noise schedules that involve more frequent iterations of timesteps at lower noise levels, thereby improving image generation and convergence speed. However, application of these ideas for solving inverse problems with diffusion models remain challenging, as these noise schedules do not perform well when using empirical tuning for the forward model log-likelihood term weights. To tackle these challenges, we propose zero-shot approximate posterior sampling (ZAPS) that leverages connections to zero-shot physics-driven deep learning. ZAPS fixes the number of sampling steps, and uses zero-shot training with a physics-guided loss function to learn log-likelihood weights at each irregular timestep. We apply ZAPS to the recently proposed diffusion posterior sampling method as baseline, though ZAPS can also be used with other posterior sampling diffusion models. We further approximate the Hessian of the logarithm of the prior using a diagonalization approach with learnable diagonal entries for computational efficiency. These parameters are optimized over a fixed number of epochs with a given computational budget. Our results for various noisy inverse problems, including Gaussian and motion deblurring, inpainting, and super-resolution show that ZAPS reduces inference time, provides robustness to irregular noise schedules and improves reconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS.", "sections": [{"title": "1 Introduction", "content": "The forefront of deep generative models is now dominated by diffusion models [16, 29, 31, 33, 35] in the intricate task of image generation [11]. Their capabilities extend across various domains, including computer vision [2], natural language processing [17] and temporal data modeling [1]. Recently, diffusion models also showed great success in solving noiseless [5,7,34,35] and noisy inverse problems [6, 21, 30, 32], owing to their capability to model complicated high-dimensional distributions. Linear inverse problems utilize a known forward model given by\n\\[y = Ax_o + n,\\]\nand aim to deduce the underlying signal/image \\(x_o \\in \\mathbb{R}^n\\) from measurements \\(y \\in \\mathbb{R}^m\\), where \\(n \\in \\mathbb{R}^m\\) is measurement noise. In practical situations, the forward operator \\(A: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is either incomplete or ill-conditioned, necessitating the use of prior information about the signal. Posterior sampling approaches use diffusion models as generative priors and incorporates information from both the data distribution and the forward physics model, allowing for sampling from the posterior distribution \\(p(x|y)\\) using the given measurement y [21]. In this context, using Bayes' rule, \\(p(x|y) = \\frac{p(x)p(y|x)}{p(y)}\\), the problem-specific score is\n\\[\\nabla_x \\log p(x|y) = \\nabla_{x_t} \\log p(x) + \\nabla_{x_t} \\log p(y|x),\\]\nwhere \\(\\nabla_{x_t} \\log p(x)\\) is approximated via the learned score model \\(s_\\theta(x_t, t)\\). Many of these strategies utilize a plug-and-play (PnP) approach, using a pre-trained unconditional diffusion model as a prior [4, 9, 13, 18, 24, 38], and integrate the forward model during inference to address various inverse problem tasks.\nThe complexity for these approaches arises in obtaining the latter forward model log-likelihood term in Eq. (1), which guides the diffusion to a target class [11, 29]. While exact calculation is intractable, several approaches have been proposed to approximate this term. Among these, RED-diff [26] employs a variational sampler that uses a combination of measurement consistency loss and score matching regularization. Another technique, DSG [44], uses a spherical Gaussian constraint for denoising steps, allowing for larger step sizes. A class of methods utilize projections onto the convex measurement subspace after the unconditional update through score model [5,8,35]. Although these projections improve consistency between measurements and the sample, they are noted to lead to artifacts, such as boundary effects [7]. Thus, more recent approaches aimed to approximate the log-likelihood term in Eq. (1) different ways. Noting\n\\[p_t(y|x_t) = \\int_{X_0} p(x_0|x_t)p(y|x_0)dx_0,\\]\nDPS [6] uses the posterior mean \\(\\hat{x_0} = \\mathbb{E}_{\\hat{x_0}}(x_t) = \\mathbb{E}[x_0|x_t] = \\mathbb{E}_{x_0 \\sim p(x_0|x_t)} [x_0]\\), to approximate \\(p(y|x_t) = \\mathbb{E}_{x_0 \\sim p(x_0|x_t)} [p(y|x_0)]\\) as\n\\[p(y|x_t) = \\mathbb{E}_{x_0 \\sim p(x_0|x_t)} [p(y|x_0)] = p(y|\\mathbb{E}_{x_0 \\sim p(x_0|x_t)} [x_0]) = p(y|\\hat{x_0}).\\]\nAnother technique, IGDM [32] approximates Eq. (2) as a Gaussian centered around \\(A\\hat{x_0}\\)\n\\[\\int_{X_0} p(x_0|x_t)p(y|x_0)dx_0 \\approx N(A\\hat{x_0}, \\tau^2 A A^T + \\sigma_I^2),\\]\nand uses it for guidance. In these works, log-likelihood weights (or gradient step sizes), \\(\\{\\zeta_t\\}\\) are introduced to further control the reconstruction as\n\\[\\nabla_{x_t} \\log p(x|y) = \\nabla_{x_t} \\log p(x) + \\zeta_t \\nabla_{x_t} \\log p(y|x).\\]\nWhile DPS demonstrates high performance in various inverse problem tasks, it suffers from the drawback of requiring a large number of sampling steps, resulting in prolonged reconstruction time. IGDM accelerates this process by adopting regular (linear) jumps approach across the schedule. However, utilizing more complicated schedules, where the jumps are irregular introduces a challenge, as it requires distinct log-likelihood weights, \\(\\zeta_t\\), for each timestep. Heuristic adjustment of these weights is difficult and frequently leads to undesirable outcomes. In this work, by taking an inspiration from zero-shot/test-time self-supervised models [36,43] we propose to learn the log-likelihood weights for a fixed number of sampling steps and fine-tune them over a few epochs. It is crucial to note that fine-tuning DPS (or IGDM) entails saving computational graphs for each unroll, leading to memory issues and slow backpropagation. Thus, we also propose to approximate the Hessian of the data probability using a wavelet-based diagonalization strategy [12], and learn these diagonal values for each timestep as well. Our key contributions include:\n\u2022 We introduce zero-shot approximate posterior sampling (ZAPS), leveraging zero-shot learning for dynamic automated hyperparameter tuning in the inference phase to improve solution of noisy inverse problems via diffusion"}, {"title": "2 Related Works", "content": "Diffusion Models. During training, diffusion models [16,35] add Gaussian noise to an image with a fixed increasing variance schedule, e.g. linear or exponential, \\(\\beta_1, \\beta_2, ..., \\beta_T\\) until pure noise is obtained, and learns a reverse diffusion process, where a neural network is trained to gradually remove noise and reconstruct the original image. Let \\(x_0 \\sim p_{data}(x)\\) be samples from the data distribution, and \\(x_{1:T} \\in \\mathbb{R}^d\\) be noisy latent variables. By taking \\(\\alpha_t = 1 - \\beta_t\\) and \\(\\bar{\\alpha}_t = \\Pi_{s=1}^t \\alpha_s\\), the Markovian forward process can be written as\n\\[q(x_t|x_0) = N(x_t|\\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t)I).\\]\nBy using the reparameterization trick and Eq. (5), \\(x_t\\) can be sampled as\n\\[x_t(x_0, \\epsilon) = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\text{ where } \\epsilon \\sim N(\\epsilon; 0, 1).\\]\nConsequently, denoising diffusion probabilistic models (DDPMs) [16] learns the reverse process by minimizing a lower bound on the log prior via:\n\\[L_t(\\theta) = \\mathbb{E}_{t,x_0,\\epsilon} ||\\epsilon - \\epsilon_{\\theta}(x_t(x_0, \\epsilon), t)||^2.\\]\nFurhtermore, it can be shown that epsilon matching in Eq. (7) is analogous to the denoising score matching (DSM) [33,40] objective up to a constant:\n\\[min \\mathbb{E}_{x_t, x_0, \\epsilon} ||s_\\theta(x_t, t) - \\nabla_{x_t} \\log q(x_t|x_0)||^2,\\]\nin which \\(s_\\theta(x_t, t) = \\epsilon_\\theta(x_t,t)\\). Using Tweedie's formula and Eq. (6), posterior mean for \\(p(x_0|x_t)\\) can be found as:\n\\[\\hat{x_0} = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (x_t + (1 - \\alpha_t)\\epsilon_{\\theta}(x, t)).\\]"}, {"title": "Sampling", "content": "Sampling \\(x_{t+1}\\) from \\(p(x_{t+1}|x_t)\\) can be done using ancestral sampling by iteratively computing:\n\\[x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t,t)) + \\sigma_t z,\\]\nwhere \\(z \\sim N(0,I)\\) and \\(\\sigma_t^2 = \\beta_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_t\\). It is also worth noting that the DDPM is equivalent to the variance preserving stochastic differential equations (VP-SDEs) [35].\nSolving Inverse Problems via Diffusion Models. When solving inverse problems via diffusion models, the main challenge is to find an approximation to the log-likelihood term, \\(\\nabla_{x_t} \\log p(y|x)\\), as discussed earlier. One recent method, denoising diffusion restoration models (DDRM) [21], utilizes a spectral domain approach, allowing the incorporation of noise from the measurement domain into the spectral domain through singular value decomposition (SVD). However, the application of SVD is computationally expensive [6]. Manifold Constrained Gradient (MCG) [7] method applies projections after the MCG correction as:\n\\[\\begin{aligned}\n\\hat{X_{t+}} &= f(x_t, s_\\theta) - (\\nabla_{x_t} ||K(y - Ax_0)||^2 + g(x_t)z, \\; z \\sim N(0,I), \\\\\nx_{t-1} &= H \\hat{x_{t-1}} + b,\n\\end{aligned}\\]\nwhere \\(K\\) and H are dependent on noise covariance. MCG update of Eq. (11) projects estimates onto the measurement subspace, thus they may fall off from the data manifold [6]. Hence, DPS proposes to update without projections as:\n\\[x_{t-1} = x_{t-1} - \\zeta_t \\nabla_{x_t} ||y - Ax_0||^2,\\]\nNote Eq. (13) is equivalent to Eq. (11) when K = I, and it reduces to the following when the forward operator is linear:\n\\[x_{t-1} = x_{t-1} + \\zeta_t \\frac{\\partial \\hat{x_0}}{\\partial x_t} A^T (y - A\\hat{x_0})\\]\nIGDM [32], on the other hand, utilizes a Gaussian centered around \\(\\hat{x_0}\\) that is defined in Eq. (9) to obtain the following score approximation:\n\\[\\nabla_{x_t} \\log p_t(y|x_t) \\approx \\frac{\\partial \\hat{x_0}}{\\partial x_t} A^T (\\tau^2 A A^T + \\sigma_I^2)^{-1}(y - A\\hat{x_0}).\\]\nIn cases where there is no measurement noise (\\(\\sigma_y = 0\\)), Eq. (15) simplifies to:\n\\[\\nabla_{x_t} \\log p_t(y|x_t) \\simeq \\frac{\\sigma_y^{-2} \\partial \\hat{x_0}}{\\partial x_t} A^{\\dagger} (y - A\\hat{x_0})\\]\nwhere \\(A^{\\dagger}\\) denotes the Moore-Penrose pseudoinverse of A. We note that using Woodbury matrix identity (derived in SuppMat), one can simplify Eq. (15) to:\n\\[\\nabla_{x_t} \\log p_t(y|x_t) \\approx \\frac{\\partial \\hat{x_0}}{\\partial x_t} (A^T A + \\sigma_y^2 I)^{-1}A^T (y - A\\hat{x_0}), \\text{ where } \\eta = \\frac{\\sigma_y^2}{\\tau^2}.\\]\nFrom Eq. (17), the similarity between DPS and IGDM updates can be seen, with \\((A^T A + \\eta I)^{-1}\\) term being the difference. Note the DPS update in Eq. (13) works with non-linear operators, while IGDM's update does not rely on the differentiability of the forward operator, as long as a pseudo-inverse-like operation can be derived."}, {"title": "Improved Irregular Noise Schedules for Image Generation", "content": "Improved Irregular Noise Schedules for Image Generation. Diffusion models typically utilize well-defined fixed noise schedules, with examples including linear or exponential ones. Lately, more sophisticated methods have been developed that sweep across these schedules and take samples in irregular timesteps [11,19] for unconditional image generation. The idea behind this strategy hinges on more frequent sampling for lower noise levels, making it possible to use considerably less number of sampling steps.\nMost of the aforementioned studies that solve inverse problems via diffusion models used the same number of steps that the unconditional diffusion model was trained for [6,7,35]. Nonetheless, there has been a notable trend favoring shorter schedules characterized by linear jumps for inverse problems, where the log-likelihood weights were hand-tuned by trial-and-error [26,32] when using reduced number of steps. While these approaches have proven effective, they still require a large number of sampling steps or heuristic tuning of the log-likelihood weights, \\(\\{\\zeta_t\\}\\) in Eq. (4) to achieve good performance. The former issue leads to lengthy and potentially impractical computational times, while the latter issue results in generalizability difficulties for adoption at different measurement noise levels and variations in the measurement operators. Furthermore, the irregular jump strategy that has been powerful for image generation has not garnered significant attention for inverse problems, mainly due to the impracticality of empirically tuning the log-likelihood weights. Thus, a method that automatically selects and adjusts log-likelihood weights based on the provided measurements for arbitrary noise schedules, instead of requiring manual tuning, holds significant potential for improving robustness and image quality."}, {"title": "3 Methodology", "content": "3.1\nZero-shot Fine Tuning of Log-Likelihood Weights\nIn this work, we propose a robust automated approach for setting the log-likelihood weights at each timestep for arbitrary noise sampling schedules to improve posterior sampling with the given measurements during inference. This allows for a stable reconstruction for different sweeps across noise schedules. Furthermore, the weights themselves are image-specific, which improves the performance compared to the former approaches. For estimating the likelihood in Eq. (1), we use the update in DPS [6]:\n\\[\\nabla_{x_t} \\log p(y|x_t) \\sim \\frac{\\partial \\hat{x_0}}{\\partial x_t} ||y - A\\hat{x_0}||^2 \\approx \\frac{\\partial \\hat{x_0}}{\\partial x_t} A^T (y - A\\hat{x_0}),\\]"}, {"title": "3.2 Approximation for the Hessian of the Log Prior", "content": "Implementing the zero-shot update for Eq. (20) poses various challenges, since backpropagation through the unrolled network to update all \\(\\{\\zeta_t\\}\\) requires another backpropagation through the Jacobian of the score function at each time step. This can only be done by retaining the computational graphs that are created when calculating the Jacobian term in Eq. (20), which quickly explodes memory requirements, especially when the number of sampling steps increases. Also, backpropagating through multiple graphs at the end to only update the log-likelihood weights is time-inefficient and causes prolonged sampling times. Hence, we propose to approximate the Jacobian using inspirations from wavelet-based signal processing techniques and propose to learn this approximation to improve the overall outcome. Noting that \\(s_\\theta(x_t, t)\\) in Eq. (19) is an approximation of the log-gradient of the true prior p(x), we have\n\\[\\frac{\\partial \\hat{x_0}}{\\partial x_t} \\approx \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (I + (1 - \\bar{\\alpha}_t) \\frac{\\partial^2 \\log p_t(x_t)}{\\partial x_t^2}).\\]\nIn order to make a backpropagation to update these weights, one needs to calculate the Hessian matrix, \\(\\frac{\\partial^2 \\log p_t(x_t)}{\\partial x_t^2}\\) given in Eq. (21). This matrix is the negative of the observed Fisher information matrix, whose expected value is the Fisher information matrix. It is also known that in the limit, it approximates"}, {"title": "4 Evaluation", "content": "4.1 Experimental Setup and Implementation Details\nWe comprehensively evaluated our method, examining its performance through both qualitative and quantitative analyses using FFHQ [20] and ImageNet [10] datasets with size 256 \u00d7 256 \u00d7 3. Pre-trained unconditional diffusion models trained on FFHQ and ImageNet were taken from [5] and [11] respectively, and used without retraining. For our experiments, we sampled 1000 images from FFHQ and ImageNet validation sets. All images underwent pre-processing to be normalized in the range [0, 1]. During all the evaluations, a Gaussian measurement noise with \u03c3 = 0.05 was used. For the orthogonal DWT, Daubechies 4 wavelet was utilized. For our quantitative evaluations, we employed 30 sampling steps with a schedule of \"15,10,5\", and 10 epochs for fine-tuning, resulting in a total of 300 NFEs. As noted in [11], superior schedules may exist but it requires substantial computational time to try out all possible schedules. Thus, we opted a schedule that is simple, and samples more frequently at the lower noise levels [11]. More details about the network architectures and hyperparameter choices are given in SuppMat.\n4.2 Experiments on Linear Inverse Problems\nProblem Setup. We focused on the following linear inverse problems: (1) Gaussian deblurring, (2) inpainting, (3) motion deblurring, (4) super-resolution. For"}, {"title": "4.3 Ablation Studies", "content": "We conducted three distinct ablation studies to investigate critical aspects of our algorithm's performance. The first ablation study compared combinations of different timesteps and epochs with a fixed NFE budget, providing a nuanced exploration into the influence of specific combinations on the model's behavior. Specifically, we explored the reconstruction capabilities of the model qualitatively and quantitatively by varying the length of model timesteps, \\(S\\in \\{20,30,60\\}\\). For a fixed NFE budget of 300, these corresponded to 15, 10 and 5 epochs for zero-shot fine-tuning respectively. Notably, all the estimates are similar, though sharpness improves slightly as S increases. However, the trade-off for choosing a high S is the low number of epochs. Especially for cases, where the measurement system or noise level changes, this makes fine-tuning susceptible to initialization of the hyperparameters as it is more difficult to converge to a good solution in ~ 5 epochs. Thus, for improved generalizability and robustness, we opted to use S = 30 and 10 epochs for our database testing.\nOur second ablation study analyzed the performance of ZAPS with respect to other state-of-the-art methods when all methods used the same NFE."}, {"title": "4.4 Limitations", "content": "The loss function we use, \\(L(y, x_0) = ||y - Ax_0||^2\\), resembles a deep image prior-like loss [39]. However, note that there is a subtle difference in our context, where it corresponds to the log-likelihood of \\(p(y|x_0)\\), which is different then the (approximate) log-likelihood guidance term \\(p(y|x_t)\\) used at each time-step. This allows for more robustness to overfitting that is typically observed in DIP-type methods. Further overfitting avoidance measures can be taken by data-splitting [3, 23, 27, 42, 43], though this was not necessary for the small number of epochs used for fine-tuning. Additionally, while our approximation in Eq. (22) produces competitive results, it is important to keep in mind that wavelets may not fully decorrelate the observed Fisher information matrix. Finally, we note that while we chose DPS as a baseline for its versatility in inverse problem tasks, the adaptive weighting strategy in ZAPS, as well as our Hessian approximation, are applicable to other posterior sampling diffusion models for inverse problems."}, {"title": "5 Conclusion", "content": "In this work, we proposed a novel approach named zero-shot approximate posterior sampling (ZAPS), which harnesses zero-shot learning for dynamic automated hyperparameter tuning during the inference phase to enhance the reconstruction quality of solving linear noisy inverse problems using diffusion models. In particular, learning the log-likelihood weights facilitates the usage of more complex and irregular noise schedules, whose feasibility for inverse problems was shown, to the best of our knowledge, for the first time in this paper. These irregular noise schedules enabled high quality reconstructions with 20 50x fewer timesteps. When number of epochs for fine-tuning is also considered, our approach results in a speed boost of approximately 3\u00d7 compared to state-of-the-art methods like DPS. Quantitative and qualitative evaluations on natural images illustrate our method's ability to attain state-of-the-art performance across diverse inverse problem tasks."}, {"title": "A Implementation Details", "content": "A.1 Irregular Noise Schedules\nSampling process for diffusion models can be accelerated via skipping some steps in the diffusion process [11,19,31]. A straightforward approach is to use uniformly spaced jumps across the noise schedule where the sampling path is uniformly spaced out by the selected number of steps in a regular manner. A schedule we commonly use in this study is a \"15, 10, 5\" schedule, which is pictorially depicted in Fig. 6b. This amounts to partitioning the total number of steps used in training into 3 parts and taking uniformly spaced 5, 10, and 15 samples from the respective segments, leading to increased sampling frequency at the lower noise levels. Although the samples are taken uniformly inside a given segment, each segment has different number of steps, making the whole schedule irregular . We note that this irregular noise schedule is based on the ones proposed in [11], and the number of segments and the number of steps in each segment are chosen for the inverse problem setup based on computation time constraints while ensuring generalizability. We also note that a superior schedule may exist for a specific inverse problem, and optimization of these irregular noise schedules is an open problem to the best of our knowledge.\nA.2 Model Details\nPre-trained models for FFHQ and ImageNet were taken from [5] and [11], respectively. Both score models were used without any retraining. For our approximation for the Hessian of the log prior, we utilized Daubechies 4 (db4) wavelet as the orthogonal wavelet transform. For our database evaluation, we employed 30 timesteps with \"15, 10, 5\" schedule for 10 epochs. Furthermore, for simplicity, we opted to initialize the learnable \\(\\{\\zeta_t\\}\\) and \\(\\{D_t\\}\\) values uniformly across steps and diagonals, respectively. For \\(\\{\\zeta_t\\}\\) initialization in Gaussian and motion blur, 0.2 was chosen. For random inpainting and super-resolution, 0.1 was used. For all inverse problem tasks, diagonals of \\(\\{D_t\\}\\) were initialized to 0.2. Adam optimizer with default settings was used.\nA.3 Baseline Implementations\nDDRM. We followed the original implementation code provided by [21], and used the default values of \u03b7 = 0.85 and \u03b7\u03b2 = 1.0 with 20 NFE DDIM.\nScore-SDE, MCG, and DPS. For DPS implementation, we followed the original code provided by [6], while for MCG, we additionally performed projections onto the measurement set. For Score-SDE, we again employed projections onto the measurement set, without any gradient term to guide the diffusion process. We used 1000 NFE for each unless otherwise stated."}, {"title": "A.4 Different Sampling Strategies", "content": "It is possible to use deterministic sampling schemes, such as denoising diffusion implicit models (DDIM) [31], to sample from a pre-trained DDPM model. Forward process for DDIM can be expressed as\n\\[q_\\theta (X_t|X_{t-1}, X_0) = \\frac{q_\\theta (X_{t-1}|X_t, X_0) \\cdot q_\\theta (X_t|X_0)}{q_\\theta (X_{t-1}|X_0)}.\\]\nAs evident from observation, each \\(x_t\\) is not solely dependent on \\(x_{t-1}\\) but also on \\(x_0\\), rendering the forward process non-Markovian. Given a noisy observation \\(x_t\\), the reverse process involves initially predicting the corresponding denoised \\(X_0\\) via Tweedie's formula\n\\[\\hat{x_0} = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon_{\\theta}(x, t)}{\\sqrt{\\bar{\\alpha}_t}}.\\]\nUsing this estimate, one can generate a sample \\(x_{t-1}\\) from a sample \\(x_t\\) via:\n\\[x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\hat{x_0} + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma^2} \\cdot \\epsilon_{\\theta}(x_t, t) + \\sigma z,\\]\nwhere \\(\\sigma = \\eta \\cdot \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_{t}}}\\) and \\(z \\sim N(0, I)\\). When \\(\\sigma = 0\\), sampling process becomes deterministic. We utilized both DDPM and DDIM with the"}, {"title": "B Additional Quantitative Results", "content": "B.1 ImageNet Results\nTab. 6 depicts quantitative evaluation of the state-of-the-art methods using LPIPS, SSIM, and PSNR for noisy inverse problems (\u03c3 = 0.05) on the ImageNet database. ZAPS shows competitive quantitative results either as the best or the second best among all the state-of-the-art methods.\nB.2 Comparisons with DDNM and DiffPIR\nWe further compared ZAPS with the recently proposed DDNM [41] and DiffPIR [45] for Gaussian deblurring and super-resolution (\u00d74) tasks (see Tab. 7)."}, {"title": "B.3 Different Total Epochs - Timesteps Combinations with Fixed Total NFEs", "content": "As explained in our first ablation study in the main text, we also assessed the effectiveness of various combinations of total timesteps in the posterior sampling and number of epochs for fine-tuning quantitatively while keeping the NFE constant. As anticipated, decreasing the number of epochs to 5 to allocate more timesteps had an adverse impact, where for some measurements, log-likelihood weights and approximated diagonals did not have the time to stabilize during the fine-tuning. Also as expected, 15 epochs \u00d7 20 timesteps combination and 10 epochs \u00d7 30 timesteps had similar outcomes, in which the latter outperformed the former slightly, and was used in the study."}, {"title": "C Additional Qualitative Results", "content": "C.1 Effect of Using Distinct Weights {$\\zeta_t$}\nAs part of our ablation studies, we examined the influence of selecting a shared weight for every step versus using distinct weights \\(\\zeta_t\\) for each timestep.  shows that the shared approach leads to artifacts that are highlighted in the"}, {"title": "C.2 Effect of Higher Number of Epochs for Fine-Tuning in ZAPS", "content": "We evaluated our method on a representative ImageNet sample over 30 epochs for motion deblurring inverse problem task using 30 steps. As seen from Fig. 9, both the reconstruction faithfulness, and the visual quality, measured by PSNR and LPIPS respectively, demonstrate an increase via fine-tuning. However, after the 10th epoch, the performance saturates and the gain through the log-likelihood weight update is diminished. Furthermore, no DIP-like overfitting was observed owing to the differences between the training loss function and the log-likelihood update term, as discussed in the main text. Thus, 10 epochs were used as the maximum number of epochs for the 30 step ZAPS setting to minimize total NFES."}, {"title": "C.3 Effect of Wavelet Transform Choice", "content": "We further investigated using different types of orthogonal wavelets from the Daubechies wavelet family in Fig. 10. As seen from the results, the effect of the wavelet selection is negligible. Therefore, we opted to use Daubechies 4 wavelet as it is commonly used in sparse signal processing literature [25]."}, {"title": "C.4 Additional Experimental Results", "content": "Further qualitative experimental results, comparing ZAPS with our state-of-the-art baseline, DPS, for various noisy inverse problem tasks (\u03c3 = 0.05) are given in Figs. 11 to 16. We also provide inpainting task outcomes in Fig. 17 for various types of masks in addition to random and rectangular box inpainting. Our approach, involving the adjustment of the log-likelihood weights during fine-tuning and integration of irregular noise schedules with fewer sampling steps, results in a notable acceleration of approximately \u00d73 on the FFHQ dataset and around \u00d74 on ImageNet dataset, while also delivering superior performance."}]}