{"title": "A Survey of Model Extraction Attacks and Defenses in Distributed Computing\nEnvironments", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "abstract": "Model Extraction Attacks (MEAs) threaten modern\nmachine learning systems by enabling adversaries\nto steal models, exposing intellectual property and\ntraining data. With the increasing deployment of\nmachine learning models in distributed computing\nenvironments, including cloud, edge, and federated\nlearning settings, each paradigm introduces dis-\ntinct vulnerabilities and challenges. Without a uni-\nfied perspective on MEAs across these distributed\nenvironments, organizations risk fragmented de-\nfenses, inadequate risk assessments, and substan-\ntial economic and privacy losses. This survey is\nmotivated by the urgent need to understand how\nthe unique characteristics of cloud, edge, and fed-\nerated deployments shape attack vectors and de-\nfense requirements. We systematically examine\nthe evolution of attack methodologies and defense\nmechanisms across these environments, demon-\nstrating how environmental factors influence secu-\nrity strategies in critical sectors such as autonomous\nvehicles, healthcare, and financial services. By\nsynthesizing recent advances in MEAs research\nand discussing the limitations of current evaluation\npractices, this survey provides essential insights for\ndeveloping robust and adaptive defense strategies.\nOur comprehensive approach highlights the impor-\ntance of integrating protective measures across the\nentire distributed computing landscape to ensure\nthe secure deployment of machine learning models.", "sections": [{"title": "1 Introduction", "content": "Model extraction attacks (MEAs) and their defenses represent\na critical challenge for the security of modern machine learn-\ning systems. In these attacks, adversaries aim to reconstruct a\ntarget model's functionality by exploiting various interfaces,\npotentially compromising both intellectual property and sen-\nsitive training data. The prevalence of such attacks has grown\nsignificantly with the emergence of Machine-Learning-as-a-\nService (MLaaS) platforms, where pre-trained models are de-\nployed as services accessible through standardized Applica-\ntion Programming Interfaces (APIs). These platforms, while\nfacilitating rapid deployment and scalability, create oppor-\ntunities for systematic query-based attacks that can recon-\nstruct model functionality with high fidelity by leveraging\nrich output information such as confidence scores and prob-\nability distributions [Tram\u00e8r and others, 2016]. The secu-\nrity challenges of model extraction become increasingly com-\nplex as machine learning systems are deployed across di-\nverse distributed computing environments, including cloud,\nedge, and federated learning settings, each introducing dis-\ntinct vulnerabilities. In cloud computing environments, the\nwidespread adoption of MLaaS platforms exposes models\nthrough APIs, making them particularly vulnerable to query-\nbased extraction attacks [Gong and others, 2020]. Edge com-\nputing environments face unique challenges from hardware-\nlevel threats, where physical accessibility enables exploita-\ntion through power analysis [Xiang and others, 2020] and\nelectromagnetic emanations [Yu and others, 2020]. In feder-\nated learning settings, the collaborative nature of model train-\ning creates additional attack surfaces through gradient sharing\nmechanisms, potentially exposing both model parameters and\ntraining data [Zhu and others, 2019].\nThese emerging threats raise several critical questions. Q1:\nWhat are the unique attack surfaces and challenges in dif-\nferent computing environments? Different computing envi-\nronments exhibit distinct vulnerabilities. For example, cloud\nplatforms are exposed to query-based extraction due to rich\nAPI outputs, edge devices face risks from physical access and\nside-channel leakage, and federated learning systems are sus-\nceptible to information leakage via shared gradients. Fail-\ning to understand and address these differences can result\nin inadequate defenses, leaving systems highly vulnerable to\nexploitation. Q2: What are the key applications and secu-\nrity requirements across computing environments? Each de-\nployment scenario imposes unique security demands. Cloud\nMLaaS requires robust protection of intellectual property,\nedge computing demands real-time inference security un-\nder resource constraints, and federated learning necessitates\nprivacy-preserving collaborative training. If these diverse re-\nquirements are not properly met, the consequences may in-\nclude financial losses, compromised system safety, and ero-\nsion of public trust in AI services. Q3: How can we effec-\ntively evaluate and measure the security of ML models across\nenvironments? Effective evaluation calls for unified metrics"}, {"title": "2 Preliminaries", "content": "2.1 Model Extraction Basics\nAttack Definition. Model extraction attacks (MEA) pose a\nsignificant security threat to deployed machine learning sys-\ntems by enabling adversaries to recover either the exact pa-\nrameters or an approximation of the target model M. We\ndefine model extraction as an attack in which an adversary\naims to steal, approximate, or replicate a target model us-\ning query access to its predictions. The goal of MEA varies:\nsome attacks attempt to extract the exact parameters of M,\nwhile others seek to construct a functionally similar substi-\ntute model M' that mimics the decision boundary of M with\nhigh fidelity.\nThe attack process involves querying M with an input\nx \u2208 X and collecting the corresponding output M(x). Using\nthese query-response pairs, the adversary constructs an ex-\ntracted dataset: $D_{ext} = {(x_i, M(x_i)) | x_i \\sim X, 1 \\leq i \\leq N}$,\nwhere X denotes the input domain, N is the number of\nqueries made to M. The adversary then optimizes a surro-\ngate function f'(\u00b7) in order to approximate the target model's\nfunction f(.). This is typically achieved by minimizing a loss\nfunction l(,), resulting in the extracted model M':\n$M' = arg \\min_{M'} \\sum_{(x,M(x)) \\in D_{ext}} l(f'(x), M(x)),$                                  (1)\nwhere l(,) quantifies the discrepancy between the extracted\nmodel's output f'(x) and the original model's output M(x).\nThreat Model. The threat model for model extraction attacks\nis primarily defined by the extent of an attacker's knowledge\nand capabilities. In practice, two settings are commonly ob-\nserved. In the black-box setting, which is the mainstream sce-\nnario in cloud-based MLaaS environments, the attacker has\naccess only to the model's input-output behavior via APIs.\nIn contrast, in the gray box setting, which is more frequently\nencountered in edge computing and federated learning, the at-\ntacker also gains partial information, such as details about the\nmodel architecture or training data distribution, though with-\nout full access to the model parameters [Jagielski and others,\n2020]. This distinction is practically significant: black-box\nattacks are easier to execute in publicly accessible cloud ser-\nvices, whereas gray-box attacks, which leverage additional\ninformation such as side-channel data or gradient updates,\ntend to be more difficult to defend against. In both set-\ntings, the effectiveness of the attack is constrained by practi-\ncal limitations, including the query budget B, computational\nresources, and time constraints, all of which strongly influ-\nence the choice and success of attack strategies. In our survey,\nwe classify extraction attacks based on the adversary's knowl-\nedge. In cloud computing, attacks are predominantly black-\nbox, relying solely on API query-response interactions. In\ncontrast, in edge computing and federated learning, attacks\nare generally gray-box, as attackers may also exploit addi-\ntional information such as side-channel data or shared gradi-\nent updates. This clear distinction is essential for designing\nenvironment-specific defense strategies.\nDefense Strategies. To counter model extraction attacks, de-\nfense mechanisms are designed to modify the model's out-\nput in a manner that increases the difficulty for an adversary"}, {"title": "2.2 Computing Environment Overview", "content": "Cloud Computing Infrastructure. Cloud computing envi-\nronments [Qian and others, 2009] provide centralized model\nserving through APIs, where models are typically accessed\nremotely through well-defined interfaces. This environment\nfaces challenges from query-based attacks, where adversaries\ncan systematically probe the model through its API. The main\nsecurity implications in cloud settings involve managing API\naccess, monitoring query patterns, and protecting model in-\nputs and outputs [Azodolmolky and others, 2013]. Cloud-\nbased defenses typically focus on API-level protection and\nquery monitoring systems [Abbasov, 2014].\nEdge Computing Systems. Edge computing [Khan and oth-\ners, 2019] moves model deployment closer to data sources,\nintroducing distinct security considerations. Models de-\nployed on edge devices may be vulnerable to physical ac-\ncess and side-channel attacks [Satyanarayanan, 2017]. Ad-\nversaries can exploit hardware-level information such as tim-\ning patterns, power consumption O(M, x), or electromag-\nnetic emissions [Ahmed and others, 2017]. The distributed\nnature of edge computing also creates challenges in maintain-\ning consistent security measures across multiple deployment\npoints. Edge environments require specialized hardware se-\ncurity measures and physical access controls.\nFederated Learning Framework. Federated learning en-\nables collaborative model training across distributed devices\nwithout sharing raw data [McMahan and others, 2017]. In\ntypical federated learning settings, a central server coordi-\nnates multiple clients to jointly train a model, where clients\nperform local training and only share model updates while\nkeeping their training data private [Li and others, 2020a].\nMEA in this context can occur from two perspectives: (1) a\nmalicious server attempting to reconstruct client training data\nor local models from received updates [Zhu and others, 2019;\nNasr and others, 2019], or (2) corrupt clients seeking to\nextract information about other participants' private data\nthrough the globally shared model [Wang and others, 2019].\nSpecifically, during each training round, clients download the\nglobal model, compute local updates using private data, and\nsend these updates to the server for aggregation. To mitigate\nthese risks, federated systems commonly implement secure\naggregation protocols [Bonawitz and others, 2017] and differ-\nential privacy mechanisms [Abadi and others, 2016] to pro-\ntect both local updates and the global model while preserving\nthe benefits of collaborative learning."}, {"title": "3 Model Extraction in Cloud Computing", "content": "MLaaS Overview and Vulnerabilities. Machine Learn-\ning as a Service (MLaaS) platforms have become increas-\ningly popular, offering pre-trained models and deployment\nservices through cloud interfaces. These platforms expose\nmodels through API endpoints, making them primary tar-\ngets for model extraction attacks [Tram\u00e8r and others, 2016;\nWang and Gong, 2018]. The key vulnerabilities stem from\nthe standardized API interfaces where attackers can system-\natically query the model, collecting input-output pairs to train\nsubstitute models. The effectiveness of these attacks is often\nenhanced by the high-quality responses provided by cloud\nAPIs, which may include confidence scores or probability\ndistributions [Papernot and others, 2017]. Cloud service in-\nterfaces present multiple exploitation opportunities beyond\nbasic query-response interactions, where attackers can lever-\nage batch processing capabilities, exploit rate limiting mecha-\nnisms through distributed queries, and utilize multiple service\ntiers to gather different levels of model information [Shokri\nand others, 2017]. In cloud settings, the adversary is typi-\ncally constrained by a query budget, B, and collects a set of\nquery-response pairs:\n$D_{ext} = {(x_i, M(x_i)) | x_i \\in X, 1 \\leq i \\leq N}, N \\leq B$.\nThe attacker then trains a substitute model by solving\n$M' = arg \\min_{M'} \\sum_{i=1}^{N} l(f'(x_i), M(x_i)),$                                     (5)\nwhere l(,) quantifies the discrepancy between the substitute\nmodel's prediction and the target model's output.\nApplications and Impact. Model extraction attacks in\ncloud environments significantly impact several key indus-\ntries where high-value ML models are deployed. In financial\nservices, proprietary trading models and risk assessment sys-\ntems are prime targets, where successful extraction could lead\nto substantial financial losses and market manipulation [Ke-\nsarwani and others, 2018]. Credit scoring models, particu-\nlarly vulnerable to query-based attacks, could reveal sen-\nsitive decision-making criteria and compromise competitive\nadvantages. Enterprise ML services handling business intel-\nligence face threats of corporate espionage, where extracted\nmodels could expose strategic insights and customer behav-\nior patterns [Gong and others, 2020]. Healthcare providers\nusing cloud-based diagnostic models risk both intellectual\nproperty theft and patient privacy breaches through model ex-\ntraction attempts. Public cloud APIs serving general-purpose\nmodels, such as computer vision or natural language pro-\ncessing services, face widespread extraction attempts due to\ntheir accessibility and valuable training data [Yang and oth-\ners, 2024]. The impact varies by sector - financial institutions\nmay lose proprietary trading advantages, healthcare providers\nrisk compromising patient care quality, and technology com-\npanies may suffer decreased market competitiveness.\nDefense Mechanisms and Challanges. To counter these\nthreats, cloud providers deploy a range of defense mecha-\nnisms. For example, query monitoring systems are used to\ndetect abnormal query patterns, while access control mea-\nsures restrict unauthorized API usage. In addition, model\nprotection techniques, such as prediction perturbation and\nconfidence score truncation, are applied to obscure the de-\ntailed output information that attackers exploit [Juuti and oth-"}, {"title": "4 Model Extraction in Edge Computing", "content": "Edge Computing Vulnerabilities. Edge computing environ-\nments present unique vulnerabilities for model extraction at-\ntacks due to their distributed nature and physical accessibil-\nity. The deployment of ML models on resource-constrained\ndevices like smartphones, IoT sensors, and embedded sys-\ntems creates distinctive attack surfaces [Kumar and others,\n2021]. Unlike cloud environments, edge devices are phys-\nically accessible to attackers, enabling hardware-level side-\nchannel attacks that exploit power consumption [Breier and\nothers, 2021], electromagnetic emanations [Batina and oth-\ners, 2019], and timing information [Hu and others, 2019].\nThe resource constraints of edge devices often necessitate the\nuse of compressed or quantized models, which may be more\nsusceptible to extraction attempts [Rakin and others, 2022].\nAdditionally, the distributed architecture of edge computing\nsystems expands the attack surface, as adversaries can target\nmultiple interconnected devices to piece together model in-\nformation [Meyers and others, 2024]. In an edge scenario, an\nattacker may collect not only query-response pairs but also\nside-channel measurements S(x) for each query. The aug-\nmented extracted dataset can be modeled as\n$D_{edge} = {(x_i, M(x_i), S(x_i)) | x_i \\in X, 1 \\leq i \\leq N}$.\nThe adversary then trains a substitute model by minimizing\na joint loss that accounts for both the model output and the\nside-channel signal:\n$M' = arg \\min_{M'} \\sum_{i=1}^{N} [l(f'(x_i), M(x_i)) + \\lambda l_s(s'(x_i), S(x_i))],$         (6)\nwhere l(,) measures the discrepancy in the model outputs,\nls(,) quantifies the error in the side-channel signal estima-\ntion, and \u03bb is a weighting parameter.\nApplications and Impact. The impact of model extraction\nattacks in edge computing spans various critical industries. In\nautonomous vehicles, edge-deployed perception models are\nprime targets, where successful extraction could compromise\nvehicle safety and reveal proprietary driving algorithms [Mao\nand others, 2017]. These models, processing real-time sen-\nsor data for object detection and path planning, are particu-\nlarly vulnerable to side-channel attacks through physical ac-\ncess to vehicle systems [Nazari and others, 2024]. In smart\nmanufacturing, industrial IoT devices running quality con-\ntrol or predictive maintenance models face extraction risks\nthat could expose trade secrets and manufacturing processes.\nSmart healthcare devices operating at the edge contain sen-\nsitive diagnostic models where extraction could compromise"}, {"title": "5 Model Extraction in Federated Learning", "content": "Federated Learning Vulnerabilities. Federated Learning\n(FL) introduces unique vulnerabilities to model extraction at-\ntacks due to its distributed and collaborative nature. Unlike\ntraditional centralized systems, FL exposes model updates\nand gradients during the training process, creating new attack\nsurfaces [Nasr and others, 2019]. The primary vulnerabil-\nity stems from the necessity to share model updates between\nparticipants, which can leak information about local training\ndata and model architectures [Zhu and others, 2019]. Ma-\nlicious participants can exploit these shared updates through\ngradient leakage attacks to reconstruct training samples or in-\nfer model properties [Zhao and others, 2020]. Additionally,\nthe iterative nature of FL allows adversaries to accumulate in-\nformation over multiple training rounds, potentially enabling\nmore sophisticated reconstruction attacks [Wang and others,\n2019]. The heterogeneous nature of participating devices and\nvarying data distributions also creates opportunities for tar-\ngeted attacks against specific participants [Ganju and others,\n2018]. Formally, let Gt denote the gradient update shared\nby clients at training round t over T rounds. The adversary\ncollects the set\n${G_1, G_2,...,G_T}$.\nThe attacker then trains a substitute model M' by minimizing\nthe discrepancy between the predicted gradient of the substi-\ntute model g'(x, t) and the observed aggregated gradient Gt:\n$M' = arg \\min_{T'} \\sum_{t=1}^{T} l(g'(x, t), G_t),$                               (7)\nwhere l(,) measures the difference between the substitute\nmodel's gradient and the actual gradient, thereby capturing\nthe iterative leakage inherent in FL.\nApplications and Impact. The impact of model extraction\nattacks in FL environments is particularly significant across"}, {"title": "6 Evaluation Measures", "content": "General Evaluation Measures. Across all computing en-\nvironments, researchers typically evaluate model extraction\nattacks by measuring (i) how accurately the substitute model\nreplicates the target model's behavior (e.g., prediction accu-\nracy), (ii) the degree of agreement between the outputs of the\nextracted model and those of the target model, and (iii) the\nnumber of queries required to achieve a given level of replica-\ntion fidelity, as reported in [Jagielski and others, 2020]. In ad-\ndition, some studies consider the trade-off between preserv-\ning model utility for legitimate users and introducing pertur-\nbations or other modifications as a defensive measure, follow-\ning discussions in [Kariyappa and Qureshi, 2020].\nEvaluation in Cloud Computing. In cloud environments,\nwhere API access serves as the primary attack vector, eval-\nuation is centered on the efficiency and cost-effectiveness of\nthe extraction process. For example, Tram\u00e8r et al. [Tram\u00e8r\nand others, 2016] evaluate the number of API queries neces-\nsary to reconstruct the target model under a constrained query\nbudget, while Juuti et al. [Juuti and others, 2019] assess how\ndefensive measures impact service-level metrics such as la-\ntency and throughput. Additionally, research by Kesarwani\net al. [Kesarwani and others, 2018] measures the effective-\nness of detection systems by quantifying the rate at which\nabnormal query patterns are flagged.\nEvaluation in Edge Computing. For edge computing envi-\nronments, evaluation must account for resource constraints\nand the risks posed by physical side channels. Rakin et\nal. [Rakin and others, 2022] examine the overhead imposed\non edge devices in terms of memory, computational load,\nand energy consumption when executing extraction attacks\nand their corresponding defenses. Moreover, studies such\nas Batina et al. [Batina and others, 2019] evaluate how ef-\nfectively defense mechanisms mitigate side-channel attacks\n(e.g., those based on power consumption and electromagnetic\nemissions), and Breier et al. [Breier and others, 2021] inves-\ntigate whether these defenses can preserve the low latency\nrequired for real-time edge applications.\nEvaluation in Federated Learning. In federated learning,\nevaluation focuses on the leakage of information through\nshared gradients and the impact on collaborative model per-\nformance. Nasr et al. [Nasr and others, 2019] quantify leak-\nage by analyzing the gradient updates exchanged during train-\ning, while Zhu et al. [Zhu and others, 2019] assess the degree\nto which the extracted model approximates the target model's\ndecision boundaries. In addition, the cumulative privacy loss\nover multiple training rounds is often measured using frame-\nworks based on differential privacy as introduced by Abadi\net al. [Abadi and others, 2016], and the influence of defense\nmechanisms on model convergence and overall performance\nis carefully evaluated."}, {"title": "7 Challenges and Future Directions", "content": "Evolution of Attack Methodologies. The landscape of\nmodel extraction attacks continues to evolve distinctly across\ncomputing environments, presenting new challenges and re-\nsearch opportunities. In cloud computing, we anticipate the\nemergence of more sophisticated query optimization tech-\nniques that can circumvent rate limiting and detection mech-\nanisms while maintaining high extraction accuracy with min-\nimal API calls [Juuti and others, 2019]. Edge computing en-\nvironments face increasing threats from hybrid attacks that\ncombine physical access with digital techniques - adversaries\nmay simultaneously leverage side-channel information from\nhardware and strategic model queries, making defense par-\nticularly challenging [Batina and others, 2019]. In federated\nlearning settings, advanced gradient manipulation techniques\nare likely to emerge, enabling more precise extraction while\nevading current privacy-preserving mechanisms [Nasr and\nothers, 2019]. The interaction between these different attack\nvectors across computing paradigms presents a significant re-\nsearch challenge, as models increasingly operate across mul-\ntiple environments simultaneously. Understanding how at-\ntacks can transition and adapt across these environments is\ncrucial for developing comprehensive defense strategies."}, {"title": "8 Conclusion", "content": "In this survey, we provide an examination of model extraction\nattacks and defenses. We trace the evolution of these attacks\nfrom basic query-based techniques to multi-channel methods\nthat exploit diverse information channels across cloud, edge,\nand federated learning environments. Our proposed taxon-\nomy, built around core information channels and computing\nparadigms, highlights the unique vulnerabilities and defense\nchallenges inherent in different deployment scenarios. For in-\nstance, cloud-based MLaaS platforms are primarily exposed\nthrough API interfaces, making them vulnerable to query-\nbased extraction, while edge devices suffer from additional\nrisks due to physical accessibility and resource limitations.\nFederated learning systems, with their collaborative training\nprocesses, introduce new attack surfaces through shared gra-\ndient updates that can leak sensitive information. Our analy-\nsis further reveals that the interplay between attack methods\nand the operating environment creates distinct security chal-\nlenges. Cloud services must balance accessibility and protec-\ntion, edge devices need to address both physical security and\nlimited computational resources, and federated learning sys-\ntems require privacy-preserving techniques that do not com-\npromise collaborative benefits. We also review a range of\ndefense strategies and evaluation measures from the litera-\nture, emphasizing that protection mechanisms must be tai-\nlored to each environment in order to maintain an optimal\nbalance between security and performance. Overall, the in-\nsights provided by this survey offer a comprehensive refer-\nence for understanding the current threat landscape and the\nstate of defense mechanisms against model extraction attacks.\nThis work lays a solid foundation for future research aimed\nat developing more robust, adaptive, and scalable protection\nstrategies, which are essential for ensuring the safe and secure\ndeployment of machine learning models across the diverse\nlandscape of modern computing environments."}]}