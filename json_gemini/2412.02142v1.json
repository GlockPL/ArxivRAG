{"title": "Personalized Multimodal Large Language Models: A Survey", "authors": ["Ishita Kumar", "Junda Wu", "Hanjia Lyu", "Yu Xia", "Zhehao Zhang", "Joe Barrow", "Mehrnoosh Mirtaheri", "Hongjie Chen", "Ryan A. Rossi", "Franck Dernoncourt", "Tong Yu", "Ruiyi Zhang", "Jiuxiang Gu", "Nesreen K. Ahmed", "Yu Wang", "Xiang Chen", "Subrata Mitra", "Hanieh Deilamsalehy", "Namyong Park", "Sungchul Kim", "Huanrui Yang", "Julian McAuley", "Zhengmian Hu", "Nedim Lipka", "Dang Nguyen", "Yue Zhao", "Jiebo Luo"], "abstract": "Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.", "sections": [{"title": "1 Introduction", "content": "Multimodal Large Language Models (MLLMs)\u00b9 have recently become important for generating and reasoning with diverse types of complex data such as text, images, and audio (Yang et al., 2023). These models that process, generate, and combine information across modalities have found many applications such as healthcare (Lu et al., 2024; AlSaad et al., 2024), recommendation (Lyu et al., 2024b; Tian et al., 2024), autonomous vehicles (Cui et al., 2024; Chen et al., 2024b). However, to further enhance the performance and utility of these models, personalization plays a crucial role, enabling them to adapt more effectively to a user's specific preferences, context, and needs (Chen et al., 2024a). Personalization offers an improved user experience by saving time and increasing accuracy, for instance, by generating content that is more closely aligned with the user's interests.\nPersonalization in multimodal large language models comes with its own set of unique challenges. In particular, in order to create personalized experiences, it is essential to have individual-level data from users. In multimodal scenarios, this requires data that spans multiple modalities. For instance a user might generate an image from a text prompt and then provide feedback, such as a thumbs-up or thumbs-down. In this case, we have two modalities-text and image\u2014along with implicit feedback on the text prompt and explicit feedback on the generated image in the form of a like or dislike.\nWe propose an intuitive and symmetric taxonomy for the techniques used for personalized multimodal large language models, including text generation (Section 3), image generation (Section 4), recommendation (Section 5), and retrieval (Section 6). Each category classifies techniques based on how they follow multimodal instructions, enable multimodal alignment, generate personalized responses, or incorporate personalization through model fine-tuning. We illustrate an overview of techniques for personalized multimodal models in Table 1. In parallel to the discussion of techniques, we also summarize various applications in personalized MLLMs (Appendix B).\nSummary of Main Contributions. The key contributions of this work are as follows:\n\u2022 A comprehensive survey of existing work for personalized MLLMs. We also survey the problem settings, evaluation metrics, and datasets used in the literature.\n\u2022 We introduce a few intuitive taxonomies for personalization in MLLMs and survey existing work using these taxonomies.\n\u2022 Key open problems and challenges are identified and discussed. These problems are important for future work to address in this rapidly growing but vitaly important field.\nScope of the survey. In this survey, we focus entirely on recent work that leverages multimodal large language models (MLLMs) to generate personalized text, images, audio, or other modalities. We consider techniques for personalization that elicit and incorporate user preferences when generating multimodal outputs. To study these techniques, we decompose works across the following three dimensions:\n\u2022 the modality of the content being generated (e.g., text, images, audio, or other);\n\u2022 the personalization technique being employed (e.g., prompt-based, prefix tuning, finetuning/adapters);\n\u2022 the application of the personalized MLLM (e.g., chat/assistant, recommendation systems, retrieval, classification, image generation, text generation)."}, {"title": "2 Overview of Challenges and Techniques", "content": "Personalization in multimodal large language models presents several significant challenges, due to the complexity of combining diverse types of data, extracting relevant information, and delivering user-specific insights. To tackle these challenges, researchers have introduced techniques such as multimodal instruction, alignment, and generation."}, {"title": "2.1 Integration of Heterogeneous Data", "content": "Multimodal large language models need to combine information from various modalities, such as text, images, audio, video, and user engagement (Wei et al., 2024a; Xu et al., 2024). Each modality has distinct characteristics and may convey different types of information. For example, text might describe a product, while an image conveys its visual appearance. Integrating these heterogeneous data types is challenging because they require different encoding methods, processing pipelines, and alignment strategies. Misalignment or incomplete fusion (Lyu et al., 2024b; Zhou et al., 2023) can lead to inconsistent or inaccurate user preferences being captured, thus reducing the effectiveness of personalized recommendations. Multimodal alignment can help resolve inconsistencies that may arise from mismatched modalities."}, {"title": "2.2 Data Noise and Redundancy", "content": "Different modalities often include noisy, redundant, or irrelevant information (Liu et al., 2024c; Lyu and Luo, 2022). For example, images of the same product in e-commerce platforms may have varying quality or redundant features, while textual descriptions might include unnecessary details. Extracting meaningful insights from such noisy data is challenging because the model needs to filter out irrelevant content without losing important context. This process becomes even more difficult when handling large amounts of data, as the noise accumulates and complicates the extraction of relevant user preferences. Multimodal instruction can help filter out noisy or redundant data by guiding the model to focus on the most relevant modalities and inputs for each user. By directing the model's attention to key features in user interactions, this method reduces the impact of irrelevant or repetitive information, ensuring that the generated outputs are more meaningful and concise."}, {"title": "2.3 Granular Understanding of Multimodal Data", "content": "Text-based LLMs are adept at processing linguistic information such as item descriptions (Lyu et al., 2024a), and some approaches seek to transform non-textual data into the text space (Ye et al., 2024b). However, visual inputs often contain nuances\u2014such as color, texture, and context\u2014that are difficult to capture with language alone (Shen et al., 2024). For instance, subtle preferences in fashion, home decor, or art may be driven by visual factors that are abstract or subjective. Multimodal LLMs may struggle to extract these fine-grained visual details and relate them meaningfully to textual descriptions, leading to a loss of personalization depth. Multimodal alignment facilitates a mre granular understanding by ensuring that the relationships between different modalities are preserved."}, {"title": "2.4 Scalability and Efficiency", "content": "As the volume of multimodal engagement grows, so do the computational demands for processing and personalizing recommendations. Models need to handle a large number of user interactions across various modalities in real-time environments (Ye et al., 2024b; Shen et al., 2023), such as social media platforms or e-commerce sites. This necessitates advanced resource allocation strategies, as multimodal large language models often require significant GPU or TPU resources to process images, videos, or audio in parallel with text."}, {"title": "2.5 Capturing Diverse and Dynamic User Preferences", "content": "Users interact with multimodal content in diverse ways, and their preferences can evolve over time (Rafailidis et al., 2017). Accurately capturing these preferences across modalities is challenging because different data types might signal conflicting or evolving interests. For instance, a user's engagement with both product reviews and product images may shift over time, requiring the model to adapt its understanding of their preferences in real-time. Additionally, the model needs to continuously update its understanding to reflect new patterns of user behavior."}, {"title": "3 Personalized MLLM Text Generation", "content": "Personalized multimodal instruction focuses on guiding MLLMs to generate more tailored content through structured prompts and contextual signals. For example, CGSMP (Yong et al., 2023) demonstrates controllable text summarization using multimodal prompts based on image entities, reducing hallucinations and improving summarization quality. Li et al. (2024c) further propose multimodal in-context tuning leveraging in-context learning abilities of MLLMs to dynamically generate product descriptions based on visual and textual cues."}, {"title": "3.2 Personalized Multimodal Alignment", "content": "To better reflect user intents in generated texts, a few works explore aligning multimodal inputs to personalized user preferences. For instance, MPDialog (Agrawal et al., 2023) aligns character personas and visual scenes to generate context-consistent dialogues. Athena 3.0 (Fan et al., 2023) apply this concept to conversational agents, fusing neuro-symbolic strategies with multimodal dialogue generations, aligning responses with user preferences in dynamic contexts. In addition, Sugihara et al. (2024) aligns video summarization with user-defined semantics by matching textual and visual content, ensuring personalized summaries."}, {"title": "3.3 Personalized Multimodal Generation", "content": "To generate text that aligns more closely with user-specific preferences, Wu et al. (2024b) introduce a framework for personalized video commenting, where clip selection and text generation processes are tailored to user preferences. Wang et al. (2024a) generate personalized time-synchronized comments on videos by leveraging a multimodal transformer to integrate visual elements with user-specific commentary."}, {"title": "3.4 Personalized Multimodal Fine-tuning", "content": "While prompting and instructions might not always achieve satisfying performance, several fine-tuning methods have been developed to help better adapt pre-trained MLLMs to specific user contexts and tasks. Wang et al. (2023) propose prefix-tuning for personalized image captioning, reducing computation costs while retaining high-quality, user-specific outputs. Pi et al. (2024) propose visual instruction tuning to address the limitations of generic MLLMs by enabling them to recognize individuals in images through visual instructions and generate personalized dialogues."}, {"title": "4 Personalized MLLM Image Generation", "content": "Zhong et al. (2024) propose a novel multimodal prompt to include complex user queries for customized instructions. Gal et al. (2022) enables multimodal input to be tokenized into a lookup table, whose indexes are further used for generation based on a text transformer. MuDI (Jang et al., 2024) addresses identity mixing in multi-subject text-to-image personalization by leveraging segmented subjects using the Segment Anything Model (Kirillov et al., 2023). MuDI employs data augmentation (Seg-Mix) during training and an innovative inference initialization technique to generate distinct multi-subject images without mixing identities. Subject-Diffusion (Ma et al., 2024) introduces an open-domain personalized text-to-image generation framework that does not require test-time fine-tuning and relies on a single reference image for generating personalized images. The method combines text and image features using a custom prompt format, integrates fine-grained object features and location control for enhanced fidelity, and employs cross-attention map control to handle multiple subjects simultaneously."}, {"title": "4.2 Personalized Multimodal Alignment", "content": "MoMA (Song et al., 2024) is a tuning-free, open-vocabulary model for personalized image generation which combines reference image features with text prompts, enabling flexible re-contextualization and texture editing while preserving high detail fidelity and identity. A-ECLIPSE (Patel et al., 2024) leverages CLIP latent space to accelerate and facilitate personalized generation."}, {"title": "4.3 Personalized Multimodal Generation", "content": "Kim et al. (2024) propose Layout-and-Retouch, an approach to achieve better diversity in the personalization of image generation. Shi et al. (2024a) propose Instantbooth for personalized generation without test-time fine-tuning."}, {"title": "4.4 Personalized Multimodal Fine-tuning", "content": "MS-Diffusion (Wang et al., 2024d) introduces a zero-shot, layout-guided method for multi-subject image personalization in diffusion models. It integrates a Grounding Resampler to enhance subject detail extraction and a Multi-Subject Cross-Attention mechanism to manage conflicts in multi-subject scenarios, ensuring accurate representation of subjects in specific regions while preserving overall image fidelity. UNIMO-G (Li et al., 2024a) unifies the multimodal end-to-end fine-tuning with visual and language transformer models."}, {"title": "5 Personalized MLLM Recommendation", "content": "Multimodal instructions in recommendations allow for the personalization of user intentions and preferences (Ye et al., 2024b; Zhou et al., 2023), rich contexts of item information (Zhou et al., 2023; Lyu et al., 2024b), and more diverse user-system interactions (Karra and Tulabandhula, 2024). To better express preferences for novel items, the user can provide the reference image as part of the recommendation instruction (Zhou et al., 2023). Based on the user's interaction with visual items, the multimodal recommender system could analyze user preferences and intentions (Ye et al., 2024b; Yu et al., 2024; Liu et al., 2024c). PMG (Shen et al., 2024) further transform multimodal user behaviors into language to model user preferences for a recommendation task. For items in recommendation tasks, when meta data of a item is lacking, MLLMs are beneficial to extract rich descriptive information of items for better recommendation (Zhou et al., 2023; Lyu et al., 2024b). On the other hand, many works also encode visual information with textual information into latent representations for better item modeling (Tian et al., 2024; Wei et al., 2024a). In addition, multimodal instructions enable interactions through screenshots (Karra and Tulabandhula, 2024), personalized item design (Wei et al., 2024a), and conversational recommendations based on reference images (Zhou et al., 2023)."}, {"title": "5.2 Personalized Multimodal Alignment", "content": "To unify the understanding and reasoning of multimodal information, MLLMs can serve to transcript visual information into texts (Shen et al., 2024; Ye et al., 2024b), enable latent representation fusion and alignment (Tian et al., 2024; Xu et al., 2024; Hu et al., 2024), and unify multimodal modeling (Wei et al., 2024a; Liu et al., 2024c; Yu et al., 2024). Based on the user's previous interaction with visual items, the MLLMs can provide explanations on the interacted items' descriptions (Ye et al., 2024b), and also the user's interaction behaviors (Shen et al., 2024), which can be further used as part of textual prompts for further textual-only LLM-based reasoning. On the other hand, other works consider using MLLMs' abilities in encoding multimodal representations, to enable item-level augmentation (Tian et al., 2024), user-item fusion (Xu et al., 2024), and across-task multimodal knowledge transferring (Hu et al., 2024). Recent developments in end-to-end multimodal learning, where multimodal instructions input as a sequence, enables a novel paradigm of generative recommendation, which regards recommends as a next-token prediction task. Some preliminary works directly prompt advanced MLLMs (e.g., GPT-4V) to understand the multimodal instruction and recommendations based on reasoning. Liu et al. (2024c) designs such prompting methods in sequential recommendation tasks, whose recommendation results are re-ranked after generation. Some works also enable tokenized items and users (Yu et al., 2024) with multimodal information, which can directly generate items from the model's embedding space."}, {"title": "5.3 Personalized Multimodal Generation", "content": "Generative recommender systems leverage next-token generation as a unified recommendation policy. The LLM can directly generate items as language tokens by further encoding items into the LLM's embedding space. Recent ID-based representation learning methods encode item IDs into language embeddings, learned from multimodal and collaborative knowledge (Yu et al., 2024). In addition, some unified framework (Wei et al., 2024a) enables encoding of multi-channel information, and recommendation generation as well as modified images of the user's potentially interested items. Such multimodal generation provides better explainability of the recommended items and better convinces users to accept the items. However, since item re-ranking is one of the essential steps for post-processing, how to leverage multimodal output for item re-ranking can be still under-explored."}, {"title": "5.4 Personalized Multimodal Fine-tuning", "content": "To achieve more efficient alignment of personalized MLLM recommendations, several works also propose fine-tuning methods on MLLMs. GPT4Rec (Zhang et al., 2024) incorporates graph modality information which enables structure-level prompting. Based on the novel prompt design, GPT4Rec performs prompt tuning to benefit streaming recommendations on both the node level and the view level. InstructGraph (Wang et al., 2024b) also leverages the graph structure to unify NLP, information retrieval, and recommendation tasks, and thus further enables fine-tuning and RLHF for alignment. MMSSL (Wei et al., 2023) is a unified learning framework to first decompose users' modality-aware preferences, and then collaboratively learn the inter and inter-dependency and inter-modality preference signals through self-augmentation. Deng et al. (2024) further propose a unified transformer model that enables inputs of multimodal information, and outputs of content features which can be used to pair with item representations for direct recommendation tasks."}, {"title": "6 Personalized MLLM Retrieval", "content": "Personalized multimodal instruction focuses on improving the ability of MLLMs to tailor their outputs based on user-specific needs and preferences. Existing benchmarks, such as ConCon-Chi (Rosasco et al., 2024), raise challenges in personalized text-to-image retrieval by introducing complex and varied contexts and instructions for personalized concept learning and compositionality assessment. In a different direction, the Learnable Agent Collaboration Network (Shi et al., 2024b) proposes a framework where multiple agents with distinct instructions and roles collaborate to deliver user-specific responses in multimodal search and retrieval engines. Med-PMC (Liu et al., 2024a) creates a simulated clinical environment where MLLMs are instructed to interact with a patient simulator decorated with personalized actors for multimodal information retrieval and decision making. These works highlight the need for MLLMs to effectively integrate multimodal information and personalize their responses across diverse user interactions."}, {"title": "6.2 Personalized Multimodal Alignment", "content": "To enhance the interaction between MLLMs and user-specific inputs, personalized multimodal alignment ensures that models can adapt to unique preferences and contexts. AlignBot (Chen et al., 2024c) aligns robot task planning with user reminders by using a tailored LLaVA-7B model as an adapter for GPT-40. The alignment translates user instructions into structured prompts enabling a dynamic retrieval mechanism that recalls relevant past experiences and improves task execution. In contrast, the Align and Retrieve framework (Xu et al., 2024) focuses on image retrieval with text feedback, using a composition-and-decomposition learning strategy to unify visual and textual inputs. This approach creates a robust multimodal representation for precise alignment between composed queries and target images. Both methods underscore the importance of aligning multimodal inputs for complex retrieval tasks with personalized user needs."}, {"title": "6.3 Personalized Multimodal Generation", "content": "Capturing personalized user intents for more accurate retrieval results is another challenge for MLLMs. Ye et al. (2024a) propose an iterative user intent expansion framework, demonstrating how MLLMs can parse and compose personalized multimodal user inputs. It refines the image search process through stages of parsing and logic generation, which also allows user to iteratively refine their search queries. Similarly, Wang et al. (2024e) develop a multimodal query suggestion method leveraging multi-agent reinforcement learning to generate more personalized and diverse query suggestions based on user images, thereby improving the relevance of retrieval results. Additionally, Nguyen et al. (2024) present Yo\u2019LLaVA, a personalized assistant that embeds user-specific visual concepts into latent tokens, enabling model tailored interactions and retrievals. These methods collectively emphasize the integration of generation techniques into retrieval systems for more precise and personalized retrieval outcomes."}, {"title": "6.4 Personalized Multimodal Fine-tuning", "content": "To further improve the retrieval capabilities of MLLMs in personalized contexts, various fine-tuning techniques are developed. FedPAM (Feng et al., 2024) introduces a federated learning approach for fine-tuning text-to-image retrieval models, allowing them to adapt to user-specific data without sharing confidential information, thereby addressing the data heterogeneity challenge. VITR (Gong et al., 2023) enhances vision transformers for cross-modal information retrieval by refining their ability to understand relationships between image regions and textual descriptions. Yeh et al. (2023a) further demonstrate how models can be adapted to identify specific user-defined instances, such as objects or individuals in videos, by extending the model's vocabulary with learned instance-specific features. Additionally, Chen et al. (2023) explore task-personalized fine-tuning for visually-rich document entity retrieval, utilizing meta-learning to extract unique entity types with few examples. Furthermore, Li et al. (2024b) propose a generative cross-modal retrieval framework that fine-tunes MLLMs to memorize and retrieval visual information directly from model parameters, offering a novel approach to image retrieval. These works show great potential of fine-tuning MLLMs to enhance their retrieval performance in personalized and diverse multimodal tasks."}, {"title": "7 Evaluation", "content": "The evaluation of personalized MLLMs is typically categorized based on the target task. UniMP (Wei et al., 2024b) explores various personalized tasks, such as personalized preference prediction, personalized explanation generation, and user-guided image generation, among others. Several models focus on personalized recommendation tasks, as detailed in Section 5 (Karra and Tulabandhula, 2024; Wei et al., 2023; Zhang et al., 2024; Ye et al., 2024b). In the recommendation setting, the goal is to rank the true target (e.g., item or movie) highest on the list relative to other items. Commonly used metrics for this task include MRR, Recall@k, Hit@k, AUC, HR@k, and NDCG@k, which evaluate how well the model ranks the true target item in comparison to other options.\nPersonalized multimodal generation focuses on creating customized content, such as images or text, by considering user-specific behavior. This includes generating personalized images, posters for movies, or emojis, as demonstrated by Shen et al. (2024). Shen et al. (2024) utilize various image similarity techniques to evaluate the similarity between the generated content and historical or target items, employing metrics like LPIPS (Learned Perceptual Image Patch Similarity) (Zhang et al., 2018) and SSIM (Structural Similarity Index Measure) (Wang et al., 2004). Additionally, this area encompasses personalized chatbots (Nguyen et al., 2024; Fei et al., 2024; Abuzuraiq and Pasquier, 2024), which assess models' abilities to recognize personalized subjects in images, handle visual and text-based question answering (Alaluf et al., 2024; Nguyen et al., 2024), and evaluate emotional intelligence by measuring emotion detection accuracy and response diversity (Fei et al., 2024).\nGal et al. (2022) introduce personalized text-to-image generation, synthesizing novel scenes based on user-provided concepts and natural language instructions. They evaluate the model by calculating the average pair-wise CLIP-space cosine similarity between generated images and the concept-specific training set, as well as the editability of prompts by measuring the similarity between the generated images and their textual descriptions using CLIP embeddings. Other methods in this domain (Kim et al., 2024; Song et al., 2024) focus on two main aspects: identity preservation, which assesses the model's ability to maintain the subject's identity, and prompt fidelity, which ensures alignment between the generated images and the textual prompts. Identity preservation is typically measured by I-CLIP (Radford et al., 2021) and I-DINO (Caron et al., 2021), which compute subject similarity using CLIP and DINO as backbones. Prompt fidelity is evaluated through the CLIP-based text-image similarity score (T-CLIP). Image diversity is assessed using the Inception Score (IS) to capture the variation within generated sets. Jang et al. (2024) introduce a new metric, Detect-and-Compare (D&C), to evaluate multi-subject fidelity, addressing the limitations of existing metrics (like I-CLIP or DINOv2) that do not to prevent identity mixing in multi-subject scenarios. Wang et al. (2024d) and others (Ma et al., 2024; Li et al., 2024a) focus on multi-subject personalized text-to-image generation, using M-DINO to capture subject fidelity by avoiding subject neglect, which average fidelity metrics may overlook.\nOther tasks include personalized image retrieval, where the Vision-Language model is expected to retrieve a collection of relevant images based on a textual query, using personalized context previously provided by the user (either in the form of images or text). Cohen et al. (2022) first introduce the concept of Personalized Vision and Language, along with a benchmark to evaluate models on tasks like personalized image retrieval and personalized image segmentation. ConCon-Chi (Rosasco et al., 2024) further extend this by proposing a new benchmark that evaluates models' ability to learn new meanings and their compositionality with known concepts. The setting of personalized retrieval has also been expanded to videos in the works of (kor, 2022; Yeh et al., 2023b). Zero-shot Composed Image Retrieval (ZS-CIR) evaluates the model's capability to retrieve images based on compositional queries, without requiring prior examples for new combinations of known concepts. The metrics typically used for these tasks include measuring the rank of the first ground truth (GT) image using Mean Reciprocal Rank (MRR), Recall@k to determine if any GT image appears in the top-k results, and Mean Average Precision (MAP) to assess the ranking of all GT images. Additionally, MAP@k evaluates the precision of GT images up to the top-k retrieved results. Lastly, personalized semantic segmentation focuses on segmenting an instance of a personalized concept in an image, based on a textual query that refers to that concept. Cohen et al. (2022) use the intersection-over-union (IoU) metric to evaluate this, reporting the rate of predictions with IoU above a specified threshold."}, {"title": "8 Datasets", "content": "In recent years, the field of multimodal and personalized learning has seen an increase in datasets, each designed to address specific research challenges. These datasets span various domains, including vision-language models, agent collaboration networks, fashion retrieval, and cross-modal retrieval tasks. For instance, ConCon-Chi (Rosasco et al., 2024) and MSMTPInfo (Shi et al., 2024b) provide benchmarks for evaluating complex, dynamic user interactions in multimodal contexts, while fashion-focused datasets such as FashionIQ (Wu et al., 2021), and Fashion200k (Han et al., 2017) offer rich collections of images and triplets for advancing research in fashion retrieval and recommendation. UniMP (Wei et al., 2024b) uses Amazon review data including the user-item interactions and the items' images. Other datasets like RefCOCOg (Mao et al., 2016) and CLEVR (Johnson et al., 2017) focus on relationships between objects and regions in images, contributing to cross-modal retrieval research. In addition, a wide range of datasets in multimodal recommendation and information retrieval are used in developing and evaluating personalized MLLMs. We summarize a series of comprehensive datasets with detailed descriptions in Table 2 (Appendix A)."}, {"title": "9 Open Problems & Challenges", "content": "In this section, we discuss open problems and highlight important challenges for future work."}, {"title": "9.1 Benchmark Datasets", "content": "For developing better personalized MLLMs, there is a need for more robust and comprehensive benchmark datasets to improve both training and evaluation. Currently, there are limited multimodal benchmark datasets with user-specific information."}, {"title": "9.2 Evaluation Metrics", "content": "Many works have focused mainly on evaluating downstream tasks such as recommendation, rather than directly assessing the quality of generated outputs. However, direct evaluation of generation quality is crucial for improving these models."}, {"title": "9.3 Multimodality Diversity and Complexity", "content": "Most existing work leverages only standard modalities such as text and images. Future work should explore other more diverse types of modalities such as audio, video, graphs, among others. Furthermore, there is a need to develop techniques for supporting many more modalities all at once, as most work has focused only on two such modalities."}, {"title": "9.4 Modality Fusion", "content": "In MLLMs, a common challenge is the dominance of text during modality fusion. Since these models are typically pre-trained on vast amounts of text data, they become highly proficient at processing and interpreting textual information. Consequently, when integrating multiple modalities, there is a tendency for the model to over-rely on text, which can overshadow other crucial data sources like images or audio. This text bias often results in suboptimal performance in tasks that require a deeper understanding of non-textual information, where visual or audio cues are key to provide the full context."}, {"title": "9.5 Theoretical Foundations", "content": "Developing theoretical foundations for the techniques behind personalized MLLMs remains an open problem (Wu et al., 2024a). Understanding their theoretical limits and trade-offs is also of fundamental importance. In spite of this, understanding the theoretical limits of these techniques remains an open problem for future work."}, {"title": "10 Conclusion", "content": "In this work, we present a comprehensive survey on personalized multimodal large language models, focusing on their architectures, training methods, and applications. We introduce an intuitive taxonomy for categorizing the techniques used to personalize MLLMs for individual users and provide a detailed discussion of these approaches. Additionally, we explore how these techniques can be combined or adapted when appropriate, highlighting both their advantages and underlying principles. We offer a concise summary of the personalization tasks addressed in existing research and review the evaluation metrics used for each task. We also summarize key datasets that are valuable for benchmarking personalized MLLMs. Finally, we identify important open challenges that remain to be addressed. This survey serves as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models."}, {"title": "Limitations", "content": "In this paper, the extent of personalization in MLLMs is inherently limited by the available datasets and applications. Moreover, our focus is on MLLMs that incorporate specific personalized multimodal instructions, but we do not address inherent model biases that may affect personalization. Addressing such biases could be a valuable direction for future research in MLLM personalization."}, {"title": "B Applications", "content": "Personalized MLLMs have an extensive range of applications, targeting various tasks in the textual, visual, audio, and other domains."}, {"title": "B.1 Personalized MLLM Recommendation", "content": "Liu et al. (2024b) develop a multimodal knowledge graph that recommends missing entities in triplet structures. Their approach predicts relationships between entities (e.g., people) within images."}, {"title": "B.2 Personalized MLLM Retrieval", "content": "Choudhury et al. (2024) classify Electronic Theses and Dissertations (ETD) through a combination of visual and textual learning."}, {"title": "B.3 Personalized MLLM Text Generation", "content": "Wei et al. (2024a) propose a multimodal learning framework where a vision model extracts features from images and a language model learns from texts. The extracted features are jointly modeled to yield personalized product recommendation, preference prediction, explanation generation. Additionally, Wang et al. (2024c) leverage multimodal learning to answer science-related questions using chain-of-thought reasoning."}, {"title": "B.4 Personalized MLLM Image Generation", "content": "Shen et al. (2024) utilize MLLMs to generate movie posters tailored to users' preferences. Similarly, Song et al. (2024) and Wei et al. (2024c) leverage LLMs to generate images based on visual and textual prompts."}, {"title": "B.5 Miscellaneous Applications", "content": "MLLMs have been applied in various other fields, such as helping visually impaired individuals by verifying images and offering outfit suggestions (Xie et al., 2024). Moreover, MLLMs are used on brain tumor segmentation and tumor identification (Dai et al., 2024)."}]}