{"title": "TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning", "authors": ["Yujie Feng", "Xu Chu", "Yongxin Xu", "Zexin Lu", "Bo Liu", "Philip S. Yu", "Xiao-Ming Wu"], "abstract": "Language model continual learning (CL) has recently garnered significant interest due to its potential to adapt large language models (LLMs) to dynamic real-world environments without re-training. A key challenge in this field is catastrophic forgetting, where models lose previously acquired knowledge when learning new tasks. Existing methods commonly employ multiple parameter-efficient fine-tuning (PEFT) blocks to acquire task-specific knowledge for each task, but these approaches lack efficiency and overlook the potential for knowledge transfer through task interaction. In this paper, we present a novel CL framework for language models called Task Skill Localization and Consolidation (TaSL), which enhances knowledge transfer without relying on memory replay. TaSL first divides the model into 'skill units' based on parameter dependencies, enabling more granular control. It then employs a novel group-wise skill localization technique to identify the importance distribution of skill units for a new task. By comparing this importance distribution with those from previous tasks, we implement a fine-grained skill consolidation strategy that retains task-specific knowledge, thereby preventing forgetting, and updates task-shared knowledge, which facilitates bi-directional knowledge transfer. As a result, TaSL achieves a superior balance between retaining previous knowledge and excelling in new tasks. TaSL also shows strong generalizability, suitable for general models and customizable for PEFT methods like LoRA. Additionally, it demonstrates notable extensibility, allowing integration with memory replay to further enhance performance. Extensive experiments on two CL benchmarks, with varying model sizes (from 220M to 7B), demonstrate the effectiveness of TaSL and its variants across different settings.", "sections": [{"title": "I. INTRODUCTION", "content": "EQUIPPING large language models (LLMs) with continual learning (CL) capabilities to sequentially learn different tasks is essential for their deployment in real-world scenarios [1], [2]. This capability enables LLMs to dynamically adapt to new tasks and acquire additional knowledge [3], [4]. An effective CL system must address two critical challenges: (1) Catastrophic Forgetting (CF) [5], the phenomenon where a model's proficiency in previous tasks deteriorates as it learns new ones, and (2) Knowledge Transfer (KT) [6], enhancing task performance through the transfer of knowledge. KT can be categorized into forward transfer, which improves new task performance using knowledge from previous tasks, and backward transfer, which enhances performance on previous tasks after learning a new relevant task. Achieving a balance between retaining previous knowledge and excelling in new tasks is vital for success.\nGiven the considerable computational demands, recent efforts have explored CL for LLMs using parameter-efficient fine-tuning (PEFT) methods [7], [8], such as Low-Rank Adaptation (LoRA) [7]. Traditional rehearsal-based approaches, which require storing data examples from past tasks for replay, face significant privacy and memory issues [3], [9], [10].\nAnother prominent line of work in recent studies is the parameter isolation CL methods [11]\u2013[14]. These methods assign a dedicated PEFT block for each new task to acquire task-specific knowledge, preserving the block for selective activation during testing. However, they exhibit significant limitations in effectively addressing the KT and CF challenges.\nOn one hand, the prevalent design of these frameworks, which utilizes multiple PEFT blocks, introduces redundancy and inefficiency [15]. The number of required PEFT blocks increases with each new task, leading to substantial memory storage costs and making it difficult to handle long task sequences [16], [17]. Moreover, such approaches essentially create separate expert models for each task, which limits their ability to generalize to unseen tasks [18] (Limitation 1).\nOn the other hand, these parameter-isolation CL methods do not consider interactions between tasks, which hinders KT. For instance, the method in [12] involves learning each PEFT block separately within individual tasks. Similarly, Orthogonal Low-Rank Adaptation (O-LoRA) [19] updates parameters via gradient projection in orthogonal subspaces. Although these approaches may mitigate CF to some extent, they cut off the potential transfer of knowledge stored across different PEFT blocks, thus impeding bi-directional KT among various tasks and leading to suboptimal performance (Limitation 2).\nTo address these limitations, we introduce Task Skill Localization and Consolidation (TaSL) [20], a novel CL framework designed to improve KT between tasks without relying on memory replay. Our approach is motivated by recent findings that model parameters contribute unevenly to performance [21]. For instance, the authors in [22] uncovered a core region on LLMs crucial for all languages, while distinct monolingual regions exist for different languages. Preserving these important regions can help avoid forgetting. The research in [15] also found that during fine-tuning with LoRA, many redundant parameter modifications are often retained, suggesting that PEFT blocks contain significant redundancies.\nBased on these insights, TaSL facilitates KT by identifying and consolidating the importance distribution of model parameters across tasks. TaSL initially employs a group-wise importance-aware skill localization technique that utilizes gradient trajectories to identify tiny regions within the parameter space that store crucial knowledge for the current task. By comparing the importance distribution with those of previous tasks, we can then differentiate between task-specific and task-shared regions, as illustrated in Figure 1. Our innovative skill consolidation phase then categorically integrates weights from previous tasks with the current one, enabling effective KT while minimizing forgetting.\nIn detail, TaSL first reconstructs the model or PEFT block into fine-grained \"skill units\". A skill unit refers to a distinct subset of model parameters that encapsulates specific functional capabilities or knowledge relevant to a particular task, such as the Query matrix within the self-attention layer. By operating at this finer granularity, we can localize and consolidate task-specific and shared knowledge within a single PEFT block, rather than adapting a separate PEFT block for each task as in previous works (addressing Limitation 1).\nThe importance-aware skill localization method employs a new group-wise metric to compute importance scores, effectively quantifying the significance of each skill unit for the current task. Our approach, focusing on parameter space rather than dataset-driven categorization of task-specific and task-shared knowledge, offers a more effective solution to managing KT in LLMs, overcoming inaccuracies caused by dataset noise. Our skill consolidation stage, then based on a fine-grained model averaging strategy, effectively manages different types of knowledge. During this phase, we promote forward KT by initializing new tasks with previously fine-tuned weights. For backward KT, we merge knowledge from both current and past tasks into localized task-shared skill units, enhancing their capability. To prevent CF, we preserve the integrity of skill units containing previous task-specific knowledge, ensuring they remain unaffected by new task learning (addressing Limitation 2).\nGiven the widespread adoption and success of LoRA in fine-tuning LLMs, there is a compelling opportunity to optimize the TaSL framework specifically for LoRA. While the original TaSL framework has shown promising results, it has only been validated on a specific sub-task, and its effectiveness on general CL benchmarks has yet to be explored. Additionally, on the technical side, the skill localization process in TaSL relies on first-order gradients, which may not always provide precise importance localization. Furthermore, the skill consolidation phase in TaSL involves many hyperparameters, potentially complicating the averaging process.\nTo address these issues, we propose TasLoRA, a LoRA-tailored version of TaSL. TasLoRA reconstructs the LoRA adapter into new skill units based on parameter dependencies, ensuring more efficient management of knowledge through sequential task learning. During the fine-tuning process, an additional orthogonal loss is added to ensure that skill units within the same LoRA adapter capture distinct latent semantic features. For skill localization, TasLoRA employs a novel second-order gradient approximate group-wise metric to compute importance scores more precisely. For skill consolidation, we transition from the previous hard-mask model averaging approach to a soft-masking strategy, using an adaptive averaging technique. This allows for a more flexible integration of task-specific and shared parameters, dynamically adjusting the contribution of each skill unit based on its importance.\nFurthermore, to adapt to a broader range of application scenarios, we combine TaSL with memory replay, introducing the TaSL-M model. Through extensive experiments on two CL benchmarks with different parameter-level backbones (from 220M to 7B), our TaSL framework, along with its variants, excels in mitigating CF and showcases remarkable capabilities for KT, outperforming state-of-the-art (SOTA) methods.\nThe main contributions are summarized as follows:\n\u2022 We propose a novel Task Skill Localization and Consolidation (TaSL) framework for language model continual learning. By identifying and consolidating task-specific and task-shared knowledge at a granular skill unit level, TaSL achieves effective knowledge transfer and mitigates catastrophic forgetting, overcoming the limitations of previous approaches.\n\u2022 We develop various group-wise skill localization and fine-grained skill consolidation techniques. For instance, the parameter importance metric in skill localization can be based on first-order gradients or a new second-order gradient approximation method. In skill consolidation, our model averaging strategies include a categorical hard-mask approach and an adaptive soft-mask method.\n\u2022 The TaSL framework demonstrates strong generalizability and extensibility. The flexible design of skill units enables TaSL to be easily tailored to PEFT methods, such as LoRA, optimizing performance for specific model architectures. Additionally, TaSL can be integrated with memory replay technique to further enhance performance and adapt to a broader range of application scenarios.\n\u2022 Extensive evaluation on two CL benchmarks underscores the superiority of our TaSL framework and its variants in facilitating knowledge transfer and mitigating catastrophic forgetting, especially in memory-free scenarios. Furthermore, TaSL consistently excels across diverse model sizes (ranging from 220M to 7B), different model architectures (T5 and LLaMA-2), and unseen tasks."}, {"title": "II. RELATED WORK", "content": "Continual learning [3] aims to develop learning algorithms capable of accumulating knowledge from non-stationary data.\nConventional Continual Learning methods can be divided into three categories: (i) Regularization-based methods add explicit regularization terms to preserve the knowledge of previous tasks [13], [23]. (ii) Rehearsal-based methods address catastrophic forgetting by storing old training samples [24], [25] or by training generative models to provide pseudo samples of previous tasks [26], [27]. Due to the memory and data privacy issues associated with these methods, we focus on rehearsal-free CL methods. (iii) Parameter isolation-based methods dynamically expand model capacity or assign isolated parameters dedicated to each task throughout the CL process to prevent interference between tasks [28]\u2013[30].\nLanguage Model Continual Learning with PEFT. Based on PEFT methods, current approaches for the continual learning of LLMs adopt the concept of parameter isolation, using a pipeline fashion to learn and select PEFT blocks for each task [12], [31], [32]. However, these approaches are limited in effectively addressing the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) [33]. For instance, the authors in [19] and [34] constrain the learning of PEFT blocks to maintain orthogonality, restricting KT among different tasks. Although the recent SAPT method [16] and DAP method [35] achieves KT, it relies on pseudo sample replay or unlabeled domain corpora.\nOur method achieves KT without memory replay, offering unique advantages in data privacy and parameter efficiency. By using layering low-rank adapters on the key and value projection matrices of transformer blocks, we balance retaining previous knowledge and excelling in new tasks.\nResearch shows that model parameters do not all contribute equally to performance [36]. The authors in [21] introduced the concept of \"skill localization\" to identify critical parameters in pre-trained language models, suggesting that fine-tuning only these critical parameters can achieve results comparable to full model fine-tuning. However, their method involves additional steps to identify and retrain these key parameters during the post-fine-tuning phase, which reduces efficiency.\nInspired by techniques from the pruning community, previous studies have utilized gradient-based metrics to pinpoint important parameters during fine-tuning. Sensitivity-based scoring [37], [38] evaluates the impact on training loss, and sensitivity smoothing, as employed by [39], helps remove unnecessary parameters for more efficient fine-tuning. Nonetheless, these methods often result in element-wise pruning, where individual parameters are removed based on their importance scores. This approach can be computationally and storage-intensive, as it requires detailed management of numerous parameters throughout the model.\nBuilding on these advancements, we propose a novel importance-aware skill localization method. This approach uniquely differentiates between task-specific and shared parameters to mitigate forgetting and achieve KT in CL."}, {"title": "III. PRELIMINARIES", "content": "Continual learning [40] aims to create learning algorithms that can continuously accumulate knowledge from ongoing sequences. In supervised continual learning, a sequence of tasks {T1,..., Tk} is presented in a streaming manner. Each\nNk\ntask Th includes a distinct target dataset Dk = {(x,y)} of size Nk, where x \u2208 Xk and y\u2208 Vk. The model must\ni=1\nadapt to these tasks sequentially, having access only to Dk during the k-th task. Generally, with a prediction model f parameterized by \u0398, continual learning aims to optimize the following objective across all tasks:\nK\nmax \u2211\u2211 log p\u03b8(y | x)\n\u0398\nk=1 x,yEDk\n(1)\nThe notation fk refers to the model after training on task Tk, while fk denotes the model after averaging for fk\u22121 and fk. Our TaSL framework aims to address a more challenging scenario where the model cannot access any historical data during training [19]. Nevertheless, to adapt to a wider range of application scenarios, we have also combined TaSL with memory replay. In this setting, we randomly save |M| samples from the training set of each previous task Ti in memory Mi and jointly train the model on new task data Dk and memory M<k. This extension, called TaSL-M, leverages memory replay to enhance performance."}, {"title": "IV. PROPOSED METHOD: TASL", "content": "TaSL includes two key components: (i) Skill Localization, utilizing a group-wise importance metric to accurately identify the importance distribution of parameters across tasks, and (ii) Skill Consolidation, which employs a novel fine-grained model averaging strategy to integrate model weights from both current and past tasks for effective knowledge transfer.\nFigure 2 provides a comprehensive overview of our proposed TaSL framework, with the following subsections detailing each component and the corresponding extensions.\nBefore we localize the importance of parameters, we need to define a structure that helps us better organize the knowledge stored in the model. This structure will provide a clearer understanding of how to mitigate CF and achieve KT. Therefore, we refer to the basic unit of stored skills or knowledge in the model as a \"skill unit.\" Based on different usage scenarios, we offer the following two structures for skill units:\n1) Matrix-Level Skill Unit: In this division strategy, we define skill units as individual matrices in the model, such as the query or key matrices in the self-attention layer or the A matrix in LoRA. This approach aligns with the original TaSL framework. The advantage of this division is that it is model-agnostic, making it applicable to both traditional full-parameter fine-tuning and the latest parameter-efficient fine-tuning methods, such as LoRA. This provides a higher degree of generalizability.\n2) LoRA-Tailored Skill Unit: While the matrix-level division is simple and intuitive, it did not sufficiently consider the intra-matrix redundancy and inter-matrix dependencies. To address these limitations and the inefficiencies noted in recent work [19], which treats each LoRA adapter as a container for task-specific knowledge, we decompose the model matrices into new, finer-grained \u201cskill units\u201d based on parameter dependencies tailored for LoRA. The construction process for these LoRA-tailored skill units is as follows.\nAs shown in Eq (2), A and B can be viewed as a combination of a set of vectors: A = [a1,a2,\u2026,ar], B =\n[b1, b2,..., br], where ai \u2208 Rin, bi \u2208 Rout. Thus BA can be further disassembled as:\nW = W(0) + b1a1 + b2a2 + \u00b7\u00b7\u00b7 + brar\n= W(0) + u1 + u2 + \u00b7\u00b7\u00b7 + ur,\n(3)\nwhere skill unit uz is a matrix obtained by the product of the vectors bi, ai. Therefore, LoRA can be viewed as a fusion of knowledge learned from multiple skill units:\nr\nr\nW = W(0) + \u2211biai = W(0) + \u2211 Ui\ni=1\ni=1\n(4)\nThis division strategy effectively decomposes the parameters of a single LoRA adapter, reducing the redundancy in the traditional approach of employing separate LoRA adapters for each task [19]. To ensure that different skill units within the same layer learn distinct latent semantic features, we apply a regularization term as introduced by [39]:\nR(A, B) = ||ATA \u2013 I|| + ||BTB \u2013 I||, (5)\nwhere I is an identity matrix, this forces A and B to be orthogonal after training.\nTo calculate the importance of each skill unit u, we introduce a group-wise metric that also addresses the significant computational and storage burdens associated with previous parameter-level importance calculation methods [42]:\nI(u) = \u2211 \u2211s(wij)\nin\u00d7out in out\ni=1 j=1\n(6)\nwhere wij denotes the trainable parameters, and in\u00d7out represents the total parameter count in a skill unit u. I(u) measures the collective importance of all parameters within each skill unit, where higher values signify increased importance. The importance function s(\u00b7) for individual parameters, inspired by the pruning community [43], is quantified by measuring the impact of its removal on the loss. Specifically, to estimate the importance of Wi, we calculate the change in loss when the parameter is zeroed out using:\nIw\u1d62 = |\u2206L(D)| = |Lw\u1d62(D) \u2013 LW\u1d62=0(D)|\n= |\u2013\u25bdw\u1d62L\u1d62W\u1d62 \u2013 1/2 W\u1d62H\u1d62\u1d62W\u1d62 + O(||W||\u00b3)|\n= |1/2 H\u1d62\u1d62W\u1d62W\u1d62 + O(||W\u1d62||\u00b3)|\n(7)\nwhere H is the hessian matrix, W\u1d62 represents the i-th parameter in W, and L represents the next-token prediction loss. If removing a parameter has a significant influence, then the model is sensitive to it, and we should retain it [44]. Based on Eq. (7), we can derive the following two metrics for calculating parameter importance:\n1) First-Order Gradient-Based Metric: In original TaSL framework, we followed the standard approach, defining s(.) as the magnitude of the gradient-weight product:\nIw\u1d62 = W\u1d62\u2207wL (8)\nThis metric only considers the first-order gradient term in the importance calculation. However, the importance of a parameter is also reflected in the second-order gradients (Hessian matrix). Ignoring second-order gradient information may lead to biases in localization, thus affecting model performance. Therefore, we propose the following new metric.\n2) Second-Order Gradient Approximation Metric: Direct computation of the Hessian matrix for LLMs is impractical due to its O(N\u00b2) complexity. To address this, we propose\na second-order gradient approximation for the importance metric, which balances the efficiency of first-order methods with the precision of second-order gradients. To reduce the computational complexity, the diagonal of the Hessian H\u1d62\u1d62 can be approximated by the Fisher information matrix [45], and the importance can be defined as:\nIw\u1d62 \u2248 1/N \u2211 j=1 ( \u2202L(D)/\n\u2202W\u1d62 )\u00b2\n(9)\nHowever, calculating the importance as specified in Eq. (9) across the entire training set introduces significant challenges, as model training typically only has access to mini-batch data.\nThis means the metric is subject to variability due to stochastic sampling and training dynamics, introducing large uncertainty in estimating sensitivity. To mitigate this, we propose a more reliably importance metric based on sensitivity smoothing and uncertainty quantification [39]:\n\u012a(t)(wij) = \u03b1\u2081\u012a(t\u22121)(wij) + (1 \u2212 \u03b1\u2081)I(t)(wij) (10)\n\u016a(t)(wij) = \u03b1\u2082\u016a(t\u22121)(wij) +\n(1 \u2212 \u03b1\u2082) |I(t)(wij) \u2013 \u012a(t)(wij)| (11)\nwhere \u03b1\u2081 and \u03b1\u2082 are smoothing factors, and t is the iteration number. \u012a(t) represents smoothed sensitivity and \u016a(t) is the uncertainty term quantified by the local variation between\nI(t) and \u012a(t). Using the exponential moving average of the importance metric, we can retain and explore the trajectory gradient for a longer time, providing a more robust and precise importance assessment. Importance is then determined by:\ns(t)(wij) = \u012a(t)(wij) \u22c5 \u016a(t)(wij) (12)\nTo compute the importance score of each skill unit for current task Tk, we employ Eq. (6) during fine-tuning. The model f with n skill units is denoted as U = {u1,..., un}, with their importance scores for task Te denoted by I(Uk) \u2208 Rn. The detailed computation process is provided in Algorithm 1.\nAfter computing importance scores for each skill unit at the current task Tk, it is essential to compare these with scores from all previously learned tasks to distinguish between task-specific and task-shared skill units. To avoid the inefficiency of storing scores for each past task, we aggregate importance scores from all prior tasks into a cumulative score for tasks up to Tk\u22121. This method allows for the iterative refinement of accumulated scores without separately saving past task scores. The skill units with these cumulative scores up to Tk\u22121 are denoted as \u00dbk\u22121, calculated using:\nI(\u00dbk\u22121) = \u03b2Norm(I(\u00dbk\u22122)) + (1 \u2212 \u03b2)Norm(I(Uk\u22121)) (13)\nwhere \u03b2\u2208 [0,1], and Norm(\u00b7) normalizes importance scores to the [0, 1] range, thus resolving discrepancies across models. The initial scores, I(\u00db1), are set to be equal to I(U1). Following this, the importance distribution for skill units up to task Tk\u22121 is combined with that of the current task, Tk, to facilitate the skill consolidation process."}, {"title": "V. TASL WITH MEMORY REPLAY: TASL-M", "content": "To adapt TaSL to scenarios where historical data can be used, we propose a memory replay-enhanced version, TaSL-M, to further improve model performance. The implementation process is shown in Figure 3.\nAfter training on task Tk using the skill localization and skill consolidation, we obtain the model fk. Before the next task arrives, we introduce a replay stage, where the model fk is fine-tuned using historical data stored in the memory buffer M<k. This process results in a new model fm, which helps\nrecover forgotten knowledge. Specifically, during the replay stage, we do not fine-tune the parameters of all skill units using historical data, as this could negatively impact the performance on the current task. Instead, we fix the current task-specific skill units and fine-tune only the remaining parameters. This approach achieves a better balance between retaining previous knowledge and excelling in new tasks."}, {"title": "VI. EXPERIMENTS AND ANALYSIS", "content": "We utilize the SuperNI Benchmark [48], a comprehensive benchmark designed to evaluate diverse NLP tasks using expert-written instructions. This benchmark facilitates thorough evaluation in more practical settings for the CL of LLMs. It includes tasks in dialogue generation, information extraction, question answering, summarization, and sentiment analysis. Following the established CL setup [16], three tasks are chosen from each type, creating a sequence of 15 tasks in total for evaluation. For each task, 1,000 instances from the dataset are randomly selected for training, and 100 instances are used for validation and testing.\nAdditionally, we employ the Long Sequence Bench-mark [18], a continual learning benchmark consisting of 15 classification datasets. In line with [19], we randomly select 1,000 samples for training each task and reserve 500 samples per class for validation and testing. We explore two different task orders for each benchmark. Please refer to the Appendix for more details about the tasks and orders.\nWe denote aj,i as the testing performance (Accuracy for classification task and Rouge-L [51] for others) on the i-th test set of task right after training on j-th task. The performance of CL is assessed using three metrics:\nAP = 1/K \u2211 aK,i\ni=1\n(18)\nAverage Performance (AP) [52] calculates the average performance across all tasks after training on the final task. Forward Transfer (FWT) [53] evaluates a model's generalization ability by measuring the averaged zero-shot performance. Backward Transfer (BWT) [54] assesses the impact of new learning on previous tasks. Negative BWT indicates the model lost some previously acquired knowledge.\nWe evaluate TaSL against the following PEFT-based continual learning baseline methods: SeqLoRA: sequentially trains the LoRA on the task orders. IncLoRA: incremental learning of new LoRA parameters on a sequential series of tasks. Replay: replays real samples from old tasks when learning new tasks to avoid forgetting. EWC [46]: fine-tune the model with a regularization loss that prevents updating parameters that could interfere with previously learned tasks. L2P [49]: uses the input to dynamically select and update prompts from a fixed prompt pool. LFPT5 [50]: continuously trains a soft prompt for each task with generative replay and an auxiliary loss. Prog-Prompt [18]: sequentially concatenates previous learned prompts to the current one during the training and testing time. O-LoRA [19]: learns tasks in different LORA subspaces that are kept orthogonal to each other and sums all LORA weights up at testing time. SAPT [16]: leverages pseudo samples and a shared attention framework to align PEFT block learning and selection.\nTable II compares our TaSL with these baselines, highlighting three distinct advantages: data privacy-friendliness, model parameter-friendliness, and generalization-friendliness. To clarify the specific localization and consolidation techniques used in TaSL and its variants: TaSL: our base framework utilizes original components, including matrix-level skill units, first-order gradient-based metric for skill localization (Eq. 8), and static weighted consolidation (Eq. 15). TasLoRA: as the latest extension of TaSL, it employs LoRA-tailored skill units, the second-order gradient approximation metric for skill localization (Eq. 9), and adaptive weighted consolidation (Eq. 17). TasL-M and TasLoRA-M: these variants combine TaSL or TasLoRA with memory replay techniques to further enhance performance.\nWe utilize two language models adopted by the previous lines of works in CL for NLP: encoder-decoder T5 model [57] and decoder-only LLaMA model [58]. In TaSL, the hyperparameters \u03b11 and \u03b12 in Eq. (10) and Eq. (11) are set to 0.85. We set \u03b2 in Eq. (13) to 0.7 and \u03c4 in Eq. (16) to 0.15. Following [59], the volume of replay samples is set to 2% of the original training set for all replay-based baseline methods. For different backbones, we utilized the following hyperparameters:\nFWT = 1/K-1 \u2211 Ai-1,i (19)\nBWT = 1/K-1 \u2211 ak,i - aii (20)"}]}