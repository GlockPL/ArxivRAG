{"title": "Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices", "authors": ["Shengyuan Ye", "Liekang Zeng", "Xiaowen Chu", "Guoliang Xing", "Xu Chen"], "abstract": "On-device Deep Neural Network (DNN) training has been recognized as crucial for privacy-preserving machine learning at the edge. However, the intensive training workload and limited onboard computing resources pose significant challenges to the availability and efficiency of model training. While existing works address these challenges through native resource management optimization, we instead leverage our observation that edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources beyond a single terminal. We propose Asteroid, a distributed edge training system that breaks the resource walls across heterogeneous edge devices for efficient model training acceleration. Asteroid adopts a hybrid pipeline parallelism to orchestrate distributed training, along with a judicious parallelism planning for maximizing throughput under certain resource constraints. Furthermore, a fault-tolerant yet lightweight pipeline replay mechanism is developed to tame the device-level dynamics for training robustness and performance stability. We implement Asteroid on heterogeneous edge devices with both vision and language models, demonstrating up to 12.2\u00d7 faster training than conventional parallelism methods and 2.1\u00d7 faster than state-of-the-art hybrid parallelism methods through evaluations. Furthermore, Asteroid can recover training pipeline 14\u00d7 faster than baseline methods while preserving comparable throughput despite unexpected device exiting and failure.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Neural Networks (DNNs) have driven diverse intelligence in today's smart applications, ranging from voice assistance, smart robotics to city surveillance, etc. While existing works have extensively studied the inference aspect of DNN models, the growing proliferation of human-in-the-loop intelligent services urgently emphasizes the necessity for privacy-preserving personalization and continuous model refinement, raising the need for advanced on-device learning ability. For instance, in Federated Learning [9, 38], user devices are required to provision a local model training task in order to contribute to and share the learning procedure. In Continual Learning [7, 58], user devices periodically retrain their local models with newly-collected data so as to adapt the model performance to the contextual factors.\nDespite the increasing demand, efficient in-situ learning still suffers from its prohibitively long training time and vulnerable convergence stability. As we will empirically show in \u00a72.1, even training a mobile-oriented compact DNN model in typical edge devices (i.e., Jetson Nano) takes 160\u00d7 longer epoch time than that in a GPU server, showing the fundamental contradiction between intensive training workload and constrained on-board resources. Moreover, insufficient memory capacity can be a real game-stopper for on-device learning. Towards alleviating these issues, existing wisdom has explored extensive optimizations from various aspects. For example, a number of works adopt model compression techniques (e.g., pruning, sparsification, and quantization) or manually designed lightweight model architectures to reduce the computation complexity for DNN training [8, 27, 32, 45, 59]. Other leading research works have explored to design sophisticated management mechanisms (e.g., tensor rematerialization [12, 23, 43], memory budget adapting [17, 53]) on native resources, but are still bottlenecked by the intrinsic deficiency of physical resource shortage.\nIn this paper, we alternatively observe that prevalent edge scenarios like smart homes and smart factories usually comprise a group of trusted idle devices beyond a single terminal (e.g., pads, laptops, and smart-home devices owned by the same user or family) [61, 62, 64]. These accompanying devices are typically in physical proximity to the primary one running on-device learning tasks and can be associated as a resource augmentation for in-situ DNN training acceleration. This motivates us to regard adjacent available devices as a resource pool and collaborate with them in a distributed manner to render expedited model training at the edge.\nWe note that distributed training acceleration has been comprehensively studied in cloud datacenters for years, but is much less understood with edge devices. Nevertheless, scheduling efficient distributed training over edge devices is non-trivial given the unique challenges inherent in edge environments: (1) In contrast to accelerator clusters in cloud, edge devices are extremely limited in terms of computing power, memory capacity, and communication bandwidth. (2) Edge devices are much more heterogeneous compared to cloud server configurations, which necessitates a heterogeneity-aware strategy to maximize the utilization of computing potential. (3) Edge devices frequently exhibit more potential dynamics than dedicated cloud environments, due to the devices' mobility and accessibility. Unfortunately, no existing work can address all of the aforementioned challenges.\nTo this end, we propose Asteroid, a general distributed training system that is able to orchestrate multiple heterogeneous edge devices for expedited, resource-efficient, and fault-tolerant model training. Asteroid's contribution goes beyond merely leveraging distributed edge devices for training acceleration, instead it addresses the above challenges in three levels. First, from a parallelism perspective, a hybrid pipeline parallelism (HPP) is employed as a principle to manage the distributed training workflow, which combines the best of data parallelism and pipeline parallelism and allows significantly larger optimization space for parallelism planning in heterogeneous edge environments. Second, to maximize resource utilization of HPP among heterogeneous edge devices, a novel dynamic-programming based parallelism planning algorithm is designed, as well as a memory-efficient batch ingestion strategy for multidimensional resources optimization including memory budget, limited communication capacity, and resource heterogeneity. Finally, to adapt to dynamic participants during the runtime, a fault-tolerant mechanism is applied through lightweight coarse-granularity workload migration and topology-driven model replication. We implement Asteroid in four realistic testbeds with each consisting of at least 4 heterogeneous edge devices. Extensive evaluations on both vision and language models show that Asteroid is able to deliver up to 12.2x speedup over conventional parallel training counterparts, and achieves up to 2.1\u00d7 throughput improvement over the state-of-the-art hybrid parallelism methods. Besides, Asteroid can well adapt to device-level dynamics (i.e., device exiting or failure) in agile pipeline recovering time (14\u00d7 faster than baseline) with minimal throughput sacrifice.\nThe main contributions are summarized as follows.\n\u2022 Through extensive measurement studies on on-device and parallel training performance, we employ a hybrid pipeline parallelism as a principled tool to collaborate trusted edge devices for model training acceleration.\n\u2022 We design a novel planning algorithm tailored for hybrid parallelism mechanism, which comprehensively considers memory budget, limited communication capacity, and resource heterogeneity of edge devices.\n\u2022 We propose Asteroid, a general distributed edge training system that is able to orchestrate resource-efficient, expedited training across heterogeneous edge devices with fault-tolerant robustness. Asteroid reveals another path towards efficient in-situ model training at the edge.\n\u2022 We implement Asteroid and evaluate it in realistic heterogeneous testbeds. Experimental results show up to 12.2\u00d7 throughput improvement over baselines and strong robustness to device dynamics with swift pipeline recovering."}, {"title": "2 MOTIVATION AND PRELIMINARIES", "content": "2.1\nDNN Training on Resource-Constrained\nEdge Devices\nOn-device training can leverage locally collected data to improve model performance while fully preserving data in-situ, making it a widely utilized approach in privacy-sensitive edge applications [7, 38, 41, 48, 66]. However, the resource-intensive and computation-demanding nature of DNN training presents significant challenges for resource-constrained edge devices [17, 32, 53, 57]."}, {"title": "2.2 Edge Collaborative Training with Data\nParallelism and Pipeline Parallelism", "content": "Data Parallelism. The most common way to train DNN models in parallel is data parallelism (DP) [18, 31]. In DP, inputs are partitioned across workers, and each worker maintains a replica of the entire model and performs training on its local data while periodically synchronizing gradients with other workers (i.e., AllReduce). The simplicity of its workload induces better scalability with multiple devices. However, due to the loose and varying edge connections, the communication overhead caused by synchronization can usually dominate training time, as shown in Fig. 1(Left).\nPipeline Parallelism. Another widely-used parallelism is pipeline parallelism (PP). PP is an advanced model parallelism-based training strategy that executes the DNN model in a pipelined manner across multiple workers [22]. Specifically, in PP, the DNN model is partitioned into multiple stages and each stage is mapped to a separate processor for stage-wise forward/backward pass execution. The partitioning ensures each stage has an approximately equal workload, optimizing parallel efficiency. For language model (e.g., Bert-small [14]) or crafted edge model with tiny activations, PP is far more communication-efficient than DP, since each worker only needs to exchange a subset of output activations with neighboring workers. Nonetheless, pipeline parallelism is also followed with shortcomings: (1) Weak scalability. The straight-forward implementation of PP for edge clusters can create too many stages, which amplifies the impact of inter-stage communication latency. (2) Unoverlapping inter-stage communication. When communication occurs between layers with huge intermediate activations, PP can not effectively overlap the inter-stage communication with forward and backward execution. Our experiment, implemented with Gpipe [22] for PP, shows that the inter-stage communication latency can be up to 24\u00d7 longer than the stage execution time, which dominates the entire training process. As shown in Fig. 1(Right), for CNN-based models, the per-sample data size communicated in PP even surpasses the communication volume necessitated by DP."}, {"title": "2.3 Key Insight: Combining Data\nParallelism with Pipeline Parallelism", "content": "The above analysis motivates us to employ a hybrid parallelism architecture that incorporates the best of both DP and PP, so as to achieve superior performance in complex and heterogeneous edge environments. Hybrid Data Parallelism (HDP), as adopted by Hetpipe [42], organizes devices into groups for inter-group DP and intra-group PP, necessitating a centralized parameter server (PS) for full gradient exchange. Alternatively, another hybrid approach, Hybrid Pipeline Parallelism (HPP), utilized by PipeDream [39] and Dapple [16], arranges devices into groups for inter-group PP and intra-group DP, as depicted in Fig. 2.\nTo better understand the communication efficiency of these two architectures, we quantitatively analyze their communication volume across devices in a formal way. We first focus on HDP. Assume that each edge device can be and only be divided into a specific group gi and the considered architecture counts G device groups in total. Let P be the gradient size of the global model, the communication volume for the parameter server bidirectional synchronization is 2GP. The pipelined communication data size within a group is $2\\beta_i \\sum_{j=1}^{g_i-1}a_{i,j}$, where $\\beta_i$ is the batch size handled by group $g_i$ and $a_{i,j}$ is the size of $j$-th intermediate tensor in $g_i$. When there are multiple device groups (e.g., in Fig. 2(a)), we can summarize the above analysis to derive the case of G > 1 in Eq. (1). If there is exactly one device group (G = 1), however, PS synchronization is free and thus only intra-group communication volume is charged.\n$V_{HDP} = \\begin{cases}\n2GP + 2 \\beta_1 (\\sum_{j=1}^{g_i-1}a_{i,j}), & G > 1,\\\\ 2\\beta_1 (\\sum_{j=1}^{g_i-1}a_{i,j}), & G = 1.\n\\end{cases}$   (1)\nWe next target at HPP. Different from HDP which shares the global model across groups, each group in HPP may take charge of only a part of the model, whose size is denoted as Pi for group gi. For example, group g\u2081 in Fig. 2(b) takes a model segmentation of size P\u2081. Given \u03b2 as the global mini-batch size and aias the size of the intermediate tensor exported by group gi, each group in HPP requires 2(|gi| - 1) round of ring AllReduce of sub-model Pi in group gi and the total intra-group communication is thus $ \\sum_{i=1}^{G}[2(|g_i| - 1)P_i]$. With the inter-group pipelined communication data size as $2\\beta\\sum_{i=1}^{G-1}a_j$, the total communication volume of HPP can be derived in the case of G > 1 of Eq. (2). When G = 1, inter-group communications are eliminated and the formula can be simplified into the case of G = 1 of Eq. (2).\n$V_{HPP} = \\begin{cases}\n\\sum_{i=1}^{G}[2(|g_i| - 1)P_i] + 2\\beta \\sum_{j=1}^{G-1}a_j, & G > 1,\\\\ 2(|g_1|-1)P_i, & G = 1.\n\\end{cases}$   (2)\nUpon the above formulas, we further empirically evaluate representative models in Table 1 in an edge environment with five Jetson Nano devices. For HDP, we adopted Hetpipe's allocation recommendations, and for HPP, we adopt Asteroid's planning method (refer to \u00a73.3). The results in Table 2 reveal HDP's communication volume exceeds HPP's by 1.9x - 2.7x. This is because HDP employed by HetPipe necessitates full parameter exchange between groups once the number of groups surpasses one. Conversely, HPP's architecture, through parallelism planning, confines AllReduce operations to initial parameter-light convolution layers, thereby circumventing the final parameter-dense layers.\nOpportunities with HPP across Edge Devices. In light of the foregoing analysis, Asteroid employs HPP to facilitate collaboration with edge devices. Beyond breaking the resource wall of a single device, employing HPP offers the following benefits: (1) Each device stores only a subset of the entire model, leading to a smaller memory footprint, which is particularly advantageous for models with huge parameters. (2) HPP offers a highly flexible parallelism architecture that can effectively minimize communication volume by preventing AllReduce in parameter-dense layers. (3) Through layer-wise planning, HPP can avoid inter-stage communication between layers with huge intermediate tensors. By fully overlapping computation and communication, HPP conceals the limitations of network capacity in edge environments. (4) HPP provides higher scheduling flexibility and expands a larger optimization space of parallelism planning in our considered complex and heterogeneous edge environments."}, {"title": "2.4 Technical Challenges", "content": "Despite the benefits, realizing HPP in complex edge environments still suffers from a set of challenges.\nScarce Memory and Network Capacity. Edge devices, constrained by limited memory and bandwidth through links such as WiFi and cabled network [21], require meticulous planning for the HPP architecture to prevent out-of-memory issues and maximize bandwidth efficiency. Successfully leveraging the architecture's flexibility to address these constraints presents a significant challenge.\nHeterogeneous Computing Resource. Edge environments are typically heterogeneous [66], with facilities ranging from small devices and gateways [10] to much powerful cloudlets [46]. Efficiently applying HPP in such highly heterogeneous settings is a particularly challenging aim of maximizing its resource utilization, which requires judiciously matching workload distribution to diverse edge resources.\nDynamic Training Participants. Further complicating the problem is the inherent, unpredictable dynamics of available edge resources, due to devices moving across networks and multitasking [51, 53]. To render stable, reliable training performance with HPP therefore poses significant challenges in designing a robust scheduling such that training participants' failure is tolerable."}, {"title": "3 ASTEROID SYSTEM DESIGN", "content": "3.1 System Overview\nFig. 3 depicts an overview of our proposed Asteroid which features three primary phases: Preprocessing Phase, Planning Phase and Execution Phase. Preprocessing Phase is an offline procedure that runs once before deployment. Asteroid Profiler performs a training process using calibration data as input on the physical edge devices to record the runtime profile necessary for parallelism planning (step 1). During Planning Phase, Asteroid Planner takes profiling results as input to generate planning configurations that include DNN model partitioning points, device grouping strategies, and micro-batch allocations within groups (step 2). These configurations comprehensively addresses challenges including memory budget, limited communication capacity, and resource heterogeneity, and is subsequently applied to target DNN models and training participants. In Execution Phase, Asteroid Worker which is deployed on each participant will be responsible for model execution, intermediate output exchange and gradient synchronization (step 3). To account for the challenge of runtime dynamics, a fault-tolerant pipeline replay mechanism will be applied (step 4). A central coordinator (a user-specified device) is required to apply planning configuration and detect device failure."}, {"title": "3.2 Hybrid Pipeline Parallelism in Asteroid", "content": "HPP Architecture and Workflow in Asteroid. As illustrated in Fig. 4(a), Asteroid's HPP first divides a DNN model into multiple stages where each contains a stage model composed of a set of consecutive network layers. Edge devices in the resource pool will be divided into a corresponding number of device groups, where each contains one or multiple devices. HPP combines pipeline parallelism across these groups with data parallelism within them. During training, a mini-batch will be split into M smaller micro-batches (with the size of B) and injected into the pipeline concurrently to increase parallelism. Micro-batches are broken up further if device groups contain multiple devices. Each device performs the forward pass (FP) and backward pass (BP) for the stage model it takes in charge and accumulates gradients for all micro-batches in each mini-batch. At the end of a mini-batch, gradients in each device group are synchronized using AllReduce and then applied to stage model parameters. The entire mini-batch training process is called an HPP-Round. Fig. 4(b) shows a well-designed scheduling arrangement for Asteroid HPP training that models inter-stage network communication, which cannot be neglected due to the significant latency under low-speed links in edge environments. Bubbles are idle periods when a stage waits for data from the previous one, represented by gray blocks.\nMemory-efficient 1F1B Micro-batch Scheduling. Our memory breakdown experiment in Fig. 5 shows that the peak memory footprint during DNN training can be classified into three categories: (1) model weight memory (including model parameters and accumulated gradients), (2) optimizer memory, and (3) activation memory (intermediate outputs of FP). HPP architecture enables each device to only store the corresponding stage model in memory. In stage p, the memory footprints for model weights, optimizer, and activations for a single micro-batch of size \u03b2 are denoted as $Mem_p^{(MOD)}$, $Mem_p^{(OPT)}$, and $Mem_p^{(ACT)}(\\beta)$, respectively.\nThe experiment in Fig. 5 shows that the intermediate activations are the main contributor to memory footprint, especially for the CNN-based models which usually have large inter-layer feature maps. Gpipe [22] schedules micro-batches in a backward-after-forward manner, resulting in a peak memory demand that scales proportionally with the number of concurrently resident micro-batches (O(M)), which is memory-unfriendly for edge devices. Inspired by the idea of gradient accumulation, we adopt a fine-grained micro-batch scheduling that works in a one-forward-one-backward (1F1B) manner, which schedules the BP early to release the activation memory produced by FP for reuse. We propose performing Kp FP for each stage p before strictly enforcing 1F1B (as illustrated in Figure 4(b), where K\u2081 = 5, K\u2081 = 3, K2 = 1), resulting in an activation memory requirement of O(Kp) (Kp < M) for each stage p. Setting a smaller Kp can reduce memory footprint but compromise stage-level pipeline concurrency. Specifically, with Kp = 1 for all stages, only one stage will be activated concurrently. Our experiments in \u00a75.4 reveal that setting $K_p = 2 \\times (P \u2013 p) \u2013 1$ (P is the total number of stages) can minimize the peak memory footprint of each stage without sacrificing parallelism efficiency. With the above modeling, the total memory footprint of stage p with a micro-batch size \u03b2 in Asteroid is as follows:\n$Mem_p(\\beta) = Mem_p^{(MOD)} + Mem_p^{(OPT)} + K_p \\times Mem_p^{(ACT)}(\\beta)$.  (3)"}, {"title": "3.3 Parallelism Planning", "content": "DNN Model and Asteroid Profiler. We consider a DNN model as a directed acyclic graph. The graph nodes represents modules like Conv, MaxPool, Attention, etc while the graph edge encodes the data dependency between modules. In order to split the model into multiple sequential stages, we topologically sort the graph nodes and transform the DNN model into layers sequence. Asteroid profiler precisely collects the total output size and weight parameters in bytes for each layer. We denote the output activations (and corresponding input gradients) and weight parameters of layer l as $a_l$, and $w_l$, respectively.\nWe further profile the FP and BP execution time of each layer. Existing works [25, 35] simply assume a linear relationship between batch size and execution time. However, our experiment reveals that smaller batch size fails to fully leverage the parallelism capacity of GPUs, leading to a non-linear correlation between them, as shown in Fig. 6. Therefore, Asteroid profiler measures the execution time of each layer on realistic edge hardware for various batch sizes. We denote $t_{d,l}^f(\\beta)$ and $t_{d,l}^b(\\beta)$ as the FP and BP execution time for layer l on device d using a batch size of \u03b2. We profile the D2D bandwidth between devices d and d' as $b_{d,d'}$.\nOptimization Objective Formulation. As illustrated in Fig. 7, to emphasize the non-negligible communication latency in edge environments, we abstract the process into separate steps, categorizing them as either execution steps or communication steps for inter-stage communication and stage model execution. We divide the training process on each step in an HPP-Round into three phases: Waiting Phase, Execution Phase, and AllReduce Phase, with corresponding times denoted as $T_s^W$, $T_s^E$, and $T_s^A$. Our optimization objective is to minimize the HPP-Round Latency, which is determined by the step with the largest total latency of three phases:\nHPP-Round Latency = $\\max_{s \\in \\{0,1,...,S-1\\}}(T_s^W+T_s^E+T_s^A)$,  (4)\nwhere S denotes the total steps in pipeline. For notation simplicity, we denote the forward (backward) execution or communication time of a step s as $E_s^f$ ($E_s^b$), which is associated with a device group $G_s$ and a sub-model $D_s$ depending on the pipeline stage of step s ($G_s$ and $D_s$ are empty sets for communication steps). The time of Waiting Phase and AllReduce Phase can be estimated by:\n$T_s^W = \\sum_{i=0}^{s-1} E_i^f$; $T_s^A = \\frac{2 (|G_s|-1) \\cdot \\sum_{l\\in D_s} W_l }{ |G_s| \\cdot \\min_{d,d'\\in G_s} b_{d,d'}}$.  (5)\n$T_s^W$ is the sum of FP time till step s \u2013 1. The size of communication volume per device in $G_s$ during AllReduce is $\\frac{2(|G_s|-1) \\cdot (\\sum_{l \\in D_s} W_l)}{}$. The time taken by the AllReduce operation, denoted by $T_s^A$, is further decided by the minimum connection bandwidth among all devices [35, 47].\nA remaining hurdle is how to estimate $T_s^E$ of each step. As illustrated in Fig. 4, the Execution Phases of all steps always form a trapezoid shape due to data dependency. Therefore, with an accurate latency estimation of one step, we can infer the $T_s^E$ of the other steps by considering the shift in FP and BP time before and after it. We observe that there always exists a step such that the FP and BP are compactly injected in its Execution Phases, which is the dominant factor in estimating Execution Phase latency. Thus, we define the step with the fewest number of bubbles during Execution Phase as dominant step and its Execution Phases latency can be well approximated as the accumulated time of FP and BP of M micro-batches. Although the dominant step may contain a small fraction of bubbles, leading to an approximation to the true pipeline latency, it has been practically effective in all our evaluations. As shown in Fig. 7, step 2 is the dominant step without bubble. Conversely, the remaining steps contain scattered bubbles that cannot be accurately estimated, rendering them unsuitable as the dominant step. Assuming that the index of the dominant step for a pipeline is dm, the $T_s^E$ of other steps can be estimated by:\n$T_s^E = M \\times \\begin{cases}\n\\sum_{l=i}^{d_m-1} (E_l^f+E_l^b) +{\\frac{\\sum_{l=i}^{s-1} (E_l^f +E_l^b) } {\\sum_{l=i}^{d_m-1} (E_l^f +E_l^b) }} \\cdot E_{d_m}^f& s< dm,\\\\ \\sum_{l=i}^{d_m} (E_l^f +E_l^b) +{ \\frac{\\sum_{l=d_m+1}^{s} (E_l^f +E_l^b)} {\\sum_{l=d_m+1}^{L} (E_l^f +E_l^b) }} \\cdot E_{d_m}^b, & s \\geq dm.\n\\end{cases}$   (6)\nBased on the aforementioned formulation, once $E_l^f$ and $E_l^b$ are determined, we can calculate the HPP-Round Latency according to Eq. (4). In the following, we first derive and formulate algorithms to estimate $E_l^f$ and $E_l^b$. We then design an innovative dynamic programming algorithm to identify the optimal HPP architecture from all possible configurations, effectively minimizing HPP-Round Latency.\nEstimating $E_l^f$ and $E_l^b$ for step s. For the communication steps, $E_l^f$ and $E_l^b$ can be estimated by the size of the intermediate tensors required for transmission between stages and the profiled D2D communication bandwidth. In the following, we concentrate on estimating $E_l^f$ and $E_l^b$ for execution steps, with a key objective being the optimal allocation of a micro-batch's samples among resource-diverse devices. This allocation seeks to minimize data parallel execution time within the memory budget of each device. We denote $T(i\\rightarrow j, G_s)$ as the optimal time taken by device group $G_s$ spanning layers i through j for both FP and BP. The optimization target can be formulated as follows:\n$T(i \\rightarrow j, G_s) = \\min_{\\Upsilon_s} \\max_{d \\in G_s} \\sum_{l=i}^{j} [t_{d,l}^f(y_d) + t_{d,l}^b(y_d)], $ s.t. $\\sum_{d \\in G_s} y_d = \\beta,  Mems_d(y_d) \\leq u_d,  (7)$\nwhere $\\Upsilon_s = \\{y_d|d \\in G_s\\}$ defines the set of the allocation of a micro-batch across devices in step s, with each element $y_d$ representing the number of samples allocated to device d. The peak memory footprint is given by $Mems_d(y_d)$ (refer to Eq. (3)). $u_d$ denotes the memory budget of device d. $E_l^f$ and $E_l^b$ can be estimated as follows once the $\\Upsilon_s$ is determined.\n$E_l^f = \\max_{d\\in G_s} \\sum_{l=i}^{j} t_{d,l}^f(y_d), E_l^b = \\max_{d\\in G_s} \\sum_{l=i}^{j} t_{d,l}^b(y_d)$. (8)\nTo efficiently solve the aforementioned constrained optimization problem, we propose an iterative planning algorithm as outlined in Algorithm 1. The algorithm can be divided into two distinct phases: memory-aware workload balancing phase and workload offloading phase. In the first phase, we recursively distribute the workload among devices in a manner that balances the workload based on their computing capacities, while strictly adhering to the memory budgets (line 1-12). A device's computing capacity $v_d$ is defined as the inverse of the sum of FP and BP execution latency with a micro-batch:\n$v_d = \\frac{1} {\\sum_{l=i}^{j} [t_{d,l}^f(\\beta) + t_{d,l}^b(\\beta)]} $.  (9)\nAs previously stated, our experiment reveals the non-linear relationship between batch size and execution time. Consequently, relying solely on the first phase will lead to suboptimal outcomes. In the second phase, we iteratively transfer the sample workload (one block at a time) from the straggler (the slowest device) to the fastest device with sufficient memory. The iteration terminates when the workload offloading results in a slower straggler (line 13-20). We can adjust the block size according to the micro-batch size to trade-off between planning overhead and balancing.\nDynamic Programming HPP Planning. Our dynamic programming algorithm searches for optimal HPP configurations to minimize HPP-Round Latency. To reduce the searching complexity for orchestration across heterogeneous devices, we sort the devices in descending order by their memory capacity and map stages accordingly. This sorting is inspired by the observation in \u00a75.4 that the earlier stages in Asteroid typically require more storage space for activations compared to the later stages.\nWe consider a DNN model consisting of L layers and aim to leverage the computing resource from an edge resource pool with N edge devices to efficiently perform HPP training. To achieve this, a novel dynamic programming algorithm is devised which facilitates optimal parallelism planning. We denote $Q(l, n, p)$ as the HPP-Round Latency of the optimal pipeline planning when slicing the last I consecutive layers into p stages and putting them onto the last n devices. The goal of our algorithm is to calculate $\\min_{p} Q(L, N, p)$. $Q(l, n, p)$ is updated as follows:\n$Q(l, n, p) = \\min_{l',n'} Q(l, n, l', n', p \u2013 1)$,  (10)\nwhere $Q(l, n, l', n', p \u2013 1)$ represents the HPP-Round Latency of a pipeline which comprises two parts (see Fig. 8 as illustration): (1) an optimal sub-pipeline $Q(l', n', p \u2013 1)$ consisting of the last l' layers with p \u2013 1 stages cross the last n' devices. (2) a new single stage (execution step) with layers L \u2013 I to L - l' replicated over remaining n n' devices using DP.\nTo obtain $Q(l, n, l', n', p \u2013 1)$, its dominant step must first be determined. Finding the step with the fewest bubbles during Execution Phase is equivalent to finding the step with the largest total FP and BP execution time after alignment. Consequently, we align and compare the total execution or communication time of the dominant step of sub-pipeline $Q(l', n', p \u2013 1)$, the new execution step at the head, and the communication step between the first step and $Q(l', n', p-1)$. The largest one contains the fewest bubbles during Execution Phase will be updated as the new dominant step:\n$max\\begin{cases}M \\times  {\\frac{\\sum_{l=i}^{d_m^{*}-1} (E_l^{f} + E_l^{b})}{\\sum_{l=i}^{L'-1} (E_l^{f} + E_l^{b}) }} + d_m^* , \\\\ Mx \\left|\\sum_{n=n'} (t_{d,i}^f+t_{d,i}^f) + E_{n^e}^f+E_{n^e}^b\\right|, \\\\ Mx \\left|E_{nd+1}^f E_{nd+1}^b + (E_{nc}^f + E_{nc}^b)\\right| \\end{cases}$ (11)\nwhere dm\u2217 denotes the original index of dominant step in sub-pipeline $Q(l', n', p \u2013 1)$, and ne and nc represent the index of the new execution step and new communication step, respectively. After determining the dominant step, we can estimate $T_w, T_e$ and $T_a$ for each step according to Eq. (5) and 6, and then infer the HPP-Round Latency by Eq. (4)."}, {"title": "3.4 Fault-Tolerant Pipeline Replay", "content": "Devices at the edge exhibit strong dynamics, as they may leave training at any time, or disconnect due to energy depletion or network anomalies. Such single-device failures can cause the following issues: (1) The device departing can result in the loss of the trained weights. (2) An abnormal device in pipeline can lead to blockages, which necessitate pipeline re-planning. A straw-man proposal is to aggregate stage models to coordinator, rerun the planning algorithm, and redistribute weights based on the new configuration. However, this heavy rescheduling may induce considerable latency in the re-planning and model transmission. To tackle these issues, Asteroid incorporates an on-the-fly fault-tolerant pipeline-replay lightweight rescheduling, featuring three modules that efficiently respond to resource fluctuations:\n1. Heartbeat-guided Failure Detection. As illustrated in Fig. 9(Left), each device periodically emits heartbeat signals to the central coordinator as proof of liveness. The absence of these signals within an expected timeframe may indicate a potential device failure. The coordinator will further dispatch a probe message specifically to the suspected device for confirmation of its operational status.\n2. Topology-driven Model Replication. Asteroid adopts a topology-driven model replication to mitigate the loss of trained weights due to device failure. As depicted in Fig. 9(Left), single-device stage (A and D) periodically backs up its stage model to a dedicated device (backup node) in the next stage, with the last stage being backed up to the first stage. The model weights can be restored from backup node when device failure occurs. In the presence of device failure in multi-device stages (B and C), model weights can be restored from other surviving devices within the same stage. We note that our topology-driven replication can complement other fault-tolerance mechanisms to address more dynamic scenarios, such as the simultaneous failure of multiple devices.\n3. Layer-wise Lightweight Pipeline Re-planning. To expedite the pipeline re-planning, we employ a layer-wise lightweight approach as a substitute for rerunning the entire planning algorithm. We quantify the workload by profiling the FLOPs of all model layers. In the case of a device failure, the associated workload is reallocated among the remaining stages based on their computing capacity ($\\sum_{d\\in G_s} u_d$). This can be achieved by making a minor adjustment to the layer partitioning points of the current planning configuration. As illustrated in Fig. 9(Right), our mechanism facilitates concurrent layer migration between adjacent stages according to the updated configuration, maximizing bandwidth utilization while minimizing redundant weights transmission.\nPutting It All Together. An illustrative example with four devices is presented in Fig. 9. Devices A, B, and D emit periodic heartbeat signals to coordinator C to confirm their liveness. Device A periodically checkpoints its stage model to backup node B, while Device D checkpoints its stage model to A. When detecting a failure in Device D, our lightweight FLOPs-based approach recalibrates the original layer partitioning. Subsequently, all stages concurrently migrate layers based on the new partitioning points. The layer weights initially on failed device D can be restored from backup node A. After re-planning, a refined two-stage pipeline involving three devices will take over the collaborative model training."}, {"title": "4 IMPLEMENTATION", "content": "We have fully implemented a prototype system of Aster"}]}