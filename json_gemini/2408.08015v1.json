{"title": "Asteroid: Resource-Efficient Hybrid Pipeline Parallelism for Collaborative DNN Training on Heterogeneous Edge Devices", "authors": ["Shengyuan Ye", "Liekang Zeng", "Xiaowen Chu", "Guoliang Xing", "Xu Chen"], "abstract": "On-device Deep Neural Network (DNN) training has been recognized as crucial for privacy-preserving machine learning at the edge. However, the intensive training workload and limited onboard computing resources pose significant challenges to the availability and efficiency of model training. While existing works address these challenges through native resource management optimization, we instead leverage our observation that edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources beyond a single terminal. We propose Asteroid, a distributed edge training system that breaks the resource walls across heterogeneous edge devices for efficient model training acceleration. Asteroid adopts a hybrid pipeline parallelism to orchestrate distributed training, along with a judicious parallelism planning for maximizing throughput under certain resource constraints. Furthermore, a fault-tolerant yet lightweight pipeline replay mechanism is developed to tame the device-level dynamics for training robustness and performance stability. We implement Asteroid on heterogeneous edge devices with both vision and language models, demonstrating up to 12.2\u00d7 faster training than conventional parallelism methods and 2.1\u00d7 faster than state-of-the-art hybrid parallelism methods through evaluations. Furthermore, Asteroid can recover training pipeline 14\u00d7 faster than baseline methods while preserving comparable throughput despite unexpected device exiting and failure.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep Neural Networks (DNNs) have driven diverse intelligence in today's smart applications, ranging from voice assistance, smart robotics to city surveillance, etc. While existing works have extensively studied the inference aspect of DNN models, the growing proliferation of human-in-the-loop intelligent services urgently emphasizes the necessity for privacy-preserving personalization and continuous model refinement, raising the need for advanced on-device learning ability. For instance, in Federated Learning [9, 38], user devices are required to provision a local model training task in order to contribute to and share the learning procedure. In Continual Learning [7, 58], user devices periodically retrain their local models with newly-collected data so as to adapt the model performance to the contextual factors.\nDespite the increasing demand, efficient in-situ learning still suffers from its prohibitively long training time and vulnerable convergence stability. As we will empirically show"}, {"title": "2 MOTIVATION AND PRELIMINARIES", "content": null}, {"title": "2.1 DNN Training on Resource-Constrained Edge Devices", "content": "On-device training can leverage locally collected data to improve model performance while fully preserving data in-situ, making it a widely utilized approach in privacy-sensitive edge applications [7, 38, 41, 48, 66]. However, the resource-intensive and computation-demanding nature of DNN training presents significant challenges for resource-constrained edge devices [17, 32, 53, 57]."}, {"title": "2.2 Edge Collaborative Training with Data Parallelism and Pipeline Parallelism", "content": "Data Parallelism. The most common way to train DNN models in parallel is data parallelism (DP) [18, 31]. In DP, inputs are partitioned across workers, and each worker maintains a replica of the entire model and performs training on its local data while periodically synchronizing gradients with other workers (i.e., AllReduce). The simplicity of its workload induces better scalability with multiple devices. However, due to the loose and varying edge connections, the communication overhead caused by synchronization can usually dominate training time, as shown in Fig. 1(Left).\nPipeline Parallelism. Another widely-used parallelism is pipeline parallelism (PP). PP is an advanced model parallelism-based training strategy that executes the DNN model in a pipelined manner across multiple workers [22]. Specifically, in PP, the DNN model is partitioned into multiple stages and each stage is mapped to a separate processor for stage-wise forward/backward pass execution. The partitioning ensures each stage has an approximately equal workload, optimizing parallel efficiency. For language model (e.g., Bert-small [14]) or crafted edge model with tiny activations, PP is far more communication-efficient than DP, since each worker only needs to exchange a subset of output activations with neighboring workers. Nonetheless, pipeline parallelism is also followed with shortcomings: (1) Weak scalability. The straight-forward implementation of PP for edge clusters can create too many stages, which amplifies the impact of inter-stage communication latency. (2) Unover-lappable inter-stage communication. When communication occurs between layers with huge intermediate activations, PP can not effectively overlap the inter-stage communication with forward and backward execution. Our experiment, implemented with Gpipe [22] for PP, shows that the inter-stage communication latency can be up to 24\u00d7 longer than the stage execution time, which dominates the entire training process. As shown in Fig. 1(Right), for CNN-based models, the per-sample data size communicated in PP even surpasses the communication volume necessitated by DP."}, {"title": "2.3 Key Insight: Combining Data Parallelism with Pipeline Parallelism", "content": "The above analysis motivates us to employ a hybrid parallelism architecture that incorporates the best of both DP and PP, so as to achieve superior performance in complex and heterogeneous edge environments. Hybrid Data Parallelism (HDP), as adopted by Hetpipe [42], organizes devices into groups for inter-group DP and intra-group PP, necessitating a centralized parameter server (PS) for full gradient exchange. Alternatively, another hybrid approach, Hybrid Pipeline Parallelism (HPP), utilized by PipeDream [39] and Dapple [16], arranges devices into groups for inter-group PP and intra-group DP, as depicted in Fig. 2."}, {"title": "Opportunities with HPP across Edge Devices", "content": "In light of the foregoing analysis, Asteroid employs HPP to facilitate collaboration with edge devices. Beyond breaking the resource wall of a single device, employing HPP offers the following benefits: (1) Each device stores only a subset of the entire model, leading to a smaller memory footprint, which is particularly advantageous for models with huge parameters. (2) HPP offers a highly flexible parallelism architecture that can effectively minimize communication volume by preventing AllReduce in parameter-dense layers. (3) Through layer-wise planning, HPP can avoid inter-stage communication between layers with huge intermediate tensors. By fully overlapping computation and communication, HPP conceals the limitations of network capacity in edge environments. (4) HPP provides higher scheduling flexibility and expands a larger optimization space of parallelism planning in our considered complex and heterogeneous edge environments."}, {"title": "2.4 Technical Challenges", "content": "Despite the benefits, realizing HPP in complex edge environments still suffers from a set of challenges.\nScarce Memory and Network Capacity. Edge devices, constrained by limited memory and bandwidth through links such as WiFi and cabled network [21], require meticulous planning for the HPP architecture to prevent out-of-memory issues and maximize bandwidth efficiency. Successfully leveraging the architecture's flexibility to address these constraints presents a significant challenge.\nHeterogeneous Computing Resource. Edge environments are typically heterogeneous [66], with facilities ranging from small devices and gateways [10] to much powerful cloudlets [46]. Efficiently applying HPP in such highly heterogeneous settings is a particularly challenging aim of maximizing its resource utilization, which requires judiciously matching workload distribution to diverse edge resources.\nDynamic Training Participants. Further complicating the problem is the inherent, unpredictable dynamics of available edge resources, due to devices moving across networks and multitasking [51, 53]. To render stable, reliable training performance with HPP therefore poses significant challenges in designing a robust scheduling such that training participants' failure is tolerable."}, {"title": "3 ASTEROID SYSTEM DESIGN", "content": null}, {"title": "3.1 System Overview", "content": "Fig. 3 depicts an overview of our proposed Asteroid which features three primary phases: Preprocessing Phase, Planning Phase and Execution Phase. Preprocessing Phase is an offline procedure that runs once before deployment. Asteroid Profiler performs a training process using calibration data as input on the physical edge devices to record the runtime profile necessary for parallelism planning (step 1). During Planning Phase, Asteroid Planner takes profiling results as input to generate planning configurations that include DNN model partitioning points, device grouping strategies, and micro-batch allocations within groups (step 2). These configurations comprehensively addresses challenges including memory budget, limited communication capacity, and resource heterogeneity, and is subsequently applied to target DNN models and training participants. In Execution Phase, Asteroid Worker which is deployed on each participant will be responsible for model execution, intermediate output exchange and gradient synchronization (step 3). To account for the challenge of runtime dynamics, a fault-tolerant pipeline replay mechanism will be applied (step 4). A central coordinator (a user-specified device) is required to apply planning configuration and detect device failure."}, {"title": "3.2 Hybrid Pipeline Parallelism in Asteroid", "content": "HPP Architecture and Workflow in Asteroid. As illustrated in Fig. 4(a), Asteroid's HPP first divides a DNN model into multiple stages where each contains a stage model composed of a set of consecutive network layers. Edge devices in the resource pool will be divided into a corresponding number of device groups, where each contains one or multiple devices. HPP combines pipeline parallelism across these groups with data parallelism within them. During training, a mini-batch will be split into M smaller micro-batches (with the size of B) and injected into the pipeline concurrently to increase parallelism. Micro-batches are broken up further if device groups contain multiple devices. Each device performs the forward pass (FP) and backward pass (BP) for the stage model it takes in charge and accumulates gradients for all micro-batches in each mini-batch. At the end of a mini-batch, gradients in each device group are synchronized using AllReduce and then applied to stage model parameters. The entire mini-batch training process is called an HPP-Round. Fig. 4(b) shows a well-designed scheduling arrangement for Asteroid HPP training that models inter-stage network communication, which cannot be neglected due to the significant latency under low-speed links in edge environments. Bubbles are idle periods when a stage waits for data from the previous one, represented by gray blocks.\nMemory-efficient 1F1B Micro-batch Scheduling. Our memory breakdown experiment in Fig. 5 shows that the peak"}, {"title": "3.3 Parallelism Planning", "content": "DNN Model and Asteroid Profiler. We consider a DNN model as a directed acyclic graph. The graph nodes represents modules like Conv, MaxPool, Attention, etc while the graph edge encodes the data dependency between modules. In order to split the model into multiple sequential stages, we topologically sort the graph nodes and transform the DNN model into layers sequence. Asteroid profiler precisely collects the total output size and weight parameters in bytes for each layer. We denote the output activations (and corresponding input gradients) and weight parameters of layer l as $a_l$, and $w_l$, respectively.\nWe further profile the FP and BP execution time of each layer. Existing works [25, 35] simply assume a linear relationship between batch size and execution time. However, our experiment reveals that smaller batch size fails to fully leverage the parallelism capacity of GPUs, leading to a non-linear correlation between them, as shown in Fig. 6. Therefore, Asteroid profiler measures the execution time of each layer on realistic edge hardware for various batch sizes. We denote $t^d_{al}(\\beta)$ and $t^d_{bl}(\\beta)$ as the FP and BP execution time for layer l on device d using a batch size of $\\beta$. We profile the D2D bandwidth between devices d and d' as $b_{d,d'}$.\nOptimization Objective Formulation. As illustrated in Fig. 7, to emphasize the non-negligible communication latency in edge environments, we abstract the process into separate steps, categorizing them as either execution steps or communication steps for inter-stage communication and stage model execution. We divide the training process on each step in an HPP-Round into three phases: Waiting Phase, Execution Phase, and AllReduce Phase, with corresponding times denoted as $T^s_w$, $T^s_E$, and $T^s_A$. Our optimization objective is to minimize the HPP-Round Latency, which is determined by the step with the largest total latency of three phases:\n$HPP-Round\\ Latency = \\underset{s\\in \\{0,1,...,S-1\\}}{max}(T^s_w + T^s_E + T^s_A).$ (4)"}, {"title": "Estimating $T_E^s$ and $T_E^s$ for step s", "content": "For the communication steps, $T_E^s$ and $T_E^s$ can be estimated by the size of the intermediate tensors required for transmission between stages and the profiled D2D communication bandwidth. In the following, we concentrate on estimating $T_E^s$ and $T_E^s$ for execution steps, with a key objective being the optimal allocation of a micro-batch's samples among resource-diverse devices. This allocation seeks to minimize data parallel execution time within the memory budget of each device. We denote $T(i\\rightarrow j, G_s)$ as the optimal time taken by device group $G_s$ spanning layers i through j for both FP and BP. The optimization target can be formulated as follows:\n$T(i \\rightarrow j, G_s) = \\underset{ \\{ Y_d \\} }{min} \\underset{ d \\in G_s }{max} \\lbrack \\underset{ l=i }{ \\sum^j } (t^d_{al}(y_d) + t^d_{bl}(y_d) ) \\rbrack$,\ns.t. $\\underset{ d \\in G_s }{\\sum} y_d = \\beta$, $Mem_s(y_d) \\le u_d$, (7)"}, {"title": "3.4 Fault-Tolerant Pipeline Replay", "content": "Devices at the edge exhibit strong dynamics, as they may leave training at any time, or disconnect due to energy depletion or network anomalies. Such single-device failures can cause the following issues: (1) The device departing can result in the loss of the trained weights. (2) An abnormal device in pipeline can lead to blockages, which necessitate pipeline re-planning. A straw-man proposal is to aggregate stage models to coordinator, rerun the planning algorithm, and redistribute weights based on the new configuration. However, this heavy rescheduling may induce considerable latency in the re-planning and model transmission. To tackle these issues, Asteroid incorporates an on-the-fly fault-tolerant pipeline-replay lightweight rescheduling, featuring three modules that efficiently respond to resource fluctuations:\n1. Heartbeat-guided Failure Detection. As illustrated in Fig. 9(Left), each device periodically emits heartbeat signals to the central coordinator as proof of liveness. The absence of these signals within an expected timeframe may indicate a potential device failure. The coordinator will further dispatch a probe message specifically to the suspected device for confirmation of its operational status.\n2. Topology-driven Model Replication. Asteroid adopts a topology-driven model replication to mitigate the loss of trained weights due to device failure. As depicted in Fig. 9(Left), single-device stage (A and D) periodically backs up its stage model to a dedicated device (backup node) in the next stage, with the last stage being backed up to the first stage. The model weights can be restored from backup node when device failure occurs. In the presence of device failure in multi-device stages (B and C), model weights can be restored from other surviving devices within the same stage. We note that our topology-driven replication can complement other"}, {"title": "4 IMPLEMENTATION", "content": "We have fully implemented a prototype system of Asteroid with ~2,000 LOC in Go and Python in total atop PyTorch [4]. Although we use PyTorch for auto-differentiation and computation graph execution, Asteroid is extensible and can work well with other lightweight ML frameworks such as TF-Lite [6] and MNN [26].\nStage Replication. Fig. 10 illustrates an example where stage n is replicated onto two devices and stage n + 1 spans three devices. During FP, 1/3 of the activations derived by each device on stage n is sent to each device on stage n + 1. During BP, each device on stage n + 1 splits its gradients into two sets and sends back to each device on stage n. We"}, {"title": "5 EVALUATION", "content": null}, {"title": "5.1 Experimental Setup", "content": "Models and Datasets. We evaluate Asteroid with 4 typical DNN models that are widely used in computer vision (i.e.,"}, {"title": "5.2 Comparison with DP and PP", "content": "Table 4 summarizes the training throughput results comparing Asteroid with on-device, DP and PP training methods. To facilitate a better comparison, we implement heterogeneous workload balancing for both DP and PP, and further employ our 1F1B scheduling for PP. To evaluate Asteroid's performance across clusters with diverse computational capabilities and network bandwidths, we conduct experiments on three different edge environments for each model: Env. A, Env. B and Env. B with 1000Mbps D2D bandwidth. For a fair comparison, all methods for each model share a same global mini-batch size of 2048 for EfficientNet-B1, MobileNetV2, Bert-small and 256 for ResNet50. While prior works have focused on optimizing DP and PP, our evaluation results indicate that Asteroid's HPP is the best for complex edge environment. In CNN-based models, feature map size decreases as layers deepen, and most parameters are in the end's fully connected layers. To reduce inter-stage communication with large activations and minimize AllReduce overhead, Asteroid employs DP in earlier layers and PP in later layers. For Transformer-based language models with huge parameters and small inter-layer activations, Asteroid's planner suggests a straight pipeline. As shown in Table 4, Asteroid achieves a training speedup of 2.1\u00d7-6.8\u00d7 and 1.3\u00d7-12.8\u00d7 compared to DP and PP methods, respectively. Specifically, for ResNet50"}, {"title": "5.3 Comparison with Existing Systems", "content": "Training Throughput Performance. We compare Asteroid with other state-of-the-art distributed training approaches. To showcase Asteroid's robustness in heterogeneous environments, we conducted experiments for all four models in two edge environments, B and C, each exhibiting different levels of heterogeneity. The results are demonstrated in Fig. 13. Our key observation is that Asteroid consistently and remarkably outperforms other existing parallelism under heterogeneous and resource-constrained edge environments. Specifically, Asteroid surpasses the DP method EDDL, achieving a throughput increase of 1.6\u00d7-6.9\u00d7. When compared to HPP methods PipeDream and Dapple, Asteroid attains throughput improvements of 1.3\u00d7-2.1\u00d7 and 1.2x-1.8\u00d7, respectively. This is because both PipeDream and Dapple are designed for homogeneous accelerator clusters in data-center, which results in unbalanced and suboptimal workload partition and device grouping for both inter-stage and intra-stage. When compare to HetPipe, Asteroid achieves a throughput increase of 1.2\u00d7-1.9\u00d7. HetPipe considers device heterogeneity but requires full model gradient exchange after each training iteration, leading to increased D2D communication volume. Also, HetPipe necessitates a centralized parameter server (PS) for asynchronous gradient exchange. Utilizing a bandwidth-limited edge device as a PS can become a bottleneck in the distributed system, and may also disrupt"}, {"title": "5.4 Optimization Implication", "content": "This subsection investigates the performance boost of each individual optimization technique introduced in \u00a73.\nAsteroid Parallelism Planning. We conduct an ablation study using EfficientNet-B1 and MobileNetV2 on Env. C to assess the contributions of our inter-stage and intra-stage planning to the overall system performance, as depicted in Fig. 15(a). The naive approach without any planning optimization treated all devices as homogeneous and overlooked memory and bandwidth constraints. We observe that our inter-stage planning, which considers both computing heterogeneity between stages and gradient synchronization overhead, substantially boosts training throughput. Our intra-stage planning further enhances throughput by taking into account intra-stage heterogeneity and judiciously managing memory budgets to prevent run-time OOM issues.\n1F1B Micro-batch Scheduling. Our analysis of the effectiveness of our 1F1B micro-batch scheduling revealed that, when applied to a 3-stage pipeline composed of three Jetson"}, {"title": "5.5 Fault Tolerance and Pipeine Replay", "content": "We evaluate our lightweight fault-tolerant pipeline replay module with EfficientNet-B1 on Env. D, with device orchestration illustrated in Fig. 16(Left). We simulate the individual dropout of four devices, contrasting our lightweight approach with the heavy rescheduling. Heavy rescheduling involves aggregating stage models, rerunning the planning algorithm, and redistributing weights according to the new configuration. The planning algorithm is re-executed on the most powerful remaining device. As illustrated in Fig. 16(Mid) and 16(Right), our mechanism recovers significantly faster than the heavy rescheduling and can achieve a comparable system throughput across diverse scenarios. Specifically, Fig. 17 showcases the real-time training throughput over a time window, wherein we deliberately halt device B at the 100th timestamp. We observe that our lightweight mechanism can achieve 90% throughput of heavy rescheduling, while recovering 14\u00d7 faster."}, {"title": "5.6 Scalability", "content": "We analyze the scalability of Asteroid on an 8-node homogeneous Jetson Nano cluster with a fixed micro-batch size of 32 per device (e.g. B=128 for 4 Jetson Nano) and conduct experiments on both EfficientNet-B1 and MobileNetV2 under 100Mbps network bandwidth. Fig. 18 shows that Asteroid's"}, {"title": "5.7 System Overhead", "content": "Energy Consumption. We measure the energy consumption during the EfficientNet-B1 training process on Env. D. The experiment shows that Asteroid consumes ~0.13 J per training sample and achieves 2-fold reductions in energy consumption compared to DP. This improvement can be attributed to both the reduced on-device training time and network communication overhead.\nPlanning Overhead. We evaluate the overhead of Asteroid's parallelism planning on Env. C involving six edge devices. The planning time across diverse models using Jetson NX is detailed in Table 7. Specifically, Asteroid Planner takes 480 seconds to partition the 213-layer EfficientNet-B1 optimally, compared to 69 seconds for the 56-layer Bert-small. As the number of model layers increases, the planning time rises significantly. To mitigate this overhead in practical deployment, we can partition models at a coarser granularity (e.g., residual blocks), thereby narrowing the search space.\nProfiling Overhead. Profiling is another major overhead in Asteroid. We profile models including EfficientNet-B1, MobileNetV2 and Bert-small with batch sizes ranging from 1 to 256, as well as ResNet50 with batch sizes from 1 to 32. The total profiling times for all models on each edge devices are detailed in Table 8. The profiling overhead can be linearly scaled down by concurrent profiling on more devices."}, {"title": "6 RELATED WORK", "content": "On-Device DNN Training. POET [43] manages to finetune a BERT model on an embedded device in both training and energy efficiency. Lin et al. [33] make on-device training possible with only 256KB of memory. Sage and Melon [17, 53] adopt hybrid memory managing and saving techniques such as operator fusion and dedicated memory pool to address the memory constraints. Mandheling [57] adopt mixed-precision training with DSP offloading for learning acceleration.\nCollaborative Edge Computing for DNNs. BlastNet, CoDL and \u00b5Layer [24, 30, 34] perform a collaborative DL inference on CPU and GPU concurrently. CoEdge, DeepThings and MoDNN [36, 62, 64] distributed execution of CNN-based inference applications on resource-constrained edge clusters. EDDL [19] adopts DP training across embedded devices.\nParallel DNN Training in Datacenter. DP [18, 31, 44, 47] is the most extensively used distributed training method in datacenter. PP [22, 29] has been proposed to conquer the memory issues of training large-scale transformer-based models. HPP and HDP [16, 25, 35, 39, 40, 42, 50, 65] which combine merits of both DP and PP has been further proposed to address the resource efficiency and scalability. Fig. 19 provides a comparison of Asteroid with other existing works, emphasizing the distinctions between them."}, {"title": "7 CONCLUSION", "content": "This paper proposes Asteroid for collaborative DNN training across heterogeneous and resource-constrained edge devices. Asteroid addresses multiple challenges faced in edge environments and achieves 12.2\u00d7 faster training than traditional methods and 2.1x faster than state-of-the-art HPP methods."}]}