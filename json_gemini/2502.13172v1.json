{"title": "Unveiling Privacy Risks in LLM Agent Memory", "authors": ["Bo Wang", "Weiyi He", "Pengfei He", "Shenglai Zeng", "Zhen Xiang", "Yue Xing", "Jiliang Tang"], "abstract": "Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated revolutionary capabilities in language understanding, reasoning, and generation (OpenAI, 2023; Zhao et al., 2023). Building on these advances, LLM agents use LLMs and supplement with additional functionalities to perform more complex tasks (Xi et al., 2023). Its typical pipeline consists of the following key steps: taking user instruction, gathering environment information, retrieving relevant knowledge and past experiences, giving an action solution based on the above information, and finally executing the solution (Wang et al., 2024a). This pipeline enables agents to support various real-world applications, such as healthcare (Abbasian et al., 2023; Tu et al., 2024), web applications (Yao et al., 2022, 2023), and autonomous driving (Cui et al., 2024; Mao et al., 2023).\nDespite their success in advancing various domains, LLM agents often utilize and store private information, causing potential privacy risks, particularly in privacy-intensive applications such as healthcare. The private information of an LLM agent mainly originates from two sources: (1) The data the agent retrieves from external databases, containing sensitive and valuable domain-specific information (Li et al., 2023; Kulkarni et al., 2024), e.g., patient prescriptions used in healthcare agents. (2) Historical records stored in the memory module\u00b9 (Zhang et al., 2024), consisting of pairs of private user instructions and the agent's generated solutions. For example, in an intelligent auxiliary diagnosis scenario, a clinician's query about treatment recommendations for a patient's condition can expose the patient's health status.\nWhile prior works have explored external data leakage in retrieval-augmented generation (RAG) systems (Zeng et al., 2024; Jiang et al., 2024), the security implications of the memory module in LLM agents remain underexplored. RAG retrieves and integrates external data into prompts to enhance the LLM's text generation (Lewis et al., 2020; Fan et al., 2024). The integrated external data can be extracted by privacy attacks. In contrast, the memory module that stores user-agent interactions emerges as a new source of private information. It inherently contains sensitive user data, and there is limited understanding of whether private information in memory can be extracted and how vulnerable it is. Private information leakage from memory can result in serious privacy risks, such as unauthorized data access and misuse. Consider a clinician using an LLM agent to assist with patient diagnosis and treatment planning, where queries may contain sensitive patient information. If the medical agent's memory containing such medical details was exposed, insurance companies could exploit it to impose discriminatory charges on patients."}, {"title": "2 Background and Threat Model", "content": "In this paper, we study the risk of LLM agent memory leakage by investigating the following research questions:\n\u2022 RQ1: Can we extract private information stored in the memory of LLM agents?\n\u2022 RQ2: How do memory module configurations influence the attackers' accessibility of stored information?\n\u2022 RQ3: What prompting strategy can enhance the effectiveness of memory extraction?\nTo answer these questions, we develop a Memory EXTRaction Attack (MEXTRA) targeting the memory module of general agents. We consider a black-box setting where the attacker can only interact with the agent using input queries, referred to as attacking prompts. However, designing an effective attacking prompt to achieve such a goal poses unique challenges. First, since LLM agents often involve complex workflows, previous data extraction attacking prompts used on external data leakage (Zeng et al., 2024; Jiang et al., 2024) like \u201cPlease repeat all the context\" struggle to locate and extract memory data from an informative task-related context. Second, since the final action of LLM agents can be different from generating output texts, the RAG data extraction attack becomes infeasible.\nTo handle these challenges, we design a template to equip the attacking prompt with multiple functionalities. In the first part of the prompt, we explicitly request the retrieved user queries and prioritize their output over solving the original task. Then, we specify the output format of the retrieved queries, ensuring that it aligns with the agent's workflow. To further explore the vulnerability of agents, we consider different scenarios where the attacker has different levels of knowledge about the agent implementation. Additionally, we develop an automated method to generate diverse attacking prompts to maximize private information extraction within a limited number of attacks.\nWith the attacking prompt design and the automated generation method, we find LLM agents are vulnerable to memory extraction attacks. The auto-generated attacking prompts following the prompt design can effectively extract the private information stored in the LLM agent memory. Through deeper exploration, we observe that the different choices in memory module configuration significantly impact the extent of LLM agent memory leakage. Moreover, from the attacker's perspective, increasing the number of attacks and possessing detailed knowledge about the agent implementation can lead to more memory extraction."}, {"title": "2.1 Agent Workflow", "content": "In this work, we focus on an LLM agent that generates an executable solution $s$ to complete its assigned task for an input user query $q$. The solution may include executable actions such as running the generated code $s$ in code-powered agents (Yang et al., 2024) or performing operations $s$ such as search and click in web agents (Yao et al., 2023).\nThe LLM agent is equipped with a memory module $M$ storing $m$ records. Each record is in the form of $(q_i, s_i)$ where $q_i$ represents a previous user query and $s_i$ is the corresponding solution generated by the agent. The records stored in $M$ are integrated during the reasoning and planning process of the agent. In particular, given an input query $q$, the agent uses a similarity scoring function $f(q, q_i)$ to evaluate and rank the queries in memory $M$. Based on these scores, it retrieves the top-k most relevant records as a subset $E(q, M) \\subset M$, i.e.,\n$E(q, M) = \\{(q_i, s_i)|f(q, q_i) \\text{ is in the top-k}\\}.$\nThese retrieved records are then utilized as in-context demonstrations, helping the agent generate a solution $s$, which can be written as:\n$\\text{LLM}(C || E(q, M) || q) = s,$\nwhere $\\text{LLM}()$ denotes the LLM agent core, $C$ represents the system prompt including all task-related context, and $||$ denotes the concatenation. Finally, the LLM agent executes $s$ through tool calling to complete the user query, formulated as:\n$o = \\text{Execute}(s, T),$\nwhere $T$ denotes the tools, and $o$ denotes the final output of the agent, which may include execution results from code, interactions with web applications, or other task-specific actions, depending on the type of solution and the agent's application scenario. If the solution is executed successfully, the new query-solution pair will be evaluated and then selectively added to the memory for reflection."}, {"title": "2.2 Threat model", "content": "Attacker Objective. LLM agent memory stores past records $(q_i, s_i)$, where $q_i$ may contain private information about the user. The attacker's goal is to craft attacking prompts to extract as many past user queries $q_i$ from memory as possible. Once the user queries are obtained, the corresponding agent responses can be easily reproduced.\nThe attacking prompt $\\tilde{q}$ induces the LLM agent to generate a malicious solution $\\tilde{s}$, formulated as:\n$\\text{LLM}(C || E(\\tilde{q}, M) || \\tilde{q}) = \\tilde{s}.$\nThen the execution of $\\tilde{s}$ is expected to output all user queries in $E(\\tilde{q}, M)$, allowing the attacker to extract them from memory, formulated as:\n$\\tilde{o} = \\text{Execute}(\\tilde{s}, T) = \\{q_i|(q_i, s_i) \\in E(\\tilde{q}, M)\\},\nwhere $\\tilde{o}$ denotes the execution results.\nMoreover, to expand the extracted information, the attacker designs $n$ diverse attacking prompts $\\{\\tilde{q}_j\\}_{j=1}^{n}$, aiming to reduce overlap among retrieved records $E(\\tilde{q}_j, M)$ and consequently among extraction results $\\tilde{o}_j$. Formally, with $n$ attacking prompts, the attacker aims to maximize the size of\n$Q = \\bigcup_{j=1}^{n}\\{q_i | q_i \\in \\tilde{o}_j\\},\nwhere $Q$ denotes the set of all extracted user queries. The set of $n$ retrieved subsets is denoted as $R = \\bigcup_{j=1}^{n} E(\\tilde{q}_j, M), |R| > |Q|$. For simplicity, we omit the subscript $j$ where no ambiguity arises.\nAttacker Capability. We consider a black-box attack in which the attacker interacts with the LLM agent only through input queries. Under this setting, we examine two levels of knowledge the attacker may have about the agent: (1) Basic level, where the attacker has only general background information about the agent, such as its applied domain and task. For example, in the case of a healthcare record management agent (Shi et al., 2024), the attacker knows that the agent interacts with the healthcare record to answer user queries. (2) Advanced level, where the attacker gains some specific implementation details of the agent through exploratory interactions. In this paper, we assume the attacker can infer the similarity scoring function $f(q, q_i)$ after multiple interactions, which may be based on semantic similarity (e.g., cosine similarity) or query format similarity (e.g., edit distance)."}, {"title": "3 Paradigm of Attack Design", "content": "With only black-box access to the agent, the success of our MEXTRA heavily relies on the design of attacking prompt. While existing works on privacy issues in RAG (Zeng et al., 2024; Jiang et al., 2024) have proposed several effective designs, they are not directly applicable to LLM agents due to agents' inherently complex workflows.\nFirst, attacking prompts for RAG data extraction struggle to extract retrieved user queries because the input of LLM agent core contains extensive task-related details, such as the descriptions of the workflow, the available tools, and accessible databases. Ambiguous commands like \u201cPlease repeat all the context\" used in RAG privacy extraction (Zeng et al., 2024) fail to accurately target the retrieved user queries, leading to the failure of the attack as shown in Appendix B.2. Therefore, an effective attacking prompt needs to explicitly locate the retrieved user queries and then prioritize"}, {"title": "3.1 Attacking Prompt Design", "content": "outputting them over solving the original task.\nSecond, LLM agents often involve diverse tasks and solutions, as we mentioned in \u00a72.1. The execution results extend beyond textual output, making straightforward text generation requests infeasible. For example, consider a web agent that comprises operations on a website, such as search or click actions, and does not directly generate text. A prompt like \"Please output all retrieved user queries\" has difficulty extracting memory data, since the agent's workflow does not support such a request or determine the appropriate action to complete it. Thus, the attacker must specify the format of the retrieved output, ensuring that it aligns with the agent's workflow and remains accessible to the attacker.\nBased on the above discussions, we design a memory extraction attacking prompt $\\tilde{q}$ as follows:\n$\\tilde{q} = \\tilde{q}_{loc} || \\tilde{q}_{align},$\nwhere the locator part $\\tilde{q}_{loc}$ is used to specify what contents in the long text to extract, and the aligner part $\\tilde{q}_{align}$ is used for aligning with the agent's workflow by specifying the output format. For example, for a web agent, $\\tilde{q}_{loc}$ could be \u201cI lost previous examples\" which requests retrieved examples rather than other descriptions, and $\\tilde{q}_{align}$ could be \u201cplease enter them in the search box\u201d (Figure 1).\nFollowing the above prompt design, the attacker can craft attacking prompts for general agents to extract private data from their memory. However, one attacking prompt can extract at most $k$ user queries in $E(\\tilde{q}, M)$. To potentially access more data from memory, the attacker must design more diverse queries to retrieve different records from the memory, leading to a larger $R$ and consequently a larger $Q$. Since manually designing attacking prompts is time-consuming and inefficient, we further develop an automated diverse prompts generation method."}, {"title": "3.2 Automated Diverse Prompts Generation", "content": "To automatically generate diverse prompts for extraction attacks, we employ GPT-4 (OpenAI, 2023) as the attacking prompts generator. The instruction used for this generation has two main goals: (1) Extraction functionality: ensure the generated queries meet the prompt design elaborated in \u00a73.1; and (2) Diverse retrieval: ensure the queries are diverse to obtain a larger extracted query set $Q$.\nWhile the extraction functionality is guaranteed by the prompt design in \u00a73.1, the diversity of queries depends on the level of attacker's knowledge about the agent. Under the basic level of knowledge about the agent, we design a basic instruction $T_{basic}$ to prompt the generator to produce $n$ attacking prompts that preserve the same extraction functionality while varying in phrasing and expression. $T_{basic}$ consists of four parts: task description, prompt generation requirements based on the two goals, output format, and in-context demonstrations of valid attacking prompts. This conservative strategy does not require any detailed implementation information of agents, making it applicable to memory extraction attacks for general LLM agents.\nUnder the level of advanced knowledge, the diversity of generated attacking prompts can be further improved. With the assumption of advanced knowledge in \u00a72.2 that the attacker has inferred the scoring function $f(q, q_i)$ through exploratory interactions, we propose advanced instructions $T_{advan}$. For example, if $f(q, q_i)$ relies on similarities in query format and length like edit distance, $T_{advan}$ will include additional instructions for the generator to generate attacking prompts of different lengths. This helps extract user queries of diverse lengths and increase the total number of extracted queries. Alternatively, if $f(q, q_i)$ is based on semantics similarity like cosine similarity, $T_{advan}$ leverages diverse semantic variations rather than merely differing expressions as in $T_{basic}$. Specifically, it prompts the generator to produce $n$ domain-specific words or phrases $s$. For example, in an online shopping scenario, the phrases could be \"furniture\u201d or \u201celectronic products\u201d to capture semantically similar queries. These generated phrases $s$ are then separately added to the same attacking prompt $\\tilde{q}$ to create multiple semantic-oriented attacking prompts, formulated as $\\tilde{q}_s = s || \\tilde{q}$."}, {"title": "4 RQ1: LLM Agent Memory Extraction", "content": "With the attacking prompts generated through the basic instruction $T_{basic}$, we empirically investigate the privacy leakage of the LLM agent memory on two real-world application agents. Our evaluation reveals the LLM agent's high vulnerability to our memory extraction attack MEXTRA."}, {"title": "4.1 Experiments Setup", "content": "Agent Setup. We select two representative real-world agents for different applications: EHRAgent (Shi et al., 2024) and Retrieval-Augmented Planning (RAP) framework (Kagaya et al., 2024)."}, {"title": "4.2 Attacking Results", "content": "LLM agent is vulnerable to our proposed memory extraction attack. We present the attacking results of 30 prompts for our attacks and baselines. With a memory size of 200 and only basic knowledge of the LLM agent, our 30 prompts generated by attacking prompt generator with $T_{basic}$ extract 50 private queries from EHRAgent and 26 from RAP. Moreover, the CER values for the two agents are 0.83 and 0.87, closely matching to AER, which indicates that most attacking prompts successfully extract all retrieved queries. We achieve an EE of over 0.4 on EHRAgent and approximately 0.3 on RAP, demonstrating the high efficiency of the proposed extraction attack. These results re-"}, {"title": "5 RQ2: Impact of Memory Module Configuration", "content": "In this section, we explore the impact of memory module configuration on LLM agent memory leakage. Our analysis highlights which configurations are more susceptible to memory extraction attacks."}, {"title": "5.1 Memory Module Configuration", "content": "We consider five alternative design choices in memory module configuration for LLM agent memory: (1) the similarity scoring function $f(q, q_i)$, we alternate it between cosine similarity and edit distance; (2) the embedding model $E(\u00b7)$ used to encode queries when $f$ is cosine similarity, i.e., $f(q, q_i) = cos(E(q), E(q_i))$. We select three models varying in model size under the SBERT architecture (Reimers and Gurevych, 2019): MiniLM (Wang et al., 2020), MPNet (Song et al., 2020), and ROBERTa_large (Liu et al., 2019), (3) the retrieval depth $k$ ranging from 1 to 5, determining the number of retrieved records; (4) the memory size $m$ ranging from 50 to 500, with smaller memory sets being subsets of larger ones; and (5) the backbone of the LLM agent core, we alter it between GPT-4 (OpenAI, 2023), GPT-4o and Llama3-70b (Dubey et al., 2024). To explore the impact of different configurations, we change one or several configurations at a time while keeping others fixed. All default settings for the agents are set according to their original configurations detailed in \u00a74.1."}, {"title": "5.2 Results Analysis", "content": "Scoring Function. We modify the implementations of the two agents to alter their scoring functions. The extracted numbers for both agents under two different scoring functions are presented in Table 2. The results indicate that when $f(q, q_i)$ is edit distance, the extraction performance consistently surpasses that of cosine similarity, regardless of memory size. This significant difference highlights the crucial role of the scoring function in an LLM agent's susceptibility to extraction attacks. Also, the results suggest that when no specific implementation details are known, the retrieval based on edit distance is more vulnerable to extraction attacks.\nEmbedding Model. When $f(q, q_i)$ is set to cosine similarity, we compare extraction performance across different embedding models to analyze their"}, {"title": "6 RQ3: Impact of Prompting Strategies", "content": "In this section, we further explore the impact of different prompting strategies used by the attacker. Specifically, we examine the number of attacking prompts and the two prompt generation instructions introduced in \u00a73.2. The results indicate that increasing the number of attacks and having more implementation knowledge about the agent enhance the effectiveness of memory extraction."}, {"title": "6.1 Experiment Settings", "content": "We vary the number of attacking prompts from 10 to 50 in increments of 10, with smaller sets being subsets of larger ones. To explore the effectiveness of the advanced instruction $T_{advan}$, which assumes the attacker has inferred the scoring function $f(q, q_i)$, we set $f(q, q_i)$ as either edit distance or cosine similarity for both agents. In this way, we design $T_{advan}$ for four cases: EHRAgent and RAP, each with edit distance and cosine similarity."}, {"title": "6.2 Results Analysis", "content": "The number of attacking prompts. The EN and RN results across different numbers of attacking prompts and prompt generation instructions are summarized in Figure 4. As the number of attacking prompts increases, both the EN and the RN continue to rise, with no significant slowdown in growth rate. When n reaches 50, regardless of the prompt generation instructions, agents using edit distance as their scoring function leak more"}, {"title": "7 Related Work", "content": "LLM Agent with Memory. Memory storing user-agent interactions provides valuable insights for LLM agents in solving real-word applications, making it an essential component of LLM agents (Zhang et al., 2024). However, while equipping LLM agents with memory improves performance, it also introduces privacy risks. For instance, healthcare agents (Shi et al., 2024; Li et al., 2023) store sensitive information about patients, web applications"}, {"title": "8 Conclusion", "content": "In this paper, we unveil the privacy risks of LLM agent memory leakage through a memory extraction attack, MEXTRA. It consists of two parts: attacking prompt design and automated attacking prompt tailored to different levels of knowledge about the agent. Empirical evaluations demonstrate the vulnerability of LLM agents to MEXTRA. Moreover, we explore the key factors that influence memory leakage from both the agent's and the attacker's perspectives."}, {"title": "Limitation", "content": "Our memory extraction attack has only been evaluated on a single-agent setup. Extending it to a multi-agent setup, where agents communicate or share memory, would be an interesting direction for future research. Investigating how inter-agent interactions impact the risk of memory leakage could provide deeper insights into privacy vulnerabilities in LLM agents. In addition, the agent framework we consider does not incorporate session control: multiple users may share the same session, causing the memory module to store historical records from all users. Introducing user-level and session-level memory isolation would limit attackers' access to private data and mitigate the impact of memory extraction. However, since there is no standard method for integrating session control into agent frameworks, we leave its exploration for future work."}, {"title": "Appendix", "content": ""}, {"title": "A Instructions for Prompts Generation", "content": ""}, {"title": "A.1 Basic Instruction", "content": "The basic instruction $T_{basic}$ consists of four parts: task description, prompt generation requirement, output format, and in-context demonstrations. It instructs the attacking prompt generator to generate 10 attacking prompts per run and repeat the process multiple times until the required total number $n$ is reached."}, {"title": "A.2 Advanced Instruction", "content": "Advanced Instruction for Edit Distance. Compared with $T_{basic}$, the advanced instruction $T_{advan}$ tailored for edit distance further instructs the generator to adjust the length of attacking prompts to extract user queries of diverse lengths.  Advanced Instruction for Cosine Similarity. The advanced instruction $T_{advan}$ tailored for cosine similarity is much different from $T_{basic}$."}, {"title": "A.3 Instructions for Baselines", "content": "The instructions for baselines are revised based on the $T_{basic}$. For \"w/o aligner\u201d, we eliminate the part of aligner by removing the second query generation requirement and replacing the demonstrations, as shown in Table 7. For \u201cw/o req\u201d, we remove the prompt generation requirement from the original $T_{basic}$. And for \u201cw/o demos\u201d, we remove the examples from the original $T_{basic}$."}, {"title": "B More Details about Experiments", "content": ""}, {"title": "B.1 Experiment Setup", "content": "Agent Setup. EHRAgent enables autonomous code generation and execution, helping clinicians directly interact with EHRs using natural language.  RAP is a general paradigm designed to leverage past records dynamically based on the current situation and context."}, {"title": "B.2 More Experiments", "content": "Case study.  Overlap Analysis. To explore the overlap in the retrieved record set |R|, we visualize the retrieved times of queries within the retrieved records set R and their corresponding counts in Figure 5. It is"}, {"title": "C Discussion about Potential Mitigation", "content": "The vulnerability of LLM agent to MEXTRA highlights the need for mitigation strategies. We consider two categories of defenses: (1) Input and output control , which aims to prevent private information exposure during query processing and response generation. However, the attacking prompt can be tailored to appear contextually harmless and normal, making the filter fail to detect. And paraphrasing may not fully eliminate sensitive information. (2) Memory sanitation, which focuses on ensuring that stored data does not contain private information."}]}