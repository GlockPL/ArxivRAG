{"title": "Towards Real-world Deployment of NILM Systems: Challenges and Practices", "authors": ["Junyu Xue", "Yu Zhang", "Xudong Wang", "Yi Wang", "Guoming Tang"], "abstract": "Non-intrusive load monitoring (NILM), as a key load monitoring technology, can much reduce the deployment cost of traditional power sensors. Previous research has largely focused on developing cloud-exclusive NILM algorithms, which often result in high computation costs and significant service delays. To address these issues, we propose a three-tier framework to enhance the real-world applicability of NILM systems through edge-cloud collaboration. Considering the computational resources available at both the edge and cloud, we implement a lightweight NILM model at the edge and a deep learning based model at the cloud, respectively. In addition to the differential model implementations, we also design a NILM-specific deployment scheme that integrates Gunicorn and NGINX to bridge the gap between theoretical algorithms and practical applications. To verify the effectiveness of the proposed framework, we apply real-world NILM scenario settings and implement the entire process of data acquisition, model training, and system deployment. The results demonstrate that our framework can achieve high decomposition accuracy while significantly reducing the cloud workload and communication overhead under practical considerations.", "sections": [{"title": "I. INTRODUCTION", "content": "To monitor the status and energy consumption of household electrical appliances, load monitoring technologies are widely used to analyze energy system data. Among these, Non-Intrusive Load Monitoring (NILM) is a crucial research area, as it eliminates the need for separate sensors for each appliance. In general, NILM is highly significant for energy conservation, emission reduction, fault diagnosis, and power grid planning. Particularly, by providing detailed consumption data, NILM can achieve significant energy savings of around 15%, as it encourages households to use energy more sustainably [1]. Furthermore, NILM can enhance protection plans, improve load forecasting accuracy, and serve as a benchmark for grid management. With real-time NILM, utilities can recommend specific appliance operations, such as switching off air conditioners during peak hours, to manage the power load more effectively [2].\nAlthough the concept of NILM was introduced in the past, its widespread application was initially limited by low accuracy and scarce data resources. However, recent advancements in big data, deep learning, and related technologies have led to an abundance of electrical data and significant improvements in NILM accuracy and decomposition, thus promoting its applications. To further enable the wide application of NILM in reality, distributed learning technologies, particularly edge-cloud collaboration, are becoming essential. Cloud computing provides robust computational power and inferential capabilities for training large machine-learning models used in NILM, allowing enterprises to enhance model accuracy and broaden service scope based on data characteristics and scale. Meanwhile, edge computing preprocesses electrical data at the edge, reducing the load on the cloud. With adequate edge computing power, lightweight models can be deployed locally to minimize data transmission delays and mitigate data leakage risks. This collaboration leverages the strengths of both technologies, offering users more efficient and secure services.\nSeveral studies have combined NILM with edge-cloud collaboration techniques. Hong et al. [3] proposed a NILM framework based on edge-cloud collaboration, utilizing the deep learning Adaboost algorithm for matching at the edge. However, this framework relies on high-frequency sampling data to improve accuracy, resulting in high communication costs. Hudson et al. [4] designed a framework that integrates federated learning with edge-cloud collaboration to address NILM challenges, reducing communication costs and privacy leakage risks, but it does not thoroughly discuss the cooperative functioning of edge and cloud components. Generally, current research focuses on cloud-exclusive functions, leading to high consumption of cloud computing resources and significant delays. Moreover, these studies often overlook real-world settings and lack practical applications. Motivated by this, we emphasize the practical significance of edge-cloud collaboration in addressing NILM application challenges, particularly in balancing resource consumption and managing concurrency issues. Our contributions can be summarized as follows:\n\u2022\tWe propose a three-tier collaborative framework involving the cloud, edge, and client to enhance the applicability of NILM technology, aiming to reduce the cloud's"}, {"title": "II. RELATED WORK", "content": "Intrusive Load Monitoring (ILM) requires a sensor for each monitored device, leading to high costs. In response, Non-Intrusive Load Monitoring (NILM) has been proposed, requiring only the measurement of aggregated data and using decomposition algorithms to estimate individual device consumption. NILM technology was originally introduced to minimize the number of installed electricity meters, thereby reducing wiring harnesses and increasing retrofit capacity. Over the past decades, NILM has found applications in various fields, including public administration and energy management, such as optimizing load schedules in smart grids and improving customer satisfaction [1]. In private sectors, NILM technology is used for fault detection and diagnosis in both industrial and residential settings [2], and it can also evaluate socioeconomic information and consumer behavioral patterns [5].\nSince appliances usually exhibit different electrical and operational characteristics, non-intrusive load monitoring techniques can obtain power aggregate signals and then estimate the energy consumption of individual appliances using decomposition algorithms. NILM methods can be divided into three main categories: Machine Learning (ML), Pattern Matching (PM), and Single-Channel Source Separation (SS) [6]. With the continuous advancement of artificial intelligence, most mainstream NILM techniques today are based on machine learning. The core idea is to extract features from the electrical data and use them to train machine learning algorithms.\nAt the same time, the development of deep learning and big data has led to a significant increase in data-driven methods using large-scale datasets. Models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs) [7], and Long Short-Term Memory networks (LSTM) [8] have been used to solve NILM problems. Some studies have focused on models such as Generative Adversarial Networks (GAN) and Transformer [9] to integrate self-attention mechanisms to further improve the performance of decomposition algorithms. At present, the most mainstream models are Seq2Seq [10], Seq2Point [11], and their variants. Too many parameters will lead to long inference time and greatly reduce practicability. Similarly, some research focuses on the design of lightweight networks and the application of methods such as federated learning to realize parameter sharing."}, {"title": "B. Edge-Cloud Collaboration", "content": "With the advancement of deep learning and edge hardware, there is a growing trend to integrate the two paradigms of cloud computing and edge computing. Edge-cloud collaboration can fully merge the benefits of each and has been implemented in some scenarios. In the smart grid, technologies such as edge computing and the Internet of Things can provide comprehensive control and monitoring solutions to improve energy efficiency, reliability, and real-time service response time [12]. In the NILM system, it can enhance overall applicability. For example, it can shift the processing and storage of power data, NILM model decomposition, and other tasks to the cloud, making the system easier to use and maintain [13]. With the support of the cloud's ample computing power, the resource limitation of the edge side can be overcome, and the accuracy of the NILM algorithm can be enhanced [14], [17]. The edge side can carry out operations such as data cleaning, preprocessing, and even machine learning model deployment if the computing power allows. This can reduce the amount of data received by the cloud, lessen the load, and reduce the system's cost.\nMost existing edge-cloud collaborative architectures applied to NILM are two-tier. Smart meters and similar hardware perform data collection and processing at the edge, while servers handle load decomposition in the cloud. In some implementations, load decomposition is conducted at the edge, with the cloud responsible for data dumping and business services. The proposed structure differs by decoupling the edge-end and providing a more fine-grained division of labor at the edge. Data acquisition can be fully delegated to the CT or the meter, greatly reducing the demands on the original hardware performance in the monitored environment. Furthermore, the additional edge-end system carrier can be chosen according to the specific task, lowering deployment costs while maintaining the architecture's benefits."}, {"title": "III. FRAMEWORK OF EDGE-CLOUD COLLABORATION", "content": "Currently, most NILM systems function exclusively within cloud environments, neglecting the potential benefits of client-side collaboration. This reliance on cloud-based processing leads to substantial consumption of computing resources and significant latency. To address these issues, we propose an innovative edge-cloud collaborative NILM framework. This framework aims to optimize the deployment of NILM models by distributing relatively light computational tasks to edge devices, such as data preprocessing, thereby enhancing efficiency and reducing delays. Furthermore, our study examines the applicability and effectiveness of this hybrid approach in real-world scenarios."}, {"title": "A. Overview", "content": "Most existing edge-cloud architectures adopt a two-tier structure, which leads to high computational costs and compromises on latency. To address this limitation, we propose a novel three-tier architecture comprising a central cloud, edge cloud, and user, where the edge and central clouds synergistically collaborate to deliver services to users. detailed design of the three components is presented below."}, {"title": "a) Cloud Side", "content": "The cloud side consists of a powerful server with substantial computing resources. In the proposed architecture, the NILM server is designed to operate independently of the business server, allowing for greater flexibility and scalability. This separation is necessary because NILM requires the implementation of a deep learning model, which can be efficiently supported by a microservice architecture. This approach facilitates more nuanced business differentiation, while also providing enhanced maintainability, scalability, and portability."}, {"title": "b) Edge Side", "content": "The edge side comprises two primary components: terminal equipment and a system carrier. Terminal equipment mainly refers to smart meters. Notably, smart meters can be substituted with hardware devices featuring monitoring capabilities, such as smart CT devices. Currently, various brands offer products for power monitoring, including the Sense Energy Monitor, Emporia Vue Smart Home Energy Monitor, TP-Link Kasa Smart Wi-Fi Plug with Energy Monitoring, and Zendure Satellite Monitor CT, among others. The system carrier can be any hardware that supports data preprocessing and model deployment, with its performance contingent upon the system's required functionality. It is worth noting that if the smart meter possesses sufficient performance and scalability, the need for a system carrier may be obviated."}, {"title": "c) Client Side", "content": "The client side of the system comprises the end-users, typically comprising individuals and their families, who reside in households with diverse electrical appliances. These appliances can range in size from small devices like heaters and light bulbs to large equipment such as washing machines and air conditioners. When a user initiates a request, the system's NILM service is triggered, facilitating the analysis of the electrical consumption patterns of these households. This service enables the system to provide personalized insights, recommendations, and feedback to the users, empowering them to optimize their energy usage, reduce their carbon footprint, and improve their overall quality of life."}, {"title": "B. Communication Middleware", "content": "During the implementation of the proposed NILM framework, we encountered two significant challenges in the actual application scenario:\n\u2022\tAsynchronous processing: Electrical data is not always delivered to the model immediately after it is sent to the cloud. Although data is transmitted in real-time, batch inference is typically performed after reaching a predetermined threshold to reduce the consumption of computing resources, resulting in an asynchronous processing phenomenon."}, {"title": "\u2022 High concurrent traffic", "content": "NILM systems need to process a large amount of electrical information from different homes simultaneously. When a large number of users request NILM services at the same time, the system can use RabbitMQ as a buffer to ensure that the service runs normally."}, {"title": "Therefore", "content": "we propose to leverage the RabbitMQ Message Queue to achieve efficient communication between the cloud and edge. RabbitMQ is an open-source messaging middleware that implements AMQP(Advanced Message Queuing Protocol) in Erlang. It is widely used by various companies because of its reliability, flexible messaging policy, and support for clustering."}, {"title": "C. NILM Model", "content": "We propose distinct decomposition models tailored to the cloud and edge environments. Due to its limited computing power, the edge requires a more lightweight model, whereas the cloud's abundant computing resources can support more complex network architectures and larger models, leading to enhanced accuracy in NILM. By leveraging the strengths of both edge and cloud, we can achieve a more precise and efficient NILM service for clients."}, {"title": "a) Edge-End Lightweight Model", "content": "For NILM model deployment on the edge, we use the lightweight XGBoost [18]. More specifically, XGBoost (eXtreme Gradient Boosting) is an implementation of the Gradient Boosting Decision Tree (GBDT) that relies on the gradient boosting algorithm. This ensemble learning technique corrects the errors of previous models by progressively adding new models, each focusing on samples where the previous model's predictions were inaccurate. XGBoost leverages decision trees as a base learner and employs the gradient descent algorithm to minimize the loss function, guiding the model's error correction and updates. Unlike traditional GBDT, XGBoost includes a regularization term in its objective function, helping to control model complexity and prevent overfitting, thereby improving generalization capability. Due to its lightweight nature, hardware devices at the edge, such as smart CT devices, can use XGBoost for NILM prediction. However, for complex decomposition problems, XGBoost alone at the edge is insufficient, necessitating the use of cloud-based NILM models."}, {"title": "b) Cloud-End Oriented Deep Learning Model", "content": "To ensure decomposition accuracy, the Seq2Point model [11] is employed on the cloud side. The Seq2Point model architecture is a variant of Seq2Seq, where the midpoint of the output window is represented as a nonlinear regression of the total power window. In other words, the state of the midpoint is related to the electrical information before and after it. The input to the Seq2Point model is the total power $y_t$, and the output is the predicted state of the midpoint $X_t$ of the output window. The loss function of the neural network is [11]:\n\n$L_p = \\sum_{t=1}^{T-W+1} log p(x_t | Y_{t:t+W-1}, \\theta_p)$    (1)\n\nwhere $\\theta_p$ is the parameter of network $f_p$.\nBased on the Seq2Point model, the input is processed through a sliding window during training and inference, implemented using a queue data structure in actual deployment. After data processing, we develop a neural network architecture that includes convolutional encoders and a Transformer model to predict the state of each household appliance."}, {"title": "IV. IMPLEMENTATION", "content": "Nowadays, most NILM methods are data-driven, and sufficient, high-quality data can greatly improve the efficiency and accuracy of NILM models. Household data is primarily collected through monitoring equipment and system carriers at the edge. The monitoring device can detect current, voltage, active power, reactive power, and other information of the corresponding device through its clamp structure (or a similar method). To meet the data requirements of the NILM model, the monitoring device must achieve a certain sampling rate. These collected data are then sent to the system carrier via a fixed communication protocol. The system carrier is responsible for cleaning and formatting the raw data and sending the processed data to the NILM cloud server through the RabbitMQ queue, where the data is also stored. This process reduces the transmission and storage resource consumption caused by dirty data and decreases the data-processing load on the cloud side. Additionally, the configuration of the edge-end system carrier can be adjusted according to the task. If only data preprocessing is required, lower-cost hardware such as an MCU can be used to reduce costs."}, {"title": "B. NILM Model Development and Deployment", "content": "Many existing deep learning models can be applied to NILM, though some additional data processing may be required for different model architectures. If the decomposition model is based on supervised learning, it is necessary to manually label the switching states of the appliances in the data. There are various ways to obtain appliance labels; for instance, a separate monitoring device can be installed for each appliance to collect labeled data.\nThe general process of leveraging deep learning models for NILM tasks involves model design, training, and evaluation. After developing the deep learning models, the structure and parameters need to be saved, either in the original training format or converted to a standard format (e.g., ONNX, PMML). The standard format allows the deployment environment to be isolated from the training environment. Using the raw format eliminates the need to revalidate the inference but complicates the deployment environment. This deployment challenge is addressed in this paper by using Docker."}, {"title": "C. System Building and Testing", "content": "NILM services can be categorized into two types: online prediction and offline prediction. Online prediction is typically a real-time service, requiring the system to provide an API for client calls, with the REST interface being the most popular. Offline prediction involves setting a fixed task on the server, reading data from the database for prediction, and then storing the results back in the database for subsequent access. When constructing the system, it is essential to determine the service type based on different business requirements and then design the specific system architecture accordingly.\nTo facilitate model deployment, the proposed system is primarily developed in Python and offers online prediction services, as household data are collected in real time."}, {"title": "a) Backend and Interface", "content": "The NILM model needs to provide uWSGI services and REST APIs using a web backend framework. Common frameworks include Django, Flask, and Falcon, with Flask being particularly convenient for encapsulating models as microservices. However, in production environments, frameworks like Flask and FastAPI often do not meet performance requirements on their own. Therefore, components such as Gunicorn and NGINX need to be integrated to provide efficient concurrency, load balancing, and other functionalities."}, {"title": "b) NGINX Server", "content": "Nginx is an asynchronous framework web server that serves as a reverse proxy, load balancer, and HTTP cache. It features low memory usage, quick startup, and strong concurrency."}, {"title": "c) WSGI Server(Gunicorn Server)", "content": "Flask's native server is not suitable for production environments, and using only NGINX as a reverse proxy can lead to unresponsiveness. Therefore, we propose incorporating the Gunicorn server to address this performance issue."}, {"title": "d) Database", "content": "The type of database is determined based on the overall data level of the system, read/write ratio, and other requirements. Commonly used databases include MySQL, Oracle, etc. In addition, Redis can be introduced to improve the caching performance of the system."}, {"title": "V. EXPERIMENTS", "content": "We propose a novel three-tier NILM framework based on edge-cloud collaboration to deliver fast and accurate NILM services for clients. To validate the method's effectiveness in real-world settings, we create a home-like environment through laboratory simulations. This environment includes typical household electrical appliances, and we simulate their operational states based on common household behaviors. The collected electrical data will be used for training the NILM model and testing the edge-cloud collaborative architecture."}, {"title": "a) Datasets", "content": "The datasets used to train and assess the model are gathered in a laboratory environment. The original data, sampled every 2 seconds, is cleaned and relabeled before being used to develop the relevant machine learning models. These datasets include variables such as timestamp, mains voltage, frequency, current transformer (CT) probe readings, active power, mains current, reactive power, apparent power, power factor, and load labels. The datasets feature six types of electrical appliances: air purifier, heater, light bulb, fan, air compressor, and air conditioner.\nIn addition to appliance type, we also consider different appliance levels as decomposition targets. Some appliances are represented by models from various manufacturers. Due to changes in the experimental environment, two distinct datasets are used for training and evaluating the edge-end and cloud-end models. Given the sparsity of heater data in cloud datasets, we consider the two levels as one."}, {"title": "b) Edge and Cloud System Carrier", "content": "In our proposed three-tier NILM system, the monitor CT functions as an edge monitoring device, enabling real-time monitoring of the power consumption of bus and terminal equipment within the electrical system. The edge system is supported by a Raspberry Pi 5, which has a 64-bit quad-core Arm Cortex-A76 processor and 8 GB of RAM. For the cloud system, we use a server equipped with an Intel i7-10875H processor, 16 GB of RAM, and an RTX 2060 GPU."}, {"title": "c) Evaluation Metrics", "content": "We employ Accuracy, Recall, Precision and F1-Score to evaluate the performance of NILM models.\nThe F1-Score for each appliance is calculated by\n\n$F1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$  (2)\n\n$Precision = \\frac{TP}{TP + FP}, Recall = \\frac{TP}{TP+FN}$\n\nThe Accuracy for each appliance is calculated by\n\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$ (3)\n\nwhere $TP$ is the number of true positives, meaning the appliance is in the ON state and correctly identified; $TN$ is the number of true negatives, meaning the appliance is in the OFF state and correctly identified; $FP$ is the number of false positives, meaning the appliance is actually in the OFF state but identified as ON; $FN$ is the number of false negatives, meaning the appliance is actually in the ON state but identified as OFF.\nThe NILM system service performance is evaluated using four metrics: Average Response Time, Median Response Time, 90% Response Time, Max Response Time, and Throughput."}, {"title": "B. Evaluations of NILM Model", "content": "To demonstrate the effectiveness of the NILM model, we use the collected laboratory datasets to compare the performance of edge-end and cloud-end NILM models, specifically XGBoost and Seq2Point. These results indicate a relatively high accuracy (92.6%) yet a low Fl-score (74.1%) on average. This discrepancy may be due to the data sparsity of some appliances, such as different levels of the fan, and the limited expressiveness of the lightweight XGBoost model. In contrast, the decomposition results of the cloud-end model, demonstrate both satisfactory accuracy and F1-score, highlighting the effectiveness of our proposed cloud-end model structure that combines CNN and Transformer. However, the Seq2Point model on the cloud end requires significantly more computational resources than the edge-end model. Therefore, by combining cloud and edge models, we can provide accurate and cost-effective services to clients."}, {"title": "C. Evaluation of Edge-Cloud Collaboration Framework", "content": "To demonstrate the impact of data preprocessing at the edge, we extracted one hour of data daily for a week. The edge server successfully filters out 230 dirty data points from 12,758 total data points, reducing the transmission cost by approximately 1.8%. These data points may have field defaults or negative values due to firmware versions or environmental impacts."}, {"title": "b) Impact of Model Deployment On Response Time", "content": "We deploy the NILM model exclusively on the edge and exclusively on the cloud to compare the response times of the services provided by these two approaches. We conduct concurrent tests on the microservice interface of model inference to verify the effectiveness of this architecture. We test scenarios with 1, 3, 5, 10, 30, 50, and 100 concurrent threads, each with an access interval of 2 seconds, and repeat each test 10 times. The response times for deploying the model solely on the edge are shown , while the response times for deploying the model solely on the cloud are shown. By comparing these tables, we find that deploying the model on the edge can significantly reduce service response time, especially with many concurrent threads. Notably, with 100 concurrent threads, the response time of the edge-deployed model is only 16.7% of the response time of the cloud-deployed model. This is because after the model inference is placed on the edge side, the cloud only needs to call the resulting data."}, {"title": "c) Impact of Load Balancing Module", "content": "To improve the system's concurrent performance and achieve load balancing, we adopt the deployment scheme . We test the interface in the range of 0-100 concurrent threads, each with an access interval of 2 seconds, and repeat each test 10 times. The comparison results of different deployment schemes are shown in Fig. 5, from which we make the following observations:\n1) The average response time of deploying with NGINX and Gunicorn with the Flask framework is much lower than deploying with the Flask framework alone, highlighting the necessity of incorporating the NGINX and Gunicorn module.\n2) When using Gunicorn as a WSGI server, the number of workers affects performance; more workers generally result in shorter response times. For example, the response time is shortest when there are 4 workers. Therefore, in a real deployment, the number of workers should be chosen based on the server configuration.\nIt should be noted that all our previous experiments were conducted with a response error rate of 0. However, the NILM service interface does experience response errors when the number of concurrent threads becomes too high. the NGINX and Gunicorn deployment with the Flask framework only begins to fail after 400 concurrent threads. Overall, this proposed deployment scheme significantly increases the threshold and enhances the reliability of the service."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a three-tier framework as well as collaborative edge-cloud schemes to address the challenges in applying NILM in real-world deployment. Our solution reduces service response time, alleviates cloud workload, and enhances data security. Unlike previous works that focus primarily on the algorithm level, neglecting real-world deployment details, we systematically design the NILM serv-ing architecture to achieve load balancing and low-latency service. We implement XGBoost and Seq2Point models for load decomposition on the edge and the cloud, respectively. Experimental results demonstrate that edge deployment can reduce service response time and communication overhead, while cloud deployment offers more accurate load decomposition and facilitates management. Furthermore, by integrating Gunicorn and NGINX, we achieve load balancing and address cloud-side response errors during high concurrency. By paving the way toward real-world NILM deployment, this work provides a foundational framework for future in-depth NILM research, allowing for the exploration of personalized solutions for different households and the iterative update of models."}]}