{"title": "Balancing Immediate Revenue and Future Off-Policy Evaluation in Coupon Allocation", "authors": ["Naoki Nishimura", "Ken Kobayashi", "Kazuhide Nakata"], "abstract": "Coupon allocation drives customer purchases and boosts revenue. However, it presents a fundamental trade-off between exploiting the current optimal policy to maximize immediate revenue and exploring alternative policies to collect data for future policy improvement via off-policy evaluation (OPE). While online A/B testing can validate new policies, it risks compromising short-term revenue. Conversely, relying solely on an exploitative policy hinders the ability to reliably estimate and enhance future policies. To balance this trade-off, we propose a novel approach that combines a model-based revenue maximization policy and a randomized exploration policy for data collection. Our framework enables flexibly adjusting the mixture ratio between these two policies to optimize the balance between short-term revenue and future policy improvement. We formulate the problem of determining the optimal mixture ratio between a model-based revenue maximization policy and a randomized exploration policy for data collection. We empirically verified the effectiveness of the proposed mixed policy using both synthetic and real-world data. Our main contributions are: (1) Demonstrating a mixed policy combining deterministic and probabilistic policies, flexibly adjusting the data collection vs. revenue trade-off. (2) Formulating the optimal mixture ratio problem as multi-objective optimization, enabling quantitative evaluation of this trade-off. By optimizing the mixture ratio, businesses can maximize revenue while ensuring reliable future OPE and policy improvement. This framework is applicable in any context where the exploration-exploitation trade-off is relevant.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background", "content": "Marketing campaigns like discount coupon promotions are crucial for web service providers to encourage customer purchases and drive revenue growth [11,13,14]."}, {"title": "1.2 Related Work", "content": "OPE methods such as inverse probability weighting (IPW) [12], doubly robust [2], and their variants [3,6] use importance sampling. These methods assume the data collection policy satisfies the common support assumption, assigning positive probability to all actions the evaluation policy may take [2,12]. However, real-world policies often have incomplete support, using deterministic bandits or selecting the top-predicted action until the budget is exhausted [8,9]. In such cases, the logged data lacks information about rewards for actions not taken, hindering accurate OPE.\nPrior approaches treat multiple data collection policies as a single policy with complete support [12], predict rewards from actions and contexts [2], or use local subsamples near decision boundaries [10]. However, these methods do not control the data collection process for improving future OPE.\nKiyohara et al. [7] proposed deploying a mixture of policies online on the basis of a risk-return metric. Unlike their work, our approach focuses on improving OPE accuracy for future evaluation policies.\nIn contrast, we propose optimizing the mixture ratio of model-based deterministic policies and random exploration policies during data collection. This balances maximizing expected revenue with enabling accurate OPE for future policy improvement."}, {"title": "1.3 Our Results", "content": "The main contributions of this research are twofold:"}, {"title": "2 Problem Definition and Approach", "content": "Let $x \\in X$ be the feature vector of users, $a \\in A$ the action pattern of coupon allocation, and $r \\in R$ the observed reward when action $a$ is taken for user $x$, where $X$ and $A$ denote the feature and action spaces, respectively.\nWe introduce the decision policy $\\pi : X \\rightarrow \\Delta(A)$ as a conditional probability distribution over the action space $A$, with $\\Delta(A)$ being the probability simplex over $A$. Let the past data collection policies be $\\pi_i (i = 1, ..., K)$ and the new evaluation policy be $\\pi_e$. The dataset collected by the data collection policy $\\pi_i$ is denoted as $D_i = \\{(x_j^{(i)}, a_j^{(i)}, r_j^{(i)})\\}_{j=1}^{N_i}$, where $n_i$ is the sample size of the dataset $D_i$."}, {"title": "2.1 Existing Approaches", "content": "The naive policy evaluation estimator $V_{\\text{naive}}(\\pi_e)$ [12] based on the dataset collected by the data collection policy $\\pi_i$ is expressed as follows:\n$V_{\\text{naive}}(\\pi_e) = \\frac{1}{N_i} \\sum_{j=1}^{N_i} r_j^{(i)} \\mathbb{1}\\{h_e(x_j^{(i)}) = a_j^{(i)}\\}$ (1)\nwhere $h_e(x) : X \\rightarrow A$ is the action determined by the evaluation policy $\\pi_e$.\nThe naive estimator averages the objective variables by limiting them to data where the decisions of the data collection and evaluation policies match, so it"}, {"title": "2.2 Mixed Data Collection Policy Application", "content": "Figure 1 illustrates the application of mixed data collection policies in coupon allocation. We consider two policies: a random allocation policy $\\pi_1$ and a model-based allocation policy $\\pi_2$, each applied to users with a 0.5 ratio. The left side shows users under the random policy $\\pi_1$, where the decision to provide a coupon $a_1$ is made with probability $p$, regardless of the model prediction score $\\phi(x)$. The right side shows users under the model-based policy $\\pi_2$, where users with $\\phi(x) \\geq z$ receive a coupon with probability 1, and those with $\\phi(x) < z$ receive none.\nThe probability of selecting $a_1$ under the average policy $\\pi_{1:2}$, with equal mixture ratios $\\alpha_1 = \\alpha_2 = 0.5$, is:\n$\\pi(a_1 | \\pi_{1:2}(a | x)) = \\begin{cases} 0.5(p + 1) & (\\phi(x) \\geq z) \\\\ 0.5p & (\\phi(x) < z) \\end{cases}$ (5)\nWhen using the BIPS estimator (3), the weights for evaluating $a_1$ are smaller when $\\phi(x) \\geq z$ due to higher frequency in the average policy, and larger when"}, {"title": "2.3 Multi-Objective Optimization for Mixture Ratio", "content": "This section formulates the problem of determining the mixture ratio of data collection policies $\\alpha = (\\alpha_1, ..., \\alpha_K)$ that balances the trade-off between revenue and OPE performance.\nIn this study, we consider the revenue metric $f^r(\\alpha)$ as the weighted average of the past achieved values of each policy used for data collection, where the weights are based on the composition ratio of each policy:\n$f^r(\\alpha) = V_{\\text{naive}}(\\pi_{1:K}) = V_{\\text{naive}} (\\sum_{i=1}^{K} \\alpha_i \\pi_i(a | x)) = \\sum_{i=1}^{K} \\alpha_i V_{\\text{naive}}(\\pi_i).$ (6)\nThe OPE performance metric $f^e(\\alpha)$ can utilize error metrics such as the mean squared error (MSE) between the ground-truth policy evaluation value $V(\\pi_e)$ and the BIPS estimator $V_{\\text{BIPS}}(\\pi_e)$ in situations where information about the"}, {"title": "3 Numerical Experiments", "content": "This section verifies the proposed policy mixing strategy model using synthetic and real-world data. First, we comprehensively evaluate the case where the number of policies to be mixed is three, using synthetic data where the true policy performance is known. Next, we present a practical application example of the proposed model using coupon allocation data collected by multiple policies in the real world. The code for the synthetic data experiments is publicly available online\u00b9."}, {"title": "3.1 Synthetic Experiments", "content": "In this section, we conducted numerical experiments using an artificially generated dataset to verify the effectiveness of the proposed policy. The purpose of this experiment is to generate data that mimic user features and evaluate revenue performance under different allocation policies."}, {"title": "Data Generation Process", "content": "We generated a synthetic dataset with $J = 10,000$ users. For each user $j \\in J$, we sampled a 4-dimensional feature vector $x_j = (x_{j,1}, x_{j,2}, x_{j,3}, x_{j,4})$ where each $x_{j,d} \\sim \\text{Uniform}(0, 1)$. The action space for coupon allocation was $a \\in \\{0, 1\\}$, with $a_j = 1$ indicating a coupon was"}, {"title": "Data Collection and Evaluation Policies", "content": "We designed three data collection policies:\n$\\pi_1$: a random allocation policy using $x_{j,1}$ as the allocation probability\n$\\pi_2$: a deterministic policy allocating when $x_{j,2} \\geq 0.5$\n$\\pi_3$: a deterministic policy allocating when $x_{j,3} \\geq 0.5$\nThe mixture ratios of $\\pi_1, \\pi_2, \\pi_3$ are denoted as $\\alpha_1, \\alpha_2, \\alpha_3$, respectively. Additionally, we prepared two evaluation policies:\n$\\pi_{e,1}$: a probabilistic policy positively correlated with $\\pi_2$, allocating with probability 0.8 when $x_{j,2} \\geq 0.5$ and 0.2 when $x_{j,2} < 0.5$\n$\\pi_{e,2}$: a probabilistic policy negatively correlated with $\\pi_2$, allocating with probability 0.2 when $x_{j,2} \\geq 0.5$ and 0.8 when $x_{j,2} < 0.5$"}, {"title": "Performance Metrics", "content": "The revenue metric $f^r(\\alpha)$ is the true expected revenue under the mixed policy $\\pi_{1:K}$ defined as:\n$f^r(\\alpha) = V(\\pi_{1:K}) = \\frac{1}{n} \\sum_{i=1}^{K} \\sum_{j=1}^{N_i} r_j(x_j^{(i)}, h_i(x_j^{(i)}))$, (12)\nwhere $n$ is the total sample size, $n_i$ is the sample size for policy $\\pi_i$, $r_j(x_j^{(i)}, h_i(x_j^{(i)}))$ is the realized revenue when action $h_i(x_j^{(i)})$ determined by $\\pi_i$ is taken for user features $x_j^{(i)}$. The OPE performance metric $f^e(\\alpha)$ is the MSE between the true policy value $V(\\pi_e)$ and the BIPS estimate $V_{\\text{BIPS}}(\\pi_e)$:\n$f^e(\\alpha) = (V(\\pi_e) - V_{\\text{BIPS}}(\\pi_e))^2$. (13)"}, {"title": "Optimization for Mixture Ratios", "content": "We used the black-box optimization tool Optuna v3.6.0 [1] and used NSGA-II, the default algorithm in Optuna, for multi-objective optimization calculations. In each experiment, the number of trials in the optimization loop was set to 1,000."}, {"title": "3.2 Real-world Experiments", "content": "This section evaluates the proposed method using real-world coupon allocation log data from HOTPEPPER GOURMET, a restaurant portal site in Japan, for a specific month. We also illustrate decision-making on the basis of the results."}, {"title": "Data Description", "content": "Log data was collected using two data collection policies: a random and a model-based allocation policy leveraging machine learning. The log data $D$ records:\nData collection policy $\\pi_i$ used\nUser features $x_j^{(i)}$ for each user $j \\in I$\nPolicy score $\\phi(x_j^{(i)})$ predicted for the user\nCoupon allocation decision $a_j^{(i)}$\nResulting revenue $r_j^{(i)}$\nThis can be expressed as $D = \\{\\pi_i, x_j^{(i)}, \\phi(x_j^{(i)}), a_j^{(i)}, r_j^{(i)}\\}_{j=1}^J$. The random policy allocates coupons with a fixed probability, regardless of the score $\\phi(x_j^{(i)})$. The"}, {"title": "Performance Metrics", "content": "Similar to synthetic experiments, the revenue metric is the average value of the true expected revenue calculated by (12) over resampled datasets. The policy evaluation performance metric uses the variance of the BIPS estimator (3) over resampled datasets, as the true counterfactual revenue is unknown. Experiments show the variance dominates over bias."}, {"title": "4 Conclusion", "content": "This research proposes a novel policy mixture ratio optimization method balancing revenue maximization and reliable OPE. A key aspect is demonstrating the effectiveness of a mixed policy combining model-based deterministic allocation and randomized exploration for coupon allocation. This enables flexible adjustment of the trade-off between improving data collection efficiency for future OPEs and maximizing near-term revenue. Another contribution is formulating the optimal mixture ratio \u03b1 that balances this trade-off as a multi-objective optimization problem, enabling quantitative evaluation and control over data collection efficiency and revenue acquisition balance.\nExperiments on synthetic data demonstrated a Pareto frontier between revenue and OPE metrics based on \u03b1. Using an evaluation policy closer to the data collection policy improved the OPE for the same revenue level. Real-world coupon allocation logs showed mixed policies achieving better trade-offs than single policies. Revenue change exhibited a non-linear relationship with \u03b1, suggesting prioritization of high-impact users.\nBy optimizing \u03b1, businesses can maximize expected revenue while ensuring reliable future OPE and policy improvement. To maximize revenue given a target, the mixture ratio maximizing OPE among Pareto optimal points satisfying the target can be selected. This framework is applicable in any context where the exploration-exploitation trade-off is relevant. Future work includes incorporating advanced OPE methods and unified metrics considering multiple indicators."}, {"title": "Disclosure of Interests.", "content": "The authors have no competing interests to declare that are relevant to the content of this article."}]}