{"title": "Balancing Immediate Revenue and Future Off-Policy Evaluation in Coupon Allocation", "authors": ["Naoki Nishimura", "Ken Kobayashi", "Kazuhide Nakata"], "abstract": "Coupon allocation drives customer purchases and boosts revenue. However, it presents a fundamental trade-off between exploiting the current optimal policy to maximize immediate revenue and exploring alternative policies to collect data for future policy improvement via off-policy evaluation (OPE). While online A/B testing can validate new policies, it risks compromising short-term revenue. Conversely, relying solely on an exploitative policy hinders the ability to reliably estimate and enhance future policies. To balance this trade-off, we propose a novel approach that combines a model-based revenue maximization policy and a randomized exploration policy for data collection. Our framework enables flexibly adjusting the mixture ratio between these two policies to optimize the balance between short-term revenue and future policy improvement. We formulate the problem of determining the optimal mixture ratio between a model-based revenue maximization policy and a randomized exploration policy for data collection. We empirically verified the effectiveness of the proposed mixed policy using both synthetic and real-world data. Our main contributions are: (1) Demonstrating a mixed policy combining deterministic and probabilistic policies, flexibly adjusting the data collection vs. revenue trade-off. (2) Formulating the optimal mixture ratio problem as multi-objective optimization, enabling quantitative evaluation of this trade-off. By optimizing the mixture ratio, businesses can maximize revenue while ensuring reliable future OPE and policy improvement. This framework is applicable in any context where the exploration-exploitation trade-off is relevant.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background", "content": "Marketing campaigns like discount coupon promotions are crucial for web service providers to encourage customer purchases and drive revenue growth [11,13,14]."}, {"title": "1.2 Related Work", "content": "OPE methods such as inverse probability weighting (IPW) [12], doubly ro- bust [2], and their variants [3,6] use importance sampling. These methods assume the data collection policy satisfies the common support assumption, assigning positive probability to all actions the evaluation policy may take [2,12]. However, real-world policies often have incomplete support, using deterministic bandits or selecting the top-predicted action until the budget is exhausted [8,9]. In such cases, the logged data lacks information about rewards for actions not taken, hindering accurate OPE.\nPrior approaches treat multiple data collection policies as a single policy with complete support [12], predict rewards from actions and contexts [2], or use local subsamples near decision boundaries [10]. However, these methods do not control the data collection process for improving future OPE.\nKiyohara et al. [7] proposed deploying a mixture of policies online on the basis of a risk-return metric. Unlike their work, our approach focuses on improving OPE accuracy for future evaluation policies.\nIn contrast, we propose optimizing the mixture ratio of model-based deter- ministic policies and random exploration policies during data collection. This balances maximizing expected revenue with enabling accurate OPE for future policy improvement."}, {"title": "1.3 Our Results", "content": "The main contributions of this research are twofold:"}, {"title": "2 Problem Definition and Approach", "content": "Let x \u2208 X be the feature vector of users, a \u2208 A the action pattern of coupon allocation, andr\u2208 R the observed reward when action a is taken for user x, where X and A denote the feature and action spaces, respectively.\nWe introduce the decision policy \u03c0 : X \u2192 A(A) as a conditional probability distribution over the action space A, with \u2206(A) being the probability simplex over A. Let the past data collection policies be \u03c0i(i = 1,..., K) and the new evaluation policy be re. The dataset collected by the data collection policy \u03c0\u2081 is denoted as D\u2081 = {(x), a, r()) } 1, where ni is the sample size of the dataset Di."}, {"title": "2.1 Existing Approaches", "content": "The naive policy evaluation estimator Vnaive(e) [12] based on the dataset col- lected by the data collection policy \u03c0i is expressed as follows:\n$$V_{\\text{naive}} (\\pi_e) = \\frac{1}{N_i} \\sum_{j=1}^{N_i} \\mathbb{1}\\{h_e(x_j^{(i)}) = a_j^{(i)}\\} r_j^{(i)},$$\nwhere he(x): X \u2192 A is the action determined by the evaluation policy \u043f\u0435.\nThe naive estimator averages the objective variables by limiting them to data where the decisions of the data collection and evaluation policies match, so it"}, {"title": "2.2 Mixed Data Collection Policy Application", "content": "Figure 1 illustrates the application of mixed data collection policies in coupon allocation. We consider two policies: a random allocation policy \u03c0\u2081 and a model- based allocation policy \u03c0\u2082, each applied to users with a 0.5 ratio. The left side shows users under the random policy \u03c0\u2081, where the decision to provide a coupon a1 is made with probability p, regardless of the model prediction score (x). The right side shows users under the model-based policy \u03c02, where users with (x) \u2265 z receive a coupon with probability 1, and those with f(x) < z receive none.\nThe probability of selecting a\u2081 under the average policy 1:2, with equal mixture ratios a\u2081 = a2 = 0.5, is:\n$$P(a_1 | \\pi_{1:2}(a) | \\phi(x)) = \\begin{cases} 0.5(p + 1) & \\text{if } \\phi(x) \\geq z \\\\ 0.5p & \\text{if } \\phi(x) < z \\end{cases}$$\nWhen using the BIPS estimator (3), the weights for evaluating a\u2081 are smaller when (x) \u2265 z due to higher frequency in the average policy, and larger when"}, {"title": "2.3 Multi-Objective Optimization for Mixture Ratio", "content": "This section formulates the problem of determining the mixture ratio of data collection policies a = (a1, ..., \u03b1\u03ba) that balances the trade-off between revenue and OPE performance.\nIn this study, we consider the revenue metric fr (a) as the weighted average of the past achieved values of each policy used for data collection, where the weights are based on the composition ratio of each policy:\n$$f^r (\\alpha) = V_{\\text{naive}} (\\pi_{1:K}) = \\sum_{i=1}^K \\alpha_i \\mathbb{E} [r_j^{(i)}] = \\sum_{i=1}^K \\alpha_i V_{\\text{naive}} (\\pi_i).$$\nThe OPE performance metric fe(a) can utilize error metrics such as the mean squared error (MSE) between the ground-truth policy evaluation value V(\u03c0\u03b5) and the BIPS estimator VBIPS (\u03c0e) in situations where information about the"}, {"title": "3 Numerical Experiments", "content": "This section verifies the proposed policy mixing strategy model using synthetic and real-world data. First, we comprehensively evaluate the case where the num- ber of policies to be mixed is three, using synthetic data where the true policy performance is known. Next, we present a practical application example of the proposed model using coupon allocation data collected by multiple policies in the real world. The code for the synthetic data experiments is publicly available online\u00b9."}, {"title": "3.1 Synthetic Experiments", "content": "In this section, we conducted numerical experiments using an artificially gen- erated dataset to verify the effectiveness of the proposed policy. The purpose of this experiment is to generate data that mimic user features and evaluate revenue performance under different allocation policies.\n= Data Generation Process We generated a synthetic dataset with J = 10,000 users. For each user j\u2208 J, we sampled a 4-dimensional feature vec- tor xj = (xj,1,Xj,2, Xj,3, xj,4) where each xj,d ~ Uniform(0, 1). The action space for coupon allocation was a \u2208 {0,1}, with aj = 1 indicating a coupon was"}, {"title": "3.2 Real-world Experiments", "content": "This section evaluates the proposed method using real-world coupon allocation log data from HOTPEPPER GOURMET, a restaurant portal site in Japan, for a specific month. We also illustrate decision-making on the basis of the results.\nData Description Log data was collected using two data collection policies: a random and a model-based allocation policy leveraging machine learning. The log data D records:\nData collection policy \u03c0\u2081 used\nUser features x) for each user j\u2208 I\nPolicy score \u03a6(x)) ) predicted for the user\nCoupon allocation decision a)\n(i)\nResulting revenue r\nThis can be expressed as D = {\u03c0\u03ad, \u03b5), \u03c6(x)), \u03b1\u03ad), r() }1. The random policy allocates coupons with a fixed probability, regardless of the score \u03a6(x)). ). The"}, {"title": "4 Conclusion", "content": "This research proposes a novel policy mixture ratio optimization method balanc- ing revenue maximization and reliable OPE. A key aspect is demonstrating the effectiveness of a mixed policy combining model-based deterministic allocation and randomized exploration for coupon allocation. This enables flexible adjust- ment of the trade-off between improving data collection efficiency for future OPEs and maximizing near-term revenue. Another contribution is formulating the optimal mixture ratio a that balances this trade-off as a multi-objective optimization problem, enabling quantitative evaluation and control over data collection efficiency and revenue acquisition balance.\nExperiments on synthetic data demonstrated a Pareto frontier between rev- enue and OPE metrics based on a. Using an evaluation policy closer to the data collection policy improved the OPE for the same revenue level. Real-world coupon allocation logs showed mixed policies achieving better trade-offs than single policies. Revenue change exhibited a non-linear relationship with a, sug- gesting prioritization of high-impact users.\nBy optimizing a, businesses can maximize expected revenue while ensuring reliable future OPE and policy improvement. To maximize revenue given a tar- get, the mixture ratio maximizing OPE among Pareto optimal points satisfying the target can be selected. This framework is applicable in any context where the exploration-exploitation trade-off is relevant. Future work includes incorporating advanced OPE methods and unified metrics considering multiple indicators."}]}