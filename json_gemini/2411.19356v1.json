{"title": "MAPPING PUBLIC PERCEPTION OF ARTIFICIAL INTELLIGENCE: EXPECTATIONS, RISK-BENEFIT TRADEOFFS, AND VALUE AS DETERMINANTS FOR SOCIETAL ACCEPTANCE", "authors": ["Philipp Brauner", "Felix Glawe", "Luisa Vervier", "Gian Luca Liehner", "Martina Ziefle"], "abstract": "Understanding public perception of artificial intelligence (AI) and the tradeoffs between potential risks and benefits is crucial, as these perceptions might shape policy decisions, influence innovation trajectories for successful market strategies, and determine individual and societal acceptance of AI technologies. Using a representative sample of 1100 participants from Germany, this study examines mental models of AI. Participants quantitatively evaluated 71 statements about AI's future capabilities (e.g., autonomous driving, medical care, art, politics, warfare, and societal divides), assessing the expected likelihood of occurrence, perceived risks, benefits, and overall value. We present rankings of these projections alongside visual mappings illustrating public risk-benefit tradeoffs. While many scenarios were deemed likely, participants often associated them with high risks, limited benefits, and low overall value. Across all scenarios, 96.4% (r2 = 96.4%) of the variance in value assessment can be explained by perceived risks (\u03b2 = \u2212.504) and perceived benefits (\u03b2 = +.710), with no significant relation to expected likelihood. Demographics and personality traits influenced perceptions of risks, benefits, and overall evaluations, underscoring the importance of increasing AI literacy and tailoring public information to diverse user needs. These findings provide actionable insights for researchers, developers, and policymakers by highlighting critical public concerns and individual factors essential to align AI development with individual values.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in Artificial Intelligence (AI) and Deep Learning (DL) in general and large language models (LLMs) in particular, has ignited widespread interest and concern across multiple domains. These technologies become increasingly integrated into almost all sectors ranging from education and healthcare to journalism, forestry and farming as well as production and manufacturing. They offer benefits in terms of efficiency, convenience, and innovation, but also pose significant risks in terms of privacy infringement, job displacement, and ethical dilemmas for individuals, organizations, and society as a whole.\nAlthough AI's origins stretch back several decades its development has accelerated significantly in recent years, driven by advancements in computing power, greater availability of digital data enhanced algorithms, and a considerable surge in funding. The expectations surrounding AI appear to be split: while some researchers and consumers regard AI as a transformative tool that will enhance our lives others voice concerns about its ethical implications and associated risks.\nFor decades, it has been recognized that computers and algorithms are not value-neutral but inherently embody values and potential biases . Through the currently has a unique yet ambiguous position within the human cognitive and social landscape prompting a critical evaluation of how people perceive AI and the broader societal implications they associate with its adoption. This underscores the need for critical scrutiny in the design and deployment of technology, as the embedded values can influence decisions and outcomes in ways that may perpetuate inequality or reinforce existing biases. Further, the deployment and successful use of technologies, such as AI, can be accelerated by higher acceptance or hindered by perceived obstacles.\nThere is a shared concern about the ethical and societal impacts of AI, with a need for careful design and forward-looking research policies to avoid setbacks. Hence, understanding public perception of artificial intelligence, particularly how people balance its perceived benefits and risks, is essential as these views shape policy decisions, influence innovation trajectories, and determine the individual and societal acceptance of AI technologies. This article examines how the public evaluates AI's potential impact and future capabilities, exploring the tradeoffs they consider between AI's utilities and associated risks. The results are analysed both at the level of individual differences, meaning how personality factors influence the perception of AI, as well as at the technological level, meaning how the risk and benefit perception of the different projections shape the overall evaluation of AI. We further illustrate the risk and benefit tradeoff by placing the evaluations on visual maps. Overall, this article helps to identify topics where risk and benefit expectations are aligned, as well as topics with greater differences and potential for conflict in terms of public perception and acceptance.\nThe structure of the article is as follows: Section 2 gives an overview on the public perception of AI as well as an overview on technology and risk perception. Section 3 presents the concept of using micro scenarios as the basis for our empirical approach, the survey, and the sample. In Section 4 we present the results of the study starting with the overall perception of the various AI-related statements and their visual mapping, followed by an analysis of the influence of the individual differences. Section 5 discusses the findings and their implications, as well as the limitations of the study. Lastly, Section 6 suggests policy implications and future research."}, {"title": "2 Related Work", "content": "We first provide a brief overview on risk perception and the psychometric model for measuring subjective risk. Thenm we then give an overview on studies on the public opinion and perception of AI."}, {"title": "2.1 Risk Perception and the Psychometric Model", "content": "Across many domains, individual risk and benefit perceptions influence attitudes, usage intentions, or actual behaviors. Risk can be conceptualised from two perspectives: On one hand, risk can be modeled as the expected utilities of negative events, their potential consequences, and related to individual responses. However, a key challenge is to accurately model the probabilities and consequences, making it difficult to calculate expected utilities and link them to individual responses. On the other hand, risk can be viewed as events and their perceived consequences. According to this psychometric model of risk perception, an individual's perceived threat from these consequences can be measured, for example, by means of rating scales. For example, it was used to explore the role of risk perception in areas such as gene technology , genetically modified food, nuclear energy general climate change, and Carbon Capture and Utilization technologies. It represents a framework for understanding how individuals perceive and balance risks and benefits and link these weightings with related dispositions and properties of the technologies. Therefore, it is particularly useful for studying perceptions of emerging technologies or technologies whose implications are not easily understood by laypeople. This is even more important in the early phases of technology development, to assess potential social and societal consequences through the lens of laypeople, even though they might have a limited and fragmented understanding of the impacts of AI and the resulting changes in society, the job market, or other areas. Independently from the specific context, a commonly observed pattern is an inverse relationship between perceived risks and perceived benefits: When a technology is perceived as risky, its benefits are often viewed as less significant, whereas technologies perceived as safer tend to be associated with higher perceived benefits, and vice versa."}, {"title": "2.2 Perception of Artificial Intelligence", "content": "Studies reveal a complex landscape of attitudes and perceptions towards AI that vary by time, context, and individual. But as AI spans numerous tasks and domains, it is challenging to provide a comprehensive overview of the research on public perceptions of AI, particularly in relation to the associated risk-benefit tradeoffs. Especially the rapid and unprecedented adoption of ChatGPT following its public release in 2022 spurred academic interest on perception of AI and much of this research remains to be consolidated. Henceforth, the following section offers a brief and necessarily incomplete overview of this rapidly evolving field."}, {"title": "2.2.1 General AI Perception", "content": "conducted a media analysis of AI coverage in the New York Times spanning three decades. They found a growing public interest in AI after 2009, marked by both optimism and concern, with AI generally receiving more positive than negative coverage. Yet, recent years have seen rising concerns about control loss and ethical dilemmas, contrasting with optimism, particularly regarding AI's potential for healthcare. People often hold inflated expectations about AI's potential, driven in part by optimistic portrayals in news and entertainment media.\nRecently, examined how news outlets convey \u201cAI anxiety\u201d by depicting AI as an autonomous, opaque entity independent of human control. They derived an AI anxiety index and analyzed headlines across major newspapers before and after ChatGPT's launch. Their findings indicate that ChatGPT's introduction not only increased AI-related coverage but also intensified negative sentiments, with regional media driving the heightened AI anxiety index.\nResearch indicates that public awareness of AI technologies is still generally limited, with many people struggling to differentiate between specific forms like machine learning, robotics, and automation. A survey by Ipsos found that the general population often lacks a nuanced understanding of AI's technical achievements and limitations. Pew Research found that only a fraction of Americans could correctly identify AI in everyday scenarios, highlighting a general lack of clarity about AI's scope and capabilities. This limited awareness contributes to misconceptions and oversimplified views of AI's impact and applications, potentially impeding informed public discourse on AI's ethical and societal implications. The Alan Turing Institute also highlighted that public understanding varies significantly depending on education level and context, with frequent concerns about automation and robotics in particular, such as in employment and security applications.\nPublic discourse often includes both, unfocused and generic fears on the one hand as well as high expectations about AI on the other, particularly around the concept of artificial general intelligence (AGI), which still remains largely fictional. Cave, Coughlan, and Dihal investigated prevalent narratives about AI using a sample from the UK, identifying eight primary themes-four optimistic and four pessimistic. Their findings suggest that perceptions of Al's impact are often tinged with anxiety, with only two of the narratives viewing benefits as outweighing concerns (such as the idea that AI could make life easier). Additionally, participants expressed a sense of powerlessness over AI development, viewing it as largely driven by government and corporate interests. About half of the respondents were able to provide plausible definitions of AI, while 25% associated AI primarily with robots.\nFurther research indicates that public concern over Al's ethical use is on the rise, particularly as awareness of biased algorithms and discriminatory outcomes has grown . Scholars argue that transparency, accountability, and fairness are key factors in building public trust in AI systems.\nIn an earlier study, we examined the expectations (is the AI projection likely or not likely to occur) and sentiment (is the use of AI negative or positive) of laypersons towards AI-related scenarios by utilizing a younger convenience sample. The findings suggest greater disparities in the perceptions of these subjects: Of particular concern was the expectation of cyber security threats, a factor deemed both highly likely and least favorable by the participants."}, {"title": "2.2.2 Context dependency of AI perception", "content": "Context-wise, studies on the (public) perception of domain-specific uses of AI are relatively common. Perceptions of Al risks and benefits vary across different domains such as healthcare, education, and creative arts, with healthcare often seen as more beneficial . Alessandro et al. suggest that AI's perceived social risk and value are inversely related; higher perceived risks lead to lower attributed social value. In this study, medical AI applications were perceived as particularly risky. Gao et al. studied the perception of AI in medical care in China through content analysis of social media posts. Key concerns associated with a negative evaluation were the immaturity of the technology and distrust in the related companies. Further, in the majority of the posts replacing human doctors being replaced with AI was expected.\nWith an experimental approach Liehner et al. studied the willingness to delegate morally sensitive tasks to automated AI-agents and found that context and reliability (i.e., risk of an error) of the automation shapes the perception of and trust in AI. A meta-analysis on risk perception of narrow AI (AI for specific tasks) found that key influences that mitigate risk perception are familiarity, trust, whereas privacy concerns exacerbate the perceived risks of a technology. Araujo et al. explored how individual differences related to the perceptions of automated decision-making by AI and how AI perception differs by context (media, (public) health, and judicial). People were concerned about risks and had mixed opinions about fairness and usefulness of automated decision-making at a societal level, although AI-based decisions were evaluated on par or even better than human experts for specific decisions. In the study, Al knowledge had a positive influence on perceived benefits and fairness of AI, whereas privacy concerns were linked to the perceived risks."}, {"title": "2.2.3 AI Perception and Individual Differences", "content": "Individual differences, such as demographics (gender, age) but also experience with and knowledge of AI influence the perception of AI . For instance, people with higher technological competence and AI familiarity tend to trust AI more. Kaya et al. investigated general attitudes towards artificial intelligence and the influence of personality traits with a Turkish sample. Again, computer use and knowledge about AI had a positive influence on attitude towards AI. Agreeableness, AI learning anxiety, and AI configuration anxiety had a negative influence on the attitudes towards AI.\nWinter, Dodou, and Eisma studied the relationship between personality factors, performance expectations, and intention to use and actual use of ChatGPT. Perceived effectiveness and concerns correlated with ChatGPT use frequency. Further, intention to use was linked to the personality trait Machiavellianism (i.e., use of manipulation tactics).\nKelley et al. conducted a study on public opinion regarding AI, surveying over 10,000 participants from the eight countries Australia, Canada, the USA, South Korea, France, Brazil, India, and Nigeria. The study examined the anticipated societal impact of AI and participants' attitudes toward it, using four key descriptors: exciting, useful, worrying, and futuristic. In developed countries, such as the USA, Canada, and Australia, respondents predominantly expressed concerns about AI, coupled with futuristic expectations. In contrast, developing countries like India, Brazil, and Nigeria exhibited a greater sense of excitement about Al's potential. South Korea stood out for its focus on AI's usefulness and future applications, reflecting its advanced technological landscape. Across all regions, there was a broad consensus that AI would have a significant societal impact, though the exact implications were still uncertain.\nStudying the perceptions of participants from China on a few selected AI-based technologies Cui and Wu found an overall positive attitude towards AI and that the perceived benefits of AI outweigh the perceived risks. Notably, media usage was solely correlated with positive perceptions of Al's advantages, potentially influenced by government-controlled media that presents AI in a favourable manner. Further, individuals with a high level of personal relevance exhibited reduced susceptibility to media influence, fostering a more critical stance towards AI.\nA comparative media analysis explored the similarities and differences in coverage of the historic chess match between Lee Sedol and AlphaGo . The analysis examined 27 Chinese and 30 American newspaper articles. Chinese media more frequently portrayed AlphaGo as non-threatening compared to American media, highlighting cultural differences in attitudes toward AI .\n\u201cIn summary, despite the growing body of literature on AI perception and use, significant gaps remain. One critical area of focus is the perceptions of the general public, as public acceptance and deliberate usage are essential for the successful development and deployment of human-centered AI. Given that AI is a relatively new technology for most individuals-many of whom lack substantial understanding and experience with its applications in daily life-their views on its perceived benefits and drawbacks are important. Understanding these perceptions can inform researchers, technical designers, policymakers, and educational strategists, providing insights into the areas that require targeted"}, {"title": "3 Method", "content": "The goal of this study was to explore public perceptions of AI, specifically regarding its risks, benefits, and overall evaluation. It also examined how individuals weigh the trade-off between risks and benefits, and whether personal characteristics influence these perceptions."}, {"title": "3.1 Risk-Benefit Tradeoff using Micro Scenarios", "content": "To achieve this, we built on Slovic's psychometric model , meaning we assess perceived risks and benefits by quantifying people's subjective judgments associated with the technology. A common approach to study technology perception is to let participants evaluate a specific or a few selected scenarios using a battery of scales. While this yields a detailed evaluation of a specific topic, it does not fit to AI with its many potential applications and implications for individuals, organisations, and society. Hence, we asked subjects to evaluate a large range of topics with potential capabilities and impacts that AI could have in the next decade using micro scenarios , that is, the subjects assessed brief statements such as \u201cAI raises living standards\" on a short set of single-item scales.\nThis approach offers two distinct but complementing perspectives: 1) For each participant, the average evaluations across many topics can be considered as a reflexive measurement of an underlying latent construct and thus be interpreted as user factors or individual differences. This facilitates the analysis of how participants differ in regard to the evaluations and what other personality states and traits shape the assessment. 2) For each topic, the average evaluations of the participants can be considered as a topic factor. The scores for each topic can be placed on visual maps and analysed for outliers, relationships, and patterns.\nFor creating the list of topics and statements, we drew on existing research and expert workshops. Through multiple rounds of refinement, we optimized the selection, eliminated redundancies, and improved the statements for clarity and conciseness. The list of topics encompassed a range of 71 statements, from more obvious ones to more speculative ones, such as AI creating jobs, fostering innovation, operating according to moral principles, and perceiving humans as a threat. We let each subject assess a randomized random subset of 15 out of the 71 topics."}, {"title": "3.2 Demographics and Exploratory Personality Traits", "content": "In addition to the micro scenarios, we collected demographic data from the participants, including age (in years), gender (following as closed-choice male, female, diverse, no response), current job, and highest educational attainment."}, {"title": "3.3 Sample Acquisition, Data Cleaning, and Data Analysis", "content": "The sample was recruited via an independent online research participant pool. The study was approved by our university's institutional review board (IRB) under ID"}, {"title": "3.4 Description of the sample", "content": "By using an online research participant pool, we ensured that the sample represents the population of Germany across key demographic variables such as age, gender, education, employment status, and geographical background. The final sample consists of 1100 participants (570 (51.8%) women; 524 (47.8%) men, 5 (0.5%) diverse or non-binary, 1 (0.1%) person did not disclose their gender identity). The age ranged from 18 to 85 years with a median age of 51 years. There is no association between age and gender in the sample ($\\chi^2$ = \u2212.031, p = .210 > .05).\nThe participants in the sample report to have a diverse range of educational backgrounds. The majority of participants have completed their education at the university level, with 27.1% having an academic degree and 20.4% having a university entrance certificate (\u201cAbitur\u201d or \u201cFachabitur\u201d). Another significant portion of participants completed a high school diploma (\u201cRealschulabschluss\u201d) (23.5%) or have vocational training (18.4%). A smaller percentage of participants have completed a secondary school certificate (\u201cHauptschulabschluss\", 10.5%), while only a few participants have no formal education (0.2%).\nParticipants reported a diverse range of current employment statuses. The largest proportion of participants are currently employed full-time (48.2%), followed by those who are retired (22.5%). 14.8% of participants are employed part-time, while a smaller percentage are currently unemployed (7.8%) or in other employment relations, such as vocational training (0.7%), study programs (2.5%), or parental leave (1.3%). A very small percentage of participants are engaged in voluntary military or social services (0.1%), have irregular or mini jobs (1.9%), or are currently in school (0.2%). Overall, the sample consists of individuals with a wide range of employment statuses, reflecting different stages in their professional lives and personal circumstances.\nIn the sample, higher age is associated with lower technical readiness (r = -.224, p < .001) and lower AI-readiness (r = -.250, p < .001), but not with interpersonal trust (r = .067, p = .114), general self-efficacy (r = .061, p = .131), or openness (r = -.071, p = .114). Gender is associated with lower technology readiness scores (r = -0.210, p < .001), AI-readiness (r = -.156, p < .001 with women reporting lower experience with AI, and openness (r = .093, p = .014), with women, on average, reporting to be slightly more open than men. Gender is neither associated with interpersonal trust (KUSIV3) (r = .008, p = .779), and general self-efficacy (r = -.048, p = .218).\""}, {"title": "4 Results", "content": "First, we present the evaluations of the four assessment dimensions-Expectancy, Perceived Risk, Perceived Benefit, and overall Valence- averaged across each queried topic and each participant. These serve as an assessment of the general perception of AI in society. Second, we present the individual evaluations of selected topics in regard to the four assessment dimensions and analyse their interrelationships. Lastly, we analyse the individual risk-benefit tradeoffs and the role of user-diversity and individual differences in the evaluation of AI."}, {"title": "4.1 Overall Assessment of AI", "content": "On average, the expectancy that these AI topics will come true is above neutral (12.7%), meaning the participants believe that most of the projections will become reality within the next decade. The average perceived risk across all topics is rather high (34.7%) and, as illustrated by the gray distribution in Figure 2, only a few topics are below a neutral evaluation thus being evaluated as at least safer than others. Average benefit is about neutral (-5.2%), although the distribution shows that some of the projections are seen as more useful and others as more useless. Lastly, the overall valence or sentiment of the participants towards the queried topics is rather negative (-19.7%), although some of the queried topics are evaluated positively.\nFigure 2 illustrates these attributions and the left side of Table 1 presents the average scores across all topics and across all participants."}, {"title": "4.2 Evaluations of the queried AI statements", "content": "Due to the large number of topics queried, we will not address each individual topic and its evaluations in detail. Interested readers find the average ratings of all topics in Table 5 in the Appendix. Instead, we report the three highest and lowest rated topics for each evaluation dimension in the following.\nRegarding the expectancy of various statements related to artificial intelligence (AI), the top three most expected items were \"AI will independently drives automobiles\u201d with a score of +68.9%, followed by \"is misused by criminals\" (+67.7%), and \u201clearns faster than humans\u201d (+60.5%). Conversely, the least expected statements were \u201cAI helps us to have better relationships\u201d with a score of -46.3%, \u201cis a family member\u201d (-43.0%), and \u201chas a sense of responsibility", "AI is misused by criminals": "ith a score of +68.4%, followed by \"supervises our private life\u201d (+67.1%), and", "warfare": 66.4, "AI is humorous\" with a score of -22.5%, \u201ccreates valuable works of art that are traded for money": -11.2, "serves as a conversation partner in elderly care\" (-5.9%).\"\n    },\n    {\n      \"title\": \"4.2.1 Relationships among the topic evaluations\",\n      \"content\": \"We analyzed if the evaluations of the topics in regard to the assessment dimension are associated. The right side of Table 1 shows the correlations between the assessment dimensions. The average expectancy if the statements will occur in the next decade is neither related to the evaluation of their perceived risk (r = .150, p = .632 > .05), benefit (r = .277, p = .077 > .05), and valence (r = .054, p = .449 >.05). Apparently, the felt distance of AI projections does not impact evaluations in term of risk or benefit. However, perceived risk is negatively associated with both perceived benefit (r = -524, p < .001) and overall valence (r = -.800, p < .001). Hence, topics perceived as riskier are also perceived as less useful and as less positive. Further, perceived benefit is strongly associated with perceived valence (r = +.904, p < .001): Topics perceived as more useful are also perceived as more positive and vice versa.\nSince both perceived risk (negatively) and perceived benefit (positively) impact the overall valence of the topics, and risk and benefit are themselves coupled, we calculated a multiple linear regression to separate and examine their individual contribution in explaining the overall valence (as dependent variable).\nThe significant model included both risk ($\\beta$ = \u2212.490, p < .001) and benefit ($\\beta$ = +.672, p < .001) as strong and significant predictors. Neither the interaction term of both predictors ($\\beta$ = +.138, p = .305), nor the intercept term (I = 0.014, p = .265) were statistically significant. The overall model fit was strong (r2 = .965, F(3,67) = 612.3, p<.001), indicating that perceived risk and benefit significantly predicted the overall sentiment towards the queried topics. Variance inflation due to collinearity of the predictors was not an issue (VIF < 1.5). Table 2 shows the regression table of the model. Consequently, even after controlling for the correlation between risk and benefit, both perceived risk and benefit have a strong effect on the evaluation of the AI-related statements.\"\n    },\n    {\n      \"title\": \"4.2.2 Expectancy and Valence of the Queried Topics\",\n      \"content\": \"Figure 3 shows both the overall average sentiment of the participants towards each of the 71 queried projections (on the x-axis), as well as their assessment expectation if this projection will come true in the next decade (on the y-axis). The diagram can be read as follows: Points on the left are considered as rather negative implications that AI might have, whereas points on the right indicate potential developments that are evaluated as positive. Likewise, points on the lower side of the diagram are seen as developments that are perceived as less likely, whereas points on the upper side are considered as rather likely developments. Consequently, the diagram can further be split in four sectors with a) topics that are seen as positive and expected, b) positive and unexpected, c) negative and unexpected, d) negative and expected.\"\n    },\n    {\n      \"title\": \"4.2.3 Risk-Benefit Tradeoff of the Queried Topics\",\n      \"content\": \"Figure 4 shows both the overall average perceived benefit the participants attribute to each of the 71 queried topics (on the y-axis), as well as their assessment of the perceived risk (on the y-axis). As above, the diagram can be read as follows: Points on the left are AI-related statements that the participants, on average, perceive as rather safe, whereas points on the right side are perceived as rather risky. Likewise, statements shown on the lower side of the diagram are evaluated as rather useless and statements on the upper side are considered as rather useful. Again, the diagram can be split into the four sectors with a) topics that are seen as higher risk and also higher benefit, b) higher risk but lower benefit, c) lower risk and lower benefit, and d) lower risk and high benefit. Figure 4 illustrates the visual mapping of the AI-related statements in terms of risk and benefit and their strong relationship. As the figure illustrates, only few topics are perceived as low risk and are placed on the left side of the diagram. In particular, that AI will create valuable art is perceived as being of lower risk and also as useless. That AI might be funny is seen as rather safe and rather neutral in terms of benefits. AI being a conversation partner in elderly care is not seen as risky but as useful. Most other AI statements range from a neutral risk evaluation to being perceived as risky. In contrast to the previous figure, Figure 4 suggests the strong relationship between risk and benefit that is also illustrated by the black regression line.\"\n    },\n    {\n      \"title\": \"4.3 Perception of AI as Individual Difference\",\n      \"content\": \"Next, we analyse the perception of AI interpreted as individual differences to explore how demographics and personality factors influence the evaluations. For that, we interpret the average evaluations of the selected topics per participant as a reflexive measurement of the latent constructs perceived expectancy, risk, benefit, and overall valence .\nAs the lower part of Table ?? shows, some of the queried demographic and explanatory variables are associated with the evaluation of the AI statements. The expectancy if the AI projections will come true within the next decade is associated with the participants' openness from the Big Five personality model (r = .136, p =< .001) and their AIRS (r = .094, p = .036). Participants reporting being more open, rated the likelihood of the projections becoming true higher. Higher experience with AI is weakly linked to higher expectations on AI. The perceived risk of AI is associated to the age of the participants (r = .197, p < .001), their reported technology readiness (r = -.152, . < 001), as well as their AIRS (r = -.175, . < 001). Older participants perceive AI are riskier, whereas higher technology readiness and higher AIRS lowers the perceived risk of AI. Regarding the perceived benefit, age (r = -.182, . < 001), technology readiness (r = .233, . < 001), as well as AIRS (r = .274, . < 001) are associated. With higher age, the AI's perceived benefit decreases. In contrast, the perceived benefit increases with higher technology readiness and higher AIRS. Neither participants' gender nor their general self-efficacy is associated with the overall evaluations of AI.\nThe upper part of Table 3 shows that the four assessment dimensions are closely interwoven: Perceived expectancy is weakly associated with higher perceived risk (r = .212, p < .001) and benefit (r = .143), but not with the overall valence. The association between perceived risk and perceived benefits is negative, strong, and significant (r = -.639, p < .001). The overall valence of AI is linked to both perceived risk (r = -.749, p < .001) and perceived benefit (r = +.869, p < .001).\nTo understand how demographics and the user factors influence perceived Al risk, AI benefit and overall AI valence, we conducted hierarchical multiple regressions with two blocks of variables. In the first block, we included the demographic\"\n    },\n    {\n      \"title\": \"4.4 Desired Foci of AI governance\",\n      \"content\": \"Lastly, we asked for the primary foci for effective AI governance (as single choice). From the participants perspective, the predominant requirement, with 45.3% of respondents, was Human Control and Supervision of AI usage and development. Other significant demands included Transparency at 13.0%, Data Protection and Data Management at 11.7%, and Social and Ecological Well-being at 9.3%. Lesser but still notable concerns were Diversity, Non-discrimination, and Fairness at 4.8%, Robustness and Security at 4.7%, and Accountability at 4.5%. 6.7% of the participants did not respond to this question at all.\"\n    },\n    {\n      \"title\": \"5 Discussion\",\n      \"content\": \"Artificial Intelligence (AI) could become one of the defining technologies of the 21st century. Understanding how people perceive and weigh the risks and benefits of this technology is essential for ensuring that AI research and implementation align with human values and for developing effective AI governance. Drawing on the psychometric paradigm , we examined the trade-offs between perceived risks and benefits of various AI-related micro-scenarios from both individual and technological perspectives, using a sample of 1100 participants from Germany.\nAcross the various topics surveyed, we observed an overall negative sentiment across most of the topics: The majority of the topics are perceived as rather risky for the individuals and of little use. This may, of course, be a bias due to the selection of topics that included a large range of statements that are apparently negative or challenging, ranging from \\\"AI will be misused by criminals": "nd \u201cAI determines warfare", "AI creates many jobs\u201d or \u201cAI will independently drive automobiles": "hat participants neither liked nor disliked, but attributed higher risks to them. Overall, less than 20% of the statements received a positive evaluation and many of these were nonetheless perceived as risky.\nBeyond the absolute evaluation, we also analysed how the overall sentiment is formed. First, we observed an inverse relationship between perceived risks and perceived benefits. This corroborates prior findings, from both studies on AI perception in particular as well as from risk perception studies in different contexts. While this may sound trivial at first sight, this is meaningful as it highlights a cognitive bias where individuals tend to downplay the benefits of a technology if they perceive it as risky, and vice versa, which can influence public attitudes and policy decisions regarding the adoption of new technologies. Further, we found that the overall sentiment (negative to positive) is formed by both the perceived risk and the perceived benefits, with the benefits having a stronger influence on the overall valence than the perceived risks. On the one hand, perception of risk negatively affects emotional responses to AI, making people feel more negatively about it. On the other hand, perception of benefit has a strong positive effect, more than offsetting the negative impact of risk in shaping positive valence. This, again, corroborates many findings from risk research that found that technology perception is mostly driven by the perceived benefits rather than the perceived risks. This is interesting insofar, as AI and its implications are frequently seen as negative or worrying at least in many western countries.\nOverall, these results suggest that improving and demonstrating the benefit of AI is key to fostering positive attitudes, but risk management is also crucial, as high risk can erode the positive impact of benefit on emotional perceptions.\nFrom the perspective of individual differences, our results indicate that perceptions of AI's risks, benefits, and overall evaluation are shaped by both demographic factors and individual attitudes. Younger respondents tended to view AI-related topics as less risky, more beneficial, and rated them with a higher overall valence. Gender also played a role, though to a lesser extent, with women generally giving lower evaluations of AI than men. These findings align with current research on AI perception, which shows that age and gender can influence AI attitudes and that individuals with higher levels of technology or AI literacy are often less apprehensive about AI .\nHowever, these effects get smaller, if people have higher technology or AI readiness. Consequently, increasing technology and AI literacy may be suitable to address the perceived risks, lower benefits, and overall lower acceptance of AI in the society. If people have a better understanding about the basic functioning of AI, the many ethical challenges involved in building sophisticated AI models and the implications AI for individuals and the society, we can have a broad and substantial democratic debate about Al's potential, its limits, and potential caveats on different levels of social and societal consequences. This can be a foundation to decide where we want AI to take over control, where it should support us, and which areas should be free of AI. Offering free online courses for adults, such as \"Elements of AI\" and updated school curricula that incorporate digitalisation and AI literacy are a necessity .\nHowever, even after controlling for the influence of technology and AI readiness, a significant age bias remained. As the development of AI algorithms and the applications of AI in various contexts is predominantly driven by younger developers (that are further predominantly male), that raises the concern if the current and future developments are well-aligned with the norms and values of the later actual users of the technology. Here, it is essential to educate the developers in methods for human-centered and participatory development methods and to integrate an ethical perspective in the development of AI applications."}, {"title": "5.1 Limitations", "content": "This study is not without limitations. Firstly, although the sample of participants is large and diverse in terms of age, gender, education, and employment, we only surveyed participants from Germany. Based on studies that suggest cultural differences in AI perception , future work may therefore extend our work and analyse how cultural dimensions, such as country of origin or an individual's cultural heritage, influence the perception of AI and the involved risk-benefit tradeoffs.\nSecondly, although the sample of queried topics builds on existing research, the selection may be biased, yielding spurious findings due to Berkson'"}]}