{"title": "SIMPLETOM: EXPOSING THE GAP BETWEEN\nEXPLICIT TOM INFERENCE AND IMPLICIT TOM\nAPPLICATION IN LLMS", "authors": ["Yuling Gu", "Oyvind Tafjord", "Hyunwoo Kim", "Jared Moore", "Ronan Le Bras", "Peter Clark", "Yejin Choi"], "abstract": "While prior work has explored whether large language models (LLMs) possess\na \"theory of mind\" (ToM) - the ability to attribute mental states to oneself and\nothers - there has been little work testing whether LLMs can implicitly apply such\nknowledge to predict behavior, or to judge whether an observed behavior is ra-\ntional. Such skills are critical for appropriate interaction in social environments.\nOur approach to study such capabilities is to create a new dataset, called Sim-\npleToM, containing concise, diverse stories (e.g., \u201cThe can of Pringles has moldy\nchips in it. Mary picks up the can in the supermarket and walks to the cashier.\"),\neach with three questions that test different degrees of ToM reasoning, asking\nmodels to predict (a) mental state (\"Is Mary aware of the mold?\"), (b) behavior\n(\"Will Mary pay for the chips or report the mold?\"), and (c) judgment (\"Mary\npaid for the chips. Was that reasonable?\"). To our knowledge, SimpleToM is the\nfirst dataset to systematically explore downstream reasoning requiring knowledge\nof mental states in realistic scenarios. Our experimental results are intriguing:\nWhile most models can reliably predict mental state on our dataset (a), they often\nfail to correctly predict the behavior (b), and fare even worse at judging whether\ngiven behaviors are reasonable (c), despite being correctly aware of the protago-\nnist's mental state should make such secondary predictions obvious. We further\nshow that we can help models do better at (b) and (c) via interventions such as\nreminding the model of its earlier mental state answer and mental-state-specific\nchain-of-thought prompting, raising the action prediction accuracies (e.g., from\n49.5% to 93.5% for GPT-40) and judgment accuracies (e.g., from 15.3% to 94.7%\nin GPT-40). However, while this shows that models can be coaxed to perform\nwell, it requires task-specific interventions, and the natural model performances\nremain low, a cautionary tale for LLM deployment. SimpleToM thus breaks new\nground in probing real-world ToM reasoning, and reveals surprising, new insights\nabout current model capabilities. We hope the dataset enables further exploration\nby the community into this critical area of model behavior.", "sections": [{"title": "INTRODUCTION", "content": "People infer what others know, anticipate their actions, and expect them to choose cost-minimizing\nbehaviors (Gergely & Csibra, 2003; Liu & Spelke, 2017). Performing such social reasoning in-\nvolves attributing mental states to oneself and others, an ability widely known as Theory of Mind\n(ToM) (Premack & Woodruff, 1978). ToM has been extensively studied in psychology in a range of\nscenarios, for instance, studies of manipulation, secrecy, deception, lying, and misleading behavior\n(Doherty, 2008). Examples of classical tests in developmental psychology include the unexpected\ntransfer false belief task, e.g., the Sally-Anne task (Baron-Cohen et al., 1985), and the unexpected\ncontents false belief task, e.g., the Smarties task (Perner et al., 1987). Given the increasing use\nof LLMs in human interactions and as decision-making agents within complex, human-centered\nenvironments, it is crucial to assess their ToM capabilities."}, {"title": "SIMPLETOM DESIGN", "content": "We design the stories in SimpleToM to contain diverse types of information asymmetry, using a\nconcise format and associated with specific question types testing explicit and applied ToM."}, {"title": "DIFFERENT TYPES OF INFORMATION ASYMMETRY", "content": "To expand beyond the classical false belief task, we seed the creation of SimpleToM with ten di-\nverse scenarios where information asymmetry occurs naturally (Table 1). This is inspired by social\npsychology literature to cover asymmetries like manipulation, deception, secrecy, lying, and mis-\nleading behavior (Doherty, 2008), seen in real-world contexts like sales of \"lemon\" products, where\nitems with hidden flaws are purchased due to a lack of information (Akerlof, 1978). These are\nunder-examined in existing ToM tests. We further describe the scenarios with examples in Table 7\n(Appendix E.1)."}, {"title": "SIMPLE STORY FORMAT WITHOUT EXPLICIT PERCEPTS OR MENTAL STATES", "content": "The SimpleToM example story from Figure 1 reads: The can of Pringles has moldy chips in it. Mary\npicks up the can in the supermarket and walks to the cashier. Each story has exactly two sentences,"}, {"title": "QUESTIONS TESTING EXPLICIT AND APPLIED TOM", "content": "We use three types of questions (Figure 1) to probe a model's grasp of each story, covering both\nexplicit theory of mind (conceptual knowledge about others' mental states; i.e., via (a) mental state\nquestion about information awareness) and applied theory of mind (the ability to use theory of mind\nin real-life contexts; i.e., via (b) behavior and (c) judgment questions) (Lee et al., 2024).\nMental state (MS) question about information awareness: We test ability of models to infer men-\ntal states, specifically information awareness, through a simple yes/no question(Is <Person X>\nlikely to be aware that \"<key information>\"?) To infer whether a character is\naware of something in SimpleToM stories, a model has to make implicit commonsense inferences\nabout what the character can perceive or know in the given situation (including commonsense rea-\nsoning about physical objects, space, intent, goals of others, and so on).\nBehavior prediction question: This question asks which of two possible actions the main subject\n(Person X) is likely to perform next. For instance, beyond answering that a person shopping for\nchips in the supermarket is unlikely to know that \"the can of Pringles has moldy chips in it\", a\nmodel that successfully applies this inference for behavior prediction should also infer that a person\nwho picked up such a can in the supermarket would likely \"pay for the chips\" rather than \"report\nthe moldy chips.\" To answer these questions correctly, models need to reason over the situation\nto infer the mental state of character, and realize how the character's lack of awareness of the key\ninformation would impact their likely next action."}, {"title": "SIMPLETOM CREATION", "content": "Specifically, the construction of SimpleToM consists of the following steps:\nStep 1: Manually create one example seed story for each scenario.\nStep 2: For each scenario, using the seed story as example, prompt the LLM to suggest 10 diverse\nsets of entities compatible with an information asymmetry. (See prompt in E.5.)\nStep 3: For each set of suggested entities, along with the seed story, prompt the LLM to write\nthree new stories at different levels of \u201cseverity.\u201d With each story, also generate likely next\n\"unaware\u201d and \u201caware\u201d behaviors (see Section 2.2). Appendix E.4 provides further details.\nWe went through two rounds of this process. First, we used GPT-4 and Claude-3-Opus\u00b9 to generate\na total of 1200 stories.\u00b2 After annotating and filtering this initial set (Section 3.2), we picked a\nnew set of top-scoring seed stories and sourced 10 additional sets of entities from each of GPT-\n4o and Claude-3.5-Sonnet. We used these two newer models to generate stories for all 40 sets\nof entities, for a total of 2400 more stories. By using several generator models, varied entities\nand different seed stories, the resulting stories in SimpleToM have a wide range of information\nasymmetries instantiated in different real-world situations, effectively broadening neural ToM tests\nbeyond traditional settings (Section 6). These contexts also allow for nuanced and implicit traits\n(e.g., buyers would avoid products with defects if they know about them)."}, {"title": "STRICT QUALITY CONTROL ON STORIES THAT GOES INTO SIMPLETOM", "content": "We gather human annotations on each story (and unaware/aware next actions). We asked annotators\nfour questions for each story, summarized in Figure 2. This process verifies that the key information\nin each story is something that Person X has false belief about. We also carefully verify that the next\nlikely \"unaware action\" is appropriate if and only if Person X is unaware of the key information.\nWe similarly verify the \"aware action,\" for the counterfactual situation where Person X is some-\nhow aware of the key information. Appendix D provides further details about the crowdsourcing\nprocedure, with instructions, examples and question templates.\nOur annotators passed a rigorous qualification test (Appendix D.2) and met other high-standard\nrequirements (Appendix D.3). Only stories for which all crowdworkers (3) judged all aspects to be\nvalid were included in SimpleToM.\u00b3 This results in 1147 stories (out of the original 3600) in the final\nSimpleToM dataset. Table 8 (Appendix E.2) provides statistics and further details for SimpleToM."}, {"title": "EXPERIMENTAL SETUP", "content": "We evaluate SimpleToM on ten frontier LLMs from different sources and with different levels of\ncapabilities: Llama-3.1-8B, Llama-3.1-405B, (Dubey et al., 2024) Claude-3-Haiku, Claude-3-Opus\n(Anthropic, 2024b), Claude-3.5-Sonnet,(Anthropic, 2024a) GPT-3.5, GPT-4, GPT-40, 01-mini, and\n01-preview (OpenAI, 2024; 2023) (refer to Appendix C Table 6 for more details). We use the most\ndeterministic setting with a generation temperature of 0.4"}, {"title": "RESULTS AND ANALYSIS", "content": ""}, {"title": "FRONTIER LLMS CAN INFER MENTAL STATES, BUT STRUGGLE TO USE IT", "content": "The overall evaluation results on SimpleToM for the 10 models are summarized in Table 2, spanning\nthe different question types (as detailed in Section 2.3). We analyze models' performance for each\ntype of question below. Note that these are binary questions where random performance is 50%.\nMental state (MS) question about information awareness: Our results (Table 2, \"mental state\"\ncolumn) show that reasoning over implicit information in given situations to infer mental states is\nstill challenging for models like GPT-3.5 (36.5% accuracy), while newer and/or bigger models like\nClaude-3-Haiku, o1-mini, and Llama-3.1-8B perform reasonably well (around 88%). In fact, all\nrecent frontier models are proficient at inferring characters' awareness in our dataset \u2013 GPT-40,\nLlama-3.1-405B, Claude-3-Opus, GPT-4, Claude-3.5-Sonnet, and o1-preview all achieved accura-\ncies of more than 95%. This result also confirms the quality of our dataset, in that characters'\nmental states in SimpleToM stories are implicit but reasonably easy to infer, as designed.\nBehavior prediction: On behavior prediction questions (Table 2, \"behavior\u201d column), smaller and\nolder models perform extremely poorly (with GPT-3.5 achieving only 7.6% accuracy and several\nother models scoring less than 40%). Even for the larger models, like Llama-3.1-405B, Claude-3.5-\nSonnet, and GPT-40, performance on the behavior prediction task is much worse than on the mental\nstate task with at least a 30% performance drop. This large inconsistency suggests that while fron-\ntier LLMs may have the right conceptual knowledge/information about others' mental states when\ndirectly asked, they struggle to apply this knowledge in real-life contexts to make predictions about\ncharacters' behavior. Only the latest o1-preview model, with its built-in inference time reasoning\ntokens, manages a decent score on this question type (84.1%).\nJudgment of behavior: Our results (Table 2, \"judgment\" column) show that this additional, seem-\ningly trivial, inference step (beyond the behavior prediction) makes the task much more difficult for"}, {"title": "NO APPLIED TOM IN LLMS? EXPLORING THE RABBIT HOLE OF HUMAN\nHAND-HOLDING", "content": "We explore four different inference interventions to help LLMs answer questions requiring applied\nTOM. Apart from the first intervention, we focus these experiments on the strongest or latest models\nfrom each source (and we do not consider the o1 models in this section).\nMental state reminder (MS): Here we remind the model of its answer to the mental state\nquestion by including this question (with the model's answer) in the prompt. This also puts the model\non alert that \"awareness\" might be relevant. Table 3 summarizes the effect of this when answering\nthe behavior and judgment questions. On the behavior prediction questions, this intervention\nresults in substantial boosts in accuracy, for instance, from 58.3% to 89.5% for Llama-3.1-405B,\nand from 49.5% to 82.8% for GPT-40. On Claude-3.5-Sonnet, the performance increases by almost\n30% to 96.9%, largely closing the gap between the mental state and behavior prediction question\nscores. However, on the judgment questions, the performance boost is much more modest, and\nmost models still score below or at random, except for Claude-3.5-Sonnet where this intervention\nbrings the score up from 24.9% to a reasonable 84.1%. This highlights how such interventions,\nwhile seemingly effective in some cases, are generally fragile band-aids with limited scope.\nSystem prompt guiding (SysP and SysP*): We also explore the effect of guiding\nthe models to remember to account for mental state inferences by modifying the system\nprompt. We try two different prompts, SysP which includes the phrase \"consider\nthe mental state of all the entities involved\" and SysP* which further in-\ncludes the more direct hint \"E.g., think carefully about what each person is\naware or not aware of.\". The results are summarized in Table 4. On behavior pre-\ndiction, we see that generically guiding models to consider the mental state using SysP is only\neffective to a limited extent (accuracy changes ranging from -2.2% to +6.3%), while providing more"}, {"title": "RELATED WORK", "content": "Theory of Mind has been extensively studied in psychology in a range of scenarios (see Appendix\nB). ToM reasoning, and broadly social commonsense, has also been shown to be important by\nthe different parts of the AI community including in conversations (Kim et al., 2023b;a), games\n(Zhou et al., 2023b), and even multi-modal setups (Jin et al., 2024), with most popular ToM tests\nusing stories to probe LLMs. Relying on stories from small test sets in cognitive science studies\nto benchmark ToM abilities in LLMs (Bubeck et al., 2023; Kosinski, 2024) could produce results\nthat differ given minor alterations (Ullman, 2023) and would be more robust if tested on larger\nsamples. Yet expert-crafted or naturally occurring self-contained stories that can serve as targeted\ntests of ToM are scarce and human story-writing is expensive, leading to the use of automatically\ngenerated datasets for studying ToM behavior in LLMs (Jung et al., 2024; Wilf et al., 2023; Sap et al.,\n2023; Shapira et al., 2023; Sclar et al., 2023). Existing generated datasets allow studies of ToM\nto be carried out at scale, but templated stories often limit settings where information asymmetry\narises. For example, the entire dataset might only contain stories about some object being moved\n(over-reliance on classical Sally-Anne task, e.g., in ToMi (Le et al., 2019), ToM-bAbI (Nematzadeh\net al., 2018), Hi-ToM (Wu et al., 2023), OpenToM (Xu et al., 2024)). Or the dataset might focus\non whether some character has witnessed a sudden external event (BigToM (Gandhi et al., 2024)).\nThese stories are often systematically generated with the explicit use of mentalizing words to convey\npercepts and beliefs, e.g., \u201cunknown to Amy\u201d and \u201cAmy thinks that\u201d in Xu et al. (2024) or \"Noor\nsees\" and \"Mei does not notice\" in Gandhi et al. (2024). However, the explicit use of mentalizing\nwords also makes the stories (i) unnaturally simplistic, having removed the need for commonsense\ninferences about percepts or beliefs, and (ii) sometimes unrealistic, with combinations like \u201cCheng\ndoes not notice the power outage\u201d when he \u201cuse[s] a projector to show a documentary\u201d(Gandhi et al.,\n2024). Other existing datasets could be improved by addressing issues such as lacking exploration\nof applied ToM beyond action prediction (Zhou et al., 2023a; Gandhi et al., 2024), confounding\nfactors like memory loads or tracking requirements (Le et al., 2019; Xu et al., 2024), and violating\nQuesque & Rossetti (2020)'s criteria (see Appendix B) for validating ToM (Chen et al., 2024). Our\nwork extends existing datasets by following Tian et al. (2024) in combining the generative strength\nof LLMs and the verification ability of human annotators, and extends the existing efforts toward\nrobust, generalizable evaluation (Kiela et al., 2021; Srivastava et al., 2024), avoiding known pitfalls\nwhile preserving the systematic and scalable nature of the dataset creation process."}, {"title": "CONCLUSION", "content": "SimpleToM is the first dataset of its kind testing both explicit and applied ToM using a large set\nof diverse, concise, simple stories, covering different ways in which information asymmetry may\narise. The dataset opens up new opportunities for evaluating and debugging ToM abilities of LLMs,\nespecially in the relatively under-studied area of applied ToM. Our analyses reveal a jarring gap\nbetween explicit and applied ToM capabilities in current frontier LLMs. Thus, if our goal is LLM\nagents capable of applying ToM in complex, human-centered environments, we need to look beyond\ntesting LLMs with psychology-inspired ToM questions, and also start testing them more rigorously\non applied ToM (e.g., behavioral prediction and judgment) in different situations.\nWe show that this gap can be largely closed in the best models by well-designed interventions at\ninference time, like reminding the model of its answers to important questions or guiding it with\na custom chain-of-thought prompt. We argue that a robust LLM should perform well on Simple-\nTOM without such interventions, so it can independently and flexibly apply ToM-related reasoning\nwhenever required within potentially complex and multi-faceted environments. Model developers\ninterested in real-world deployment of their models, should be alert to closing this performance gap\nso the models can interact with society appropriately, ideally without the higher inference costs asso-\nciated with explicit chain-of-thought reasoning or implicit 01-preview reasoning tokens (see further\ndiscussion in Appendix H).\nSimpleToM can also facilitate the community in pursuing various exciting directions for future work,\nincluding studying how ToM performance may differ with stories that involve different levels of\nharmfulness and unethicality (see Appendix E.3), and innovative modeling approaches that can help\nclose the gap between explicit and applied ToM in AI models."}, {"title": "ETHICS STATEMENT", "content": "All annotators that participated in the data collection process have been anonymized. The only\npersonal information we collect is the worker IDs from Amazon Mechanical Turk, which we will\nnot release. No personally identifiable information is contained in our dataset or otherwise released.\nWe took great care to pay fair wages, and were responsive to feedback and questions throughout the\ndata collection process.\nThis study involves the study of large-scale language models. We are careful in prompting models\nduring the story generation stage to follow our desired content and simple story format, avoiding\ngenerations that may contain offensive statements. Like any other experiments with large-scale\nlanguage models, despite the best intentions, there is a risk of the examined models producing\nbiased or offensive statements as part of a free-form generation (e.g., CoT reasoning). We release\nour data for research purposes only."}, {"title": "REPRODUCIBILITY", "content": "We make our SimpleToM dataset and the full evaluation data for the analyzed models publicly\navailable. This will allow researchers to reproduce and build on top of our work in studying the\nneural ToM capabilities of LLMs.\nFurther, we provide all prompts used for SimpleToM creation see Appendix E.5 for the entity\nbrainstorming prompt, and Appendix E.4 for the story generation prompt. We also carefully doc-\nument the instructions used in our crowdsourcing process (Appendix D.1) and how we qualified\nworkers (Appendix D.2). All prompts used for the different inference interventions are provided in\nAppendix G."}, {"title": "FAQS", "content": "Q: How is SimpleToM different from existing datasets?\nSimpleToM addresses limitations in previous efforts to examine Theory-of-Mind (ToM)\nreasoning in LLMs, by (1) having diverse false belief setups (e.g., beyond those in Sally-\nAnn task where some object is moved when a character is not present), (2) requiring LLMs\nto make commonsense inferences in situations rather explicit use of mentalizing words\nto convey what characters perceive or believe, and (3) going beyond explicit ToM to test\nmodels' ability to apply inferred knowledge in follow-up applied ToM questions (such as\nbehavior prediction and judgment of behavior).\nQ: What new insights does SimpleToM help uncover about models' ToM capabilities?\nOur analysis reveals novel insights on how frontier models are generally proficient in ex-\nplicit awareness inference questions but this success does not transfer to applied ToM\n(applying this knowledge is applied to \u201cbehavior\u201d and \u201cjudgment\u201d questions). We show\nthat these capabilities are decoupled in LLMs: inferring characters' awareness and apply-\ning them in downstream reasoning. Although models seem to answer awareness ques-\ntions correctly, they have not yet learned to perform ToM-based reasoning for downstream\nquestions. As a result, we argue that achieving ToM in LLMs is not just about getting\npsychology-inspired ToM questions correct (stopping at the mental state question), but they\nhave to be able to apply them (which is precisely what SimpleToM extends to examine).\nAnalysis by scenarios further highlights the need to test on different scenarios, and ones\nthat are varied and different from those in classical ToM tests to ensure that we are effec-\ntively testing the ToM reasoning abilities of models (rather than models' ability to match\nsimilar situations in training data).\nQ: What do the different inference interventions reveal about models' ToM capabilities?\nPatch, guide, think aloud combined method results show belief-to-judgment inference ro-\nbustness. When reminded of their answer to mental state questions about information\nawareness and using ToM-specific CoT, models' belief-to-judgment inference is pretty ro-\nbust in existing LLMs. The models have some knowledge linking irrationality and un-\nawareness (e.g., unawareness should be accounted for seemingly irrational behavior). So\narriving at the \u201ccorrect belief\u201d will be crucial part of ToM-based reasoning, but this is not\ndone implicitly in current frontier models' reasoning.\nQ: Are the poor performance on the applied ToM questions a reflection of fundamental\nflaws in ToM capabilities of models or specific question-wording?\nWe illustrate in Appendix F some prompt variations that we have experimented with for the\njudgment question. Across Llama-3.1-405B, Claude-3.5-Sonnet and GPT-40, the scores\nusing different variants were all consistently below random (never exceeding 30% accu-\nracy), indicating that the low scores on the judgment questions come more from funda-\nmental flaws in the applied ToM capabilities of models rather than an effect of specific\nformatting/wording."}, {"title": "STUDIES OF TOM IN PSYCHOLOGY", "content": "Theory of Mind has been extensively studied in psychology in a range of scenarios, for instance,\nstudies of manipulation, secrecy (Peskin & Ardino, 2003), deception, lying (Lewis et al., 1989;\nPerner, 1993; Peskin, 1992), misleading behavior (Chandler et al., 1989; Wimmer & Perner, 1983;\nDoherty, 2008), autism (Frith & Happ\u00e9, 1994), and analysis of rational behavior (Gergely & Csibra,\n2003; Liu & Spelke, 2017). Classical tests of ToM in developmental psychology include testing the\ndevelopment of this ability in children via false belief prediction \u2013 using the unexpected transfer\nfalse belief task, the Sally-Anne task (Baron-Cohen et al., 1985), or the unexpected contents false\nbelief task, the Smarties task (Perner et al., 1987). Quesque & Rossetti (2020) review classic tests of\nToM and outline two important criteria for tasks that validate ToM: (1) The task must indicate that\nthe respondents can differentiate between the other's mental state and their own. (2) Lower-level\nprocesses, like associative learning, should be ruled out as explanations for achieving successful\nperformance. Given the wide applicability of ToM reasoning in various real-life situations such\nas analyzing people's behavior (Liu et al., 2024; Jara-Ettinger et al., 2016) and making judgments"}, {"title": "DETAILS OF DIFFERENT INFERENCE INTERVENTIONS", "content": "We illustrate the interventions used to give LLMs various levels of hints, such as reminding the mod-\nels about their mental state answers, using chain-of-thought prompts to encourage careful reasoning,\nor doing so via the system prompt\nPATCHING MENTAL STATE INFERENCE IN THE PROMPT (MS REMIND)\nExample prompt which includes a reminder of the model's answer to the mental state question:\nGiven the following story, answer the question by giving the correct\nanswer choice, (A) or (B).\nStory: The bag of potato chips has moldy chips in it. Mary picks up the\nbag in the supermarket and walks to the cashier.\nQuestion: Is Mary likely to be aware that \"The bag of potato chips has\nmoldy chips in it.\"?\n(A) No\n(B) Yes\nAnswer: (A)\nQuestion: What will Mary likely do next?\n(A) pay for the chips\n(B) report the moldy chips\nWhat is the correct answer? Respond with just \"(A)\" or \"(B)\"\nGUIDING VIA SYSTEM PROMPT (SYSP)\nSysP:\nYou are a helpful assistant. Before responding, you always consider\ncarefully all implicit and explicit aspects of the input, including the\nmental state of all the entities involved.\nSysP*: You are a helpful assistant. Before responding, you always consider\ncarefully all implicit and explicit aspects of the input, including the\nmental state of all the entities involved. E.g., think carefully about\nwhat each person is aware or not aware of.\nENCOURAGING REASONING/THINKING BEFORE ANSWERING (COT)\nWe give these chain-of-thought prompts after the presenting the question (bold for highlighting\npurposes only):\nCoT (generic):\nThink step by step to arrive at an answer. Start your response by\nexplaining your reasoning process and end your response with \"Therefore,\nthe answer is: followed by (A) or (B)\nCoT* (ToM oriented):\nThink step by step to arrive at an answer. Think carefully about what\neach person is aware or not aware of. Start your response by explaining\nyour reasoning process and end your response with \"Therefore, the answer\nis: followed by (A) or (B)"}, {"title": "INFERENCE COSTS FROM INTERVENTIONS", "content": "Doing chain-of-thought reasoning generates a lot more output tokens than just providing the answer.\nThis overhead is not present when just updating the system prompt (just a fixed cost from the longer\nprompt). The o1 family of models also adds hidden \u201creasoning tokens\" to the output."}, {"title": "DISCUSSION ON PROMPT VARIATIONS", "content": "In formatting the judgment questions, we experiment using with different versions of answer options\nas well as question formatting.\nWe illustrate some versions of the answer options we tried here:\nVersion 1: Reasonable vs not reasonable\nThe behavior of Mary can be best described as:\n(A) Not reasonable\n(B) Reasonable\nVersion 2: Generated option vs not reasonable\nThe behavior of Mary can be best described as:\n(A) Irresponsible\n(B) Reasonable\nVersion 3: 3-way QA with generated option\nThe behavior of Mary can be best described as:\n(A) Implausible\n(B) Reasonable\n(C) Irresponsible\nSpecifically, in versions 2 and 3, options like \"Irresponsible\" were dynamically generated with\nClaude-3.5-Sonnet and GPT-40 (picking the shorter generation) using the following prompt:\nRead the following story and judge the person's behavior:\nSTORY: {story} Next, {PersonX} \"{action_unaware}\" and not \"{action_aware}\".\nAssume that {PersonX} is/are somehow aware of \"{key_information}\",\nhow would you describe {PersonX}'s behavior? Consider the negative\nconsequence of the behavior. Answer with only 1-2 words.\nWe also tried the following alternate variations in preliminary experiments to evaluate the effect of\nchanging the wording of the judgment question:\n{PersonX} chose to \"{action_unaware}\" and not \"{action_aware}\". Was it\nwrong of {PersonX} to choose \"{action_unaware}\" over \"{action_aware}\"?\n{PersonX} chose to \"{action_unaware}\" and not \"{action_aware}\".\nWas it reasonable for {PersonX} to choose \"{action_unaware}\" over\n\"{action_aware}\"?\n{PersonX} chose to \"{action_unaware}\" and not \"{action_aware}\". Was it\nwrong or reasonable for {PersonX} to choose \"{action_unaware}\" over\n\"{action_aware}\"?\nNext, {PersonX} \"{action_unaware}\" and not \"{action_aware}\". Was it wrong\nor reasonable of {PersonX} to \"action_unaware}\" and not \"{action_aware}\"?\nAcross Llama-3.1-405B, Claude-3.5-Sonnet and GPT-40, the scores using these variants were all\nconsistently below random (never exceeding 30% accuracy), indicating that the low scores on the\njudgment questions come more from fundamental flaws in the applied capabilities of models rather\nthan an effect of specific formatting/wording."}]}