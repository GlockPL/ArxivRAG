{"title": "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation", "authors": ["To Eun Kim", "Fernando Diaz"], "abstract": "Many language models now enhance their responses with retrieval capabilities, leading to the widespread adoption of retrieval-augmented generation (RAG) systems. However, despite retrieval being a core component of RAG, much of the research in this area overlooks the extensive body of work on fair ranking, neglecting the importance of considering all stakeholders involved. This paper presents the first systematic evaluation of RAG systems integrated with fair rankings. We focus specifically on measuring the fair exposure of each relevant item across the rankings utilized by RAG systems (i.e., item-side fairness), aiming to promote equitable growth for relevant item providers. To gain a deep understanding of the relationship between item-fairness, ranking quality, and generation quality in the context of RAG, we analyze nine different RAG systems that incorporate fair rankings across seven distinct datasets. Our findings indicate that RAG systems with fair rankings can maintain a high level of generation quality and, in many cases, even outperform traditional RAG systems, despite the general trend of a tradeoff between ensuring fairness and maintaining system-effectiveness. We believe our insights lay the groundwork for responsible and equitable RAG systems and open new avenues for future research. We publicly release our codebase and dataset.", "sections": [{"title": "1 Introduction", "content": "In recent years, the concept of fair ranking has emerged as a critical concern in modern information access systems [11]. However, despite its significance, fair ranking has yet to be thoroughly examined in the context of retrieval-augmented generation (RAG) [28], a rapidly advancing trend in natural language processing (NLP) systems [26]. To understand why this is important, consider the RAG system in Figure 1, where a user asks a question about running shoes. A classic retrieval system might return several documents containing information from various running shoe companies. If the RAG system only selects the top two documents, then information from the remaining two relevant companies will not be relayed to the predictive model and will likely be omitted from its answer. The fair ranking literature refers to this situation as unfair because some relevant companies (i.e., in documents at position 3 and 4) receive less or no exposure compared to equally relevant company in the top position [11].\nUnderstanding the effect of fair ranking in RAG is fundamental to ensuring responsible and equitable NLP systems. Since retrieval results in RAG often underlie response attribution [14], unfair exposure of content to the RAG system can result in incomplete evidence in responses (thus compromising recall of potentially relevant information for users) or downstream representational harms (thus creating or reinforcing biases across the set of relevant entities). In situations where content providers are compensated for contributions to inference, there can be financial implications for the unfairness [3, 18, 30]. Indeed, the fair ranking literature indicates that these are precisely the harms that emerge"}, {"title": "2 Background & Related Work", "content": "Retrieval-Augmented Generation. RAG, a specific type of retrieval-enhanced machine learning (REML) [26, 53], has been widely adopted in various domains, including language modeling [25], question-answering [21], personalization [27, 34, 44, 45], and recommendation [55]. Studies on"}, {"title": "3 Experimental Methodology", "content": "In traditional RAG systems, a user input is used to query a retrieval system for recommended items from some corpus, which are then used for generation. Given user input x, a query q generated by the query generation function $q(x), and a corpus of documents C, a deterministic retriever R(q, C) returns a fixed ranked list L every time q is seen. Retrieval is followed by a top k truncation which is passed to a prompt generation function op(x, L1:k) that returns a final prompt 7, which is subsequently passed to the language model G(7). Because deterministic retrievers allocate exposure to the same item over repeated samples, RAG systems with deterministic retrievers present a challenge to ensuring equal exposure of relevant items to the generator.\nTo address the issue of unfairness in the rankings passed to the generator, we can convert a deterministic retriever into a stochastic retriever, which has been shown to provide fair rankings in expectation"}, {"title": "3.1 Construction of a Test Collection with Utility Labels", "content": "Setting an appropriate proxy for measuring item-worthiness is crucial in the evaluation of fairness [2]. Drawing on the insight that utility-based judgments are more suitable than relevance judgments in the context of RAG [42, 56], we annotate item-level utility labels for all items in the corpus.\nWe define an item's worthiness by the additional utility (utility-gain) it provides to a language model (specifically, the generator in a RAG system) when used to solve a specific task as part of the aug- mentation process. To assess this utility-gain, each item in the corpus is individually supplied to the generator along with an input question. The utility-gain is then calculated as the difference between the utility of the augmented generator and that of a baseline language model without any information about the item. Formally, let uz denote the baseline string utility score from the vanilla language model prompted only with the input question, and let u; represent the utility score from the language model with a prompt augmented by the j'th item dj in the corpus. The item dj is considered useful if the utility-gain \u03b4j = uj \u2212 u\u017e is positive, and not useful otherwise (see Appendix B).\nTherefore, the item-level utility labels are designed to be both task- and generator-dependent, as the utility of each item varies depending on the task and the language model used. This labeling process also aligns with the principles of task-based information retrieval [24]."}, {"title": "3.2 Fairness-Aware Stochastic Retriever", "content": "Stochastic retrievers have been used for various purposes, such as optimization of retrieval models [5, 15, 35, 52], as well as ensuring equitable exposure of items [9, 35, 36]. Many of these studies use Plackett-Luce sampling [37] to achieve the stochasticity of retrieval. We follow the line of research and formally define how we derive a fairness-aware stochastic retriever through Plackett-Luce sampling. To enhance sampling efficiency, we adopt the methodology of Oosterhuis [35], and for controllable randomization, we utilize the approach proposed by Diaz et al. [9].\nGiven n items in a corpus C, a vector of retrieval scores s \u2208 Rn can be obtained from R(q, C), which can be used to generate a ranked list L. We then min-max normalize retrieval scores to be in [0, 1] in order to construct a multinomial distribution over items [4]. The probability of an item d being selected as the i'th item in a new ranking & through Plackett-Luce sampling is given by\np(d|L_{1:i-1}) = \\frac{exp(s_d)1[d \\notin L_{1:i-1}]}{\\sum_{d'\\in C\\setminus L_{1:i-1}} exp(s_{d'})} \\quad(1)\nwhere L1:i-1 is the partial ranking up to position i \u2013 1, s represents the normalized retrieval score vector, and sd is the normalized score of item d. Using this probability, we iteratively sample an item, set its probability to 0, renormalize the distribution, and repeat the process. The probability of generating a complete ranking is then given by the product of the placement probabilities for each item, i.e., p(\u03c3|q) = \\Pi_{i=1}^{n}P(\\sigma_i| \\sigma_{1:i\u22121}).\nThis repeated sampling and renormalization process can be efficiently managed using the Gumbel- Softmax trick [16, 31], which enables the sampling of rankings to be performed at the speed of sorting [35]. To do so, for each sampling iteration, we draw Ui ~ Uniform(0, 1), followed by generating a Gumbel noise G\u2081 = \u2013 log(-log(U\u2081)). The probability of each sampled ranking oi is then obtained by sorting the items based on their perturbed scores Sd\u2081 = $d\u2081 + Gi."}, {"title": "3.2.1 Controlling the Level of Fairness", "content": "Adjusting the level of randomization directly controls the degree of item-fairness, aligning with our goal to observe how varying levels of fairness in rankings affect the ranking and generation quality of a RAG model. To obtain the controllability, we follow the work of Diaz et al. [9] and use a temperature parameter a. We apply the scalar a to each value in the normalized score vector s by raising each value to the power of a. This process is done before the scores are passed to the sampling policy. Therefore, the modified sampling distribution is thus defined as:\np(d|L_{1:i-1}) = \\frac{exp(s_d^\\alpha)1[d \\notin L_{1:i-1}]}{\\sum_{d'\\in C\\setminus L_{1:i-1}} exp(s_{d'}^\\alpha)} \\quad(2)\nThis implies that the sharpness of the sampling distribution is controlled by the a. A higher a amplifies the probability of items with higher retrieval scores being sampled. Therefore, if multiple rankings are sampled by the stochastic retriever with high a, it results in high disparity (i.e., item-side unfairness) of sampled rankings. At extreme, with considerably high a, the procedure results in the identical rankings which is the behavior of a deterministic ranker (i.e., maximum item-unfairness). On the other hand, a lower a reduces the disparity of sampled rankings, making the exposure distribution fairer. At extreme, when a = 0, the sampling procedure becomes uniformly random and achieves the lowest disparity (i.e., maximum item-fairness) in the sampled rankings."}, {"title": "3.3 Evaluation", "content": "As mentioned in Section 3, because we are dealing with stochastic retrievers, we need to measure the expected behavior of the system. Let S(s, N, k) be the stochastic sampler that samples a set of N rankings \u03a3 = [01, 02,\u00b7\u00b7\u00b7, \u03c3\u03bd], given the retrieval scores s, where each ranking \u03c3\u03b5 is truncated to the size of k. From each ranking, we can get an output \u00ee\u0177i = G($p(x, oi)). With an arbitrary fairness metric \u03bc\u03b5 (\u03a3) and a ranking quality metric \u03bcr(\u03a3) that takes a set of rankings as an input, we can measure the degree of fairness and ranking quality of the sampled rankings. Similarly, an arbitrary string utility metric pu (y, \u0177i), such as ROUGE, can be used to assess an expected effectiveness of a RAG system by calculating the average of the N metric scores.\nIn this paper, based on the empirical investigation done by Raj and Ekstrand [38], we use expected exposure disparity (EE-D) and expected exposure relevance (EE-R) [9] as \u00b5f and \u03bcr, respectively (\u00a73.3.1). For \u03bcu, we select the metric depending on the task, and we get the expectation of the utility of a RAG model which we call an expected utility (EU) (\u00a73.3.2)."}, {"title": "3.3.1 Expected Exposure in the Context of Machine Users", "content": "Expected Exposure (EE) [9] works by estimating the exposure of items across rankings (e.g., \u03a3) created by a subject model, and comparing them with an optimal set of rankings that always satisfy the item-fairness. To represent the attention over n items given by the consumer (generator in RAG), an n \u00d7 1 system exposure vector e is created. This is then compared with an n \u00d7 1 target exposure vector \u20ac*, where it represents the exposure of items allocated by an oracle retriever that always rank useful items above non-useful ones [9].\nWith the system and target exposure vector \u20ac \u2208 Rn and e* \u2208 Rn, we can get the difference between the two by the squared 12 distance:\n||\u20ac \u2013 \u20ac*||2 = ||e||2 \u2013 2(\u20ac, \u20ac*) + ||\u20ac*||2 \\quad(3)\nThis difference yields two metrics useful for fairness and ranking quality evaluation. ||6||2 can be a measure for disparity of rankings (EE-D), and (\u03f5, \u03f5*) can be a measure of ranking quality (EE-R) by calculating the degree of alignment of system exposure to the target exposure (i.e., how much of the exposure is on useful items). Therefore, the higher the value of EE-D, the more unfair the set of rankings are, and the higher the value of EE-R, the closer the set of system rankings are to the optimal set of rankings with respect to the ranking quality.\nThe exposure of an item is calculated by modeling users' (e.g., generators in RAG) attention to each item in a ranking. For example, one can assume that the user is affected by position bias and gives"}, {"title": "3.3.2 Expected Utility", "content": "Given the set of N sampled rankings \u2211, we individually augment the generator with each ranking \u03c3\u03b9, resulting in N outputs from the generator. The utility of these outputs is then measured using an arbitrary string utility metric \u03bc\u03b1. \u03a4\u03bf determine the anticipated utility of a RAG model with fair rankings-represented by the tuple of a stochastic ranking sampler S and a generator G-we calculate the expected utility (EU) of the RAG system given an instance x.\nEU((S,G)|x) = E_{\u03c3\u223cs}[\u03bc_u(y, \u0177_\u03c3)] = \\sum_{\u03c3 \\in S_n}p(\u03c3/q)\u03bc\u03b1(y, \u0177_\u03c3) \u2248 \\frac{1}{N} \\sum_{i=1}^{N}\u03bc\u03b1 (\u03c8, \u03b8\u03b5) \\quad(7)\nwhere yo is the prediction of a system given the ranking \u03c3, Sn is the symmetric group of a ranked list L from the deterministic retriever R, and \\sum_{\u03c3 \\in S_n}p(\u03c3|q) = 1."}, {"title": "3.3.3 Normalization of Metrics", "content": "From Equation 3, we decompose the metric into EE-D and EE-R. Since the bounds of these metrics depend on the number of useful items, normalization must be applied per query. Both metrics are min-max normalized based on their theoretical lower and upper bounds. We denote the normalized EE-D and EE-R as EE-Dq and EE-Rq, respectively.\nHowever, theoretically determining the bounds of the expected utility (EU) of a RAG model is challenging. To address this, we normalized the EU by the model's empirical upper bound, the maximum observed utility across all runs of the experiment with the same generators. To approach the true upper bound, these runs include RAG models with an oracle retriever that consistently ranks useful items (i.e., those with positive utility labels) above non-useful ones, stochastically returning one of the m!(n - m)! different rankings, where m represents the number of useful items in the corpus. We denote the normalized EU as EUq, which can be interpreted as the distance to the optimal utility. From this section onwards, we omit the symbol q from the normalized metrics for brevity. Proofs and details on how each metric is normalized by its lower and upper bounds can be found in the Appendix. C."}, {"title": "4 Experiment setup", "content": "We choose the LaMP benchmark [45] for our dataset. It assesses the personalization capability of language models through retrieval-augmentation of users' interaction history in a platform. LaMP includes various prediction tasks, such as classification, regression, and generation, and is well-suited for tasks where multiple items can be relevant/useful, unlike QA tasks with typically one or two provenance items. The retrieval items in LaMP have clear providers and consumers, aligning with"}, {"title": "5 Results", "content": "RQ1: Is there a tradeoff between ensuring item-fairness in rankings and maintaining high ranking quality when utility labels are used for evaluation?\nBy gathering all four repeated runs of the experiments with different a values, we can plot the trend of ranking quality (EE-R) against item fairness (EE-D), as shown in Figure 4.\nAs shown in previous studies [9, 50], there is a well-known tradeoff between fairness and ranking quality for human users. Similarly, we observe a general tradeoff for machine users. However, unlike past findings, this tradeoff is not always strict. For instance, in Figure 4, both SPLADE and Contriever maintain consistently high ranking quality while being considerably fairer, and for BM25, ranking quality even improves as fairness increases, up to a certain point.\nAt the rightmost side of the lines, where EE-D = 1 (representing the performance of deterministic rankers), we observe that these rankers do not always deliver the highest ranking quality. This suggests that commonly used deterministic rankers in RAG systems may be suboptimal, and that ranking quality can be improved while ensuring item fairness. This becomes even clearer when examining the impact of fair ranking on the downstream performance of a RAG system.\nThe leftmost side of the lines, where EE-D = 0, represents the performance of a uniformly random ranking policy. At this point, the measured ranking quality should approximate the proportion of positively labeled items in the corpus, which is 31% based on data statistics. This is notably higher"}, {"title": "RQ2: Is there a tradeoff between ensuring item-fairness in ranking and maintaining high generation quality of a RAG model?", "content": "Before examining the relationship between fairness and RAG utility, Figure 5a shows an auxiliary result confirming a strong correlation between utility-based ranking quality and the effectiveness of RAG models. This is unsurprising, as item-worthiness judgments were based on the utility-gain provided by the generator. However, this correlation suggests that the tradeoff observed in the disparity-ranking quality curve (Figure 4) is likely to manifest similarly due to this strong relationship.\nIn fact, as observed from the disparity-utility curve (Figure 5b), we see a global trend of a non-strict tradeoff (i.e., RAG models maintain high generation quality while being considerably fair, and often even achieve higher quality).\nHowever, a closer look at the local trend offers a significant insight: RAG systems with fair ranking can often achieve higher system-effectiveness compared to models with deterministic rankers. In Table 1, we divided the fairness levels into five intervals based on the normalized EE-D. As shown in the table and Appendix I, improving fairness to the level of EE-D \u2208 [0.8, 1.0), and even EE-D \u2208 [0.6, 0.8), can often enhance the utility of many RAG models across most LaMP tasks. For example, having EE-D in the range of [0.8, 1.0) outperforms the baseline for all models in LaMP-2 and for seven out of nine models in LaMP tasks 4, 5, and 6."}, {"title": "6 Discussion and Conclusion", "content": "Why do we often see higher system utility in fairer RAG models? Although there is a general trend of a fairness-utility tradeoff, we observe that certain levels of fairness can actually improve the utility of a baseline RAG model. Recent line of research have uncovered relevant findings: 1) generators are not robust to changes in the position of useful information [29]; 2) items with high retrieval scores often include distracting content that can reduce the system-effectiveness [8, 40]; and 3) introducing some random documents can significantly boost the utility of RAG [8].\nBuilding on these existing results, we find that perturbing the initial ranking through stochastic sampling often can impact the performance of certain inference decisions and lead to changes in the system's expected end-performance. In our experiments, we observe that the expected utility generally increases within the fairness interval of [0.8, 1.0). This suggests that a fixed ranking from a deterministic ranker may be suboptimal for the generator, and that perturbing the ranking, along with the repositioning of items, not only improves expected end-performance but also enhances the fairness of the rankings.\nMoreover, in fairness intervals where the system's expected utility improves, it is possible that either fewer distracting items were included in the ranking passed to the generator or useful, previously overlooked items (which may have been considered random) were introduced due to the ranking perturbation. However, while higher utility paired with increased item-fairness (even within fairness intervals as low as [0.4, 0.6)) may seem advantageous, practitioners should exercise caution. This could result in compensating providers of items irrelevant to user requests, particularly in scenarios where content providers are rewarded for contributing to inference outcomes.\nMachine-user browsing model. Developing more sophisticated machine-user browsing models will result in more consistent and accurate evaluations of item-side fairness in RAG models, as the exposure of each item is influenced by the attention allocated by the machine-user. Initial research can draw inspiration from Liu et al. [29], who found that LLMs tend to allocate more attention to the beginning and end of a context, with less focus on the middle. This line of inquiry aligns with the broader effort to create search engines tailored for machine-users [43], specifically focusing on fairer search engines in this context. It should involve studying how LMs attend to each retrieved result within a context, analogous to how traditional search engine research models human browsing behavior [10].\nMeasurement of string utility. In line with the recent call for evaluating various valid output strings [57], we recognize the need for a similar approach to better measure system utility across different rankings given. Recall that our experiments were designed to provide the generator with different rankings for the same query, leading to varied outputs. This approach is motivated by the idea that items not appearing in the top positions of deterministic rankings may still hold value and should be fairly considered by the system. In this context, the diverse outputs generated from different rankings may still be valid. However, we currently rely on a single target output string for comparison with predictions. Future work could focus on calculating the utility of diffuse predictions, enabling a more nuanced evaluation.\nLimitations. We acknowledge that the evaluation cost of fair RAG systems can be high due to repeated sampling and inference steps. However, in production, only a single ranking is sampled, minimizing the impact on system latency (Appendix E). Also, a limitation in our utility labeling is that it considers single items, while multiple items may yield contrasting utility gains. Despite this, the strong correlation between ranking quality and system effectiveness suggests this approach reasonably approximates item-worthiness for evaluating the impact of fair ranking on RAG systems.\nConclusion. This study highlights the impact of fair rankings on both the ranking and generation quality of RAG systems. Through the extensive analysis, we show that fairer RAG systems not only maintain high generation quality but can also outperform traditional RAG models, challenging the notion of a strict tradeoff between fairness and effectiveness. Our findings provide valuable insights for developing responsible and equitable RAG systems and pave the way for future research in fair ranking and retrieval-augmented generation. We encourage future researchers to extend this work by incorporating graded or missing judgments and exploring the different notions of fairness in RAG systems, and ultimately advancing the field of trustworthy RAG systems research."}]}