{"title": "Is F\u2081 Score Suboptimal for Cybersecurity Models? Introducing Cscore, a Cost-Aware Alternative for Model Assessment", "authors": ["Manish Marwah", "Asad Narayanan", "Stephen Jou", "Martin Arlitt", "Maria Pospelova"], "abstract": "The cost of errors related to machine learning classifiers, namely, false positives and false negatives, are not equal and are application dependent. For example, in cybersecurity applications, the cost of not detecting an attack is very different from marking a benign activity as an attack. Various design choices during machine learning model building, such as hyperparameter tuning and model selection, allow a data scientist to trade-off between these two errors. However, most of the commonly used metrics to evaluate model quality, such as F\u2081 score, which is defined in terms of model precision and recall, treat both these errors equally, making it difficult for users to optimize for the actual cost of these errors. In this paper, we propose a new cost-aware metric, $Cscore$, based on precision and recall that can replace F\u2081 score for model evaluation and selection. It includes a cost ratio that takes into account the differing costs of handling false positives and false negatives. We derive and characterize the new cost metric, and compare it to F\u2081 score. Further, we use this metric for model thresholding for five cybersecurity related datasets for multiple cost ratios. The results show an average cost savings of 49%.", "sections": [{"title": "I. INTRODUCTION", "content": "Applications of machine learning in cybersecurity are widespread and rapidly growing, with models being deployed to prevent, detect, and respond to threats such as malware, intrusion, fraud, and phishing. The main metric for assessing the performance of classification models is F\u2081 score [1], also known as F\u2081 measure, which is the harmonic mean of precision and recall.\nWhile F\u2081 score is used for assessing models, it is not directly used as a loss function since it is not differentiable (or convex). A simpler and commonly used approach is a two-stage optimization process. First, a model is trained using a conventional loss function such as cross-entropy, and then an optimal threshold is selected based on F\u2081 score [2].\nF1 score works particularly well in highly imbalanced settings prevalent in cybersecurity. However, it treats both kinds of errors a machine learning classifier can make \u2013 false positives (FPs) and false negatives (FNs) \u2013 equally. Usually, the cost of these errors is unequal and depends on the application context. For example, in cybersecurity applications, while both these errors can have severe negative consequences, one error might be preferred over the other. Specifically, false positives lead to alarm fatigue, a phenomenon where a high frequency of false alarms causes operators to ignore or dismiss all alarms. This problem is often exacerbated by base rate fallacy, where people underestimate the potential volume of false positives due to a high true positive rate while ignoring a low base rate 1. False negatives, on the other hand, imply that a vulnerability or attack has gone undetected. While ideally one would want to minimize both these errors, in practice there is a trade-off between the number of FPs and FNs. An organization, based on its goals and requirements may assign differing costs to these errors. For example, for a ransomware detection model the damage caused by a FN may be several orders of magnitude greater than the cost of a security analyst handling a FP; while for other models, e.g., port scanning detection, the costs may be similar or even higher for handling a FP. By using F\u2081 score such cost considerations are usually ignored\u00b2. So a natural question to ask is: given the cost difference (or ratio) between the consequences of a FP and a FN for an particular use case, how can an organization incorporate that information while building machine learning models for that application?\nThere is considerable prior work in cost-sensitive learning [8]\u2013[12], [16]. These aim to modify the model learning process, e.g., by altering the loss function to incorporate cost, adding weights to the training samples, or readjusting class frequencies in the train set such that the trained model intrinsically produces results that are cost sensitive. In this paper, we do not change the underlying learning process and instead propose a new cost-aware metric as a replacement of F\u2081 score that can be used for model thresholding, comparison"}, {"title": "II. RELATED WORK", "content": "While F\u2081 score is preferred to any one of accuracy, precision, or recall, especially for an imbalanced dataset, its primary drawback in our context is that all misclassifications are considered equal [3]. The other drawbacks [4], [5] include 1) lack of symmetry with respect to class labels, e.g., changing the positive class in a binary classifier produces a different result; and, 2) no dependence on true negatives. A more robust though not as popular alternative addressing some of these problems while still working well for imbalanced datasets is the Matthew Correlation Coefficient (MCC) [7], which in many cases is preferred to F\u2081 score [6]. It is symmetric and produces a high score only if all four confusion matrix entries (see Table II) show good results [6]. However, it treats FPs and FNs equally. Unlike F\u2081 score and MCC, our proposed metric is not symmetric with respect to FNs and FPs, taking their distinct impacts into consideration through a cost ratio. Further, like MCC but unlike F\u2081 score, our metric is symmetric in the treatment of true positives and true negatives. Our metric is not normalized like MCC and F\u2081 score, and varies between 0 (best) and \u221e (worst). This does not impact model thresholding, or comparison, however, the actual value of the cost metric in itself is not very meaningful, but can be converted to the corresponding recall and precision values. Since neither MCC nor F\u2081 score considers differing costs of errors, and the latter is more widely used, we compare our proposed metric with F\u2081 score in the rest of the paper."}, {"title": "B. Cost sensitive learning and assessment", "content": "Since in real-world applications cost differences between types of errors can be large, cost-sensitive machine learning has been an active area of research since the past few decades [8]\u2013[11], especially in areas such as security [14], [15] and medicine [13]. For example, Lee et al. [14] proposed cost models for intrusion detection; Liu et al. [15] incorporate cost considerations both in feature selection and classification for software defect prediction. Some of this and similar work could be used to estimate cost ratios for our proposed cost metric.\nAt a high-level, cost-sensitive machine learning [16] can be categorized into two different approaches: 1) where the machine learning methods are modified to incorporate the unequal costs of errors; and, 2) where existing machine learning models trained with cost oblivious methods are converted into cost-sensitive ones using a wrapper [10], [11]. In this paper, we focus on the second approach, which is also referred to as cost-sensitive meta learning. While there are various methods to implement this approach, we will focus on thresholding or threshold adjusting [11], where the decision threshold of a probabilistic model is selected based on a cost function. Sheng et al. [11] showed that thresholding outperforms several other cost sensitive meta learning methods such as MetaCost [10].\nIn the most general case, the cost function for thresholding can be constructed from the entries of a confusion matrix with a weight attached to each of them, that is, FPs, FNs, TPs and TNs [12]. Our proposed cost metric uses a similar formulation, however, it is expressed in terms of precision and recall, metrics that data scientists already know well and understand. We are not aware of any existing cost metric defined in terms of precision, recall, and a cost ratio. Unlike F1 score or MCC, the proposed metric is directly proportional to the total cost of misclassification. We believe it can serve as a cost-aware replacement for F\u2081 score or MCC."}, {"title": "III. PROPOSED METRIC: COST SCORE", "content": "While the proposed metric is applicable to any machine learning classification model, including multiclass and multi-label settings, for simplicity we will assume a binary classification task in the following discussion. The notation used is summarized in Table I.\nStarting with the cost of misclassifications, we derive expressions for cost score that can replace F\u2081 score. In particular, we derive two equivalent expressions one in terms of TPR (recall) and FPR; the other in terms of precision and recall. They both include an error cost ratio ($r_c$), which is a ratio between the cost of a FN to that of a FP. The first one is dependent on the base rate ($P(V)$), while the second, similar to F1-score, does not directly depend on it.\nThe basic evaluation metrics for a binary classifier can be defined from a confusion matrix, shown in Table II. One can also look at a confusion matrix from a probabilistic perspective, where the four possible outcomes define a probabilistic space, with each outcome a joint probability, as shown in Table"}, {"title": "III. The total probability along a row or a column are the corresponding marginal probabilities.", "content": "$P(A, \u00acV)$ and $P(\u00acA, V)$ represent the probability of false positives and false negatives, respectively. Thus, their number can be expressed as:\n$N_{FP} = N \\cdot P(A, \u00acV)$\n$N_{FN} = N \\cdot P(\u00acA, V)$\nMultiplying by the corresponding costs gives us the total cost of errors:\n$C = C_{FP} \\cdot N_{FP} + C_{FN} \\cdot N_{FN}$\n$= C_{FP} \\cdot N \\cdot P(A, \u00acV) + C_{FN} \\cdot N \\cdot P(\u00acA, V)$\nFactoring out the common terms and introducing $r_c$ gives:\n$C = C_{FP} \\cdot N(P(A, \u00acV) + r_c \\cdot P(\u00acA, V))$   (1)\n$= K \\cdot [P(A, \u00acV) + r_c \\cdot P(\u00acA, V)]$  (2)"}, {"title": "B. Cost score in terms of TPR and FPR", "content": "Here we model the cost function in terms of TPR (recall) and FPR. Data scientists frequently evaluate a model in terms of TPR, which corresponds to the fraction of positive cases detected and FPR, which is the fraction of the negatives that were misclassified as positives. In fact, an ROC curve (a plot between TPR and FPR) is widely used for thresholding a probabilistic classifier.\nUsing the product rule, we rewrite the probability distribution for false positives in terms of FPR and P(V).\n$P(A, \u00acV) = P(A|\u00acV) \\cdot P(\u00acV)$   (3)\n$= FPR \\cdot (1 \u2013 P(V))$   (4)\nSimilarly, we rewrite the joint distribution for false negatives in terms of TPR and P(V).\n$P(\u00acA, V) = P(\u00acA|V) \\cdot P(V)$   (5)\n$= (1 \u2013 P(A|V)) \\cdot P(V)$   (6)\n$= (1 \u2013 TPR) \\cdot P(V)$  (7)\nSubstituting 4 and 7 in the cost expression, 2, and rearranging, we get:\n$C = K \\cdot [FPR + P(V)(r_c - r_c \\cdot TPR \u2013 FPR)]$\nTo minimize the cost, we can ignore K, and thus the cost score becomes:\n$C_{score} = FPR + P(V) \\cdot (r_c - r_c\\cdot TPR - FPR)$ (8)"}, {"title": "C. Cost score in terms of precision and recall", "content": "While FPR is a useful metric as it captures the number of false positives, it can be tricky to understand, especially when the base rate, P(V), is low, which is usually the case in cybersecurity problems. For problems such as intrusion or threat detection, FPs add overhead to the workflow of a security analyst. For phishing website detection, a FP may result in a website being blocked in error for an end user. In either case, setting a target FPR requires knowledge of the base rate and would change as the base rate changes. In other words, even a seemingly low FPR may not be good enough, given a low base rate. Further, variance in base rate would affect overhead of a security analyst in case of intrusion detection or the fraction of erroneously blocked websites for a user in case of phishing detection even if the FPR stays constant. Precision on the other hand directly captures the operator overhead or fraction of erroneously blocked websites independent of the base rate.\nA main attraction of F\u2081 score is its use of precision instead of FPR. When the costs of FP and FN are similar, F\u2081 score is an effective evaluation metric, however, with unequal costs of misclassification, we can usually find a better solution by incorporating this cost differential in the metric. Below, we derive an expression for $C_{score}$ in terms of precision and recall, similar to F\u2081 score, but that includes a cost ratio.\nWe can rewrite the probability of a false positive in terms of precision (Prec) and marginal probability of alarm.\n$P(A, \u00acV) = P(\u00acV|A) \\cdot P(A)$  (9)\n$= (1 \u2013 Prec) \\cdot P(A)$ (10)\n$P(A)$ can be expressed in terms of P(V), Prec and R (recall) using Bayes rule:\n$P(V|A) \\cdot P(A) = P(A|V) \\cdot P(V)$\n$P(A) = \\frac{P(A|V)}{P(V|A)} \\cdot P(V)$\n$= \\frac{R}{Prec} \\cdot P(V)$\nSubstituting this value of P(A) in Equation 10, we get:\n$P(A, \u00acV) = \\frac{1- Prec}{Prec} R \\cdot P(V)$ (11)\nAs in the previous section (Equation 7), the probability of a false negative can be written as:\n$P(\u00acA, V) = (1 \u2013 R) \\cdot P(V)$ (12)\nTherefore, substituting the values of probabilities of a false positive and a false negative from Equations 11 and 12, respectively, into the cost expression (Equation 1), we get\n$C_{score} = N\\cdot C_{FP}\\cdot P(V) \\cdot [\\frac{1-Prec}{Prec} R + r_c(1-R)]$\nSince N, CFP and P(V) are constant for a given dataset, we can rewrite the cost expression as:\n$C_{score} = (\\frac{1}{Prec} - 1) \\cdot R + r_c(1 \u2013 R)$ (13)"}, {"title": "D. Cscore Isocost Contours", "content": "To better understand the cost score metric, we will examine its dependence on precision and recall, and compare it with F1 score. Figure 1 shows a precision-recall (PR) plot with F\u2081 score isocurves or contours. Each curve corresponds to a constant value of F\u2081 score as specified next to the curve. If recall and precision are identical, F\u2081 score computes to the same value. However, if there is a wide gap between them, F\u2081 tends to be closer to the lower value, as can be seen in the top-left and bottom-right regions of the plot. As expected, the highest (best) value contours are towards the top-right corner of the plot (that is, towards perfect recall and precision). Further, the slope of the curves is always negative (as shown in Appendix A), implying there is always a trade-off between recall and precision."}, {"title": "E. How does F\u2081 score Compare with Cscore when rc = 1?", "content": "While F1-score varies from 0 to 1, with 1 indicating perfect performance, Cscore is proportional to the actual cost of handing model errors, with a zero-cost indicating perfect performance (that is, no FPs or FNs). F\u2081 score treats FNs and FPs uniformly, as does Cscore when rc = 1. So a natural question is if Cscore differs from F\u2081 score when rc = 1? To compare, we transform F\u2081-score to a cost metric:\n$F1_{cost} = \\frac{1}{F} -1$ (15)\nWhen F\u2081 is 1, $F1_{cost}$ = 0, and when F\u2081 is 0, $F1_{cost}$ \u2192 \u221e, and thus exhibits behavior of a cost function and can be directly compared with Cscore.\nTo compare Cscore and F1cost, we reduce both in terms of the elements of the confusion matrix and find that:\n$F1_{cost} \\propto \\frac{FP+FN}{TP}$ (16)\n$C_{score} \\propto FP + FN$ (17)\nThus, when rc = 1, Cscore and $F1_{cost}$ are not identical; while Cscore is proportional to the total number of errors, $F1_{cost}$ is also inversely proportional to the number of true positives. Cscore only considers the cost of errors; it assigns zero cost to both TPs and TNs. In that sense, it treats TP and TN symmetrically unlike F\u2081 score."}, {"title": "F. Multiclass and multilabel classifiers", "content": "While we derived the cost metric assuming a binary classification problem, its extension to multiclass and multilabel classification problems is straightforward. A cost ratio per class would need to be defined. For a multiclass classifier, a user would have to assign cost ratios considering each class as positive and the rest as negative. Similarly, for a multilabel classifier, a user would have to assign independent ratios for each class. This will allow a Cscore to be computed per class. To compute a single cost metric, the per-class Cscore would need to be aggregated. The simplest aggregation function is an arithmetic mean, although a class weighted mean based on class importance, or another type of aggregation, e.g., a harmonic mean, can also be performed. The \"one class versus the rest\" approach is similar to how F\u2081 score and other metrics are computed in a multiclass setting."}, {"title": "G. Minimizing Cscore based on model threshold and other hyperparameters", "content": "In Section III-D, we described use of isocost contours to visually determine the lowest cost point on a PR curve. In practice, to find the minimum cost based on model threshold, and the corresponding precision-recall values, precision and recall can be considered functions of the threshold value (t) with the optimal value of t determined by minimizing the cost function with respect to t.\n$t = argmin_t[ (\\frac{1}{Prec(t)}-1) R(t) + r_c \\cdot (1 \u2013 R(t))]$\nIn addition to model threshold, Cscore can also be used for selecting other model hyperparameters such as the number of neighbors in k-NN; number of trees, maximum tree depth, etc. in tree based models; number and type of layers, activation functions, etc. in neural networks; and for model comparison and selection. Hyperparameter tuning [22] is typically performed using methods such as grid search, random search, gradient-based optimization, etc. Typically, cross-validation is used in conjunction to evaluate the quality of a particular choice of a dataset. In all these methods, the proposed Cscore can replace another cost-oblivious metric such as F\u2081 score."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "The datasets used for experiments were chosen based on their relevance to security and the varying cost of misclassification between target classes. To comprehensively analyze the impact of costs, we selected five different datasets, four publicly available datasets and one privately collected dataset. The publicly available datasets include the UNSW-NB15 intrusion detection data, KDD Cup 99 network intrusion data, credit card transaction data, and phishing URL data."}, {"title": "A. Datasets", "content": "focus on a binary classification task: class 0 represents normal instances, while class 1 aggregates all other attack types. Also, to explore the impact of different thresholds on the model's performance, training was conducted using only 1% of the dataset. The dataset is accessed through the datasets available in the Python sklearn package [21]."}, {"title": "B. Experiment Setup", "content": "We train a classification model using a RandomForest algorithm for each dataset. The goal is not to train the best possible model for the dataset but to obtain a reasonably good model with a probabilistic output.\nThe steps are as follows:"}, {"title": "C. Results", "content": "The proposed cost metric used is tailored for enhancing the performance of machine learning models in scenarios where the cost of false negatives greatly differs from the cost of false positives. This in turn helps in optimizing predictions based on cost considerations, thereby addressing a critical limitation in existing evaluation methods."}, {"title": "V. CONCLUSIONS", "content": "How organizations handle errors from machine learning models is highly dependent on context and application. In the cybersecurity domain, the cost of a security analyst's time and effort spent in reviewing and investigating a false positive varies considerably from the cost of a model's failure to detect a real security incident (a false negative). However, widely used metrics like F\u2081 score assign them equal costs. In this paper, we derived a new cost-aware metric, Cscore defined in terms of precision, recall, and a cost ratio, which can be used for model evaluation and serve as a replacement for F1 score. In particular, it can be used for thresholding probabilistic classifiers to achieve minimum cost. To demonstrate the effectiveness of Cscore in cybersecurity applications, we applied it to threshold models built on five different datasets assuming multiple cost ratios. The results showed substantial savings in cost through the use of Cscore over F\u2081 score. At cost ratio 1, the results are similar, however, as the cost ratio is increased or decreased, the gap in costs between using Cscore and F1 score increases. All datasets show consistent improvements in cost. Through this work, we hope to raise awareness among machine learning practitioners building cybersecurity applications regarding the use of cost-aware metrics such as Cscore instead of cost-oblivious ones like F\u2081 score."}, {"title": "APPENDIX", "content": "F\u2081 score is defined as:\n$F_1=\\frac{2 \\cdot Prec \\cdot R}{Prec \\cdot R}$\nRearranging:\n$Prec = \\frac{F_1R}{2R-F_1}$"}, {"title": "A. Slopes of the F\u2081 score and Cscore isocurves", "content": "Slope of F\u2081 isocurves can be calculated as:\n$\\frac{\\partial Prec}{\\partial R} = \\frac{2RF_1}{2R - F_1}= \\frac{2RF_1}{(2R-F_1)^2} - F_1$ thus, slope of F\u2081 isocurves is always negative.\nCscore is defined as:\n$C_{score} = (\\frac{1}{Prec} - 1) \\cdot R + r_c$ it can be rearranged as:\n$Prec = \\frac{R}{C_{score} + R(r_c + 1) - r_c}$\nSlope of the Cscore isocurves can be computed as:\n$\\frac{\\partial Prec}{\\partial R} = \\frac{(r_c+1) R}{C_{score} + R(r_c+1) - r_c} = (C_{score} + R(r_c + 1) - r_c)^2 $\n$(C_{score} + R(r_c + 1) \u2013 rc)^2$\nAs described in Section III-D, these curves can have negative, positive or zero slopes depending on the value of $C_{score}$."}]}