{"title": "DIFFERENTIATION AND SPECIALIZATION OF ATTENTION HEADS VIA THE REFINED LOCAL LEARNING COEFFICIENT", "authors": ["George Wang", "Jesse Hoogland", "Stan van Wingerden", "Zach Furman", "Daniel Murfet"], "abstract": "We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for developmental interpretability, which aims to understand models through their evolution across the learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks.", "sections": [{"title": "Introduction", "content": "Structure in the data distribution has long been recognized as central to the development of internal structure in artificial and biological neural networks (Rumelhart et al., 1986; Olshausen & Field, 1996; Rogers & McClelland, 2004). Recent observations have renewed interest in this topic: language models progress through distinct stages of development during training, acquiring increasingly sophisticated linguistic and reasoning abilities in ways that seem to reflect the structure of the data distribution (Olsson et al., 2022; Chen et al., 2024; Belrose et al., 2024; Tigges et al., 2024; Edelman et al., 2024; Hoogland et al., 2024).\n\nA deeper understanding of how structure in the data determines internal structure in trained models requires tools that provide information about which components of a model are being shaped in response to what structure in the data distribution. Our foundation for the study of such questions begins with the local learning coefficient (LLC; Lau et al. 2023) from singular learning theory (SLT; Watanabe 2009), which is a measure of model complexity. In this paper, we introduce the refined local learning coefficient (rLLC), which measures the complexity of a component of the model with respect to an arbitrary data distribution.\n\nWe focus mainly on the rLLCs of individual attention heads and demonstrate the utility of these metrics in studying the progressive differentiation and specialization of heads. The diversity of attention heads at the end of training has been established in recent years through mechanistic interpretability, which has provided numerous examples of attention heads that appear to have specialized functions, including previous-token heads (Voita et al., 2019; Clark et al., 2019) and induction heads (Olsson et al., 2022) among other kinds (Wang et al., 2023; Gould et al., 2024). In this paper, we"}, {"title": "Setup", "content": "Following Elhage et al. (2021), Olsson et al. (2022), and Hoogland et al. (2024), we study two-layer attention-only (without MLP layers) transformers (architecture and training details in Appendix F) trained on next-token prediction"}, {"title": "Methodology", "content": "The (global) learning coefficient \\(\\lambda\\) is the central quantity in singular learning theory (Watanabe, 2009). In this section we review the local learning coefficient (LLC) from Lau et al. (2023) before defining the refined variants that are new to this paper. The LLC at a neural network parameter \\(w^*\\), denoted \\(\\lambda(w^*)\\), is a positive scalar measuring the degeneracy of the geometry of the population loss \\(l\\) near \\(w^*\\). The geometry is more degenerate (lower LLC) if there are more ways in which \\(w\\) can be varied near \\(w^*\\) such that \\(l(w)\\) remains equal to \\(l(w^*)\\)."}, {"title": "LLC In Practice", "content": "In the setting of Section 2, where we have a compact parameter space \\(\\mathcal{W}\\), a model with parameter \\(w\\) of the conditional distribution of outputs \\(y\\) given inputs \\(x\\) (in our case a transformer neural network with weights \\(w\\) parametrizes predictions of next-tokens \\(y\\) given contexts \\(x\\)), and samples \\(\\mathcal{D}_n\\) from a true distribution with associated empirical loss \\(l_n\\), we define the estimated local learning coefficient at a neural network parameter \\(w^*\\) to be\n\n\\begin{equation}\n\\hat{\\lambda}(w^*) = \\frac{\\eta \\beta}{n} \\left[ \\mathbb{E}_{\\substack{w \\sim w^*, \\beta, \\gamma} } \\left[l_n(W)\\right] - l_n(w^*)\\right],\n\\end{equation}\nwhere \\(\\mathbb{E}_{\\substack{w \\sim w^*, \\beta, \\gamma} }\\) is the expectation with respect to the Gibbs posterior (Bissiri, 2016)\n\n\\begin{equation}\np(w; w^*, \\beta, \\gamma) \\propto \\exp \\left\\{-n\\beta l_n(w)- \\frac{\\gamma}{2}||w-w^*||_2^2\\right\\}.\n\\end{equation}\nThe hyperparameters are the sample size \\(n\\), the inverse temperature \\(\\beta\\) which controls the contribution of the loss, and the localization strength \\(\\gamma\\) which controls proximity to \\(w^*\\). For a full explanation of these hyperparameters the reader is referred to Watanabe (2013); Lau et al. (2023); Furman & Lau (2024); Hoogland et al. (2024). Further, the expectation is approximated by using stochastic-gradient Langevin dynamics (SGLD; Welling & Teh, 2011) which introduces additional hyperparameters such as the step size; see Appendix F.2 for the settings used in this paper.\n\nIntuitively, the quantity in (2) represents the typical deviation in empirical loss \\(l_n(w) \u2013 l_n(w^*)\\) under perturbations away from \\(w^*\\) that are likely according to a local tempered posterior distribution. For more theoretical insight into this intuition see Appendix A."}, {"title": "Refined LLC", "content": "The LLC \\(\\lambda(w^*)\\) depends on the parameter space and the true distribution. If we view some directions in parameter space at \\(w^*\\) as fixed and view the model as a function of the remaining directions we obtain the weight-refined LLC. If we instead allow the true distribution to vary we obtain the data-refined LLC. We now explain both in more detail."}, {"title": "Weight- and data-refined LLC (wdrLLC)", "content": "Let \\(q'\\) be a data distribution, not necessarily the training distribution \\(q\\), and let \\(l'\\) and \\(l'_n\\) be the corresponding population and empirical loss. Given a product decomposition \\(\\mathcal{W} = \\mathcal{U} \\times \\mathcal{V}\\) corresponding to choosing a set of weights \\(\\mathcal{V}\\) belonging to a particular component of the model (with \\(\\mathcal{U}\\) denoting the rest of the weights), with associated decomposition of the parameter \\(w^* = (u^*, v^*)\\), we let \\(\\mathcal{B}\\) be a neighborhood of \\(v^*\\) small enough that \\(l'(u^*, v) > l'(u^*, v^*)\\) for all \\(v \\in \\mathcal{B}\\) and define\n\n\\begin{equation}\n\\text{vol}(t, w^*, \\mathcal{V}, q') = \\int_{\\substack{v\\in \\mathcal{B}, |l'(u^*,v)-l'(u^*,v^*)|<t}} dv.\n\\end{equation}\nThe weight- and data-refined LLC is\n\n\\begin{equation}\n\\lambda(w^*; \\mathcal{V}, q') = - \\lim_{t\\to 0^+} \\log_2 \\left[\\frac{\\text{vol}(t, w^*, \\mathcal{V}, q')}{\\text{vol}(t, w^*, \\mathcal{V}, q')}\\right].\n\\end{equation}\nThe associated estimator \\(\\hat{\\lambda}(w^*; \\mathcal{V}, q')\\) is defined by modifying (2) as follows: the expectation \\(\\mathbb{E}_{\\substack{w \\sim w^*, \\beta, \\gamma} }\\) is replaced by the expectation with respect to a Gibbs posterior defined over \\(\\mathcal{V}\\) by\n\n\\begin{equation}\np(v; v^*, \\alpha', \\beta, \\gamma) \\propto \\exp \\left\\{-n\\beta l_n(u, v) \u2013 \\frac{\\gamma}{2}||v - v^*||_2^2\\right\\}.\n\\end{equation}\nIn practice, the estimator is implemented by projecting the SGLD update steps used to produce approximate posterior samples onto \\(\\mathcal{V}\\) and computing both SGLD updates and average posterior loss using samples from \\(q'\\). For further background on the theoretical definition in (5) see Appendix A.\n\nWhen \\(q' = q\\), we suppress \\(q\\) and refer to this as the weight-refined LLC (wrLLC) \\(\\hat{\\lambda}(w^*; \\mathcal{V})\\), and when \\(\\mathcal{V} = \\mathcal{W}\\), we suppress \\(\\mathcal{V}\\) and refer to this as the data-refined LLC (drLLC) \\(\\hat{\\lambda}(w^*; q')\\). When both \\(q' = q\\) and \\(\\mathcal{V} = \\mathcal{W}\\), we recover the original LLC \\(\\lambda(w^*)\\)."}, {"title": "Limitations", "content": "There are numerous limitations to LLC estimation in its present form, including:\n\n\u2022 The justification of the LLC estimator \\(\\hat{\\lambda}(w^*)\\) presumes that \\(w^*\\) is a local minima of the population loss but there is no clear way to ascertain this in practice, and we typically perform LLC estimates during training where this is unlikely.\n\n\u2022 Accurate estimates can be achieved in some cases where we know the true LLC (Furman & Lau, 2024), but in general, the ground truth LLC is unknown. As such, we cannot guarantee the accuracy of estimated LLC values in transformer models, but we do have reason to believe that the ordinality is correct, e.g. that if the wrLLC estimates of two attention heads are in a particular order, then this is also true of the underlying true wrLLCs.\n\nWe expect SGLD-based LLC estimation to mature as a technique. In the meantime, a series of papers (Lau et al., 2023; Chen et al., 2023; Furman & Lau, 2024; Hoogland et al., 2024) have demonstrated that despite these limitations, the estimated LLC does in practice seem to offer a useful signal for studying neural network development. In the appendix, we compare our analysis using rLLCs against Hessian-based methods (Appendix D) and ablation-based methods (Appendix E). Some analysis is given for other seeds (Appendix G)."}, {"title": "Empirical Results", "content": ""}, {"title": "Differentiation via weight-refined LLCs", "content": "The weight-refined LLC for an attention head is a measure of the amount of information needed to specify a configuration of the weights in the head which achieves a certain relative improvement in the loss (see Section 3.2). Thus we should expect complexity to be the principal axis along which the wrLLC differentiates attention heads. However, a priori it is not obvious how the complexity as measured by the wrLLC should relate to other properties of the attention heads, such as the classification of heads by their functional behavior or the number of multigrams that they memorize.\n\nOur first key contribution, contained in Figure 1 and Figure 2, is to show that there is in fact a very natural relation between the wrLLC and these other axes of differentiation.\n\nAs explained in detail in Appendix B, we classify attention heads as previous-token heads (resp. current-token heads) if they strongly and systematically attend to the previous token (resp. current token). Induction heads are identified as"}, {"title": "Specialization via data-refined LLCs", "content": "In the previous section we saw that the weight-refined LLC reveals the differentiation of heads into functional types, which are useful for prediction on different kinds of patterns (e.g. induction patterns vs. multigrams). We now demonstrate how further refining the LLC measurements by changing the data distribution (such as to one more heavily featuring certain kinds of patterns) provides additional information on model components specializing to particular patterns in the data.\n\nIf we think of the weight-refined LLC \\(\\lambda(w^*; \\mathcal{V})\\) as a measure of the information in \\(\\mathcal{V}\\) about all patterns in the pre-training distribution, then the simultaneous weight- and data-refinement \\(\\lambda(w^*; \\mathcal{V}, q')\\) measures the information about the subset of those patterns that occur in a subdistribution \\(q'\\).\n\nFor example, when a head \\(\\mathcal{V}\\) is specialized to multigrams that are uncommon in code we predict that \\(\\lambda(w^*; \\mathcal{V}, q_{\\text{GitHub}}) < \\lambda(w^*; \\mathcal{V})\\) when \\(q' = q_{\\text{GitHub}}\\) is a distribution of code (CodeParrot, 2023). In the opposite direction, since induction patterns are frequent in code (e.g. repeated syntax, repeated variable names), we expect that \\(\\lambda(w^*; \\mathcal{V}, q_{\\text{GitHub}}) > \\lambda(w^*; \\mathcal{V})\\) when \\(\\mathcal{V}\\) is an induction head."}, {"title": "A new multigram circuit", "content": "A multigram of length m is a common sequence of tokens \\(t_1, t_2,..., t_m\\) in the data distribution, where the tokens may appear non-contiguously in context (Shen et al., 2006). In this paper, multigrams are typically of length 3 or 4 and often do involve consecutive tokens. It is well-known that transformers can implement the prediction of bigrams (\\(m = 2\\)) using the embedding and unembedding layers, and the prediction of skip-trigrams (a subset of \\(m = 3\\) multigrams) using individual attention heads (Elhage et al., 2021). However, the prediction of more complex multigrams may require coordination between attention heads in different layers. In this section, we explain how we used refined LLCs to investigate this coordination and provide evidence for a new circuit involved in multigram prediction.\n\nAs noted in Hoogland et al. (2024) and revisited in Figure 13, two-layer attention-only transformers pass through consecutive developmental stages where they behave like zero- and one-layer transformers. It is around stage LM3 that the behavior of the two-layer transformer starts to diverge from that of a one-layer transformer. Thus, if it exists, coordination between heads for complex multigram prediction is likely to emerge in LM3.\n\nThe development of such coordination might require \"displacing\" learned structure for the prediction of simpler multigrams. To test this hypothesis, we investigated how the information in attention heads about simple multigrams changes over training by using data-refined LLCs with a one-layer transformer as the generating process for the data distribution (see Appendix F.3). These drLLCs are shown in Figure 5 and compared with the wrLLCs for the full pre-training distribution. In line with our expectations, we see that the two sets of LLCs are similar early in training and start to diverge around LM3, which we interpret as a relative decrease across all attention heads of the information about simple multigrams that can be predicted by the one-layer model.\n\nIf we examine the cluster of heads with the largest relative decrease, we see some examples (e.g. the previous-token, current-token and induction heads, classified as such according to their behavior at the end of training) for which the"}, {"title": "Related Work", "content": "Data distributional structure. It is clear that structure in the data distribution plays a significant role in the kinds of structures learned in neural networks and how they are learned (Rumelhart et al., 1986; Olshausen & Field, 1996; Rogers & McClelland, 2004). For instance, properties of the data distribution have been linked to the emergence of in-context learning by Chan et al. (2022b), and Belrose et al. (2024) note that networks learn lower-order moments before higher-order ones.\n\nIn the experiments and accompanying theory of Rogers & McClelland (2004, p.103, p.169), \u201cwaves of differentiation"}, {"title": "Discussion", "content": "In this paper we have introduced the refined LLCs (rLLCs) as a principled tool for understanding internal structure in neural networks, and shown that this tool can be used to study the differentiation and specialization of attention heads over training. This builds on the theoretical foundations of (Watanabe, 2009), the introduction of the ordinary LLC (Lau et al., 2023) and recent results showing that changes in the LLC over training reflect developmental stages Hoogland et al. (2024).\n\nThis section puts these contributions into the broader context of the science of deep learning and interpretability. From a structural point of view, the problem of interpretability for neural networks is to understand internal structure and how it determines the map from inputs to outputs. We take the point of view that this problem cannot be solved in a deep way without first addressing the question: what is the true conception of internal structure in neural networks?\n\nThere is a long tradition in mathematics (Langlands, 1970), computer science (Howard et al., 1980) and physics (Maldacena, 1999; Greene & Plesser, 1996) of understanding the nature of a mathematical object or phenomena by putting it in correspondence or duality with other phenomena. It is therefore interesting to note the literature (reviewed in Section 5) arguing that data distributional structure is an important factor in shaping internal structure in neural networks, and that this structure is further linked to structure in the learning process. To this we may add the singular learning theory perspective, which relates geometric structure of the population loss landscape to the structure of the (singular) learning process (Watanabe, 2009; Chen et al., 2023)."}, {"title": "The synthesis of these perspectives suggests a novel approach to the problem of understanding the fundamental nature of internal structure in neural networks, which is to place the problem in the broader context of studying the correspondence between four categories of structure", "content": "\u2022 Data distributional structure: the inherent patterns and regularities in the data which exist independently of any model (Cristianini & Shawe-Taylor, 2004). In this paper: induction patterns, Dyck patterns and multigrams (Appendix B.1).\n\n\u2022 Geometric structure: the analytic and algebraic geometry of the level sets of the population loss (Watanabe, 2009; Amari, 2016). In this paper: the learning coefficient (or real log canonical threshold, as it is known in geometry).\n\n\u2022 Learning process structure: developmental stages, critical periods, and the sequence in which different capabilities or internal structures emerge (Rogers & McClelland, 2004). In this paper: the overall developmental stages of (Hoogland et al., 2024) and the staggered development of individual attention heads.\n\n\u2022 Computational structure in the model: the functional organization within the neural network itself and computational motifs that emerge during training. In this paper: attention heads, the induction and multigram circuits.\n\nHere by structure we loosely mean the \"arrangement of and relation between parts or elements of something complex\" (McKean, 2005). Since there can be no structure without differentiation of the whole into parts, the foundation of these correspondences is a relation between the \"parts or elements\u201d in each of the four categories. From this perspective, the contribution of the present paper is to begin establishing such a correspondence for two-layer attention-only transformers by using refined LLCs to quantitatively track which components of the model are being shaped in response to what structure in the data distribution. More precisely:\n\n\u2022 In Section 4.1 we related computational structure (the behavioural type of attention heads) to learning process structure and geometric structure as measured by the learning coefficient, by showing that the behavioural type can be recognized by clustering wrLLC curves (Figure 1 and Figure 2).\n\n\u2022 In Section 4.2 we related data distributional structure (the difference between the frequency of certain kinds of induction patterns in code versus natural language) to the differences in geometry between particular induction heads (Figure 3).\n\n\u2022 In Section 4.3 we related data distributional structure (nested brackets in Dyck patterns) to a new multigram circuit whose emergence (a structure in the learning process) seems linked to geometric changes in the layer 0 multigram heads (Figure 5).\n\nFinally, we highlight that many of these results depend on a developmental perspective. We could not confidently cluster attention heads by their weight-refined LLCs without seeing their evolution over the course of training, nor could we so clearly see the connection between induction patterns and performance on code samples without observing changes in the data-refined LLC curves. Even the ablation and composition score analyses of the multigram circuit depend on examining results from different points in training.\n\nThe techniques pioneered in this paper for understanding internal structure in two-layer attention-only transformers can of course be applied to models at a larger scale or with different architecture. We refer to this approach, which combines the refined LLCs from singular learning theory with an emphasis on studying networks over the course of development, as developmental interpretability. Using this set of ideas, we hope to open new paths towards a systematic understanding of advanced Al systems."}, {"title": "A LLC In Theory", "content": "What is not necessarily clear from (2) is that this is an estimator for a theoretical quantity \\(\\lambda(w^*)\\) which is an invariant of the geometry of the population loss. To explain we recall one form of the definition of the learning coefficient from Watanabe (2009). For a triple \\((p, q, \\phi)\\) consisting of a parameter space \\(\\mathcal{W}\\) with model \\(p(y|x, w)\\), truth \\(q(y|x)\\) and prior \\(\\phi\\) on \\(\\mathcal{W}\\) we consider the volume\n\n\\begin{equation}\n\\text{vol}(t) = \\int_{\\{w\\in \\mathcal{W} : K(w)<t\\}} \\phi(w)dw\n\\end{equation}\nwhere \\(K(w) = \\int D_{KL}(q(y|x)||p(y|x,w))q(x)dx\\). Under some conditions (Watanabe, 2009, Theorem 7.1) the learning coefficient is given by\n\n\\begin{equation}\n\\lambda = -\\lim_{t\\to 0} \\log_2 \\left[\\frac{\\text{vol}(t)}{\\text{vol}(t)}\\right].\n\\end{equation}\nThe local learning coefficient \\(\\lambda(w^*)\\) at a local minima \\(w^*\\) of \\(K(w)\\) is defined by restricting the volume integral to a neighborhood \\(\\mathcal{B}\\) of \\(w^*\\) where \\(K(w) \\geq K(w^*)\\)\n\n\\begin{equation}\n\\text{vol}(t, w^*) = \\int_{\\{w\\in \\mathcal{B} : |K(w)-K(w^*)|<t\\}} \\phi(w)dw\n\\end{equation}\nand then defining (Lau et al., 2023; Furman & Lau, 2024)\n\n\\begin{equation}\n\\lambda(w^*) = \\lim_{t\\to 0} \\log_2 \\left[\\frac{\\text{vol}(t, w^*)}{\\text{vol}(t, w^*)}\\right].\n\\end{equation}\nThis is the asymptotic number of bits necessary to specify a parameter near \\(w^*\\) which is half again closer to the truth. This has some relation to intuitive notions of \u201cflatness\u201d (Hochreiter & Schmidhuber, 1997) or description length (Gr\u00fcnwald, 2007).\n\nIn practice we do not have access to the function \\(K\\) since it depends on the true distribution. Nonetheless there are several methods available to estimate these quantities empirically (Watanabe, 2013, 2009), using the negative log-likelihood of a set of samples \\(\\mathcal{D}_n\\). In practice we substitute the population loss \\(l\\) for the KL divergence \\(K\\) and use (2) to approximate the LLC, rather than try to directly approximate (9). Nonetheless this formula offers a valuable information-theoretic interpretation of the LLC that we employ in this paper."}, {"title": "Classification of Attention Heads", "content": "In this section, we classify attention heads by their behavior. At a high level, the attention heads can be thought of as being a part of one of four groups, illustrated in Table 1. Unless specified otherwise, all description of attention heads refers to behaviour at the end of training."}, {"title": "Methodology: Tokens in context", "content": "Identifying tokens in context. To identify which patterns in data each attention head is associated to (independently of rLLCs), we mean-ablate that head (Appendix E), then filter a subset of 100k samples from the training dataset for the 1k most affected \u201ctokens in context (i.e., a sample index combined with a position index), as measured by the increase in the per-token loss. This results in pairs of a token in context and local attention pattern for the ablated attention head that generated that sample."}, {"title": "Classifying tokens in context", "content": "We attempt to classify each (token in context, attention pattern) pair as belonging to one of the following \u201cpatterns:\"\n\n\u2022 Induction patterns (Appendix B.2)\n\n\u2022 Dyck patterns (Appendix B.3)\n\n\u2022 Skip n-grams (Appendix B.4)\n\n\u2022 n-grams (Appendix B.5)\n\nThis classification is automated and serial: if a token in context cannot be classified as an induction pattern, we subsequently check if it can be described as a Dyck pattern, then a skip n-gram, then an n-gram. The criterion for inclusion varies for each pattern (and is described in the associated subsection) but typically consists of checking whether the token receiving maximum attention (\u201cmax-attention token\"), current token, and next token match a particular template. If a pattern matches, we say that pattern \"explains\" the token in context.\""}, {"title": "Induction patterns", "content": "An induction pattern is a sequence of tokens that repeats within a given context, where the model learns to predict the continuation of the repeated sequence. The simplest form of an induction pattern is an in-context bigram, [A] [B] [A] \u2192 [B], where [A] and [B] are arbitrary placeholder tokens. In this pattern, after seeing the sequence [A] [B] once, the model learns to predict [B] when it encounters [A] again later in the context. In our analysis, we extend the definition of induction patterns to include arbitrary in-context n-grams of the form ([1]... [N]) ([1]...[N-1]) \u2192 [N]."}, {"title": "Classification", "content": "Olsson et al. (2022) showed that two-layer attention-only transformers can develop an induction circuit which completes such patterns, predicting the second [N] from the second [N-1]. This circuit involves two components:\n\n1. A previous-token head in layer 0, which attends to the second [N-1].\n\n2. An induction head in layer 1, which attends to the first [N].\n\nWe classify a token in context as part of an induction pattern if it fits this form: if (1) the next token is the second [B]) and (2) the max-attention token is the second [A] (previous-token head) or the first [B] (induction head)."}, {"title": "Results", "content": "Hoogland et al. (2024) identified heads 0:1 and 0:4 as previous-token heads and 1:6 and 1:7 as induction heads, using the previous-token score and induction score introduced in Olsson et al. (2022). These heads can also be identified as such by their tokens in context, as illustrated by the selection in Figure 4 and the automated classification in Figure 9."}, {"title": "Dyck patterns", "content": "In predicting the next token in natural language, some of the most explicit occurrences of hierarchy come in the form of nested brackets and punctuation. This is formalized in the context-free grammars Dyck-k (\u201cdeek\u201d), which involve sequences with matching brackets of k kinds (Chomsky & Sch\u00fctzenberger, 1959).\n\nWe take a wide definition of bracket, including:\n\n\u2022 Delimiters:\n\nTraditional brackets/braces/parentheses: (...), [...], {...}, <...>,\n\nQuotation marks: \". . . \", '. . . ', \u201c\u2026 \u2026 \u2026\u201d, as well as mistokenized \ufffd\ufffd. . . \", Markdown formatting symbols: _ . . . _, **. . . **,\n\n\u2022 Split constructions:\n\nCorrelative conjunctions: \u201cboth...and", "not only...but also": "neither...nor", "either...or": "n\nCorrelative Comparatives: \u201cmore...th\u201d, \u201cless...th\u201d, \u201cbetter...th", "Superlatives": "most...ever", "least...ever\",\n\nQuestions: \\\"Why...?\", \"What...?\", \"How...?\".\n\nThere are similar constructions that we did not analyze but that would make natural candidates for follow-up analysis, such as additional correlative comparatives (\\\"as ... as\\\") conditional statements (\u201cif ... then": "cleft sentences (\"it is ... who", "the [more you practice": "the [better you get]", "[it was": "so [cold] that [the lake froze]"}, {"title": "Classification", "content": "We classify a token in context as part of a Dyck pattern if (1) the next token contains a closing bracket corresponding to an earlier opening bracket, and (2) the subsequence spanned between those two brackets has valid nesting.\n\nOften, we find that the max-attention token is the corresponding opening bracket, especially for layer 1 Dyck heads. However, we do not require this to be true to classify a token in context as a Dyck pattern."}, {"title": "Results", "content": "Figure 9 shows that automated classification identifies three heads as Dyck heads: 1:5 (98% explained), 1:3 (43.2%), and 0:7 (23.4%). Manual inspection of their tokens in context confirms these diagnoses and reveals further subspecial-izations.\n\n1:5 Delimiter-matching head. Head 1:5 is specialized to traditional brackets/braces/parentheses, quotation marks, and other symbolic delimiters.\n\n1:3 Split-construction-matching head. Head 1:3 is specialized to the natural language Dyck patterns, and one variant of quotation marks.\n\n0:7 Dyck-support head. Head 0:7 is specialized to similar tokens in context as 1:5. However, only about a quarter of 0:7's tokens in context are recognized by our automatic procedure as Dyck patterns. Manual inspection reveals that many remaining unexplained tokens in context are either misclassified as non-Dyck (e.g., because we identify whether quotation marks are opening or closing by checking whether the previous/subsequent character is not a letter, which is sometimes too restrictive) or Dyck-like (e.g., all-caps text surrounded by multiple spaces, where the spaces serving as delimiters). Most of this head's other remaining tokens in context seem to be involved in skip n-grams (analyzed in the next subsection)."}, {"title": "Discussion", "content": "The ability to correctly close brackets underlies all nonregular context-free languages, in the formal sense that by the Chomsky-Sch\u00fctzenberger theorem, any context-free language arises from a variant of Dyck-2 through intersection with a regular language and homomorphisms (Chomsky & Sch\u00fctzenberger, 1959). For this reason the ability of transformers to recognise Dyck languages has been studied at some length (Hahn, 2020; Bhattamishra et al., 2020; Ebrahimi et al., 2020). In Weiss et al. (2021) an algorithm in RASP is given which compiles to a transformer which recognises Dyck-k languages for any k. We did not examine how this relates to the heads investigated here. It is unclear whether transformers actually learn similar algorithms to these in practice (Wen et al., 2023).\n\nAs far as we know this paper is the first time that a circuit recognizing a nested Dyck language has been found \u201cin the wild\", that is, in a transformer trained on natural language. This seems interesting in connection with the ability of transformers to learn the hierarchical and recursive structure in natural language. It is however well-known that, in general, syntactic structure in natural language is represented within transformers (Hewitt & Manning, 2019).\""}, {"title": "Skip n-grams", "content": "The simplest form of a skip n-gram is a skip trigram of the form [A", "B": ["C"], "A": "and [B"}]}