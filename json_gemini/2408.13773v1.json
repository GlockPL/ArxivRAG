{"title": "SAB:A Stealing and Robust Backdoor Attack based on Steganographic Algorithm against Federated Learning", "authors": ["Weida Xu", "Yang Xu", "Sicong Zhang"], "abstract": "Federated learning, an innovative network architecture designed to safeguard user privacy, is gaining widespread adoption in the realm of technology. However, given the existence of backdoor attacks in federated learning, exploring the security of federated learning is significance. Nevertheless, the backdoors investigated in current federated learning research can be readily detected by human inspection or resisted by detection algorithms. Accordingly, a new goal has been set to develop stealing and robust federated learning backdoor attacks. In this paper, we introduce a novel approach, SAB, tailored specifically for backdoor attacks in federated learning, presenting an alternative gradient updating mechanism. SAB attack based on steganographic algorithm, using image steganographic algorithm to build a full-size trigger to improve the accuracy of backdoors and use multiple loss joint computation to produce triggers. SAB exhibits smaller distances to benign samples and greater imperceptibility to the human eye. As such, our triggers are capable of mitigating or evading specific backdoor defense methods. In SAB, the bottom-95% method is applied to extend the lifespan of backdoor attacks. It updates the gradient on minor value points to reduce the probability of being cleaned. Finally, the generalization of backdoors is enhanced with Sparse-update to improve the backdoor accuracy.", "sections": [{"title": "1. Introduction", "content": "With the development of artificial intelligence, and machine learning, especially deep learning, has been applied to all aspects of our lives, such as smart cities [5], intelligent transportation, the Internet of Things [30], Autonomous Driving [8], Smart Healthcare [1] . In these application fields, Internet of Things (IoT) devices are widely used and generate a large amount of actual data anytime, anywhere, as cameras in smart cities are used to record real-time information on the street, location sensors in automatic driving are used to assist automatic driving in combination with high-definition maps, sensors in intelligent traffic are used to determine vehicle speed and the number of vehicles. Before we use deep learning, the model must be trained using a large amount of data. It leads to some problems which exist in the model itself will be exploited by adversaries, such as adversarial attacks [11], data poisoning attacks [29] , and model stealing [18].\nIn addition, considering the huge demand for training data when training models, models cannot achieve optimal results if the amount of training data is not enough. And the privacy issues involved in data result in the inability to obtain data directly from IoT devices which results in companies and organizations with large amounts of data cannot legally share these data to promote the training of models. Consequently, many isolated data islands are created. To solve the problem of isolated data islands, Brendan McMahan of Google et al. [16] proposed a new computational paradigm for deep learning, called federated learning. This new computing paradigm can be well applied in IoT scenarios, bringing realistic and good training data to the model without leaking sensitive data from IoT devices.\nAlthough federated learning is a new computational paradigm, it mostly follows the ideas of deep learning and has characteristics of deep learning. That makes it be affected by attacks in deep learning, such as adversarial attacks [4] and data poisoning attacks[25]. And because of its characteristics, some new attacks have been derived, such as distributed backdoor attacks[28]. Whether federated learning or not, most backdoor attacks such as the approach[28, 9, 2], will put a trigger into the image. Most of these attacks are easily detectable by the human eye and difficult to resist existing backdoor defense methods, and even more difficult to apply in the real world. During the continuous training of these backdoors, the model overwritten by poisoned gradients. Our approach introduces an image-based steganographic algorithm trigger in federated learning, making the trigger hidden in the benign sample and challenging to distinguish visually. The trigger is related to the benign sample and scattered in all regions of the image, making the trigger not easy to be cleaned. Since federated learning continuously aggregates all client gradients before updating the global model. It will cause the backdoor to be continuously cleaned by benign gradients during model training. We change the location of the gradient updates according to this feature to make our backdoor have a longer survival time. And our triggers only make small changes to the image, it is harder to be detected. In addition, the introduction of aggregated gradients in the federated learning paradigm makes it difficult to ensure that smaller gradients can successfully affect the model itself after aggregation, so the effectiveness of triggers based on small values needs to be verified. This paper provides the following contributions:\n\u2022 In federated learning, a backdoor implantation method based on steganographic algorithm was attempted, expanding the trigger size to the same size as the image. With this method, the backdoor is demanding to be recognized by the human eye and to be detected by some existing backdoor detection methods, which makes it highly covert.\n\u2022 There are relatively few studies on backdoor attacks based on steganographic algorithm in federated learning. This paper verifies the effectiveness of that kind of attack.\n\u2022 The primary contribution of this study resides in the introduction of a pioneering approach to model updates within the realm of federated learning, furnishing a novel perspective tailored specifically for addressing back-door vulnerabilities. By leveraging the bottom-95% and sparse-update techniques, we empower our controlled data to engender a heightened impact on the model, ensuring its enduring and unwavering integration within the model's architecture. Our proposed methodology exhibits particular efficacy in the context of thwarting backdoor attacks. Full-size triggers, in our method, can effectively utilize the bottom-95% gradient update locations to conceal the backdoor in an imperceptible manner. By combining this approach with the Sparse-update method, the backdoor gradient update ratio can be adjusted, thus enhancing the robustness of the backdoor. Consequently, the impact of the backdoor becomes less substitutable by benign samples, significantly augmenting both the duration and accuracy of the backdoor. We refer to our overall approach as SAB, showcasing its comprehensive efficacy."}, {"title": "2. Background and related work", "content": ""}, {"title": "2.1. Federated Learning", "content": "Conception. Federated learning unites a large amount of real and highly sensitive data for model training. And makes the model more realistic. Meanwhile, the data can be easily labeled by the interaction between the user and software, making the data available for supervised learning. The federated learning method can boost communication efficiency, reducing the time spent from 1/10 to 1/100 of the original.\nUsually, there exist K participants, each with a different dataset, and one central server with a global model. In training, the central server randomly selects M participants.\nEach client downloads the global model and trains with their private datasets, gets its gradient update, and uploads it to the central server. The central server aggregates the gradients, calculates a global gradient according to a certain aggregation method, and updates the global model. In the next round of training, the central server selects M clients again to train on the updated new model.\nAssuming that each client trains only once with the local datasets and set batchsize = 1, the goal of federated learning at this time is Eq.(1), and it will be called Federated Stochastic Gradient Descent (FedSGD). The dk is the local dataset for client k, and f\u2081(w) = a(x, y\u2081, w) is loss of the local model with parameters w for instances (x, y) in the dataset dk. The m is number of clients, and F\u2081(w) is the objective function of device k, k is clients in client set, nk is the number of samples on k, n is the total number of samples for all selected clients.\nmin f(w) = \u2211Fk(w) where Fk(w) = 1/nk \u2211(x,y)\u2208dk fi(w)  (1)"}, {"title": "2.2. Backdoor Attack", "content": "Artificial intelligence is used in many aspects of modern life, such as face recognition[21], natural language processing[12], intelligent healthcare, autonomous driving, machine translation[22], etc. Currently, researchers focus on the security of these practical applications, the number of attack methods against AI models is increasing. The main one is adversarial sample that acts on the inference phase of the model to makes the model less effective. Data poisoning attack aims to destroy the model and make it hard to be trained and unusable. On the other hand, backdoor attack makes the model's accuracy in the main task nondecreasing, although for some specific samples, the model classifies as the adversary's desired category. That means backdoor attacks is stealthier and more deceptive, and it presents a challenge for users to detect the existence of backdoors. Thus, deploying the model with backdoors to make the model produce the adversary's desired results in practical applications, will cause unknowable results and make it more threatening.\nBackdoor Attack in Deep Learning In BadNets[9], the U.S. stop signs database was used and a stop sign was selected to inject a backdoor, from left to right, as a yellow square sticker, a bomb sticker, and a flower sticker like Fig. 1. The backdoor with these three methods has achieved a success rate of more than 90%. BadNets explores machine learning backdoor attack and finds a new security issue that can arise when customers use models obtained from machine learning model training outsourcing companies or online model zoos. For the main task of interest to customers, models implanted with BadNets backdoor still maintain high accuracy rates. Since the backdoor is carefully constructed by the adversary. When the model encounters an input containing a trigger, the model outputs the result pre-set by the adversary, and BadNets does not require any structural changes to the network to enable the model to achieve complex functionality.\nAs the demand for more sophisticated backdoors grows, Chen et al. [6] propose that in order to evade human scrutiny, backdoors should be imperceptible. They introduce a blended strategy-based trigger that makes backdoors difficult to detect, and they discover that injecting a small amount of random noise as a trigger can also successfully implant backdoors. In Poison Ink[31], an adversary will first use an edge extraction algorithm to extract the edge of an image. Then mathematically encode the toxic information into an RGB color and use this color to color the extracted image edges as Fig. (2). Finally add a trigger to the image to make poisoned data to implant a backdoor. As a result of the wide RGB color gamut and the diversity of image edge extraction algorithms, different combinations can give rise to multiple backdoor results, thereby expanding the variety of trigger patterns for this method. Liu et al.[15] suggest utilizing the phenomenon of reflection as a trigger, rendering it challenging for humans to perceive and enhancing the concealment of the backdoor. Mauro et al.[3] present the SIG method, which superimposes a ramp backdoor signal onto the data, rendering the backdoor signal invisible, particularly in images with dark backgrounds, making it difficult for the human eye to detect the presence of the ramp backdoor signal."}, {"title": "Backdoor Attack in Federated Learning", "content": "At this stage of research, most backdoor attacks consider the trigger as a whole. In Distributed Backdoor Attacks(DBA)[28], benefits for federated learning paradigm of model training, the authors consider a distributed trigger for federated learning thinking, divide a trigger into multiple parts and implant the multiple parts in different Poison data, as displayed in Fig. (3).\nThe idea of DBA is to implant the triggers in different data separately and combine these scattered triggers by aggregation algorithm of federated learning. It will constitute a complete trigger that plays the role of a backdoor. By Grad-Cam[20], we can find that the model focuses on the color block in the upper left corner of the image when the global model makes an inference. It proves that the backdoor injected by the DBA method has a superior validity."}, {"title": "2.3. Backdoor Defense", "content": "Grad-Cam[20] utilizes the gradient information in the neural network when the gradient information flows into the last convolutional layer, which retains more spatial information of the image with spatial invariance compared to the fully connected layer. The author's experiments demonstrate, more shallow-level features in Grad-Cam retain more spatial information, while the deeper-level features retain more semantic information. The method forward-propagates the original input of the image and then does guided backprop one time. The results obtained by the above steps are fused with the importance calculated by the last convolutional layer to obtain the final heat map. It exhibits that the model mainly focuses on the image during the process of inference. In BadNets, Grad-Cam demonstrates that model focuses on the obvious trigger pattern, it is easy to artificially determine the problematic location in the image.\nSTRong Intentional Perturbation[7] approach argues that the STRIP method has input-agnostic features, mainly by using the detection of whether the backdoor trigger is included in the input of the image. The key idea is that when the image does not contain a backdoor trigger, the output of the model will change when the detector adds a strong perturbation to the image. When the input contains a trigger, the inference of model is independent of how much perturbation has been added to the image. If the detector adds a strong perturbation to the image with trigger, the model will still categorize the image as the target class expected by the adversary, and this non-change is considered an anomaly. When a perturbation is added to a benign image, the change in the model's prediction of image is related to strength of added perturbation. By adding different patterns to the image, the model produces different results, and the distribution of entropy for each set of results is then calculated. This can be found, when a trigger is added to image, the distribution of the entropy of the model output prediction results between the benign sample and poisoned sample will be more different. It can be easily distinguished between the poisoned image with the trigger implanted and the benign image by setting a threshold value.\nThe defense method Differential Privacy(DP)'s[26] core idea is to add Gaussian or Laplace noise to the gradient in the process of federated learning gradient exchange to perturb the gradient. The gradient with the noise added can no longer represent 100% of the direction of the model update, making the trained model carry a certain error to attenuate the impact brought by the backdoor attack. Whereas the correct gradient will be affected by the perturbation as well, the accuracy of main task is reduced to varying degrees, thus affecting the performance of the model. Although differential privacy can reduce the accuracy of the backdoor attacks. Owing to the negative impact on the precision of benign samples, a minor perturbation is commonly incorporated.\nA dropout-based defense approach proposed by Y. Zhao et al.[34] is based on information theory to optimize stability and generalization of model by increasing the uncertainty of the parameters in training phase. Consequently, federated learning can optimize the generalization degree of the model by dropping some optimization information. The degree of model dropout is directly proportional to the strength of its resistance against adversarial attacks induced by backdoor samples. This is attributed to the fact that the perturbed gradients are subjected to dropout as well. Dropout refers to the process of stochastic gradient descent (SGD) optimization, in which the weight of the gradient is randomly set to zero."}, {"title": "3. Method", "content": "In this section, we introduce the threat model and our backdoor approach. Specifically, Section III-A presents the threat model and summarizes two potential attack scenarios that an adversary may employ in federated learning. We describe the information available to the attacker in each scenario, as well as the goals the attacker aims to achieve. In Section III-B, we focus on explaining why steganographic algorithm is effective in a federated learning backdoor attack. Additionally, we outline our approach, which consists of four parts: why the steganographic algorithm work, stealing trigger, lengthen lifespan, and gradient upload method."}, {"title": "3.1. Threat model", "content": "Attacker's Capacities. In federated learning, the complete model will sent to clients, thereby allowing attackers to obtain the overall model structure. Moreover, attackers are permitted to modify and upload their gradient directly, even to the extent of violently changing its value. They can alter model parameters what obtained from the central server to receive the desired gradient updates. Each client trains locally with private data. Any changes to the client's training data would affect the global model. Modifications of the private training data of client will change the client's local parameters.\nIt is worth noting that we consider two cases. One is that when organizations such as enterprises, hospitals, schools, and government agencies have the need to train a machine learning model, they need entrust the training task of the model to a company with arithmetic power (outsourcing company). Considering the large number of organizations and the sensitive data they hold, it is incapable to share their data. In such situations, an outsourcing company may have a certain degree of influence and can be chosen to gain the trust of these organizations through federated learning. Then the outsourcing company can obtain the gradients of sensitive data and update model, make model better meet the needs of these organizations. However, outsourcing companies can carefully constructed it to implant a backdoor in the model. The second scenario involves us acting solely as a client for federated learning, receiving the model, using local data to update it, and uploading the updates to a central server. The key difference between the two scenarios lies in the fact that, in the first case, we can make changes to the global model of federated learning and the aggregation method, whereas in the second case, we can only make changes to the local model and the uploaded model updates.\nAttacker's Goals. Backdoor attacks in deep learning typically cause the model to predict a specific class for inputs containing a trigger, such as classifying all inputs with a trigger as \"frogs.\" Similarly, backdoor attacks in federated learning aim to achieve the same outcome. However, current federated learning backdoor attacks have a noticeable and easily detectable trigger, although some triggers are distributed in appearance. They can be detected by visual inspection, while such triggers may be hard to apply to the physical world. Thus, these backdoors are removed along with the federated learning process. We desire the trigger to be stealthy, hard to detect by eyes, able to survive for a long lifespan in the training process of federated learning, and robust against some defenses."}, {"title": "3.2. Approach", "content": "Why does the steganographic algorithm work? In existing work, we have found that a significant number of current backdoor attacks use a fixed image as the trigger, and these triggers are placed in specific location in the image. According to the principle of convolutional neural network, it is known that these triggers will eventually be transformed into partial features. The position of these features will not be changed in different images. Therefore, when local clients are trained with backdoor images containing fixed triggers during the federated learning training process, the gradients affected by the backdoor will always appear at fixed locations. When these gradients are uploaded, there is a probability that the content of the gradients at these fixed locations will even be blurred to zero, when the central server deploys some defense methods such as DP and PartFedAvg. This is a major limitation of the application of existing methods in federated learning. Increasing the number of blurred or zeroed gradients can cause fixed triggers to fail immediately, making it challenging for these attack methods to avoid defense mechanisms like DP and PartFedAvg. Hence, finding a backdoor that is challenging to eliminate is crucial. Additionally, in real-world implementations of backdoor attacks, it is necessary to ensure that the backdoors are difficult to detect by the human eye.\nIn this paper, we propose an approach for a federated learning backdoor attack based on image steganographic algorithm[24], the attack can be a suitable approach for our needs. We do not simply fix the trigger at a certain location on the image. Instead, we resize the trigger to a size comparable to the image itself and overlay it onto the image. Furthermore, our attack can make the trigger size comparable to the image, while it is challenging for the victim to detect the anomaly. The trigger is equivalent to a complete image for the model, and the model will map the trigger to the class specified by the adversary. This is similar to an image classification model, which, unlike other existing methods, does not correspond to a small trigger or a few data points to the target class. Instead, like a dataset of image classification, it learns a large number of trigger features. The trigger should not overfit while being learned, so we use an overfitting prevention method to enhance the success rate of the backdoor attack. This approach ensures that the failure of triggers does not occur abruptly due to the overwriting of certain data points, which underlies our method's high stealthiness and robustness.\nMoreover, in the context of federated learning, a backdoor attack is usually set for a certain number of rounds and stops after completing the specified number of rounds. After stopping, the success rate of the backdoor attack decreases as the number of model training rounds continues to increase. We refer to the number of rounds during which the backdoor remains effective as the lifespan. The decrease in attack success rate is because the gradient impact of backdoor attacks is cleaned by benign samples. We utilized two methods to extend the lifespan. The first method is called bottom-95% method, which involves continuously uploading the gradient during the training process of federated learning. The larger the gradient value, the stronger the impact on the model. On account of the attacker's backdoor gradient updates in the top-5% large values will not be uploaded. The model will be completely updated in benign clients, and the backdoor gradient will be hidden in the bottom-95%. The data points in the bottom-95% will not be frequently updated or updated with smaller values, which can reduce the chance of the gradient being overwritten. The second method is Sparse-update, which randomly sets the updated gradient to zero. When the gradient of the trigger position is set to zero, the effect of the trigger will be greatly reduced. However, since our trigger covers the whole image, it will take time to be eliminated because of the Sparse-update. Moreover, this method improves the generalizability of the backdoor.\nIn Chapter 5 of this paper, we expound on the performance of our approach under different backdoor defense methods. Compared to the baseline, our method demonstrates superior robustness and is more adept at evading detection.\nStealing Trigger. We utilized the stegastamp method from image steganographic algorithm to create our stealthy trigger, as illustrated in Fig. (4). The final loss function comprises two critical and two auxiliary losses. To generate the trigger, we input an image Porg and a character string S that is to be embedded into Porg. We use U-Net[19] style model as an encoder to encode the image and output a 3-channel RGB residual image. Before encoding, a suitable encoder needs to be trained. The encoder training requires inputting a 3-channel image and a string, converting them into a tensor, and then concatenating them. The result of the concatenation is fed to the model for convolution and upsampling to obtain a residual image. The residual image is convolved during decoding to obtain the final string result. Finally, we get a string-written image Pen and a string obtained by decrypting Sdecode by two losses. One is the loss between the original image before encryption and the encrypted image Eq.(3), and the other is the cross-entropy loss between the original string before decryption and the string decrypted from the image Eq.(4). As Eq.(5), we added the critic loss. To better produce a stealthy trigger that is hard to be detected by eyes, LPIPS is added to the final loss[32]. The perceptual loss within LPIPS demonstrates the ability to discern the distinctions between two images, while incorporating the average of the resultant vectors derived from the discriminator's evaluations of a set of fabricated images, thus culminating in the final loss Eq.(6).\nLossimage = Pen - Porg (3)\nLosssecret = CrossEntropyLoss (S, Sdecode ) (4)\nLosscritic = Discriminatorreal \u2212 Discriminatorfake (5)\nmin Loss W\u2081 * (Lossimage ) + W2 * (LossLPIPS ) + W3 * (Losssecret) + W4 * (Losscritic) (6)\nThrough the joint computation of these loss functions, we obtain an image encoder with better performance, by which we can generate a residual image of each image according to a string as our trigger.\nLengthen Lifespan. Based on the inherent characteristics of federated learning and the specificity of application scenarios. When studying backdoor attacks for federated learning, it is a question worth exploring how to obtain a higher attack lifespan or a slower decay rate of attack success rate using fewer attacks within a limited attack round. In our study, we were inspired by Neurotoxin[33] and find that gradients containing attacks can be cleaned more slowly when updated on the gradient with the smallest value, result in the attacks to survive longer in the model. This phenomenon is primarily attributed to the disparity between the backdoor gradient value and the benign gradient value, with the former exerting a persistent dominant influence in relation to the gradients generated by benign clients. The backdoor gradient corresponding to the position where the benign gradient value is relatively smaller is uploaded. Although the model attempts to mitigate the excessive impact of each customer's gradient through an averaging algorithm, the influence of the backdoor gradient remains substantial compared to the impact of benign samples. Consequently, it proves challenging to effectively evade the repercussions brought about by the backdoor gradient. Such an idea can be well integrated with our approach. Our trigger is a residual image of the same size as the dataset image and be spread over the whole picture. It will apply the impact of triggers on the maximum value within the bottom-95% of the data points. That makes triggers be decoupled from top-5% of data points. Top-5% data points will not be uploaded as part of the triggers and will only be updated as zero. The updates as triggers will be avoided to be covered quickly by the updates of benign samples because of the two reasons of decreasing update frequency and small update values. The gradient is calculated, and the model is updated as Eq.(7) and (8), I is batchsize of train data D, \u03b8 is local model for each client, L is the loss of \u03b8, \u03b7p is learn rate of poisoned data, Value(g) is the value of g.\nConsequently, our implanted backdoor does not fail instantly even when backdoor gradient is covered by some benign gradients. We have a trigger present for each backdoor image, which makes the rest of backdoors still work when some backdoor influences are cleaned. In summary, our backdoor attacks can last longer.\n8 = 1/I \u2211x\u2208D V\u03b8L(\u03b8,Dx) (7)\n\u03b8ei+1 = \u03b8ei  - \u03b7p8 where top5% (Value (g)) \u2209 8 (8)\nGradient upload method. PartFedAvg is commonly used as a defensive method for backdoor. In [23] and [14] PartFedAvg has been convergent in both the i.i.d. and non-i.i.d. cases respectively. We take inspiration from this and propose a method called Sparse-update. It will blur the gradient by randomly and set the update of partial gradient to zero to improve the security and attack success rate of the model. That is, it will still have a defensive function and part of defense method will use this kind of ideal to fortify security. The reason we use this method is that we try to increase our backdoor lifespan by this method. The sparse-update approach selectively removes a mere 20% of the data points. Moreover, owing to the utilization of the Bottom-95% technique, the remaining 80% of data points, which are unaffected by the sparse-update method, may not necessarily comprise backdoor data points. This allows our backdoor method to conceal itself more effortlessly within the updates. It makes Sparse-update elevate the generalization of the backdoor attack and increase the success rate of the backdoor attack. Benefiting from our trigger affects all the data points. Sparse-update and Bottom-95% methods can be combined. Our method will lengthen lifespan first and Sparse-update on top of it Eq.(9), which will make the attack method in this paper improve generalization and an extended lifespan. The overall success rate of the attack is elevated.\nGi+1 = Gi - \u2211random80%(8k(0ei+1)) (9)\nIn summary, this study endeavors to build full-size triggers while rendering them imperceptible to the naked eye. During injection, they are embedded in sparsely updated data points, and during gradient propagation, only 80% of the gradients is randomly transmitted. Consequently, our triggers are based on full-size implementation, selectively sampling data points as triggers within the less frequently updated regions. Throughout the iterative process, two primary challenges arise: 1. limited scope of the triggers, and 2. frequent updates to trigger locations. When the triggers exhibit a small spatial range, for instance, a fixed 5x5 pixel block, the rapid obliteration of the trigger's influence occurs with each update to its location, rendering the backdoor entirely ineffective. In contrast, as exemplified by the SAB method, when triggers are stochastically dispersed across the entire image and updated with lower probability, the backdoor exhibits prolonged persistence, augmented robustness, and enhanced accuracy.\nWe will call our backdoor method based on steganographic algorithm SAB and list the overall pseudo-code of the algorithm below."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Settings", "content": "Datasets\nFashion-MNIST [27]. It is a dataset about fashion items, which internally contains a total of 70,000 images from 10 categories, each image is 28 * 28 pixels in size and is grayscale. There are 60,000 images in the training set and 10,000 images in the test set. The size and type of the Fashion-MNIST images are identical to MNIST, and the authors have produced it by positioning Fashion-MNIST as a replacement for MNIST to be used in machine learning. In order to make it as easy as possible for users to replace MNIST with Fashion-MNIST as convenient as possible, the splitting structure inside the data of both datasets is the same. It is even possible to directly overwrite the MNIST file with the Fashion-MNIST file when replacing it.\nCIFAR-10 [13]. It is a dataset containing 60,000 images, of which there are 10 categories with 6,000 images in each category, each image is a 32 * 32 pixel RGB image. Among them, 50,000 are the training set and 10,000 are the test set. In the 6 packages of datasets, there are 5 train batches, and 1 test batch, each batch include 10,000 images. The test batch contains 1000 images from each class. Train batch contains the remaining images in random order, the 10 classes in each batch contain uneven images, possibly some of these classes are completely mutually repulsive in CIFAR-10. For example, there is no overlap between cars and trucks. The class \"car\" includes things like cars, SUVs, and so on, and \"Trucks\" includes only large trucks. Neither includes pickup trucks.\nCIFAR-100 [13]. The size and format of the images are the same as CIFAR-10. By contrast CIFAR-100 has 100 classes, each containing 600 images, of which 500 are training images and 100 are test images. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs). Specifically, we provide TABLE 1 to provide easy viewing of the structure and content of the datasets we use."}, {"title": "Model", "content": "ResNet [10]. It is a network model that is widely used in deep learning and proposed by four scholars from Microsoft Research as a convolutional neural network. In deep learning, as the number of layers increases, the probability of gradient disappearance and gradient explosion increase when model back-propagates gradient. ResNet mitigates this problem by using jump connections in the internal residual blocks, and the network is easy to optimize and easy to enhance its accuracy by increasing the number of layers. It won the ImageNet Large Scale Visual Recognition Challenge in 2015 for image classification and object recognition. Specifically, we provide TABLE 2 to provide a convenient view of the layer number structure and avg pool2D kernel size of the model we are using on the corresponding dataset."}, {"title": "Evaluation Metrics", "content": "\u2022 Attack Success Rate (ASR): represents the ratio of the number of test data selected from the backdoor test set containing triggers that are predicted as target tags by the model to the number of test data. For model, a higher ASR represents a higher backdoor accuracy rate.\n\u2022 Benign Accuracy (BA):represents the prediction accuracy of the benign sample (main task) obtained by testing a portion of the data taken from the test set of the benign sample.\n\u2022 Test Accuracy Loss (TAL):the accuracy loss of the model's main task before and after backdoor implantation is used to assess the overall impact of the backdoor injection method on the model. If the model injected with the backdoor has a large degree of degradation or fluctuation in the performance of main task, it may lead to the model being perceived as an attack or having poor performance, allowing the model to be replaced and failing to achieve the attacker's objective.\n\u2022 Perturbation Hash Stealthiness (pHash)[17]: a fingerprint derived by obtaining various features from the content of multimedia files. Unlike the usual hashing algorithm, which can make the result completely different through an avalanche effect due to some minor changes, perceptual hashing will make two multimedia contents with similar features yield a eclosee result."}, {"title": "4.2. Parameter Settings", "content": "Baseline Settings. We compare SAB with BadNets and DBA. BadNets is a typical example of backdoor attack in deep learning. It will adds patches to sample. DBA is a new backdoor attack based on federated learning with distributed trigger. For defense, we use STRIP, DP, and Grad-CAM to evaluate the performance of our approach against some defense methods. To imitate BadNets, we added an obvious trigger in the corner of image. In cifar10 and cifar100, added a 5 * 5 pixel white block with a black cross filled in the center of the white block. And in Fashion-MNIST, we added a mosaic-style black and white block in the bottom right corner as a trigger. In the DBA method, we add 4 blocks of trigger in the top left corner of cifar10, cifar100, and Fashion-MNIST, each block is 1 * 2 pixels in size and there is at least 1 pixel spacing between the left, right, top, and bottom adjacent trigger. It is worth noting that the trigger we insert in cifar10 and cifar100 are colored triggers, which are four different colors. In comparison, while Fashion-MNIST itself is a grayscale map, we insert four different grayscale values in Fashion-MNIST, they all have a difference of at least greater than 30.\nAttack Settings. A total of 3000 clients are selected, and 10 of them are selected by the central server in each round and include an adversary who extracts attack data from the poisoned dataset for training. We set the batchsize = 64, use the dirichlet distribution when extracting the samples, set dirichlet\u0251lpha to divide the samples that conform to the dirichlet distribution. In the pre-training, the benign learning rate = 0.001 and decay = 0.0005 per round, in the attack rounds the poisonlearningrate = 0.02 and decay = 0.005 per round. To reducing the possibility of our attack being cleaned in the constant model updates and increase the lifespan, we limit the range of gradient updates to the bottom-95% of each gradient value, the gradient value of top-5% will not be updated during the attack. Specifically, we will set the top-5% gradient value to zero. When the parameters of the model are aggregated, the effect of the backdoor in top-5% is not carried. However, gradients from other benign clients affect the model normally. A backdoor attack differs from a poisoning attack. Poisoning attacks only expect the model to produce false predictions, while backdoor attacks require the model to produce false predictions that match the attacker's expectations. Our attack method can produce different triggers based on a different string, and thus can fix a string by a class to produce a class trigger. Sparse-update can enhance ASR by blurring the gradient and reduce the impact of each client on the global model. When updating, the upload limit is set to 20%, clients will randomly set zero to the 20% gradient uploaded.\nDifferential Privacy is generally deployed in federated learning as a defense method for backdoor attacks. We add differential privacy to the training of the model and set mean = 0.000001, sigma = 0.001."}, {"title": "4.3. Performance comparison", "content": "As Fig. 6, Fig. 7, Fig. 8, we compared our method SAB, BadNets-based method, and DBA-based backdoor implantation method on federated learning on three datasets Cifar10, Cifar100, and Fashion-MNIST, respectively. Our method is better than both methods in terms of ASR and lifespan"}]}