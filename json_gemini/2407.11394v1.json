{"title": "DREAMCATALYST: FAST AND HIGH-QUALITY 3D EDITING VIA CONTROLLING EDITABILITY AND IDENTITY PRESERVATION", "authors": ["Jiwook Kim", "Seonho Lee", "Jaeyo Shin", "Jiho Choi", "Hyunjung Shim"], "abstract": "Score distillation sampling (SDS) has emerged as an effective framework in text-driven 3D editing tasks due to its inherent 3D consistency. However, existing SDS-based 3D editing methods suffer from extensive training time and lead to low-quality results, primarily because these methods deviate from the sampling dynamics of diffusion models. In this paper, we propose DreamCatalyst, a novel framework that interprets SDS-based editing as a diffusion reverse process. Our objective function considers the sampling dynamics, thereby making the optimization process of DreamCatalyst an approximation of the diffusion reverse process in editing tasks. DreamCatalyst aims to reduce training time and improve editing quality. DreamCatalyst presents two modes: (1) a faster mode, which edits the NeRF scene in only about 25 minutes, and (2) a high-quality mode, which produces superior results in less than 70 minutes. Specifically, our high-quality mode outperforms current state-of-the-art NeRF editing methods both in terms of speed and quality. See more extensive results on our project page:\nhttps://dream-catalyst.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-driven Neural Radiance Field (NeRF) (Mildenhall et al., 2021) aims to generate or edit 3D objects based on the given text prompt. This task faces challenges in data collection due to the need for images from diverse views of a 3D scene. Poole et al. (2022) addressed this issue by leveraging the rich priors of a large web-scale pretrained diffusion model (Rombach et al., 2022), proposing Score Distillation Sampling (SDS). It enables training parameterized models, especially NeRFs, without additional data collection.\nWhile 3D scene generation has garnered substantial interest (Zhu et al., 2023; Wang et al., 2024), comparatively fewer studies have focused on 3D scene editing. The text-driven 3D editing task modifies a source scene to align with a target text prompt. Unlike 3D generation, 3D editing must consider not only alignment with the target text prompt but also identity preservation of the source scene. Posterior Distillation Sampling (PDS) (Koo et al., 2023) achieves this by considering text-aligned editability and identity preservation through minimizing the proposed stochastic latent matching loss (Wu & De la Torre, 2023; Huberman et al., 2024).\nHowever, we observe that PDS suffers from slow 3D editing and inferior editing quality due to its theoretical foundation in stochastic latent matching. First, the formulation of stochastic latent matching heavily prioritizes identity preservation over editability at low noise perturbation. This results in insufficient editing results since fine details are primarily generated (Choi et al., 2022) at the low noise perturbation. Second, the stochastic latent matching loss complicates the adaptation of decreasing timestep sampling instead of random sampling. In recent SDS-based 3D generation studies (Zhu et al., 2023; Huang et al., 2023; Lee et al., 2024), decreasing timestep sampling is adopted for fast convergence and high-quality generation. However, large noise perturbations in the early stages of editing lead to hindering identity preservation of the source scene (Meng et al., 2021) with PDS, challenging the balance between identity preservation and editability. Therefore, the stochastic latent matching loss has several drawbacks from the balance between identity preservation and editability. It conflicts with the well-known roles of diffusion timesteps in low timesteps (Choi et al., 2022) and the decreasing timestep sampling algorithm in large timesteps, as demonstrated in Fig. 2a.\nTo address these issues, we propose (1) a novel objective function to rebalance the weights of identity preservation and editability with respect to the level of noise perturbation. Additionally, we present (2) an improved model architecture for high-quality results. For rebalancing, we introduce a general formulation of PDS by introducing a new perspective of Delta Denoising Score (DDS), which is implicitly incorporated in PDS for editability. Our interpretation indicates that SDS-based editing methods are equivalent to a diffusion reverse process. Moreover, we propose a specialized formulation that is suitable for SDS-based editing and capable of applying the unexplored decreasing timestep sampling in 3D editing to boost editing speed.\nOur loss function contrasts with the stochastic residual loss of PDS, which provides more weight to identity preservation when timesteps are high and reduces the emphasis on identity preservation when timesteps are low. Accordingly, our loss function considers the role of diffusion timesteps for the editing task, providing two advantages. First, our loss ensures superior editing performance by rebalancing editability and identity preservation. Second, the proposed loss enables the application of decreasing timestep sampling, which allows for a training process similar to the diffusion inference stage (Huang et al., 2023). The decreasing timestep sampling results in diffusion-friendly SDS editing that enhances both speed and editing quality. When employing decreasing timestep sampling to traditional methods, there is a problem of information loss from the source due to strong perturbation at the early stages of training. However, our loss function mitigates the loss of source information, enabling the application of decreasing timestep sampling as demonstrated in Fig. 2b. To the best of our knowledge, we first conquer the adoption of decreasing timestep sampling in SDS-based general 3D editing tasks, which requires maintaining structures, i.e. backgrounds, and is valid in various scenarios.\nOur main goal is to achieve faster editing and improved quality in both identity preservation and editability. However, there are limitations to quality improvement with reweighting formulation alone, as identity preservation and editability are trade-offs (Koo et al., 2023). Modifying the model architecture is a conventional solution to overcome this issue (Cao et al., 2023). Especially in SDS-based methods, many studies (Koo et al., 2023; Wang et al., 2024) finetune diffusion models to"}, {"title": "2 PRELIMINARIES", "content": "overcome the trade-offs for improving quality with Low-Rank Adaptation (LoRA) (Hu et al., 2021) or Dreambooth (Ruiz et al., 2023). Instead, we introduce leveraging FreeU (Si et al., 2023) in SDS. LORA and Dreambooth require extra computation and training for the network, leading to longer training times and additional memory costs, which conflicts with our main goal. In contrast, FreeU does not require any time consumption and additional memory while improving quality. Moreover, FreeU enhances the editability by suppressing the high-frequency features, while preserving the identity by amplifying the low-frequency features. These characteristics of FreeU harmonize with our formulation since the formulation ensures identity preservation and FreeU enhances editability without compromising the identity preservation, as shown in Fig. 1.\nWe evaluate our method through qualitative comparisons and user studies, and we provide quantitative comparisons with baseline methods. Our results demonstrate that the proposed method outperforms the baselines in both editing speed and quality. In summary, our key contributions are as follows:\n\u2022 We suggest a general formulation for 3D editing by introducing a new interpretation of DDS as SDEdit process and propose a specialized formulation for fast editing and quality improvement.\n\u2022 We adopt decreasing timestep sampling, a prevalent sampling algorithm in the 3D generation task for accelerating training speed, in general, 3D editing tasks, addressing challenges faced in previous works.\n\u2022 We first introduce using FreeU for 3D editing to enhance editability to overcome the trade-offs inherent in reweighting the formulation of editing objectives.\n2.1 DIFFUSION MODELS\nA diffusion model (Song & Ermon, 2019; Ho et al., 2020) consists of a forward process that gradually perturbs a data point $x$ with noise $\\epsilon$, and a reverse process that progressively denoises the noisy data. The forward process is defined as follows:\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon, \\epsilon \\sim N(0,I),$ (1)\nwhere $N(0, I)$ is a Gaussian distribution, $t \\in [0,T]$ denotes the timestep, $x_t$ represents perturbed $x_0$ at $t$, and $\\bar{\\alpha}_t$ is a predefined noise scheduling coefficient. In contrast, the reverse process utilizes the score function, which is predicted with a neural network and a sampler. The score function, which is equivalent to denoising network $\\epsilon_\\theta$, parameterized by $\\theta$, of a diffusion model, is trained via denoising score matching as follows:\n$\\min_\\theta L(x_0) = E_{t,\\epsilon}[|| \\epsilon_\\theta(x_t, t) - \\epsilon ||^2].$ (2)\nDenoising Diffusion Implicit Model (DDIM). In the context of DDIM (Song et al., 2020a), the reverse process can be regarded through Tweedie's formula as\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}x_{0|t} + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\tilde{\\epsilon},$ (3)\nwhere\n$x_{0|t} = (x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t))/\\sqrt{\\bar{\\alpha}_t},$\n$\\tilde{\\epsilon} = \\frac{\\sqrt{1 - \\bar{\\alpha}_{t-1}} - \\eta^2 \\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t,t) + \\eta \\sqrt{\\beta_t} \\epsilon,$ (5)\nwhere $x_{0|t}$ is a predicted $x_0$ with $x_t$, and $\\tilde{\\epsilon}$ is a noise term consisting of a deterministic term and a stochastic term $\\epsilon \\sim N(0,I)$. The deterministic sampling is acheived when $\\eta\\sqrt{\\beta_t} = 0$, as the stochasticity of noise term $\\tilde{\\epsilon}$ can be manipulated with the stochastic hyperparameters $\\eta$ and $\\beta_t$.\nSDEdit. SDEdit (Meng et al., 2021) is proposed to edit the images with the trained score network, which can be regarded as a denoising network in diffusion models, by solving the stochastic differential equations (SDEs) (Song et al., 2020b). SDEdit moves the data point sequentially by leveraging the estimated score function $\\nabla \\log p(x_t)$ with the reverse Variance Preserving (VP) SDE:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}x_{0|t} + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\epsilon, \\epsilon \\sim N(0, I).$ (6)\nIteratively shifting the data point with the reverse VP-SDE synthesizes the edited image."}, {"title": "2.2 SCORE DISTILLATION SAMPLING", "content": "In this section, we introduce previous SDS methods, which utilize pre-trained diffusion models as prior, since we aim to edit NeRFs without additional data collection. Compared to the diffusion models that sample in image space, DreamFusion, inspired by DIP, proposes the SDS framework to sample in the parameter space. SDS optimizes parameterized models such as NeRF or differentiable image generators using the diffusion training objective. Specifically, in 3D generation, SDS perturbs rendered images $x = g(\\psi, c)$, where $g$ is a NeRF model, $\\psi$ are parameters of the NeRF $g$, and $c$ is a camera parameter, with noise $\\epsilon$. It then distills from a pre-trained diffusion model with rich 2D priors to train NeRF. The training objective is defined as follows:\n$\\min_\\psi L_{SDS}(x_0 = g(\\psi,c)) = E_{t,\\epsilon}[|| \\epsilon_\\theta(x_t, y,t) - \\epsilon ||^2],$ (7)\nwhere the predicted noise with Classifier-free guidance (CFG) $\\epsilon_\\theta(x_t,y,t) := \\epsilon_\\theta(x_t,y_y,t) + w_y(\\epsilon_\\theta(x_t,y,t) - \\epsilon_\\theta(x_t, y_\\emptyset,t))$ (Ho & Salimans, 2022), $w_y$ indicates the scale of text-guidance, $y$ is a text prompt, and $y_\\emptyset$ is a null-text. Particularly, SDS omits the U-Net Jacobian term for computation efficiency as\n$\\nabla L_{SDS}(x_0 = g(\\psi, c)) = E_{t,\\epsilon}[(\\epsilon_\\theta(x_t, y, t) - \\epsilon)\\frac{\\partial x_t}{\\partial \\psi}].$ (8)\nDelta Denoising Score (DDS). The editing task consists of two key aspects: (1) preserving the source content's identity and (2) aligning with the target text prompt. Since the objective of SDS is designed for the generation task, it struggles to preserve the source identity. To address this, DDS is proposed to preserve the source identity by modifying the SDS loss function. Specifically, DDS subtracts the SDS loss of the source data $x^{src}$ from the target data $x^{tgt}$, where $x^{tgt}$ is rendered from the generator, as defined below:\n$L_{DDS}(x^{tgt}_t = g(\\psi,c)) = E_{t,\\epsilon}[|| \\epsilon_\\theta(x^{tgt}_t, y^{tgt}, t) - \\epsilon_\\theta(x^{src}_t, y^{src}, t) ||^2],$ (9)\nwhere $x^{tgt}_t$ and $x^{src}_t$ are perturbed $x^{tgt}$ and $x^{src}$ at $t$, and $y^{tgt}$ and $y^{src}$ are target prompt and source prompt, respectively.\nDecreasing Timestep Sampling. Recently, SDS-based text-to-3D generation tasks have frequently utilized decreasing timestep sampling. Traditional SDS methods, as proposed in DreamFusion, employ random timestep sampling. However, these approaches differ from the diffusion inference process, resulting in slower convergence and degraded 3D content creation. Several works (Huang et al., 2023; Lee et al., 2024) demonstrate faster convergence and better quality by using decreasing timestep sampling, which is similar to the diffusion reverse process. In the 3D editing task, Instruc-tHumans (Zhu et al., 2024) utilizes the decreasing timestep sampling for 3D human texture editing."}, {"title": "2.3 NERF EDITING WITH DIFFUSION MODELS.", "content": "However, InstructHumans is limited to only human avatars and struggles to preserve backgrounds due to the poor conditioning of source features. While decreasing timestep sampling has shown remarkable performance in text-to-3D generation tasks, to our best knowledge, it has not yet been conquered in general text-driven 3D editing tasks, such as maintaining the backgrounds and editing various objects.\nInstruct-NeRF2NeRF. Instruct-Nerf2NeRF (IN2N) (Haque et al., 2023), introduces Iterative Dataset Update (IterativeDU) to perform 3D editing of source NeRF scenes. IN2N leverages Instruct Pix2Pix (IP2P) (Brooks et al., 2023), which preserves the identity in 2D image editing by incorporating image guidance in the training objective. However, IN2N results in large variations in 3D space, which leads to inferior editing outcomes (Chen et al., 2024). This is attributed to IN2N fine-tuning source NeRF scenes with the edited images in 2D space, which indirectly affects 3D editing. The indirect 3D editing induces view-inconsistent results due to the complexity of 3D space (Koo et al., 2023).\nPosterior Distillation Sampling. We focus on SDS-based editing, which is a direct 3D editing method, for 3D-consistent results compared to IN2N. While DDS shows remarkable editing in 2D images, DDS suffers from quality degradation in 3D editing. Although 3D editing requires stronger identity preservation than 2D editing, DDS optimizes NeRF by minimizing the residual between the SDS loss of the source and target without any additional identity preservation regularization. PDS introduces a stochastic latent matching loss to add an explicit identity preservation term in DDS loss. The stochastic latent $z_t$, which contains the structural details of $x_0$, is calculated as $z_t(X_0,Y) = (X_{t-1} - \\mu_\\theta(x_t,y))/\\sigma_t$, where $\\sigma_t := \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{\\bar{\\alpha}_t} \\beta_t}$ and posterior mean $\\mu_\\theta(x_0,y) = (\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\bar{\\alpha}_t)x_{0|t} + \\sqrt{\\bar{\\alpha}_t}(1 - \\bar{\\alpha}_{t-1})x_t)/(1 - \\bar{\\alpha}_t)$. Therefore, the stochastic latent matching loss is as follows:\n$L_{PDS}(x_t = g(\\psi,c)) = E_{t,\\epsilon} [|| z_t(x^{tgt}, y^{tgt}) - z_t(x^{src}, y^{src}) ||^2]$ (10)\nBy ignoring the U-Net Jacobian term as SDS, the gradient of stochastic latent matching loss $L_{PDS}$ is represented as\n$\\nabla_{\\psi} L_{PDS} = E_{t,\\epsilon}[(z_t(x^{tgt}, y^{tgt}) - z_t(x^{src}, y^{src}))\n= E_{t,\\epsilon}[(\\Phi_{PDS}(t)(x^{tgt} - x^{src}) + \\Psi_{PDS}(t)(\\epsilon_\\theta(x^{tgt}_t, y^{tgt}, t) - \\epsilon_\\theta(x^{src}_t, y^{src}, t))\\frac{\\partial x_0}{\\partial \\psi}],$ (12)\nwhere $\\Phi_{PDS}(t)$ and $\\Psi_{PDS}(t)$ are the defined coefficients with respect to the timestep t. The gradient of stochastic latent matching is equivalent to eq. 12, which means the PDS loss implicitly involves the explicit identity preservation term and DDS gradient term, which is for editing."}, {"title": "3 DREAMCATALYST", "content": "3.1 MOTIVATION\nWe aim to design an objective function that, like PDS, includes an explicit term for strong identity preservation while aligning with the roles of diffusion timesteps and allowing for the application of decreasing timestep sampling. To achieve this goal, identity preservation has to be stressed in large noise perturbation and does not diverge in small levels of perturbation by reweighting each term of eq. 12. However, the nature of the formulation of stochastic latent matching implicitly includes an identity preservation term and the gradient of the DDS loss, making it incapable of directly adjusting the coefficients. Therefore, we provide a new interpretation of DDS and introduce a general formulation of PDS through this perspective for reweighting the terms. Furthermore, we propose a specialized formulation that aligns with the diffusion timestep roles and supports decreasing timestep sampling. The specialized formulation has mainly two advantages: (1) our formulation leads to fine-detailed 3D edited results by considering diffusion timestep roles and (2) immensely reduces training time with decreasing timestep sampling by diffusion-friendly sampling."}, {"title": "3.2 GENERAL FORMULATION OF PDS", "content": "In this section, we unveil the relationship between the reverse SDEdit process and DDS. The key insight of DreamCatalyst is that the objective of DDS is equivalent to the single-step DDIM-based SDEdit sampling. Equation 8 enables stochastic editing by solving the SDEs with random sampled noise. However, recent editing studies (Tumanyan et al., 2023; Cao et al., 2023) utilize the DDIM inversion to preserve the source identity. By combining the SDEdit and DDIM scheduling to preserve the source identity, the DDIM-based SDEdit sampling is defined as\n$x^{tgt}_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}x^{tgt}_{0|t} + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\tilde{\\epsilon},$ (13)\nwhere $x^{tgt}$ indicates the image to edit. Especially when we define $\\eta\\sqrt{\\beta_t} = 0$ for deterministic sampling, the noise is as $\\tilde{\\epsilon} = \\epsilon_\\theta(x^{src}_t, y^{src},t)$. In this case, DDIM inversion-based perturbed image $x = \\sqrt{\\bar{\\alpha}_t}x^{src} + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x^{src}_t, y^{src}, y^{src}, t)$ as the forward process. We can rewrite the eq. 13 as follows:\n$x^{tgt}_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}(\\frac{x^{tgt}_{t}}{\\sqrt{\\bar{\\alpha}_{t}}} - \\frac{\\sqrt{1-\\bar{\\alpha}_{t}}}{\\sqrt{\\bar{\\alpha}_{t}}} \\epsilon_\\theta(x^{tgt}_t, y^{tgt}, t)) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\tilde{\\epsilon}$ (14)\n$= \\sqrt{\\bar{\\alpha}_{t-1}}(\\frac{x^{tgt}_{t}}{\\sqrt{\\bar{\\alpha}_{t}}} + \\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}} \\epsilon_\\theta(x^{src}_t, y^{src}, t)) + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\tilde{\\epsilon}.$ (15)\nEven though the single-step denoising process of SDEdit is clear in the diffusion process with eq. 13, inspired by Dreamsampler (Kim et al., 2024), we can interpret the process as an optimization problem as follows:\n$x^{tgt}_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}x + \\sqrt{1 - \\bar{\\alpha}_{t-1}}\\tilde{\\epsilon},$ (16)\nwhere\n$x = \\underset{x^{tgt}}{\\text{arg min}} || \\frac{x^{tgt}_{t}}{\\sqrt{\\bar{\\alpha}_t}} ||^2 = \\underset{x^{tgt}}{\\text{arg min}} \\frac{\\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_t}} || \\epsilon_\\theta(x^{tgt}_t, y^{tgt}, t) - \\epsilon_\\theta(x^{src}_t, y^{src}, t) ||^2.$ (17)\nEquation 17 indicates that DDS objective is equivalent to the objective of the optimization problem, when $x^{tgt}_t = g(\\psi, c)$. Thus, solving DDS objective ensures equivalence to the single-step process of SDEdit, and optimizing the DDS objective with decreasing timestep sampling becomes the overall SDEdit framework. We notice that the proposed inversion is a proximal inversion. Conventional DDIM-inversion calculates $\\tilde{\\epsilon}$ with a multi-step inversion for pivoting. However, this multi-step inversion method necessitates extensive calculations for each multi-view image in 3D editing. To mitigate the computational burden, DreamCatalyst samples a single-step $\\tilde{\\epsilon}$ for each level of noise perturbation, which enables the proximal inversion due to different $\\tilde{\\epsilon}$ with respect to t.\nAlthough DDS objective enables the diffusion reverse process, optimizing solely with DDS objective induces suboptimal results due to the lack of identity preservation. Therefore, we attach additional"}, {"title": "3.3 DIFFUSION-FRIENDLY SDS-BASED EDITING", "content": "regularization terms since the DDS objective can be regarded as the optimization problem. Therefore, the general formulation of PDS is adding the identity preservation regularizer $R_{Iden}$ to the DDS objective as\n$L_{Iden}(x^{tgt}_t = g(y,c)) = E_{t,\\in}[\\Phi(t)R_{Iden} + \\Psi(t)L_{DDS}], where R_{Iden} =|| \\frac{\\partial x_0}{\\partial \\psi} ||^2,$ (18)\nand $\\Phi(t)$ and $\\Psi(t)$ are weighting functions of the general formulation. We notice that the regularizer $R_{Iden}$ preserves the identity and DDS loss $L_{DDS}$ takes charge of editing. We can now reweight the formulation of PDS with eq. 18.\nIn this section, we propose a specialized formulation of eq. 18, which considers diffusion timestep roles and the alignment with the decreasing timestep sampling. The design choice of formulation in DreamCatalyst aims to satisfy two conditions: (1) strong identity preservation in high timesteps and (2) reducing identity preservation in low timesteps. The first condition, strong identity preservation in high timesteps, reduces the information loss of source features in large levels of noise perturbation. This condition enables utilizing the decreasing timestep sampling. The second condition, weak identity preservation in low timesteps, leads to synthesizing fine details as the role of diffusion. The proposed specialized formulation of DreamCatalyst, which satisfies two conditions, is as follows:\n$L_{DreamCatalyst}(x^{tgt}_t = g(\\psi, c)) = E_{t,\\epsilon}[\\Phi^*(t)R_{Iden} + \\Psi^*(t)L_{DDS}],$ (19)\nwhere\n$\\Phi^*(t) = \\chi e^{t/T}, \\Psi^* (t) = \\delta + \\gamma\\sqrt{t/T}$ (20)\nand $\\chi, \\delta, \\gamma$ are hyperparameters, respectively. We set $\\chi = 0.075, \\delta = 0.2$, and $\\gamma = 0.8$ for all experiments. As shown in Fig. 2b, the formulation of DreamCatalyst fulfills the two conditions, thereby decreasing timestep sampling is applicable in our formulation.\nThe SDEdit process with minimizing $L_{DreamCatalyst}$ requires diffusion reverse process-likely timestep sampling. To achieve the objective, we adopt decreasing timestep sampling, which uniformly samples timestep $t = T \\rightarrow 1$. We notice that despite the non-increasing timestep sampling (Huang et al., 2023) is also a good option, we employ decreasing timestep sampling for fulfilling eq. 16 as possible in entire timesteps. Comprehensively, the proposed objective function and decreasing timestep sampling enable the SDEdit process with a parameterized model, especially NeRF in this paper. The overall framework of DreamCatalyst is shown in Fig. 3.\nFinally, we omit the U-Net Jacobian term as previous works to calculate the gradient of $L_{DreamCatalyst}$ as\n$\\nabla L_{DreamCatalyst} (x^{tgt} = g(\\psi, c)) = E_{t,\\epsilon}[(\\Phi^*(t)\\nabla_{\\psi} R_{Iden} + \\Psi^*(t)\\nabla_{\\psi} \\epsilon^{tgt}_{\\theta}(t) L_{DDS})].$ (21)\nWe notice that we demonstrate fulfilling two conditions enables effective 3D editing with a special case as Fig. 1. We leave the exploration of more optimal design choices for the formulation to future work."}, {"title": "3.4 ENHANCING EDITABILITY WITH FREEU", "content": "Although the SDEdit process with the proposed loss function improves editing quality, designing the objective function has limited improvements due to inherent trade-offs. Editability and identity preservation are well-known trade-offs in editing tasks (Meng et al., 2021). Modifying the model architecture presents an alternative approach to enhancing editing quality, offering performance gains that cannot be achieved through loss function design alone.\nWe introduce utilizing FreeU in 3D editing to enhance editability without additional memory usage and computational costs. FreeU suppresses high-frequency features by scaling up the backbone features, which contain a large amount of low-frequency information (Si et al., 2023). The amplifying backbone features stresses low-frequency features, thereby relatively reducing the impact of high-frequency features. Consequently, suppressing high-frequency features increases editability as sharp characteristics of high-frequency features are smoothed as the edge features are weakened. Moreover, identity preservation, corresponding to the low-frequency domain, is maintained by amplifying backbone features. In conclusion, FreeU enhances the editability without compromising identity preservation."}, {"title": "3.5 TEXT-GUIDANCE IN DREAMCATALYST", "content": "In DreamCatalyst, we employed InstructPix2Pix (IP2P), which is prevalently used in NeRF and 3D Gaussian Splatting editing (Kim et al., 2023; Palandra et al., 2024), for instructive editing. The guidance of IP2P is composed of image and text conditioning. DreamCatalyst sets $w_y = 0$ for $\\epsilon(x, y^{src}, t)$ as Collaborative Score Distillation (CSD) (Kim et al., 2023), because contents in target and source prompts are often intersected. This setting prevents interruption in guidance toward the intersected contents. The image and text-guided noise prediction is calculated as follows:\n$\\epsilon^{tgt}_{\\theta}(x, y^{tgt}, t) = \\epsilon_{\\theta}(x, y_{\\emptyset}, t) + w_r(\\epsilon_{\\theta} (x_{t}, y^{tgt}_{t}, x^{tgt},t) - \\epsilon_{\\theta} (x_{t}, y_{\\emptyset}, x^{tgt},t))$ (22)\n$\\epsilon^{src}_{\\theta}(x, y^{src}, t) = \\epsilon_{\\theta}(x, y_{\\emptyset}, t) + w_r(\\epsilon_{\\theta} (x_{t}, y^{src}_{t}, x^{src},t) - \\epsilon_{\\theta} (x_{t}, y_{\\emptyset}, x^{src},t)),$ (23)\nwhere $w_r$ is a scale of image-guidance, and $x^{src}$ and $x^{tgt}$ are conditions of a source image and a null-image, respectively."}, {"title": "4 EXPERIMENTS", "content": "We conduct experiments on real scenes using datasets from IN2N and PDS. The types of scenes include a sitting person, a full-body person, a face, objects, and outdoor scenes. We evaluate our method and baselines in 8 scenes with 40 pairs of source and target text prompts. For comparisons,"}, {"title": "4.5 ABLATIONS", "content": "FreeU. We empirically demonstrate the effectiveness of FreeU in our method. FreeU is based on a U-Net architecture of diffusion models, modifying the scale of upsampling features in its decoder using a parameter b. As illustrated in Fig. 6 (a)-(d), the results from using FreeU with b = 1.1 exhibit better quality compared to those obtained without FreeU (i.e., using the vanilla U-Net model). We maintained the s value consistent with the original FreeU settings, which control the scale of low-frequency features in skip connections. In this framework, increasing the value of b leads to the suppression of high-frequency components in an image. We hypothesize that this characteristic of FreeU facilitates easier editing. However, if b is set too high, the editing process becomes excessively easy. This hypothesis is supported by the results shown in Fig. 6 (b) and (c), where the use of FreeU with b = 1.3 results in excessive editing. This over-editing is not confined to the primary subject but extends to the background as well. While increasing b in FreeU can enhance the editing process, excessive suppression of high-frequency components can lead to overly smooth results and unintended editing artifacts.\nDecreasing timestep sampling. In this section, we demonstrate the validity of decreasing timestep sampling. For a fair comparison, the experimental settings are identical, with 500 iterations used in each case, except for the timestep sampling algorithm. As shown in Fig. 6, the editing result with decreasing timestep sampling converges to a fine-detailed result, while the result with random timestep sampling exhibits over-saturated colors and fails to maintain background consistency within the same editing time. Thus, decreasing timestep sampling leads to faster convergence."}, {"title": "5 CONCLUSION", "content": "We propose a general formulation for 3D editing by unveiling the relationship between the reverse SDEdit process and DDS. Based on this formulation, we introduce DreamCatalyst, which considers the dynamics of a diffusion process, to edit 3D scenes with an SDS-based approach as the reverse SDEdit process. Moreover, we suggest using FreeU in Score Distillation to overcome the trade-offs between editability and identity preservation inherent in the formulation. As a result, DreamCatalyst achieves fast and high-quality 3D editing. Through comparative analysis and user studies, we demonstrate that DreamCatalyst surpasses state-of-the-art methods in both performance and editing speed."}]}