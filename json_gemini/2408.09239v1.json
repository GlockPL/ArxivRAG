{"title": "Towards Effective Top-N Hamming Search via Bipartite Graph Contrastive Hashing", "authors": ["Yankai Chen", "Yixiang Fang", "Yifei Zhang", "Chenhao Ma", "Yang Hong", "Irwin King"], "abstract": "Searching on bipartite graphs serves as a fundamental task for various real-world applications, such as recommendation systems, database retrieval, and document querying. Conventional approaches rely on similarity matching in continuous Euclidean space of vectorized node embeddings. To handle intensive similarity computation efficiently, hashing techniques for graph-structured data have emerged as a prominent research direction. However, despite the retrieval efficiency in Hamming space, previous studies have encountered catastrophic performance decay. To address this challenge, we investigate the problem of hashing with Graph Convolutional Network for effective Top-N search. Our findings indicate the learning effectiveness of incorporating hashing techniques within the exploration of bipartite graph reception fields, as opposed to simply treating hashing as post-processing to output embeddings. To further enhance the model performance, we advance upon these findings and propose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel dual augmentation approach to both intermediate information and hash code outputs in the latent feature spaces, thereby producing more expressive and robust hash codes within a dual self-supervised learning paradigm. Comprehensive empirical analyses on six real-world benchmarks validate the effectiveness of our dual feature contrastive learning in boosting the performance of BGCH+ compared to existing approaches.", "sections": [{"title": "INTRODUCTION", "content": "Bipartite graphs are ubiquitous in the real world for the ease of modeling various Web applications, e.g., as shown in Figure 1(a), user-product recommendation [1], [2], and online query-document matching [3]. The fundamental task of Top-N search involves selecting the best-matched graph nodes for a given query node, enabling effective information filtering. Machine learning advancements have popularized the use of vectorized representations (a.k.a. embeddings) for similarity matching [4], [5], with Graph Convolutional Networks (GCNs) standing out for their remarkable performance in capturing high-order connection information and enriching node embeddings [6], [7]. In addition to embedding informativeness, addressing computation latency and memory overhead is crucial for practical application deployment. Recently, learning to hash [8], [9] recently provides an alternative option to graph-based models for optimizing the model scalability. Generally, it learns to convert continuous values into the finite binarized hash codes. In lieu of using full-precision\u00b9 embeddings, this approach offers space reduction and computation acceleration for Top-N object matching and retrieval in the Hamming space, providing scalability amidst the context of explosive data growth.\nDespite the promising advantages of bridging GCNs and learning to hash, simply stacking these two techniques is trivial and thus falls short of performance satisfaction. Firstly, compared to continuous embeddings, hash codes with the same vector dimension are naturally less expressive with finite encoding permutation in Hamming space (e.g., 2\u1d48 if the dimension is d). Consequently, this not only leads to a coarse-grained information encoding of the graph nodes, but further derives inaccurate estimation of the pairwise node similarity. Therefore, the model exhibits a conspicuous performance decay in Top-N matching and ranking. Secondly, for O(1) complexity encoding, $sign(.)$ function is usually adopted in recent work [10], [11], [12], [13]. Despite the simplicity, optimizing neural networks with $sign(.)$ is not easy, as $sign(.)$ is not differentiable at 0 and its derivatives are 0 anywhere else. Previous models usually use visually similar but not necessarily theoretically relevant functions, e.g., $tanh(.)$, for gradient estimation. This may lead to inconsistent optimization directions between forward and backward propagation. Their associated loss landscapes are usually steep and bumping [14], which further increases the difficulty in optimization. These factors jointly lead to an intractable model training process.\nTo tackle these aforementioned challenges, we have progressively studied the problem of learning to hash with GCN on bipartite graphs in [15]. We identify the critical effect of mining the high-order correlation knowledge in bipartite graph exploration and hashing. In response to this, we have developed an effective learning framework namely BGCH (short for Bipartite Graph Convolutional Hashing) [15]. Generally, the primary objective of BGCH is to enhance the expressivity of the learned hash codes while ensuring an accordant and robust model optimization flow. By leveraging BGCH, the empirical results demonstrate notable performance superiority and competitiveness when compared to both hashing-based and full-precision-based counterparts, respectively. Therefore, BGCH strikes a delicate balance between matching accuracy and computational efficiency, making it a desirable solution for real-world scenarios with limited computation resources.\nHowever, there may still exist two major inadequacy of BGCH [15]. Firstly, due to the discreteness of hash codes, the learning of BGCH [15] in generating informative hash codes is characterized by unpleasant inefficiency, as it typically necessitates substantial training iterations for convergence. Secondly, BGCH currently focuses on alleviating the information loss during the process of graph convolutional hashing, but ignores another fundamental point in learning quality of graph information extraction, especially for sparse graphs. As we include six real-world datasets, their graph densities vary from 0.0419, 0.00084, 0.00267, to 0.0013, 0.00062, and 0.02210. Since the graph convolutions learn and extract the topological information, these sparse graphs may however provide limited structural knowledge as the supervision signal of model learning, resulting in a less effective subsequent graph hashing. To overcome these inadequacies, we seek to absorb the self-supervised learning capability by introducing additional regularization signals to BGCH. Notably, we expect to empower BGCH with the contrastive learning methodology [16], [17], [18] that can extract meaningful and discriminative features from unlabeled data and regularize representations with good generalization capabilities. The conventional way to apply contrastive learning on graphs is to first obtain different views of graph structures by explicit data augmentation, e.g., stochastic dropout of nodes, edges, or subgraphs with a certain probability [18], [19], [20]. Then the learning objective is to maximize the consistency of the same samples under different views and distinguish different samples simultaneously. While it seems straightforward to apply similar augmentation techniques to our topic of interest, such stochastic data manipulation may however introduce undesired variations to the learning process, as graph hashing is more sensitive to such structure variations due to its inherent discreteness property.\nTo provide a more robust contrastive learning paradigm for graph hashing, in this work we put forward the model Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ builds upon the topology-aware hashing paradigm of BGCH while proposing a novel dual feature contrastive learning framework. Specifically, different from the conventional augmentation to input data, we propose to conduct feature-level augmentations to both intermediate information and binarized hash codes when training. Via random noise addition, BGCH+ generates perturbation between contrastive views while maintaining the noise with controlled magnitude. This feature-based augmentation approach, as opposed to explicit manipulation of the graph structure, is thus more flexible and efficient to implement. Subsequently, these two streams of augmentations are employed within a dual feature contrastive learning objective, incorporating distinct learning granularities. As shown in the lower section of Figure 1(b), this learning mechanism of BGCH+ eventually enhances the distribution uniformity of the resultant target hash codes, thereby improving their robustness when confronted with structural variations.\nBased on the well-learned hash codes, BGCH+ substantially improves the performance in Top-N Hamming space retrieval. The quality-cost trade-off is summarized in the upper section of Figure 1(b), where BGCH+ is compared to several representative counterparts, including float32-based and hashing-based models. The evaluation is conducted on a real-world bipartite graph with over 10 million observed edges, and further experimental details can be found in \u00a7 5.1. Notably, as the figure lower-right corner indicates the ideal optimal performance, BGCH+ surpasses BGCH and even achieves a comparable prediction accuracy to existing full-precision models, while still maintaining over 8\u00d7 computation acceleration. To summarize, our early model BGCH [15] has the following contributions:\n\u2022 BGCH studies the problem of learning to hash on bipartite graphs with graph representation learning, and proposes an effective approach for effective and efficient Top-N search in Hamming space.\n\u2022 BGCH provides both theoretical and empirical effectiveness on several datasets and demonstrates efficiency in both time and space complexity.\nExtending these early findings, our advanced model BGCH+ further presents the new contributions as follows:\n\u2022 BGCH+ focuses on improving the learning quality of graph convolutional hashing via leveraging self-supervised learning. To the best of our knowledge, BGCH+ is the first to elucidate benefits of contrastive learning for graph hashing.\n\u2022 BGCH+ proposes a novel dual feature contrastive learning paradigm that operates on feature augmentations of intermediate information and output hash codes, which is different with the conventional manner of complicated structural manipulation.\n\u2022 We conduct a comprehensive evaluation on six real-world datasets. The empirical analyses demonstrate the efficacy of learning high-quality hash codes and its superiority in surpassing its predecessor BGCH and other counterparts.\nWe organize this paper as follows. We introduce the preliminaries in \u00a7 2 and formally present BGCH+ methodology in \u00a7 3. The complexity analysis is conducted in \u00a7 4. Then"}, {"title": "PRELIMINARIES AND PROBLEM FORMULATION", "content": "Graph Convolution Network (GCN). The general idea of GCN is to learn node embeddings by iteratively propagating and aggregating latent features of node neighbors via the graph topology [21], [7], [22]:\n$V^{(l)} = AGG (V^{(l-1)}, \\{V_z^{(l-1)} : z \\in N(x)\\})$,\nwhere $V^{(l)} \\in R^d$ denotes node x's embedding after l-th iteration of graph convolutions, indexed in the embedding matrix V. $N(x)$ is the set of x's neighbors. Function $AGG(.,.)$ is the information aggregation function, with several implementations in previous work [22], [6], [23], [24], mainly aiming to transform the center node feature and the neighbor features. In this work, we adopt the state-of-the-art graph convolution paradigm [7].\nBipartite Graph and Adjacency Matrix. The bipartite graph is denoted as $G = \\{V_1, V_2, E\\}$, where $V_1$ and $V_2$ are two disjoint node sets and E is the set of edges between nodes in $V_1$ and $V_2$. We can use $Y \\in R^{|V_1|\u00d7|V_2|}$ to indicate the edge transactions, where 1-valued entries, i.e., $Y_{x,y} = 1$, indicate there is an observed edge between nodes $x \\in V_1$ and $y \\in V_2$, otherwise $Y_{x,y} = 0$. Then the adjacency matrix A of the whole graph can be defined as:\n$A = \\begin{bmatrix}\n0 & Y\\\\\nY^T & 0\n\\end{bmatrix}$.\nProblem Formulation. Given a bipartite graph $G = \\{V_1, V_2, E\\}$ along with its adjacency matrix A, we devote to learn a hashing function:\n$F(A|\u0398) \u2192 2^d$,\nwhere $\u0398$ is the set of all learnable parameters and embeddings and F maps nodes into the d-dimensional Hamming space. Given two nodes in the bipartite graph, e.g., $x \\in V_1$ and $y \\in V_2$, their hash codes are $Q_x$ and $Q_y$. Then the probability of edge existence $Y_{x,y}$ between nodes $x \\in V_1$ and $y \\in V_2$ can be effectively and efficiently measured by the hash codes $Q_x$ and $Q_y$, i.e., $\\hat{Y}_{x,y} = f(Q_x, Q_y)$ where $f$ is a score function. Intuitively, the larger value $\\hat{Y}_{x,y}$ is, the more likely x and y are matched, i.e., an edge between x and y exists. We use bold uppercase and calligraphy characters for matrices and sets. The non-bolded denote graph nodes or scalars. Explanations of key notations used in this paper are attached in Table 1."}, {"title": "BGCH+: METHODOLOGY", "content": "Overview\nWe hereby formally introduce our BGCH+ model, which incorporates the following key modules: (1) adaptive graph convolutional hashing (\u00a7 3.2) provides an effective encoding approach to enhance the expressivity of hashed features significantly while ensuring efficient matching in Hamming space; (2) dual feature augmentation for contrastive learning (\u00a7 3.3) constructs effective data augmentations for both intermediate continuous information and target hash codes in dual manner, striving to improve the quality and discriminability of the ouputs. (3) Fourier serialized gradient\nAdaptive Graph Convolutional Hashing\nTo enhance expressivity and smooth loss landscapes, one approach is to incorporate the relaxation strategy. Apart from the topology-aware embedding hashing with $sign(.)$:\n$Q^{(l)} = sign(V^{(l)})$,\nwe introduce an effective way to increase the flexibility and smoothness of the learned representations, via additionally computing a layer-wise positive rescaling factor for each node as follows, such that $\u03b1^{(l)} \\in R^+$ and $V^{(l)} \u2248 \u03b1^{(l)}Q^{(l)}$:\n$\\alpha_x^{(l)} = \\frac{1}{\\left\\| V_x^{(l)} \\right\\|_1}$,\nInstead of treating these factors as learnable parameters, such deterministic computation substantially prunes the parameter search space while attaining the adaptive approximation functionality for different graph nodes. We demonstrate this in \u00a7 5.5 of experiments.\nAfter L iterations of hashing, we obtain the table of adaptive hash codes $Q = \\{\\alpha, Q\\}$, where $\u03b1 \\in R^{(|V_1|+|V_2|)\u00d7(L+1)}$ and $Q \\in R^{(|V_1|+|V_2|)\u00d7(L+1)\u00d7d}$. For each node x, its corresponding hash codes are indexed and assembled:\n$\\alpha_x = \u03b1_x^{(0)} \\left\\| \u03b1_x^{(1)} \\right\\| \u2026 \\left\\| \u03b1_x^{(L)}$, and $Q_x = Q_x^{(0)} \\left\\| Q_x^{(1)} \\right\\| \u2026 \\left\\| Q_x^{(L)}$.\nIntuitively, table Q encodes bipartite structural information that is propagated back and forth at different iteration steps l, i.e., from 0 to the maximum step L. It not only tracks the intermediate knowledge hashed for all graph nodes, but also maintains the value approximation to their original continuous embeddings, e.g., $v_x$. In addition, with the slightly more space cost (complexity analysis in \u00a7 4), such detached hash encoding approach still supports the bitwise operations (\u00a7 3.5) for accelerating inference and matching.\nDual Feature Contrastive Learning\nDual Feature Augmentation\nDiverging from conventional methods that manipulate graph structures, such as dropout of edges and nodes [18], [19], [20], which often prove to be intractable and time-consuming, our research focuses on exploring contrastive learning directly on features within the embedding spaces. Specifically, we introduce a procedure involving the addition of random noises [25], [26] to both the embeddings before and after hashing. Given the node x with the segments of its continuous embedding $V_x^{(l)}$ and binarized hash code $Q_x^{(l)}$ at the l-th layer of graph convolution, we achieve the feature-level augmentations as follows. Firstly, we conduct the augmentation for continuous embedding $V_x^{(l)}$ as:\n$V_x^{(l)'} = V_x^{(l)} + \\epsilon_x^{(l)'}, V_x^{(l)''} = V_x^{(l)} + \\epsilon_x^{(l)''}$.\nThese perturbation vectors $\u03f5_x^{(l)'}, \u03f5_x^{(l)''} \\in R^d$ are drawn by:\n$\\left\\| \\epsilon_x^{(l)} \\right\\|_2 = \u03c4$, $\u03f5_x^{(l)} = \\epsilon_x^{(l)} \\odot Q_x^{(l)}$,\nwhere $\u03f5^{(l)} \\in R^d \u223c U(0, 1)$ and $\\odot$ is the Hadamard product. The hyperparameter \u03c4 controls the embedding uniformity which will be empirically analyzed later in \u00a7 5.4. In Equation (8), the first constraint mainly shapes the magnitude of perturbation vector, e.g., $\u03f5^{(l)'}_x$, and it numerically corresponds to the point on a hypersphere with the radius \u03c4. The second constraint ensures that $V_x^{(l)}, \u03f5^{(l)'}_x$, and $\u03f5^{(l)''}_x$ are located in the same region. This requirement constrains them to have the same direction with $Q_x^{(l)}$ and prevents the addition of noises from causing significant deviations in $V_x^{(l)}$. We visually depict the process in Figure 3(a).\nBy incorporating these scaled noise vectors to $V_x^{(l)}$, we essentially rotate them by two small angles, denoted as $\u03b8_x^{(l)'}$ and $\u03b8_x^{(l)''}$. Each rotation corresponds to a modification in $V_x^{(l)}$ and produces a pair of augmented representations, i.e., $V_x^{(l)'}$ and $V_x^{(l)''}$. Due to the small rotation magnitude, the augmented embeddings capture both the essential features of the original information and introduce slight and acceptable variations.\nLikewise, we implement the feature augmentation on the output hash codes. However, due to the discreteness of Hamming space, it would introduce significant variations\nDual Feature Contrastive Learning Objectives\nAfter iterative graph convolutions, we achieve two views of feature-based augmentations. For continuous intermediate information, we have the contrastive optimization term as:\n$L_{cl}^a = \u2211_{x \\in B} - log(\\frac{exp(V_x'V_x''/\u03c3)}{\u2211_{y \\in B} exp(V_x'V_y''/\u03c3)})$,\nwhere B denotes a training batch and the hyperparameter $\u03c3 > 0$. $V_x'$ and $V_x''$ are concatenated representations from:\n$V_x' = V_x^{(0)'} \\left\\| V_x^{(1)'} \\right\\| \u2026 \\left\\| V_x^{(L)'}, V_x'' = V_x^{(0)''} \\left\\| V_x^{(1)''} \\right\\| \u2026 \\left\\| V_x^{(L)''}$.\nOn the other hand, for the hash code $Q_x$, we can similarly obtain two augmented views $Q_x'$ and $Q_x''$ and have:\n$L_{cl}^b = \u2211_{x \\in B} - log(\\frac{exp(Q_x'^TQ_x''/\u03c3)}{\u2211_{y \\in B} exp(Q_x'^TQ_y'')/\u03c3})$.\nwhere $\\left\\| Q_x' \\right\\|_2$ and $Q_x'^TQ_y''$ can be efficiently computed in Hammming space (introduced later in \u00a7 3.5).\nGenerally, these two contrastive loss terms encourage consistency between the augmented representations of the same node x, e.g., $Q_x'$ and $Q_x''$, while maximizing the dis-\nFourier Serialized Gradient Estimation\nTo provide the accordant gradient estimation for hash function $sign(.)$, we approximate it by utilizing its Fourier Series decomposition in the frequency domain. Specifically, $sign(.)$ can be regarded as a special instance of the periodical Square Wave Function $t(x)$ within the interval of length 2H, i.e., $sign(\u03c6) = t(\u03c6), |\u03c6| < H$. By decomposing $t(x)$ into its Fourier Series form, we shall have:\n$sign(\u03c6) = \\frac{4}{\u03c0} \u2211_{i=1,3,5,...}^{+\u221e} \\frac{1}{i} sin(\\frac{\u03c0i\u03c6}{H}), where |\u03c6| < H$.\nFourier Series decomposition of sign(.) with infinite terms is a lossless transformation [27]. Therefore, as depicted in Figure 2(c), we can set the finite expanding term n to obtain its approximation version as follows:\n$sign(\u03c6) = \\frac{4}{\u03c0} \u2211_{i=1,3,5,...}^{n} \\frac{1}{i} sin(\\frac{\u03c0i\u03c6}{H})$.\nThe corresponding derivatives can be obtained as:\n$\\frac{dsign(\u03c6)}{\u2202\u03c6} = \\frac{4}{H} \u2211_{i=1,3,5,...}^{n} cos(\\frac{\u03c0i\u03c6}{H})$.\nUnlike other gradient estimators such as tanh-alike [28], [10] and SignSwish [29], approximating the $sign(.)$ function with its Fourier Series does not distort the main direction of the true gradients during model optimization [30]. This characteristic is advantageous as it facilitates a coordinated transformation from continuous values to their corresponding binarizations for node representations. Consequently, it effectively preserves the discriminability of hash codes and leads to improved retrieval accuracy. We present this performance comparison in \u00a7 5.7 of experiments. In summary, as indicated in Equation (16), we utilize the strict $sign(\u00b7)$ during forward propagation to encode hashing embeddings, while estimating the gradients $\\frac{\u2202sign(\u00b7)}{\u2202\u03c6}$ for backward propagation.\n$Q^{(l)} = sign(\u03c6)$, Forward propagation.\n$\\frac{\u2202Q^{(l)}}{\u2202\u03c6} = \\frac{4}{H} \u2211_{i=1,3,5,...}^{n} cos(\\frac{\u03c0i\u03c6}{H})$, Backward propagation.\nModel Prediction and Optimization\nMatching score prediction\nGiven two nodes $x \\in V_1$ and $y \\in V_2$, one natural manner to implement the score function is inner-product, mainly for its simplicity as:\n$\\hat{Y}_{x,y} = (\u03b1_xQ_x)^T (\u03b1_yQ_y)$.\nHowever, the inner product in Equation (17) is still conducted in the (continuous) Euclidean space with full-precision arithmetics. To bridge the connection between the inner product and Hamming distance measurement, we introduce the theorem:"}, {"title": "Theorem 1 (Hamming Distance Matching)", "content": "Given two hash codes, we have $(\u03b1_xQ_x)^T (\u03b1_yQ_y) = \u03b1_x\u03b1_y (d\u22122D_H(Q_x, Q_y))$.\nProof.\n$Q_xQ_y$ \n$= |\\{i|(Q_x)_i = (Q_y)_i = 1\\}| + |\\{i|(Q_x)_i = (Q_y)_i = \u22121\\}|$\n$- |\\{i|(Q_x)_i \u2260 (Q_y)_i\\}|$\n$= d - 2\u00b7 |\\{i|(Q_x)_i \u2260 (Q_y)_i\\}| = d \u2013 2D_H(Q_x, Q_y))$,\nwhich completes the proof.\n$D_H(\u00b7, \u00b7)$ is Hamming distance between two inputs. By applying Theorem 1, we transform the score computation to the Hamming distance matching. This transformation allows to significantly reduce the number of floating-point operations (#FLOPs) in the original score computation formulation (Equation (17)) to efficient Hamming distance matching. Consequently, this can develop substantial computation acceleration, as further analyzed in \u00a7 4.\nMulti-loss Objective Function\nOur objective function comprises two components, i.e., BPR loss $L_{bpr}$ and contrastive learning loss $L_{cl}$. Generally, these two loss functions harness the regularization effect to each other. The rationale behind this design is:\n\u2022 $L_{bpr}$ ranks the matching scores computed from the hash codes of a given pair of graph nodes.\n\u2022 $L_{cl}$ measures the consistency of nodes under different augmentation views represented by both continuous and binarized hash codes.\nConcretely, we implement $L_{bpr}$ with Bayesian Personalized Ranking (BPR) loss as follows:\n$L_{bpr} = - \u2211_{x \\in V_1} \u2211_{y \\in N(x)} \u2211_{y' \\notin N(x)} ln \u03c3(\u0176_{x,y} \u2013 \u0176_{x,y\u2019})$.\n$L_{bpr}$ encourages the predicted score of an observed edge to be higher than its unobserved counterparts [7]. As for $L_{cl}$, we take summation of both loss terms, i.e., $L_{cl}^a$ and $L_{cl}^b$, introduced in \u00a7 3.3, for dual feature contrastive learning:\n$L_{cl} = L_{cl}^a + L_{cl}^b$\nLet $\u03bb_1$ be a hyperparameter and $\u0398$ denote the set of trainable embeddings regularized by the parameter $\u03bb_2$ to avoid overfitting. Our final objective function is defined as:\n$L = L_{bpr} + \u03bb_1 L_{cl} + \u03bb_2 \\left\\| \u0398 \\right\\|_2^2$.\nWhile $L_{bpr}$ focuses on pairwise preferences and rankings, the dual contrastive learning loss helps in capturing fine-grained similarities and differences between node representations. By incorporating these loss terms, the model not only prioritizes the learning of ranking information embedded in the outputs, but also produces high-quality hash codes with better performances. The ablation study of such multi-loss optimization design is conducted in \u00a7 5.5.\nSo far, we have introduced all technical parts of BGCH+ and attached the pseudocodes in Algorithm 1. To better understand the model scalability of BGCH+, we provide the complexity analysis in the following section."}, {"title": "COMPLEXITY ANALYSIS", "content": "Training time complexity. |E|, |B|, s, and n are the edge number, batch size, numbers of train iterations and Fourier Series decomposition terms, respectively. As shown in Table 2, we derive that: (1) The time complexity of the graph normalization to the original adjacency matrix, i.e., $D^{-1/2}AD^{-1/2}$, is O(2|E|). (2) In graph convolution and hashing, the time complexity is O(2dsL|E|), where L \u2264 4 is a common setting [7], [31], [22], [6] to avoid over-smoothing [32]. (3) BGCH+ takes O(2sd|E|) to compute $L_{bpr}$ loss. As for $L_{cl}$ loss, for each node x, we need to conduct random noise addition to all other nodes in B, as shown in Equations (10-12), which is known as the widely-used in-batch sampling [17]. Thus the total loss complexity for our dual feature contrastive learning would be O(2|B|sd|E|). (4) Lastly, BGCH+ takes O(snd|E|) to estimate the gradients for the d-dimension hash codes. Thus, the total complexity in total is quadratic to the graph edge numbers, i.e., |E|. This is common in GCN frameworks and actually acceptable for large bipartite graphs, which may dispel the concerns of large training cost in practice.\nHash codes space cost. As shown in Table 3, the total space cost of hash codes is O(|V\u2081 U V\u2082|(d + 32(L + 1))) bits, supposing that we use float32 for those rescaling factors in L + 1 iterations. Compared to the continuous embedding size, i.e., 32 |V\u2081 U V\u2082| d bits, the size reduction ratio thus is:\nratio = $\\frac{32 \\left| V_1 \\cup V_2 \\right| d}{\\left| V_1 \\cup V_2 \\right|(d + 32(L + 1))} = \\frac{32d}{d + 32(L + 1)} = \\frac{32}{\\frac{d}{L + 1} + 32}$.\nBased on our previous explanation, it has been noted that excessively stacking iteration layers can lead to a decline in performance [32]. Therefore, if L \u2264 4, BGCH+ has the potential to achieve a significant increase in the size compression ratio, if the embedding dimension is increased.\nOnline matching. The original score formulation in"}, {"title": "Theorem 1 (Hamming Distance Matching)", "content": "Equation (17) contains d floating-point operations (#FLOPs). As shown in Table 3, using Hamming distance matching can convert the most of floating-point arithmetics to binary operations (#BOPs), with a few #FLOPs for scalar computations, i.e., 4 \u00ab d."}, {"title": "EXPERIMENTAL EVALUATION", "content": "We evaluate BGCH+ with the aim of answering the following research questions:\n\u2022 RQ1. How does BGCH+ compare to the state-of-the-art hashing-based models in Top-N Hamming retrieval?\n\u2022 RQ2. What is the performance gap between BGCH+ and full-precision models w.r.t. long-list retrieval quality?\n\u2022 RQ3. How does our proposed dual feature contrastive learning contribute to the performance of BGCH+?\n\u2022 RQ4. What are advantages of other model components?\n\u2022 RQ5. What is the resource consumption of BGCH+?\n\u2022 RQ6. How does Fourier Series decomposition perform in terms of retrieval accuracy and training efficiency?\nExperiment Setup\nDatasets and Evaluation Metrics. We include six real-world bipartite graphs in Table 4 that are widely evaluated [7], [33], [34], [35], [31], [36] as follows:\n1) MovieLens\u00b2 is a widely adopted benchmark for movie review. Similar to the setting in [13], if the user x has rated item y, we set the edge $Y_{x,y} = 1$,0 otherwise.\n2) Gowalla\u00b3 [31], [13], [7], [37] is the dataset [38] between customers and check-in locations collected from Gowalla.\n3) Pinterest\u2074 is an open dataset for image recommendation between users and images. Edges represent the pins over images initiated by users.\n4) Yelp2018\u2075 is from Yelp Challenge 2018 Edition, bipartitely modeling between users and local businesses.\n5) AMZ-Book\u2076 is the bipartite graph between readers and books, organized from Amazon-review [39].\n6) Dianping is a commercial dataset between users and local businesses recording their diverse interactions, e.g., clicking, saving, and purchasing.\nEvaluation Metrics. To assess the model performance in Hamming space retrieval over bipartite graphs, we use the hash codes to find the Top-N answers for a given query node based on the closest Hamming distances. We then evaluate the ranking capability using two commonly used evaluation protocols, i.e., Recall@N and NDCG@N.\nBaselines. In addition to BGCH, we include the following representative hashing-based models for (1) general"}, {"title": "Top-N Hamming Space Query (RQ1)", "content": "To assess the fine-to-coarse Top-N ranking capability, we fix N=1000. We initially present the results of Recall@20\u2081\u2080\u2080\u2080 and NDCG@20\u2081\u2080\u2080\u2080\u2078 for the Top-1000 search in Table 6. Additionally, we plot the holistic Recall and NDCG metric curves for the \\{20, 50, 100, 200, 500, 1000\\} of Top-1000 ranking in Figure 4. For fair comparison, we ensure that both BGCH+ and the baselines had the convolution iteration number of 2 and the embedding dimension of 256.\n\u2022 The results clearly establish the superiority of our BGCH+ over previous hashing-based models. (1) Firstly, as shown in Table 6, HashGNN outperforms traditional hashing-based baselines such as LSH, HashNet, CIGAR. This suggests that directly adapting conventional non-graph-based hashing methods may struggle to achieve comparable performance due to the effectiveness of graph convolutional architecture in capturing latent information within the bipartite graph topology for hash encoding preparation. (2) Secondly, both BGCH and BGCH+ consistently outperform HashGNN over all datasets, thanks to the proposed adaptive graph convolutional hashing. BGCH+ further achieves improvement over its vanilla version BGCH by 2.75%~20.19%, and 2.62%~19.00% w.r.t. Recall@20 and NDCG@20, respectively. This highlights the effectiveness of our newly proposed dual feature contrastive learning paradigm. A comprehensive analysis will be conducted in \u00a7 5.4. (3) Thirdly, we conduct the Wilcoxon signed-rank tests on BGCH+. The results confirm that all improvements of BGCH+ over BGCH are statistically significant at a 95% confidence level. Considering the performance improvement of BGCH+ and BGCH over all other methods, this demonstrates the effectiveness of all proposed modules contained therein. Detailed ablation study will be introduced in \u00a7 5.5.\n\u2022 By varying N from 20 to 1000, both BGCH and BGCH+ consistently demonstrate competitive performance compared to the baselines. The observations from Figure 4 are as follows: (1) Compared to the approximated version of HashGNN, i.e., HashGNN\u00a7, both BGCH and BGCH+ consistently exhibit stable and significant"}, {"title": "Comparing to FT32-based Models (RQ2)", "content": "In this section, we compare both BGCH and BGCH+ with several full-precision (FT32-based) models to evaluate the long-list search quality. The observations from Table 7 are as follows: (1) Both BGCH and BGCH+ consistently deliver competitive performance compared to early full-precision models, e.g., NeurCF and NGCF, across all datasets. In terms of the state-of-the-art model LightGCN, both BGCH and BGCH+ achieve over 87% of the Recall@1000 and 90% of NDCG@1000 capability. (2) We notice that, compared to NDCG@1000 metric, the Recall@1000 results are generally with larger numerical values. This is because the recall metric focuses on how many ground-truth items are re-"}, {"title": "Study of Dual Feature Contrastive Learning (RQ3)", "content": "In this section, we conduct a comprehensive empirical analysis to examine the impact of our dual feature contrastive learning on the quality of hashed representations.\nStructure Manipulation V.S. Feature Augmentation. The conventional contrastive learning usually requires explicit graph structure manipulation [50] such as:\n\u2022 Node dropout (denoted by ND): with a certain probability, the graph node and its connected edges are discarded.\n\u2022 Edge dropout (denoted by ED): it drops out the edges in a graph with a dropout ratio.\n\u2022 Graph random walk (denoted by GRW): it essentially operates as the multi-layer edge dropouts.\nWe implement these structural manipulation strategies and show the comparison results in Table 8. We can clearly observe that: (1) Edge dropout (ED) generally achieves the most competitive performance among all structure augmentation strategies. (2) Our model BGCH+ incorporates dual feature augmentation in the embedding space for contrastive learning, further improving performance across all datasets compared to the ED variant. This highlights the effectiveness of our proposed approach. (3) To provide a more fine-grained comparison, we further combine these strategies. As shown in Table 8, we observe that variants with ED generally perform well, compared to the other; however, even with all these strategies integrated, it still consistently under-performs our model BGCH+. (4) Considering the heavy time cost of all these explicit structural manipulation, our feature-wise augmentation offers more flexibility when training BGCH+ in batch on the fly. This makes it better suited for handling larger bipartite graphs.\nRegularization Effect of Representation Uniformity. Previous work [51] identifies that contrastive learning can help to improve the uniformity of image representations. To study its effect in learning hash codes, we visualize the"}, {"title": "Ablation Study (RQ4)", "content": "We evaluate the necessity of model components with Top-20 search metrics and report the results"}]}