{"title": "Towards Effective Top-N Hamming Search via Bipartite Graph Contrastive Hashing", "authors": ["Yankai Chen", "Yixiang Fang", "Yifei Zhang", "Chenhao Ma", "Yang Hong", "Irwin King"], "abstract": "Searching on bipartite graphs serves as a fundamental task for various real-world applications, such as recommendation systems, database retrieval, and document querying. Conventional approaches rely on similarity matching in continuous Euclidean space of vectorized node embeddings. To handle intensive similarity computation efficiently, hashing techniques for graph-structured data have emerged as a prominent research direction. However, despite the retrieval efficiency in Hamming space, previous studies have encountered catastrophic performance decay. To address this challenge, we investigate the problem of hashing with Graph Convolutional Network for effective Top-N search. Our findings indicate the learning effectiveness of incorporating hashing techniques within the exploration of bipartite graph reception fields, as opposed to simply treating hashing as post-processing to output embeddings. To further enhance the model performance, we advance upon these findings and propose Bipartite Graph Contrastive Hashing (BGCH+). BGCH+ introduces a novel dual augmentation approach to both intermediate information and hash code outputs in the latent feature spaces, thereby producing more expressive and robust hash codes within a dual self-supervised learning paradigm. Comprehensive empirical analyses on six real-world benchmarks validate the effectiveness of our dual feature contrastive learning in boosting the performance of BGCH+ compared to existing approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Bipartite graphs are ubiquitous in the real world for the ease of modeling various Web applications, e.g., as shown in Figure 1(a), user-product recommendation [1], [2], and online query-document matching [3]. The fundamental task of Top-N search involves selecting the best-matched graph nodes for a given query node, enabling effective information filtering. Machine learning advancements have popularized the use of vectorized representations (a.k.a. embeddings) for similarity matching [4], [5], with Graph Convolutional Networks (GCNs) standing out for their remarkable performance in capturing high-order connection information and enriching node embeddings [6], [7]. In addition to embedding informativeness, addressing computation latency and memory overhead is crucial for practical application deployment. Recently, learning to hash [8], [9] recently provides an alternative option to graph-based models for optimizing the model scalability. Generally, it learns to convert continuous values into the finite binarized hash codes. In lieu of using full-precision\u00b9 embeddings, this approach offers space reduction and computation acceleration for Top-N object matching and retrieval in the Hamming space, providing scalability amidst the context of explosive data growth.\nDespite the promising advantages of bridging GCNs and learning to hash, simply stacking these two techniques is trivial and thus falls short of performance satisfaction. Firstly, compared to continuous embeddings, hash codes with the same vector dimension are naturally less expressive with finite encoding permutation in Hamming space (e.g., 2d if the dimension is d). Consequently, this not only leads to a coarse-grained information encoding of the graph nodes, but further derives inaccurate estimation of the pairwise node similarity. Therefore, the model exhibits a conspicuous performance decay in Top-N matching and ranking. Secondly, for O(1) complexity encoding, sign() function is usually adopted in recent work [10], [11], [12], [13]. Despite the simplicity, optimizing neural networks with sign(.) is not easy, as sign() is not differentiable at 0 and its derivatives"}, {"title": "2 PRELIMINARIES AND PROBLEM FORMULATION", "content": "Graph Convolution Network (GCN). The general idea of GCN is to learn node embeddings by iteratively propagating and aggregating latent features of node neighbors via the graph topology [21], [7], [22]:\nV = AGG (V^{-1}, {V^{-1} : z \u2208 N(x)}),\nwhere V \u2208 Rd denotes node x's embedding after l-th iteration of graph convolutions, indexed in the embedding matrix V. N(x) is the set of x's neighbors. Function AGG(,) is the information aggregation function, with several implementations in previous work [22], [6], [23], [24], mainly aiming to transform the center node feature and the neighbor features. In this work, we adopt the state-of-the-art graph convolution paradigm [7].\nBipartite Graph and Adjacency Matrix. The bipartite graph is denoted as G = {V1, V2, E}, where V\u2081 and V2 are two disjoint node sets and E is the set of edges between nodes in V\u2081 and V2. We can use Y \u2208 R|V1|\u00d7|V2| to indicate the edge transactions, where 1-valued entries, i.e., Y x,y = 1, indicate there is an observed edge between nodes x \u2208 V\u2081 and y \u2208 V2, otherwise Yx,y = 0. Then the adjacency matrix A of the whole graph can be defined as:\nA = \nProblem Formulation. Given a bipartite graph G = {V1, V2, E} along with its adjacency matrix A, we devote to learn a hashing function:\nF(\u0391\u0399\u0398) \u2192 2,\nwhere is the set of all learnable parameters and embeddings and F maps nodes into the d-dimensional Hamming space. Given two nodes in the bipartite graph, e.g., x \u2208 V1 and y \u2208 V2, their hash codes are Q and Qy. Then the probability of edge existence Yz,y between nodes x \u2208 V\u2081 and y \u2208 V2 can be effectively and efficiently measured by the hash codes Q and Qy, i.e., \u0176x,y = f(Qx, Qy) where f is a score function. Intuitively, the larger value Yx,y is, the more likely x and y are matched, i.e., an edge between x and y exists. We use bold uppercase and calligraphy characters for matrices and sets. The non-bolded denote graph nodes or scalars."}, {"title": "3 BGCH+: METHODOLOGY", "content": "We hereby formally introduce our BGCH+ model, which incorporates the following key modules: (1) adaptive graph convolutional hashing (\u00a7 3.2) provides an effective encoding approach to enhance the expressivity of hashed features significantly while ensuring efficient matching in Hamming space; (2) dual feature augmentation for contrastive learning (\u00a7 3.3) constructs effective data augmentations for both intermediate continuous information and target hash codes in dual manner, striving to improve the quality and discriminability of the ouputs. (3) Fourier serialized gradient"}, {"title": "3.2 Adaptive Graph Convolutional Hashing", "content": "To enhance expressivity and smooth loss landscapes, one approach is to incorporate the relaxation strategy. Apart from the topology-aware embedding hashing with sign(.):\nQ = sign(V),\nwe introduce an effective way to increase the flexibility and smoothness of the learned representations, via additionally computing a layer-wise positive rescaling factor for each node as follows, such that \u03b1\u2208 R+ and V \u2248 \u03b1Q:\n\u03b1 = 1.\nInstead of treating these factors as learnable parameters, such deterministic computation substantially prunes the parameter search space while attaining the adaptive approximation functionality for different graph nodes. We demonstrate this in \u00a7 5.5 of experiments.\nAfter L iterations of hashing, we obtain the table of adaptive hash codes Q = {a, Q}, where a \u2208 R(|V1|+|V2|)\u00d7(L+1) and Q\u2208 R(|V1|+|V2|)\u00d7(L+1)\u00d7d. For each node x, its corresponding hash codes are indexed and assembled:\n\u03b1 = \u03b1 ||\u03b1 || \u00b7\u00b7\u00b7||\u03b1L), and Q\u2081 = Q||Q1)||\u2026\u2026 ||QL).\nIntuitively, table Q encodes bipartite structural information that is propagated back and forth at different iteration steps l, i.e., from 0 to the maximum step L. It not only tracks the intermediate knowledge hashed for all graph nodes, but also maintains the value approximation to their original continuous embeddings, e.g., v. In addition, with the slightly more space cost (complexity analysis in \u00a7 4), such detached hash encoding approach still supports the bitwise operations (\u00a7 3.5) for accelerating inference and matching."}, {"title": "3.3 Dual Feature Contrastive Learning", "content": "Diverging from conventional methods that manipulate graph structures, such as dropout of edges and nodes [18], [19], [20], which often prove to be intractable and time-consuming, our research focuses on exploring contrastive learning directly on features within the embedding spaces. Specifically, we introduce a procedure involving the addition of random noises [25], [26] to both the embeddings before and after hashing. Given the node x with the segments of its continuous embedding V and binarized hash code Q at the l-th layer of graph convolution, we achieve the feature-level augmentations as follows. Firstly, we conduct the augmentation for continuous embedding V as:\nV = V + \u2208', V\" = V + \u2208).\nThese perturbation vectors \u2208', e\" \u2208Rd are drawn by:\n||\u2208 ||2 = \u03c4, \u2208 = \u2208 Q),\nwhere \u2208 \u2208 Rd ~ U(0, 1) and \u2299 is the Hadamard product. The hyperparameter \u03c4 controls the embedding uniformity which will be empirically analyzed later in \u00a7 5.4. In Equation (8), the first constraint mainly shapes the magnitude of perturbation vector, e.g., e', and it numerically corresponds to the point on a hypersphere with the radius \u03c4. The second constraint ensures that V, e', and e\" are located in the same region. This requirement constrains them to have the same direction with Q and prevents the addition of noises from causing significant deviations in V. We visually depict the process in Figure 3(a).\nBy incorporating these scaled noise vectors to V, we essentially rotate them by two small angles, denoted as \u03b8' and \u03b8\". Each rotation corresponds to a modification in V and produces a pair of augmented representations, i.e., V' and V\". Due to the small rotation magnitude, the augmented embeddings capture both the essential features of the original information and introduce slight and acceptable variations.\nLikewise, we implement the feature augmentation on the output hash codes. However, due to the discreteness of Hamming space, it would introduce significant variations"}, {"title": "3.3.2 Dual Feature Contrastive Learning Objectives", "content": "After iterative graph convolutions, we achieve two views of feature-based augmentations. For continuous intermediate information, we have the contrastive optimization term as:\nLcl = \u2211 - log\nwhere B denotes a training batch and the hyperparameter \u03c3 > 0. V and V\" are concatenated representations from:\nVa' = V\u00ba' ||V || ... ||V(L)', V\u2081\" = V\u00ba ||V ||...||V(L)\".\nOn the other hand, for the hash code Qx, we can similarly obtain two augmented views Q and Q\" and have:\nL\u00b2 = -log\nwhere ||Q||2 and QQy can be efficiently computed in Hammming space (introduced later in \u00a7 3.5).\nGenerally, these two contrastive loss terms encourage consistency between the augmented representations of the same node x, e.g., Q and Q\", while maximizing the dis-"}, {"title": "3.4 Fourier Serialized Gradient Estimation", "content": "To provide the accordant gradient estimation for hash function sign(), we approximate it by utilizing its Fourier Series decomposition in the frequency domain. Specifically, sign(.) can be regarded as a special instance of the periodical Square Wave Function t(x) within the interval of length 2H, i.e., sign(\u03c6) = t(\u03c6), |\u03c6| < H. By decomposing t(x) into its Fourier Series form, we shall have:\nsign(\u03c6) =  sin( ), where |\u03c6| < H.\nFourier Series decomposition of sign() with infinite terms is a lossless transformation [27]. Therefore, as depicted in Figure 2(c), we can set the finite expanding term n to obtain its approximation version as follows:\nsign(\u03c6) =  sin().\nThe corresponding derivatives can be obtained as:\ndsign(\u03c6)/ d\u03c6 =  cos( ).\nUnlike other gradient estimators such as tanh-alike [28], [10] and SignSwish [29], approximating the sign() function with its Fourier Series does not distort the main direction of the true gradients during model optimization [30]. This characteristic is advantageous as it facilitates a coordinated transformation from continuous values to their corresponding binarizations for node representations. Consequently, it effectively preserves the discriminability of hash codes and leads to improved retrieval accuracy. We present this performance comparison in \u00a7 5.7 of experiments. In summary, as indicated in Equation (16), we utilize the strict sign(\u00b7) during forward propagation to encode hashing embeddings, while estimating the gradients sign() for backward propagation.\nQ = sign(\u03c6), Forward propagation.\ndQ (1)/ d\u03c6 =  cos(), Backward propagation."}, {"title": "3.5 Model Prediction and Optimization", "content": "Given two nodes x \u2208 V\u2081 and y \u2208 V2, one natural manner to implement the score function is inner-product, mainly for its simplicity as:\nYx,y = (AxQx)T (AyQy).\nHowever, the inner product in Equation (17) is still conducted in the (continuous) Euclidean space with full-precision arithmetics. To bridge the connection between the"}, {"title": "3.5.2 Multi-loss Objective Function", "content": "Our objective function comprises two components, i.e., BPR loss Lbpr and contrastive learning loss Lcl. Generally, these two loss functions harness the regularization effect to each other. The rationale behind this design is:\nLbpr ranks the matching scores computed from the hash codes of a given pair of graph nodes.\nLcl measures the consistency of nodes under different augmentation views represented by both continuous and binarized hash codes.\nConcretely, we implement Lbpr with Bayesian Personalized Ranking (BPR) loss as follows:\nLbpr = - \u2211 \u2211 ln\u03c3(\u0176x,y \u2013 \u0176x,y').\nLbpr encourages the predicted score of an observed edge to be higher than its unobserved counterparts [7]. As for Lcl, we take summation of both loss terms, i.e., La and L introduced in \u00a7 3.3, for dual feature contrastive learning:\nLcl = L + L\nLet \u03bb\u2081 be a hyperparameter and \u0398 denote the set of trainable embeddings regularized by the parameter \u03bb\u2082 to avoid overfitting. Our final objective function is defined as:\nL = Lbpr + \u03bb1Lcl + \u03bb2||\u0398||2.\nWhile Lbpr focuses on pairwise preferences and rankings, the dual contrastive learning loss helps in capturing fine-grained similarities and differences between node representations. By incorporating these loss terms, the model not only prioritizes the learning of ranking information embedded in the outputs, but also produces high-quality hash codes with better performances."}, {"title": "4 COMPLEXITY ANALYSIS", "content": "Training time complexity. |E|, |B|, s, and n are the edge number, batch size, numbers of train iterations and Fourier Series decomposition terms, respectively. As shown in Table 2, we derive that: (1) The time complexity of the graph normalization to the original adjacency matrix, i.e., D\u00af\u00bd AD\u00af, is O(2|E|). (2) In graph convolution and hashing, the time complexity is O(2dsL/ER, \u3134), where L \u2264 4 is a common setting [7], [31], [22], [6] to avoid over-smoothing [32]. (3) BGCH+ takes O(2sd|E|) to compute Lbpr loss. As for Led loss, for each node x, we need to conduct random noise addition to all other nodes in B, as shown in Equations (10-12), which is known as the widely-used in-batch sampling [17]. Thus the total loss complexity for our dual feature contrastive learning would be O(2|B|sd|E|). (4) Lastly, BGCH+ takes O(snd|E|) to estimate the gradients for the d-dimension hash codes. Thus, the total complexity in total is quadratic to the graph edge numbers, i.e., |E|. This is common in GCN frameworks and actually acceptable for large bipartite graphs, which may dispel the concerns of large training cost in practice.\nHash codes space cost. As shown in Table 3, the total space cost of hash codes is O(|V\u2081 U V2|(d + 32(L + 1))) bits, supposing that we use float32 for those rescaling factors in L + 1 iterations. Compared to the continuous embedding size, i.e., 32 V\u2081 U V2 d bits, the size reduction ratio thus is:\nratio = .\nBased on our previous explanation, it has been noted that excessively stacking iteration layers can lead to a decline in performance [32]. Therefore, if L \u2264 4, BGCH+ has the potential to achieve a significant increase in the size compression ratio, if the embedding dimension is increased.\nOnline matching. The original score formulation in"}, {"title": "5 EXPERIMENTAL EVALUATION", "content": "We evaluate BGCH+ with the aim of answering the following research questions:\nRQ1. How does BGCH+ compare to the state-of-the-art hashing-based models in Top-N Hamming retrieval?\nRQ2. What is the performance gap between BGCH+ and full-precision models w.r.t. long-list retrieval quality?\nRQ3. How does our proposed dual feature contrastive learning contribute to the performance of BGCH+?\nRQ4. What are advantages of other model components?\nRQ5. What is the resource consumption of BGCH+?\nRQ6. How does Fourier Series decomposition perform in terms of retrieval accuracy and training efficiency?"}, {"title": "5.2 Top-N Hamming Space Query (RQ1)", "content": "To assess the fine-to-coarse Top-N ranking capability, we fix N=1000. We initially present the results of Recall@201000 and NDCG@2010008 for the Top-1000 search in Table 6. Additionally, we plot the holistic Recall and NDCG metric curves for the {20, 50, 100, 200, 500, 1000} of Top-1000 ranking in Figure 4. For fair comparison, we ensure that both BGCH+ and the baselines had the convolution iteration number of 2 and the embedding dimension of 256.\nThe results clearly establish the superiority of our BGCH+ over previous hashing-based models. (1) Firstly, as shown in Table 6, HashGNN outperforms traditional hashing-based baselines such as LSH, HashNet, CIGAR. This suggests that directly adapting conventional non-graph-based hashing methods may struggle to achieve comparable performance due to the effectiveness of graph convolutional architecture in capturing latent information within the bipartite graph topology for hash encoding preparation. (2) Secondly, both BGCH and BGCH+ consistently outperform HashGNN over all datasets, thanks to the proposed adaptive graph convolutional hashing. BGCH+ further achieves improvement over its vanilla version BGCH by 2.75%~20.19%, and 2.62%~19.00% w.r.t. Recall@20 and NDCG@20, respectively. This highlights the effectiveness of our newly proposed dual feature contrastive learning paradigm. A comprehensive analysis will be conducted in \u00a7 5.4. (3) Thirdly, we conduct the Wilcoxon signed-rank tests on BGCH+. The results confirm that all improvements of BGCH+ over BGCH are statistically significant at a 95% confidence level. Considering the performance improvement of BGCH+ and BGCH over all other methods, this demonstrates the effectiveness of all proposed modules contained therein.\nBy varying N from 20 to 1000, both BGCH and BGCH+ consistently demonstrate competitive performance compared to the baselines. The observations from Figure 4 are as follows: (1) Compared to the approximated version of HashGNN, i.e., HashGNN5, both BGCH and BGCH+ consistently exhibit stable and significant"}, {"title": "5.3 Comparing to FT32-based Models (RQ2)", "content": "In this section, we compare both BGCH and BGCH+ with several full-precision (FT32-based) models to evaluate the long-list search quality. The observations from Table 7 are as follows: (1) Both BGCH and BGCH+ consistently deliver competitive performance compared to early full-precision models, e.g., NeurCF and NGCF, across all datasets. In terms of the state-of-the-art model LightGCN, both BGCH and BGCH+ achieve over 87% of the Recall@1000 and 90% of NDCG@1000 capability. (2) We notice that, compared to NDCG@1000 metric, the Recall@1000 results are generally with larger numerical values. This is because the recall metric focuses on how many ground-truth items are re-"}, {"title": "5.4 Study of Dual Feature Contrastive Learning (RQ3)", "content": "In this section, we conduct a comprehensive empirical analysis to examine the impact of our dual feature contrastive learning on the quality of hashed representations.\nStructure Manipulation V.S. Feature Augmentation. The conventional contrastive learning usually requires explicit graph structure manipulation [50] such as:\nNode dropout (denoted by ND): with a certain probability, the graph node and its connected edges are discarded."}, {"title": "5.5 Ablation Study (RQ4)", "content": "We evaluate the necessity of model components with Top-20 search metrics and report the results in Table 10.\nEffect of Adaptive Graph Convolutional Hashing. We study this model component by setting two variants, where: (1) w/o AH-TA only disables the topology-awareness of hashing and uses the final encoder after all graph convolutions (similar to conventional approaches [13], [41]); (2) w/o AH-RF removes the rescaling factors. The results from Table 10 results produce to the following observations:\n1) The variant w/o AH-TA underperforms BGCH+. This indicates that solely relying on the final output embeddings from the Graph Convolutional Network (GCN) framework may not adequately capture the unique latent node features necessary for effective hashing, especially considering the rich structural information present at different graph depths. In contrast, BGCH+ leverages intermediate information to enrich the representations, resulting in topology-aware hashing that effectively addresses the limited expressivity of discrete hash codes.\n2) In addition to topology-aware hashing, the inclusion of rescaling factors (as introduced in Equation (5)) plays a crucial role in performance improvement. The removal of these factors from BGCH+ (variant w/o AH-RF) leads to significant performance decay. Although the computation of these factors is based on direct calculations and may not be theoretically optimal, they capture the numerical uniqueness of embeddings for subsequent hash encoding, which substantially enhances BGCH+'s prediction capability."}, {"title": "5.6 Resource Consumption Analysis (RQ5)", "content": "Due to the various value ranges over all six datasets, we compactly report the value ratios of BGCH+ over the state-of-the-art hashing-based model HashGNN, in Figure 7.\nModel Training Time Cost. The training time cost, represented by the metric \u201cT-Time\u201d in Figure 7, reveals that training HashGNN, is more time-consuming compared to our proposed model BGCH+. This difference can be attributed to the architectural disparities between the two models. HashGNN utilizes the earlier Graph Convolutional Network (GCN) framework [6] as the model back-"}, {"title": "5.7 Study of Fourier Gradient Estimation (RQ6)", "content": "We take our largest dataset Dianping for illustration and the analysis can be generally popularized to the other datasets.\nEffect of Decomposition Term n. We vary the decomposition term n from 1 to 16. As shown in Figure 8, we have two observations: (1) The choice of the decomposition term has a significant impact on the retrieval quality. Theoretically, larger values of n can provide more accurate gradient estimations. However, in practice, excessively large n may lead to overfitting. Therefore, it is advisable to choose a moderate value, such as n = 4 in Figure 8(a), to maximize model performance. (2) As we vary n from 1 to 16, the training time per iteration of BGCH+ gradually increased. This observation aligns with our complexity analysis in \u00a7 4, where we identified that the training cost is primarily associated with other modules like graph convolutional hashing, rather than the gradient estimation process.\nComparison with Other Gradient Estimators. We include several recent gradient estimators, i.e., Tanh-like [10], [28], SignSwish [29], Sigmoid [54], and projected-based estimator [55] (denoted as PBE). (1) The results summarized in Table 11 clearly demonstrate the superiority of our method over sign() function approximation in gradient estimation. As we have briefly explained, most existing estimators, which employ the visually similar function approximation to sign(.), generally provide better gradient estimation than Straight-Through Estimator (STE). (2) However, for bipartite graphs with high sparsity, e.g., Gowalla (0.00084) and AMZ-Book (0.00062), graph-based models may struggle to collect sufficient structural information for effective training of hash codes."}, {"title": "6 RELATED WORK", "content": "Graph Convolution Network (GCN). Early research primarily studies the graph convolutions in the spectral domain, such as Laplacian eigen-decomposition [56] and Chebyshev"}, {"title": "7 CONCLUSION", "content": "In this paper, we revisit the learning to hash for efficient Hamming space search over graph structure data and propose BGCH+ for performance improvement. Compared to its predecessor, BGCH+ is further equipped with a novel dual feature contrastive learning paradigm, which operates on the latent features instead of the input graphs. Such design well enhances the robustness of learned hash codes against variations and thereby promotes the extraction of graph semantics in hash encoding. The empirical analyses over six real-world datasets demonstrate that the proposed method consistently outperforms existing hashing-based models while providing an alternative to full-precision models in scenarios with limited resources."}]}