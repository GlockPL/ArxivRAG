{"title": "EyeDentify: A Dataset for Pupil Diameter Estimation based on Webcam Images", "authors": ["Vijul Shah", "Ko Watanabe", "Brian B. Moser", "Andreas Dengel"], "abstract": "In this work, we introduce EyeDentify, a dataset specifically designed for pupil diameter estimation based on webcam images. EyeDentify addresses the lack of available datasets for pupil diameter estimation, a crucial domain for understanding physiological and psychological states traditionally dominated by highly specialized sensor systems such as Tobii. Unlike these advanced sensor systems and associated costs, webcam images are more commonly found in practice. Yet, deep learning models that can estimate pupil diameters using standard webcam data are scarce. By providing a dataset of cropped eye images alongside corresponding pupil diameter information, EyeDentify enables the development and refinement of models designed specifically for less-equipped environments, democratizing pupil diameter estimation by making it more accessible and broadly applicable, which in turn contributes to multiple domains of understanding human activity and supporting healthcare.", "sections": [{"title": "1. Introduction", "content": "The cognitive state of humans is closely linked to features observable through their eyes. Fortunately, the accessibility of eye monitoring in everyday life is rapidly increasing, exemplified by recent advancements such as Apple's incorporation of camera-based eye tracking features [3, 14]. However, research in this domain primarily targets blink detection [17] and gaze estimation [37, 52], employing various methodologies, including the use of biomarkers [28], infrared spectrum reflected from the eyes [12], or image-based techniques [16]. In comparison, fewer explore pupil diameter estimation [5, 43], which also plays an undeniably crucial role in determining various physiological and psychological states. This oversight highlights a critical gap in the field, underscoring the need for more comprehensive approaches to fully leverage eye monitoring for cognitive state analysis:\nPrevious studies show that the analysis of pupil diameter serves as an indicator of stress [38], attention [29, 46], or cognitive work loads [21, 25, 39]. In addition, the diameter of the pupil is also closely linked to the activity of the locus coeruleus [20, 34], a brain region critical for managing both short-term and long-term memory functions [21, 26]. Pupil diameter is also used for health check purposes, such as checking pupillary light reflex of patients with intracranial lesions in an intensive care unit [24]. Accurate pupil diameter estimation is thus fundamental to enhancing the capabilities of image-based eye tracking.\nHowever, we identify three significant challenges in advancing the field of image-based pupil diameter estimation, which we want to address. The first challenge lies in collecting ground truth data. Previous works relied on capturing pupil images and subsequently measuring the diameter in pixels, a time-consuming process that is complicated by increasing participant numbers [5, 43]. We overcome this issue by applying a sensor substitution approach using Tobii eye-tracker with Tobii Pro SDK [1] as a reliable ground truth sensor. This approach allows for efficient data collection by acquiring ground truth diameter values from the eye-tracker and facial recordings via webcam.\nThe second challenge concerns data diversity. Previous studies have varied pupil diameter in participants by altering illumination displays [44]. We apply a similar approach, changing the computer display's color during our data collection. Unlike previous work [5, 43, 44], we impose fewer constraints, allowing them to choose their seating position and distance from the screen. This approach enables us to collect data under more natural, \u201cin the wild\" conditions, potentially enhancing the empirical validity of findings.\nThe third challenge is the prediction of pupil diameter itself. Previous studies [4, 22, 53] have highlighted that estimating gaze coordinates with a camera involves analyzing images of approximately 60 \u00d7 36 pixels [54]. The scale of our images will be smaller for pupil diameter estimation, necessitating analysis at an even finer resolution. This makes accurately predicting pupil diameters more complex than gaze estimation and presents a challenging task.\nIn conclusion, we contribute to the image-based pupil diameter estimation field as follows:\n1. Creation of the Pupil Diameter Dataset EyeDentify:\nWe use the Tobii eye-tracker to collect accurate ground truth pupil diameter measurements. Concurrently, we captured facial video recordings with a built-in webcam. In total, data from 51 participants were gathered, comprising 212,073 images (i.e., blinking eyes removed from the initial 226,912 images).\n2. Data-Collecting Application ChameleonView: Our own implemented web application called ChameleonView captures various pupil diameters. It displays various screen colors during webcam recordings to ensure a diversity of pupil diameters. Moreover, it captures the timestamp and records videos while participants look at the display.\n3. Initial Pupil Diameter Estimations: We evaluated several models to estimate pupil diameter sizes for both left and right eyes in a 5-fold cross-validation manner. For this, we tested ResNet-18 and ResNet-50, with ResNet-18 emerging as the most accurate, achieving a mean absolute error of 0.1340 \u00b1 0.0196 and 0.1403 \u00b1 0.0328 for the left and right eye, respectively."}, {"title": "2. Related Work", "content": "This section will examine existing datasets for eye monitoring, including gaze estimation and blink detection (Section 2.1). Furthermore, it surveys datasets for pupil diameter estimation (Section 2.2), as well as methods to estimate pupil diameters (Section 2.3)."}, {"title": "2.1. Gaze Estimation and Blink Detection Datasets", "content": "Various gaze estimation, blink detection, and facial image data have been collected under different conditions, which we want to examine in the following. For instance, MAEB [17] has collected 20 subjects' images from nine different angles for robust image-based blink detection purposes. The study aims to complement the open-source dataset HUST-LEBW [19] with additional comparisons. Another example is MPIIFaceGaze [54], which has collected a daily life gaze coordinates dataset from 15 subjects. The strength of this dataset is that it has different illumination conditions and a collection of various day conditions, ranging from nine days to three months. Another dataset is presented by Dembinsky et al. [9], which collected a 360-minute dataset of 19 subjects with their corresponding gaze coordinates while speaking and/or listening. Thus, it incorporates different cognitive activities during recording. An alternative data collection setting is presented by Gaze360 [22], which contains 238 subjects with 172,000 frame images. The distinguishing feature of this dataset is that it also contains outdoor data. Moreover, the camera used during data collection captured 360-degree view recordings, thus including the subjects' environment. Another varying axis during dataset collection is proposed by ETH-XGaze [53], which collected data from 110 subjects with 1,083,492 images under various hardware settings. More specifically, they used 18 high-resolution Canon 250D digital SLR cameras and four Walimex Daylight 250 light boxes. Lastly, VideoGazeSpeech [18] collected 35,231 frames from 29 videos. As a result, they extracted eye gaze data from existing videos; hence, the feature extraction approach varies from that of other studies.\nMost mentioned datasets focus on enhancing gaze estimation and blink detection through diverse and complex recording conditions, but none incorporate pupil diameters."}, {"title": "2.2. Pupil Diameter Datasets", "content": "To the best of our knowledge, two data datasets have been used in publications for pupil diameter estimation that employ diverse approaches for data collection. For instance, Ricciuti et al. recruited 17 subjects ranging in age from 15 to 74 years and collected 20-second RGB video recordings under varying light intensities using a GoPro Hero 6 camera at a frame rate of 30fps [41]. Similarly, Caya et al. gathered data from 16 subjects aged 20 to 40 years, employing a camera mounted 10 cm from the subject's face in a controlled setting. Their study quantified a pupil diameter of 20 pixels as equivalent to 5 mm [5].\nDespite these efforts, the mentioned works have not made their datasets publicly available. In contrast, our work and its dataset EyeDentify is not only inspired by their effort of collecting the necessary data for pupil diameter estimation but also presents the largest publicly accessible dataset for pupil diameter estimation from RGB images, significantly contributing to the broader field of eye monitoring."}, {"title": "2.3. Pupil Diameter Estimation Approaches", "content": "The estimation of pupil diameters has been done in various research studies. For instance, Ni et al. proposed a pupil diameter estimation method called BI\u039d\u039f\u039c\u0391\u03a1 [35]. Their approach uses master and slave cameras as a binocular geometric constraint for the input gaze images. The estimation model they implement is based on Zhang's algorithm [55], which achieved 0.022\u00b10.017mm mean absolute error. Similar to our work, Caya et al. uses their own data recording platform for data collection [5]. They captured the participant's facial images with a fixed camera distance of 10cm from the face. Then, the facial image is sent to Raspberry Pi for the pupil diameter estimation process. This estimation includes a RGB to grayscale conversion, contrast and brightness adjustment, image reshaping, and applying the Tiny-YOLO algorithm [23]. They achieved pupil measurements with a percent difference of 0.58% for the left eye and 0.48% for the right eye.\nIn summary, previous works have strong limitations in condition dependencies, such as having two cameras or keeping the face at a constant and fixed distance from the camera. Also, the data collection and evaluation must consider various pupil diameter sizes. We aim to address these limitations with the presented work."}, {"title": "3. Methodology", "content": "In this section, we describe the hardware (Section 3.1) and application we used for data collection (Section 3.2), the procedure for collecting the data (Section 3.3), as well as how we processed the raw recordings (Section 3.4)."}, {"title": "3.1. Data Recording Hardware", "content": "The ground truth data for our pupil diameters is captured using the Tobii eye-tracker [1], a remote device capable of recording gaze coordinates and pupil diameters at 90Hz. It separately measures the diameters of the left and right pupils in millimeters with six-decimal precision. For camera-based recordings of the face and eyes, we utilize the built-in webcam of the Microsoft Surface Studio 1, which offers a screen resolution of 1280 \u00d7 720 pixels and a frame rate of 30.0 frames per second."}, {"title": "3.2. Data Collection Application", "content": "A distinctive aspect of our data collection methodology is using an application developed explicitly for EyeDentify. We implemented a web application called ChameleonView 1, which we used for our webcam data collection. The web application shows a button in the center of the screen that, when clicked by subjects, triggers a three-second webcam video recording. Additionally, we capture timestamps marking the start and end of each recording. These timestamps are crucial as they ensure synchronization between the webcam video and the Tobii eye-tracker data, as shown in Figure 1. During one data collection period, the subjects were asked to repeat this button-clicking process 50 times, resulting in 50 sessions for each participant.\nTo collect diverse pupil diameter sizes, we changed the computer screen background color [40, 50]. Thus, the 50 recordings had the following background color conditions (with corresponding HEX codes): the first ten recordings had a white display color (#ffffff), and the remaining 40 recordings were: black (#000000), red (#ff0000), blue (#0000ff), yellow (#ffff00), green (#008000), and gray (#808080), each in turn, five times displayed and a final display of white (#ffffff) for the last ten recordings again."}, {"title": "3.3. Data Collection Procedure", "content": "In this study, subjects began by completing a consent form and providing statistical information. Following this initial step, they proceeded to calibrate the Tobii eye-tracker. After calibration, the subjects started using the data collection application outlined in Section 3.2.\nUpon completion of the experiment, the data from the Tobii, webcam video recordings, and corresponding timestamps were consolidated."}, {"title": "3.4. Data Preprocessing", "content": "After recording the raw webcam images alongside the measurements of the eye features, we merge them together for the final dataset EyeDentify. This involves two phases: Aligning the recordings (Section 3.4.1) and cropping the eyes (Section 3.4.2)."}, {"title": "3.4.1 Aligning the Recordings", "content": "As mentioned in Section 3.2, we first synchronized the data collected from the Tobii eye-tracker with the webcam recordings for each participant, as shown in Figure 1.\nWe utilized a CSV file generated by the eye-tracker, which records various metrics, including the diameters of the right and left pupils, the average diameter of both pupils, and coordinates for both the left and right gaze positions. A separate timestamp CSV file also captured the start and end times for each participant's webcam recordings. The synchronization involves iterating through these timestamps and selecting all corresponding rows from the Tobii CSV file that match the start and end times specified for each recording segment. Each recording in our data collection app lasts three seconds, corresponding to three distinct timestamps per recording.\nSince the video recording frame rate is 30 frames per second, this results in 90 frames per recording from our data collection application ChameleonView. Additionally, the Tobii eye-tracker operates at a frequency of 90Hz, providing 90 data points per second, resulting in 270 data points for each three-second recording.\nTo synchronize the 90 frames with the 270 Tobii-captured data points effectively, we concatenate each metric column separately across the 90 data points from the three unique timestamps (horizontally) in the Tobii-captured CSV file and compute a row-wise mean. This process is repeated for each recording. The first timestamp of the trio is designated as the primary timestamp for that recording, ensuring consistency in data alignment. Subsequently, we extract frames from the video recordings, yielding 90 images - aligned with 90 data points from the Tobii-eye tracker. This procedure is meticulously repeated for each participant to ensure uniformity and precision in the dataset preparation process."}, {"title": "3.4.2 Cropping the Eyes", "content": "Once the frames are aligned with the Tobii eye-tracker data, the subsequent step involves cropping the eyes from each extracted frame. We used Mediapipe [30] for its robust face and eye detection capabilities. This tool not only identifies faces within the frames but also precisely locates facial landmarks, including the eyes, nose, iris, eyebrows, and mouth. For our purposes, we specifically utilize Mediapipe to crop the left and right eyes.\nWe utilize Mediapipe's face landmark detection bundle [2], which includes a series of models designed for face detection and the precise mapping of facial landmarks. The face mesh model identifies 478 distinct facial landmarks and accurately determines the coordinates for the left and right eyes and irises. Given the variability in image intensity and participants' distance from the webcam, the regions defined by these coordinates may vary in size, leading to inconsistencies in the dimensions of the cropped regions for each eye and iris. To standardize our data, we crop a region around each eye by centering on the detected eye region and fixing the dimensions to 32 pixels in width and 16 in height. This method preserves the natural shape and scale of the eye, avoiding distortions that could result from resizing.\nTo improve the quality of the set of cropped images, we excluded frames where participants were blinking. For this reason, we first calculated the Eye Aspect Ratio (EAR), a concept adapted from Soukupova et al. [45] and tailored to suit the coordinates provided by Mediapipe's landmark detection outputs. This adaptation ensures the precise detection of blinks using the specific facial landmarks identified by Mediapipe and is described by\n$$eyes\\_ratio = \\frac{EAR_{left} + EAR_{right}}{2}$$\nThe EAR for the left and right eye is defined as\n$$EAR_{left} = \\frac{||V_{l1} - V_{l2}||}{||H_{l1} - H_{l2}||} \\text{ and } EAR_{right} = \\frac{||V_{r1} - V_{r2}||}{||H_{r1} - H_{r2}||}$$\nwhere || || denotes the Euclidean distance between the specified points, $V_{r1}, V_{r2}$ and $V_{l1}, V_{l2}$ are the vertical landmarks of the right and left eyes, respectively, and $H_{r1}, H_{r2}$ and $H_{l1}, H_{l2}$ are the horizontal landmarks of the right and left eyes, respectively. Using the EAR, we categorize frames as follows: frames with an EAR of 0.22 or lower are classified as 'blink detected' (indicating closed eyes), while those with an EAR above 0.25 are classified as 'blink not detected' (indicating open eyes). Frames with EAR values between 0.22 and 0.25 fall into a gray area, where classification is influenced by individual differences in eye structure. To enhance accuracy in this gray area and minimize both false positives and negatives in our blink detection, we use a pre-trained Vision Transformer (ViT) model [10] for open or closed-eye detection, trained on the MRLeyes dataset [13]. If the ViT model predicts closed eyes with a confidence score of 0.50 or higher, we classify the eyes as blinked and discard the frame; otherwise, we save it. This dual-measure approach ensures robust blink detection."}, {"title": "4. Result", "content": "In this section, we describe the results of our dataset collection by describing the statistics of the participants (Section 4.1), the dataset distribution (Section 4.2), and first pupil diameter estimation results (Section 4.3). We use one common participant (number 6) to exemplify all visualizations in the following."}, {"title": "4.1. Dataset Statistics", "content": "In this study, we recruited 51 participants, including 39 males, 11 females, and 1 individual who preferred not to disclose their gender, all of whom consented to make their eye-cropped data public. The participants' ages ranged from 21 to 44 years (M=27.58). They were predominantly workers or university students residing in Germany with origin nationalities in Eastern Europe, South Asia, Southeast Asia, North Africa, and North America. We obtained informed consent from all participants prior to the experiment to ensure compliance with the General Data Protection Regulation (GDPR). Additionally, participants were informed that they could opt out of the experiment at any time.\nFrom an initial total of 226,912 image frames, our final dataset (after preprocessing described in Section 3.4) consists of 212,073 images of left and right eyes, curated from 51 participants after removing frames where blinks were detected. We meticulously organize each participant's left and right eye images into separate directories. Additionally,"}, {"title": "4.2. Dataset Distribution", "content": "Figure 5 shows the distribution of pupil diameters using a boxplot visualization. An in-depth look at the individual pupil diameters within one recording is shown in Figure 6 for white and black display colors. For the same participant, Figure 7 shows exemplary the corresponding webcam recording images for both eyes (cropped). As explained in Section 3.2, each session shows varied display colors. The Chameleon View web application captures a three-second webcam video recording for each session, synchronized with the pupil diameter data collected by the eye-tracker over the same duration. Since the eye-tracker records data at 90Hz, each session generates 270 data points. The mean and variance depicted in the boxplots are computed from these recordings."}, {"title": "4.3. Pupil Diameter Estimation Result", "content": "Table 2 shows initial 5-fold cross-validation results on EyeDentify with the CNN models ResNet-18 and ResNet-50 [15]. For the left eye, the ResNet-18 model outperformed ResNet-50 with a lower MAE in both validation (0.0837 vs. 0.1001) and testing phases (0.1340 vs. 0.1426). Similarly, ResNet-18 again showed better performance for the right eye with a validation MAE of 0.1054 compared to ResNet-50's 0.1089 and a test MAE of 0.1403 versus 0.1588 for ResNet-50. The results indicate that ResNet-18 consistently yields lower error rates than ResNet-50 across both eyes and both evaluation phases. The uncertainties represented by the standard deviations suggest some variability in the model performances across different subsets of the data, where ResNet-50 seems to be more consistent on the respective test partitions.\nFigure 8 shows the class activation map (CAM) [56] of the last convolution layer of ResNet18 and ResNet50 respectively. The figure illustrates that ResNet18 primarily focuses on the external regions of the eyes, capturing environmental and skin color variations, with minimal attention (left eye) to no attention (right eye) given to the iris. For the left eye, ResNet50 also tends to focus strongly on the outer regions of the eye and mostly outside the iris, attempting to learn color intensities rather than concentrating on the iris or pupil. And, for the right eye, ResNet50 primarily focuses on the inner regions of the eye but neglects the surrounding color variations. These observations indicate that while focusing on the iris is crucial for accurately estimating pupil diameter, accounting for environmental variations can enhance performance and make the model more consistent."}, {"title": "5. Limitations", "content": "EyeDentify substantially contributes to the eye monitoring research community. Nonetheless, it is important to acknowledge some limitations of our work to guide future research and application.\nFirstly, the data was recorded using a single camera model, limiting the diversity in recording devices. Given that webcams vary widely in resolution and optical characteristics, we recommend future studies to validate and possibly enhance our initial pupil diameter estimation models across multiple camera types to ensure robustness.\nSecondly, our EyeDentify primarily consists of participants without eyeglasses to maintain consistent conditions across all data. This exclusion limits the applicability of our findings to the general population, as eyeglass wearers comprise a significant demographic. Future work should consider this when they train models on EyeDentify for practical applications where users with eyeglasses are possible.\nLastly, EyeDentify does not account for personal eye characteristics that might influence measurements, such as eye color or health conditions. While we avoided collecting detailed personal meta eye data due to privacy concerns and the complexities of accurately self-reporting eye conditions, this omission can affect the applicability of our findings. Collaborating with ophthalmologists could enhance the collection process, ensuring accurate and privacy-sensitive inclusion of such data in future studies."}, {"title": "6. Future Work", "content": "Future work includes leveraging super-resolution models to enhance image quality, thereby providing more detailed features for deeper and more complex neural networks [32, 33]. For instance, face restoration or enhancement models such as GFPGAN [49] and CodeFormer [57], which have pre-learned facial features, can be particularly beneficial. Additionally, other state-of-the-art super-resolution models like regression-based transformed (SwinIR [27] or HAT [6]) or generative diffusion models (SR3 [42] or YODA [31]) can improve the resolution of webcam-captured eye images. These models can upscale images by factors of 2x, 3x, or 4x, facilitating the training of models on high-resolution images to estimate pupil diameter accurately, similar to gaze estimation using super-resolution [37, 52].\nFurthermore, the features learned by models trained on high-resolution eye images for tasks such as gaze estimation (e.g., Appearance-based Gaze Estimation [7] and Super-Resolution for Appearance-Based Gaze Estimation [37]), blink detection (e.g., Real-Time Eye Gaze and Blink Estimation in Natural Environments [8]), or segmentation of eye parts and regions (e.g., Iris Segmentation Model [51]) can be employed for transfer learning. This approach can be applied to the readily available low-resolution images or the high-resolution images generated by super-resolution models, enhancing pupil detection's overall performance and accuracy."}, {"title": "7. Societal Impact", "content": "We focus on publishing a dataset of everyday webcam images, which we hope will advance research about various fundamental physiological and psychological states and contribute to multiple human activity and healthcare domains. As such, these images and their associated pupil diameter information pose no immediate threat to individuals or organizations. The images were properly anonymized (by providing only cropped eyes). They do not depict any human or personal data that can be associated with any particular person due to the insufficient resolution of eye images for identification. As discussed in the limitations, EyeDentify may introduce biases inherent in the training data, i.e., missing nationalities or no eyeglasses. These biases could affect the quality and fairness of the pupil diameter estimations, particularly in diverse or underrepresented populations. Future work should aim to mitigate these biases and ensure that pupil diameter estimation technologies are inclusive."}, {"title": "8. Conclusion", "content": "We introduced EyeDentify, a novel dataset aimed at significantly enhancing eye monitoring research by enabling the development of models that estimate pupil diameter using standard webcam images. Such advancements are critical for understanding various physiological and psychological states through non-invasive methods, broadening the scope of applications in human-computer interaction, behavioral studies, and health diagnostics. Moreover, EyeDentify represents a significant advancement by addressing the scarcity of publicly available datasets that combine eye images with precise pupil diameter annotations. By focusing on standard webcam recordings, EyeDentify democratizes access to pupil-related research, potentially expanding its applicability in low-resource settings and everyday computing environments. Furthermore, our findings demonstrate that models trained on EyeDentify, particularly the ResNet-18 architecture, achieve promising results in estimating pupil diameters with a high degree of accuracy."}]}