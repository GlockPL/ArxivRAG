{"title": "TransMLA: Multi-Head Latent Attention Is All You Need", "authors": ["Fanxu Meng", "Zengwei Yao", "Muhan Zhang"], "abstract": "Modern large language models (LLMs) often face communication bottlenecks on current hardware, rather than purely computational limitations. Multi-head Latent Attention (MLA) addresses this issue by utilizing low-rank matrices in the key-value layers, enabling the caching of compressed latent key-value (KV) states. This design significantly reduces the KV cache size compared to traditional multi-head attention, thus accelerating inference. Furthermore, MLA incorporates an up-projection matrix to enhance expressiveness, effectively trading extra computation for reduced communication overhead. Despite its demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers continue to rely on Group Query Attention (GQA), with no public plans to adopt MLA. This paper shows that GQA can always be represented by MLA with the same KV cache overhead-the reverse does not hold. To encourage broader adoption, we introduce TransMLA, a post-training method that converts widely used GQA-based pre-trained models (e.g. LLaMA, Qwen, Mixtral) into MLA-based models. After this conversion, further training is performed to boost the model's expressiveness without increasing the KV cache size. Additionally, we plan to develop MLA-specific inference acceleration strategies to maintain low latency in transformed models, thereby facilitating more effective distillation of Deepseek R1.", "sections": [{"title": "Introduction", "content": "In recent years, Large Language Models (LLMs) have become indispensable tools for productivity (OpenAI, 2024; Anthropic, 2024; Team et al., 2024a), with open-source models (AI@Meta, 2024; Mistral, 2024; Qwen, 2024; Liu et al., 2024a; Team et al., 2024b; Abdin et al., 2024) narrowing the performance gap to closed-source alternatives. The effectiveness of LLMs is primarily due to Next Token Prediction (Radford, 2018; Brown et al., 2020), where tokens are predicted sequentially, with attention computed between each token and its preceding ones. To avoid redundant calculations, key-value (KV) pairs are cached. However, as model sizes grow, caching overhead increases, leading to memory and communication bottlenecks. For example, LLaMA-65B (Touvron et al., 2023), with 8-bit key-value quantization, requires over 86GB of"}, {"title": "Related Work", "content": "In large language models (LLMs), the key-value (KV) cache becomes a significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each token's KV pair must be recomputed for every new token, the memory de-mands of the cache grow quickly, limiting the abil-ity to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs.\nOne approach is linear attention, as seen in meth-ods like Linear Transformer (Katharopoulos et al., 2020; Wang et al., 2020), RWKV (Peng et al., 2023), and Mamba (Gu and Dao, 2023), which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory de-mands, it can reduce model expressivity, leading to performance degradation in tasks requiring com-plex, long-range token dependencies.\nAnother technique is dynamic token pruning, employed by LazyLLM (Fu et al., 2024), A2SF (Jo and Shin, 2024), and SnapKV (Li et al., 2024). These methods selectively prune less important to-kens from the KV cache, reducing memory us-age without sacrificing performance. Although dy-namic pruning improves efficiency, it risks discard-ing essential tokens, especially for tasks requiring a detailed understanding of distant context. Addi-tionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining.\nPruning head dimensions, seen in approaches like SliceGPT (Ashkboos et al., 2024), Sheared (Xia et al., 2023), and Simple Pruning (Sun et al., 2023), reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the model's ability to capture important token relation-ships.\nSharing KV representations across layers, as in YONO (Sun et al., 2024), MiniCache (Liu et al., 2024b), and MLKV (Zuhri et al., 2024), reduces memory by reusing the same KV cache across mul-tiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.\nFinally, KV quantization techniques like GPTQ (Frantar et al., 2022), Kivi (Liu et al., 2024c), and KVQuant (Hooper et al., 2024) reduce the preci-sion of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computa-tional overhead, enabling longer contexts and faster inference with minimal performance loss."}, {"title": "Preliminary", "content": "We will first introduce Multi-Head Attention (MHA) (Vaswani et al., 2017), Group Query Atten-tion (GQA) (Ainslie et al., 2023), and the special case of GQA where the group is equal to 1, namely Multi-Query Attention (MQA) (Shazeer, 2019), as well as Multi-Head Latent Attention (MLA) (DeepSeek-AI, 2024).\n3.1 Multi-Head Attention (MHA)\nLet the input sequence be $X \\in \\mathbb{R}^{T\\times D}$, where $T$ represents the sequence length and $D$ is the hidden dimension. Let $W_Q, W_K, W_V \\in \\mathbb{R}^{D\\times (n_h\\cdot d_h)}$, and $W_O \\in \\mathbb{R}^{(n_h\\cdot d_h)\\times D}$. First, MHA computes\n$Q = XW_Q \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$,\n$K = XW_K \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$,\n$V = XW_V \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$.\nThese are then split into $n_h$ attention heads:\n$[Q_1; Q_2; ... ; Q_{n_h}] = Q$,\n$[K_1; K_2; . . . ; K_{n_h}] = K$,\n$[V_1; V_2; . . . ; V_{n_h}] = V$.\nwhere $Q_i, K_i, V_i \\in \\mathbb{R}^{T\\times d_h}$. The matrix $W_O$ is applied to the outputs of each attention head:\n$[W_{O1}; W_{O2}; . . .;W_{On_h}] = W_O$,\nwhere $W_{Oi} \\in \\mathbb{R}^{d_h\\times D}$. Finally, the output of each attention head is computed as:\n$O_i = \\text{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_h}})V_iW_{Oi} \\in \\mathbb{R}^{T\\times D}$,\nand the final output is the sum of all head outputs: $O = \\sum_{i=1}^{n_h} O_i$. In MHA, each $Q_i$ corresponds to a $K_i$, and each $O_i$ corresponds to a $V_i$.\n3.2 Group Query Attention (GQA)\nTo reduce the overhead of the KV cache, GQA divides the Query heads into $g_h$ groups, with each group sharing a single Key and Value for $n_h$ heads. Specifically, let $W_Q \\in \\mathbb{R}^{D\\times (n_h\\cdot d_h)}$, $W_K, W_V \\in \\mathbb{R}^{D\\times (g_h\\cdot d_h)}$, and $W_O \\in \\mathbb{R}^{(n_h\\cdot d_h)\\times D}$ First, compute\n$Q = XW_Q \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$,\n$K = XW_K \\in \\mathbb{R}^{T\\times (g_h\\cdot d_h)}$,\n$V = XW_V \\in \\mathbb{R}^{T\\times (g_h\\cdot d_h)}$.\nThen, split $K$ and $V$ into $g_h$ heads, and split $Q$ into $g_h$ groups, with each group containing $\\frac{n_h}{g_h}$ attention heads:\n$[Q_1; \\dots \\dots \\dots ; Q_{\\frac{n_h}{g_h}}; \\dots \\dots ; Q_{n_h}] = Q$,\n$[K_1; K_2; ... ; K_{g_h}] = K$,\n$[V_1; V_2; . . . ; V_{g_h}] = V$.\nwhere $Q_i, K_i, V_i \\in \\mathbb{R}^{T\\times d_h}$. The matrix $W_O$ is then applied to each Query head:\n$[W_{O1}; W_{O2}; . . . ;W_{On_h}] = W_O$,\nwhere $W_{Oi} \\in \\mathbb{R}^{d_h\\times D}$. For the $j$-th group and the $k$-th head in that group, the output is computed as:\n$O_i = \\text{softmax}(\\frac{Q_iK_j^T}{\\sqrt{d_h}})V_jW_{Oi} \\in \\mathbb{R}^{T\\times D}$,\nwhere $i = \\frac{n_h}{g_h}j + k$ and the final output is the sum of all head outputs: $O = \\sum_{i=1}^{n_h}O_i$.\nIn GQA, each $K_j$ corresponds to $\\frac{n_h}{g_h}$-th $Q_k$ heads, and each $V_j$ corresponds to $\\frac{n_h}{g_h}$-th $O_k$ heads. When $g_h = n_h$, GQA becomes MHA, and when $g_h = 1$, GQA becomes MQA.\n3.3 Multi-Head Latent Attention (MLA)\nTo make it easier to understand and align with the forms of MHA and GQA, we omit the ROPE and allow each head to compute with its own latent vectors, and do not apply compression to the query. Let $W_Q \\in \\mathbb{R}^{D\\times (n_h\\cdot d_h)}$, $W_u, W_a\\in \\mathbb{R}^{D\\times r}$, $W_b, W_c \\in \\mathbb{R}^{r\\times (n_h\\cdot d_h)}$, and $W_O\\in\\mathbb{R}^{(n_h\\cdot d_h)\\times D}$, where r is the KV compression dimension. MLA computes\n$Q = XW_Q \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$,\n$K = XW_uW_b \\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$,\n$V = XW_uW_c\\in \\mathbb{R}^{T\\times (n_h\\cdot d_h)}$.\nthen splits Q, K, and V into $n_h$ attention heads and computes attention:\n$O = \\sum_{i=1}^{n_h} \\text{softmax}(\\frac{Q_iK_i^T}{\\sqrt{d_h}})V_iW_{Oi} \\in \\mathbb{R}^{T\\times D}$\nFor saving the KV cache, only the intermediate latent representations need to be stored: $K' = XW_u \\in \\mathbb{R}^{T\\times r}$, $V' = XW_u \\in \\mathbb{R}^{T\\times r}$, where r is much smaller than $n_h d_h$. During inference, MLA uses an absorb operation to merge $W_b$ into $W_u$ and $W_c$ into $W_o$, thus avoiding increasing the KV latent dimension. At this point, MLA can be viewed as an MQA with each head having a dimension of r."}, {"title": "TransMLA", "content": "This section begins by presenting the following theorem:\nTheorem 1. The expressiveness of MLA is greater than that of GQA when both have the same size of KV cache.\nProof: Sections 4.1, 4.2, and 4.3 show that any GQA configuration can be equivalently trans-formed into MLA with the same size of KV cache. In Section 4.4, we demonstrate that there are cases where MLA cannot be represented by GQA.\n4.1 Replicating the Keys in GQA\nFigure 1a illustrates the typical structure of Group Query Attention (GQA). In GQA, the query Q is split into $n_q = 2^{\\frac{d_h}{n_k}}$ heads, each of dimension $d_h$. In order to reduce the number of heads for the keys and values, the key K is defined to have $n_k$ heads (with $n_k < n_q$), each of dimension $d_h$.\nLet $X \\in \\mathbb{R}^{T\\times D}$ be the input sequence of length T and hidden dimension D, and let $W_K \\in \\mathbb{R}^{D\\times (n_k d_h)}$ be the projection matrix for the keys. Then,\n$K = XW_K\\in \\mathbb{R}^{T\\times (n_k d_h)}$.\nBecause standard multi-head attention requires Q and K (as well as V) to have the same number of heads, we must expand K from $n_k$ heads to $n_q$ heads. Define the replication factor $s = \\frac{n_q}{n_k}$. Partition K along its columns into $n_k$ blocks, each corresponding to one head:\n$K^{(i)} = K[:, i d_h:(i+1) d_h], i = 0,1,..., n_k - 1,$\nwhere each block $K^{(i)} \\in \\mathbb{R}^{T\\times d_h}$. Replicating each $K^{(i)}$ by s times and concatenating yields the ex-panded matrix K' $\\in \\mathbb{R}^{T\\times D}$:\n$K' = [K^{(0)},..., K^{(0)},\\underbrace{K^{(n_k-1)},..., K^{(n_k-1)}}_{s \\text{ times}}].$\n4.2 Moving Replication to the Parameter Side\nFigure 1b shows an alternative approach to replicat-ing keys. Instead of computing K and then repli-cating each head, we can replicate the projection matrix $W_K$ before computing K.\nFirst, split $W_K$ along its columns into $n_k$ parts, where each $W_K^{(i)} \\in \\mathbb{R}^{D\\times d_h}$ corresponds to one original key head:\n$W_K^{(i)} = W_K[:,:, i d_h:(i+1) d_h], i = 0,1,..., n_k-1.$\nThen, replicate each $W_K^{(i)}$ $s = \\frac{n_q}{n_k}$ times and con-catenate them in sequence to form a new projection matrix $W'_K \\in \\mathbb{R}^{D\\times D}$:\n$W'_K = \\underbrace{[W_K^{(0)},..., W_K^{(0)}}_{s \\text{ times}},\\underbrace{W_K^{(n_k-1)},..., W_K^{(n_k-1)}]}_{s \\text{ times}}.$\nApplying $W'_K$ to X directly yields\n$K' = XW'_K.$\nThis approach is mathematically equivalent to first computing K and then replicating its heads.\n4.3 Low-Rank Factorization Form MLA\nFigure 1c demonstrates that $W'_K$, formed by repli-cating $W_K$, has at most $n_k d_h$ degrees of freedom. Consequently, its rank is at most $n_k d_h$. To see this more formally, we factorize $W'_K$ using the Singular Value Decomposition (SVD):\n$W'_K = U_K S_K V_K^T,$\nwhere $U_K$ and $V_K$ are $D \\times D$ orthogonal matrices, and $S_K$ is a $D \\times D$ diagonal matrix of singular val-ues. Only the top $n_k d_h$ (or fewer) of these singular values can be nonzero. Hence, we can truncate the SVD to keep just the top-r singular values, with $r\\leq n_k d_h$:\n$W'_K = U_K[:,:r] S_K[:r,:r] V_K[:r,:].$\nDefine\n$W_u = U_K[:,:r] \\sqrt{S_K[:r,:r]} \\in \\mathbb{R}^{D\\times r}$,\n$W_b = \\sqrt{S_K[:r,:r]} V_K[:r,:] \\in \\mathbb{R}^{r\\times D}.$\nThen\n$W'_K = W_u W_b\\text{ and }K' = XW_u W_b.$\nThis construction shows how the GQA \"repeat KV\" scheme can be interpreted as a low-rank fac-torization reminiscent of the MLA approach. In practice, when caching the key and value matrices, one only needs to store the low-rank representation $X W_u$. During the actual attention computation, the representation can be \"expanded\" by multiply-ing with $W_b$, thus regaining the full dimensional-ity and enhancing expressiveness."}, {"title": "MLA Not Representable in GQA", "content": "Consider a scenario where the vectors in $W_u$ are orthogonal. In this case, the output of each channel, after multiplying $XW_u$ with $W_b$, remains dis-tinct across the channels. However, in GQA, within each group, the heads are replicated, meaning that the output for all heads within a group is identi-cal. This difference in structure implies that there are certain cases in MLA that cannot be captured by GQA, as MLA allows for more diversity in the outputs across the channels.\nBased on the steps outlined above, we have proven Theorem 1. By transforming GQA into an equivalent MLA representation, we can enhance the expressiveness of the model. The next section will present experimental results to validate this claim."}, {"title": "Experiment", "content": "5.1 Experiment Setting\nThis section demonstrates how to convert a GQA-based model into an MLA model using the Qwen2.5 framework, and compares the training performance of the two models on downstream tasks. The Qwen2.5-7B model has 28 query heads and 4 key/value heads per layer, with each head having a dimension of 128 and a KV cache dimension of 1024. Similarly, the Qwen2.5-14B model features 40 query heads and 8 key/value heads per layer, each with a dimension of 128, and a KV cache dimension of 2048.\nUpon transforming the Qwen2.5-7B model into the MLA model, the output dimensions of the weight matrices $W_b$ and $W_c$ are adjusted to 512, while the KV cache dimension remains un-changed at 1024. Unlike the original setup of the GQA model, the TransMLA approach projec-tion the dimensions of $W_b$ and $W_c$ from 512 to 28 \u00d7 128 = 3584 dimensions. Since the 28 query heads can now interact with 28 distinct queries, forming diverse functional representations, this ad-justment significantly enhances the model's expres-sive power.\nThis transformation allows TransMLA to im-prove the GQA model's expressive ability without increasing the KV cache size. Notably, the increase in parameters is minimal. Specifically, for the Q-K pair, an additional 512 \u00d7 3584 matrix is intro-duced, while the original matrix had dimensions of 3584 \u00d7 3584 + 512 \u00d7 3584. Therefore, the added parameters account for only 1/8 of the original ma-"}, {"title": "Comparing The Fine-Tuning Performance", "content": "To evaluate the performance improvement of the transformed MLA model, we train both the original GQA-based Qwen model and the TransMLA model on an instruction fine-tuning dataset, SmolTalk (Allal et al., 2025). The SmolTalk dataset includes a rich set of instruction-tuning data, including mathematical tasks (Meta-MathQA (Yu et al., 2023)) and coding tasks (Self-OSS-Starcoder2-Instruct (Wei et al., 2024)). The training was conducted using the torchtune (torch-tune maintainers and contributors, 2024) frame-work, with a batch size of 16, a learning rate of 2e-5, and 2 epochs. To minimize the impact on the original model, only the Key-Value layers were trained. For the GQA model, only $W_K$ and $W_V$ were trained, while for the TransMLA model, $W_K$, $W_u$, $W_b$, and $W_c$ were trained. The loss during training and the model's performance after training are shown in Figure 2.\nAs shown in Figure 2a, the transformed MLA"}, {"title": "Conclusion and Future Work", "content": "In this paper, we demonstrate that the expressive power of Multi-Head Linear Attention (MLA) surpasses that of Group Query Attention (GQA). We prove that any GQA model can be equivalently transformed into an MLA model. Through extensive experiments, we validate that the MLA model, once transformed and fine-tuned, exhibits significantly enhanced performance. This work fills the gap left by the DeepSeek paper, which lacked both a theoretical and empirical comparison between GQA and MLA.\nIn future work, we aim to extend our approach by adapting large-scale models such as LLaMA, Qwen, and Mistral to MLA-based architectures. Additionally, we plan to employ DeepSeek R1 distillation to further optimize the performance of the converted models, ultimately achieving even greater results."}]}