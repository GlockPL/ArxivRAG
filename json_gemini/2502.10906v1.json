{"title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning", "authors": ["In-Chang Baek", "Sung-Hyun Park", "Sam Earle", "Zehua Jiang", "Noh Jin-Ha", "Julian Togelius", "Kyung-Joong Kim"], "abstract": "Reward design plays a pivotal role in the training of game Als, requiring substantial domain-specific knowledge and human effort. In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs). In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators. This work introduces PCGRLLM, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques. We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach. Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks. The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model. Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.", "sections": [{"title": "I. INTRODUCTION", "content": "The design of reward functions plays a pivotal role in the training of deep reinforcement learning (DRL) agents and evolutionary algorithms for procedural content generation (PCG) in games [1]. Reward functions guide agent behaviors and define the objectives that align generated content with desired outcomes, such as game difficulty or aesthetic appear-ance. Traditionally, designing reward functions relies heavily on researchers' game-specific knowledge and time-consuming reward shaping process. In the procedural content generation via RL (PCGRL) literature [2], the controllability of reward function has been achieved by parameterization of reward function in two- and three- dimensional level generation tasks [3], [4]. This significant human dependency not only requires significant time and resources but also introduces barriers to accessibility and scalability of game AIs. Additionally, the controllability of RL-based generative models has been dependent on pre-defined environmental features. Therefore, reward generation is necessary to alleviate the dependency on humans and dependency on controllable features.\nRecent advancements in LLMs have shown their poten-tial to mitigate these challenges by leveraging pre-trained expert knowledge from large datasets. Several studies have explored LLM-based reward generation approaches in robotic control [5]\u2013[7] and the gameplay [8], [9] domain, utilizing LLMs' reasoning and coding capabilities. One such approach, ChatPCG [10], introduced an early-stage LLM-driven reward generation method for PCGRL, which transforms high-level game descriptions into low-level reward function code. This work proposed a reward fine-tuning method, self-alignment, to align the reward function for a specific environment. However, a limitation of this approach is the absence of a refinement process that incorporates results of the trained agent. As a result, it is uncertain whether the trained policy accurately reflects the intended reward function generation conditions.\nTo address these limitations, we propose an improved ar-chitecture based on prior work [10], PCGRLLM, a feedback-based reward generation framework for content generation.Fig. 1 illustrates the overview of the proposed method with the sequence of improving reward function with two major"}, {"title": "II. BACKGROUND", "content": "A. Prompt Engineering\nPrompt engineering (PE) has become an effective method-ology to enhance the performance of large language models (LLMs) in a gradient-free manner, with various techniques developed to improve their logical reasoning and planning abilities. These techniques can be described as utilizing dif-ferent structures to extend thought processes, as illustrated in Fig. 2. Compared to traditional input-output (IO) methods that query results in a single step, these structured approaches show superior performance in solving problems sequentially.\nThese techniques can be broadly categorized into three approaches based on chain, tree, and graph structures:\nChain-of-Thought (CoT) [12], [13] expands reasoning process with multiple steps to solve a problem in step-by-step. This method is particularly effective for problems requir-ing long-horizon reasoning. An enhanced variant, CoT with self-consistency (CoT-SC) [14], selects the most consistent response through a majority vote mechanism.\nTree-of-Thoughts (ToT) [15] expands reasoning in multiple directions, increasing the scope of exploration. This fault-tolerant method, equipped with backtracking capability, not only broadens the exploration space but also allows recovery by redirecting to alternative paths when a wrong direction is taken, providing resilience against getting trapped in local optima. The multi-path reasoning requires a fitness function to evaluate and select nodes for expansion, ensuring efficient exploration of the solution space.\nGraph-of-Thoughts (GoT) [16] is an extended version of ToT, designed to improve the sample efficiency of reasoning by leveraging multiple thoughts generated during node expan-sion. When expanding a node, GoT retrives related nodes to enhance the reasoning process and ensure efficient exploration of the solution space.\nB. Reward Generation Problem\nThe reward design problem [17] refers to the process of searching for a reward function (R) that maximizes the fitness"}, {"title": "III. PRELIMINARIES", "content": "A. Procedural Content Generation via RL\nPCGRL [2] a DRL-based content generation method is a machine learning-based content generation methods. The generator agent is trained with a hand-crafted reward function and gets a positive reward when the content gets closer to the goal condition. The benefits of PCGRL stem from its data-free nature and computational efficiency during inference, making it well suited for real-time content generation in games [23]. Originally introduced for 2D level generation in games such as Sokoban and Zelda [2], PCGRL has been expanded through subsequent research. These advancements include support for conditional metric inputs [3], the ability to freeze specific tiles during generation [11], applications in 3D Minecraft level generation [4], and integration with vectorized multiplayer game skill data [22].\nIn PCGRL, the level design process is framed as a Markov Decision Process (MDP), where level generation is learned through a trial-and-error approach. At each step $t$, the agent observes the game level as a state $s_t$, selects an action $a_t$ to modify a tile of the level, and transitions to a new state $s_{t+1}$. The agent then receives a reward: $r_t = R(s_t, s_{t+1})$, deter-mined by a reward function (R) that evaluates the transition between states. In PCGRL, reward function design requires identifying a computable target based on the given genera-tion objective and appropriately combining different weights. When multiple sub-functions need to be combined to achieve the desired artifact, designing a reward function in a single attempt is highly challenging. Instead, the reward functions has been refined by humans through multiple attempts, with iterative modifications made based on the observed results. Therefore, the reward design process involves iteratively com-bining functions to generate a reward function that produces game-like levels while satisfying the given conditions.\nB. The 2D Level Generation Environment\nThis study uses the PCGRL-Jax [11] environment, a GPU-accelerated implementation of the widely used two-dimensional level generation framework [2], [3]. The selected environment ensures a deterministic reward setting compared to the stochastic reward signal used in the previous study [10], so that the generated reward function is relatively accurately"}, {"title": "IV. STORY-BASED REWARD FUNCTION GENERATION TASK", "content": "Recent advancements in text-based generative models have showcased the potential for translating textual descriptions into diverse domains such as human-like motion [25], high-fidelity images [26], music composition [27], and game content gen-eration. The gaming domain has also benefited significantly from text-based generative approaches. For example, text-conditioned generative models have been applied to specific tasks such as generating Super Mario Bros levels [28] or Sokoban puzzles [29], where models synthesize playable and contextually relevant game content. Extending beyond level design, recent works have explored generating entire games from textual descriptions [30], [31], thereby transforming ab-stract narratives into interactive environments and mechanics. The generated content is evaluated to ensure it aligns with the instructions (i.e., textual conditioning).\nOur method is evaluated on the text-to-reward generation task, which aims to bridge narrative-driven descriptions with a trainable reward function. This evaluation checks whether the generated content satisfies the given text instructions, such as ensuring that the player encounters specific conditions during gameplay-for example, encountering Bat and Spider as required objectives. The two input instructions used in this work are as follows:\n\u2022 \"The player needs to obtain a key and escape through the door. To pick up the key, the player encounters bat monsters.\"\n\u2022 \"The player needs to obtain a key and escape through the door. To pick up the key, the player encounters bat and spider monsters.\"\nWe measure coherence-based accuracy by evaluating how well the player's experience along the path to the door aligns with the given instructional conditions. To measure the game entities (keys and enemies) encountered by the player, we adopted a deterministic pathfinding algorithm for evaluation. We evaluate the generated levels based on how well they align with specific gameplay scenarios, placing emphasis on ensuring a coherent player experience."}, {"title": "V. PROPOSED METHOD", "content": "Our proposed framework PCGRLLM, an improved reward generation framework for PCG, employs a three-step sequen-tial approach: (1) refine the reward function through feedback, (2) align the reward function to the environment and train the agent, and (3) provide feedback to the reward function based on the generated content. Fig. 3 illustrates the comprehensive architecture of the proposed framework. While the prior study [10] was the first to incorporate self-alignment into the re-ward generation task in content generation domain, this work extends the framework by incorporating feedback feature and a refinement process, forming an outer loop that enhances the overall system's adaptability and performance. The following subsections detail the process of refining the reward function to align with the instruction inputs, with the corresponding pseudo-code provided in Algorithm 1."}, {"title": "VI. EXPERIMENT", "content": "A. Experiment Setting\nReward generation The number of self-feedback iterations was set to $N_{feedback} = 6$, and the self-alignment iterations were set to $N_{align} = 5$. For generating the LLM-based reward, OpenAI's gpt-40-2024-08-06 [32] served as the primary backend language model. The breadth for the ToT and GoT methods was set to $N_{breadth} = 2$, balancing the depth of the thought nodes with exploration. The fitness function utilized an accuracy-based evaluation metric, calculated as the average score over 30 inferenced levels. To minimize variability in LLM responses, the temperature parameter, which governs stochasticity, was set to 0. The prompts used in the experi-ments are noted in Appendix D.\nWhile we attempted to control the LLM's determinism, we still observed variance in first-iteration zero-shot accuracy across different prompt engineering and feedback types. Subtle differences in prompt structure (e.g., placeholders for auxiliary data) can affect on the reasoning process. We focus on how accuracy improves over subsequent iterations rather than focusing on zero-shot performance."}, {"title": "VII. DISCUSSION", "content": "A. Specificity of Feedback\nIn RQ1 experiment, we observed that the involvement of feedback influences on the reward function improvement. To further investigate this, we conducted an experiment to evalu-ate whether the quality of feedback impacts performance. This experiment benchmarks three different feedback types-No Feedback, Generic, and Specific feedback on performance across multiple iterations. The general feedback provides two lines of tips to enhance the reward function that are unrelated to the generated content, while specific feedback represents the default setting in this study. To isolate the effect of fitness evaluation, the experiment was conducted within the CoT framework.\nAs shown in Table IV, the performance comparison across feedback quality types highlights the significant impact of feedback specificity on performance improvement as iterations progress. Additionally, Fig. 7 illustrates the relative accuracy change ($\\Delta y$) over iterations for each feedback type, empha-sizing the role of specific feedback in driving more consistent performance gains.\nAccording to the data, the no feedback and generic feedback conditions exhibit occasional improvements in $\\Delta y$; however,"}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "This study introduces an advanced reward generation archi-tecture for game content generation. PCGRLLM frames the reward function refinement as thought units and incorporates various PE methods to enhance exploration within the reward space. The extended architecture analyzes the content gener-ated by the trained policy and incorporates policy feedback into reward refinement. The results indicate that feedback-based reflection significantly improves the reward function, highlighting the critical role of feedback quality. The state-of-the-art PEs, such as ToT and GoT, has capability to enhance reward function through a fault-tolerant reasoning process and sample efficiency. The generality of the framework is evaluated using two popular foundation LLMs, demonstrating substantial improvements in low and zero-shot generation performance.\nFurthermore, this work investigates two essential abilities of LLMs for game content generation: content evaluation and vision-based content reasoning. The experimental results suggest that content evaluation using language models remains a challenging problem, despite it plays a crucial role in guiding the direction of reward refinement. Future work aims to en-hance content evaluation performance by developing objective assessment methods, such as few-shot retrieval techniques, to achieve more balanced score distributions. This improvement would reduce reliance on human experts, paving the way for an end-to-end LLM framework."}, {"title": "A. Accuracy Evaluation Method", "content": "The accuracy measurement involves two sequential steps: (1) identifying solution trajectories and (2) determining the types of encountered enemies. The encountered enemies are identified based on the solution trajectories, simulating player traversal along these paths. Fig. 5a illustrates the process of determining solution trajectories. There are three possible solutions corresponding to the three keys in the level. The trajectory for Key 1 (blue) passes through Key 2 (brown), rendering it ineligible as an independent solution. The algo-rithm excludes duplicated paths and prioritizes the shortest paths, based on the assumption that players prefer the shortest traversal routes. In contrast, the trajectories leading to Key 2 (brown) and Key 3 (green) do not overlap with those of other keys, making them independent solutions. As a result, the number of solutions is two, corresponding to Key 2 (brown) and Key 3 (green)."}, {"title": "Algorithm 2 Enemy Encounter Detection Logic", "content": "Require: Path finding algorithm $P$, enemy tiles $E$\nRequire: Player position $P$, door position $D$, key position list $K$, generated level $S_T$\n1: $p \\gets$ [False, False, False]\n2: for key $k \\in K$ do\n3: $\\begin{aligned} T_{S\\rightarrow k} & \\gets P(S, k), \\\\ T_{k\\rightarrow D} & \\gets P(k, D) \\end{aligned}$\n4: if $T_{S\\rightarrow k} = \\emptyset$ or $T_{k\\rightarrow D} = 0$ or $\\sum_{x \\in T_{S\\rightarrow k}} [x \\in K] \\neq 1$ then\n5: continue // Not connected or duplicated path\n6: end if\n7: $T_{P\\rightarrow D} \\gets T_{S\\rightarrow k} + T_{k\\rightarrow D}$ // Solution trajectory\n8: $k_5 \\gets 5x5$ kernel to match enemy tiles\n9: for $(x, y) \\in T_{P\\rightarrow D}$ do\n10: $kernel\\_tiles \\gets \\{S_T[x + i, y + j] | (i, j) \\in k_5\\}$\n11: for $t \\in kernel\\_tiles$ do\n12: if $t \\in E$ then\n13: $p[t] \\gets True$ // Update prediction\n14: end if\n15: end for\n16: end for\n17: end for\n18: return Encountered enemies $p$\nAlgorithm 2 describes the process of detecting enemies encountered from the player to door positions. For each key $k \\in K$, the algorithm calculates the path $T_{S\\rightarrow k}$ from the start $S$ to the key and the path $T_{k\\rightarrow D}$ from the key to the door $D$ using the flood-fill pathfinding algorithm. Determine whether there is connectivity between the player, key, and door, and whether there is a single unique key on the path, defining this as the solution trajectory. Using the coordinates of the trajectory path, perform a convolution operation with a 5 by 5-sized kernel to detect the presence of enemies. Update the prediction (p) based on the types of detected enemies."}, {"title": "B. Vision-based Feedback Analysis", "content": "The state-of-the-art Vision-Language Models (VLMs), such as gpt-4o with vision capabilities, can process visual inputs to perform tasks like question answering based on a given image. This section investigate the reasoning capabilities of VLMs, specifically evaluating whether they have been trained on and can effectively reason about the distribution of game-rendered images. Fig. 9 illustrates the difference between two input methods: textual input, where a 2D array is represented as plain text, and image input, where the same array is processed in visual form. We evaluated how the modality of information provided when generating feedback influences changes in accuracy performance.\nTable V provides quantitative evidence of this improvement. In a chain-of-thought (CoT) reasoning framework, both textual and image inputs show performance gains with the addition of feedback (+FB). For textual input, the accuracy improves from 0.033 to 0.156, marking a gain of +0.122. Similarly, for image input, the accuracy increases from 0.031 to 0.157, with a comparable gain of +0.126. These results suggest that feedback is highly effective across both modalities, leading to substantial performance enhancements. Moreover, the compa-rable improvements indicate that both text and image inputs benefit similarly from feedback, emphasizing the importance of leveraging feedback mechanisms in multimodal frameworks to align reward functions with complex, goal-oriented tasks. This also highlights the potential for evaluating and provid-ing feedback on high-dimensional outputs, such as gameplay videos, to improve the quality of generated content in complex scenarios."}, {"title": "C. Hyperparameters", "content": "TABLE VI: Hyperparameters and DRL agent network architecture used in the experiments."}, {"title": "D. Natural Language Prompt", "content": "Reward Refinement\nINSTRUCTION\nPCG Agent Reward Function Generation Task\nYou are a reward function engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.\nPCGRL Environment\nThe gym-pcgrl project provides an environment for Procedural Content Generation via Reinforcement Learning (PCGRL), where agents learn to generate game levels. One of the environments, \"binary,\" involves creating maps using given tile types. In the \"narrow\" representation, the agent modifies the map by scanning through it, changing tiles one by one. This localized approach encourages more precise level design. The agent receives rewards based on how well the generated level meets the predefined objectives. If the agent generates a level that aligns closely with the goal-such as creating a functional and balanced map-it receives a higher reward. This reward-driven mechanism incentivizes the agent to improve its design strategies over time, learning to create optimal levels through trial and error.\nReward Function\nThe reward function is a function that calculates the reward value for the agent based on the playtested results. The function is written in Python and loads the playtested results from the json file and calculates the reward value based on the results.\n'prev_array' and 'curr_array' are two-dimensional array representing the game level with tile numbers. The range of width is different by the array input (e.g., 10-20), so write a function works on any width and height.\nprev_array: previous game level represented with tile numbers (jnp.array, (h,w))\ncurr_array: current game level represented with tile numbers (jnp.array, (h,w))\nThe array is a 2D array with the shape of (height, width) to represent the game level.\nThe level is represented with tile numbers. The tile number is an integer value.\nTile Number\nEMPTY = 1, WALL = 2, PLAYER = 3, BAT = 4, SCORPION = 5, SPIDER = 6, KEY = 7, DOOR = 8\nThought Tips\nYou have only one chance to generate a reward function. Make a reward function to work on the environment\nTarget Scenario\nThe 'Player' needs to obtain the Key and escape through the 'Door'. To pick up the key, the player must encounter one of the monsters: BAT. The players can figure out all of the monsters when they play the level several times.\nEnsure that the generated map includes a reasonable number of monsters and keys. First, create a reward function that minimizes the generation of monsters other than those mentioned above. Second, design the reward function so that the map does not contain an excessive number of monsters or keys."}]}