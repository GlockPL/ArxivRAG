{"title": "RLSA-PFL: Robust Lightweight Secure Aggregation with Model Inconsistency Detection in Privacy-Preserving Federated Learning", "authors": ["Nazatul H. Sultan", "Yan Bo", "Yansong Gao", "Seyit Camtepe", "Arash Mahboubi", "Hang Thanh Bui", "Aufeef Chauhan", "Hamed Aboutorab", "Michael Bewong", "Praveen Gauravaram", "Rafiqul Islam", "Sharif Abuadbba"], "abstract": "Federated Learning (FL) allows users to collaboratively train a global machine learning model by sharing local model only, without exposing their private data to a central server. This distributed learning is particularly appealing in scenarios where data privacy is crucial, and it has garnered substantial attention from both industry and academia. However, studies have revealed privacy vulnerabilities in FL, where adversaries can potentially infer sensitive information from the shared model parameters. In this paper, we present an efficient masking-based secure aggregation scheme utilizing lightweight cryptographic primitives to mitigate privacy risks. Our scheme offers several advantages over existing methods. First, it requires only a single setup phase for the entire FL training session, significantly reducing communication overhead. Second, it minimizes user-side overhead by eliminating the need for user-to-user interactions, utilizing an intermediate server layer and a lightweight key negotiation method. Third, the scheme is highly resilient to user dropouts, and the users can join at any FL round. Fourth, it can detect and defend against malicious server activities, including recently discovered model inconsistency attacks. Finally, our scheme ensures security in both semi-honest and malicious settings. We provide security analysis to formally prove the robustness of our approach. Furthermore, we implemented an end-to-end prototype of our scheme\u00b9. We conducted comprehensive experiments and comparisons, which show that it outperforms existing solutions in terms of communication and computation overhead, functionality, and security.", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) is a distributed machine learning (ML) approach designed to enhance user privacy during the model training process. Instead of sending user data to a central server, FL only shares local model updates after training happens on the users' devices. The central server then combines these updates to create a global model without ever accessing the actual data [1]. This distributed system reduces the chance of sensitive information being exposed, improving both privacy and security. FL is already being used in areas like healthcare [2], telecommunications [3], the Internet of Things (IoT) [4], and smart cities [5]. However, FL on its own cannot fully eliminate privacy risks. Zhu et al. [6] showed that attackers, including the central server, could still reconstruct users' local datasets by exploiting the gradients shared during model updates. Similar risks have been found in other studies [7]-[9], where researchers pointed out that sharing gradients with the central server could reveal sensitive user data. As a result, additional security measures are needed to fully protect privacy in FL. One popular solution is combining FL with privacy-preserving techniques like differential privacy (DP) [1]. However, DP comes with a trade-off: the stronger the privacy protection, the more it deteriorates the utility of the global model [10]. Secure aggregation is an alternative promising privacy-preserving technique that has gained significant attention for its advantages over DP, such as maintaining accuracy, simplicity in implementation, and avoiding trade-offs in utility [11]. In this method, users cryptographically hide their local model updates before sharing them with the central server. The server then aggregates these hidden updates without being able to view the individual contributions. This ensures that no single party, including the central server, can reconstruct individual model parameters while still obtaining an accurate aggregated global model. Secure aggregation effectively mitigates privacy risks like gradient leakage, which could otherwise expose sensitive data [1]. In general, secure aggregation relies on cryptographic primitives such as homomorphic encryption (HE), secure multi-party computation (SMC), and masking-based techniques [1]. Among these, masking-based secure aggregation is often considered more practical than the others due to its relatively lower computational and communication costs [11]. Several masking-based secure aggregation"}, {"title": "2. Related Work", "content": "Secure aggregation in FL protects individual users' local model parameters from being disclosed to the central aggregator server. This mechanism is also referred to as privacy-preserving aggregation [1]. Differential privacy (DP), homomorphic encryption (HE), secure multi-party computation (SMC), and masking methods are being used for privacy-preserving secure aggregation in FL [11], [1]. Our proposed scheme falls within the masking-based method. Therefore, we briefly introduce the other secure"}, {"title": "3. Cryptographic Primitives", "content": "In this section, we briefly introduce the core concepts of symmetric homomorphic encryption and digital signature, which are integral to the design of our scheme."}, {"title": "3.1. Symmetric Homomorphic Encryption", "content": "Our scheme employs a key negation method, similar to that in [37], to mask the user's local model parameters. This method is based on the symmetric homomorphic encryption mechanism proposed by [38]. In this section, we will briefly describe the work of [38]. The key negation method will be explained in Section 5.1.\nLet $m_i \\in \\mathbb{Z}_q$ represent a secret message, where $q$ is a large public prime. The message m can be encrypted as follows:\n$c_i = Enck_i(m_i) = m_i + k_i \\mod q.$\nwhere $k_i \\in \\mathbb{Z}_q$. A receiver of $c_i$ with the given secret key $k_i$ can recover the secret message $m_i$ as follows:\n$m_i = Deck_i(c_i) = c_i - k_i \\mod q.$\n$Dec_{(k_i+k_j)}(c_i + c_j) = Dec_{k_i}(c_i) + Dec_{k_j}(c_j)$\n$=c_i - k_i + c_j - k_j \\mod q$\n$=m_i + m_j + [(k_i+k_j) - (k_i+k_j) \\mod q$\n$=m_i + m_j.$\nThe scheme described above has additive homomorphic properties [38]. The addition of two ciphertexts is illustrated in Equation 3. This property can also be extended"}, {"title": "3.2. Signature Scheme", "content": "Our scheme employs digital signatures to ensure the integrity and authenticity of messages exchanged between entities, leveraging a UF-CMA secure scheme as described in [12]. In general, a digital signature scheme consists of the following probabilistic polynomial-time (PPT) algorithms:\n* ($priv_{u_i}$, $pub_{u_i}$) \u2190 SIG.Gen(): This algorithm takes a security parameter \u03bb as input and outputs a pair of private and public keys ($priv_{u_i}$, $pub_{u_i}$) for an entity, denoted as $u_i$.\n* \u03c3\u2190 SIG.Sign($priv_{u_i}$, m): This algorithm takes as input the private key $priv_{u_i}$ and a message m, and outputs a signature for the message m.\n* {0,1} \u2190 SIG.Ver($pub_{u_i}$, m, \u03c3): This algorithm takes the public key $pub_{u_i}$, a message m, and the signature \u03c3 as input, and outputs a bit indicating whether the signature is valid or invalid."}, {"title": "4. System Architecture, Threat Model, Assumptions, and Goals of Our Scheme", "content": "In this section, we present the system architecture, threat model, and security assumptions, as well as the functionality and security goals of our proposed scheme. We first start with the system architecture."}, {"title": "4.1. System Architecture", "content": "Figure 1 illustrates the system architecture of our scheme, which comprises three primary entities: users, intermediate servers, and the central server which we termed as Aggregator.\n* Users are the edge devices, such as smartphones and IoT devices, for cross-device FL2, or organizations like hospitals, banks, universities, and government agencies for cross-silo FL3. These users generate and own data locally. They perform local model training on their own data, downloading the current global model from the aggregator, training it, and then sending the updated model parameters to the intermediate servers and aggregator. Before transmission, the parameters are masked, ensuring that only the aggregator can recover the final aggregated model parameter, thereby maintaining the confidentiality of individual users' local model data.\n* Intermediate servers act as aggregation points between users and the main aggregator. They are to reduce communication overhead by combining model updates from multiple users before sending them to the aggregator. These servers receive model updates from users, aggregate the updates, and then forward the aggregated results to the aggregator for final processing. Additionally, they can assist users in detecting model inconsistency attacks originating from the aggregator. In practical applications, fog nodes, edge servers, and third-party cloud services can function as intermediate servers, especially in edge computing or distributed system scenarios. By facilitating localized processing, these components reduce latency and bandwidth usage.\n* Aggregator is the central server that handles the entire FL process. It maintains the global model, coordinates the FL training rounds, and aggregates model updates directly from the intermediate servers and users. The aggregator initiates the training process by distributing the initial global model to users. After receiving aggregated updates from the intermediate servers, the aggregator combines these updates to improve the global model. It then sends the updated global model back to users for the next round of training."}, {"title": "4.2. Threat Model", "content": "Our threat model considers two types of adversaries: semi-honest and malicious.\nIn the semi-honest model, we assume the aggregator is honest in executing assigned tasks but attempts to learn individual users' local training models to infer"}, {"title": "4.3. Assumption", "content": "We have made several assumptions in designing our scheme, which are listed below:\n* The aggregator and intermediate servers remain online at all times.\n* The aggregator and intermediate servers are aware of the participating users before initiating the FL process.\n* Each entity possesses a private and public key through a Public Key Infrastructure (PKI).\n* All users are identified by a unique identifier.\n* Each participating user has knowledge of the online intermediate servers.\n* An authenticated and private channel exists between users and intermediate servers, users and the aggregator, and between intermediate servers and the aggregator."}, {"title": "4.4. Goals", "content": "In this section, we present some of the primary security and functionality goals of our scheme.\n* Local Model Parameter Privacy: The primary objective of our scheme is to safeguard the privacy and confidentiality of individual users' local model parameters. This ensures that no entity can infer or deduce these local model parameters, except for the aggregated global model that is shared with the aggregator.\n* Flexible User Dropout: Our scheme supports dynamic user participation in the training process. Users can join or leave at any FL round due to issues like connectivity, device limitations, or power constraints. Despite this, the global model continues to train effectively. It remains robust and converges, even if some users fail to provide their local updates.\n* Model Inconsistency Attack Detection: Our scheme is designed to detect any malicious behavior by the aggregator attempting to exploit the model inconsistency attack, as demonstrated in [20]. Our scheme addresses this vulnerability by ensuring that any such malicious tampering by the aggregator is promptly identified, maintaining the integrity of the FL process.\n* Elimination of Inter-User Communication: Our scheme aims to eliminate any interaction among users, which helps reduce both communication and computational overhead, in contrast to the approach used by [12] and its variants."}, {"title": "5. Our Proposed Scheme", "content": "In this section, we provide a detailed explanation of our scheme. We first present the technical intuition behind the scheme, followed by a description of its main construction."}, {"title": "5.1. Technical Intuition", "content": "Our protocol has three main entities: users, intermediate servers, and the aggregator, as described in Section 4.1. Users mask their locally trained models, intermediate servers partially aggregate these masked models, and the aggregator completes the final aggregation, eventually unmasking the global model. In this section, we explain how masking and unmasking work, along with how our scheme manages user dropouts and detects model inconsistency. Let $x_u$ be the private vectors of a user u, and U be the set of participating users in a round.\nThe main goal of our scheme is to compute $\\sum_{v \\in U} x_u$ in such a way that the aggregator cannot see the individual private vectors $x_u$ of any user. Our scheme can tolerate up to m -2 compromised users, where m = |U|. This means that the aggregator will only get at most the aggregated vectors of the two honest users, without learning their individual private vectors. Moreover, our scheme ensures that no user learns any useful information about another user's private vector."}, {"title": "Our Proposed Secure Aggregation Scheme", "content": "* Setup\n    * All the entities agree on the security parameter \u03bb, a pseudo-random function PRF, a hash function $H: {0,1}* \u2192 {0, 1}^l$, a set of integer modulo q $\\mathbb{Z}_q$, and a threshold value t.\n    * Each user, intermediate server, and aggregator are assigned unique identifiers (denoted as $u_i$ for the $i$th user, $f_i$ for the $i$th intermediate server, and $Agg$ for the aggregator).\n    * Each user, intermediate server, and aggregator are assigned a public and private key pair $(pub_{u_i}, priv_{u_i}), (pub_{f_i}, priv_{f_i})$, and $(pub_{Agg}, priv_{Agg})$, respectively, for signature generation.\n    * All users know the participating intermediate servers and the aggregator set N, where |N| = n.\n    * All intermediate servers and the aggregator have the universal set of the users M, where a random subset U (m = |U|) of users participate in each round of training.\n    * All intermediate servers communicate with the aggregator over private, authenticated channels.\n    * All users have private, authenticated channels with both the intermediate servers and the aggregator.\n* Masking Round\n    * Users:\n        * Each user $u_i$ chooses random numbers {rj}vjen \u2208 Zq, and then generates the mask vectors {kj = PRF(rj)}vjen.\n        * Each user $u_i$ generates a directed cycle for the participating entities in N, as described in Section 5.1.\n        * For each jth node in the directed cycle, each user $u_i$ masks the trained model $x_i^t$ as follows: $c_{i,j}^t = x_i^t + k_j - k_{j-1}$ mod q and sends <$c_{i,j}^t, \u03c3_i^t$> to the $j$th node, where $\u03c3_i^t = SIG.Sign(priv_{u_j}, c_{i,j}^t)$.\n    * Intermediate Servers:\n        * Each intermediate server, say $f_j$ chooses an empty list Fj.\n        * Each intermediate server $f_j$ receives the tuple <$u_i, c_{i,j}^t, \u03c3_i^t$> from each participating user $u_i$.\n        * The intermediate server computes $SIG.Ver(pub_{u_i}, c_{i,j}^t, \u03c3_i^t)$. If the signature is valid, it adds the user identity $u_i$ to the participating user list Fj.\n        * Each intermediate server $f_j$ sends the tuple <Fj, \u03c3_{f_j}$> to the aggregator if and only if |Fj| \u2265 t, where \u03c3_{f_j}$ = $SIG.Sign(priv_{f_j}, F_j)$. Otherwise, it aborts.\n    * Aggregator Agg:\n        * The aggregator chooses an empty list A.\n        * The aggregator receives the tuple <$u_i, c_{i,Agg}^t, \u03c3_{i,Agg}^t$> from each user $u_i$.\n        * It performs $SIG.Ver(pub_{u_i}, c_{i,Agg}^t, \u03c3_{i,Agg}^t)$. If the signature is valid, it adds the user identity $u_i$ to the participating user list A.\n        * If |A| \u2265 t and the aggregator receives all tuples < {Fj, \u03c3_{F_j}$>vj\u2208(N\\Agg) > from all the intermediate servers, it verifies the signatures {$SIG.Ver(pub_{f_j}, F_j, \u03c3_{F_j}$>vj\u2208(N\\Agg); otherwise it aborts.\n        * The aggregator computes the common active user list I = A \u2229 {Fj}vj\u2208(N\\Agg).\n        * If |I| \u2265 t, the aggregator sends back the tuple < I, \u03c3_{Agg}$> to each intermediate server $f_j$, where \u03c3_{Agg}$ = $SIG.Sign(priv_{Agg}, I).\n    * Partial Aggregation by the Intermediate Servers\n        * Each participating intermediate server, say fj receives the tuple < I, \u03c3_{Agg}$ > from the aggregator.\n        * If |I| \u2265 t, the intermediate server fj checks $SIG.Ver(pub_{Agg}, I, \u03c3_{Agg})$. If the signature is valid, the intermediate server fj computes the partially aggregated masked model PartialAggfj using the masked model received from the users in I.\n            $PartialAgg_{f_j} = \\sum_{v_i \\in I} c_{i,j}^t$\n        * The intermediate server fj sends the tuple < PartialAgg_{f_j}, \u03c3_{PartialAgg_{f_j}}$$> to the aggregator, where \u03c3_{PartialAgg_{f_j}}$ = $SIG.Sign(priv_{f_j}, PartialAgg_{f_j}).\n    * Final Aggregation and Unmasking by the Aggregator\n        * Once the aggregator receives all the tuples {PartialAgg_{f_j}, \u03c3_{PartialAgg_{f_j}}$}\u2200f\u00bf\u2208(N\\Agg), it checks {$SIG.Ver(pub_{f_j}, PartialAgg_{f_j}, \u03c3_{PartialAgg_{f_j}}$)}vj\u2208(N\\Agg). If the signatures are valid, the aggregator performs the final aggregation as follows:\n            $\\theta_t = \\sum_{V_i \\in I} c_{i, Agg}^t + \\sum_{V_j \\in (N\\setminus Agg)} PartialAgg_{f_j}$\n        * Aggregator sends the global model parameter \u03b8t and the common active user list I to each participating user for the next round of training.\n* Global Model Parameter Verification\n    * Aggregator:\n        * Aggregator chooses a random number $s_t \\in \\mathbb{Z}_q$.\n        * Aggregator computes $R = H(\u03b8_t) + s_t$ and a message authentication code $S = MAC_{s_t}(\u03b8_t)$\n        * Aggregator sends the tuple V =< T =< R, S >, A, \u03c3_{Agg}$ > to each intermediate server.\n    * Intermediate Servers:\n        * Each intermediate server, say fj verifies \u03c3_{Agg}$ SIG.Sign(priv_{Agg}, <T, I, A >) once it received the tuple < T =< R,S > , A, \u03c3_{Agg}$$> from the aggregator.\n        * If the verification is successful, the intermediate server fj forwards the tuple < T, I, A > to the users in I along with the user list Fj.\n    * User:\n        * Each user ui receives the tuple < T =< R, S >, I, A > and the user list {Fj}vj\u2208(N\\Agg) from the intermediate servers.\n        * The user first verifies if I = A \u2229 {Fj}vj\u2208(N\\Agg) and |I| \u2265 t. If verification fails, the user stops from participating in future rounds. Otherwise, the user goes to the following steps.\n        * The user ui recovers the secret key $s_t = R - H(\u03b8_t)$ (where the user already has $\u03b8_t$ from the aggregator).\n        * If $S = MAC_{s_t}(\u03b8_t)$, the user ui compares the remaining S values received from the other intermediate servers to verify their consistency. If the verification fails or any mismatches are found, the user ui detects a model inconsistency attack and ceases participation."}, {"title": "6. Security Analysis", "content": "In this section, we present the security claims and their respective proofs for our proposed scheme discussed in Section 5.2."}, {"title": "6.1. Semi-Honest Model", "content": "In this section, we demonstrate that even if a threshold number of users, denoted as $t_e$, and intermediate servers, denoted as $t_f$, collude among themselves or with the aggregator, they cannot learn about the remaining genuine users' local weighted models. We follow a similar security model to those in [12] and [17].\nWe consider three scenarios: first, a subset of users are dishonest and collude with adversaries or among themselves, while the intermediate servers and the aggregator remain honest. Second, a subset of intermediate servers are dishonest and can collude, while the users and the aggregator are honest. Third, a subset of users and intermediate servers, along with the aggregator, are dishonest and can collude. Our goal is to show that if fewer than $t_e$ (where $t_c > 2$) and $t_f$ (where $t_f > 1$) entities are compromised, our scheme still protects the individual locally trained models of the remaining genuine users. We assume that U and F be the total number of users and intermediate servers, respectively. We also assume that $U_i$ be the number of participating users in the $i$th round, and $x_u$ is the locally trained models of the users U.\nLet $Real_{C,t_e}(X_U, U_1, U_2, U_3)$ be a random variable representing the joint views of participants in $C$ during the real execution of our protocol. Let $Sim_{C}^{F,F,t_f}(x_c, U_1, U_2, U_3)$ be the combined views of par- ticipants in $C$ when simulating the protocol, with the inputs of honest participants selected randomly and uniformly, denoted by $x_c$. Following the aforementioned idea, the distributions of $Real_{C,t_e}(x_u, U_1, U_2, U_3)$ and $Sim_{C}^{F,F,t_f}(x_c, U_1, U_2, U_3)$ should be indistinguishable.\nTheorem 1. (Security Against Semi-Honest Users only) For all $U, F, t_c, A$ with $|C_c| < t_c, X_U, U_1, U_2, U_3$ and $C_c$ such that $C \\subseteq U, U_3 \\subseteq U_2 \\subseteq U_1 \\subseteq U$, there exists a probabilistic-time (PPT) simulator $Sim_{t_e}^{C}$, which output is perfectly indistinguishable from the output of $Real_{C,t_e}$,\n$Sim_{C}^{C mu,c,d}(x_{c0}, U_1, U_2, U_3) = Real_{C,t_e}(x_u, U_1, U_2, U_3)$\nProof 2. In our scheme, users randomly choose their input values (i.e., random secret keys) to mask the local model parameters, similar to a one-time pad. Each user's input values are independent of those of other users. Since the aggregator and intermediate servers are considered honest entities, the combined view of the users in $C_c$ will not reveal any useful information about the input values of users not in $C_c$. Additionally, the honest-but-curious users in $C_c$ only receive the identities of the honest users not in $C_c$. This enables the simulator $Sim_{t_e}$ to use dummy input values for the honest users not in $C$ while keeping the combined views of the users in $C_e$ identical to that of $Real_{t_e}^{C}$,\nTheorem 3. (Security Against Semi-Honest Intermedi- ate Servers only) For all $U, F, t_f, with |C_f| \\leq t_f, X_U, U_1, U_2, U_3$ such that $C_f \\subseteq F, U_3 \\subseteq U_2 \\subset U_1 U$, there exists a probabilistic-time (PPT) sim- ulator $Sim_{C}^{F, F, t_f}$ which output is perfectly indistinguish- able from the output of $Real_{t_f}^{F}$,\n$Sim_{C}^{F, F, t_f}(X_C, U_1, U_2, U_3) = Real_{t_f}^{F}(X_U, U_1, U_2, U_3)$\nProof 4.\nIn this proof, we consider that the intermediate servers in $C_f$ are dishonest. Since users randomly and independently choose their masking parameters (i.e., random secret keys), the intermediate servers in $C_f$ will not gain any useful information from the public values. Similarly, users send the masked local models (which are independent of each other and similar to the one-time pads) to each intermediate server separately using a secure and authenticated channel. Therefore,"}, {"title": "6.2. Malicious Model", "content": "In this section, we follow the standard security model as in [12] and [17] to demonstrate that our scheme is secure against active malicious attackers.\nIn this security model, we need to consider three cases. The first case is the Sybil Attack, where the adversaries can simulate a specific honest user ui to get its inputs. Please note that our scheme is resistant to this attack, as it uses digital signatures to verify the origin of the messages. The second case involves the aggregator sending different lists of participating users in a round to the honest intermediate servers. This could lead to the disclosure of the local weighted models of the honest users. Our scheme detects this type of attack during the model parameter verification phase, where each user can verify the list of participating users in a round received from the fog servers. Any difference in the lists will indicate an attack.\nThe third case involves an aggregator dishonestly dropping honest users at any round. We use the Ran- dom Oracle model to prove that our scheme is secure against any malicious dropout of honest users by adver- saries. Let's consider Mc as a probabilistic polynomial- time algorithm representing the next message function of participants in C, which enables users in C to dynamically select their inputs at any round of the protocol and the list of participating users. The Random Oracle can output the sum of a dynamically selected subset of honest clients for the simulator Sim, which is indistinguishable from the combined view of adversaries in the real protocol execution Real (Mc). There are three possible scenarios for this type of active attack: first, the malicious users can collude among themselves; second, the malicious intermediate servers can collude among themselves; and finally, the malicious users and intermediate servers can collude with the aggregator. We present the following three theorems to demonstrate our security proofs.\nTheorem 7. (Security Against Dishonest Users only) For all $U, F, t_c, A$ with $|C_c| < t_c, X_{U\\backslash C}, U$ and $C_c$ such that $C \\subseteq U$, there exists a probabilistic-time (PPT)"}, {"title": "7. Performance Evaluation", "content": "In this section, we first present a theoretical perfor- mance evaluation, covering computation and communica- tion complexity, as well as dropout resilience, followed by the experimental results of our scheme."}, {"title": "7.1. Theoretical Analysis", "content": "We structure our theoretical analysis into two main categories: computation and communication complex- ity, and security and functionality. We compare our scheme with closely related works, including SecAgg [12], SecAgg+ [13], TurboAgg [30], FastSecAgg [21], and EDRAgg [17].\nSection 7.1.1 provides a detailed analysis of the com- putation and communication costs for users, intermediate servers, and the main aggregator, highlighting the worst- case scenarios. While our approach introduces an interme- diate server layer, which adds some additional cost, this layer contributes to improved overall performance, as we will show. Please also note that our scheme assumes a constant a fix number of intermediate servers.\nIn Section 7.1.2, we evaluate our scheme against related works based on essential security and function- ality criteria, including threat model support, resistance to model inconsistency attacks, and tolerance for user dropouts."}, {"title": "7.1.1. Computation and Communication Complexity Analysis", "content": "In our scheme, each user experiences a com- putation complexity of O(|v|), primarily due to the gen- eration of v masking parameters for the input vector as shown in Table 1. Both the intermediate server and the aggregator have a computation complexity of O(m+|v|), stemming from the partial and final aggregation of mask- ing parameters from m participating users. As highlighted"}, {"title": "7.1.2. Security and Functionality Comparison", "content": "Our scheme offers robust security against both semi-honest and malicious adversaries by carefully configuring system parameters, and it maintains tolerance for user dropouts of up to m 2 users at any time. Unlike many existing schemes such as [12], its variants and [19], our approach does not require fixed participation from the start; users can join or dropout at any stage of the training process, enhancing practical applicability. Additionally, our scheme is efficient, requiring only two rounds of communication per iteration and a single setup phase, in contrast to other methods that involve a setup phase in each iteration, leading to increased overhead. Furthermore, our scheme can detect model inconsistency attacks, adding valuable security with a manageable increase in computation and communication costs."}, {"title": "7.2. Experimental Result", "content": "We developed a prototype of our scheme and tested it on a VM with 2 vCPUs (Intel Xeon), 16GB of RAM, and Ubuntu Server 22.04. The prototype is implemented in Python (Python 3.7.11) using PyTorch 1.13.14, NumPy 1.21.65, and PyCryptodome 3.206."}, {"title": "7.2.1. Running Time Analysis Without User Dropout", "content": "Figure 4 and Figure 6 present the running time of an intermediate server and the aggregator during a round without any user dropouts in the Semi-Honest Model, with varying numbers of users and input vector sizes, respectively. Similarly, Figure 5 and Figure 7 show the running time for the same scenario in the Malicious Model. We considered randomly generated input vectors with 64-bit entries as the users' local gradients. In this analysis, the average running time of one intermediate server was measured, as all intermediate servers operate in parallel. The results indicate that as the number of users"}, {"title": "7.2.2. Running Time Analysis With User Dropout", "content": "In the next sets of experiments, we consider user dropouts to measure the total running time of the user, intermediate server, and aggregator, along with capturing the running time of each main phase of our scheme. Table 2 and Table 3 present the running times (in microseconds) per round for users, intermediate servers, and the aggregator dur- ing various phases under the semi-honest and malicious models, respectively. It compares scenarios with different percentages of user dropouts (0%, 10%, 20%, 30%) and varying user counts (500 and 1000). For both models, the experiments used 50k random input vectors with 64-bit entries and 10 intermediate nodes.\nFor each scenario, the masking time at the user re- mains relatively stable, showing minimal variation as the percentage of user dropouts and the number of users increase, as it is independent of both. However, the partial aggregation time at the intermediate server and final aggregation time at the aggregator increase with the number of users. The running time decreases with more user dropouts due to a reduction in the number of active participants. The global model verification time at both the aggregator and user remains consistent across different scenarios, independent of user participation and dropouts. The total running times for the aggregator and intermediate server also increase with the number of users but decrease as user dropouts increase. The total running time for the user remains stable as it is unaffected by the number of participants or dropouts."}, {"title": "8. Conclusion", "content": "In this paper, we introduced a lightweight secure aggregation scheme for FL that uses a key negation-based masking technique. Our approach adds an interme- diate server layer, which eliminates the need for users to communicate with each other and only requires one setup phase. This significantly reduces the burden on users. Thanks to this intermediate layer, our scheme can handle user dropouts effectively, allowing users to join the training phase at any time with just one additional round of communication between the intermediate servers and the aggregator.\nAdditionally, we integrated a lightweight verification mechanism for model parameters to detect inconsisten- cies, enhancing security against malicious aggregators. We conducted a thorough formal security analysis showing that our scheme is robust against both semi-honest and malicious environments. Our comparisons with closely re- lated works demonstrate that our scheme is better in terms of security, functionality, and both communication and computation costs. We also implemented our scheme and presented comprehensive experimental results, proving its practical usability in real-world applications."}]}