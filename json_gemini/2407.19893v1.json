{"title": "Leveraging Foundation Models for Zero-Shot IoT Sensing", "authors": ["Dinghao Xue", "Xiaoran Fan", "Tao Chen", "Guohao Lan", "Qun Song"], "abstract": "Deep learning models are increasingly deployed on edge Internet of Things (IoT) devices. However, these models typically operate under supervised conditions and fail to recognize unseen classes different from training. To address this, zero-shot learning (ZSL) aims to classify data of unseen classes with the help of semantic information. Foundation models (FMs) trained on web-scale data have shown impressive ZSL capability in natural language processing and visual understanding. However, leveraging FMs' generalized knowledge for zero-shot IoT sensing using signals such as mmWave, IMU, and Wi-Fi has not been fully investigated. In this work, we align the IoT data embeddings with the semantic embeddings generated by an FM's text encoder for zero-shot IoT sensing. To utilize the physics principles governing the generation of IoT sensor signals we propose to use cross-attention to combine a learnable soft prompt to derive more effective prompts for semantic embedding extraction, that is optimized automatically on training data and an auxiliary hard prompt that encodes domain knowledge of the IoT sensing task. To address the problem of IoT embeddings biasing to seen classes due to the lack of unseen class data during training, we propose using data augmentation to synthesize unseen class IoT data for fine-tuning the IoT feature extractor and embedding projector. We evaluate our approach on multiple IoT sensing tasks. Results show that our approach achieves superior open-set detection and generalized zero-shot learning performance compared with various baselines. Our code is available at https://github.com/schrodingho/FM_ZSL_IoT.", "sections": [{"title": "1 Introduction", "content": "With the advancement of edge hardware accelerators, deep learning is increasingly employed for IoT sensing tasks on edge devices, such as Wi-Fi human sensing [28], sound event detection [26], and activity recognition using motion sensor [22]. However, although deep learning models show excellent performance in classifying samples from a set of seen classes included in the training dataset, identifying and classifying data samples from unseen classes using deep models trained under the supervised setting are challenging. To address this, an intuitive solution is to include as many classes as possible during training. However, unlike images, text, and audio, which humans can easily interpret, IoT data often lacks readability and requires costly labeling processes. Thus, IoT datasets usually contain a limited number of classes. For example, most inertial measurement unit (IMU)-based activity recognition datasets contain fewer than 20 activity classes [22], while the ImageNet contains 21,814 classes.\nZero-shot learning (ZSL) [16] is a promising learning paradigm to address the aforementioned challenge. ZSL classifies data from unseen classes with the help of semantic information that transfers knowledge from seen classes to unseen ones. Previous studies rely on manually-engineered attributes as semantic information for zero-shot IoT sensing [23, 15], which are labor-intensive to design and difficult to scale to complex datasets. Some works build semantic spaces using word representation models like Word2Vec [12, 24], BERT [26], and GloVe [22]. The word vectors are automatically generated using large text corpus, e.g., Wikipedia. However, the text descriptions may contain information unrelated to the target IoT task. In IMU-based activity recognition, the training dataset may contain redundant text about the IMU sensors and lack motion-related information useful for activity classification. Thus, the word vectors may contain task-irrelevant noise, causing a semantic gap between the IoT data and word embeddings. The work in [22] constructs visual semantic space using human activity videos for zero-shot IMU-based human activity recognition, which may raise privacy concerns. In this work, we aim to explore using foundation models, which are considered to have a generalized understanding of the world acquired from diverse and extensive training data, to generate more effective and contextually relevant semantic embeddings for zero-shot IoT sensing.\nFoundation models (FMs) are large-scale deep learning models pre-trained on vast data that serve as the foundation for various downstream tasks [31]. FMs trained on extensive text corpora exhibit remarkable generalizability to a broad spectrum of new tasks, e.g., passing exams [1], code generation [14], and language translation [17]. Large vision-language FMs embed images with language inputs in a joint semantic space using hundreds of millions of image and text pairs, which achieve impressive zero-shot transferability to downstream tasks like image recognition on unseen datasets [18, 21]. Inspired by this, recent research jointly aligns audio, depth, infrared, and IMU data with the vision [8] and language [34] modalities, aiming to extend the zero-shot capability of the vision-language FMs to multiple modalities. These multi-modal FMs show excellent performance in associating unobserved data pairs of existing modalities.\nRecent research aligns IoT sensor signals to textual semantic features generated by FMs for zero-shot IoT sensing. For example, the work in [33] jointly aligns FM's textual embeddings with multiple IoT sensor signals, including video, LiDAR, and mmWave in a unified semantic space. It demonstrates FM's ZSL capability in recognizing unseen class IoT data. However, this work is built upon large quantities of multi-modal data samples where all modalities are presented together, which are expensive to acquire and impractical if new modalities are to be added to the semantic space. EdgeFM [27] leverages FMs for zero-shot sensing on resource-limited edge devices. However, EdgeFM only supports the existing modalities of FMs, including video, images, and audio.\nThis work aims to leverage FMs' generalized knowledge for zero-shot IoT sensing based on mmWave, IMU, and Wi-Fi signals by aligning the IoT data embeddings with the semantic embeddings generated by an FM's text encoder. However, connecting IoT sensor signals with semantic embeddings for effective ZSL is non-trivial. First, IoT sensor signals typically follow certain physics principles, which are strong supervision for effective prompt engineering to generate robust semantic embeddings. To address this, we employ cross-attention to combine a learnable soft prompt that is optimized automatically using training data and an auxiliary hard prompt that encodes domain knowledge. Second, given that the training only involves seen class data, the ZSL model is easily biased to seen classes. To address the bias problem, we propose using data augmentation to synthesize unseen class IoT data for fine-tuning our IoT feature extractor and embedding projector. Our approach works as follows. We apply prompt engineering on class labels and use an FM's text encoder to extract their semantic embeddings as class prototype representations. Meanwhile, we use an IoT feature extractor to extract features from IoT sensor signals followed by an IoT embedding projector to project the features to the semantic space. During model training, we use contrastive learning to align the class prototypes and IoT embeddings. During zero-shot classification, we conduct open-set detection to identify data of unseen classes and use FM to do zero-shot learning. We evaluate our approach on multiple datasets including MM-Fi (mmWave, Wi-Fi), USC-HAD (IMU), and PAMAP2 (IMU). Our approach achieves superior performance in open-set detection and generalized zero-shot learning compared with various baselines. This paper's contributions are summarized as follows.\n\u2022 To leverage the domain knowledge for zero-shot IoT sensing, we propose using cross-attention to combine a learnable soft prompt and an auxiliary hard prompt for effective prompt engineering.\n\u2022 To eliminate the problem of unseen class IoT embeddings biasing to seen class embeddings, we employ data augmentation and open-set detection for generalized zero-shot IoT sensing.\n\u2022 We evaluate our approach on multiple IoT datasets with IMU, mmWave, and Wi-Fi data. The results demonstrate that our approach outperforms various baselines in both open-set detection and generalized zero-shot learning."}, {"title": "2 Background and Related Work", "content": "Foundation Models (FMs) are general deep learning models that are pre-trained on massive amount of data to support various downstream tasks such as chatbot [17, 1] and image recognition [18]. FMs are extensively studied in natural language processing and computer vision [31]. For example, ChatGPT is fine-tuned for conversational tasks from the generative pre-trained transformer-based language foundation models, e.g., GPT-3.5 [3] and GPT-4 [1]. CLIP [18] is a vision-language foundation model that trains an image encoder and a text encoder jointly aiming to predict the correct image-text pairs. CLIP achieves zero-shot transferability to unseen image recognition tasks after training on 400 million image-text pairs. More recently, FMs are applied to other modalities, including audio, depth, IMU, and infrared [34, 8]. These multi-modal FMs use transformer-based encoders to extract embeddings of different modalities. Then, a joint embedding space is learned via contrastive learning that aligns the embeddings of different modalities with the embedding of a \"binding\" modality, i.e., vision or language. The learned joint embeddings can be used for various tasks such as cross-modal retrieval, cross-modal generation, and composing modalities with arithmetic. The multi-modal FMs trained on different cross-modal data pairs, e.g., (image, text) and (image audio), can implicitly associate unobserved data pairs, e.g., (audio, text), which is defined as emergent zero-shot classification. Different from the existing works that focus on FMs' zero-shot transferability on unseen datasets [18, 21] and unobserved data pairs [8, 34], our work aims to investigate the zero-shot capability of FM characterized by the performance of generalizing to unseen object categories in classification tasks, which represents a more practical scenario in IoT sensing tasks.\nZero-Shot Learning (ZSL) aims to classify data of unseen classes with the help of semantic information containing knowledge about both seen and unseen classes [16]. Traditional ZSL methods focus on classifying data into unseen classes. A more realistic setting is the generalized zero-shot learning (GZSL) that classifies data samples of seen and unseen classes simultaneously. GZSL methods can be categorized as embedding-based and generative-based. Embedding-based GSZL [2, 11] learns a projection function from data feature space to the semantic space. The goal is to map the data embeddings belonging to the same class to the ground-truth label in the semantic space. The embedding-based GZSL is easy to implement but is usually biased towards seen classes due to a lack of unseen class data features during training. Generative-based GZSL [25, 5] trains a model to generate synthetic features of unseen class data based on features of seen class data and semantic information of both seen and unseen classes. The generated features of unseen class data can be used to perform supervised learning, where a model is trained to classify data samples of both seen and unseen classes. The generative-based GZSL alleviates the biasing problem via synthesizing features of unseen classes. However, the generative models are unstable in training and susceptible to model collapse issue.\nZero-Shot IoT Sensing. Some works use hand-crafted attributes such as the movement of body or related objects and environment to construct semantic information for zero-shot IoT sensing [23, 15], which is labour-intensive and less scalable to large complex datasets. To circumvent manual attribute engineering processes, some studies utilize word vectors, which are numerical representations of words in a continuous vector space extracted by word representation models such as Word2Vec [12, 24], BERT [26], and GloVe [22], to construct semantic space. The word vectors are extracted by capturing the semantic relationships between words based on their contexts in large text corpus. However, these vectors may include task-irrelevant noise and may not directly suit the specific IoT sensing task. The work in [22] proposes to construct visual semantic space using videos of human activities for IMU-based zero-shot human activity recognition, which is shown to outperform the word vector semantic space. However, collecting videos of human raises privacy concerns. A recent work [33] jointly aligns multiple IoT data embeddings, including video, LiDAR, and mmWave, with text embeddings extracted from a vision-language FM, CLIP [18], for human activity recognition. With the unified semantic space, not only actions of seen classes can be identified but also the actions of unseen classes can be recognized by the closet textual embedding in the semantic space. However, this approach requires joint training on a self-collected multi-modal aligned dataset, which has limited usability in reality if additional sensor modalities are to be added to the system. EdgeFM [27]"}, {"title": "3 Problem Formulation", "content": "We target a deep learning-based IoT sensing task enabled by an edge-cloud cooperative system that contains the following components.\n\u2022 Edge Devices host a small-scale specialist deep neural network (DNN) $f(\\cdot)$, which can classify a limited set of seen classes $S = \\{c_i\\}_{i=1}^{N_s}$. The $f(\\cdot)$ is trained under supervised setting using a seen train set $D^s = \\{(x, y)\\}_{train} \\in X \\times S$, where $x \\in \\mathbb{R}^d$ is the raw IoT data, $y$ is the ground-truth label, and $X$ denotes the IoT data space. The input test data may include not only samples from known seen classes but also samples from novel unseen classes, denoted by $D^{test} = \\{x_{test}\\}_{test} \\in X$.\n\u2022 Cloud Server runs a large foundation model (FM) $\\Phi(\\cdot)$, which possesses general knowledge learned from web-scale training data and has the potential of zero-shot classification on unseen class data. The cloud maintains a list of interested unseen classes outside the set of seen classes $S$, denoted by $U = \\{c_i\\}_{i=1}^{N_u}$, where $S \\cap U = \\emptyset$. Note that $U$ can be specified by users or include the commonly seen classes in the IoT sensing task.\nThe primary goal is to effectively (1) detect data sample of unseen classes $x^u$ from $D^{test}$ fed to the specialist DNN $f(\\cdot)$ on the local edge devices and then (2) leverage the cloud's FM $\\Phi(\\cdot)$ to perform zero-shot classification by assigning correct label $y \\in U$ for the detected data of unseen classes. Note that an alternative way is to upload all the data to the cloud's FM for classification. However, in \u00a75.5, we will demonstrate that having the detection step alleviates the GZSL biasing problem. Such a cooperative system is common in IoT applications such as healthcare monitoring, autonomous driving, and AR/VR gaming. To achieve the goal, given an incoming IoT data sample, we first extract its IoT embedding and conduct open-set detection to determine whether the sample belongs to a seen class or unseen class, both on the edge. If it is detected as a seen class sample, we use the local specialist DNN to give prediction. Otherwise, if the sample is considered as unseen class data, we upload it to the cloud's FM for zero-shot learning."}, {"title": "4 Methodology", "content": "The overview of our approach is shown in Fig. 1, which consists of the class prototype extraction, IoT embedding extraction, model training, and zero-shot classification modules."}, {"title": "4.1 Class Prototype Extraction", "content": "In ZSL, class prototypes encapsulate the essential characteristics of each class in the semantic space. During inference, the similarity between the data embedding and each class prototype is measured to determine the sample's class. In this work, we utilize the text encoder of the vision-language FM, CLIP [18], to extract class prototypes from task-specific hints, namely prompt. Prompt can be engineered in the form of hard prompt, which is natural language instructions, or soft prompt, which is continuous, learnable vector representations. The hard prompt can integrate domain expert knowledge but needs to be manually engineered. The soft prompt can be automatically fine-tuned to adapt to various tasks but is not human-interpretable. To combine the advantages of both, we propose to use cross-attention to fuse the soft and hard prompts to generate effective and comprehensive class prototypes.\nLearnable Soft Prompt. The default prompt in CLIP is constructed by plugging the class name into a pre-defined prompt template, i.e., \"a photo of {class name}\". However, such a fixed prompt is difficult to adapt to downstream tasks. Because CLIP's default prompts tend to gather together in the semantic space, which is unfavorable for data-text alignment [32]. To address this and avoid laborious manual prompt engineering, we learn a soft prompt end-to-end from training data, aiming to align the text embedding with IoT data embedding. We follow the work in [32] and place the class token in the middle of the prompt. For each class $c$, the learnable soft prompt fed to the pre-trained CLIP's text encoder $text(\\cdot)$ is represented by $p'(c) = \\bigoplus (l_1,...,CLIPtokenizer[c], ...,l_M)$, where $\\bigoplus$ is the concatenation operation, $l_i, (i = 1 ... M)$ denotes the i-th learnable token vector, and $c$ is the class name, e.g., \"walking forward\". The learnable prompt is optimized over the training data using the loss defined shortly in Eq. 1. The extracted learnable text embedding $t'(c) = text(p'(c))$ has the same dimension as the IoT data embedding. The learned prompt token vectors $l_i, (i = 1 ... M)$ are shared for all classes, which are task-specific.\nAuxiliary Hard Prompt. The learnable soft prompt provides task-specific context by aligning the text embedding with the IoT data embedding in the semantic space. Meanwhile, IoT data is usually characterized by certain physics principles, which can be leveraged as a strong supervision for prompt crafting. For example, Fig. 2 shows that the data samples of two classes in the USC-HAD [30] dataset exhibit different patterns, which can be utilized to easily distinguish the data of the two classes. To leverage the physics principles governing the generation of the IoT sensor signals, we further use a hard prompt to give auxiliary class-specific information for constructing semantic embeddings. To automate the process, we use a state-of-the-art large language model (LLM), GPT-3.5 [3], to generate class-conditional descriptive text and fine-tune the text manually. For an IoT sensing task, we first feed the list of all classes to the LLM. Then, for each class $c$, we query the LLM: \"What are the important attributes and features to distinguish class $c$ from all the other classes?\". We then tokenized the answer to derive the auxiliary hard prompt $p^a(c)$, which will be fed to CLIP's text encoder to derive the auxiliary text embedding $t^a(c) = text(p^a(c))$, which has the same dimension as the IoT embeddings. Fig. 2 shows some example answers generated by GPT.\nCross-Attention for Combining Prompts. To leverage the advantages of both the learnable soft prompt and auxiliary hard prompt, we combine the text embeddings of the two prompts using the cross-attention [4], which is an attention mechanism for fusing two different sequences. In particular, we set $t^a$ as the key input, denoted by $K$, and $t'$ as the query and value inputs, denoted by $Q$ and $V$, respectively. The idea is to compute the attention weights between the query and key inputs, which embed the useful class-specific context information from $t^a$, and then use the weights to aggregate the value input $t'$. Specifically, $Q = \\rho_Q(t'), K = \\rho_K(t^a)$, and $V = \\rho_V(t')$, where $\\rho_m(\\cdot), (m \\in \\{Q, K, V\\})$ is a single-layer fully-connected neural network. $\\rho_m(\\cdot)$ is optimized over the training data on the loss defined shortly in Eq. 1. The attention weights are computed by $A = softmax(\\frac{QK^T}{\\sqrt{d_K}})$, where $d_K$ is the dimension of $K$. The output embedding, denoted by $t = AV$, is the class prototype. The semantic space is formed by the set of all class prototypes $T = \\{t(c) | c \\in S \\cup U\\}$."}, {"title": "4.2 IoT Embedding Extraction", "content": "For each input IoT data $x_i$, we first use a feature extractor $\\mu(\\cdot)$ to extract its features $h_i = \\mu(x_i)$. The feature extractor $\\mu(\\cdot)$ can be a commonly-used encoder like CNN, ResNet, and Transformer, which is decided by the IoT sensing modality. Then, we use an embedding projector $g(\\cdot)$ aiming to project the IoT features $h_i$ into the shared semantic space for alignment with class prototypes and derive the IoT embeddings $e_i = g(h_i)$."}, {"title": "4.3 Model Training", "content": "We freeze the text encoder of CLIP $text(\\cdot)$ and conduct model training under the supervised contrastive learning strategy, which trains the models to distinguish between similar (positive) and dissimilar (negative) data sample pairs. This allows us to learn effective representations by maximizing the distance between different classes and minimizing the distance within the same class [10].\nSupervised Contrastive Learning. First, we jointly train the learnable soft prompt $p'$, $\\rho_K(\\cdot)$ in the cross-attention module, IoT feature extractor $\\mu(\\cdot)$, and IoT embedding projector $g(\\cdot)$ on the seen train set $D^s$ using a supervised contrastive loss. Within a batch of randomly sampled data $\\{(x_i, y_i)\\}_{i=1}^{N_B}$ from $D^s$, the positive pairs contain (1) two IoT data samples belonging to the same class and (2) an IoT data sample and its class label text. The negative pairs consist of (1) two IoT data samples belonging to different classes; (2) an IoT data sample and a class label other than its own; and (3) two different class labels. The loss pulls together embeddings of positive pairs while pushing away the embeddings of negative pairs. Let $i \\in I = \\{1 ... N_B\\}$ be the index of the data in a train batch. Let $N_T$ represent the number of distinct classes in the batch and $j \\in J = \\{1 ... N_T\\}$ be the index of distinct classes. We define the supervised contrastive loss as:\n$\\mathcal{L} = \\sum_{i \\in I} \\mathcal{L}_i$\n$\\mathcal{L}_i = - log\\frac{\\sum_{p \\in P(i)} exp(e_i \\cdot e_p / \\tau)}{\\sum_{\\alpha \\in A(i)} exp(e_i \\cdot e_\\alpha / \\tau)}$\n$+ log(\\sum_{j \\in J} \\sum_{n \\in N(j)} (exp(e_i \\cdot t_n / \\tau) + exp(t_j \\cdot t_n / \\tau)))$   (1)\nwhere, for each IoT data sample $x_i$, $e_i$ is its IoT embedding, $t_j$ is its corresponding class prototype, $A(i) = I \\setminus \\{i\\}$, $N(j) = J \\setminus \\{j\\}$, $P(i) = \\{p \\in A(i) : Y_p = Y_i\\}$, and $\\tau$ is a positive temperature scalar.\nData Augmentation for Fine-Tuning. During the model training on $D^s$ described in the previous paragraph, the IoT feature extractor and embedding projector are only trained on data of seen classes. Consequently, the IoT embeddings of unseen classes are biased to the seen ones, and thus, the data samples of unseen classes may easily be classified as seen ones. To address this bias problem, we propose to train a generative model under the Generative Adversarial Network (GAN) setting to synthesize data samples of unseen classes. The goal is to derive more robust IoT embeddings by fine-tuning the IoT feature extractor and embedding projector using the augmented unseen class data. Given the train set $D^s$, we learn a conditional generator $G(\\cdot)$ that takes as input the class prototype $t(y)$ and a random Gaussian noise vector $z$, aiming to output the synthesized IoT data $\\hat{x} \\in X$ of class $y$. Note that the class prototype $t(y)$ is generated by the frozen text branch. To achieve this goal, we modify the loss in [25] and define the data augmentation loss as: $\\mathcal{L}_{da} = \\mathcal{L}_{wgan} + \\mathcal{L}_{CLS}$. Specifically, $\\mathcal{L}_{WGAN} = E[D(x, t(y))] - E[D(\\hat{x}, t(y))] - \\lambda E [(\\Vert \\nabla_{\\hat{x}} D(\\hat{x}, t(y)\\Vert^2 - 1)^2]$, where $D(\\cdot)$ is the discriminator, $x$ is the real data, $\\hat{x} = G(z, t)$ is the generated data, $\\tilde{x} = \\alpha x + (1 - \\alpha)\\hat{x}$ with $\\alpha \\sim U(0,1)$, and $\\lambda$ is the penalty coefficient. $\\mathcal{L}_{CLS} = -E[log P_r(y | \\cdot; \\theta)]$ is the classification loss computed by a linear softmax classifier parameterized by $\\theta$ that is pre-trained on $D^s$. The generator is trained by optimizing the objective:\n$min_G max_D \\mathcal{L}_{DA}$"}, {"title": "4.4 Zero-Shot Classification", "content": "As outlined in \u00a73, we decompose the zero-shot classification into two steps. The first step is to identify unseen class data, i.e., open-set detection, on the local edge devices. The second step is to conduct zero-shot learning using the FM located on the cloud.\nOpen-Set Detection is a binary classification problem to identify whether a data sample belongs to seen or unseen classes. Inspired by the work in [20], we develop a distance-based method for open-set detection. First, based on a train set $D^s$, we cluster the IoT embeddings of all data samples based on their classes and denote these clusters by $\\{E_i\\}_{i=1}^{N_s}$, where $N_s$ is the number of seen classes. Each class cluster $E_i, (i = 1 ... N_s)$ consists of a set of IoT embeddings $\\{e_{i,j}\\}_{j=1}^{N_i}$, where $N_i$ is the number of data samples in $E_i$. For an input data sample $x_{test} \\in D^{test}$, which may belong to either seen or unseen classes, we compute the Euclidean distances between its IoT embedding $e_{test}$ and the IoT embeddings in each class cluster $E_i$ as $d_{i,j} = ||e_{test} - e_{i,j}||_2, e_{i,j} \\in E_i$. We sort $d_{i,j}$ to obtain the $k_i$-th smallest distance for each cluster, denoted by $d_{(k_i)}^i$. We use a simple threshold-based criterion on $d_{(k_i)}^i$ to determine whether the input sample belongs to seen or unseen classes:\n$Q(x_{test}; k_i) = \\sum_{i=1}^{N_s} \\mathbb{1}(d_{(k_i)}^i < \\lambda_i)$   (2)\n$S_{open}(x_{test}) =  \\begin{cases} Unseen, Q = 0 \\\\ Seen, Q \\geq 1 \\end{cases}$   (3)\nwhere $\\mathbb{1}$ is the indicator function and $\\lambda_i$ is the class-specific distance threshold that is decided empirically by correctly associating a high fraction of seen class data samples to their corresponding class clusters using a validation set. If the value of $Q$ equals 0, it indicates that the test sample does not belong to any seen class clusters and should be considered as unseen. If the value of $Q \\geq 1$, it means that the test sample can be associated with at least one seen class cluster and should be considered as seen.\nZero-Shot Learning. For a detected \u201cunseen\u201d test sample $x_{det}$ with IoT embedding $e_{det}$, we upload it to the cloud's FM for zero-shot learning. Specifically, we compute the similarity scores, i.e., dot product, between $e_{det}$ and all the class prototypes in $\\{t(c), c \\in U\\}$. Then, the class with the highest similarity score is the predicted label $\\hat{y}_{det}$ for $x_{det}$.\n$\\hat{y}_{det} = argmax_{c \\in U} (e_{det} \\cdot t(c)^T)$,   (4)"}, {"title": "5 Evaluation", "content": "We evaluate our approach on multiple IMU, mmWave, and Wi-Fi datasets that are commonly used in IoT sensing tasks as follows.\nUSC-HAD [30]. The USC Human Activity Dataset is an IMU dataset of 12 different daily activities collected from 14 human subjects. By sampling it with a 1.28-second window and a 50% overlap rate, we obtain 42,708 samples, each consisting of 1.28-second 3-axis accelerometer and 3-axis gyroscope readings. We divide the activities into 9 seen classes and 3 unseen classes.\nPAMAP2 [19]. The Physical Activity Monitoring Dataset consists of 12 daily activities by collecting IMU data following a protocol from 9 subjects. We divide the activities into 9 seen classes and 3 unseen classes. We adopt a 1.71-second sliding window with a 10% overlap rate to extract 4,178 samples.\nMM-Fi [29]. The MM-Fi dataset is a multi-modal wireless human sensing dataset consisting of 1,080 consecutive sequences with over 320k synchronized frames from five sensing modalities. We adopt the Wi-Fi and filtered mmWave sub-datasets in environment 4 from the MM-Fi. We resample mmWave and Wi-Fi data using 1-second and 0.6-second sliding windows with 10% overlap, respectively, yielding 27,337 mmWave samples and 8,748 Wi-Fi samples. For both mmWave and Wi-Fi, we split the 27 activity classes into 22 seen classes and 5 unseen classes.\nWe adopt a K-fold evaluation strategy to split each dataset into seen classes and unseen classes. For USC-HAD and PAMAP2, we randomly select 3 unseen classes in each of K=4 folds. For mmWave and Wi-Fi, we randomly select 5 unseen classes in K=5 folds. For the seen class data samples, we divide them into training, validation, and test sets with a ratio of 8:1:1. The validation set is used to tune the parameters like $\\lambda_i$. The test set has equal number of seen class and unseen data samples."}, {"title": "5.2 Implementation Details", "content": "We use Pytorch to implement our approach. We use Vision Transformer as the IoT Feature Extractor for all modalities. For class prototype extraction, we use GPT-3.5 to generate auxiliary hard prompts. The text encoder is adopted from the frozen CLIP text encoder with ViT-B/16 backbone. The supervised contrastive loss's temperature parameter $\\tau$ is set to 0.2. For data augmentation, the random Gaussian noise vector $z$ follows a normal distribution $N \\sim (0, 1)$, and the penalty coefficient $\\delta$ is set to 10. During training, the optimization is performed via the Stochastic Gradient Descent with Momentum (SGDM) algorithm. The learning rate is 0.001 and the batch size for training is 64. In open-set detection, the $k_i$ is set to 0.08$\\times N_i$. The threshold $\\lambda_i$ is set to a number that guarantees a large percentage of seen data in validation set can be successfully classified. This percentage is set to 80% for USC-HAD, PAMAP2, MM-Fi (Wi-Fi), and 75% for MM-Fi (mmWave). All results are obtained by calculating the mean and variance on all splits for each dataset."}, {"title": "5.3 Open-Set Detection Performance", "content": "We consider the following open-set detection baselines.\nMSP [9] measures the maximum softmax probability generated by a model trained on the seen class data using cross-entropy loss to detect unseen class data. For the MSP baseline, we adopt the Vision Transformer as model architecture.\nKNN [20] computes the k-th nearest neighbor distance between an input image feature and the training set for unseen class data detection. The images are augmented, e.g., by adding Gaussian noise, for supervised contrastive learning in KNN. In the KNN baseline, we augment the IoT data also by adding noise and use supervised contrastive learning to extract IoT embeddings.\nMCM [13] measures the distance between an input image feature and its closest label embedding, both directly generated by a large vision-language FM, for unseen class data detection. For the MCM baseline, we replace the image features with our IoT embeddings and use the prompt template \u201cThe human action of [CLASS]\u201d for text encoding.\nFor all open-set detection baselines, we set the detection thresholds and parameters using the same strategy as our method.\nTo evaluate the performance of open-set detection, we employ the weighted precision, recall, and F1 score."}, {"title": "5.3.2 Results", "content": "In Table 1, we can see that our approach achieves the best open-set detection performance compared with all baselines on all three modalities' datasets. In detail, our approach outperforms the traditional softmax-based method MSP because the supervised contrastive loss can help our model obtain more distinguishable IoT embeddings than the cross-entropy loss in MSP. The KNN method performs worse than ours. This is because the image augmentation used by KNN for supervised contrastive learning, e.g., adding noise, cannot be directly applied to IoT data. Differently, our approach aligns text embeddings with IoT embeddings using supervised contrastive learning and achieves more generalized IoT embeddings. Our approach performs better than MCM since the MCM only takes hard prompts to generate text embeddings, which is undesirable for aligning IoT embeddings of different tasks with text embeddings."}, {"title": "5.4 Zero-Shot Classification Performance", "content": "We consider the following baselines for evaluating the GZSL performance of our approach.\nALE [2] measures the compatibility of image features and class label embeddings in the Euclidean space for ZSL.\nDCN [11] uses a Deep Calibration Network to map image features and class prototypes to a common embedding space for ZSL.\nBERT [7] We replace the frozen CLIP text encoder in our approach with the pre-trained BERT to process the prompt template as a baseline.\nf-CLSWGAN [25] uses an attribute conditional feature generating adversarial network to generate CNN features of unseen classes for ZSL.\nFREE [5] learns a visual feature generator jointly with a feature refinement module for ZSL.\nALE, DCN, and BERT are embedding-based methods, while f-CLSWGAN and FREE are generative-based methods. We replace the image features with IoT embeddings in all the above methods as baselines.\nWe evaluate the performance of GZSL using the following metrics. We measure the percentage of correctly classified seen and unseen class data samples, i.e., seen class accuracy $ACC_s$ and unseen class accuracy $ACC_u$, respectively. Note that these accuracies are the weighted average across all seen/unseen classes. We also compute the harmonic mean [16], which is a conventional metric to measure the inherent biasness of a GZSL method with respect to the seen classes:\n$ACCH = \\frac{2 \\times ACCS \\times ACCU}{ACCS + ACCU}$   (5)"}, {"title": "5.4.2 Results", "content": "As shown in Table 2, our approach achieves the best $ACC_u$ and $ACCH$ on all datasets compared with all baselines. Although some baselines have higher $ACC_s$, it is impractical to only consider seen classes since recognizing both seen and unseen classes is critical for most IoT sensing tasks. Specifically, our approach outperforms embedding-based approaches"}]}