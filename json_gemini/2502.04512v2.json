{"title": "Safety is Essential for Responsible Open-Ended Systems", "authors": ["Ivaxi Sheth", "Jan Wehner", "Sahar Abdelnabi", "Ruta Binkyte", "Mario Fritz"], "abstract": "AI advancements have been significantly driven by a combination of foundation models and curiosity-driven learning aimed at increasing capability and adaptability. A growing area of interest within this field is Open-Endedness \u2013 the ability of AI systems to continuously and autonomously generate novel and diverse artifacts or solutions. This has become relevant for accelerating scientific discovery and enabling continual adaptation in AI agents. This position paper argues that the inherently dynamic and self-propagating nature of Open-Ended AI introduces significant, under-explored risks, including challenges in maintaining alignment, predictability, and control. This paper systematically examines these challenges, proposes mitigation strategies, and calls for action for different stakeholders to support the safe, responsible and successful development of Open-Ended AI.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) has achieved remarkable progress driven by foundation models (Bommasani et al., 2021). Across various modalities, these models have shown incredible performance in tasks for which they were designed (Ramesh et al., 2021; Rombach et al., 2022; Achiam et al., 2023; Radford et al., 2023; Brooks et al., 2024). However, they are not yet capable of autonomously and indefinitely producing new creative, interesting and diverse discoveries. Such open-ended discovery is key to making progress on problems that cannot be solved by simply following a specified objective. Indeed humans use such open-ended processes to accumulate knowledge and solve difficult problems. Thus, it has been argued that open-endedness is a key ingredient for Artificial Superintelligence (Stanley, 2019; Team et al., 2021; Jiang et al., 2023; Nisioti et al.,\n2024; Hughes et al., 2024), which could outperform humans at a wide range of tasks (Morris et al., 2024).\nSpecifically, Open-Ended (OE) AI continuously produces artifacts that are novel and learnable to humans. This enables it to generate new, complex, creative, and adaptive solutions over time (Soros & Stanley, 2014; Soros et al., 2017; Clune, 2019; Sigaud et al., 2023; Lu et al., 2024; Akiba et al., 2025). Unlike traditional AI systems that optimize for fixed objectives, OE AI perpetually explores new solutions and adapts to changing circumstances without being given an explicit goal.\nThere is a large diversity of systems that aim to be open-ended. The Paired Open-Ended Trailblazer (POET) algorithm (Wang et al., 2019) facilitates OE exploration by co-evolving environments and agents. The environments become increasingly diverse and complex based on the weaknesses of the agent, while the agent develops solutions that may transfer across environments. The Voyager method (Wang et al., 2024a) is an LLM-powered embodied agent for lifelong learning in Minecraft. It utilizes an automatic curriculum for OE exploration, a skill library to store and retrieve complex behaviors, and an iterative prompting mechanism incorporating feedback and self-verification to refine executable actions.\nHistorically, it has been a challenge to guide the exploration"}, {"title": "2. What is Open-Endedness", "content": "Defining Open-Endedness remains an ongoing challenge, as no single definition fully captures its scope (Stanley &\nSoros, 2016; Soros et al., 2017; Stanley & Lehman, 2015;\nLehman & Stanley, 2011). One definition frames OE as\ngenerating artifacts that are novel, and learnable for an ex-ternal observer (Hughes et al., 2024). This definition intro-duces subjectivity, as novelty can be evaluated differently depending on the observer, and excludes systems generat-ing unintelligible artifacts (e.g., TV noise). Another view models OE systems via evolutionary principles, prioritizing"}, {"title": "2.1. Definition", "content": "An open-ended AI system is one that continuously generates artifacts that are novel and learnable for an observer.\nConsider a system S that generates a sequence of artifacts\nA1:t indexed by time t, where each artifact resides within a\nstate space A. The observer O has a model Mt that has\nobserved a sequence of artifacts A1:t up until t. Mt is a\nproxy for the observer's prediction capability. The observer\njudges the quality of Mt by a loss function L(Mt, At'),\nwhere At is an artifact generated in future, t' > t.\nBorrowing from Hughes et al. (2024) we consider a sys-tem to display novelty if it produces artifacts that become progressively less predictable as time advances. Formally:\n$\\forall t < t' \\exists t^* > t' : E[L(M_t, A_{t'})] < E[L(M_t, A_{t^*})]$\nThis means for a static observer there will always be an artifact in the future that is worse at getting predicted. This ensures the system keeps generating outputs that introduce new and less predictable information over time.\nOE AI is learnable if incorporating a longer history of artifacts improves the observer's ability to predict future outputs. This is formalized as:\n$\\forall t < t' < t^* : E[L(M_t, A_{t^*})] > E[L(M_{t'}, A_{t^*})]$\nHere, the loss decreases as the observer integrates more past artifacts, indicating improved understanding over time.\nIn contrast, we use the term \"traditional\" to refer to all AI systems that are not open-ended. This also includes systems that act autonomously or continually adapt, such as LLM agents or RL algorithms, as long as they are not open-ended."}, {"title": "2.2. Applications", "content": "OE AI has been proposed as the pathway for agents to evolve skills and knowledge in diverse, rich task environ-ments across infinite horizons, often as a way to achieve"}, {"title": "2.3. Safety of Open-Ended AI", "content": "Several definitions of safety exist, originating from domains with a long history of safety research, such as aerospace, healthcare, and critical infrastructure (Suyama, 2005; Kafka, 2012). In AI, safety aims to prevent AIs from being used to cause harm or themselves causing harm. Thus safety for AI is often tied to error-based definitions, where safety viola-tions occur due to identifiable faults or deviations from in-tended behavior. However, applying these definitions to OE AI presents unique challenges. For OE AI, which evolves unpredictably and generates novel outputs, errors cannot be predefined as it operates beyond the boundaries of prior design specifications. As a result, error-based definitions of safety are inapplicable to OE. Instead, we adopt a risk man-agement perspective to define safety for OE AI (Leveson, 2012). Here, safety is the ability to systematically identify, assess, and mitigate risks, even when the system's artifacts are novel. This definition implies that under high-stakes scenarios, the absence of risk management itself is a risk."}, {"title": "3. Challenges and Risks", "content": "OE AI exhibits emergent behavior, where outputs may deviate significantly from expectations due to vast input spaces, complex internal dynamics, or adaptation to changing conditions. They may develop unsafe, unethical, or misaligned behaviors. We discuss their inherent unpredictability challenges, trade-offs, difficulty to control, and broader consequential societal factors."}, {"title": "3.1. Unpredictability", "content": "OE AI is necessarily unpredictable, due to its propensity for generating novel artifacts. As artifacts become increasingly"}, {"title": "3.2. Creativity vs. Control", "content": "OE AI creates a fundamental tension between creativity and control in OE search (Ecoffet et al., 2020).\nLack of Explicit Guidance. OE AI often operates without predefined boundaries, constraints, or clear objectives. This allows it to explore vast and uncharted regions of the state space freely and generate creative solutions that are not reachable by simply specifying the desired state. While this promotes novelty and creativity, it makes it difficult to predict or control the direction of the system to ones we deem valuable and safe.\nEvolving Model and Environment. Unlike traditional sys-tems, the agent gains new skills and capabilities, generating new artifacts and adapting over time. The evolving nature of"}, {"title": "3.3. Misalignment", "content": "The ability to align AI systems with human values is a grand challenge within the field of AI Safety (Hendrycks et al., 2021; Ji et al., 2024) that is essential for ensuring the safety and usefulness of AI systems. The aim is to align the goals that an AI system intrinsically values and pursues with those of its human designers. This can include intended objectives, ethical guidelines, or safety requirements. AI alignment is usually formulated for AI systems that optimize an explicit, human-designed reward function. In such a setting misalignment can occur because the reward function does not precisely match the designers' objective (Krakovna et al., 2020) or because the AI internalizes goals that are different from the explicit incentives (Shah et al., 2022; Di Langosco et al., 2022).\nHowever, OE AI does not optimize an explicitly defined reward function with a focus on diversity. Instead, the designers may provide implicit incentives by structuring the search process in ways that are likely to lead to artifacts that they value highly. This necessitates a different lens for analyzing the alignment of OE AI (Ecoffet et al., 2020).\nThe designer might not correctly specify their values in the structure of the OE AI or process. The result would be an OE AI being driven towards an undesired goal. OE AI could still learn to intrinsically pursue goals that are different from those specified in the OE process. For example, humans evolved by evolution, which is an OE process whose structure causes it to optimize for inclusive fitness. However, humans do not value inclusive fitness intrinsically but have intrinsic drives towards sugary foods or protected sex.\nAlignment of Evolving Systems. Another difference is that the goals pursued by an OE AI can evolve throughout its lifetime, while the goals pursued by a traditional ML system remain static. This means that tests or guarantees about the alignment of an OE AI at one time become outdated as the system keeps evolving. Additionally, as OE AI explores novel situations, we cannot be sure that alignment training performed initially will generalize to new situations.\nAlignment of Interactive Components. OE AI systems often include multiple components. This might be an LLM with additional components, multiple agents or an agent in an evolving environment. Even though these individual components might be aligned, their dynamic interactions can result in emergent behaviors that are misaligned. For"}, {"title": "3.4. Traceability", "content": "Tracking and reproducing an OE AI's processes and outcomes generated is a challenging task. This could be coupled with a negative cascading effect that small changes in artifacts or system states can trigger, causing the system to diverge from its intended trajectory.\nLack of Reproducibility. Reproducing the evolving OE AI at a certain time is significantly more challenging than traditional AI due to 1) the lack of clear training objectives, and 2) not being able to reproduce the intermediate environmental feedback and states (Flageat & Cully, 2023;\nFlageat et al., 2024), making it hard to trace and attribute the exploration paths. For example, evolving to images that resemble real objects from random initial images is like \"finding needles in a haystack\u201d (Secretan et al., 2008) given the astronomically large search space. This can hinder the rigorous scientific progress in this domain which requires transparent, open-source, and auditable technologies.\nDifficulties in Attribution. A research direction that helps enable oversight, and evaluate and improve the correctness of solutions is self-consistency checks. Wang et al. (2023) used a prompting strategy that samples a diverse set of reasoning paths and then selects the most consistent answer. Fluri et al. (2024) proposed a framework to evaluate superhuman models by checking if they follow interpretable human rules, e.g., counterfactuals should flip the predicted decisions. Creating similar tests for OE AI is more difficult. One can change the parameters of the initial state of an OE AI to create a counterfactual environment; however, due to compounded cascading effects, the effects of the changed parameters cannot be easily isolated and are entangled with other novelty-related randomized intermediate states.\nSpecific outcomes from OE AI are harder to reproduce. This hinders our understanding of the technology and makes it harder to run consistency-based diagnostics."}, {"title": "3.5. Resource Constraints", "content": "As the OE AI runs longer, it generates increasingly com-plex artifacts that require more computational and human"}, {"title": "3.6. Trade-offs", "content": "As the OE AI systems evolve, they must balance competing priorities, often resulting in trade-offs that make the deployment of these systems challenging. As explained in Figure 2, OE AI inherently faces a trade-off between speed, novelty, and safety, creating a trilemma where optimizing two of these dimensions often compromises the third. Speed refers to the rate at which the system can generate new artifacts. Novelty measures the degree of uniqueness or originality in each newly generated artifact. Safety represents the system's adherence to predefined constraints, ensuring outputs avoid harmful, unethical, or undesirable outcomes.\nApplication-Specific Needs. Trade-offs can be difficult to navigate because they can depend on the types of problems we use OE AI for, which may require specific emphasis on one of these dimensions. In safety-critical applications such as drug discovery or medical diagnosis, safety is the foremost concern, often necessitating slower exploration to ensure rigorous validation and prevent harm, limiting novelty and speed. Conversely, in applications like gaming or art, novelty is prioritized to foster creativity, where the associated risks are generally lower, allowing safety to"}, {"title": "3.7. Social and Human risks", "content": "It is crucial to consider the societal risks of OE AI. While all new technologies may have negative societal consequences, the unpredictable and evolving nature of OE AI may amplify known AI harms or introduce unanticipated ones.\nThe Rate of Novelty. OE AI generates more novel artifacts than traditional AI and the rate of innovation and disruption is harder to anticipate. This might outpace society's ability to adapt, integrate, and understand new developments. History provides examples of the disruptive effects of excessive novelty, such as the Industrial Revolution, which, while transformative, led to widespread social upheaval, labor displacement, and the erosion of traditional ways of life. Purely AI-led innovation can result in a loss of human agency in shaping scientific and societal progress, leaving individuals feeling disconnected from the process of discovery and creation.\nUninteresting Artifacts. OE AI should produce results that are interesting and useful to the observer. Quantifying \"interestingly new\" progress has been one of the grand challenges in OE research. Foundation models have been used as a Model-of-Interestingness (MoI) (Zhang et al., 2024b) to denote the human notion of what can be considered \"novel\" and at the same time \u201cinteresting\u201d. However, OE AI could still produce uninteresting artifacts. This could be because its sense of interestingness might be misaligned with ours or because it may get stuck in a narrow set of artifacts without exploring more widely. Also, as artifacts can be very complex it can be difficult for humans to determine whether they are truly interesting and useful. This could lead to situations where an OE AI produces useless, uninteresting artifacts, while humans do not recognize this. If such a system is kept running it will be a waste of resources. Furthermore, it might limit human creativity if human ideas are biased by generated artifacts or if humans think there is nothing more to explore. Such problems are now discussed with LLMs and how they can homogenize individuals' beliefs and lead to a false impression of consensus (Burton et al., 2024).\nDifficulty to Plan. As discussed in Section 3.5, it is intractable to foresee, plan, or track the OE AI's progress or whether it would produce valuable solutions. Given limited resources, we may need to prioritize which problems we delegate to OE AI. This has a resemblance to funding deci-"}, {"title": "4. Technical Mitigations of Risks", "content": "To address the risks and challenges, we explore and suggest research directions that enhance safety against catastrophic risks while responsibly maintaining the benefits of OE ex-ploration."}, {"title": "4.1. Oversight", "content": "As it is hard to anticipate the safety of OE processes, it is critical to oversee, either by humans or another system, their behavior during execution. Oversight provides a mechanism"}, {"title": "4.2. Constraints", "content": "Most existing safety frameworks focus on structured environments with predefined goals. However, building guardrails to prevent the OE AI from exploring unsafe artifacts will be crucial to ensure the safety of these systems.\nConstrained Exploration. Since OE AI often pursues diversity, the exploration process can inadvertently drive the system into unsafe or misaligned state spaces. By constraining exploration to an e-ball, the system can balance novelty with safety, similar to safe exploration in RL (Garcia &\nFern\u00e1ndez, 2015). This requires constrained novelty metrics that evaluate novelty relative to both past behaviors and pre-defined safety constraints. In simple, discrete domains, such a novelty metric could be formally specified, while LLM-based judges could quantify novelty in more complex domains. Based on the novelty scores of new artifacts, it would be possible to penalize novel behaviors that exceed a prob-"}, {"title": "4.3. Adaptive Alignment", "content": "Current alignment techniques assume a model and its environment remain static, thus only requiring safety training once. New continual alignment algorithms could allow us to adapt safety as the model and its circumstances change (Zhang et al., 2024a). While Moskovitz et al. (2024) composite reward weighting dynamically and Hong et al. (2024) address overoptimization and ambiguity, they lack robust mechanisms for long-term feedback loops. Multi-agent RL for co-evolving alignment dynamics in OE systems can be a promising research direction. Using dynamic reward functions can adjust the reward signals to reflect the evolving human preferences or system performance. Adaptive preference scaling (Fang et al., 2024; Hong et al., 2024), and distributional preference reward modeling (Li et al., 2024) have been used to refine reward functions in RL-based systems by adjusting reward weights in response to shifting human feedback or performance degradation. For OE AI,"}, {"title": "4.4. Safety Evaluations", "content": "Finally, continuous safety evaluation of OE AI is important for understanding the extent of unsafe behaviors.\nBenchmarking OE Safety. Developing benchmarks specifically for OE AI is crucial for quantifying its risks and evaluating failure modes. Existing benchmarks, such as those on multi-agent risks and unintended consequences (Rivera et al., 2020), provide some insights but fail to incorporate the unique characteristics of OE algorithms. A dynamic benchmark explicitly designed for OE AI would need to address its continuous evolution, novelty generation, and dynamic complexity. For example, the difficulty of tests could be adjusted to the OE AI's changing capabilities.\nRedteaming OE Systems. The previously outlined direction of \"extrapolating risks\" is beneficial to anticipate future risks even if the OE system is aligned. On the other hand, targeted red teaming can reveal failures for individual components or the entire system. Red teaming allows us to stress-test OE systems by actively probing their vulnerabilities and finding situations in which they behave unsafely.This could involve manually or adversarially finding inputs on which the OE system misbehaves. Lehman et al. (2023);\nBradley et al. (2023); Liu et al. (2024) uses LLMs to enhance genetic programming by generating diverse, functional artifacts. These outputs could serve as adversarial artifacts to test and evaluate system robustness like in (Samvelyan et al., 2024), but here the aim would be to test the entire OE systems. Further, one could construct an environment in which the OE system is being led to produce unsafe artifacts."}, {"title": "5. Call for Action", "content": "Ensuring the safe development and deployment of OE AI requires active engagement from various stakeholders.\nFunding bodies can shape research priorities. They could urge OE researchers to consider and address the safety risks of their work. Further, they could dedicate resources toward robust safety mechanisms and evaluations for OE AI.\nResearch on the intersection of safety and OE research is crucial, impactful and under-explored. We argue that safety should be a critical part of OE research. This requires general awareness of the risks and dedicated research on safety problems. Additionally, the AI safety community should dedicate research to the specific risks of OE AI. We hope this paper can provide a bridge to foster exchange and collaboration between these communities.\nOpportunities lie in the application of OE AI to AI Safety."}, {"title": "6. Alternative Views", "content": "We argue that safety is essential for the responsible development of OE AI. In contrast, one can argue that an overemphasis on safety and being overly cautious may lead to a lack of progress in the field. Over-constraining these systems through safety mechanisms undermines their capacity for innovation. This especially holds if one thinks that current OE systems are not capable enough yet to cause large harm. Under this view, work on safety should be postponed until these systems pose concrete dangers. Lastly, some might argue that OE systems are inherently uncontrollable, hence, imposing safety mechanisms is futile."}, {"title": "7. Conclusion", "content": "Open-Ended AI is a promising paradigm for generating novel, adaptive solutions in complex and dynamic environments, driving interest across research and applied domains. However, its open-ended nature introduces specific safety challenges that must be addressed to enable responsible deployment and maximize its societal benefits. We argue that the inherent unpredictability and uncontrollability of OE AI, challenges in ensuring and maintaining alignment, traceability, and societal impacts, as well as trade-offs in resource use and safety. We highlight the critical importance of human and automated oversight over OE AI. Further, we suggest ways of giving adaptive guidelines to OE AI that retain its creativity and co-evolve with it. Lastly, we call for targeted safety evaluations and provide concrete suggestions on how different stakeholders can contribute to the responsible development of OE AI. Ultimately, we hope this paper will lead the OE and safety communities and other stakeholders to consider safety a priority in the development"}, {"title": "Impact Statement", "content": "OE AI is an emerging research direction within the AI community. If successful, such systems will have large impacts on the world. This paper aims to urge the OE and safety community to consider safety as a priority when developing OE AI. As such, it will reduce potential harms from OE AI in the real world while allowing for larger beneficial adoption of OE AI. While prioritizing safety might slow down the progress of OE AI, we argue that is essential to have safe-by-design OE AI from first principles to proactively avoid risks before they happen."}]}