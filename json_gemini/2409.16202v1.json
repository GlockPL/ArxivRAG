{"title": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese Junior High School Exam Data", "authors": ["Qian-Wen Zhang", "Haochen Wang", "Fang Li", "Siyu An", "Lingfeng Qiao", "Liangcai Gao", "Di Yin", "Sun Xing"], "abstract": "Online education platforms have significantly transformed the dissemination of educational resources by providing a dynamic and digital infrastructure. With the further enhancement of this transformation, the advent of Large Language Models (LLMs) has elevated the intelligence levels of these platforms. However, current academic benchmarks provide limited guidance for real-world industry scenarios. This limitation arises because educational applications require more than mere test question responses. To bridge this gap, we introduce CJEval, a benchmark based on Chinese Junior High School Exam Evaluations. CJEval consists of 26,136 samples across four application-level educational tasks covering ten subjects. These samples include not only questions and answers but also detailed annotations such as question types, difficulty levels, knowledge concepts, and answer explanations. By utilizing this benchmark, we assessed LLMs' potential applications and conducted a comprehensive analysis of their performance by fine-tuning on various educational tasks. Extensive experiments and discussions have highlighted the opportunities and challenges of applying LLMs in the field of education.", "sections": [{"title": "Introduction", "content": "Traditional educational paradigms, which predominantly rely on human instructors, often face limitations in scalability and resource allocation. The rise of online education platforms has mitigated some of these constraints by democratizing access to knowledge and skills through digital means. The emergence of Large Language Models (LLMs), such as ChatGPT (Brown et al., 2020; Achiam et al., 2023) and Llama (Touvron et al., 2023), signifies a transformative leap in the field of artificial intelligence, demonstrating an outstanding mastery of human language. In the context of education, these models are anticipated to fundamentally reshape the technological foundations of teaching and learning on digital platforms. Despite the promising prospects, implementing LLM-based educational systems presents significant challenges (Kasneci et al., 2023; Li et al., 2023b). These models must be adept at not only understanding the specific issues encountered by students but also at applying specialized pedagogical knowledge to offer effective solutions. This necessitates a sophisticated interplay between linguistic comprehension and domain-specific expertise, ensuring that educational interventions are both accurate and pedagogically sound.\nExam questions are the most critical data in educational settings, serving as the primary means to evaluate and measure students' understanding of knowledge (Glaser et al., 2001). Regarding the next-generation system, we examine LLMs from the perspective of utilizing exam resources to enable usable tools and explore the potential of LLM-based education systems, aiming to provide valuable insights into areas where further improvements are necessary. It should be noted that existing datasets for LLMs primarily focus on the accuracy of models in answering exam questions. For instance, benchmark tests such as MMLU (Hendrycks et al., 2020) and M3Exam (Zhang et al., 2024), as well as Chinese benchmarks like CMMLU (Li et al., 2023a) and M3KE (Liu et al., 2023), are commonly employed. However, these benchmarks predominantly focus on a single type of question: multiple-choice questions. While facilitating the automated evaluation of LLMs, this focus does not fully reflect the models' comprehensive capabilities in educational assessments. This is because multiple-choice questions typically require only simple judgments, which may lead models to take shortcuts in decision-making. In contrast, early tasks, such as knowledge concept tagging and question difficulty prediction (Chen et al., 2014; Qiu et al., 2019), often utilize datasets that focus solely on their specific task labels, lacking comprehensive multi-dimensional annotation information.\nBy integrating diverse tasks and data annotations, we aim to create a more comprehensive and robust assessment reference dataset for educational LLMs. To this end, a novel benchmark named CJEval was developed. CJEval, based on real Chinese Junior High School exam questions, includes not only questions and answers but also detailed information about question types, difficulty levels, knowledge concepts, and answer explanations. It simultaneously provides four core tasks: knowledge concept tagging, question difficulty prediction, question answering, and question generation.\nIn summary, the main contributions of this paper are as follows:\n\u2022 A new evaluation benchmark called CJEval has been developed, specifically curated from data on Chinese Junior High School Examinations. It features the most comprehensive annotation information centered on exam questions, encompassing four application-level tasks across ten subjects, thus offering a robust basis for assessing educational competencies.\n\u2022 Extensive testing was conducted on a broad range of the latest LLMs, and a thorough analysis was performed through fine-tuning. This detailed examination helped in identifying the potential applications as well as the limitations of LLMs within the educational sector."}, {"title": "Online Examination System", "content": "In this section, we first introduce the online examination system and then formally describe the details of the four application-level tasks, noting that they are all question-based."}, {"title": "System Design", "content": "For the next-generation system, a comprehensive knowledge database centered around test questions is essential. As shown in Figure 1, the next-generation system provides a smarter system control center and more flexible tools than classic system designs that focus on engineering logic. Its versatility allows developers to build LLM applications through multiple agents that can communicate with each other to accomplish tasks (Wu et al., 2023). The retrieval process, inspired by retrieval-augmented generation (RAG) (Lewis et al., 2020), is optional. Whether or not retrieval results are utilized, this module supports agent selection and activates downstream tools. These downstream tools comprise prompt engineering and application optimization. Prompt engineering entails refining and structuring the input prompts to enhance system performance, while application optimization involves integrating fine-tuned LLMs to execute various tasks. This approach ensures the effectiveness and quality of the final output. The modular and sophisticated design of the system enables it to adapt to diverse educational needs, ensuring users receive precise outputs."}, {"title": "Question-Based Tasks", "content": "In general, each test question is semi-structured data that contains multiple data fields, including the question stem, answer, and some metadata. Formally, we define the set of question stems as $Q = \\{q_1,\u2026\u2026\u2026,q_n\\}$, the set of answers as $A = \\{a_1,..., a_n\\}$, and the set of answer explanations as $AE = \\{e_1, ..., e_n\\}$. Additionally, the difficulties of questions are categorized into five levels $Dif = \\{1,2,3,4,5\\}$, and the set of knowledge concepts is represented by $K = \\{k_1,...,k_m\\}$."}, {"title": "Knowledge Concept Tagging", "content": "The task of knowledge concept tagging (KCT) (Chen et al., 2014; Sun et al., 2018; Li et al., 2024) can be defined as follows: Given a particular knowledge concept $k_i$ from a set $K$ and a question $q_j$ from a set $Q$, the goal of the tagging model is to determine a binary outcome $y \\in \\{0,1\\}$. This outcome signifies whether $q_j$ corresponds to $k_i$. Previous studies have tackled this problem by transforming both the question and the knowledge concept into dense vector representations using various embedding techniques. In this paper, we default to reducing the candidate set of knowledge concepts by computing the similarity scores between questions and knowledge concepts. Subsequently, using task-specific prompts these models generate answers that align with the most relevant knowledge concepts. This task is viewed as a multi-label classification problem."}, {"title": "Question Difficulty Prediction", "content": "Question difficulty prediction (QDP) (Qiu et al., 2019; AlKhuzaey et al., 2023; Lee et al., 2023) aims to estimate the level of difficulty of a given question, often quantified as the percentage of students who fail to answer it correctly. The difficulty of a question is always related to the problem solver, so our labels are derived from historical student user error rate statistics. Formally, question difficulties are categorized into five distinct levels: {easy, relatively easy, medium, relatively difficult, difficult}. This task is conceptualized as a multi class classification problem."}, {"title": "Question Answering", "content": "The main objective of question answering (QA) (Hendrycks et al., 2020; Liu et al., 2024) is to provide an accurate answer $a_i$ to the given question $q_i$. The complexity of QA tasks varies significantly across different types of questions. For instance, multiple-choice questions typically require the model to select the correct option from a pre-defined set, which may sometimes lead to correct answers being chosen by chance. In contrast, analytical open-ended questions, like problem-solving, need detailed and structured responses, demonstrating knowledge, synthesis, and reasoning."}, {"title": "Question Generation", "content": "Question generation (QG) (Artsi et al., 2024) is a pivotal task in the domain of educational technology, particularly for the expansion and updating of question databases. The primary objective of QG is to create new, high-quality questions that can be used for assessments, practice, and learning reinforcement. Formally, given a question $q_i$, its corresponding answer $a_i$, the associated knowledge concepts $K_{q_i}$, and the difficulty level $difi$, the goal of QG is to generate novel questions that are contextually relevant and pedagogically valuable."}, {"title": "CJEval Benchmark", "content": "CJEval is collected and processed from authentic junior high school examination questions. This dataset encompasses a wide range of subjects and question types, ensuring comprehensive coverage of the curriculum. Each question is meticulously annotated with relevant metadata, including question type, difficulty level, associated knowledge concepts, and detailed answer explanations."}, {"title": "Experiments", "content": "To assess the capabilities of LLMs in educational contexts, our experiments included several open-source models that have been instruction-tuned using Supervised Fine-Tuning (SFT) (Ouyang et al., 2022) and Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Bai et al., 2022). Specifically, we utilized the following open-source models: LLaMA3-8B-Instruct, Qwen1.5-Chat series, GLM4-9B-Chat, Baichuan2-13B-Chat and DeepSeek-V2-Chat. We also obtained evaluation data through API calls for several proprietary models. These included GPT-3.5-turbo and GPT-4 from OpenAI, ERNIE-3.5, Baichuan-4, Doubao, Moonshot and Hunyuan-pro."}, {"title": "Experiment Setups", "content": ""}, {"title": "Implementation Details", "content": "Zero-shot / One-shot Evaluation We prioritized using zero-shot settings for three key reasons: First, this approach encourages the models to replicate the problem-solving dynamics typical of real-world exams. Second, certain LLMs are constrained by limited context lengths. Third, the majority of current LLMs have been instruction-tuned, making them adept at adhering to directives and generating outputs in the specified format. We also compared these with one-shot settings. This approach allows for retrieval-augmented generation, which can introduce similar knowledge to enhance task performance. One-shot settings offer an optional strategy for engineering practice, enabling models to better understand and solve subsequent problems by using a single example.\nFine-tuning Settings We utilized both full-parameter and LoRA-based (Hu et al., 2021) fine-tuning to enhance the models' ability to answer questions and master knowledge across various subjects. The training data include 20,820 samples from our proprietary database, which are sourced"}, {"title": "Evaluation Metrics", "content": "Considering that different tools are implemented by combining various tasks in real systems, our evaluation metrics for the four question-based tasks are as follows:\nFor KCT, we calculated precision, recall, and F1-score for each subject. By default, we use a text similarity model to retrieve the top 20 most similar knowledge concepts, denoted as $Kop20$, from the set K. This step ensures that the ground truth $Kqr$ is included in $Kop20$ through manual adjustment, thus focusing on the impact of downstream evaluation on the models' capabilities.\nFor QDP, we use \"Tolerant Accuracy\" (TAcc) as the evaluation metric instead of Mean Square Error (MSE), because small errors within one level are not perceptually significant to the users. Given the predicted difficulty level $di f\u2081$ and the true difficulty level $difi$, we define a tolerance function $\u03b4(di fi, di fi)$ as follows:\n$\u03b4(di fi, di fi) =\\begin{cases}\n1 & \\text{if } |di fi - di fi| < 1\\\\\n0 & \\text{otherwise}\n\\end{cases}$\nThe TAcc is then defined as:\n$TAcc = \\frac{1}{N} \\sum_{i=1}^{N} \u03b4(di fi, difi)$\nwhere N is the total number of samples.\nFor QA, different question types were evaluated using different methods. MCQs and TFQs employed regular expression matching, with a full match considered correct. Inspired by Chateval (Chan et al., 2023), GPT-4 was used as a reviewer for FBQs and AQs to determine correctness.\nFor QG, we engaged GPT-4 to evaluate the quality of the generated questions from two aspects: whether the question content included all required knowledge concepts and whether the difficulty level met the requirements. We calculated the proportion of questions meeting these criteria as evaluations, referred to as GPT-K and GPT-D."}, {"title": "Results and Analysis", "content": "To identify the strengths and weaknesses of different LLMs, we present and analyze their performance from multiple perspectives."}, {"title": "Multi-task Evaluation", "content": "Table 2 presents the results of four question-based tasks across different LLMs. The results indicate that Hunyuan-pro, GPT-4, and Doubao-pro are ahead of other proprietary and open-source models in terms of EduScore. Notably, the fine-tuning of Qwen1.5-14B significantly improved its performance in various tasks, with the highest EduScore being obtained by Qwen1.5-14B-LoRAw/AE. This phenomenon aligns with the understanding that high-quality data could help LLMs develop capabilities effectively.\nSpecifically, QA and KCT are particularly challenging tasks, with models that were not fine-tuned generally scoring below 70. In QG, these models demonstrated a strong ability to mimic the style and content of existing questions, achieving scores around 90 in both GPT-K and GPT-D. Among the fine-tuned LLMs, the model trained with answer explanations (AE) consistently outperformed the one without. This improvement can be attributed to the inclusion of reasoning processes in the answer explanations, which help the model better understand the underlying logic and context of the questions. Interestingly, full-parameter fine-tuning did not yield the best results. This could be due to several factors. One possibility is that full-parameter tuning may lead to overfitting, where the model becomes too specialized in the training data and loses its generalization capability. Another reason could be the increased computational complexity and resource requirements associated with full-parameter tuning, which might not always translate to proportional performance gains. In summary, we prefer to fine-tune smaller models on high-quality data rather than simply relying on larger language models. When it comes to training, low-loss, high-efficiency schemes like LoRA are highly valuable."}, {"title": "Multi-subject Evaluation", "content": "To more precisely demonstrate the models' capabilities across various subjects, . Using GPT-4o as a reference baseline, it is clear that models such as Qwen, which specialize in Chinese, demonstrate strong capabilities in educational tasks. All models demonstrate robust performance in subjects such as history, geography, and biology, which primarily rely on memory skills. Conversely, performance drops significantly in subjects that demand higher-order reasoning skills, such as math, physics, and chemistry. This decline can be attributed to the scarcity of existing data and the need for extensive inference and computation. Additionally, anomalies in predicting scientific knowledge concepts may arise from their customized definitions and the models' inability to form associations through generalized understanding. This underscores the necessity for further training of LLMs in applied contexts.\nTable 2 demonstrates that, even after fine-tuning, the accuracy metrics for QA tasks remain below 76%. To further analyze this, we have detailed the performance across different question types in various subjects . MRQs are more challenging than SCQs due to the necessity for complex logical reasoning to evaluate multiple options. TFQs are relatively easier for the models to comprehend. The accuracy for AQs remains low, indicating that these models struggle with tasks requiring the generation of precise and contextually appropriate responses. This finding underscores the need to enhance reasoning and language generation capabilities. Notably, GPT-4 performs well on AQ tasks, which can be attributed to its sophisticated architecture and extensive pre-training.\nOverall, Chinese models exhibit an advantage in Chinese language tasks, while GPT-4 excels in English language tasks. The fine-tuned Qwen-14B model has become comparable to GPT-4, even surpassing it in several subjects. This phenomenon underscores the feasibility of deploying large language models in various commercial applications."}, {"title": "Zero-shot vs. One-shot", "content": "To empirically study the impact of one-shot demonstrations, we selected one-shot examples for KCT from the development dataset. These examples were chosen based on having the same subject as the target question and at least one overlapping knowledge point within the $Kop20$. For QDP, the one-shot examples were selected based on having the same question type and subject, with a medium difficulty level as the standard. From the experimental results shown in Table 3, we can observe that using a small number of examples to provide effective additional information for KCT tasks can improve the performance of the model in most cases. However, in QDP, performance declined. This decline might be attributed to the fact that the one-shot examples did not adequately capture the complexity and variability of question difficulty, leading to less effective guidance for the model. Based on the above, the RAG mentioned in the system design, while important in real production, should be applied judiciously."}, {"title": "Related Work", "content": "Intelligent tutoring systems (ITS) Intelligent Tutoring Systems (ITS) have been a focal point of research in educational technology for several decades, aiming to provide personalized instruction and feedback akin to that offered by a human tutor (Nwana, 1990; Muirhead, 2000). These systems are particularly pivotal in the STEM field, where enhancing educational accessibility through online platforms has proven to be a cost-effective approach (Chirikov et al., 2020). Moreover, problem-centered instruction has been identified as a significant trend in current research, emphasizing the importance of addressing real-world issues within educational contexts (Guo et al., 2021). Research on ITS is inherently multidisciplinary, necessitating that researchers adopt innovative ideas and maintain a high level of perseverance in the face of challenging research tasks (Shneider, 2009). This comprehensive approach helps in creating systems that not only mimic the pedagogical guidance of human tutors but also enhance the educational experience through personalized learning trajectories.\nAdvancements in Large Language Models Large Language Models (LLMs) have made remarkable strides in recent years, showcasing their ability to generate human-like text, answer complex questions, and perform a wide array of natural language processing (NLP) tasks. Models such as"}, {"title": "Conclusion", "content": "In this work, we developed CJEval, a dataset sourced from Chinese Junior High School Examinations, which features diverse exam questions with detailed annotations. CJEval ensures reliability, validity, and comprehensiveness by encompassing four application-level tasks across ten subjects. This dataset provides a robust basis for assessing educational competencies and demonstrates the practicality of employing LLMs within educational contexts. We anticipate that CJEval will contribute significantly to future advancements of LLMs, particularly in handling educational tasks."}, {"title": "Limitation", "content": "Online education systems encompass a diverse array of applications, such as personalized recommendations and adaptive learning systems, which are beyond the scope of this paper. In addition, our study concentrates on the capabilities of LLMs in handling question-based tasks. We did not delve into the advantages and disadvantages of RAG methods. Future research should explore the integration of RAG methods to fully understand their impact on the performance and applicability of LLMs in instructional applications."}, {"title": "Ethics", "content": "CJEval is derived from actual junior high school test questions, which have been meticulously rewritten and scrutinized. The CJEval dataset is intended solely for academic and research purposes. Commercial use or any misuse that deviates from these intended purposes is strictly prohibited. Adhering to these guidelines is essential to maintain the dataset's integrity and ensure ethical use."}]}