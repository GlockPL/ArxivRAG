{"title": "SANGRIA: Surgical Video Scene Graph\nOptimization for Surgical Workflow Prediction", "authors": ["\u00c7a\u011fhan K\u00f6ksal", "Ghazal Ghazaei", "Felix Holm", "Azade Farshad", "Nassir Navab"], "abstract": "Graph-based holistic scene representations facilitate surgi-\ncal workflow understanding and have recently demonstrated significant\nsuccess. However, this task is often hindered by the limited availabil-\nity of densely annotated surgical scene data. In this work, we introduce\nan end-to-end framework for the generation and optimization of sur-\ngical scene graphs on a downstream task. Our approach leverages the\nflexibility of graph-based spectral clustering and the generalization ca-\npability of foundation models to generate unsupervised scene graphs with\nlearnable properties. We reinforce the initial spatial graph with sparse\ntemporal connections using local matches between consecutive frames to\npredict temporally consistent clusters across a temporal neighborhood.\nBy jointly optimizing the spatiotemporal relations and node features of\nthe dynamic scene graph with the downstream task of phase segmenta-\ntion, we address the costly and annotation-burdensome task of semantic\nscene comprehension and scene graph generation in surgical videos using\nonly weak surgical phase labels. Further, by incorporating effective inter-\nmediate scene representation disentanglement steps within the pipeline,\nour solution outperforms the SOTA on the CATARACTS dataset by 8%\naccuracy and 10% F1 score in surgical workflow recognition.", "sections": [{"title": "1 Introduction", "content": "Surgical videos capture pivotal moments of surgery, providing a valuable source\nof information that can facilitate better insights into the quality of surgery. Au-\ntomated analysis of these videos can significantly enhance surgical procedures\nvia online or offline feedback. Surgical workflow prediction has been a focal point\nof numerous studies, highlighting its critical role in enhancing surgical precision\nand efficiency through video analysis [27,22,4,23,11,17] Recently, methods based"}, {"title": "2 Methodology", "content": "We tackle the problem of semantic scene understanding and scene graph gener-\nation with an end-to-end graph-based pipeline. Formulating semantic segmenta-\ntion as a graph partitioning task, we patchify input images and establish sparse\ntemporal links with the neighboring frame patches via correspondence match-\ning, constructing a dynamic graph (the patch-based graph hereafter). We then\nperform a spectral, temporal clustering of the patch-based graph to generate a"}, {"title": "2.1 Dynamic Scene Graph Generation", "content": "For an input image I, DINO key features f are obtained by partitioning I into n\npatches and passing them to DINO. The adjacency matrix A is then generated\nby patchwise dot product as follows:\n$$A_{ij} =\n\\begin{cases}\nf_i f_j \\text{ if } f_i \\cdot f_j > 0\\\\\n0 \\text{ otherwise}\n\\end{cases}$$\nNext, we threshold the values in A >\u315c and connect highly similar nodes to-\ngether, generating a static patch graph representation $G_t = (V_t, A_t)$ for a given\nframe t. An extension of graph clustering to sequences of frames without con-\nsidering the temporal inter-dependencies leads to inconsistent clusters among\nframes with close proximity. A na\u00efve solution could be an expansion of adja-\ncency matrix calculation across the third dimension of time to find spatiotempo-\nral similarities across patches. This leads to high computational costs $O(wn^2d)$\nspecifically with the increasing length of the temporal window for n number of\npatches, patch feature of length d, and w time steps. As temporal relations re-\nquire a coarser level of attention compared to spatial dependencies [7], we suggest\na sparse dynamic linking mechanism between patches along the time dimension.\nIn this work, we leverage correspondence matching to find prominent features\nwithin frames and match those efficiently. We incorporate LightGlue [15], a dis-\ntilled deep neural network powered with self- and cross-attention, into our patch\ngraph construction setup. It is designed explicitly for low-latency problems and\nsparse inputs by predicting matches from two sets of local features. Next, for\na clip with w frames, we construct a dynamic patch-based graph, $G_{t_i \\rightarrow t_{i+w}} =$\n$(V,E)$ with node set V, edge set E, node features $X \\in \\mathbb{R}^{w \\times n \\times d}$. Spatial edges,\n$E_t$ are established using pairwise correlation similarity between those features\n(Equation 1), while for temporal edges $E_{t_i \\rightarrow t_{i+1}}$, LightGlue correspondences be-\ntween frames within a temporal sequence of w time stamps are exploited. Dy-\nnamic graph edges can be represented as follows: $E = \\cup_{1<t\\leq w} E_{t_i} + E_{t_i \\rightarrow t_{i+1}}$\nWe further reinforce the graph nodes with temporal and spatial encodings\nto accentuate the dynamic relations between objects in the scene. Temporal\nencodings capture the temporal order of object interactions and actions in a\nvideo sequence, while spatial encodings capture objects' relative positions and\norientations in a scene. For temporal encoding, we incorporate the location of\neach frame along the temporal window by adding a temporal feature vector to\nthe node feature matrix X. For spatial encoding, we incorporate the position of\npatches within the frame by adding a spatial feature vector to feature matrix X.\nThe graph clustering is performed by employing deep modularity networks\n(DMON) [26] featuring a collapse regularization objective to improve unsuper-"}, {"title": "DSG Optimization", "content": "We establish a probabilistic estimation of edge weights\nwithin the DSGs via equation $W_{i,j}^{pool} = \\sigma(MLP(X_{i,j}^{pool};\\theta_{MLP}))$, where\n$W_{i,j}^{pool}$ refers to the edge weights between the clusters of DSG, in which\n$W_{i,j} \\in [0, 1]$ indicating the strength of the relations between clusters i, j. $\\theta_{MLP}$\nrepresents the set of trainable parameters. The probabilistic setup allows for\nflexibility in optimizing the edge weights while learning the downstream tasks\nand equips the DSG generation to account for the inherent uncertainty and\nvariability present in the unsupervised graph clustering results, leading to more\nrobust and accurate inference."}, {"title": "2.2 End-to-end Pipeline", "content": "To tackle the task of phase segmentation, we propose a multi-layer GCN [12,11]\nthat takes the DSG as input. The GCN consists of multiple layers, each of which\nenables learning increasingly complex representations of the scene graph. The\noutput of the GCN is fed to a global sum-pooling layer aggregating features\nfrom all nodes in the graph. A fully-connected layer and a softmax function pre-\ndict probabilities for each phase class. A cross-entropy objective function $L_{CE}$ is\nemployed to optimize the model parameters for the surgical phase segmentation\ntask. The final objective function of the end-to-end pipeline can therefore be for-"}, {"title": "DSG Optimization", "content": "We establish a probabilistic estimation of edge weights\nwithin the DSGs via equation $W_{i,j}^{pool} = \\sigma(MLP(X_{i,j}^{pool};\\theta_{MLP}))$, where\n$W_{i,j}^{pool}$ refers to the edge weights between the clusters of DSG, in which\n$W_{i,j} \\in [0, 1]$ indicating the strength of the relations between clusters i, j. $\\theta_{MLP}$\nrepresents the set of trainable parameters. The probabilistic setup allows for\nflexibility in optimizing the edge weights while learning the downstream tasks\nand equips the DSG generation to account for the inherent uncertainty and\nvariability present in the unsupervised graph clustering results, leading to more\nrobust and accurate inference."}, {"title": "2.2 End-to-end Pipeline", "content": "To tackle the task of phase segmentation, we propose a multi-layer GCN [12,11]\nthat takes the DSG as input. The GCN consists of multiple layers, each of which\nenables learning increasingly complex representations of the scene graph. The\noutput of the GCN is fed to a global sum-pooling layer aggregating features\nfrom all nodes in the graph. A fully-connected layer and a softmax function pre-\ndict probabilities for each phase class. A cross-entropy objective function $L_{CE}$ is\nemployed to optimize the model parameters for the surgical phase segmentation\ntask. The final objective function of the end-to-end pipeline can therefore be for-"}, {"title": "3 Implementation Details", "content": "Datasets We experiment on 3 datasets: CATARACTS [3,2] consists of 50 cataract\nsurgery videos of 1920 \u00d7 1080 pixels at 30 fps. The dataset is split 25-5-20 for\ntraining, validation, and testing, with videos annotated on 19 surgical phases.\nCaDIS dataset, a subset of CATARACTS, consists of 4670 pixel-wise annotated\nimages. We use Task II of CaDIS, which defines 17 classes of objects, includ-\ning surgical tools, anatomical structures, and miscellaneous. Cataract101 (C101)\ncomprises 101 videos of 720 \u00d7 540 pixels and 25 fps performed by surgeons with\nvarious levels of expertise. The videos are annotated based on the 11 most com-\nmon phases of cataract surgery and used with 45-5-50 train-validation-test splits.\nTraining details Frames are resized to 224x224 to generate DINO-B embed-\ndings. For graph generation, a frame similarity threshold of 0.9 is chosen. We\ntrained phase segmentation models for 100 epochs using an Adam optimizer with\na learning rate of 0.0001 and a batch size of 32 on a single A40 GPU.\nMetrics For phase segmentation, we compute the accuracy and F1 score. For\nsemantic segmentation, we measure the mean intersection over union (mIoU)\nand pixel-wise accuracy (PAC)."}, {"title": "4 Results & Discussion", "content": "Surgical Workflow Prediction Table 1 presents an ablation study on window\nsize and spatial and temporal embeddings as well as a comparative analysis of\nour proposed method against existing techniques on phase segmentation tasks for\nCATARACTS and Cataract101 datasets. We show that increasing window size\ntogether with temporal embeddings improves phase segmentation performance,\nwhile spatial embeddings have minimal impact. Our method demonstrates su-\nperior performance in terms of accuracy and F1 score by indicating 8% accuracy\nand 10% F1 score improvement over previous graph-based phase segmentation"}, {"title": "5 Conclusion", "content": "We introduce SANGRIA, an end-to-end graph-based solution for concurrent\nsurgical workflow recognition, semantic scene segmentation, and dynamic scene\ngraph generation. Our jointly optimized setup featuring sparse temporal con-\nnections and graph clustering, prioritizes the graph generation for the down-\nstream task by disambiguating the graph and highlighting the most influential\ncomponents and their connections. By focusing on downstream task-specific fea-\ntures, we achieve state-of-the-art results in surgical phase segmentation on the\nCATARACTS dataset while generating scene explanations with minimal anno-\ntation."}]}