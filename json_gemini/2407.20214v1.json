{"title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction", "authors": ["\u00c7a\u011fhan K\u00f6ksal", "Ghazal Ghazaei", "Felix Holm", "Azade Farshad", "Nassir Navab"], "abstract": "Graph-based holistic scene representations facilitate surgical workflow understanding and have recently demonstrated significant success. However, this task is often hindered by the limited availability of densely annotated surgical scene data. In this work, we introduce an end-to-end framework for the generation and optimization of surgical scene graphs on a downstream task. Our approach leverages the flexibility of graph-based spectral clustering and the generalization capability of foundation models to generate unsupervised scene graphs with learnable properties. We reinforce the initial spatial graph with sparse temporal connections using local matches between consecutive frames to predict temporally consistent clusters across a temporal neighborhood. By jointly optimizing the spatiotemporal relations and node features of the dynamic scene graph with the downstream task of phase segmentation, we address the costly and annotation-burdensome task of semantic scene comprehension and scene graph generation in surgical videos using only weak surgical phase labels. Further, by incorporating effective intermediate scene representation disentanglement steps within the pipeline, our solution outperforms the SOTA on the CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow recognition.", "sections": [{"title": "1 Introduction", "content": "Surgical videos capture pivotal moments of surgery, providing a valuable source of information that can facilitate better insights into the quality of surgery. Automated analysis of these videos can significantly enhance surgical procedures via online or offline feedback. Surgical workflow prediction has been a focal point of numerous studies, highlighting its critical role in enhancing surgical precision and efficiency through video analysis [27,22,4,23,11,17] Recently, methods based on scene graph representations overperformed non-graph methods thanks to its holistic scene understanding capabilities[11,24,16,9,17]. A primary barrier to developing such technologies is the lack of dense annotations. Moreover, surgical video annotation is not only inherently burdensome but requires specialized annotation platforms and expert annotators, which can be prohibitively expensive for specialized surgeries. Given the inherent interdependence of surgical scene understanding and workflow prediction during surgical procedures, it is imperative to address these two tasks simultaneously. In this regard, we introduce a novel approach of Surgical Scene GRaph Optimization (SANGRIA) that tackles both problems using solely surgical phase labels.\nPrior works on unsupervised scene and video segmentation take advantage of optical flow [10] or similar to CutLER [28,29] use variations of NormalizedCut [25] and self-training [30] to find salient objects in the scene. For surgical scene understanding, similarly, optical flow or shape priors have been used [31,21]. Shape priors lack adaptability and flexibility to new tools or setups and struggle when visually similar tools are present. While optical flow is more generalizable, it introduces noise from anatomical movement or fluid during surgery. The main shortcoming with solutions such as [28,29] is the burdensome process of mask generation followed by rounds of self-training, which would be needed on every new dataset. The advent of foundation models [6,18,13] has opened new opportunities for scene understanding with minimal annotations. In the context of surgical scenes, this impact is still less pronounced, considering the significant domain gap between common computer vision use cases and medical applications, requiring further fine-tuning. Previous research demonstrated that (dynamic) scene graph representations provide a more holistic and interpretable representation of surgical procedures for surgical workflow recognition [11,17]. To mitigate the annotation scarcity in surgical videos and address both"}, {"title": "2 Methodology", "content": "We tackle the problem of semantic scene understanding and scene graph generation with an end-to-end graph-based pipeline. Formulating semantic segmentation as a graph partitioning task, we patchify input images and establish sparse temporal links with the neighboring frame patches via correspondence matching, constructing a dynamic graph (the patch-based graph hereafter). We then perform a spectral, temporal clustering of the patch-based graph to generate a dynamic semantic scene graph. This DSG is augmented with a dynamic relation prediction module to be further refined for the downstream task of surgical phase segmentation. Finally, a prototype matching mechanism is developed for a final refinement, few-shot segmentation, and scene graph generation evaluations."}, {"title": "2.1 Dynamic Scene Graph Generation", "content": "For an input image I, DINO key features f are obtained by partitioning I into n patches and passing them to DINO. The adjacency matrix A is then generated by patchwise dot product as follows:\n$$A_{ij} = \\begin{cases} f_i f_j & \\text{if } f_i \\cdot f_j > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\nNext, we threshold the values in A > \u03c4 and connect highly similar nodes together, generating a static patch graph representation Gt = (Vt, At) for a given frame t. An extension of graph clustering to sequences of frames without considering the temporal inter-dependencies leads to inconsistent clusters among frames with close proximity. A na\u00efve solution could be an expansion of adjacency matrix calculation across the third dimension of time to find spatiotemporal similarities across patches. This leads to high computational costs O(wn\u00b2d) specifically with the increasing length of the temporal window for n number of patches, patch feature of length d, and w time steps. As temporal relations require a coarser level of attention compared to spatial dependencies [7], we suggest a sparse dynamic linking mechanism between patches along the time dimension.\nIn this work, we leverage correspondence matching to find prominent features within frames and match those efficiently. We incorporate LightGlue [15], a distilled deep neural network powered with self- and cross-attention, into our patch graph construction setup. It is designed explicitly for low-latency problems and sparse inputs by predicting matches from two sets of local features. Next, for a clip with w frames, we construct a dynamic patch-based graph, Gti\u2192ti+w = (V,E) with node set V, edge set E, node features X \u2208 Rwxnxd. Spatial edges, Et are established using pairwise correlation similarity between those features (Equation 1), while for temporal edges Et\u2081\u2192ti+1, LightGlue correspondences between frames within a temporal sequence of w time stamps are exploited. Dynamic graph edges can be represented as follows: E = U1<t\u2264w Eti + Etiti+1\nWe further reinforce the graph nodes with temporal and spatial encodings to accentuate the dynamic relations between objects in the scene. Temporal encodings capture the temporal order of object interactions and actions in a video sequence, while spatial encodings capture objects' relative positions and orientations in a scene. For temporal encoding, we incorporate the location of each frame along the temporal window by adding a temporal feature vector to the node feature matrix X. For spatial encoding, we incorporate the position of patches within the frame by adding a spatial feature vector to feature matrix X.\nThe graph clustering is performed by employing deep modularity networks (DMON) [26] featuring a collapse regularization objective to improve unsupervised-"}, {"title": "DSG Optimization", "content": "We establish a probabilistic estimation of edge weights within the DSGs via equation Woo = \u03c3(MLP(Xpoolti\u2192ti+w; \u03b8MLP)), where Wow refers to the edge weights between the clusters of DSG, in which Wij \u2208 [0, 1] indicating the strength of the relations between clusters i, j. OMLP represents the set of trainable parameters. The probabilistic setup allows for flexibility in optimizing the edge weights while learning the downstream tasks and equips the DSG generation to account for the inherent uncertainty and variability present in the unsupervised graph clustering results, leading to more robust and accurate inference."}, {"title": "2.2 End-to-end Pipeline", "content": "To tackle the task of phase segmentation, we propose a multi-layer GCN [12,11] that takes the DSG as input. The GCN consists of multiple layers, each of which enables learning increasingly complex representations of the scene graph. The output of the GCN is fed to a global sum-pooling layer aggregating features from all nodes in the graph. A fully-connected layer and a softmax function predict probabilities for each phase class. A cross-entropy objective function LCE is employed to optimize the model parameters for the surgical phase segmentation task. The final objective function of the end-to-end pipeline can therefore be for-"}, {"title": "Prototype Matching", "content": "To assign semantic classes to DSG nodes, we create prototypes by leveraging DINO patch features and ground truth (GT) segmentation annotations. Exploiting only 5 annotations per class, patch features corresponding to GT segmentation masks are used as prototype patches. We use the mean of the patch features as a prototype for that object class. To predict the semantic category of a node (cluster), a pairwise cosine similarity with prototypes is calculated. Since different clusters might represent the same region, such as surgical tape, we used argmax to assign class labels to clusters."}, {"title": "3 Implementation Details", "content": "Datasets We experiment on 3 datasets: CATARACTS [3,2] consists of 50 cataract surgery videos of 1920 \u00d7 1080 pixels at 30 fps. The dataset is split 25-5-20 for training, validation, and testing, with videos annotated on 19 surgical phases. CaDIS dataset, a subset of CATARACTS, consists of 4670 pixel-wise annotated images. We use Task II of CaDIS, which defines 17 classes of objects, including surgical tools, anatomical structures, and miscellaneous. Cataract101 (C101) comprises 101 videos of 720 \u00d7 540 pixels and 25 fps performed by surgeons with various levels of expertise. The videos are annotated based on the 11 most common phases of cataract surgery and used with 45-5-50 train-validation-test splits.\nTraining details Frames are resized to 224x224 to generate DINO-B embeddings. For graph generation, a frame similarity threshold of 0.9 is chosen. We trained phase segmentation models for 100 epochs using an Adam optimizer with a learning rate of 0.0001 and a batch size of 32 on a single A40 GPU.\nMetrics For phase segmentation, we compute the accuracy and F1 score. For semantic segmentation, we measure the mean intersection over union (mIoU) and pixel-wise accuracy (PAC)."}, {"title": "4 Results & Discussion", "content": "Surgical Workflow Prediction Table 1 presents an ablation study on window size and spatial and temporal embeddings as well as a comparative analysis of our proposed method against existing techniques on phase segmentation tasks for CATARACTS and Cataract101 datasets. We show that increasing window size together with temporal embeddings improves phase segmentation performance, while spatial embeddings have minimal impact. Our method demonstrates superior performance in terms of accuracy and F1 score by indicating 8% accuracy and 10% F1 score improvement over previous graph-based phase segmentation"}, {"title": "5 Conclusion", "content": "We introduce SANGRIA, an end-to-end graph-based solution for concurrent surgical workflow recognition, semantic scene segmentation, and dynamic scene graph generation. Our jointly optimized setup featuring sparse temporal connections and graph clustering, prioritizes the graph generation for the downstream task by disambiguating the graph and highlighting the most influential components and their connections. By focusing on downstream task-specific features, we achieve state-of-the-art results in surgical phase segmentation on the CATARACTS dataset while generating scene explanations with minimal annotation."}]}