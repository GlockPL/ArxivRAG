{"title": "Offline Policy Learning via Skill-step Abstraction for Long-horizon Goal-Conditioned Tasks", "authors": ["Donghoon Kim", "Minjong Yoo", "Honguk Woo"], "abstract": "Goal-conditioned (GC) policy learning often faces a challenge arising from the sparsity of rewards, when confronting long-horizon goals. To address the challenge, we explore skill-based GC policy learning in offline settings, where skills are acquired from existing data and long-horizon goals are decomposed into sequences of near-term goals that align with these skills. Specifically, we present an 'offline GC policy learning via skill-step abstraction' framework (GLVSA) tailored for tackling long-horizon GC tasks affected by goal distribution shifts. In the framework, a GC policy is progressively learned offline in conjunction with the incremental modeling of skill-step abstractions on the data. We also devise a GC policy hierarchy that not only accelerates GC policy learning within the framework but also allows for parameter-efficient fine-tuning of the policy. Through experiments with the maze and Franka kitchen environments, we demonstrate the superiority and efficiency of our GLVSA framework in adapting GC policies to a wide range of long-horizon goals. The framework achieves competitive zero-shot and few-shot adaptation performance, outperforming existing GC policy learning and skill-based methods.", "sections": [{"title": "1 Introduction", "content": "In goal-oriented environments, task outcomes are directly related to the achievement of specific goals. This poses a challenge for goal-conditioned (GC) policies when trained through reinforcement learning (RL) algorithms, particularly for tasks with long-horizon goals [Ma et al., 2022; Lu et al., 2020; Chebotar et al., 2021]. The challenge is primarily attributed to the reward sparsity associated with achieving long-horizon goals. Meanwhile, skill-based RL [Pertsch et al., 2021; Shi et al., 2023; Jiang et al., 2022] has exhibited great potential in addressing long-horizon and sparse reward tasks by leveraging skills. These skills are temporally abstracted patterns of expert behaviors spanning over multiple timesteps and they can be acquired offline from expert datasets through imitation learning methods.\nTo address the challenge associated with long-horizon goal-reaching tasks, we investigate skill-based RL in the context of offline GC policy learning. Incorporating the learned skills into the procedure of learning a long-horizon GC policy looks promising, as a long-horizon goal can be decomposed into near-term subgoals that can be achievable by the skills. However, the nature of imitation-based skill acquisition techniques brings certain issues when they are applied to GC policy learning via RL (GCRL), particularly in scenarios with diverse and varied goals. Imitation-based skills, acquired offline from existing trajectory data, tend to specify a constrained range of achievable goals for GCRL. Thus, this can restrict the adaptation capabilities of GC policies with respect to achievable goals, confining them to the limits of the offline data. This limitation often results in diminished performance in GC tasks, especially those with goal distribution shifts; such shifts occur when there is a discrepancy in the distribution of goals between the dataset used for training and the target goals for evaluation.\nIn this work, we explore skill-based GC policy learning approaches for long-horizon GC tasks, particularly those affected by goal distribution shifts, by presenting a framework for 'offline GC policy learning via skill-step abstraction' (GLVSA). The GLVSA framework harnesses data to obtain skill-based GC policies in offline settings, aiming at enhancing their adaptability and efficiency in the environment with long-horizon GC tasks. At its core, the framework employs an iterative process for joint learning of a skill-step model and a GC policy. The model encapsulates expert behaviors in the embeddings of temporally abstracted skills and represents environment dynamics in a skill-aligned latent space. This concept of skill-step abstraction in GLVSA involves distilling the complexity of long-horizon GC tasks into higher-level, distinct segments (i.e., a set of consecutive action sequences). Each segment or skill-step condenses a series of actions into a more manageable form, facilitating a clearer understanding of the GC task and environment dynamics. In each iteration, the model is employed to generate skill-step rollouts, weaving distinct trajectories from the data to derive new imaginary ones. These derived trajectories are then used to acquire additional plausible skills, which in turn, broaden the range of"}, {"title": "2 Preliminaries", "content": "GCRL addresses the problem of goal-augmented Markov decision processes (GA-MDPS) MG, which are represented by an MDP \\(M = (S, A, P, R, \\gamma)\\) and an additional tuple of a goal space, a goal distribution, and a state-goal mapping function (G, \\(p_g\\), \u03a6) [Liu et al., 2022]. In a GA-MDP MG, S is a state space, A is an action space, P:S\u00d7A\u00d7S \u2192 [0, 1] is a transition probability, and \\(\\gamma\\) is a discount factor. A reward function R: S \u00d7 A \u00d7 G \u2192 R is determined by a given goal and is formed as R : S \u00d7 A \u00d7 G \u2192 R that represents spare rewards; i.e.,\n\\[\nR(s, a, g) = 1(\\Phi(s' \\sim P(\\cdot|s,a)) = g)\n\\]\nwhere \u03a6 : S \u2192 G maps states s to a goal g\u2208 G. This reward structure can be used for RL-based GC policy learning. A GC policy such as \\(\\pi : S \\times G \\times A \\rightarrow [0, 1]\\) is learned by\n\\[\n\\mathbb{E}_{g\\sim P_g} \\begin{bmatrix} \\sum_{t=0}^{T-1} \\gamma^t R(s_t, a \\sim \\pi (.\\mid s_t,g), g) \\end{bmatrix}\n\\]\nIn principle, GCRL facilitates multi-task learning by treating distinct tasks as goals, and it also addresses multi-stage complex tasks by incorporating the composite goal of multiple subtasks into its goal representations.\nSkill-based RL is a hierarchical learning paradigm [Sutton et al., 1999] that leverages the notion of skills, which refers to reusable units of prior knowledge and useful behavior patterns extracted from the data. This hierarchical RL aims to accelerate the process of policy learning through supervising low-level policies on skills, particularly when tackling long-horizon, sparse reward tasks. In [Pertsch et al., 2021], a skill corresponds to an abstracted H-step consecutive action sequence and is represented in a variational auto-encoder (VAE)-based embedding space. Accordingly, skill embeddings z are learned on H-step sub-trajectories \\(\\tau_{sub}\\) such as\n\\[\n\\tau_{t:t+H} = (s_t,..., s_{t+H}, a_t,..., a_{t+H-1}) = (s_{0:H}, a_{0:H-1})\n\\]\nthat are sampled from trajectories \\(\\tau\\) in the data. These embeddings are obtained offline by the skill loss \\(L_{skill}\\) such as\n\\[\n\\mathbb{E}_{\\tau_{sub}\\sim \\tau} \\begin{bmatrix} \\sum_{i=0}^{H-1} (\\pi_{\\theta} (s_i, z) - a_i)^2 + \\beta \\cdot KL(q_{\\phi}(z|\\tau_{sub}) ||P(z)) \\end{bmatrix}\n\\]\nwhere KL denotes the KL divergence and P(z) ~ N(0, I) is a prior distribution of skills. Here, a skill encoder \\(q_\\phi\\) converts action sequences to skill embeddings z and a low-level policy \\(\\pi_{\\theta}\\) serves as a skill decoder, producing an action for a state-skill pair. By this encoding-decoding structure, skills are captured from diverse experiences and establish building blocks that enable an agent to tackle complex tasks effectively.\nUnlike the conventional RL approach that directly selects actions, in skill-based RL, the agent's GC policy \\(\\pi(a|s, g)\\) is composed of two modules, a low-level policy \\(\\pi_{\\theta}(a|s, z)\\) and a skill-based high-level policy \\(\\pi_{\\psi}(z|s, g)\\), i.e.,\n\\[\n\\pi(a|s, g) = \\pi_{\\theta} (a|s, z) \\circ \\pi_{\\psi} (z|s, g).\n\\]\nIn the hierarchy, the behavior of \\(\\pi_{\\psi}\\) adheres to the constraints of a prior \\(p_{\\xi}\\), which regularizes the policy search space, following the entropy maximization RL scheme in [Haarnoja et al., 2018]. Thus, the learning objective of a GC policy in Eq. (2) can be rewritten using the two policy modules \\(\\pi_{\\theta}\\), \\(\\pi_{\\psi}\\) and the prior \\(p_{\\xi}\\) optimized by\n\\[\n\\mathbb{E}_{g\\sim P_g} \\begin{bmatrix} \\sum_{t=0}^{T-1} \\sum_{k=tH}^{(t+1)H-1} [R(s_k \\sim \\pi_{\\theta}(\\cdot|s_k, z_{tH}), g)] \\\\ - KL(\\pi_{\\psi}(z_{tH}|s_{tH}, g)||p_{\\xi}(z_{tH}|s_{tH})) \\end{bmatrix}\n\\]"}, {"title": "3 Our Framework", "content": "To address the challenge of tackling long-horizon goal-reaching tasks, we incorporate the skill-based RL strategy into the offline learning process for GC policies. We achieve this integration in our GLVSA framework, in which a skill-step model encapsulating skills and environment dynamics at the skill abstraction level, and a GC policy based on the model are jointly and iteratively learned during the offline training phase. In the online adaptation phase, the GC policy learned offline can be evaluated in a zero-shot manner without any policy updates or it can be parameter-efficiently fine-tuned through few-shot learning. Adaptation is required for the cases when confronting downstream tasks with significant goal distribution shifts. These two phases of GLVSA; the offline training phase and the adaptation phase. The offline training phase consists of two tasks using the current dataset: joint learning of the skill-step model and GC policy, and model-guided rollouts. During joint learning, modules for the skill-step model and GC policy are updated using trajectories from the dataset. Subsequently, the model-guided rollouts produce trajectories, which are added to the dataset for use in future iterations. For downstream GC tasks, the GC policy can be directly deployed and evaluated in a \"zero-shot\" manner. It can be also fine-tuned via \"few-shot\" updates in a parameter-efficient way to better adapt to a specific task, particularly for the cases subject to goal distribution shifts. Our GC policy hierarchy employs a modular scheme for the policy network. This involves dividing the network into three distinct modules, each serving a specific inference function. The modular scheme allows for parameter-efficient policy adaptation by confining policy updates within one for skill-step goal generation, thereby maintaining the stability of the other modules."}, {"title": "3.1 Skill-step Model Learning", "content": "To broaden the range of skills and goals beyond the scope of merely imitating individual trajectories in the offline dataset, we employ the iteratively refined skill-step model, involving skill acquisition, model refinement, policy updates, and trajectory generation. From the iterations onwards, we use the augmented dataset comprising both the original trajectories and those derived from prior iterations' rollouts. In each iteration, all the modules are jointly optimized, using the losses in Eq. (11). This approach bridges distinct trajectories incrementally in the latent state space, enhancing skill and goal coverage.\nSkill Acquisition\nWe use conditional-\\(\u03b2\\)-VAE networks [Sohn et al., 2015; Higgins et al., 2017] to obtain skill embeddings z, following a similar approach in [Pertsch et al., 2021]. A skill encoder \\(q_\\phi\\) transforms a sub-trajectory \\(\\tau_{sub}\\) in Eq. (3) into an embedding z using Eq. (4), while a skill decoder \\(\\pi_{\\theta}\\) reconstructs the sub-trajectory from z. The skill encoder is also updated later through Eq. (8). Furthermore, for minibatches \\(B = {\\tau_{sub}}_1^b\\) sampled for the dataset D, we obtain a skill prior \\(p_{\\xi}\\) that infers the distribution of skills for a given latent state by optimizing the skill prior loss \\(L_{prior}\\) defined as\n\\[\n\\mathbb{E}_{B\\sim D} [KL(p_{\\xi} (z|h_t) || sg(q_{\\phi}(z|\\tau_{sub})))].\n\\]\nwhere sg is a stop-gradient function and \\(h_t = sg(\\mathbb{E}_{\\theta}(s_t))\\) is a latent state for the first state \\(s_t\\) in sub-trajectory \\(\\tau_{sub}\\). The skill prior \\(p_{\\xi}\\) facilitates rollouts in the latent state space."}, {"title": "3.2 Skill-step GC Policy Hierarchy", "content": "Following skill-based RL approaches, we structure our GC policy as Eq. (5) in a hierarchy with the (high-level) skill policy \\(\\pi_{\\psi}\\) and the (low-level) skill decoder \\(\\pi_{\\theta}\\). To accelerate policy learning and adaptation, we also devise a modular GC policy structure that encompasses a skill-step goal generator \\(f_{\\psi}\\) and an inverse skill-step dynamics module \\(P_1^{-1}\\). Then, we decompose the skill policy into\n\\[\n\\pi(s_t, g) = P_1^{-1}(z|h_t, h_{t+H}) \\circ f_{\\psi}(h_{t+H}|h_t, g) \\circ \\mathbb{E}_{\\theta}(h_t|s_t).\n\\]\nBy this decomposition, for a pair of current state \\(s_t\\) and long-horizon goal g, the GC policy first infers a skill-step goal \\(h_{t+H}\\) and then obtains its corresponding skill z via the_inverse skill-step dynamics. Note that the skill decoder \\(\\pi_{\\theta}\\) is learned via reconstruction in Eq. (4) and it is responsible for translating z into an action sequence.\nIn this hierarchy, the inverse skill-step dynamics \\(P_1^{-1}\\) is trained as part of the skill-step model in Eq. (8). The adaptation capabilities of the inverse skill-step dynamics rest on such conditions where the environment dynamics remain consistent between the training datasets and downstream tasks; i.e., they share the same underlying world model. In this respect, for downstream tasks with goal distribution shifts in the same environment, we leverage this hierarchy in a way that only the skill-step goal generator needs to be updated. This facilitates parameter-efficient policy updates in the adaptation phase. In this sense, we optimize the skill-step goal generator \\(f_{\\psi}\\) by the skill-step goal loss \\(L_{sg}\\) such as\n\\[\n\\mathbb{E}_{B\\sim D} \\begin{bmatrix} (h_{t+H} - f_{\\psi}(h_t, g))^2 + (f_{\\psi}(h_t, g) - P_{\\phi} (h_t, z))^2 \\end{bmatrix}\n\\]\nwhere \\(h_{t+H} = \\mathbb{E}_{\\bar{\\theta}}(s_{t+H})\\) and \\(z \\sim P_{\\theta}^{-1}(\\cdot|h_t, f_{\\psi}(h_t, g))\\). In Eq. (10), the first term represents behavior cloning error, and the second term serves as a sanity check for cyclic consistency, ensuring that the generated skill-step goal is consistent with the outcome produced by the execution of the skill, which is determined by the skill-step goal."}, {"title": "3.3 Offline Training", "content": "In each iteration during the training phase, all the learnable modules (9 modules are jointly optimized by the losses based on skills, a skill prior, environment dynamics, and skill-step goals, incorporating Eq. (4)-(10).\n\\[\nL = L_{skill} + L_{prior} + L_{model} + L_{sg}\n\\]\nSee Algorithm 1 in Appendix listing the iterative learning procedure for the offline training phase."}, {"title": "3.4 Online Adaptation", "content": "After the offline training phase, we can immediately construct the GC policy (structured as Eq. (5) and (9)) using the learned modules and evaluate it in a zero-shot manner. For downstream tasks with different goal distributions, we can also adapt the policy efficiently through few-shot online updates. In this case, we tune only the skill-step goal generator \\(f_{\\psi}\\) via RL, while freezing the other modules. The skill-step goal generator \\(f_{\\psi}\\) is updated through value prediction-based reward maximization, in addition to prior regularization and state consistency regularization, i.e.,\n\\[\n\\mathbb{E}_{B'} \\begin{bmatrix} -Q(h_t, \\pi_{\\psi} (h_t, g)) + \\alpha \\cdot KL(\\pi_{\\psi}(z|h_t, g)||p_{\\theta}(z|h_t)) \\\\ + (P_{\\phi}(h_{t+H}|h_t, \\pi_{\\psi} (h_t, f_{\\psi}(h_{t+H}|h_t, g)) - f_{\\psi}(h_{t+H}|h_t, g) \\end{bmatrix}\n\\]\nwhere \\(h_t = sg(\\mathbb{E}_{\\theta}(s_t))\\) and B' has skill-step transitions \\((s_t, z, s_{t+H})\\) which are collected online from the environment. The state consistency regularization ensures the skill-step reachability of the skill-step goal \\(f_{\\psi}(h_{t+H}|h_t, g)\\). This regularization is similar to the second term of Eq. (10). For this policy adaptation via RL, the modules parameterized by \\(\\theta\\) and \\(\\phi\\) except for \\(f_{\\psi}\\) remain frozen."}, {"title": "4 Experiments", "content": "The experiment involves two environments from D4RL [Fu et al., 2020]: maze navigation (maze) and Franka kitchen simulation (kitchen). In the maze, the agent's objective is to reach a target location from starting location, with states including the agent's coordinates, velocity, and the goal which is defined by pairs of starting and target locations, and rewards are given only upon reaching this target location. In the Franka kitchen environment, a robot arm manipulates kitchen objects, receiving rewards for achieving the desired state of each target object such as Microwave, Kettle, Bottom burner, Top burner, Light switch, Slide cabinet, and Hinge cabinet. The robot's state includes its joint and gripper positions, as well as the status of kitchen objects, with goals represented by the states of four target objects. Both environments use offline datasets for training; i.e., 3,046 trajectories in the maze and 603 in the kitchen from [Pertsch et al., 2022] and [Fu et al., 2020], respectively. In the following experiments, the GC policies' adaptation capabilities are tested under different goal conditions (None, Small, Medium, and Large), which represent the degree of difference between the goal distribution in the offline dataset and that for the adaptation phase.\nFor evaluation metrics, we adopt a normalized score, ranging from 0 to 100, which is calculated by\n\\[\n\\mathbb{I}\\[(f(\\Phi(s) - g)^2 \\leq \\epsilon] \\times 100 \\quad for \\quad maze,\\\\\n(achieved\\_obj/target\\_obj) \\times 100 \\quad for \\quad kitchen\n\\]\nwhere \\(\\Phi\\) maps states s to a specific goal g, and achieved_obj and target_obj represent the number of target objects successfully manipulated and the number of target objects in the goal, respectively. The detailed settings for goal specifications and goal distribution shifts are in Appendix A.\nFor comparison, we use several baselines, encompassing state-of-the-art GC policy learning and skill-based RL methods. Our GC problem settings are characterized by two main constraints: (1) offline GC policy learning, wherein data collection by the policy is prohibited during the training phase, and (2) online GC policy adaptation, which permits either zero-shot evaluation or few-shot updates. The policy adaptation aims at tackling a range of tasks with different degrees of goal distribution shifts. Given the aforementioned constraints, we adapt several baselines, which typically rely on online environment interactions and data collection, to function with offline pre-trained GC policies.\nGCSL [Ghosh et al., 2021] is used as a baseline for GCRL experiments to evaluate offline GC policy learning without skill-based strategies. It iteratively collects data through online environment interactions and performs hindsight relabeling and imitation learning to optimize the GC policy. In the experiments, online data collection is replaced with an expert dataset for offline settings, similar to [Yang et al., 2021; Ma et al., 2022]. WGCSL [Yang et al., 2021] is a GCSL variant, employing the advantage-based behavior cloning weights. This method is used to evaluate the state-of-the-art performance of offline GCSL. SPiRL+GCSL is a GC variant of the skill-based RL method, SPiRL [Pertsch et al., 2021] to evaluate the GC performance of skill-based RL in offline settings. In our experiments, we train a GC policy using the skills in a supervised manner. We also incorporate the GCSL's objective into the skill-based RL objective structure. SkiMo+GCSL is a GC variant of the skill-based model-based RL, SkiMo [Shi et al., 2023] which jointly optimizes a model and skills. We adapt SkiMo with GC policy training, and combine the objectives of GCSL and skill-based RL, in the same way of SPiRL+GCSL. This method relies solely on the offline data for model and skill learning, unlike our approach which employs iterative learning through the model-guided skill rollouts. It shows the state-of-the-art GC performance of skill-based RL in offline settings."}, {"title": "4.2 Zero-shot Evaluation Performance", "content": "In zero-shot scenarios, we evaluate the GC policy of each method without any policy updates. Table 2 reveals that our GLVSA achieves superior zero-shot performance in both maze and kitchen environments, where the \"Dist. shift\" column specifies different degrees of goal distribution shifts between the offline dataset and the evaluation tasks. Compared with the most competitive baseline SkiMo+GCSL, GLVSA demonstrates a gain of 14.6, 20.2, and 34.2 in normalized scores for Small, Medium, and Large distribution shifts, respectively, in the maze environment, as well as a gain of 33.8, 39.1, 30.5, respectively, in the kitchen. As the degree of distribution shifts increases, we observe a greater performance drop, particularly for the action-step baselines (GCSL, WGCSL) which do not incorporate skill-based strategies. In contrast, GLVSA displays robustness against these distribution shifts, demonstrating the ability of our GC policy to generalize across a wider range of goals. Importantly, our approach employs the skill-step model to guide trajectory generation, using iterative joint learning of skills, goals, and the model. This renders the skills and goals obtained beyond the scope of the initial offline dataset, marking a distinction from the other skill-based baselines such as SPiRL+GCSL and SkiMo+GCSL, which more rely on the dataset. This growth indicates that a variety of goals are being learned through trajectory generation, forming the basis for strong zero-shot performance."}, {"title": "4.3 Few-shot Adaptation Performance", "content": "Table 3 presents the adaptation performance in normalized scores, obtained through few-shot policy updates, with each shot representing an episode. In these few-shot adaptation scenarios, we focus on the cases with significant distribution shifts (i.e., \"Dist. shift = Large\"). Such shifts tend to diminish the zero-shot performance of GC policies, as observed in Table 2 previously. As shown, GLVSA consistently outperforms the baselines across the shots. For smaller shot counts (1, 10), GLvSA outperforms the most competitive baseline (SkiMo+GCSL) by margins of 50.9 and 58.3 in the maze, and 49.4 and 58.1 in the kitchen, respectively. While the baseline's 1-shot and 10-shots performance degrades, compared to its zero-shot performance, GLVSA exhibits comparable performance by 1-shot and an improvement by 10-shots, compared to its respective zero-shot; e.g., in the kitchen environment with Large shift, our performance increases from a zero-shot score of 72.2 to 78.4 at 10-shots. As the shot counts increase to 25 and 50, the performance gaps between our GLVSA and the baseline narrow to 32.9 and 15.3 in the maze, and 48.7 and 38.3 in the kitchen, respectively. Nevertheless, these gaps remain substantial. Such results highlight the efficiency of our GC policy hierarchy, which facilitates parameter-efficient updates specifically targeting the skill-step goal generation, thereby enabling few-shot policy adaptation."}, {"title": "4.4 Ablations", "content": "We conduct ablation studies in the kitchen environment.\nSkill-step model structure. Table 4 contrasts the zero-shot performance by our model structure employing the skill-step dynamics \\(P_{\\phi}\\) for model-guided rollouts against a conventional model structure that solely relies on the flat dynamics \\(P_{\\theta}\\). We also include the \u201cNo rollout\" case. This comparison sheds light on the advantages of using the skill-step dynamics. Our approach consistently yields gains over its counterpart. For the cases of larger distribution shifts, we observe a significant performance drop of the compounding error, a known drawback of model-based RL approaches [Janner et al., 2019; Lu et al., 2020]. The integration of \\(P_{\\phi}\\) into the model regulates the latent state space using the skills, enabling more dependable rollouts. This, in turn, facilitates enhancements in skill acquisition, model refinement, and policy optimization, even upon substantial goal distribution shifts. We observe that performs worse than the No rollout case, which is also associated with the compounding error effect. The impact of the model errors can be more pronounced than the benefits of enhanced diversity from trajectory generation.\nSkill horizon. Table 5 specifies the impact of skill horizon H (defined in Eq. (3)) on the zero-shot performance, where H sets to 1, 5, 10, and 40 in timesteps. We observe that the performance peaks at H = 10. As the skill horizon H decreases, the temporal abstraction capability of skills diminishes, leading to a performance drop. This is because the range regularized by the skill-step dynamics \\(P_{\\phi}\\) in the latent state space contracts, which in turn impacts the state and skill"}, {"title": "Skill-step goal generation.", "content": "To assess the effectiveness of the policy hierarchy incorporating the skill-step goal generation, we compare the adaptation performance of the proposed policy hierarchy in Section 3.2 to its counterpart implemented without the skill-step goal generation (w/o \\(f_{\\psi}\\)). In addition, we introduce another variant, denoted as (w/ 2-skill-steps), which specifically employs a goal state two skill-steps away. This contrasts with our approach that not only uses a goal state a single skill-step away (a skill-step goal) but also harnesses the alignment of skills and goals in terms of temporal abstraction. Table 6 shows that our policy hierarchy outperforms both the counterpart and the variant. Our policy hierarchy effectively mitigates the exploration inefficiency that emerges due to multiple skill combinations to achieve a long-horizon goal, exploiting the skill-step goal generation.\nSanity check by cyclic consistency. Table 7 shows the effect of the sanity check in Eq. (10) which ensures the cyclic consistency between the deduced skill-step goals and the outcomes of skill executions. We compare an implementation using only the behavior cloning term without the sanity check (denoted as BC) to our approach (w/ sanity check). We also implement another method (denoted as BC+SR) in which the high-level policy \\(\\pi_{\\psi}\\) is regularized by skill embeddings z through KL-Divergence with the inverse skill-step dynamics, similar to SAC [Haarnoja et al., 2018] and SPiRL [Pertsch et al., 2021]. Our approach outperforms both BC and BC+SR across all degrees of distribution shifts. The sanity check regularizes \\(\\pi_{\\psi}\\) robustly, assisting the skill-step goal generator to predict achievable near-term goals by the skills."}, {"title": "5 Related Work", "content": "GCRL represents one of the RL approaches for solving diverse tasks using reward-relevant goal representations [Liu et al., 2022; Wang et al., 2023]. Several GCRL methodologies, particularly for addressing long-horizon goals that inherently incur the reward sparsity, have been introduced, including graph-based planning [Kim et al., 2023; Pitis et al., 2022], incremental goal generation [Cho et al., 2023], and subgoal generation [Fang et al., 2022]. Our skill-step model approach also tackles the long-horizon goal problem, exploring the skill-based RL strategy in GCRL contexts.\nSkill-based RL exploits the skill-level abstraction of expert action sequences to facilitate the online process of policy learning, leveraging task-agnostic trajectory data in offline skill learning [Pertsch et al., 2021; Nair et al., 2020; Hussonnois et al., 2023; Jiang et al., 2022]. This skill-based strategy has been applied in meta-learning [Nam et al., 2022], model-based learning [Shi et al., 2023], and cross-domain settings [Pertsch et al., 2023]. Yet, it has not been fully explored to adopt the skill-based strategy in GCRL. Particularly, upon goal distribution shifts of target tasks, both the learned skills and policies using the skills are required to be retrained. We employ the model-guided goal exploration at the skill-level, hence enhancing the generalization of a learned GC policy against goal distribution shifts.\nIn model-based RL and planning [Hafner et al., 2019; Shi et al., 2023; Egorov and Shpilman, 2022; Hafner et al., 2020], several methods have been introduced for facilitating imaginary exploration [Wang et al., 2021; Mendonca et al., 2021; Hu et al., 2023]. In GCRL contexts, the model predictive control employs subgoal generation [Hansen et al., 2022], and the skill embedding is integrated into long-horizon environments [Shi et al., 2023]. Unlike these methods directly exploiting the model for goal learning either at the action-level or the skill-level, we devise effective model-based goal learning that uses the cyclic consistency of skills and near-term goals, thus broadening the goal range and enabling to generalize the GC policy against goal distribution shifts."}, {"title": "6 Conclusion", "content": "In this work, we presented a novel offline GC policy learning framework designed to address long-horizon GC tasks subject to goal distribution shifts. By employing the skill-step model-guided rollouts, our framework extends the range of achievable goals and enhances the adaptation capabilities of the GC policy within offline scenarios. We also proposed a modular policy hierarchy that features the ability to generate near-term goals, attainable by the skills. This hierarchy bolsters both the robustness of the GC policy and its adaptation efficiency against goal distribution shifts. In our future work, we aim to extend the skill-step model for more complex circumstances where distribution shifts in both environment dynamics and goals occur concurrently. Our focus will be on generalizing the skills to better capture the intricacies of environment dynamics patterns during the training phase."}]}