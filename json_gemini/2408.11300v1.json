{"title": "Offline Policy Learning via Skill-step Abstraction for Long-horizon\nGoal-Conditioned Tasks", "authors": ["Donghoon Kim", "Minjong Yoo", "Honguk Woo"], "abstract": "Goal-conditioned (GC) policy learning often faces\na challenge arising from the sparsity of rewards,\nwhen confronting long-horizon goals. To address\nthe challenge, we explore skill-based GC policy\nlearning in offline settings, where skills are ac-\nquired from existing data and long-horizon goals\nare decomposed into sequences of near-term goals\nthat align with these skills. Specifically, we present\nan 'offline GC policy learning via skill-step ab-\nstraction' framework (GLVSA) tailored for tack-\nling long-horizon GC tasks affected by goal distri-\nbution shifts. In the framework, a GC policy is pro-\ngressively learned offline in conjunction with the\nincremental modeling of skill-step abstractions on\nthe data. We also devise a GC policy hierarchy that\nnot only accelerates GC policy learning within the\nframework but also allows for parameter-efficient\nfme-tuning of the policy. Through experiments with\nthe maze and Franka kitchen environments, we\ndemonstrate the superiority and efficiency of our\nGLVSA framework in adapting GC policies to a\nwide range of long-horizon goals. The framework\nachieves competitive zero-shot and few-shot adap-\ntation performance, outperforming existing GC\npolicy learning and skill-based methods.", "sections": [{"title": "1 Introduction", "content": "In goal-oriented environments, task outcomes are directly\nrelated to the achievement of specific goals. This poses a\nchallenge for goal-conditioned (GC) policies when trained\nthrough reinforcement learning (RL) algorithms, particularly\nfor tasks with long-horizon goals [Ma et al., 2022; Lu et\nal., 2020; Chebotar et al., 2021]. The challenge is primar-\nily attributed to the reward sparsity associated with achiev-\ning long-horizon goals. Meanwhile, skill-based RL [Pertsch\net al., 2021; Shi et al., 2023; Jiang et al., 2022] has exhib-\nited great potential in addressing long-horizon and sparse re-\nward tasks by leveraging skills. These skills are temporally\nabstracted patterns of expert behaviors spanning over multi-\nple timesteps and they can be acquired offline from expert\ndatasets through imitation learning methods.\nTo address the challenge associated with long-horizon\ngoal-reaching tasks, we investigate skill-based RL in the con-\ntext of offline GC policy learning. Incorporating the learned\nskills into the procedure of learning a long-horizon GC pol-\nicy looks promising, as a long-horizon goal can be decom-\nposed into near-term subgoals that can be achievable by the\nskills. However, the nature of imitation-based skill acquisi-\ntion techniques brings certain issues when they are applied\nto GC policy learning via RL (GCRL), particularly in sce-\nnarios with diverse and varied goals. Imitation-based skills,\nacquired offline from existing trajectory data, tend to specify\na constrained range of achievable goals for GCRL. Thus, this\ncan restrict the adaptation capabilities of GC policies with re-\nspect to achievable goals, confining them to the limits of the\noffline data. This limitation often results in diminished per-\nformance in GC tasks, especially those with goal distribution\nshifts; such shifts occur when there is a discrepancy in the\ndistribution of goals between the dataset used for training and\nthe target goals for evaluation.\nIn this work, we explore skill-based GC policy learning\napproaches for long-horizon GC tasks, particularly those af-\nfected by goal distribution shifts, by presenting a framework\nfor 'offline GC policy learning via skill-step abstraction'\n(GLVSA). The GLVSA framework harnesses data to obtain\nskill-based GC policies in offline settings, aiming at enhanc-\ning their adaptability and efficiency in the environment with\nlong-horizon GC tasks. At its core, the framework employs\nan iterative process for joint learning of a skill-step model and\na GC policy. The model encapsulates expert behaviors in the\nembeddings of temporally abstracted skills and represents en-\nvironment dynamics in a skill-aligned latent space. This con-\ncept of skill-step abstraction in GLVSA involves distilling the\ncomplexity of long-horizon GC tasks into higher-level, dis-\ntinct segments (i.e., a set of consecutive action sequences).\nEach segment or skill-step condenses a series of actions into\na more manageable form, facilitating a clearer understanding\nof the GC task and environment dynamics. In each iteration,\nthe model is employed to generate skill-step rollouts, weav-\ning distinct trajectories from the data to derive new imaginary\nones. These derived trajectories are then used to acquire ad-\nditional plausible skills, which in turn, broaden the range of"}, {"title": "2 Preliminaries", "content": "GCRL addresses the problem of goal-augmented Markov de-\ncision processes (GA-MDPS) MG, which are represented by\nan MDP M = (S, A, P, R, \u03b3) and an additional tuple of a\ngoal space, a goal distribution, and a state-goal mapping func-\ntion (G, pg, \u03a6) [Liu et al., 2022]. In a GA-MDP MG, S is a\nstate space, A is an action space, P:S\u00d7A\u00d7S \u2192 [0, 1] is\na transition probability, and y is a discount factor. A reward\nfunction R: S \u00d7 A \u00d7 G \u2192 R is determined by a given goal\nand is formed as R : S \u00d7 A \u00d7 G \u2192 R that represents spare\nrewards; i.e.,\n$R(s, a, g) = 1(\u03a6(s' ~ P(\u00b7|s,a)) = g)$\nwhere : S \u2192 G maps states s to a goal g\u2208 G. This reward\nstructure can be used for RL-based GC policy learning. A GC"}, {"title": "3 Our Framework", "content": "To address the challenge of tackling long-horizon goal-\nreaching tasks, we incorporate the skill-based RL strategy\ninto the offline learning process for GC policies. We achieve\nthis integration in our GLVSA framework, in which a skill-\nstep model encapsulating skills and environment dynamics\nat the skill abstraction level, and a GC policy based on the\nmodel are jointly and iteratively learned during the offline\ntraining phase. In the online adaptation phase, the GC pol-\nicy learned offline can be evaluated in a zero-shot manner\nwithout any policy updates or it can be parameter-efficiently\nfme-tuned through few-shot learning. Adaptation is required\nfor the cases when confronting downstream tasks with sig-\nnificant goal distribution shifts. These two\nphases of GLVSA; the offline training phase and the adap-\ntation phase. The offline training phase consists of two tasks\nusing the current dataset: joint learning of the skill-step model\nand GC policy, and model-guided rollouts. During joint learn-\ning, modules for the skill-step model and GC policy are up-\ndated using trajectories from the dataset. Subsequently, the\nmodel-guided rollouts produce trajectories, which are added\nto the dataset for use in future iterations. For downstream GC\ntasks, the GC policy can be directly deployed and evaluated in\na \"zero-shot\" manner. It can be also fine-tuned via \"few-shot\"\nupdates in a parameter-efficient way to better adapt to a spe-\ncific task, particularly for the cases subject to goal distribution\nshifts. Our GC policy hierarchy employs a modular scheme\nfor the policy network. This involves dividing the network\ninto three distinct modules, each serving a specific inference\nfunction. The modular scheme allows for parameter-efficient\npolicy adaptation by confining policy updates within one for\nskill-step goal generation, thereby maintaining the stability of\nthe other modules.\n They in-\nclude the skill-based GC policy with low-level policy (skill\ndecoder), high-level policy (skill policy), and inverse skill-\nstep dynamics modules, as well as the skill-step model with\ndual dynamics, skill encoder, and skill prior modules. These\nalso include a state encoder-decoder, by which the latent state\nspace can align with the skills. Each module subscript denotes\nthe role in policy adaptation. \u03c8, \u03b8, and 4 indicate fine-tuned,\nfrozen, and unused modules, respectively, in policy adapta-\ntion; e.g., q and D are used only for the offline training."}, {"title": "3.1 Skill-step Model Learning", "content": "To broaden the range of skills and goals beyond the scope of\nmerely imitating individual trajectories in the offline dataset,\nwe employ the iteratively refined skill-step model, involving\nskill acquisition, model refinement, policy updates, and tra-\njectory generation. From the iterations onwards, we use the\naugmented dataset comprising both the original trajectories\nand those derived from prior iterations' rollouts. In each iter-\nation, all the modules in Table 1 are jointly optimized, using\nthe losses in In This approach\nbridges distinct trajectories incrementally in the latent state\nspace, enhancing skill and goal coverage.\nSkill Acquisition\nWe use conditional-B-VAE networks [Sohn et al., 2015;\nHiggins et al., 2017] to obtain skill embeddings z, following\na similar approach in A skill encoder\nq transforms a sub-trajectory sub in Eq. (3) into an embed-\nding z using Eq. (4), while a skill decoder reconstructs the\nsub-trajectory from z. The skill encoder is also updated later\nthrough Eq. (8). Furthermore, for minibatches B = {sub}\nsampled for the dataset D, we obtain a skill prior pe that infers\nthe distribution of skills for a given latent state by optimizing\nthe skill prior loss Lprior defined as\n$E_{B~D} [KL(po (z|ht) || sg(q\u00a2(z|tsub)))]$\nwhere sg is a stop-gradient function and h\u2081 = sg(Eo(st)) is\na latent state for the first state st in sub-trajectory sub. The\nskill prior pe facilitates rollouts in the latent state space."}, {"title": "3.2 Skill-step GC Policy Hierarchy", "content": "Following skill-based RL approaches, we structure our GC\npolicy as Eq. (5) in a hierarchy with the (high-level) skill pol-\nicy and the (low-level) skill decoder \u03c0\u03b9. \u03a4\u03bf accelerate\npolicy learning and adaptation, we also devise a modular GC\npolicy structure that encompasses a skill-step goal generator\nfy and an inverse skill-step dynamics module P\u012b\u00b9. Then, we\ndecompose the skill policy into\n$\u03c0(St, 9) = Po^{-1}(z|ht, ht+H) 0 f\u2084(ht+H|ht, 9) 0 Eo(ht|st).$\nBy this decomposition, for a pair of current state st and long-\nhorizon goal g, the GC policy first infers a skill-step goal\nht+H and then obtains its corresponding skill z via the_in-\nverse skill-step dynamics. Note that the skill decoder \u03c0\u03cc is\nlearned via reconstruction in Eq. (4) and it is responsible for\ntranslating z into an action sequence.\nIn this hierarchy, the inverse skill-step dynamics P\u012b is\ntrained as part of the skill-step model in Eq. (8). The adap-\ntation capabilities of the inverse skill-step dynamics rest on\nsuch conditions where the environment dynamics remain\nconsistent between the training datasets and downstream\ntasks; i.e., they share the same underlying world model. In\nthis respect, for downstream tasks with goal distribution shifts\nin the same environment, we leverage this hierarchy in a way\nthat only the skill-step goal generator needs to be updated.\nThis facilitates parameter-efficient policy updates in the adap-\ntation phase. In this sense, we optimize the skill-step goal\ngenerator fy by the skill-step goal loss Lsg such as\n$E_{B~D} [2 behavior cloning"}, {"title": "3.3 Offline Training", "content": "In each iteration during the training phase, all the learnable\nmodules (9 modules in Table 1) in GLVSA are jointly opti-\nmized by the losses based on skills, a skill prior, environment\ndynamics, and skill-step goals, incorporating Eq. (4)-(10).\n$L = Lskill + Lprior + Lmodel + Lsg$\nSee Algorithm 1 in Appendix listing the iterative learning\nprocedure for the offline training phase."}, {"title": "3.4 Online Adaptation", "content": "After the offline training phase, we can immediately con-\nstruct the GC policy (structured as Eq. (5) and (9)) using the\nlearned modules and evaluate it in a zero-shot manner. For\ndownstream tasks with different goal distributions, we can\nalso adapt the policy efficiently through few-shot online up-\ndates. In this case, we tune only the skill-step goal generator\nfy via RL, while freezing the other modules. The skill-step\ngoal generator fy is updated through value prediction-based\nreward maximization, in addition to prior regularization and"}, {"title": "4 Experiments", "content": "The experiment involves two environments from D4RL [Fu et\nal., 2020]: maze navigation (maze) and Franka kitchen simu-\nlation (kitchen). In the maze, the agent's objective is to reach\na target location from starting location, with states including\nthe agent's coordinates, velocity, and the goal which is de-\nfined by pairs of starting and target locations, and rewards are\ngiven only upon reaching this target location. In the Franka\nkitchen environment, a robot arm manipulates kitchen ob-\njects, receiving rewards for achieving the desired state of each\ntarget object such as Microwave, Kettle, Bottom burner, Top\nburner, Light switch, Slide cabinet, and Hinge cabinet. The\nrobot's state includes its joint and gripper positions, as well\nas the status of kitchen objects, with goals represented by\nthe states of four target objects. Both environments use of-\nfline datasets for training; i.e., 3,046 trajectories in the maze\nand 603 in the kitchen from [Pertsch et al., 2022] and [Fu\net al., 2020], respectively. In the following experiments, the\nGC policies' adaptation capabilities are tested under different\ngoal conditions (None, Small, Medium, and Large), which\nrepresent the degree of difference between the goal distribu-\ntion in the offline dataset and that for the adaptation phase.\nFor evaluation metrics, we adopt a normalized score, rang-\ning from 0 to 100, which is calculated by\n$SI I (achieved_obj/target_obj) \u00d7 100  for kitchen$\nwhere I maps states s to a specific goal g, and achieved_obj\nand target_obj represent the number of target objects success-\nfully manipulated and the number of target objects in the goal,\nrespectively. The detailed settings for goal specifications and\ngoal distribution shifts are in Appendix A.\nFor comparison, we use several baselines, encompassing\nstate-of-the-art GC policy learning and skill-based RL meth-\nods. Our GC problem settings are characterized by two main\nconstraints: (1) offline GC policy learning, wherein data col-lection by the policy is prohibited during the training phase,\nand (2) online GC policy adaptation, which permits either\nzero-shot evaluation or few-shot updates. The policy adapta-\ntion aims at tackling a range of tasks with different degrees of"}, {"title": "4.2 Zero-shot Evaluation Performance", "content": "In zero-shot scenarios, we evaluate the GC policy of each\nmethod without any policy updates. Table 2 reveals that our\nGLVSA achieves superior zero-shot performance in both\nmaze and kitchen environments, where the \"Dist. shift\" col-\numn specifies different degrees of goal distribution shifts be-\ntween the offline dataset and the evaluation tasks. Compared\nwith the most competitive baseline SkiMo+GCSL, GLVSA\ndemonstrates a gain of 14.6, 20.2, and 34.2 in normalized\nscores for Small, Medium, and Large distribution shifts, re-\nspectively, in the maze environment, as well as a gain of\n33.8, 39.1, 30.5, respectively, in the kitchen. As the degree\nof distribution shifts increases, we observe a greater perfor-\nmance drop, particularly for the action-step baselines (GCSL,\nWGCSL) which do not incorporate skill-based strategies. In\ncontrast, GLVSA displays robustness against these distribu-\ntion shifts, demonstrating the ability of our GC policy to gen-\neralize across a wider range of goals. Importantly, our ap-\nproach employs the skill-step model to guide trajectory gen-\neration, using iterative joint learning of skills, goals, and the\nmodel. This renders the skills and goals obtained beyond the\nscope of the initial offline dataset, marking a distinction from\nthe other skill-based baselines such as SPiRL+GCSL and\nSkiMo+GCSL, which more rely on the dataset. Figure 4 il-\nlustrates the expanding goal coverage in the latent state space\nof the kitchen environment, achieved by GLVSA over itera-tions. This growth indicates that a variety of goals are being\nlearned through trajectory generation, forming the basis for\nstrong zero-shot performance."}, {"title": "4.3 Few-shot Adaptation Performance", "content": "presents the adaptation performance in normalized\nscores, obtained through few-shot policy updates, with each\nshot representing an episode (e.g., in the \"Shot\" column,\n\"Shot = 1\" specifies that the policy is updated via the envi-ronment interaction of a single episode). In these few-shotadaptation scenarios, we focus on the cases with significant\ndistribution shifts (i.e., \"Dist. shift = Large\"). Such shifts\ntend to diminish the zero-shot performance of GC policies,\nas observed previously. As shown, GLVSA consis-tently outperforms the baselines across the shots. For smaller\nshot counts (1, 10), GLvSA outperforms the most compet-\nitive baseline (SkiMo+GCSL) by margins of 50.9 and 58.3\nin the maze, and 49.4 and 58.1 in the kitchen, respectively.\nWhile the baseline's 1-shot and 10-shots performance de-grades, compared to its zero-shot performance, GLVSA ex-hibits comparable performance by 1-shot and an improve-ment by 10-shots, compared to its respective zero-shot; e.g.,in the kitchen environment with Large shift, our performanceincreases from a zero-shot score of 72.2 to 78.4 at 10-shots.\nAs the shot counts increase to 25 and 50, the performance\ngaps between our GLVSA and the baseline narrow to 32.9\nand 15.3 in the maze, and 48.7 and 38.3 in the kitchen, re-"}, {"title": "4.4 Ablations", "content": "We conduct ablation studies in the kitchen environment.\nSkill-step model structure. Table 4 contrasts the zero-shot performance by our model structure employing the skill-step dynamics Pe for model-guided rollouts against a conventional model structure that solely relies on the flat dynamics P. We also include the \u201cNo rollout\u201d case. This compar-ison sheds light on the advantages of using the skill-step dynamics. Our approach (w/ Po) consistently yields gains over its counterpart (w/o Po). For the cases of larger distribution shifts, we observe a significant performance drop of (w/o Po), which is attributed to the compounding error, a known draw-back of model-based RL approaches Janner et al., 2019;\nLu et al., 2020]. The integration of Pe into the model reg-ulates the latent state space using the skills, enabling more dependable rollouts. This, in turn, facilitates enhancements in skill acquisition, model refinement, and policy optimization, even upon substantial goal distribution shifts. We observe that (w/o Po) performs worse than the No rollout case, which is also associated with the compounding error effect. The impact of the model errors can be more pronounced than the benefits of enhanced diversity from trajectory generation."}, {"title": "5 Related Work", "content": "GCRL represents one of the RL approaches for solving di-verse tasks using reward-relevant goal representations Liu et al., 2022; Wang et al., 2023]. Several GCRL methodolo-gies, particularly for addressing long-horizon goals that in-herently incur the reward sparsity, have been introduced, in-cluding graph-based planning Kim et al., 2023; Pitis et al.,2022], incremental goal generation [Cho et al., 2023], andsubgoal generation Fang et al., 2022]. Our skill-step modelapproach also tackles the long-horizon goal problem, explor-ing the skill-based RL strategy in GCRL contexts.\nSkill-based RL exploits the skill-level abstraction of ex-pert action sequences to facilitate the online process of pol-icy learning, leveraging task-agnostic trajectory data in of-fline skill learning Pertsch et al., 2021; Nair et al., 2020;\nHussonnois et al., 2023; Jiang et al., 2022]. This skill-basedstrategy has been applied in meta-learning [Nam et al., 2022],model-based learning [Shi et al., 2023], and cross-domainsettings Pertsch et al., 2023]. Yet, it has not been fully explored to adopt the skill-based strategy in GCRL. Particularly,upon goal distribution shifts of target tasks, both the learnedskills and policies using the skills are required to be retrained.\nWe employ the model-guided goal exploration at the skill-level, hence enhancing the generalization of a learned GCpolicy against goal distribution shifts.\nIn model-based RL and planning Hafner et al., 2019; Shi et al., 2023; Egorov and Shpilman, 2022; Hafner et al., 2020],several methods have been introduced for facilitating imaginary exploration [Wang et al., 2021; Mendonca et al., 2021;\nHu et al., 2023]. In GCRL contexts, the model predictive control employs subgoal generation [Hansen et al., 2022], andthe skill embedding is integrated into long-horizon environments Shi et al., 2023]. Unlike these methods directly exploiting the model for goal learning either at the action-levelor the skill-level, we devise effective model-based goal learning that uses the cyclic consistency of skills and near-termgoals, thus broadening the goal range and enabling to gener-alize the GC policy against goal distribution shifts."}, {"title": "6 Conclusion", "content": "In this work, we presented a novel offline GC policy learningframework designed to address long-horizon GC tasks sub-ject to goal distribution shifts. By employing the skill-stepmodel-guided rollouts, our framework extends the range ofachievable goals and enhances the adaptation capabilities ofthe GC policy within offline scenarios. We also proposed amodular policy hierarchy that features the ability to generatenear-term goals, attainable by the skills. This hierarchy bol-sters both the robustness of the GC policy and its adaptationefficiency against goal distribution shifts. In our future work,\nwe aim to extend the skill-step model for more complex cir-cumstances where distribution shifts in both environment dy-namics and goals occur concurrently. Our focus will be ongeneralizing the skills to better capture the intricacies of en-vironment dynamics patterns during the training phase."}]}