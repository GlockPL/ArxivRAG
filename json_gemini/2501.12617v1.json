{"title": "Deep Learning Based Identification of Inconsistent Method Names: How Far Are We?", "authors": ["Taiming Wang", "Yuxia Zhang", "Lin Jiang", "Yi Tang", "Guangjie Li", "Hui Liu"], "abstract": "For any software system, concise and meaningful method names are critical for program comprehension and maintenance. However, for various reasons, the method names might be inconsistent with their corresponding implementations. Such inconsistent method names are confusing and misleading, often resulting in incorrect method invocations. To this end, a few intelligent deep learning-based approaches based on neural networks have been proposed to identify such inconsistent method names in the industry. Existing evaluations suggest that the performance of such DL-based approaches is promising. However, the evaluations are conducted with a perfectly balanced dataset where the number of inconsistent method names is exactly equivalent to that of consistent ones. In addition, the construction method of this balanced dataset is flawed, leading to false positives in this dataset. Consequently, the reported performance may not represent their efficiency in the field where most method names are consistent with their corresponding method bodies and only a small part of method names are inconsistent with corresponding method bodies. To this end, in this paper, we conduct an empirical study to assess the state-of-the-art DL-based approaches in the automated identification of inconsistent method names. We first build a new benchmark (dataset) by using both automatic identification from commit history and manual inspection by developers, aiming to reduce the number of false positives. Based on the benchmark, we evaluate five representative DL-based approaches to identifying inconsistent", "sections": [{"title": "1 Introduction", "content": "Identifiers, i.e., the names of software entities, make up approximately 70% of source code (Deissenboeck and Pizka 2006). Such identifiers play an important role in program comprehension and software maintenance (Allamanis et al. 2015; Butler et al. 2010; Lin et al. 2019). Therefore, their quality is critical (Lawrie et al. 2006; Schankin et al. 2018; Lin et al. 2017; Binkley et al. 2013; Liu et al. 2015). As an important type of identifier, concise and meaningful method names can provide intuitive information about the method behaviors (Allamanis et al. 2016; Alsuhaibani et al. 2021). Developers often guess the functionalities of methods according to the short and meaningful method names instead of complex and lengthy implementations (method bodies) (Gethers et al. 2011; Bavota et al. 2013; Deissenboeck and Pizka 2015). However, naming a method properly is not an easy case for developers. Studies suggest that naming software entities is one of the most difficult tasks for programmers (Johnson 2018a,b). As a result, developers often write poor (i.e., inconsistent) method names in programs due to a lack of thesaurus, conflicting styles during collaboration, and improper code cloning (Kim and Kim 2016). The improper (inaccurate) method names tend to lead misunderstandings (Takang et al. 1996; Liblit et al. 2006; Arnaoudova et al. 2016; Hofmeister et al. 2017; Arnaoudova 2010) and may result in incorrect method invocation and software defects (Butler et al. 2009; Amann et al. 2018; Abebe et al. 2011, 2012; Aghajani et al. 2018). Furthermore, these naming issues could negatively affect other software applications that"}, {"title": "2 Related Work", "content": "Inconsistent names tend to lead to misunderstandings among develop- ers (Takang et al. 1996; Liblit et al. 2006; Arnaoudova et al. 2016; Hofmeister et al. 2017; Arnaoudova 2010) and may result in incorrect method invocation and software defects (Butler et al. 2009; Amann et al. 2018; Abebe et al. 2011, 2012; Aghajani et al. 2018). To identify inconsistent method names, a few automatic approaches have been proposed. The task of the identification of inconsistent method names is to identify the method names that do not fully de- scribe the functionality and semantics of their method bodies. The mainstream methods include information retrieval-based approaches, i.e., retrieving from a large code corpus, and generation-based approaches, i.e., generating a method name first, and then validating whether it is consistent with method bodies. We will elaborate on the mainstream approaches in the below subsections. There are also other approaches, such as data mining-based approach (H\u00f8st and \u00d8stvold 2009) or classifiers based on DNNs (Li et al. 2021a). However, since they are not mainstream methods for identifying inconsistent method names, we did not include them in the evaluation in this paper."}, {"title": "2.1 Empirical Studies on Identification of Inconsistent Method Names", "content": "Notably, some empirical studies that focus on the topic of inconsistent method names are also presented (Minehisa et al. 2021; Kim et al. 2023). Minehisa et al. (Minehisa et al. 2021) presented a comparative study of the vectorization approaches used in inconsistency detection. This work compares the compu- tational cost and the performance of four different vectorization approaches, proving that Sent2Vec is the best approach which can build a vectorization model 14 times faster than CNN without sacrificing the performance of detect- ing inconsistent method names. This work only focuses on the vectorization approaches used in the approaches. By contrast, our work extensively evaluated the performance of state-of-the-art approaches in different application scenarios and investigated the reason why they succeed or fail. Kim et al. (Kim et al. 2023) also presented an empirical study similar to this paper. As far as we know, it is the first work to propose to evaluate the approaches designed to detect inconsistency between method names and bodies from a different perspective"}, {"title": "2.2 Information Retrieval-based Approaches", "content": "Liu et al. (Liu et al. 2019) proposed a DL-based approach to identify inconsistent method names. To the best of our knowledge, this is the first approach combining deep learning techniques with an information retrieval mechanism for the automated identification of inconsistent method names. In addition, Liu et al. (Liu et al. 2019) constructed the first benchmark for the task of detecting inconsistent method names, which are extensively used for the evaluations of the subsequent approaches. Consequently, we presented it as a single category. It converts the method bodies and names in training data into fixed- length vectors by training the models through deep learning techniques, i.e., convolutional neural network (CNN) (Matsugu et al. 2003), Word2vec (Mikolov et al. 2013), and Paragraph2vec (Le and Mikolov 2014). After the training, the models that can convert method names and bodies into vectors are obtained, and all the converted vectors of method names and bodies in training data constitute the name vector space $U_{sname}$ and body vector space $U_{sbody}$, respectively. For a method < Mbody, Mname > to be tested, it first converts Mbody and Mname into vectors using the pre-trained model, then it searches the body vector space $U_{Sbody}$ for method bodies whose vectors are highly similar to that of Mbody and collects the method names (noted as MNs1) associated with the resulting method bodies. It also searches the name vector space $u_{sname}$ for method names (noted as MNs2) that are both lexically and semantically similar to Mname. If MNs\u2081 has no method name that shares the same first sub-token with any method name in MNs2, the test method name will be identified as an inconsistent name. To evaluate the proposed approach, Liu et al. (Liu et al. 2019) discovered the renaming of methods by mining version control systems. Exactly half of the renaming (randomly selected) was exploited to create positive items (inconsistent method names) by extracting the elder"}, {"title": "2.3 Generation-based Approaches", "content": "The rationale of generation-based approaches is to first generate a method name for a specific method body. Then they calculate the lexical similarity between the generated method name and the original one. Finally, a similarity threshold is adopted to identify whether the original method name is consistent with its method body according to the lexical similarity calculated above. There are three approaches (Nguyen et al. 2020; Li et al. 2021b; Wang et al. 2021a) proposed with evaluations on the task of identification of method name consistency. In addition, there are many approaches designed for method name generation without the evaluation of our task. For these approaches, we only consider the latest one (Liu et al. 2022), and the pioneering one (Allamanis et al. 2016) as the baselines in this paper. MNIRE proposed by (Nguyen et al. 2020) is another DL-based approach. To the best of our knowledge, this is the first generation-based approach to automated identification of inconsistent method names. For a method < Mbody, Mname > to be tested, MNIRE first leverages a deep neural network to generate a method name (noted Ng) for the given method body Mbody. Then it computes the lexical similarity between the generated method name Ng and the original one Mname based on the overlapping rate of sub-tokens. If and only if the similarity Sim(Ng, Mname) is smaller than a threshold (0.94 in their evaluation), method name Mname will be regarded as inconsistent. One of the key contributions is that MNIRE leverages additional contexts (e.g., the methods' parameter types, return type, and the enclosing class name) besides the method body to generate the method name. MNIRE was evaluated on the dataset Original Data created by Liu et al. (Liu et al. 2019). Their evaluation results suggest that MNIRE Outperforms baseline approaches by improving precision from 56.8% to 62.7% and recall from 84.5% to 93.6%. Besides, in the live study conducted by Nguyen et al., 31 out of 42 pull requests for renaming suggestions were acknowledged by the developers. The evaluation results indicate that MNIRE works well on the identification of inconsistent method names. DeepName proposed by Li et al. (Li et al. 2021b) is another DL-based ap- proach to the automated identification of inconsistent method names. Different from existing approaches, Deep Name exploits tokens in the caller and callee method of the method under test. Furthermore, they proposed a Non-copy"}, {"title": "3 Experimental Setup", "content": "An overview of the approaches that are designed to identify inconsistent method names (or are capable of) is presented in Table 1. Notably, the first two approaches were not named officially by their authors. For simplicity's sake, we refer to the first one proposed by Allamanis et al. (Allamanis et al. 2016), and the second approach proposed by Liu et al. (Liu et al. 2019) as CAN, and IRMCC, respectively. Although CAN and GTNM were not evaluated in the original paper, they are still capable of identifying inconsistent method names. We take them into evaluation due to their representativeness which is discussed in Section 2.3. Notably, although the implementation of DeepName is publicly available (Li 2024), it cannot run smoothly. Moreover, we contacted the authors and did not get feedback from them. Consequently, in this paper, we select CAN (Allamanis et al. 2016), IRMCC (Liu et al. 2019), MNIRE (Nguyen et al. 2020), Cognac (Wang et al. 2021a), and GTNM (Liu et al. 2022) for evaluation."}, {"title": "3.1 Evaluated Approaches", "content": "We first explain the reasons why constructing Ben Mark is necessary. As shown in Table 1, the state-of-the-art DL-based approaches (Liu et al. 2019; Nguyen et al. 2020; Li et al. 2021b) are all evaluated on the same dataset, created by (Liu et al. 2019). In this dataset, called OriginalData (Liu 2024) for short in this paper, each instance in OriginalData is a triplet, i.e., < BuggyName, FixedName, MethodBody >. Liu et al. take half of the FixedName and MethodBody as consistent methods, and the other half of the BuggyName and MethodBody as inconsistent methods for testing, resulting in a balanced dataset. However, the ratio of inconsistent and consistent methods is extremely imbalanced in real-world scenarios. To investigate how the ratio"}, {"title": "3.2 Dataset", "content": "For the reasons above, we build BenMark by reusing the subject projects exploited by existing DL-based approaches (Liu et al. 2019; Nguyen et al. 2020; Li et al. 2021b; Wang et al. 2021a). There are 430 projects (i.e., GitHub repositories) collected by Liu et al., and they are coming from four well-known communities (namely Apache, Spring, Hibernate, and Google) and have at least 100 commits, making sure that they have been well-maintained. The list of project names and GitHub URLs can be found in (Liu 2024). Detailed construction procedures are shown in Section 3.2.2."}, {"title": "3.2.1 Reasons for Building BenMark", "content": "The construction of BenMark includes two major steps, i.e., automatic identi- fication and manual inspection. We first leveraged the basic logic adopted by Liu et al. to automatically identify inconsistent and consistent method names from projects' commit history. Since the developers could rename a method for various reasons, the above inconsistent method names obtained from renamings in commit history must include noise. To reduce these noise data, we surveyed three developers on how to pick out the genuine inconsistent method names and then performed a manual inspection to maximally reduce the false positives. Automatic Identification The automatic identification of inconsistent and consistent method names are as follows:"}, {"title": "3.2.2 Construction of BenMark", "content": "- Data cleaning. Following Liu et al. (Liu et al. 2019), we exclude main methods, empty methods, constructor methods, example methods, and methods with non-alphabetic names. - Identifying inconsistent method names. Following Liu et al. (Liu et al. 2019), we identify inconsistent method names by mining the version control systems. If a method has been renamed (while its body remains unchanged) in a commit and the method remains untouched ever since then, we treat the elder name of the method (i.e., before renaming) as an inconsistent name. Finally, we obtained 4,597 inconsistent methods. Note that Liu et al. filtered out the method names whose first sub-tokens remain the same after renaming. While this filtering is effective in avoiding false positives, it could also miss some true cases. We removed this rule and further validated the data manually. - Identifying consistent method names. We only identify consistent method names in a single snapshot (i.e., the last commit version) of the application. A method in this snapshot is taken as consistent if 1) the method body is not associated with any inconsistent method names identified in the preceding step; 2) both the method body and the method name are untouched during the last n commit versions. For each subject project, n is a constant, i.e., the largest duration between the creation time of a method and its first rename. The value of n for each project can be found in the online appendix (Wang 2024). Finally, we obtained 1,296,743 consistent methods."}, {"title": "3.2.3 Construction of Training Data", "content": "To maximize the performance of the baselines, we also filtered out the extremely complex methods as Liu et al. did. during the construction of training data. To facilitate the different empirical settings, we further constructed two sets of training data based on BenMark: - CORPUS_WP represents the training data for within-project setting. - CORPUS_CP represents the training data for cross-project settings. Notably, the evaluated approaches request a large number of projects as a corpus for code retrieval (Liu et al. 2019), or for the training of method name generation models (Nguyen et al. 2020; Allamanis et al. 2016; Wang et al. 2021a; Liu et al. 2022). Neither of the evaluated approaches requests any labeling of the methods in the code corpus. To improve the reliability of the results"}, {"title": "3.2.4 Survey", "content": "Inconsistent method names mined from commit histories could involve various false positives because they can not guarantee the method name changes are associated with the inconsistency between names and bodies (Wen et al. 2020, 2022; Kim et al. 2023). To reduce the false positives of the inconsistent method names obtained from the automatic identification, we surveyed devel- opers to investigate how to accurately identify genuine inconsistent method names based on renamings mined from commit histories. To reduce the burden on developers, we first conducted an initial inspection of the automatically identified 4,597 inconsistent method names and found three typical cases of false positives: - Typos correction, e.g., changing \"getActoveWebflow\" to \"getAc- tive Webflow\"; - Format correction, e.g., changing \"getTaskId\" to \"getTaskID\", or changing \"reset\" to \"_reset\"; - Add Trailing Number, e.g., changing \"getEventFilter\" to \"getEvent- Filter0\"; These false positives are not renamed due to the inconsistency between method names and bodies. After the initial inspection, the number of inconsistent method names is reduced to 4,102. From the remaining 4,102 inconsistent method names, we randomly sampled 351 inconsistent methods for the survey, with a confidence level of 95% and a"}, {"title": "Testing Data", "content": "To facilitate the RQ1 and RQ2, we further construct two new testing datasets (we call BalancedData, NaturalData) based on BenMark. Note that these two datasets are constructed from each fold of Ben Mark, which means that there are 10 Balanced Data and 10 Natural Data for all the 10 folds of BenMark respectively. Here are the construction processes: - Natural Data is constructed by reusing all the testing data of BenMark. This dataset is natural because we do not intentionally control the ratio of inconsistent and consistent method names. - To investigate the performance of the evaluated approaches in within-project setting and cross-project setting, we have to construct a balanced dataset containing the same number of inconsistent and consistent method names, i.e., BalancedData. BalancedData is constructed by the following two steps. First, we reused all the inconsistent method names in each fold of BenMark (let the number of inconsistent method names be Ni for each fold i), and such samples serve as positive items. Second, we randomly sampled the same number of consistent method names (i.e., Ni) from the negative items in each fold of BenMark, and such samples serve as the negative items in the resulting testing data. To ensure a fair sample, we made certain that the samples were evenly distributed across projects. Specifically, we sampled consistent method names based on the number of inconsistent method names in each project. In other words, a project with more inconsistent method names should sample a larger number of"}, {"title": "3.3 Parameter Tuning", "content": "To maximize the potential of the evaluated approaches, we perform hyperpa- rameter tuning for such approaches on a GPU server (OS: Ubuntu 18.04.1; CPU: 32 * Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz; GPU: 4* TITAN RTX; RAM: 128 GB). For IRMCC, we follow the grid-search tuning approach to tune the pa- rameters of Word2vec and Paragraph2vec, i.e., Size of vector, Learning rate, and Window size. Given that Word2vec and Paragraph2vec are both unsuper- vised learning approaches, we can only identify the performance by conducting the entire experiments but the whole process is time-consuming. To this end, we empirically selected the next-to-be-tested setting, i.e., selecting the value that yields higher accuracy for a single parameter. In the end, we take the combination which yields the best accuracy value as the final combination of parameters. Parameter k of IRMCC represents the number of the most similar method names and bodies retrieved from the training data. As reported by Liu et al. (Liu et al. 2019), k = 1 yields the best performance of IRMCC when identifying inconsistent method names. To this end, we set k = 1 all through the evaluation in this paper. For MNIRE, we also follow the grid-search tuning approach to tune the parameters with the help of NNI (Microsoft 2024) which is a widely used toolkit to help developers design and tune machine learning models effectively. We tuned the parameters, i.e., Learning rate, Embedding size, Batch size, Hidden size, and Vocabulary size. For each of the to-be-tested settings, we train MNIRE with the given setting on nine in ten of the training data and then validate the performance on the validation set (i.e., one in ten of the training data). The combination which yields the highest accuracy is selected as the final"}, {"title": "4 RQ1: Cross-Project VS. Within-Project", "content": "To answer this research question, we evaluate the selected DL-based approaches with cross-project and within-project settings, independently. Both within- project settings and cross-project settings are valuable in different application scenes. For within-project settings, instead of ignoring all data from the target project, developers can put the confirmed consistent data into training data to train a new model, and then further conduct the prediction on the remaining data. However, this requires repetitious training of the models, which is often time-consuming and resource-consuming. For cross-project settings, users can train the model once and use this model to predict any new test data. Therefore, they do not have to train models on each new project they encounter, which significantly facilitates usage. By comparing the performance of the same DL- based approaches under two empirical settings, we can reveal whether the"}, {"title": "4.1 Process", "content": "The testing data is BalancedData (see Section 3.2.4). The training dataset (i.e., CORPUS_CP, see Section 3.2.3) is constructed over the rest of 90% of all the projects, i.e., the other 9 fold of data. This dataset is leveraged to train"}, {"title": "4.1.1 Cross-Project Evaluation", "content": "The testing data is BalancedData (see Section 3.2.4). The training dataset (i.e., CORPUS_CP, see Section 3.2.3) is constructed over the rest of 90% of all the projects, i.e., the other 9 fold of data. This dataset is leveraged to train"}, {"title": "4.1.2 Within-Project Evaluation", "content": "The testing dataset in within-project settings is the same as that in cross- project settings, i.e., BalancedData. The training dataset is CORPUS_WP (see Section 3.2.3) The evaluation is within-project settings because the methods in the testing dataset (i.e., BalancedData) and the methods in the training dataset (i.e., CORPUS_WP) may come from the same project. Notably, cross- project evaluation and within-project evaluation use the same subject projects selected by Liu et al. (Liu et al. 2019). The major difference is that cross- project evaluation partitions projects into testing and training projects that are exploited to create testing data and training data respectively. In contrast, within-project evaluation selects some methods as testing data, and others are taken as training data regardless of where they come from."}, {"title": "4.2 Results", "content": "Following existing evaluations (Liu et al. 2019), we leverage the precision, recall, and F-score of identifying positive and negative items as well as the overall accuracy for all testing items. Due to limited space, we only report the"}, {"title": "5 RQ2: Artificial Data VS. Natural Data", "content": "To answer RQ2, we should evaluate the selected DL-based approaches multiple times by changing the ratio of inconsistent method names in the testing data. Notably, the evaluation of the DL-based approach is time-consuming. Conse- quently, we only investigate two special settings of the ratio. The first setting is to balance positive and negative items in the testing dataset. Subsequently, the testing data is BalancedData, and we have experimented on this dataset (see Section 4). Given that the performance of evaluated approaches with within- project setting and cross-project setting do not have significant difference (see Section 4), we take the performance of cross-project setting in Section 4 as the performance of BalancedData for comparison because the cross-project setting is wildly used and time-saving for the potential users. To make sure that there is only one independent variable, i.e., the ratio of inconsistent method names, the evaluation in this research question is also based on a cross-project setting. The second special setting is to simulate the ratio of inconsistent and consistent method names in real scenarios. We construct a corresponding dataset, i.e., NaturalData (see Section 3.2.4). This dataset is natural because the number of negative items (consistent method names) is significantly more than positive ones (inconsistent method names). Notably, the two settings share the same training data (i.e., CORPUS_CP). The only difference (variable) among the two settings is the ratio of positive and negative items in the testing data. As a result, we may reveal the effect of the ratio by analyzing how the evaluated approaches perform in these two settings. The adopted training data and testing data are presented in Table 8."}, {"title": "5.1 Process", "content": "The performance of each fold of data in different settings is presented in Table 9 - Table 13. Table 19 - Table 23 present the average evaluation results, and from these tables, we make the following observations:"}, {"title": "5.2 Results", "content": "- First, the precision of the evaluated approaches on inconsistent names is significantly impacted when we change the ratio of inconsistent and con- sistent method names in testing data. Switching from BalancedData to NaturalData (the ratio of inconsistent method names declines sequentially) results in a dramatic decline in precision. The precision is reduced substan- tially from 54.4% to 0.3% (IRMCC), from 54.5% to 0.3% (CAN), from 56.4% to 0.3% (MNIRE), from 53.3% to 0.5%(Cognac), and from 55.1% to 0.4%(GTNM). The reason is that the more consistent methods we include, the more false positives we get. As a result, the number of true positives is stable whereas false positives are increased, which results in a reduced precision that equals true positives divided by the sum of true positives and false positives. - Second, the decline of the ratio of inconsistent method names leads to better precision in identifying consistent method names. The precision is increased from 59.0% to 99.8% (IRMCC), from 84.6% to 99.9% (CAN), from 78.7% to 99.9% (MNIRE), from 72.8% to 99.9%(Cognac), and from 68.0% to 99.9%. This is because the number of inconsistent method names is too tiny, and it can barely impact the overwhelming number of consistent method names. - Third, the recall is all slightly changed. The reason is that the evaluated approaches are trained with the same training data regardless of the differ- ence in testing data. As a result, the resulting models (approaches) have the same ability to identify inconsistent and consistent methods. For example, from Table 23, we observe that GTNM can accurately classify 83.7% of the inconsistent names in BalancedData. Increasing the number of consistent names (in NaturalData) would not influence the fact that around 83.7% of the inconsistent names are classified correctly (which should result in a recall of around 83.7%). Notably, Because of the random sampling during dataset construction and high randomness of DL-based approach (Scarda- pane and Wang 2017; Zhuang et al. 2021), the recall in BalancedData, and Natural Data is not the same."}, {"title": "6 RQ3: Analysis of IR-based Approaches", "content": "In Section 5, we learn that IRMCC works in many cases (the accuracy can be 41.8%). However, it remains unclear where and why it works or fails. Answering these questions could shed light on the direction of developing more advanced DL-based approaches. To this end, in this research question, we investigate in what cases and for what reason the IR-based approach, i.e., IRMCC, works or fails. To answer this research question, we follow the widely-used mixed-method approach (Creswell and Creswell 2017) and combine a qualitative analysis of the sampled methods with a quantitative examination of the whole set of testing data. On the qualitative analysis, we first randomly sample 383 methods (resulting in 185 successfully identified cases and 198 failed ones) from all the methods in NaturalData, with a confidence level of 95% and a margin of error of 5%. We analyze where and why IRMCC works or fails from the perspectives of method name and body, with a qualitative analysis based on the rationale of IRMCC. From the method name perspective, we analyze the popularity of the first sub-token of method names since IRMCC considers the first sub-token to conduct consistency checking. From the method body perspective, we analyze the code complexity by measuring lines of code (noted as LOC) and McCabe's Cyclomatic Complexity. At the last, for the qualitative analysis, we analyze the names and bodies of the methods retrieved by IRMCC."}, {"title": "6.1 Process", "content": "Given that IRMCC heavily relies on the first sub-token of a method name to conduct the consistency checking, we analyze the first sub-tokens (noted as F-token) of the sampled 383 method names to investigate how the F-tokens affect the performance of IRMCC from the method name perspective. We collect the F-tokens of the sampled methods and sort them by their pop- ularity in successfully identified cases, failed identification cases, and all sampled"}, {"title": "Popularity of Method Name's F-token Matters", "content": "Through the observations above, we assume that there might be a corre- lation between the popularity of F-tokens and their rates of being identified successfully (noted as success rate). Therefore, we calculate the success rate of all the method names, and we found that the success rate of the methods that start with the top three F-tokens are also the highest ones, i.e., 73.8%=96/130 for get*, 60.0%=36/60 for set*, and 52.9%=9/17 for is*. In contrast, the average success rate (i.e., Accuracy) of the whole dataset is only 48.3% (see Table 9). To further validate the correlation between the popularity of F-tokens and success rate, we present their relation in Fig. 1. The horizontal axis specifies the frequency of the F-token in the training dataset (CORPUS-CP). The vertical axis specifies the percentages of the method names beginning with the given F-token are identified correctly. From this figure, we observe that the success rate increases substantially with the increase of F-tokens' popularity. To validate the statistical significance of the correlation, we conduct the Spearman correlation (Artusi et al. 2002) for the two factors. Our results (p-value = 3.4E-69, rho = 0.4) suggest that they do have a positive correlation, which statistically validates our observation. Based on the preceding analysis, we conclude that IRMCC works well on the methods whose names start with popular F-tokens in CORPUS_CP."}, {"title": "6.2 Method Body Complexity Matters", "content": "From the method body perspective, we measure the complexity of method bodies by two widely used metrics, i.e., LOC and McCabe's Cyclomatic Com- plexity (McCabe 1976). We first investigate the LOC of the sampled 383 methods to see how LOC impacts the performance of IRMCC. We find that the average LOC of the success and failure methods is 1.8 and 3.8, respectively. To figure out whether the LOC of success and failure methods has a significant difference, we conduct the Wilcoxon-Mann-Whitney u-test (i.e., Wilcoxon rank sum test) (Ott and Longnecker 2015) on these two groups of LOC. The results (statistic -5.2, p-value=2.1E-07) suggest that there is a significant difference between the LOC of success and failure methods, which further indicates that the methods that are predicted incorrectly may have more LOC in their method bodies. We then analyze the LOC of each method in NaturalData, the average number of LOC of success and failure methods is 2.0 and 3.2, respectively. The results of conducting the Wilcoxon-Mann-Whitney u-test on NaturalData (statistic=-80.4, p-value=0.0) further validate that the more LOC a method has, the more likely it will be predicted incorrectly by IRMCC. We then investigate the complexity of method bodies by McCabe's Cyclo- matic Complexity which measures the code complexity better. Intuitively, we analyze the code structure of the sampled 383 methods because methods with more loop structures and branched structures tend to be more complicated. We categorize the sampled methods by three basic code structures, i.e., sequential structure, branched structure, and loop structure ordered by complexity, and calculate the average success rate of methods with these three structures. If a method consists of more than one structure, we take the more complex one. The results suggest that the average success rate of methods with sequential"}, {"title": "6.3 Requesting for More Advanced Representation Techniques", "content": "Given that IRMCC is based on information retrieval", "method": "one (noted as mn2 \u2208 MN82) is the name which is the most similar to the test method name, and the other (noted as mn\u2081 \u2208 MNS1) is the name of the method whose body is the most similar to the test method's body. IRMCC will regard the test method name as consistent when the F-token of mn\u2081 and mn2 are identical. We first collect the method names and bodies which are the most similar ones retrieved by IRMCC from"}]}