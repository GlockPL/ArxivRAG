{"title": "Performance Control in Early Exiting to Deploy Large Models at the Same Cost of Smaller Ones", "authors": ["Mehrnaz Mofakhami", "Reza Bayat", "Ioannis Mitliagkas", "Jo\u00e3o Monteiro", "Valentina Zantedeschi"], "abstract": "Early Exiting (EE) is a promising technique for speeding up inference by adaptively allocating compute resources to data points based on their difficulty. The approach enables predictions to exit at earlier layers for simpler samples while reserving more computation for challenging ones. In this study, we first present a novel perspective on the EE approach, showing that larger models deployed with EE can achieve higher performance than smaller models while maintaining similar computational costs. As existing EE approaches rely on confidence estimation at each exit point, we further study the impact of overconfidence on the controllability of the compute-performance trade-off. We introduce Performance Control Early Exiting (PCEE), a method that enables accuracy thresholding by basing decisions not on a data point's confidence but on the average accuracy of samples with similar confidence levels from a held-out validation set. In our experiments, we show that PCEE offers a simple yet computationally efficient approach that provides better control over performance than standard confidence-based approaches, and allows us to scale up model sizes to yield performance gain while reducing the computational cost.", "sections": [{"title": "1. Introduction", "content": "Scale, both in terms of model size and amount of data, is the main driver of recent AI developments, as foreseen by Kaplan et al. (2020) and further evidenced by Hoffmann et al. (2022), which has led to significant improvements in computer vision and natural language processing tasks. However, the enhanced predictive performance enabled by scaling comes with substantial computational costs and increased latency during inference. To address these challenges, several approaches have been proposed, such as quantization (Dettmers et al., 2022; Ma et al., 2024; Dettmers et al., 2024), knowledge distillation (Hinton et al.,"}, {"title": "2. Background and Setting", "content": "We focus on the K-way classification setting where data instances correspond to pairs x, y ~ X \u00d7 Y, with X C Rd and y = {1, 2, 3, ..., K}, K \u2208 N. Classifiers then parameterize data-conditional categorical distributions over y. That is, a given model f \u2208 F : X \u21a6 \u2206K-1 will project data onto the probability simplex \u2206K\u22121.\nEarly Exit Neural Networks Early Exit Neural Networks enable dynamic resource allocation during model inference, reducing computational demands by not utilizing the entire model stack for every query. These approaches strategically determine the exit point for processing based on the perceived difficulty of the data, allowing for a reduction in resource use for simpler examples while allocating more compute power to more complex cases.\nThis is commonly achieved through a confidence threshold \u03b4\u2208 [0, 1], where the decision to exit early is made if"}, {"title": "3. Benefits of Increasing Model Size Coupled with Early Exiting", "content": "Our first contribution is to show that Early Exiting does not necessarily compromise performance for faster inference, but can be used to run larger models at the cost of smaller ones. This observation underscores a significant insight: scaling up model size can enhance computational efficiency by enabling early exits in the inference process. Larger models can leverage their deeper architecture to make correct predictions at earlier stages for easy samples, while benefiting from later layers for hard ones, reducing the need for extensive computation across all layers for all samples. This efficiency is crucial for practical applications, where computational resources and time are often limited. Therefore, our findings challenge the conventional view that larger models are inherently more computationally expensive. Instead, we show that larger models can be more efficient in terms of accuracy for a fixed compute budget, providing a compelling case for scaling up models to improve inference computational efficiency while maintaining or even enhancing prediction accuracy.\nFinally, note that these compute gains also translate to reduced latency when using dynamic batching, so that inference is batchified (as for any model without EE) and resources are used at full capacity. Indeed, techniques such as on-the-fly batching (Neubig et al., 2017)\u00b9 enable dynamic batching during inference, allowing the system to start processing new requests as soon as other requests in the batch are completed."}, {"title": "4. Performance Control Early Exiting", "content": "In this section, we first examine the miscalibration of Early Exit Neural Networks, demonstrating through experiments that they tend to be overconfident, with miscalibration escalating as layer depth increases. Then we introduce PCEE (Performance Control Early Exiting), a method that ensures a lower bound on accuracy by thresholding not on the confidence estimate of a given test example, but on the average accuracy of samples with similar confidence from a held-out dataset. Our early exit method requires selecting a single threshold rather than one per layer. This threshold is a simple accuracy lower bound, based on the target accuracy chosen by the user, rather than a confidence level that might not relate directly to prediction performance. We emphasize this advantage by highlighting that selecting a threshold per layer involves an exhaustive search over a large space as in existing methods (Elbayad et al., 2019b). For instance, with a 8-layer model, searching for the best threshold for each layer to maximize validation accuracy, even within a narrow range of (0.8,0.9] and a step size of 0.01, results in 108 combinations. This extensive search, performed before inference, demands significant computational resources. Additionally, if we need to adjust for lower accuracy due to budget constraints, the entire process must be repeated. In contrast, our method allows easy adjustment of the threshold based on the desired accuracy level, offering significant computational savings and flexibility."}, {"title": "4.1. Checking for Miscalibration in Early Exit Neural Networks", "content": "Performing EE at inference to allocate adaptive computation to unseen data requires reliable confidence estimation at each exit point in a multi-layer model. However, this is non-trivial to achieve as it's well-known that neural networks are typically overconfident (Wang, 2023; Guo et al., 2017). That is, simply relying on commonly used confidence indicators would trigger very early exits at a high rate, damaging overall model performance. Moreover, commonly used confidence estimates are typically somewhat abstract quantities, decoupled from metrics of interest such as prediction accuracy, and it's not easy to decide on confidence thresholds that guarantee a certain performance metric of interest. Jiang et al. (2018) highlights that the model's reported confidence, e.g. probabilities from the softmax layer, may not be trusted especially in critical applications such as medical diagnosis.\nIndeed, if one considers the VIT (Dosovitskiy et al., 2020) with multiple classifiers (i.e., one classifier or exit point per layer) trained on IMAGENET-1K (Deng et al., 2009) illustrated in Figure 3, the overconfidence issue becomes noticeable."}, {"title": "4.2. Performance Control Early Exiting (PCEE)", "content": "We now introduce PCEE, a method to gain control over performance in Early Exit Neural Networks. The method is illustrated in Figure 5. For a multi-layer model with n layers {Li}_1, we incorporate exit points at the end of each layer. At any layer i, the input representation of sample x is processed through an exit layer block, denoted as Ei, which determines whether the model should terminate at this stage or continue. The exit layer E\u00bf transforms the representation ri = Li(x) into a vector of size corresponding to the number of classes.\nAt this step, a confidence score, ci, for sample x, is computed. This score can be derived either as the maximum value or the entropy of the probability distribution obtained after applying softmax. The decision to exit at this layer is then based on the confidence score. As discussed, existing methods rely only on the confidence score itself, which reduces control over accuracy because of the miscalibration issue. To make this decision, we instead employ the reliability diagram for layer i, which is constructed from the validation dataset. This diagram provides an estimate of the average accuracy for samples with a confidence level similar to ci at layer i. Suppose ci falls into bin m of the reliability diagram for layer i. If the accuracy corresponding to bin m exceeds a predefined threshold \u03b4, the model exits at layer i, outputting the prediction derived from the exit layer. Otherwise, the model proceeds to the next layer. The representation passed to layer i + 1 is ri, the one produced at the end of layer i before it goes through E\u2081. Further details of PCEE are outlined in Algorithm 1.\nPCEE-WS PCEE-WS is a variant of PCEE with a smoothing technique applied to the reliability diagrams of the validation dataset. We observed that some bins in the reliability diagrams could contain very few examples, leading to inaccurate representations of the bin's accuracy. To address this, we smooth the accuracy of each example from a binary value (0 or 1) to the average accuracy of its H nearest neighbors based on confidence scores, where H is a hyperparameter."}, {"title": "5. Experiments", "content": "We evaluate PCEE and PCEE-WS on widely used image classification benchmarks, and report performance both in terms of accuracy, and computational efficiency. In all experiments, we use 10% of the training data for the CIFAR datasets and 4% for IMAGENET respectively as held-out validation set to learn the confidence-to-accuracy mappings in reliability diagrams for our method, and the hyperparameters for the baselines. These portions are standard for validation sets on these datasets. For fair comparison, we run all EE methods with thresholds set to the same value for all intermediate layers.\nBaselines We compare our methods with four baseline approaches: Oracle, Confidence Thresholding (referred to as \"Confidence\" in the tables and figures), the Laplace approximation introduced by Meronen et al. (2024), and Confidence Thresholding with Temperature Scaling (referred to as \"TS+Confidence\"). Oracle refers to a setting with privileged information whereby exits happen as soon as an intermediate layer's prediction matches that of the final layer, showing the potential compute gain of an optimal exiting strategy. The results of Oracle do not depend on the threshold d. Confidence Thresholding checks the confidence of the prediction; if it is above the threshold, it exits. The Laplace approximation is a post-hoc calibration method that does not require retraining, like our approach. It approximates a Bayesian posterior for each exit layer with a multi-variate Gaussian, centered on the deterministic exit layer and with covariance equal to the inverse of the exit layer Hessian. Predictions are then obtained via a Monte Carlo estimate that we perform with sample size equal to 1, and with temperature and prior variance set to their default values, following the released codebase. Finally we compare our method to temperature scaling (Guo et al., 2017), a post-hoc calibration technique that divides logits by a scalar parameter T before applying softmax. In our implementation, we learn one temperature parameter per layer, starting with T = 1, and determine its optimal value using the validation set\u00b3.\nPerformance Control Figure 6 reports results for models of increasing size. We first notice that PCEE (orange) and PCEE-WS (green) show higher controllability relative to Confidence Thresholding: resulting accuracy is consistently higher than the threshold for PCEE and PCEE-WS, which is by design and enables simpler inference pipelines where one can compromise accuracy for compute (or vice-versa) more easily than with Confidence Thresholding.\nTables 4 provide a detailed comparison across methods along with computation cost on CIFAR-100. For various threshold values (\u03b4), PCEE and PCEE-WS exhibit higher accuracy compared to baselines. Notably, for the MSDNET-SMALL model, PCEE and PCEE-WS achieve up to 71.81% accuracy at d = 0.71, outperforming the Confidence's 71.35%. Similarly, PCEE and PCEE-WS reach up to 73.97% accuracy at d = 0.73 for MSDNET-LARGE, surpassing the Confidence's 72.72% that does not meet the desired threshold. We also highlight that, despite the increase in average number of used layers, PCEE and PCEE-WS achieve higher performance, potentially justifying the computational trade-offs in situations where accuracy is of priority. For example, at d = 0.73, the MSDNET-LARGE model with PCEE-WS uses 3.02 layers on average, compared to the Confidence's 2.43, reflecting a balance between computational resources and accuracy gains. The Laplace baseline, although using the fewest average layers, falls below the threshold for most of 8 values and therefore does not provide performance control.\nEffect of Calibration Another finding from Table 4 is that the integration of Temperature Scaling with confidence thresholding enhances performance relative to the confidence baseline. This is an expected result, as TS improves model calibration, and hence accuracy of predictions when early exiting. Still, TS results are slightly worse than those of our proposed PCEE and PCEE-WS methods. It is also important to note that temperature scaling requires additional training and hyperparameter tuning, while our approach offers a simpler alternative that does not necessitate any extra training and remains effective in mitigating overconfidence in models.\nMore notably, PCEE and PCEE-WS can also be combined with temperature scaling. While our methods are designed to perform well without the need for additional calibration, applying temperature scaling can yield even better results.\nWe observe an interesting result in Table 4, where our methods can achieve higher accuracy than the Oracle while using only a fraction of layers. For example, for the small MSD-Net, PCEE-WS can achieve 71.76% accuracy for d = 0.68 and 71.81% for d = 0.71, surpassing Oracle's 71.64% accuracy with using only around 50% of the available layers on average. Finally, like when running the original model without EE, our method does not handle out-of-distribution data well and suffers from discrepancies between the validation and test sets (a weakness shared with Temperature Scaling that is shown to be not effective for calibration under distribution shifts (Ovadia et al., 2019; Tada & Naganuma, 2023; Chidambaram & Ge, 2024)). This issue can be mitigated in deployment by continuously updating our reliability diagrams using fresh data as they come to account for distribution shifts over time. Another solution to this problem is to enable rejection (e.g., by adding an \"I don't know\" class) to make the model more robust to distribution shifts (see for instance Liu et al. (2019)). Studying the compatibility of such an approach with EE is the subject of future work."}, {"title": "6. Related Work", "content": "Inference Efficiency Inference efficiency has been tackled in many different ways. For instance, quantization approaches (Dettmers et al., 2022; Ma et al., 2024; Dettmers et al., 2024) reduce the numerical precision of either model parameters or data, although typically at the expense of accuracy. Knowledge distillation approaches (Hinton et al., 2015; Gu et al., 2023; Hsieh et al., 2023) were also introduced with the aim of accelerating inference by training a small model to imitate a large one. While yielding improvements in inference speed, distilled models may miss certain"}, {"title": "7. Discussions and Future Work", "content": "We have presented a computationally efficient method for reliably early exiting and showed that we can achieve the accuracy of large models with a fraction of the compute required even by small ones. Our method makes use of a held-out validation set to estimate the mapping from confidence to accuracy in intermediate layers. This provides the user with better control over the model to match a desired accuracy target and simplifies the threshold selection procedure. Compared to confidence thresholding, we have shown that our method consistently improves the final accuracy when applied to models that are overconfident, as typically observed in the literature. We note however that this behavior is not necessarily true for underconfident models, as reported in Appendix D.4."}, {"title": "A. Implementation details", "content": "MSDNets MSDNets, proposed by (Huang et al., 2017), serve as a benchmark for EENNs (Jazbec et al., 2023; Laskaridis et al., 2021; Han et al., 2022) known for their overconfidence issues (Meronen et al., 2024). MSDNet's architectural design addresses significant challenges in incorporating intermediate exits into Convolutional Neural Networks (CNNs). One major challenge is the lack of coarse features in the early layers, which are essential for effective classification. MSDNet incorporates vertical convolutional layers also known as scales that transform finer features into coarse features at every layer and introduce dense connectivity between the layers and scales across the network, effectively reducing the impact of conflicting gradients.\nViT The ViT (Dosovitskiy et al., 2020) model we used for the experiments on CIFAR datasets is a 12-layer self-attentive encoder with 8 heads, trained with AdamW with a learning rate of le-3, a weight decay of 5e-5, a cosine annealing learning rate scheduler and a training batch size of 64."}, {"title": "B. ECE formal definition", "content": "As discussed in the Background Section 2, calibration of a multi-class classifier refers to how well the predicted confidence levels (e.g., max softmax(\u00b7)) match the actual probabilities of correct predictions. More formally, ECE is defined as follows if we split the range of confidences observed by f \u2208 F from a sample of data points X into M bins:\nECE(f, X) = \u2211 ( |Bm| / n ) |acc(Bm, f) \u2013 con f (Bm, f)|\nm=1\nwhere Bm is the set of data instances whose predicted confidence scores fall into the m-th bin, acc(Bm, f) is the accuracy of the model measured within Bm, and con f (Bm, f) is the average confidence of predictions in Bm, assuming measures of confidence within the unit interval."}, {"title": "C. Additional results evaluating overconfidence", "content": "In this section, we provide more experimental details and results to complement those of Section 4.1. Results led to the two following observations:\n\u2022 Effect of depth: Calibration degrades and models become overconfident for deeper layers.\n\u2022 Effect of model size: MSDNET-LARGE demonstrates a higher level of overconfidence than MSDNet Small, particularly towards the later layers"}, {"title": "D. Further experimental results", "content": "D.1. CIFAR-10\nFor the results of MSDNet on CIFAR-10, refer to Figure 10 and table 7. All methods perform well on this relatively simple dataset, achieving top-1 accuracies above the threshold.\nD.2. IMAGENET\nIn Table 8, we report results for MSDNet Large on ImageNet where our method consistenly achieves accuracy higher than the target ones (as indicated by the chosen threshold)."}]}