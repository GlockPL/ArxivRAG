{"title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies", "authors": ["Zixuan Chen", "Xialin He", "Yen-Jen Wang", "Qiayuan Liao", "Yanjie Ze", "Zhongyu Li", "S. Shankar Sastry", "Jiajun Wu", "Koushil Sreenath", "Saurabh Gupta", "Xue Bin Peng"], "abstract": "Reinforcement learning combined with sim-to-real transfer offers a general framework for developing locomotion controllers for legged robots. To facilitate successful deployment in the real world, smoothing techniques, such as low-pass filters and smoothness rewards, are often employed to develop policies with smooth behaviors. However, because these techniques are non-differentiable and usually require tedious tuning of a large set of hyperparameters, they tend to require extensive manual tuning for each robotic platform. To address this challenge and establish a general technique for enforcing smooth behaviors, we propose a simple and effective method that imposes a Lipschitz constraint on a learned policy, which we refer to as Lipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can be implemented in the form of a gradient penalty, which provides a differentiable objective that can be easily incorporated with automatic differentiation frameworks. We demonstrate that LCP effectively replaces the need for smoothing rewards or low-pass filters and can be easily integrated into training frameworks for many distinct humanoid robots. We extensively evaluate LCP in both simulation and real-world humanoid robots, producing smooth and robust locomotion controllers. All simulation and deployment code, along with complete checkpoints, is available on our project page: https://lipschitz-constrained-policy.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Humanoid research aims to develop intelligent, human- like machines capable of autonomously operating in everyday environments [1]-[4]. One of the most fundamental challenges in this field is achieving reliable mobility. Developing robust locomotion controllers and adapting them to real robots would greatly improve their capabilities."}, {"title": "II. RELATED WORK", "content": "Legged robot locomotion has long been a crucial yet challenging problem in robotics due to legged systems' high dimensionality and instability. Classic model-based control methods have achieved impressive behaviors on legged robots [12]-[14]. In recent years, learning-based methods have shown great potential to automate the controller development process, providing a general approach to building robust controllers for quadrupedal locomotion [15]-[18], bipedal locomotion [11], [19]\u2013[21], and humanoid locomotion [7], [22]-[24].\n\na) Sim-to-Real Transfer: One of the main challenges in RL-based methods is sim-to-real transfer, where policies are first trained in simulation and then deployed in real-world environments. Substantial effort is often necessary to bridge the domain gap between simulations and the real world, such as developing high-fidelity simulators [25], [26], and incorporating domain randomization techniques during training [18], [22], [27], [28]. Another widely adopted approach is the teacher-student framework, where a privileged teacher policy, with access to full state information, is trained first, followed by the training of an observation-based student policy through distillation [15], [20], [22], [24], [29]\u2013[31]. To further facilitate sim-to-real transfer, our framework also leverages a teacher-student framework [6], [16], [23], which trains a latent representation of the dynamics based on the observation history. These methods have been successful in transferring controllers for both quadruped robots [9], [32], [33], and humanoid robots [7], [23]. Some work also explores utilizing a single policy to control robots with different morphologies zero-shot in real world [34]. However, the policy's performance on real humanoid robots has yet to be validated, and it is not easy to plug into any existing training pipeline.\n\nb) Learning Smooth Behaviors: Due to the simplified dynamics of simulators, policies trained in simulation often exhibit jittery behaviors that cannot be transferred to the real world. Therefore, smooth policy behaviors are critical for successful sim-to-real transfer. Common smoothing techniques include the use of smoothness rewards, such as penalizing sudden changes in actions, degree of freedom (DoF) velocities, DoF accelerations [24], [29], [31], [32], [35], [36], and energy consumption [6], [9]. In addition to smoothness rewards, low-pass filters have also been applied to the output actions of a policy to ensure smoother behaviors [10], [11], [18], [37]. However, smoothness rewards typically require careful manual design and tuning, while low-pass filters often dampen policy exploration, resulting in sub-optimal policies. These techniques are also generally not directly differentiable, requiring sample-based gradient estimators to optimize, such policy gradients.\n\nc) Gradient Penalty: In this work, we propose a simple and differentiable method to train RL policies that produce smooth behaviors by leveraging a gradient penalty. Gradient penalty is a common technique for stabilizing training of generative adversarial network (GAN), which is susceptible"}, {"title": "III. BACKGROUND", "content": "Our method leverages ideas from Lipschitz continuity to train reinforcement learning policies to produce smooth behaviors. This section will review some fundamental concepts for Lipschitz continuity and reinforcement learning to provide a comprehensive background of our proposed method.\n\nA. Lipschitz Continuity\nIntuitively, Lipschitz continuity is a property that limits how fast a function can change. This property is a good way of characterizing the smoothness of a function. An intuitive visualization is shown in Fig. 2. Formally, we give the definition of Lipschitz continuity as follows:\nDefinition III.1 (Lipschitz Continuity). Given two metric spaces (X,dx) and (Y,dy), where dx denotes the metric on the set X and dy is the metric on set Y, a function f: X \u2192 Y is deemed Lipschitz continuous if there exists a real constant K such that, for all x\u2081 and x2 in X,\n$d_y (f(x_1), f(x_2)) \\le K d_x (x_1, x_2)$."}, {"title": "IV. LIPSCHITZ-CONSTRAINED POLICIES", "content": "In this section, we introduce Lipschitz-Constrained Policies (LCP), a method for training policies to produce smooth behaviors by incorporating a Lipschitz constraint during training. We begin with a simple experiment to illustrate the motivation behind our method. This is then followed by a detailed description of our proposed method.\n\nA. Motivating Example\nWe will first illustrate the motivation of LCP with a simple experiment. We know that RL-based policies are prone to producing jittery behaviors, and the most common method for mitigating these behaviors is to incorporate smoothness rewards during training. The smoothness of a function is typically evaluated using the first derivative. Therefore, we compare the l2-norm of the gradient of policies trained with and without smoothness rewards. Although no specific"}, {"title": "V. TRAINING SETUP", "content": "To evaluate the effectiveness of our method, we apply LCP to train policies for a variety of humanoid robots, where the task is for the robots to walk while following steering commands.\n\na) Observations: The input observations to the policy consists of a gait phase variable (a periodic clock signal represented by its sine and cosine components), command ct, measured joint positions and velocities srobot, and the previous output action of the policy at 1. To enable robust sim-to-real transfer, the policy also takes privileged information et as input, which consists of the base mass, center of mass, motor strengths, and root linear velocity. Observations of are normalized with a running mean and standard deviation before being passed as input to the policy.\n\nb) Commands: The command input to the policy ct =\n[vemd, vemd, vmd] consists of the desired linear velocities\n along x-axis vemd \u2208 [0m/s,0.8m/s] and y-axis vemde\n[-0.4m/s, 0.4m/s], and the desired yaw velocity vemde [-0.6rad/s, 0.6rad/s], both are in the robot frame. During training, commands are randomly sampled from their respective ranges every 150 timestep or when the environment is reset.\n\nc) Actions: The policy's output actions specify target joint rotations for all joints in the robot's body, which are then converted to torque commands by PD controllers with manually specified PD gains.\n\nd) Training: All policies are modeled using neural networks and trained using the PPO algorithm [48]. The policies are trained solely in simulation with domain randomization and then deployed directly on the real robots [27]. Sim-to-real transfer is performed using Regularized Online Adaptation (ROA) [6], [32]."}, {"title": "VI. EXPERIMENTS", "content": "LCP's effectiveness is evaluated on a set of diverse hu- manoid robots to show its generalization ability. We conduct an extensive suite of simulation and real-world experiments, comparing LCP to commonly used smoothing techniques from prior systems.\n\nA. Robot Platforms\nWe evaluate our framework on three real-world robots: the human-sized Fourier GR1T1, Fourier GR1T2, Unitree H1, and the smaller Berkeley Humanoid. We will first provide an overview of each robot's body structure. Then, our experiments show that LCP is a general smoothing technique that can be applied widely to several distinct robots.\n\na) Fourier GRITI & Fourier GRIT2: The Fourier GR1T1 and Fourier GR1T2 have the same mechanical structure. They both comprise 21 joints, with 12 joints in the lower body and 9 in the upper body. Notice that the torque limit for the ankle roll joint is minimal; we treat this as a passive joint during training and deploying. This means we control 19 joints of GR1 in total.\ncoefficients (e.g., Agp = 0.001), the policy can develop jittery\nbehaviors that are dangerous to deploy in the real world. However, excessively larger coefficients (e.g., Agp = 0.01)\nlead to a substantial decline in task return due to overly smooth and sluggish behaviors. As shown in Fig. 6, large values of Agp may also lead to slower learning speeds. Our experiments suggest that Agp = 0.002 strikes an effective balance between policy smoothness and task performance."}, {"title": "VII. CONCLUSION", "content": "In this work, we present Lipschitz Constrained Policies (LCP), a simple and general method for training controllers to produce smooth behaviors amenable to sim-to-real transfer. LCP approximates a Lipschitz constraint on the policy, implemented in the form of a differentiable gradient penalty applied during training. Through extensive simulation and real-world experiments, we show the effectiveness of LCP in training locomotion controllers for a wide range of real humanoid robots. While LCP has demonstrated its effectiveness in real-world locomotion experiments, our results are still limited to basic walking behaviors. Evaluating LCP on more dynamic skills, such as running and jumping, would help further validate this method's generality."}]}