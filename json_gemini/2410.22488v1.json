{"title": "Privacy-Preserving Dynamic Assortment Selection", "authors": ["Young Hyun Cho", "Will Wei Sun"], "abstract": "With the growing demand for personalized assortment recommendations, concerns over data privacy have intensified, highlighting the urgent need for effective privacy-preserving strategies. This paper presents a novel framework for privacy-preserving dynamic assortment selection using the multinomial logit (MNL) bandits model. Our approach employs a perturbed upper confidence bound method, integrating calibrated noise into user utility estimates to balance between exploration and exploitation while ensuring robust privacy protection. We rigorously prove that our policy satisfies Joint Differential Privacy (JDP), which better suits dynamic environments than traditional differential privacy, effectively mitigating inference attack risks. This analysis is built upon a novel objective perturbation technique tailored for MNL bandits, which is also of independent interest. Theoretically, we derive a near-optimal regret bound of O(\\sqrt{T}) for our policy and explicitly quantify how privacy protection impacts regret. Through extensive simulations and an application to the Expedia hotel dataset, we demonstrate substantial performance enhancements over the benchmark method.", "sections": [{"title": "1 Introduction", "content": "Assortment optimization has become a crucial element of online retail platforms, significantly influencing product recommendation on e-commerce websites, marketing campaigns, and promotional slots. The primary objective is to select a subset of items that maximizes expected revenue based on a user's choice model. With the advent of big data and the widespread adoption of machine learning algorithms, personalized recommendations have become feasible, enabling companies to present relevant products to the right audience and significantly enhancing potential revenue. In a recent McKinsey report\u00b9, consumer demand for personalization is growing, with 71% of consumers expecting personalized interactions and 76% expressing frustration when these expectations are not met.\n\nThe literature on revenue management increasingly addresses the development of assortment optimization algorithms in online retail, commonly referred to as dynamic assortment selection. This involves dynamically optimizing and adapting product offerings in response to changing conditions, leveraging customer data such as past purchases, browsing history, demographics, and other relevant information. However, the rapid growth of personalized recommendations has also led to heightened concerns about privacy (Bi and Shen, 2023; Su, 2024). A KPMG survey\u00b2 revealed that 70% of companies have increased their collection of personal consumer data, while 86% of individuals expressed growing concerns about data privacy. Additionally, 40% of individuals do not trust companies to use their data ethically, and 30% are unwilling to share personal data for any reason. These concerns have prompted stringent regulations such as the General Data Protection Regulation (GDPR)3 in the European Union and the California Consumer Privacy Act (CCPA)4. Moreover, in 2022, China introduced the \u201cInternet Information Service Algorithmic Recommendation"}, {"title": "1.1 Our Contributions", "content": "Our contributions can be categorized into two main areas: methodological advancements and theoretical developments.\n\nMethodological Contributions: We propose the first privacy-preserving dynamic assortment selection policy via the MNL contextual bandit framework that satisfies JDP.\nWe highlight that addressing the MNL model requires novel solutions, as existing privacy-preserving methods for bandit problems based on linear and generalized linear models (GLM) are not applicable to this setting. For instance, privacy-preserving linear contextual bandit studied in Shariff and Sheffet (2018) extensively relies on a tree-based aggregation (Chan et al., 2011) which continuously releases private sum statistics, leveraging that its UCB has sufficient statistics in the form of sum statistics. However, this approach is infeasible for our MNL bandits due to the lack of such explicit sufficient statistics. On the other hand, while GLM bandit studied in Chen et al. (2022) relies on private MLE through objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012), there is no established technique in objective perturbation that is applicable to our multinomial models. To bridge this gap, we develop a novel objective perturbation technique tailored to MNL bandits, which is also of independent interest in the field of privacy.\nWe reformulate the presentation of Joint Differential Privacy (JDP) to minimize the noise required for privacy protection in our MNL bandit model. Unlike traditional DP, which has various formulations (Bun and Steinke, 2016; Dong et al., 2022; Su, 2024), JDP has traditionally relied on a likelihood ratio-based definition that is less efficient in online settings. In online learning, it is often necessary to compose multiple privacy mechanisms to safeguard newly arising sensitive information. This challenge is particularly pronounced in our multinomial models, where privacy concerns involve multiple items simultaneously, in contrast to linear contextual bandits or GLM bandits, which focus on a single item at a time. To tackle this issue, we employ R\u00e9nyi divergence (R\u00e9nyi, 1961) to establish tighter"}, {"title": "1.2 Related Literature", "content": "Our work is closely related to dynamic assortment selection and differentially private contextual bandits literature. In the following, we review these two areas and highlight how our approach differs from existing studies.\nDynamic Assortment Selection: The dynamic assortment problem with unknown customer preferences began with Caro and Gallien (2007). While noncontextual MNL models have been extensively studied (Rusmevichientong et al., 2010; Saur\u00e9 and Zeevi, 2013; Agrawal et al., 2017, 2019; Chen and Wang, 2018), recent works focus on personalized contexts (Cheung and Simchi-levi, 2017; Chen et al., 2020; Ou et al., 2018; Oh and Iyengar, 2019, 2021; Lee et al., 2024; Lee and Oh, 2024). UCB-based policies for MNL contextual bandits were first introduced by Chen et al. (2020), where UCB was applied to the expected revenue of each possible assortment. In a different approach, Oh and Iyengar (2021) applied UCB to each item's utility individually, selecting the assortment that maximizes the expected revenue based on these optimistic utility estimates. Despite inherent privacy concerns, privacy in dynamic assortment selection remains unexplored. We address this gap by building on Oh and Iyengar (2021) to develop the first private dynamic assortment selection algorithm. Unlike Lei et al. (2023) which deals with offline personalized assortment, our method handles online and adaptive assortments.\nDifferentially Private Contextual Bandits: The literature on private contextual bandits is rapidly emerging, with a focus on both linear and generalized linear bandits. There are two primary privacy regimes in this context. The first is the Central DP regime, where a trusted agent manages raw user data and injects noise to ensure privacy. On the other hand, the Local DP regime (Zheng et al., 2020; Han et al., 2021; Chen et al., 2022) assumes no trusted agent; users themselves perturb the signal before communicating with the central server, preventing the server from accessing raw data. Our paper considers the central DP. In the Central DP, perturbed UCB-based policies have been studied for both linear contextual bandit (Shariff and Sheffet, 2018) and generalized linear bandit (Chen et al., 2022; Su et al., 2023). Our work is closely related to Chen et al. (2022), which studies GLM bandits in the Central DP regime using UCB-based strategy. However, our multinomial model introduces higher"}, {"title": "1.3 Notation", "content": "Throughout this paper, we denote [T] = {1,2,\u2026\u2026,T} for any positive integer T. For a vector x \u2208 \\mathbb{R}^d, ||x|| denotes its l\u2082-norm. The weighted l\u2082-norm with respect to a positive-definite matrix V is defined by ||x||_V = \\sqrt{x^TVx}. The minimum and maximum eigenvalues of a symmetric matrix V are written as \\lambda_{min}(V) and \\lambda_{max}(V), respectively. For two positive sequences {a_n}_{n\u22651} and {b_n}_{n>1}, we say a_n = O(b_n) if a_n \u2264 Cb_n for some positive constant C for all large n. We let \\tilde{O}(\u00b7) represent the same meaning of O(\u00b7) except for ignoring a log factor. In addition, a_n = \u03a9(b_n) if a_n \u2265 Cb_n for some positive constant C for all large n."}, {"title": "2 Problem Setting", "content": "In this section, we first introduce the problem of dynamic assortment selection in a MNL contextual bandit setting, describing the modeling of user preferences and the objective of minimizing the expected cumulative regret. We then formalize the privacy requirements by defining DP and JDP using the R\u00e9nyi divergence, discuss their application to JDP, and present key theoretical results that facilitate the design of a privacy-preserving policy."}, {"title": "2.1 Multinomial Logit Contextual Bandit", "content": "At each round t \u2208 [T], a customer comes to the platform and the agent observes feature vector x_{ti} \u2208 \\mathbb{R}^d for all items i \u2208 [N] which contains contextual information on both customer and item. Using the given information as well as the historical interactions until t \u2013 1 period, the agent offers an assortment set S_t = {i_1, ..., i_K} \u2208 S that consists of K items out of N total items among the set of candidate assortments S. Then the agent observes the user's purchase decision c_t \u2208 S_t \u222a {0}, where {0} denotes the case the user does not purchase any item offered in S_t. This decision serves as feedback for the agent to update its knowledge on users' demand on items.\nTo model users' preference, we consider a widely adopted multinomial logit (MNL) choice model (McFadden, 1972), where the choice probability for item i_k \u2208 S_t\u222a {0} is defined as\np(i_k | S_t, \\theta^*) = \\frac{\\exp {x_{ti_k}^T\\theta^*}}{1 + \\sum_{j \\in S_t}\\exp {x_{tj}^T\\theta^*}},\\quad p(0 | S_t, \\theta^*) = \\frac{1}{1 + \\sum_{j \\in S_t}\\exp {x_{tj}^T\\theta^*}},\nwhere \\theta^* is a true time-invariant parameter unknown to the agent. Note that x_{ti}^T\\theta^* can be understood as t^{th} user's utility parameter for product i. The MNL model assumes that choice response for items in S_t is sampled from the following multinomial distribution:\nY_t = (Y_{t0}, Y_{t1}, ..., Y_{ti_K}) \\sim Multi\\{1, p_t(0|S_t, \\theta^*), p_t(i_1|S_t, \\theta^*), ..., p_t(i_K|S_t,\\theta^*)\\\\}.\nMoreover, we denote the noise \\epsilon_{ti} = Y_{ti} - p_t(i|S_t, \\theta^*) for i \u2208 S_t\u222a {0} and t \u2208 [T]. As \\epsilon_{ti} is a bounded random variable, it is \\gamma\u00b2-subgaussian random variable with \\gamma\u00b2 = 1/4.\nFollowing Oh and Iyengar (2021), we assume the revenue r_{ti}'s are public information to the agent and satisfy |r_{t,i}| \u2264 1. The expected revenue of the assortment S_t is given by\nR_t(S_t, \\theta^*) = \\sum_{i \\in S_t}r_{ti}p_t(i|S_t, \\theta^*).\nFor a dynamic assortment selection policy that sequentially decides the assortment S_t \u2208 S, its performance is measured by expected cumulative regret which is the gap between the expected revenue of this policy and that of the optimal assortments in hindsight:\nR_T = E\\bigg[ \\sum_{t=1}^T\\big(R_t(S_t^*, \\theta^*) - R_t(S_t, \\theta^*)\\big)\\bigg],"}, {"title": "2.2 Joint Differential Privacy", "content": "Unlike the existing literature in MNL contextual bandit (Chen et al., 2020; Oh and Iyengar, 2019, 2021; Lee et al., 2024; Lee and Oh, 2024), we aim to design a private dynamic assortment selection policy that satisfies JDP which, while being a relaxed form of DP, still provides strong enough protection against inference attack. Both JDP and DP require that outputs from two neighboring datasets, differing by only one entry, have similar distributions. In bandit problems with a finite time horizon T, a sequence of T users constitutes the dataset U = {u_t}_{t=1}^T, where each user u_t \u2208 U is characterized by their context, the prescribed assortment, and their purchase decision, {\\{x_{ti}\\}_{i\u2208[N]}, S_t, Y_t}. To provide privacy for any t^{th} user, we consider neighboring datasets as those that differ by a single user at t while sharing the remaining T \u2013 1 users.\nDefinition 1. (t-neighboring datasets). Two datasets U \u2208U^T and U' \u2208 U^T are said to be t-neighboring if they differ only in their t-th entry.\nDefinition 2. A contextual bandit algorithm M : U^T \u2192 S^T satisfies \\rho-zero concentrated differential privacy (\\rho-zCDP) (Bun and Steinke, 2016) if for any t-neighboring user sequences U and U',\nD_\\alpha (M(U)||M(U')) \u2264 \\rho_\\alpha, \\text{ for all } \\alpha > 1,\nwhere D_\\alpha(P||Q) = \\frac{1}{\\alpha - 1}\\log \\big( \\sum_E P(E)^\\alpha Q(E)^{(1-\\alpha)} \\big) is the R\u00e9nyi divergence of order \u03b1 of the distribution P from the distribution Q.\nOn the other hand, M satisfies \\rho-joint zCDP if for any t-neighboring user sequences U and U',\nD_\\alpha (M_{-t}(U)||M_{-t}(U')) \u2264 \\rho_\\alpha, \\text{ for all } \\alpha > 1,"}, {"title": "3 Methodology", "content": "In this section, we present a privacy-preserving MNL bandit policy DPMNL (Algorithm 1), supported by two essential subroutines: PrivateMLE (Algorithm 2) and PrivateCov (Algorithm 3). Figure 3 illustrates the overall mechanism design. PrivateMLE satisfies \\rho_1-zCDP by constructing a private MLE for the model parameter \\theta^*, allowing x_{ti}^T\\theta_{to} to serve as the point estimate of the utility of the i^{th} item for user t. PrivateCov, on the other hand,"}, {"title": "3.1 Main Algorithm: DPMNL", "content": "We present the detailed explanation on DPMNL algorithm, which balances exploration and exploitation using a perturbed optimistic utility approach while ensuring privacy. The algorithm operates in two main phases: a pure exploration phase followed by an exploration-exploitation phase driven by the perturbed UCB strategy."}, {"title": "3.2 Intuivie Overview on Perturbed UCB Strategy", "content": "Our perturbed UCB strategy addresses the exploration-exploitation tradeoff in a privacy-preserving manner by introducing calibrated noises into the utility estimates. The utility for user t and item i \u2208 [N] in the UCB algorithm is given by:\nz_{ti} = \\underbrace{x_{ti}^T\\hat{\\theta}_{t-1}}_{\\text{point estimate}} + \\underbrace{\\alpha_t||x_{ti}||_{V_t^{-1}}}_{\\text{exploration bonus}},\nwhere \\hat{\\theta}_{t-1} is the perturbed MLE estimate of the true parameter \\theta^*, and the exploration bonus is essential for balancing exploration and exploitation.\nCompared to the non-private UCB, both the point estimate and the exploration bonus are perturbed due to privacy constraints, thus incurring a \u201cprivacy cost.\u201d The point estimate, for instance, requires a longer exploration phase T\u2080 due to smaller values of \\rho\u2081, which demands more time to gather sufficient information for reliable estimates. Additionally, the MLE itself has a higher estimation error compared to its non-perturbed counterpart. This is reflected in the confidence bound \u03b1\u209c, which increases as \u03c1\u2081 decreases, showing that there is a larger uncertainty in the point estimate compared to the non-private setting.\nThe exploration bonus plays a crucial role in balancing the trade-off between exploration and exploitation by allocating a smaller bonus to well-explored items and a larger bonus to under-explored ones. To demonstrate this, we decompose V_{t-1} as follows:\nV_{t-1} = \\sum_{t-1} + N_{t-1} + 2\\lambda I,\nwhere \\sum_{t-1} = \\sum_{n=1}^{t-1} \\sum_{i \\in S_n} x_{ni}x_{ni}^T is the non-perturbed Gram matrix, and N_{t-1} is the noisy matrix added by PrivateCov (Algorithm 3) to satisfy \\rho\u2082-zCDP. Further spectral decomposition yields that\n\\alpha_t || x_{ti} ||_{V_{t-1}^{-1}} = \\alpha_t \\sqrt{x_{ti}^T(\\sum_{t-1} + N_{t-1} + 2\\lambda I)^{-1} x_{ti}} = \\alpha_t \\sqrt{x_{ti}^T(\\sum_{t-1} + N_{t-1} + 2\\lambda I)^{-1} x_{ti}} \\approx \\alpha_t \\sqrt{\\frac{(x_{vi})^2}{\\lambda_i+\\eta} +\u2026 + \\frac{(x_{iv})^2}{\\lambda_v+\\eta}},\nwhere v_i are the eigenvectors of V_{t-1}, \\lambda_1 \u2265 \\lambda_2 \u2265 \u2026 \u2265 \\lambda_d > 0 are the eigenvalues of \\sum_{t-1}, and \u03b7 represents the eigenvalues of the perturbed matrix N_{t-1} + 2\\lambda I.\nThis decomposition explicitly reveals the cost of privacy. Without the injected noise, the context vector x_{ti} would project onto the eigenvectors of the true Gram matrix \\sum_{t-1}, scaled by the eigenvalues. For well-explored context vectors, projections onto larger eigenvectors would result in smaller exploration bonuses, as these are scaled by larger eigenvalues. However, since we use the perturbed matrix V_{t-1}, the projections are instead taken onto the eigenvectors of the noisy matrix to protect user's privacy. Furthermore, the scaling by \u03bb\u2081 + \u03b7 inflates the exploration bonus uniformly, particularly when \u03bb\u1d62 is smaller than \u03b7, reducing the algorithm's precision in distinguishing between well-explored and under-explored directions.\nAdditionally, the confidence width \u03b1\u209c, which depends on both \u03c1\u2081 and \u03c1\u2082, prolongs the exploration phase, potentially leading to extended periods of suboptimal decision-making and a higher regret. Nevertheless, as exploration progresses, the exploration bonus diminishes, eventually converging to zero as \u03bb\u1d62 grows with increasing t. Once sufficient exploration has been conducted, the perturbed MLE closely approximates the true parameter \u03b8*, enabling effective exploitation of the learned preferences. Consequently, we can expect the regret to approach the rate seen in non-private settings in sufficiently large time horizon T."}, {"title": "3.3 Design and Analysis of Privacy Algorithms", "content": "In this section, we provide detailed explanations on the two subroutines PrivateMLE and PrivateCov used in Algorithm 1."}, {"title": "3.3.1 Private MLE via Objective Perturbation", "content": "We derive a differentially private MLE using the objective perturbation framework. The objective perturbation algorithm solves:\n\\hat{\\theta}(Z) = \\mathop{\\text{argmin}}_{\\theta \\in \\Theta} L(\\theta; Z, b) = \\mathop{\\text{argmin}}_{\\theta \\in \\Theta} \\sum_{z \\in Z} l(\\theta; z) + \\frac{\\Delta}{2} ||\\theta||^2 + b^T \\theta,\\qquad(2)\nwhere b ~ N(0, \u03c3\u00b2I) and the parameters \u2206 and \u03c3\u00b2 are chosen to satisfy the privacy guarantee.\nObjective perturbation is a well-established technique for satisfying DP in optimization problems, yet existing methods, whether for (\u03b5, \u03b4)-DP (Chaudhuri et al., 2011; Kifer et al., 2012) or \u03c1-zCDP (Redberg et al., 2024), are not directly applicable to our multinomial model due to their assumption that the hessian of the objective function has a rank of at most 1. This condition, applicable in generalized linear models, does not hold in the multinomial logit model, where the Hessian has a higher rank. In addition to this limitation, current methods also diverge from the requirements of our setting. The approaches developed by Chaudhuri et al. (2011) and Kifer et al. (2012), while satisfying (\u03b5, \u03b4)-DP, do not satisfy \u03c1-zCDP that we aim to achieve. Furthermore, the method proposed by Redberg et al. (2024), though aligned with \u03c1-zCDP, is formulated for unbounded DP, whereas we require bounded DP.\nTo address these limitations, we develop a novel objective perturbation algorithm that satisfies bounded \u03c1-zCDP and is applicable to multinomial models.\nTheorem 1. (Bounded \u03c1-zCDP guarantee of objective perturbation). Let l(\u03b8; z) be convex and twice-differentiable with ||\u2207l(\u03b8,z)||\u2082 \u2264 L and the eigenvalues of \u2207\u00b2l(\u03b8; z) is upper bounded by \u03b7 for all \u03b8 \u2208 \u0398 and z \u2208 Z. In addition, let R be the rank of the hessian matrix of l(\u03b8; z). Then the objective perturbation in Equation (2) satisfies \u03c1-zCDP when \u0394 > \\frac{\\eta}{exp((1-q)\\rho/R)-1} and"}, {"title": "3.3.2 Private Cov via Tree-based Aggregation", "content": "PrivateCov in Algorithm 3 is designed to continuously output a sequence of noisy gram matrices while controlling the noise accumulation over time. Given the set S_t and the context vectors {x_{ti}}_{i\u2208S_t}, Algorithm 3 continuously updates a binary tree structure to efficiently update and release privatized Gram matrices."}, {"title": "4 Regret Analysis", "content": "Regret analysis evaluates the performance of bandit policies by comparing their outcomes to those of an optimal policy with full knowledge of the environment. In our setting, regret"}, {"title": "Assumption 1.", "content": "Each feature vector x_{ti} is drawn i.i.d from an unknown distribution p_x, with ||x_{ti}|| \u2264 1 for all t,i and there exists a constant \u03c3\u2080 > 0 such that \\lambda_{min} (E[x_{ti}x_{ti}^T]) \u2265 \u03c3\u2080."}, {"title": "Assumption 2.", "content": "There exists \u03ba > 0 such that for every item i \u2208 S and any S \u2208 S and all round t, \\min_{||\u03b8 - \u03b8^*|| \u2264 1} p_t(i|S, \u03b8)p_t(0|S, \u03b8) \u2265 \u03ba."}, {"title": "Theorem 4.", "content": "Under Assumptions 1 and 2, the expected cumulative regret of our DPMNL with T\u2080 = \\frac{1}{\\kappa} \\big( \\sqrt{\\frac{C_1\\sqrt{d} + C_2\\sqrt{2\\log T}}{\\sigma\u2080}} \\big)^2 + \\frac{2 C_{p_1}T}{\\kappa \\sigma\u2080}, where C\u2081 and C\u2082 are some constants and C_{p_1,T} is defined in Lemma S9, can be bounded as:\nR_T \u2264 \\tilde{O} \\bigg( \\Big(d + \\frac{d^{5/2}}{\\rho_1} + \\frac{d^{3/4}}{\\rho_2^{1/4}} \\Big) \\sqrt{T} \\bigg).\nTheorem 4 provides a simplified regret bound for our privacy-preserving algorithm. The explicit exact upper bound of R_T is provided in Equation (S5) of Proof of Theorem 4.\nFor MNL contextual bandits with a d-dimensional contextual vector, Chen et al. (2020) established an \u03a9(d\u221aT/K) lower bound on the cumulative regret, where K is the maximum number of displayed items. Treating K as constant, this simplifies to \u03a9(d\u221aT), and our result matches this optimal rate in T, confirming that our algorithm achieves near-optimal performance despite the privacy constraints.\nThe regret bound further reflects the impact of the privacy parameters \u03c1\u2081 and \u03c1\u2082. The term d^{5/2}/\u03c1\u2081 indicates that PrivateMLE has a stronger influence on the regret than that of PrivateCov, which contributes d^{3/4}/\u03c1\u2082^{1/4}. This suggests that a greater share of the privacy budget should be allocated to PrivateMLE, as the accuracy of the MLE becomes more crucial in later stages when exploration diminishes and exploitation becomes dominant.\nFinally, the dependency on d is stronger in our private setting due to the noise added to mask the sensitive information contained in the contextual vectors. The need to ensure JDP requires noise that scales with the dimension of the context vector, leading to a larger influence of d on the regret.\nRemark 3. It is remarkable that the regret in Theorem 4 has no dependency on the total number of items N. This is because the context information is used via d \u00d7 d gram matrices and the noise is injected into the gram matrices. In this regard, a naive local DP algorithm that utilizes the perturbed context for all N items would result in the regret that depends on N. It is prohibitive as N is typically large in the real application of assortment selection."}, {"title": "5 Numerical Study", "content": "In this section, we evaluate the cumulative regret of our DPMNL policy across varying scenarios, including different privacy parameters \u03c1\u2081, \u03c1\u2082, dimension of contextual vectors d, and capacity of the assortment K. We begin by examining it using synthetic data in Section 5.1 and subsequently validate its practicability in a real-world dataset in Section 5.2."}, {"title": "5.1 Synthetic Data Analysis", "content": "In this section, we empirically examine the trade-off between privacy parameters and cumulative regret, as well as the effect of privacy budget allocation as outlined in Theorem 4. Additionally, we investigate the dependencies on the dimension of the contextual vector d and the assortment size K. Since this work introduces the first privacy-preserving MNL contextual bandit, there is no direct comparison available in the existing literature. As a result, we construct a benchmark by extending the privacy-preserving GLM bandit approach studied in Chen et al. (2022), adapting it to satisfy (\u03b5, \u03b4)-JDP in the multinomial model. A significant challenge in adapting existing methods for generalized linear models is that there is no existing objective perturbation result for multinomial model that satisfies (\u03b5, \u03b4)-DP. To address this, we develop such a mechanism, detailed in Theorem 5 of the Appendix. This adaptation allows for a fair comparison between our \u03c1-zCDP-based definition and the benchmark (\u03b5, \u03b4)-JDP policy.\nThroughout the experiment, the total number of items is fixed at N = 100, and we assign uniform revenues r_{ti} = 1 for all items i and time steps t. This setup aligns with the objective of maximizing the online click-through rate, as discussed in Oh and Iyengar (2021). Additionally, we use c\u03b1_t in place of \u03b1_t, with c = 10\u207b\u2074, to uniformly reduce the magnitude of the exploration bonus across all items. This adjustment enables the observation of asymptotic behavior even within a shorter time horizon. The dimension of the contextual vector, d, is set to d = 5 unless we are specifically investigating dependency on d. The contextual vectors are generated from an i.i.d. multivariate Gaussian distribution N(0, I_d), and the true parameter \u03b8* is sampled from a uniform distribution over [0,1]^d. Similarly, the assortment size K is set to K = 10 unless K is being varied to examine its dependency. For each experimental configuration, we perform 30 independent replicates and report the average cumulative regret"}, {"title": "5.2 Real Data Analysis", "content": "In this section, we evaluate our algorithm on the \"Expedia Hotel\" dataset (Adam et al., 2013) to assess its real-world performance. Protecting privacy in hotel booking data is critical, as these records can reveal sensitive details such as travel destinations, preferences, and financial status. Moreover, booking habits might expose personal routines like frequent business trips or vacations. Implementing private mechanisms can help prevent profiling, targeted marketing, and more serious risks like identity theft, thereby upholding ethical standards and fostering trust in the platform."}, {"title": "5.2.1 Data Pre-processing", "content": "The dataset consists of 399,344 unique searches on 23,715 search destinations each accompanied by a recommendation of maximum 38 hotels from a pool of 136,886 unique properties. User responses are indicated by clicks or hotel room purchases. The dataset also includes features for each property-user pair, including hotel characteristics such as star ratings and location attractiveness, as well as user attributes such as average hotel star rating and prices from past booking history.\nWe follow the pre-processing procedure in Lee et al. (2024) to address missing values and conduct feature transformation to aviod outliers. The details are provided in Section S.3.4 of the Appendix. After pre-processing, we have T = 4465 unique searches encompassing N = 124 different hotels, with d\u2082 = 10 hotel features and d\u2081 = 18 user features, where the description is given in Table 1. We normalize each feature to have mean 0 and variance 1."}, {"title": "5.2.2 Analysis of Expedia Dataset", "content": "The experiment setup includes a scaled confidence width c\u03b1_t with c = 10\u207b\u2077 and a pure exploration phase of T\u2080 = 10,000 during the total time horizion T = 100,000. Each plot of Figure 8 shows the average cumulative regret across 10 independent runs. In our implementation, we allocate 90% of the privacy budget to PrivateMLE and 10% to PrivateCov, based on our findings from synthetic data experiments.\nIn Figure 8, DPMNL consistently outperforms the benchmark across all \u03c1 values, with the performance gap widening as \u03c1 increases. For smaller \u03c1, neither policy has yet exhibited a sublinear pattern in regret, suggesting that longer time horizon T would be needed to further accentuate the differences. However, with larger privacy budgets (\u03c1 = 1, 5), DPMNL demonstrates a clear sublinear regret, while the benchmark does not. DPMNL also exhibits significantly lower regret, confirming its superior effectiveness.\nThese results demonstrate that DPMNL performs robustly across both simulated and real-world data, underscoring its practicality. By leveraging \u03c1-joint zCDP in the bandit framework, DPMNL achieves better privacy-utility trade-offs than existing approaches, establishing it as a strong choice for privacy-preserving algorithms in personalized recommendation systems."}]}