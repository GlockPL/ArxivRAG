[{"title": "Privacy-Preserving Dynamic Assortment Selection", "authors": ["Young Hyun Cho", "Will Wei Sun"], "abstract": "With the growing demand for personalized assortment recommendations, concerns over data privacy have intensified, highlighting the urgent need for effective privacy-preserving strategies. This paper presents a novel framework for privacy-preserving dynamic assortment selection using the multinomial logit (MNL) bandits model. Our approach employs a perturbed upper confidence bound method, integrating calibrated noise into user utility estimates to balance between exploration and exploitation while ensuring robust privacy protection. We rigorously prove that our policy satisfies Joint Differential Privacy (JDP), which better suits dynamic environments than traditional differential privacy, effectively mitigating inference attack risks. This analysis is built upon a novel objective perturbation technique tailored for MNL bandits, which is also of independent interest. Theoretically, we derive a near-optimal regret bound of O(\u221aT) for our policy and explicitly quantify how privacy protection impacts regret. Through extensive simulations and an application to the Expedia hotel dataset, we demonstrate substantial performance enhancements over the benchmark method.", "sections": [{"title": "1 Introduction", "content": "Assortment optimization has become a crucial element of online retail platforms, significantly influencing product recommendation on e-commerce websites, marketing campaigns, and promotional slots. The primary objective is to select a subset of items that maximizes expected revenue based on a user's choice model. With the advent of big data and the widespread adoption of machine learning algorithms, personalized recommendations have become feasible, enabling companies to present relevant products to the right audience and significantly enhancing potential revenue. In a recent McKinsey report\u00b9, consumer demand for personalization is growing, with 71% of consumers expecting personalized interactions and 76% expressing frustration when these expectations are not met.\n The literature on revenue management increasingly addresses the development of assortment optimization algorithms in online retail, commonly referred to as dynamic assortment selection. This involves dynamically optimizing and adapting product offerings in response to changing conditions, leveraging customer data such as past purchases, browsing history, demographics, and other relevant information. However, the rapid growth of personalized recommendations has also led to heightened concerns about privacy (Bi and Shen, 2023; Su, 2024). A KPMG survey\u00b2 revealed that 70% of companies have increased their collection of personal consumer data, while 86% of individuals expressed growing concerns about data privacy. Additionally, 40% of individuals do not trust companies to use their data ethically, and 30% are unwilling to share personal data for any reason. These concerns have prompted stringent regulations such as the General Data Protection Regulation (GDPR)\u00b3 in the European Union and the California Consumer Privacy Act (CCPA)4. Moreover, in 2022, China introduced the \u201cInternet Information Service Algorithmic Recommendation"}, {"title": "1.1 Our Contributions", "content": "Our contributions can be categorized into two main areas: methodological advancements and theoretical developments.\n Methodological Contributions: We propose the first privacy-preserving dynamic assortment selection policy via the MNL contextual bandit framework that satisfies JDP. We highlight that addressing the MNL model requires novel solutions, as existing privacy-preserving methods for bandit problems based on linear and generalized linear models (GLM) are not applicable to this setting. For instance, privacy-preserving linear contextual bandit studied in Shariff and Sheffet (2018) extensively relies on a tree-based aggregation (Chan et al., 2011) which continuously releases private sum statistics, leveraging that its UCB has sufficient statistics in the form of sum statistics. However, this approach is infeasible for our MNL bandits due to the lack of such explicit sufficient statistics. On the other hand, while GLM bandit studied in Chen et al. (2022) relies on private MLE through objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012), there is no established technique in objective perturbation that is applicable to our multinomial models. To bridge this gap, we develop a novel objective perturbation technique tailored to MNL bandits, which is also of independent interest in the field of privacy.\n We reformulate the presentation of Joint Differential Privacy (JDP) to minimize the noise required for privacy protection in our MNL bandit model. Unlike traditional DP, which has various formulations (Bun and Steinke, 2016; Dong et al., 2022; Su, 2024), JDP has traditionally relied on a likelihood ratio-based definition that is less efficient in online settings. In online learning, it is often necessary to compose multiple privacy mechanisms to safeguard newly arising sensitive information. This challenge is particularly pronounced in our multinomial models, where privacy concerns involve multiple items simultaneously, in contrast to linear contextual bandits or GLM bandits, which focus on a single item at a time. To tackle this issue, we employ R\u00e9nyi divergence (R\u00e9nyi, 1961) to establish tighter"}, {"title": "1.2 Related Literature", "content": "Our work is closely related to dynamic assortment selection and differentially private contextual bandits literature. In the following, we review these two areas and highlight how our approach differs from existing studies.\n Dynamic Assortment Selection: The dynamic assortment problem with unknown customer preferences began with Caro and Gallien (2007). While noncontextual MNL models have been extensively studied (Rusmevichientong et al., 2010; Saur\u00e9 and Zeevi, 2013; Agrawal et al., 2017, 2019; Chen and Wang, 2018), recent works focus on personalized contexts (Cheung and Simchi-levi, 2017; Chen et al., 2020; Ou et al., 2018; Oh and Iyengar, 2019, 2021; Lee et al., 2024; Lee and Oh, 2024). UCB-based policies for MNL contextual bandits were first introduced by Chen et al. (2020), where UCB was applied to the expected revenue of each possible assortment. In a different approach, Oh and Iyengar (2021) applied UCB to each item's utility individually, selecting the assortment that maximizes the expected revenue based on these optimistic utility estimates. Despite inherent privacy concerns, privacy in dynamic assortment selection remains unexplored. We address this gap by building on Oh and Iyengar (2021) to develop the first private dynamic assortment selection algorithm. Unlike Lei et al. (2023) which deals with offline personalized assortment, our method handles online and adaptive assortments.\n Differentially Private Contextual Bandits: The literature on private contextual bandits is rapidly emerging, with a focus on both linear and generalized linear bandits. There are two primary privacy regimes in this context. The first is the Central DP regime, where a trusted agent manages raw user data and injects noise to ensure privacy. On the other hand, the Local DP regime (Zheng et al., 2020; Han et al., 2021; Chen et al., 2022) assumes no trusted agent; users themselves perturb the signal before communicating with the central server, preventing the server from accessing raw data. Our paper considers the central DP. In the Central DP, perturbed UCB-based policies have been studied for both linear contextual bandit (Shariff and Sheffet, 2018) and generalized linear bandit (Chen et al., 2022; Su et al., 2023). Our work is closely related to Chen et al. (2022), which studies GLM bandits in the Central DP regime using UCB-based strategy. However, our multinomial model introduces higher"}, {"title": "1.3 Notation", "content": "Throughout this paper, we denote $[T] = \\{1,2,\\ldots,T\\}$ for any positive integer $T$. For a vector $x \\in \\mathbb{R}^d$, $||x||$ denotes its $l_2$-norm. The weighted $l_2$-norm with respect to a positive-definite matrix $V$ is defined by $||x||_V = \\sqrt{x^TVx}$. The minimum and maximum eigenvalues of a symmetric matrix $V$ are written as $\\lambda_{\\min}(V)$ and $\\lambda_{\\max}(V)$, respectively. For two positive sequences $\\{a_n\\}_{n\\geq 1}$ and $\\{b_n\\}_{n>1}$, we say $a_n = O(b_n)$ if $a_n \\leq Cb_n$ for some positive constant $C$ for all large $n$. We let $\\tilde{O}(\\cdot)$ represent the same meaning of $O(\\cdot)$ except for ignoring a log factor. In addition, $a_n = \\Omega(b_n)$ if $a_n \\geq Cb_n$ for some positive constant $C$ for all large $n$."}, {"title": "2 Problem Setting", "content": "In this section, we first introduce the problem of dynamic assortment selection in a MNL contextual bandit setting, describing the modeling of user preferences and the objective of minimizing the expected cumulative regret. We then formalize the privacy requirements by defining DP and JDP using the R\u00e9nyi divergence, discuss their application to JDP, and present key theoretical results that facilitate the design of a privacy-preserving policy."}, {"title": "2.1 Multinomial Logit Contextual Bandit", "content": "At each round $t \\in [T]$, a customer comes to the platform and the agent observes feature vector $x_{ti} \\in \\mathbb{R}^d$ for all items $i \\in [N]$ which contains contextual information on both customer and item. Using the given information as well as the historical interactions until $t \\text{ \u2013 }1$ period, the agent offers an assortment set $S_t = \\{i_1, ..., i_K\\} \\in \\mathcal{S}$ that consists of $K$ items out of $N$ total items among the set of candidate assortments $\\mathcal{S}$. Then the agent observes the user's purchase decision $c_t \\in S_t \\cup \\{0\\}$, where $\\{0\\}$ denotes the case the user does not purchase any item offered in $S_t$. This decision serves as feedback for the agent to update its knowledge on users' demand on items.\n To model users' preference, we consider a widely adopted multinomial logit (MNL) choice model (McFadden, 1972), where the choice probability for item $i_k \\in S_t \\cup \\{0\\}$ is defined as\n $$p(i_k \\mid S_t, \\theta^*) = \\frac{\\exp \\{x_{ti_k}^T\\theta^*\\}}{1 + \\sum_{j \\in S_t} \\exp \\{x_{tj}^T\\theta^*\\}}, \\quad p(0 \\mid S_t, \\theta^*) = \\frac{1}{1 + \\sum_{j \\in S_t} \\exp \\{x_{tj}^T\\theta^*\\}},$$\n where $\\theta^*$ is a true time-invariant parameter unknown to the agent. Note that $x_{ti}^*$ can be understood as $t^{th}$ user's utility parameter for product $i$. The MNL model assumes that choice response for items in $S_t$ is sampled from the following multinomial distribution:\n $$Y_t = (Y_{t0}, Y_{t1}, ..., Y_{ti_K}) \\sim Multi\\{1, p_t(0|S_t, \\theta^*), p_t(i_1|S_t, \\theta^*), ..., p_t(i_K|S_t,\\theta^*)\\}$$\n Moreover, we denote the noise $\\epsilon_{ti} = Y_{ti} - p_t(i|S_t, \\theta^*)$ for $i \\in S_t \\cup \\{0\\}$ and $t \\in [T]$. As $\\epsilon_{ti}$ is a bounded random variable, it is $\\gamma^2$-subgaussian random variable with $\\gamma^2 = 1/4$.\n Following Oh and Iyengar (2021), we assume the revenue $r_{ti}$'s are public information to the agent and satisfy $|r_{t,i}| \\leq 1$. The expected revenue of the assortment $S_t$ is given by\n $$R_t(S_t, \\theta^*) = \\sum_{i \\in S_t} r_{ti}p_t(i|S_t, \\theta^*).$$ \n For a dynamic assortment selection policy that sequentially decides the assortment $S_t \\in \\mathcal{S}$, its performance is measured by expected cumulative regret which is the gap between the expected revenue of this policy and that of the optimal assortments in hindsight:\n $$R_T = \\mathbb{E} \\bigg[ \\sum_{t=1}^T \\big(R_t(S_t^*, \\theta^*) - R_t(S_t, \\theta^*)\\big) \\bigg],$$"}, {"title": "2.2 Joint Differential Privacy", "content": "Unlike the existing literature in MNL contextual bandit (Chen et al., 2020; Oh and Iyengar, 2019, 2021; Lee et al., 2024; Lee and Oh, 2024), we aim to design a private dynamic assortment selection policy that satisfies JDP which, while being a relaxed form of DP, still provides strong enough protection against inference attack. Both JDP and DP require that outputs from two neighboring datasets, differing by only one entry, have similar distributions. In bandit problems with a finite time horizon $T$, a sequence of $T$ users constitutes the dataset $\\mathcal{U} = \\{u_t\\}_{t=1}^T$, where each user $u_t \\in \\mathcal{U}$ is characterized by their context, the prescribed assortment, and their purchase decision, $(\\{x_{ti}\\}_{i\\in [N]}, S_t, Y_t)$. To provide privacy for any $t^{th}$ user, we consider neighboring datasets as those that differ by a single user at $t$ while sharing the remaining $T \\text{ \u2013 }1$ users.\n Definition 1. (t-neighboring datasets). Two datasets $\\mathcal{U} \\in \\mathcal{U}^T$ and $\\mathcal{U}' \\in \\mathcal{U}^T$ are said to be t-neighboring if they differ only in their t-th entry.\n Definition 2. A contextual bandit algorithm $\\mathcal{M} : \\mathcal{U}^T \\rightarrow \\mathcal{S}^T$ satisfies $\\rho$-zero concentrated differential privacy ($\\rho$-zCDP) (Bun and Steinke, 2016) if for any t-neighboring user sequences $\\mathcal{U}$ and $\\mathcal{U}'$,\n $$\\mathbb{D}_\\alpha (\\mathcal{M}(\\mathcal{U})||\\mathcal{M}(\\mathcal{U}')) \\leq \\rho_\\alpha, \\text{ for all } \\alpha > 1,$$\n where $\\mathbb{D}_\\alpha (P||Q) = \\frac{1}{\\alpha - 1} \\log \\big(\\sum_E P(E)^\\alpha Q(E)^{(1-\\alpha)}\\big)$ is the R\u00e9nyi divergence of order $\\alpha$ of the distribution $P$ from the distribution $Q$.\n On the other hand, $\\mathcal{M}$ satisfies $\\rho$-joint zCDP if for any t-neighboring user sequences $\\mathcal{U}$ and $\\mathcal{U}'$,\n $$\\mathbb{D}_\\alpha (\\mathcal{M}_{-t}(\\mathcal{U})||\\mathcal{M}_{-t}(\\mathcal{U}')) \\leq \\rho_\\alpha, \\text{ for all } \\alpha > 1,$$"}, {"title": "3 Methodology", "content": "In this section, we present a privacy-preserving MNL bandit policy DPMNL (Algorithm 1), supported by two essential subroutines: PrivateMLE (Algorithm 2) and PrivateCov (Algorithm 3). Figure 3 illustrates the overall mechanism design. PrivateMLE satisfies $\\rho_1$-zCDP by constructing a private MLE for the model parameter $\\theta^*$, allowing $x_{ti}^T \\hat{\\theta}_{t-1}$ to serve as the point estimate of the utility of the $i^{th}$ item for user $t$. PrivateCov, on the other hand,"}, {"title": "3.1 Main Algorithm: DPMNL", "content": "We present the detailed explanation on DPMNL algorithm, which balances exploration and exploitation using a perturbed optimistic utility approach while ensuring privacy. The algorithm operates in two main phases: a pure exploration phase followed by an exploration-exploitation phase driven by the perturbed UCB strategy."}, {"title": "3.2 Intuivie Overview on Perturbed UCB Strategy", "content": "Our perturbed UCB strategy addresses the exploration-exploitation tradeoff in a privacy-preserving manner by introducing calibrated noises into the utility estimates. The utility for user $t$ and item $i \\in [N]$ in the UCB algorithm is given by:\n $$z_{ti} = \\underbrace{x_{ti}^T \\hat{\\theta}_{t-1}}_{\\text{point estimate}} + \\underbrace{\\alpha_t ||x_{ti}||_{V_t^{-1}}}_{\\text{exploration bonus}},$$\n where $\\hat{\\theta}_{t-1}$ is the perturbed MLE estimate of the true parameter $\\theta^*$, and the exploration bonus is essential for balancing exploration and exploitation.\n Compared to the non-private UCB, both the point estimate and the exploration bonus are perturbed due to privacy constraints, thus incurring a \u201cprivacy cost.\u201d The point estimate, for instance, requires a longer exploration phase $T_0$ due to smaller values of $\\rho_1$, which demands more time to gather sufficient information for reliable estimates. Additionally, the MLE itself has a higher estimation error compared to its non-perturbed counterpart. This is reflected in the confidence bound $\\alpha_t$, which increases as $\\rho_1$ decreases, showing that there is a larger uncertainty in the point estimate compared to the non-private setting."}, {"title": "3.3 Design and Analysis of Privacy Algorithms", "content": "In this section, we provide detailed explanations on the two subroutines PrivateMLE and PrivateCov used in Algorithm 1."}, {"title": "3.3.1 Private MLE via Objective Perturbation", "content": "We derive a differentially private MLE using the objective perturbation framework. The objective perturbation algorithm solves:\n $$\\hat{\\theta}(Z) = \\underset{\\theta \\in \\Theta}{\\text{argmin }} \\mathcal{L}(\\theta; Z, b) = \\underset{\\theta \\in \\Theta}{\\text{argmin }} \\sum_{z \\in Z} l(\\theta; z) + \\frac{\\Delta}{2}||\\theta||_2^2 + b^T \\theta,$$\n where $b \\sim \\mathcal{N}(0, \\sigma^2 I)$ and the parameters $\\Delta$ and $\\sigma^2$ are chosen to satisfy the privacy guarantee.\n Objective perturbation is a well-established technique for satisfying DP in optimization problems, yet existing methods, whether for $(\\epsilon, \\delta)$-DP (Chaudhuri et al., 2011; Kifer et al., 2012) or $\\rho$-zCDP (Redberg et al., 2024), are not directly applicable to our multinomial model due to their assumption that the hessian of the objective function has a rank of at most 1. This condition, applicable in generalized linear models, does not hold in the multinomial logit model, where the Hessian has a higher rank. In addition to this limitation, current methods also diverge from the requirements of our setting. The approaches developed by Chaudhuri et al. (2011) and Kifer et al. (2012), while satisfying $(\\epsilon, \\delta)$-DP, do not satisfy $\\rho$-zCDP that we aim to achieve. Furthermore, the method proposed by Redberg et al. (2024), though aligned with $\\rho$-zCDP, is formulated for unbounded DP, whereas we require bounded DP.\n To address these limitations, we develop a novel objective perturbation algorithm that satisfies bounded $\\rho$-zCDP and is applicable to multinomial models.\n Theorem 1. (Bounded $\\rho$-zCDP guarantee of objective perturbation). Let $l(\\theta; z)$ be convex and twice-differentiable with $|\\nabla l(\\theta,z)||_2 \\leq L$ and the eigenvalues of $\\nabla^2 l(\\theta; z)$ is upper bounded by $\\eta$ for all $\\theta \\in \\Theta$ and $z \\in Z$. In addition, let $R$ be the rank of the hessian matrix of $l(\\theta; z)$. Then the objective perturbation in Equation (2) satisfies $\\rho$-zCDP when $\\Delta > \\frac{\\eta R}{\\exp((\\frac{q}{1-q}) \\rho) - 1}$ and $\\sigma^2 \\geq \\big(\\frac{L(\\sqrt{d}+2q\\rho + \\sqrt{d})}{\\Delta q}\\big)^2$, for any $q \\in (0,1)$."}, {"title": "3.3.2 Private Cov via Tree-based Aggregation", "content": "PrivateCov in Algorithm 3 is designed to continuously output a sequence of noisy gram matrices while controlling the noise accumulation over time. Given the set $S_t$ and the context vectors $\\{x_{ti}\\}_{i\\in S_t}$, Algorithm 3 continuously updates a binary tree structure to efficiently update and release privatized Gram matrices."}, {"title": "4 Regret Analysis", "content": "Regret analysis evaluates the performance of bandit policies by comparing their outcomes to those of an optimal policy with full knowledge of the environment. In our setting, regret"}, {"title": "Assumption 1.", "content": "Each feature vector $x_{ti}$ is drawn i.i.d from an unknown distribution $p_x$, with $||x_{ti}|| \\leq 1$ for all $t,i$ and there exists a constant $\\sigma_0 > 0$ such that $\\lambda_{min} (\\mathbb{E}[x_{ti}x_{ti}^T]) \\geq \\sigma_0$."}, {"title": "Assumption 2.", "content": "There exists $\\kappa > 0$ such that for every item $i \\in S$ and any $S \\in \\mathcal{S}$ and all round $t, \\min_{||e-\\theta^*||\\leq 1} p_t(i|S, \\theta)p_t(0|S, \\theta) \\geq \\kappa$."}, {"title": "Theorem 4.", "content": "Under Assumptions 1 and 2, the expected cumulative regret of our DPMNL with $T_0 = \\frac{1}{K} \\Big( \\frac{C_1\\sqrt{d}+C_2\\sqrt{2\\log T}}{\\sigma_0} \\Big)^2 + \\frac{2C_{\\rho_1}T}{K\\sigma_0}$, where $C_1$ and $C_2$ are some constants and $C_{\\rho_1,T}$ is defined in Lemma S9, can be bounded as:\n $$R_T \\leq \\tilde{O} \\Bigg( \\bigg( d + \\frac{d^{5/2}}{\\rho_1} + \\frac{d^{3/4}}{\\rho_2^{1/4}} \\bigg) \\sqrt{T} \\Bigg).$$"}, {"title": "5 Numerical Study", "content": "In this section, we evaluate the cumulative regret of our DPMNL policy across varying scenarios, including different privacy parameters $\\rho_1, \\rho_2$, dimension of contextual vectors $d$, and capacity of the assortment $K$. We begin by examining it using synthetic data in Section 5.1 and subsequently validate its practicability in a real-world dataset in Section 5.2."}, {"title": "5.1 Synthetic Data Analysis", "content": "In this section, we empirically examine the trade-off between privacy parameters and cumulative regret, as well as the effect of privacy budget allocation as outlined in Theorem 4. Additionally, we investigate the dependencies on the dimension of the contextual vector $d$ and the assortment size $K$. Since this work introduces the first privacy-preserving MNL contextual bandit, there is no direct comparison available in the existing literature. As a result, we construct a benchmark by extending the privacy-preserving GLM bandit approach studied in Chen et al. (2022), adapting it to satisfy $(\\epsilon, \\delta)$-JDP in the multinomial model. A significant challenge in adapting existing methods for generalized linear models is that there is no existing objective perturbation result for multinomial model that satisfies $(\\epsilon, \\delta)$-DP.\n To address this, we develop such a mechanism, detailed in Theorem 5 of the Appendix. This adaptation allows for a fair comparison between our $\\rho$-zCDP-based definition and the benchmark $(\\epsilon, \\delta)$-JDP policy.\n Throughout the experiment, the total number of items is fixed at $N = 100$, and we assign uniform revenues $r_{ti} = 1$ for all items $i$ and time steps $t$. This setup aligns with the objective of maximizing the online click-through rate, as discussed in Oh and Iyengar (2021). Additionally, we use $c \\alpha_t$ in place of $\\alpha_t$, with $c = 10^{-4}$, to uniformly reduce the magnitude of the exploration bonus across all items. This adjustment enables the observation of asymptotic behavior even within a shorter time horizon. The dimension of the contextual vector, $d$, is set to $d = 5$ unless we are specifically investigating dependency on $d$. The contextual vectors are generated from an i.i.d. multivariate Gaussian distribution $\\mathcal{N}(0, I_d)$, and the true parameter $\\theta^*$ is sampled from a uniform distribution over $[0,1]^d$. Similarly, the assortment size $K$ is set to $K = 10$ unless $K$ is being varied to examine its dependency. For each experimental configuration, we perform 30 independent replicates and report the average cumulative regret"}, {"title": "5.2 Real Data Analysis", "content": "In this section, we evaluate our algorithm on the \"Expedia Hotel\" dataset (Adam et al., 2013) to assess its real-world performance. Protecting privacy in hotel booking data is critical, as these records can reveal sensitive details such as travel destinations, preferences, and financial status. Moreover, booking habits might expose personal routines like frequent business trips or vacations. Implementing private mechanisms can help prevent profiling, targeted marketing, and more serious risks like identity theft, thereby upholding ethical standards and fostering trust in the platform."}, {"title": "5.2.1 Data Pre-processing", "content": "The dataset consists of 399,344 unique searches on 23,715 search destinations each accompanied by a recommendation of maximum 38 hotels from a pool of 136,886 unique properties. User responses are indicated by clicks or hotel room purchases. The dataset also includes features for each property-user pair, including hotel characteristics such as star ratings and location attractiveness, as well as user attributes such as average hotel star rating and prices from past booking history.\n We follow the pre-processing procedure in Lee et al. (2024) to address missing values and conduct feature transformation to aviod outliers. The details are provided in Section S.3.4 of the Appendix. After pre-processing, we have $T = 4465$ unique searches encompassing $N = 124$ different hotels, with $d_2 = 10$ hotel features and $d_1 = 18$ user features, where the description is given in Table 1. We normalize each feature to have mean 0 and variance 1."}, {"title": "5.2.2 Analysis of Expedia Dataset", "content": "The experiment setup includes a scaled confidence width $c \\alpha_t$ with $c = 10^{-7}$ and a pure exploration phase of $T_0 = 10,000$ during the total time horizion $T = 100,000$. Each plot of Figure 8 shows the average cumulative regret across 10 independent runs. In our implementation, we allocate 90% of the privacy budget to PrivateMLE and 10% to PrivateCov, based on our findings from synthetic data experiments.\n These results demonstrate that DPMNL performs robustly across both simulated and real-world data, underscoring its practicality. By leveraging $\\rho$-joint zCDP in the bandit framework, DPMNL achieves better privacy-utility trade-offs than existing approaches, establishing it as a strong choice for privacy-preserving algorithms in personalized recommendation systems."}, {"title": "S.1.1 Proof of Lemma 1: Billboard Lemma", "content": "Let $\\mathcal{U}$ and $\\mathcal{U}'$ be the $t$-neighboring databases so that $\\Pi_i\\mathcal{U} = \\Pi_i\\mathcal{U}'$ for all $i \\neq t$ and $\\mathcal{M}$ be a mechanism that satisfies $\\rho$-zCDP. It is sufficient to show that for all $\\alpha > 1$,\n$$\\mathbb{D}_\\alpha (\\mathcal{M}_{-t}(\\mathcal{U})||\\mathcal{M}_{-t}(\\mathcal{U}')) \\leq \\rho_\\alpha.$$\n We use the following property:\n Lemma S3. (Invariance to Post-processing(Bun and Steinke, 2016)) Let $P$ and $Q$ be distributions on $\\Omega$ and let $f : \\Omega \\rightarrow \\Theta$ be a function. Let $f(P)$ and $f(Q)$ be the distributions on $\\Theta$ induced by applying $f$ to $P$ or $Q$ respectively. Then\n $$\\mathbb{D}_\\alpha (f(P)||f(Q)) \\leq \\mathbb{D}_\\alpha (P||Q) .$$\n Note then $\\mathcal{M}_{-t}(\\mathcal{U})$ can be regarded as $\\mathcal{M}_{-t}(\\mathcal{U}) = f (\\mathcal{M}(\\mathcal{U}); \\mathcal{U}_{-t})$ for some function $f$, where $\\mathcal{U}_{-t}$ is treated as constant since it is known for the adversaries. Therefore,\n$$\\mathbb{D}_\\alpha (\\mathcal{M}_{-t}(\\mathcal{U})||\\mathcal{M}_{-t}(\\mathcal{U}')) = \\mathbb{D}_\\alpha (f(\\mathcal{M}(\\mathcal{U}))||f(\\mathcal{M}(\\mathcal{U}')))$$\n$$\\leq \\mathbb{D}_\\alpha (\\mathcal{M}(\\mathcal{U})||\\mathcal{M}(\\mathcal{U}')) \\leq \\rho_\\alpha,$$\n where the first inequality is by Lemma S3.\n Intuitively, $\\mathcal{U}_{-t} := {\\Pi_j\\mathcal{U}\\}_{j\\neq t} = {\\Pi_j\\mathcal{U}'\\}_{j\\neq t}$ is already a public knowledge for adversaries and does not contain the target user $t$. Therefore, adversaries can't gain further information on user $t$ by the transformation using $\\mathcal{U}_{-t}$."}, {"title": "S.1.2 Proof of Theorem 1: Objective Perturbation", "content": "Recall the objective perturbation is\n$$\\mathcal{L}(\\theta; Z, b) = \\sum_{z \\in Z} l(\\theta; z) + \\frac{\\Delta}{2}||\\theta||_2^2 + b^T \\theta,$$\n and $\\hat{\\theta}(Z)$ is the minimizer, where $b \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n Let $\\alpha > 1, q \\in (0,1)$ be given and take $Z, Z'$ be neighboring datasets that share $T \\text{ \u2013 }1$ individuals but only differ in one individual. Without loss of generality, we take T-neighboring"}, {"title": "S.1.3 Proof of Corollary 1: Privacy Guarantee of PrivateMLE", "content": "It is sufficient to identity $L, \\eta$ and $R$ in our multinomial model.\n Identifying $L$: bound on the gradient. We begin with rewriting the log-likelihood with denoting the chosen item at time $t$ as $i^*$. Therefore, $Y_{ti^*} = 1$ and $Y_{ti} = 0$ for $i \\neq i^*$ and\n$$l_t(\\theta) = - \\log \\bigg( \\frac{\\exp(x_{ti^*}^T \\theta)}{1 + \\sum_{j \\in S_t} \\exp(x_{tj}^T \\theta)} \\bigg)."}, {"title": "S.1.4 Proof of Theorem 2: Privacy Guarantee of PrivateCov", "content": "For every node in the binary tree, we inject a noisy matrix by the gaussian mechanism:\n Lemma S4. (Gaussian mechanism, Lemma 12, (Steinke, 2022)) Let $q : \\mathcal{X}^n \\rightarrow \\mathbb{R}^d$ have sensitivity $\\Delta$, that is, $||q(X)-q(X')||_2 \\leq \\Delta$ for all $X, X' \\in \\mathcal{X}^n$ such that $X, X'$ differ in a single record. Let $\\sigma > 0$. Define a randomized algorithm $\\mathcal{M} : \\mathcal{X}^n \\rightarrow \\mathbb{R}^d$ by $\\mathcal{M}(X) = \\mathcal{N}(q(X), \\sigma^2 I_d)$. Then $\\mathcal{M}$ is $\\rho$-zCDP for $\\rho = \\frac{\\Delta^2}{2 \\sigma^2}$."}, {"title": "S.1.5 Proof of Theorem 3: Privacy Guarantee of DPMNL", "content": "Finally, we provide the privacy guarantee of our main policy DPMNL.\n Step 1. Privacy guarantee of perturbed optimistic utilities\n To apply Lemma 1, note that the perturbed optimistic utility estimate on each item for all time horizon $\\{\\{z_{ti}\\}_{i\\in [N", "T": ""}, "is a function of true context vectors of the given user and outputs of two privacy subroutines PrivateMLE and PrivateCov that satisfy $\\rho_1$-zCDP and $\\rho_2$-zCDP, respectively. Additionally, the composition of PrivateMLE and PrivateCov satisfies $\\rho_1 + \\rho_2$-zCDP by Lemma S17.\n Therefore, by Lemma 1, $\\{\\{z_{ti}\\}_{i\\in [N"]}, {}]