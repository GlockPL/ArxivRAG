{"title": "A Self-Supervised Framework for Improved Generalisability\nin Ultrasound B-mode Image Segmentation", "authors": ["Edward Ellis", "Andrew Bulpitt", "Nasim Parsa", "Michael F Byrne", "Sharib Ali"], "abstract": "Ultrasound (US) imaging is clinically invaluable due to\nits noninvasive and safe nature. However, interpreting US images is\nchallenging, requires significant expertise, and time, and is often prone\nto errors. Deep learning offers assistive solutions such as segmentation.\nSupervised methods rely on large, high-quality, and consistently labeled\ndatasets, which are challenging to curate. Moreover, these methods\ntend to underperform on out-of-distribution data, limiting their clinical\nutility. Self-supervised learning (SSL) has emerged as a promising\nalternative, leveraging unlabeled data to enhance model performance and\ngeneralisability. We introduce a contrastive SSL approach tailored for B-\nmode US images, incorporating a novel Relation Contrastive Loss (RCL).\nRCL encourages learning of distinct features by differentiating positive\nand negative sample pairs through a learnable metric. Additionally, we\npropose spatial and frequency-based augmentation strategies for the\nrepresentation learning on US images.\nOur approach significantly outperforms traditional supervised segmen-\ntation methods across three public breast US datasets, particularly in\ndata-limited scenarios. Notable improvements on the Dice similarity met-\nric include a 4% increase on 20% and 50% of the BUSI dataset, nearly\n6% and 9% improvements on 20% and 50% of the BrEaST dataset, and\n6.4% and 3.7% improvements on 20% and 50% of the UDIAT dataset,\nrespectively. Furthermore, we demonstrate superior generalisability on\nthe out-of-distribution UDIAT dataset with performance boosts of 20.6%\nand 13.6% compared to the supervised baseline using 20% and 50% of\nthe BUSI and BrEaST training data, respectively. Our research highlights\nthat domain-inspired SSL can improve US segmentation, especially under\ndata-limited\nconditions.", "sections": [{"title": "I. INTRODUCTION", "content": "US imaging provides a non-invasive, portable and low-cost imaging\nsolution for clinicians compared to alternative imaging modalities,\nsuch as CT and MRI. US is a popular and widely used imaging tool\nwithin clinical practice for imaging various organs, such as cardiac,\nbreast and abdominal examination [1]. However, US imaging suffers\nfrom large variability in image quality and is considered a highly\noperator-dependent procedure [2]. Data acquisition and interpretation\nof US images require significant operator skill, is time-consuming\nand erroneous. Using recent advancements in artificial intelligence\n(AI) approaches can assist clinicians in identifying key anatomical\nfeatures to support clinical diagnosis, reduce human-related errors\nand minimise inter-operator variability.\nDeep learning (DL) approaches for US image segmentation have\nbeen extensively researched in many clinical domains. State-of-the-\nart supervised learning approaches have been developed using both\nrecent transformers and convolutional neural network (CNN) based\narchitectures [3]\u2013[5]. Furthermore, memory banks have been used in\nsupervised learning to store image features [5] or class features [6],\nproviding additional information to enhance image segmentation\nthrough cross-image feature aggregation [5], [6]. However, supervised"}, {"title": "II. RELATED WORK", "content": "AI techniques in US B-mode imaging are typically centred around\nDL model development within a supervised learning framework,\noften tailored to a specific clinical application. SOTA approaches for\nsegmentation in US imaging have developed from pure convolutional\napproaches using CNNs. CNN-based approaches often use variants of\nU-Net [19]. Shareef et al. [20] introduced the ESTAN network aimed\nat improving small breast tumour segmentation. ESTAN uses two\nencoder branches with different kernel shapes and sizes and three skip\nconnections to improve multi-scale contextual information. Banerjee\net al. [21] proposed SIU-Net for segmenting lumbar and thoracic\nbony features using their proposed inception block. This block uses\nmultiple filter sizes with improved computational efficiency through\ncombined 1 \u00d7 1 and 3 \u00d7 3 convolutions. They also combined features\nof multiple scales through dense skip connections. Meshram et\nal. [22] improved carotid plaque segmentation using dilated convo-\nlutional layers in a U-Net model and Qiao et al. [23] used dilated\nconvolution and squeeze excitation blocks on skip connections to\nimprove fetal skull segmentation.\nWith advancements in transformer-based architectures, the latest\nSOTA approaches for US segmentation combine transformer and\nconvolutional methods leveraging complementary global and local\nfeature information, respectively. Zhang et al. [4] used a CNN-\nTransformer combination in a U-Net framework. The authors used\na ResNet backbone and a novel local-global transformer block\nnested into skip connections to capture long-range feature information\nefficiently for breast US segmentation. Jiang et al. [3] also explored\na CNN-Transformer U-Net model to improve US segmentation of\nthe breast, thyroid and left ventricle. The authors used a coordinate\nresidual block to extract local feature information with absolute\nposition information and enhanced channel self-attention blocks to\nextract global features. Wu et al. [5] introduced a BUSSeg model for\nbreast US segmentation by using a parallel bi-encoder in a U-Net\nstyle architecture consisting of a transformer and CNN blocks. In\naddition, the authors use a cross-image dependency module to capture\ncross-image long-range dependencies utilising feature memory banks.\nThese supervised learning models have demonstrated promising\nsegmentation results across various US clinical domains. However,\ndue to the limited availability of US data, self-supervised learning\n(SSL) offers a more robust approach, enabling high segmentation\nperformance while minimizing performance degradation when ap-\nplied to out-of-distribution data. Also, combined transformer and\nconvolutional approaches [3]\u2013[5] often require higher computational\ntraining and inference time that hinders clinical translation, whereas\nSSL techniques are independent of the model choice and can boost\nperformance significantly."}, {"title": "B. Self-Supervised Learning", "content": "Self-supervised learning often follows a two-stage training strat-\negy. Firstly, pretext learning is focused on learning representations\nfrom unlabelled data. Secondly, these learnt weights are then used\nin the fine-tuning downstream supervised learning tasks, such as\nsegmentation. The objective is to learn semantically meaningful\nfeature representations without requiring labels, thereby improving\nthe performance of a downstream task on limited labelled datasets.\nSince the labels are not required during the pretext task, a large\nnumber of available unlabelled samples can be used which makes\nthe SSL approach more generalisable to out-of-distribution samples.\nThe pretext learning task is critical for developing meaningful\nrepresentations of the target domain in SSL [12]. Often a combination\nof image augmentation benefits contrastive SSL pretext learning,\nwith this unsupervised learning stage also benefiting from stronger\naugmentation than supervised learning [18]. For example, geometric\nrotation transformation was applied to an image in [24] while the\nJigsaw pretext task with a set of shuffled patches within an image\nwas used in [17]. The Jigsaw approach provides a strong geometric\ntransformation to an image, a common strategy used in contrastive\nSSL [11], [25], [26]. The traditional Jigsaw approach learns a rep-\nresentation that is covariant to the perturbation, the pretext-invariant\nrepresentation learning (PIRL) [16] approach adopts the Jigsaw task\nin an invariant learning strategy.\nSeveral SSL approaches have been established involving gener-\native, contrastive and generative-contrastive techniques applied to\nmedical image analysis [15]. The pretext learning strategy differs\nfor each approach with a generative task focused on reconstruction,\nfor example, recovering masked areas of an image [27]. However,\nthe contrastive approach aims to discriminate similar and dissimilar\nsamples [16].\nContrastive approaches are often preferred to generative ap-\nproaches for downstream discriminative applications [15]. By avoid-\ning low-level abstraction objectives, such as pixel-level reconstruc-\ntion, contrastive learning tends to be more lightweight, as it does not\nrequire a decoder during pretext learning [15].\nWidely known contrastive learning approaches include MoCo\nv3 [28], PIRL [16] and BYOL [29]. MoCo v3 was introduced using\na momentum encoder. Using a dynamic dictionary and a moving\naverage encoder allows key feature representations to be decoupled\nfrom the minibatch size resulting in a large consistent dictionary,\ncontaining many negative samples [28]. BYOL was introduced using\ntwo interacting neural networks to learn from each other. The online\nnetwork predicts the target network representation, both using the\nsame image, but under different augmented views [29]. PIRL intro-\nduced a method to learn invariant representations rather than covariant\nrepresentations to the pretext task used [16]. These approaches\ndemonstrate improved performance in self-supervised learning."}, {"title": "C. Metric Learning", "content": "Metric learning aims to learn a function to effectively compare\nsimilarities between data samples. Siamese networks [30] was used\nto learn a similarity function to map input pairs into a shared\nembedding space. The network was trained to bring similar pairs\ntogether and push dissimilar pairs apart using a linear distance\nmetric, e.g., Euclidean distance. Prototypical networks [31] learn\nan embedding that is a non-linear transformation of the input data,\nmapping it into an embedding space where the nearest neighbour\nclassification is effective. The classification is based on the proximity\nof query instances to class prototypes in this learned embedding\nspace. Relation Networks [32] use an embedding module to obtain\nsample feature embeddings and a relation module to compute sample\npair similarity. Unlike [30], [31], the addition of the relation modules\nenables learning of similarity metrics in a data-driven way.\nOur work focuses on contrastive learning because it excels in\ndiscriminative downstream applications, is more lightweight during\npretext learning, and favours high-level abstract feature learning\ncompared to generative approaches [15]. With contrastive learning\ndependent upon data transformations in the pretext learning task,\nwe utilise a combination of data-specific augmentations, shown\nto improve SSL feature learning [18]. In this work, we explore\nnovel combined spatial and frequency-based augmentation strategies\naimed at US images to improve representation learning in US data."}, {"title": "III. METHODOLOGY", "content": "We propose a novel self-supervised framework that explores the\nimpact of a domain-inspired pretext task for US B-mode image data\nalong with a novel integration of relation networks for contrastive\nself-supervised learning (see Fig. 1). We also explore the impact\nof perceptual loss on its ability to focus on model understanding\nof high-level abstract features. Our pretext task explores a novel\ndata engineering Cross-patch Jigsaw strategy aimed at providing\nUS B-mode image data-specific augmentation to support distinctive\nand meaningful representation learning during the pretext task. In\naddition, we apply frequency augmentations to enhance the model's\nability to distinguish lesion patterns in low-contrast US images.\nWe propose using Relation Networks in contrastive SSL. Com-\nmonly, contrastive loss approaches, for example Noise Contrastive\nEstimation loss use a fixed metric like cosine similarity to deter-\nmine the similarity between samples. However, we employ relation\nnetworks to compute similarity in a learnable data-driven way. This\nallows us to model non-linear interactions between feature embed-\ndings enabling higher-order relationships to be computed beyond\nmeasuring just the linear alignment of feature embeddings from\ncomputing the cosine similarity. Our SSL framework complements\nlearning meaningful feature embeddings and a robust similarity score.\nSee Section III-C for more details."}, {"title": "A. Pretext Task", "content": "Our pretext tasks combine a spatial and frequency component,\noccurring in each iteration during training. The frequency domain of a\nUS image contains rich information on high-frequency texture vari-\nations and low-frequency tissue deformations. Our spatial transfor-\nmation builds upon the Jigsaw task used in the PIRL approach [16].\nThis Jigsaw task provides a strong transformation to the image that\nshows high performance in self-supervised learning. We apply our\nfrequency augmentation to a random cropped region of the image\nbefore applying our spatial patch transformation.\nWith real-world US images often\nsuffering from a range of artefacts and noise, e.g., reverberation\nartefacts, speckle noise, and harmonic distortions, we utilise band\nstop filtering in the frequency domain to increase model robustness\nto noise and artefacts. The frequency domain of a US image can be\nobtained by computing the 2D discrete Fourier (DFT):\n1) Frequency transformation:\n$F(u, v) = \\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1} f(x,y) \\cdot e^{-j2\\pi(\\frac{ux}{H}+\\frac{vy}{W})}$\nF(u,v) denotes the frequency domain value at coordinates (u, v),\nwhile f(x, y) denotes the spatial domain value at coordinates (x, y).\nHere, H and W are the height and width of pixels of the image,\nrespectively. According to Euler's formula ($e^{i\\theta} = cos \\theta + i sin \\theta$)\nEq. (1) can be written in the form:\n$F(u, v) = \\sum_{x=0}^{H-1}\\sum_{y=0}^{W-1}f(x,y) \\left[cos \\left(2 \\pi \\left(\\frac{ux}{H}+\\frac{vy}{W}\\right)\\right) - i sin \\left(2 \\pi \\left(\\frac{ux}{H}+ \\frac{vy}{W}\\right)\\right)\\right]$\nIn Eq. (2), real $F_r$ (cosine term) and imaginary part $F_i$ (sine term)\ncan be written as $F(u, v) = F_r(u, v)+iF_i(u, v)$. The amplitude and\nphase can be obtained from:\n$\\begin{cases}\n|F(u, v)| = \\sqrt{F_r(u, v)^2 + F_i(u, v)^2} \\\\\n\\angle F(u, v) = arctan\\left(\\frac{F_i(u, v)}{F_r(u, v)}\\right)\n\\end{cases}$\nThe amplitude and phase indicate the strength and position of\nfrequency components in the US image. We experiment with filtering\nfrequency components within a random cropped section of the US\nimage to distort textual information, whilst maintaining critical low-\nfrequency components related to structures in the image."}, {"title": "2) Spatial transformation:", "content": "A frequency augmented random\ncropped area of the image $I \\in \\mathbb{R}^{H \\times W}$ (see III-A1) is divided into 36\npatches (Fig.3). We select a random patch from these patches (shown\nin filled blue in Fig. 3), say $\\{P_{ij}\\}_{i=1,j=1}^{6,6}$, where $P_{ij}$ is the patch\nlocated at row i and column j. These are then used to determine two\nsets of patches, namely focal patches $P_f$ and non-focal patches $P_{nf}$.\n$P_f$ represents patches in the same row and column as the random\ninitial patch (outlined in blue in Fig. 3).\n$P_f = \\{P_{r,j} | j = 1, . . ., 6\\} \\cup \\{P_{i,c} | i = 1, ..., 6\\}.$"}, {"content": "$P_{nf}$ are the complement of focal patches (outlined in red in Fig. 3).\n$P_{nf} = \\{P_{ij} | i = 1, . . ., 6, j = 1,...,6\\}\\\\P_{nf}.$ All non-focal patches are shuffled ($P_{nf}$). Focal patches $P_f$ undergo\nhorizontal and vertical flips before each patch is rotated 180\u00b0 to\nensure visual coherence between focal patches.\n$\\widehat{P_f} = Rotate180^\\circ (Flipv (FlipH(Pf)))$ This spatial operation provides a strong transformation to non-\nfocal patches while weaker augmentation to the focal patches. This\noperation maintains partial layer-wise structure information within\nthe image. In Fig. 1, $\\{P_f, P_{nf}\\} \\in I_2$."}, {"title": "B. Feature Extraction", "content": "Consider an ultrasound B-mode image dataset $\\mathbb{U}_D$ consisting of\nN image samples in the training data, $\\mathbb{U}_D = \\{I_1, I_2, I_3, ..., I_N \\}$ and\na set of image transformations, $t \\in \\mathbb{T}$. Original images ($I \\in \\mathbb{U}_D$)\nundergo simple geometric and photometric transformations, referred\nto as $t_1$. These include random horizontal flipping, vertical flipping"}, {"title": "C. Novel Relation Contrastive Loss", "content": "We adopt a relation network (RN) [32] with network parameters $B$\n(Fig. 1) for pretext learning. Here, we proposed to compute relation"}, {"title": "IV. EXPERIMENTS", "content": "Three publicly available breast ultrasound datasets have been used\nin this study: BUSI [37], BrEaST [38] and UDIAT [39]. BUSI\nwas collected from 600 women aged 25-75 years old from Baheya\nHospital, Egypt. BUSI contains 780 anonymised images with 487\nbenign, 210 malignant, and 133 normal cases. Images were acquired\nusing a LOGIQ E9 Agile US system using a 1-5 MHz ML6-15-\nD linear probe. Ground truth mask annotations for all images were\nreviewed and adjusted by expert radiologists at Baheya Hospital.\nBrEaST dataset contains 256 anonymised breast ultrasound images\nfrom 256 patients (18-87 years old) collected in medical centres\nin Poland in 2019 2022. Benign (154 images), malignant (98\nimages), and normal (4 images) cases were collected. Four different\nUS systems and transducers were used in the acquisition.\nUDIAT dataset B contains 163 anonymised Breast Ultrasound\nimages collected and labelled at the UDIAT-Centre Diagnostic,\nCorporacio Parc Taul, Sabadell, Spain. Benign (110 images) and\nmalignant (53 images) breast cases were acquired using a Siemens\nACUSON Sequoia C512 US system with 17L5 HD linear array probe\n(8.5 MHz). A summary of all datasets with samples for training\n(train), validation (val) and test are outlined in Table I."}, {"title": "B. Experimental Setup", "content": "All methods were implemented using Pytorch and performed on a\nTesla V100 GPU. A summary of the settings for the pretext learning\ntask and the segmentation task is provided in Table I. We adjusted\nthe batch size (B) for experiments with different datasets, due to\nvariations in dataset sizes. To ensure experimental reproducibility, all\nexperiments were conducted using a random seed of 42. For pretext\nlearning we trained for 2000 epochs, using the last checkpoint for\ndownstream task training. An SGD optimiser is used with a learning\nrate optimized for each method, (see Table II). All input images were\nresized to 224 x 224 pixels and the ResNet50 model was pre-trained\non ImageNet before pretext learning.\nFor all downstream tasks, a polynomial Ir scheduler was used with\ninitial and final learning rates of $1e^{-3}$ and $1e^{-6}$, respectively, with\na learning rate decay of 0.9. An Adam optimiser and cross-entropy\nloss were used for all downstream tasks. Downstream training was\nrun for 500 epochs with convergence occurring below 200 epochs.\nWe set a stopping criteria with a patience of 50 after an initial 100\nepochs of training."}, {"title": "Hyperparameters:", "content": "We investigate an optimal $\\lambda$ weighting in\nEq. (13) for both RCL with perceptual loss and PIRL with perceptual\nloss. This is shown in section V-D, table VII, with $\\lambda$ = 0.1 and 0.75\nfor these methods respectively. Our memory bank settings are the\nsame as described in [16] with $m_w$ = 0.5 to compute the exponential\nmoving average. Furthermore, we use $w$ = 0.5 in Eq. (10). This gives\nan equal weighting to both $L_{CRCL}^{total}$ and $L_{patch}^{CRCL}$."}, {"title": "C. Evaluation Metrics", "content": "The metrics we used to evaluate segmentation performance\nCoefficient ($JC = \\frac{|Y_{pred}\\bigcap Y_{true}|}{|Y_{pred}\\bigcup Y_{true}|}$), Hausdorff distance (HD\n= $max(sup_{a\\in Y_{pred}} inf_{b\\in Y_{true}} d(a, b), sup_{b\\in Y_{true}} inf_{a\\in Y_{pred}} d(b,a))$),"}, {"title": "V. RESULTS", "content": "The Res-UNet model has been used as the baseline network\nfor supervised and self-supervised methods. We compare our novel\ndata-engineered pretext learning strategies and loss components with\nfully supervised Res-UNet [40] and baseline Jigsaw PIRL self-\nsupervised [16] approaches. The following abbreviations represent\nthe method variations in this paper: Jig refers to the Jigsaw pretext\ntask. CP-Jig denotes our Cross-patch Jigsaw spatial pretext task\nstrategy (see Section III-A2). Freq represents our frequency-based\naugmentation (see Section III-A1). Finally, percep corresponds to\nthe perceptual loss."}, {"title": "A. Quantitative Results on Individual datasets", "content": "This section outlines the segmentation results of our proposed\npretext learning method variations on each US dataset.\nUsing\nthe complete dataset during downstream training the top two performing\nmethods both use Jig+Freq, using the PIRL method and RCL+percep\nSSL methods. These configurations obtain a dice score of 0.910 and\n0.907, respectively, showing an improvement of 4.5% and 4% com-\npared to the supervised only (Res-UNet) baseline [40], but perform\nsimilarly to the PIRL (SSL) baseline [16] when 100% of training\ndata is used. These methods maintain the top two performances in\nthe JC and HD. Similar gains in JC are reported over the supervised\nonly method, whilst maintaining only slight improvement over the\nPIRL baseline. However, the Jig+Freq pretext task using the PIRL\nmethod shows improvements in HD by 37.6% and 14.2%, whilst the\nJig+Freq task using RCL+percep, also shows a decrease in HD by"}, {"title": "3) Performance on UDIAT dataset:", "content": "Table V presents the segmen-\ntation results on the held-out test set of the UDIAT dataset. The top\ntwo performing methods using all training samples are: PIRL+percep\nand RCL+percep, using Jig+Freq and CP-Jig+Freq tasks respectively.\nThe best performing approach: Jig+Freq with PIRL+percep achieves\nDSC, JC and HD of 0.918, 0.906 and 16.16, respectively. This is an\nimprovement of 5.1%, 5.3% and 40.5% compared to the supervised"}, {"title": "B. Quantitative Results on Generalisability", "content": "Table VI shows the segmentation performance of our method\nvariations on the held-out UDIAT dataset after training on combined\nBUSI and BrEaST datasets. All methods perform strongly when\ntrained on the full BUSI and BrEaST datasets. The supervised (Res-\nUNet [40]) baseline achieves 0.903, 0.889 and 19.19 in DSC, JC\nand HD, respectively. Our best approach using all training data is\nJig+Freq with PIRL+percep. This performs similarly to the Jig PIRL\n[16] baseline with a performance of 0.927, 0.915 and 13.46 in DSC,\nJC and HD, respectively. However, we observe a significant decrease\nin DSC performance by 23.6% in the supervised (Res-UNet [40])"}, {"title": "C. Qualitative Analysis", "content": "Segmentation predictions from small regular to large irregular\ntumour shapes for each approach is shown in Fig. 4 for our\ngeneralisability study. We can observe that all methods effectively"}, {"title": "D. Ablation Study", "content": "We include an ablation study to investigate the $\\lambda$ weighting\nfor methods with combined loss functions (i.e., PIRL+percep and\nRCL+percep) in Table VII. Ablation results are demonstrated on\nthe BUSI dataset using the validation set. We also use the baseline\npretext learning Jigsaw task for all ablation results. Our ablation\nresults demonstrate a $\\lambda$ value of 0.75 and 0.1 are optimal weights in\nPIRL+percep and RCL+percep, respectively."}, {"title": "VI. DISCUSSION", "content": "Supervised learning models have shown promising improvements\nin US image segmentation [3]-[5]. However, performance is signifi-\ncantly affected when training samples are limited [12]. Our results in\nall datasets demonstrate that the supervised baseline is consistently\namong the lowest performing methods when either 50% or 20% train-\ning samples are used. This finding suggests that our SSL approaches\ncan effectively learn meaningful representations for US data during\npretext learning that supports the downstream segmentation task\n(Table III-VI). Furthermore, supervised model generalisability is sig-\nnificantly impacted when training samples are limited. This is evident"}, {"title": "VII. CONCLUSION", "content": "To the best of our knowledge, this is the most comprehensive"}]}