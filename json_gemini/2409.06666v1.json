{"title": "LLAMA-OMNI: SEAMLESS SPEECH INTERACTION WITH LARGE LANGUAGE MODELS", "authors": ["Qingkai Fang", "Shoutao Guo", "Yan Zhou", "Zhengrui Ma", "Shaolei Zhang", "Yang Feng"], "abstract": "Models like GPT-40 enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), represented by ChatGPT (OpenAI, 2022), have become powerful general-purpose task solvers, capable of assisting people in daily life through conversational interactions. However, most LLMs currently only support text-based interactions, which limits their application in scenarios where text input and output are not ideal. Recently, the emergence of GPT-40 (OpenAI, 2024) has made it possible to interact with LLMs through speech, responding to user's instruction with extremely low latency and significantly enhancing the user experience. However, there is still a lack of exploration in the open-source community on building such speech interaction models based on LLMs. Therefore, how to achieve low-latency and high-quality speech interaction with LLMs is a pressing challenge that needs to be addressed.\nThe simplest way to enable speech interaction with LLMs is through a cascaded system based on automatic speech recognition (ASR) and text-to-speech (TTS) models, where the ASR model transcribes the user's speech instruction into text, and the TTS model synthesizes the LLM's response into speech. However, since the cascaded system sequentially outputs the transcribed text, text response, and speech response, the overall system tends to have higher latency. In contrast, some multimodal speech-language models have been proposed (Zhang et al., 2023; Rubenstein et al., 2023), which discretize speech into tokens and extend the LLM's vocabulary to support speech input and output. Such speech-language models theoretically can generate speech responses directly"}, {"title": "2 MODEL: LLAMA-OMNI", "content": "In this section, we introduce the model architecture of LLaMA-Omni. As shown in Figure 2, it consists of a speech encoder, a speech adaptor, an LLM, and a speech decoder. We denote the user's speech instruction, text response, and speech response as XS, YT, and YS respectively."}, {"title": "2.1 SPEECH ENCODER", "content": "We use the encoder of Whisper-large-v32 (Radford et al., 2023) as the speech encoder E. Whisper is a general-purpose speech recognition model trained on a large amount of audio data, and its encoder is capable of extracting meaningful representations from speech. Specifically, for the user's speech instruction XS, the encoded speech representation is given by H = E(XS), where H = [h1, ..., hv] is the speech representation sequence of length N. We keep the speech encoder's parameters frozen throughout the entire training process."}, {"title": "2.2 SPEECH ADAPTOR", "content": "To enable the LLM to comprehend the input speech, we incorporate a trainable speech adaptor A that maps the speech representations into the embedding space of the LLM. Following Ma et al. (2024c), our speech adaptor first downsamples the speech representations H to reduce the sequence length. Specifically, every k consecutive frames are concatenated along the feature dimension:\nH' = [h\u00b4,..., h(N/k)], , where h\u2081 = [hk\u00d7(i\u22121)+1 \u2295hkx(i\u22121)+2\u2295 \u2295hkxi].\n(1)\nNext, H' is passed through a 2-layer perceptron with ReLU activation between the linear layers, resulting in the final speech representation S. The above process can be formalized as follows:\nS = A(H) = Linear(ReLU(Linear(DownSample(H)))).\n(2)"}, {"title": "2.3 LARGE LANGUAGE MODEL", "content": "We use Llama-3.1-8B-Instruct\u00b3 (Dubey et al., 2024) as the LLM M, which is currently the state- of-the-art open-source LLM. It has strong reasoning capabilities and is well-aligned with human preferences. The prompt template P(\u00b7) is shown in Figure 3. The speech representation sequence S is filled into the position corresponding to , and then the entire sequence P(S) is input into the LLM. Finally, the LLM autoregressively generates the text response YT = [y,..., y] directly based on the speech instruction and is trained using cross-entropy loss:\nMLLLM = log P(y\\P(S), YZ).\n(3)"}, {"title": "2.4 SPEECH DECODER", "content": "For the speech response YS, we first follow Zhang et al. (2023) to discretize the speech into discrete units. Specifically, we use the pretrained HuBERT (Hsu et al., 2021) model to extract continuous representations of the speech, and then convert these representations into discrete cluster indices using a K-means model. Subsequently, consecutive identical indices are merged into a single unit, resulting in the final discrete unit sequence YU = [y], ..., y], y \u2208 {0,1, ..., K \u2212 1}, \u22001 < i < L, where K is the number of clusters, and L is the length of discrete unit sequence. The discrete units can be converted into waveform with an additional unit-based vocoder V (Polyak et al., 2021).\nTo generate speech responses simultaneously with text responses, we add a streaming speech de- coder D after the LLM. It consists of several standard Transformer (Vaswani et al., 2017) layers with the same architecture as LLaMA (Dubey et al., 2024), each containing a causal self-attention module and a feed-forward network. Similar to Ma et al. (2024a); Zhang et al. (2024b), the speech decoder runs in a non-autoregressive manner, which takes the output hidden states from the LLM as input, and generates the discrete unit sequence corresponding to the speech response. Specifically, the output hidden states corresponding to the text response are denoted as C = [c1, ..., cM], where c\u2081 = M(P(S), YZ\u2081). We first upsample each hidden state into a chunk by a factor of \u5165, resulting in an upsampled hidden state sequence = [C1, ...\u0109x.M], where \u0109\u2081 = C[i/x]. Next, \u0108 is fed into the speech decoder D, and the output hidden state sequence is denoted as O = [01, ..., 0x.M]. We use connectionist temporal classification (CTC; Graves et al., 2006a) to align O with the discrete unit sequence YU. Specifically, CTC extends the output space with a special blank token \u0454:\nP(a\u00bfO) = softmax(Woi + b)[ai], \u2200a\u017c \u2208 {0, 1, ..., K \u2212 1, \u20ac},\n(4)\nwhere W \u2208 R(K+1)\u00d7d and b \u2208 RK+1 are weights and biases of the linear layer, and the sequence A = [\u03b11, ..., \u03b1\u03bb.M] is known as the alignment. To model the variable-length mapping between input and output, CTC introduces a collapsing function \u1e9e(A), which first merges all consecutive repeated tokens in A and then eliminates all blank tokens e. For instance: \u03b2([1, 1, 2, \u20ac, \u20ac, 2, 3]) = [1, 2, 2, 3]. During training, CTC performs marginalization over all possible alignments as follows:\nLctc = \u2212 log P(YU|0) = \u2212 log \u2211 P(AO) = -log \u2211\u03a0P(a0),\n(5)\n\u0391\u03b5\u03b2-1(YU) \u0391\u20ac\u03b2-1(YU) i=1\nwhere B-1(YU) denotes all possible alignments of length \u5165\u00b7 M that can be collapsed to YU. The alignment is modeled in a non-autoregressive way. During inference, we select the best alignment A* = arg max\u0104 P(A|O), and apply the collapsing function to obtain the discrete unit sequence \u03b2(A*), which is then fed into the vocoder to synthesize waveform."}, {"title": "2.5 TRAINING", "content": "As shown in Figure 2, we adopt a two-stage training strategy for LLaMA-Omni. In the first stage, we train the model to generate text responses directly from the speech instructions. Specifically, the speech encoder is frozen, and the speech adaptor and the LLM are trained using the objective LLLM in Eq. (3). The speech decoder is not involved in training during this stage. In the second stage, we train the model to generate speech responses. During this stage, the speech encoder, speech adaptor, and LLM are all frozen, and only the speech decoder is trained using the objective Lctc in Eq. (5)."}, {"title": "2.6 INFERENCE", "content": "During inference, the LLM autoregressively generates the text response based on the speech instruction. Meanwhile, since our speech decoder uses causal attention, once the LLM generates a text response prefix YZ, the corresponding upsampled hidden states C\u22641.i can be fed into the speech decoder to generate a partial alignment A<x.i, which in turn yields the discrete units corresponding to the generated text prefix. To further enable streaming synthesis of speech waveforms, when the number of generated units reaches a pre-defined chunk size \u03a9, we input this unit segment into the vocoder to synthesize a speech segment, which is then immediately played to the user. As a result, users can start listening to the speech response without waiting for the complete text response to be generated, ensuring low response latency that is not affected by the length of the text response. Algorithm 1 describes the above process. Additionally, since the speech decoder uses non-autoregressive modeling, the alignment corresponding to each text token yt, specifically Ax.(i-1)+1:1\u00b7\u03af, is generated in parallel within the chunk. Therefore, the decoding speed for generating both text and speech simultaneously is not significantly different from the speed of generating text alone."}, {"title": "3 CONSTRUCTION OF SPEECH INSTRUCTION DATA: INSTRUCTS2S-200K", "content": "To train LLaMA-Omni, we need triplet data consisting of . However, most publicly available instruction data is in text form. Therefore, we construct speech instruction data based on existing text instruction data through the following process:\nStep 1: Instruction Rewriting Since speech input has different characteristics compared to text input, we rewrite the text instructions according to the following rules: (1) Add appropriate filler words (such as \"hey\", \"so\", \"uh\", \"um\", etc.) to the instructions to simulate natural speech patterns. (2) Convert non-text symbols in the instructions (such as numbers) into their corresponding spoken forms to ensure correct synthesis by TTS. (3) Modify the instructions to be relatively brief without excessive verbiage. We use the Llama-3-70B-Instruct\u2074 model to rewrite the instructions according to these rules. The prompt can be found in Appendix A.\nStep 2: Response Generation In speech interactions, existing responses from text instructions are not suitable for direct use as speech instruction responses. This is because, in text-based interactions, models tend to generate lengthy responses, using complex sentences and possibly including non-verbal elements like ordered lists or parentheses. However, in speech interactions, concise yet informative responses are typically preferred (Anonymous, 2024). Therefore, we use the Llama-3-70B-Instruct model to generate responses for speech instructions according to the following rules: (1) The response should not contain content that cannot be synthesized by the TTS model, such as parentheses, ordered lists, etc. (2) The response should be very concise and to the point, avoiding lengthy explanations. The prompt can be found in Appendix A.\nStep 3: Speech Synthesis After obtaining the instructions and responses suitable for speech interactions, we need to further convert them into speech using TTS models. For the instructions, to make the synthesized speech sound more natural, we use the Cosy Voice-300M-SFT (Du et al., 2024) model, randomly selecting either a male or female voice for each instruction. For the responses, we use the VITS (Kim et al., 2021) model trained on the LJSpeech (Ito & Johnson, 2017) dataset to synthesize the responses into a standard voice.\nFor the basic text instructions, we collect around 50K instructions from the Alpaca dataset7 (Taori et al., 2023), which covers a wide range of topics. Additionally, we gather around 150K instructions from the UltraChat dataset (Ding et al., 2023), which primarily consist of questions about the world. Note that UltraChat is a large-scale multi-turn conversation dataset, but we only select the first 150K entries and use only the first-round instruction. Using the above datasets and data processing pipeline, we ultimately obtain 200K speech instruction data, referred to as InstructS2S-200K."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUPS\nDatasets For the training data, we use the InstructS2S-200K dataset mentioned in Section 3, which includes 200K speech instruction data. To extract discrete units corresponding to the target speech, we use a pre-trained K-means quantizer, which has learned 1000 clusters from the HuBERT features. The pretrained HiFi-GAN vocoder (Kong et al., 2020; Polyak et al., 2021) is used to synthesize discrete units into waveform. For the evaluation data, we select two subsets from Alpaca-Eval10 (Li et al., 2023): helpful_base and vicuna, as their questions are more suitable for speech interaction scenarios. We remove questions related to math and code, resulting in a total of 199 instructions. To obtain the speech version, we use the CosyVoice-300M-SFT model to synthesize the instructions into speech. We refer to this test set as InstructS2S-Eval in the following sections.\nModel Configuration We use the encoder of Whisper-large-v3 as the speech encoder, and use Llama-3.1-8B-Instruct as the LLM. The speech adapter performs a 5\u00d7 downsampling on the speech representations. The speech decoder consists of 2 Transformer layers with the same architecture as LLaMA, with a hidden dimension of 4096, 32 attention heads, and a feed-forward network dimension of 11008, which contains 425M parameters. The upsample factor A is set to 25. For the minimum unit chunk size \u03a9 input to the vocoder, we set \u03a9 = +\u221e in the main experiment, meaning we wait for the entire unit sequence to be generated before inputting it to the vocoder for speech synthesis. In subsequent experiments, we will analyze how adjusting the value of \u03a9 can control response latency, as well as the trade-off between latency and speech quality.\nTraining LLaMA-Omni follows a two-stage training process. In the first stage, we train the speech adapter and the LLM with a batch size of 32 for 3 epochs. We use a cosine learning rate scheduler with the first 3% of steps for warmup, and the peak learning rate is set to 2e-5. In the second stage, we train the speech decoder, using the same batch size, number of steps, and learning rate scheduler as the first stage, but with the peak learning rate set to 2e-4. The entire training process takes approximately 65 hours on 4 NVIDIA L40 GPUs."}, {"title": "4.2 EVALUATION", "content": "Since LLaMA-Omni can generate both text and speech responses based on speech instructions, we evaluate the model's performance on two tasks: speech-to-text instruction-following (S2TIF) and speech-to-speech instruction-following (S2SIF). We use greedy search to ensure reproducible experimental results. The model is evaluated from the following aspects:\nChatGPT Score To evaluate the model's ability to follow speech instructions, we use GPT- 40 (OpenAI, 2024) to score the model's responses. For the S2TIF task, scoring is based on the transcribed text of the speech instructions and the model's text response. For the S2SIF task, we first transcribe the model's speech responses into text using the Whisper-large-v3 model, and then score it in the same manner as the S2TIF task. GPT-40 gives scores on two aspects: content and style. The content score evaluates whether the model's response adequately addresses the user's instruction, while the style score assesses whether the model's response style is suitable for speech interaction scenarios. The detailed prompt can be found in Appendix A.\nSpeech-Text Alignment To evaluate the alignment between text responses and speech responses, we use the Whisper-large-v3 model to transcribe the speech responses into text, and then calculate the Word Error Rate (WER) and Character Error Rate (CER) between the transcribed text and the text response. We refer to these metrics as ASR-WER and ASR-CER, respectively."}, {"title": "4.3 BASELINE SYSTEMS", "content": "We include the following speech-language models as baseline systems:\nSpeechGPT SpeechGPT (Zhang et al., 2023) is a speech-language model that supports both speech input and output. We use the chain-of-modality prompting adopted in the original paper for decoding, which sequentially outputs the text instruction, text response, and speech response based on the speech instruction.\nSALMONN (+TTS) SALMONN (Tang et al., 2024) is a LLM capable of accepting speech and audio inputs and responding with text, enabling it to perform the S2TIF task. For the S2SIF task, we add a VITS TTS model after SALMOON to generate speech responses in a cascaded manner.\nQwen2-Audio (+TTS) Qwen2-Audio (Chu et al., 2024) is a powerful general-purpose audio un- derstanding model capable of performing various audio-related tasks, including the S2TIF task. We also build a cascaded system with Qwen2-Audio and VITS to complete the S2SIF task."}, {"title": "4.4 MAIN RESULTS", "content": "Table 1 presents the main results on the InstructS2S-Eval benchmark. First, for the S2TIF task, from the content perspective, LLaMA-Omni shows significant improvement compared to previous models. This is mainly because LLaMA-Omni is developed based on the latest Llama-3.1-8B- Instruct model, leveraging its strong text instruction-following capabilities. From the style perspective, SALMONN and Qwen2-Audio receive lower scores, as they are speech-to-text models. Their output style is not aligned with speech interaction scenarios, often producing formatted content and containing a lot of redundant explanations. In contrast, SpeechGPT, as a speech-to-speech model, achieves a higher style score. Similarly, our LLaMA-Omni attains the highest style score, indicating that after being trained on our InstructS2S-200K dataset, the output style has been well-aligned with speech interaction scenarios. For the S2SIF task, LLaMA-Omni also outperforms previous models in both content and style scores. This further confirms that LLaMA-Omni is capable of effectively addressing user's instructions with speech in a concise and efficient manner.\nAdditionally, in terms of alignment between speech and text responses, LLaMA-Omni achieves the lowest ASR-WER and ASR-CER scores. In contrast, SpeechGPT performs poorly in aligning speech and text responses, likely due to its sequential generation of text and speech. The speech- text alignment of cascaded systems, such as SALMONN+TTS and Qwen2-Audio+TTS, is also suboptimal, primarily because the generated text responses may contain characters that cannot be synthesized into speech. This issue is especially evident in Qwen2-Audio, which occasionally out- puts Chinese characters, introducing errors in the speech responses. In comparison, LLaMA-Omni achieves the lowest ASR-WER and ASR-CER scores, demonstrating a higher degree of alignment between generated speech and text responses, and further validating the advantage of our approach in simultaneously generating both text and speech responses."}, {"title": "4.5 TRADE-OFF BETWEEN SPEECH QUALITY AND RESPONSE LATENCY", "content": "LLaMA-Omni can simultaneously generate both text responses and discrete units corresponding to the speech response. As described in Section 2.6, to further enable streaming waveform generation, when the number of generated discrete units reaches a certain chunk size \u03a9, the unit chunk is fed into the vocoder to synthesize and play the speech. By adjusting the value of \u03a9, we can control the system's latency, where a smaller \u03a9 corresponds to lower system latency. When \u03a9 = +\u221e, it equates to waiting for all units to be generated before synthesizing the speech. At the same time, the value of \u03a9 also affects the quality of the generated speech. A smaller \u03a9 means that the speech is divided into more segments for synthesis, which may result in discontinuities between the segments, potentially reducing the overall coherence of the speech.\nTo better understand the impact of 2, we explore the system's latency, the alignment between speech and text responses, and the quality of the generated speech under different \u03a9 settings. As shown in Table 2, when \u03a9 is set to 10, the system's response latency is as low as 226ms, which is even lower than GPT-40's average audio latency of 320ms. At this point, the speech response lags by an average of 1.82 words at the start. When \u03a9 is set to +\u221e, the latency increases to around 2 seconds. For the ASR-WER and ASR-CER metrics, we are surprised to find that as the chunk size increases, the error rates also increase. We believe there may be two reasons for this. On one hand, the vocoder may handle short unit sequences more reliably than long ones, as it is typically trained on shorter sequences. On the other hand, the ASR model we use, Whisper-large-v3, has strong robustness. Even when the speech is somewhat discontinuous with smaller \u03a9, it has little impact on ASR recognition accuracy. Therefore, we further evaluate the naturalness of the generated speech using the UTMOS metric. It shows that as increases, the naturalness of the speech improves, since discontinuities in the speech decrease. In summary, we can adjust the value of \u03a9 based on different scenarios to achieve a trade-off between response latency and speech quality."}, {"title": "4.6 DECODING TIME", "content": "Table 3 lists the average decoding times of different models on the S2TIF and S2SIF tasks. For the S2TIF task, SpeechGPT needs to first output the text instruction and then the text response, while SALMONN and Qwen2-Audio tend to produce lengthy responses. In contrast, LLaMA-Omni provides concise answers directly, resulting in significantly lower decoding times, with an average of only 1.49 seconds per instruction. For the S2SIF task, SpeechGPT serially outputs the text and speech responses, resulting in a de- coding time approximately 6 times longer than when generating only text responses. In con- trast, LLaMA-Omni outputs both text and speech responses simultaneously and employs a non-"}, {"title": "4.7 CASE STUDY", "content": "To intuitively understand the differences in responses from different models, we provide an example in Table 4. It can be observed that the response of Qwen2-Audio are quite lengthy and include elements such as line breaks and parentheses that cannot be synthesized into speech. The response from SALMONN is also a bit long. The style of SpeechGPT's response is more appropriate for speech interaction scenarios, but the amount of information contained in its responses is less. In contrast, the response given by LLaMA-Omni is more detailed and helpful while maintaining a concise style, outperforming previous models in speech interaction scenarios."}, {"title": "5 RELATED WORK", "content": "Speech/Audio Language Models With the success of language models in the field of natural lan- guage processing (Brown et al., 2020), researchers have begun exploring how to model speech or audio using language models. Early work attempted to train language models on semantic tokens or acoustic tokens of audio, enabling the generation of audio without the need for text (Lakhotia et al., 2021; Nguyen et al., 2023; Borsos et al., 2023). Furthermore, by jointly training speech tokens and text, decoder-only models like VALL-E (Wang et al., 2023b) and VioLA (Wang et al., 2023c) can perform tasks such as speech recognition, speech translation, and speech synthesis. However, the above models are not built upon LLMs. To harness the power of LLMs, many studies ex- plore how to build speech-language models based on LLMs like LLaMA, which can be further divided into two types. The first type, represented by SpeechGPT (Zhang et al., 2023; 2024a) and AudioPaLM (Rubenstein et al., 2023), involves creating native multimodal speech-text models by adding speech tokens to the LLM's vocabulary and continuing pretraining using speech and text data. However, this approach typically requires a large amount of data and substantial computa-"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose an innovative model architecture, LLaMA-Omni, which enables low- latency and high-quality speech interaction with LLMs. LLaMA-Omni is built upon the latest Llama-3.1-8B-Instruct model, with the addition of a speech encoder for speech understanding and a streaming speech decoder that can generate both text and speech responses simultaneously. To align the model with speech interaction scenarios, we construct a speech instruction dataset InstructionS2S-200K, which contains 200K speech instructions along with the speech responses. Experimental results show that, compared to previous speech-language models, LLaMA-Omni de- livers superior responses in both content and style, with a response latency as low as 226ms. More- over, training LLaMA-Omni requires less than 3 days on 4 GPUs, enabling rapid development of speech interaction models based on the latest LLMs. In the future, we plan to explore enhancing the expressiveness of generated speech responses and improving real-time interaction capabilities."}, {"title": "A PROMPT", "content": "Prompt for ChatGPT Scoring (Model: GPT-40)\nI need your help to evaluate the performance of several models in the speech interaction scenario. The models will receive a speech input from the user, which they need to understand and respond to with a speech output. Your task is to rate the model's responses based on the provided user input transcription [Instruc- tion] and the model's output transcription [Response]. Please evaluate the response from two perspectives: content and style, and provide a score for each on a scale of 1 to 5.\nContent (1-5 points):\n1 point: The response is largely irrelevant, incorrect, or fails to address the user's query. It may be off-topic or provide incorrect information.\n2 points: The response is somewhat relevant but lacks accuracy or completeness. It may only partially answer the user's question or include extraneous information.\n3 points: The response is relevant and mostly accurate, but it may lack conciseness or include unnecessary details that don't contribute to the main point.\n4 points: The response is relevant, accurate, and concise, providing a clear answer to the user's question without unnecessary elaboration.\n5 points: The response is exceptionally relevant, accurate, and to the point. It directly addresses the user's query in a highly effective and efficient manner, providing exactly the information needed.\nStyle (1-5 points):\n1 point: The response is poorly suited for speech interaction, possibly including structured elements like lists or being overly complex, disjointed, or difficult to understand.\n2 points: The response is somewhat suitable but may be too long, too short, or awkwardly phrased, making it less effective in a speech interaction context.\n3 points: The response is generally suitable for speech interaction, but it may have minor issues with length, clarity, or fluency that detract slightly from the overall effectiveness.\n4 points: The response is well-suited for speech interaction, with appropriate length, clear language, and a natural flow. It is easy to understand when spoken aloud.\n5 points: The response is perfectly suited for speech interaction. It is the ideal length, highly clear, and flows naturally, making it easy to follow and understand when spoken.\nBelow are the transcription of user's instruction and models' response:\n### [Instruction]: {instruction}\n### [Response]: {response}\nAfter evaluating, please output the scores in JSON format: {\"content\": content score, \"style\": style score}.\nYou don't need to provide any explanations.\nPrompt for Instruction Rewriting (Model: Llama-3-70B-Instruct)\nBelow is an instruction data containing the user's instruction. I would like to generate a speech version of this instruction for training a large language model that supports speech input. Therefore, please rewrite my instruction data according to the following requirements:\n1. Modify the instruction to simulate human speech, adding fillers as appropriate (but not too many 'you know', 'like', etc.).\n2. The question should not contain content that cannot be synthesized by the TTS model. Numbers should be written in English words rather than Arabic numerals.\n3. The question should be relatively brief without excessive verbiage.\n[instruction]: {instruction}\nPlease output in JSON format as follows: {\"question\": {question}}.\nPrompt for Response Generation (Model: Llama-3-70B-Instruct)\nBelow is the transcribed text of a user's speech query. Please provide a response to this question, which will be converted to speech using TTS. Please follow these requirements for your response:\n1. Your response should not contain content that cannot be synthesized by the TTS model, such as paren- theses, ordered lists, etc. Numbers should be written in English words rather than Arabic numerals.\n2. Your response should be very concise and to the point, avoiding lengthy explanations.\n[instruction]: {instruction}\nPlease output in JSON format as follows: {\"response\": {response}}."}]}