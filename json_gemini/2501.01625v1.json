{"title": "ICPC: IN-CONTEXT PROMPT COMPRESSION WITH FASTER\nINFERENCE", "authors": ["Ziyang Yu", "Yuyu Liu"], "abstract": "Despite the recent success of Large Language Models (LLMs), it remains challenging to feed LLMs\nwith long prompts due to the fixed size of LLM inputs. As a remedy, prompt compression becomes a\npromising solution by removing redundant tokens in the prompt. However, using LLM in the existing\nworks requires additional computation resources and leads to memory overheads. To address it, we\npropose ICPC (In-context Prompt Compression), a novel and scalable prompt compression method\nthat adaptively reduces the prompt length. The key idea of ICPC is to calculate the probability of each\nword appearing in the prompt using encoders and calculate information carried by each word through\nthe information function, which effectively reduces the information loss during prompt compression\nand increases the speed of compression. Empirically, we demonstrate that ICPC can effectively\ncompress long texts of different categories and thus achieve better performance and speed on different\ntypes of NLP tasks.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities\nacross various tasks such as text generation, question answering, and semantic understanding [26, 16, 6]. However,\nLLMs encounter significant challenges when processing long prompts or extended contexts, as their computational cost\nscales quadratically with sequence length due to the attention mechanism [13, 27, 2]. This limitation hinders their ability\nto handle real-world applications requiring long-context understanding, such as document summarization, multi-turn\nconversations, and knowledge-intensive reasoning. Researchers have explored efficient attention mechanisms, sparse\nrepresentations, and distributed processing approaches to address these scalability issues, enabling LLMs to manage\nlong contexts more effectively and maintain their performance on extended inputs. However, existing approaches often\nutilize LLM to compress texts, leading to memory overhead challenges.\nThere are numerous existing approaches aimed at addressing the memory overhead of large language models (LLMs)\nduring inference, including LoRA [8] and Sparse Attention [4]. Several prompt compression methods, such as Selective\nContext [12], have demonstrated strong performance with limited memory requirements. Unlike these existing methods,\nwhich primarily focus on utilizing large language models to compress prompts, we propose a novel approach to prompt\ncompression leveraging language models with millions of parameters. Our approach offers faster inference compared to\nexisting methods and achieves excellent compression performance.\nThe motivation for the proposed approach stems from the structure of transformer encoders. First, the pretraining of\ntransformer encoders enables them to capture and understand the context of words effectively. Second, transformer\nencoders typically have significantly fewer parameters than large language models, resulting in a 10x to 100x increase\nin inference speed over existing compression methods, shown in Appendix 6."}, {"title": "Preliminaries", "content": "In this paper, we introduce a novel framework named In-Context Prompt Compression (ICPC). ICPC operates by\nfirst segmenting sentences or paragraphs at both the phrase and clause levels. It then uses a pre-trained transformer\ncoder to calculate the loss associated with each word in the sentence, removing words to make the paragraph concise\nwithout loss of essential meaning. We conducted experiments across various encoder architectures to demonstrate the\ngeneralizability and superiority of our method over existing approaches."}, {"title": "Entropy", "content": "Entropy, rooted in information theory, measures the average level of uncertainty or surprise in a probability distribution\nover linguistic units [21]. In Natural Language Processing (NLP), each token t \u2208 T (e.g., word or subword) is associated\nwith a probability p(t) reflecting how frequently t appears in a given context. Formally, the Shannon entropy of p is\ndefined as\n$H(p) = \\sum_{t\\in T}p(t) \\log p(t)$,\nwhere p(t) captures the likelihood of observing token t. Entropy thus provides a principled way of quantifying how\n\"spread out\" or \"concentrated\" the distribution is. In language modeling, for example, a lower entropy typically indicates\na model that makes confident predictions about the next token, whereas a higher entropy signals more uncertainty.\nExtensions like cross-entropy and KL divergence further leverage this concept to compare model-generated distributions\nagainst ground-truth or target distributions. Minimizing these metrics during training encourages NLP models to\nproduce more accurate and reliable predictions, ultimately improving language understanding and generation."}, {"title": "Masked Language Modeling", "content": "The Transformer encoder, as popularized by models like BERT, leverages self-attention to capture contextual depen-\ndencies between tokens in a sentence. Instead of processing input sequentially, it computes pairwise interactions\nbetween tokens in parallel, enabling more efficient handling of long-range dependencies. A key training objective\nfor Transformer encoders is Masked Language Modeling (MLM). In MLM, a subset of the input tokens is randomly\nmasked (e.g., replaced with a special \u2018[MASK]' token), and the model learns to predict these masked tokens based on\ntheir surrounding context. Formally, if xi represents a masked token, the model estimates p(xi | Xi). By inferring\nmissing pieces of text, the model learns robust internal representations that benefit downstream tasks such as text\nclassification, question answering, and semantic similarity."}, {"title": "Method", "content": "Our method compresses the input text for LLM by removing redundant words and phrases to increase the ability of\nLLM understanding on long context understanding and reduce the computational cost without extra large language\nmodels (e.g., GPT-4, Llama-3) needed [1, 23]."}, {"title": "Participle", "content": "If participle-based filtering is directly applied at the word level, it may fail to capture the nuanced structure of linguistic\npatterns. Therefore, we implement participle-based filtering beyond word-level processing at both phrase and clause\nlevels. In our framework, a participle unit is a fundamental building block, encompassing words, phrases, or clauses\ndepending on the required granularity. To support this, we group tokens with contextual embeddings into participle\nunits, enabling the model to retain richer semantic and syntactic information during filtering [12]."}, {"title": "Loss Computation", "content": "Given a list of words $C = (x_{i-k}, ..., x_{i+k})$, the loss function by removing $x_i$ is defined as\n$L(x_i) = a * \\sum_{\\substack{n=-k\\\\n\\neq 0}}^{k} sim(x_{i+n}, x_i) + log p(x_i | x_{i,k})$"}, {"title": "Redundant Words Removal", "content": "To minimize the loss of information while retaining the original key information during compression, we rank the units\nbased on their calculated loss in descending order and compute the p-th percentile of loss among all units.\n$L_p = np.percentile([L(x_0), .., L(x_n)], P)$\nThen, we remove all the lexical units that will lose greater or equal to Lp and merge the remaining words as the output\nC':\n$C' = {Xi | L(Xi) < L_p}$\nThis adaptive filtering strategy provides a more flexible mechanism to discard redundant units while retaining the most\nessential content. By dynamically adjusting the threshold, the method ensures that the selection process accounts for\nvariations in the loss distribution."}, {"title": "Experiments", "content": "In this section, we present the performance of our method against other state-of-the-art approaches with both quantitative\nand qualitative analysis. For all experiments, we simulate an evaluation environment using an EC2 p4d.24xlarge\nvirtual machine (VM) instance on AWS, which has 8 NVIDIA A100 GPUs, 96 vCPUs, and 320 GB main memory. Other\nimportant information including operation system version, Linux kernel version and CUDA version are summarized in\nTable 5."}, {"title": "Experimental Settings", "content": "For a fair comparison, we adopt the same input format (tokens, phrases, or sentences) and inference settings for\nall experimental conditions. For parameters specific to our method, such as the compression ratio and lexical unit\ngranularity, we tune these to optimize efficiency without degrading performance. Baseline methods such as random\ncompression and full context usage retain default configurations. All metrics (e.g., BLEU) are computed under identical\nevaluation protocols to ensure consistency across tasks, with multiple runs for stochastic outputs to mitigate randomness."}, {"title": "Datasets", "content": "Our method reduces redundancy in the input context, enabling efficient processing of very long contexts for LLMs.\nHowever, existing benchmarks such as SQUAD [19] and Piqa [3] are mostly single-round question-answer datasets with\nshort question length, which is not appropriate to evaluate our proposed method. Therefore, we compile four datasets,\nwith long context and conversations to demonstrate the effectiveness of our method. The statistics and compilation\ndetails are presented in the appendix.\nWikipedia: A dataset containing articles from Wikipedia, a free online encyclopedia covering an extensive range of\ntopics across numerous domains, including history, science, arts, and culture. For our experiments, we utilize the\nintroductory sections of each article, which provide concise and informative summaries of the topics.\narXiv Papers: A dataset comprising academic papers from arXiv, spanning diverse fields such as computer science,\nphysics, mathematics, and biology. Due to the length of many arXiv papers, we focus on processing our experiments'\nabstract and introduction sections, ensuring a balance between content depth and computational efficiency.\nReddit: A dataset derived from user-generated posts and comments on Reddit, a social media platform organized into\ncommunities covering various topics, from technology and science to hobbies and entertainment. For our experiments,\nwe use a curated subset of posts and their associated top-level comments to capture meaningful discussions and\ninteractions."}, {"title": "ICPC Performance Evaluation", "content": ""}, {"title": "Compression Speed Analysis", "content": "As shown in Table 2, the ICPC achieves faster compression speed compared to Selective Context and LLMLingua. It is\nevident that the ICPC reduces compression time multiple times by using models of smaller size. It is noted that the"}, {"title": "Readability Analysis", "content": "As shown in Figure 2, the ICPC compresses the prompt by calculating the importance of each word in the sentence\nand removing unnecessary words to make the prompts shorter and more concise without losing information. The\ncompressed text also preserves good readability and makes it easier for people to grasp the meaning of the long prompt."}, {"title": "Scalability on Very-Long Texts", "content": "The prompts are segmented into fixed-length token chunks to align with the input constraints of language models.\nThis segmentation ensures compatibility while enabling the model to process information effectively. During our\nexperiments, we observed that the input limit of 512 tokens, defined by BERT, is sufficient to capture the necessary\ncontextual information for downstream tasks. This token limit ensures that the most important context is preserved"}, {"title": "Conclusion", "content": "In-context Prompt Compression is a good improvement with regarding to problems presented by large language models,\nsuch as memory overhead and computation speed. In this paper, we present ICPC (In-context Prompt Compression), a\nnovel and scalable prompt compression method that improves performance without the utilization of large language\nmodels. We formulate the important tokens selection task as an information calculation task. Extensive experiments\nover various comparison methods on multiple benchmarks with different encoders demonstrate that our proposed ICPC\ncan significantly boost the performance of existing hard prompt compression methods Moreover, it achieves faster\nconvergence speed."}, {"title": "Ethics Statement", "content": "This research did not involve any studies with human participants or animals performed by any authors. Therefore, no\nethical approval was required for this study. All data and materials were collected in a manner consistent with ethical\nguidelines, ensuring no ethical concerns were present."}, {"title": "Appendix", "content": "In this appendix, we describe the detailed comparison between LLM and LM and detailed descriptions of encoders and\nmetrics, etc."}, {"title": "Comparison between LLM and LM", "content": "To show the superiority of using an encoder to compress prompts, we take BERT Base and GPT-3 as example, shown in\nTable 6."}, {"title": "Detailed description of encoders", "content": "BERT: BERT introduces bidirectional context into NLP tasks using a transformer architecture. It is trained on large\ncorpora and fine-tuned for tasks like question answering. BERT's widespread influence and robust baseline performance\nmake it essential for experiment comparisons.\nROBERTa: ROBERTa optimizes BERT by removing the next-sentence prediction objective and using more extensive\ndatasets and more extended training. Its superior performance on multiple benchmarks makes it a strong candidate for\nevaluating enhanced pretraining techniques.\nXLNet: XLNet employs permutation-based training to capture bidirectional context without masking. Its ability to\noutperform BERT on tasks like GLUE demonstrates the advantages of its innovative pretraining objective.\nALBERT: ALBERT reduces memory and computation costs via parameter sharing and embedding factorization while\nmaintaining strong benchmark performance. Its efficiency and scalability make it ideal for resource-constrained settings.\nT5: T5 frames all NLP tasks as text-to-text problems using a unified transformer architecture. Its state-of-the-art results\nacross diverse benchmarks highlight its flexibility and generalization capabilities.\nDeBERTa: DeBERTa enhances BERT with disentangled attention and improved position encoding. Its strong\nperformance on GLUE and SQUAD makes it valuable for evaluating innovative attention mechanisms."}, {"title": "Detailed description of metrics", "content": "BLEU: BLEU evaluates machine translation by measuring n-gram overlap between machine and reference translations,\nemphasizing precision. Its consistency and simplicity make it a standard for translation benchmarks.\nROUGE: ROUGE measures overlap between predicted and reference summaries using recall-based metrics like\nn-grams and longest common subsequence. It is widely adopted for summarization due to its focus on content retention.\nTF-IDF Similarity: TF-IDF computes text similarity by balancing term frequency against inverse document frequency,\nhighlighting distinctive terms. Its interpretability makes it useful for document comparison.\nJaccard Similarity: Jaccard similarity compares sets by their intersection-over-union ratio, often used for token or\nn-gram overlap. Its simplicity makes it effective for assessing basic textual similarity.\nBERTScore: BERTScore uses contextual embeddings to evaluate semantic alignment between texts. Its ability to\ncapture deep contextual meaning makes it ideal for tasks requiring nuanced comparisons.\nCompression Rate: Compression Rate evaluates text conciseness by comparing original and compressed sizes. It is\neffective for gauging redundancy and assessing information density.\nFlesch-Kincaid Readability Score: This score evaluates readability based on sentence length and word complexity. It\nis essential to analyze the accessibility of generated or written content.\nMETEOR: METEOR combines precision and recall with synonyms and stemming for flexible text alignment. Its\nnuanced approach is valuable for evaluating natural language generation and translation."}]}