{"title": "DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework", "authors": ["Zhifei Xie", "Jacques Klein", "Daniel Tang", "Tegawend\u00e9 F. Bissyand\u00e9", "Dingwei Tan", "Saad Ezzini"], "abstract": "Current video generation models excel at creating short, realistic clips, but struggle with longer, multi-scene videos. We introduce DreamFactory, an LLM-based framework that tackles this challenge. DreamFactory leverages multi-agent collaboration principles and a Key Frames Iteration Design Method to ensure consistency and style across long videos. It utilizes Chain of Thought (COT) to address uncertainties inherent in large language models. DreamFactory generates long, stylistically coherent, and complex videos. Evaluating these long-form videos presents a challenge. We propose novel metrics such as Cross-Scene Face Distance Score and Cross-Scene Style Consistency Score. To further research in this area, we contribute the Multi-Scene Videos Dataset containing over 150 human-rated videos. DreamFactory\u00b9 paves the way for utilizing multi-agent systems in video generation.", "sections": [{"title": "1 Introduction", "content": "Video, integrating both visual and auditory modalities-the most direct sensory pathways through which humans perceive and comprehend the world-effectively conveys information with compelling persuasiveness and influence, progressively becoming a powerful tool and medium for communication [(Tang and Isaacs, 1992), (Owen and Wildman, 1992), (Armes, 2006), (Harris, 2016), (Merkt et al., 2011)]. Traditional video production is an arduous and time-intensive process, particularly for capturing elusive real-life scenes. Owing to the rapid advancements in deep learning, AI-driven video generation techniques now facilitate the acquisition of high-quality images and video segments with ease [ (pika), (Blattmann et al., 2023), (openai), (Blattmann et al., 2023), (runway), (Gu et al., 2023)]. However, crafting practical, multi-scene videos that meet real-world needs remains a formidable challenge. This includes ensuring consistency in character portrayal, stylistic coherence, and background across different scenes, proficiently maneuvering professional linguistic tools, and managing complex production steps beyond merely assembling brief video clips generated by current technologies. Therefore, there is an urgent need within the field of video generation for a model capable of directly producing long-duration, high-quality videos with high consistency, thus enabling AI-generated video to gain widespread acceptance and become a premier producer of content for human culture and entertainment.\nAt the current stage, substantial advancements in the video domain utilize diffusion-based generative models, achieving excellent visual outcomes [ (Blattmann et al., 2023), (runway), (openai)]. Nonetheless, due to the intrinsic characteristics of diffusion models, the videos produced are typically short segments, usually limited to four seconds. For generating longer videos, models like LSTM and GANs are employed (Gupta et al., 2022), however, these models struggle to meet the demands for high image quality and are restricted to synthesizing videos of lower resolution. These state-of-the-art approaches attempt to use a single model to address all sub-challenges of video generation end-to-end, encompassing attractive scriptwriting, character definition, and artistic shot design. However, these tasks are typically collaborative and not the sole responsibility of a single model.\nIn addressing complex tasks and challenges in problem-solving and coding, researchers have begun utilizing LLM multi-agent collaborative techniques, modeled on human cooperative behaviors, and have observed numerous potent agents. With the integration of large models that include visual capabilities, multi-agent collaborative technologies have now developed an AI workflow capable of"}, {"title": "2 Related work", "content": "LLM-based Agents. In recent years, the capabilities of large language models have been continually enhanced, exemplified by advancements such as GPT-4 (openai), Claude-3 (Claude), and LLama-2 (meta), among others. Subsequently, exploration into enhancing the abilities of these large language models has emerged, introducing methodologies such as CoT (Wei et al., 2022), ToT (Yao et al., 2024), ReACT (Yao et al., 2022), Reflexion (Shinn et al., 2024), and various other approaches to facilitate iterative output and correction cycles. Within this context, the notion of Multi-agents has surfaced, with early research efforts including notable works such as Camel (Li et al., 2024), Voyager (Wang et al., 2023), MetaGPT (Hong et al., 2023), ChatDev (Qian et al., 2023), and Auto-GPT (Yang et al., 2023). Recently, powerful Multi-agents frameworks have proliferated across diverse domains, with prominent instances in fields such as coding, including notable contributions such as CodeAgent (Tang et al., 2024), CodeAct (Wang et al., 2024), and Codepori (Rasheed et al., 2024). Utilitarian tools such as Toolformer (Schick et al., 2024), HuggingGPT (Shen et al., 2024), Tool-Ilm (Qin et al., 2023), and WebGPT (Nakano et al., 2021) have also been employed. Other noteworthy endeavors encompass projects like WebArena (Zhou et al., 2023), RET-LLM (Modarressi et al., 2023), and OpenAGI (Ge et al., 2024), each contributing to the advancement and proliferation of Multi-agents paradigms.\nVideo synthesis. In the field of video generation, traditional methods primarily utilize Generative Adversarial Networks (GANs) for video creation, as demonstrated in the works of Tim Brooks et al. (Brooks et al., 2022) and the foundational contributions of Ian Goodfellow et al. (Goodfellow et al., 2014) However, in recent years, a significant shift has occurred towards leveraging the potent capabilities of diffusion processes, with pioneering research conducted by Jascha et al. (Esser et al., 2023), and Song et al. (Song et al., 2020). The forefront of this evolution is marked by the development of Latent Video Diffusion Models. This approach is exemplified in the seminal efforts of Andreas Blattmann et al. (Blattmann et al.,"}, {"title": "3 DreamFactory", "content": "Our DreamFactory framework utilizes multiple large language models (LLMs) to form a simulated animation company, taking on roles such as CEO, Director, and Creator. Given a story, they collaborate and create a video through social interaction and cooperation. This framework allows LLMs to simulate the real world by using small video generation models as tools to accomplish a massive task. This section details the methodology behind our innovative DreamFactory framework. We first describe the defined role cards in Section 3.1 and discuss the pipeline in Section 3.2. Finally, we will discuss the keyframe iteration design method."}, {"title": "3.1 Role Definition", "content": "In the architecture of our simulation animation company DreamFactory, the following roles are included: CEO, movie director, film producer, Screenwriter, Filmmaker, and Reviewer. Within the DreamFactory framework, they function similarly to their real-world counterparts, taking on roles such as determining the movie's style, writing scripts, and drawing.\nThe definition prompts for their roles primarily consist of three main parts: Job, Task and Requirements. For instance, the definition prompt for a movie's creator would include the following sentences: (a) You are the Movie Art Director. Now, we are both working at Dream Factory,... (b) Your job is to generate a picture according to the scenery given by the director...and (c) you must obey the real-world rules, like color unchanged... For tasks such as plot discussions, we also limit their discussions to not exceed a specific number of rounds (depending on the user's settings and the company's size definition). We have included the following prompt to ensure this: \"You give me your thought and story, and we should brainstorm and critique each other's idea. After discussing more than 5 ideas, any of us must actively terminate the discussion by picking up the best style and replying with a single word <INFO>, followed by our latest style decision, e.g., cartoon style.\""}, {"title": "3.2 Dream Factory Framework pipeline", "content": "In this section, we introduce the specific pipeline of DreamFactory. Figure 2 illustrates the main phases and indicates which agents engage in conversations. Before delving into our entire pipeline, it's essential to first outline its fundamental components: phases and conversations. As depicted in Figure 3 (c, a phase represents a complete stage that takes some textual or pictorial content as input. Agents, composed of GPT, engage in roleplay, discussion, and collaboration for processing, ultimately yielding some output. A conversation is a basic unit of a phase, with typically more than one round of conversation encompassed within a phase. After a fixed number of conversations, a phase is approaching its conclusion, at which point DreamFactory will save certain interim conclusions generated within this phase that we wish to retain. For instance, in the Phase style decision, the final conclusion will be preserved. Furthermore, during subsequent phases, DreamFactory will provide the necessary precedents, such as invoking previous styles and scripts when designing keyframes later on.\nRecently, large language models were found to have their capabilities limited by finite reasoning abilities, akin to how overly complex situations in real life can lead to carelessness and confusion. Therefore, the main idea of this framework, in the video domain, is to decompose the creation of long videos into specific stages, allowing specific large models to play designated roles and leverage their powerful capabilities in analyzing specific problems. Like a real-life film production company, DreamFactory adopts a classic workflow, starting with scriptwriting followed by drawing. Overall, the framework encompasses six primary stages:"}, {"title": "3.3 Keyframe Iteration Design", "content": "During the generation of long videos, the most challenging problem to address is that a video comprises a long sequence of image collections. Therefore, when generating, the model needs to maintain a long-term, consistent memory to ensure that each frame produced by the model coherently composes a consistent video. This type of memory includes two kinds: short-term memory knowledge and long-term memory system.\nshort-term memory knowledge is embedded within videos of a fixed scene. Between adjacent frames, the animation in each frame should be connected, the characters should be unified, and there should be no significant changes in color, style, etc. As of now, the latest video models perform very well in terms of short-term memory. Nonetheless, we have still added a Monitor to supervise whether our video model is performing sufficiently well. As illustrated in Figure 4, there is a review process after the generation of each frame. Therefore, to maintain short-term consistency, the supervisory mechanism we introduced has addressed this issue.\nlong-term memory system, however, pose a challenge that troubles most current models and represents the most pressing issue in video generation today. Particularly, within a GPT-based fully automated multi-agent framework, the inherent randomness and drift phenomena of large language models make this problem difficult to tackle. Long-term memory implies that across scene transitions, the model should be able to maintain the consistency of the drawing style, character continuity, and narrative flow. To uphold long-term memory, we have introduced the Keyframe Iteration Design method, which transforms long-term memory into short-term memory by guiding the generation of consecutive, consistent images, iterating and generating forward with each step. Figure 4 demonstrates the process of each iteration.\nKeyframe Iteration Design Method leverages the inferential capabilities of large language models to transform long-term memory into iterations of short-term memory to ensure consistency. The first frame of the image is the beginning of the entire video and establishes essential information such as the style, painting technique, characters, and background for the entire long video. Therefore, we refer to the first frame as the Base. At the beginning, we will generate a painter P, a director D and a monitor M, represented by $P \\leftarrow F_P P$, $D\\leftarrow F_DD$, $M \\leftarrow F_MM$,these models played by visual large language models, will engage in a cyclical process of generation and discussion until they produce a crucial frame, which is the first keyframe, referred to as the Base Frame. At this point, the Monitor D, composed of a visual large language model as well, will conduct a thorough analysis to extract information, detailed description of features such as style, background, and character traits that should be preserved for an extended period. This results in the Base Description, note as $B_D$. $S_1$ represents the script for the first frame. We have $O_t = Gen(p_t, d_t, S_1)$, where $B_D \\leftarrow M(O_t)$.\nIn subsequent generations, when iterating the keyframe for moment t, we will use the previously input $S_t$ as the description of the scene. To maintain continuity in the context of adjacent scenes, we will employ the nurtured method to generate the description for the moment $t - 1$, which we also refer to as the contextual environment denoted as $C_{t- 1}$. At the same time, to maintain long-distance memory, $B_D$ will also serve as an input. By referencing the basic features of the previous frame and the Base features, it can ensure that the necessary information is essentially grasped in the next iteration, enabling the drawing of continuous keyframes with the same style, consistent characters, and uniform background. We have $O_t = Gen(p_t, d_t, S_t, C_{t-1})$.\nUpon the previous generation of keyframes, we can obtain the contextual environment and proceed with the next round of generation. We have $C_t = M(O_t)$, $P_{t+1} = P(S_t, C_t)$, $d_{t+1} = D(S_t, C_t, P_{t+1})$. Ultimately, we achieve the generation of the keyframes for the moment $t + 1$.\nIn practical application, controlling the details of characters proves to be the most challenging aspect."}, {"title": "4 Experiments", "content": "4.1 Traditional Video Quality Evaluatilon\nEvaluatilon Metrics - To validate the continuity of the keyframes and the quality of the videos produced by the framework, we embedded various tool models (such as Runway, Diffusion, GPT) within the architecture to assess the quality of videos generated by different tools. In our experiments, we principally employed the following evaluation metrics: (1) Fr\u00e9chet Inception Distance (FID) score: measures the similarity between generated images and real images. (2) Inception Score (IS): gauges the quality and diversity of generated images. (3) CLIP Score: evaluates the textual description accuracy of generated images. (4) Fr\u00e9chet Video Distance (FVD) score: extension of the FID for videos, comparing the features distribution of real videos versus synthesized ones based on Fr\u00e9chet distance and (5) Kernel Video Distance (KVD): utilizes kernel function to compare the features distribution of real videos versus synthesized ones.\nOur dataset, during the Regular phase, comprised conventional prompts consisting of 70 keywords and brief sentences randomly selected by experimental personnel from the COCO dataset. This was utilized to evaluate the generated image quality of the fundamental tool models and the degree of alignment between the images and the text. For the Script phase, scripts pertaining to 70 randomly extracted tasks from our provided dataset were employed during the script-filling stage. This guided the model generation based on the relevant plot to assess the function of the \"Animation Department\" within the DreamFactory framework. The DreamFactory label denotes the keyframe images produced by the framework that corresponds to the Script.\nOutput Quality Statistics - The images generated using models such as DALL\u00b7E and Diffusion are of high quality and have reached the state-of-the-art level in various indices. To quantitatively analyze the quality of the generated images, we input the images corresponding to the original prompts into GPT to get the GPT-Script and then used original prompts or the GPT-Script as prompts to generate 1400 images, from which we calculated FID, IS, and CLIP Score. As for FVD and KVD, we selected 100 samples from our multi-scene video dataset and manually extracted 10 keyframes for each one, Which can be used to generate multi-scale videos.\nData in Table 1 indicates that the quality of images generated using scripts is on average more refined than those produced using everyday prompt words. This may be attributable to the extent to which GPT acts as a prompt, and contemporary models are generally adept at processing longer prompts. However, within the DreamFactory framework, the application of keyframe iterative design, in conjunction with storyboard creation, detailed descriptions of characters, settings, lighting, and style determination, has led to a marked improvement in the quality of image generation. A similar enhancement is also evident in videos which is shown in Table 2."}, {"title": "4.2 Multi-scene Videos Evaluation Scores", "content": "Cross-Scene Face Distance Score - In the generation of sequential videos, addressing character consistency is paramount. Discrepancies in the appearance of characters can lead not only to poor"}, {"title": "5 Conclusion", "content": "We introduce Dream Factory: a multi-agent-based framework for generating long videos with multiple scenes. Dream Factory incorporates the idea of multi-agents into the field of video generation, producing consistent, continuous, and engaging long videos. Dream Factory introduces a keyframe iteration method to ensure alignment of style, characters, and scenes across different frames and can be built on top of any image or video generation tool. Furthermore, Dream Factory proposes new metrics to validate its capabilities by measuring the quality of generated content through cross-scene face and style consistency, as well as text-to-visual alignment. On the test set, the DreamFactory framework can achieve highly consistent sequential story generation, marking a groundbreaking development."}, {"title": "6 Limitations", "content": "In this paper, we present a multi-agent video generation framework capable of producing videos with high consistency across multiple scenes and plot-lines. However, we still face several limitations. Firstly, our current reliance on prompts to control agents means that the agents are not capable of highly creative tasks, such as devising plots with artistic merit. Such tasks require the accumulation of specific datasets for model fine-tuning. Secondly, the editing of all video segments is centered around synthesized speech content, which results in a final product that may appear as a mere assembly of clips. This necessitates the introduction of a unique framework design to enhance the fluidity of the videos. Lastly, video generation still involves substantial resource consumption."}, {"title": "7 Ethics Statements", "content": "The development and deployment of DreamFactory, a multi-agent framework for long video generation, raise several ethical considerations that must be addressed. The potential for the misuse of generated videos, such as the creation of deepfakes or the propagation of misinformation, is a significant concern. To mitigate these risks, we commit to implementing robust safeguards, including watermarking generated content and collaborating with fact-checking organizations. Additionally, we will ensure transparency in our research and make our methods and datasets publicly available, subject to ethical use guidelines. We also recognize the importance of diversity and inclusion in the training data to prevent biases in the generated content. Finally, we will engage with the broader community to establish ethical standards for the use of AI-generated video content, promoting responsible innovation and use of this technology."}, {"title": "A Appendix", "content": "A.1 DreamFactory Responsibility allocation\nAs shown in Figure 8, our DreamFactory framework utilizes multiple large language models (LLMs) to form a simulated animation company, taking on roles such as CEO, Director, and Creator. Given a story, they collaborate and create a video through social interaction and cooperation. This framework allows LLMs to simulate the real world by using small video generation models as tools to accomplish a massive task. As illustrated in Figure 8, under their collaboration, it is possible to generate a series of consistent, stable, multi-scene long videos as the plot progresses.\nA.2 User Study\nQuantitative evaluation of human preference for video is a complex and difficult proposition, so we employed human evaluators to verify the quality of multi-scene videos generated by our framework. We collected 150 multi-scene short videos generated by AI from the internet and compare them with videos from our framework. Through this approach, we aimed to assess whether our videos could achieve an advantage in human preferences compared to existing AI videos on the network.\nIn our study, We adopt the Two-alternative Forced Choice (2AFC) protocol, as used in previous works [(Blattmann et al., 2023), (Blattmann et al., 2023), (Bar-Tal et al., 2024)]. In this protocol, each participant will be randomly shown a pair of videos with the same story, one is a short video collected on web platforms and the other is generated by our framework. Participants were then asked to select the superior side on five metrics: role consistency, scene consistency, plot quality, storyboard fluency, and overall quality. We collected 1320 human scores for this study, utilizing schools, communities, and network platforms. As illustrated in Figure 9, our method was preferred better.\nA.3 Case Study\nComprehensive Keyframe Count Statistics - The version currently provided to users is balanced between cost and user experience, using the Short generation mode, typically around ten scenes. The specific number is related to the user's task input. The length of videos generated using random prompts is shown in the figure 10."}]}