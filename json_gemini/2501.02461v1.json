{"title": "FedRSClip: Federated Learning for Remote Sensing Scene Classification Using Vision-Language Models", "authors": ["Hui Lin", "Chao Zhang", "Danfeng Hong", "Kexin Dong", "Congcong Wen"], "abstract": "Remote sensing image classification is essential for various applications, including agricultural monitoring, urban planning, and land use classification. However, remote sensing data is often distributed across multiple institutions, and due to privacy concerns and data-sharing restrictions, leveraging large-scale datasets in a centralized training framework is challenging. Federated learning offers a promising solution by enabling collaborative model training across distributed data sources without requiring data centralization. However, current Vision-Language Models (VLMs), which typically contain billions of parameters, pose significant communication challenges for traditional federated learning approaches based on model parameter updates, as they would incur substantial communication costs. In this paper, we propose FedRSCLIP, the first federated learning framework designed for remote sensing image classification based on a VLM, specifically CLIP. FedRSCLIP addresses the challenges of data heterogeneity and large-scale model transmission in federated environments by introducing Prompt Learning, which optimizes only a small set of tunable parameters. The framework introduces a dual-prompt mechanism, comprising Shared Prompts for global knowledge sharing and Private Prompts for client-specific adaptation. To maintain semantic coherence between shared and private prompts, we propose the Dual Prompt Alignment Constraint to balance global consistency and local adaptability across diverse client distributions. Additionally, to enhance cross-modal representation learning, we introduce the Cross-Modal Feature Alignment Constraint to align multimodal features between text and image prompts. To validate the effectiveness of our proposed model, we construct a Fed-RSIC dataset based on three existing remote sensing image classification datasets, specifically designed to simulate various federated learning configurations. Experimental results on the Fed-RSIC dataset demonstrate the effectiveness and superiority of FedRSCLIP in addressing the challenges of federated remote sensing image classification.", "sections": [{"title": "I. INTRODUCTION", "content": "Remote sensing image classification has emerged as a critical technique in diverse fields, such as agricultural monitoring [1], urban planning [2], land use classfication [3], and environmental forecasting [4]. These applications rely on the ability to interpret vast amounts of data captured by satellites, drones, and aerial platforms to extract meaningful insights. The large-scale nature of remote sensing data, combined with the growing demand for timely and accurate analysis, underscores the importance of developing robust image classification models. However, due to the distributed nature of remote sensing data, which is often collected across various geographic locations and stored in different institutions, privacy concerns arise when centralizing data. In response to these concerns, federated learning [5] has emerged as a powerful approach, enabling collaborative learning across distributed datasets while preserving data privacy by keeping raw data local. This advancement not only addresses data privacy concerns but also enables widespread, cross-institutional insights that can benefit industries reliant on accurate, large-scale image analysis.\nIn recent years, federated learning has achieved notable success in several applications [6], [7], [8], leveraging deep learning models to enable decentralized training across clients. These models have demonstrated the potential to handle the non-iid (non-independent, non-identical) data distributions commonly seen in remote sensing. However, despite these successes, current federated learning approaches have largely focused on traditional deep learning models, which primarily learn from pixel-level image data without incorporating the rich contextual information that could be gained from other modalities. Vision-Language Models (VLMs) [9], [10], [11], such as CLIP [12], offer a promising solution to this limitation. VLMs learn joint representations of images and text, mapping them into a shared feature space, enabling models to capture semantic relationships that go beyond visual features alone. By integrating VLMs into federated learning, we can poten- tially enhance model performance on remote sensing tasks, where both visual and textual descriptions, could significantly improve classification accuracy.\nDespite recent successes in applying VLMs to remote sensing tasks [13], [14], [15], [16], [17], adapting them to federated learning frameworks presents significant challenges. Traditional federated learning typically involves uploading each client's local model parameters to a central server for unified updates, which are then sent back to each client. However, as illustrated in Fig. 1, VLMs are large-scale mod- els with millions or even billions of parameters, making it impractical to transmit the full model between clients and a central server due to substantial communication overhead. Additionally, the heterogeneity of data across clients, including variations in resolution, terrain, and spectral characteristics, further complicates the alignment of multimodal features within distributed settings. Thus, traditional federated learning algorithms, initially designed for smaller models, are not suited to handle the complexity and scale of VLMs. Therefore, an efficient strategy is essential to minimize communication load while maintaining VLM capabilities in federated setups.\nTo address these challenges, we propose FedRSCLIP, a novel Federated learning framework for Remote Sensing image classification based on CLIP, specifically designed to overcome the communication limitations of VLMs in federated learning. To the best of our knowledge, FedRSCLIP is the first federated learning framework to integrate VLMs into remote sensing image classification, leveraging both visual and tex- tual information to enhance representation and classification. FedRSCLIP enables robust and generalized learning across distributed, non-iid datasets while preserving data privacy. Specifically, FedRSCLIP introduces Prompt Learning, which optimizes a small set of tunable parameters, significantly reducing communication overhead while maintaining adapt- ability to local data. To balance global consistency with local flexibility, FedRSCLIP employs a dual-prompt mechanism comprising Shared Prompts for global knowledge sharing and Private Prompts for client-specific customization. This mechanism ensures the model captures overarching patterns across all clients while allowing for fine-grained adaptations to diverse local data distributions. To maintain semantic con- sistency between shared and private prompts, we propose the Dual Prompt Alignment Constraint, which ensures that private prompts remain aligned with the global knowledge encapsulated by shared prompts, even as they adapt to local data. Additionally, we propose the Cross-Modal Feature Align- ment Constraint, which aligns multimodal features across text and image prompts, facilitating more effective cross-modal representation learning and enhancing the overall coherence of the model's representations. Finally, we validate the effec- tiveness of FedRSCLIP on our newly constructed Fed-RSIC dataset, which is built upon three popular remote sensing image classification datasets: Optimal-31, UCMerced, and NWPU. Experimental results demonstrate that FedRSCLIP achieves state-of-the-art remote sensing image classification performance across various federated learning configurations.\nThe contributions of this paper are summarized as follows:\n\u2022 We propose FedRSCLIP, the first framework to integrate Vision-Language Models into federated learning for re- mote sensing image classification. FedRSCLIP enhances representation and classification performance while opti- mizing communication efficiency, addressing challenges such as high communication costs and data heterogeneity.\n\u2022 We introduce Prompt Learning for VLMs in federated learning, which optimizes a small set of tunable param- eters instead of transmitting the entire model. Further- more, we propose a dual-prompt mechanism comprising Shared Prompts for global knowledge sharing and Private Prompts for client-specific customization, enabling the model to balance global consistency and local flexibility.\n\u2022 We develop two innovative constraints to enhance prompt alignment and representation learning in federated learn- ing with VLMs. The first is the Dual Prompt Alignment Constraint, which ensures semantic consistency between shared and private prompts by aligning their represen- tations during training. The second is the Cross-Modal Feature Alignment Constraint, which aligns multimodal features between text and image representations, enhanc- ing the model's ability to capture relevant features and improving its classification performance.\n\u2022 We construct the Fed-RSIC dataset by integrating three popular remote sensing image classification datasets. This dataset is specifically designed to simulate diverse feder- ated learning scenarios, enabling comprehensive evalu- ation of the proposed framework. Experimental results demonstrate that FedRSCLIP achieves state-of-the-art performance in remote sensing image classification across various federated learning configurations."}, {"title": "II. RELATED WORK", "content": "A. Federated Learning\nFederated Learning has emerged as a key decentralized machine learning approach, preserving data privacy while allowing collaborative model training [5]. The foundational algorithm, FedAvg [6], aggregates model updates from dis- tributed clients without requiring local data sharing. While robust in handling non-IID data under synchronous updates, FedAvg's performance declines significantly when faced with heterogeneous data or evolving client datasets [18], [19], [20]. Several methods have been proposed to address these challenges. For example, FedProx [7] introduces a proximal term to FedAvg, stabilizing local updates by constraining model divergence from the global model. Elastic aggregation [21] adjusts update magnitudes based on parameter sensitivity, ensuring that the global model adapts appropriately to diverse client data. Moreover, approaches like FedSeg [22] modify cross-entropy loss to address class heterogeneity in specific tasks, such as semantic segmentation, while FedH2L [23] and FedAlign [8] leverage distillation techniques to enhance gener- alization and reduce communication overhead by exchanging only selective information.\nTo further mitigate data heterogeneity, methods like FedFed [24] and FedOTP [25] employ feature distillation and prompt learning, enabling models to capture both global consensus and client-specific features. In contrast, aggregation-focused techniques such as FedAF [26] utilize client-condensed data to reduce client drift and accelerate convergence. In scenarios where client data evolve over time, methods like CFL [27], FedTHE [28], and pFedEM [29] address temporal heterogene- ity by adapting model architectures to dynamically changing distributions. Additionally, frameworks such as FBL [20] and FedRC [30] mitigate the forgetting of old classes and handle multiple types of distribution shifts through semantic compensation and robust clustering techniques, respectively."}, {"title": "B. Federated Learning in Remote Sensing", "content": "Federated learning has become increasingly relevant for remote sensing applications due to its ability to enable distributed data processing while maintaining data privacy. Various methods have been developed to address specific challenges in this domain. For instance, FedPM [31], based on prototype matching, enhances object extraction performance in Deep Convolutional Neural Networks. Similarly, GeoFed [32] targets semantic segmentation in earth observation, refining its objective function through Tail Regeneration and Essential Feature Mining strategies. Additionally, the architecture com- bining the deep memory connected neural network with the data-decoupled federated learning framework [33] facilitates image restoration while preserving privacy.\nIn the context of remote sensing image classification, re- cent advances in FL have focused on addressing challenges related to resource optimization, secure communication, and the handling of non-IID data. For instance, an adaptive model communication scheme [34] leverages deep Q-learning to optimize resource control in multi-access edge computing environments, using an epsilon-greedy strategy to allocate computation resources efficiently. To enhance security and ef- ficiency, FLBIC-CUAV [35] integrates clustering, blockchain, and FL, utilizing beetle swarm optimization (BSO) for UAV clustering, blockchain for secure data transmission, and FL with a Residual Network model for cloud-based image classifi- cation. Similarly, the blockchain-empowered PPFL framework [36], which employs the CKKS cryptosystem [37], enables satellite imagery owners to collaborate globally while ensur- ing data privacy and transparency. To improve classification performance across diverse data modalities, a multi-modal FL framework [38] associates images from different clients with various modalities, enhancing the robustness of the model. Ad- ditional strategies, such as feature-centric communication and pseudo-weight amalgamation, have been explored to improve the efficiency of model aggregation in FL [39]. In the context of image generation for land use and cover classification, the integration of deep convolutional generative adversarial net- works into FL frameworks [40] has proven effective, enabling client devices to generate high-quality images. To address the challenge of non-IID data across clients, transformer architec- tures have been introduced [41], particularly for multi-label classification tasks in RS. Additionally, FedDiff [42] offers a novel multi-modal diffusion-based FL framework, which combines dual-branch diffusion models for feature extraction with a lightweight communication module, ensuring efficient and private collaboration among clients. These advancements illustrate the growing sophistication of FL in RS, as it contin- ues to tackle diverse challenges while maintaining data privacy and improving model performance."}, {"title": "C. CLIP in Remote Sensing", "content": "Contrastive Language-Image Pretraining (CLIP) [12] is an advanced vision-language model composed of a vision encoder and a text encoder, which generate vector representations for images and text, respectively. CLIP is trained using contrastive learning, where the model learns to associate correct image- text pairs while distinguishing them from incorrect ones. In recent years, the application of CLIP in remote sensing (RS) has attracted significant attention, as researchers have begun exploring how its vision-language capabilities can be adapted to the unique challenges of this domain.\nRecently, several adaptations of CLIP have been developed for various RS tasks. For example, RS-CLIP [43] combines contrastive vision-language pretraining with pseudo-labeling and curriculum learning to enhance semantic-aware visual representations and improve overall model performance. Sim- ilarly, ChangeCLIP [44], designed for remote sensing change detection, modifies CLIP to extract bitemporal features and introduces a differential features compensation module to cap- ture detailed semantic changes. This is further complemented by a vision-language-driven decoder to enhance image se- mantics. Additionally, PIR-CLIP [45] applies prior instruction representation learning, pre-training on coarse-grained remote sensing data before fine-tuning on fine-grained data. It also introduces a cluster-wise attribution loss to reduce semantic confusion, further improving the model's ability to handle complex RS data. Moreover, SG-CLIP [46] integrates geo- graphic information with CLIP's vision-language capabilities to boost species recognition accuracy, especially in few-shot learning scenarios. Similarly, GeoChat [47], built upon CLIP-ViT(L-14) [12] and fine-tuned with LLaVA-1.5 [48] using the LORA [49] technique, extends CLIP's conversational abilities while enhancing its domain-specific knowledge for RS tasks. Furthermore, a methodology [50] has been proposed to align RS imagery with the visual and textual modalities of CLIP through a two-stage process. This approach involves fine-tuning CLIP and performing cross-modal alignment, signifi- cantly improving performance in RS image classification and retrieval tasks."}, {"title": "III. DATASET CREATION", "content": "Remote sensing datasets are inherently diverse, captur- ing variations in geographic regions, land-use types, spatial resolutions, and imaging conditions. However, most exist- ing datasets are structured for centralized machine learning paradigms, lacking the characteristics needed to reflect the challenges of federated learning. To address this gap and better evaluate the performance of federated learning in re- mote sensing image classification tasks, we propose a novel dataset named Fed-RSIC, specifically designed for federated learning environments. Fed-RSIC is constructed by integrat- ing three widely recognized remote sensing image classifica- tion datasets: Optimal-31, UCMerced, and NWPU-RESISC45. These datasets are carefully selected due to their diversity in land-use categories, image resolutions, and spatial character- istics, providing a robust foundation for simulating federated learning scenarios. To reflect their adaptation for federated learning, these datasets are restructured and renamed within the FedRSIC framework as Fed-Optimal, Fed-UCMerced, and Fed-NWPU, maintaining their inherent diversity while tailoring them to address federated learning challenges."}, {"title": "A. Source Datasets", "content": "a) Optimal-31: The OPTIMAL-31 dataset [51], col- lected from high-resolution Google Earth imagery, consists of 31 land-use classes, with a total of 1,860 images. Each class is represented by 60 images, all with a resolution of 256 \u00d7 256 pixels and a spatial resolution of 0.3 meters per pixel. This fine-grained spatial resolution allows the dataset to capture detailed ground features, making it suitable for various land- use classification and remote sensing tasks. The dataset covers a wide variety of categories, including both natural and man-made environments such as airports, baseball fields, basketball courts, churches, round farmland, dense housing areas, deserts, forests, golf courses, and meadows.\nb) UCMerced: The UC Merced Land-Use dataset [52] is a well-curated ground-truth image dataset, manually extracted from the USGS National Map Urban Area Imagery collection. It contains 2,100 RGB images, each measuring 256 \u00d7 256 pixels, with a spatial resolution of 0.3 meters per pixel, enabling fine-grained analysis of land-use patterns. The dataset is divided into 21 distinct land-use categories, with each category comprising 100 images. These categories cover a diverse range of environments, including agricultural fields, airplanes, beaches, buildings, chaparral, dense residential ar- eas, and medium-density residential areas.\nc) NWPU: The NWPU-RESISC45 dataset [53] is a large-scale benchmark designed for remote sensing image scene classification, developed by Northwestern Polytechnical University (NWPU). It contains 31,500 images, each with a resolution of 256 \u00d7 256 pixels. The spatial resolution of the images varies from 20 cm per pixel to over 30 meters per pixel, providing a diverse range of scales for analysis. The dataset is organized into 45 scene classes, with 700 images per class, featuring high intra-class diversity and inter-class similarity. These classes cover a wide array of environments, including airports, baseball diamonds, forests, harbors, freeways, over- passes, and ships."}, {"title": "B. Federated Learning Simulation", "content": "To simulate federated learning scenarios, the integrated datasets are partitioned into subsets corresponding to client configurations of 2, 5, 10, 15, 20, and 40 clients. These datasets, renamed as Fed-Optimal, Fed-UCMerced, and Fed- NWPU, are specifically tailored for federated learning experi- ments. For each configuration, the images within each dataset are evenly distributed among the specified number of clients. When the number of images per class is not perfectly divisible by the number of clients, the remaining images are distributed as evenly as possible across clients. This approach minimizes imbalance while ensuring fairness in data allocation. Before client-specific partitioning, the datasets are first split into training and testing sets. Specifically, 50% of the images in Fed-Optimal and Fed-UCMerced are allocated for training, with the remaining 50% reserved for testing. For the Fed- NWPU dataset, 20% of the images are used for training, and the remaining 80% are designated for testing. Table I illustrates the image distribution per client for training and testing across each dataset under different client configurations. For instance, in the 40-client configuration for Fed-NWPU, each client receives approximately 158 training images and 630 testing images, with the remainder distributed among clients to maintain near-equal allocation. This partitioning strategy ensures that each client is assigned a balanced subset of data, preserving the inherent characteristics of the original datasets. By simulating federated learning conditions, the Fed-RSIC dataset provides a robust benchmark for evaluating federated learning models. The diversity of client configurations enables comprehensive exploration of various federated setups, allow- ing researchers to study the impact of client heterogeneity, data distribution, and scalability on model performance."}, {"title": "IV. METHODS", "content": "A. Problem Statement\nIn the task of federated learning for remote sensing image classification, there are N clients distributed across different geographic locations, denoted as {C\u2081, C\u2082, . . ., C\u0274}. Each client C\u1d62 possesses a local dataset of remote sensing images D\u1d62 = {(I\u2c7c, y\u2c7c)}\u1d50\u1d62\u2c7c=\u2081, where I\u2c7c represents a remote sensing image, and y\u2c7c\u2208 \ud835\udcb4 is the corresponding land cover class label. The goal is to collaboratively train a global classification model f\u0398 that can accurately classify images across all clients while adapting to the specific data distribution of each client. To achieve this goal, each client C\u1d62 is equipped with a local model f\u0398\u1d62 which is trained to classify remote sensing images in the local dataset D\u1d62. For each image I\u2c7c, the local model computes the predicted class \u0177\u2c7c = f\u0398\u1d62(I\u2c7c) using the parameters \u0398\u1d62. The objective of the local model is to minimize its loss function on the local data:\nL\u1d62(\u0398\u1d62) = (1/m\u1d62) \u2211 l(f\u0398\u1d62(I\u2c7c), y\u2c7c)\nwhere l(.) is the loss function used to measure the difference between the predicted values and the true labels (e.g., cross- entropy loss).\nTo leverage the strengths of each local model while main- taining data privacy, federated learning aggregates these lo- cally optimized parameters into a global model. In federated learning, each client C\u1d62 locally optimizes its model parameters \u0398\u1d62 and periodically sends these parameters to a central server. The server collects the parameters {\u0398\u1d62}\u1d62=\u2081\u1d3a from all clients and aggregates them into global model parameters \u0398 using a weighted average:\n\u0398 = (1/m) \u2211 w\u1d62\u0398\u1d62\nwhere w\u1d62 = m\u1d62 / \u2211 m\u1d62 represents the data weight of each client, and m = \u2211i=1 mi is the total number of samples across all clients. The aggregated global model f\u0398 is then sent back to each client to update their local model parameters:\n\u0398\u1d62 \u2190 \u0398\nThis process is repeated iteratively, with multiple rounds of model updates and parameter aggregation, ultimately forming a global model that generalizes well across all clients. The goal of federated learning is to maximize the classification performance of the global model f\u0398 while preserving data privacy, ensuring that it maintains high accuracy on the data distributions present at each client."}, {"title": "B. Overview", "content": "In this paper, we propose FedRSCLIP, a federated learning framework for remote sensing image classification based on CLIP, designed to address the complexities of non-iid data across diverse clients. FedRSCLIP leverages CLIP's ability to learn cross-modal representations, capturing fine-grained relationships between images and textual descriptions, which is crucial for remote sensing image classification tasks. To minimize the communication overhead associated with trans- mitting the large CLIP model across clients, FedRSCLIP incorporates Prompt Learning, optimizing only a small set of learnable prompt parameters. This approach enables efficient adaptation to local client data while significantly reducing communication costs. Furthermore, FedRSCLIP employs a dual prompt mechanism consisting of Shared Prompts and Private Prompts to achieve a balance between global consis- tency and local adaptability. Shared Prompts capture global features across all clients, facilitating knowledge sharing, while Private Prompts are customized to the unique data distribution of each client, enabling personalized model ad- justments. To further ensure semantic consistency between Shared and Private Prompts, FedRSCLIP introduces the Dual Prompt Alignment Constraint, which maintains the alignment of Private Prompts with global knowledge while allowing them to adapt to local data distributions. Additionally, to improve cross-modal alignment between textual and image features, FedRSCLIP incorporates the Cross-Modal Feature Alignment Constraint, which enhances the model's ability to extract relevant features from both modalities. This constraint improves the coherence of multimodal representation learning, ensuring robust and accurate classification across federated settings. By integrating these techniques, FedRSCLIP effec- tively addresses the challenges posed by data heterogeneity in federated learning environments, resulting in improved global generalization and robust local adaptability for remote sensing image classification."}, {"title": "C. CLIP and Prompt Learning", "content": "Inspired by the recent success of VLMs in many vision tasks, we attempt to introduce VLMs into the federated learning framework to address the key challenges in remote sensing image classification. In our work, we employ the classical VLM model, CLIP, as the classification model on each client. CLIP learns joint representations of images and text, mapping both into a shared embedding space, which allows it to capture fine-grained relationships between images and textual descriptions. This ability makes CLIP particu- larly well-suited for cross-modal information extraction and matching, especially in complex tasks such as remote sens- ing image classification. By utilizing textual prompts, CLIP can precisely identify different land cover classes, enhancing both classification accuracy and generalization across diverse remote sensing scenarios. However, one major limitation of the CLIP model lies in its large number of parameters. In federated learning, where each client must frequently exchange model parameters with the central server, transmitting the entire CLIP model would drastically increase communication costs. This problem becomes even more pronounced when involving multiple heterogeneous clients. Therefore, it is cru- cial to reduce communication overhead while maintaining the powerful representation capabilities of CLIP.\nTo tackle this issue, we introduce the Prompt Learning method. Instead of transmitting the entire CLIP model, we optimize a small number of prompt parameters, allowing them to adapt to the local data on each client. This significantly reduces communication costs while enabling each client to quickly adapt to its local data, preserving the effectiveness of CLIP in different tasks. CLIP consists of two branches: the image branch, which contains rich information, and the text branch, which typically has weaker information. The textual prompts are often manually designed, such as \"a photo of a [Class]\". However, such fixed template prompts are overly broad and cannot sufficiently capture class-specific character- istics, limiting classification performance. Additionally, man- ually designing prompts is time-consuming and lacks gener- alization. Therefore, through Prompt Learning, we transform the text prompts into a learnable format, allowing the prompts to be automatically optimized based on the downstream task, replacing the need for hand-crafted prompts.\nSpecifically, Prompt Learning introduces h learnable vectors into the text prompts. We place the class token ([CLASS]) in the middle of the sequence, and the prompt s is structured as follows:\ns = [\ud835\udcb1]\u2081[\ud835\udcb1]\u2082 . . . [\ud835\udcb1]\u1d65[CLASS][\ud835\udcb1]\u1d65\u208a\u2081 . . . [\ud835\udcb1]\u2095,\nwhere each [\ud835\udcb1]\u2098 (m\u2208 {1, ...,h}) is a vector with the same dimension as word embeddings. By forwarding the prompt s to the text encoder g(.), we obtain prediction probability, which is computed as:\np(y = i|I) = exp(cos(g(s\u1d62), f(I))/\u03c4) /  \u2211 exp(cos(g(s\u2c7c), f(I))/\u03c4)\nwhere s\u1d62 denotes the prompt for class i, where the class token [CLASS] is replaced by the corresponding word embedding vector(s) for class i. And g(.) is the text encoder, f(.) is the image encoder, cos(,) denotes the cosine similarity, \u03c4 is the temperature parameter for the softmax function, and K is the total number of classes."}, {"title": "D. Shared and Private Prompt Learning", "content": "Considering that each client's data has its own local distribu- tion, using a single prompt for each client that is aggregated and updated globally on the server may lead to overfitting on shared features, thereby failing to capture the unique characteristics of individual client datasets. To mitigate this issue, we propose a dual prompt mechanism, comprising a Shared Prompt and a Private Prompt, that balances global consistency with local adaptability.\nShared Prompt Learning. The Shared Prompt sg is designed to capture common features across all clients and is updated globally. During federated learning, each client C\u1d62 locally updates its shared prompt sg,\u1d62 and transmits it to the central server. The server aggregates these prompts from all clients to produce an updated global shared prompt, which is distributed back to the clients. The aggregation of the shared prompts can be formulated as:\ns_g^(t+1) = (1/N) \u2211 w\u1d62 s_g,i^(t)\nwhere s_g^(t+1) is the global shared prompt at round t + 1, N is the number of participating clients, and s_g,i^(t) represents the shared prompt for client C\u1d62 at round t. The primary objective of the shared prompt is to capture generalized knowledge across clients, enabling the model to better generalize to new data across different client distributions. Through this mechanism, the model learns global patterns that are critical for handling diverse datasets across all clients.\nPrivate Prompt Learning. In contrast, the Private Prompt sp,\u1d62 is specific to each client C\u1d62 and captures the unique characteristics of the local data. Given that the data distribution varies between clients, the private prompt enables each client to fine-tune the model to better align with its local dataset. Unlike the shared prompt, the private prompt is optimized locally and is not shared with the central server, thereby preserving data privacy while enabling personalized model adjustments.\nThe final prediction for an input I\u2c7c on client C\u1d62 is generated by combining the shared prompt sg and the private prompt sp,i. This approach leverages global knowledge from sg while allowing for client-specific adaptation via sp,\u1d62. The prediction for a given input I\u2c7c can be formulated as:\n\u0177\u2c7c = f([sg, sp,\u1d62]; I\u2c7c)\nwhere \u0177\u2c7c is the predicted label for the input image I\u2c7c, and f([sg, sp,\u1d62]; I\u2c7c) represents the model's decision function based on the combination of the shared and private prompts. The private prompt sp,\u1d62 is trained using the local dataset D\u1d62, with the following loss function Lp,i:\nLp,i = (1/|D\u1d62|) \u2211 l(\u0177\u2c7c, y\u2c7c)\nwhere l(\u0177\u2c7c, y\u2c7c) is a loss function (e.g., cross-entropy) between the predicted label \u0177\u2c7c and the ground-truth label y\u2c7c. This loss function enables each client to tailor its model to its local data while preserving the global knowledge derived from the shared prompt.\nFederated Prompt Updates. During each round of federated learning, the shared prompt sg is updated across all clients, while each client independently updates its private prompt sp,\u1d62 based on its local data. The shared prompt ensures model consistency across the federation, while the private prompt provides the flexibility to adapt to individual client needs. This iterative process allows the model to continuously learn from both global and local data, achieving a balance between global consistency and local adaptation. Over multiple rounds, this framework fosters a model that generalizes well across het- erogeneous client environments while still addressing specific local needs."}, {"title": "E. Dual Prompt Alignment Constraint", "content": "As described in the last section, we introduce the shared prompt and private prompt mechanisms to address the chal- lenge of balancing global consistency and local adaptability in federated learning. The shared prompt is designed to capture global features across clients, ensuring that the model learns shared information from all client data. In contrast, the private prompt is tailored to each client's specific data, allowing the model to adapt to diverse local data distributions. To ensure that the private prompt and the shared prompt remain semantically aligned during the learning process, we propose the Dual Prompt Alignment Constraint. This constraint acts as a loss function that aligns the features learned by the shared and private prompts, maintaining semantic coherence while balancing global and local learning objectives.\nIn each client, the private prompt feature is denoted as ETp,i, while the shared prompt feature is represented as ETs. To ensure that each client's private prompt feature ETp,i aligns with its corresponding shared prompt feature ETs, we define the following alignment constraint loss function. Specifically, \ud835\udf11 and \ud835\udf13 are embedding functions that map the private prompt feature ETp,i and the shared prompt feature ETs, respectively, into a common feature space where alignment is enforced. This loss encourages each client\u2019s private prompt feature to be closer to its corresponding shared prompt feature than to the shared prompts of other clients:\nL'\u209a\u2090c = log ( 1 + \u2211 exp[- s\u22c5\u03c6(E_T\u209a,\u1d62)^T \u03c8(E_Ts) / s\u22c5\u03c6(E_T\u209a,\u1d62)^T \u03c8(E_Ts,\u2c7c)])\nwhere ETp,i represents the private prompt features of the i-th client, ETs represents the global shared prompt features, and ETs,j represents the shared prompt features of other clients. The embedding functions \ud835\udf11 and \ud835\udf13 ensure that both private and shared prompt features are mapped into a shared feature space for effective comparison. The parameter s serves as a scaling factor to modulate the alignment. By minimizing this loss, we ensure that each client's private prompt feature is aligned with its corresponding global shared prompt, preventing significant deviations. To generalize this across the federated learning framework, we aggregate the alignment losses from all clients into a global optimization objective. The overall dual prompt alignment constraint is defined as:\nL_PAC = (1/N) \u2211 log(exp(s\u22c5\u03c6(E_T\u209a,\u1d62)^T \u03c8(E_Ts)) / \u2211exp(s\u22c5\u03c6(E_T\u209a,\u1d62)^T \u03c8(E_Ts,\u2c7c)))\nwhere N is the total number of clients. By minimizing this loss function, the federated learning system updates both the shared prompt and the private prompt during each communication round, ensuring semantic alignment between global and local prompts. This optimization allows each client's model to effec- tively incorporate shared global knowledge while remaining flexible to adapt to the unique characteristics of local data distributions.\nThe dual prompt alignment constraint provides an efficient mechanism for maintaining semantic relevance between the shared and private prompts. It ensures that private prompts can adapt to local data without deviating from the semantic structure of global features. This approach employs a metric- learning-based strategy to align features, achieving semantic coherence while minimizing additional computational over- head. Furthermore, it enhances the robustness of federated learning by improving semantic alignment across heteroge- neous data distributions, ultimately promoting better general- ization and performance across clients."}, {"title": "F. Cross-Modal Feature Alignment Constraint", "content": "To further align the text features Er, including both the shared and private features obtained through the text encoder, with the image features Er in our modified CLIP model, we employ Cross-Modal Feature Alignment Constraint to handle the alignment between the two modalities. This constraint addresses the alignment challenges between the two modalities by leveraging Optimal Transport algorithm [54], enabling FedRSCLIP to effectively align cross-modal features through the cooperative use of global and local prompts. Specifically, we define the cost matrix C as the cosine distance between the image and text features, which is formulated as:\nC = 1 - cos(E\u2081, E_T)\nwhere C is a matrix based on cosine distance, representing the discrepancy between the image features E\u2081 and the text features \u0415\u0442. Cosine distance is used to measure the differ- ence between the two feature vectors, where a smaller value indicates greater similarity. Inspired by [25], we introduced an entropic regularization term into the original optimal transport objective to ensure that the prompts focus on relevant regions of the image, avoiding capturing irrelevant information. The final optimization objective is given by:\ndc,k(\u03b1, \u03b2) = min \u2211CT + \u03bbTlogT\nwhere U(\u03b1, \u03b2) is the constraint set of the transport plan, defined as:\nU(\u03b1, \u03b2) = {T \u2208 R\u00b2 | Tv \u2264 \u03b1, T1v = \u03b2}\nHere, T represents the transport plan matrix, while \u03b1 \u2208 R\u1d65 and \u03b2 \u2208 R\u00b2 are the marginal distributions of the image and text features. By adjusting the size of these marginal distributions, we can flexibly control the mapping between the prompts and the image feature map. Referring to [55], we further reformulate the above objective as a Kullback- Leibler (KL) projection. To accelerate the optimization of the transport plan, we adopt the fast implementation of the Dykstra algorithm [56]. Through the Dykstra algorithm, we efficiently scale the KL projection to rapidly solve the transport plan. After initializing Q = exp(-C/\u03bb) and v(\u2070) = 1\u1d67, the optimal solution is obtained through the following iterative update:\nT* = diag(u^(t))Qdiag(v^(t))\nwhere u^(t) and v^(t) are updated vectors at each iteration, with the initial value v^(\u2070) set as a vector of ones. After a few iterations, we quickly converge to the optimal transport plan T*. Once the optimal transport plan T* is obtained, we compute the Wasserstein distance dc,k, and the prediction probability p is rewritten from Equation (5) as follows:\np(y=k|I\u2c7c) = exp((1-dc,k)/\u03c4) / \u2211 exp((1-dc,c)/\u03c4)\nAfter obtaining the prediction probability p, we fix the trans- port plan T* and simultaneously optimize the learnable vectors in the shared and private prompts to ensure precise alignment between the visual and textual features, thereby improving both global generalization and local adaptability."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setting\nTo validate the effectiveness and advancements of our proposed method in remote sensing image classification, we benchmarked against several state-of-the-art federated learning algorithms. Specifically, we selected the original FedAvg[6", "FedProx[7": "and FedOTP[25", "in[57": "."}]}