{"title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability", "authors": ["Zhuoyan Xu", "Zhenmei Shi", "Yingyu Liang"], "abstract": "Large language models (LLMs) have emerged as powerful tools for many Al problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite LLM's tremendous success, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open question and largely ununderstood. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks that include linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving reasoning multiple steps, where each step represent one task, models typically underperform, and scaling up generally provide no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at https://github.com/OliverXUZY/LLM_Compose.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLM) have revolutionized the natural language processing (NLP) and general Al community. Recent advances, including ChatGPT (OpenAI, 2022), GPT4 (OpenAI, 2023), and Claude 3 (Anthropic, 2024) have shown success in various fields. As model scale increases, larger models exhibit new behavior known as emergence ability. One remarkable emergence is the in-context learning ability (ICL) (Brown et al., 2020), where a model can solve new tasks given only a few examples as input, without any parameters update. However, despite recent success, how LLM solves complex reasoning tasks, particularly not seen in pre-training, remains an open question and largely lacks understanding.\nIn this paper, we focus on the problem of how LLMs tackle composite tasks that incorporate multiple simple tasks. Specifically, we investigate whether a model trained/in-context learned on individual tasks can effectively integrate these skills to tackle combined challenges, which are intuitive and simple for humans. For instance, in Figure 1, if a human is given examples where words following an asterisk (*) will be capitalized and words surrounded by parenthesis will be permuted, one can also know words following an asterisk (*) surrounded by parenthesis will be capitalized and permuted simultaneously. This basic generalization seems trivial, yet we observe that LLMs fail to generalize this way."}, {"title": "1.1 Related Work", "content": "We briefly summarize related work and provide a detailed discussion in Appendix A."}, {"title": "2 Warm-up: A Failure Case for Composition", "content": "Our goal is to understand the behavior of LLMs on compositional reasoning tasks. As a warm-up, we evaluate the Capitalization & Swap tasks (Figure 1) on different models. Recall the tasks: given words of common objects, * represents the operation of capitalizing the letters, () represents swapping the positions of the two words. We consider the standard in-context learning setting, which concatenates input-output examples K = 10 and one test input as the prompt for LLM. We perform experiments across various LLM families, e.g., Llama families (Touvron et al., 2023) and GPTs (Radford et al., 2019; Black et al., 2021), see model details in Appendix B.\nEvaluation settings. To make thorough evaluations, we con- sider four settings: (1) capi- tal: only on the capitalization task; (2) swap: only on swap; (3) composite: in-context ex- amples are from simple tasks while the test input is about the composite task; (4) com- posite in-context: in-context examples and the test input are all drawn from the com- posite task. The composite in-context setting reduces the evaluation to another simple task, not requiring the model to composite the simple task ability but directly learning from the in-context examples. It serves as the gold standard performance for the composite task. See Table 1 for illustration.\nResults. In Figure 2, somewhat surprisingly, we observe that LLMs cannot solve the composite task, although they perform well on simple tasks. There is a significant gap between the performance in these settings. Models in Llama families can solve capital and swap with nearly ~90% accuracy but only achieve around 20% or below on the composite"}, {"title": "3 Variability of Compositional Performance", "content": "The experiment on Capitalization & Swap shows failure cases while existing studies reported some successful composite abilities (Levy et al., 2022; An et al., 2023b). This observation suggests a more refined perspective: LLMs exhibit variable compositional abilities, excelling in certain composite tasks while struggling in others. This section expands our exploration to additional composite tasks to further examine and understand this variability.\nWe introduce more composite tasks, including linguistic and logical challenges, wrapped as a testing suite. Similar to the Capitalization & Swap experiment, we design composite tasks that compose two simple tasks and evaluate the model in four settings: the two simple tasks, the composite setting, and the composite in-context setting (Table 1 show examples for the latter two). We consider two kinds of task: logical rules and linguistic translation.\nWe first choose two simple tasks and compose them to construct a composite task.\nTo address concerns about data leakage and the possibility that models encounter similar tasks during pretraining, we opt for synthetic data in this work. While it is challenging to guarantee that test data has never been seen during pretraining, we take significant steps to mitigate this risk. Specifically, we construct our compositional test data using a unique syntax and mapping mechanism. This approach substantially shifts the data distribution away from existing web-scale data, making it highly improbable that our test data has been encountered during pretraining. By doing so, we aim to create novel composite tasks that comprehensively evaluate the models' compositional abilities.\nSection 3.1 investigates logical tasks and Section 3.2 investigates translation tasks.\nWe perform experiments to answer the following questions: (Q1) How do LLMs perform in various tasks, where models might perform well in some scenarios while failing in others? (Q2) Does scaling up the model help in general? (Q3) Is the variability in performance relevant to the nature of tasks? Our experiments provide the following answers: (A1) A pattern of variable performance is observable across a range of composite tasks. (A2) Scaling-up helps when the model exhibits compositional ability for certain tasks but may not help when the model initially struggles. (A3) In tasks that involve processing inputs from varied segments or perspectives, especially simpler ones, the model tends to demonstrate compositional capabilities."}, {"title": "3.1 Composite Logical Rules", "content": "We enhance our suite of logical tasks by introducing a series of straightforward tasks that process either simple words or numerical values, with the output being a specific functional transformation of the input. These tasks are detailed in Table 2.\nComposite tasks are created by merging two simple tasks. We conceptualize simple tasks as functions, $f(\u00b7)$ and $g(\u00b7)$ that map inputs to their respective outputs. We identify two dis- tinct approaches to creating composite tasks: (1) Compose by parts: For inputs x, y, the result is f(x), g(y). One example is (A) + (F) in Table 3. If a numerical number is given, it will increment by one; if the word is given, the letters will be capitalized; if both are given, perform both operations.\n(2) Compose by steps: Given input x, the result is $f(g(x))$. One example is (A) + (B) in Table 3. We use customized symbols as function mapping for composing two simple tasks. Examples are in Figure 1 and Table 3. Following existing work, we use exact match accuracy to evaluate the performance since the output for these tasks is usually simple and short.\nResults. We provide our main results on composite tasks in Table 4. For the composed by parts tasks (A) + (F) and (D) + (F), the models show strong compositional ability: the composite accuracy is high, improves with increasing scale, and eventually reaches similar performance as the \u201cgold standard\u201d composite in-context setting, as highlighted in red numbers. We refer to these tasks as \u201cseparable composite tasks\u201d, which are relatively easy for the model to solve. On the compose-by-step tasks, we observe the models have various performances. For composite tasks with sequential reasoning steps, the models exhibit various performances. For tasks involving capitalization (A) or swap (B), the model has poor performance on a small scale (7b or lower) but has increased performance in increased model scale, such as 44% accuracy in (A) + (C) and 66% accuracy in (B) + (D). One exception is Llama1-65b, which has lower accuracy than a smaller-scale model. We conjecture it is due to some unknown inductive bias during the pretraining. On composite steps tasks in- volving the arithmetic calculation of numerical numbers (G) + (H), the model has the worst performance, and increasing the model scale does not provide benefits. A key observation is that compose-by-part tasks are separable compositions where the input can be broken down into two distinct segments. Such tasks are typically straightforward for a model to address. In all experiments, providing composed examples as in-context demonstrations"}, {"title": "3.2 Composite Linguistic Translation", "content": "Inspired by previous work in compositional generalization (An et al., 2023b; Levy et al., 2022; An et al., 2023a; Kim & Linzen, 2020), here we design our composite tasks by formal language translation tasks.\nOur translation tasks are mainly derived from semantic parsing task COGS (Kim & Linzen, 2020) and compositional generalization task COFE An et al. (2023b). These two datasets contain input as natural English sentences and output as a chain-ruled sentence following a customized grammar (see details in Appendix C). We construct two composite tasks centered on compositional generalization utilizing the training datasets to create in-context examples. See details in Appendix C.\nWe use the word error rate (WER) as the metric. It measures the minimum number of editing operations (deletion, insertion, and substitution) required to transform one sentence into another and is common for speech recognition or machine translation evaluations.\nResults. Figure 3 shows that LLMs can handle these composite tasks. The WER on the composite task is decent and improves with increasing model scale, particularly in Llamma2 models. These confirm the composite abilities of the models in these tasks.\nHere, we notice that both composite tasks are separable composite tasks. If we break down these sentences into sub-sentences and phrases, the simple task operations occur in different parts or perspectives of the input sentences. So, the results here provide further support for composite abilities on separable composite tasks, where simple tasks that form the composite task are related to inputs from different parts or perspectives.\nWe also observed the LLM ex- hibits better compositional abil- ity on linguistic tasks than on logical tasks. We conclude nat- ural language inputs can indeed help language models understand concepts better than special sym- bols or code. Natural language provides a richer context, which aligns better with how these mod- els are trained on large text cor- pora. In contrast, logical and nu- merical tasks often rely on more rigid structures, which makes it harder for models to generalize without explicit training on such patterns.\nDiscussion. We observe the ca- pability of models to handle com- posite tasks is significantly influ- enced by the task characteristics. If composite tasks contain sim- ple tasks related to different parts or perspectives of the input, the model will tackle the composite tasks well.\nOne natural explanation is that the model processes the input in some hidden embedding space and de- composes the embedding of the input into different \u201cregions\u201d. Here, each region is dedicated to specific types of information and thus related to different tasks, such as word-level modifications, arithmetic calcula- tions, mapping mechanisms, semantic categorization, linguistic acceptability, or sentiment analysis. Then, suppose the two simple tasks correspond to two different task types that relate to separate regions of the embedding. In that case, the model can effectively manage"}, {"title": "4 Theoretical Analysis", "content": "Despite the complex nature of non-linearity in transformers in LLMs, we note it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help us better understand the phenomenon. In this section, we analyze a linear attention module and aim to provide rigorous proof about why LLMs can achieve compositional ability in some simple cases that could shed light on the more intricate behaviors observed in LLMs.\nIn-context learning. We follow existing work (Aky\u00fcrek et al., 2023; Garg et al., 2022; Mahankali et al., 2023) with slight generalization to K simple tasks. A labeled example is denoted as (x, y) where x \u2208 Rd, y \u2208 RK. In a simple task k \u2208 [K], y has only one non-zero entry $y^{(k)}$. In a composite task, y can have non-zero entries in dimensions corresponding to the combined simple tasks. The model takes a prompt $(X_1,Y_1,..., X_N, Y_N, x_q)$ as input, which contains N in-context examples $(x_i, y_i)'s$ and a query $x_q$, and aims to predict $\\hat{y}_q$ close to the true label $y_q$ for $x_q$. The prompt is usually stacked into an embedding matrix:\n$E := \\begin{pmatrix}X_1 & X_2 & ... & X_N & X_q \\\\ Y_1 & Y_2 & ... & Y_N & 0\\end{pmatrix} \\in \\mathbb{R}^{d_e \\times (N+1)}$ where $d_e = d + K$. In in-context learning, we first pretrain the model using training prompts and then evaluate the model with evaluation prompts; see details below.\nPretraining procedure. We have B training data indexed by $\\tau$, each containing an input prompt $(X_{\\tau,1}, Y_{\\tau,1},\\cdots, X_{\\tau,N},Y_{\\tau,N},X_{\\tau,q})$ and a corresponding true label $Y_{\\tau,9}$. Consider the following empirical loss: $\\hat{L}(\\theta) = \\sum_{\\tau=1}^{B} L_k(\\theta) = \\frac{1}{2B} \\sum_{k=1}^{K} \\sum_{\\tau=1}^{B} ||\\hat{Y}_{\\tau,9} - Y_{\\tau,9}||^2$ and the population loss (i.e., $B \\rightarrow \\infty$): $L(\\theta) = \\mathbb{E}_{X_{\\tau,1},Y_{\\tau,1},\\cdots,X_{\\tau,N},Y_{\\tau,N},X_{\\tau,q}} [(\\hat{Y}_{\\tau,q} - Y_{\\tau,q})^2]$.\nEvaluation procedure. We now detail how to evaluate the model on downstream composite tasks. We consider the downstream classification task to be a multi-class classification problem, where the output label is a K-dimensional vector, and each entry corresponds to a simple binary classification task. For any given simple task k, the binary classification label is given by $sgn(y_q^{(k)})$, where sgn is the sign function. Similarly, our prediction is $\\widehat{sgn}(\\hat{y}_q^{(k)}) = sgn(\\hat{y}_q^{(k)})$. The accuracy of a composite task is defined as $Acc_\\theta(\\langle X_1, Y_1,..., X_N, X_q \\rangle) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{I}(sgn(y_q^{(k)}) = sgn(\\hat{y}_q^{(k)}))$.  We denote it as $Acc_\\theta(\\{x_i, Y_i\\}_{i=1}^{N})$. Here we denote the model performance on each task as separate dimension, (e.g.letter capitalization, num- bers increment), and the performance of composite tasks as the aggregation of multiple dimensions.\nData. Assume $x \\stackrel{i.i.d}{\\sim} \\mathcal{N}(0, A)$, where $A \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix. Assume $y = Wx$, where $W \\in \\mathbb{R}^{K \\times d}$. For any simple task $k \\in [K]$, its label is the k-th entry of y, which is $y^{(k)} = (w^{(k)}, x)$, where $w^{(k)}$ is the k-th row of W. We assume each task weight $w^{(k)} \\stackrel{i.i.d}{\\sim} \\mathcal{N} (0, I_a)$.\nLinear self-attention networks. These networks are widely studied (Von Oswald et al., 2023; Aky\u00fcrek et al., 2023; Garg et al., 2022; Zhang et al., 2023b; Shi et al., 2023b). Following them, we consider the following linear self-attention network with parameters"}, {"title": "4.1 Problem Setup", "content": "Despite the complex nature of non-linearity in transformers in LLMs, we note it is useful to appeal to the simple case of linear models to see if there are parallel insights that can help us better understand the phenomenon. In this section, we analyze a linear attention module and aim to provide rigorous proof about why LLMs can achieve compositional ability in some simple cases that could shed light on the more intricate behaviors observed in LLMs.\nIn-context learning. We follow existing work (Aky\u00fcrek et al., 2023; Garg et al., 2022; Mahankali et al., 2023) with slight generalization to K simple tasks. A labeled example is denoted as (x, y) where x \u2208 Rd, y \u2208 RK. In a simple task k \u2208 [K], y has only one non-zero entry $y^{(k)}$. In a composite task, y can have non-zero entries in dimensions corresponding to the combined simple tasks. The model takes a prompt $(X_1,Y_1,..., X_N, Y_N, x_q)$ as input, which contains N in-context examples $(x_i, y_i)'s$ and a query $x_q$, and aims to predict $\\hat{y}_q$ close to the true label $y_q$ for $x_q$. The prompt is usually stacked into an embedding matrix:\n$E := \\begin{pmatrix}X_1 & X_2 & ... & X_N & X_q \\\\ Y_1 & Y_2 & ... & Y_N & 0\\end{pmatrix} \\in \\mathbb{R}^{d_e \\times (N+1)}$ where $d_e = d + K$. In in-context learning, we first pretrain the model using training prompts and then evaluate the model with evaluation prompts; see details below.\nPretraining procedure. We have B training data indexed by $\\tau$, each containing an input prompt $(X_{\\tau,1}, Y_{\\tau,1},\\cdots, X_{\\tau,N},Y_{\\tau,N},X_{\\tau,q})$ and a corresponding true label $Y_{\\tau,9}$. Consider the following empirical loss: $\\hat{L}(\\theta) = \\sum_{\\tau=1}^{B} L_k(\\theta) = \\frac{1}{2B} \\sum_{k=1}^{K} \\sum_{\\tau=1}^{B} ||\\hat{Y}_{\\tau,9} - Y_{\\tau,9}||^2$ and the population loss (i.e., $B \\rightarrow \\infty$): $L(\\theta) = \\mathbb{E}_{X_{\\tau,1},Y_{\\tau,1},\\cdots,X_{\\tau,N},Y_{\\tau,N},X_{\\tau,q}} [(\\hat{Y}_{\\tau,q} - Y_{\\tau,q})^2]$.\nEvaluation procedure. We now detail how to evaluate the model on downstream composite tasks. We consider the downstream classification task to be a multi-class classification problem, where the output label is a K-dimensional vector, and each entry corresponds to a simple binary classification task. For any given simple task k, the binary classification label is given by $sgn(y_q^{(k)})$, where sgn is the sign function. Similarly, our prediction is $\\widehat{sgn}(\\hat{y}_q^{(k)}) = sgn(\\hat{y}_q^{(k)})$. The accuracy of a composite task is defined as $Acc_\\theta(\\langle X_1, Y_1,..., X_N, X_q \\rangle) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{I}(sgn(y_q^{(k)}) = sgn(\\hat{y}_q^{(k)}))$.  We denote it as $Acc_\\theta(\\{x_i, Y_i\\}_{i=1}^{N})$. Here we denote the model performance on each task as separate dimension, (e.g.letter capitalization, num- bers increment), and the performance of composite tasks as the aggregation of multiple dimensions.\nData. Assume $x \\stackrel{i.i.d}{\\sim} \\mathcal{N}(0, A)$, where $A \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix. Assume $y = Wx$, where $W \\in \\mathbb{R}^{K \\times d}$. For any simple task $k \\in [K]$, its label is the k-th entry of y, which is $y^{(k)} = (w^{(k)}, x)$, where $w^{(k)}$ is the k-th row of W. We assume each task weight $w^{(k)} \\stackrel{i.i.d}{\\sim} \\mathcal{N} (0, I_a)$.\nLinear self-attention networks. These networks are widely studied (Von Oswald et al., 2023; Aky\u00fcrek et al., 2023; Garg et al., 2022; Zhang et al., 2023b; Shi et al., 2023b). Following them, we consider the following linear self-attention network with parameters"}, {"title": "4.2 Theoretical Results", "content": "In this section, we present our theoretical results. We explain the observation in empirical results through the lens of confined supports in input embeddings corresponding to separate subspaces (modeling separable composition). We provide theoretical justification showing that separable composite task composite tasks whose inputs are composed by components adhere to certain conditions where models exhibit satisfactory performance. Models will fail when such conditions are violated. We first introduce the basic setup and definitions.\nDisjoint subspaces of simple tasks. Recall that x lies in a d-dimensional space where each dimension represents a different characteristic. A simple task may depend only on a subset of these dimensions since its label only depends on a few features. Let S = [d] represent the dimensions of x. For a task k, the output $y^{(k)} = (w^k, x)$ depends on a subset of dimensions in x. Denote this subset by $I_K \\subset S$ and call it the active index set for task k.\nIn the following, we always assume that the K tasks have disjoint subspaces: for any two tasks k \u2260 g, their active index sets K, and G are disjoint, i.e., $K \\cap G = \\O$. In practice, the dimensions within K could be associated with numerical arithmetic operations, while those in G might pertain to semantic analysis. This illustrates the model's approach to address these tasks in their respective subspaces.\nWe now introduce a mild assumption regarding the distribution of input embeddings.\nAssumption 1. Given two disjoint subspaces K and G, the covariance matrix A of the input distribution can be segmented into block matrices AKK, AKG, AGK, and AGG, then we assume $\u03c3_{max}(A_{KG}) = \u03c3_{max}(A_{GK}) \\leq \\epsilon$ for constant e, where \u03c3(\u00b7) denote the singular value of matrix.\nAssumption 1 implies that for two separate simple tasks, each associated with its respective feature subspace IK and G, the covariance between these two sets of features is bounded by a constant value. This is a natural assumption when inputs of composite tasks can be decomposed into parts. Suppose we have input embeddings from two tasks: arithmetic computations and semantic analysis. This assumption suggests that the feature subspaces of the input embeddings for two tasks are almost independent.\nWe now define confined support, which means that each task's input embedding only has support within its feature subspace.\nDefinition 2 (Confined Support). We say a task has confined support if the input x only has larger singular values within its active index set. The norm of entries outside the active index set is bounded by a small constant d.\nThis definition shows that each simple task only has large values within its corresponding subsets of dimensions of input embeddings. For example, let K represent the first d\u2081 dimensions of an input vector x, and G account for the remaining d2 dimensions, with the total dimension being d = d\u2081 + d2. The examples from task k will have input as x = (X1, X\u03b4\u2081) where x1 \u2208 Rd1, X8\u2081 \u2208 Rd2, ||x8\u2081 || \u2264 \u03b4. Similarly, the examples from task g will have inputs as x = (X82, X2).\nWe now present our results of the compositional ability under a confined support of x.\nTheorem 1. Consider distinct tasks k and g with corresponding examples Sk, Sg. If two tasks have confined support, and Assumption 1 is true, then with high probability, the model has the"}, {"title": "5 Conclusion", "content": "In this work, we presented a distinct pattern in LLMs' behaviors when tackling composite tasks. We observed that if the composite task can be separated into two simple tasks whose inputs are from distinct perspectives, the models exhibit decent compositional ability. Otherwise, LLMs will struggle, and scaling up the model size may not offer improvement. We illustrated this behavior across a variety of logical and linguistic challenges. We extended our discussion to the role of input embeddings in affecting model performance, providing a theoretical backup that connects the nature of tasks to how inputs are processed. We anticipate that our research will shed light on the compositional capabilities and reasoning of LLMs, and stimulate further exploration in this direction."}, {"title": "Impact Statements", "content": "Our work aims to improve the understanding of in-context learning capabilities of Large Language Models (LLMs) in the handling of composite tasks. Our paper is mostly theoretical in nature, and thus, we foresee no immediate negative ethical impact. We illustrate the empirical behavior of LLMs on complex reasoning tasks and provide a theoretical explanation for it. In the long term, we hope our work may inspire effective algorithm design and better understanding and employment of LLMs."}]}