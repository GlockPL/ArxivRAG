{"title": "Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability", "authors": ["Zhuoyan Xu", "Zhenmei Shi", "Yingyu Liang"], "abstract": "Large language models (LLMs) have emerged as powerful tools for many\nAl problems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or\nmore simple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach com-\nposite tasks, especially those not encountered during the pretraining phase,\nremains an open question and largely ununderstood. In this study, we\ndelve into the ICL capabilities of LLMs on composite tasks, with only sim-\nple tasks as in-context examples. We develop a test suite of composite tasks\nthat include linguistic and logical challenges and perform empirical studies\nacross different LLM families. We observe that models exhibit divergent\nbehaviors: (1) For simpler composite tasks that apply distinct mapping\nmechanisms to different input segments, the models demonstrate decent\ncompositional ability, while scaling up the model enhances this ability; (2)\nfor more complex composite tasks that involving reasoning multiple steps,\nwhere each step represent one task, models typically underperform, and\nscaling up generally provide no improvements. We offer theoretical anal-\nsis in a simplified setting, explaining that models exhibit compositional\ncapability when the task handles different input parts separately. We believe\nour work sheds new light on the capabilities of LLMs in solving composite\ntasks regarding the nature of the tasks and model scale. Our dataset and\ncode are available at https://github.com/OliverXUZY/LLM_Compose.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLM) have revolutionized the natural language\nprocessing (NLP) and general Al community. Recent advances, including ChatGPT (OpenAI,\n2022), GPT4 (OpenAI, 2023), and Claude 3 (Anthropic, 2024) have shown success in various\nfields. As model scale increases, larger models exhibit new behavior known as emergence\nability. One remarkable emergence is the in-context learning ability (ICL) (Brown et al.,\n2020), where a model can solve new tasks given only a few examples as input, without any\nparameters update. However, despite recent success, how LLM solves complex reasoning\ntasks, particularly not seen in pre-training, remains an open question and largely lacks\nunderstanding.\nIn this paper, we focus on the problem of how LLMs tackle composite tasks that incorporate\nmultiple simple tasks. Specifically, we investigate whether a model trained/in-context\nlearned on individual tasks can effectively integrate these skills to tackle combined chal-\nlenges, which are intuitive and simple for humans. For instance, in Figure 1, if a human is\ngiven examples where words following an asterisk (*) will be capitalized and words sur-\nrounded by parenthesis will be permuted, one can also know words following an asterisk\n(*) surrounded by parenthesis will be capitalized and permuted simultaneously. This basic\ngeneralization seems trivial, yet we observe that LLMs fail to generalize this way."}, {"title": "1.1 Related Work", "content": "We briefly summarize related work and provide a detailed discussion in Appendix A."}, {"title": "2 Warm-up: A Failure Case for Composition", "content": "Our goal is to understand the behavior of LLMs on compositional reasoning tasks. As a\nwarm-up, we evaluate the Capitalization & Swap tasks (Figure 1) on different models.\nRecall the tasks: given words of common objects, * represents the operation of capitalizing\nthe letters, () represents swapping the positions of the two words. We consider the standard\nin-context learning setting, which concatenates input-output examples K = 10 and one test\ninput as the prompt for LLM. We perform experiments across various LLM families, e.g.,\nLlama families (Touvron et al., 2023) and GPTs (Radford et al., 2019; Black et al., 2021), see\nmodel details in Appendix B.\nEvaluation settings. To make\nthorough evaluations, we con-\nsider four settings: (1) capi-\ntal: only on the capitalization\ntask; (2) swap: only on swap;\n(3) composite: in-context ex-\namples are from simple tasks\nwhile the test input is about\nthe composite task; (4) com-\nposite in-context: in-context\nexamples and the test input\nare all drawn from the com-\nposite task. The composite\nin-context setting reduces the\nevaluation to another simple\ntask, not requiring the model\nto composite the simple task\nability but directly learning from the in-context examples. It serves as the gold standard\nperformance for the composite task. See Table 1 for illustration.\nResults. In Figure 2, somewhat surprisingly, we observe that LLMs cannot solve the\ncomposite task, although they perform well on simple tasks. There is a significant gap\nbetween the performance in these settings. Models in Llama families can solve capital and\nswap with nearly ~90% accuracy but only achieve around 20% or below on the composite"}, {"title": "3 Variability of Compositional Performance", "content": "The experiment on Capitalization & Swap shows failure cases while existing studies reported\nsome successful composite abilities (Levy et al., 2022; An et al., 2023b). This observation\nsuggests a more refined perspective: LLMs exhibit variable compositional abilities, excelling\nin certain composite tasks while struggling in others. This section expands our exploration\nto additional composite tasks to further examine and understand this variability.\nWe introduce more composite tasks, including linguistic and logical challenges, wrapped\nas a testing suite. Similar to the Capitalization & Swap experiment, we design composite\ntasks that compose two simple tasks and evaluate the model in four settings: the two simple\ntasks, the composite setting, and the composite in-context setting (Table 1 show examples\nfor the latter two). We consider two kinds of task: logical rules and linguistic translation.\nWe first choose two simple tasks and compose them to construct a composite task.\nTo address concerns about data leakage and the possibility that models encounter similar\ntasks during pretraining, we opt for synthetic data in this work. While it is challenging to\nguarantee that test data has never been seen during pretraining, we take significant steps\nto mitigate this risk. Specifically, we construct our compositional test data using a unique\nsyntax and mapping mechanism. This approach substantially shifts the data distribution\naway from existing web-scale data, making it highly improbable that our test data has been\nencountered during pretraining. By doing so, we aim to create novel composite tasks that\ncomprehensively evaluate the models' compositional abilities.\nSection 3.1 investigates logical tasks and Section 3.2 investigates translation tasks.\nWe perform experiments to answer the following questions: (Q1) How do LLMs perform in\nvarious tasks, where models might perform well in some scenarios while failing in others?\n(Q2) Does scaling up the model help in general? (Q3) Is the variability in performance\nrelevant to the nature of tasks? Our experiments provide the following answers: (A1)\nA pattern of variable performance is observable across a range of composite tasks. (A2)\nScaling-up helps when the model exhibits compositional ability for certain tasks but may not\nhelp when the model initially struggles. (A3) In tasks that involve processing inputs from\nvaried segments or perspectives, especially simpler ones, the model tends to demonstrate\ncompositional capabilities."}, {"title": "3.1 Composite Logical Rules", "content": "We enhance our suite of logical tasks by introducing a series of straightforward tasks that\nprocess either simple words or numerical values, with the output being a specific functional\ntransformation of the input. These tasks are detailed in Table 2.\nComposite tasks are created\nby merging two simple tasks.\nWe conceptualize simple tasks\nas functions, $f(\u00b7)$ and $g(\u00b7)$ that\nmap inputs to their respective\noutputs. We identify two dis-\ntinct approaches to creating\ncomposite tasks: (1) Compose\nby parts: For inputs $x, y$, the\nresult is $f(x), g(y)$. One exam-\nple is (A) + (F) in Table 3. If\na numerical number is given,\nit will increment by one; if the\nword is given, the letters will be capitalized; if both are given, perform both operations.\n(2) Compose by steps: Given input x, the result is $f(g(x))$. One example is (A) + (B) in\nTable 3. We use customized symbols as function mapping for composing two simple tasks.\nExamples are in Figure 1 and Table 3. Following existing work, we use exact match accuracy\nto evaluate the performance since the output for these tasks is usually simple and short.\nResults. We provide our main results on composite tasks in Table 4. For the composed\nby parts tasks (A) + (F) and (D) + (F), the models show strong compositional ability: the\ncomposite accuracy is high, improves with increasing scale, and eventually reaches similar\nperformance as the \u201cgold standard\u201d composite in-context setting, as highlighted in red\nnumbers. We refer to these tasks as \u201cseparable composite tasks\u201d, which are relatively easy\nfor the model to solve. On the compose-by-step tasks, we observe the models have various\nperformances. For composite tasks with sequential reasoning steps, the models exhibit\nvarious performances. For tasks involving capitalization (A) or swap (B), the model has\npoor performance on a small scale (7b or lower) but has increased performance in increased\nmodel scale, such as 44% accuracy in (A) + (C) and 66% accuracy in (B) + (D). One exception\nis Llama1-65b, which has lower accuracy than a smaller-scale model. We conjecture it is\ndue to some unknown inductive bias during the pretraining. On composite steps tasks in-\nvolving the arithmetic calculation of numerical numbers (G) + (H), the model has the worst\nperformance, and increasing the model scale does not provide benefits. A key observation\nis that compose-by-part tasks are separable compositions where the input can be broken\ndown into two distinct segments. Such tasks are typically straightforward for a model to\naddress. In all experiments, providing composed examples as in-context demonstrations"}, {"title": "3.2 Composite Linguistic Translation", "content": "Inspired by previous work in compositional generalization (An et al., 2023b; Levy et al.,\n2022; An et al., 2023a; Kim & Linzen, 2020), here we design our composite tasks by formal\nlanguage translation tasks.\nOur translation tasks are mainly derived from semantic parsing task COGS (Kim & Linzen,\n2020) and compositional generalization task COFE An et al. (2023b). These two datasets\ncontain input as natural English sentences and output as a chain-ruled sentence following\na customized grammar (see details in Appendix C). We construct two composite tasks\ncentered on compositional generalization utilizing the training datasets to create in-context\nexamples. See details in Appendix C.\nWe use the word error rate (WER) as the metric. It measures the minimum number of\nediting operations (deletion, insertion, and substitution) required to transform one sentence\ninto another and is common for speech recognition or machine translation evaluations."}, {"title": "4 Theoretical Analysis", "content": "Despite the complex nature of non-linearity in transformers in LLMs, we note it is useful to\nappeal to the simple case of linear models to see if there are parallel insights that can help\nus better understand the phenomenon. In this section, we analyze a linear attention module\nand aim to provide rigorous proof about why LLMs can achieve compositional ability in\nsome simple cases that could shed light on the more intricate behaviors observed in LLMs."}, {"title": "4.1 Problem Setup", "content": "In-context learning. We follow existing work (Aky\u00fcrek et al., 2023; Garg et al., 2022;\nMahankali et al., 2023) with slight generalization to K simple tasks. A labeled example is\ndenoted as $(x, y)$ where $x \\in \\mathbb{R}^d$, $y \\in \\mathbb{R}^K$. In a simple task $k \\in [K]$, $y$ has only one non-zero\nentry $y^{(k)}$. In a composite task, $y$ can have non-zero entries in dimensions corresponding\nto the combined simple tasks. The model takes a prompt $(X_1,Y_1,..., X_N, Y_N, x_q)$ as input,\nwhich contains N in-context examples $(x_i, y_i)$'s and a query $x_q$, and aims to predict $\\hat{y}_q$\nclose to the true label $y_q$ for $x_q$. The prompt is usually stacked into an embedding matrix:\n$E := \\begin{pmatrix}\nX_1 & X_2 & ... & X_N & X_q \\\\\nY_1 & Y_2 & ... & Y_N & 0\n\\end{pmatrix} \\in \\mathbb{R}^{d_e \\times (N+1)}$ where $d_e = d + K$. In in-context learning, we\nfirst pretrain the model using training prompts and then evaluate the model with evaluation\nprompts; see details below.\nPretraining procedure. We have B training data indexed by $\\tau$, each containing an input\nprompt $(\\mathcal{X}_{\\tau,1}, \\mathcal{Y}_{\\tau,1},\\cdots, \\mathcal{X}_{\\tau,N},\\mathcal{Y}_{\\tau,N},\\mathcal{X}_{\\tau,q})$ and a corresponding true label $\\mathcal{Y}_{\\tau,q}$. Consider the\nfollowing empirical loss: $\\hat{\\mathcal{L}}(\\theta) = \\sum_{\\tau=1}^{B} \\mathcal{L}_k(\\theta) = \\frac{1}{2B} \\sum_{\\tau=1}^{B} ||\\hat{\\mathcal{Y}}_{\\tau,q} - \\mathcal{Y}_{\\tau,q}||_2^2$ and the population\nloss (i.e., $B \\rightarrow \\infty$): $\\mathcal{L}(\\theta) = \\mathbb{E}_{\\mathcal{X}_{\\tau,1},\\mathcal{Y}_{\\tau,1},\\cdots, \\mathcal{X}_{\\tau,N},\\mathcal{Y}_{\\tau,N},\\mathcal{X}_{\\tau,q}}[ (\\mathcal{Y}_{\\tau,q} - \\hat{\\mathcal{Y}}_{\\tau,q})^2]$.\nEvaluation procedure. We now detail how to evaluate the model on downstream composite\ntasks. We consider the downstream classification task to be a multi-class classification\nproblem, where the output label is a K-dimensional vector, and each entry corresponds to\na simple binary classification task. For any given simple task k, the binary classification\nlabel is given by $\\text{sgn}(y_q^{(k)})$, where sgn is the sign function. Similarly, our prediction is\n$\\text{sgn}(\\hat{y}_q^{(k)})$. The accuracy of a composite task is defined as $\\text{Acc}_c(\\mathcal{X}_1,..., \\mathcal{Y}_N, \\mathcal{X}_q) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathbb{I}(\\text{sgn} (y_q^{(k)}) = \\text{sgn}(\\hat{y}_q^{(k)}))$. We denote it as\n$\\text{Acc}_c((\\{X_i, Y_i\\}_{i=1}^N)$. Here we denote the\nmodel performance on each task as separate dimension, (e.g.letter capitalization, num-\nbers increment), and the performance of composite tasks as the aggregation of multiple\ndimensions.\nData. Assume $x_i \\stackrel{i.i.d}{\\sim} \\mathcal{N}(0, A)$, where $A \\in \\mathbb{R}^{d \\times d}$ is the covariance matrix. Assume $y = Wx$,\nwhere $W \\in \\mathbb{R}^{K \\times d}$. For any simple task $k \\in [K]$, its label is the k-th entry of y, which is $y^{(k)} =$\n$\\langle w^{(k)}, x \\rangle$, where $w^{(k)}$ is the k-th row of W. We assume each task weight $w^{(k)} \\stackrel{i.i.d}{\\sim} \\mathcal{N}(0, I_d)$.\nLinear self-attention networks. These networks are widely studied (Von Oswald et al.,\n2023; Aky\u00fcrek et al., 2023; Garg et al., 2022; Zhang et al., 2023b; Shi et al., 2023b). Fol-\nlowing them, we consider the following linear self-attention network with parameters"}, {"title": "4.2 Theoretical Results", "content": "In this section, we present our theoretical results. We explain the observation in empirical\nresults through the lens of confined supports in input embeddings corresponding to separate\nsubspaces (modeling separable composition). We provide theoretical justification showing\nthat separable composite task composite tasks whose inputs are composed by components\nadhere to certain conditions where models exhibit satisfactory performance. Models will\nfail when such conditions are violated. We first introduce the basic setup and definitions.\nDisjoint subspaces of simple tasks. Recall that x lies in a d-dimensional space where each\ndimension represents a different characteristic. A simple task may depend only on a subset\nof these dimensions since its label only depends on a few features. Let S = [d] represent the\ndimensions of x. For a task k, the output $y^{(k)} = \\langle w^k, x \\rangle$ depends on a subset of dimensions\nin x. Denote this subset by $I_K \\subset S$ and call it the active index set for task k.\nIn the following, we always assume that the K tasks have disjoint subspaces: for any two\ntasks k \u2260 g, their active index sets K, and G are disjoint, i.e., $K \\cap G = \\emptyset$. In practice, the\ndimensions within K could be associated with numerical arithmetic operations, while those\nin G might pertain to semantic analysis. This illustrates the model's approach to address\nthese tasks in their respective subspaces.\nWe now introduce a mild assumption regarding the distribution of input embeddings.\nAssumption 1. Given two disjoint subspaces K and G, the covariance matrix A of the input\ndistribution can be segmented into block matrices $A_{KK}$, $A_{KG}$, $A_{GK}$, and $A_{GG}$, then we assume\n$\\sigma_{\\max}(A_{KG}) = \\sigma_{\\max}(\\bar{A}_{GK}) \\leq \\epsilon$ for constant $\\epsilon$, where $\\sigma(\\cdot)$ denote the singular value of matrix.\nAssumption 1 implies that for two separate simple tasks, each associated with its respective\nfeature subspace $I_K$ and G, the covariance between these two sets of features is bounded\nby a constant value. This is a natural assumption when inputs of composite tasks can be\ndecomposed into parts. Suppose we have input embeddings from two tasks: arithmetic\ncomputations and semantic analysis. This assumption suggests that the feature subspaces\nof the input embeddings for two tasks are almost independent.\nWe now define confined support, which means that each task's input embedding only has\nsupport within its feature subspace.\nDefinition 2 (Confined Support). We say a task has confined support if the input x only has\nlarger singular values within its active index set. The norm of entries outside the active index set is\nbounded by a small constant $\\delta$.\nThis definition shows that each simple task only has large values within its corresponding\nsubsets of dimensions of input embeddings. For example, let K represent the first $d_1$\ndimensions of an input vector x, and G account for the remaining $d_2$ dimensions, with the\ntotal dimension being $d = d_1 + d_2$. The examples from task k will have input as $x = (X_1, X_{\\delta_1})$\nwhere $x_1 \\in \\mathbb{R}^{d_1}, X_{\\delta_1} \\in \\mathbb{R}^{d_2}, ||X_{\\delta_1} || \\leq \\delta$. Similarly, the examples from task g will have inputs\nas $x = (X_{\\delta_2}, X_2)$.\nWe now present our results of the compositional ability under a confined support of x.\nTheorem 1. Consider distinct tasks k and g with corresponding examples $S_k, S_g$. If two tasks\nhave confined support, and Assumption 1 is true, then with high probability, the model has the"}, {"title": "5 Conclusion", "content": "In this work, we presented a distinct pattern in LLMs' behaviors when tackling composite\ntasks. We observed that if the composite task can be separated into two simple tasks\nwhose inputs are from distinct perspectives, the models exhibit decent compositional ability.\nOtherwise, LLMs will struggle, and scaling up the model size may not offer improvement.\nWe illustrated this behavior across a variety of logical and linguistic challenges. We extended\nour discussion to the role of input embeddings in affecting model performance, providing\na theoretical backup that connects the nature of tasks to how inputs are processed. We\nanticipate that our research will shed light on the compositional capabilities and reasoning\nof LLMs, and stimulate further exploration in this direction."}, {"title": "Impact Statements", "content": "Our work aims to improve the understanding of in-context learning capabilities of Large\nLanguage Models (LLMs) in the handling of composite tasks. Our paper is mostly theoreti-\ncal in nature, and thus, we foresee no immediate negative ethical impact. We illustrate the\nempirical behavior of LLMs on complex reasoning tasks and provide a theoretical explana-\ntion for it. In the long term, we hope our work may inspire effective algorithm design and\nbetter understanding and employment of LLMs."}]}