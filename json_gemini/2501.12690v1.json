{"title": "Growth strategies for arbitrary DAG neural architectures", "authors": ["Stella Douka", "Manon Verbockhaven", "Th\u00e9o Rudkiewicz", "St\u00e9phane Rivaud", "Fran\u00e7ois P. Landes", "Sylvain Chevallier", "Guillaume Charpiat"], "abstract": "Deep learning has shown impressive results obtained at the cost of training huge neural networks. However, the larger the architecture, the higher the computational, financial, and environmental costs during training and inference. We aim at reducing both training and inference durations. We focus on Neural Architecture Growth, which can increase the size of a small model when needed, directly during training using information from the backpropagation. We expand existing work and freely grow neural networks in the form of any Directed Acyclic Graph by reducing expressivity bottlenecks in the architecture. We explore strategies to reduce excessive computations and steer network growth toward more parameter-efficient architectures.", "sections": [{"title": "1 Introduction", "content": "A common practice to train a deep architecture on a novel problem is to rely on over-parametrization - meaning overly large and deep networks as it facilitates optimization and yields better results. While it is possible to start with small models that are faster to train, they often lack expressivity, preventing them from accurately fitting the data. Hence, most literature focuses on training large neural networks, and then using pruning, distillation, or compression to reduce energy consumption in the inference phase. This includes training the large models and fine-tuning them, requiring a tremendous amount of computational power and training time. On the contrary, Neural Architecture Search methods (NAS) usually train multiple architectures from a finite set and choose the one that performs the best, occasionally with a cost trade-off. This is extremely resource-consuming and even Differential Architecture Search requires 1.5 GPU days to train on CIFAR-10 (Liu et al., 2019). This is where Neural Architecture Growth comes at hand. The idea is to start with the simplest possible neural network and grow it by adding neurons in existing layers or adding entirely new layers, according to the information brought by the backpropagation. Such information can indeed be used to go beyond the usual limitations in small network training, to tackle potential optimization and expressivity issues. Evci et al. (2022) propose GradMax, an approach that initializes all new output weights to maximize the gradient, and all new input weights to zero, thus preserving the function's output. Maile et al. (2022) introduce NORTH, which measures the redundancy of the network as the orthogonality between post-activations. The authors of Firefly (Wu et al., 2020) choose every time to split existing neurons or create new ones, which also includes adding new layers. They keep all changes local and decide where to grow by solving the steepest-descent optimization problem. Verbockhaven et al. (2024) introduce the notion of expressivity bottleneck to solve optimization issues in a sequential architecture by increasing its layers' width during training. In this paper, we extend the work of Verbockhaven et al. by adding new layers on the fly, thus being able, for the first time, to grow neural networks in the form of any Directed Acyclic Graph. We test different strategies to grow the network efficiently and reduce energy costs."}, {"title": "2 Methodology", "content": "Expressivity bottleneck. In Figure 1 we present the concept of expressivity bottleneck (Verbockhaven et al., 2024). We define the manifold $F_A := \\{f_\\theta | \\theta \\in \\Theta\\}$ as the functional space parameterized by a neural network, that is, the set of all possible functions one can represent by instantiating parameters of a fixed architecture A. The tangent space at $f_\\theta$, namely $T_A := \\{ \\frac{\\partial f_\\theta}{\\partial \\theta} \\delta\\theta | s.t. \\delta\\theta \\in \\Theta_A\\}$, consists of all the possible functions one can reach on the current manifold $F_A$ using small parameter updates, e.g. by gradient descent. Now, let us denote by $v_\\nabla$ the desired update for the function $f_\\theta$ when we are not constrained by the current architecture:\n\n$v_{\\nabla}(x) := -\\nabla_{f_{\\theta}(x)}L(f_{\\theta}(x), y(x)) := -\\nabla_{a}L(a, y(x))|_{a=f_{\\theta}(x)}$\n\nThis is the functional gradient, i.e., the gradient of the loss w.r.t. the output of the network. The best update we can perform with the current architecture A is the projection $v^*$ of that desired update onto the tangent space $T_A$. As a result, the residual that should be completed by extending the network is:\n\n$v_{\\perp} := v_{\\nabla} - v^*$\n\nwhere $v^* := Proj_{T_A}(v_{\\nabla}) := \\arg\\min_{v \\in T_A} E_{(x,y)\\sim P} [||v_{\\nabla}(x) - v(x)||^2]$\n\nand its norm $\\Psi := ||v_{\\perp}||$ is named the expressivity bottleneck of the architecture. One can add more neurons to a hidden state to mitigate the expressivity bottleneck at a given layer, thus growing the network. After the addition of neurons we multiply their output with an amplitude factor $\\gamma$ found by line-search. For further details, we refer the reader to the original paper."}, {"title": "Growing an arbitrary DAG.", "content": "The contribution of this paper consists in the extension of the work by Verbockhaven et al. (2024) to non-sequential networks in the form of any Directed Acyclic Graph (DAG) of fully connected layers. The graph in Figure 2 shows an example of a non-sequential network where every edge represents a fully connected layer and each node represents a hidden state (or an addition thereof). We optimize the new weights $\\alpha$ and $\\omega$ to decrease the expressivity bottleneck as follows:\n\n$\\alpha^*, \\omega^* = \\arg \\min_{\\alpha, \\omega} || \\omega \\sigma(\\alpha \\cdot x) - v_{\\perp} ||$\n\nWith the current setting, we can create a network starting from an empty graph, rather than needing a starting point. To achieve this, we consider a single constant layer that always outputs zero. At each growth step, we have the option to add a direct edge (1 layer), add a new node (with 2 edges, i.e. 2 new layers), or increase the size of an existing node by adding new neurons to its input and output layers (increase width). Expanding a node or adding a new one is the same process, as we only need to specify the input and output edges of the new neurons to be added. The peculiarity of this case lies in the fact that the best possible updates $v^*(x)$ of a specific node should take into account at least all the parameters contributing directly to this node. For reference, in Figure 2, when calculating $v^*(x)$ at the hidden state $i + 1$, we take into account the pre-existing weights $W_2$ and $W_3$.\nWe split our training dataset into 3 equal parts named train-opt, train-ls, and train-gr. At each growth step, first, for all potential network expansions, we optimize the potential new neurons' directions using train-opt, then we perform line-search to optimize their amplitude factor using train-ls, and finally, we select the best expansion using the estimation of the loss on train-gr. We then train this newly expanded architecture using the concatenation of train-opt and train-ls which we refer to as inter-train."}, {"title": "Strategies for Growth.", "content": "The whole search space is a greedy strategy, where we let the network grow freely based on the train-gr loss. It is an exhaustive search, and the search space inflates very fast with every growth step, together with the associated GPU energy consumption. The bottleneck restricted space strategy attempts to reduce this space by restricting the available network expansions. In this strategy, we find the node with maximum expressivity bottleneck $A^* = arg\\,max \\Psi_A$ and evaluate only the expansions that contribute to this pre-activity, that is, expanding or adding new layers that output to $A^*$ or expanding the node $A^*$ itself. This way we greatly reduce the search space, and thus the search time and GPU energy consumption. In a third strategy, we aim at a trade-off between performance and complexity, within the bottleneck restricted search space. We use the Bayesian Information Criterion as BIC = klog(n) \u2013 2log(L), where k is the number of parameters and n is the sample size. This strategy is named BIC + restricted space. To compare these strategies, we consider an ideal situation where we already know the perfect architecture for the task, that is the Teacher's, thought of as an oracle."}, {"title": "3 Experiments and Results", "content": "Proof of concept. In Verbockhaven et al. (2024), we evaluated growing networks on CIFAR-100 with sequential architectures. This study considers further experiments with DAG networks. As we have implemented only fully-connected layers so far, we are conducting a first evaluation on MNIST. To the best of our knowledge, there are no official results of NAS methods on MNIST except for Maile et al. (2022). For this reason, we use the results of Lecun et al. (1998) with fully-connected layers as our baselines. We use the whole search space strategy and at each growth step, we increase the size of the architecture by 10 neurons. We perform intermediate training between growth steps for 500 epochs and evaluate on the test set. We see the results of our approach in Figure 3. We notice that this intermediate training pushes parameters towards overfitting, but immediately after growing the architecture, the overfitting gap between the inter-train and the train-gr sets is reduced and the network finds itself in a more advantageous position, so it can continue learning. In general, by growing we manage to escape potential local minima that force the training accuracy to converge and we gain significantly more accuracy on train-gr. The test accuracy sits just below,\nsince we also overfit on train-gr after a few steps, as it is used for the expansion selection. Nevertheless, the gain in test performance is slow but significant. The experiment was run for 20 growth steps, requiring a little less than 7 GPU hours (0.29 GPU days). The final architecture achieves an inter-train accuracy of 99.7% and a test accuracy of 96.2%. We could keep growing the architecture for more steps but the improvement in accuracy is not significant and the drain on power consumption and additional complexity are not worth the added efficiency. In Figure 4 we see that we do not achieve state-of-the-art test accuracy but our method is extremely competitive in terms of model complexity and is thus cost-efficient.\nGrowth Strategies. To compare strategies in a general framework, we construct a Teacher-Student experiment. We randomly initialize a Teacher network for a regression task, with an input size of 20, two hidden states of size 50, a direct connection from the input to the second hidden state, and selu activations, for a total of 4701 parameters. We can then generate input samples from a uniform distribution and ask this Teacher for labels, to create our train and test datasets. Predicting this output is non-trivial as the intrinsic dimension is the same as the embedding dimension. Indeed, the random initialization of the Teacher parameters and the independent sampling of the input features create many degrees of freedom. We want to compare how our three strategies perform at growing a Student with arbitrary DAG, from scratch, to imitate the Teacher. At each growth step, we can add 10 neurons and we perform intermediate training for 100 epochs. The experiments are shown in Figure 5. We notice a temporary slight drop in performance when we restrict the search space, but it is not a significant one and it disappears after a certain number of epochs. However, GPU energy consumption is decreased by 23% when restricting the search space, for the same loss. We note also that with this strategy, we consume only 70% energy more than the oracle, which is the ideal scenario where the original Teacher architecture is known and trained, while we in plus search for the architecture (cf. Eq. (4)).\nBased on these results we estimate that with a grid-search NAS technique instead, we would need to train 7 different architectural structures for 100 epochs to roughly evaluate all the possible DAGs we can achieve for a network of 5 layers. Assuming they would all have the same number of neurons, chosen among a grid of 5, we would have to train and test at least 35 architectures, plus the best architecture fully once, for a total of \u2248 51 Wh. We achieve a more granular result with only 25% of the energy when restricting the search space. In retrospect, reducing the search space based on the bottleneck seems to perform very well in terms of efficiency and cost.\nThe use of BIC reduces the size of the resulting architecture, with an average of 723 parameters compared to 1479 for the bottleneck restricted space and 1454 for the whole search space, but consumes more energy during the search phase than just reducing the space, although with a smaller variance. This is because it tends to choose architectures that create more options for the next search phases. More studies on variations over BIC have to be performed to make it achieve an effective trade-off between performance and architectural complexity."}, {"title": "4 Conclusion and Future work", "content": "In this work, we grow neural architectures in the form of any DAG by adding new layers and direct connections on the fly during training. Our work is based on Verbockhaven et al. (2024) that introduced the notion of expressivity bottleneck to increase the width of layers in a pre-defined architecture structure. Our contribution is to create arbitrary non-sequential fully connected architectures starting from an empty graph without any predefined structure. We show that our method is competitive in terms of number of parameters, thus reducing inference time. We compare various strategies to grow an architecture and achieve lower complexity. We manage to reduce the overall training time and thus the GPU energy consumption compared to a grid search among architectures. The next line of research is to further improve our strategy to fulfill an efficient trade-off between performance and complexity. We intend to further extend our work to introduce growable modules for convolutional layers."}]}