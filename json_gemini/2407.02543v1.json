{"title": "Towards the Next Frontier in Speech Representation Learning Using Disentanglement", "authors": ["Varun Krishna", "Sriram Ganapathy"], "abstract": "The popular frameworks for self-supervised learning of speech representations have largely focused on frame-level masked prediction of speech regions. While this has shown promising downstream task performance for speech recognition and related tasks, the representations have mostly ignored factors of speech that are encoded at coarser level, like characteristics of the speaker or channel that remain consistent through-out a speech utterance. In this work, we propose a framework for Learning Disentangled Self Supervised (termed as Learn2Diss) representations of speech, which consists of frame-level and an utterance-level encoder modules. The two encoders are initially learned independently, where the frame-level model is inspired by existing self supervision techniques, thereby learning pseudo-phonemic representations, while the utterance-level encoder is inspired by constrastive learning of pooled embeddings, thereby learning pseudo-speaker representations. The joint learning of these two modules consists of disentangling the two encoders using a mutual information based criterion. With several downstream evaluation experiments, we show that the proposed Learn2Diss framework achieves state-of-the-art results on a variety of tasks, with the frame- level encoder representations improving semantic tasks, while the utterance-level representations improve non-semantic tasks.", "sections": [{"title": "1 Introduction", "content": "In recent years, self-supervised learning (SSL) of speech has enabled substantial advances in down- stream applications by generating succinct representations of the speech signal. Broadly, the modeling frameworks can be classified as generative, constrastive and predictive approaches, as described by Mohamed et al. [2022]. The approaches in generative modeling include variational auto-encoder (VAE) based (Fraccaro et al. [2016], Hsu et al. [2017a]), auto-regressive predictive coding based (Chung et al. [2019]), masked reconstruction based (Liu et al. [2020], Wang et al. [2020]) and waveform based (Pascual et al. [2019]). The contrastive approaches involve prediction of the latent representations in the continuous domain with a contrastive loss (for example, contrastive predictive coding (CPC) (Van Den Oord et al. [2017]) and wav2vec models (Baevski et al. [2020])). The predictive approaches have been partially inspired by similar large scale pre-training efforts in text by Devlin et al. [2018]. In an analogous manner, the self-supervised learning in speech is formulated using a pretext task. The popular examples of self-supervised approaches of this nature are HuBERT (Hsu et al. [2021]), wavLM (Chen et al. [2022]) and data2vec (Baevski et al. [2022]).\nThe paradigm in most of these works involved the frame-level (20-30ms) contrastive or predictive modeling of speech representations. Some of the notable investigations with utterance level modeling include works by Stafylakis et al. [2019], Cho et al. [2022], Zhang and Yu [2022]. These frameworks"}, {"title": "2 Related Work", "content": "Self-Supervised disentangled representation learning for speech first emerged in the work by Hsu et al. [2017b], where a factorized hierarchical variational autoencoder was used to learn sequence-level and frame-level information. This work evaluates on speech conversion and speaker recognition tasks. In contrast, our proposal initializes prior models for frame-level and utterance-level representations and performs a continued learning with disentanglement loss. Further, the proposed Learn2Diss is evaluated for semantic and non-semantic tasks.\nRecent work by Mu et al. [2024] explored disentangled self-supervised representation learning for speaker embedding extraction. The goal was to develop speaker embeddings which are devoid of semantic attributes of speech. The downstream task was the speech separation of a target speaker. However, our approach learns a joint semantic and non-semantic encoder through a paradigm that mutually enriches each of the embeddings.\nThe approach, termed as content2vec (Qian et al. [2022]), is based on the masked language modeling paradigm (MLM) (Devlin et al. [2018]). It consists of a student-teacher network initialized using pre-trained HuBERT (Hsu et al. [2021]). The model deploys a cascade of voice conversion systems to transform all the utterances to single target speaker voice, followed by frozen teacher network to generate speaker insensitive discrete targets. The student network learns speaker invariant representa- tions by minimizing the constrastive (SIMCLR -Chen et al. [2020]) loss between representation of original speech and it's pitch perturbed version obtained by Choi et al. [2021]. In our work, we do not perform any voice conversion or a have teacher-student learning framework, rather we attempt to learn disentangled representations directly from a single speech utterance."}, {"title": "3 Method", "content": null}, {"title": "3.1 Background", "content": "Contrastive Log-ratio Upper Bound (CLUB) Cheng et al. [2020] proposes a technique to estimate the upper bound of mutual information between random variables X and Y. Suppose {(xi, Yi)}N1 are the samples drawn from the joint distribution p(x, y), then CLUB ICLUB is given as,\n\\[ I_{CLUB} = \\frac{1}{N} \\sum_{i=1}^{N} \\log q_{\\phi}(Y_i | X_i) - \\frac{1}{N} \\sum_{i=1}^{N} [\\log q_{\\phi}(Y_j | X_i)], \\\\ \\]\nwhere, q (y|x) is the variational network parameterized by approximating p(y|x). The distribution q is learned by minimizing the KL-divergence between KL(p(y|x)||q\u2084(y|x)). As shown in Cheng et al. [2020], the KL-divergence between the variational and conditional distribution boils down to negative log-likelihood:\n\\[ L(\\phi) = - \\frac{1}{N} \\sum_{i=1}^{N} \\log q_{\\phi}(Y_i | X_i) \\]"}, {"title": "3.2 Learn2Diss Approach", "content": "The block schematic of the proposed Learn2Diss framework is depicted in Figure 1. The method leverages a combination of three key components: pre-trained frozen feature extractor f(.), learnable dual encoder consisting of frame-level encoder g(.) and utterance-level encoder h(.) and a module to reduce the mutual information between the frame-level and the utterance-level encoder. The goal is"}, {"title": "3.2.1 Feature extractor", "content": "The feature extractor f(.) is a frozen model and the output of this module is fed to both the frame- level and utterance-level encoder. The feature extractor takes raw waveform as input and is a cascade of convolution layers (Krizhevsky et al. [2012]) followed block of N-layers of transformer (Vaswani et al. [2017]) encoder. Let the feature representation of the utterance X be denoted as X = (x1, x2....). The N frozen layers of the feature extractor are obtained from the pre-trained frame-level encoder like wavLM (Chen et al. [2022]) or HuBERT (Hsu et al. [2021]) in this work. Note that, N = 0 corresponds to using only the convolutional waveform encoder of the pre-trained model with all the other model parameters learned using the joint disentanglement loss."}, {"title": "3.2.2 Frame-level encoder", "content": "The Frame-level encoder g(.) is block of M-layers of transformer architecture (Vaswani et al. [2017]). The feature representation X' is first masked following the masking procedure given in Baevski M et al. [2020]. With the frame-level encoder g(.), the input X is transformed to Y = (Y1, Y2\u2026\u2026\u2026\u2026YT). Here, y \u2208 Rd, for layer l = 1...M and time-step t = 1....T. The final layer representations yM t are fed to a linear projection layer (denoted as W\u2084) followed by a softmax operation. Let Yt = softmax(W9yM) denote the model predictions of the discrete targets. The discrete targets, vt \u2208 {1...K}, obtained by the quantization (k-means) of representations from pre-trained models like HuBERT (Hsu et al. [2021]) or wavLM (Chen et al. [2022]), provides supervision to train the g(.) network.\nThe semantic properties of the frame-level representations are enriched by minimizing the constrastive loss proposed by Krishna and Ganapathy [2023], termed as pseudo-con loss. For every frame y in a mini-batch of size B, the pseudo-con loss is given by,\n\\[ L_{CE} = -\\sum_{k=1}^{K} \\gamma^k_t \\cdot log(\\gamma_t^k); L_{pc} = \\frac{1}{|P(t)|} \\sum_{p \\in P(t)} \\frac{exp(y_t^M \\cdot y_p^M / \\tau)}{\\sum_{a \\in A(t)} exp(y_t^M \\cdot y_a^M / \\tau ) }, \\\\ \\]\n\\[ P(t) = { p : v_t = v_p \\forall p \\in { t }_{-1}^{+1} }; A(t) = { a : v_t \\neq v_a \\forall a \\in { t }_{-1}^{+1} } \\]\nHere, \uba00,, are the kth index value of vt and the one-hot vector representation of targets Yt respectively, while 7 is the temperature parameter. The cross entropy loss (L&E) and the constrastive loss (LP) are applied only on frame-indices t which are masked. Further, A(t), P(t) denote the positive and negative anchor sets for defining the contrastive loss at the batch level.\nThe total loss,\n\\[ L_{frame} = L_{CE} + L_{PC}, \\]"}, {"title": "3.2.3 Utterance-level encoder", "content": "The Utterance-level encoder h(.) consists of L layers of enhanced channel attention propagation and aggregation (ECAPA)-time delay neural network (TDNN) architecture, proposed by Desplanques et al. [2020]. This architecture consists of convolutional front-end followed by squeeze-excitation residual network (Resnet) blocks and a final attention-based global pooling. In our work, the ECAPA- TDNN based encoder is pre-trained using the contrastive learning objective (SimCLR Chen et al. [2020]).\nLet the internal representations of the h() encoder be denoted as {z\u2081...z'\u0131}, l = 1...L and z\u0142 \u2208 Rd\u00b2. For this encoder, the final representation is a pooled embedding (TL = 1), while the other layers have the same time-steps as the frame level encoder T\u00b9 = T,\u2200l = 1..L \u2212 1 with d\u00b2 = d' \\l = 1..L \u2013 1.\nInitially, the model is trained only with Info-NCE loss (Chen et al. [2020]). Following this training, the final layer representations z\u00b9\u00b9 are clustered with k-means algorithm to generate Q clusters. Similar to"}, {"title": "3.2.4 Disentanglement with Mutual Information Minimization", "content": "Mutual information minimization module allows the disentanglement of the representations generated by both the encoders. This module is designed by minimizing the estimate of the upper bound on the mutual information given by the Contrastive Log ratio Upper Bound (CLUB) formulation (Cheng et al. [2020]). Given the internal representations from frame-level and utterance-level encoders for the i-th utterance {g(X);h(X)} = {({{y}{-1}{1; {{2,}=1}{=1)} and for a mini-batch of B utterances, the mutual information loss is given by,\n\\[ L_{MI} = \\frac{1}{BT} \\sum_{i=1}^{B} \\sum_{t=1}^{TL} [log(\\tilde{q}_{\\phi}(Y_{i,t} | Z_{i})) - \\frac{1}{B} \\sum_{j=1}^{B} \\tilde{q}_{\\phi}(Y_{i,t} | Z_{j})], \\\\ \\]\n\\[ Y_{i,t} = \\sum_{l=1}^{M-1} y_{it}; Z_{i} = \\sum_{l=1}^{L-1} \\Lambda^l z_{it} \\]"}, {"title": "3.3 Implementation", "content": "Following the prior works of Baevski et al. [2020], Hsu et al. [2021], we use 960 hours of the Librispeech dataset Panayotov et al. [2015] to train the models\u00b9. For the frame-level encoder, we use the base version of the HuBERT (Hsu et al. [2021]) or wavLM (Chen et al. [2022]) model with 12 encoder layers and an embedding dimension of d = 768. The utterance-level feature encoder is designed with d = 1024,1 = 1..L \u2013 1 having the ECAPA-TDNN architecture (Desplanques et al. [2020]) with output dimension of dL = 256. The utterance-level model is initialized with ECAPA-TDNN model pre-trained with the InfoNCE SimCLR (Chen et al. [2020]) loss. The model configurations explored in this work are given in Table 1. The variational network consists of P = 2 neural layers to predict the mean and variance of the variational distribution. The mean and variance networks are made of 2 feed-forward layers with input dimension of 256, hidden dimension of 2048 and output dimension of 768.\nFor the frame-level encoder training, the initial model of HuBERT/wavLM is additionally trained with the pseudo-con (Krishna and Ganapathy [2023]) loss. The discrete targets are obtained with k-means clustering using K = 2048 of the 9th layer representations of HuBERT model, while the wavLM setup used the 11th layer representations (as suggested by the respective references).\nFor the utterance-level encoder, the initial learning is performed with the SimCLR Chen et al. [2020] style pretext task with Info-NCE loss using two non-overlapping feature segments each of 1 second duration from the given speech utterance (all Librispeech utterances were more than 2 seconds in duration). The discrete targets for training utterance-level feature encoder is obtained by performing k-means clustering (Q = 2000) on the features from pre-trained ECAPA-TDNN model (trained only with the Info-NCE loss). The optimal value of Q was determined using the knee-point algorithm (Satopaa et al. [2011], Sugar and James [2003])."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 SUPERB Benchmark", "content": "We demonstrate the effectiveness of representations from proposed models using a battery of tasks proposed in Speech Processing Universal PERformance Benchmark (SUPERB Yang et al. [2021]). The pre-trained model is the frozen embedding extractor. For each downstream task, a weighted sum of layer representations from the embedding extractor is generated and these aggregated features are input to light-weight prediction heads. Only the weights in the layer-wise pooling and the prediction head parameters are fine-tuned on the downstream tasks. Table 2 reports the results for various baseline systems as well as the proposed models.\nFor the semantic tasks, we use the representations from the frame-level encoder while for non- semantic tasks, the representations from utterance-level encoder are chosen. On all the tasks except intent classification and slot filling, we achieve state-of-the-art results. The proposed (L2D-W12) model, on average, shows relative improvement of 9.36% over it's parent model (wavLM).\nOn the non-semantic tasks, the L2D-W6 model provides the best performance among models compared here. Even in comparison with large models published in leaderboard2, the proposed setting improves significantly on speaker recognition and verification tasks, indicating the benefits of learning utterance representations that are devoid of semantic content. On average, the L2D-W6 representations show relative improvement of 22.59% over the wavLM base model."}, {"title": "4.2 Acoustic Unit Discovery", "content": "To examine the sub-word discovery capability of the proposed approach, we consider the acoustic unit discovery task of the ZeroSpeech2021 (Dunbar et al. [2021]) challenge. In this task, the quality of the representations are evaluated without any subsequent fine-tuning using the ABX metric. Given a triplet of 3 phoneme words A, B and X, where A and B differ only in center phoneme, while X is same as A (for eg. A='apa', B='aba' and X='apa'), the ABX metric computes the fraction of cases when the distance of representations between A and X is more than the distance between A and B. The distance is computed as frame-wise cosine distance averaged over dynamic-time-warped (DTW) path. More details on the ABX computation are available in Dunbar et al. [2021]. The ABX score computed when all the 3 \u201cwords\u201d are derived from same speaker is called \"within\" speaker ABX, while the case where A, B are from same speaker, while X is from different speaker is called \"across\" speaker ABX. Results are reported on the \"clean\" and \"other\" splits of the ZeroSpeech Challenge (Dunbar et al. [2021]) datasets.\nThe results are reported in Table 3. For these experiments, we have extracted the representations from last layer of the frame-level encoder. The HuBERT based Learn2Diss encoder models achieves the"}, {"title": "4.3 ASR Finetuning", "content": "Following the ASR setup discussed in the prior works, Baevski et al. [2020], Hsu et al. [2021], Chen et al. [2022], Baevski et al. [2023], we fine-tune only the transformer layers of the frame-level encoder using connectionist temporal classification loss (CTC) (Graves et al. [2006]) loss. The CTC target vocabulary includes 26 English alphabets, space token, blank symbol and an apostrophe. The models are evaluated using 1 hour or 10 hours of labeled data from LibriLight (Kahn et al. [2020]) or using 100 hours from Librispeech data (Panayotov et al. [2015]). The Torch-audio ASR toolbox\u00b3 is used to perform the fine-tuning with all the hyper-parameters set to default. The CTC decoding is done using the Librispeech official 4-gram language model. The word error rates (WER%) are reported on the \"clean\" and \"other\" splits of test set of Librispeech (Panayotov et al. [2015]) corpus in Table 4. The results demonstrate that proposed L2D approach achieves state-of-the-art (SOTA) results or similar to previous SOTA for all the settings. In particular, it is seen that the improvements for the proposed models over the parent models are consistent even when the model is fine-tuned with 100 hours of supervised data. Similar trends are also seen on the development set of Librispeech, which are reported in Appendix A.2."}, {"title": "4.4 Impact of Disentanglement Loss", "content": "The proposed Learn2Diss involves an additional level of pre-training over the self-supervised models. To study the effect of mutual information minimization module, we compare models that are trained with and without mutual information loss. In the \"without mutual information\" setting, we train the frame-level encoder and utterance-level encoder with Lframe and Chatt alone (X = 0 in Equation 10). Without the disentanglement loss (MI loss term in the total loss), the optimization simply reverts to"}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Limitations", "content": "The proposal involves additional pre-training overhead over the existing SSL frameworks. It maybe possible to circumvent this limitation through early stopping of individual encoder modules before the mutual information based optimization. However, this early stopping optimization has not been explored in the current work. Also, it is to be noted that the Learn2Diss framework is only a pre- training framework, and hence, the inference cost of the proposed models are identical to those of the parent models.\nThe proposal explored the disentanglement objective over frame-level encoding provided by wavLM or HUBERT base models. However, we hypothesize that this work represents a plug-in framework applicable to any existing frame-level SSL model in order to enrich its semantic and non-semantic encoding.\nThe work is based on the assumption that every utterance contains only a single source of variability in the non-semantic component. The extension to multi-speaker audio or for transient content like music is not explored in this work.\nFinally, the proposed work is evaluated only on speech understanding tasks. The exploration of the proposed models for generation tasks like voice conversion or speech synthesis is left for future perusal."}, {"title": "5.2 Summary", "content": "We propose a novel approach for pre-training a dual encoding model to disentangle both semantic and non-semantic information of speech without any supervision. The proposed framework is initialized with a frame-level and utterance-level encoder that are trained independently. Further, the two encoders are jointly trained by mutually disentangling the information content present in each of their representations. This is achieved by minimizing the variational upper bound of the mutual information loss between the internal representations of the two encoder modules. To the best of our knowledge, the validation of disentanglement for both semantic and non-semantic representations, in improving their respective downstream task performance, is a first-of-its kind. Further, the paper establishes new state-of-art results for some of the tasks using only the base version of the SSL models. We speculate that the current proposal opens a new frontier for exploring speech representation learning with disentanglement as a key objective for optimizing dual or multi-way encoders."}, {"title": "A Appendix / supplemental material", "content": null}, {"title": "A.1 Importance of hidden representation aggregation in mutual information minimization", "content": "As described in section 3.2.4, mutual information estimation and subsequent minimization is done between the layer-wise aggregation of hidden representation from frame-level (y) and utterance-level (z) encoders. In this ablation setting, we evaluate the impact of this aggregation.\nWe compare the scenario of mutual information estimation (and minimization) using representation aggregation (y, z) against mutual information minimization between final layer representation of frame-level and utterance-level encoder (yM, zL).\nResults reported in Table 6 on the various downstream tasks in SUPERB challenge show an average relative improvement of 11.26% for frame-level encoder tasks and an average relative improvement of 15.53% for utterance-level encoder tasks for the aggregate representations. The reason for the improvement could be the fact that disentanglement of information in internal representations and the final representations jointly maybe more realistic compared to achieving the same goal with only the final layer representations."}, {"title": "A.2 ASR Finetuning results", "content": "We report in Table 7 the ASR results for 3 different setups followed in prior works by Baevski et al. [2020], Hsu et al. [2021], Chen et al. [2022] using 1 hour / 10 hour split from LibriLight (Kahn et al. [2020]) or 100 hours from LibriSpeechPanayotov et al. [2015]. Word error rates (WER%) are reported on the \"clean\" and \"other\" splits of development set of LibriSpeech corpus.\nProposed model L2D_W12 achieves state-of-the-art or comparable to state-of-the-art results for all the setups."}, {"title": "A.3 Quality of representations", "content": "We measure the quality of representation from frame-level encoder and utterance-level using phone purity and speaker purity metrics. The aggregate representations from frame-level and utterance-level encoders are chosen for computing phone and speaker cluster purity respectively.\nPhone purity measures the homogeneity of phonemes within clusters. It can be interpreted as frame- level phone accuracy, if each of the frame-level discrete targets (k-means clusters) is associated with a unique phoneme label. A higher purity indicates better quality of representing unique phoneme label with the given representation. For the models in Table 8, frame level representations from the final layer are quantized into 2048 clusters using k-means algorithm. The k-means clustering is trained on LibriSpeech(100h) (Panayotov et al. [2015]) subset. Once the clustering is performed, the clusters are compared with the frame-level phonetic targets (available through ASR force-alignment Panayotov et al. [2015]).\nThe proposed models show an average relative improvement of 4.46% in phone purity compared to their parent models. This may be primarily attributed the disentanglement objective introduced in the model using the MI loss function.\nSimilarly, the speaker cluster purity is the ratio of number of audio utterances belonging to the dominant speaker in a cluster to the total number of audio utterances present in the same cluster. The"}]}