{"title": "FovealNet: Advancing Al-Driven Gaze Tracking Solutions for Optimized Foveated Rendering System Performance in Virtual Reality", "authors": ["Wenxuan Liu", "Monde Duinkharjav", "Qi Sun", "Sai Qian Zhang"], "abstract": "Leveraging real-time eye-tracking, foveated rendering optimizes hardware efficiency and enhances visual quality virtual reality (VR). This approach leverages eye-tracking techniques to determine where the user is looking, allowing the system to render high-resolution graphics only in the foveal region-the small area of the retina where visual acuity is highest, while the peripheral view is rendered at lower resolution. However, modern deep learning-based gaze-tracking solutions often exhibit a long-tail distribution of tracking errors, which can degrade user experience and reduce the benefits of foveated rendering by causing misalignment and decreased visual quality.\nThis paper introduces FovealNet, an advanced AI-driven gaze tracking framework designed to optimize system performance by strategically enhancing gaze tracking accuracy. To further reduce the implementation cost of the gaze tracking algorithm, FovealNet employs an event-based cropping method that eliminates over 64.8% of irrelevant pixels from the input image. Additionally, it incorporates a simple yet effective token-pruning strategy that dynamically removes tokens on the fly without compromising tracking accuracy. Finally, to support different runtime rendering configurations, we propose a system performance-aware multi-resolution training strategy, allowing the gaze tracking DNN to adapt and optimize overall system performance more effectively. Evaluation results demonstrate that FovealNet achieves at least 1.42\u00d7 speed up compared to previous methods and 13% increase in perceptual quality for foveated output.", "sections": [{"title": "1 INTRODUCTION", "content": "Human visual acuity varies across the visual field. The fovea, the central region of the retina, is responsible for our sharpest vision. This region, although small, is densely packed with photoreceptor cells, allowing us to perceive fine details and vibrant colors within our direct line of sight. As we move away from the fovea, our vi-"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Generally, gaze tracking methods can be broadly classified into model-based and appearance-based approaches [15, 16]. Model-based techniques estimate gaze direction by utilizing a 3D eye model that mimics physiological structures [17, 18, 19, 20]. These approaches generally involve two stages: (1) employing an eye feature extraction neural network to produce salient eye features and fitting a geometric eye model, and (2) predicting the gaze direction based on the fitted eye model. Essentially, model-based methods transform the gaze tracking problem as an eye segmentation task.\nMost of model-based approaches utilize U-Net with convolutional"}, {"title": "2.2 Gaze-Tracked Foveated Rendering", "content": "The growing popularity of virtual reality has led to a significant demand for rendering high-resolution images on resource-limited hardware platforms, such as head-mounted displays (HMDs) [2, 32]. In these contexts, minimizing system latency is crucial to prevent visual distortions and visual artifacts. If the system fails to keep up, users may experience a disconnect between what they see and what they feel, leading to a quality degradation of user experience [33, 34]. Early HMDs employed full-resolution rendering, where each pixel within the user's field of view was rendered uniformly. While this method maintained visual quality, it resulted in unnecessary computational overhead and increased system latency.\nTFR is a VR technique that optimizes rendering using gaze-tracking, typically powered by DNNs. It detects the user's gaze location in real-time, rendering the foveal region at the highest resolution (Fig. 2(a)), while surrounding areas, namely the inter-foveal region and peripheral region, are rendered from fine to coarse level without introducing visual artifacts [32, 34, 35]. As depicted in Fig. 2(b), in TFR, the radius rf of the foveal region (in pixels) is set based on rf = ri + c = pd\u00b7 tan(0; + \u2206\u03b8) = dtan(0f), where p represents the display's pixel density, d is the distance between the user and the screen, 0; is the eccentricity angle subtended by"}, {"title": "2.3 DNN Pruning", "content": "Pruning techniques are commonly used in DNNs to reduce memory and computational costs. Beyond weight pruning, research has focused on pruning intermediate tokens in ViT [38, 39]s. For instance, SPViT [40] removes redundant tokens with a token selector; S2ViTE [41] uses sparse training to prune tokens and attention heads; and Evo-ViT [42] employs a slow-fast token evolution mechanism to retain essential information.\nAlthough previous work has utilized cropping techniques to remove redundant surrounding pixels from eye images [11], the specific characteristics of the gaze-tracking task allow us to further eliminate redundant input tokens within near-eye images, such as those representing the eyelashes. Compared to tokens depicting the iris and pupil, these elements are relatively irrelevant to gaze tracking results [17]. This insight led us to implement fine-grained token-wise pruning on top of the cropped input of ViT and its intermediate activations (step 2 in Fig. 3). Our proposed token-wise pruning approach ranks input tokens based on their importance scores (attention scores) with respect to the final gaze prediction and eliminates unimportant tokens, leading to a significant reduction in computational costs with negligible accuracy impact."}, {"title": "3 MOTIVATION", "content": "A TFR system in modern VR devices (e.g., HMDs) usually comprises three main components (Fig. 4(a)): a near-eye camera, a host processor, and an interconnection link (MIPI [43]). A typical TFR pipeline is shown in Fig. 4(b). The process begins with capturing an eye image using a near-eye monochrome camera. This image is then preprocessed by the image signal processor (ISP) and readout before being transferred to the host processor via the MIPI connection. After the host processor receives the image, it is sent to the eye-tracking DNN, which estimates the gaze direction. This estimated gaze direction is then utilized to guide the foveated rendering process to produce the rendered VR scene."}, {"title": "3.4 Tracking Performance and Latency Tradeoffs in TFR", "content": "In practice, rendering resolution settings for in AR/VR devices can vary significantly during usage, depending on the context and the specific needs of the experience. For example, in highly detailed, visually rich environments like gaming or virtual simulations, users may prefer higher resolutions to capture intricate details and en-"}, {"title": "4 FOVEALNET DESIGN", "content": "The overall structure of FovealNet is illustrated in Fig. 7. When an eye image is received, it is initially compared with previous buffer frames to determine if the earlier gaze prediction results can be reused, as detailed in Sec. 4.1. If the previous results are not applicable, the eye image is cropped using the method outlined in Sec. 4.1. The cropped image is then passed to the gaze tracking ViT, which is explained in Sec. 4.2 and Sec. 4.3. Finally, the multi-resolution training approach is described in Sec. 4.4."}, {"title": "4.1 Event-based Cropping for Efficient TFR", "content": "Eye images captured by near-eye cameras often contain redundant pixels (e.g., background, facial muscles) that are irrelevant for gaze tracking prediction. These pixels can negatively impact the prediction results and increase the computational cost. To tackle this, we propose an event-based analytical approach that efficiently crops the informative regions of the eye. Additionally, since eye frames are streamed into the TFR system consecutively and share content similarities, we can reuse the gaze prediction results of previous frames, further reducing computational cost."}, {"title": "4.1.1 Region Cropping Algorithm", "content": "Given that the pupil is the most relevant area for human gaze, we focus on cropping the informative region of the input frame around the pupil's position. To do this, we employ an efficient analytical approach to precisely detect the pupil location, enabling us to accurately crop a fixed-size region centered on the detected pupil.\nWe begin by applying a masking process to eliminate the background region around the image edges. Then, leveraging the prior knowledge that the pupil is typically darker than the surrounding sclera and iris [66], we perform inverse binarization to emphasize the darker regions of the image. Next, we apply morphological opening to reduce noise in the image. At this stage, only the pupil and other dark regions like eyelashes remain, as shown in the connected components (CC) maps in Fig. 8. Given the fact that the pixel density in the pupil region is significantly higher than in other areas. Thus, we can identify the pupil by searching for the largest connected component (LCC) in the image and using the center of this component to represent the pupil's center. Once the LCC is"}, {"title": "4.1.2 Gaze Reuse Criterion", "content": "Usually the eye movements is small, resulting in no significant change in gaze direction [60, 68]. Therefore we can reuse the previous gaze direction result for save computation. To achieve this, we use the activated pixels in the event map as a basis for the decision.\nThe event map is obtained by processing consecutive near-eye image inputs. First, we calculate the pixel-wise inter-frame difference \\(AF\\) between two successive frames Ft-1 and Ft. Next, each spatial element \\(AF_{i,j} \u2208 AF\\), is normalized using the corresponding values in F_{t-1}. Based on the normalized values, we determine whether each pixel should be represented in the event map. Finally, we compare the number of activated pixels with a predefined threshold to decide whether the current frame can reuse the previous gaze direction result. This process can be formulated as:\n\\[M_{i j} \\leftarrow \\sigma\\left(\\frac{\\Delta F_{i j}}{F_{i, j}^{t-1}}, \\beta_{1}\\right)\\]\\[\\varepsilon_{i j} \\leftarrow \\sigma\\left(\\frac{\\sum_{i, j} M_{i, j}}{\\sigma}, \\beta_{2}\\right)\\]", "content_math": ["M_{i j} \\leftarrow \\sigma\\left(\\frac{\\Delta F_{i j}}{F_{i, j}^{t-1}}, \\beta_{1}\\right)", "\\varepsilon_{i j} \\leftarrow \\sigma\\left(\\frac{\\sum_{i, j} M_{i, j}}{\\sigma}, \\beta_{2}\\right)"]}, {"title": "4.2 Efficient Gaze Tracking Network", "content": "The cropped eye images containing informative content are first resized to a smaller square (224x224) and then processed by the gaze tracking DNN to predict gaze direction. FovealNet employs a vision transformer (ViT) architecture [30] for this task, as it provides superior performance compared to convolutional neural network (CNN)-based architectures. The ViT operates by dividing the input image into patches, which are then tokenized and appended with positional information, the outputs are then passed through the transformer block. The ViT contains 8 transformer block, each block consists of 6 heads with an embedding dimension of 384. We also modify the original ViT by replacing the classifier MLP layers with a sequence of linear layers to output the 2D gaze direction. A completed version of gaze tracking ViT is depicted in Fig. 9.\nA key advantage of ViT over CNN is its ability to fine-grain prune input tokens, enabling the removal of image tokens with unimportant content, as shown in Fig. 3. In the self-attention mechanism, tokens are linearly transformed into Query, Key, and Value matrices. The attention score is then computed by performing a dot product between the Query and Key matrices, followed by scaling and Softmax operation. The attention score reflects the importance of each token in relation to the gaze prediction result. Using these scores, we employ a top-k selector to remove unimportant tokens, which further reduces the computational cost of subsequent ViT blocks by decreasing the number of input tokens, outlined in Fig. 9."}, {"title": "4.3 Performance-aware Training Strategy", "content": "As discussed in Section 3.3, most existing gaze-tracking DNNs focus on minimizing the average tracking error. However, this often leads to a higher 95th percentile error in gaze tracking \\(\\Delta\\theta\\), which increases rendering times in the TFR system. We propose a training strategy to addresses this issue by minimizing the maximum tracking error during training, which can be formulated as:\n\\[\\min \\max (\\|\\theta_{d}^{\\prime} - \\theta_{d}\\|_{2})\\]", "content_math": ["\\min \\max (\\|\\theta_{d}^{\\prime} - \\theta_{d}\\|_{2})"]}, {"title": "4.4 Multi-resolution Training Mechanism", "content": "In Sec. 4.3, we depict the design of the loss function (Eq. (5)) by directly optimizing the rendering latency. In practice, the processing latency Ttracking of gaze tracking DNN will also contribute to the total latency Trotal, and the hardware processing latencies for rendering and tracking can vary due to user settings and resource sharing with other applications.\nTo maintain adaptability and performance, we develop a multi-resolution DNN training approach that optimizes multiple subnetworks across various DNN architectures simultaneously (Fig. 10 (a)). This joint-optimization training framework will produce a multi-resolution model that can execute at varying depths, allowing for the selection of the optimal gaze tracking DNN based on the current system conditions.\nTo achieve this, we attach a linear layer at the end of each encoder block within the gaze tracking ViT, which will produce a prediction on gaze direction based on the intermediate results from the early-exit points, as shown in Fig. 10 (b). Specifically, let L denote the total number of layer blocks within the ViT, and Ul denote the loss generated from the output of layer l \u2208 L. The loss function Umulti for the multiresolution training can be formalized as:\n\\[U_{\\text {multi }}=\\sum_{l \\in L} U_{l}\\]", "content_math": ["U_{\\text {multi }}=\\sum_{l \\in L} U_{l}"]}, {"title": "5 TRACKING PERFORMANCE EVALUATION", "content": "We evaluate the tracking performance of FovealNet using the OpenEDS2020 dataset [60], which consists of 128,000 images from 32 participants in the training set and 70,400 images from 8 participants in the validation set. All participants wore a VR-HMD, and the images were captured with a near-eye camera operating at 100Hz and a resolution of 640 \u00d7 400 pixels. The dataset includes ground truth 3D gaze vectors, which we converted into 2D gaze"}, {"title": "5.2 Accuracy Evaluation by Minimizing Average Gaze Error", "content": "We train all the models using an objective function designed to minimize the average gaze tracking error, defined as: Lmse = \u03a3b\u2208B \u03a3d\u2208Dtrain (yd \u2013 \u0177d)2. B denotes the number of batches within the training data, yd and \u0177d represent the predicted and ground truth gaze direction for training data d. This is the same training objective used by all other previous works on gaze tracking. For Deep-VoG [10] and Seg [11], we use their pretrained model weights and deploy them directly for evaluation. For FovealNet, we evaluate its performance under three settings with tokenwise pruning ratios of 0.2, 0.1, and 0.0 (no pruning), labeled as FovealNet (0.2), FovealNet (0.1), and FovealNet (0.0), respectively."}, {"title": "5.3 Accuracy Evaluation by Performance-aware Loss", "content": "In this section, we train FovealNet using the performance-aware loss function specified in Eq. (5) with a scaling factor N of 100. Specifically, we use the rendering latency measurements of the Meta Quest Pro on input images with a size of 1080 \u00d7 1920, as shown in Fig. 6(c), to generate the piecewise linear function U(.) and evaluate its impact on the gaze tracking error for FovealNet.\nFig. 11 shows the effectiveness of the performance-aware training strategy from Sec. 4.3. The blue bars indicate the error distribution using the objective function that minimizes average gaze error, as in Tab. 1, while the orange bars represent the gaze error distribution using the performance-aware loss function."}, {"title": "5.4 Perceptual Quality Measurement", "content": "To evaluate the impact of gaze-tracking errors on the perceptual quality of the foveated output, we use the FovVideoVDP metric in Sec. 3.2. Specifically, we sampled 400 random images from the MS COCO test dataset [70], and applied the foveation algorithm [71, 72], configured with a 0; = 5\u00b0 of the eccentricity angle subtended by fovea, when displayed on an HTC Vive Pro HMD (i.e., 13.2\u00b0 pixels per degree [54])."}, {"title": "5.5 Multi-Resolution Accuracy", "content": "While Sec. 5.2 and Sec. 5.3 focus on single-resolution training with the performance-aware training strategy, this section evaluates the performance of FovealNet using the multi-resolution training method from Sec. 4.4. Specifically, six early exits branches are introduced at the end of each transformer block of gaze tracking ViT from block 3 to 8, with a small linear layer for gaze prediction at each exit, resulting in six subnetworks of varying depths. During training, we compute the training losses from the six loss functions and sum them to generate the final multi-resolution loss, as shown in Eq. (6). The rest of the training settings follows Sec. 5.3."}, {"title": "5.6 Ablation Study", "content": "In this section, we first examine the effect of the input cropping method, introduced in Sec. 4.1, on the gaze tracking performance and computational cost of FovealNet. Tab. 3 presents the tracking errors and computational cost of FovealNet when using cropped versus uncropped input. For the gaze tracking DNN to process the uncropped input, it is also resized to a square shape of 224x224 pixels. We observe that the involvement of the cropping method results in a mean error decrease from 1.1\u00b0 to 0.98\u00b0, and 90th-percentile and 95th-percentile error drops 0.2\u00b0 and 0.34\u00b0 respectively. Meanwhile, the introduction of cropping only increase 11M FLOPs, results in a <0.5% FLOPs increase."}, {"title": "6 TFR SYSTEM PERFORMANCE EVALUATION", "content": "In this section, we evaluate the system performance by measuring processing latency for various methods. First, in Sec. 6.1, we evaluate the system performance of the FovealNet trained with the performance-aware loss defined in Eq. (5). We then demonstrate the system performance of FovealNet under different rendering system conditions by switching the rendering resolution in Sec. 6.2."}, {"title": "6.1 Evaluation with Performance-aware Training Loss", "content": "In this section, we compare the system performance across different approaches with the performance-aware training of FovealNet, considering both Ttracking and various Tfr values. For Tfr, we evaluate the foveated rendering configurations that account for the 90th-percentile and 95th-percentile gaze tracking errors to ensure greater versatility. The Tfr values are derived by determining latency based on the latency analysis outlined in Sec. 3.3 based on gaze tracking error. We compare the latency performance of various approaches on both HMD (Meta Quest Pro) and a mobile GPU (Quadro RTX 3000). For FovealNet, we train it using the performance-aware loss function described in Eq. (5), where U(.) represents the processing latency under different eccentricity angle Of on either the HMD or mobile GPU. To compute the Of, we set the eccentricity angle \u03b8 subtended by the fovea to 5\u00b0, and \u2206\u03b8 is set to P95 or P90 of the gaze error distribution on OpenEDS 2020 for different approaches. We adopt the single resolution FovealNet that contains 8 ViT blocks under different pruning ratios."}, {"title": "6.2 Latency Evaluation under Varying Conditions", "content": "In this section, we evaluate the performance of the multi-resolution FovealNet by using the same model described in Sec. 5.5 and simulating the corresponding execution latencies on the Meta Quest Pro, following the methods outlined in Sec. 6.1. We simulate variations in system configuration by adjusting the rendering resolution to 720P (720 \u00d7 1080), 1080P (1080 \u00d7 1920), and 1440P (1440 x 2560). For each rendering resolution, we assess the Ttracking and Tfr of the FovealNet at different depths, as produced by the multi-resolution training mechanism detailed in Sec. 4.4, and"}, {"title": "7 CONCLUSION", "content": "In this work, we introduce FovealNet, an AI-based gaze tracking solution designed to enhance the performance of TFR systems. FovealNet can be directly optimized using a loss function that incorporates system performance metrics, resulting in superior outcomes compared to baseline algorithms. To further reduce the implementation cost of the gaze tracking algorithm, FovealNet utilizes an event-based cropping technique that discards irrelevant pixels from the input image. Moreover, it features an efficient token-pruning strategy that dynamically eliminates tokens during processing without sacrificing tracking accuracy."}]}