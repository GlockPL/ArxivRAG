{"title": "Hybrid Graphs for Table-and-Text based Question Answering using LLMs", "authors": ["Ankush Agarwal", "Ganesh S", "Chaitanya Devaguptapu"], "abstract": "Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context.", "sections": [{"title": "1 Introduction", "content": "In today's data-rich world, information is scattered across various sources, which can be broadly divided into two types: structured (tables, databases) and unstructured (raw text from various files). The ability to effectively answer questions that require reasoning and aggregation across diverse data sources has become increasingly crucial. Such questions are often referred to as hybrid questions and the task is often referred to as Table-Text Question Answering (QA). For instance, consider the question: \"What place was achieved by\nthe person who finished the Berlin marathon in 2:13.32 in 2011, the first time he competed in a marathon?\", answering this question requires extracting the name of the person who finished the Berlin marathon in 2011 from raw unstructured text and linking it with a structured data source like a database to determine their place. The ability to answer hybrid questions has immense real-world value, as it enables the combination of relevant information from multiple sources, leading to more comprehensive and capable QA systems.\nExisting QA methods primarily focus on single data sources, either structured or unstructured. These approaches are limited when answering questions that require reasoning and aggregation across both structured and unstructured data. Datasets like Hybrid-QA (Chen et al., 2020b) and OTT-QA (Chen et al., 2020a) necessitate combining information from tables and text, however current methods applied to these datasets (Lee et al., 2023; Eisenschlos et al., 2021; Kumar et al., 2023; Lei et al., 2023) rely on the availability of training data"}, {"title": "2 Related Work", "content": "Question Answering: With the advent of language models, the task of question answering over structured and unstructured data sources has gained a significant traction and interest. For structured data sources like tables, recent methods have focused on generating SQL queries from natural language input (Wang et al., 2020; Hui et al., 2022; Li et al., 2023a,b; Gao et al., 2023), converting structured data to graphical forms (Jiang et al., 2023; Perozzi et al., 2024; Tan et al., 2024), or allowing language models to directly interact with the tables (Herzig et al., 2020; Wang et al., 2021). Similarly, efforts have been made to develop efficient QA systems for unstructured text (Seo et al., 2016; Hu et al., 2018; Perez et al., 2020; Seonwoo et al., 2020). While many approaches have been proposed for answering questions on either structured or unstructured data sources while treating them in isolation, there is a scarcity of methods that focus on Table-Text QA systems, i.e., questions that require reasoning and aggregation of information from both structured and unstructured data sources simultaneously.\nTable-Text Question Answering: The task of Table-Text QA based on text-and-tabular data has recently started gaining traction because of the creation and availability of datasets like Hybrid-QA (Chen et al., 2020b) and OTT-QA (Chen et al., 2020a), both of which are based on Wikipedia as the primary data source containing both text and tabular data.\nTo tackle challenges associated with Table-Text"}, {"title": "3 Methodology", "content": "In this section, we explain the key parts of our approach for solving Hybrid-QA in a fine-tuning-free manner. We begin by describing the problem setting, followed by a detailed explanation of our method, which includes question analysis, graph construction and graph traversal. Finally, we discuss the retrieval and the information that is passed into the LLM."}, {"title": "3.1 Problem Definition", "content": "Assume a structured data source $T$ with rows denoted using $R$, where $r_i \\in R$ denotes the $i^{th}$ row, and headers denoted using $H$, where $h_j \\in H$ denotes the $j^{th}$ header. Let $T(r_i, h_j)$ represent the value in the $i^{th}$ row and the $j^{th}$ header / column. For a few cells in every row, certain cells in column $h_j$ contain unstructured data sources linked to them. Specifically, $T(r_i, h_j)$ is linked to $k$ documents ${d_{ij}^1, d_{ij}^2,...,d_{ij}^k}$ which contain unstructured information.\nGiven a natural language question $Q$ related to both the structured data $T$ and the unstructured data $D$ (comprising the linked documents), the objective is to output an answer $A$.\nTo aid in the explanation of our methodology, we will use the following example throughout this section. Consider the following question $Q$ from one of the datasets we consider in our experiments: \"The driver who finished in position 4 in the 2004 United States Grand Prix was of what nationality?\". This question requires reasoning and aggregation"}, {"title": "3.2 Proposed Method (ODYSSEY)", "content": "Our proposed method consists of the following key steps:\n1.  Question Analysis: We analyze the question to identify key entities and entity types that will later aid in graph construction, traversal, and answering the question (Section 3.2.1).\n2.  Hybrid Graph Construction: We locate the tabular evidence through structure modeling and model the relationships among heterogeneous evidence (Section 3.2.2).\n3.  Hybrid Graph Traversal: We prune the Hybrid Graph based on question-derived entities and perform multi-hop reasoning and traversal (Section 3.2.3).\n4.  Reader LLM: We use a Language Model to generate the final answer using the pruned graph and linked documents (Section 3.2.4)."}, {"title": "3.2.1 Question Analysis", "content": "To effectively address questions based on a given context, our approach utilizes an LLM to identify entities and entity types within the question. For example, in figure 3 a, we identify entities such as 'position 4' and '2004 United States Grand Prix', along with entity types 'driver' and 'nationality'. These entities and entity types guide us in determining relevant table headers, such as 'Pos' and 'Driver', where 'Pos' corresponds to 'position 4' and 'Driver' corresponds to the entity type. We establish an entity-header mapping by aligning entities with the retrieved relevant headers. Entities that match specific headers are mapped accordingly, while those that do not fit any header are categorized as 'Others' (see figure 3 - ). The effectiveness of this mapping technique will be demonstrated in Section 3.2.3. The Question Analysis prompt used in our method is detailed in Appendix A.3.1."}, {"title": "3.2.2 Hybrid Graph Construction", "content": "Our Hybrid Graph consists of two connected components: the table and documents. By selecting relevant table headers, we retrieve the sub-table. Simultaneously, from the documents, we construct"}, {"title": "3.2.3 Hybrid Graph Traversal", "content": "After constructing the Hybrid Graph, we prune it based on the question to filter out noise. Using the entity-header mapping dictionary (described in Section 3.2.1), we perform a Breadth-First Search (BFS) to semantically match the question entities with table column cells and entities in the entity-document graph. For semantic matching, we first gather all document entities into a set called entity_total. In the entity-header mapping, entities mapped to a header are aligned with the corresponding column, while those mapped to \u2018Others\u2019 are aligned with entity_total (as explained in Section 3.2.1). For example, as shown in Figure 3, the entity 'Positon 4' is mapped to the header 'Pos', and after semantic matching, it aligns with the entity '4'. In contrast, the entity \u20182004 United States Grand Prix' does not match any entity in entity_total. If the header-mapped entities are not successfully matched, we expand the matching process to include other table columns to account for potential noise in the entity-header mapping. Once the semantically matched entities are obtained, we initiate a BFS traversal using them as starting points. In figure 3, the starting point is entity '4'. We perform a 3-hop traversal, which results in a pruned graph (as shown in Figure 3 - \u2462). During traversal, we record the paths in the graph and store them in a hop-wise dictionary, cat-"}, {"title": "3.2.4 Reader LLM prompt", "content": "After obtaining the pruned graph and storing it in a hop-wise dictionary, we use an LLM to answer the question. Initially, only 1st-hop items are provided as context. If no answer is found, the LLM returns 'None'. We then include 2nd-hop items as context, along with relevant linked passages given as output from the 1st-hop LLM call, and concatenate the 1st-hop and 2nd-hop tables. This iterative process continues up to 3 hops. If the LLM still returns 'None' after 3 hops, the entire table-text is used as context. The number of such cases is mentioned in Section 5.1. We limit the process to 3 hops due to minimal improvements observed in LLM performance beyond that during hyperparameter tuning (see Section 4). The LLM reader prompt used in our method is detailed in Appendix A.3.2."}, {"title": "4 Experimental Setup", "content": "This section details our experimental setup, including the datasets used to evaluate our method, the baselines for comparison, evaluation metrics, and implementation specifics."}, {"title": "4.1 Experimental Datasets", "content": "We evaluate ODYSSEY on two Table-Text Hybrid-QA datasets: Hybrid-QA (Chen et al., 2020b) and OTT-QA (Chen et al., 2020a). A detailed explanation of the experimental datasets is provided in Appendix 4.1.\nHybrid-QA is a large-scale, complex, multihop Table-Text Hybrid-QA benchmark comprising tables and texts from Wikipedia. Each table row describes various attributes of an instance, linked to corresponding Wikipedia passages that provide detailed descriptions.\nOTT-QA extends Hybrid-QA into a large-scale open-domain QA dataset over tables and text, requiring both table and passage to be retrieved before answering questions. It samples around 2,000 questions from the in-domain Hybrid-QA dataset, mixing them with newly collected out-domain questions for the dev and test sets. OTT-QA consists of 41,469 questions in the training set, 2,214 in the dev set, and 2,158 in the test set. Unlike Hybrid-QA, many questions in OTT-QA have multiple plausible"}, {"title": "4.2 Baselines", "content": "We operate in a fine-tuning-free, zero-shot and closed-domain setting. Our experiments involve both closed-source and open-source large language models. For closed-source models we utilize two popular LLMs - GPT-3.5-turbo-1106 and GPT-4-1106-preview (Achiam et al., 2023), both at a temperature setting of 0. For open source LLMs, we employ Llama3-8B. We compare our method against three baselines:\n*   Base: To test the parametric knowledge of the reader LLM, we provide only the question and elicit a response.\n*   Base w/ Table & Text (Zhang et al., 2023): To test the hybrid tabular and textual QA understanding of the LLM, we provide the full context (table and passages) following the prompt outlined in Zhang et al. (2023).\n*   Base w/ Table & summarized Text6: To reduce the noise in the unstructured information and to make it easier for LLM to find the answer, we employ LangChain's proposed approach to summarize (Jin et al., 2024) the text and then pass the summary of the text with the entire table to the LLM for answering the questions.\n*   We also show results when both the text and the table are summarized. Refer to Appendix A.1.2 for details.\nAll prompts used for the above mentioned baselines are detailed in Appendix A.4."}, {"title": "4.3 Evaluation Metrics", "content": "For evaluation, we employ several metrics: Exact Match (EM) (Zhang et al., 2023), F1-Score (Lei et al., 2023), Precision (P), and Recall (R) to assess the correctness of predicted answers. These metrics are implemented using the same codebase as Hybrid-QA (Chen et al., 2020b). Additionally, for semantic evaluation, we employ BERTScore-F1 (B) (Zhang et al.) and utilize the bert-base-uncased model (Devlin et al., 2018) for computing similarity matching."}, {"title": "4.4 Implementation Details", "content": "ODYSSEY hyperparameters We use GPT-3.5-turbo-1106 for question analysis, including entity extraction, relevant header fetching from tables, and entity-header mapping. To fetch entities from linked passages, we use the SpaCy7 transformers model with en_core_web_trf, which utilizes a ROBERTa-base (Liu et al., 2019) model. These components collectively enable us to construct the Hybrid Graph.\nFor graph traversal, we match question entities with the Hybrid Graph using the instructor-xl model, running on a GPU with 6GB of VRAM. We select the highest-ranked entity surpassing a 0.8 threshold. During the breadth-first search traversal, we store up to 3-hops to pass to the LLM reader."}, {"title": "5 Results and Analysis", "content": "In this section, we presents the results of our method and compare it with the baselines discussed in Section 4.2. We show results using Llama3-8B, GPT-3.5 and GPT-4, along with the fine-tuned models on Hybrid-QA and OTT-QA datasets (see Table 3, 4, 5) across various evaluation metrics discussed in Section 4.3. Additionally, we analyze the token efficiency, i.e., the number of tokens passed as input to the LLM (see Table 6). Furthermore, we provide a detailed analysis of our method, including an ablation study, error analysis (shown in Appendix 5.4), and a hop-wise breakdown of the results, which are illustrated in Figure 4."}, {"title": "5.1 Evaluation on Tabular-and-Text QA", "content": "We show evaluation on Hybrid-QA and OTT-QA datasets in Table 3, 4, 5.\nTable 3 shows the performance of ODYSSEY, our method with hop-wise extraction (denoted as 'w/ hopwise' in the table). It consistently outperforms all baseline methods across Exact Match (EM), F1-score, Precision, Recall and BERTScore-F1 on the Hybrid-QA and OTT-QA datasets for all three LLMs. This improvement underscores the effectiveness of our approach in efficiently extracting crucial information while effectively filtering noise from the table-text in a question specific manner. It is worth noting that our model performs best in the zero-shot setting for closed-source LLMs like GPT-3.5 and GPT-4, and also shows strong performance with the comparatively smaller open-source model Llama3-8B for the task of Table-Text QA.\nAs mentioned in Section 3.2.4, if the LLM outputs 'None' after 3-hops, it becomes necessary to pass the entire table and text as input to the LLM. These occurrences, where graph traversal is not possible or the LLM requires the full context, account for approximately 10% and 8% of the experiment data for Hybrid-QA and OTT-QA, respectively, across all models. However, it is important to note that, even without utilizing the complete"}, {"title": "5.2 Efficient Query Context Handling", "content": "In complex QA tasks, such as Hybrid tabular and textual QA with LLMs like GPT-3.5 and GPT-4, retrieving relevant information from the data corpus is crucial. This enables the LLMs to better connect the links between the structured and unstructured data for more accurate QA. Our method achieves this task by effectively filtering out noise, which not only increases QA accuracy (as shown in Table 3) but also results in a reduced reader input context size compared to the original context (as shown in Table 6). Specifically, the Question Analysis component of our method, discussed in Section 3.2.1 requires on average an input token size of 989 for Hybrid-QA and 850 for OTT-QA respectively. This amounts to total input token sizes to 4846 and 3595, respectively, representing a 32.65% and 38.7% reduction compared to the original context for Hybrid-QA and OTT-QA datasets respectively. Additionally, the output token size is smaller across all methods; our method produces around 70 tokens, while the baselines typically produce 4 tokens.\nWe analyze the questions answered by our method using a hop-wise criteria on the Hybrid-QA dataset (as shown in Figure 4). Our method answers 144 questions with GPT-3.5 and 190 questions with GPT-4, achieving EM scores of 50.7% and 64.8% in 1-hop (see left-side of Figure 4) with average token sizes of 1369 and 1479 (see right-side of Figure 4), respectively. For both Hybrid-QA and OTT-QA, almost 90% questions were answered using 1-hop and 2-hop connections. This"}, {"title": "5.3 Ablation Study", "content": "We present following ablation studies in this section: i) Hop-wise retrieval, and ii) Pruned Graph.\nHop-wise We passed pruned information all at"}, {"title": "5.4 Error Analysis", "content": "We performed a detailed error analysis on 100 random samples from the HybridQA development set using our method, ODYSSEY, with GPT-4 in terms of EM score. Out of these, 41 answers were incorrect. The breakdown of our findings is as follows:\n1.  Expression Mismatch (17 cases): Our method provided semantically correct answers expressed differently from the gold standard. For example, it might output \"hosted by Regis"}, {"title": "6 Conclusion", "content": "In this paper, we introduce ODYSSEY, a zero-shot fine-tuning-free approach for Table-Text QA. By leveraging a novel Hybrid Graph-based approach, Odyssey effectively navigates the complexities of multi-hop reasoning across structured and unstructured data sources. Our method achieves significant improvements of 10% and 4.5% in Exact Match (EM) scores using GPT-3.5 on the Hybrid-QA and OTT-QA datasets, respectively, compared to the baseline, while reducing the input token size for the LLM reader by 45.5% on Hybrid-QA and 53% on OTT-QA, demonstrating its efficiency in representing relevant information concisely. We believe the insights gained from our method can pave the way for more advanced and efficient QA systems capable of navigating the ever-growing landscape of heterogeneous data sources."}, {"title": "Limitations", "content": "The limitations of our work are as follows: 1) While our method achieves the highest accuracy"}, {"title": "A Appendix", "content": "In this section, we provide additional results and details that we could not include in the main paper due to space constraints. In particular, this appendix contains the following:\n*   Additional Results\n*   ODYSSEY Walkthrough with a Detailed Example\n*   Prompts used for ODYSSEY\n*   Prompts used for baselines"}, {"title": "A.1 Additional Results", "content": "This section is divided into 2 parts: i) Input Complete Hybrid Graph, and ii) Results on Summarizing both the table and text. Additionally, it includes a table comparing the execution time per instance between our method and the baselines (see Table 9)."}, {"title": "A.1.1 Input Complete Hybrid Graph", "content": "Table 7 compares our method with the Complete Hybrid Graph approach, where the entire Hybrid Graph is passed directly without any traversal or"}, {"title": "A.1.2 Summarising both the table and text", "content": "For this baseline, we adopted the method proposed by LangChain\u00b2, which involves summarizing tables and text and pass them as input to the LLM for QA. However, converting structured tables into summarized text led to a loss of information, resulting in lower scores Although summarizing the context reduces token size, it often fails to effectively filter out noise and may erase relevant information. We address this limitation in our method by pruning relevant information."}, {"title": "A.2 ODYSSEY Walkthrough with a Detailed Example", "content": "In this section, we will understand ODYSSEY with an illustrated example. Below, we describe the context of the chosen example, including the question, table, and passages.\nQuestion: The driver who finished in position 4 in the 2004 United States Grand Prix was of what nationality ?\nTable Headers: [Pos, Driver, Constructor, Time, Gap]\nThe table contains 20 rows, with some cells linked to passages. We will list some of these passages below.\nPassages:\n1.  Michael_Schumacher : Michael Schumacher (born 3 January 1969 ) is a retired German racing driver who raced in Formula One for Jordan Grand Prix , Benetton and Ferrari , where he spent most of his career , as well as for Mercedes upon his return to the sport . Widely regarded as one of the greatest Formula One drivers ever , and regarded by some as the greatest of all time . Schumacher is the only driver in history to win"}]}