{"title": "MODEL MIMIC ATTACK: KNOWLEDGE DISTILLATION FOR PROVABLY TRANSFERABLE ADVERSARIAL EXAMPLES", "authors": ["Kirill Lukyanov", "Andrew Perminov", "Denis Turdakov", "Mikhail Pautov"], "abstract": "The vulnerability of artificial neural networks to adversarial perturbations in the black-box setting is widely studied in the literature. The majority of attack methods to construct these perturbations suffer from an impractically large number of queries required to find an adversarial example. In this work, we focus on knowledge distillation as an approach to conduct transfer-based black-box adversarial attacks and propose an iterative training of the surrogate model on an expanding dataset. This work is the first, to our knowledge, to provide provable guarantees on the success of knowledge distillation-based attack on classification neural networks: we prove that if the student model has enough learning capabilities, the attack on the teacher model is guaranteed to be found within the finite number of distillation iterations.", "sections": [{"title": "INTRODUCTION", "content": "The robustness of deep neural networks to input perturbations is a crucial property to integrate them into various safety-demanding areas of machine learning, such as self-driving cars, medical diagnostics, and finances. Although neural networks are expected to produce similar outputs for similar inputs, they are long known to be vulnerable to adversarial perturbations [Szegedy et al. (2014)] \u2013 small, carefully crafted input transformations that do not change the semantics of the input object, but force a model to produce a predefined decision. The majority of methods to study the adversarial robustness of neural networks are aimed at crafting adversarial perturbations which indicate that, in general, the predictions of a neural network are unreliable. The most effective and stealthy attacks require access to the model's gradients and are therefore of little practical use on their own [Goodfellow et al. (2014); Madry et al. (2017); Carlini & Wagner (2016)]. However, in real-world scenarios, machine learning models are often deployed as services"}, {"title": "RELATED WORK", "content": "In this section, we provide a brief overview of existing black-box adversarial attacks and applications of knowledge distillation."}, {"title": "TRANSFERABLE ADVERSARIAL PERTURBATIONS", "content": "In this work, we focus on the transferability of an adversarial attack from a white-box model to a black-box one, emulating a black-box attack. Black-box adversarial attacks can be divided into two categories: query-based and transfer-based. In a query-based attack, an adversary uses an output of the target model to compute an adversarial example. One way to do this is to estimate the gradient of the model to the input object [Bhagoji et al. (2018); Chen et al. (2017); Ilyas et al. (2019); Guo et al. (2019)]. However, these methods usually require a lot of queries to the target model, which makes them infeasible in practice. In a transfer-based attack, an adversary generates adversarial examples by attacking one or several surrogate models [Liu et al. (2022); Qin et al. (2023)]. The transferability of adversarial examples generated for surrogate models to the target model can be improved by utilizing data augmentations [Xie et al. (2019)], exploiting gradients [Wu et al. (2020)], gradient aggregation [Liu et al. (2023)] or direction tuning [Yang et al. (2023)].\nThere are plenty of black-box attack methods known, for example, ZOO [Chen et al. (2017)] and NES [Ilyas et al. (2018)]. ZOO attack sequentially adds a small positive or negative perturbation to each pixel of the target image. It then queries the black-box model to estimate the gradient in the vicinity of the target image. NES attack works similarly. However, instead of changing pixel by pixel, a set of random images is generated, which are used to approximately estimate the gradients.\nCurrent SOTA methods are Square Attack [Andriushchenko et al. (2020)], NP-Attack [Bai et al. (2020)], MCG [Yin et al. (2023)] and Bayesian attack [Li et al. (2023)]. Square Attack works differently. The attack selects an area of the image that is subject to attack and then gradually changes this area as the algorithm runs. And within the selected area, random pixels are selected that are changed. NP-Attack leverages a neural predictor model to guide the search for adversarial perturbations by predicting the model's output with fewer queries. MCG is a meta-learning-based black-box attack that leverages a meta-classifier to generalize adversarial attacks across different black-box models. The idea is to train a meta-classifier to guide the adversarial example generation. Bayesian attack enhances the transferability of adversarial examples by using a substitute model with Bayesian properties. The key idea is to make the substitute model more Bayesian through techniques like Monte Carlo dropout or stochastic weights, which results in better uncertainty estimation. This improved uncertainty estimation enhances the transferability of adversarial examples crafted on the substitute model to the target black-box model.\nNote that Bayesian attack [Li et al. (2023)] belongs to the transfer-based category and implies access to part of the training data of the black-box model. In our work, we assume that an adversary has no access to the training data and, thus, we do not compare our approach against methods from the transfer-based category."}, {"title": "KNOWLEDGE DISTILLATION AND ADVERSARIAL ROBUSTNESS", "content": "Knowledge distillation (KD) is a method to transfer the performance of a large teacher neural network to a smaller, lightweight student neural network [Hinton (2015)]. Given a teacher model T, the framework is used to train a student network S by solving an optimization problem:\n$S = arg min E_{(x,y)~D} [AL(S'(x), y) + (1 \u2212 \u03b1)\u03c4\u00b2KL(S'(x),T(x))],$\nwhere D is the distillation dataset, L is the classification loss function used to assess the performance of the student model, KL is the Kullback-Leibler divergence and \u03b1,\u03c4 are the scalar parameters. Knowledge distillation has been used in a large scope of problems, such as model compression [Sun et al. (2019); Wang et al. (2019); Li et al. (2020)], data privacy [Lyu & Chen (2020); Chourasia et al. (2022); Galichin et al. (2024); Pautov et al. (2024)], adapted for large language models [McDonald et al. (2024); Gu et al. (2024); Kang et al. (2024)] and diffusion models [Huang et al. (2024); Yao et al. (2024); Yin et al. (2024)].\nIt has recently been shown that knowledge distillation can be used to enhance the adversarial robustness of additive perturbations [Papernot et al. (2016); Kuang et al. (2024); Huang et al. (2023)]. In contrast to a large teacher model which can attain a satisfactory level of adversarial robustness, it is challenging to make a small student model both robust and similar to the teacher one in performance [Huang et al. (2023)]. To deal with this issue, adversarially robust distillation was proposed [Goldblum et al. (2020)]. This approach takes into account clean predictions [Goldblum et al. (2020)] or probability vectors [Zi et al. (2021)] of robust teacher model during the distillation procedure."}, {"title": "PROBLEM STATEMENT", "content": "In this section, we formally discuss a problem statement, introduce the notations used throughout the paper, and formulate the research question."}, {"title": "ADVERSARIAL EXAMPLE FOR A CLASSIFICATION NEURAL NETWORK", "content": "Suppose that f: Rd \u2192 \u2206K is the classification neural network that maps input object x \u2208 Rd to the vector f(x) \u2208 \u2206K of probabilities of K classes and\nh(f,x) = arg max f(x)i\ni\u2208 [1,..., \u039a]\nis the associated classification rule. We begin by formally defining an adversarial example for the given classification neural network and the transferability of an adversarial example between the two networks.\nDefinition 3.1 (Adversarial Example). Suppose that x \u2208 Rd is the input object correctly assigned to class y\u2208 [1,..., K] by the network f, namely, h(f, x) = y. Let d > 0 be a fixed constant. Then, the object x' \u2208 Rd : ||x \u2212 x\u2032||2 \u2264 d is the untargeted adversarial example for f at point x, if\nh(f,x') \u2260h(f,x).\nIf h(f, x') = t for some predefined class index t, then x' is called targeted adversarial example.\nDefinition 3.2 (Transferable Adversarial Example). Let x' be the adversarial example computed for the network f at point x and let g : Rd \u2192 \u25b3K be the separate network. Then, x' is transferable from f to g, if\n[h(f,x) = h(g,x),\n[h(f,x') = h(g, x')."}, {"title": "KNOWLEDGE DISTILLATION OF A BLACK-BOX MODEL", "content": "In this paper, we focus on using knowledge distillation [Hinton (2015)] to construct adversarial perturbations for the given classification model deployed in the black-box setting. Namely, let T : Rd \u2192 AK be the black-box teacher model trained on an unknown dataset D(T) and S : Rd \u2192 \u25b3K be the white-box student model, possibly of a different architecture, and let D(S) be its training dataset. To approximate the teacher model, we apply soft-label knowledge distillation, which is done in two steps. Firstly, the teacher model is used to collect the training dataset for the student model. In our setting, we use a hold out dataset Dh = {(xi, Yi)}=1 to construct D(S) :\nD(S) = {(xi,T(xi))}=1,\nwhere xi \u2208 Dh and T(xi) \u2208 \u2206\u039a. Then, the student network S is trained on the dataset D(S) by minimizing an empirical risk\nL(S,D(S)) = \\frac{1}{m}\\sum_{(xi,Yi) \\in D(S)}1(S, Xi, Yi),\nwhere l(S, x, y) = \u2212 log(S(x)y) is the cross-entropy loss function.\nWhen the student model is trained, we ask the following research question. Given x \u2208 Rd : h(S,x) =\nh(T, x) and d > 0 from the definition 3.1, is it possible to compute an adversarial example for the model S\nat point x which is provably transferable to T? In the next section, we answer this question and propose a\nknowledge distillation-based adversarial attack with transferability guarantees."}, {"title": "METHODOLOGY", "content": "In this section, we describe the proposed approach to generate adversarial examples for the black-box teacher model via knowledge distillation. In the last subsection, we prove that, under several assumptions, our approach generates an adversarial example that is transferable to the teacher model within the finite number of iterations."}, {"title": "MODEL MIMIC ATTACK: STUDENT FOLLOWS ITS TEACHER", "content": "To perform an adversarial attack on the black-box teacher model T, we first apply soft-label knowledge dis-tillation and obtain the white-box student model S. The training dataset for the student model is constructed\nby querying the teacher model and collecting its predictions for the points from the hold-out dataset Dh,\npossibly disjoint from the teacher's training dataset (D(T) : Dh \u2229D(T) = (\u00d8). In our setup, we use the test\nsubset of the teacher's dataset as the hold-out dataset Dh.\nRecall that D(S) = {(xi,T(xi))} 1, according to equation 5. Assuming that the student model has enough\nlearning capability, we train it until it perfectly matches the teacher model on D(S), namely,\nSh(S, xi) = h(T, xi) = Yi\n(||S(xi) - T(xi)||\u221e <,\nfor all (xi, Yi) \u2208 D(S), where \u025b > 0 is the predefined constant. In equation 7, the second condition reflects\nthe ability of the student model to confidently mimic the teacher model on D(S)."}, {"title": "MODEL MIMIC ATTACK: STUDENT UNDER ATTACK", "content": "In this subsection, we describe a procedure to generate a single adversarial example for the student model."}, {"title": "MODEL MIMIC ATTACK: PROVABLY TRANSFERABLE ADVERSARIAL EXAMPLES", "content": "It should be mentioned that, under several assumptions, the Algorithm 1 is guaranteed to find an adver-sarial example that is transferable from the student model to the teacher model within the finite number of"}, {"title": "EXPERIMENTS", "content": "This section will describe the experiments and everything needed to reproduce them. In particular, a de-scription of the datasets, a method for evaluating the experiments, a description of the methods we compare with, and the methodology for conducting the experiments."}, {"title": "SETUP OF EXPERIMENTS", "content": "In our experiments, we use CIFAR-10 and CIFAR-100 [Krizhevsky et al. (2009)] as the training datasets for the teacher model. We use ResNet50 [He et al. (2016)] as the teacher model T, which was trained for 250 epochs to achieve high classification accuracy (namely, 82% for CIFAR-10 and 47% for CIFAR-100. To train the teacher model, we use the SGD optimizer with the learning rate of 0.1, the weight decay of 10-4, and the momentum of 0.9.\nWe use ResNet18 and SmallCNN as the white-box student models. The architecture of SmallCNN is presented in the Appendix. We conduct the PGD attack on the student models with the following parameters: the number of PGD steps is set to be M = 10, the gradient step is set to be a = 0.005, the distance threshold is set to be d = 0.05. The detailed architecture of the Small CNN model is presented in the appendix A.\nIn this section, we briefly list the set of methods we compare our approach against. We evaluate MMAttack against ZOO [Chen et al. (2017)], NES [Ilyas et al. (2018)] as the main competitors. Among the black-box attack methods based on a random search, we choose Square attack [Andriushchenko et al. (2020)] as the state-of-the-art in terms of an average number of queries to conduct an attack. In the group of methods using gradient estimation, NP-Attack [Bai et al. (2020)] is among the most efficient attacks. In the category of combined methods, we choose MCG [Yin et al. (2023)]. The hyperparameters that were used in the experiments with Methods for Comparison are described in detail in the appendix \u0412.\nNote that the MCG algorithm originally assumes the training on the data from a distribution that is close to the teaches model's one, which in general may not be known. Here, we highlight that our method does not have such a limitation.\nTo illustrate the efficiency of the proposed approach, we report the Average Query Number (AQN) and demonstrate the trade-off between AQN and the Average Success Rate (ASR). AQN denotes the number of queries required to generate all the adversarial examples for the black-box model, averaged over all the examples. ASR measures the fraction of adversarial examples assigned to a different class in an untargeted attack setting or to the predefined other class in the targeted attack setting. For AQN, a lower value indicates better attack performance, while for ASR, a higher value indicates a better attack performance. Note that both metrics are calculated over successful adversarial attacks only. In this paper, the emphasis is made on minimizing the AQN."}, {"title": "RESULTS OF EXPERIMENTS", "content": "In the experiments, ZOO, NES, and Square attack methods were executed 100 times with different random seeds, NP-Attack, MCG, MMA methods were executed 30 times."}, {"title": "ABLATION STUDY", "content": "Note that the success of our black-box attack crucially depends on the architecture of the white-box student model. On the one hand, the student model does not have to have many training parameters since it implies several retraining iterations. On the other hand, it has to have enough learning capacity to mimic the behavior of the black-box model in the vicinity of the target point. In Table 2, we report the AQN values for the different pairs of teacher and student models on the CIFAR-10 dataset. Together with the average number of queries, we report the size of the initial training dataset D(S1) of the student model and the number of adversarial examples to generate for the student model, l. We found that the simpler the architecture of the student model, the fewer queries to the teacher model are required to conduct a successful attack.\nThe initial set size, |D(S1)|, represents the number of random data points to be included in the initial training dataset of the white-box student model. It can be seen from Table 2, that the more complex the student model is, the larger this parameter should be. The same is true for the number of adversarial examples for the student model, l.\nNote that there is no AQN value corresponding to |D(S1)| = 5 and l = 5. This is because the Algorithm 1 does not succeed in finding a single adversarial example for the black-box teacher model until it reaches the maximum iterations threshold."}, {"title": "LIMITATIONS", "content": "Note that the transferability guarantee from Theorem 4.1 is given for the soft-label distillation. It is worth mentioning that the Theorem can not be adapted to the hard-label distillation without significant changes. Instead, to provide the transferability guarantee in hard-label distillation, when the teacher model outputs the predicted class label only, one can estimate the probability of transferability of an adversarial example within the finite number of iterations, conditioned on the white-box attack. If the lower bound of this probability is separated from zero, one can estimate the expected number of distillation iterations required to yield the transferable adversarial example."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose the Model Mimic Attack, the first framework to compute adversarial perturbations for a black-box neural network that is guaranteed to find an adversarial example for the latter. To conduct an attack, we apply knowledge distillation to obtain the student model, which is essentially the functional copy of the black-box teacher network. Then, we perform the white-box adversarial attack on the student model and theoretically show that, under several assumptions, the attack transfers to the teacher model. We demonstrate experimentally that a successful adversarial attack can be found within a small number of queries to the target model, making the approach feasible for practical applications. Possible directions for future work include an extension of the transferability guarantees to the hard-label distillation and adaptation of the proposed method for other domains, in particular, for attacking large language models."}, {"title": "APPENDIX: ARCHITECTURE OF SMALLCNN", "content": "SmallCNN (\n)\n(features): Sequential(\n(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1),\npadding=(1, 1))\n(1): ReLU(inplace)\n(2): MaxPool2d (kernel_size=2, stride=2, padding=0,\ndilation=1, ceil_mode=False)\n(3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1),\npadding=(1, 1))\n(4): ReLU(inplace)\n(5): MaxPool2d(kernel_size=2, stride=2, padding=0,\ndilation=1, ceil_mode=False)\n(6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1),\npadding=(1, 1))\n(7): ReLU(inplace)\n(8): MaxPool2d(kernel_size=2, stride=2, padding=0,\ndilation=1, ceil_mode=False)\n(classifier): Sequential(\n)\n)\n(0): Linear(in_features=4096, out_features=512, bias=True)\n(1): ReLU(inplace)\n(2): Linear(in_features=512, out_features=10 or 100,\nbias=True)\nB APPENDIX: HYPERPARAMETERS OF THE COMPARED ATTACK METHODS"}]}