{"title": "ToolFactory: Automating Tool Generation by Leveraging LLM to Understand REST API Documentations", "authors": ["Xinyi Ni", "Qiuyang Wang", "Yukun Zhang", "Pengyu Hong"], "abstract": "LLM-based tool agents offer natural language interfaces, enabling users to seamlessly interact with computing services. While REST APIs are valuable resources for building such agents, they must first be transformed into AI-compatible tools. Automatically generating AI-compatible tools from REST API documents can greatly streamline tool agent development and minimize user learning curves. However, API documentation often suffers from a lack of standardization, inconsistent schemas, and incomplete information. To address these issues, we developed ToolFactory, an open-source pipeline for automating tool generation from unstructured API documents. To enhance the reliability of the developed tools, we implemented an evaluation method to diagnose errors. Furthermore, we built a knowledge base of verified tools, which we leveraged to infer missing information from poorly documented APIs. We developed the API Extraction Benchmark, comprising 167 API documents and 744 endpoints in various formats, and designed a JSON schema to annotate them. This annotated dataset was utilized to train and validate ToolFactory. The experimental results highlight the effectiveness of ToolFactory. We also demonstrated ToolFactory by creating a domain-specific AI agent for glycomaterials research. ToolFactory exhibits significant potential for facilitating the seamless integration of scientific REST APIs into Al workflows.", "sections": [{"title": "1 Introduction", "content": "Agents based on large language models (LLM) [1] are rapidly proving their versatility and universal applicability in various fields, such as scientific research [2], healthcare [3], finance [4], and so on. These agents show impressive reasoning and problem-solving capabilities, addressing real-world challenges. Tool agents, built on LLMs, aim at interacting autonomously with existing software or web services. For example, ChemCrow [5], a tool agent recently developed for chemistry research, leverages domain-specific software to complete various tasks, such as searching, instruction, analysis, and prediction. However, constructing such agents often requires significant software engineering effort and domain knowledge. On the other hand, a tool agent can utilize existing REST APIs, especially those associated well-designed schemas that clearly explain in machine-understandable ways how the APIs should be used correctly and precisely. Developing such tool agents involves minimal programming effort. Mastering REST APIs enables tool agents to access a variety of data and computing services. For example, Tool-Llama [6] utilizes over 16,000 APIs from RapidAPI, creating a comprehensive toolkit to address various problems in multiple application domains. However, it relies on API platforms for generating API tools, which cannot be transferred to build research A\u0399 agent.\nSeveral obstacles hinder the development of tool agents in research domains. Unlike commercial APIs, scientific APIs often lack comprehensive documentation and rarely adhere to standardized schemas. Even when a schema is provided, it may be incomplete or missing critical information, complicating the creation of a generalized tool generation process. Additionally, scientific APIs and their documentation are typically updated less frequently than commercial counterparts, necessitating extensive validation and refinement before they can be effectively utilized.\nWe consider it essential to develop an automated pipeline for generating agent-compatible tools in research domains. For scientific developers, such a pipeline enables the rapid creation of AI-usable tools, provided that APIs and their documentations are available in natural language. This improves development efficiency and allows developers to focus on their core research tasks. For end users, Al agents offer a natural language interface to interact with APIs, significantly reducing the learning curve and making tools more accessible. For the Al agent community, an automated pipeline allows for seamless integration of any available APIs, thereby greatly enhancing the versatility and capability of Al agents.\nTo address the limitations in tool agent development, we propose an open-source pipeline, ToolFactory\u00b9 (Figure 2), which autonomously generates tools from any REST API documentation written in natural language, eliminating the need for human intervention. To demonstrate our approach, we also introduced an API Extraction Benchmark specifically constructed for automatic tool generation:\n\u2022 Data collection: We gathered 167 free API documentations from APIList.com. Unlike previous benchmarks, we focused on APIs with diverse documentation formats (see Figure 1 for examples). This deliberate choice was made to ensure generalizability of our approach, as relying too heavily on APIs with uniform schemas (e.g., OpenAPI spec, RapidAPI schema) could limit the applicability of our approach. Documentation quality varies significantly among open-source APIs, which introduces challenges in understanding the APIs. We selected APIs that do not require authentication to simplify the evaluation framework and to avoid potential safety concerns. By using authentication-free APIs, we isolate the key"}, {"title": "2 Related Works", "content": "LLM for universal information extraction Structured information extraction is important as it builds bridge between human-readable formats into machine-usable data. Lu et. al.[10] proposed a universal text-to-structure framework for universal information extraction(UIE) using transformers, proving the potential of language models in encoding complex structures and producing structured outputs. Dagdelen et. al. [11] finetuned LLAMA2 [12] and GPT-3 [13] to extract the desired information from scientific text that can be fit into defined JSON schema. Zhu et. al. [14] proposed a multi-agent approach, encoporating GPT4 and GPT3.5 for automatic knowledge graph extraction and reasoning. OpenAI released GPT40 and GPT40-mini structured mode, which supports generating structured outputs in an update in July [15]. It forces GPT to follow the provided Pydantic structure or JSON schema. In this work, we employed both OpenAI model for data synthesis and finetuned LLAMA3 model for an opensource information extraction solution.\nTool agents LLM-based tool agents are able to reason through user queries, select and apply appropriate actions, and return the results of the chosen action. For example, Bran et. al. [5] develop Chem-Crow by integrating GPT-4 and 18 tools designed by experts. It was demonstrated that ChemCrow was able to effectively automate a variety of chemical tasks. Qin et. al. [6] collected 16k public REST APIs from the RapidAPI platform [16], and trained a tool retriever that can choose the most appropriate API in response to a user query. However, their tool-building approach relied on RapidAPI, limiting adaptability to other platforms without significant human effort. Similarly, RESTgpt[17] used OpenAPI Specification (OAS), restricting its applicability to APIs that provide OAS. Our goal is to create a universal tool-generation pipeline that works with any natural language API documentation."}, {"title": "3 API Extraction Benchmark Dataset", "content": "Previous research on tool agents, such as ToolLLM, has focused primarily on instructing LLM agents to use AI-usable tools and develop solution paths to answer user queries. Their Al-usable tools can be easily developed from APIs, supported by well-structured documentation, using relatively simple scripts. However, they did not focus on the challenges of developing Al-usable tools from APIs with less structured documentation. To fill this gap, we collected such APIs from APIList.com. We introduce our dataset as follows:\n\u2022 Documentation styles The API documentation we collected can be categorized into three levels based on the organization and clarity of the API descriptions: (1) Fully organized The documentation follows a well-defined template, providing all necessary information to call the API in a structured and comprehensive way. Use cases are clearly explained, often with example code. API documentation on platforms like RapidAPI Hub and Postman API typically fall into this category. (2) Semi-organized This type of documentation includes basic descriptions but lacks clarity for each endpoint. Some essential information may not be labeled with specific keywords and is instead embedded within general text. Additional effort is often required to identify key details. (3) Unorganized These documents are minimal, often missing example code or detailed descriptions. They require some level of inference and reasoning to understand the API's usage, with clues only available through endpoint names. We show that our benchmark consists of mostly semi-structure documentations, and only a few documentations are fully organized(Appendix B.1).\n\u2022 Selection criteria We filtered for API documents that do not require API keys, allowing for easier and more convenient API access without needing authentication, which often involves submitting forms or linking payment methods. Although automating API key sign-up is feasible using web agents [18], we opted for APIs without authentication requirements to streamline the process. In total, 347 unique API documents were selected and downloaded in HTML format. Since some links pointed to index pages or API information that was dynamically loaded via JavaScript,"}, {"title": "4 ToolFactory Pipeline", "content": "we employed a large language model to identify pages containing static HTML code with API endpoints. This approach ensured that we captured only the documentation with accessible and actionable API details.\nDue to variations in the quality and completeness of API documentation, we extracted only the essential information needed for tool generation. Specifically, for each API documentation, we captured the base URL and a list of endpoints. For each endpoint, we extracted the endpoint path, required parameters, optional parameters, and a brief description. In cases where the base URL was not specified, human annotation was necessary. The detailed schema used for this extraction is provided in the Appendix A.\nTo implement this extraction, we defined the schema using a Pydantic model and employed GPT-40 in structured mode to parse the HTML documents and extract the desired information. After filtering out pages that lack API information (primarily product index pages), we obtained 167 API documentation with 744 endpoints and extracted their structured information in JSON format. An example of input API documentation and output JSON structure is in Appendix B.2."}, {"title": "4.1 APILLAMA", "content": "We fine-tuned the LLAMA3 model to develop an open-source solution for extracting structured information from API documentation. Recent LLM-based structured information extraction approaches rely on in-context learning, which involves providing a detailed JSON schema description or a one-shot input-output example. These contexts must be included in every inference, leading to a significant overhead of redundant input tokens. Our approach is straightforward and efficient since we focus on extracting information based on a fixed schema (Appendix A), we employ soft prompt tuning to encode the task, compressing the JSON schema into fewer tokens (Appendix F). This method significantly reduces token overhead during inference, as we compressed the original instruction which consisted of approximately 572 tokens (18 for the instruction and 554 for the JSON schema definition), into just 20 virtual tokens.\nDespite its simplicity, our approach achieves competitive results. Even a fine-tuned 8B model demonstrates performance comparable to OpenAI's GPT models(Section 5.1). Prompt tuning also addresses the challenge of data availability; after filtering, the number of public, free APIs is relatively small, making smaller fine-tuned models less prone to overfitting.\nWe used 20 virtual tokens as the trainable prompt and applied 4-bit quantization to the main model, optimizing memory usage and training efficiency. We used Adam optimizer with learning rate 0.001. The training experiment was conducted on an NVIDIA A40 GPU and completed in 2 hours.\nTask Formalization\nAPILLAMA aims to generate the structured annotation of an API from its documentation. Specifically, given the embedding of an API documentation \\(X \\in R^{d \times n_x}\\) that contains \\(n_x\\) tokens with hidden dimension \\(d\\), a pre-defined output schema \\(S\\), a trainable instruction prompt \\(\\Phi \\in R^{d \times n_i}\\), a LLM \\(f_{\theta}\\) with frozen weight \\(\theta\\), the goal is to predict the corresponding structured information \\(Y^{(n_y)} = (Y_1, Y_2, \\dots, Y_{n_y}) \\in R^{d \times n_y}\\). Basically, we want to maximize the probability of conditional generation \\(P(Y|X; S; f_{\theta})\\). For a LLM \\(f_{\theta}\\), we used the"}, {"title": "4.2 Tool-generation Process", "content": "Once the necessary API information is extracted, the extracted JSON files are processed by scripts to generate AI-usable tools. These tools are designed to adapt the API's functionality into formats compatible with popular agent-building frameworks. We support exporting tool functions as Python functions, which can be directly integrated into open-source frameworks like LangChain[9] and OpenAgents[21]. Additionally, tools can be exported as OpenAPI YAML files, enabling seamless import into systems like OpenAI GPTs [22]. The details of the tool generation algorithm, including format adaptation and compatibility handling, are provided in Appendix C."}, {"title": "4.3 Tool Validation", "content": "An evaluation process is employed to validate each generated tool. Only tools that pass this evaluation will be made available for Al agents. We test a tool using example parameter values and consider it valid only if: (1) it returns a status code of 200, and (2) its content is neither empty nor an error message, as verified by an GPT-40 based evaluator. Otherwise, the tool is considered to have failed validation. We categorize the tool errors into 6 types: Missing Endpoint Path, Missing Base URL, Failed Validation, Abnormal Response, No Parameter Value and Wrong Parameter Value. A detailed explanation can be referred in Appendix E\nIn addition, we categorize error causes into four main categories: C1 Missing API Documentation Details, C2 Incorrectly Extracted URL Path, C3 Incorrect Parameter Values, and C4 Server-Side Errors. For each category, we provide a range of possible error diagnosis, from the most conservative to the most aggressive (see Appendix E1). We find that most of the errors are caused by C3, which inspires us to develop an better approach to produce high-quality parameter values."}, {"title": "4.4 Parameter Value Inference", "content": "API documentations vary in quality, and parameter value specifications are often missing, especially in poorly documented APIs. To tackle this problem, we propose the following method to infer missing parameter value specifications of an API from validated tools in the same application domain."}, {"title": "5 Results", "content": "Using the API Extraction Benchmark, we evaluated the capability of APILLAMA to extract API information from documentations in natural language and tested the automated tool generation process. We also demonstrated a case study where we built an Al-agent using the tools automatically generated by APILLAMA."}, {"title": "5.1 Structured API Knowledge Extraction via APILLAMA", "content": "We investigated the feasibility of combining LLM and soft prompt for extracting API knowledge.\nModel APILLAMA Consists of the LLAMA3-8B-instruct model, which is frozen, and 20 trainable soft prompt tokens. In addition, we extend the token limit from 8,192 to 10,240 to handle longer documents.\nTraining setting The input of the model is the text in an API documentation, and the output is the corresponding structured information in JSON. We randomly split the API Extraction Benchmark into 80% for training and 20% for testing. The test set contains 34 API documents and 117 different endpoints. More setting details are provided in Appendix F.\nMetrics See Section 4.1.\nBaselines We chose the original LLaMA3-8B-instruct and GPT3.5 as the baseline models. We tested how the supporting information will affect the model performance. In the \"one-shot learning\" setting, we include an example in the input prompt that consists of an API document as the input along with its corresponding JSON annotation as the target. On the other hand, we defined the target JSON schema with Pydantic[23] package, which can be used by the function calling mode[24] of GPT-3.5. This guarantees GPT-3.5 model to understand and produce the desired JSON structure.\nThe results are shown in Table 1. APILLAMA demonstrates strong performance, achieving competitive or superior scores across most metrics. In contrast, the vanilla LLaMA3 model performs poorly even when provided with a one-shot example. APILLAMA significantly improves the retrieval of API endpoints and the generation of accurate API annotations. Compared to GPT-3.5 in structured mode, APILLAMA retrieves fewer endpoints but excels in accurately extracting endpoint descriptions and parameters. The limited number of retrieved endpoints is likely due to the reasoning capability of the base LLAMA3-8B model, as its weights were not modified during fine-tuning. We attribute APILLAMA's performance improvements to two key factors: (a) The soft prompt effectively describes the structured information extraction task, allowing the model to better understand the schema; (b) By minimizing the number of tokens used for instructions, APILLAMA enables its base model to focus more on the actual data within the prompt."}, {"title": "5.2 Automatic Tool Generation", "content": "We evaluated the performance of ToolFactory in generating AI-usable tools. These tools were derived from the structured API information extracted in Section 5.1 and tested following the procedure described in Section 4.3. Each tool was implemented as a Python function that takes an API endpoint and its corresponding parameters as inputs, processes the request, and returns the API response. An example of such a tool is provided in Appendix D.\nThe results are summarized in Table 2. While APILLAMA extracted fewer endpoints likely due to the reasoning limitations of its base model-it produced the highest number of tools that passed validation. In the API Extraction Benchmark, 50 tools generated from ground-truth JSON files passed the validation test. APILLAMA was able to generate 26 tools (successful rate of 52%)."}, {"title": "5.3 Case Study: Automated Tool Agent Generation for Glyco Research", "content": "We applied ToolFactory to generate tools to support Glyco research and built an autonomous tool agent as the proof-of-principle. We collected the REST APIs of the most frequently used glyco databases, including GlycoData [25], GlyGen [26], GlyTouCan [27], KEGG_GLYCAN [28], Glycosmos [29], Glyconnect [30], The O-GlcNAc Database [31], GLYCAM [32], Protein API [33], PubChem [34] and UniLectin [35]. In total, 92 automatically generated tools passed validation, their functionalities cover data searching, calculation, format translation, correction, and visualization (Figure 4).\nContribution to Glyco Research Community\nOur AI agent provides natural language interfaces for researchers to access web services (e.g., databases and utility APIs) without requiring technical expertise, which greatly facilitates the usage of online resources and benefits researchers in several ways:\n(1) Automatic cross-database integration No single database fulfills all information needs due to their specific focuses. For example, GlyTouCan catalogs glycan structures, KEGG GLYCAN maps pathways and reactions, and PubChem offers general molecular data. Our Al agent integrates these diverse sources for seamless information access.\n(2) Tool synergy Individual tools are often limited to specific scenarios, but our Al agent enhances their applicability by integrating them into cohesive workflows, including ID conversion, database querying, data normalization, visualization, and resolution of inconsistencies. Appendix Table H3 shows an example of a glycan being represented using various formats. This integration broadens the applicability of certain tools. For instance, the GLYCAM 3D visualization tool, initially limited to GLYCAM strings, now supports multiple glycan formats through automated conversion."}, {"title": "6 Conclusion", "content": "In this work, we designed a pipeline, ToolFactory, that can automatically convert natural language REST API documentation into tools usable by AI agents. This pipeline not only translates natural language into agent-compatible tools but also diagnoses and optimizes non-standard or inaccurate information in REST API documentation. We first built the API Extraction Benchmark, which collects API documentation of varying quality and standardizes it into JSON format. Next, we trained APILLAMA using prompt tuning, enabling it to extract structured information from the documentation effectively. Our experimental results demonstrate that APILLAMA achieves strong performance in information extraction, with the number of tools generated being comparable to GPT-based models. We identified inaccurate parameter values as a key factor affecting APILLAMA's performance and proposed a method to enhance parameter quality by constructing a domain-specific knowledge base. Finally, we applied ToolFactory to glycomaterial research and successfully developed an Al agent. Our work facilitates the creation of domain-specific agents, helping developers reduce the development and learning costs associated with these APIs."}]}