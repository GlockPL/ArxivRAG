{"title": "CtrlNeRF: The Generative Neural Radiation Fields for the Controllable Synthesis of High-fidelity 3D-Aware Images.", "authors": ["Liu, Jiana", "Yu, Zhen"], "abstract": "The neural radiance field (NERF) advocates learning the continuous representation of 3D geometry through a multilayer perceptron (MLP). By integrating this into a generative model, the generative neural radiance field (GRAF) is capable of producing images from random noise z without 3D supervision. In practice, the shape and appearance are modeled by zs and za, respectively, to manipulate them separately during inference. However, it is challenging to represent multiple scenes using a solitary MLP and precisely control the generation of 3D geometry in terms of shape and appearance. In this paper, we introduce a controllable generative model (i.e. CtrlNeRF) that uses a single MLP network to represent multiple scenes with shared weights. Consequently, we manipulated the shape and appearance codes to realize the controllable generation of high-fidelity images with 3D consistency. Moreover, the model enables the synthesis of novel views that do not exist in the training sets via camera pose alteration and feature interpolation. Extensive experiments were conducted to demonstrate its superiority in 3D-aware image generation compared to its counterparts.", "sections": [{"title": "1. Introduction", "content": "In 2014, Goodfellow et al. proposed a generative adversarial network (GAN) [4], which is a deep generative model inspired by game theory. Subsequently, various GAN- derived models were developed for image generation and translation tasks [5]. A typical GAN comprises a generator and discriminator that compete with each other to attain Nash equilibrium. The purpose of the generator is to produce as much synthetic data as possible that aligns with the potential distribution of real data, whereas the discrimina- tor's aim is to accurately differentiate between genuine and fabricated data. Although GANs have achieved significant success in 2D image synthesis, the generated images cannot preserve the 3D consistency. In contrast, the neural radiation field (NERF) [3], which is briefly summarized as the use of an MLP network to learn a 3D geometric representation from a set of posed images, enables the rendering of images from an arbitrary view because it is a continuous 3D presentation of 2D images with camera poses. Due to the inherent features of the radiance fields, rendered images can enforce mul- tiview consistency. Currently, neural radiation fields have been successful in applications of reverse rendering, novel view synthesis, 3D object editing, digital human bodies, and image/video processing. The primary limitations of NERFs are that they require posed images for training and are unable to learn multiple scenes using a single MLP.\nBy integrating a neural radiance field into the generator, a generative radiance field (GRAF) [2] was implemented to produce 3D-aware images from random noise with a Gaus- sian distribution, and the model was trained on unstructured datasets without 3D supervision. The conditional radiance field in the generator uses 5D coordinates with the spatial location (x, y, z) and viewing direction (0, $) as inputs, and novel views are synthesized by projecting the output color c and density @ into an image using differential volume rendering. A patch-based discriminator was employed to distinguish between fake and real images. Furthermore, the shape and appearance are modeled by zs and za respec- tively, to manipulate them separately during the inference. The shape variable z, and the appearance variable za were obtained separately by sampling a Gaussian distribution.\nGRAF is capable of disentangling shapes from appear- ance using shape and appearance codes and taking precise"}, {"title": "2. Related Works", "content": "2D Image Synthesis: Generative adversarial networks (GANs) are deep generative models that perform advanced unsupervised tasks [1] such as image generation, image superresolution, and text-to-image synthesis, etc. For uncon- ditional GANs, for example DCGAN [6], the input of the generator is random noise, which is an unrestricted input that probably leads to low quality images in some cases, and the formation of an image is uncontrollable due to the randomness of the input. In contrast, conditional GANs, such as InfoGAN [9], CGAN [7], and ACGAN[8], incorpo- rate conditional variables (labels and text) into the generator and discriminator, allowing the generation of high-quality images with control. To stabilize the training process, the Wasserstein generative adversarial networks [10][11][12] used the Earth-Mover (EM) distance to optimize the objec- tive function, producing a better gradient behavior than other distance metrics.\nFor a typical GAN, obtaining high-resolution images is challenging because the discriminator can easily distinguish between false and true images at high resolution. Several strategies have been implemented to enhance the stability of the training process and progressively improve image resolution [13] [14] [15]. For example, a GAN HD pixel- to-pixel [16] can produce high-resolution images up to 2048 \u00d7 2048 pixels. To improve image generation control, several studies [17][18][19][20] have been conducted to disentangle the underlying factors of variation. Two-dimensional images are essentially projections of three-dimensional objects. However, they cannot ensure multiview consistency owing to the absence of 3D geometric constraints.\nImplicit Representation: Implicit representations of 3D geometry are popular for deep learning 3D reconstruction [21]. The advantages of voxel-based [22][23] [24][25] or mesh-based methods [26][27][28][29] are that implicit rep- resentations are continuous and are not restricted to topol- ogy. Recently, hybrid grid representations [30][31] have been extended to large-scale scenes, but all of the above methods require 3D input without considering texture. To overcome the limitations of 3D supervision, some stud- ies [32][33][34][35] presented differentiable rendering tech- niques to learn continuous shape and texture representations from 2D-posed images. Mildenhall et al. [3] proposed neural radiance fields, in which they combined an implicit neural model with volume rendering for novel view synthesis. NeRF requires multi-view images with camera poses for supervision and trains a single network per scene.\n3D-Aware Image Generation: To date, neural scene representations have been integrated into generative models to enable the synthesis of 3D-aware images from latent code. Voxel-based GANs [36][37][38] learn textured 3D voxel representations from two-dimensional images using differ- entiable rendering techniques. However, such voxel-based models are memory intensive, impeding high-resolution im- age synthesis. Radiance field-based methods [39] achieve higher quality and better 3D consistency, but have difficul- ties in training high-fidelity images due to the cost of the rendering process.\nMildenhall et al. [3] proposed neural radiance fields (NERF) that can implicitly represent 3D geometries and synthesize novel views using volume rendering. NERF and their variants are valuable tools for generating 3D-aware images. Despite their strengths, they are limited by slow training and inference, inability to handle dynamic scenes, generalization shortcomings, and the necessity for a great number of perspectives. To address these challenges, Garbin et al. [42] introduced FastNeRF, a method that can generate high-quality images at a rate of up to 200 Hz. To apply NERF to unknown scenes, studies on this issue include pixelNeRF [?] and IBRNet[45]. Furthermore, J. Gu et al. [46] proposed styleNeRF to synthesize high-resolution images at interac- tive rates, allowing control of camera poses and different levels of styles. Huang et al. [47] designed a framework for stylizing 3D scenes through 2D-3D mutual learning.\nTaking advantage of both GAN and NeRF, Schwarz et al. [2] introduced generative neural radiance fields (GRAF). Although the shape and appearance are disentangled in the model, they are restricted to a single-object scene without explicit control of the image synthesis. Niemeyer et al. [40] presented GIRAFFE to learn 3D representation of a compositional scene as synthetic neural feature fields, which employs MLP to represent each object in the scene and re- construct them afterwards, significantly increasing memory consumption and computational cost. Most recently, sev- eral SOTA generative models inspired by GRAF have been introduced. Its outstanding advantages lie in real-time performance and support for separate control of camera pose, facial identity, expression, and appearance. GRAM [51] is an innovative method designed to control point sampling and learning of radiance fields on 2D manifolds, represented as a collection of implicit surfaces within a 3D volume. Clip-NeRF [52] is a versatile framework that enables intuitive manipulation of NeRF through brief text prompts or exemplar images. It combines NeRF's capability for novel view synthesis with the controllable manipulation of latent representations in generative models."}, {"title": "3. Method", "content": "The neural radiance field (NERF) has achieved impressive results in a novel view synthesis using a set of posed images. Combined with the generative model, the generative radiance field (GRAF) has been successfully employed in 3D-aware image synthesis from latent code. The generated images preserve multiview consistency due to the benefits of the neural radiance field. Moreover, the GRAF prototype can be trained using unposed images and provides explicit control over the camera pose. The shapes and appearances in GRAF were disentangled using the shape code za and the appearance code z5. However, shapes and appearances are subject to a certain level of unpredictability because of the randomness of latent codes. Our approach employs a single MLP to learn multiple scenes and achieves precise control over the synthesis of 3D images based on labels. To support the rationale behind our model design, we initially present the fundamentals of NERF and GRAF.\nNeural Radiance Fields (NERF): The radiance field is a continuous representation of a scene, denoted by the function Fo, which takes the 3D location x, the viewing direction d as input and the color c along with the volume density values o as output. The mapping function Fo:(x, d) \u2192(c, \u03c3) was implemented using a fully connected network that optimizes the weights to map each of the 5D coordinate inputs to their appropriate density and color. Due to the bias of deep networks towards lower frequency functions, the function Fe, when applied directly to the 5D coordinate input, proved inadequate to capture high frequency varia- tions. Hence, positional encoding y() is used to translate a 3D location and viewing direction into a high-dimensional space, thus facilitating Fe to approach a high-frequency function with greater ease, formally defined in Equation 1.\n$\\gamma(p) =[sin(2^0\\pi p), cos(2^0\\pi p), (sin(2^1\\pi p), cos(2^1 \\pi p), ..., (sin(2^{L-1}\\pi p), cos(2^{L-1}\\pi p)]$\n(1)\nThe function y() is applied independently to the three coor- dinate values (x,y,z) of the position x and two components of the unit vector of the viewing direction d. The MLP network assigns the resulting characteristics to the color value c\u2208 R\u00b3 and the volume density \u03c3\u2208 R\u207a, as shown in Equation 2. Here, Lx=10 and L\u2081=4.\n$\\gamma(x), \\gamma(d) \\rightarrow (c, \\sigma)$\n$R^{L_x} XR^{L_d} \\rightarrow R^3 X R^+$\n(2)\nThe neural radiance field is a representation of a scene as the volume density and emitted radiance at every point in space. The volume density, denoted by 6, can be thought of as the probability differential of a ray terminating at an infinitesimal particle at a specific location x. The expected color C(r) of the camera ray: r(t) = o + td is defined in Equation 3, with near and far bounds t\u2081 and tf.\n$C(r) = \\int_{t_n}^{t_f} T(t)\\sigma(r(t))c(r(t), d))dt$\n$where T(t) = exp(-\\int_{t_n}^t \\sigma(r(s))ds)$\n(3)\nwhere the function T(t) denotes the accumulated transmit- tance along the ray from t\u2081 to t and the probability that the ray travels from t\u2081 to t without hitting any other particles. Rendering a 2D image from a neural radiance field requires estimating the integral C(r) for each camera ray r traced through the pixels of a virtual camera.\nThe integral of C(r) is typically estimated using a deterministic quadrature, which inherently restricts the res- olution of rendered images because the MLP is merely interrogated at a discrete points. A stratified sampling ap- proach was implemented to divide the data into uniformly spaced intervals, from which a single representative sample was randomly selected from each interval. This method computes the integral value by aggregating data points from a discrete collection of samples. Moreover, a hierarchical volume sampling technique was used to enhance rendering efficiency.\nGenerative Radiance Fields (GRAF): Generative Ra- diance Field (GRAF) is a generative model comprising a generator based on the radiance field and a multi-scale patch discriminator. This model enables the synthesis of 3D- aware images from random noise, and is trained on unposed datasets.\n(1) Generator: The inputs of the generator is made up of the intrinsic camera parameter K, the camera pose \u00a7, the sampling pattern v, the shape code z_s, and the appearance code za. The generator generates predicted image patches, denoted as P', as its output. The pose of the camera, denoted \u00a7, is randomly selected from the pose distribution, denoted by p\u025b. The center (u, s) and scale of the virtual patches are determined using a uniform distribution. Furthermore, the shape and appearance codes, denoted by z, and za, are drawn from the shape and appearance distributions, denoted by pa and ps.\nRay Sampling:The real patch P(u, s) is determined by utilizing the 2D image coordinates that specify the position of each pixel in the image domain. The corresponding rays are determined by these coordinates, the intrinsic camera parameter K, and the camera pose \u00a7.\n3D Point Sampling: The sampling method involves sam- pling N points {x}\u2081 along each ray r for the numerical in- tegration of the expected color C(r). Instead of using a single network to represent the scene, stratified sampling optimizes two networks: one 'coarse' and one 'fine' simultaneously. This procedure allocates more samples to the visible region to increase the quality of the images.\nConditional Radiance Field: The conditional radiance field is implemented by a fully connected neural network with parameter 0. More than a regular radiance field, it is subject to the inputs of shape code z5 and appearance za. The encoding for shape h is obtained by concatenating the posi- tional encoding y(x) and shape code z, and is subsequently converted to the volume density o through a density head \u03c3\u03b8. Nevertheless, in order to separate the shape and appearance, the volume density was independently predicted without employing the view direction d and appearance code za during the inference process. To estimate the predicted color c, a concatenating vector comprising the shape encoding h, positional encoding of the direction y(d), and appearance code za is fed into the color head co for further inference.\nVolume Rendering: The acquisition of the color c and volume density \u03c3 of N points along the ray (c,o) was achieved using volume rendering. The synthesized patch P' was obtained by combining the result of every sampling ray, and the value of the color c, was calculated using equation 4.\n$C = \\sum_{i=1}^{N} T_i \\alpha'_i c'_i \\alpha'_i = \\Pi_{j=1}^{i-1}(1 - \\alpha_j)$\n$\\alpha_i = 1 \u2013 exp(-\\sigma_i \\delta_i)$\n(4)\nThe transmittance (T) and alpha value (a) of sample point i along the ray r are denoted by T and a, respectively, and the distance between neighboring sample points is defined by 8 = ||x+1 - x1||2\n(2) Discriminator: The development of a discriminator involves the construction of a deep convolutional neural network (CNN) with ReLU as an activation function. The discriminator accelerates both training and inference by comparing the synthesized patch P' with the real patch P, which is obtained by accessing a real image at 2D coordinates P(u, s) through bilinear interpolation, referred to as \u0393(I, v). The discriminator was adequate for all patches randomly sampled on various scales. The size of the patch determines its receptive field, where larger receptive fields are utilized to capture global content, and smaller receptive fields are used to progressively discern local details.\n(3) Training and inference: In adversarial training, the generator G(0) seeks to minimize the function V(0, $), while the discriminator D($) seeks to maximize it. The non- saturating objective function V (0, $) with R1 regularization is defined in Equation 5.\nV(0, $) = Ezs~Ps,za~Pa,5~P\u025b,v~py [f(D\u00a2(Go(Z5, Za, 5, v)))]\n+ EI~pp,v~p, [f(\u2212D\u2084(\u0393(I, v))) \u2013 \u03bb||VD\u2084(\u0393(I, v))||2]\n(5)\nWhere f(t) = -log(1 + exp(-t)), I signifies an image sampled from the data distribution pp, and p\u2081 refers to the distribution over random patches. Furthermore, the parameter a controls the level of regularization. The dis- criminator utilizes both spectral normalization and instance normalization.\nCtrlNeRF: Actually, GRAF can generate a 3D geometry from latent code; however, its shape and appearance are not easily manipulated. To address this issue, we developed a GRAF-derived model (i.e. CtrlNeRF). This model allows us to use a single MLP to learn multiple 3D representations and to explicitly control object formation. We modified the output of the MLP to disentangle the shape and appearance and added an extra discriminator to distinguish between the object category and style.\n(1) Generator: Based on the GRAF generator, we manipulated the input of the MLP by embedding label codes in shape and appearance codes, allowing the generator to utilize the label-embedded codes to generate a 3D geometry with precise controls of shape and color.label embedding technique is illustrated in Fig.4.\nConditional Radiance Field: The inference for the volume density and color resembles that of the GRAF prototype. However, the MLP in the generator is conditional on the inputs of the label-embedding latent code, and the outputs are the density and color arrays associated with class and style. The structure of the proposed conditional radiance field is depicted in Fig.3. In this design, y(x) and y(d) refer to the positional encoding for the coordinates x in the 3D space and the directions d of the rays associated with each point, respectively. z and z are the label-embedded latent codes.\n(2) Discriminator: In addition to using the typical discriminator in GRAF to evaluate the generated patch P compared to the real patch P, we employed a discriminator based on VGG16 [?] to effectively classify various classes and styles of objects. The VGG network is well known for its exceptional performance in multi-classification tasks. Initially, the discriminator Dugg was trained using annotated images I' that were down-scaled from the real image I. The pre-trained discriminator was utilized as an auxiliary classifier for the patches generated P'. To further improve image quality, we adopted posed images for training and replaced adversarial loss with reconstruction loss.\n(3) Training and Inference: During supervised learn- ing, the network parameters are optimized using loss func- tions. In particular, the discriminator was trained by a real patch P and a generated patch P' to improve computational efficiency. The discriminator Dugg was trained in real images resized I'. The loss function to update the weights of the network is defined in Equation 6, and the pseudocode for training is shown in Algorithm 1.\nL(G(0)) =Ladv (D($)|Pi,j) + A1Lcls(Dugg|Pi,j)\n+ A2Lsty(Dugg|Pij)\n(6)\nwhere, Lady refers to adversarial loss between P and P', Lels and Lsty refer to the loss of class and style, respectively,"}, {"title": "4. Experiments", "content": "4.1. Datasets\nIn this study, due to the lack of annotations in the datasets used for generative models such as GRAF and GIRAFFE, we began by developing a synthetic dataset named CARS (I) utilizing 3D editing software. First, a car was situated at the origin of the coordinate system, and a virtual camera was placed on the surface of the upper hemisphere oriented towards the origin. where 0 and $ represent the pitch and yaw angles of the camera, respectively. The camera was placed in a hemisphere with a radius of r. By manipulating the pose of the virtual camera, we could obtain the views of an object with variable respect. The captured images with a size of 800x800 were automatically labeled by class, color, and pose. Four types of cars (classic, roadster, sporty, and wagon) were included in the CARs dataset, each of which was presented in four color modes: red, green, blue, and yellow. In addition, we used publicly accessible NERF datasets [3], specifically Synthetic (II) and LLFF (III), for demonstration.\n4.2. Baselines\nTo demonstrate its excellence in 3D-aware image gener- ation, we compared our model with the latest NeRF-based generative models, including CLIP-NeRF. NeRF can learn the continuous representation of 3D geometry from posed images using neural radiance fields and render a novel view using differentiable volumetric rendering. GRAF [2] is a generative model capable of producing images with 3D con- sistency using latent codes related to shape and appearance, without requiring 3D supervision. GRIFFEE [40] is another generative model derived from the GRAF model, which uses multiple MLPs to represent compositional scenes. CLIP- NeRF is the SOTA multimodal 3D object manipulation method for neural radiance fields using a short text prompt. For these generative models, qualitative and quantitative analyzes were performed to evaluate the performance of the proposed model by comparing it with other generative models.\n4.3. Evaluation Metrics\nThe FID score, which comprises human assessments of realism and diversity, has been widely used to evaluate the quality and variety of the generated images. This metric was first introduced by Kanazawa et al. in 2018 [?]. The lower the FID score, the better the model performance. FID score was derived from Equation 7.\nFID = d\u00b2((m,, C,), (mg, Cg))\n= ||m, \u2013 mg||2 + Tr(C, + Cg - 2(C,Cg)1/2)\n(7)\nThe pair (m,, C,) corresponds to real images, while the pair (mg, Cg) corresponds to generated images. In each pair, m represents the mean and C represents the covariance. The KID score [41], which is an unbiased estimate that does not require a normal distribution hypothesis, was introduced for image evaluation.\nIn addition, to measure the reconstructed shapes and their closest shapes in the ground truth, the peak signal-to- noise ratio (PSNR) and the structural similarity (SSIM) [49] were used for a quantitative comparison between real and synthetic images."}, {"title": "5. Results", "content": "5.1. Controllable 3D-aware Image Synthesis based on the Labels.\nThe model was trained on the CARs(I), Synthetic (II), and LLFF(III) datasets. Label-associated latent codes z and z are the input of the MLP in the generator, and the view direction d(0,4) is sampled within a specific pose range. In the experiment, we sampled 1024 rays for an image and 64 points along each ray. Therefore, 1024 \u00d7 64 points were sampled, each with 3D coordinates and ray directions. The RMSprop optimizer was used, with a learning rate of 0.0001 for the discriminator and 0.0005 for the generator, and class and color labels were used for the prediction. The synthesized images are shown in Fig.5, 6, 7 for the three datasets. Optimization for multiple scenes typically requires approximately 100\u2013200k iterations to converge on a single NVIDIA A4500 GPU(approximately 14 h).\nThe FID score reflects the similarity and variance be- tween real and synthesized images, which is an essential metric to quantitatively evaluate the performance of the generative model. The FID scores of the images generated in dataset I, grouped by class and color, are shown in Fig.8. When a single MLP handles multiple scenes within the model, it is evident that the image quality decreases as the number of scenes increases. The mean FID scores of the model trained on Datasets (I), (II), and (III) are presented in Fig.9. As shown in Fig. 10. Image quality decreased with an increase in the number of classes and styles.\nFurthermore, the model performance on the \"LLFF\" dataset was noticeably poorer compared to the \"Car\" dataset, likely due to the higher complexity of the images in the \"LLFF\" dataset, particularly when dealing with multiple scenes. Higher complexity implies greater entanglement when using a shared-weight MLP to represent multiple scenes.\n5.2. Novel View Generation via Camera Manipulation.\nAs shown in the following three figures, novel views of an object can be obtained by alternating the poses of the rendering camera. For example, in Fig.11, the virtual camera captures images of the object with the poses of \u03b8\u2208 [-180\u00b0, 180\u00b0] and \u00a2 \u2208 [0\u00b0,90\u00b0]. In Fig. 12, we changed the radius of the rendering sphere stepwise, ranging from 3.5 to 5.0, with an interval of 0.5. Finally, we performed horizontal translation of the synthesized objects within the range of (- 1.0, 1.0) in Fig.13.\n5.3. New Feature Synthesis via Linear Interpolation.\nAs shown in Fig.14, the new color of the car, which is unseen in the training set, is synthesized using the color interpolation: c=(1.0-\u03bb)c[i]+\u03bbc[j]. where is a linear co- efficient ranging from 0 to 1. In the same way, we can also simulate other features, such as texture, material, and environmental illumination. As shown in Fig.15, the shape of the car can also be altered step by step through density interpolation.\n5.4. Ablation Studies\nThe development of the proposed model entailed adap- tation of the GRAF prototype by altering the input and output components of the MLP and integrating an additional discriminator. Consequently, in our ablation studies, we compared the results of our model with those of Models I, II, and III, which eliminated the specific modifications for the input, output, and VGG discriminators. In Table 1, we present a quantitative comparison of the FID and KID scores of Models I, II, and III with those of our model, indicating that the manipulation of the MLP output in the GRAF prototype plays a significant role in our model because train- ing does not converge without it. Using density and color arrays, we effectively deployed the output to multiple slots corresponding to classes and styles. Then these outputs were used to render the images independently. We also observed that the image quality degraded without embedding labels for the input and the VGG discriminator. The two strategies not only increased the quality of the generated images, but also shortened the training time.\n5.5. Comparison to SOTA Methods\nGenerative radiance fields combine GAN and NERF techniques to synthesize 3D-aware images. In both qualita- tive comparisons with state-of-the-art generative methods, our approach yields re- sults on par with the CLIP-NERF method and exceeds the GRAF and GIRAFFE methods in terms of PSNR and SSIM. During the experiment, we noticed that the generative models facilitate creating new views with 3D consistency using the unposed dataset, but our method can store multiple scenes in a single MLP without significantly sacrificing image quality and also allowing for manipulation of 3D- aware image creation based on the given labels and camera pose. Although GRAF and GIRAFFE are both generative models, GRAF is unable to represent multiple scenes within a single MLP. However, GIRAFFE employs MLPs to repre- sent each object in a composite scene, leading to substantial memory consumption in multiobject scenes. As anticipated, our model falls short of CLIP-NERF with respect to PSNR and SSIM due to the use of a single MLP for implicit repre- sentation of multiple scenes simultaneously. Furthermore, to highlight the advantages of our model over CLIP-NERF, we performed a quantitative analysis in Tab. 3, concentrating on storage requirements and computational costs. CLIP- NERF allows for the manipulation of the shape and color of objects according to text or image prompts, but it cannot learn multiple scene representations in a single model and requires separate training for each scene. As the number of scenes to be represented grows, both the model storage and the training time expand proportionally. In contrast, the demands of our model remain unchanged."}, {"title": "6. Conclusion", "content": "The study aimed to achieve a sophisticated level of con- trol in 3D-aware image synthesis. To achieve this goal, we improved the GRAF to allow for precise manipulation of 3D object creation in terms of pose, class, color style, and other attributes. By modifying the input and output of the MLP as well as the incorporation of an additional discriminator, we successfully entangled and disentangled the label codes into and out of the latent code, thereby enabling 3D-aware image generation from label prompts during the inference phase. Using our model, various scenes can be implicitly represented using a single MLP with shared weights, which significantly minimizes memory usage when handling mul- tiple scenes. Additionally, it has been demonstrated that while the image quality produced by our model surpasses that of the NeRF-based generative models, it is marginally less impressive than CLIP-NERF, which is attributed to the shared weights within the MLP. Another limitation of the model is that the image quality diminishes as the quantity and complexity of the scenes increase."}]}