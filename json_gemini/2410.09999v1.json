{"title": "Leveraging Customer Feedback for Multi-modal Insight Extraction", "authors": ["Sandeep Sricharan Mukku", "Abinesh Kanagarajan", "Pushpendu Ghosh", "Chetan Aggarwal"], "abstract": "Businesses can benefit from customer feedback in different modalities, such as text and images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs of text segments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image and text information in a latent space and decodes it to extract the relevant feedback segments using an image-text grounded text decoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectively mine actionable insights from multi-modal customer feedback, outperforming the existing baselines by 14 points in F1 score.", "sections": [{"title": "Introduction", "content": "Customer feedback is essential for businesses to design and improve their products and services, according to customer expectations. (Luo et al., 2022) observe that the multi-modal feedback is growing rapidly. However, most existing solutions (Mukku et al., 2023; Sircar et al., 2022; Liu et al., 2022) do not take into account the rich information that image and video feedback contain, which can enhance the actionability and improve the customer experience. To address this challenge, we propose a novel multi-modal architecture that extracts pairs of text segments and corresponding images that are relevant and actionable for a given product from customer feedback. These pairs can help businesses to increase the actionability, improve the product catalogue quality, enhance the customer experience and thereby reduce returns and replacements."}, {"title": "Related work", "content": "In recent years, multi-modal tasks, combining various data types such as images and text, have garnered significant interest in artificial intelligence and natural language processing (Goyal et al., 2017; Zhou et al., 2020; Lu et al., 2023). Among these tasks, Visual Question Answering (VQA) stands out as a prominent domain (Antol et al., 2015), wherein the aim is to generate textual answers to questions based on images. VQA has evolved significantly over the years, thanks to various advancing contributions. Early works on VQA (Antol et al., 2015) laid the groundwork, delineating the fundamental framework and challenges of the task. Subsequent research delved into innovative methodologies, such as leveraging transformer networks (Zhou et al., 2020; Lu et al., 2023), and devising techniques for seamlessly integrating visual and textual information (Li et al., 2019; Lu et al., 2019; Kim et al., 2021). The field progressed with more advanced models and datasets, such as VQAv2 (Goyal et al., 2017), which improved the robustness and performance benchmarking of VQA models. Moreover, the strides in pre-training on both vision and language have significantly influenced the landscape, as evidenced by groundbreaking models like OSCAR (Li et al., 2020), BEiT-3 (Wang et al., 2022), and VLMo (Bao et al., 2021, 2022), which have achieved remarkable results. Additionally, techniques such as counterfactual data augmentation (Chen et al., 2020) have further enhanced the performance of VQA models.\nMeanwhile, in the NLP domain, verbatim extraction and text summarization tasks have attracted a lot of attention. Models like BERT (Devlin et al., 2018) and GPT-3 (Brown et al., 2020) have shown remarkable abilities in extracting and summarising textual feedback. Abstractive text summarization models, such as the T5 (Raffel et al., 2019), have emerged as the state-of-the-art (SOTA) solutions for generating short and coherent summaries from longer texts.\nOur work falls at the intersection of these two domains, where we propose a novel problem at the intersection of VQA and text summarization, where"}, {"title": "Problem Statement", "content": "Given a customer feedback consisting of a set of images $I = {I_1, I_2, ...I_n}$ and a text T, we extract m verbatims from T, denoted by $V = {v_i|1 \\le i \\le m, v_i \\in T}$. The aim is to extract all the relevant and actionable verbatims ${v_k \\in V}$ that corresponds to the given image $I_k$."}, {"title": "Proposed Approach", "content": "In this section, we first present our data generation method that creates training data from raw feedback. Then, we introduce our model architecture that extracts pairs of verbatims and images that are relevant and actionable for a given product."}, {"title": "Weakly supervised Training Data Generation", "content": "We segment raw feedback text to extract actionable verbatim\u00b9 (refer appendix section A.1 for exact process). We then generate training data by obtaining m \u00d7 n possible verbatim-image pairs for each feedback text, where m is the number of verbatim and n is the number of images. Next, we compute cosine similarity scores for every verbatim-image pair using pre-trained CLIP (Radford et al., 2021). To evaluate the relevance of the feedback image-verbatim pairs predicted by base CLIP\u00b2, we had them manually annotated as positive or negative pairs with clearly defined annotation guidelines (refer section 4.1.1). We established a validation set, denoted as StratSet-1k, comprising 1,000 image-verbatim pairs, carefully stratified (Algorithm 1) across 27 distinct product categories sourced from"}, {"title": "Image Encoder", "content": "Image encoder is built using the visual transformer ViT-B/16 (Dosovitskiy et al., 2021), which consists of an image encoder and a transformer encoder. The image encoder splits the input image into patches of size 16 \u00d7 16 pixels and converts each patch into a vector of 768 dimension embedding. The transformer encoder (which has 12 layers and each layer performs different operations on the embeddings, such as attention, normalization, and feed-forward networks) takes the sequence of embeddings and outputs a new sequence of embeddings that contains more information about the image content and context. We formulate the transformer encoder as a function g that maps a sequence of embeddings E to another sequence of embeddings E':\n$E' = g(E) = [e'_{CLS}, e'_1, e'_2, ..., e'_N]$ (3)\nwhere $e'_{CLS}$ is the updated [CLS] token (added to represent the embeddings as image features), $e'_k$ is the updated embedding for the k-th patch ($e_k$), and N is the number of patches in the image."}, {"title": "Image Grounded Text Encoder", "content": "The text encoder is based on BERT base (Devlin et al., 2018), which has 12 transformer blocks with self-attention and feed-forward network (FFN) layers. To fuse the visual embedding from the image encoder, an extra cross-attention layer is inserted between the self-attention and FFN layers in each transformer block. This layer updates the text embeddings by attending to the image embeddings. The attention mechanism in the cross-attention layer computes attention scores for each token in the text sequence with respect to the image embeddings and determine how much importance each token in the text should place on the information in the image embeddings. A special token [ENC] is appended to the start of the input text (refer Section 5.2 for exact prompts used) to provide an identity for the encoder input. The text encoder outputs a multi-modal embedding of the image-text pair as follows:\n$z = h(T, E') = [M_{ENC}, M_1, M_2, ..., M_L]$ (4)\nwhere h is the image-grounded text encoder function, T is the text input, E' is the embedding from image encoder (g), $M_{ENC}$ is the multi-modal embedding for the [ENC] token, mi is the multi-modal"}, {"title": "Image-Text Grounded Text Decoder", "content": "The decoder follows the same architecture as the encoder, except that it uses causal self-attention instead of bidirectional self-attention. The multi-modal embedding is also fused as a cross-attention layer between FFN and attention layer. The decoder shares the parameters of FFN and cross-attention layers with the encoder, which improves training efficiency and enables multitask learning. The causal self-attention layer allows the decoder to generate text tokens (yt), conditioned on the previous tokens (y<t) and the multi-modal representation (z). For a given input token (xt), the output of the decoder at each time step t is:\n$Y_t = f(x_t,Y_{<t}, z)$ (5)"}, {"title": "Overall Architecture", "content": "We encode the review text and image pair into a multi-modal representation and decode it into a sequence of verbatims (feedback segments) that are relevant to the image. Figure 1 illustrates the overall architecture of MINE. The encoding process consists of two steps: we apply the image encoder to the review image to obtain an image embedding; then, we pass the review text and the image embedding to the image-grounded text encoder to produce a multi-modal embedding that fuses both modalities. The decoding process uses the image-text grounded text decoder, which takes the multi-modal embedding as input and generates verbatim tokens conditioned on it. During training, we provide the ground truth verbatim as input to the decoder and optimize it to predict the next token. We fine-tune our model, with an objective of extracting the insightful segment by optimizing cross entropy loss:\n$L(\\theta) = \\frac{1}{N T_n} \\sum_{n=1}^N \\sum_{t=1}^{T_n} log p(y_t | y_{<t}, x, z; \\theta)$ (6)\nwhere N is the number of review-image pairs in the dataset, Tn is the length of the verbatim sequence for the n-th pair, yt is the verbatim token at time step t for the n-th pair, $y_{<t}$ is the sequence of previous tokens, xn is the review verbatim, zn is the multi-modal embeddings from the image grounded text encoder, and $ \\theta$ are the model parameters. Dur-"}, {"title": "Experimental settings", "content": "We obtain raw reviews dataset provided by (Ni et al., 2019), which contains over 233 million reviews from 29 unique product categories. We used the sample subset (K-cores subset, released as part of the original dataset) which have 973k reviews with images of 27 product categories for our analysis."}, {"title": "Prompt Engineering", "content": "We use the similarity scores that are obtained from the training data generation, as the confidence scores for each verbatim (see appendix section A.3 for an example). We compare three different prompting approaches for extracting actionable verbatim from the review text and image pair, and are as follows:\n1. Comprehensive Segment Extraction with Confidence Scores (CSECS): We generate all possible verbatim from the text and their scores based on how well they match the image. The ground truth includes all verbatim and their scores for each pair.\n2. Matching Segment Extraction with Confidence Scores (MSECS): We generate only the verbatim that match image and their scores. The ground truth includes only matching verbatim and their scores for each pair.\n3. Matching Segment Extraction (MSE): We generate only the verbatim that match the image without any scores. The ground truth includes only the matching verbatim predicted by the fine-tuned CLIP model for each pair."}, {"title": "Approaches", "content": "As an additional multi-modal approaches, we tried ALBEF (Li et al., 2021) and VL-T5 (Cho et al., 2021) along with MINE. We use 80K verbatim-image pairs sampled from all product categories to train both approaches.\nALBEF: Given that the training data was generated using a weakly supervised methodology and"}, {"title": "Training Details", "content": "MINE was initialized using the pre-trained weights of BLIP base model and fine-tuned using AdamW optimizer (Loshchilov and Hutter, 2019) for 10 epochs with a cosine learning rate scheduler (Loshchilov and Hutter, 2017) and a minimum and initial learning rate of 1e-6 and 5e-5 respectively. In comparison, ALBEF was fine-tuned using 128 as batch size for 20 epochs with the following hyperparameters: AdamW optimizer, a learning rate of 1e-5, \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u20ac = 10-8 and"}, {"title": "Results", "content": "We found that MSE approach with beam search decoding produced the most complete and relevant verbatim. We compared different prompt settings and approaches, and measured the quality of the generated verbatim using precision (correctness), recall and completeness metrics. We gave the exact definitions of these metrics in appendix section A.2. Table 1 summarized the results of our experiments. We see that our approach is effective than existing baselines like ALBEF and VL-T5 in both identifying the exact intent and extracting the complete verbatim, present in the image. In addition to beam search, we experimented with different decoding methods for MINE and reported the results in Table 2."}, {"title": "Conclusion", "content": "In this paper, we introduced a novel architecture MINE, for extracting insights from multi-modal customer feedback encompassing both text and images. We proposed a weakly-supervised data generation approach that leveraged raw feedback text and images to create relevant verbatim and image"}, {"title": "Appendix", "content": "We use the following steps to extract verbatims (actionable feedback text segments) from raw customer feedback source:\n1. Text Pre-processing: We apply text pre-processing techniques to remove html tags, urls, accented characters, extra spaces and other noise from the feedback text.\n2. Segmentation: We use Trankit (Nguyen et al., 2021), a fast and lightweight transformer-based toolkit for natural language processing, to segment each feedback into meaningful units. We also implement some heuristics to avoid having segments with only single word.\n3. Removing non-actionable (neutral) segments: We use a RoBERTa (Liu et al., 2019) based 3-class sentiment classifier to evaluate the sentiment of each feedback segment. It has been rigorously evaluated and demonstrates an impressive 92% F1-score on our specific sentiment classification task. We make the decision to exclude segments classified as neutral, as these segments may not provide actionable feedback for brands and selling partners who seek feedback to improve their products and services."}, {"title": "Verbatim evaluation metrics", "content": "This metric measures the proportion of segments that convey a complete meaning. A segment is considered complete if it expresses a coherent and relevant idea about the feedback."}]}