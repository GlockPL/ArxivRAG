{"title": "Intelligent Video Recording Optimization using Activity Detection for Surveillance Systems", "authors": ["Youssef Elmir", "Hayet Touati", "Quassila Melizou"], "abstract": "Surveillance systems often struggle with managing vast amounts of footage, much of which is irrelevant, leading to inefficient storage and challenges in event retrieval. This paper addresses these issues by proposing an optimized video recording solution focused on activity detection. The proposed approach utilizes a hybrid method that combines motion detection via frame subtraction with object detection using YOLOv9. This strategy specifically targets the recording of scenes involving human or car activity, thereby reducing unnecessary footage and optimizing storage usage. The developed model demonstrates superior performance, achieving precision metrics of 0.855 for car detection and 0.884 for person detection, and reducing the storage requirements by two-thirds compared to traditional surveillance systems that rely solely on motion detection. This significant reduction in storage highlights the effectiveness of the proposed approach in enhancing surveillance system efficiency. Nonetheless, some limitations persist, particularly the occurrence of false positives and false negatives in adverse weather conditions, such as strong winds.", "sections": [{"title": "Introduction", "content": "The widespread installation of surveillance cameras has led to a significant increase in visual data, with estimates predicting over 1.4 billion cameras worldwide by 2024, generating vast amounts of video data daily. This surge poses major challenges in terms of storage, requiring hundreds of petabytes to manage this critical information. Efficient methods to manage and facilitate the search of this data have become imperative.\nSurveillance cameras, essential for security, create an information overload. A single HD camera can produce nearly 650 megabytes of data per minute, resulting in a substantial data management challenge when multiplied across thousands of cameras in urban areas. Much of this data is redundant or irrelevant, necessitating careful consideration of optimal storage methods to ensure the rapid retrieval of important information.\nThe key problem is optimizing the storage of surveillance camera data to prevent storage congestion while maintaining necessary recordings. Object and motion detection algorithms emerge as promising solutions, filtering sequences to record only significant activities. This approach addresses the challenge of information overload in video surveillance.\nThis study aims to optimize storage space through innovative methods using motion and object detection algorithms and to implement a solution capable of discriminating important scenes. This dual objective seeks to balance storage efficiency with the relevance of the recordings. The structure of the remain of this paper is as follows: Section 2 reviews related works in intelligent surveillance systems, comparing their strengths and weaknesses. In section 3, a proposed methodology of activity detection is introduced with a overall architecture of the proposed system. Implementation is presented in section 4, and experiment results and discussion in section 5. The paper concludes with a summary of findings and a discussion of future research directions."}, {"title": "Related Works", "content": "Previous studies have explored various methods for optimizing video recording. Background subtraction and frame differencing are commonly used for motion detection, while object detection methods like Faster R-CNN and YOLO have shown promising results in identifying specific objects in video streams.\n\u2022 Arham et al [1] developed a comprehensive real-time object detection system integrating motion detection, face detection, and human activity recognition, effective in real-world applications but lacking comparison with existing systems and detailed dataset descriptions.\n\u2022 Pal et al [2] proposed a composite block matching algorithm for efficient motion estimation in video sequences, enhancing accuracy and processing speed but introducing computational complexity that may limit real-time applicability."}, {"title": "", "content": "\u2022 Sadoun et al [3] addressed challenges such as illumination changes and shadow detection, creating a background modeling and subtraction algorithm. The model detects mobile objects but struggles with scene changes and fixed cameras, needing more quantitative measures.\n\u2022 Sreenu et al [4] reviewed deep learning techniques in intelligent video surveillance, highlighting advancements and challenges like computational complexity and the need for large datasets. They called for more robust models and multi-sensor data integration.\n\u2022 Xia et al [5] presented a hybrid LSTM-CNN model for human activity recognition, showing superior accuracy but facing high computational costs and the need for extensive labeled data.\n\u2022 Adarsh et al [6] proposed a model using YOLO and ResNet-34 for detecting suspicious behavior, achieving significant precision but requiring substantial computational resources.\n\u2022 Lys et al [7] developed a motion detection and object recognition system using OpenCV but lacked detailed results on detection efficiency.\n\u2022 Alajrami et al [8] proposed AI techniques to enhance human identification in surveillance, improving accuracy and speed but facing high computational demands and privacy concerns.\n\u2022 Ullah et al [9] combined CNN and LSTM for human activity recognition, improving accuracy but facing computational challenges and the need for broader dataset evaluation.\n\u2022 Boumediene et al [10] created a real-time object detection model using YOLO, achieving good accuracy but requiring a more diverse dataset for better generalization.\n\u2022 Suradkar et al [11] proposed an automatic motion detection system improving surveillance efficiency but needing evaluation in varied environments.\n\u2022 Dhulekar et al [12] developed a two-step system for motion and object detection, suggesting deeper learning techniques for more complex object detection.\n\u2022 Kapania et al [13] combined YOLOv3 and RetinaNet for robust object detection and tracking, demonstrating high performance but suggesting YOLOv3 for speed and efficiency.\n\u2022 Dave et al [14] proposed a real-time action detection system addressing class imbalance and multi-label actions, achieving state-of-the-art performance but needing more representative datasets."}, {"title": "", "content": "These studies collectively advance the field of video surveillance, addressing various challenges and proposing innovative solutions, though they often highlight the need for further research to overcome limitations in real-world applications as some of them are presented in Table 1.\nTo address limitations in human detection and surveillance, several research directions are proposed, including hybrid approaches that combine background subtraction with deep learning or optical flow with Support Vector Machines (SVM) for improved accuracy in dynamic environments. Enhancing generalization through diverse training datasets, synthetic data, data augmentation, and transfer learning can also improve model performance in real-world conditions. Developing versatile models for both indoor and outdoor settings using mixed datasets and context-aware mechanisms can enhance surveillance system adaptability.\nThis paper focuses on optimizing storage in surveillance systems by recording only significant actions using advanced activity detection techniques. Unlike traditional systems like Hikvision\u00b9, which use continuous or basic motion detection, our approach selectively captures important activities, thus reducing storage needs. We evaluate our method against Hikvision to demonstrate its effectiveness in reducing storage without sacrificing surveillance quality.\nThe study explores intelligent video recording optimization through various techniques, including motion detection, background subtraction, optical flow, and advanced object detection methods like YOLOv3 and Faster R-CNN. A hybrid approach that combines initial motion detection with precise object tracking is identified as optimal for performance and resource efficiency. Future research should refine these models, expand datasets, and utilize advanced training devices. While integrating these techniques for real-time surveillance remains challenging, our hybrid approach aims to achieve more efficient and effective surveillance."}, {"title": "Methods", "content": "The Hikvision Surveillance System (HikvisionSS) was selected as the baseline for comparison in this study due to its existing deployment within our institution, \u00c9cole sup\u00e9rieure en Sciences et Technologies de l'Informatique et du Num\u00e9rique (ESTIN2). The primary objective of this project is to optimize and reduce the storage space required for video recordings, which directly impacts the operational efficiency of the Hikvision system. As such, it was essential to evaluate the performance of our proposed approach in comparison to HikvisionSS, particularly in terms of the storage space required for recording, to ensure that our method provides tangible benefits within the context of its real-world application. We have chosen the Hikvision surveillance system as the baseline for comparison due to its widespread use in our institution and its standard recording modes, which include continuous recording and motion detection-based recording. Our proposed method focuses on an intelligent approach that records only significant actions, identified through advanced activity detection algorithms. This strategic selection allows us to directly assess the improvements in storage efficiency offered by our approach."}, {"title": "System Architecture", "content": "The proposed system integrates motion detection and object detection in a hybrid approach. As illustrated in Figure 1, the system begins with frame subtraction to identify regions of interest where motion is detected. These regions are then processed by the YOLO model, which detects and classifies objects, with a particular focus on human activity. This architecture ensures that only relevant scenes are recorded, significantly reducing unnecessary footage."}, {"title": "Activity Detection", "content": ""}, {"title": "Motion Detection:", "content": "Frame subtraction is employed to detect changes between consecutive frames, indicating potential activity. This method is computationally efficient and suitable for real-time applications [21]. To examine the different algorithms used for motion detection, we reviewed various works such as [3] and [16] utilizing background subtraction, [8] employing optical flow, and [2] proposing a composite block matching algorithm."}, {"title": "Object Detection:", "content": "The YOLO model is used to detect and classify objects within the video frames. YOLOv9 is chosen for its balance between speed and accuracy, making it ideal for real-time surveillance.\nAn example of using the YOLO method is found in [6], where the authors detect suspicious individuals and hostile behavior. They use the YOLO model, pre-trained on the COCO dataset, for human detection in video frames. These frames are then processed by ResNet-34 to recognize activities, achieving a precision of 82%. Despite its effectiveness, the model's reliance on two deep learning models demands substantial computational resources\nThe workflow for recording surveillance videos driven by object detection is detailed in Algorithm 1. The algorithm outlines how the system processes video streams, detects motion, and records activity only when objects of interest are present in the frame, ensuring efficient storage usage and effective monitoring."}, {"title": "Implementation", "content": ""}, {"title": "Data Preparation and Analysis", "content": "The data preparation involved collecting, labeling, and processing images for training the model. The dataset was sourced from various public repositories, focusing on human and car detection. Key data preparation steps included labeling objects using the Roboflow platform, preprocessing images (auto-orientation, resizing, contrast adjustment), and applying data augmentation techniques like grayscale conversion.\nThe dataset was split into training, validation, and testing sets using a 70:20:10 ratio. The dataset comprised 300 images for human detection from Kaggle and 100 images for car detection from a GitHub repository, supplemented by additional images sourced from Google Images and other websites. The preparation process involved auto-orienting the images, resizing them to 646x640 pixels, and applying contrast adjustment and grayscale conversion to a portion of the dataset. The final dataset included 2,664 images labeled as 'Person' and 1,735 as 'Car', ensuring a well-balanced distribution across object classes.\nThe analysis highlighted the efficiency of the data preparation process, ensuring that the training model was robust and well-balanced across different scenarios and object classes. Visual tools like heatmaps and histograms were used to further analyze the distribution of annotations and objects within the dataset."}, {"title": "Model Training", "content": ""}, {"title": "Model Pre-training", "content": "The YOLO algorithm was implemented by cloning its repository from GitHub. Pre-trained weights from the MS COCO dataset were used as a starting point for fine-tuning on the custom dataset. The pre-training process involved running a script with key parameters, including 8 CPU workers for faster data loading, GPU utilization, a batch size of 16, and 500 training epochs. The model was trained from scratch using specific configurations and hyperparameters, with mosaic augmentation disabled after 15 epochs to enhance focus on original images. This pre-trained model was then fine-tuned on the custom dataset."}, {"title": "Model Fine-Tuning", "content": "The fine-tuning of the YOLO model on the custom dataset was performed using a modified script, which involved resizing input images to 640x640 pixels, setting a batch size of 16, and training for 25 epochs. The data configuration file was pointed to the custom dataset, while the model configuration file specified the YOLO architecture. Pre-trained weights from the initial training on the MS COCO dataset were used as the starting point."}, {"title": "Evaluation of Training Results", "content": "After completing the YOLO model training and fine-tuning, a thorough evaluation was performed using both qualitative and quantitative metrics to assess the model's performance. The validation process utilized a separate dataset to ensure the model's ability to generalize to unseen data. Key parameters for validation included using the prepared dataset, the best model weights from training, a batch size of 16, images resized to 640x640 pixels, a confidence threshold of 0.001, an IoU threshold of 0.5, and a maximum of 300 detections per image."}, {"title": "Comparative Analysis of Trained Models", "content": "The comparative analysis of the trained models focuses on identifying the most suitable model for deployment by evaluating various performance metrics, such as precision, recall, and mean Average Precision (mAP), in addition to considering computational resources and deployment constraints. This analysis not only aids in selecting the optimal model but also informs iterative improvement strategies by highlighting areas for refinement. The 7th Model, with the highest precision (87.0%), recall (82.0%), and mAP (90.1%), was selected for the proposed system, as it offers the best balance between precision and recall, ensuring accurate object detection and classification in video feeds."}, {"title": "Model Deployment", "content": "The deployment process involves preparing the chosen YOLOv9 based model by converting it into a deployable format and configuring it for integration. Once deployed, the model is accessible via an API, allowing seamless integration into the application environment. This setup ensures the model performs accurate inference tasks and provides reliable object detection capabilities, making it ready for real-world application scenarios."}, {"title": "Model Evaluation and Training Visualization", "content": "The model's performance was evaluated using key metrics such as Precision (P), Recall (R), mean Average Precision at IoU threshold 0.5 (mAP50), and mean Average Precision across IoU thresholds from 0.5 to 0.95 (mAP50-95). Precision, which indicates the accuracy of positive predictions, was 0.869 overall (Car: 0.855, Person: 0.884). Recall, reflecting the model's ability to detect all relevant instances, was 0.824 overall (Car: 0.83, Person: 0.819). The mAP50 was 0.891 (Car: 0.899, Person: 0.883), showing high precision and recall at an IoU threshold of 0.5, while the mAP50-95 was 0.558 (Car: 0.638, Person: 0.478), demonstrating robustness across different IoU settings. These metrics, detailed in Table 3, highlight the model's strong performance in detecting cars and persons with high precision and recall, and reasonable generalization across IoU thresholds.\nThe confusion matrix, which evaluates the model's performance by comparing predicted labels to true labels, includes metrics such as True Positives (TP), False Positives (FP), False Negatives (FN), and True Negatives (TN). The model demonstrates efficient performance, with a pre-processing time of 0.5 milliseconds, an inference time of 25.1 milliseconds, and Non-Maximum Suppression (NMS) time of 5.2 milliseconds per image. The inference time is particularly significant, as it reflects the model's capability for real-time image processing, which is crucial for applications requiring rapid decisions."}, {"title": "", "content": "Figure 3 shows the confusion matrix for activity classification (car, person, and background) in surveillance footage. The diagonal elements represent correctly classified instances, while the off-diagonal elements correspond to mis-classifications. The matrix indicates an accuracy of 87% for detecting cars and persons, with slightly lower accuracy for the background class. Misclassifications mainly occurred between 'person' and 'background,' highlighting areas for potential model improvement.\nThe training progress is illustrated through plots showing the evolution of loss components and performance metrics over epochs. The bounding box regression loss (train/box_loss) decreased from 1.5 to 1.1, indicating improved accuracy in predicting bounding box coordinates. The classification loss (train/cls_loss) fell from 1.6 to about 0.6, reflecting enhanced classification performance. The Distribution Focal Loss (train/dfl_loss) also decreased from 1.5 to 1.25, demonstrating increased precision. Precision (metrics/precision) and recall (metrics/recall) metrics improved from 0.65 to approximately 0.85 and 0.82, respectively. Validation losses (val/box_loss, val/cls_loss, val/dfl_loss) followed similar trends, indicating good generalization. The mean Average Precision at an IoU threshold of 0.5 (metrics/mAP_0.5) increased from 0.45 to 0.90, and the mean Average Precision across IoU thresholds from 0.5 to 0.95 (metrics/mAP_0.5:0.95) rose from 0.40 to 0.75, highlighting the model's robust performance.\nFigure 4 displays training and validation losses, precision, recall, and mean Average Precision (mAP) metrics over 25 epochs. The training and validation losses decrease consistently, indicating effective learning. Precision and recall improve steadily, with recall nearing 0.90 by the end of training. The mAP metrics also show positive trends, with mAP_0.5 surpassing 0.8. These results suggest the model continues to improve, with further training potentially enhancing performance. The visualizations in Figure 4 provide insights into the model's learning progress, crucial for assessing overfitting or underfitting. Overall, the best YOLOv9 model, fine-tuned with the custom dataset, exhibits high accuracy and efficiency, making it suitable for deployment in the proposed intelligent video recording optimization solution."}, {"title": "Deployment and Results", "content": ""}, {"title": "Storage Optimization", "content": "This setup involves two different scenarios: one using the proposed intelligent recording approach and the other employing the Hikvision surveillance system's traditional continuous recording and motion detection features. The primary objective is to quantify the reduction in storage space achieved by our method, without sacrificing the accuracy and reliability of the surveillance footage. To validate the performance of the proposed application and the study objective, which focuses on the Optimization of Recording Storage for Surveillance Systems, a comparative study was conducted using Hikvision monitoring software4 at ESTIN. Both systems were tested simultaneously under identical conditions to measure their effectiveness in optimizing storage by comparing the length of recorded videos. All recordings were made using a unified format (MP4, codec H.264, 1280x720, 16:9, 30 fps, bitrate 7 703 kbps)."}, {"title": "Equipment and Configuration", "content": "Two types of cameras were used: a fixed camera and a flexible dome camera, positioned strategically in front of ESTIN's main gate and the building of Labs' building. The Hikvision system was configured with its specific hardware and software setup, while the proposed system was deployed on a local machine, specifically an i7 Core ThinkPad. This setup created a controlled environment to accurately measure and compare the performance of both systems in terms of storage optimization.\nFor the experimental evaluation, two distinct video records were used. The first video was a real-time test conducted live with the first camera This period of time in a workday, particularly from 2 PM to 3 PM, is likely one of the most active for students, teachers, and workers at ESTIN. The first video captures the scene in front of the main gate of ESTIN during this busy period, from 2 PM to 3 PM. The second video was recorded offline, after the real-time scene was captured. This second recording was done using a flexible camera positioned in front of the Labs' building from 2:30 PM to 2:45 PM. This dual approach allowed us to test the proposed method under both live and post-recording conditions, providing a comprehensive evaluation of its performance."}, {"title": "Real-Time Test", "content": "\u2022 Objective: Compare the storage optimization between the proposed system and the Hikvision software over a one-hour period of live surveillance with motion detection enabled.\n\u2022 Methodology: Both systems were launched simultaneously and ran for one hour with motion detection enabled on both."}, {"title": "Video Upload Test", "content": "\u2022 Objective: Compare the performance of the proposed system and the Hikvision software using a pre-recorded video.\n\u2022 Methodology: A 15-minute video was processed by both systems with motion detection enabled.\n\u2022 Metrics Collected: Length of the resulting videos after processing.\nDue to practical constraints, the number of video records used in this study was limited. However, the selected videos were chosen for their representativeness in assessing the effectiveness of the proposed system in optimizing storage space compared to the HikvisionSS baseline. Despite these limitations, the focus of our analysis remains on demonstrating the efficiency and applicability of our approach within the context of real-world surveillance scenarios."}, {"title": "Analysis of Storage Efficiency", "content": "The performance of the two systems was evaluated based on the following criteria:\n\u2022 Recorded Video Length: The total duration of the videos recorded by each system under the same conditions. A shorter recorded length indicates better storage optimization.\n\u2022 Detection Accuracy: The ability to correctly identify and record relevant activities (human and vehicular) without missing any significant events.\nTo evaluate the efficiency of the proposed method, a comparative analysis was conducted using two test types: a 1-hour real-time test and a 15-minute video test. The results in Table 5 compare the recorded lengths and storage sizes between the traditional Hikvision surveillance system and the proposed activity detection system. In the 1-hour real-time test, the Hikvision system recorded 60 minutes, consuming 1 917 MB, while the proposed system recorded only 13 minutes and 45 seconds, using 769 MB. This discrepancy is largely due to Hikvision's motion detection being highly sensitive to minimal motion, such as the movement of trees and palms caused by the wind, which triggered recording frequently. In contrast, the proposed system only initiated recording after detecting significant motion, using object detection to ensure that the recorded scenes contained relevant objects like cars or persons. In the second 15-minute video test, there was no wind, and both systems recorded only significant motions when detected. Therefore, the Hikvision system recorded 6 minutes and 53 seconds, consuming 379 MB, whereas the proposed system recorded 6 minutes and 20 seconds, using 355 MB.\nThese results demonstrate significant storage savings by recording only scenes with detected activity, reducing storage requirements without compromising important data. The storage savings chart underscores the efficiency of the proposed method compared to traditional continuous recording.\nHowever, the proposed system has some limitations, such as false positives and false negatives in adverse weather conditions like strong winds, which can cause motion artifacts that affect the accuracy of detection as it is illustrated in Figure 3 and in some amples of Figure 6.\nThe validation results demonstrate that the proposed system significantly outperforms the Hikvision software in optimizing storage. In the one-hour real-time test, the proposed system recorded only 13 minutes and 45 seconds of video, compared to Hikvision's 60 minutes, even with Hikvision's intelligent motion detection option enabled. This highlights the proposed system's superior ability to filter out irrelevant footage and focus on significant activities.\nIn a 15-minute video upload test, the proposed system recorded 6 minutes and 20 seconds, while Hikvision recorded 6 minutes and 53 seconds. Although Hikvision captured the necessary scenes, it encountered some bugs. The proposed system proved more efficient by accurately detecting and recording pertinent activities, thereby minimizing storage usage.\nThese results confirm the effectiveness of the proposed model in optimizing recording storage for surveillance systems. Compared to Hikvision, which relies on continuous or basic motion-based recording, our method significantly reduces the storage space required by recording only meaningful activities. This optimization does not compromise the ability to capture critical events, as evidenced by the comparative analysis, making our approach a valuable tool for enhancing the efficiency and cost-effectiveness of surveillance operations."}, {"title": "Conclusion", "content": "In conclusion, this work effectively demonstrates that intelligent activity detection can optimize storage space in surveillance systems. By selectively recording only meaningful actions, our approach significantly reduces storage requirements compared to traditional systems like Hikvision, which rely on continuous or motion-based recording.\nThis study developed a hybrid system that combines motion detection via successive frame subtraction with the YOLOv9 model for detecting significant activities. YOLOv9 was selected for its superior detection capabilities and efficiency, achieving an accuracy of 87%. The combination of motion detection for initial screening with advanced activity detection optimizes both performance and resource usage, particularly benefiting the recording system at ESTIN by addressing storage and resource management challenges. Validation results showed substantial optimization in both time and storage space.\nFuture work should focus on refining model structures to further enhance detection precision and efficiency, leveraging more powerful training devices and larger, high-quality datasets. While our project made significant strides, further advancements are possible with additional resources and improved data. The methodologies developed hold potential applications beyond security, benefiting any field requiring real-time object detection and intelligent video recording.\nAlthough the scope of our experiments was limited due to practical constraints, the results highlight the potential of the proposed method for optimizing video recording in surveillance systems. Future studies should consider expanding the dataset and exploring additional comparative methods to further validate the system's performance."}]}