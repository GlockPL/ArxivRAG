{"title": "Palu: Compressing KV-Cache with Low-Rank Projection", "authors": ["Chi-Chih Chang", "Wei-Cheng Lin", "Chien-Yu Lin", "Chong-Yan Chen", "Yu-Fang Hu", "Pei-Shuo Wang", "Ning-Chi Huang", "Luis Ceze", "Kai-Chiang Wu"], "abstract": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens or quantize it into lower bits. However, these methods cannot exploit the redundancy of the hidden dimension of KV tensors. This paper investigates a unique hidden dimension approach called Palu, a novel KV-Cache compression framework that utilizes low-rank projection. Palu decomposes the linear layers into low-rank matrices, caches the smaller intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU kernels. Our extensive experiments with popular LLMs show that Palu can compress KV-Cache by more than 91.25% while maintaining a significantly better accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache quantization methods at a similar or even higher memory usage. When compressing KV-Cache for 50%, Palu delivers up to 1.61\u00d7 end-to-end speedup for the attention module. Our code is publicly available at: https://github.com/shadowpa0327/Palu.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are revolutionizing the AI industry and providing a high-level intelligence that previous machine learning (ML) models could not achieve. To speed inference, caching key-value states (KV-Cache) in memory is a simple yet effective technique. However, the size of the KV-Cache can grow rapidly, straining memory capacity and bandwidth; further, the memory-bounded characteristics of the decoding stage also limit inference speed when loading KV-Cache data.\nTherefore, KV-Cache compression has become a central research topic for building efficient LLMs. Research on KV-Cache compression falls into two categories: quantization and token eviction. Quantization methods aim to use less bit-width to represent each datum; token eviction techniques seek ways to retain only a partial set of KV-Cache. However, both categories fail to explore the hidden dimensions of KV-Cache where high redundancy often occurs.\nThis work examines the problem from a fundamentally different perspective, i.e., ways to reduce the hidden dimensions. We introduce Palu, a KV-Cache compression framework that reduces the size of the KV-Cache through low-rank projection. Although low-rank projection has been shown to improve LLM efficiency, e.g., by reducing model size (Yuan et al., 2023; Wang et al., 2024) or minimizing memory footprint for finetuning (Hu et al., 2022; Dettmers et al., 2023), it has not been studied for compressing the KV-Cache, especially in post-training.\nA naive way to compress the KV-Cache with low-rank projection is by directly mapping cached matrices into low-rank space (Jolliffe and Cadima,"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Multi-Head Attention Mechanism", "content": "The multi-head attention (MHA) mechanism (Vaswani et al., 2017) is a core component of the transformer architecture. Given a new input token x \u2208 Rd, an MHA with n heads projects the input into multiple queries, keys, and values using weight matrices Wq, Wk, and Wv, respectively, for each head i, as shown by\nqi = xWqi, ki = xWki, vi = xWvi.  (1)\nHere, ki and vi represent the key and value at time step t for head i. We can then compute the attention score for each head i and the corresponding attention output as\npt,i = Softmax(qKTi\u221adh), ai = pt,iVi,  (2)\nwhere K\u2081 and Vi denote the concatenation of current and all previous keys and values corresponding to the i-th head. The final MHA output is obtained by concatenating the outputs of all heads and then applying the out-projection layer Wo, as shown by\nMHA(x) = \u2211hi=1 aiWo = \u2211hi=1(pt,iVi)Wo. (3)\nwhere Wo \u2208 Rdh\u00d7d represents the submatrices of the out-projection matrix for each head i."}, {"title": "2.2 Singular Value Decomposition (SVD)", "content": "SVD (Jolliffe and Cadima, 2016) is a commonly used technique for computing the low-rank approximation for a given matrix. We now introduce our specific use of SVD for Palu's default low-rank decomposition method.\nGiven a weight matrix W \u2208 Rm\u00d7n, SVD decomposes W into three matrices: W = U\u2211VT. Here, U and V are orthogonal matrices containing the left and right singular vectors, respectively. The matrix \u2211 is a diagonal matrix that consists of singular values. We describe the decomposition as\nW\u2248 AB, A = Ur\u22111/2, B = \u22111/2VT,\nwhere A \u2208 Rm\u00d7r, B \u2208 Rr\u00d7n, \u2211r \u2208 Rr\u00d7r. \u2211r is a diagonal matrix containing the largest r singular values, and U,, VT are corresponding singular vectors truncated from U and VT. This truncation and subsequent matrix formation let us approximate weight matrix W with two low-rank matrices A and B, thereby reducing the storage by mr+rn."}, {"title": "3 The Palu Framework", "content": ""}, {"title": "3.1 Compressing the KV-Cache via Low-Rank Projection", "content": "As noted, a naive approach to compress the KV-Cache with low-rank projection is to apply SVD directly on the KV-Cache and store the top-r singular vectors. However, this approach poses significant computational challenges during runtime that make it impractical for deployments (see Section 1).\nTo apply low-rank projection more efficiently, Palu uses SVD to decompose the key and value projection weight matrices. This approach is based on the observation that low-rank decomposition rewrites the linear projection layer from y = xW into y = xAB. Here, A \u2208 Rd\u00d7r is the low-rank projection matrix, and B \u2208 Rr\u00d7d is the reconstruction matrix derived by SVD. The forward process first down-projects the input token x \u2208 Rd into a low-dimensional latent space h \u2208 Rr and then up-projects it back to the original space:\nh = Ax, y = Bh (4)\nThis two-step process lets Palu (1) store the lower dimension latent representation instead of the origin keys and values states, and (2) reconstruct them during decoding."}, {"title": "3.1.1 Integration with the Attention Mechanism and Offline Matrix Fusion", "content": "We now describe how Palu decomposes the key and value linear layers for the attention mechanism. For each attention head i, Palu applies SVD and maps the key-projection matrix Wq and value-projection matrix Wv into AB and AB.\nBased on the formula of attention output in equation 2, Palu absorbs the reconstruction matrix B into the output projection matrix Wo offline:\naiWo = (pt,iVi)Wo\n= (pt,iHBi)Wo\n= pt,iH(BiWo) (5)\nSuch fusion lets Palu skip the explicit reconstruction of the full value vectors, reduce the number of matrix multiplications, and improve efficiency. A similar approach applies for calculating attention scores. Matrix B can be fused into the query projection matrix Wq offline, as shown by\nqiKTi = qi(HBi)T\n= xT(Wq(Bi)T)(HBi)T\n= xT(Wq(Bi)T)Hi. (6)\nHere, Bi \u2208 Rr\u00d7dh and Wq \u2208 Rd\u00d7dh, so the fused matrix (Wq(Bi)T) has size Rd\u00d7r. This fusion boosts computational efficiency by reducing the matrix dimension during attention score calculation."}, {"title": "3.1.2 Mechanism with Positional Embedding", "content": "Recent LLMs, such as the Llama family, apply positional embedding (i.e., RoPE (Su et al., 2021)) onto the query and key states before multiplication. The non-linear operations of adding positional embedding prevent the fusion for the attention score, as described in equation 6. In this case, Palu reconstructs the keys from latent representations on-the-fly during decoding. We improve reconstruction efficiency by designing an optimized kernel, which we present in Sec. 3.5.\nNote that for some positional embedding methods, such as ALiBi (Press et al., 2022), or new attention mechanisms, such as MLA (DeepSeek-AI et al., 2024), positional embedding is not directly applied on the key states; therefore, the fusion of equation 6 remains valid. We expect the end-to-end speedup to be larger on these models. Fig. 2 presents a detailed workflow of Palu based on Llama's attention with RoPE."}, {"title": "3.2 Decomposition Granularity", "content": ""}, {"title": "3.2.1 Multi-Head Low-Rank Decomposition", "content": "We name the per-head decomposition scheme in Sec.3.1.1 as multi-head low-rank decomposition (M-LRD). We found M-LRD often causes a non-negligible accuracy degradation (discussed further in Sec.4.1), possibly because SVD fails to capture the common information shared across heads. Therefore, alternative approaches are needed for preserving model accuracy."}, {"title": "3.2.2 Joint-Head Low-Rank Decomposition", "content": "An alternative approach is to jointly decompose weight matrices for all heads. By considering the combined weight matrix Wjoint = [W1,W2,...,Wn] \u2208 [Rdx(dh\u00b7n), we can perform a single low-rank decomposition Wjoint \u2248 AjointBjoint, where Ajoint \u2208 Rdxrjoint and Bjoint \u2208 Rrjoint \u00d7(dh\u00b7n). We call this scheme joint-head low-rank decomposition (J-LRD).\nJ-LRD has the advantage of preserving the common principal components shared among different heads. This occurs because SVD is particularly effective at capturing the dominant components when applied to a larger, combined matrix, resulting in a more accurate approximation.\nFor J-LRD, the joint latent representation shared among all heads can be computed with hjoint = xAjoint. During decoding, the original states for each head can be reconstructed via\n[y1,\u2026, yn] = hjointBjoint.\nDespite better preserving model information, J-LRD introduces significant computational and memory overhead during decoding. Specifically, the total number of floating point operations (FLOPs) to reconstruct the key or values state of one head now becomes rjoint dh\u00b7n. Assuming the same size as the total low-rank latent representations (i.e., rjoint = \u2211ni=1 ri), the total reconstruction cost is n times higher than M-LRD, whose total FLOPs is ridh.n. When considering the matrix fusion in Sec.3.1.1, the fused matrix size of J-LRD has a size of rjoint\u00b7d n, which is also n times larger than M-LRD, leading to a substantial higher memory consumption."}, {"title": "3.2.3 Group-Head Low-Rank Decomposition", "content": "To balance the trade-off between accuracy and reconstruction cost, we propose group-head low-rank decomposition(G-LRD). G-LRD decomposes the matrices for a group of heads together. With combined weight matrices, it captures shared information within each group while limiting computational overhead and preserving accuracy.\nTo illustrate the G-LRD process, consider the weight matrices for a group of s heads, Wgj = [Wj,1... Wj,s], where Wj,i \u2208 Rdx(dn's). We low-rank decompose Wgj \u2248 AgjBgj, where Agj \u2208 Rdxrg and Bgj \u2208 Rrg\u00d7(dh's). The latent representation shared among attention heads in the same group can be computed as hgj = xAgj. During decoding, the original key or value for each head can be reconstructed via\n[yj,1... yj,s] = hgj Bgj\nThe FLOPs for reconstructing the keys and values for each head in G-LRD is rgdhnng, where ng = ns is the number of groups. Comparing the cost to J-LRD and assuming the same total rank"}, {"title": "3.3 Automatic Rank Allocation", "content": "To allocate an ideal rank size to the decomposition target, it is crucial to accurately estimate the importance of the target matrix (e.g., grouped weights). In Palu, we identify Fisher information (Ly et al., 2017; Liu et al., 2021) as an accurate approximator since it can quantify the amount of information for each parameter. We then employ the sum of Fisher information to estimate the importance of the weight matrix of each linear layer.\nAssuming that the compression sensitivity is proportional to Fisher information, we determine the rank for each weight matrix by computing the ratio of its Fisher information to the total Fisher information across all decomposition targets. We use this ratio to allocate the compression rate (i.e., rank level r), ensuring that more important layers retain higher rank levels."}, {"title": "3.4 Low-Rank-Aware Quantization", "content": "We integrate quantization into Palu to further compress the KV-Cache. However, we observe that low-rank compressed latent representations have severe outliers, which limit quantization applicability in Palu. Unlike natural outliers described in previous KV-Cache quantization literature (Liu et al., 2024; Hooper et al., 2024), these outliers are induced by SVD-based low-rank factorization.\nFig. 4 (a) shows the distribution of low-rank compressed key states from a layer of Llama-2 with G-LRD. Repeating outlier patterns appear at the beginning of each decomposed group because SVD arranges larger eigenvalues in the initial rows or columns, resulting in rapidly descending values in the latent representation. This pattern stretches the data distribution and hurts quantization accuracy.\nInspired by recent LLM quantization literature (Ashkboos et al., 2024b; Tseng et al., 2024), we apply the Walsh-Hadamard transform (WHT, Fino and Algazi) to eliminate outliers (Fig. 4 (b)), en-"}, {"title": "3.5 Efficient Inference Implementation", "content": "Support for RoPE-based Attention. To support RoPE-based attention, we fuse the reconstruction of original keys, applying positional embedding (i.e., ROPE (Su et al., 2024)) and the final matrix-vector multiplication of query and reconstructed keys by implementing customized GPU kernels in Triton (Tillet et al., 2019). We leverage the tiling techniques akin to FlashAttention (Dao et al., 2022) to minimize the memory reads/writes of GPU high bandwidth memory."}, {"title": "4 Experiments", "content": "Models and Tasks. We evaluate Palu on three LLM families, Llama2(Touvron et al., 2023), Mistral(Jiang et al., 2023) and Vicuna(Chiang et al., 2023). For accuracy evaluation, we measure perplexity on the WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets and use LM-Evaluation-Harness (Gao et al., 2023) to measure zero-shot accuracy on six common sense tasks. We also evaluate long context accuracy on 8 tasks in LongBench (Bai et al., 2023). See Appendix I for further details on the dataset and settings.\nCompression Settings. We implemented Palu using the Huggingface library (Wolf et al., 2020). Decomposition of the key and value projection layers was performed using the truncation-aware SVD method proposed by SVD-LLM (Wang et al., 2024). Unless otherwise specified, results for Palu are reported using G-LRD with a group size (gs) of 4 with equal rank size for each group. To calculate Fisher information in rank searching, we"}, {"title": "4.1 Main Results", "content": "Perplexity Evaluation. We first present perplexity evaluation results on Wikitext-2 and C4 in Table 1. For the Llama2-7B model, we observe that the M-LRD method fails to maintain low perplexity at a 50% compression rate. In contrast, the J-LRD method performs significantly better given the same compression rate, achieving perplexity values of 5.62 on Wikitext-2. The G-LRD method, with a group size of 4, achieves an optimal balance between perplexity and computational cost, yielding perplexity values of 6.01 on Wikitext-2 at a 50% compression rate. The same trend is consistent across the Llama2-13B model.\nZero-shot Evaluation Results. We further evaluate Palu on six common-sense zero-shot tasks, as shown in Table 1. Similar to the perplexity evaluation, the J-LRD method demonstrates the best performance on Llama2-7B, with only a 0.23% average accuracy degradation, while the M-LRD method results in the lowest average performance, with a 7.26% drop in accuracy compared to the baseline. In comparison, G-LRD experiences only a 2.4% average accuracy decline.\nLongBench Results. To assess whether Palu can effectively compress the KV-Cache for long-context scenarios, we present the performance of Palu across eight tasks from LongBench. We select Mistral-7B and Vicuna-7B (Chiang et al., 2023) as our evaluation targets, which support sequences up to 8k and 16k, respectively. Table 2 reports experiment results. We find that at a 50% compression level, it is relatively difficult to fully preserve accuracy. However, when compressing KV-Cache for 30%, Palu only has a 1.47% averaged accuracy degradation for Vicuna-7B, and even achieves a 1.09% enhancement for Mistral-7B. These results evidence of Palu's ability to shrink the KV-Cache while maintaining performance."}, {"title": "4.2 Integrating Palu with Quantization", "content": "Table 3 shows the results of integrating quantization with Palu. With 3-bit quantization, Palu exhibits only a negligible perplexity increase of 0.08 at a 30% compression rate and a minimal 0.23 at a 50% compression rate, both compared to Palu at 16-bit. This indicates a minimal trade-off for substantial compression of integrating quantization. Furthermore, at 2-bit quantization, Palu significantly outperforms KVQuant, which is a SOTA quantization-based method. Palu demonstrates a compelling reduction in perplexity by 1.19 and 0.54 points, while consuming 30% and 50% less memory, respectively. These results underscore the effectiveness of Palu, establishing it as a superior KV-Cache compression strategy."}, {"title": "4.3 Latency Evaluation", "content": "We now provide the latency evaluation for Palu. In our experiment, latency is measured on an A6000 GPU, and the averaged results over 100 runs are reported. For baseline, we use the default implementation from HuggingFace in PyTorch. We do not apply FlashAttention (Dao et al., 2022) for an apple-to-apple comparison, as our current implementation does not fuse the Softmax operation. We leave the integration with FlashAttention as the future work."}, {"title": "5 Related Work", "content": "SVD for LLM Compression. There are several related works applying SVD for LLM compression. An early work (Noach and Goldberg, 2020) simply compresses the weight matrices using standard SVD, resulting in high compression errors. To address this, FWSVD (Hsu et al., 2022) incorporates Fisher information to prioritize parameters. Additionally, ASVD (Yuan et al., 2023) taken the activation outlier phenomenon in LLM into consideration, and propose and scaling techniques to mitigate the compression error. SVD-LLM (Wang et al., 2024) further advances this work by understanding the compression loss from each singular value, improving compression accuracy. Unlike previous related SVD works that focus on compressing model weights, Palu focus on using SVD to reduce KV-Cache size.\nKV-Cache Quantization. Quantization appears to be an effective compression technique to compress KV-Cache. Atom (Zhao et al., 2023) uses a simple per-token quantization scheme to compress KV-Cache. WKVQuant (Yue et al., 2024) proposes a two-level quantization scheme to improve quantization accuracy. KIVI (Liu et al., 2024) utilizes different quantization schemes, per-channel and per-token, for key and value states respectively. KVQuant (Hooper et al., 2024) follow a similar setting but adopts non-uniform quantization and sparse matrix for preserving outliers. In Palu, we eliminate outliers with WHT to enhance quantization friendliness and, therefore, only need a simple per-token quantization and achieve a remarkable accuracy."}, {"title": "6 Conclusion", "content": "We introduce Palu, a novel KV-Cache compression framework that decomposes key and value linear projection weight matrices and caches the compressed latent representations. We propose various optimizations, including group-head low-rank decomposition, rank search algorithm, low-rank-aware quantization, matrix fusion, and kernel fusion. With these optimizations, Palu can maintain accuracy while achieving significant memory reduction and faster inference. Experiments show that Palu can accelerate the end-to-end attention module by up to 1.61\u00d7 with 50% KV-Cache compression; when combined with 2-bit quantization, Palu achieves a remarkable 91.25% compression rate, both maintaining strong accuracy."}, {"title": "Limitations and Future Work", "content": "We introduce a novel KV-Cache compression framework based on low-rank projection. However, our experiments cover model variants with up to only 13B parameters due to limited computational resources. Future work needs to establish how Palu performs for models with larger sizes (e.g., 70B or beyond). Also, we leave for future work the evaluation of end-to-end speedup when combining low-rank compression and quantization. It would also be interesting to investigate how to combine Palu with other efficient LLM methods, such as token eviction or weight quantization, to achieve the highest possible acceleration."}, {"title": "7 Experiment Details", "content": ""}, {"title": "8 Extended Experiments on LongBench and comparisons to quantization baselines", "content": "We conducted extensive experiments using the Mistral-Instruct-7B-v0.2 (Jiang et al., 2023) and LongChat-7b-v1.5-32k (Li et al., 2023) models, both with extended context lengths of 32k tokens. We included 16 available English datasets from LongBench to ensure a comprehensive evaluation. We report the average score for each type of task separately, as well as the overall average across all 16 tasks. This evaluation aimed to broadly assess the effectiveness of Palu in long-context scenarios. For reference, we also include comparisons with KIVI.\nOur findings indicate that at a 30% compression level, Palu achieves only a minor average accuracy drop (<1%) compared to the baseline for both models. These impressive results are attributed to our effective use of low-rank projection methods to explore redundancy in hidden dimensions. Furthermore, by fusing the Hadamard matrix offline to improve quantization friendliness, we were able to quantize the low-rank projected KV-Cache down to 3 bits. This quantization process resulted in less than a 1% accuracy degradation, achieved with a straightforward per-token quantization approach. When compared to KIVI, our framework demonstrates the ability to harness an additional 30% of redundancy. Notably, Palu does not require the complex grouped quantization and mixed-precision techniques employed by KIVI, highlighting the quantization friendliness that Palu offers."}, {"title": "9 Integrating Palu with LoRA Finetune", "content": "LORA (Hu et al., 2022) has become one of the most widely used efficient fine-tuning techniques for adapting models to particular tasks or domains with limited data. It has also been applied with"}]}