{"title": "Palu: Compressing KV-Cache with Low-Rank Projection", "authors": ["Chi-Chih Chang", "Wei-Cheng Lin", "Chien-Yu Lin", "Chong-Yan Chen", "Yu-Fang Hu", "Pei-Shuo Wang", "Ning-Chi Huang", "Luis Ceze", "Kai-Chiang Wu"], "abstract": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens or quantize it into lower bits. However, these methods cannot exploit the redundancy of the hidden dimension of KV tensors. This paper investigates a unique hidden dimension approach called Palu, a novel KV-Cache compression framework that utilizes low-rank projection. Palu decomposes the linear layers into low-rank matrices, caches the smaller intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU kernels. Our extensive experiments with popular LLMs show that Palu can compress KV-Cache by more than 91.25% while maintaining a significantly better accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache quantization methods at a similar or even higher memory usage. When compressing KV-Cache for 50%, Palu delivers up to 1.61\u00d7 end-to-end speedup for the attention module. Our code is publicly available at: https://github.com/shadowpa0327/Palu.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are revolutionizing the AI industry and providing a high-level intelligence that previous machine learning (ML) models could not achieve. To speed inference, caching key-value states (KV-Cache) in memory is a simple yet effective technique. However, the size of the KV-Cache can grow rapidly, straining memory capacity and bandwidth; further, the memory-bounded characteristics of the decoding stage also limit inference speed when loading KV-Cache data.\nTherefore, KV-Cache compression has become a central research topic for building efficient LLMs. Research on KV-Cache compression falls into two categories: quantization and token eviction. Quantization methods aim to use less bit-width to represent each datum; token eviction techniques seek ways to retain only a partial set of KV-Cache. However, both categories fail to explore the hidden dimensions of KV-Cache where high redundancy often occurs.\nThis work examines the problem from a fundamentally different perspective, i.e., ways to reduce the hidden dimensions. We introduce Palu, a KV-Cache compression framework that reduces the size of the KV-Cache through low-rank projection. Although low-rank projection has been shown to improve LLM efficiency, e.g., by reducing model size (Yuan et al., 2023; Wang et al., 2024) or minimizing memory footprint for finetuning (Hu et al., 2022; Dettmers et al., 2023), it has not been studied for compressing the KV-Cache, especially in post-training.\nA naive way to compress the KV-Cache with low-rank projection is by directly mapping cached matrices into low-rank space (Jolliffe and Cadima,\nHowever, this approach imposes an unacceptably heavy overhead of computing the decomposition matrices during runtime, after the prefill stage. To avoid this, Palu statically decomposes the key and value-projection weight matrices and caches the latent representations of the low-rank decomposition (Fig. 1), instead of directly decomposing the KV-Cache. This innovative design enables Palu to reduce memory while substantially mitigating runtime overhead.\nDespite its promising memory reduction, we found that naively decomposing the key and value projection matrices along the head dimension causes a substantial loss in accuracy. Although we found that decomposing the entire matrix at one time can maintain accuracy, it significantly increases reconstruction overhead. To find a balance, Palu proposes a medium-grained, group-head low-rank decomposition optimization that is both accurate and less costly in terms of reconstruction.\nFor LLMs, each linear projection module has a different sensitivity to compression (Sharma et al., 2023; Yuan et al., 2023). To exploit the sensitivity and improve accuracy, we design an efficient rank search algorithm based on Fisher information. Our algorithm automatically assigns a larger rank for important matrices and smaller ranks for less critical ones, boosting accuracy at the same compression rate.\nIn addition to low-rank decomposition, we integrate quantization into Palu. We found that low-rank decomposition often induces severe outliers in the latent representation. Inspired by recent LLM quantization methods (Tseng et al., 2024; Ashkboos et al., 2024b), we use Hadamard transformation matrices with a low-rank-aware quantization algorithm to eliminate outliers and increase quantization accuracy. To avoid runtime overhead, Palu fuses the transformation matrices into the forward and backward decomposed matrices, concealing the overhead completely.\nTo enhance inference latency, we integrate the decomposed matrix into the subsequent linear layer, thereby mitigating the online reconstruction overhead. In scenarios where RoPE is applied and matrix fusion is infeasible, we developed an optimized GPU kernel to accelerate the reconstruction.\nWe evaluate Palu on popular LLMs and benchmarks. Our experiments show that Palu maintains strong zero-shot accuracy and perplexity with up to 50% low-rank compression. Combining low-rank compression and quantization, Palu compresses KV-Cache by over 91.25% (11.4\u00d7 compression) while maintaining perplexity. Compared to a state-of-the-art KV-Cache quantization (Hooper et al., 2024) with 87.5% compression, Palu achieves a significantly lower perplexity of 1.19. Additionally, with 50% compression without quantization, Palu demonstrates a 1.61\u00d7 end-to-end speedup, highlighting Palu's ability to reduce memory usage and enhance speed for LLMs.\nOur key contributions include:\n\u2022 Palu, a new post-training KV-Cache compression framework that caches low-rank latent representations.\n\u2022 Group-head low-rank decomposition (G-LRD), an optimization for balancing accuracy and efficiency.\n\u2022 An automated rank search algorithm for adaptively assigning ranks to each decomposed matrix, given a target compression rate.\n\u2022 A low-rank-aware quantization algorithm that eliminates low-rank-induced outliers and imposes zero runtime overhead."}, {"title": "Background", "content": ""}, {"title": "Multi-Head Attention Mechanism", "content": "The multi-head attention (MHA) mechanism (Vaswani et al., 2017) is a core component of the transformer architecture. Given a new input token $x \\in R^d$, an MHA with $n$ heads projects the input into multiple queries, keys, and values using weight matrices $W_q^i$, $W_k^i$, and $W_v^i$, respectively, for each head $i$, as shown by\n$q_i = xW_q^i, k_i = xW_k^i, v_i = xW_v^i.$\nHere, $k_i$ and $v_i$ represent the key and value at time step $t$ for head $i$. We can then compute the attention score for each head $i$ and the corresponding attention output as\n$p_{t,i} = Softmax(\\frac{q_iK_i^T}{\\sqrt{d_h}}), a_i = p_{t,i}V_i,$\nwhere $K_i$ and $V_i$ denote the concatenation of current and all previous keys and values corresponding to the $i$-th head. The final MHA output is obtained by concatenating the outputs of all heads and then applying the out-projection layer $W_o$, as shown by\n$MHA(x) = \\sum_{i=1}^h a_i W_o^i = \\sum_{i=1}^h (p_{t,i}V_i) W_o^i,$\nwhere $W_o^i \\in R^{dh \\times d}$ represents the submatrices of the out-projection matrix for each head $i$."}, {"title": "Singular Value Decomposition (SVD)", "content": "SVD (Jolliffe and Cadima, 2016) is a commonly used technique for computing the low-rank approximation for a given matrix. We now introduce our specific use of SVD for Palu's default low-rank decomposition method.\nGiven a weight matrix $W \\in R^{m \\times n}$, SVD decomposes $W$ into three matrices: $W = U \\Sigma V^T$. Here, $U$ and $V$ are orthogonal matrices containing the left and right singular vectors, respectively. The matrix $\\Sigma$ is a diagonal matrix that consists of singular values. We describe the decomposition as\n$W \\approx AB, A = U_r\\sqrt{\\Sigma_r}, B = \\sqrt{\\Sigma_r}V_r^T,$\nwhere $A \\in R^{m \\times r}, B \\in R^{r \\times n}, \\Sigma_r \\in R^{r \\times r}$. $\\Sigma_r$ is a diagonal matrix containing the largest $r$ singular values, and $U_r$, $V_r^T$ are corresponding singular vectors truncated from $U$ and $V^T$. This truncation and subsequent matrix formation let us approximate weight matrix $W$ with two low-rank matrices $A$ and $B$, thereby reducing the storage by $\\frac{mr+rn}{mn}$."}, {"title": "The Palu Framework", "content": ""}, {"title": "Compressing the KV-Cache via Low-Rank Projection", "content": "As noted, a naive approach to compress the KV-Cache with low-rank projection is to apply SVD directly on the KV-Cache and store the top-$r$ singular vectors. However, this approach poses significant computational challenges during runtime that make it impractical for deployments (see Section 1).\nTo apply low-rank projection more efficiently, Palu uses SVD to decompose the key and value projection weight matrices. This approach is based on the observation that low-rank decomposition rewrites the linear projection layer from $y = xW$ into $y = xAB$. Here, $A \\in R^{d \\times r}$ is the low-rank projection matrix, and $B \\in R^{r \\times d}$ is the reconstruction matrix derived by SVD. The forward process first down-projects the input token $x \\in R^d$ into a low-dimensional latent space $h \\in R^r$ and then up-projects it back to the original space:\n$h = Ax, y = Bh$\nThis two-step process lets Palu (1) store the lower dimension latent representation instead of the origin keys and values states, and (2) reconstruct them during decoding."}, {"title": "Integration with the Attention Mechanism and Offline Matrix Fusion", "content": "We now describe how Palu decomposes the key and value linear layers for the attention mechanism. For each attention head $i$, Palu applies SVD and maps the key-projection matrix $W_k^i$ and value-projection matrix $W_v^i$ into $A_kB_k^i$ and $A_vB_v^i$.\nBased on the formula of attention output in equation 2, Palu absorbs the reconstruction matrix $B_v^i$ into the output projection matrix $W_o^i$ offline:\n$a_iW_o^i = (p_{t,i}V_i)W_o^i$\n$= (p_{t,i}H_v^iB_v^i)W_o^i$\n$= p_{t,i}H_v^i (B_v^iW_o^i).$\nSuch fusion lets Palu skip the explicit reconstruction of the full value vectors, reduce the number of matrix multiplications, and improve efficiency. A similar approach applies for calculating attention scores. Matrix $B_k^i$ can be fused into the query projection matrix $W_q^i$ offline, as shown by\n$q_iK_i^T = q_i(H_k^iB_k^i)^T$\n$= x^TW_q^i(B_k^i)^T(H_k^i)^T$\n$= x^T (W_q^i (B_k^i)^T)(H_k^i)^T.$\nHere, $B_k^i \\in R^{r \\times d_h}$ and $W_q^i \\in R^{d \\times d_h}$, so the fused matrix $(W_q^i (B_k^i)^T)$ has size $R^{d \\times r}$. This fusion boosts computational efficiency by reducing the matrix dimension during attention score calculation."}, {"title": "Mechanism with Positional Embedding", "content": "Recent LLMs, such as the Llama family, apply positional embedding (i.e., RoPE (Su et al., 2021)) onto the query and key states before multiplication. The non-linear operations of adding positional embedding prevent the fusion for the attention score, as described in equation 6. In this case, Palu reconstructs the keys from latent representations on-the-fly during decoding. We improve reconstruction efficiency by designing an optimized kernel, which we present in Sec. 3.5.\nNote that for some positional embedding methods, such as ALiBi (Press et al., 2022), or new attention mechanisms, such as MLA (DeepSeek-AI et al., 2024), positional embedding is not directly applied on the key states; therefore, the fusion of equation 6 remains valid. We expect the end-to-end speedup to be larger on these models."}, {"title": "Decomposition Granularity", "content": ""}, {"title": "Multi-Head Low-Rank Decomposition", "content": "We name the per-head decomposition scheme in Sec.3.1.1 as multi-head low-rank decomposition (M-LRD). We found M-LRD often causes a non-negligible accuracy degradation (discussed further in Sec.4.1), possibly because SVD fails to capture the common information shared across heads. Therefore, alternative approaches are needed for preserving model accuracy."}, {"title": "Joint-Head Low-Rank Decomposition", "content": "An alternative approach is to jointly decompose weight matrices for all heads. By considering the combined weight matrix $W_{joint} = [W_1, W_2,..., W_n] \\in [R^{d \\times (dh \\cdot n)}]$, we can perform a single low-rank decomposition $W_{joint} \\approx A_{joint}B_{joint}$, where $A_{joint} \\in R^{d \\times r_{joint}}$ and $B_{joint} \\in R^{r_{joint} \\times (dh \\cdot n)}$. We call this scheme joint-head low-rank decomposition (J-LRD).\nJ-LRD has the advantage of preserving the common principal components shared among different heads. This occurs because SVD is particularly effective at capturing the dominant components when applied to a larger, combined matrix, resulting in a more accurate approximation.\nFor J-LRD, the joint latent representation shared among all heads can be computed with $h_{joint} = xA_{joint}$. During decoding, the original states for each head can be reconstructed via\n$[y_1,\\dots, y_n] = h_{joint}B_{joint}.$\nDespite better preserving model information, J-LRD introduces significant computational and memory overhead during decoding. Specifically, the total number of floating point operations (FLOPs) to reconstruct the key or values state of one head now becomes $r_{joint} d_h \\cdot n$. Assuming the same size as the total low-rank latent representations (i.e., $r_{joint} = \\sum_{i=1} r_i$), the total reconstruction cost is $n$ times higher than M-LRD, whose total FLOPs is $r_i d_h \\cdot n$. When considering the matrix fusion in Sec.3.1.1, the fused matrix size of J-LRD has a size of $\\frac{r_{joint} \\cdot d}{n}$, which is also n times larger than M-LRD, leading to a substantial higher memory consumption."}, {"title": "Group-Head Low-Rank Decomposition", "content": "To balance the trade-off between accuracy and reconstruction cost, we propose group-head low-rank decomposition(G-LRD). G-LRD decomposes the matrices for a group of heads together. With combined weight matrices, it captures shared information within each group while limiting computational overhead and preserving accuracy.\nTo illustrate the G-LRD process, consider the weight matrices for a group of $s$ heads, $W_{g_j} = [W_{j,1}... W_{j,s}]$, where $W_{g_j} \\in R^{d \\times (d_h \\cdot s)}$. We low-rank decompose $W_{g_j} \\approx A_{g_j}B_{g_j}$, where $A_{g_j} \\in R^{d \\times r_g}$ and $B_{g_j} \\in R^{r_g \\times (d_h \\cdot s)}$. The latent representation shared among attention heads in the same group can be computed as $h_{g_j} = xU_{g_j}$. During decoding, the original key or value for each head can be reconstructed via\n$[y_{j,1}... y_{j,s}] = h_{g_j} B_{g_j}.$\nThe FLOPs for reconstructing the keys and values for each head in G-LRD is $r_g d_h \\cdot \\frac{n}{n_g}$, where $n_g$ is the number of groups. Comparing the cost to J-LRD and assuming the same total rank size ($r_g \\cdot n_g = \\frac{r_{joint}}{n}$), G-LRD reduces the reconstruction cost by $n_g$. Similarly, G-LRD also reduces the fused matrix size by $\\frac{n}{n_g}$. To sum up, G-LRD offers a middle ground between computation overhead and approximation accuracy. We illustrate M-LRD, J-LRD and G-LRD in Fig.3.\nFor further discussion on decomposed weight matrix size, please refer to Appendix B, where more detailed information is provided."}, {"title": "Automatic Rank Allocation", "content": "To allocate an ideal rank size to the decomposition target, it is crucial to accurately estimate the importance of the target matrix (e.g., grouped weights). In Palu, we identify Fisher information (Ly et al., 2017; Liu et al., 2021) as an accurate approximator since it can quantify the amount of information for each parameter. We then employ the sum of Fisher information to estimate the importance of the weight matrix of each linear layer.\nAssuming that the compression sensitivity is proportional to Fisher information, we determine the rank for each weight matrix by computing the ratio of its Fisher information to the total Fisher information across all decomposition targets. We use this ratio to allocate the compression rate (i.e., rank level r), ensuring that more important layers retain higher rank levels.\nFor a detailed ablation study on our Automatic Rank Allocation method, please refer to Appendix H.3, where we analyze the impact of our strategy on model performance."}, {"title": "Low-Rank-Aware Quantization", "content": "We integrate quantization into Palu to further compress the KV-Cache. However, we observe that low-rank compressed latent representations have severe outliers, which limit quantization applicability in Palu. Unlike natural outliers described in previous KV-Cache quantization literature (Liu et al., 2024; Hooper et al., 2024), these outliers are induced by SVD-based low-rank factorization.\nFig. 4 (a) shows the distribution of low-rank compressed key states from a layer of Llama-2 with G-LRD. Repeating outlier patterns appear at the beginning of each decomposed group because SVD arranges larger eigenvalues in the initial rows or columns, resulting in rapidly descending values in the latent representation. This pattern stretches the data distribution and hurts quantization accuracy.\nInspired by recent LLM quantization literature (Ashkboos et al., 2024b; Tseng et al., 2024), we apply the Walsh-Hadamard transform (WHT, Fino and Algazi) to eliminate outliers (Fig. 4 (b)), enabling a high quantization accuracy. However, this transformation introduces an extra matrix multiplication with associated runtime overhead. Unlike earlier methods (Ashkboos et al., 2024b) that must apply online WHT when quantizing KV-Cache, we optimize this process by integrating the Hadamard matrix into low-rank decomposed weights with no additional overhead, as described by\n$W \\approx AB = (AR)(R^TB) = \\hat{A}\\hat{B},$\nwhere $R$ is the Hadamard matrix. This optimization allows Palu can integrate the proposed low-rank compression technique with low-bit quantization. Our experiments show that, on top of the low-rank compression, our quantization method only negligibly increases perplexity, even at extreme levels such as 3-bit or 2-bit (see Sec.4.2)."}, {"title": "Efficient Inference Implementation", "content": "Support for RoPE-based Attention. To support RoPE-based attention, we fuse the reconstruction of original keys, applying positional embedding (i.e., ROPE (Su et al., 2024)) and the final matrix-vector multiplication of query and reconstructed keys by implementing customized GPU kernels in Triton (Tillet et al., 2019). We leverage the tiling techniques akin to FlashAttention (Dao et al., 2022) to minimize the memory reads/writes of GPU high bandwidth memory."}, {"title": "Experiments", "content": "Models and Tasks. We evaluate Palu on three LLM families, Llama2(Touvron et al., 2023), Mistral(Jiang et al., 2023) and Vicuna(Chiang et al., 2023). For accuracy evaluation, we measure perplexity on the WikiText-2 (Merity et al., 2016) and C4 (Raffel et al., 2020) datasets and use LM-Evaluation-Harness (Gao et al., 2023) to measure zero-shot accuracy on six common sense tasks. We also evaluate long context accuracy on 8 tasks in LongBench (Bai et al., 2023). See Appendix I for further details on the dataset and settings.\nCompression Settings. We implemented Palu using the Huggingface library (Wolf et al., 2020). Decomposition of the key and value projection layers was performed using the truncation-aware SVD method proposed by SVD-LLM (Wang et al., 2024). Unless otherwise specified, results for Palu are reported using G-LRD with a group size (gs) of 4 with equal rank size for each group. To calculate Fisher information in rank searching, we used 2048 samples from Wikitext-2, each with a sequence length of 1024. For quantization, we aligned our settings with Atom (Zhao et al., 2023), and use a simple per-token, asymmetric quantization to quantize the low-rank latent representation of both key and value states."}, {"title": "Main Results", "content": "Perplexity Evaluation. We first present perplexity evaluation results on Wikitext-2 and C4 in Table 1. For the Llama2-7B model, we observe that the M-LRD method fails to maintain low perplexity at a 50% compression rate. In contrast, the J-LRD method performs significantly better given the same compression rate, achieving perplexity values of 5.62 on Wikitext-2. The G-LRD method, with a group size of 4, achieves an optimal balance between perplexity and computational cost, yielding perplexity values of 6.01 on Wikitext-2 at a 50% compression rate. The same trend is consistent across the Llama2-13B model.\nZero-shot Evaluation Results. We further evaluate Palu on six common-sense zero-shot tasks, as shown in Table 1. Similar to the perplexity evaluation, the J-LRD method demonstrates the best performance on Llama2-7B, with only a 0.23% average accuracy degradation, while the M-LRD method results in the lowest average performance, with a 7.26% drop in accuracy compared to the baseline. In comparison, G-LRD experiences only a 2.4% average accuracy decline.\nLongBench Results. To assess whether Palu can effectively compress the KV-Cache for long-context scenarios, we present the performance of Palu across eight tasks from LongBench. We select Mistral-7B and Vicuna-7B (Chiang et al., 2023) as our evaluation targets, which support sequences up to 8k and 16k, respectively. Table 2 reports experiment results. We find that at a 50% compression level, it is relatively difficult to fully preserve accuracy. However, when compressing KV-Cache for 30%, Palu only has a 1.47% averaged accuracy degradation for Vicuna-7B, and even achieves a 1.09% enhancement for Mistral-7B. These results evidence of Palu's ability to shrink the KV-Cache while maintaining performance.\nFor detailed information on the integration of Palu with quantization and the performance measurements on LongBench, please refer to Appendix E."}, {"title": "Integrating Palu with Quantization", "content": "Table 3 shows the results of integrating quantization with Palu. With 3-bit quantization, Palu exhibits only a negligible perplexity increase of 0.08 at a 30% compression rate and a minimal 0.23 at a 50% compression rate, both compared to Palu at 16-bit. This indicates a minimal trade-off for substantial compression of integrating quantization. Furthermore, at 2-bit quantization, Palu significantly outperforms KVQuant, which is a SOTA quantization-based method. Palu demonstrates a compelling reduction in perplexity by 1.19 and 0.54 points, while consuming 30% and 50% less memory, respectively. These results underscore the effectiveness of Palu, establishing it as a superior KV-Cache compression strategy."}, {"title": "Latency Evaluation", "content": "We now provide the latency evaluation for Palu. In our experiment, latency is measured on an A6000 GPU, and the averaged results over 100 runs are reported. For baseline, we use the default implementation from HuggingFace in PyTorch. We do not apply FlashAttention (Dao et al., 2022) for an apple-to-apple comparison, as our current implementation does not fuse the Softmax operation. We leave the integration with FlashAttention as the future work.\nWe evaluate the performance of our fused reconstruction kernel (Sec. 3.5) for computing RoPE-based attention score. Specifically, we measure the latency from pre-RoPE query vector to the post-GEMV attention score (refer to Fig. 2 for details). Here, we set the compression rate for our kernel as 75%. It's because we observe the key states often achieve such compression rate when setting a 50% overall compression rate (see Appendix H.3 for detailed distribution).\nWe compare Palu's runtime against the baseline, which is simply a GEMV, and report the speedup in Fig. 5 (a). For group size 32 (J-LRD), although it's the most accurate decomposition, the high reconstruction leads to a significant 7.09\u00d7 slowdown at sequence length 64K. For group size 1 (M-LRD), our fused kernel achieves up to a 2.48\u00d7 speedup at a sequence length 16k, showcasing a high speedup when more accuracy loss is acceptable. For group size 2 and 4 (G-LRD), our fused kernel demonstrates 1.75\u00d7 and 1.16\u00d7 speedup at sequence length 16K. These results highlight the importance of exploring trade-off between decomposition granularity, reconstruction cost, and accuracy. Furthermore, we observe our kernel speedup decreases for sequence lengths over 16k, due to the increasing reconstruction cost shifts the attention operation from memory- to compute-bounded."}, {"title": "Conclusion", "content": "We introduce Palu, a novel KV-Cache compression framework that decomposes key and value linear projection weight matrices and caches the compressed latent representations. We propose various optimizations, including group-head low-rank decomposition, rank search algorithm, low-rank-aware quantization, matrix fusion, and kernel fusion. With these optimizations, Palu can maintain accuracy while achieving significant memory reduction and faster inference. Experiments show that Palu can accelerate the end-to-end attention module by up to 1.61\u00d7 with 50% KV-Cache compression; when combined with 2-bit quantization, Palu achieves a remarkable 91.25% compression rate, both maintaining strong accuracy."}, {"title": "Limitations and Future Work", "content": "We introduce a novel KV-Cache compression framework based on low-rank projection. However, our experiments cover model variants with up to only 13B parameters due to limited computational resources. Future work needs to establish how Palu performs for models with larger sizes (e.g., 70B or beyond). Also, we leave for future work the evaluation of end-to-end speedup when combining low-rank compression and quantization. It would also be interesting to investigate how to combine Palu with other efficient LLM methods, such as token eviction or weight quantization, to achieve the highest possible acceleration."}, {"title": "Appendix", "content": ""}, {"title": "Quantization Basics", "content": "Quantization techniques use discrete low-bit values to approximate high-precision floating points. The general asymmetric uniform quantization function is defined as:\n$\\hat{X} = clamp(\\lfloor \\frac{X}{s} + z, 0, 2^B - 1\\rceil),$\nwhere $\\hat{X}$ denotes the approximated tensor with low-bit representations (i.e., 4-bit integers), $X$ is the floating-point tensor, $s = \\frac{X_{max}-X_{min}}{2^B-1}$ is the scaling factor, and $z = -\\frac{X_{min}}{s}$ is a zero-point. The $\\lfloor . \\rceil$ is the rounding operation."}, {"title": "Discussion regarding memory usage", "content": "In this work, the experimental results focus on the compression rate of the KV-Cache as a key metric. However, it is crucial to consider overall memory savings as a more significant factor. For instance, as demonstrated in Sec. 2.2, a typical compression rate of 30% can lead to an increase in weight size by approximately 40%. This increase is calculated under the assumption that $m = n$ and $r = 0.7n$, resulting in the equation $\\frac{mr+nr}{mn}$ = 1.4. Such an increase indicates substantial extra memory usage.\nThis issue primarily arises in J-LRD decomposition schemes, where the projections of all heads are decomposed jointly. In contrast, our M-LRD decomposition schemes and optimized G-LRD schemes involve non-square target matrices. For example, in the G-LRD scheme with a group size of 4, the target matrix is formed by concatenating the original projection matrices of each attention head in the group. In the Llama-2-7b model, with an embedding dimension of 4096 and head dimensions of 128, each projection matrix is 4096x128, resulting in a concatenated matrix of size 4096x512. In this case, the dimensions should be considered as m = 8n. Applying the referenced equation $\\frac{mr+nr}{mn}$ with r = 0.7n, we find that $\\frac{mr+nr}{mn}$ = 0.7875, indicating no additional storage cost and, in fact, achieving an additional 21.25% memory savings.\nFurthermore, it is important to highlight that the weights associated with the K and V projections account for only 2 out of 7 linear layers within transformer blocks, comprising merely 16% of the parameters in Llama-2-7b models. This limits the overall impact on memory usage. Thus, while J-LRD may incur overhead, the M-LRD and G-LRD schemes provide efficient alternatives that do not lead to increased memory usage, making them viable options for practical applications."}, {"title": "Overall Memory Reduction", "content": "In Fig. 6, we show the total memory usage, including model weights and KV-Cache under different sequence lengths. We observe that KV-Cache quickly becomes the memory bottleneck as sequence length grows. At a sequence length of 64k, the KV-Cache constitutes 78% of the total memory consumption. With 50% low-rank compression, Palu effectively reduces memory usage by 1.7\u00d7, and when combined with 2-bit quantization, it further decreases total memory usage by 4.6\u00d7."}, {"title": "End-to-End Latency Measurements", "content": "We provide the latency evaluation of integrating our optimized attention modules into the Llama-2 models in Table 4. Measurement times are expressed in milliseconds (ms). Our measurements indicated that Palu achieved significant end-to-end speedup during the decoding phase, especially with longer sequence lengths. Specifically, for a sequence length of 32K, the speedups were 1.40x, 1.55x, and 1.71x for gs=4, gs=2, and gs=1, respectively. The results demonstrate the effectiveness of our methods and the potential to explore the redundancy in the hidden dimension."}, {"title": "Extended Experiments on LongBench and comparisons to quantization baselines", "content": "We conducted extensive experiments using the Mistral-Instruct-7B-v0.2 (Jiang et al., 2023) and LongChat-7b-v1.5-32k (Li et al., 2023) models, both with extended context lengths of 32k tokens. We included 16 available English datasets from LongBench to ensure a comprehensive evaluation. We report the average score for each type of task separately, as well as the overall average across all 16 tasks. This evaluation aimed to broadly assess the effectiveness of Palu in long-context scenarios. For reference, we also include comparisons with KIVI.\nOur findings indicate that at a 30% compression level, Palu achieves only a minor average accuracy drop (<1%) compared to the baseline for both models. These impressive results are attributed to our effective use of low-rank projection methods to explore redundancy in hidden dimensions. Furthermore, by fusing the Hadamard matrix offline to improve quantization friendliness, we were able to quantize the low-rank projected KV-Cache down to 3 bits. This quantization process resulted in less than a 1% accuracy degradation, achieved with a straightforward per-token quantization approach. When compared to KIVI, our framework demonstrates the ability to harness an additional 30% of redundancy. Notably, Palu does not require the complex grouped quantization and mixed-precision techniques employed by KIVI, highlighting the quantization friendliness that Palu offers."}, {"title": "Integrating Palu with LoRA Finetune", "content": "LORA (Hu et al., 2022) has become one"}]}