{"title": "WebApp1K: A Practical Code-Generation Benchmark for Web App Development", "authors": ["Yi Cui"], "abstract": "We introduce WebApp1K, a practical code-generation benchmark to measure LLM ability to develop web apps. This benchmark aims to calibrate LLM output and aid the models to progressively improve code correctness and functionality. The benchmark is lightweight and easy to run.\nWe present the initial version of WebApp1K, and share our findings of running the benchmark against the latest frontier LLMs. First, open source LLMs deliver impressive performance, closely trailing behind GPT-40 and Claude 3.5. Second, model size has strong correlation with code correctness. Third, no prompting techniques have been found to lift performance either universally to all models, or significantly to a single model.", "sections": [{"title": "Introduction", "content": "Large language models (LLM) have been widely adopted in the software industry to enhance project velocity and code quality. Many coding benchmarks have been developed focusing on different aspects such as algorithms[4], data science[14], class-level code[6], execution[24], resolving software bugs[10], etc., more efforts are needed to calibrate LLMs' ability to assist software development for consumer-grade applications. While the current norm of LLM application is in the form of copilot or code assistant to existing software developers [9], we argue that LLMs already, and will continue to, empower more and more non-developers (by traditional standards) to write code. Below are some scenarios.\n\u2022 A handful of first-time startup builders brainstorm a new social network helping a group of underprivileged individuals, and bootstrap themselves to launch a prototype to the public. LLMs play pivotal role during each phase of the project, e.g. planning, tech stack choice, task assignment, code generation, and deployment.\n\u2022 A small group of employees volunteer to a greenfield project funded by a mid-sized software company. While enjoying the freedom of scope and high-level blessing, the team finds itself understaffed and under-resourced. The founding members spares no time to reach an all-hands-on-deck decision, and turns everyone into a developer with the aid of LLMs, very similar to the above startup scenario.\n\u2022 A college student in their senior year, determined to be a top competitor on the job market, decided to boost their GitHub profile by some impressive demos. They first consulted LLMs on popular frameworks and best practices, then delved into a project and continue to utilize LLMs to help maximize velocity. In particular, LLMs are used to generate, from scratch, functional code expected to be bug-free and ready-to-build.\nArguably, the ensemble of users exemplified above is a much larger group than the professional developer community. Undoubtedly LLMs will deliver tremendous and life-altering values to this group of users. However, as will be revealed by this study, the improvement space for LLMs is huge."}, {"title": "", "content": "To this end, we present a preliminary work to benchmark LLM performance on practical application development.\nWe aim the consumers of this benchmark to be practitioners working closely with LLMs, e.g. those who ship models (pre-training and post-training), or those who build LLM applications and decide how to choose/use their models. Lots of considerations are taken into the benchmark usability.\n\u2022 Marketability Since the benchmark aims to evaluate usefulness of LLMs to practical software development, the model artifact, i.e. the generated code, must meet the needs of customers or employers. As such, LeetCode-type algorithmic problems, while extremely valuable in other contexts, are not applicable here. Moreover, the evaluation would be a lot easier if the knowledge and capabilities to create marketable code is inherently possessed by LLMs, i.e. present in their training dataset.\n\u2022 Metric In order to present a clear direction to model hill climbing, the metric should be simple with unambiguous answer to each problem of the benchmark. Fortunately, low-hanging fruits are available because the code correctness as the most basic criteria is still an unsolved problem.\n\u2022 Overhead Training LLMs is a high-budget and high-risk process introducing lots of stress. To exacerbate the problem, training environments and tools are primitive, also vulnerable to deprecation given the continuously escalating model scale and the training infra it demands. Therefore, one should assume that bare-bone Linux is all we can count on. Anything more sophisticated, e.g. sandbox or virtual environment, will decrease the benchmark adoptability, also taking more time to run, potentially causing training delays.\nWe are blessed by achievements of open source contributors to address all these challenges. By building our benchmark around the most successful open-source frameworks, the generated code is widely considered marketable by the industry. Also these frameworks have abundant quality code and best practices shared on the Internet, already handily included in the training dataset of modern LLMs. Finally, the rich ecosystem of these frameworks equips us with stable and well-distributed testing tools to address the metric and overhead challenges.\nWe introduce WebApp1K, a benchmark to evaluate web app code generation performance of LLMs. We start with React, a JavaScript framework widely recognized as the top choice for web app development. Below are our main findings:\n1. Open source models deliver strong results. The top performing model is DeepSeek Coder V2 which closely trails behind GPT-40 and Claude 3.5 Sonnet, the top two LLMs.\n2. There is significant performance gap between the big model and the small model of the same family, proving that the scaling laws still apply in our evaluation context of code generation.\n3. No prompting techniques have been found to universally improve performance for all models or significantly lift performance of a single model. On the other hand, the strong performance of top performers (GPT and Claude) under a very simple prompt suggests that LLM should be able to code correctly as long as the expectations (i.e. test cases) are clear.\nWe share three artifacts on GitHub and Huggingface: the dataset containing all 1000 problems of WebApp1K[11], the script[12] to run WebApp1K, and the leaderboard[13].\nThe rest of this report is organized as follows. Section 2 details how WebApp1K was built. Section 3 presents our evaluation methodology, various experiments, and performance results of the latest LLMs. Finally, Sec 4, conclude contributions and discusses future works."}, {"title": "Benchmark", "content": "We choose React[2] as the first evaluation subject of our benchmark. Introduced in 2013, it has remained a top choice for web app builders for the quality and stability of the framework itself, productivity gains to developers, and the rich ecosystem. In addition to these marketabiliy benefits, it also constructs a higher level abstraction above HTML syntax and DOM manipulation, allowing developers to encapsulate their business logic to succinct and reusable code. These technical advantages also benefit the code generation evaluation on LLMs."}, {"title": "Evaluation Results", "content": "The most straightforward way for us to access LLMs are public token-based APIs. For top close-sourced models, our only option is via the owners' APIs. The top open-sourced models are hosted by a few platforms, among which we choose Fireworks[1].\nWe evaluate three closed-source, and three open-source model families listed in Table 4."}, {"title": "Experiment Setup", "content": ""}, {"title": "Parameter Tuning", "content": "Although each API bears its minor difference, all APIs are heavily influenced by the design of OpenAI API."}, {"title": "Prompt Experiments", "content": "We also study whether more sophisticated can lift the model performance.\nThe first experiment is system prompt, which assigns an explicit role to the LLM and raises its awareness. Available in all APIs we run, it complements the user prompt (Equation (1)) which gives detailed instructions to LLM. Equation (2) shows our system prompt.\nYou are a code generator.\n(2)\nThe second experiment is verbose comment, which aims to help LLMs better understand the semantics of tests it tries to pass. For each of the 1000 problems, we feed its test code to GPT-40 and ask for English summary of the expectation in multiple sentences. The summary is then inserted into the test code. Table 7 shows the verbose comment variant of the test code in Table 1.\nThe third experiment is error debugging. If the generated code fails the test, we add the failed code and the error log to the prompt, hoping the LLM will generate the correct code by learning from its"}, {"title": "Results", "content": "Table 9 shows results of full-scale evaluation in the form of leaderboard. Models are ranked by Pass@10. where gpt-4o is the winner. Results of Pass@5 completely agrees with this ranking. Results of Pass@1 also largely agrees, except claude-3.5-sonnet which surpasses gpt-4o as the winner.\nWe believe the ranking of this leaderboard meets expectations of the LLM community. It is worth calling out the truly impressive performance of open source LLMs, especially Deepseek.\nAlso when comparing the large and small models of the same family, it is evident that scaling laws still holds. In fact, of the six open source models whose parameter counts are available, their"}, {"title": "Conclusion and Future Works", "content": "This report presents our preliminary contribution to LLM code generation. We proposed WebApp1K, a practical and comprehensive benchmark focusing on web app development. To recap findings via this benchmark:\n1. Open-source LLMs deliver impressive results\n2. Model size plays pivotal role to code generation performance\n3. No prompting techniques have been found to either universally help all LLMs or significantly help a single LLM\nWe plan for the following future works.\nFirst, more frontier LLMs are released as of the writing of this report, namely Llama 3.1[17] and Mistral Large 2[20]. We will report our evaluation results as soon as possible.\nSecond, we need to make the benchmark more difficult, as it is already saturated by top performers, i.e. GPT-4o and Claude 3.5. While this benchmark, as it stands now, can help other models to improve themselves, we need to make improvement space for the best models, as bigger and stronger models will be released sooner rather than later.\nFinally, this work produce lots of error logs which we will dive deeper. We expect to uncover useful insights, which could help models of all rankings to continue the hill climbing."}, {"title": "Appendix: Applications of WebApp1K", "content": ""}]}