{"title": "Resilience to the Flowing Unknown: an Open Set Recognition Framework for Data Streams", "authors": ["Marcos Barcina-Blanco", "Jesus L. Lobo", "Pablo Garcia-Bringas", "Javier Del Ser"], "abstract": "Modern digital applications extensively integrate Artificial Intelligence models into their core systems, offering significant advantages for automated decision-making. However, these AI-based systems encounter reliability and safety challenges when handling continuously generated data streams in complex and dynamic scenarios. This work explores the concept of resilient AI systems, which must operate in the face of unexpected events, including instances that belong to patterns that have not been seen during the training process. This is an issue that regular closed-set classifiers commonly encounter in streaming scenarios, as they are designed to compulsory classify any new observation into one of the training patterns (i.e., the so-called over-occupied space problem). In batch learning, the Open Set Recognition research area has consistently confronted this issue by requiring models to robustly uphold their classification performance when processing query instances from unknown patterns. In this context, this work investigates the application of an Open Set Recognition framework that combines classification and clustering to address the over-occupied space problem in streaming scenarios. Specifically, we systematically devise a benchmark comprising different classification datasets with varying ratios of known to unknown classes. Experiments are presented on this benchmark to compare the performance of the proposed hybrid framework with that of individual incremental classifiers. Discussions held over the obtained results highlight situations where the proposed framework performs best, and delineate the limitations and hurdles encountered by incremental classifiers in effectively resolving the challenges posed by open-world streaming environments.", "sections": [{"title": "1 Introduction", "content": "The rapid expansion of Artificial Intelligence (AI) systems over the last decade has raised major concerns about their resilience and safety. In real-world scenar-"}, {"title": "2 Related Work", "content": "Before delving into the work done in OSR, it is important to mention that the appearance of UC has been tackled from other research areas in Machine Learning, such as Novelty Detection (ND) and Out-of-Distribution (OoD) detection [27]. Although these two research areas focus on detecting and dealing with novel instances, they possess some key differences with OSR. ND is mainly considered to be an unsupervised task and does not require the classification of instances from KC. On the other hand, OoD differs in the training and benchmarking procedure. Unlike OoD, OSR avoids training with real outliers to control the positively labeled open space. While adding a \"background\" class is effective, it does not restrict the over-occupied space. OSR benchmarks split a dataset into KC (in-distribution) and UC (out-of-distribution) instances. In contrast, OoD typically regards one dataset as in-distribution and other datasets as OoD. Contributions done so far in the field agree on the division of OSR approaches into two main categories [8,16]: discriminative and generative. We now revisit contributions in these two categories (Subsection 2.1), followed by a justification of the contribution of this work (Subsection 2.2)."}, {"title": "2.1 OSR Approaches", "content": "This first category of OSR approaches (discriminative) focuses on modeling the data into smaller areas in the feature space to distinguish between KC and UC. Early approaches adapted traditional classification models to the open-world [10, 17,23]. Other approaches employ methods better suited for predicting UC, such"}, {"title": "2.2 Contribution", "content": "In discriminative approaches, clustering and classification models can be combined to address the over-occupied space problem. Clustering models adjust KC to a smaller area, while classification models are used to discriminate between them. Approaches in the field employ sequential [26] or simultaneous [4, 29] strategies. Finally, the research in streaming conditions tackles challenges like identifying new classes from unknown instances and updating models with new knowledge, which still remain untackled [7,13]. Few works explicitly address OSR in streaming setups [20]. However, the literature lacks a detailed exploration of the over-occupied space problem and the connection of OSR to ND and concept evolution in stream learning [1]."}, {"title": "3 Problem Statement", "content": "Traditional streaming classification tasks assume that there is an infinite sequence of pairs $(x_t, y_t)$, where $x_t \\in X$ and $y_t \\in C_{KC} = \\{Y_1, Y_2,...,Y_N\\}$ (KC). In the absence of unexpected classes and non-stationarities, classifiers are either trained offline with instances from $C_{KC}$ or, alternatively, learn incrementally from the stream if no extreme verification latency is held. Classifiers compute a probability distribution over the training classes $p(y|x)$ with $y \\in C_{KC}$. Since this conditional distribution is defined for any $x \\in X$, feature space is over-occupied by the classifier as it characterizes regions in X that may eventually belong to UC. Since any predicted label $\\hat{y}_t$ belongs to $C_{KC}$, whenever a new query instance $x_t$ whose true label does not belong to $C_{KC}$ appears in the stream, a classification error occurs.\nBy contrast, streaming OSR classification models are designed to work in an environment similar to the above, but where $y_t$ does not necessarily belong to $C_{KC} \\forall t$. They are trained to correctly classify instances $x_t$ from $C_{KC}$, but instances may also belong to $C_{UC}$, with $C_{KC} \\cap C_{UC} = \\emptyset$. A streaming OSR model should be able to determine whether any new instance $x_t$ belongs to $C_{KC}$ or $C_{UC}$. Additionally, if $x_t$ is determined to be from $C_{KC}$, the model should also predict its class label $\\hat{y}_t \\in C_{KC}$."}, {"title": "4 Proposed Streaming OSR Framework", "content": "As explained in the previous section, the predictions of a single closed-set classifier are limited to $C_{KC}$. The addition of a clustering model can help circumvent this flaw by capturing the structure of arriving data, whereas a closed-set classification model can be deployed and updated incrementally with instances arriving from the stream verified to belong to $C_{KC}$.\nThis rationale guides the design of the proposed OSR framework for data streams depicted in Figure 2. Stream instances $\\{x_t\\}$ are received over time, each first processed in order to estimate the degree to which stream instance $x_t$ is unknown. This estimation exploits an incremental clustering algorithm to characterize the feature space, continuously updating cluster arrangements upon each arrival of instances. Let $M_t$ denote the number of clusters discovered by the cluster algorithm at time t, and $c_m \\in X$ the centroid corresponding to the m-th cluster. A measure of novelty of $x_t$ can be produced through the use of entropy [18, 24]. Based on the assumption that instances closer to each other are more similar than those further apart, we can compute a pseudo-probability $p(m|x_t)$ of every arriving instance $x_t$ to every cluster $m \\in \\{1, ..., M_t\\}$:\n$p(m/x_t) = \\frac{1/d(x_t, c_m)^2}{\\sum_{m'=1}^{M_t} 1/d(x_t, c_{m'})^2}$ (1)\nwhere $d(\\cdot,\\cdot)$ stands for the Euclidean distance. The $1/d(x_t, c_m)^2$ is adopted as the weight representation function because it quickly diminishes with distance, granting that those elements nearer to the clusters show much higher weights than those farther away. Based on the pseudo-probability vector $\\{p(m|x_t)\\}_{m=1}^{M_t}$, the framework computes the entropy associated to $x_t$ as:\n$H(x_t) = \\sum_{m=1}^{M_t} -p(m/x_t) log_{M_t} p(m|x_t)$ (2)\n, in which instances with lower $H(x_t)$ belong to $C_{KC}$, while instances with higher entropy are declared to be unknown."}, {"title": "5 Experimental Setup", "content": "Before delving into the limitations of the proposed framework and forthcoming challenges, we review the results of an experimental benchmark. We aim"}, {"title": "6 Results and Discussion", "content": "The results of the experiments are shown in Tables 2 (synthetic datasets) and 3 (real dataset) as average \u00b1 standard deviation for the three baselines: static (top position in each cell), incremental (middle) and SOSR (bottom). Best results for every (\u03b2, dataset, score) combination considering the statistical significance as per the Wilcoxon test are highlighted in gray. As expected, the regular static classifier lacks the ability to detect any instances from the UC. The UC-Acc of the incremental classifier grows with higher levels of missing classes and higher number of instances from the UC. The KC-Acc and F1 are generally higher for the single classifier methods. Overall, the UC-Acc is the highest for the proposed SOSR approach, as it is able to better detect instances from UC, and to manage the over-occupied space than the other classifiers."}, {"title": "7 Conclusions, Limitations and Future Work", "content": "This work has elaborated on the over-occupied space issue of OSR, focusing on its complexity in streaming environments. We have proposed a streaming OSR framework, that combines an incremental classifier with incremental clustering, to yield a generic processing workflow to perform classification modeling tasks over open-world data streams. We have compared three approaches: a regular classifier, an incremental classifier, and the streaming OSR framework configured with an incremental version of the K-Means clustering. Our results have exposed that the proposed framework can help a classifier overcome the over-occupied space problem by virtue of a simple entropy-based detection based on the incrementally updated distribution of clusters.\nAlthough our proposal effectively learns to detect UC, it still undergoes similar issues to a regular classifier and is not enough to solve the issue. As discussed in Section 4, the reliability of the entropy measure depends on the ability of the clustering model to find representative centers. This statement is buttressed by Figure 3, which depicts the Davies-Bouldin (D-B) index of the SOSR approach for all datasets at each level of B. The higher the D-B index is, the lower the quality of the clustering will be. The core limitation of the STREAMKmeans algorithm, together with the fixed value of $Y_H$ used in the experiments, make the overall framework not to be suitable to properly characterize all data streams in the benchmark, as evinced by the correlation between the D-B indices and the AUC scores of the SOSR framework in Tables 2 and 3. An incorrect threshold also damages the performance of the closed-set classifier due to many KC instances being identified as UC. Future work will explore the use of automated configuration techniques and other incremental clustering algorithms, potentially better suited for complex streaming data distributions.\nAnother aspect demanding further thought is the distinction between the different types of concept drift and OSR. Unlike virtual $(p_X(x)$ changes) and real drift $(p_{Y|X}(y|x)$ changes), in OSR what changes is the discrete support $y$ of the statistical variable Y, which represents the different classes in the stream. It can be challenging for the UC\u2192KC consolidation module to decide whether a set of UC instances detected over time corresponds to a sharp distributional change of X (i.e. a virtual drift) or, alternatively, should be consolidated as a new class and incorporated as such to the classifier. These limitations and other"}]}