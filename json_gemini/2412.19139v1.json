{"title": "PlanLLM: Video Procedure Planning with Refinable Large Language Models", "authors": ["Dejie Yang", "Zijing Zhao", "YangLiu"], "abstract": "Video procedure planning, i.e., planning a sequence of action steps given the video frames of start and goal states, is an essential ability for embodied AI. Recent works utilize Large Language Models (LLMs) to generate enriched action step description texts to guide action step decoding. Although LLMs are introduced, these methods decode the action steps into a closed-set of one-hot vectors, limiting the model's capability of generalizing to new steps or tasks. Additionally, fixed action step descriptions based on world-level commonsense may contain noise in specific instances of visual states. In this paper, we propose PlanLLM, a cross-modal joint learning framework with LLMs for video procedure planning. We propose an LLM-Enhanced Planning module which fully uses the generalization ability of LLMs to produce free-form planning output and to enhance action step decoding. We also propose Mutual Information Maximization module to connect world-level commonsense of step descriptions and sample-specific information of visual states, enabling LLMs to employ the reasoning ability to generate step sequences. With the assistance of LLMs, our method can both closed-set and open vocabulary procedure planning tasks. Our PlanLLM achieves superior performance on three benchmarks, demonstrating the effectiveness of our designs.", "sections": [{"title": "Introduction", "content": "Mastering procedure planning is of great importance for building artificial intelligence systems capable of comprehending and imitating human actions, and eventually assisting humans in solving goal-directed problems (Tellex et al. 2011; Jansen 2020; Ahn et al. 2022; Mishra et al. 2018; Mysore et al. 2019). Previous research (Chang et al. 2020) has pointed out that instructional videos are natural resources for acquiring the skill of procedure planning, proposing the problem of video procedure planning. For example, given the visual frames of start state \"raw food\" and goal states \u201chummus\", the model is expected to produce a sequence of action steps to achieve the goal, i.e., first \"peel the garlics\", then \"add garlics and beans into food crusher\", and finally \"crush the food\".\nTo address the above challenges, we propose PlanLLM, a cross-modal joint learning framework with LLMs for video procedure planning. As shown in Figure 1b, we utilize LLM's planning priors to enhance the model's generalization ability through an LLM Enhanced Planning module, producing free-form and open-vocabulary procedure planning outputs. Notably, we are the first to use a trainable LLM to output action step sequences that are not confined to a predefined set and can generalize to new steps or novel tasks. To build cross-modal interaction between visual states and step descriptions, we introduce a Mutual Information Maximization mechanism to connect sample-specific visual information with world-level commonsense descriptions. This allows the LLM to use its generalizable reasoning ability, grounded in sample-specific visual commonsense, to enhance procedure learning. We propose a progressive alignment scheme during training. We first freeze the LLM to align the visual embeddings to textual space using a mutual information maximization loss, and then fine-tune the LLM jointly with other modules for procedure planning. Finally, the output from the LLM branch can contribute to conventional decoding by providing insights into semantic relevance of steps, thereby enabling our approach to handle both closed-set and open-vocabulary procedure planning tasks.\nOverall, the main contributions of our work are as follows: (1) We propose PlanLLM, a cross-modal joint learning framework with a trainable LLM for video procedure planning, which is the first to consider both the model's generalization ability in the open world and its planning performance in closed sets. (2) We introduce the LLM-Enhanced Planning module, which utilizes the LLM's textual planning priors to enhance the capability of video procedure planning models. Additionally, we propose a Mutual Information Maximization module to connect world-level commonsense in step descriptions with sample-specific visual state information, enabling LLMs to utilize reasoning abilities for generating step sequences. (3) PlanLLM achieves superior performance on three commonly used datasets: COIN (Tang et al. 2019), CrossTask (Zhukov et al. 2019), and NIV (Alayrac et al. 2016), and improves cross-dataset procedure planning performance, demonstrating the effectiveness and generalization of our method."}, {"title": "Related Works", "content": "Video Procedure Planing. Video procedure planning task (Chang et al. 2020) aims to generate a sequence of action steps based on visual observations of the start and goal states.\nIn this paper, we achieve weakly supervised video procedure planning relying only on textual action step labels during training. Unlike existing methods that decode action steps into a closed-set of one-hot vectors, we harness the generalization ability of LLMs to generate free-form procedure planning outputs. Our approach not only enhances the decoding of closed-set labels, but also allows the model to generate free-form outputs for new action steps and planning tasks. Note that the step descriptions used in both our method and the Language As Supervision approaches such as (Niu et al. 2024) are dataset-specific, aggregated from step labels and enhanced by the LLM, and are independent of the sample-specific visual states.\nLLMs for Planning. Pre-trained large language models (LLMs) (Brown et al. 2020; Chowdhery et al. 2023) have demonstrated their ability to generate high-level plans (Xie et al. 2023; Wang et al. 2023c; Liu et al. 2023a; Ye et al. 2024; Yang and Liu 2024) and video understanding"}, {"title": "Methodology", "content": "Problem Formulation and Overview\nVideo procedure planning task aims to generate a sequence of action steps based on visual observations of the start and goal states. As shown in Figure 2, given the visual observations (images) of the start frame $v_0$ and the goal $v_T$, the target is to plan a procedure, represented as a sequence of action steps $a_{1:T}$, which are sequentially performed to achieve the transition from $v_0$ to $v_T$. The video procedure planning problem can be formulated as $p(a_{1:T}| v_0, v_T)$. We follow the weakly supervised video procedure planning setting (Wang et al. 2023b; Niu et al. 2024), which rely only on the ground truth textual action sequence $a_{1:T}$ without requiring corresponding intermediate visual states (keyframes between two states). Following previous work (Niu et al. 2024), we use the textual information of action step labels, treating video procedure planning as a multi-modal task. Notably, the textual information of step descriptions are collected from the training set and shared across different visual state inputs, making them independent of specific inputs.\nAs depicted in Figure 2, our PlanLLM encompasses three parts: (1) Feature Extraction: We use a visual encoder and a language encoder to extract features from the visual inputs ${v_0, v_T}$ representing the start and goal states, capturing sample-specific visual details essential for task context, and from the textual descriptions D of action steps, capturing world-level commonsense. (2) Mutual Information Maximization: To fuse sample-specific visual information with world-level textual commonsense, we introduce Q-Former architecture which takes visual embeddings processed by the state interaction module and step description embeddings as inputs to generate cross-modal joint step embeddings. (3) LLM Enhanced Planning: To address the limitations of one-hot decoding of action steps, which ignores semantic relevance between steps and cannot generalize to new steps or tasks, we propose the LLM-Enhanced Planning module. This module takes multi-modal step embeddings from the Mutual Information Maximization module as inputs, allowing the LLM to be fine-tuned via LoRA (Hu et al. 2022) to directly produce free-form procedure planning outputs, capable of handling new steps and tasks. Additionally, when combined with our proposed knowledge fusion and step refinement module, the general reasoning ability of LLMs is effectively leveraged to refine the original step decoding process, improving performance.\nFeature Extraction\nWe first introduce a visual state encoder $E_V$ and a language encoder $E_l$ to extract features from sample-wise visual state inputs ${v_0, v_T}$ and world-level action step descriptions D, respectively.\nVisual State Feature Extraction: The visual state encoder takes the video frames of the start and goal states as input and outputs embeddings that represent sample-specific visual information. We adopt a pre-trained vision backbone $f_v$ together with a trainable projection layer $p_v$ as the visual state encoder $E_V$. The embedding $x_t$ of visual state $v_t, t \\in {0, T}$ is extracted by:\n$x_t = E_V(v_t) = p_v(f_v(v_t)), t \\in {0,T}$"}, {"title": "Textual Step Feature Extraction:", "content": "As the original step texts are too concise, following (Niu et al. 2024), we extend all the action step labels $A = {a_i}_{i=1}^N$ to enriched action step descriptions $D = {d_i}_{i=1}^N$ using large language models (such as GPT-3.5) with world-level commonsense knowledge:\n$d_i = LLM(prompt, a_i), i \\in {1, 2, ..., N}$\nwhere N denotes the number of all possible action step in the dataset. The details of description generation are provided in supplemental materials. We adopt a fixed pre-trained language encoder $E_l$ to extract the embedding $y_i$ of the action step descriptions $d_i$:\n$y_i = E_l(d_i)$"}, {"title": "Mutual Information Maximization", "content": "As the visual state embeddings contain only sample-specific information while the textual step embeddings capture only world-level commonsense knowledge from LLMs, we propose a Mutual Information Maximization module to integrate both embeddings for a better representation of action steps.\nVisual State Interaction: To learn the mutual context awareness of visual states, we introduce a self-attention layer to process the original embeddings $x_0, x_T$ to interacted embeddings $x_0^v, x_T^v$:\n$x_0^v, x_T^v = SelfAtten (x_0, x_T)$\nInteracted visual embeddings provide a better representation when integrated with world-level textual step embeddings and when aligned with LLM input space.\nMutual Information Maximization(MIM) Q-Former: To integrate sample-specific visual state embeddings and textual step embeddings with world-level commonsense knowledge, we adopt a Q-Former (Li et al. 2023a) architecture to learn fused step embeddings. Specifically, we use the integrated visual start and goal state embeddings $x_0^v, x_T^v$ as the image input tokens for Q-Former, and the textual action step embeddings $y_{1:T}$ corresponding to the ground-truth step sequence $a_{1:T}$ as the textual input tokens. Using learnable step queries $q_{1:T}$, the Q-Former outputs the integrated step embeddings $x_{1:T}^q$:\n$x_{1:T}^q = QFormer (q_{1:T}; [x_0^v, x_T^v]; y_{1:\u03c4})$\nFollowing Q-Former, we optimize the Vision-Language Contrastive (VLC) loss and Vision-Language Matching (VLM) loss to maximize the mutual information between vision and language embeddings. Within a batch, we treat the visual state embeddings and their corresponding ground truth step embeddings as positive pairs, and unmatched vision and textual embeddings as negative pairs. The Vision-Language Contrastive (VLC) loss contrasts the embedding similarity of a positive pair against the negative ones:\n$L_{VLC} = -log \\frac{\\sum_{j \\in S} e^{s(x_v^q[j],x_q[j])}}{\\sum_{j \\in S} e^{s(x_v^q[j],x_q[j])} + \\sum_{j\\neq k \\in S} e^{s(x_v^q[j],x_q[k])}}$\nwhere s(,) denotes the similarity function, and $x_v^q[j]$ and $x_q[j]$ represent the interacted visual state embeddings from"}, {"title": "the visual state interaction module and the integrated step embeddings from Q-Former", "content": "respectively, for the jth sample in a batch, with $j,k \\in {1,2, . . ., B}$, where B is the batch size. For the Vision-Language Matching (VLM) loss, the model is tasked with a binary classification to predict whether an image-text pair is positive (matched) or negative (unmatched):\n$L_{VLM} = \\sum_{j,k} L_{BCE} (x_v^q[j], x_q[k]; 1(j = k))$\nThe total Mutual Information Maximization (MIM) loss combines VLC loss and VLM loss:\n$L_{MIM} = L_{VLC} + L_{VLM}$"}, {"title": "LLM Generated Free Form Planning", "content": "Different from previous methods that decode action steps into a closed set of one-hot vectors, our approach fully leverages the LLM's generalization ability to directly generate free-form procedure planning outputs, enabling it to handle open vocabulary procedure planning and new planning tasks.\nFree Form Procedure Planning Output: We introduce a generative LLM, i.e., Vicuna-7B (Chiang et al. 2023). Taking the integrated step embeddings $x_{1:T}^q$ from the MIM Q-Former along with the visual state embeddings $x_0^v, x_T^v$ as inputs, the LLM's encoder provides hidden state embeddings $h_{1:T}$, representing the enhanced action step embeddings with sample-specific commonsense, based on the general reasoning ability of the LLM. Its decoder then directly generates free-form output tokens O as the captioning of the action steps:\n$h_{1:T} = LLM_{enc} (x_0^v, x_T^v, x_{1:T}^q)$\n$O = LLM_{dec} (h_{1:T})$\nProgressive Alignment Training Scheme: To effectively train the modules to produce free-form procedure planning outputs, we propose a two-stage progressive training scheme. Specifically, in the first stage, we fix the generative LLM and train the feature extractors and MIM modules using $L_{MIM}$ as defined in Equation 8. This stage aligns the visual state embeddings and fused action step embeddings with the LLM's input space, enabling the LLM to understand and reason with this information. In the second stage, we fine-tune the LLM using LoRA (Hu et al. 2022), along with other trainable modules, optimizing an Action Step Captioning loss $L_{ASC}$ and the original MIM loss $L_{MIM}$.\nWith our progressive alignment training scheme, the LLM is effectively trained to produce free-form procedure planning outputs."}, {"title": "LLM Enhanced Close-set Step Decoding", "content": "Existing methods decode action steps as a closed set of one-hot vectors. With the help of LLMs which understand the"}, {"title": "semantic information of different action step labels, the conventional action step decoding process can also be enhanced to improve performance.", "content": "Action Step Decoder: Following previous work (Niu et al. 2024), we employ a cross-attention module to process the visual state embeddings, action step description embeddings, and learnable queries to produce final action step representations, which are then passed through a classifier to distinguish specific action steps. The final action step representations $r_{1:T}^{SD}$ are obtained by:\n$r_{1:T}^{SD} = CrossAtten (q_{1:T}^l, [x_0^v, x_T^v, x_{1:T}^q])$\nwhere $x_0^v, x_T^v$ denotes the visual start and end state embeddings, $x_{1:T}^q$ denotes fused action step embeddings from MIM Q-Former, and $q_{1:T}^l$ denotes learnable queries.\nKnowledge Fusion: To integrate the semantic understanding knowledge of LLMs, we fuse the sample-specific commonsense of hidden states embeddings $h_{1:T}$ from the LLM encoder with the final action step representations $r_{1:T}^{SD}$ from the step decoder to produce step representations:\n$r_{1:T}^{KF} = CrossAtten (r_{1:T}^{SD}; h_{1:T})$\nWith knowledge fusion module, the knowledge fused step representations are aware of the semantic relevance between action step labels with the help of LLM general knowledge.\nStep Refiner: Given the knowledge fused representations $r_{1:T}^{KF}$ as query, we finally refine the step representations from the embeddings of the action step descriptions with another cross-attention module :\n$r_{1:T}^{SR} = CrossAtten (r_{1:T}^{KF};y)$,\nwhere y is the embeddings of the action step descriptions via Equation (3). The refined step representations $r_{1:T}^{SR}$ are projected into a one-hot vector $a_{1:T}$ for classification, supervised by cross-entropy loss as the Step Decoding (SD) loss:\n$L_{SD} = L_{CE}(A_{1:\u03a4}, \u03b1_{1:T})$\nWhere $L_{CE}$ denotes cross-entropy loss, and $\u03b1_{1:T}$ denotes ground truth action step labels."}, {"title": "Training and Inference", "content": "Training: As has been discussed in Progressive Alignment Training Scheme, in the first stage, we fix the visual backbone encoder, the language encoder and the generative LLM, and only train the Mutual Information Maximization Module with $L_{MIM}$ in Equation 8 to align the visual state embeddings and the action step embeddings to the LLM input space. In the second stage, we jointly fine-tune the LLM through LORA(Hu et al. 2022) and the other modules with the corresponding losses. The overall objective can be elaborated as:\n$L = L_{MIM} + L_{ASC} + L_{SD}$\nwhere $L_{ASC}$ is the Action Step Captioning loss, $L_{SD}$ is the State Decoding loss.\nInference: Our method can handle both closed-set action step classification tasks and free-form open-vocabulary procedure planning tasks. For conventional action step classification, our proposed LLM-Enhanced step decoding branch outputs the action step IDs following (Niu et al. 2024; Wang et al. 2023b; Zhao et al. 2022). For open-vocabulary procedure planning, the generative LLM provides free-form procedure planning outputs and encodes the captions and new textual action step labels into vectors using a frozen language encoder. We then retrieve the top T action labels based on the similarity between captions and textual action step labels, where T is the number of action steps in a sequence."}, {"title": "Evaluation", "content": "Experiments\nDatasets For our evaluation, we employ three commonly used instructional video datasets: CrossTask (Zhukov et al. 2019), NIV (Alayrac et al. 2016), and COIN (Tang et al. 2019). The CrossTask dataset comprises 2,750 videos, illustrating 18 unique procedural tasks. NIV (Narrated Instructional Videos) contains 150 videos encompassing five procedures. COIN stands out as the largest dataset in our evaluation, boasting 11,827 videos, covering 778 procedures.\nMetrics We evaluate performance using three metrics as outlined in (Niu et al. 2024): (1) Mean Intersection over Union (mIoU) measures whether the model correctly identifies the set of steps required to execute the plan, regardless of the order of actions. (2) Mean Accuracy (mAcc) compares the predicted and actual action sequences element-wise, considering the correct order of actions. (3) Success Rate (SR) assesses the plan's success only if it exactly matches the ground truth, requiring precise correspondence between the predicted and actual sequences. Note that SR is a stricter metric, offering a stringent assessment of the model's performance and requires exact correspondence between the predicted and actual sequences for a plan to be successful.\nImplementation details Following recent advancements (Niu et al. 2024; Zhao et al. 2022; Wang et al. 2023b,a), we leverage a pretrained S3D network (Miech et al. 2020) as the visual backbone and the textual encoder of the pretrained CLIP (Radford et al. 2021) as our language encoder. Additionally, we deploy the trainable Q-Former architecture initialized from BLIP2 (Li et al. 2023a). Following (Li et al. 2023b), we utilize the pretrained Vicuna(Chiang et al. 2023) as the Large Language Model (LLM). During the frozen-LLM training stage, we set the learning rate to 1 \u00d7 10-4 for the Q-Former and 1 \u00d7 10-3 for other modules, training the model with a batch size of 32 on NVIDIA A800 GPUs."}, {"title": "Results", "content": "Comparisons on CrossTask Table 1 shows the comparisons between our method and others on CrossTask. (1) Compared to previous fully-supervised methods (Ehsani et al. 2018; Abu Farha and Gall 2019) that require intermediate visual states as inputs, recent weakly-supervised methods (Zhao et al. 2022; Li et al. 2023c; Liu et al. 2023b; Niu et al. 2024) achieve better results due to more advanced model designs. We also follow the weakly-supervised setting in our method. (2) Methods (Zhao et al. 2022; Li et al."}, {"title": ""}]}