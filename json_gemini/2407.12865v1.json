{"title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering", "authors": ["Derek Austin*", "Elliott Chartock"], "abstract": "Prompt engineering for large language mod-\nels (LLMs) is often a manual time-intensive\nprocess that involves generating, evaluating,\nand refining prompts iteratively to ensure high-\nquality outputs. While there has been work\non automating prompt engineering, the solu-\ntions generally are either tuned to specific tasks\nwith given answers or are quite costly. We\nintroduce GRAD-SUM, a scalable and flexi-\nble method for automatic prompt engineering\nthat builds on gradient-based optimization tech-\nniques. Our approach incorporates user-defined\ntask descriptions and evaluation criteria, and\nfeatures a novel gradient summarization mod-\nule to generalize feedback effectively. Our\nresults demonstrate that GRAD-SUM consis-\ntently outperforms existing methods across vari-\nous benchmarks, highlighting its versatility and\neffectiveness in automatic prompt optimization.", "sections": [{"title": "Introduction", "content": "The introduction of machine learning and artificial\nintelligence has automated many tasks, but cre-\nating and refining prompts for LLM applications\nremains largely a manual process. Practitioners\noften struggle to find the right prompt to elicit ac-\ncuate and high-quality outputs. Common prac-\ntice is to judge output quality by implementing\nfully automated evaluation feedback loops, where\nLLMs assess outputs based on user-provided eval-\nuation criteria, these are often called LLM-as-a-\njudge methods. Seeing as evaluation is an auto-\nmatic process in LLM-as-a-judge scenarios, the\nmain bottleneck in automating prompt engineering\nis the actual refinement of the prompt. Although\nthere have been attempts to automate this process\n(Yang et al., 2023) (Pryzant et al., 2023) (Zhou\net al., 2023), we found that these methods either\ncould not be adapted seamlessly to all tasks or were\ntoo costly to implement at scale.\nAutomatic prompt optimization methods broadly\nfall into two categories: Monte Carlo search meth-\nods (Yang et al., 2023) and feedback-based (or\ngradient-based) methods (Pryzant et al., 2023)(Fer-\nnando et al., 2024)(Prasad et al., 2023). Monte\nCarlo search methods start with a base prompt and\niterate over hundreds if not thousands of possible\nprompts in order to find the best performer, an inef-\nficient process. These methods use minimal feed-\nback on why their responses might be incorrect and\ninstead search somewhat blindly through prompt\nspace to identify the best possible prompt, mak-\ning small changes along the way. Feedback-based\nmethods, on the other hand, use ratings and ex-\nplanations of how their answer could be improved\nfrom previous iterations to generate new prompts.\nThey aim to emulate a traditional optimization pro-\ncess by iteratively refining prompts based on de-\ntailed feedback, closely mimicking the approach of\na human prompt engineer. As shown in (Yang et al.,\n2023), Monte Carlo methods take many iterations\nto converge, and thus are likely to be too costly\nfor most practitioners. Therefore, in approaching\nthe problem of automatic prompt engineering we\nchoose to focus on feedback-based methods.\nOne of the seminal works that inspired our\nfeedback-based approach was presented in \"Au-\ntomatic Prompt Optimization with \u2018Gradient De-\nscent' and Beam Search\" (Pryzant et al., 2023).\n'Gradients' in this context, refers to natural lan-\nguage descriptions of errors in LLM responses and\nsuggestions for improving the system's response\nin the future. The method presented in the afore-\nmentioned paper uses each of these gradients in-\ndividually to generate a new prompt. Their ap-\nproach addressed classification tasks such as hate\nspeech detection, lie detection, jailbreak detection,\nand sarcasm detection, using a restrictive matching\nevaluator where predictions must exactly match the\nexpected answers. As most LLM use cases do not\nhave a strict expected output we improve upon their"}, {"title": "Related Work", "content": "Customizing an LLM to a user's specific task can\nfit into two broad categories: parametric and non-\nparametric approaches. Parametric approach's usu-\nally consist of parameter efficient fine-tuning (Hu\net al., 2022) (Li and Liang, 2021) where only a por-"}, {"title": "Method", "content": "Our method consists of 5 distinct modules as seen\nin Figure 1: generation, evaluation, gradient gener-\nation, gradient summarization, and prompt editing.\nThroughout our optimization process, we utilize\nbeam search with a beam size of three candidate\nprompts, feeding each potential prompt to the mod-\nules introduced below. Figure 1 showcases one\niteration for one potential prompt."}, {"title": "Generation", "content": "The generation module necessitates a dataset, a\nprompt, and an LLM with which to generate out-\nputs. In usual workflows the prompt will consist\nof formatting 'slots' where the dataset will provide\ntext that will be used to fill in the slots as shown"}, {"title": "Evaluation", "content": "Our evaluation module then takes in the generations\nfrom the generation function, user-defined evalu-\nation criteria, and optionally an expected answer.\nThe module in turn returns a rating as to how well\nthe response performs as judged by the evaluation\ncriteria as well as an explanation for the rating. We\nask for the explanation before the evaluator module\nreturns a rating as chain of thought has been shown\nto dramatically improve LLM performance, espe-\ncially on reasoning tasks (Wei et al., 2022). For use\ncases with an expected answer, evaluation criteria\ncan be as simple as:\n\"Does the LLM generated response se-\nmantically match the expected answer?\nIf a reasonable human would judge it to,\ngive the response a 1, otherwise a 0.\"\nWe note that we only ask for a binary indicator\nas empirically we found LLM's are far better and\nmore consistent at binary indicators than a sliding\nnumeric scale."}, {"title": "Gradients", "content": "The gradient module receives the current prompt,\nthe evaluation criteria, a description of the task at\nhand, and a maximum of 5 generation responses\nthat received a rating of 0 from the evaluation mod-\nule. Note that any generation receiving a rating of\n1 in the evaluate step will be excluded from this\nprocess as it cannot be improved, thus it is possible\nthe gradient module will receive less than 5 inputs.\nAn LLM is then asked to evaluate the input and\noutput for the specific prompt and provide action-\nable methods for improving the prompts to address\ndeficiencies noted in the rating explanation. The\ngradient module leverages all available informa-\ntion-the task description, input to the generation\nfunction, the LLM response from the generate func-\ntion, evaluation criteria, and explanations for poor\nratings-to identify areas for improvement. Below\nis a sample of what a gradient will look like:\n\"The model should draw upon the en-\ntire context and state its reasoning explic-\nitly.\"\nThis step aims to emulate human evaluation of gen-\nerations, determining how to adjust the prompt to\nachieve the desired outcomes."}, {"title": "Gradient Summarization", "content": "While methods like (Pryzant et al., 2023) use\nthe gradients individually to then generate new\nprompts (ie. 5 gradients would lead to generat-\ning 5 new prompts), we find that this leads to\nprompts that are far too specific to certain ques-\ntions. Additionally, the task of evaluating these can-\ndidate prompts consumes API calls that can lead to\ncostly compute bills. Therefore, we found the most\neffective way of generating new general-purpose\nprompts was to summarize all gradient feedback\ninto one general paragraph that could apply to the\ndataset as whole. We feed the task description and\nthe gradients computed in the previous step to the\ngradient summarization module and use an LLM to\ngenerate a one-paragraph summary of the critiques\ntaking into consideration the task at hand. This\nstep can be thought of as analogous to averaging\ngradients over a mini-batch to stabilize training."}, {"title": "Prompt Editor", "content": "Our prompt editor module then takes in the current\nprompt, the summarized gradient, and the task de-\nscription and outputs a candidate prompt for each\nprompt within our beam (doubling our beam size\nmomentarily). The new prompts should likely ad-\ndress the critiques provided in the previous itera-\ntion. Each candidate prompt is checked to ensure\nevery slot in the current prompt is present in the\nnew prompt in order to avoid information loss of\nany kind. We then perform an evaluation of each\nnew candidate prompt. Specifically, we take a ran-\ndomly sampled subset (five rows) of the dataset\nand feed the responses to our generate and evalu-\nate modules, choosing the two candidate prompts\nwith the highest average rating. As our beam is\nnow 5 prompts wide we reduce to a beam size of 3\nthrough selecting the top 3 performers as given by\nthe upper confidence bound (Kuleshov and Precup,\n2014). This reduction step thus always retains the\nbest-performing prompt from previous iterations,"}, {"title": "Experiments & Results", "content": "In order to benchmark our method we utilize com-\nmon datasets: GSM8k (Cobbe et al., 2021), Orca\nMath (Mitra et al., 2024), Neural Bridge RAG (AI),\nHella Swag (Zellers et al., 2019), HotPot QA (Yang\net al., 2018), MMLU (Hendrycks et al., 2021) and\na mixture of the first turn from MT Bench and\nVicuna Bench (Zheng et al., 2023). We do not\nuse any expected answers in our MT Bench and\nVicuna Bench results, highlighting our use of user-\nprovided evaluation criteria extends our technique\nto datasets without expected answers. For each\ndataset if there is an available training dataset we\nextract 30 random samples for our training set and\n200 random samples from the validation or test\nset in order to simulate typical industry evaluation\nflows. If there is no available test or validation set\nwe sample 200 from the train set and hold them out\nduring training. 10 iterations are run over the train\nset with 10 rows being evaluated each iteration for\neach prompt (thus 30 calls for 3 candidate prompts)\nand the best prompt (the best performing prompt\nleft in our beam), is extracted and used in our fi-\nnal validation. All datasets used can found on our\nGitHub.\u00b9 Seeing as (Pryzant et al., 2023) is not di-\nrectly comparable to our approach on these bench-\nmarks due to their classification / exact-matching\nframework2, we instead compare to the popular\nprompt optimizer, DSPY (Khattab et al., 2023). As\nDSPY is an abstracted prompting tool that does not\nallow users to directly control the initial prompt,\nwe extracted DSPY's initial prompt and used it\nas the starting point for our method to ensure a\nfair comparison. The same evaluation criteria is\nused with DSPY and we create a custom LLM-as-\na-judge metric within their system in order to make\nour methods as comparable as possible. We uti-\nlize the DSPY COPRO optimizer and allow for 10\npasses over the dataset to ensure a fair comparison.\nFor our method we conduct evaluation, gradient\ngeneration and summarization with GPT 3.5 ('gpt-\n3.5-turbo-0125'). Prompt editing is performed with\nGPT40 ('gpt-4o-2024-05-13') in order to ensure\nhigh-quality new prompts. The initial prompts, the\nfinal prompts, the task descriptions and evaluation\ncriteria used for the respective tasks are provided\nin the appendix.\nAs you can see in Table 2 our method improves\nand outperforms DSPY on all use cases. We av-\nerage an improvement of 14% over our initial\nprompts, showcasing the robustness and strength\nof our optimizer."}, {"title": "Gradient Summarization", "content": "We also conduct an\nablation where we remove the summarization mod-\nule in order to assess it's efficacy. Removing the\nsummarization module is analogous to comparing\nour method with (Pryzant et al., 2023) after extend-\ning their method to use LLM-as-a-judge evalua-\ntion. We note an average validation increase of 5%\nwith the introduction of the summarization module.\nUpon examining the HotPot QA final prompt, it\nbecame obvious the prompt became far too specific\nto single gradients found throughout the training\nloop. Below is an extract from the final HotPot QA\nprompt that clearly attaches to information relevant\nto individual examples:\n\"Reasoning: Let's think step by step to\nlogically deduce the answer. Focus on\nthe relevant information in the context to\narrive at the correct answer. Ensure you\nconsider all details, specific terms, and\nnames mentioned in the context, such as\nthe regiment within the Guards Division,\nthe term 'shorts,' the founding year of\nImage Comics, middle names, the spe-\ncific crime Brian Nichols was on trial for,\nand the full names of individuals. Pro-\nvide a comprehensive response covering\nall individuals mentioned in the context\nwhere applicable.\"\nThus, we find that the gradient summarization\nmodule is essential for maintaining high-quality\nprompts that can generalize to a validation dataset."}, {"title": "Conclusion", "content": "We introduce a scalable, flexible prompt optimiza-\ntion framework that improves LLM performance\nby an average of 14% across many popular datasets.\nIn all scenarios our framework is also able to find\na better prompt than our initial prompt. Finally,\nour framework can be used across many of sce-\nnarios including those without expected answers,\nillustrating the flexibility of our framework. By\nautomating the prompt engineering process and\ndemonstrating consistent performance gains across\nmultiple datasets and models, our work enables\nefficient, flexible and scalable utilization of LLMs\nin real-world applications, reducing the time and\neffort required to achieve high-quality outputs."}, {"title": "Limitations", "content": "Our system currently supports only LLM-as-a-\njudge metrics. While these metrics are effective in\nmany scenarios, they may not be suitable for all\ntasks. Expanding the system's capability to sup-\nport any user-defined metric, including numerical\nand domain-specific metrics, is left for future work.\nThe use of LLMs for evaluation also introduces\npotential biases inherent in the models themselves.\nThis has the potential to affect the accuracy of the\nevaluation process. Finally, there is still a depen-\ndence on the user to define the task descriptions\nand evaluation criteria, which have a large impact\non final prompt quality. Streamlining these aspects\nto require minimal user input while maintaining\neffectiveness remains a challenge."}, {"title": "Prompt Optimization Results", "content": "For each dataset we list out the initial prompt, the\ntask description, the evaluation criteria and the final\nprompt obtained. We provide the final prompt for\nall methods: GPT 3.5 with DSPY, and GPT 3.5\nwith our method."}, {"title": "GSM8k", "content": "Task Description:\nThe task is answering grade school math questions.\nEvaluation Criteria:\nDoes the output align with the expected answer?\nThe questions are math questions. Check if the\nanswer matches the expected answer. Give it a\n1 if a math teacher would consider the answer\ncorrect. Give it a 0 if the answer is incorrect. Do\nnot worry about intermediate calculations, only the\nfinal answer.\nInitial prompt for our method & DSPY:\nThe task is to answer math questions."}, {"title": "Orca Math", "content": "Task Description:\nThe task involves solving math word problems.\nEvaluation Criteria:\nDoes the output align with the expected answer?\nThe questions are math questions. Check if the\nanswer matches the expected answer. Give it a\n1 if a math teacher would consider the answer\ncorrect. Give it a 0 if the answer is incorrect. Do\nnot worry about intermediate calculations, only the\nfinal answer.\nInitial prompt for our method & DSPY:\nThe task is to answer math questions."}, {"title": "Neural Bridge RAG", "content": "Task Description:\nThe task is a question answering task given specific\ncontext that should have the answer.\nEvaluation Criteria:\nDoes the llm output match the expected answer?\nIf the model says it does not have enough context\nto answer the question give it a 0. Otherwise\njudge whether a human would grade the output as"}, {"title": "HellaSwag", "content": "Task Description:\nThe task is to complete a sentence with the most\nlogical of 4 possible options.\nEvaluation Criteria:\nDoes the output match the the expected answer? If\nit does give it a 1. If it does not give it a 0. It does\nnot have to match exactly but it should be close\nenough that a reasonable human would consider\nthe output to match the expected answer. Make\nsure that the chosen completion is the correct\ncompletion.\nInitial prompt for our method & DSPY:\nThe task is to complete a sentence with the most\nlogical of 4 possible options."}, {"title": "Hotpot QA", "content": "Task Description:\nThe task is to reason over context given a question"}, {"title": "MMLU", "content": "Task Description:\nThe task is to choose the correct answer from a\nlist of possible answers on a variety of knowledge\nquestions.\nEvaluation Criteria:\nDoes the output match the the expected answer? If\nit does give it a 1. If it does not give it a 0. It does\nnot have to match exactly but it should be close\nenough that a reasonable human would consider\nthe output to match the expected answer. Make\nsure that the chosen completion is the correct\ncompletion."}, {"title": "MT & Vicuna Bench", "content": "Task Description:\nThe task is to be a chat bot assistant that provides\nhelpful answers to questions.\nEvaluation Criteria:\nAct as an impartial judge and evaluate the quality\nof the response provided by an AI assistant\nto the user question displayed below. Your\nevaluation should consider how helpful, thoughtful,\ninformative and thorough an answer is. Only give\nperfect answers a 1.\nInitial Prompt for our method & DSPY:\nThe task is to be a chat bot assistant that provides\nhelpful answers to questions."}]}