{"title": "GRAD-SUM: Leveraging Gradient Summarization for Optimal Prompt Engineering", "authors": ["Derek Austin", "Elliott Chartock"], "abstract": "Prompt engineering for large language models (LLMs) is often a manual time-intensive process that involves generating, evaluating, and refining prompts iteratively to ensure high-quality outputs. While there has been work on automating prompt engineering, the solutions generally are either tuned to specific tasks with given answers or are quite costly. We introduce GRAD-SUM, a scalable and flexible method for automatic prompt engineering that builds on gradient-based optimization techniques. Our approach incorporates user-defined task descriptions and evaluation criteria, and features a novel gradient summarization module to generalize feedback effectively. Our results demonstrate that GRAD-SUM consistently outperforms existing methods across various benchmarks, highlighting its versatility and effectiveness in automatic prompt optimization.", "sections": [{"title": "Introduction", "content": "The introduction of machine learning and artificial intelligence has automated many tasks, but creating and refining prompts for LLM applications remains largely a manual process. Practitioners often struggle to find the right prompt to elicit accurate and high-quality outputs. Common practice is to judge output quality by implementing fully automated evaluation feedback loops, where LLMs assess outputs based on user-provided evaluation criteria, these are often called LLM-as-a-judge methods. Seeing as evaluation is an automatic process in LLM-as-a-judge scenarios, the main bottleneck in automating prompt engineering is the actual refinement of the prompt. Although there have been attempts to automate this process (Yang et al., 2023) (Pryzant et al., 2023) (Zhou et al., 2023), we found that these methods either could not be adapted seamlessly to all tasks or were too costly to implement at scale.\nAutomatic prompt optimization methods broadly fall into two categories: Monte Carlo search methods (Yang et al., 2023) and feedback-based (or gradient-based) methods (Pryzant et al., 2023)(Fernando et al., 2024)(Prasad et al., 2023). Monte Carlo search methods start with a base prompt and iterate over hundreds if not thousands of possible prompts in order to find the best performer, an inefficient process. These methods use minimal feedback on why their responses might be incorrect and instead search somewhat blindly through prompt space to identify the best possible prompt, making small changes along the way. Feedback-based methods, on the other hand, use ratings and explanations of how their answer could be improved from previous iterations to generate new prompts. They aim to emulate a traditional optimization process by iteratively refining prompts based on detailed feedback, closely mimicking the approach of a human prompt engineer. As shown in (Yang et al., 2023), Monte Carlo methods take many iterations to converge, and thus are likely to be too costly for most practitioners. Therefore, in approaching the problem of automatic prompt engineering we choose to focus on feedback-based methods.\nOne of the seminal works that inspired our feedback-based approach was presented in \"Automatic Prompt Optimization with \u2018Gradient Descent' and Beam Search\" (Pryzant et al., 2023). 'Gradients' in this context, refers to natural language descriptions of errors in LLM responses and suggestions for improving the system's response in the future. The method presented in the aforementioned paper uses each of these gradients individually to generate a new prompt. Their approach addressed classification tasks such as hate speech detection, lie detection, jailbreak detection, and sarcasm detection, using a restrictive matching evaluator where predictions must exactly match the expected answers. As most LLM use cases do not have a strict expected output we improve upon their work by introducing LLMs-as-a-judge evaluations which enable accurate judgment of any response for all possible tasks.\nOur approach, GRAD-SUM, introduces a task description input that allows models to incorporate high-level, domain-specific information provided by the user. Additionally, we introduce natural language evaluation criteria also provided by the user, enabling our method to use LLM-as-a-judge metrics, extending (Pryzant et al., 2023) method to all possible use cases. We also improve upon (Pryzant et al., 2023) method, by introducing a new gradient summarization module after discovering that editing the prompt based on feedback from a single output, as done in (Pryzant et al., 2023) often resulted in highly specific candidates prompts that failed to generalize to the broader dataset population. To address this, our gradient summarization module takes all computed gradients and produces a generalized summary of the feedback, akin to averaging gradients over many samples, a common practice in machine learning. Overall, our approach combines a novel gradient summarization module, user-provided task descriptions and evaluation criteria to create a more flexible, scalable and performant solution for automated prompt engineering than any existing today."}, {"title": "Related Work", "content": "Customizing an LLM to a user's specific task can fit into two broad categories: parametric and non-parametric approaches. Parametric approach's usually consist of parameter efficient fine-tuning (Hu et al., 2022) (Li and Liang, 2021) where only a portion of parameters are updated based on backpropagating gradients. Prefix tuning introduces new learnable tokens that one can think of as a prompt in continuous space, that are learned for specific tasks (Li and Liang, 2021). However, as these tokens defined in continuous space they are not easily interpretable. Other approaches like (Shin et al., 2020) backpropagate gradients and choose the best discrete prompts. However, even this technique suffers from interpretability as most prompts are not grammatically correct or coherent sentences.\nMost practitioners currently rely on black box API based LLM's for language generation where there is no potential for backpropagation of any kind (Brown et al., 2020). Therefore, prompt optimization becomes a discrete optimization problem over an extraordinarily large search space. Some methods attempt to tackle this problem through the use of complementary models. (Deng et al., 2022) attempt to use a policy network that receives rewards from an evaluator function in order to update a discrete prompt. (Chen et al., 2023) uses an open source model to convert a soft-prompt defined in embedding space into a discrete prompt that was then fed to the LLM API. Others use evolutionary algorithms in order to modify, delete and add a list of candidate prompts that are evaluated each iteration (Fernando et al., 2024) (Prasad et al., 2023). Simple Monte Carlo sampling around candidate prompts (Yang et al., 2023) can complement these methods or be implemented entirely on their own. Most of these methods attempt to use some sort of signal from an evaluator LLM in order to asses the quality of the output. However, it has been shown that LLMs are not great at self-correction without use of auxiliary knowledge or tools (Stechly et al., 2023) (Ma et al., 2024) (Huang et al., 2024) (Gou et al., 2024). Therefore the introduction of specific evaluation criteria, as in our method, the use of tools, as in (Gou et al., 2024), or modifying LLM critiques to better find the root of the errors (Ma et al., 2024) becomes of paramount importance to finding an optimized prompt."}, {"title": "Method", "content": "Our method consists of 5 distinct modules as seen in Figure 1: generation, evaluation, gradient generation, gradient summarization, and prompt editing. Throughout our optimization process, we utilize beam search with a beam size of three candidate prompts, feeding each potential prompt to the modules introduced below. Figure 1 showcases one iteration for one potential prompt."}, {"title": "Generation", "content": "The generation module necessitates a dataset, a prompt, and an LLM with which to generate outputs. In usual workflows the prompt will consist of formatting 'slots' where the dataset will provide text that will be used to fill in the slots as shown"}, {"title": "Evaluation", "content": "Our evaluation module then takes in the generations from the generation function, user-defined evaluation criteria, and optionally an expected answer. The module in turn returns a rating as to how well the response performs as judged by the evaluation criteria as well as an explanation for the rating. We ask for the explanation before the evaluator module returns a rating as chain of thought has been shown to dramatically improve LLM performance, especially on reasoning tasks (Wei et al., 2022). For use cases with an expected answer, evaluation criteria can be as simple as:\n\"Does the LLM generated response semantically match the expected answer?\nIf a reasonable human would judge it to,\ngive the response a 1, otherwise a 0.\"\nWe note that we only ask for a binary indicator as empirically we found LLM's are far better and more consistent at binary indicators than a sliding numeric scale."}, {"title": "Gradients", "content": "The gradient module receives the current prompt, the evaluation criteria, a description of the task at hand, and a maximum of 5 generation responses that received a rating of 0 from the evaluation module. Note that any generation receiving a rating of 1 in the evaluate step will be excluded from this process as it cannot be improved, thus it is possible the gradient module will receive less than 5 inputs. An LLM is then asked to evaluate the input and output for the specific prompt and provide actionable methods for improving the prompts to address deficiencies noted in the rating explanation. The gradient module leverages all available information-the task description, input to the generation function, the LLM response from the generate function, evaluation criteria, and explanations for poor ratings-to identify areas for improvement. Below is a sample of what a gradient will look like:\n\"The model should draw upon the entire context and state its reasoning explicitly.\"\nThis step aims to emulate human evaluation of generations, determining how to adjust the prompt to achieve the desired outcomes."}, {"title": "Gradient Summarization", "content": "While methods like (Pryzant et al., 2023) use the gradients individually to then generate new prompts (ie. 5 gradients would lead to generating 5 new prompts), we find that this leads to prompts that are far too specific to certain questions. Additionally, the task of evaluating these candidate prompts consumes API calls that can lead to costly compute bills. Therefore, we found the most effective way of generating new general-purpose prompts was to summarize all gradient feedback into one general paragraph that could apply to the dataset as whole. We feed the task description and the gradients computed in the previous step to the gradient summarization module and use an LLM to generate a one-paragraph summary of the critiques taking into consideration the task at hand. This step can be thought of as analogous to averaging gradients over a mini-batch to stabilize training."}, {"title": "Prompt Editor", "content": "Our prompt editor module then takes in the current prompt, the summarized gradient, and the task description and outputs a candidate prompt for each prompt within our beam (doubling our beam size momentarily). The new prompts should likely address the critiques provided in the previous iteration. Each candidate prompt is checked to ensure every slot in the current prompt is present in the new prompt in order to avoid information loss of any kind. We then perform an evaluation of each new candidate prompt. Specifically, we take a randomly sampled subset (five rows) of the dataset and feed the responses to our generate and evaluate modules, choosing the two candidate prompts with the highest average rating. As our beam is now 5 prompts wide we reduce to a beam size of 3 through selecting the top 3 performers as given by the upper confidence bound (Kuleshov and Precup, 2014). This reduction step thus always retains the best-performing prompt from previous iterations, reducing the variance from evaluating candidate prompts on a small sample size."}, {"title": "Experiments & Results", "content": "In order to benchmark our method we utilize common datasets: GSM8k (Cobbe et al., 2021), Orca Math (Mitra et al., 2024), Neural Bridge RAG (AI), Hella Swag (Zellers et al., 2019), HotPot QA (Yang et al., 2018), MMLU (Hendrycks et al., 2021) and a mixture of the first turn from MT Bench and Vicuna Bench (Zheng et al., 2023). We do not use any expected answers in our MT Bench and Vicuna Bench results, highlighting our use of user-provided evaluation criteria extends our technique to datasets without expected answers. For each dataset if there is an available training dataset we extract 30 random samples for our training set and 200 random samples from the validation or test set in order to simulate typical industry evaluation flows. If there is no available test or validation set we sample 200 from the train set and hold them out during training. 10 iterations are run over the train set with 10 rows being evaluated each iteration for each prompt (thus 30 calls for 3 candidate prompts) and the best prompt (the best performing prompt left in our beam), is extracted and used in our final validation. All datasets used can found on our GitHub.\u00b9 Seeing as (Pryzant et al., 2023) is not directly comparable to our approach on these benchmarks due to their classification / exact-matching framework\u00b2, we instead compare to the popular prompt optimizer, DSPY (Khattab et al., 2023). As DSPY is an abstracted prompting tool that does not allow users to directly control the initial prompt, we extracted DSPY's initial prompt and used it as the starting point for our method to ensure a fair comparison. The same evaluation criteria is used with DSPY and we create a custom LLM-as-a-judge metric within their system in order to make our methods as comparable as possible. We utilize the DSPY COPRO optimizer and allow for 10 passes over the dataset to ensure a fair comparison. For our method we conduct evaluation, gradient generation and summarization with GPT 3.5 ('gpt-3.5-turbo-0125'). Prompt editing is performed with GPT4o ('gpt-4o-2024-05-13') in order to ensure high-quality new prompts. The initial prompts, the final prompts, the task descriptions and evaluation criteria used for the respective tasks are provided in the appendix.\nAs you can see in Table 2 our method improves and outperforms DSPY on all use cases. We average an improvement of 14% over our initial prompts, showcasing the robustness and strength of our optimizer."}, {"title": "Gradient Summarization", "content": "We also conduct an ablation where we remove the summarization module in order to assess it's efficacy. Removing the summarization module is analogous to comparing our method with (Pryzant et al., 2023) after extending their method to use LLM-as-a-judge evaluation. We note an average validation increase of 5% with the introduction of the summarization module. Upon examining the HotPot QA final prompt, it became obvious the prompt became far too specific to single gradients found throughout the training loop. Below is an extract from the final HotPot QA prompt that clearly attaches to information relevant to individual examples:\n\"Reasoning: Let's think step by step to logically deduce the answer. Focus on the relevant information in the context to arrive at the correct answer. Ensure you consider all details, specific terms, and names mentioned in the context, such as the regiment within the Guards Division, the term 'shorts,' the founding year of Image Comics, middle names, the specific crime Brian Nichols was on trial for, and the full names of individuals. Provide a comprehensive response covering all individuals mentioned in the context where applicable.\"\nThus, we find that the gradient summarization module is essential for maintaining high-quality prompts that can generalize to a validation dataset."}, {"title": "Conclusion", "content": "We introduce a scalable, flexible prompt optimization framework that improves LLM performance by an average of 14% across many popular datasets. In all scenarios our framework is also able to find a better prompt than our initial prompt. Finally, our framework can be used across many of scenarios including those without expected answers, illustrating the flexibility of our framework. By automating the prompt engineering process and demonstrating consistent performance gains across multiple datasets and models, our work enables efficient, flexible and scalable utilization of LLMs in real-world applications, reducing the time and effort required to achieve high-quality outputs."}, {"title": "Limitations", "content": "Our system currently supports only LLM-as-a-judge metrics. While these metrics are effective in many scenarios, they may not be suitable for all tasks. Expanding the system's capability to support any user-defined metric, including numerical and domain-specific metrics, is left for future work. The use of LLMs for evaluation also introduces potential biases inherent in the models themselves. This has the potential to affect the accuracy of the evaluation process. Finally, there is still a dependence on the user to define the task descriptions and evaluation criteria, which have a large impact on final prompt quality. Streamlining these aspects to require minimal user input while maintaining effectiveness remains a challenge."}]}