{"title": "Large Language Model-Enhanced Multi-Armed Bandits", "authors": ["Jiahang Sun", "Zhiyong Wang", "Runhan Yang", "Chenjun Xiao", "John C.S. Lui", "Zhongxiang Dai"], "abstract": "Large language models (LLMs) have been\nadopted to solve sequential decision-making tasks\nsuch as multi-armed bandits (MAB), in which an\nLLM is directly instructed to select the arms to\npull in every iteration. However, this paradigm of\ndirect arm selection using LLMs has been shown\nto be suboptimal in many MAB tasks. There-\nfore, we propose an alternative approach which\ncombines the strengths of classical MAB and\nLLMs. Specifically, we adopt a classical MAB\nalgorithm as the high-level framework and lever-\nage the strong in-context learning capability of\nLLMs to perform the sub-task of reward predic-\ntion. Firstly, we incorporate the LLM-based re-\nward predictor into the classical Thompson sam-\npling (TS) algorithm and adopt a decaying sched-\nule for the LLM temperature to ensure a transition\nfrom exploration to exploitation. Next, we incor-\nporate the LLM-based reward predictor (with a\ntemperature of 0) into a regression oracle-based\nMAB algorithm equipped with an explicit explo-\nration mechanism. We also extend our TS-based\nalgorithm to dueling bandits where only the pref-\nerence feedback between pairs of arms is avail-\nable, which requires non-trivial algorithmic modi-\nfications. We conduct empirical evaluations using\nboth synthetic MAB tasks and experiments de-\nsigned using real-world text datasets, in which\nthe results show that our algorithms consistently\noutperform previous baseline methods based on\ndirect arm selection. Interestingly, we also demon-\nstrate that in challenging tasks where the arms\nlack semantic meanings that can be exploited by\nthe LLM, our approach achieves considerably bet-\nter performance than LLM-based direct arm se-\nlection.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have demonstrated impres-\nsive capabilities in various tasks (OpenAI, 2023a;b; Liu\net al., 2024a). As a result, many recent works have lever-\naged LLMs as agents to solve real-world sequential decision-\nmaking tasks. Specifically, some recent works have adopted\npowerful pre-trained LLMs to solve multi-armed bandit\n(MAB) problems (Krishnamurthy et al., 2024; Chen et al.,\n2024; Xia et al., 2024; Mukherjee et al., 2024). These works\nusually directly instruct a pre-trained LLM to select the next\narm to pull and do not require the costly LLM fine-tuning.\nHowever, this paradigm has been demonstrated to lead to\nsub-optimal MAB algorithms in many scenarios (Krishna-\nmurthy et al., 2024). In other words, it has been observed\nthat directly using an LLM for arm selection often struggles\nto explore efficiently in real-world environments. To this\nend, we propose an alternative paradigm which combines\nclassical MAB algorithms with LLMs such that we can\nachieve the best of both worlds. Specifically, we leverage a\nclassical MAB algorithm as the high-level framework, and\nadopt a pre-trained LLM (without fine-tuning) to perform\nthe sub-task of reward prediction based on the history of (the\nfeatures of) the selected arms and their observed rewards.\nCompared to the previous approach of directly employing\nan LLM for arm selection (Krishnamurthy et al., 2024), this\nallows us to leverage the strength of LLMs in in-context\nlearning (ICL) to solve prediction (i.e., supervised learning)\ntasks. In other words, instead of using an LLM to replace the\nMAB algorithm, we leverage LLMs to enhance classical\nMAB algorithms.\nWe further motivate our approach by drawing analogy to\nrecent works aiming to improve the performance of LLMs\nin complex reasoning tasks via tree search methods (Hao\net al., 2023; Yao et al., 2024; Zhang et al., 2024; Bi et al.,\n2024). Specifically, these methods often adopt a classi-\ncal tree search algorithm as the high-level framework (e.g.,\nMonte-Carlo tree search), and use LLMs to perform dif-\nferent sub-tasks such as reward/value prediction, action\ngeneration, etc. Therefore, their overall paradigm aligns\nwith our approach of using classical algorithms to guide the\nhigh-level decision-making while leveraging the strengths\nof LLMs in performing some sub-tasks. For example, the\nwork of Koh et al. (2024) has also used a pre-trained LLM\nfor reward prediction based on the past history to improve"}, {"title": "2. Problem Setting", "content": "In our problem setting, every arm $i = 1, ..., K$ is associated\nwith a d-dimensional feature vector $x_i \\in \\mathbb{R}^d$ and the reward\nof an arm $i$ is a function of its feature vector $x_i$: $f(x_i)$.\nFor example, in the classical linear bandits, the reward of\narm $i$ is given by a linear function: $f(x_i) = \\theta^T x_i$ with an\nunknown $\\theta$. In every iteration $t$, an MAB algorithm selects\nan arm $i_t$ to pull, and observes a corresponding noisy reward\n$Y_t = f(x_{i_t}) + \\epsilon$ where $\\epsilon$ is usually a zero-mean Gaussian\nnoise. The goal of an MAB algorithm is usually to minimize\nthe cumulative regret: $R_T = \\sum_{t=1}^T [f(x_{i^*}) - f(x_{i_t})]$ where\n$i^* = \\arg \\max_{i=1,...,K} f(x_i)$ represents the optimal arm.\nWe also consider the setting of contextual MAB (Sec. 4.3),\nin which in every iteration $t$, we receive a new set of $K$ arms\ndenoted as $I_t = \\{i_1, ..., i_K\\}$ and choose an arm $i_t$ from"}, {"title": "2.2. Dueling Bandits", "content": "In dueling bandits, in every iteration $t$, we select a pair of\narms $i_{t,1}$ and $i_{t,2}$ and observe binary preference feedback\n$r_t = 1(i_{t,1} \\succ i_{t,2})$, which is equal to 1 if $i_{t,1}$ is preferred\nover $i_{t,2}$ and 0 otherwise. We assume that the preference\nobservation $r_t$ is generated by the commonly adopted BTL\nmodel (Luce, 2005; Hunter, 2004). Specifically, there exists\na latent reward function $f$ which maps the feature vector $x_i$\nof an arm $i$ to its corresponding latent reward value $f(x_i)$.\nFor a pair of arms $i_{t,1}$ and $i_{t,2}$, the preference probability\n(i.e., the probability that arm $i_{t,1}$ is preferred over arm $i_{t,2}$)\nunder the BTL model is given by\n$P(i_{t,1} \\succ i_{t,2}) = \\mu(f(x_{i_{t,1}}) - f(x_{i_{t,2}})),$\nin which $\\mu : \\mathbb{R} \\rightarrow [0, 1]$ is the logistic function: $\\mu(z) =$\n$1/(1+e^{-z})$. The preference observation $r_t = 1(i_{t,1} \\succ i_{t,2})$\nis then assumed to be sampled from a Bernoulli distri-\nbution with the probability $P(i_{t,1} \\succ i_{t,2})$. The perfor-\nmance of a dueling bandit algorithm is also often mea-\nsured by regret. A common notion of regret is $R_T =$\n$\\sum_{t=1}^T[2f(x_{i^*}) - f(x_{i_{t,1}}) - f(x_{i_{t,2}})]$. However, in prac-\ntical applications, we usually need to devise a method to\nrecommend an arm during the dueling bandit algorithm\n(Lin et al., 2024). Our LLM-based algorithm for duel-\ning bandits recommends the first selected arm $i_{t,1}$ as the\nbest arm (more details in Sec. 3.3). Therefore, in our ex-\nperiments (Sec. 4.2), we report the regret of the first arm:\n$R_T = \\sum_{t=1}^T[f(x_{i^*}) - f(x_{i_{t,1}})]$, which we believe is more\nrelevant in practice."}, {"title": "3. LLM-Enhanced MAB Algorithms", "content": null}, {"title": "3.1. Thompson Sampling with LLM (TS-LLM)", "content": "Our TS-LLM algorithm (Algo. 1) employs the LLM to pre-\ndict the reward of every arm and leverages the inherent\nrandomness in the LLM-generated text to achieve explo-"}, {"title": "3.2. Regression Oracle-Based Bandit with LLM\n(RO-LLM)", "content": "A line of works have proposed to adopt a generic regres-\nsion oracle for reward prediction in MAB, and incorporated\nexplicit exploration mechanisms to derive theoretically prin-"}, {"title": "3.3. Thompson Sampling with LLM for Dueling Bandits\n(TS-LLM-DB)", "content": "Here we introduce our TS-LLM-DB algorithm for dueling\nbandit, in which we select a pair of arms $i_{t,1}$ and $i_{t,2}$ in\nevery iteration and collect a binary observation indicating\ntheir relative preference $r_t = 1(i_{t,1} \\succ i_{t,2})$.\nIn contrast to\nour TS-LLM (Algo. 1) and RO-LLM (Algo. 2) which\nuse an LLM to predict the reward of every arm, our"}, {"title": "4. Experiments", "content": "We firstly apply our TS-LLM and RO-LLM algorithms to\nsynthetic stochastic MAB tasks with both linear and non-\nlinear reward functions (Sec. 4.1). Next, we apply our\nTS-LLM-DB algorithm to solve synthetic dueling bandit\nproblems (Sec. 4.2). Lastly, we adopt MAB tasks designed\nusing two real-world text datasets (Sec. 4.3) to unveil some\ninteresting insights about our algorithms. We adopt GPT-\n3.5-Turbo (OpenAI, 2023a) as the black-box LLM in the\nmajority of our experiments, and also use DeepSeek-V3\n(Liu et al., 2024a) in the experiments in (Sec. 4.3)."}, {"title": "4.1. TS-LLM and RO-LLM for Classical Stochastic MAB", "content": "Here we compare our TS-LLM and RO-LLM algorithms\nwith some baseline algorithms from the work of Krishna-\nmurthy et al. (2024). Specifically, we adopt the best prompt"}, {"title": "4.2. Dueling Bandits", "content": "Here we apply our TS-LLM-DB algorithm to solve dueling\nbandit problems with two different latent reward functions\n$f$: a linear function and a square function. Same as the\nexperiments in Sec. 4.1, we also let $d = 4$ and $K = 16$. In\nour experiments here, when selecting the first arm, we use\n$N = 15$ uniformly sampled arms to approximate the Borda\nfunction (Sec. 3.3). Similar to the experiments on classi-\ncal stochastic MAB (Sec. 4.1), we also adopt a decaying"}, {"title": "4.3. Real-World Datasets with Text Features", "content": "Here we perform experiments using two real-world text\ndataset: the OneShotWikiLinks dataset (Singh et al.,\n2012; Vasnetsov, 2018) and the AmazonCat-13K dataset\n(Bhatia et al., 2016), both of which have been widely used\nin previous works on contextual bandits (Chen et al., 2024).\nThe One ShotWikiLinks dataset (Singh et al., 2012; Vas-\nnetsov, 2018) is a named-entity recognition task in which\nthe contexts consist of text phrases surrounding the mention\ntext (both preceding and following it), and the arms are text\nphrases representing concept names. AmazonCat-13K\n(Bhatia et al., 2016) is an extreme multi-label dataset where\nthe contexts are text phrases derived from the title and con-\ntent of an item, and the arms are integers representing item"}, {"title": "5. Ablation Study", "content": null}, {"title": "5.1. Impact of Different Temperatures", "content": "Here we investigate the impact of the temperature of the\nLLM on the performance of our TS-LLM (Algo. 1). As we\nhave discussed in Sec. 3.1, we adopt a decaying schedule\nfor the LLM temperature to ensure a transition from explo-\nration to exploitation. We follow the same experimental\nsetting as Sec. 4.1 and adopt the linear reward function. The\nresults in Fig. 5 show that the best performance is achieved\nby adopting decaying LLM temperatures, whereas fixing\nthe temperature to various values leads to inferior perfor-"}, {"title": "5.2. Impact of the Number N of Samples When\nSelecting the First Arm in TS-LLM-DB", "content": "Recall that our TS-LLM-DB algorithm selects the first arm\nby approximately maximizing the Borda function $f_{borda}$\n(Sec. 3.3), in which we use $N$ randomly sampled arms to\napproximate the expectation in $f_{borda}$ (lines 3-5 of Algo. 3).\nFig. 6 presents the results of our TS-LLM-DB with different\nvalues of $N$, which demonstrate that a larger $N$ improves\nthe performance because it leads to a better approximation\nof $f_{borda}$. However, also note that the use of a larger $N$\nincreases the number of API calls to the LLM and hence\nincurs more cost. Therefore, in practice, the value of $N$\nshould be selected based on the trade-off between the de-"}, {"title": "5.3. Impact of The Exploration Parameter in Our\nRO-LLM Algorithm", "content": "The parameter $\\gamma$ in our RO-LLM can be used to control\nthe degree of exploration. As can be seen from lines 4-7\nof Algo. 2, a larger value of $\\gamma$ results in a larger weight\n(in the arm sampling distribution $p_t$) on the arm $j_t$ that is\npredicted to be the best arm. Therefore, a larger $\\gamma$ leads to\ngreater emphasis on exploitation, thereby reducing the focus\non exploration. Here we test three values of $\\gamma$ and display"}, {"title": "6. Related Work", "content": "The work\nof Krishnamurthy et al. (2024) has used an LLM to sequen-\ntially choose the arms in MAB. They have consider standard\nMAB problems with a finite number of arms, and their re-\nsults have shown that LLMs struggle in MAB tasks in most\nscenarios (i.e., for most of their prompt designs). More re-\ncently, the work of (Chen et al., 2024) has proposed to adopt\nan LLM-based arm selection strategy in the initial stage of\nMAB and gradually switch to classical MAB algorithms in\nlater stages. However, their method requires the availability\nof the likelihood of the LLMs and are hence not able to\nadopt the typically more powerful black-box LLMs such as"}, {"title": "7. Conclusion", "content": "In this work, we propose an alternative paradigm of LLM-\nbased sequential decision-making and focus on the MAB\nproblem. We adopt a classical MAB algorithm as the high-\nlevel framework and leverage the strong in-context learning\ncapability of LLMs to perform the sub-task of reward pre-\ndiction in MAB. We propose our TS-LLM and RO-LLM for\nclassical stochastic MAB and our TS-LLM-DB for dueling\nbandits. Synthetic experiments demonstrate that our algo-\nrithms consistently outperform baseline methods of LLM-\nbased direct arm selection. Through contextual MAB exper-\niments designed using two real-world text datasets, we show\nthat in challenging tasks where the arm features are not as-\nsociated with semantic meanings exploitable by the LLM,\nour TS-LLM achieves dramatically better performance than\nLLM-based direct arm selection. As future work, we plan to\napply our algorithms to handle more complicated sequential\ndecision-making problems, such as those from commonly\nused benchmarks for LLM-based agents such as Liu et al."}, {"title": "Impact Statements", "content": "This paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here."}, {"title": "A. More Experimental Details", "content": "In all our synthetic experiments (Secs. 4.1 and 4.2), the MAB tasks have $K = 16$ features and the feature vectors of the\narms are 4-dimensional."}, {"title": "A.1. The Prompt Template Adopted by Our Algorithms", "content": "Below is the prompt we have used for our TS-LLM algorithm (Algo. 1) and RO-LLM algorithm (Algo. 2) in classical\nstochastic bandits experiment in Sec. 4.1. Here every [INPUT] contains the feature vectors of an arm, and every [OUTPUT]\ncorresponds to its corresponding observed reward."}, {"title": "A.2. More Details on The Synthetic Experiments (Secs. 4.1 and 4.2)", "content": "In our synthetic experiments in Sec. 4.1, we have adopted synthetic functions as the reward functions $f$, including linear\nfunction: $f(x) = \\theta^T x$, square function: $f(x) = (\\theta^T x)^2$, sinusoidal function: $f(x) = sin(\\theta^T x)$, and a function sampled\nfrom a Gaussian process with a length scale of 0.4. We repeat each experiment 10 times with a different random seed for\neach repetition. We run each method for 100 iterations, with the initial 2 arms randomly selected. We add a Gaussian noise"}, {"title": "A.3. More Details about the Baseline Algorithms", "content": "Here we present the prompts we have used for different baseline algorithms we have used in Sec. 4.1. Specifically, how we\nhave modified the prompt from the LLM-based MAB method from (Krishnamurthy et al., 2024) in different ways in order\nto incorporate the features of the arms, to make their method comparable with our algorithms. We have highlighted the arm"}, {"title": "A.4. More Details on the Text Experiments", "content": "Here we present more details on the experiment in Sec. 4.3 in which we have adopted a real-world text dataset. Every\nexperiment in this section is repeated 10 times with a different random seed in every repetition.\nIn the experiment using the OneShotWikiLinks dataset, contexts exceeding 400 words were first removed. Then, 10"}]}