{"title": "Automating a Complete Software Test Process Using LLMs: An Automotive Case Study", "authors": ["Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "abstract": "Vehicle API testing verifies whether the interactions between a vehicle's internal systems and external applications meet expectations, ensuring that users can access and con- trol various vehicle functions and data. However, this task is inherently complex, requiring the alignment and coordination of API systems, communication protocols, and even vehicle simulation systems to develop valid test cases. In practical industrial scenarios, inconsistencies, ambiguities, and interde- pendencies across various documents and system specifications pose significant challenges. This paper presents a system designed for the automated testing of in-vehicle APIs. By clearly defining and segmenting the testing process, we enable Large Language Models (LLMs) to focus on specific tasks, ensuring a stable and controlled testing workflow. Experiments conducted on over 100 APIs demonstrate that our system effectively automates vehicle API testing. The results also confirm that LLMs can efficiently handle mundane tasks requiring human judgment, making them suitable for complete automation in similar industrial contexts.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are revolutionizing soft- ware engineering. In the past few years, we have witnessed the application of LLMs for assisting or automating numer- ous software engineering tasks like requirements engineering, software design, coding, and testing [1] [2]. Software testing, in particular, is one area where LLMs have been applied with vigor. Facing ever-increasing needs for automation due to the volume and intensity of work involved, testing is rapidly benefiting from the generative capabilities of LLMs. As systematically surveyed in [3], LLMs have been applied in many testing tasks including system input generation, test case generation, test oracle generation, debugging, and program repair.\nWhile a considerable amount of recent literature has focused on applying LLMs in narrowly scoped tasks [4] such as specific unit tests [5][6], isolated integration tests [7], or individual verification scenarios [8][9] \u2013 few have reported on their application to automate a complete test process. Practical testing processes are a diverse mix of steps that are mechanical, creative, and anything in between [10][11]. They also involve several (teams of) engineers and tools, whose harmonious cooperation is essential to ensure the quality and cadence of testing. The challenge is only greater when testing automotive embedded systems, where software coexists with mechatronics and other physical systems. Under such heterogeneous conditions, it is not immediately apparent how one can effectively integrate LLMs into a testing process and gain efficiencies. In response to these challenges, we present a case study that (1) focuses upon a real-world test process in the automotive industry that is largely performed manually, and (2) automates it using a recipe that seamlessly combines selective use of LLMs with conventional automation.\nThe focus of this case study our system under test is SPAPI, a web server that is deployed in trucks made by a leading vehicle manufacturer. SPAPI exposes a set of REST APIs which can be used by clients to read or write selected vehicle states. For example, SPAPI exposes / speed that can be used to read the vehicle speed, and /climate that can be used to change the cabin climate. Essentially, SPAPI serves as a gateway between web clients (like apps on a tablet) on one side, and in-vehicle control and monitoring applications on the other side. More importantly for the purposes of this pa- per, since SPAPI enables crucial customer-facing applications, considerable effort is spent in ensuring its quality.\nTesting SPAPI requires a dedicated team of 2-3 full-time engineers. As shown in Figure 1 (left), when new APIs are released, the team first reviews the API specifications. They then (2-3) consult multiple documentation sources to understand the associated vehicle states, (4-5) organize this information to determine appropriate mocks and test inputs,"}, {"title": "II. BACKGROUND", "content": "Since SPAPI is a web server that exposes REST APIs, our case study falls within the ambit of API testing [12]. Aspects of the SPAPI test process are therefore recognizable within the larger universe of API testing, but there are also several case-specific adjustments, which we now highlight.\nA. System architecture\nAs jointly illustrated in Figures 2 and 3, SPAPI follows the typical 3-tier architecture of decoupling presentation [13], business logic, and data, each of which we discuss below.\nPresentation \u2013 Like any web server, SPAPI presents REST- ful endpoints with GET and PUT methods and JSON pay- loads/responses. Each API transacts an object of the form $S = \\{(k_i, v_i)\\}_{i=1}^{N}$, with $N$ attribute-value pairs. Each pair $(k_i, v_i)$ in the object corresponds to some vehicle state $(k^*, v^*)$ that is managed by a control or monitoring application deployed in an Electronic Control Unit (ECU) in the vehicle. Figure 3 shows an example where / speed endpoint provides a GET method that returns the instantaneous speed of the vehicle which, in turn, is calculated by a SpeedEstimation application in a vehicle master control ECU. The same figure also illustrates the/climate endpoint with a PUT method that sets different cabin climate states by communicating with an ACControl application in a climate control ECU. Thus, the essence of SPAPI is presenting APIs for reading or writing an object $S = \\{(k_i, v_i)\\}_{i=1}^{N}$. This corresponds to interacting with vehicle states $S^* = \\{(k_i^*, v_i^*)\\}_{i=1}^{N}$ managed by applications distributed across the in-vehicle embedded system.\nData and data access \u2013 The typical web server may hold its data in a database, but, clearly, 'data' for SPAPI is vehicle state information managed by different in-vehicle control ap- plications. As shown in Figure 2, these in-vehicle applications are distributed across several ECUs, interconnected using Controller Area Network (CAN) links. While the typical web server may access data by executing database queries, SPAPI accesses data by exchanging CAN signals $S' = \\{(k_i', v_i')\\}_{i=1}^{N}$ with in-vehicle applications. A CAN signal is a pre-defined typed quantity sent through a CAN link between designated sender and receiver applications. In the simplest case, each vehicle state $(k^*,v^*)$ maps to one CAN signal and value pair $(k_i', v_i')$, which SPAPI sends or receives to access the state. We also clarify that this case study focuses upon testing SPAPI in a rig, and not in the real vehicle. In the test rig (see Figure 2), vehicle state is emulated by a Virtual Vehicle (VV) system, which maintains the superset $N$ of all vehicle states $S^* = \\{(k_i^*, v_i^*)\\}_{i=1}^{N}$ in a single table, emulating the state managed by distributed control applications. To maintain consistency of interaction, VV allows state $(k^*,v^*)$ to be accessed using the same CAN signal $(k_i', v_i')$ that SPAPI uses in the real vehicle. In addition to easing testing using virtual means, unlike many other API testing cases, VV offers the advantage of being able to freely mock vehicle state for testing purposes. Due to the continuous evolution of CAN signals and the VV platform, it is essential to monitor the vehicle's state to accurately capture relevant state changes.\nAPI logic Since SPAPI is a gateway, the logic for each endpoint is relatively lean. When a client invokes an endpoint, SPAPI does the mapping $(k_i, v_i) \u2192 (k_i', v_i')$ of each attribute- value pair in the API object to the corresponding CAN signal- value pair. Then, by sending or receiving the CAN signal and value $(k_i', v_i')$, SPAPI reads or writes the corresponding vehicle state $(k_i^*, v_i^*)$. Based upon the result of state manipulation, SPAPI sends an appropriate response to the client.\nB. Current manual API testing\nThe current manual workflow for API testing, as shown in Figure 1, involves steps such as: understand the API specification, look up related information, write test cases, run and access the test cases. Specifically, the tester should first identify the specific object set $S$ by understanding the documentation. Following this, the tester will retrieve the corresponding CAN signal documentation $S'$ and the VV system documentation $S^*$. It is crucial to ensure that each attribute in $S$ can be mapped to both $S'$ and $S^*$. This means verifying that every attribute can be converted into a CAN signal and can be simulated in the VV system, and testers can write test cases based on the matched results. Typically, two key aspects need to be checked during API testing. The first aspect is to verify whether the virtual vehicle's state aligns with expectations after setting certain attributes to specific values via API:\n$S^* \\leftarrow PUT(S)$\n$S^* \\stackrel{?}{=} S_{expected}$\n(1)\nThe second aspect is to check whether the API returns the expected values under a specific virtual vehicle state:\n$S = GET()$\n$S \\stackrel{?}{=} S_{expected}$\n(2)\nIn the following content, we will introduce the details of each step.\n1) Understand API specification: Test engineers need to understand the API documentation to extract the basic objects about the API. The documentation, like Swagger file, always details each API's essential information, such as all available endpoints, expected request formats, and possible response formats for each endpoint. Additionally, Swagger defines the data structures used in the API, including objects, properties, and their types. An example of a Swagger file snippet de- scribing the Climate object is shown in Figure 5(a). In this file, testers should parse the object's acMode and its corresponding details in the pairs. In summary, a thorough understanding the API documentation manually is essential for constructing a comprehensive object set $S = \\{(k_i, v_i)\\}_{i=1}^{N}$ from the original system documentations.\n2) Retrieve related information: After obtaining the at- tributes and values corresponding to the object, denoted as $S$, it is necessary to search for related documentation, including the information about CAN signals and the details about the virtual vehicle. The search process is illustrated in Figure 4. First, the tester needs to locate the relevant CAN signal documentation from CAN signal table. Then, by matching the corresponding key and value, the original state $S$ is converted into the CAN signal $S'$. Afterward, the relevant virtual vehicle documentation is consulted, and the corresponding key and value are mapped to obtain the specific operation $S^*$ that needs to be performed on the VV."}, {"title": "III. FULLY AUTOMATED SPAPI TESTING WITH LLMS", "content": "This section presents the details of our automated testing tool, SPAPI-Tester, which can integrate with LLMs to fully automate the entire API testing process. The overall process, as shown in Figure 5, can be divided into four main steps. These steps are detailed in Process 1.\nProcess 1: Overall Workflow of SPAPI-Tester\n1 TestTracker = InitializeTestTracker()\n2 For APISpec in List(APISecifications)\n3    S = ExtractTestObjects(APISpec)\n4    S' = APITOCANMapping(S, CANTable)\n5    S* = CANTOVVMapping(S', VVTable)\n6    TestCases = GenerateTestCase(S, S*)\n7    TestCode = WritingTestCode(TestCases)\n8    TestTracker.analyzeTestRun(TestCode)\n9 TestReport = PushToTestRepo(TestTracker)\nAfter initializing SPAPI (line 1), the entire testing process is divided into four parts: (1) Documentation understanding (line 3): This part involves identifying test objects based on the API specifications. (2) Information matching (lines 4, 5): This part entails look up relevant CAN table and virtual vehicle documents to matching all these objects. (3) Test case generation (line 6): Using the matched data, this step focuses on generating test cases for the API's return results and verifying the virtual vehicle's status. (4) Executing test cases and generating test reports (line 7, 8, 9).\nThe purpose of documentation understanding is to extract the test objects from the API documentation. Standard API documentation, commonly in YAML or Json format [14], as shown in Figure 5(a), is structured to list attributes and values associated with various objects. This structured format lends itself well to template-based parsing. We parse these documents and use predefined templates to extract the relevant attributes and values. Based on existing templates [15], we define a few simple and common rules to ensure the method's general applicability. These templates focus on fundamental elements, such as endpoint names, attribute names, and data types. Additionally, if sample API calls are provided in the documentation, we extract these directly to test the basic accessibility and functionality of the API.\nHowever, using templates alone is insufficient for deter- mining reasonable attribute values. We have identified the following issues with relying only on templates:\n(1) Cannot utilize attribute description: API documentation often includes natural language descriptions of attributes that templates cannot interpret or utilize. These descriptions typ- ically contain constraints on the attributes, which are crucial to prevent generating incorrect values.\n(2) Lack of robustness: API documentation can some- times be informal or inconsistent. For instance, attributes of enumeration types are usually presented as [\"STANDARD\", \"ECONOMY\"], but some documents might incorrectly use \"STANDARD or ECONOMY\". Only using templates makes it difficult to address these random and informal issues effec- tively.\nTo overcome these two issues, we introduce LLMs to en- hance the process. LLMs are utilized to analyze the entire API documentation, leveraging natural language descriptions to understand attribute constraints more effectively. Since LLMs are capable of semantic understanding, they also mitigate the impact of informal formatting or inconsistencies. This allows the system not only to parse API properties but also to map them to CAN signals, which is covered in detail in the subsequent section. LLMs further generate constraints based on attribute descriptions, producing reasonable values within these constraints. The contextual insights provided by LLMs help create a broader set of valid test values, thereby improving the coverage and reliability of our test cases.\nIn practice, to ensure the stability of LLM outputs and reduce the effect of the specific prompt formulations, we employ DSPy [16] to automate prompt optimization. DSPy enables us to write declarative LLM invocations as Python code. Figure 6 illustrates a simplified example of one of our prompts, along with the DSPy Signature. This APIPropertyToCANSignal signature outlines the process of converting structured API properties to CAN signals, which automates the time-consuming task of constructing an API property $(k_i, v_i)$ and mapping it to a corresponding CAN signal $(k_i', v_i')$.\nTo further improve the accuracy and ease of extracting structured data from the LLM, we format the LLM inputs and outputs as dictionaries. We define dictionary-based prompt templates to make tasks more comprehensible for the LLM [17], as demonstrated in Figure 7. By specifying the expected output fields, the signature directs the LLM to navigate incon- sistencies in documentation and accurately associate API prop- erties with CAN signal values. Furthermore, by typing fields in the signature, we enable the use of a TypedPredictor in DSPy, which validates the LLM response. If the response does not conform to the specified types, DSPy re-prompts the LLM, repeating this up to a maximum threshold until compli- ance is achieved. This structured approach capitalizes on the improved format adherence of LLMs, enhancing consistency and reliability."}, {"title": "B. Information Matching", "content": "As illustrated in Figure 4, the mapping of information in our system encompasses two stages: mapping API properties to CAN signals and mapping CAN signals to Virtual Vehicle (VV) signals. These mappings are crucial for enabling signal transmission within the vehicle as well as setting or verifying the vehicle's state. Since the processes and methods for these two mappings are similar, we will detail the approach for mapping API properties to CAN signals as an example.\nFirst, we retrieve a set of candidate CAN signal key-value pairs $\\{(k_i', v_i')\\}$ from a CAN signal library through solely matching the name of endpoint. Subsequently, we use the extracted API attributes $S = \\{(k_i, v_i)\\}_{i=1}^{N}$ and the candidate CAN signals $\\{(k_i', v_i')\\}$ as input to an LLM, enabling many- to-many matching between API properties and CAN signals. In many cases, attributes may have multiple enumerated val- ues. For instance, as shown in Figure 7, an API property 'valueFour' might take the values 'True' or 'False', while the corresponding CAN signal might represent these states as 'AA' and 'BB'. This type of mapping is common, and to increase the stability of SPAPI-Tester, we utilize a separate DSPy module specifically for matching enumerated values. The input consists of enumerated values from both the API property and the CAN signal, and the output is a mapping of these values.\nAs discussed in Section II.C, there are several challenges in the mapping process. First, for fuzzy matching, the LLM's strong semantic understanding is well-suited to handle these cases. Second, for pseudocode mappings, we enhance template robustness by embedding examples directly into the prompt, as shown in Figure 8. For example, we map \u201cAAsignal:BB OR PV_AnotherSignal:CC\u201d to \u201ccan_value\u201d : \u201cBB\u201d, thereby mini- mizing document noise while extracting relevant information. Third, for unit inconsistencies, we apply a dedicated DSPy module that uses a Chain-of-Thought (CoT) approach to ex- tract and normalize units within values. This module converts units (e.g., 'kW' to 'Kilowatts'), ensuring unit alignment in the test case generation phase."}, {"title": "C. Test case generation", "content": "After matching all the necessary information, we integrate these details into a structured document, as illustrated in Figure 5(b), which then serves as the basis for generating test cases.\nGiven the need to address multiple constraints during test case generation such as unit consistency-we employ a stepwise CoT approach to progressively incorporate these constraints. Specifically, for inconsistent units, we prompt the LLM to identify relationships between units and perform any necessary conversions. For inter-parameter dependencies, the LLM captures relationships among parameters, ensuring compatibility and avoiding value conflicts. Additionally, the LLM identifies property types and manages specialized for- mats, such as date-time strings. Finally, we guide the LLM in handling cases common in industrial contexts, such as shared CAN signals among multiple properties or specific constraints on value ranges.\nTo ensure these constraints are applied consistently, we leverage DSPy's TypedChainOfThought method, which consolidates all conditions within a single prompt. Figure 8 provides a simplified example of this prompt. For ease of use, we specify that the module outputs test cases in dictionary format, as depicted in Figure 5(c).\nAfter generating the test cases, we use them to create test code. The test code generally consists of two sections: a setup section, which includes essential elements such as package imports and requests to enable program execution, and a validation section containing assertions. Since the setup code remains consistent across tests, we design distinct Jinja\u00b9 templates for PUT and GET test cases. Using a simple code renderer, we inject the generated API and VV test objects into the Jinja template to render the Pytest test case. Figure 5(d) shows an example test case rendered by the test-writing module."}, {"title": "D. Executing test cases and generating test reports", "content": "To ensure the automation of the entire process, the system automatically executes the test code on the test rig [18], and then generates a comprehensive test report. This report docu- ments the details of the automated testing process, including the test objects, the matching results, the generated test cases, and the execution logs. Such documentation ensures that our system maintains a high level of transparency, rather than functioning as a black box."}, {"title": "IV. EXPERIMENTS", "content": "Our evaluation investigates the following questions.\nRQ1: What are the pass rate, coverage, and failure-detection capability of the test cases generated by the SPAPI- Tester?\nRQ2: To what extent can LLMs overcome the obstacles out- lined in Section II.C to achieve end-to-end automated testing?\nRQ3: How efficient is this automated API testing?\nRQ4: How effective is SPAPI-Tester in testing real-world in- dustrial APIS?\nSpecifically, RQ1-RQ3 focus on ablation studies of SPAPI- Tester, using controlled experiments to evaluate its capabilities and performance. RQ4 examines the application of SPAPI- Tester in the real-world, industrial setting with newly devel- oped (and thus guaranteed to be unseen) APIs to demonstrate the effectiveness of our end-to-end automated testing system.\nA. Experimental Setup\nIn this section, we describe our experimental setup.\n1) Subjects: Our research focuses on automating vehicle API testing within an industrial setting, addressing unique challenges such as inconsistencies across documentation and system specifications. As no existing methods directly address these issues in vehicle API automation, we could not compare our approach with general API testing techniques, as they lack the capability to handle the specific requirements of our industrial setting.\nWe evaluated the quality of generated test cases for 41 truck APIs using metrics such as pass rate and coverage. To assess SPAPI-Tester's error detection capabilities, we annotated an additional 109 APIs developed by a leading vehicle manufac- turer. These APIs were supported by system documentation from in-house truck experts, CAN signal protocols from the CAN-bus team, and virtual vehicle documentation from the Virtual Vehicle team.\nWe tested four LLMs: two classic models-GPT-3.5-turbo (OpenAI, 2023-07-01-preview) and LLaMA3-70B (2024-04- 18)\u2014and two recent advancements, GPT-40 (2024-05-13) and LLaMA3.1-70B (2024-07-23). To ensure flexibility and reduce maintenance, we opted not to fine-tune these models with company-specific data, allowing seamless adaptation to new models or data without retraining."}, {"title": "B. Pass Rate, Coverage, and Failure Detection (RQ1)", "content": "Pass rate: Since APIs with similar functions typically call the same electronic control unit (ECU) in embedded systems and, thus, share documentation within the same domain, we grouped 41 truck APIs into 6 categories based on their functions to present the results more clearly. Table II details the pass rates for each category.\nThese 41 APIs are online and pre-verified, ensuring that any failures observed during testing were due to issues within the generated test cases or code. Results show that for the majority of categories, all the APIs can pass the tests successfully, with all four LLMs achieving high pass rates. Notably, SPAPI- Tester achieved a 98% pass rate when using LLaMA3 and GPT-40, demonstrating the method's accuracy in generating valid test samples. However, GPT-3.5 exhibited slightly lower performance in handling structured input-output, failing in two cases due to improper CAN connection settings. Additionally, a common error across all LLMs stemmed from missing unit descriptions in API specifications. For example, when docu- mentation omitted units for battery power, LLMs incorrectly defaulted to watts (W) instead of kilowatts (kW), leading to test case failures. Broad patterns of errors like this could likely be addressed by further refining the prompts.\nCoverage: In addition to pass rate analysis, we evaluated the coverage of generated test cases to assess whether they adequately test each API. A vehicle expert group was invited to create ground truth test cases for 12 representative APIs, each including 5 to 30 test cases across 4 categories. The results are presented in Table III. All LLMs demonstrated high precision, with precision rates exceeding 0.97 across the board and reaching 1.0 for half of the APIs, showcasing the high quality of test cases generated by our model. For cases where precision was below perfect, errors originated from limitations in the fuzzy matching step.\nHowever, recall rates did not reach optimal levels primarily due to missing information in the API documentation, such as absent units or variable types for some attributes. To maintain high precision, SPAPI-Tester skips samples that lack sufficient context for accurate matching, resulting in a recall loss of approximately 15 percentage points. All untested attributes are logged in the testing report, allowing developers to trace and address these underlying issues.\nFailure detection: To further assess the effectiveness of the generated test cases in detecting failures, vehicle experts labeled 109 additional truck APIs, being developed, iden- tifying 38 as buggy. SPAPI-Tester created test cases that successfully detected all buggy APIs with only four false positives, achieving a 96% accuracy rate.\nAll models performed comparably, highlighting that our stepwise, structured pipeline design reduces dependence on specific LLM choices. We seamlessly migrated SPAPI-Tester to different LLMs without requiring additional adaptation. This largely model-agnostic pipeline design allows us to focus on refining the testing process rather than selecting specific LLMs, given the abundance of options."}, {"title": "C. LLMs' ability of overcoming obstacles (RQ2)", "content": "Fuzzy matching presents a significant challenge in auto- mated API testing. We categorized common fuzzy matching examples into five classes, selecting 20 test samples per class, supplementing with manually written samples if needed. The results, shown in Table IV (upper part), indicate that all models achieved high precision rates, highlighting the LLMs' capability to accurately recognize and match fuzzy inputs, a key requirement for full automation. For semantic equivalents, logical equivalents, and similar writing formats, all the models attained an accuracy of 1.0 or nearly so, demonstrating their strong pattern matching abilities in semantics and logic. How- ever, for spelling errors, accuracy slightly dropped as some errors altered word semantics, like mistaking date for data. In the abbreviations category, some abbreviations were too short to discern, complicating the matching process.\nFor the inconsistent units issue, we selected 200 samples for experiment. The results in Table IV (lower part) indicate that while SPAPI-Tester achieves a high precision rate, the recall remains suboptimal. The reason is that some documen- tation explicitly annotates units for each attribute, while others omit these details. In these cases, it becomes necessary to infer the units based on descriptions or other contextual information, which can affect the performance.\nAnother notable challenge is informal pseudocoded map- pings, where a single test case may correspond to multiple values. We selected 100 representative test cases for this experiment. Each test case consists of two sets with multiple (key, value) pairs, and the goal is to map elements between these sets as accurately and comprehensively as possible. To increase complexity, we intentionally selected test cases where the sets contained different numbers of elements, creating"}, {"title": "D. Time efficiency (RQ3)", "content": "In practical industrial scenarios, time consumption is an important criterion for measuring tool efficiency. Therefore, we measured the total time and the time taken at each stage of the SPAPI-Tester in the testing process. Given that the LLaMA model relies on local computational resources and that the processing speeds of GPT-3.5 and GPT-40 do not significantly differ in this pipeline, we report only the results"}, {"title": "E. Performance on real-world industry APIs (RQ4)", "content": "To demonstrate the capability of SPAPI-Tester in an real- world setting, we collected 193 newly developed and unveri- fied truck APIs and their corresponding documentation from a leading truck manufacturing facility. We then employed SPAPI-Tester to conduct end-to-end automated testing, aiming to identify issues within these APIs.\nSPAPI-Tester identified 23 test failures. The test report indicates that 22 test cases failed due to issues within the API implementation, and one test case failed due to an error while parsing the API documentation. On consultation with the API developers, these were determined to be legitimate bugs in the API implementation. The team has already started addressing these issues upon receiving the checking results.\nIn addition, this demonstrates that SPAPI-Tester not only has a high accuracy in detecting API errors but also provides detailed reports that help quickly identify the root causes of failures. Even when SPAPI-Tester was unable to generate correct code, the detailed reports can help to identify the failure causes quickly, thereby minimizing misdiagnoses. This capability significantly enhances the practical utility of SPAPI- Tester by providing precise and actionable insights. In sum- mary, these results underscore the robust practical applicability of SPAPI-Tester in real industrial environments."}, {"title": "F. Performance comparison with manual testing", "content": "To illustrate the advantages of SPAPI-Tester over man- ual API testing, we conducted a comparative evaluation. As described in Section IV.B, an expert team created ground truth test cases for 12 APIs. To measure the pass rate of manual testing, two additional engineers indepen- dently created test cases for these APIs. Results showed that one engineer's test cases passed 10 APIs, while the other's passed 11. Both engineers missed one or two APIs due to confusion over similar data entries. For instance, attributes like reducedWeeklyRestsForCurrentWeek and regularWeeklyRestsForCurrentWeek proved challenging for human testers to differentiate, whereas SPAPI- Tester's LLMs handled them effortlessly. This led to an average pass rate of 87.5% for manual testing at the API level, while SPAPI-Tester, with test cases generated by four different LLMs, achieved pass rates between 93% and 98%.\nIn terms of coverage, the average rate for manually created test cases was 82%, with human testers occasionally skipping properties due to incomplete API documentation (e.g., missing units). SPAPI-Tester reached 85% coverage with GPT-40, while other models ranged between 73% and 81%.\nTo evaluate failure detection, we selected 10 APIs (5 of which contained known bugs) from the 109 APIs mentioned in Section IV.B. Both engineers identified all buggy APIs, although one created a test case that falsely flagged a correct API as erroneous, resulting in a recall rate of 100% and a precision rate of 91% for manual testing. Similarly, SPAPI- Tester achieved a recall rate of 100% with a slightly lower precision of 90%.\nIn summary, SPAPI-Tester consistently generates high-quality test cases, demonstrating comparable performance to manual testing in terms of pass rate, coverage, and failure detection."}, {"title": "V. DISCUSSION", "content": "On complete test process automation Perhaps the most significant finding from this case study is that our recipe is capable of completely automating a real world test process. Put simply, SPAPI testing a process that currently takes 2- 3 FTEs - has effectively been substituted by SPAPI-Tester, a fully automatic pipeline. This success stems from com- bining LLMs with conventional automation, allowing SPAPI testing to proceed without human intervention. Key to this achievement is the nature of the SPAPI test process: it is well-structured, decomposable, and requires human judgment but not creativity. In such cases, LLMs serve as the critical link to full automation by systematically replacing manual steps. Maintaining the existing process structure further aids automation in two ways. First, it defines clear, verifiable steps where LLMs can be applied. Second, preserving the status quo ensures that automation is achievable without imposing possibly unreasonable costs of changing the test process an observation that is crucial for real world application.\nOn the generality of LLMs as problem solvers \u2013 Preserving the design of the process no doubt identifies discrete tasks where LLMs can be used. However, the clear enabler for complete automation is that the LLM automates all manual tasks with little practical regard to the actual nature of the task. Alternative automation methods exist, such as using fuzzy matching for inconsistent key-value mappings or a formal language to specify cardinality in key-value relationships. However, LLMs, as general problem solvers, eliminate the need for multiple specialized solutions, simplifying real-world implementations. While there is a cost to recast an LLM to solve a specific problem \u2013 like defining prompts or signatures the cost turns out to be manageable.\nOn implications on dependent processes \u2013 If SPAPI testing can be fully automated, its impact on adjacent processes becomes a natural consideration. API implementation directly precedes SPAPI testing, while integration within user-facing subsystems follows it. Given SPAPI's simplicity, LLMs could potentially automate these dependent processes, extending automation across much of the development lifecycle-an important step for in-vehicle software engineering. Further,"}, {"title": "VI. RELATED WORK", "content": "Existing research on API testing mainly focus on black- box and white-box testing, depending on whether the source code of the API is accessible [19]. White-box testing typically involves generating test cases to thoroughly test the logic within the code [20] [21]. For example, EvoMaster [22] uses the Many Independent Objective (MIO) evolutionary algorithm to optimize multiple metrics simultaneously, such as line coverage, branch coverage, HTTP status coverage, and the number of errors. Building on this, some studies have employed additional tools for code instrumentation, such as JVM [23] [24] and NodeJS programs [25] [26]. Atlidakis et al. [27] calculate code coverage by pre-configuring basic block locations and use this feedback to guide test generation.\nCurrently, most studies focus on black-box API testing, aiming to enhance test case coverage for more comprehensive API testing [28]. Template-based methods, such as fixed test specifications and JSON schemas, are commonly used for gen- erating accurate test cases [29] [30] [31] [32] [33]. However, these approaches struggle to capture parameter dependencies. To address this, Stallenberg et al. [34] proposed a hierarchical clustering method, while Lin et al. [35] introduced a tree- based representation of parameter relationships. Martin et al. [36] further improved test diversity by integrating external knowledge bases to generate reasonable values. Despite these advancements, traditional methods often fail to achieve robust and comprehensive testing.\nRecently, LLMs have emerged as a promising direction for API testing [37] [38]. Kim et al. [39] demonstrated the utility of LLMs in interpreting natural language API documentation to generate test values. Building on this, Le"}, {"title": "VII. CONCLUSION", "content": "Automated API testing is a critical process in software engi- neering, essential for ensuring the reliability and functionality of software systems. Despite its importance, API testing is often time-consuming, labor-intensive, and prone to errors. In practical applications, API testing involves retrieving and organizing relevant documents, and writing test cases based on the organized information. Due to the fuzzy matching of information across documents, manual intervention is required, hindering the automation of the entire testing process.\nIn this paper, we introduced SPAPI-Tester, the first system designed for the automated testing of automotive APIs. We decomposed the API testing process into a series of steps, identifying the obstacles to automation at each stage. By leveraging LLMs, we addressed these challenges, enabling full automation of the testing workflow. The results from real- world industrial API testing demonstrate that SPAPI-Tester achieves high detection accuracy. Our comprehensive experi- ments show that our system is highly robust and effective.\nOur system offers valuable insights for other automated API testing tasks and can be extended to web server API testing. The findings underscore the potential of LLMs to transform API testing by reducing manual effort and im- proving efficiency, paving the way for broader adoption and implementation in various testing environments."}]}