{"title": "Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and Deployment", "authors": ["Yang Li", "Junfan Chen", "Feng Xue", "Jiabin Qiu", "Wenbin Li", "Qingrui Zhang", "Ying Wen", "Wei Pan"], "abstract": "Adaptive teaming, the ability to collaborate with unseen teammates without prior coordination, remains an underexplored challenge in multi-robot collaboration. This paper focuses on adaptive teaming in multi-drone cooperative pursuit, a critical task with real-world applications such as border surveillance, search-and-rescue, and counter-terrorism. We first define and formalize the Adaptive Teaming in Multi-Drone Pursuit (AT-MDP) problem and introduce AT-MDP framework, a comprehensive framework that integrates simulation, algorithm training and real-world deployment. AT-MDP framework provides a flexible experiment configurator and interface for simulation, a distributed training framework with an extensive algorithm zoo (including two newly proposed baseline methods) and an unseen drone zoo for evaluating adaptive teaming, as well as a real-world deployment system that utilizes edge computing and Crazyflie drones. To the best of our knowledge, AT-MDP framework is the first adaptive framework for continuous-action decision-making in complex real-world drone tasks, enabling multiple drones to coordinate effectively with unseen teammates. Extensive experiments in four multi-drone pursuit environments of increasing difficulty confirm the effectiveness of AT-MDP framework, while real-world deployments further validate its feasibility in physical systems. Videos and code are available at https://sites.google.com/view/at-mdp.", "sections": [{"title": "1. Introduction", "content": "Multi-drone pursuit is an increasingly critical task with wide-ranging real-world applications, such as disaster re-sponse, border surveillance, search-and-rescue operations, and many more. All of these scenarios are highly dependent on the ability of autonomous drones to coordinate effectively in dynamic environments to pursue and track uncooperative targets. Most existing approaches rely on pre-coordinated strategies, where agents either follow predefined coordination mechanisms, such as conventions, role assignments, and communication protocols, or learn them through extensive interaction over time. These methods generally fall into two categories: traditional optimization-based approaches and reinforcement learning-based approaches."}, {"title": "2. Problem Formulation and Related Work", "content": "In this section, we first formalize the concept of adaptive teaming in multi-drone pursuit. Next, we discuss the re-lationship between our work and prior research, as sum-marized in Table 1. A more detailed literature review and complete tabular comparison of related works can be found in the Appendix A."}, {"title": "2.1. Problem Formulation", "content": "Definition 2.1 (Adaptive Teaming in Multi-Drone Pursuit). Adaptive teaming in multi-drone pursuit involves training a set of $N \\in \\{1,2,... \\}$ drone agents, referred to as learners, to dynamically coordinate with $M \\in \\{1, 2, . . . \\}$ previously unseen partners. The objective is to pursue $K \\in \\{1, 2, . . . \\}$ targets without collisions, optimizing the overall return.\nLet $C$ represent the cooperative team, comprising $N$ learners and $M$ uncontrolled teammates. The set of uncontrolled teammates is denoted by $U$. In the multi-drone pursuit task, there exists a set of opponents, denoted as $E$.\nAdaptive teaming can be effectively modeled as an extended Adaptive Teaming Decentralized Partially Observable Markov Decision Process (AT-Dec-POMDP). AT-Dec-POMDP is defined by the tuple $(S, C, A, P, P_u, r, O, \\gamma,T)$, where: where $S$ is the joint state space; $C$ is the set of cooperative agents, including learners ($N$) and uncontrolled teammates ($M$); $A = \\times_{i=1}^{f} A$ is the joint action space, where $C = N + M$ is the team size; $P\u016b(M|U)$ is the uncontrolled teammate sampling function, which defines the probability of sampling a subset $M \\subset U$ of size $M$ from the set of all uncontrolled teammates $U$; $P(s'|s, a)$ is the transition probability function, representing the probabil-ity of transitioning to state $s' \\in S$ given the current state $s \\in S$ and joint action $a \\in A$; $r(s, a)$ is the reward function, representing the team's reward in state $s$ after taking action $a$; $O$ is the joint observation space, with $O(0|s)$ describing the probability of generating observation $o$ given state $s$;"}, {"title": "3. The Framework: AT-MDP", "content": "To address the adaptive teaming problem in the multi-drone pursuit task, we propose a comprehensive framework called AT-MDP framework, encompassing three key components: Simulation, Training, and Deployment, as illustrated in Fig. 2. The Simulation component allows flexible cus-tomization of multi-drone pursuit environments through the environment configurator, while the environment in-terface provides a simulated environment for training and evaluation. The Training component leverages a distributed training framework to train algorithms through interaction with the environment interface within the simulation. It also includes an algorithm zoo that features a series of baseline methods for solving AT-MDP problem, as well as an unseen drone zoo that serves as a repository of various unseen drone policies to test adaptive teaming capabilities. Finally, the Deployment component facilitates real-world applications using a motion capture system and edge nodes, enabling real-time data exchange and decision-making. Section 3.2 introduces the simulation and deployment modules in detail, while Section 3.1 provides a detailed explanation of the training component."}, {"title": "3.1. Training", "content": "The training module comprises three key components: a distributed training framework, an algorithm zoo, and an unseen drone zoo. To enable efficient interaction with en-vironments, AT-MDP framework supports distributed en-vironment sampling, ensuring scalability and adaptability in training processes. The algorithm zoo and the unseen drone zoo are the core components of the training module, providing a series of baseline methods for AT-MDP problem and a diverse population of drone policies for evaluation.\nAlgorithm Pool. The algorithm pool in AT-MDP framework includes MARL-based pre-coordinated approaches, self-play frameworks, population-based training (PBT) strate-gies, and our proposed methods for adaptive teaming. For MARL-based approaches, we use the MAPPO algo-rithm, a strong baseline designed for multi-drone pursuit tasks, which leverages centralized training with decentralized execution for effective coordination. Self-play and PBT serve as widely used methods for zero-shot coordination, with self-play enabling agents to iteratively learn through interactions with copies of themselves and PBT allowing a population of models to ex-plore various strategies and share knowledge for improved generalization. Both self-play and PBT are implemented"}, {"title": "3.2. Simulation and Deployment", "content": "Simulation. The simulation module offers a user-friendly and customizable framework through the environment con-figurator and interface, enabling seamless configuration of multi-drone pursuit scenarios. Fig. 2 (bottom left) shows three examples of custom environments with photos from our real deployed system.\nThe environment configurator enables users to define all aspects of the simulation by categorizing configuration pa-rameters into three key sections: players, site, and task, as shown in Fig. 7 in Appendix D. This modular design ensures flexibility for a variety of experimental setups. The players section configures participants, including learners, unseen teammates, and evaders, specifying parameters such as their numbers, velocities, and the inclusion of an unseen drone zoo. The site section allows users to customize the phys-ical environment, including map dimensions and obstacle layouts, enabling diverse experimental terrains. Finally, the task section defines the task-specific parameters that govern the rules and objectives of the pursuit scenario.\nEnvironment interface provides a Gymnasium-based inter-face for reinforcement learning-based training and evalua-tion. This interface ensures seamless interaction between the training algorithms and the simulation, enabling effi-cient development and testing of adaptive teaming strategies. By integrating these detailed configurations, the simulation module supports diverse and customized experimental se-tups while maintaining efficiency for training. Moreover, it bridges the gap between simulation and real-world multi-drone applications, facilitating a robust pipeline for research and deployment.\nDeployment. The AT-MDP framework provides a robust real-world deployment solution, integrating edge computing nodes such as the Nvidia Jetson AGX Orin, the OptiTrack motion capture system, and CrazyFlie drones. As shown on the right of Fig. 2, the deployment workflow combines all components into a unified decision-making and execution pipeline. The learners' policies and unseen drone partners, sampled from the unseen drone pool in the training module, are deployed on the edge nodes, which serve as inference engines for these policies. In this setup, the Jetson AGX Orin edge computing node acts as the core computational unit, offering compact, energy-efficient, on-site processing capabilities. It enables real-time policy inference to support efficient drone decision-making during pursuit tasks. Cur-rently deployed alongside CrazyFlie drones, its lightweight design holds potential for future integration directly onto drones, enabling fully autonomous and decentralized multi-drone systems.\nObservations from the physical environment, including drone positions and movements, are captured by the Opti-Track motion capture system, which provides high-precision localization and tracking. The processed observations are sent to the edge nodes, which supply real-time data to the inference engine. The inference engine interprets the data and generates appropriate actions based on the policies de-ployed. These actions are then communicated to the drones for real-time execution. The drones provide continuous feedback in the system, facilitating dynamic adjustments and enhancing coordination among agents."}, {"title": "4. Experiment", "content": "In this section, we evaluate the performance of baseline methods from the algorithm zoo and validate their feasibil-ity in real-world multi-drone pursuit scenarios. The experi-ments are structured into two main parts: (1) adaptive team-ing without teammate modeling, detailed in Section 4.1, and (2) adaptive teaming with teammate modeling, discussed in Section 4.2. Each subsection presents the experimen-tal setup, key results, and in-depth analysis. Additionally, Section 4.3 provides a case study demonstrating real-world deployment, showcasing how adaptive learners coordinate with unseen drone partners to execute a multi-stage capture strategy. Further details on the real-world implementation are available in Appendix E.\nWe begin by introducing the shared experimental setups applicable to both groups of experiments.\nEnvironments. To systematically evaluate the performance of adaptive teaming, we leverage the environment configu-rator to design four multi-drone pursuit environments with varying levels of difficulty, denoted as $4p2e3o$, $4p2e1o$, $4p2e50$, and $4p3e50$. The notation represents the number of pursuers (p), evaders (e), and obstacles (o) in each setting. Each environment is initialized with defined spawn areas, where the evaders spawn within a 3.2m wide and 0.6m high region, while the pursuers spawn in a similarly sized region. The obsta-cle layouts introduce additional complexity. Environment $4p2e30$ include three distributed barriers with different shapes: two cubes in the left area and one cylinder in the right area. Although Environment $4p2e10$ only includes a cuboid obstacle in the middle, the difficulty is slight higher than $4p2e30$ for evaders have large space to escape. In general, the difficulity of the two environments is easy. Those environments with 50 feature densely packed obstacles that significantly constrain movement. In terms of difficulty, $4p2e30$ is categorized as easy due to fewer obstacles and ample space for pursuit, $4p2e10$ presents a moderate chal-lenge with a single strategically placed obstacle, $4p2e50$ is considered hard as it requires advanced maneuvering in a confined space, and $4p3e50$ is the most difficult (super-hard) due to an additional evader and dense obstacle layout, requiring highly coordinated teamwork for successful pur-suit.\nEvaluation Metrics. To evaluate the performance of the methods, we use the following metrics: 1. Success rate (SUC), which measures the proportion of tasks successfully completed. An episode is deemed successful if the pursuers capture both evaders, defined as reducing the distance be-tween an evader and a pursuer to less than 0.2 metres; 2. Collision rate (COL), which tracks the frequency of colli-sions during task execution. A collision is recorded if the distance between any two pursuers is less than 0.2 metres or if the distance between a drone and an obstacle is less than 0.1 metres; 3. Average success timesteps (AST), which indicates the average number of steps taken to complete a task; 4. Average reward (REW), which reflects the overall efficiency and quality of the agent's performance across episodes. These metrics collectively provide a comprehen-sive evaluation of the proposed method's adaptability and effectiveness."}, {"title": "4.1. Adaptive teaming without teammate modeling", "content": "4.1.1. EVALUATION PROTOCOL\nTo ensure a rigorous and fair comparison of adaptive team-ing, we standardize the evaluation process across all meth-ods. Since AT-MDP is a newly introduced problem with no existing algorithms specifically designed for it, we establish baselines using PBT, MAPPO , and our proposed OPT.\nUnseen Drone Partners. Unseen drone partners are a crit-ical component of the evaluation protocol. To thoroughly assess the proposed method, we evaluate its performance across four distinct unseen drone zoos, with drone policies selected from the unseen drone pool in the AT-MDP frame-work. Each zoo represents a unique combination of drone behaviors, designed to create diverse and challenging scenar-ios that test the adaptability and robustness of the proposed"}, {"title": "Impact Statement", "content": "This paper advances the field of machine learning and multi-robot systems by introducing Adaptive Teaming in Multi-Drone Pursuit (AT-MDP) and proposing AT-MDP frame-work, a comprehensive framework for simulation, training, and real-world deployment. Our work contributes to multi-drone system by enabling multiple drones to dynamically coordinate with unseen teammates in continuous-action real-world tasks.\nThe potential societal benefits of this research include im-provements in search-and-rescue operations, disaster re-sponse, border surveillance, and autonomous security sys-tems, where adaptive coordination among unseen agents can enhance efficiency and safety. Additionally, the open-source implementation of our framework fosters reproducibility and further research in adaptive teaming and multi-agent collaboration.\nFrom an ethical perspective, while multi-drone pursuit has positive real-world applications, the technology also has po-tential risks if misused, such as surveillance or adversarial applications. We emphasize the importance of responsi-ble AI development and adherence to ethical guidelines in deploying autonomous systems. Future work should con-sider human-in-the-loop oversight and safety constraints to ensure the ethical use of AI-driven multi-agent systems."}, {"title": "B. Open-ended Population Training Algorithm", "content": "In this section, we define a population of drone strategies, denoted as $II = \\{\\pi_1,\\pi_2,\\cdots,\\pi_\\eta\\}$. For the task involving $C$ teammates, the interactions within the population $II$ are modeled as a hypergraph $G$. Formally, the hypergraph is represented by the tuple $(II, E, w)$, where the node set $II$ represents the strategies, $E$ is the hyperedge set capturing interaction relationships among teammates, and $w$ is the weight set representing the corresponding average outcomes. The left subfigure of Fig. 5 illustrates an example of a hypergraph representation with five nodes and a fixed hyperedge length of 4.\nBuilding on the concept of preference hypergraphs , we use the preference hypergraph to represent the population and assess the coordination ability of each node. The preference hypergraph $P_G$ is derived from the hypergraph $G$, where each node has a direct outgoing hyperedge pointing to the teammates with whom it achieves the highest weight in $G$. Formally, $P_G$ is defined by the tuple $(II, E_p)$, where the node set $II$ represents the strategies, and $E_p$ denotes the set of outgoing hyperedges. As shown in the right subfigure of Fig. 5, the dotted line highlights the outgoing edge. For instance, node 2 has a single outgoing edge (2, 3, 5, 4) because it achieves the highest outcome, i.e., a weight of 45, with those teammates in $G$, as depicted in the left subfigure.\nIntuitively, a node in $P_G$ with higher cooperative ability will have more incoming hyperedges, as other agents prefer collaborating with it to achieve the highest outcomes. Therefore, we extend the concept of preference centrality to quantify the cooperative ability of each node. Specifically, for any node $i \\in II$, the preference centrality is defined as\n$\\eta_{\\pi(i)} = \\frac{d_{p_G}(i)}{d_G(i)},$  (1)\nwhere $d_{p_G}(i)$ denotes the incoming degree of node $i$ in $P_G$, and $d_G(i)$ represents the degree of node $i$ in $G$.\nMax-Min Preference Oracle. Building on the basic definition of the preference hypergraph representation, we introduce the concept of Preference Optimality to describe the goal of our training process.\nDefinition B.1 (Preference Optimal). A set of learners $N^\\*$ of size $N$ is said to be Preference Optimal (PO) in a hypergraph"}, {"title": "C. Unseen Drone Zoo", "content": "Rule-Based Method: Greedy Drone. The Greedy Drone pursues the closest target by continuously aligning its movement with the target's position. Its state information includes its own position, orientation, distances and angles to teammates and evaders, and proximity to obstacles or walls. When obstacles or other agents enter its evasion range, the Greedy Drone dynamically adjusts its direction to avoid collisions, prioritising immediate objectives over team coordination.\nTraditional Method: VICSEK Drone. Based on the commonly used VICSEK algorithm , the VICSEK Drone adopts a bio-inspired approach to mimic swarm-like behaviours. It computes and updates a velocity vector directed towards the evader, optimising the tracking path based on the agent's current environmental state. To avoid nearby obstacles or agents, the VICSEK Drone applies repulsive forces with varying magnitudes. While the calculated velocity vector includes both magnitude and orientation, only the orientation is implemented in our experiments, making it a scalable and practical teammate model for multi-drone coordination.\nLearning-Based Method: Self-Play Drones. For the learning-based approach, we employ an IPPO-based self-play algorithm, generating diverse drone behaviours by training agents with different random seeds. This approach simulates a wide range of adaptive strategies, introducing stochasticity and complexity to the evaluation process."}, {"title": "D. Environment Configurator", "content": "The AT-MDP framework environment configurator allows users to define and modify multi-drone pursuit scenarios through a structured JSON file. Fig. 7 provides an example configuration file that specifies key parameters across three categories: players, site, and task.\nPlayers Configuration: This section defines the number and roles of agents in the environment, including the number of pursuers ($num\\_p$), evaders ($num\\_e$), controlled agents ($num\\_ctrl$), and unseen teammates ($num\\_unctrl$). Additional parameters such as random respawn behavior, reception range, and velocity settings further customize agent interactions. The $unseen\\_drones$ field allows users to specify different unseen teammate models from the unseen drone zoo.\nSite Configuration: This section defines the physical properties of the environment, including its boundary dimensions ($width$, $height$) and obstacle placements. Obstacles can be configured individually to introduce varying levels of complexity.\nTask Configuration: This section sets the pursuit task parameters, including the capture range ($capture\\_range$), safety radius ($safe\\_radius$), task duration ($task\\_horizon$), and simulation frame rate ($fps$). The $task\\_name$ field provides a label for different predefined environment scenarios.\nThis modular configuration enables flexible environment customization, facilitating experiments across diverse multi-drone pursuit scenarios."}, {"title": "E. Real-world Deployment", "content": "To validate the feasibility of our algorithms in real-world scenarios, we design and conduct hardware experiments using Crazyflie drones, the FZMotion motion capture system, and the Crazyswarm validation platform. The FZMotion motion capture system is responsible for real-time position measurement of the drones. It transmits the positional data in point cloud format to Crazyswarm, where the information is processed as input for the algorithm. The Crazyswarm platform is deployed on two different edge nodes, i.e. a laptop (Lenovo ThinkPad T590) and a Jetson Orin Nano, which handles the reception of drone position data from the motion capture system, executes the adaptive teaming algorithm, and transmits control commands to the Crazyflie drones via Crazyradio PA. Upon receiving these commands, the Crazyflie drones execute the prescribed maneuvers. The onboard Mellinger controller ensures accurate trajectory tracking, allowing the drones to follow the control signals with high precision. This real-world deployment setup enables the direct evaluation of our learned policies in physical drone systems, bridging the gap between simulation and real-world execution."}, {"title": "F. Implementation Details of Adaptive Teaming with Modeling", "content": "Adaptive Teaming with Modeling (ATM) extends the MAPPO algorithm by incorporating an additional teammate modeling network f. This network generates team encoding vectors to represent the behavioral characteristics of unseen teammates, improving coordination in multi-drone pursuit. The modeling network f processes three types of inputs: (1) observed evader states history, (2) self-observed states history, and (3) relative positions history between agents. These inputs are transformed using independent fully connected layers, aggregated through weighted averaging, and combined into a unified team representation. The final embedding is a fixed-dimensional vector, integrated into the actor network of MAPPO to enhance decision-making in adaptive teaming scenarios."}]}