{"title": "VARYING SHADES OF WRONG: ALIGNING LLMS WITH WRONG ANSWERS ONLY", "authors": ["Jihan Yao", "Wenxuan Ding", "Shangbin Feng", "Lucy Lu Wang", "Yulia Tsvetkov"], "abstract": "In the absence of abundant reliable annotations for challenging tasks and contexts, how can we expand the frontier of LLM capabilities with potentially wrong answers? We focus on two research questions: (1) Can LLMs generate reliable preferences among wrong options? And if so, (2) Would alignment with such wrong-over-wrong preferences be helpful? We employ methods based on self-consistency, token probabilities, and LLM-as-a-judge to elicit wrong-over-wrong preferences, and fine-tune language models with preference optimization approaches using these synthesized preferences. Extensive experiments with seven LLMs and eight datasets demonstrate that (1) LLMs do have preliminary capability in distinguishing various shades of wrong, achieving up to 20.9% higher performance than random guess; (2) Alignment with wrong-over-wrong preferences helps LLMs to produce less wrong and sometimes even outright correct answers, while overall improving model calibration. Code and data are publicly available at https://github.com/yaojh18/Varying-Shades-of-Wrong.", "sections": [{"title": "1 INTRODUCTION", "content": "Post-training with preference optimization, a.k.a. alignment, has become a crucial part of the development of large language models (LLMs) (Touvron et al., 2023). From online alignment with PPO (Schulman et al., 2017) and RLHF (Ouyang et al., 2022) algorithms, to recent developments in offline alignment with DPO (Rafailov et al., 2023), LLMs improve by learning nuanced distinctions and separability between answers: a correct answer is preferred over an incorrect answer in reasoning problems (Wang et al., 2023c), or a response that adequately completes a user instruction is preferred over a failing one (Dubois et al., 2023), etc. This response \u201ccorrectness\" in alignment procedures and implementations typically comes from datasets with ground truth answers (Wang et al., 2023c), human annotators (Ouyang et al., 2022), or even the model itself (Lee et al., 2023), which are employed to produce preference pairs and/or reward model training data.\nTo overcome the bottleneck of relying on high-quality expert-annotated data, in this work, we ask: what if there is no correct answer in the alignment process? What if the task comes without annotated ground-truths, is prohibitively expensive for human annotators, and even state-of-the-art models are too poor to consistently provide correct answers? We see an increasing number of challenging benchmarks such as theorem proving (Welleck et al., 2021; Zhou et al., 2024) and reasoning in low-resource languages (Huang et al., 2024c) where correct answers are simply unavailable. As a result, existing alignment procedures relying on \u201cright-over-wrong\" separability might struggle to expand the frontier of model capabilities. In response, we propose wrong-over-wrong alignment, where LLMs evaluate the \"wrongness\u201d of incorrect answers and align by learning to prefer less-wrong answers over more-wrong ones in the absence of correct responses. We focus on tasks with clear right-wrong distinctions and present the outline of our work in Figure 1:\nRQ1: Can LLMs discriminate between varying shades of wrong and produce wrong-over-wrong preferences? We employ four tasks where a reasonable \"proxy\" of wrongness exists, e.g., to find the shortest path in a network with a ground truth path length of 5, finding a path of length 8 is \u201cless wrong\" than a path of length 11 (Wang et al., 2023a).\u00b9 For each task, we sample multiple solutions to"}, {"title": "2 METHODOLOGY", "content": "2.1 ELICITING WRONG-OVER-WRONG PREFERENCES\nWhen LLM-generated answers are incorrect, it is often possible that some answers are less wrong than others. We term this \"varying shades of wrong\" and investigate whether LLMs could provide reliable wrong-over-wrong preferences if no reliable ground truths are available.\nFormally, given a question q and a pair of wrong answers (a1, a2), we aim to employ LLMs to approximate a \u201cground-truth\u201d wrong-over-wrong preference function f(a1 > a2 | q) \u2192 {1,0,-1}, where 1 indicates that a\u2081 is less wrong and should be preferred over a2, -1 vice versa, and 0 indicates the two answers are not separable due to ambiguity or uncertainty. Since the ground-truth preference function f(a1 > a2 | q) is usually unavailable, we propose using a silver function"}, {"title": "3 EXPERIMENT SETTINGS", "content": "Models We employ three open and proprietary LLMs for experiments spanning different scales and access levels. First, we use LLAMA3-8B (Dubey et al., 2024), GPT-3.5, and GPT-40 (Achiam et al., 2023) to sample 10 answers per problem with random option orders for multiple-choice questions. We employ a temperature of 1.0 and a max generation length of 1024. LLMs employed in this"}, {"title": "4 RESULTS", "content": "Eliciting Wrong-over-Wrong Preferences We present the accuracy of LLM-generated wrong-over-wrong preferences in Table 1.\n\u2022 Eliciting wrong-over-wrong preference from LLMs is feasible. Most (approach, LLM) combinations yield wrong-over-wrong preferences that are significantly better than random guess: the average Accwow is 0.553 across datasets, and the best Accwow is 0.709 achieved by fGPT-40 with a margin of M10. This suggests that LLMs, with the right approach, do possess preliminary capabilities to distinguish various shades of wrong.\n\u2022 Knowledge-based tasks are easier while commonsense is most challenging. The average Accwow across all methods for Knowledge Crosswords, Bio Generation, COM2, and NLGraph are 0.578, 0.560, 0.522, and 0.540 respectively. This suggests that LLMs have varying wrong-over-wrong judgement capabilities across domains: performance is best on Knowledge Crosswords, a multi-hop knowledge QA dataset, while worst on COM2, a commonsense reasoning task. The ability to determine wrong-over-wrong may be impacted by LLMs' base capability on respective task; Figure 2 shows a positive correlation between model accuracy on the task and Accwow."}, {"title": "5 ANALYSIS", "content": "Task Utility and Wrong-over-Wrong Judgement We explore the impact of two factors on the reliability of wrong-over-wrong judgements: accuracy of the evaluator's generated answers (task accuracy) and the evaluator's confidence in its generated answers (task confidence). We visualize the correlations and present Pearson correlation coefficients (Sedgwick, 2012) in Figure 2. We observe a positive correlation between task accuracy and Accwow, but a negative correlation between task confidence and Accwow. This suggests that models that perform well on the task are also good at distinguishing various shades of wrong, while over-confident and under-calibrated models harm wrong-over-wrong preferences. We also find that LLM struggles to differentiate two answers with close wrongness levels in Appendix A.\nPreference Accuracy and Alignment Improvement We examine how the quality of wrong-over-wrong judgements Accwow is related to improvements through wrong-over-wrong alignment. Figure 3 demonstrates a weak positive relationship between Accwow and improvement in Apwrong, and no significant correlation between Accwow and improvement in \u2206Acc or \u2013\u0394\u0395C\u0395. Surprisingly, this suggests that improvement resulting from wrong-over-wrong alignment is nuanced and is not sensitive to the absolute accuracy of wrong-over-wrong preference data. There is also a clear positive relationship between Accwow and Apwrong, \u2206Acc, and ECE on the Bio Generation dataset, indicating that sensitivity to wrong-over-wrong judgement quality varies by dataset.\nRight-over-Wrong Alignment We conduct experiments on right-over-wrong preferences and a 50:50 mix of right-over-wrong and wrong-over-wrong preferences. The results in Table 3 reveal that: (1) Right-over-wrong alignment has the best average improvement in more correct Apwrong = 0.090 on mix-generated data, while on self-generated data, wrong-over-wrong alignment yields the best Apwrong = 0.068. Mixing generators is especially helpful to right-over-wrong alignment, with"}, {"title": "6 RELATED WORK", "content": "LLM Alignment In recent years, aligning LLMs with human preferences has become an important research question, driven by the need for safety (Dai et al., 2024; Qi et al., 2024; Huang et al., 2024b), honesty (Yang et al., 2023; Wen et al., 2024), factuality(Liang et al., 2024; Lin et al., 2024), diversity (Ding et al., 2024), etc. The initial methods, such as Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022; Munos et al., 2024; Chakraborty et al., 2024), leverage human preferences to train reward models and employ algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) for alignment. Recent alignment methods move away from explicit reward models, as seen in Direct Preference Optimization (DPO) (Rafailov et al., 2023). Alongside this, Reinforcement Learning from AI Feedback (RLAIF) (Bai et al., 2022; Lee"}, {"title": "7 CONCLUSION", "content": "With the growing race towards bigger, better LLMs capable of solving a wider range of tasks, it becomes evident that the availability of carefully curated data is a major bottleneck. Our work investigates the potential to alleviate this limitation by eliciting preferences among wrong answers with an LLM and aligning with these wrong-over-wrong preferences. We empirically investigate wrong-over-wrong alignment with seven LLMs and eight datasets. We find that LLMs do have preliminary capability to rank wrong answers and produce reliable wrong-over-wrong preferences. The strongest approach such as score-based LLM-as-a-judge achieves up to 70.9% accuracy across datasets. In ad-"}, {"title": "LIMITATIONS AND ETHICS STATEMENT", "content": "Imperfect proxies. The proxy functions we use to measure correctness are inherently imperfect. While they serve as reasonable approximations for distinguishing varying degrees of wrongness, they are not definitive, e.g. heuristic-based proxies may merely focus on the final answer and overlook the reasoning steps and model-based proxies are inaccurate and may suffer from domain shift problem. As a result, some wrong-over-wrong preferences may be misjudged, potentially affecting the evaluation of Accwow and analysis results.\nSensitivity to hyperparameters. The alignment process we employ is sensitive to hyperparameter selection. This dependence can lead to variability in results across different models and datasets, making it challenging to guarantee consistent performance improvements. Further exploration of more robust hyperparameter configurations (Falkner et al., 2018; Arango et al., 2024) is necessary.\nLimited scope of experimental datasets. Experiments are conducted on datasets where ground-truth answers exist, and proxy functions can be employed to approximate wrongness. However, this setup doesn't fully reflect the application scenarios we aim to address tasks where no clear ground-truth answers are available (Wang et al., 2024a), or the problems are so challenging that even expert human annotators or LLMs might struggle (Welleck et al., 2021). Evaluating the generalizability of our framework in these conditions remains an important avenue for future work.\nUnique focus on knowledge and reasoning problems. We focus on tasks and problems with absolute and indisputable correct answers in this work, such as multi-hop QA (Ding et al., 2023) and graph reasoning (Wang et al., 2023a). Consequently, existing alignment works in these domains often assume access to correct answers and construct right-over-wrong pairs for preference learning (Wang et al., 2023c; Cheng et al., 2024; Lin et al., 2024). However, objective \u201ccorrect answers\" are often not feasible in general instruction following tasks, thus they employ human preference data between a pair of responses that is not necessarily right-over-wrong. Our scope is to investigate wrong-over-wrong alignment specifically focusing on the first type of knowledge/reasoning problems with absolute correctness while we leave general instruction following as future work.\nIncomparable wrongness in social contexts. In social contexts, wrongness is often subjective and potentially incomparable, which introduces biases and fairness concerns in wrong-over-wrong alignment. For instance, when judging the wrongness of statements related to sensitive topics such as politics or identity, cultural biases may influence the model's preferences (Xu et al., 2021; Bender et al., 2021; Feng et al., 2023). A statement like \"People from certain neighborhoods are more likely to commit crimes\u201d might be judged less wrong than \u201cAll people from certain neighborhoods are criminals\" but these evaluations reflect specific perspectives that may not be universally shared or appropriate in all contexts. Furthermore, aligning models with such judgments risks reinforcing harmful stereotypes or systemic biases. Ensuring fairness and transparency in these judgments is critical, and models should be designed to recognize when wrongness is subjective and abstain from making harmful comparisons."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We provide all necessary details for the implementation and evaluation of our proposed wrong-over-wrong alignment approach in Appendix B. Specifically, Appendix B includes information of 8 datasets splits and preprocessing steps, 7 LLM checkpoints, hyperparameters selection and detailed configuration for each table. Moreover, the prompts for preference elicitation methods are explained in Table 12 and Table 13. Our codes, including scripts for dataset generation, preference eliciting,"}]}