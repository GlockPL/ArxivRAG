{"title": "Evaluating Language Models on Grooming Risk Estimation Using Fuzzy Theory", "authors": ["Geetanjali Bihani", "Tatiana Ringenberg", "Julia Rayz"], "abstract": "Encoding implicit language presents a challenge for language models, especially in high-risk domains where maintaining high precision is important. Automated detection of online child grooming is one such critical domain, where predators manipulate victims using a combination of explicit and implicit language to convey harmful intentions. While recent studies have shown the potential of Transformer language models like SBERT for preemptive grooming detection, they primarily depend on surface-level features and approximate real victim grooming processes using vigilante and law enforcement conversations. The question of whether these features and approximations are reasonable has not been addressed thus far. In this paper, we address this gap and study whether SBERT can effectively discern varying degrees of grooming risk inherent in conversations, and evaluate its results across different participant groups. Our analysis reveals that while fine-tuning aids language models in learning to assign grooming scores, they show high variance in predictions, especially for contexts containing higher degrees of grooming risk. These errors appear in cases that 1) utilize indirect speech pathways to manipulate victims and 2) lack sexually explicit content. This finding underscores the necessity for robust modeling of indirect speech acts by language models, particularly those employed by predators.", "sections": [{"title": "1 Introduction", "content": "Online child sexual grooming refers to the insidious process through which an adult establishes relationships with potential under-aged victims on digital platforms, with the goal of eventual sexual gratification without detection [18]. Adults grooming children use a wide range of tactics and persuasion strategies depending on factors such as their potential goals [3, 11, 4] and level of directness [10, 11]. Prior research has identified various covert and overt persuasion and manipulation strategies of groomers including gift-giving, flattery, pressure, deception, and affection, to gain the trust and confidence of their targets [8, 6].\nResearch on automated grooming detection has predominantly framed the task as a binary classification problem to categorize entire chat instances as either grooming or non-grooming [12, 5, 7]. However, this method falls short of facilitating preventive measures. Furthermore, recent studies focusing on modeling preventive measures for grooming chats have primarily relied on training with internet sting data rather than authentic victim conversations [16], potentially leading to inaccurate generalizations.\nDespite findings highlighting differences in chat language across various participant groups (real victims, law enforcement officials, decoys) [14], current models neither measure nor account for such disparities in data distributions."}, {"title": "2 Methodology", "content": "In this section, we outline our methodology for modeling grooming risk using SBERT [13]. We define the task of Grooming risk-scoring and its objectives. We then define an evaluation protocol that uses fuzzy rules to compare different degrees of grooming with transformer language model predictions. Fuzzy annotations of grooming strategies have been taken from the dataset described in [15]."}, {"title": "2.1 Degrees of Grooming Risk", "content": "Grooming risk, rather than fitting neatly into discrete categories, exhibits gradations across multiple degrees of severity. Imagine a scenario where an array of grooming strategies is deployed within a given chat context. These strategies can encompass a spectrum of subtle to overt tactics. Each strategy contributes to the overall risk level, but the degree of influence it holds is imprecise and contextually contingent. Unlike binary categorizations, grooming risk manifests along a continuum, where various factors interplay to determine the level of vulnerability.\nTo assess whether a language model can learn human perceptions of grooming risk from natural language contexts, we compare language model predictions with human perceptions of grooming risk. Specifically, we map the extent to which humans perceive grooming strategies to be present in a given chat context to the"}, {"title": "2.2 Task Definition", "content": "We approach grooming risk-scoring as a regression task, with chat context language as the independent variable and aggregated grooming risk as the dependent variable.\nDefinition 1 - Chat Context (c): A sequence of current and last $n - 1$ chat messages. We fix n = 3, for our analysis.\nDefinition 2 - Grooming Risk ($r_{groom}$): Severity of grooming behavior determined by the total number of grooming strategies present within a given chat context (c), as shown in Equation 2.\n$r_{groom} = n_s(C)$ \nWe limit our analyses to twelve grooming strategies, including coercion, bragging, teaching, requests for images, negative comments about physique, negative comments about family, personal compliments, reverse power, asking about sexual history, checking willingness, roleplaying, and secrecy, as described in [15]. Each strategy is assigned a membership value of 0 (none), 0.5 (partial), or 1 (full). For a detailed description of these grooming strategies and the annotation process, refer to [15]. Thus, chat contexts with fewer observed grooming strategies are considered lower risk, while those with more strategies present indicate higher grooming risk.\n$\\mu_{risk}(c) = \\varphi (n_s (c) \u2013 m), where \\varphi(z) = \\frac{e^{-z^2/2}}{\\sqrt{2\\pi}}$\n$\\mu_{mod}(c) = \\mu_{risk}(c)$ for m = 0.2\n$\\mu_{sig}(c) = \\mu_{risk}(c)$ for m = 1\n$\\mu_{sev}(c) = \\mu_{risk}(c)$ for m = 2\nTo evaluate the performance of the regression model on grooming risk prediction, we categorize these risk scores ($r_{groom}$) into three risk categories using a Gaussian membership function. This function maps continuous risk scores into degrees of membership in moderate (low risk), significant (medium risk), and severe (high risk) categories. This allows each score to have fuzzy (partial) membership in multiple categories. We choose a Gaussian distribution function because it enables smooth transitions between risk levels. The discretization of the risk scores is done using"}, {"title": "2.3 Model Studied", "content": "We conducted our analysis on Sentence-BERT (SBERT) [13] which was specifically designed to encode sentence embeddings. SBERT utilizes siamese networks to generate semantically meaningful representations of sentences. These representations are optimized to capture semantic similarity between sentences in a vector space, making them suitable for various downstream tasks such as semantic search and clustering."}, {"title": "2.4 Fine-tuning Details", "content": "We adapted SBERT to predict grooming risk scores using a linear layer on top of the final layer to output regression estimates. Fine-tuning involves adapting pre-trained models to specific downstream tasks by utilizing task-specific data. During the fine-tuning process, additional layers are added on top of the pre-trained models, and the models are trained on labeled data specific to the task at hand. Fine-tuning allows the model to acquire task-specific features and enhances the model's performance on the given task.\nFor a given chat context c the regression model is optimized for estimating the severity of grooming risk r by minimizing the Mean Squared Error (MSE) between the predicted grooming risk $r_{pred}$ and actual grooming risk $r_{groom}$. The hyperparameters we used for finetuning our BERT models are listed in Table 1."}, {"title": "3 Results and Analysis", "content": "We examined how well fine-tuned SBERT predictions align with human perceptions of grooming risk in natural language contexts. Specifically, we fine-tuned and evaluated language models separately on grooming conversations involving predators interacting with distinct groups: law enforcement officers (LEO), victims, and decoys. This analysis is prompted by prior research highlighting variations in language usage across different groups in grooming behaviors [15]. Additionally, automated models of grooming classification and detection completely overlook these differences. This underscores the importance of demonstrating that language models trained on data distributions diverging from patterns observed in real-victim grooming conversations may lack reliability if deployed without due consideration."}, {"title": "3.1 Prediction Accuracy across Degrees of Grooming Risk", "content": "We report Mean Squared Error on predictions across different degrees of grooming risks in Table 2. While the overall MSE remains consistent across all three groups, discernible differences in error rates are evident. Our findings show notably higher"}, {"title": "3.2 Risk Distribution across Different Groups", "content": "We conducted an analysis of model predictions across groups, illustrating the distribution of predicted grooming risk scores ($r_{pred}$) across varying degrees of actual grooming risk, as depicted in Figure 2. Our investigation can be summarized through the following insights: while language model predictions discern between samples categorized as moderate and severe in terms of grooming risk, they struggle to make the same distinction between moderate and significant risk samples. Additionally, the distribution of predicted risk underscores the uncertainty inherent in these estimations, as evidenced by the variance across the distributions. With the exception of moderate risk samples in Victim and Decoy conversations, our predictions exhibit substantial variance. These findings underscore the complexity involved in developing robust estimators for grooming risk within natural language contexts."}, {"title": "4 Discussion and Conclusion", "content": "This paper investigates whether SBERT can effectively discern varying degrees of grooming risk inherent in predatory conversations. We fine-tune and evaluate a language model regressor on predatory conversations across different participant groups. Our analysis highlights that while fine-tuning aids language models in learning to assign grooming scores, they exhibit high variance in predictions, particularly in contexts with higher degrees of grooming risk. This discrepancy is tied to cases where surface form text does not contain explicit identifiers of grooming, but rather utilizes indirect speech pathways to manipulate victims. In such cases, fine-tuning sentence embedding models does not help the model learn nuanced tasks. The sole reliance on word form and incentivizing training loss lead to learning shortcuts while ignoring nuance [2]. Even with the integration of long-range context, the task of encoding intricate lexical semantic phenomena to enhance natural language understanding continues to be a challenge [1, 17]. This finding underscores the need for robust modeling of indirect speech acts employed in grooming contexts by language models."}]}