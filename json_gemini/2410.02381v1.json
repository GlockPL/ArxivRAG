{"title": "METAMETRICS: CALIBRATING METRICS FOR GENERATION TASKS USING HUMAN PREFERENCES", "authors": ["Genta Indra Winata", "David Anugraha", "Lucky Susanto", "Garry Kuwanto", "Derry Tanti Wijaya"], "abstract": "Understanding the quality of a performance evaluation metric is crucial for ensuring that model outputs align with human preferences. However, it remains unclear how well each metric captures the diverse aspects of these preferences, as metrics often excel in one particular area but not across all dimensions. To address this, it is essential to systematically calibrate metrics to specific aspects of human preference, catering to the unique characteristics of each aspect. We introduce METAMETRICS, a calibrated meta-metric designed to evaluate generation tasks across different modalities in a supervised manner. METAMETRICS optimizes the combination of existing metrics to enhance their alignment with human preferences. Our metric demonstrates flexibility and effectiveness in both language and vision downstream tasks, showing significant benefits across various multi-lingual and multi-domain scenarios. METAMETRICS aligns closely with human preferences and is highly extendable and easily integrable into any application. This makes METAMETRICS a powerful tool for improving the evaluation of generation tasks, ensuring that metrics are more representative of human judgment across diverse contexts.", "sections": [{"title": "1 INTRODUCTION", "content": "Evaluating machine-generated sentences has long been one of the main challenges in natural language processing (NLP). Callison-Burch et al. (2006) provide multiple examples where a high BLEU score does not necessarily indicate true sentence similarity, and conversely, highly similar sentence pairs can receive low BLEU scores. BERTScore (Zhang et al., 2019) is designed to capture semantic similarities, also falls short in capturing the all the nuances to have a comprehensive evaluation. In light of recent advancements such as Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022), ensuring that generated outputs align with human preferences has become increasingly critical. Models that optimize for human preference, rather than solely relying on traditional metrics, have demonstrated superior performance in producing content that aligns with human preference (Rafailov et al., 2024; Winata et al., 2024). This shift highlights the need for evaluation metrics that accurately reflect human subjective judgments across multiple dimensions. Such metrics are essential for guiding models to generate more human-aligned outputs by comparing their results against human judgments. Metrics that exhibit a high correlation with human ratings are considered more reliable and effective for evaluating model performance.\nThis is particularly important for NLP tasks, where the subtleties of human language and context significantly influence quality assessments. For example, in machine translation (Freitag et al., 2023; Juraska et al., 2023), the accuracy and fluency of the translated text are critical factors considered by humans. Similarly, in text summarization (Fabbri et al., 2021), the coherence, relevance, and con-"}, {"title": "2 AREN'T EXISTING METRICS GOOD ENOUGH?", "content": "The challenge of developing metrics tailored to specific tasks is not a new issue. For a long time, researchers have struggled to identify appropriate metrics that align with human preferences. In this section, we outline the rationale for the need for a new evaluation metric that is both suitable for our tasks and aligned with human preferences.\nCommonly Used Metrics are Not Robust and Unrepresentative. Metrics such as Perplexity (PPL) (Jelinek et al., 1977; Bengio et al., 2000) and BLEU (Papineni et al., 2002) are widely used to measure the quality of generated text in various generation tasks. However, they are not always the most suitable metrics, particularly when accounting for variations in writing styles and minor character differences due to diacritics (Freitag et al., 2022). Critics have noted that PPL is an English-centric metric that performs well for English but is less effective for other languages, such as Japanese (Kuribayashi et al., 2021). THumB (Kasai et al., 2022) further revealed that widely used metrics fail to adequately capture the semantic quality and diversity of high-quality human annotations, leading to misleading evaluations of model performance, particularly for captions.\nLimited Capability of an Individual Metric. Single metrics like BERTScore (Zhang et al., 2019), ROUGE (Lin, 2004), and METEOR (Banerjee & Lavie, 2005) are beneficial and capable of measuring the quality of generated content. However, they have significant limitations. For instance, in summarization tasks, BERTScore (Recall) excels in assessing consistency but falls short in evaluating coherence. Conversely, BERTScore (F1) performs well in measuring relevance but not consistency (Fabbri et al., 2021). This variability means that a single metric may perform well in one aspect but poorly in another, making it challenging to select the appropriate metric without extensive benchmarking. This ambiguity complicates the process of choosing and utilizing the most suitable metric for a specific use case.\nOne Metric with Too Many Variants and Implementations. Metrics like BERTScore allow the use of various BERT models, but the sheer number of options can make it difficult to identify the best one without a systematic approach. This can lead to an exhaustive search process. BERT models trained on English often underperform on non-English languages, and vice versa. Non-English languages in Latin script may also yield poorer results compared to those in their native scripts. Compounding these issues, many metrics are heavily parameterized, yet their settings are frequently undocumented, resulting in score variations across implementations (Post, 2018; Grusky, 2023).\nTo tackle these challenges, we propose a new paradigm for creating customizable metrics that are robust across various tasks and closely aligned with human preferences. Our approach will enable systematic evaluation and automatic selection of the most suitable combination of metrics that is aligned with human judgments on target tasks, ensuring optimal performance and relevance."}, {"title": "3 \u039c\u0395\u03a4\u0391METRICS", "content": "In this section, we outline the notations and definitions used in our proposed method, and detail the conditions required for optimizing the method based on human preferences. Additionally, we discuss the key factors influencing the optimization process."}, {"title": "3.1 PRELIMINARIES", "content": "Definition. We define $d_i$ as a metric function that maps a sample input $x$ to a score $\\hat{y}_i$, where $i\\in {1, ..., N}$ denotes different metrics. Note that each $d_i$ depends on the type of task it is applied to. For reference-based metric, the data is evaluated in the context of $x = (X_{hyp}, X_{ref})$, where $X_{hyp}$ and $X_{ref}$ correspond to the hypothesis text and the reference text, respectively. For reference-free metric, only $X_{hyp}$ will be used. In VL tasks, the input is extended to include an image, which would correspond to $x = (X_{text}, X_{image})$ where $X_{text}$ and $X_{image}$ correspond to the caption and image, respectively.\nGiven a set of N evaluation metrics {$\\hat{y}_1,..., \\hat{y}_N$}, we define $\\Phi$ to compute a scalar meta-metric score of $\\hat{y}_{MM}$. The idea of utilizing multiple metrics is to combine scores from multiple metrics"}, {"title": "3.2 HUMAN PREFERENCE OPTIMIZATION", "content": "Notations and Objective. Recall that we aim to calibrate $\\theta_{MM}$, that maximizes an objective calibration function $p(\\hat{y}_{MM}, z)$, where $z$ denotes human assessment scores-encompassing any score annotated by human evaluators. $p$ is a function that measures the alignment between $z$ and METAMETRICS scores, $\\hat{y}_{MM}$. METAMETRICS is designed to combine scores of multiple metrics $\\theta_1(x), \\theta_2(x),..., \\theta_N(x)$, learning the weights $w_i$ to assign to each $\\hat{y}_i = \\theta_i(x)$ to maximize $p(\\hat{y}_{MM}, z)$. Each metric has its score $\\hat{y}_i$ ranges within a specific minimum and maximum value. Some metrics, particularly neural-based ones, can fall outside this defined range. To standardize these metrics, we need to normalize them to a common scale that ranges from 0 to 1. In this scale, 0 represents poor translation performance, while 1 represents perfect translation performance. We apply pre-processing to $\\hat{y}_i$ before we learn to combine these scores in METAMETRICS training. The pre-processing is defined in Appendix B.2.\nThe advantage of METAMETRICS lies in its flexibility and adaptability to different tasks or domains. Certain metrics on certain tasks may exhibit strong correlations with human judgments, and by constructing a composite metric that learns to integrate these reliable metrics with human judgments, we can enhance the overall correlation with human evaluations."}, {"title": "3.3 OPTIMIZATION METHODS", "content": "In this work, we focus on two optimization methodologies to train METAMETRICS: Bayesian Optimization (BO) and Boosting. BO offers the advantage of interpretability, allowing us to clearly identify which metrics contribute most significantly to the final outcome. Conversely, the Boosting method excels in enhancing alignment and accounting for the compositionality of different metrics, even when dealing with more complex functions. Although we can measure the contribution of each metric, the clarity and distinctness of these contributions are more pronounced with BO compared to Boosting."}, {"title": "3.3.1 BAYESIAN OPTIMIZATION (BO)", "content": "BO constructs a posterior distribution of functions, typically modeled as a Gaussian Process (GP), to represent the function being optimized. As observations accumulate, the posterior distribution becomes more precise, allowing the algorithm to identify which regions of the parameter space to"}, {"title": "3.3.2 BoOSTING METHOD", "content": "The boosting method we investigate is Extreme Gradient Boosting (XGBoost) (Chen & Guestrin, 2016). Gradient-boosted trees have long been recognized as a robust technique, supported by extensive literature demonstrating their effectiveness (Friedman, 2001). To enhance the efficiency of our metric, we implement iterative pruning to eliminate less important metrics from the input, resulting in a more compact and faster metric."}, {"title": "4 EXPERIMENTS SETUP", "content": "In this work, we explore two optimization methodologies: BO and Boosting. BO provides interpretability, clearly highlighting the metrics that significantly impact the final outcome. In contrast, Boosting enhances alignment and addresses the compositionality of metrics, even for complex functions. While we can measure each metric's contribution through feature importance in Boosting, BO is more interpretable than Boosting. We also compare calibrating with all the metrics and calibrating only using top-5 correlated metrics from the tuning set.\nAbstractive Text Summarization. We use the SummEval (Fabbri et al., 2021) and BenchmarkLLM (Zhang et al., 2024) datasets for text summarization evaluation. SummEval, based on CNN/DailyMail (Hermann et al., 2015), is rated by human annotators on coherence, consistency, fluency, and relevance using a 1 to 5 Likert scale. BenchmarkLLM includes both CNN/DailyMail and XSUM (Narayan et al., 2018) articles, with summaries assessed for faithfulness (binary scale), coherence, and consistency (1 to 5 Likert scale). To evaluate the generalization capabilities of METAMETRICS, we combine coherence and consistency scores from both datasets, ensuring no overlap in articles, allowing us to assess METAMETRICS's performance across different human annotators. We use the Kendall 7 correlation function as our objective calibration function p. We refer to our metric as METAMETRICS-SUM (Table 1).\nMachine Translation. For this task, we train both reference-free and reference-based versions of the metric. We train our metric using the 3-year training data from WMT shared tasks datasets from 2020 to 2022 that are annotated using MQM annotation scores. We evaluate on MQM dataset from WMT23 shared task. We use kendall 7 correlation function as our objective calibration function p. We refer to our metric as METAMETRICS-MT (Table 2).\nQuestion Answering. For this task, we evaluate METAMETRICS performance across multiple QA subtasks, including Open Domain QA (Open-QA) (Rajpurkar, 2016; Berant et al., 2013; Petroni et al., 2019), Reading Comprehension QA (RCQA) (Bajaj et al., 2016; Fisch et al., 2019; Yang et al., 2018), and Reasoning QA. Open-QA retrieves answers from large, unstructured datasets like Wikipedia, where answers are objective. RCQA requires the model to read a passage to derive answers directly from it, while Reasoning QA emphasizes logical inference, where answers cannot be directly extracted from the passage."}, {"title": "5 RESULTS", "content": "In this section, we present the results of our METAMETRICS across five diverse tasks, covering both NLP and VL downstream applications.\nAbstractive Text Summarization. For this task, we refer to our metric as METAMETRICS-SUM. Table 9 shows the results of abstractive text summarization task. Overall, our proposed model with XGBoost outperforms all other baselines, including all ensemble models and overall best automatic metric (BERTScore (F1)). An interesting observation we find is our iterative model with fewer"}, {"title": "6 ANALYSIS AND DISCUSSION", "content": "In this section, we analyze the advantages of the methods in terms of interpretability, efficiency, and robustness. Additionally, we discuss how METAMETRICS enhances the capability of model evaluation."}, {"title": "6.1 INTERPRETABILITY", "content": "Interpretability is a key aspect in METAMETRICS, as the optimization process inherently reveals the impact of each metric on the final score. The chosen methods, such as feature importance in"}, {"title": "6.2 EFFICIENCY", "content": "Table 4 highlights the compute efficiency and superior performance of MetaMetrics-RM. METAMETRICS-RM, utilizing GP, combines GRM-Gemma-2B, Internlm2-7B-Reward, Skywork-Reward-Llama-3.1-8B, Internlm2-1.8B-Reward, and GRM-Llama3-8B totaling approximately 27B parameters. Despite this relatively small compute memory footprint, it outperforms models more than 2\u00d7 its size, achieving 0.4% higher overall accuracy than the current state-of-the-art. This demonstrates that METAMETRICS-RM offers state-of-the-art accuracy using significantly more cost-effective models, making it an ideal choice for resource-constrained applications while delivering even better performance.\nMETAMETRICS also excels at selecting a sparse yet highly effective set of metrics in scenarios with a vast number of potential evaluation metrics. As depicted in Figure 4, both GP and XGBoost are capable of identifying small subset of key metrics that correlate strongly with human preferences, reducing the need to evaluate a wide range of metrics. This selective approach identifies key metrics to a specified task while minimizing computational overhead without compromising performance. In other words, METAMETRICS is well-suited in low-resource settings while still providing better alignment with human preferences across multiple tasks."}, {"title": "6.3 ROBUSTNESS", "content": "We evaluate the robustness of METAMETRICS through cross-dataset experiments in image captioning, tuning on one dataset and testing on another. Despite variations in domain and content, our results consistently demonstrate the effectiveness of METAMETRICS. Figure 3 shows that METAMETRICS-CAP tuned on THumB 1.0 and tested on Flickr8k and vice-versa out performs all individual metrics. This robustness indicates that METAMETRICS is not overfitting to specific dataset characteristics, but rather learning to combine metrics in a way that aligns with general human judgments of caption quality. This generalization capability is crucial for real-world applications, where the evaluation metric may need to perform well on diverse and potentially out-of-domain data."}, {"title": "7 RELATED WORK", "content": "Performance evaluation metrics for natural language generation tasks can be categorized into several categories based on how the metric compares generated texts with reference texts.\nSurface-level Metrics. These metrics compare the generated text to reference text at the word level, focusing on n-gram overlap or direct lexical matching. Common metrics include BLEU that measures precision of n-grams between the generated- and reference-texts (Papineni et al., 2002), ROUGE that measures recall (Lin, 2004), METEOR (Banerjee & Lavie, 2005) that goes beyond exact word overlap and considers stemming, synonyms, and paraphrasing, and chrF (Popovi\u0107, 2015) that calculates similarity based on character n-grams rather than word n-grams using F-score.\nEmbedding-based Metrics. These metrics rely on word or sentence embeddings to measure semantic similarity between generated and reference text, capturing deeper meaning rather than surface overlap. Examples include MoverScore (Zhao et al., 2019), which considers semantic similarity at the word level and accounts for word movement (alignment) between texts, and BERTScore (Zhang et al., 2019), a neural-based metric that compares semantic similarity and understands contextual relationships using BERT embeddings. COMET (Rei et al., 2022) and BLEURT (Sellam et al., 2020) are neural-based metrics that use contextualized embeddings from models like BERT as inputs to train models to generate prediction estimates of human judgments. For vision-language tasks, ClipScore (Hessel et al., 2021) uses embeddings to compare vision and language modalities."}, {"title": "8 CONCLUSION", "content": "Understanding the quality of a performance evaluation metric is crucial for ensuring alignment with human preferences. However, it remains unclear how well each metric captures the diverse aspects of these preferences, as metrics often excel in one particular area but not across all dimensions. To address this, it is essential to systematically calibrate metrics to specific aspects of human preference, catering to the unique characteristics of each aspect. We introduce METAMETRICS, a calibrated meta-metric designed to evaluate generation tasks across different modalities in a supervised manner. METAMETRICS optimizes the combination of existing metrics to enhance their alignment with human preferences. Our method demonstrates flexibility and effectiveness in both language and vision downstream tasks, showing significant benefits across various multilingual and multi-domain scenarios. Our proposed metric aligns closely with human preferences and it is also highly extendable and easily integrable into any application. This makes it a powerful tool for improving the evaluation of generation tasks, ensuring that metrics are more representative of human judgment across diverse contexts."}, {"title": "LIMITATIONS AND FUTURE WORKS", "content": "In our experiment, we utilize metrics that are commonly employed to evaluate each downstream task. However, due to limitations in computational resources, we exclude LLMs exceeding 10.7B parameters to ensure that our metrics can be executed on commercial GPU resources with a maximum memory capacity of 40GB. Additionally, we restrict our exploration of metrics to avoid exhaustively incorporating LLM-based metrics, given our capacity and resource constraints.\nLooking ahead, there are significant opportunities for further exploration. We can investigate metrics across additional languages and in larger multilingual generation tasks, where we identify METAMETRICS for multilingual applications (Winata et al., 2019). Furthermore, exploring metrics in the context of speech modalities presents another promising avenue for research."}, {"title": "3.1 METHOD DETAILS", "content": "C.1 METHOD DETAILS\nC.1.1 MATERN KERNEL\nWe train BO using GP with Matern kernel, a generalization of the RBF, distinguished by an additional parameter that controls the smoothness of the resulting function (Williams & Rasmussen, 2006). Conversely, as the parameter approaches infinity, the Matern kernel converges to the RBF kernel. The kernel is described as below:\n$\\mathbf{k(w,w') = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\left(\\frac{\\sqrt{2\\nu}d(w,w')}{\\ell}\\right)^{\\nu}K_{\\nu}\\left(\\frac{\\sqrt{2\\nu}d(w,w')}{\\ell}\\right)}$\nwhere d(, ) is the Euclidean distance, $K_{\\nu}(\u00b7)$ is a modified Bessel function and $\\Gamma(\u00b7)$ is the gamma function."}, {"title": "C.1.2 ITERATIVE-BASED PRUNING", "content": "Below is the implementation of Iterative-based Pruning performed during training.\nAlgorithm 1 Iterative-based Pruning with XGBoost\n1: procedure ITERATIVEXGBOOST(X, y, k)\n2:\tF \u2190 {f1, f2,..., fp}\n3:\tP \u2190[]\n4:\tFleast \u2190\n5:\tfor i \u2190 1 to k do\n6:\tTrain PXGB on XF with CV\n7:\tZi \u2190 Importance(PxGB)\n8:\tP[i] \u2190 \u03c1(\u03a6xGB (XF), y)\n9:\tfleast [i] \u2190 argmin(Ii)\n10:\tFleast Fleast [i]\n11:\tF F \\{fleast}\n12:\tend for\n13:\ti* \u2190 argmax(P)\n14:\tFbest \u2190 FU Fleast [i*:]\n15:\tTrain final fxGB on XFbest\n16:\treturn fxgb\n17: end procedure"}, {"title": "C.2 HYPER-PARAMETERS", "content": "C.2.1 BAYESIAN OPTIMIZATION\nTable 7 describes the hyper-parameter settings that we use for our experiments.\nC.2.2 BoOSTING METHOD\nFor XGBoost training, we use a different objective depending on the task. We perform parameter searching with hyper-parameter values in Table 8."}, {"title": "D DETAILED RESULTS", "content": "In this section, we provide more detailed results that could not be included in the main paper due to space constraints."}]}