{"title": "Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model", "authors": ["Ben Koska", "Mojm\u00edr Horv\u00e1th"], "abstract": "We present a novel 4.5B parameter small language model that can handle multiple input and output modalities, including text, images, videos, and audio. Despite its small size, the model achieves near state-of-the-art performance on a variety of tasks, demonstrating the potential of multi-modal models to tackle complex real-world problems. Our approach leverages recent advancements in language modeling and multi-task learning to create a versatile and high-performing model that can even be deployed for edge inference. Experimental results show the model's strong performance across multiple benchmarks, paving the way for further progress in multi-modal artificial intelligence.", "sections": [{"title": "I. INTRODUCTION", "content": "Humans interact with the world through multiple senses sight, sound, touch, and language, each providing complementary information that helps us understand and reason about our environment. One of the primary goals in Artificial Intelligence has been to develop a general-purpose assistant that can mimic this multi-modal intelligence, processing and generating a diverse range of information [1]. However, current large language models (LLMs) still struggle to effectively handle non-textual inputs and outputs, limiting their applicability in real-world scenarios.\nRecent works focusing on providing LLMs with a second sense in addition to text, namely vision, have shown promising results [2] [3] [4]. Recent works further demonstrate improved performance through techniques such as increasing the pre-training data [5] or scaling up the vision encoder [6]. To evaluate the performance of Multi-Modal LLMs, various benchmarks have been proposed [7] [8] [9] [10].\nFurthermore, many models focus on text-image pairs [2] [5] [6] or more recently text-video pairs [11] [12]. To allow for a natural way of interacting with a general assistant, there exists a need to interact using natural speech instead of just using text. Recent works on allowing more modalities than just text and visuals show promising results [13] [14] [3].\nWhile some works explore the usage of small models [15], most still utilize models too large to run on-edge (on a smart phone or laptop) causing the need for expensive hardware for inference, a stable internet connection on devices and provide an attack surface for malicious actors.\nHowever, despite the promising results from recent research on multi-modal language models, no standard recipe has been established for training these models effectively. Different techniques have shown improvements on various benchmarks, but their performance can vary widely across different tasks and datasets. This lack of a unified approach highlights the need for further exploration and experimentation to develop a more robust and generalized multi-modal modeling framework.\nTo this extent, in this paper we present EAGLE (4.3B parameters), a large-language model with vision, audio and text input capabilities which outputs text, as well as EAGLE-Assistant (4.5B parameters), which extends the capabilities of EAGLE to allow it to output audio, enabling an end-to-end (audio-in, audio-out), natural verbal conversation between the user and model."}, {"title": "II. APPROACH", "content": "The architecture of EAGLE (see Fig. 3) combines 3 components, a large-language model (phi-3-mini [16], 128K context window variant - 3.7B parameters), an audio tower (whisper-small [17] - 244M parameters) and a vision tower (CLIP [18] ViT Large Patch14 - 303M parameters).\nAs the output space of the audio tower, vision tower and language model differ we employ a projection layer."}, {"title": "B. Training", "content": "We initialize the language model using the weights from Phi3.5 mini long context (128K tokens).\nPre-training\nFor pre-training we utilize a two-stage approach.\nStage 1: Pre-training projection. In this stage we freeze the image encoder, audio encoder and language model. The projectors are randomly initialized and are then trained on a random 30M token subset of our pre-training dataset.\nStage 2: Full-parameter fine-tuning: In this stage we unfreeze all modules and train on the remainder of the pre-training dataset.\nFine-tuning:\nFor training of the audio decoder and all subsequent fine-tuning (Instruction tuning, chat-tuning and Function-calling) we keep all modules unfrozen."}, {"title": "C. Data", "content": "For pre-training, we utilize a custom dataset which consists of a combination of image-text pair datasets (e.g., LAION-COCO), interleaved image-text document datasets (e.g., OBELICS [21]), synthetic OCR Data (e.g. RenderedText) and real-world OCR Data (e.g. IDL [22], PDFA), a synthetic audio version of TriviaQA [23], speech data (e.g. LibriSpeech [24]) and self-generated synthetic data. For better understanding of emotion, non-text verbal clues (e.g. such as coughing, sarcastic voice) and speed/volume of voice we a) train a transcription model with tokens to identify theseand transcribe thousands of hours of speech with said model and b) generate synthetic speech using Text-to-Speech (TTS) and Speech-to-Speech (STS) models.\nFor synthetic data we utilize a combination of approaches:\n1) Real-base-synthetic-data: We utilize real data, such as PDFs, Charts and Images which we then use as a direct base to generate synthetic data (such as generating a converstaion about a specific PDF or turning a text in a conversation into audio using text-to-speech)\n2) Double-Synthetic: To cover cases which lack a substantial amount of (accessible) real base data, we utilize a double-synthetic approach. In this approach we generate synthetic charts, letters, images, etc. which are used either as a base to create synthetic conversations, or are created in conjunction with the synthetic data (e.g. conversations)."}, {"title": "III. EVALUATION", "content": "In Table 1 we report the results for EAGLE on standard open-source benchmarks measuring the model's reasoning, vision and audio ability. We compare EAGLE to Phi-3-vision [16], LLAVA-NeXT-34B [27], InternVL2-40B [28], Gemini 1.5 Pro [29], GPT-40 [30] and Claude 3.5 Sonnet [31]. The table is a summation of publicly published numbers. For benchmarks with public leaderboard (e.g. MMBench [9] and MMMU) preference is given to the results on published the leaderboard. EAGLE is evaluated in the manner that is standard for each benchmark and an effort is made to ensure that all values for other models follow the same method of evaluation. Due to a lack of accessible modern LLMs with audio capabilities, which also report other relevant benchmarks such as MMMU and MMBench EAGLE stands alone as an LLM in the category of audio benchmarks. For price per million tokens, we report output prices and, in the case of Gemini 1.5 Pro we utilize the base 128k context window model.\nWe evaluate function calling accuracy using our internal assistant function calling benchmark (see example in Fig. 6). We test function calling with a) functions provided in the system prompt, as well as b) fine-tuning EAGLE-A with a small (750 samples per function) synthetic dataset of calling the available functions, without providing them in the system prompt. We do not notice a meaningful difference between one method and the other (97.2% in-context vs 97% fine-tuned) but do observe a significant drop (from 97% to 89.5%) in accuracy using functions provided through fine-tuning in cases where fine-tuning and in-context are mixed.\nAs modern LLMs with voice output emerge [30] [13] [32], the necessity for voice-output LLM benchmarks becomes apparent. While we have developed an internal benchmark to evaluate training progress and conduct experiments, it is not sufficiently robust to comprehensively assess a wide array of models. Consequently, the creation of a comprehensive and robust benchmark is left to future work."}, {"title": "IV. EDGE INFERENCE", "content": "To efficiently deploy the model on-edge we utilize several strategies:\n1) Mixed-precision quantization\n2) Hand-optimized implementation\n3) Quantization-aware full-parameter fine-tuning\nRunning the final version of the mobile model on an iPhone 15 Pro (see Fig. 7) yields a generation speed of nearly 17 tokens per second. Furthermore, running the EAGLE-A model in full end-to-end (audio-in, audio-out) voice assistant model is supported, and yields above real-time generation and a time-to-first-token (TTFT) of 425ms, allowing for natural, real-time voice communication with the assistant."}, {"title": "A. Quantization-Aware Fine-Tuning", "content": "Building upon the work of [33] we developed a new training technique, which we then utilized to fine-tune the base model at different quantization settings. By utilizing full-parameter quantization-aware fine-tuning we manage to regain most of the performance loss of quantization (see Table 2). Using our mixed-precision quantization configuration (resulting in, on average 5.5-bits per parameter), we manage to reduce the model size from 18GB (at float32) to just over 3GB, allowing the model to fit into the memory of most modern smart phones. Our experiments show that full parameter tuning yields significantly better results."}, {"title": "VI. CONCLUSION", "content": "We introduced EAGLE and EAGLE-A, two compact multi-modal models with 4.3 and 4.5 billion parameters, capable of processing and generating text, images, audio, and video. Despite their smaller size, both models achieve competitive performance across various benchmarks, showcases new possibilities of on edge computing and new ways to think about large language models.\nKey innovations in these models include the integration of multiple modality towers and quantization-aware fine-tuning, enabling efficient deployment on resource-constrained devices. While the models perform well, limitations such as the need for more robust image-only training data and reliance on synthetic datasets remain.\nFuture work will focus on improving training efficiency, expanding the model's capabilities to handle broader tasks, and developing better multi-modal benchmarks, particularly for audio-based tasks. EAGLE represents a step toward more capable and versatile multi-modal LLM system, paving the way for advanced general-purpose assistants without the need of a heavy-duty GPU."}]}