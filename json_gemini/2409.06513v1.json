{"title": "SINE, TRANSIENT, NOISE NEURAL MODELING OF PIANO NOTES", "authors": ["Riccardo Simionato", "Stefano Fasciani"], "abstract": "This paper introduces a novel method for emulating piano sounds. We propose to exploit the sine, transient, and noise decomposition to design a differentiable spectral modeling synthesizer replicating piano notes. Three sub-modules learn these components from piano recordings and generate the corresponding harmonic, transient, and noise signals. Splitting the emulation into three independently trainable models reduces the modeling tasks' complexity. The quasi-harmonic content is produced using a differentiable sinusoidal model guided by physics-derived formulas, whose parameters are automatically estimated from audio recordings. The noise sub-module uses a learnable time-varying filter, and the transients are generated using a deep convolutional network. From singular notes, we emulate the coupling between different keys in trichords with a convolutional-based network. Results show the model matches the partial distribution of the target while predicting the energy in the higher part of the spectrum presents more challenges. The energy distribution in the spectra of the transient and noise components is accurate overall. While the model is more computationally and memory efficient, perceptual tests reveal limitations in accurately modeling the attack phase of notes. Despite this, it generally achieves perceptual accuracy in emulating single notes and trichords.", "sections": [{"title": "Introduction", "content": "Piano sound synthesis is a challenging problem and is still a topic of great interest. Historically, accurate piano models have been developed mainly using physical modeling techniques (Chabassier et al., 2014), which are required to discretize and solve partial differential equations. This approach leads to significant computational challenges. Recently, neural-based approaches, based on autoregressive architecture (Hawthorne et al., 2019) or Differentiable Digital Signal Processing (DDSP) framework (Engel et al., 2020), have produced convincing audio signals. In particular, the DDSP framework, which involves integrating machine learning techniques into digital signal processors, has been mainly explored. Among traditional synthesis algorithms, DDSP has been integrated with wavetable (Shan et al., 2022) and frequency modulation (FM) (Caspe et al., 2022) synthesis. DDSP allows data generation conditioned by MIDI information, specifically generating parameters used for digital signal algorithms. DDSP has also been used with Spectral Modelling Synthesis (Serra & Smith, 1990) (SMS), which considers the sound signal as the sum of harmonic and noise parts. The harmonic content can be synthesized with a sum of sine waves whose parameters are directly extrapolated from the target. The noise is usually approximated with white noise processed with a time-varying filter. This method has been applied to (Kawamura et al., 2022) for a mixture of harmonic audio signals, guitar (Wiggins & Kim, 2023; Jonason et al., 2023), and piano synthesis (Renault et al., 2022). The method has been extended considering transients for the case of sound effects (Liu et al., 2023) and percussive sound synthesis (Shier et al., 2023). In the first-mentioned work (Liu et al., 2023), the transients are synthesized using the discrete cosine transform (DCT) (Verma & Meng, 2000), transforming an impulse signal in a sine wave. The authors synthesized the transient by learning the amplitudes and frequencies of its discrete cosine transforms and synthesized it using sinusoidal modeling. In the second work (Shier et al., 2023), the transients are added to the signal using a Temporal convolutional neural network. In the reviewed works, audio is mostly generated with a sampling frequency of 16kHz, except in (Liu et al., 2023) and (Jonason et al., 2023), where the sampling rates are 22.5 and 48 kHz, respectively."}, {"title": "", "content": "Artificial neural networks can learn complex system behaviors from data, and their application for acoustic modeling has been beneficial. Differentiable synthesizers aiming to synthesize acoustic instruments have already been proposed, but, as we have previously detailed, reviewed studies on guitar and piano neural synthesis employ models that learn from datasets filled with recorded songs and melodies. This approach increases the complexity of the tasks and can limit the network's ability to generalize, leading to challenges in scenarios not represented in the datasets. One example is the reproduction of isolated individual notes, where other concurrent sonic events might obscure the harmonic content of single notes in the dataset. Consequently, neural networks require a large amount of internal trainable parameters and training data to learn these complex patterns.\n\nThe work we detail in this paper extends our prior proposal for modeling the harmonic contents of piano notes (Simionato et al., 2024), and faces the problem from another perspective. We address the mentioned shortcoming by first emulating individual piano notes and then by incorporating the effects of key coupling, re-striking of played keys, and other scenarios that could occur during a real piano performance. Here, we combine the Differentiable Digital Signal Processing (DDSP) approach with the sines, transients, and noise (STN) decomposition to synthesize the sound of individual piano notes, while a convolutional-based network is employed to emulate trichords. The generation of the notes is guided by physics knowledge, which allows designing a model with significantly fewer internal parameters. In addition, the model predicts a few audio samples at a time, allowing interactive low-latency implementations. The long-term objective of this research is to develop novel approaches for piano emulation that improve fidelity compared to existing techniques and are less computationally expensive than physical modeling while requiring less memory than sample-based methods.\n\nOur approach includes three sub-modules, one for each component of the sound: harmonic, transient, and noise. The harmonic component is modeled using differentiable spectral modeling to synthesize the sines waves composing the sound. The quasi-harmonic module, guided by physics formulas, predicts the inharmonic factor to produce the characteristic inharmonicity of the piano sound and the amplitude's envelope for each partial. The model also considers phantom partials, beatings, and double decay stages. The transients are modeled by generating a sine-based waveform that is transformed using the Inverse Discrete Cosine Transform (IDCT), producing the impulsive component of the sound. The vector of samples representing the waveform is computed entirely using a convolutional-based neural network. Finally, the noise component is synthesized filtering in the frequency domain of a Gaussian noise signal. The noise sub-model predicts the coefficients of the noise filter. Once the notes are computed, we emulate the coupling between different keys for trichords. A convolutional neural network processes the sum of individual note signals to emulate the sound of real chords.\n\nThe approach synthesizes the notes separately, removing the need to have a large network learning how to synthesize all the notes using the same parameters. In this way, we need one model per key, similar to the sample-based methods, but we need to store a few parameters without storing the audio. Interpolation between different velocities is also automatically modeled. This does not preclude the modeling of coupling or re-striking of keys, which is not allowed by sample-based methods, by adding an additional small network and, in turn, without adding significant complexity as in physical modeling methods.\n\nThe rest of the paper is organized as follows. Section 2 describes the physics of the piano, focusing on aspects used to inform our model. Section 3 details our methodology, the STN architectures, the used datasets, and the experiments we have carried out. Section 4 summarizes and discusses the results we obtained, while Section 5 concludes the paper with a summary of our findings."}, {"title": "The Piano", "content": "The piano is an instrument that produces sound by striking strings with hammers. When a key is pressed, a mechanism moves the corresponding hammer, typically made of felt layers, which is accelerated to a few meters per second (1 to 6 m/s) before release. One important aspect of piano strings is the tension modulation. This modulation occurs due to the interaction between the string's mechanical stretching and viscoelastic properties. It allows for shear deformation and changes in length during oscillation. These length changes cause the bridge to move sideways, which excites the string to vibrate in a different polarization. This vibration gains energy through coupling and induces the creation of longitudinal waves (Etchenique et al., 2015). Longitudinal motion (Nakamura, 1994; Conklin Jr, 1999) is also generated due to the string stretching during the collision with the hammer. When the hammer comes into contact with the string, it causes the string to elongate slightly from its initial length. This stretching creates longitudinal waves that propagate freely. The amplitude of these waves is relatively small compared to the transverse vibrations. The longitudinal vibration of piano strings greatly contributes to the distinctive character of low piano notes, and their properties and generation are not fully understood yet. They are named phantom partials and are generated by nonlinear mixing, and their frequencies are the sum or difference of transverse model frequencies. We can distinguish two types:"}, {"title": "", "content": "even phantoms and odd phantoms. Even phantoms have double the frequency of a transverse mode, while odd ones have a frequency as the sum or difference of two transverse modes. Even and odd are pointed as the free and forced responses of the system (Bank & Sujbert, 2003).\n\nAnother important phenomenon in piano strings happens when keys feature multiple strings (Weinreich, 1977). Imperfections in the hammers cause the strings to vibrate with slightly varying amplitudes. In the case of two strings, they initially vibrate in phase, but each string loses energy faster than when it vibrates alone. As the amplitude of one string approaches zero, the bridge continues to be excited by the movement of the other string. As a result, the first string reabsorbs energy from the bridge, leading to a vibration of the opposite phase. This antiphase motion of the strings creates the piano's aftersound. Any discrepancies in tuning between the strings contribute to this effect, together with the bridge's motion, which can cause the vibration of one string to influence others. These phenomena affect the decay rates and produce the characteristic double decay. Similarly, the same effects can be caused by different polarizations in the string. Specifically, since the vertical polarization is greater than that one parallel to the soundboard, more energy is transferred to the soundboard, resulting in a faster decay. When the vertical vibrations are small, the horizontal displacement becomes more significant, producing double decay. In addition, the horizontal vibration is usually detuned with respect to the vertical one (\u00b10.1 or 0.2 Hz (Tan, 2017)), creating beatings.\n\nString stiffness leads to another significant aspect characterizing the piano sound: the inharmonicity of its spectrum (Podlesak & Lee, 1988). Specifically, the overtones in the spectra shift, so their frequencies do not have an integer multiple relationship. For this reason, they are called partials instead of harmonics. There are slightly different formulations of this aspect; in this work, we consider the following formula, derived from Timoshenko beam equations (Chabassier et al., 2014):\n\n$\\displaystyle f_m = mF_0(1 + Bm^2).$ \n\nB is the inharmonicity factor, determining the level of the sound's inharmonicity, m is the partial number, and F0 is the fundamental frequency in the ideal case, such as the string without stiffness. This equation simplifies the computation of B and F0 when designing the dataset, as detailed in Section 3.7. Slightly different formulas for the inharmonicity factor are found in literature, and determining which one more closely matches a real piano remains an open challenge. However, the differences between these formulas are negligible (Chabassier et al., 2014). Lastly, another important factor in the piano sound is the frequency-dependent damping due to energy losses while the string vibrates. One physically justified model is the Valette & Cuesta (VC) damping model (Valette et al., 1988). In this model, the damping parameter is associated with the quality factor by:\n\n$\\displaystyle Q_m = \\frac{\\pi \\cdot f_m}{\\sigma_m}$\n\nwhere fm = w/2\u03c0 and om is the decay rate of the m-th partial.\n\n$\\displaystyle \\frac{1}{Q_m} = \\frac{1}{Q_{m,air}} + \\frac{1}{Q_{m,vis}} + \\frac{1}{Q_{m,ther}}$\n\n$Q_{m,air}, Q_{m,vis},$ and $Q_{m,ther}$ are, respectively, the quality factors associated with losses due to air friction, viscoelasticity, and thermoelasticity of the string (Issanchou et al., 2017). The three items are described as follows:\n\n$\\displaystyle \\frac{1}{Q_{m,air}} = \\frac{2\\pi \\eta_{air} + 2\\lambda \\sqrt{\\pi \\eta_{air} \\rho_{air} f_m}}{2\\pi \\rho S f_m}$\n\n$\\displaystyle \\frac{1}{Q_{m,vis}} = \\frac{4 \\pi^2 S E I \\delta_{vis}}{T_0}$\n\nwhile $Q_{m,ther}$ is constant. $\\eta_{air}$ and $\\rho_{air}$ are the dynamic viscosity and density of air, $\\rho$ is the string density, $\\delta_{vis}$ is the viscoelastic loss angle, d the diameter of the string, S the cross-sectional area, E the young modulus, I the area moment of inertia, and $T_0$ is tension at rest.\n\nThe soundboard damping is a linear phenomenon of modal nature, and the attenuation increases with the frequency of the modes (Ege, 2009). Therefore, the damping function is supposed to be increasing and have a sub-linear growth at infinity:\n\n$\\displaystyle S_{loss}(f) < Af + B$\n\nwith A and B > 0."}, {"title": "Methodology", "content": "The proposed modeling method presents three trainable sub-modules synthesizing different aspects of the piano sound. The respective quasi-harmonic, transient, and noise components extracted from real piano recordings are utilized to"}, {"title": "Quasi-Harmonic Model", "content": "The architecture of the quasi-harmonic model is visible in Figure 1, while the layer composing it is in Figure 2. The quasi-harmonic model consists of 3 layers: the Inharmonicity layer, the Damping layer, and the Sine Generator layer. Physical information is embedded in the model when generating the note's partials and envelope. The Sine Generator layer takes the partial frequency values and generates the corresponding sine waves. In the quasi-harmonic model, the learning is pursued using the multi-resolution STFT loss and the MAE of the RMS energy computed per iteration, as similarly in (Simionato et al., 2024). The multi-resolution STFT loss is the average of STFT losses computed with window sizes of [256, 512, 1024, 2048, 4096] and 25% overlap. By doing so, the smallest frequency resolution is 93.75 Hz. The loss is normalized by the norm of the target spectrogram, allowing for greater weight to be assigned to the note's release phase, even when it has a small amplitude. The losses compare the predicted quasi-harmonic content against the quasi-harmonic one separated from the real recording as explained in Section 3.7."}, {"title": "Inharmonicity Layer", "content": "The Inharmonicity layer adjusts the generation of partials based on the learnable inharmonicity factor B, using Equation 1:\n\n$\\displaystyle f_m = mF_0(1 + B m^2).$\n\nThis approach provides a closer approximation to the inharmonic characteristics of the note. The tuning employs the Cent loss (Simionato et al., 2024), which takes into account the first 6 partials. The actual fundamental frequency F0 is estimated from the piano recordings in the dataset by using a peak estimation algorithm to detect the frequencies of the partials, as detailed in Section 3.7."}, {"title": "Damping Layer", "content": "The Damping layer is based on the VC damping model. Partials exponentially decay over time:\n\n$\\displaystyle Y_{harm}(n) = a_m e^{-\\sigma_m n} y_m (n)$"}, {"title": "", "content": "where Yharm(n) is the quasi-harmonic content of the signal and ym the m-th partial, am is the initial amplitudes, and om it the decay rate of the m-th partial. Deriving from the information detailed in Section 2, we can write:\n\n$\\displaystyle \\sigma_m = \\pi f_m \\left(\\frac{1}{Q_{m,air}} + \\frac{1}{Q_{m,vis}} + \\frac{1}{Q_{m,ther}}\\right)$\n\nwhere $Q_{m,air}, Q_{m,vis}$ are frequency-dependent and $Q_{m,ther}$ is constant. To integrate this information into the model, we simplify the expressions as follows:\n\n$\\displaystyle \\frac{1}{Q_{m,air}} = \\frac{b_0 + b_1}{\\sqrt{f_m}}$\n\nwhere $b_0 = \\frac{2 \\pi \\eta_{air}}{2 \\pi \\rho S}$ and $b_1 = \\frac{2 \\lambda \\sqrt{\\pi \\eta_{air} \\rho_{air}}}{2 \\pi \\rho S}$\n\n$\\displaystyle \\frac{1}{Q_{m,vis}} = b_2 f_m$\n\nwhere $b_2 = \\frac{4 \\pi^2 S E I \\delta_{vis}}{T}$\n\nand finally\n\n$\\displaystyle \\frac{1}{Q_{m,ther}} = b_3,$\n\nwhere $b_3$ represent a constant which can be measured experimentally (Tan, 2017). Substituting Equations 9, 10, 11 in 8, we obtain the following expression for the decay envelope:\n\n$\\displaystyle a_m e^{-\\sigma_m} = a_m e^{-\\pi (b_0 + b_1 \\sqrt{f_m} + b_2 f_m + b_3 f_m)n}$\n\nwhere am is predicted by the Damping layer and b0, b1, b2, and b3 are tunable parameters. The am coefficients are predicted by a network fed with the velocity vel, and the index i, which indicates how many sampling periods have passed since the key was pressed. The network presents a linear FC layer, an LSTM layer, another FC layer featuring the Gaussian Error Linear Unit (GELU) (Hendrycks & Gimpel, 2023), all having with 24 units, an attention layer computing the attention across the partials dimensions, and an output layer having a number of units equal to the desired number of harmonics H. Batch normalization layers are placed before the LSTM and before the attention layers. From the prediction, the absolute value is clipped in case they are greater than 1.\n\nThe modulus operation is applied to the learnable vector b, which is then split into the individual coefficients b0, b1, b2, and b3. These b coefficients are unique to each note and are determined solely by the string's parameters, independent of the index i. By utilizing b0, b1, b2, and b3, we eliminate the need to include soundboard losses in the model, as they are implicitly accounted for by b0 and b3. When incorporating soundboard damping from Equation 5, the bias and the term that depends linearly on frequency are modified to b0 + B and b3 + A, respectively. However, for simplicity, we retain the notation of b0 and b3.\n\nThe actual decay rate for each partial is generated using Equation 12, and the corresponding sine waves are synthesized as follows:\n\n$\\displaystyle Y_{sines} (n) = \\sum_{m=1}^{H} a_m e^{-\\sigma_m n} sin(2\\pi f_m n)$\n\nThe model can generate amplitude values for audio segments of any length without architectural constraints. Using lower sample numbers leads to reduced latency, longer training times, and increased computational costs per sample during inference. However, the accuracy of the results tends to improve, as this approach avoids additional artifacts by allowing the network to update the damping coefficients more frequently. The results presented in this paper pertain to the extreme case where the model generates a single sample per inference iteration."}, {"title": "Beatings and Double Decay", "content": "As mentioned in Section 2, piano strings present vertical and horizontal vibrations. The vibrations have different decay rates. The string exchanges energy with the bridge. The vertical vibration energy dissipates quickly, making up the initial fast decay. As the vertical displacement reduces, horizontal displacement becomes more dominant and exhibits the second slower decay. This created the so-called double decay, while the detuning between the two polarizations creates beatings. Similarly, the same effect can be created when the hammer strikes multiple strings. Strings in the same key cannot be perfectly in tune or slightly differently excited. The detuning creates out-of-phase vibrations that cancel each other out and reduce the energy exchange with the bridge. As before, it creates two different decaying rates and beatings.\n\nThis feature is integrated into the model, generating an additional set of partials. To account for the second polarization, the model employs a feedforward network consisting of two linear hidden layers with 8 units each. The first layer"}, {"title": "Phantom Partials", "content": "The model incorporates longitudinal displacements described in Section 2. Instead of employing a machine-learning approach, phantom partials are computed directly from the predicted transverse partials and their associated coefficients. Specifically, even phantom partials are obtained by doubling the frequencies of the two transverse polarizations and squaring their corresponding coefficients:\n\n$\\displaystyle Y_{even} (n) = \\sum_{m=1}^{H} a_m e^{-2 \\sigma_m n} sin(2 \\pi 2 f_m n)$\n\nNote that doubling the partials leads to doubling the decay rates om. The odd phantom partials are instead given by the sum and difference of consecutive transverse partials and the multiplication of the related coefficients as in (Bank & Sujbert, 2005):\n\n$\\displaystyle f_k = f_n \\pm f_m$\n\n$\\displaystyle a_k = a_n a_m$\n\nwhere |n - m| = 1, and k is the longitudinal mode index. Therefore, we obtain 5 new partials sets: two sets related to even phantom partials and three sets related to odd phantom partials, both originated from the two sets of transverse partials. Lastly, since for \u2265 10\u00b7 fn (Chabassier et al., 2014), we added only the partials satisfying the criterion."}, {"title": "Transient Model", "content": "The architecture of the Transient model is illustrated in Figure 3. We exploit the discrete cosine domain to synthesize the transients. In particular, we designed a deep convolutional oscillator to compute a sines-based waveform that will be"}, {"title": "Noise Model", "content": "The architecture of the Noise model is shown in Figure 4. The noise is modeled generating noise filter magnitudes n(k) that filter a white noise signal (Engel et al., 2020):\n\n$\\displaystyle Y_{noise} (n) = a \\cdot IDFT(\\eta (k) N(k))$\n\nwhere N(k) is the DFT transform of the noise and Ynoise(n) is the filtered output. A linear FC layer generates the filter magnitudes. The predicted and the target are compared using the multi-resolution STFT loss function, computed using window sizes of [32, 64, 128, 256, 512], and the MAE of the RMS energy.\n\nTo help the prediction, we also anchor the mean of the white noise signal to be generated. An FC layer with 1 unit and hyperbolic tangent activation predicts the mean of the white noise to be generated, and another FC layer with 1 unit and sigmoid activation computes the amplitudes of the noise signal. In this case, the MAE of the RMS is used as a loss function. The inputs of all the cases are a vector containing the velocity un and index in."}, {"title": "Trichords", "content": "The coupling among different keys that happens during chords is modeled by a convolutional neural network and temporal FiLM method (Birnbaum et al., 2019) followed by Gated Linear Unit (Dauphin et al., 2017) as in (Fasciani et al., 2024), which conditions the network based on the playing keys. The input vector consists of the sum of the signals of the three generated notes, while the frequencies and velocities corresponding to the three notes, together with the time index in, are concatenated to create the conditioning vector. A linear, fully connected layer with 32 units processes the conditioning vector before being fed into the FiLM layer. The target is the real recorded trichord. For training the network, we use the MAE of the RMS energy and the multi-resolution STFT detailed in Section 3.1. In this case, the window sizes for the multi-resolution STFT loss are 256, 512, and 1024. Figure 5 shows the architecture."}, {"title": "Dataset", "content": "For this study, we use the BiVib dataset\u00b9, which includes piano sounds recorded from electro-mechanically actuated pianos controllable via MIDI messages. We selected the piano recordings related to the upright-closed and grand-open collection. For both collections, we also narrowed the recordings utilized to train the models to two octaves (C3 to B4), and we considered velocities between 45 and 127. Each recording in the dataset is associated not with a specific velocity value but rather with a velocity range, likely because small variations in velocity produce negligible changes in sound. For our experiments, we have associated each recording with the lower bound of its respective velocity range. This resulted in 168 note recordings with [45, 56, 67, 78, 89, 100, 111] as velocity values and 24 different notes, which are 10 seconds long. Audio is downsampled to 24kHz, the rate at which our model generates audio samples. The first 6 partials were extrapolated from the audio recordings using a manual local peak estimation. These partials are used to estimate the inharmonicity factor B, computed from the Equation 1 and exploiting the ratio between fm and fj:\n\n$\\displaystyle \\frac{f_m}{f_j} = \\frac{m(1 + Bm^2)}{j(1 + Bj^2)}$\n\nwhere m and j are two different partial numbers. This equation leads to the following expression to compute B:\n\n$\\displaystyle B = \\sqrt{\\frac{f_m \\frac{m}{f_j} - j^3}{f_m j^3 - \\frac{f_m}{f_j} m^3}}$\n\nWe use all combinations derived from the 6 extrapolated partials to estimate B. We obtain 30 estimations of B for each note per dataset recording. This process is iterated across all the 7 velocities per note, obtaining 210 values. We then take the mean of these estimations as the initial value for B in our experiments. While this method may be prone to inaccuracies stemming from peak estimation errors and the simplifications made in deriving Equation 1, it provides a starting point that situates the model parameters near realistic values. The computed B values are subsequently used to calculate F0, which serves as an input representing the frequency of the key under the assumption of no string stiffness."}, {"title": "Experiments Setup", "content": "The models are trained using the Adam (Kingma & Ba, 2014) optimizer with a gradient norm scaling of 1 (Pascanu et al., 2013). The training was stopped earlier in case of no reduction of validation loss for 50 epochs, and the learning rate was reduced by 25% if there were no improvements after 1 epoch. The initial learning rate was 3.10-4. The test losses are computed using the model's weights, which minimize the validation loss throughout the training epochs. The raw audio is split into segments of a 1 sample and processed in batch sizes of 131072 for the case of the single notes and 24000 for the chords before updating the weights. In addition, the internal states of the LSTM layer are initialized when a different note needs to be generated. The training of the quasi-harmonic module is divided into two stages. Initially, only the inharmonic factor is tuned to determine the position of the partials, and in the second stage, the damping layer is considered.\n\nThe datasets considering single notes are partitioned by key, with the middle velocity of 78 designated as the test set. On the other hand, in the trichords scenario, the dataset is split into 90% for the training set and 10% for the test set."}, {"title": "Perceptual Evaluation", "content": "Perceptual evaluation is based on Mushra tests (Schoeffler et al., 2018), which are conducted to evaluate single notes and trichords involving 20 participants with music-related expertise. The test includes one anchor: the reference low-pass filtered at 1 kHz. Anchors low-pass filtered at higher cut-off values are not included because most examples are not perceivably affected by such filtering. In the first phase of the test, we asked participants to rate the audio quality of single notes and, in the second phase, the quality of trichords. In the first phase, we evaluate whether the model has learned the characteristics of the specific target piano, while the second phase evaluates the ability of the network to emulate a realist trichord sounding."}, {"title": "Comparative Evaluations", "content": "To further evaluate the approach, we employed the computational method proposed in Fasciani & Simionato (2023). We used the same audio descriptors in the proposal but are considering recording at 24000 Hz. In particular, the method involves the Linear Discriminant Analysis (LDA) projection to maximize the separability between the types of pianos, acoustic, sample-based models, and physics-based models while minimizing within-type variance. LDA projects the feature vectors into a lower-dimensional space with dimensionality or a number of components equal to the number of classes minus one. A large set of audio descriptors are used to compute the feature vectors. For this paper, we considered two cases, the single note and trichord stimuli. For each type of stimulus, each feature vector is associated with a label representing the type of piano, acoustic or synthetic PCM-based or synthetic physics-based. For the single-note case, each vector is a high-dimensional representation of how the generated sound changes when increasing the velocity of the stimulus, while for the trichord case, each vector represents the difference between the sum of three notes and the actual trichord. For the single note case, we compute the dataset by creating a recording of sample-based and physics-based digital pianos, using the same velocities and not the length of the dataset used for training our models. On the opposite, since the dataset used for training the trichord models uses the same sounds collected for the comparative study presented in Fasciani & Simionato (2023), we downsampled the original example to 24000 Hz."}, {"title": "Results", "content": "Figure 6 shows the RMS and STFT losses for models of each key, also detailing losses for the quasi-harmonic, transient, and noise sub-models. The bar plot presents the STFT losses, normalized by the target spectrogram norm, as used for training the quasi-harmonic sub-module. The window resolutions utilized here are the same STFT resolutions used for the quasi-harmonic component. Generally, the model presents good accuracy considering all component-related losses."}, {"title": "", "content": "The models' accuracy is also consistent in most of the keys across the octaves, except for A4 and A#4 keys, which appear to be more challenging for accurate modeling. We have also computed the same loss that was used to train the different sub-modules, comparing the sum of the three components to the actual piano recordings in the dataset. Here, inaccuracies from the decomposition process further contribute to increasing the total error, thereby having a negative impact. Generally, the loss computed on the sum is close to that of the quasi-harmonic sub-module because the transient signal is very short, and the noise signal has a very low energy. Specifically, the transient signal is non-zero only for a relatively short duration\u20141,300 samples out of the total length of the predicted notes, which is 240,000 samples while the quasi-harmonic signal is typically 22 dB greater in magnitude than the noise signal."}, {"title": "", "content": "Similar behavior is found when training the model with the grand piano dataset. In the bars plot of Figure7, the RMS mismatch is considerably lower, but the STFT one is generally slightly bigger than the upright case. This may be due to the physics equations used for the partials' decay envelope being intended for the grand piano case. The upright piano type could involve slightly different behaviors due to the different soundboards. In this case, there are no specific keys that appear to be more challenging for accurate modeling, unlike the situation with the upright piano. The Cent losses, as reported in Figure 8, indicates a good match for inharmonicity for the quasi-harmonic module, reporting a maximum cent deviation of approximately 1.81\u00b710-3 and 9,38 \u00b7 10-3, for the upright and grand piano, respectively. However, the model starts already from a realistic estimation of the inharmonic factor, which facilitates the partial distributions. In addition, this takes into account the first 6 partials. The higher keys are more challenging to be accurately modeled, especially for the grand piano dataset. Figure 9 shows the results for the percussive and noise components of an example note in the test set, corresponding to C4 with velocity 78. The predicted DCT signal is closer to the target for the lower frequencies while becoming less precise for the higher ones. This is also evident in the spectrum of the percussive component, which presents a good accuracy for the low part of the spectrum, although with less energy than the target but becoming slightly less accurate at higher frequencies. On the other hand, the noise component presents a good"}, {"title": "", "content": "frequency matching between the target and the prediction. Figure 10 shows the signal's harmonic content spectrum and the RMS energy envelope. The placement of the partials in the spectra achieves an overall good accuracy even at high frequencies", "set": "the G3-A#3-D4 trichord at velocity 80. From the spectrograms, the simple sum presents more energy-damping than the real trichord and the related prediction. In addition, the real trichord includes more"}]}