{"title": "Hierarchical Spatio-Temporal State-Space Modeling for fMRI Analysis", "authors": ["Yuxiang Wei", "Anees Abrol", "Reihaneh Hassanzadeh", "Vince D. Calhoun"], "abstract": "Recent advances in deep learning structured state space models, especially the Mamba architecture, have demonstrated remarkable performance improvements while maintaining linear complexity. In this study, we introduce functional spatiotemporal Mamba (FST-Mamba), a Mamba-based model designed for discovering neurological biomarkers using functional magnetic resonance imaging (fMRI). We focus on dynamic functional network connectivity (dFNC) derived from fMRI and propose a hierarchical spatiotemporal Mamba-based network that processes spatial and temporal information separately using Mamba-based encoders. Leveraging the topological uniqueness of the FNC matrix, we introduce a component-wise varied-scale aggregation (CVA) mechanism to aggregate connectivity across individual components within brain networks, enabling the model to capture both inter-component and inter-network information. To better handle the FNC data, we develop a new component-specific scanning order. Additionally, we propose symmetric rotary position encoding (SymRope) to encode the relative positions of each functional connection while considering the symmetric nature of the FNC matrix. Experimental results demonstrate significant improvements in the proposed FST-Mamba model on various brain-based classification and regression tasks. Our work reveals the substantial potential of attention-free sequence modeling in brain discovery.", "sections": [{"title": "Introduction", "content": "The human brain is a complex organ and understanding its underlying organizations and dynamics has been an intriguing goal for neuroscientists (Bullmore and Sporns 2009). There is evidence that the activity of different brain regions is correlated and the brain performs like an interconnected dynamic network (Yu et al. 2017; Friston 2011). Analyzing this network and exploiting its intrinsic connections with cognition, behaviors, and diseases is of paramount significance in biomedicine (Calhoun et al. 2014; Wei et al. 2023). Among the popular neuroimaging modalities, functional magnetic resonance imaging (fMRI) captures the time sequence of blood oxygenation level dependent (BOLD) signals of the whole brain and is a powerful tool for non-invasive assessment of the brain's spatiotemporal patterns. Due to the high dimensionality and noisy nature of raw fMRI data, a preprocessing pipeline is often applied to estimate the functional network connectivity (FNC) matrix either statically or dynamically by measuring the pairwise correlations between windowed time series of nodes, which are defined by a given atlas or data-dependently. Previous work has shown the association of macro-scale brain behaviors with static or dynamic FNC using statistical methods (Finn et al. 2015; Mostert et al. 2016). More recently, deep neural networks (DNN) have been widely employed to explore the nonlinear relationship between human brain cognition and disorders (Wei et al. 2022; Li et al. 2021; Asadi, Olson, and Obradovic 2023; Kan et al. 2022b; Kim et al. 2023; Kawahara et al. 2017).\nAs a powerful representation encoder, DNN is capable of disentangling latent characteristics in data and offering unique insights. Popular architectures such as convolutional neural networks (CNN) and recurrent neural networks (RNN) are employed to extract the spatial or temporal features. Additionally, graph neural networks (GNN) (Wu et al. 2020) are proposed to analyze graph-structured data. To handle the issue of the locality of these models, a transformer is applied for fMRI exploration due to its ability to learn global feature representations (Wei et al. 2023). More recently, the structured state space model (SSM), specifically Mamba (Gu and Dao 2023), has been recognized for its ability to capture long-range dependency while maintaining linear complexity. Mamba shows superb performance in downstream tasks such as computer vision (Zhu et al. 2024), natural language processing (He et al. 2024), and graph learning (Wang et al. 2024a).\nHowever, brain functional network connectivity has several unique characteristics that make directly applying Mamba-based models ineffective. First, the FNC matrix is an $N \\times N$ matrix, and each row (or column) represents a brain component's correlations with all $N$ components. Furthermore, adjacent components can be grouped into several functional networks as they share similar activation and de-activation patterns in response to stimulations (Caramazza and Coltheart 2006). As a result, the information between specific components and between larger networks is equally important. Therefore, treating the matrix as an image or directly flattening the matrix and feeding it into the Mamba results in a loss of the intrinsic topological relationships. Second, the dynamics of brain networks captured by dynamic FNC (dFNC) contains essential information about cognition and behavior (Allen et al. 2014). Although spatiotemporal extensions of Mamba are proposed (Li et al. 2024; Yang, Xing, and Zhu 2024), they study the video that is usually noise-free and has clear contextual information. However, the fMRI signals are noisy, and frames of dFNC matrices may be generated by artifacts (Liu 2016), potentially disrupting the encoded dynamics of standard Mamba-based models. Finally, Mamba is highly sensitive to the scan order of tokens due to its sequence modeling property. Nevertheless, the pre-defined order cannot reflect the actual functional relationships between brain connections. Moreover, since the FNC matrix is symmetric along its diagonal, standard scanning strategies potentially obscure its underlying patterns. As a result, ancillary modeling strategies are required to encode this information.\nIn this work, we propose Functional Spatio-Temporal Mamba (FST-Mamba), which leverages the unique properties of functional connectivity to fully exploit the power of Mamba in brain network analysis. Specifically, we adopt a hierarchical structure to capture multi-scale space and time information. To gain both inter-component and inter-network information, we propose the component-wise varied-scale aggregation (CVA) mechanism to group brain connections of different components across networks. As in Fig. 1, CVA aggregates component-wise connections from different networks, producing small patches and gradually fusing patches in deeper layers. Furthermore, we propose the component-wise varied-scale rolling (CVR) mechanism to change the sequence order of scanning, thereby enhancing the representation ability of Mamba over functional connectivity. Leveraging CVA and CVR, FST-Mamba obtains fine-grained component-wise and larger-scale network-wise information while reducing computational complexity. Additionally, to maximally extract in-network information while reducing interference, we propose the component-specific scanning order for Mamba that only scans the patches within a network. Finally, we propose symmetrical rotary position encoding (SymRope) to encode relative positional relationships between connections, while tailoring for the symmetric property of FNC."}, {"title": "Related Work", "content": "Effective encoding of the brain networks can facilitate the understanding of the brain's intricate organizational structures and temporal variations with respect to the cognitive and behavioral processes. Due to the high dimensional nature of raw 4D fMRI (3D images with a temporal dimension), researchers typically reduce each fMRI image to connectivity matrices using predefined regions of interest (ROI) or overlapping data-driven networks via data-dependent independent component analysis. To analyze and extract useful information from the dynamic brain networks, previous works generally fall into two research lines: graph-based methods or directly adopting models from other domains. For the graph-based methods, Li et al. (Li et al. 2021) designed a component-aware GNN to learn the functional connections and proposed a pooling operator to select key components. Kan et al. (Kan et al. 2022b) adapted the graph transformer and proposed a readout to gather information from the nodes of the graph. However, graph-based methods usually require additional pre-processing to the data in the correct form. For other deep learning methods, Asadi et al. (Asadi, Olson, and Obradovic 2023) utilized the transformer to learn spatiotemporal representations from fMRI by stacking spatial transformer and temporal transformer. Kim et al. (Kim et al. 2021) employed the variational auto-encoder to learn and generate representational geometry from fMRI, thereby boosting the performance of downstream task classifiers. The proposed FST-Mamba falls in between two research lines, where we utilize a new sequence modeling method called state-space model and consider the unique topological properties of FNC."}, {"title": "State Space Models for Brain Discovery", "content": "The state space model (SSM) is a linear time-invariant sequence model that maps a sequence $x(t) \\in \\mathbb{R}^{L}$ to its response $y(t) \\in \\mathbb{R}^{S}$ with a corresponding latent space $h(t) \\in \\mathbb{R}^{N \\times L}$. Specifically, SSM can be described as a linear ordinary differential equation with an evolution parameter $A \\in \\mathbb{R}^{N \\times N}$, projection parameters $B,C \\in \\mathbb{R}^{N}$ for a state size N, and the skip connection $D \\in \\mathbb{R}^{1}$, such that:\n$\\begin{aligned}\nh'(t) &= Ah(t) + bx(t) \\\\\ny(t) &= Ch(t) + Dx(t)\n\\end{aligned}$\n(1)\nwhere $h'(t) = \\frac{dh(t)}{dt}$. Since it is challenging to solve (1) analytically, recent work (Gu et al. 2020) proposes to approximate the continuous-time model by discretizing the system with a time scale parameter \u0394:\n$\\begin{aligned}\nh_t &= \\overline{A}h_{t-1} + \\overline{B}x_t \\\\\nY_t &= \\overline{C}h_t + \\overline{D}x_t\n\\end{aligned}$\n(2)\nwhere\n$\\overline{A} = exp(\\Delta A)$\n$\\overline{B} = (\\Delta A)^{-1}(exp(\\Delta A) - I) \\cdot \\Delta B$\n(3)\nTo compute (2) efficiently, the equations can be formulated as a convolution by:\n$K = (C\\overline{B},C\\overline{A}\\overline{B},\\dots,C\\overline{A}^k\\overline{B},\\dots)$\n$y = x * K$\n(4)\nAlthough the vanilla SSM Eq. (4) achieves linear complexity and can be scaled up efficiently, the lack of contextual awareness hinders its performance on discrete and information-dense data (Gu and Dao 2023). To resolve this, (Gu and Dao 2023) proposed the Mamba model that improves the SSM by introducing a data-dependent selective recurrent scan mechanism. The mechanism allows Mamba to filter out irrelevant sequences by making $A, B$, and $C$ parameters trainable.\nMamba has gained much popularity in medical diagnosis and brain discovery. Ji and colleagues (Ji et al. 2024) proposed a Mamba-based encoder-decoder structure to generate high-quality MRI images. A recent work (Wang et al. 2024b) adapted vision Mamba in the U-Net fashion and designed a new model for medical image segmentation. In a different application, another recent work (Zou et al. 2024) proposed a Mamba-based model to integrate multi-modal features for MRI reconstruction. They integrated a spatial-frequency fusion module to enhance Mamba's ability to extract information from the spatial and Fourier domains. While most previous works in the field of biomedicine explore Mamba for 2D imaging, few of them consider Mamba's ability to model temporal variations of highly structured data. In this work, we study Mamba for the dynamic functional network connectivity (dFNC) matrix calculated from fMRI, where each entry represents the dynamic correlation between two brain components."}, {"title": "Methodology", "content": "In this section, we delineate the core modules and designs of FST-Mamba for brain discovery. The overall architecture of the proposed method is depicted in Fig. 2. FST-Mamba consists of 4 stages that gradually extract relevant information from course to fine. The first stage consists of a linear embedding (which is a $1 \\times 1$ convolution that expands the channel dimension to $C$), followed by 2 consecutive FST-Mamba blocks. Then, a component merging layer is applied for the next 3 stages to fuse information from different brain components. Each merging operation reduces the number of components to half of each original number. Note that for each stage, we apply the symmetric rotary position encoding (SymRope) to encode valuable topological information in the FNC matrix. Finally, we perform average pooling over the feature map from the last stage and project to predict brain-related properties."}, {"title": "FST-Mamba Block", "content": "We propose to separately extract temporal and spatial information from dFNC to reduce the computational cost while avoiding information loss. As in Fig. 2(b), the input dFNC is averaged over time for the spatial branch. Then the proposed component varied-scale aggregation (CVA) mechanism is employed to aggregate components' features across networks, followed by the component varied-scale rolling (CVR) operation. Note that CVA and CVR are applied alternately. For instance, we only apply them in the even block of every stage, e.g., the second block at the first stage, and exclude them in the odd block, e.g., the first block at the first stage. To capture the dynamics of neural signals from dFNC, we propose a temporal Mamba encoder to process the time series signals of dFNC, which is calculated by averaging the spatial dimension. The resulting representations are subsequently activated by a sigmoid function, and then combined with the connectivity-related features by multiplication. The procedure can be described as:\n$y^s = ConnMamba(CVA(h^c)) + h^c,$\n(5)\n$y^t = TempMamba(h^t) + h^t$\n(6)\n$z = y^s \\cdot sigmoid(y^t)$\nComponent-Wise Varied-Scale Aggregation and Rolling\nAssume the input size of $x \\in \\mathbb{R}^{B \\times N \\times N}$, where B denotes the batch size and $N$ denotes the number of components, CVA with step size 2 groups all connectivity cells corresponding to a component and generates $\\in \\mathbb{R}^{2B \\times \\frac{N}{2} \\times \\frac{N}{2}}$. For example, in Fig. 1, the rows of components C1 and C3 are grouped, and rows of C2 and C4 are grouped. By reducing the relative distance during sequential scanning, CVA allows the Mamba encoder to effectively capture internetwork connectivity correlations. Since component merging is applied after the first stage, we adapt the step size of CVA by halving it after each stage to fit the dimension of the representation matrix.\nSince Mamba relies on sequence scanning to extract information, we propose CVR to enhance its representation ability. As in Fig. 1, CVR performs a circular roll over the original FNC, which changes the location of a connectivity feature in a scanning sequence. To accommodate the dimension, we also gradually decrease the rolling step of CVR."}, {"title": "Connectivity and Temporal Mamba Encoder", "content": "We use a common backbone for the temporal and connectivity Mamba encoder. As illustrated in Fig. 2(c), following the previous work (Zhu et al. 2024), we apply SSM in both the forward and backward directions to capture long-range dependencies between connectivity (or time points). This allows the model to obtain bidirectional dependencies across space and time. Moreover, since an individual component connectivity profile can be significant to the brain function, we propose a component-specific selective scan to maximally preserve information from each component. The illustration is shown in 3. Instead of directly flattening the matrix and performing scanning, we only scan the connectivity within a component, such that each row in the FNC matrix is regarded as a complete sequence. Note that the component-specific SSM is only applied for the connectivity Mamba encoder."}, {"title": "Component Merging", "content": "Following the design of CVA, we propose to merge components across networks after the initial stage as a mean of downsampling. Specifically, we first perform CVA of step size 2 over the original representation $\\check{Z} \\in \\mathbb{R}^{B \\times N \\times N \\times T}$ and produces $\\hat{Z} \\in \\mathbb{R}^{B \\times \\frac{N}{2} \\times \\frac{N}{2} \\times T \\times 2}$. Then, we linearly project the additional dimension, which can be summarized as:\n$CompMerge(Z) = LN(CVA(Z))$\n(7)"}, {"title": "Symmetric Rotary Position Encoding", "content": "Unlike CNN which is translational-invariant, Mamba performs sequence modeling to patches and therefore lacks positional sensitivity. This necessitates an additional embedding to encode the location-related information. Although previous research (Liu et al. 2024; Cai et al. 2024) excludes positional encoding due to the causal nature of SSM, we argue that this neglects the topological structure of FNC and can result in loss of contextual information.\nRotary position encoding (Rope) (Su et al. 2024) encodes the absolute positions with a rotary matrix and endows relative positional dependencies to the original representation. Assume two arbitrary vectors $m = \\mathbb{R}^{m}$ and $n = \\mathbb{R}^{n}$, Rope applies a rotary matrix $R$ such that:\n$(R_mm)^T(R_nn) = n^T R^T_{m-n}R_{n-m} = m^T R_{n-m}^T R_{m-n} n$\n(8)\nwhere\n$R_n = \\begin{pmatrix}\ncos n\\theta & -sin n\\theta \\\\\nsin n\\theta & cos n\\theta\n\\end{pmatrix} = exp {nB},$\n(9)\n$B = \\begin{pmatrix}\n0 & -\\theta \\\\\n\\theta & 0\n\\end{pmatrix},$\nTo extend to the 2D case, an intuitive solution for the $R$ can be:\n$R_{x,y} = exp(xB_1 + yB_2)$ (10)\nTo fulfill the property of $R$ and encode the relative positions, we need $R^T_{x_1,y_1}R_{x_2,y_2} = R_{x_2-x_1,y_2-y_1}$, for which previous work (Su et al. 2024) offers a straightforward solution:\n$R_{x,y} = exp\\begin{pmatrix}\n0 & -x\\theta & 0 & 0 \\\\\nx\\theta & 0 & 0 & 0 \\\\\n0 & 0 & 0 & -y\\theta \\\\\n0 & 0 & y\\theta & 0\n\\end{pmatrix} = \\begin{pmatrix}\ncos x\\theta & -sin x\\theta & 0 & 0 \\\\\nsin x\\theta & cos x\\theta & 0 & 0 \\\\\n0 & 0 & cos y\\theta & -sin y\\theta \\\\\n0 & 0 & sin y\\theta & cos y\\theta\n\\end{pmatrix}$ (11)\nUnlike CNN which is translational-invariant, Mamba performs sequence modeling to patches and therefore lacks positional sensitivity. This necessitates an additional embedding to encode the location-related information. Although previous research (Liu et al. 2024; Cai et al. 2024) excludes positional encoding due to the causal nature of SSM, we argue that this neglects the topological structure of FNC and can result in loss of contextual information.\nRotary position encoding (Rope) (Su et al. 2024) encodes the absolute positions with a rotary matrix and endows relative positional dependencies to the original representation. Assume two arbitrary vectors $m = \\mathbb{R}^{m}$ and $n = \\mathbb{R}^{n}$, Rope applies a rotary matrix $R$ such that:\nHowever, since the FNC matrix is symmetric along its diagonal, symmetrical entries should be encoded with the same positional information, i.e., $R_{x,y} = R_{y,x}$. This necessitates the additional constraint:\n$R_{x,y}R^T_{x,y} = R^T_{y,x}R_{x,y} = I$ (12)\nTo accommodate this constraint, we adapt Eq. 11 to:\n$R_{x,y} = \\begin{pmatrix}\ncos x\\theta & -sin x\\theta & 0 & 0 \\\\\nsin x\\theta & cos x\\theta & 0 & 0 \\\\\n0 & 0 & cos y\\theta & sin y\\theta \\\\\n0 & 0 & sin y\\theta & cos y\\theta\n\\end{pmatrix}$ (13)\nAt each FST-Mamba stage, we separately apply Eq. 11 on the temporal dimension and Eq. 13 on the spatial dimension, thereby independently encoding positional information in space and time. Furthermore, positional encoding is performed at each stage after downsampling, which can add excessive noise to the representation. Therefore, we propose to \"unRope\" the representations at the output of each stage by applying the inverse of $R$ and $\\hat{R}$. Note that the $R^{-1} = R^{T}$ and:\n$R^{-1}_{x,y} = \\begin{pmatrix}\ncos x\\theta & sin x\\theta & 0 & 0 \\\\\n-sin x\\theta & cos x\\theta & 0 & 0 \\\\\n0 & 0 & cos y\\theta & sin y\\theta \\\\\n0 & 0 & -sin y\\theta & cos y\\theta\n\\end{pmatrix}$ (14)"}, {"title": "Experiments", "content": "Datasets and Preprocessing We use the dFNC data of 833 healthy young subjects from the Human Connectome Project (HCP) S1200 data (Van Essen et al. 2013) to predict sex and age, 12626 middle and old adults from the UK BioBank (UKB) (Sudlow et al. 2015) to predict sex and fluid intelligence, and 474 subjects from the ADNI dataset (Jack Jr et al. 2008) to predict patients who have dementia. We use accuracy (ACC) and area under ROC curve (AUC) as the metrics for classification and mean absolute error (MAE) and mean squared error (MSE) as the metrics for regression.\nThe fMRI data from the datasets undergo a preprocessing pipeline consisting of steps to ensure data quality, including motion correction, standard template registration, resampling, and smoothing. Subsequently, we employ the spatially constrained independent component analysis (ICA) (Calhoun et al. 2001) using the Neuromark_fMRI_1.0 template (Du et al. 2020), which generates 53 brain components divided into 7 networks. To compute the dynamic functional network connectivity (dFNC), we apply the sliding window approach with a window size of 10 time resolutions (TRs, approximately 30 seconds). For each window, we compute the pair-wise Pearson correlation between 53 components.\nBaselines We use several recent deep learning networks that are proposed for fMRI-based brain network analysis, including static brain network methods:\n\u2022 Brain Network Transformer (BNT) (Kan et al. 2022b): A graph Transformer model with network-aware readout function.\n\u2022 BrainNetCNN (Kawahara et al. 2017): A CNN model that accounts for the topological uniqueness of FNC by spatial convolutional kernels.\n\u2022 FBNETGEN (Kan et al. 2022a): A graph neural network(GNN)-based method that generates task-aware functional networks and applies them to downstream tasks.\nand dynamic brain network methods:\n\u2022 SwiFT (Kim et al. 2023): A Swin Transformer-based model that originally proposed for 4D spatiotemporal fMRI. We adopt it to process FNC, with proper tuning.\n\u2022 CNNLSTM (Qayyum et al. 2022): A 1D CNN-LSTM model that utilize temporal time series signals.\n\u2022 FE-STGNN (Chen and Zhang 2023): A GNN-based methods that first extracts spatial information using GNN, then employs LSTM to obtain temporal features.\nAdditionally, we also include the vanilla Mamba (Gu and Dao 2023) that directly takes the flattened dFNC matrices.\nImplementation Details For FST-Mamba, we use the same architecture across all the experiments. Specifically, we set the channel number $C = 24$ and apply CVA and CVR only at even blocks of each stage. For example, CVA and CVR are applied at the second, fourth, and the last block of stage 3. The step sizes of CVA and CVR at each stage are set to {$S_1, S_2, S_3, S_4$} = {4,4,2,1}. For training, we employ AdamW (Loshchilov and Hutter 2017) with a batch size of 64 and a learning rate of 0.001. We train the proposed method and other baselines for 200 epochs using a cosine schedule."}, {"title": "Performance Comparison", "content": "We compare the proposed FST-Mamba with baselines on various classification and regression tasks. As in Table 1, on the sex and dementia classification tasks, the proposed FST-Mamba outperforms all baselines on HCP, UKB, and ADNI datasets. On the regression tasks, FST-Mamba shows competitive results against the best-performing baseline model, such as FBNETGEN. Meanwhile, the proposed model exhibits notable enhancement in sex and dementia classification. It should be noted that some baseline models show distinct performance on different tasks. For instance, BrainNetCNN is the best baseline model for HCP sex prediction, whereas it is the worst for HCP age prediction. Furthermore, dynamic brain network methods are not necessarily better than static brain network methods, possibly due to the noisy nature of fMRI signals. Finally, the standard Mamba, while achieving reasonable results on the ADNI and HCP datasets, performs poorly on the HCP dataset. Overall, the quantitative results highlight the effectiveness of FST-Mamba on different tasks."}, {"title": "Ablation Studies", "content": "We conduct extensive ablation studies to validate the designs of the proposed model based on the HCP dataset. The results are presented in Table 2, 3, 4, and 5.\nWe observe that both the connectivity Mamba and the temporal Mamba branches distill highly predictive signals for the proposed model. As seen in Table 2, removing the connectivity Mamba significantly degrades the performance, whereas the temporal Mamba is comparatively lesser predictive.\nComponent-Aware Information Extraction A key design of FST-Mamba is extracting component-wise information and maximally utilizing the topological characteristics. For validating the proposed model, we ablate the use of CVA, CVR, component-specific SSM, and component merging. Specifically, we show the model's performance when: without CVA, without CVR, without CVA and CVR, and without component merging (using vanilla convolutional downsampling). The results are presented in Table 3. As seen in the table, both CVA and CVR are important for FST-Mamba to extract useful information from dFNC. Moreover, the component-specific SSM contributes substantially to the HCP sex classification.\nWe further show how the performance changes with respect to the step size of CVA and CVR. Specifically, we show the performances with fixed step sizes (step sizes of 2,4, and 8), and with varied step sizes ({$S_1, S_2, S_3, S_4$} = {8,4,2,1} and {$S_1, S_2, S_3, S_4$} = {4, 2, 1, 1}). The results are shown in Table 4.\nIn this study, we propose a symmetrical rotary positional encoding to adequately encode useful positional information in FNC. We show the effectiveness of positional encoding and the proposed SymRope. We further show the necessity of performing \"unRope\" after encoding. The results are presented in Table 5. Positional encoding can be significant to FST-Mamba since the vanilla Mamba model lacks positional awareness, which is evidenced by the first row of Table 5. Moreover, using vanilla absolute positional encoding (Vaswani 2017) degrades the model's performance."}, {"title": "Interpretation Results", "content": "Using the integrated gradient (Adebayo et al. 2018) method implemented in Captum (Kokhlikyan et al. 2020), we identify brain connectivity cells that show significant explanatory power in the dementia and sex classification tasks. Specifically, we acquire the 3D IG maps based on the HCP and ADNI datasets and filter out incorrectly classified subjects. We then average the maps over the temporal dimension and across subjects. The visualizations are shown in Fig. 4. From the figure, for the sex classification task, the model primarily focuses on the connectivities among networks such as auditory (AUD) and visual (VS). Meanwhile, the connectivities among sensorimotor (SM) and default mode (DM) networks play a significant role in dementia prediction."}, {"title": "Conclusion", "content": "Analyzing the spatiotemporal dynamics of functional networks using dynamic functional network connectivity is challenging due to its topological uniqueness and dimensionality. In this paper, we present FST-Mamba, a Mamba-based spatiotemporal network designed for high-dimensional dFNC features that can learn the dynamic connectivity patterns and thereby predict the biological and cognitive outcomes. We present extensive experiments to verify the effectiveness of our models on multiple tasks. For future work, we plan to explore the scalability of FST-Mamba and improve it with pretraining."}]}