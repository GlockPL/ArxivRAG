{"title": "Improving Voice Quality in Speech Anonymization With Just Perception-Informed Losses", "authors": ["Suhita Ghosh", "Tim Thiele", "Frederic Lorbeer", "Frank Dreyer", "Sebastian Stober"], "abstract": "The increasing use of cloud-based speech assistants has heightened the need for effective speech anonymization, which aims to obscure a speaker's identity while retaining critical information for subsequent tasks. One approach to achieving this is through voice conversion. While existing methods often emphasize complex architectures and training techniques, our research underscores the importance of loss functions inspired by the human auditory system. Our proposed loss functions are model-agnostic, incorporating handcrafted and deep learning-based features to effectively capture representations about speech quality. Through objective and subjective evaluations, we demonstrate that a VQVAE-based model, enhanced with our perception-driven losses, surpasses the vanilla model in terms of naturalness, intelligibility, and prosody while maintaining speaker anonymity. These improvements are consistently observed across various datasets, languages, target speakers, and genders.", "sections": [{"title": "Introduction", "content": "Over the last few years, cloud-based speech devices, such as voice assistants, have become indispensable in life [1]. However, this also poses increasing privacy threats, as speech data contains sensitive information encompassing health, affiliations, and other private information about the speaker [2, 3]. Therefore, speech anonymization becomes pertinent, which hides the personal identifiers in the speech while retaining the linguistic content. Voice conversion (VC) is one of the ways to achieve speech anonymization, where the source utterance is modified to sound like another 'target' speaker. In cases where the response of a speech device is driven by the end-user's emotional state, preserving prosody becomes crucial, such as in health monitoring systems that adjust alert urgency based on detected stress or anxiety to ensure timely intervention.\nResearch on statistical modelling approaches [4, 5, 6, 7] provided the groundwork for the development of deep learning (DL)-based VC techniques, significantly advancing the state-of-the-art in VC research [8, 9]. Most of the VC methods are based on generative adversarial networks (GANs) [10], which produce natural-sounding conversions. This is due to the discriminator's role in guiding the generator to create conversions consistent with the target speaker's characteristics. Current GAN-based VC methods [11, 12] are trained with over seven losses in addition to adversarial losses, complicating training due to instability in the optimization process and heightened sensitivity to hyperparameter choices [13]. In contrast to GANs, variational autoencoders (VAEs) [14, 15] offer a clear advantage by providing a well-defined likelihood function, ensuring a more stable training than GANs. These methods typically disentangle the speaker and content embeddings using a reconstruction loss and relevant constraints to remove speaker information.\nOne notable variant of VAE, the vector-quantized VAE (VQVAE) [16], uses a discrete distribution over a codebook instead of a continuous distribution, which is potentially a more intuitive approach"}, {"title": "Vanilla VQVAE", "content": "VQVAE achieves VC by transforming the source mel-spectrogram x using the speaker embedding of the target speaker, typically learned during training [18]. There are three key components of VQVAE:\n1. Encoder Enc takes a mel-spectrogram \u00e6 and maps it to a discrete latent variable z = Enc(x), which is received by the vector quantization layer.\n2. Vector Quantization layer, also known as the codebook Cc, sits between the encoder Enc and the decoder Dec. This layer consists of learnable vectors representing embeddings that capture speaker-independent content information. The encoder's output z is used to select the most similar vector q from this codebook based on Euclidean distance. This selected vector q is then passed to the decoder in place of z. Since the nearest vector selection is non-differentiable, the straight-through re-parameterization trick is applied to compute the discrete latent vector qst, as $q_{st} = z + sg(q - z)$, where sg is the stop-gradient operator [16].\n3. Decoder Dec receives two inputs: the content embedding qst and a speaker embedding es, selected from speaker codebook $C_s = {e_s}_{s=1..S}$. The speaker codebook $C_s$ is jointly optimized with the other model parameters during training through back-propagation. Using both of these inputs qst and es, the decoder generates the transformed mel-spectrogram $x_{dec} = Dec(q_{st}, e_s)$. Therefore, VC using VQVAE can be achieved by just replacing the source speaker embedding with the target speaker embedding.\nWe use a hierarchical-based VQVAE [18] as our baseline, which employs L=3 levels of vector quantization layers to capture speech representations at varying semantic depths (e.g., phoneme, syllable, word), enhancing reconstruction quality. The model is trained with the loss functions shown in Equation 1: reconstruction loss for preserving linguistic content, codebook loss to ensure that the encoded representations remain close to the discrete codebook vectors, and commitment loss ensures latent representations remain consistent with specific codebook vectors [16]. Each loss is weighed by hyperparameter \u03bb.\n$L_{vanilla} = \\lambda_{recon} ||x - x_{dec}||_2^2 + \\lambda_{code} \\sum_{l=1}^L ||sg[z_l] - q_l||_2^2 + \\lambda_{com} \\sum_{l=1}^L ||z_l - sg[q_l]||_2^2$ (1)"}, {"title": "Perception-Informed Losses", "content": "Recent VC research has mainly focused on enhancing architectures to improve synthesized speech quality, often leading to complex models and overfitting [8]. In contrast, our approach introduces novel loss functions, applicable to any model, that aim to capture speech quality in line with human perception."}, {"title": "Handcrafted Feature-Based Loss:", "content": "Formants serve as a concise descriptor of the spectral content of vowels, efficiently capturing important speech features with minimal parameters [22]. Phonetically, formants are resonant frequencies that are characteristic of the shape of the human vocal tract during speech production [23] and are also affected by prosody [24]. F1 is the lowest frequency formant, followed by F2, F3, and so on. Typically, F1 and F2 suffice for vowel identification [25]. However, F3 adds an important layer of detail that enhances the precision of vowel identification [26], aids in consonant distinction and provides critical information for speaker identification [27] and speech in- telligibility [28]. We compute the formant loss $L_{formant}$ as shown in Equation 2, where $\\Phi^k(.)$ represents the kth formant. Here, K and N denote the total number of formants and frames, respectively.\n$L_{formant} = \\frac{1}{KN} \\sum_{k=1}^K \\sum_{n=1}^N (\\Phi^k(x_n) - \\Phi^k(x_{dec_n}))^2$ (2)"}, {"title": "Representation-Driven Losses:", "content": "Intermediate representations from self-supervised deep learning models capture a wide range of speech features, such as tonal quality, prosody, clarity, and background noise [29], which are vital for assessing speech quality. Different network layers capture varying levels of abstraction, from basic acoustic features to more abstract representations like phonemes [30]. Embeddings from supervised models trained for quality-related tasks offer richer information than standard element-wise loss functions [31]. Consequently, we compute the quality discrepancy as shown in Equation 3, which is a general representation-driven loss, calculated on the activations $a^j$ from the $j^{th}$ layer of a quality-based perceptual network.\n$L_{DL^j} = \\frac{1}{J \\cdot N} \\sum_{n=1}^N (a^j(x_n) - a^j(x_{dec_n}))^2$ (3)\nWe consider two kinds of representation-driven losses:"}, {"title": "Mean Opinion Score (MOS) Loss:", "content": "The MOS is a widely used subjective metric for assessing the quality or naturalness of speech [32]. However, incorporating human annotators to rate speech conversion during the training process is impractical. To address this, we use a neural network, $Net_{MOS}$, as a proxy for human evaluation, which is trained to predict the MOS score of a speech audio signal. Specifically, we employ the model proposed in [33], which consists of a fine-tuned Wav2Vec2.0 model [34] with a regression head added to the encoded features, resulting in a total of |J| = 4 layer activations. The corresponding loss function $L_{DL=mos}$ is defined in Equation 3, where the activations a are produced by the $Net_{MOS}$ model."}, {"title": "WavLM Loss:", "content": "WavLM [35] is a state-of-the-art model for comprehensive speech processing tasks, demonstrating leading performance on SUPERB benchmarks [36] in areas such as speaker verification and diarization. Studies like [37, 38] highlight WavLM's capability to extract meaning- ful phoneme embeddings, with similar-sounding phonemes clustering in its latent space. The later layers of WavLM show reduced predictive power for pitch and prosody [39], while the embeddings from layer J=6 are highly correlated with phoneme identification [37]. Therefore, we compute the WavLM loss $L_{DL=wavlm}$ using the activations from the 6th layer, resulting in|J| = 1 in Equation 3."}, {"title": "Training Objectives:", "content": "Put together, the full objective function of our proposed approach consists of the following terms that are weighted by $\\lambda_i$, where i \u2208 {recon, code, comm, mos, wavlm, formant}:\n$L = \\lambda_{recon} L_{recon} + \\lambda_{code} L_{code} + \\lambda_{com} L_{com} + \\lambda_{mos} L_{DL=mos} + \\lambda_{wavlm} L_{DL=wavlm} + \\lambda_{formant} L_{formant}$ (4)"}, {"title": "Experiment Details", "content": "We use three datasets: VCTK [40] and LibriSpeech [41] for English utterances, and mlsGerman [42] for German. Utterances are re-sampled to 16 kHz. The vanilla hierarchical VQVAE without perception-informed losses serves as our baseline model (Mbase), while our proposed model (MPL) incorporates these losses. All models are trained on the same splits and evaluated on the same test set. Log mel-spectrograms are used as input to the models. The models are optimized using the Adam optimizer with a cyclic learning rate, ranging from 5\u00d710\u22124 to 2\u00d710\u22123. The models are trained from scratch, employing early stopping with predicted mean opinion score (pMOS) [33] on the validation set as the stopping criterion. A pre-trained HiFiGAN vocoder [12] is used to generate the waveform from the model's output. Additional details are provided in the appendix."}, {"title": "Evaluation", "content": "We evaluate the baseline model (Mbase) and our approach (MPL) across three scenarios using both objective and subjective measures:\n1. English English: Both source and target speakers are English-speaking, evaluated within the same corpus (VCTK\u2192VCTK) and across different corpora (LibriSpeech\u2192VCTK). We also assess inter-accent conversion (Canadian, American, British).\n2. German\u2192 English: German utterances are converted using English VCTK target speakers.\n3. German\u2192 German: Both source and target speakers are German, using the mlsGerman dataset.\nIn each scenario, we have 10 source speakers, each providing 10 utterances, and 10 target speakers. The speakers are selected randomly, ensuring a disjoint set and balanced gender distribution, leading to 1000 total conversions.\nObjective Measures: Intelligibility is measured by character error rate (CER) using the transcriptions from Whisper [43] medium-english model for the English conversions and medium model for German conversions. For anonymization, we measure the equal error rate (EER) using the speaker verification model ECAPA-TDNN [44], as in [12]. We avoid using predicted MOS (pMOS) for quality evaluation, as in [12], as it is used as a loss function in our model and could lead to overfitting, as discussed in [45]. Instead, we rely on subjective testing for quality assessment.\nSubjective Evaluation Setup: We evaluated quality, prosody preservation, intelligibility and anonymization by user studies via the Crowdee platform\u00b9. We evaluated a random selection of 100 conversions for each of the three scenarios, as assessing all conversions would be both time-consuming and costly. 72 online participants had taken part in the studies. For English English and German German scenarios, only native speakers of English and German, respectively, were allowed to participate. In the German\u2192VCTK (English) scenario, native German speakers who were proficient in English were considered. Participants rated quality (naturalness) on a scale from 1 (poor) to 5 (excellent). They compared intonation and stress patterns between the original and converted samples for prosody preservation. Intelligibility was assessed by selecting the most intelligible sample between the two conversions, for the same source utterance. Anonymization was evaluated by rating the similarity on a scale ranging from 1 (different) to 5 (similar), between a converted sample and another utterance from the same speaker. Each task was rated by at least 3 subjects, who were unaware of whether the samples were original or converted. Trap questions and anchoring examples were used to ensure accuracy, and raters who failed trap questions twice were excluded."}, {"title": "Results and Discussion", "content": "Overall, our proposed method MPL, significantly enhances intelligibility compared to the baseline Mbase, as demonstrated in Table 1. This improvement is also accompanied by improvement in naturalness corroborated by the MOS ratings from user studies, as shown in Figure 1. Additionally, 83% of the conversions using the proposed model were rated as more intelligible than those from the baseline. In terms of speaker anonymization, there is a modest increase in EER from 41.07% to 43.21% across all scenarios, which is similarly reflected in the speaker similarity scores from user studies. For prosody preservation, our approach significantly outperforms the baseline, with 83.2% of participants favouring the proposed model having perception-informed losses, as seen in Figure 1. Similar trends are observed for within-corpus scenario VCTK\u2192VCTK, where the mean CER showed a significant improvement from 73.32% to 45.49% with the incorporation of the proposed losses, as detailed in Table 1. These improvements are also observed in cross-gender (refer to Appendix) and cross-accent conversions within the corpus. For prosody preservation and intelligibility, our model received significantly higher support with 83% and 85% of the votes, respectively. Furthermore, in inter-accent conversions, we observe a change in accent after the VC, where the converted sample adopted the accent of the target speaker, potentially leading to better anonymization. In the cross-corpus scenario LibriSpeech VCTK, similar trends are observed for all metrics.\nIn the German to English (VCTK) conversion, intelligibility did not improve much compared to intra-lingual conversions, as shown in Table 1. Listening to samples\u00b2 reveals that using English target"}, {"title": "Conclusion", "content": "We present model-agnostic perception-informed losses as an innovative approach to enhance the quality of voice conversion (VC) for speech anonymization without increasing model complexity. By integrat- ing quality-related knowledge into the training process through handcrafted acoustic features and deep learning representations, our framework significantly improves the performance of a vanilla hierarchical VQVAE-based model. Augmented solely by our proposed loss functions, the model shows notable en- hancements in naturalness, intelligibility, and prosody preservation across diverse conversion scenarios, including cross-corpus conversions, varying genders, accents, and languages. Objective and subjective evaluations validate these results, highlighting the importance of incorporating speech-specific features within the loss function, rather than increasing model complexity. Looking ahead, we plan to develop loss functions to specifically target and reduce the graininess observed in some conversions."}, {"title": "Detailed Objective Evaluation", "content": "Table 2 shows a more detailed version of the objective evaluation portrayed in Table 1, where gender-wise information is also mentioned."}, {"title": "Ablation Study", "content": "We perform ablation studies to assess the contribution of each loss component. Table 3 demonstrates that formant $L_{formant}$ individually contributes the most to naturalness and intelligibility. This suggests that calculating loss on specific frequency components effectively enhances the overall quality of VC. These components correspond to the resonant frequencies of the vocal tract, which are essential for perceiving vowel sounds and overall intelligibility. Further, listening to the samples reveals that the model not trained with Lformant has the worst prosody preservation. Removing Lformant loss (when using $L_{DL=wavlm}$ + $L_{DL=mos}$) significantly increases the CER from 50.02% to 66.85%, highlighting the critical role of formants in speech intelligibility."}, {"title": "Training Details", "content": "We trained all models on log mel-spectrograms with 80 mel bands, generated from 2-second audio clips. For STFT parameters, we used a hop length of 320 and a window length of 1024.\nFor scenarios involving English-speaking target speakers, our models were trained on approximately 5 hours of English utterances from 20 speakers in the VCTK dataset, with the data divided into a 90:10 split for training and validation. In cases requiring German-speaking targets, we utilized around 10 hours of German utterances from 20 speakers in the mlsGerman dataset, allocating 80% for training and 20% for validation.\nThe number of trainable parameters in all the voice conversion models is the same, as we only augment the vanilla model with our proposed losses. Training with all three perception-aware losses required approximately 2 days on average to complete on a 80GB A100 GPU. We set $\\lambda_{recon}$ = 1, $\\lambda_{code}$ = 1, $\\lambda_{comm}$ = 3, $\\lambda_{mos}$ = 0.1, $\\lambda_{wavlm}$ = 0.1, and $\\lambda_{formant}$ = 106, ensuring that all loss terms were within the same order of magnitude. We incorporated $L_{DL=mos}$ from epoch 0 and $L_{DL=wavlm}$, $L_{formant}$ from epoch 45 into the training based on empirical observations obtained during the development phase.\nWe used a pre-trained HiFiGAN [46] vocoder from [12] to generate the waveform from the mel-spectrogram, which produced a one-minute long waveform from the converted mel-spectrogram in 0.1 seconds on the A100.\nWe trained the $Net_{formant}$ model to derive the F1, F2, and F3 values needed to compute the $L_{formant}$ loss. The formant network consists of a transformer encoder architecture as proposed in [47], additionally featuring a regression head with three output neurons that predict based on the encodings of the input for each time frame. As training data, we used the VTR dataset [48], comprising 538 manually formant-annotated utterances from domain experts who ensured balance across phonetic contexts, speakers, genders, and dialects in the English language. We used the default VTR parameters for pre-processing and achieved a final MSE of 3.16."}]}