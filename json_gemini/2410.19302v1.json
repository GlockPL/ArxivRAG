{"title": "TEARS: Textual Representations for Scrutable Recommendations", "authors": ["Emiliano Penaloza", "Olivier Gouvert", "Haolun Wu", "Laurent Charlin"], "abstract": "Traditional recommender systems rely on high-dimensional (latent) embeddings for modeling user-item interactions, often resulting in opaque representations that lack interpretability. Moreover, these systems offer limited control to users over their recommendations. Inspired by recent work, we introduce TExtuAl Representations for Scrutable recommendations (TEARS) to address these challenges. Instead of representing a user's interests through a latent embedding, TEARS encodes them in natural text, providing transparency and allowing users to edit them. To do so, TEARS uses a modern LLM to generate user summaries based on user preferences. We find the summaries capture user preferences uniquely. Using these summaries, we take a hybrid approach where we use an optimal transport procedure to align the summaries' representation with the learned representation of a standard VAE for collaborative filtering. We find this approach can surpass the performance of three popular VAE models while providing user-controllable recommendations. We also analyze the controllability of TEARS through three simulated user tasks to evaluate the effectiveness of a user editing its summary. Our code and all user summaries are available on GitHub.", "sections": [{"title": "1 Introduction", "content": "Recommender systems are a pillar of the online ecosystem, providing personalized content by modeling user preferences. Users daily rely on these systems to infer their preferences and surface relevant items to save them from parsing through large collections.\nRecommender systems often use collaborative filtering (CF) models, such as those discussed in He et al. [16], Wang et al. [53], Zhang et al. [58], which are particularly effective for users with extensive interaction histories (e.g., clicks or ratings). These models derive latent user representations from observed preferences to generate recommendations. However, these representations are encoded using high-dimensional numeric vectors, which lack interpretability. Further, these CF systems offer limited control to users, who can influence them only through coarse item-level interactions, such as clicks, without understanding the precise impact of such actions on (future) recommendations.\nTo address these limitations, we introduce a recommender system that represents users with natural text summaries. Such user representations are easily understandable providing users with a clear view into the model's interpretation of their historical behavior (preferences) and directly editable [39] allowing them to modify these interpretations, directly influencing their recommendations.\nOur work draws inspiration from the framework developed by Radlinski et al. [39], illustrated in Figure 1. This framework, suggests transitioning from black-box user representations to more interpretable ones using scrutable (natural) language as a bottleneck to the system, similar to concept bottleneck models [25, 26, 57]. Where, they define scrutable language as being both short and clear enough for someone to review and edit directly (to impact the downstream recommendations). This approach provides several key benefits. First, it enhances transparency in the recommendation process by basing recommendations on user summaries and clarifying the system's inferred preferences. Additionally, it allows users to edit their summaries, thus giving them control over their recommended content. However, this framework assumes that text summaries can encapsulate all the information typically contained in rich numerical latents, which we find is generally not the case in practice, leading to a substantial drop in performance (see Table 1).\nWe develop TExtuAl Representations for Scrutable Recommendations (TEARS) to obtain high-quality recommendations from scrutable user representations. TEARS uses two encoders: a standard black-box model that processes historical interactions onto numerical black-box embeddings and another that takes scrutable, user-editable text summaries and transforms them into summary-based embeddings. We then apply an optimal transport (OT) regularizer to align these two embeddings, which are then merged using a convex combination. Changing the mixing coefficient allows the system designer or its users to guide their recommendations further. For example, users can choose recommendations based entirely on their user summaries, adhering to the principles of Radlinski et al. [39], opt for more black-box-based recommendations for optimal performance, or select a blend of both from text adjustments.\nOur empirical evaluations explore three key aspects of TEARS: user summaries, recommendation performance, and controllability. We test whether modern pre-trained LLMs can generate distinctive, appropriately sized user summaries. Next, we demonstrate that aligning black-box and summary-based embeddings improves recommendation performance. Due to the lack of standard metrics for controllability, we introduce new metrics and benchmark tasks, designed to evaluate how user edits influence the system. These tasks are built around the principle that there are two primary types of user edits: large-scope and small-targeted edits. Other types of edits are repetitions or combinations of both. For instance, we assess large changes by instructing GPT to \"flip\" user preferences, swapping favorite and least favorite genres, and measuring the change in the recommendations. Targeted edits are evaluated using GPT to make minor adjustments to the summary to boost the rank of a poorly ranked movie. Finally, we test an edit unique to TEARS, whereby using a mix of text and black-box embeddings, users can use short phrases to guide their recommendations. Our findings indicate that TEARS is controllable across all scenarios.\nIn summary, our main contributions are:\n\u2022 We introduce a method for generating high-quality textual user summaries that reflect user preference, enhancing transparency and controllability in recommendation.\n\u2022 We demonstrate that aligning text-based summaries with black-box embeddings improves the performance of recommender systems, leveraging TEARS.\n\u2022 Through three user-simulated tasks, we validate that TEARS provides robust controllability and enables users to effectively influence their recommendation outcomes, balancing relevance and user control."}, {"title": "2 Related Work", "content": "Scrutable Recommender Systems. Building recommender systems using latent-variable models [47] is a common practice, but it complicates explaining the system's behavior. Although there are several works focused on explainability, most emphasize feature-based explanations [10, 36, 45, 48], or on generating posthoc language-based explanations for recommendations using LLMs [5, 9, 14, 27, 28]. While both approaches provide value to users and practitioners, they lack actionability, meaning it is not simple for users or practitioners to directly influence produced recommendations through interactions with the explanations.\nScrutable recommender systems, in contrast, have been limited in flexibility, largely relying on keyword or tag-based methods [4, 9, 10, 13, 34, 51], where users can toggle tags on or off. However, this approach can hinder controllability, as users are burdened with parsing through numerous keywords or tags, leading to significant cognitive load. Few works have explored scrutable systems through natural language user summaries. Sanner et al. [42] conducted a user study that highlighted the advantages of using user summaries to generate zero or few-shot recommendations with a frozen LLM. The study found that under specific conditions, LLMs leveraging user summaries can compete with black-box models in cold-start settings. Closest to TEARS, Ramos et al. [41] developed a scrutable model using natural language profiles for next-item prediction. However, the approach relies on user reviews, which are not always available, and only approaches the performance of black-box models. In contrast, our work does not require user reviews and employs a hybrid methodology to demonstrate that TEARS can surpass black-box model performance and maintain scrutability.\nLarge Language Models for Recommendations Systems. Enhancing recommender systems with textual attributes is widely recognized for improving both performance and robustness [2, 22, 35, 59]. This integration generally takes one of two approaches: utilizing LLMs as standalone recommender systems and enhancing existing systems with LLM-generated text.\nStudies have shown that LLM-generated text can improve recommendation quality by providing item or user-specific content [1, 49, 55]. Such approaches leverage the descriptive power of LLMs to enrich the contextual understanding of user preferences and catalog items. Meanwhile, other approaches deploy LLMs directly as recommender systems. This can be done either in a zero-shot manner, where the model makes recommendations based on general pre-training [30, 56], or through fine-tuning the model on specific recommendation tasks [30]. While LLM zero-shot or few-shot methods might offer scrutability since they rely purely on natural text, they suffer from inconsistencies and are limited by confabulations and incomplete catalogue coverage [32]. We further validate this in Table 1, where we evaluate GPT-4-turbo's performance on strong generalization. On the other hand, fine-tuning can enhance the quality of recommendations by integrating new item-specific or user-specific tokens into the LLM's vocabulary [11, 12, 21]. However, it may compromise the model's ability to comprehensively navigate the full item catalogue due to the added tokens not appearing during general pre-training. While these approaches show that one can enhance performance using textual attributes, they do not focus on the development and evaluation of scrutable systems, which is the main focus of this work."}, {"title": "3 Methodology", "content": "We introduce TEARS, a method with user-interpretable and controllable representations. We begin by contrasting TEARS with standard latent-based CF methods. Then, we introduce the components of TEARS, starting with a prompting pipeline for summarizing the user preferences with an LLM. The summaries are then used to predict recommendations. To achieve this, we use two AE models: a text representation-based model, which transforms text summaries into recommendations, and a (black-box) VAE model. We propose aligning the space of the text representations model with the space of a standard recommendation VAE using optimal transport (OT)."}, {"title": "3.1 Motivation", "content": "Traditional collaborative-filtering-based recommender systems rely on a user's history to provide recommendations. A user wanting to obtain better recommendations, e.g. if current recommendations do not appear satisfying, faces a tedious process with unclear outcomes since they must interact with (e.g., consume or at least rate) items that better reflect their preferences with the hope of obtaining better recommendations. This is even more impractical in domains where users' preferences evolve rapidly or if users have short-term preferences in a given context.\nIn contrast, TEARS allows users to adjust their recommendations by adapting or even deleting their summary and creating a new one more aligned with their (current) preferences. This process is immediate and transparent. It allows users to correct representational mistakes (e.g. add a missing genre they are interested in) or adapt them to better suit their evolving preferences and the current context. Additionally, we introduce an interpolation coefficient, \u03b1, between a user's summary and black-box representations. Setting \u03b1 = 1 puts all the weight on text representations, while \u03b1 = 0 favors black-box representations. This gives users extra control over how their recommendations are influenced (details in Section 3.3).\nBackground. Autoencoder models have proven highly effective for collaborative-filtering recommender systems, consistently out-performing counterparts across various tasks [29, 43, 46]. With this in mind, we design TEARS to be compatible with existing VAE-based models and refer to the combination as TEARS VAE models. We study specific VAEs, and we denote their combinations using their names, e.g. TEARS RecVAE.\nThe auto-encoder framework involves representing the user-item feedback matrix \\(X \\in \\mathbb{N}^{U \\times I}\\), where each entry represents a rating given by a user u to an item i. Our focus is on predicting users' implicit preferences \\(Y \\in {0, 1}^{U \\times I}\\) (e.g. identifying items that a user has rated above a specified threshold r as positive targets). These models prescribe learning an encoding function \\(Q : X \\rightarrow Z\\) to compress input data into a lower-dimensional latent space, followed by a decoding function \\(D: Z \\rightarrow Y\\) to map it to the target."}, {"title": "3.2 Summary Design", "content": "Creating scrutable summaries for controllable recommender systems presents unique difficulties. Manual summary creation is impractical due to scalability and inconsistency, while the quality of earlier machine-generated summaries was low [8]. However, LLMs like the recent GPTs have significantly improved capabilities across natural language tasks [18], offering a tool for generating high-quality user summaries.\nWe propose designing user summaries by leveraging LLMs. While these generative models provide an efficient way to obtain summaries, ensuring their quality and consistency is non-trivial.\nWe believe each summary should contain enough information to be decodable into good recommendations but short enough to be easy to understand and to control by users. In that sense, it should describe the user's preferences sufficiently and uniquely. We note that these design choices may also vary by domain, and in this work, we focus on the movie and book recommendation domains.\nGiven the above criteria, we identify preference attributes that a user may wish to edit and that are essential in providing good recommendations. For each attribute, we also pinpoint relevant prompting information:\n\u2022 Inferred Preferences: What users like and dislike, prompted with user ratings.\n\u2022 High-Level Attributes: Preferences for genres, prompted with item metadata.\n\u2022 Fine-Grained Details: Specific plots or themes, prompted with the title.\nRecent LLMs encode significant knowledge about movies and books [17, 19, 52]. We believe they should be able to encode appropriate item information conditioned on titles and genres alone. To verify this, we conduct preliminary experiments using GPT-3.5, GPT-4-turbo, and GPT-4 via the OpenAI API, finding that GPT-3.5 generated poor summaries, while there was no significant difference between GPT-4 and GPT-4-turbo. We select GPT-4-turbo\u00b2 to generate summaries and refer to it as GPT for the remainder of the text. Additionally, to enhance the reproducibility of our work, we also generate summaries using LLaMA 3.1-405b3 [7].\nWhile LLMs have shown impressive capabilities in text summarization [8], we find that free-style prompting without adherence to a specific structure can make summaries generic and vary in quality. On the other hand, LLMs have excelled in instruction-based tasks [37]. With this in mind, we design a prompt asking for summaries to include the desirable characteristics mentioned to enforce consistency and quality. Consistent summaries will also likely help train a decoder and obtain high-quality recommendations. Our resulting prompting strategy is in Figure 8. We explicitly direct the model to avoid stating titles and rating information to prevent over-reliance on such details and encourage summaries to be more expressive. By design, LLM's responses are non-deterministic. In early experiments, we find that summary generation can vary, with two output summaries being significantly different for the same user. We observe higher variability in users with longer histories, leading us to limit the number of items used for each summary (to a maximum of 50 items in our studies). We further explore this effect in App. M. Note that the number of items used for each summary, \\(\\mu\\), is user-dependent since some users have shorter histories than this max. threshold. Finally, our prompt also contains the expected length of the summaries, which we set to 200 words, as we find it short enough not to incur heavy cognitive loads but can be detailed enough. We leave further exploration on the impact of summary length and if it should vary by user for future work."}, {"title": "3.3 TEARS", "content": "In this section, we define TEARS and its training process, depicted in Figure 1. With user summaries S, generated using a backbone LLM, and feedback data X, our goal is to obtain a pair of encoding functions \\(Q_s: S \\rightarrow Z_s\\) and \\(Q_r: X \\rightarrow Z_r\\) which we can constrain to map the representations \\(z_s\\) and \\(z_r\\) to a common space. We obtain \\(Q_r\\) from a backbone VAE for recommendations. After that, we aim to decode a convex combination of the representations \\(z_c = \\alpha z_s + (1 - \\alpha) z_r\\) onto recommendations using a shared decoder \\(D:Z_c\\rightarrow Y\\). When \\(\\alpha = 1\\), the recommendations are generated solely using the summary embeddings; this means the downstream recommendations are controllable through simple text edits. On the other hand, when \\(\\alpha = 0\\), the recommendations are based purely on the backbone VAE and only leverage the user feedback data. Other \u03b1 values lead to a combination of these, making it such that a user can guide their recommendations through text edits but still use their historical data, which may be richer in information, making the changes less drastic but more personalized. Overall, our training objective is composed of three components, which are detailed below.\nAlignment through Optimal Transport. While the shared decoder architecture should incentivize both the text (\\(Z_s\\)) and black-box embeddings (\\(Z_r\\)) to be naturally aligned, in practice we find that training without additional constraints is not enough (see App. 7.1). Rather, we align these embeddings using optimal transport techniques which measure the cost of shifting the mass from one probability measure to another [3]. This is achieved by calculating a cost function that reflects the underlying geometry of the distributions, known as the Wasserstein distance. Unlike other distance metrics such as KL-divergence, the Wasserstein distance is symmetric, making it particularly suitable as an optimization target for aligning two distributions. Computing this distance with Gaussian distributions has a closed-form solution [24]. To make use of these properties, we use encoders \\(Q_r\\) and \\(Q_s\\) that map inputs onto Gaussian-distributed latent encodings, as is traditional for VAEs [23, 29, 44], \\(Z_r \\sim \\mathcal{N}(\\mu_r, \\sigma_r\\mathbb{I})\\) and \\(Z_s \\sim \\mathcal{N}(\\mu_s, \\sigma_s\\mathbb{I})\\). This parameterization allows for direct computation of the minimal transportation cost between Gaussian distributions to align the two embeddings:\n\\[\\mathcal{L}_{OT} = ||\\mu_s - \\mu_r||^2 + Tr{\\Sigma_s + \\Sigma_r - 2(\\Sigma_s^{1/2}\\Sigma_r\\Sigma_s^{1/2})^{1/2}}\\]\nOther optimal transport techniques, like Sinkhorn's algorithm [6], are applicable to non-Gaussian distributions and we reserve these methods for future exploration. We find in practice, this objective greatly enhances the system's controllability (see App. 7.2).\nObjective for Recommendation For \\(Q_s\\), we use a T5-base model [40], which we fine-tune using low-rank adaptors [20], to obtain an embedding of the text summaries and train an MLP head to obtain \\(\\mu_s, \\sigma_s\\), we then use the reparametrization trick to obtain \\(Z_s\\):\n\\[\\mu_s, \\sigma_s = MLP(T5\\text{-Encoder}(S)),\\]\n\\[Z_s = \\mu_s + \\sigma_s \\odot \\epsilon, \\ \\epsilon \\sim \\mathcal{N}(0, 1).\\]\nThanks to the OT alignment, \\(Z_r\\), \\(Z_s\\) and \\(Z_c\\) share a common space and thus a shared decoder, \\(D\\) alongside the softmax function \\(\\Psi\\) can be used to produce a distribution over items for each user from each latent representation (\\(Z_s, Z_r\\)) and their combination (\\(Z_c\\)):\n\\[\\hat{Y}_c = \\Psi(D(Z_c)), \\hat{Y}_r = \\Psi(D(Z_r)), \\hat{Y}_s = \\Psi(D(Z_s)).\\]\nWe use these distributions to optimize the multinomial likelihood of each representation. During training, we fix \\(\\alpha = 0.5\\) to optimize for performance on the merged representations but note that \u03b1 can be changed at any time during inference. The model learns using the binary cross-entropy of trained-autoencoder (r), TEARS (s), and their combination:\n\\[\\mathcal{L}_{R} = \\sum_{k \\in {c,s,r}} \\sum_{i \\in I, u \\in U} y_{ui} \\log(\\hat{Y}_{ui,k}).\\]\nConstraint of Gaussian Priors Additionally, we impose a standard Gaussian prior \\(P(Z) \\sim \\mathcal{N}(0, \\mathbb{I})\\) on \\(Z_s\\) which has been shown to help improve performance [29]. Enforcing this constraint can be expressed as optimizing the KL-divergence between that prior and its inferred value:\n\\[\\mathcal{L}_{KL} = D_{KL}(Q_s(Z | S)||P(Z))\\]\nOur overall training objective is a weighted sum of the above three objectives, formulated as below:\n\\[\\mathcal{L} = \\mathcal{L}_R + \\lambda_1 \\mathcal{L}_{OT} + \\lambda_2 \\mathcal{L}_{KL}\\]\nwhere \\(\\lambda_1\\) and \\(\\lambda_2\\) are weighing parameters for their respective losses. In practice, we initialize \\(D\\) with the base model's decoder, update its weights while training, and freeze \\(Q_r\\)'s weights (see App. L.2)."}, {"title": "3.4 Genre-Based Model (GERS)", "content": "In addition to summary-based TEARS VAE models, we instantiate GERS VAE models, which help evaluate whether TEARS summaries contain information beyond genres. The only difference between TEARS VAE and GERS VAE is that the text summaries S and user representation \\(z_s\\) are replaced with a genre vector G and a genre-based representation \\(z_g\\), defined as the normalized count vector of the genres linked to the items a user has interacted with positively. Specifically, we define this representation as:\n\\[z_{g,p} = \\frac{C_u(p)}{\\sum_{p'\\in G} C_u(p')}\\]\nwhere G represents the set of all genres, and \\(C_u (p)\\) corresponds to the count of items the user has interacted with in genre p, such that \\(z_g \\in [0,1]^{|\\mathcal{G}|}\\). This modeling framework is similar to those used in keyword/tag-based systems, as each genre entry in \\(z_g\\) can be scrutinized by the user making it so they can choose how much of that genre they want to be weighted in their recommendations (see App. I). We note, that while this modeling framework can be useful for some applications, it is limited in transparency as user representations are simple statistics. Moreover, users cannot edit fine-grained details of their interests such as plots or themes they may enjoy, limiting the controllability of this system."}, {"title": "4 Datasets", "content": "We conduct experiments on subsets of the MovieLens-1M (ML-1M), Netflix, and Goodbooks datasets. As is common in other studies, we filter out cold-start items for all datasets [15, 53]. In addition, due to the high cost of using LLM APIs, we use a subset of users with enough ratings for each dataset to provide a comprehensive summary. For the Netflix and Goodbooks datasets, we filter out users with less than 100 interactions and items with less than 20, making these dataset-subsets less sparse than the full ones. For the smaller ML-1M dataset, we only filter out users with less than twenty interactions and items with less than 5. After filtering, we have 6,037 users and 2,745 items for ML-1M, 9,978 users and 3,081 items for Netflix, and 9,980 users and 8,093 items for Goodbooks.\nDescriptive statistics such as sparsity, average ratings, and number of genres per dataset are reported in App. B.\nWe construct Y using X, with r = 4, that is, we train the model to predict implicit feedback where the rating is positive (\\(y_{ui} = 1\\)) if the item is rated four and above and a negative (\\(y_{ui} = 0\\)) otherwise [33]. We evaluate under a strong generalization setting where we reserve 500 users for the validation and testing splits (250 each) for ML-1M and 2,000 (1,000 each) for Netflix and Goodbooks. We make all summaries with GPT and LLaMA using the prompt in Figure 8. We use up to 50 of the oldest ratings to construct the summaries, while the remaining, more recent ones are used for evaluation. For users that rate less than fifty items, we keep the most recent two for evaluation and generate the summary with the remaining."}, {"title": "5 Assessment of User Summaries", "content": "We begin by assessing the scrutability and uniqueness of user summaries, using descriptive statistics on their length alongside standard NLP metrics. After, we use recommendation performance as a proxy for quality, as accurate, information-rich summaries should yield better recommendations."}, {"title": "5.1 User Summary Properties", "content": "We analyze the average summary length to inspect if summaries can be appropriately scrutinized. For GPT, summaries average 168.6\u00b13.6 words across all datasets, compared to 179.6\u00b12.2 for LlaMA. These lengths suggest that the summaries are concise enough to be easily editable while still comprehensive enough to convey detailed user information, as reflected in their positive impact on recommendation performance (see \u00a75.2). To assess uniqueness, we use pairwise edit distance and BLEU scores [38], the latter measuring n-gram overlap between two texts (we use 4-grams). The average edit distance for GPT summaries is 160.1\u00b10.1 words, while LlaMA summaries average 157.0\u00b11.2. These scores are high when compared to the average summary length, indicating distinct summaries across users. Similarly, BLEU scores are low, with GPT summaries averaging 0.08\u00b10.02 and LlaMA summaries 0.19\u00b10.01, which suggest minimal n-gram overlap, so diverse phrasing. Further details and statistics are in App. A and examples in App. P."}, {"title": "5.2 Recommendation Quality", "content": "We assess the quality of information within user summaries using recommendation performance as a proxy. For this, we benchmark popular AE-based methods, GPT, and two TEARS and GERS variants, using two established top-k metrics recall@k and NDCG@k.\nModel Evaluation. We evaluate several AE-based models, including Multi-VAE [29], RecVAE [44], MacridVAE [31], EASE [46], and Multi-DAE [29]. To ensure fairness in comparison, we use the same ratings used to generate the user summaries as input for these models. Unlike the typical practice of binarizing inputs, we use the original ratings, as this better aligns with the user summaries and leads to improved performance in AEs. Additionally, we benchmark TEARS VAEs with backbones built from Multi-VAE, RecVAE, and MacridVAE. For GERS, we only train it using RecVAE as the backbone VAE, as we found it to be the best-performing AE. Additionally, we evaluate versions of TEARS and GERS without a backbone VAE-referred to as TEARS Base and GERS Base. TEARS Base most closely resembles the framework shown in Figure 1 [39]. We also evaluate GPT under strong generalization using few-shot prompting to compare against a fully LLM-based solution (details are in App. D).\nResults. Table 1 presents the results, averaged over five seeds evaluated with the top k = 20, 50 items. For TEARS and GERS VAE models, we report test set metrics using \u03b1*, being the optimal value determined using the validation set (see App. K for \u03b1 values). We also compare against Base models by including results for the LLaMA-based TEARS RecVAE using \u03b1 = 1, being the most performant when only using summary embeddings (see App. J).\nTEARS & Backbone AEs Evaluations. We find TEARS VAE models consistently outperform their backbone AEs across all but a single benchmark. This increase in performance is most prevalent in ML-1M where users are generally less active and in a sparser dataset like Goodbooks (see App. B). This result also suggests summaries have useful information not found in the black-box embeddings contributing to performance. We find that TEARS VAEs can consistently improve upon their backbone VAEs. This is even the case with RecVAE, the best-performing AE, where TEARS RecVAE outperforms it on all datasets regardless of the backbone LLM. Notably, on ML-1M, TEARS RecVAE can outperform some black-box models using an \u03b1 = 1. It is also encouraging that TEARS VAE models can offer higher performance while maintaining scrutability (see \u00a7 6 for these results). In contrast, TEARS Base does considerably worse than all AEs, showcasing the importance of TEARS VAEs' hybrid setup. GPT performs poorly in this context, reinforcing the need for model adaptation to this task.\nTEARS & GERS Evaluations. Meanwhile, GERS Base consistently outperforms TEARS Base, suggesting that simple genre statistics can be more effective than summaries for initial recommendations. However, TEARS RecVAE outperforms GERS RecVAE on the ML-1M and Goodbooks datasets, indicating that user summaries provide valuable information beyond black-box or genre representations. On Netflix, however, TEARS RecVAE and GERS RecVAE perform similarly, implying that the performance gains are likely explained by genre information. To improve this, future work could explore more dataset-specific prompts, but it is noteworthy that even in cases where summaries have a limited impact, they still offer slight performance improvements and, crucially, do not degrade performance. Furthermore, TEARS RecVAE with \u03b1 = 1 consistently outperforms TEARS Base, demonstrating that aligning black-box and summary embeddings enhances performance when using only user summaries.\nOverall, TEARS VAE models show that it is possible to significantly enhance scrutability without sacrificing-and even improving-performance compared to both AE and genre-based models. TEARS provides a plug-in to standard AEs. We provide additional analysis over components of TEARS, such as ablation studies in App. L, analysis on user activity levels in App. N, and evaluate GPT-based models using LLaMA summaries and vice versa in App. O."}, {"title": "6 Controllability Through Text Edits", "content": "We now study the controllability of user text representations, which is the ability of users to edit and adjust their representation to (better) align the system's recommendations with their preferences. Controllability is one of the main advantages of text representations compared to latent representations. We analyze GPT-based models and summaries in the main text and provide all results for LLaMA-based models in App. F. As these changes are specific to textual summaries, we do not assess the controllability of the genre-based models in the following experiments but provide analyses in App. I.\nGiven the lack of evaluation metrics for scrutable recommendations, we create three tasks to evaluate user controllability. Each task corresponds to a scenario leading users to update their text summaries. The resulting tasks are easily benchmarked across methods. First, large-scope changes, simulates correcting significant inaccuracies in a user profile. Second, finer or small-scope changes aim to adjust minor discrepancies in a user summary. Other changes can be seen as interpolating between these two cases where a large change is simply many aggregate small edits. Third, we test the ability of summaries to guide personalized recommendations. This tests a different type of user interaction where the summary is used as an instruction (e.g. in a particular context). This evaluates a model's capacity to interpolate between historical behavior and a context. The tasks are detailed below and in Figure 3."}, {"title": "6.1 Evaluating Large Scope Changes", "content": "We first evaluate how well TEARS can react to a large user interest change. To simulate such a change, we prompt GPT to \"flip\u201d a user's interest. We do so by first prompting GPT to identify a user's most and least favored genre. Using these genres, we prompt GPT to make the user's favorite genre into its least favorite and vice-versa, effectively inducing a large shift in the user's preference. We find that GPT can induce such a shift. We provide an example and the full prompting strategy in App. G. To evaluate TEARS' effectiveness at modeling such changes, we design the genre-wise Discounted Cumulative Gain at k (DCG\\(g\\)@k), which measures how favored a genre p is in the user's top-k rankings. Items from a newly-favored genre should rank higher than in the original ranking, and the difference in DCG\\(g\\) captures this shift.\nBelow, we define DCG\\(g\\)@k, where \\(w(i)\\) maps the i-th item to its corresponding set of genres (items can have multiple genres):\n\\[DCG_g@k(p) = \\sum_{i=1}^{k} \\frac{I(p \\in \\omega(i))}{\\log_2(i + 1)}\\]\nWe normalize DCG\\(g\\)@k using the Idealized Discounted Gains (IDCG) to obtain the genre-wise NDCG (NDCG\\(g\\)@k). To assess the effectiveness of the changes, we measure the \\(\\Delta\\)@k change in NDCG\\(g\\)@k between the original (denoted with a superscript O) and augmented summary (denoted with superscript A):\n\\[\\Delta@k(p) = NDCG^A_g@k(p) \u2013 NDCG^O_g@k(p).\\]\nWe evaluate each summary using two metrics: |\\(\\Delta_{up}\\)@k|, which assesses TEARS' ability to elevate the rankings of the initially least favored genre (p = least favorite), and |\\(\\Delta_{down}\\)@k|, which gauges its proficiency in lowering the rankings of the initially favored genre (p = favorite). We prompt GPT to obtain the altered summaries for all test users and use those to examine shifts in genre preferences. In this experiment, we also explore the influence of parameter \u03b1 on controllability, highlighting the trade-off between recommendation performance and controllability.\nFigure 4 illustrates the trade-off between recommendation performance (NDCG@20) calculated using the original summaries compared to controllability (|\\(\\Delta_{up}\\)@20| & |\\(\\Delta_{down}\\)@20|) as \u03b1 increases for ML-1M and Goodbooks (Netflix in App F.2). We find that controllability increases as \u03b1 does, with all TEARS variants having meaningful levels of controllability at higher \u03b1 values across all datasets, regardless of backbone LLM (see App. F). Remarkably, TEARS VAEs maintain satisfactory controllability even at reduced values of \u03b1. As \u03b1 increases, we find that NDCG@20 peaks at some intermediate value visually resembling an inverse-U shape, likely due to \\(\\mathcal{L}_R\\) (Equation (5)) optimizing for performance over various values of \u03b1. We find TEARS MacridVAE to be the best model, consistently outperforming TEARS Base in controllability-a surprising finding-and AEs in NDCG@20 across multiple values of \u03b1."}, {"title": "6.2 Fine-Grained Changes", "content": "We now simulate a task where a user wants to increase the rank of a single target item by making small edits to their summary. While we could do such a simulation by putting in the item's name", "tasks": "first, to \"summarize the item in 5 words while only referring to plot points/themes,\" and second to replace an existing sentence in the summary with one including these plot points and themes. Using this, we measure the difference in rank drank between"}]}