{"title": "Raply: A profanity-mitigated rap generator", "authors": ["Omar Manil BENDALI", "Samir FERROUM", "Ekaterina KOZACHENKO", "Youssef PARVIZ", "Hanna SHCHARBAKOVA", "Anna TOKAREVA", "Shemair WILLIAMS"], "abstract": "The task of writing rap is challenging and involves producing complex rhyming schemes, yet meaningful lyrics. In this work, we propose Raply, a fine-tuned GPT-2 model capable of producing meaningful rhyming text in the style of rap. In addition to its rhyming capabilities, the model is able to generate less offensive content. It was achieved through the fine-tuning the model on a new dataset Mitislurs, a profanity-mitigated corpus. We evaluate the output of the model on two criteria: 1) rhyming based on the rhyme density metric; 2) profanity content, using the list of profanities for the English language. To our knowledge, this is the first attempt at profanity mitigation for rap lyrics generation.", "sections": [{"title": "1 Introduction", "content": "Since its inception in the late 1980's rap music has become one of the most influential music genres owing to its ability to address various ethical and social issues affecting people worldwide.\nIn this paper, we study the problem of determining the most lyrically relevant rap line while taking into account ethical issues. Our interest in this problem is motivated by the following perspectives. Firstly, we want to study the structure of rap lyrics to generate relevant text which adheres to the features of rap lyrics. Secondly, we are aware of social and ethical issues around this topic, but we also should treat respectfully the specific range of language expressions that is often characteristic of the rap genre. Thirdly, we want to be able to give rappers the ability to create expressive works about topics relevant to their experience without the need to fall back on human ghostwriters when the works they create are not lyrically strong (i.e. lack of rap techniques).\nWe formalize this problem as an auto-completion task. Given a query $p = [A : , s(1), s(2), ..., s(n)]$, an auto completion returns $q = [B:,t(1), t(2), ..., t(n)]$, where q is an extension of p. In this specific rap lyrics generation problem, p is the set of the input tokens s(i), and q is the set of model's generated tokens t(n) that shares syntactic, semantic and rap-specific characteristics with p (rhyming), we assume that A marks the beginning of the user input line and B, the beginning of the model's generated line.\nOur main contributions can be summarized as follows:\n1. We propose a method for mitigating the number of profanities in rap lyrics generation through the construction of the Mitislurs corpus, in order to be mindful of the issues often discussed in the rap genre.\n2. We introduce a method of rap lyrics generation that is able to produce satisfying rap in terms of rhyming."}, {"title": "2 Related work", "content": "Automatic rap lyrics generation is a challenging task in natural language processing. It requires a system to be able to learn complex linguistic and semantic information from lyrics and to use this knowledge to create a new, creative song.\nThis task has become a more active research area in recent years, mainly due to the success of deep learning techniques (Potash et al., 2015; Malmi et al., 2016; Manjavacas et al., 2019; Nikolov et al., 2020; Liu et al., 2022)."}, {"title": "3 Rhyming and rap", "content": "Rhyme may be defined as the \u201cacoustic agreement of sounds (vowels and consonants), words or groups of words\u201d (Grof\u010d\u00edkov\u00e1 and M\u00e1\u010dajov\u00e1, 2020). In other words, rhyme is simply having a similar sound between two or more words. Rhyming is a defining feature of various works of art such as poetry, songwriting as well as rap."}, {"title": "3.1 Types of rhymes", "content": "Rhymes may take various forms, such as perfect rhymes or slant rhymes, to name a few. Perfect rhymes are one of the most popular and well-known types of rhyming which involves words sharing the same syllabic sound stress occurring at the end of the word. The sound similarity and syllabic stress between the words walk and talk, stalk are examples of perfect rhymes. While slant rhymes, which is a differentiation of perfect rhymes, do not have such a strict sound pattern, and the rhyme is often located internally within the words. Take for example the words, home and none, the similarity exists internally within each word as the word stress happens at the beginning of both words resulting in a rhyme.\nOther not-so-common forms of rhyme include assonance which involves the repetition of vowels for example, drown and sound. Consonance, another form of rhyme is characterized by rhyming through the repetition of consonant sounds such as staff and flat. Many of these styles of creating rhyme overlap or work together to create rhythm. For instance, alliteration is a combination of assonance and consonance where the same consonant or vowel is repeatedly stressed in each word. The sentence Peter Piper picked a peck of pickled pepper, is a perfect example of alliteration at play.\nRhymes may be further defined by their positions within a line or sentence or verse. End rhymes for instance appear at the end of a sentence or line while internal rhymes take place within a line or sentence."}, {"title": "3.2 Rap song structure", "content": "Various artists constantly make use of rhyming schemes to create rhythm and rhyming patterns, especially in rap music. Many rappers follow some form of the rhyming scheme in their music and this is quite apparent in the various rap songs chosen for our corpus. And in like manner, these rhyme patterns may be found either at the beginning or the end of a line, sentence, or verse.\nRap music makes use of bars, which is simply another name for the lyrics of the lines the rappers use in their music. These bars are then grouped by what are known as verses. Verses refer to the different segmentations that occur in a rap song. In general, rap music employs about 16 bars per verse. Each verse used by the rapper tells some form of a story and is dependent on some form of rhyming scheme. In addition to this, many verses are preceded by what is known as a hook or the punchline of the verse. This is the part of the verse which captures and maintains the listener's attention throughout the entirety of the lyrics."}, {"title": "4 Data collection", "content": "In this section, we describe the steps we made to obtain the new corpus and preprocess it.\nTo build a system that generates a rap line based on the text input, we compiled a large corpus of rap lyrics together with lyrics metadata. For this purpose, we scrapped Genius.com \u00b9, a large collection of lyrics, to obtain the necessary data.\nForemost, we assembled a list of 89 American English-speaking rap performers chosen based on their renowned writing and rhyming skills. Further, via the free Genius Song Lyrics API request, which is made using a client Access Token generated on the site, we got access to all available information about that particular performer. With the help of lyricsgenius library 2, we collected the required information, specifically the artist's name, the name of the song, the year of release of the song, and the lyrics.\nThus we obtained a raw corpus of 21,801 songs with a size of 14,296,438 tokens.\nWe use preprocessing which was designed with attention to the specifics of the data. Based on the structure of the rap song, we removed parts of the songs that we found unnecessary (e.g., instrumental) and repetitive (e.g., song remixes). In addition, we removed all the songs written in languages other than English. In songs, the parts that represent the chorus of songs are repetitive, and would not help to generate sophisticated lines; also, interludes, do contain unnecessary discourse that would not be relevant to our task. There are also confusing parts, that were dropped, which do not exist in all the songs (e.g., [Intro] or non-ASCII characters). Additionally, we have chosen to keep only verses parts. The aim of using verses only is to focus only on parts of the lyrics that contain the most rhyming pieces, other parts are considered repetitive and irrelevant to the rhymes."}, {"title": "5 Data annotation", "content": "Modern language models are trained on large text corpora from the Internet. If a model receives training data containing slurs or abusive language, it will learn to predict these words during the training step and subsequently produce the output containing these words. To minimize the occurrence of profanities in the data generated by the system, we decided to automatically filter data by offensive language beforehand. To compile the dataset, we used The Obscenity List, developed by Surge AI 3, which contains over 1600 popular English profanities and their variations 4, including non-standard spellings (e.g., canonical form damn could be written as goddamn).\nIn order to tokenize and lemmatize each lyric from the raw dataset, we used NLTK library (word_tokenize() to tokenize the texts and the Wordnet Lemmatizer to lemmatize them). Then we automatically extracted the profanities from each song. The obtained dataset with offensive language (see Figure 2) includes lyrics in which 50,130 profanities were found, as well as lyrics with no profanities at all, that is 2,225 songs.\nMoreover, we additionally used two types of annotation, which were produced by Surge AI 6: profanity categories and severity rating."}, {"title": "6 Raply", "content": "In this section, we describe Raply \u2013 an automatic rap generation system."}, {"title": "6.1 Model Overview", "content": "We used a GPT-2 (Radford et al., 2020) model, which is a generative language model based on Transformers decoder. Training such a model comes in two steps: an unsupervised pre-training on a large quantity of data followed by a supervised fine-tuning step. In the case of an auto-completion task, the input of the model is the concatenation of the query p and its extension q, with a delimiter token in between. Since our goal is to explore the effectiveness of GPT-2 on generating rap, instead of training the model from scratch, we use a pre-trained GPT-2 model and fine-tune it on our corpora for the task of rap lyrics generation."}, {"title": "6.2 Experiments", "content": "We run the experiments on the two built corpora. Due to the nature of input data required for fine-tuning GPT-2 model, we concatenate verse lines, separated by the special token 'Line:'. The pre-trained model we used was trained on BookCorpus (Yao and Huang, 2018), a dataset of over 7,000 unpublished fiction books from various genres. We reuse the byte-pair encoding vocabulary of this model.\nWe randomly select 10% examples from data as the test set and use the remaining examples as the training set. For both corpora, we use Adam optimizer with a learning rate of le-5 and a batch size of 32. We train the models on 3 epochs. The training phase was done in both Kaggles platform as well as Grid 50009 on Nvidia GPU P100.\nAfter the training phase, we proceed with the generation phase. We split each instance of the test set into two parts, an input part, and a reference part, the input is fed to the model during the generation and the reference is used as a baseline. Our code is made available on GitHub 10.\nWe request the model to generate sequences using top-k sampling method (Fan et al., 2018), the value of k is set to 50 and the minimum and maximum length of the output sequence to 4 and 50 respectively.\nFor each generated sequence, we compute its rhyme density, a metric introduced by (Malmi et al., 2016) to evaluate the quality of rhyming fluency. Rhyme density has shown a correlation with a human evaluation of rap lyrics' technical quality, which makes it a suitable metric for evaluating the models. A rhyme density greater than 1 can be considered as high. It should be taken into account though, that this metric only considers one aspect of rap, namely rhyme, and ignores other crucial components of rap like figurative language, the context, the meaning of the words, etc.\nSince rhyming is not always identifiable through orthography, we use the program eSpeak\u00b9\u00b9 to transform each token into its phoneme form in the IPA 12 and then find the longest matching vowel sequence near each token. We then average the length of the sequence overall words to obtain the value of rhyme density. This process is applied to both the generated sequence and the reference part of the test set. We choose to mainly focus on the rhyming aspect. We justify this by the fact that GPT-2 model already showcases satisfying semantic preservation abilities and that rap lyrics are inherently not strict when it comes to semantic preservation in comparison with poetry for example. We also measure the slur score on the generated text as defined in the formula defined in Section 5."}, {"title": "6.3 Results and discussion", "content": "We compare the performances of the GPT-2 model on our two corpora (Slurs vs Mitislurs) and compare the perplexity and rhyme density values of the generated text with the human-generated one from our test set. Perplexity is frequently used to assess the quality of language models, and measures how well a probabilistic model is able to predict a sample. A lower perplexity means that the model is more confident in its predictions and, as a result, is less startled by the data."}, {"title": "7 Further work", "content": "Pertaining to the direction of this paper, we aim to improve text generation by implementing a ranking algorithm during the lyrics generation phase. Therefore, we should be able to select the line which adheres the most to a rhyming pattern. In order to achieve the aforementioned task, we also aim to train the model on a curated corpus that follows better rhyming composition. That is to say, the source data should have a clearer rhyming pattern that will be able to train the model to produce the desired text output. Additionally, the text generation will be tested using more efficient models like GPT-3 and compared to previously generated data to compare and measure the results obtained from previous models. Lastly, we plan to focus on qualitative analysis of model results."}, {"title": "8 Conclusion", "content": "In this paper, we present Raply, a GPT-2-based model for rap generation, which takes into account offensive content and tries to mitigate its occurrence. We achieved it through the collection of a more ethical corpus Mitilslurs and thus we reduced the presence of offensive content in the generated rap. The developed model is also able to generate close-to-human-like rhyming patterns according to the rhyme density metric.\nTo our knowledge, Raply is the first model that tackles the issue of offensive content in the context of rap lyrics generation."}, {"title": "9 Environmental impact", "content": "We do care about the environmental effects of our NLP models on energy consumption and carbon emissions. Below we provide the information considering our models' training. Assuming that in order to train each of the models we applied the maximum GPU power, i.e. 250 watts, then the energy consumed can be obtained using the following formula:\n$E = Pt$\nWe have therefore calculated the energy consumption for each trained model per day (Table 5). In total, therefore, we spent 3,25 kilowatts hour per day on training the models. For comparison, we have borrowed the energy consumption statistics for 2019 from the EIA15 website. Thus, each resident in France consumes 17,6 kilowatts of energy per day, which is 5 times as much as our trained models."}, {"title": "10 Ethical considerations", "content": "Working with text generation models is challenging in the sense that the trained model is a kind of \"black box\" which is difficult to supervise. This raises significant ethical issues that must be addressed. As we deal with rap lyrics generation, we anyway encounter ethical issues, in particular, discrimination, as the focus of our paper is shifted to handling profanity categories. This paper contains examples of words that may be abusive to certain categories of people. Unfortunately, the occurrence of slurs in rap lyrics is virtually inevitable, since the presence of profanities is part of rap culture.\nHowever, in this paper, we aim to reduce the number of slurs in the generated data. It is also essential to stress that the examples of generated lines containing slurs by no means reflect the attitude of the authors toward any categories of people. There is no way we can affect models already trained on enormous amounts of data, but we attempt to retrain the model on our own data to somehow mitigate occurring slurs and obtain less offensive verses.\nAnother critical concern that we have to deal with is the bias of our model. Since the Obscenity list consists of unbalanced categories of slurs, and certain categories of slurs (e.g. sexual anatomy/acts) tend to occur in the training data, there is a risk that the model will generate more slurs of the same category. Consequently, one should attempt to mitigate the bias, for example, by balancing the songs in the training lyrics dataset according to the presence of profanities of the particular category, or by trying to control the number of slurs directly during generation.\nA further matter of no small significance is that of legal issues. The possibility of user-generated content on websites like Genius violating intellectual property rights (IPR) also needs to be addressed, as Genius disclaims any responsibility for any content posted by users. Because it could be difficult or even impossible for others to utilize the data or build upon it if it is not legally permitted to do so, this can pose issues for reproducibility. It is crucial that Genius, or any other platform, take proactive steps to address these issues and establish a clear, consistent policy for handling user-generated content that can violate IPR."}]}