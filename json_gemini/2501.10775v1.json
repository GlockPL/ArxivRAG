{"title": "MedFILIP: Medical Fine-grained Language-Image Pre-training", "authors": ["Xinjie Liang", "Xiangyu Li", "Fanding Li", "Jie Jiang", "Qing Dong", "Wei Wang", "Kuanquan Wang", "Suyu Dong", "Gongning Luo", "Shuo Li"], "abstract": "Medical vision-language pretraining (VLP) that leverages naturally-paired medical image-report data is crucial for medical image analysis. However, existing methods struggle to accurately characterize associations between images and diseases, leading to inaccurate or incomplete diagnostic results. In this work, we propose MedFILIP, a fine-grained VLP model, introduces medical image-specific knowledge through contrastive learning, specifically: 1) An information extractor based on a large language model is proposed to decouple comprehensive disease details from reports, which excels in extracting disease deals through flexible prompt engineering, thereby effectively reducing text complexity while retaining rich information at a tiny cost. 2) A knowledge injector is proposed to construct relationships between categories and visual attributes, which help the model to make judgments based on image features, and fosters knowledge extrapolation to unfamiliar disease categories. 3) A semantic similarity matrix based on fine-grained annotations is proposed, providing smoother, information-richer labels, thus allowing fine-grained image-text alignment. 4) We validate MedFILIP on numerous datasets, e.g., RSNA-Pneumonia, NIH ChestX-ray14, VinBigData, and COVID-19. For single-label, multi-label, and fine-grained classification, our model achieves state-of-the-art performance, the classification accuracy has increased by a maximum of 6.69%.", "sections": [{"title": "I. INTRODUCTION", "content": "MEDICAL vision-language pretraining (VLP) that leverages naturally-paired image-report data has proven to be a pivotal component for the medical image analysis tasks (MIA) [1]\u2013[3]. Data scarcity, an ongoing challenge in MIA can be significantly mitigated by utilizing medical reports as supervision [4]\u2013[6], because reports occur alongside images naturally in clinical records, without requiring human labeling. Additionally, reports are derived from real patient cases and inherently relate to the corresponding images, better reflecting real-world clinical scenarios and practice [7], [8]. Medical VLP methods that exploit this organically paired data excel in capturing the interplay between images and reports. This synergy is crucial for the effective modeling of image-disease relationships and essential for a variety of downstream medical applications.\nHowever, existing medical VLP methods have limitations in adequately characterizing the relationships between images and diseases, primarily due to the challenge of harnessing the"}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. Medical Vision-Language Contrastive Learning.", "content": "To address the scarcity of annotated medical image-related data, numerous studies have employed text as supervisory information [1], [12], [13]. The text contains rich and precise signals, and using data where images accompanied by text can reduce the need for additional manual labeling [14]\u2013[16]. For example, CLIP [9] collects data from the web to obtain a large number of image-text pairs. ConVIRT [4] pairs diagnostic reports obtained during clinical diagnostic processes with corresponding images. MedCLIP [6] augments unpaired data by converting it into paired image-text data as a supplement. However, previous methods lack the ability to represent fine-grained visual features of the disease. Our approach improves the model\u2019s fine-grained representation ability by establishing a mapping between disease categories and their visual-attribute descriptions, thereby associating disease categories with their relevant attributes within the model.\nThe medical image-text contrastive learning method employs a two-stream flow contrastive learning strategy to map images and text into a unified representation space, achieving semantic alignment between visual and textual modalities. Image-report paired methods pair medical images with radiology reports for contrastive learning [4], [17]; however, they encounter limitations in scenarios involving current diseases and repeated occurrences. Image-entity paired methods apply named entity recognition (NER) [7] tools to extract disease categories from diagnostic texts, then combine category information with additional information as the textual supervision for contrastive learning [5], [6]; however, they struggle to handle intra-class differences and generalize to novel classes. Our approach decouples disease information from diagnostic reports, thus addressing the common problem of concurrent diseases and repeated occurrences. And through the preservation of detailed information, our method offers an accurate understanding of various disease subclasses."}, {"title": "B. Fine-grained Representation Learning.", "content": "Fine-grained representation learning refers to the process of learning detailed and discriminative representations for objects within a specific category by capturing the fine-grained relationship between image regions and textual attributes [18]\u2013[21]. Previous methods such as PLIP [22] focused on the attribute-level distinctions between different categories, overlooking the connections that exist between categories. Using additional medical knowledge can help the model implicitly establish relationships between medical entities [23], [24].\nIn our method, disease categories are not only transformed into combinations of image-specific attributes but are also enriched with additional medical knowledge, such as anatomical, pathological, and imaging correlates, which are missing in the traditional fine-grained representation learning approach. This allows the model to leverage both more fine-grained visual features and prior knowledge from the medical domain, providing a more comprehensive characterization of the relationships between images and diseases."}, {"title": "III. METHODOLOGY", "content": null}, {"title": "A. GPT-IE for Fine-grained Label Extraction", "content": "GPT-IE leverages the extensive knowledge encapsulated within large language model (LLM) to extract fine-grained entities from diagnostic reports. This extraction process decouples disease information from lengthy reports, converting the one-to-one image-report relationship into a one-to-many relationship between image and diseases, which is instrumental in differentiating concurrent diseases and various instances of the same disease. Additionally, GPT-IE captures detailed disease characteristics, such as location and severity, enabling the creation of a fine-grained medical dataset, which is pivotal for training deep learning models to handle intra-class variations, ultimately enhancing patient-specific care.\nDistinct from traditional Named Entity Recognition (NER) systems [7], which rely on predefined rules and lack flexibility, GPT-IE uses the LLM to extract entities without more human design. The innovative use of prompt engineering enables LLM to perform specialized entity extraction without extensive setup [25], [26]. Designing suitable prompts can guide LLM to look for specific patterns, we employ prompts such as \u201cEach extracted piece of disease-related information should follow the format: severity of disease + location of disease + category of disease.\u201d Thus, GPT-IE can extract expected fine-grained entities, directly addressing the specific needs of medical data labeling. The extraction process is defined by:\n{severity, location, category} = fIE(report + prompt), (1)\nwhere fIE is the information extractor, and the fine-grained entities consist of severity (e.g., acute, chronic, mild), location (e.g., left-sided, bilateral, mediastinal), and category (e.g., pneumonia, pneumothorax, pulmonary nodules). Note that not all triples contain complete information; those missing severity or location will be preserved, while triples lacking the category will be removed. For a balanced consideration of both cost and performance, our primary experiments were conducted using GPT-3.5-Turbo [27]. The granular details of the extraction method, along with the LLM\u2019s prompt design strategies, are comprehensively elaborated in the supplementary material.\nSummarized advantages. GPT-IE introduces a cost-efficient framework capable of diverse information extraction needs. By dynamically adjusting prompts based on actual needs, GPT-IE converts diagnostic reports expected into fine-grained disease information, which can reduce the complexity of supervisory text while retaining the disease details."}, {"title": "B. IKI for Improving Generalization", "content": "The proposed Image-specific Knowledge Injector (IKI) enables unseen category classification by correlating disease categories with visual features and prior knowledge. Distinguishing between \u201clocalized patchy shadowing or consolidative opacities\u201d and \u201cdarkened area between the lung and chest wall\u201d is more straightforward than discerning between \u201cpneumonia\u201d and \u201cpneumothorax\u201d [28]\u2013[30]. Building on this, IKI deconstructs abstract disease categories into these distinct, image-specific attributes. This decomposition enables the model to understand the relationships between visual features"}, {"title": "C. SSM for Finer-grained Image-text Alignment", "content": "The proposed semantic similarity matrix (SSM) offers a more precise label representation that better captures the relationships between medical images and their textual annotations. SSM is used to replace the multi-hot matrix for enhanced visual-language alignment, by using a continuum of values ranging from 0 to 1 instead of binary states (0 or 1), the SSM can not only realize clustering of different diseases but also realize internal clustering of different subclasses, as shown in Fig. 4. Meanwhile, unlike previous methods where each image contains only one positive label for a specific condition [6], [17], the SSM can handle situations where each image corresponds to one or more disease labels, thus enabling the model to perform multi-label classification. We calculate"}, {"title": "D. Multimodal Contrastive Learning", "content": "a) Training: The image and text are processed through their respective feature extractors:\ni = fimg(Eimg(Ximg)), (6)\nl = ftxt(Etxt(Xtxt)), (7)\nwhere Ximg and Xtxt represent the image and text inputs, Eimg and Etxt are the encoders that extract image-specific features and distill medical knowledge from text, respectively. fimg and ftxt are projections used to map image and text features into the same embedded space, producing the final features i and l. Then a cosine similarity is computed to predict the similarity between images and texts:\nY_{i,j} = \\frac{I \\cdot L_j}{||I|| \\cdot ||L_j||}, (8)\nwhere I is the set of image feature vectors, L is the set of text feature vectors, and Y is the cosine similarity between images and texts, which is obtained by computing the inner product of I and L.\nThe model adjusts the proximity of embeddings in this shared space based on label similarity; the higher the similarity, the closer the embeddings, as shown in Fig. 4. Embeddings from similar image and text are positioned closer together in the embedded space, embeddings from different image and text are positioned farther. Due to the high similarity between subclasses, yet not being identical, clustering of subclasses will be carried out within the same class.\nb) Image-Text Matching Loss (ITM): ITM loss refines the model\u2019s learning process by treating similarity as a continuous variable. It combines the mean squared error (MSE) with cross-entropy (CE) loss to better capture the degree of association between images and texts:\nL(y, s) = \\frac{1}{nm} \\sum_{i=1}^{n} \\sum_{j=1}^{m} (Y_{ij} - S_{ij})^2 - \\sum_{i=1}^{n} \\sum_{j=1}^{m} Y_{i,j}log(S_{i,j}), (9)\nwhere y is predicted matrix, s is the sementic similarity matrix, n represents the number of images, m represents the number of fine-grained entities.\nc) Inference: By calculating the similarity between image embedding and text embedding, the model is able to determine whether an image I belongs to a certain category C,\nP(I \u2208 C) = Sim(Eimg(I), Etxt(template(C', Dict(C)))), (10)\nC* = Argmax(P(I \u2208 C')), (11)\nCEV"}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Experimental Setup", "content": "a) Datasets: RSNA-Pneumonia [31], contains around 30k chest X-ray images including normal and pneumonia cases. We use 1,024 samples to test the model's capability for single-label classification.\nNIH ChestX-ray14 [32], contains around 112k chest X-ray images of 18 chest diseases. We use 1,024 samples containing 10 common diseases to test the model's capability for multi-label classification.\nVinBigData [33], contains around 18k chest X-ray images of 14 chest diseases. We use 1,024 samples containing 8 common diseases to test the model's capability for multi-label classification.\nCOVID [34], contains around 18k chest X-ray images including non-COVID and COVID cases. We use 1,024 samples to test the model's performance on unseen types.\nMIMIC-CXR [8], [35], contains around 370k chest X-ray images paired with associated radiology reports. We obtain four datasets from the MIMIC-CXR:\nMIMIC-train, contains 50 diseases with a positive ratio of over 0.0001, labeled as severity + lesion location + category, with 50,000 samples for model pre-training;\nMIMIC-16, contains 16 diseases with a positive ratio of over 0.01 to test the model's capability for single-label classification;\nMIMIC-multi, contains 16 diseases with a positive ratio of over 0.01 to test the model's capability for multi-label classification;"}, {"title": "B. Downstream Tasks", "content": "1) Zero-shot Classification:\na) Experimental settings: The classification results are based on the candidate texts with a cosine similarity exceeding the threshold to the input image. For our approach, the candidate texts consist of image-specific explanations related to"}, {"title": "D. Case Study", "content": "Fig. 5 evidences the promising performance of MedFILIP for matching images with both fine-grained entities and image-specific explanations. The pre-training models employ contrastive learning between images and text to acquire rich multimodal representations in the training phase, which enables the model to discern the similarity between different images and texts effectively. In the inference phase, the classification output of the pre-training models is determined by selecting the candidate text with the highest cosine similarity to the input image, thereby determining whether the image belongs to a certain type of disease. To assess the pre-training models\u2019 representation capability, we compute similarity scores between paired images and texts. A high similarity score signifies that the model can effectively extract key features and meanings from both the images and texts, and establish connections across the different modalities. In this study, the images are chest X-ray images, and the texts are of three types: (1) a prompt template combined with disease category, using"}, {"title": "V. DISCUSSION", "content": "The proposed approach mainly solves the following two types of challenges, which are crucial for the advancement of medical vision-language pretraining:\nConcurrent diseases and repeated occurrences. A diagnostic report can encompass multiple disease descriptions and"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a novel method called MedFILIP that leverages fine-grained annotations to improve medical vision-language contrastive learning. Our proposed MedFILIP effectively circumvents the issues arising from the complexity of diagnostic reports and tackles the challenges in predicting subclasses and unseen classes. The proposed GPT-IE effectively extracts fine-grained entities from diagnosis reports by a large language model, decoupling disease information, including severity, location, and categories from reports, thereby allowing the distinction of disease subclasses. Our IKI maps diseases to fine-grained image-specific explanations, enabling the model to learn relationships between diseases and their attributes, thus allowing predictions on unseen classes based on the knowledge learned from seen classes. Our SSM finally attains labels representing the correlation between images and text based on the aforementioned fine-grained annotations, thereby proceeding with fine-grained image-text comparative learning. A full set of experiments were carried out which demonstrated the proposed MedFILIP framework achieved its goal of enhancing multimodal learning via fine-grained annotations."}]}