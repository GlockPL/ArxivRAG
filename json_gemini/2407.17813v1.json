{"title": "Enhancing Model Performance: Another Approach to Vision-Language Instruction Tuning", "authors": ["Vedanshu", "MM Tripathi", "Bhavnesh Jaint"], "abstract": "The integration of large language models (LLMs) with vision-language (VL) tasks has been a transformative development in the realm of artificial intelligence, highlighting the potential of LLMs as a versatile general-purpose chatbot. However, the current trend in this evolution focuses on the integration of vision and language to create models that can operate in more diverse and real-world contexts. We present a novel approach, termed Bottleneck Adapter, specifically crafted for enhancing the multimodal functionalities of these complex models, enabling joint optimization of the entire multimodal LLM framework through a process known as Multimodal Model Tuning (MMT). Our approach utilizes lightweight adapters to connect the image encoder and LLM without the need for large, complex neural networks. Unlike the conventional modular training schemes, our approach adopts an end-to-end optimization regime, which, when combined with the adapters, facilitates the joint optimization using a significantly smaller parameter set. Our method exhibits robust performance with 90.12% accuracy, outperforming both human-level performance (88.4%) and LaVIN-7B (89.41%).", "sections": [{"title": "1 Introduction", "content": "In the landscape of modern computational linguistics, large language models (LLMs) [1-4] have been at the forefront, consistently expanding the boundaries of what machines understand about human language. This expansion has been largely driven by increases in the models' complexity, measured by the number of parameters, and"}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT) techniques, in contrast to complete fine-tuning strategies, keep most of the parameters of pretrained models fixed while achieving similar performance on subsequent tasks. In the realm of PEFT, several methods have been investigated. These include prompt tuning [18-21], Low-Rank Adaptation (LoRA)[12], and adapters [9, 22, 23]. Prompt tuning entails adding trainable prompt tokens to pre-trained large models, either in input embeddings exclusively or throughout all intermediate layers. LoRA integrates trainable rank decomposi-tion matrices into network weights, demonstrating promising fine-tuning capabilities, particularly in large generative models. Adapters incorporate lightweight adapta-tion modules into each layer of pre-trained transformers, providing flexibility across various domains. These PEFT approaches aim to find a middle ground between effi-ciency and effectiveness in fine-tuning pre-trained models for a variety of downstream tasks. LAVIN [24] examines the incorporation of lightweight adapters into transformer blocks. They propose the idea of Mixture-of-Modality Adaptation (MMA) to connect Language and Vision tasks (LLMs) with Vision-Language (VL) tasks, allowing for the concurrent improvement of image and language models. MMA utilizes a routing algo-rithm to facilitate the smooth transition of LLMs between single- and multi-modal instructions while maintaining their expertise in natural language understanding. However, this study argues that dynamic adapters such as MMA may not be essential for achieving similar levels of accuracy, which could streamline both time and space requirements."}, {"title": "3 Method", "content": "Our research introduces a distinctive approach, termed Bottleneck Adapter (BA), specifically crafted for enhancing large language models (LLMs) with vision-language capabilities. The Bottleneck Adapter seamlessly integrates into LLMs, bestowing them with multimodal functionalities. It adeptly navigates between single- and multi-modal instructions while allowing for a unified optimization of the entire multimodal LLM framework through a process known as Multi Model Tuning (MMT). This methodology is particularly noteworthy for its efficiency, significantly reducing both the computational resources and storage needed during training."}, {"title": "3.1 Bottleneck Adapter Structure", "content": "At the heart of our methodology lies the Bottleneck Adapter(BA). Traditional visual adapters typically include a non-linear function to bolster adaptation for NLP tasks. Our research, however, unveils that omitting this non-linearity does not detract from the Adapter's performance in visual tasks. As a result, we redefine the adapter function"}, {"title": "3.2 Bottleneck Adapter Placement", "content": "Recognizing the profound disparities between visual and linguistic models, we metic-ulously consider the placement of adapters within the model architecture. Inspired by the LAVIN adapter placement strategy, we select to situate each adapter prior to the Transformer block. This strategic positioning is not limited to Multi-Head Attention (MHA), but is also extended to the Feed-Forward Network (FFN) within the Vision Transformer (ViT). Such a deliberate placement strategy ensures that our adapters are optimally positioned to promote effective and efficient integration and learning within the multimodal LLMs.\nIn essence, our Adapter-based methodology marks a significant step forward in the realm of vision language adaptation for LLMs, presenting a lightweight, resource-efficient, yet highly effective solution to augmenting the multimodal functionalities of these complex models."}, {"title": "3.3 Model", "content": "In our model architecture, we integrate the Bottleneck Adapter into a large language model (LLM), specifically LLaMA-2[25], and pair it with the diht-ViT[26] as the image encoder. For processing an input image \\(I \\in R^{h \\times w \\times 3}\\), we extract the visual features from the [cls] tokens at every fourth layer of the ViT, represented as \\(V\\in R^{n \\times d}\\). In the image encoder architecture, adapters are strategically positioned before each multi-head attention module.\nFor text instructions, we utilize word embeddings, denoted as \\(T\\in R^{l \\times c}\\). To align the visual features with the LLM's dimensionality, we employ a visual adapter, which transforms V as follows:\n\\(V' = \\rho(VW + b_v)W_t + b_t\\)    (3)\nHere, \\(W \\in R^{d \\times d_v}\\) and \\(W_t \\in R^{d_v \\times c}\\) are the transformation weight matrices, while \\(b_v \\in R^{d_v}\\) and \\(b_t \\in R^c\\) serve as bias terms. The activation function pis chosen as the SwiGLU[27] function for its efficiency. Importantly, the dimension \\(d_v\\) is selected to be considerably smaller than both d and c, optimizing resource usage."}, {"title": "4 Experiments", "content": "In Section 4.1, we detail our approach's multimodal performance on the ScienceQA benchmark. An ablation study conducted on the ScienceQA validation set is presented in Section 4.2."}, {"title": "4.1 Experiment Setup", "content": "Datasets and tasks.\nModels. In our approach, we utilize the Vision Transformer Large/14 (ViT-L/14-336PX) [29], pre-trained using the DIHT framework[26], to serve as the image encoding mechanism. This process involves the extraction of visual features through six [cls] tokens, each derived from every fourth layer within the ViT-L/14-336PX structure. For the language model component, we incorporate the LLaMA-2-7B variant. The visual processing section, referred to as the visual neck, is configured with a dimension of 128. Meanwhile, the Adapter is specified to have a dimension of 16, with the temperature settings adjusted to 10 for 7B.\nBaselines. We compare our method with the recent parameter-efficient fine-tuning methods and adapters where a new adapter structure with down-projection, up-projection and non-linear function are inserted into the transformer (both vision and NLP) and only the parameters of this new module are updated. Our baseline mmodels for comparison include the LaVIN-7B lite-clip[24], the LaVIN-13B -clip [24], the LLaMA adapter[30], LLaVA[28], Chameleon[31], the MM-CoTLarge[32], InstructBLIP (FlanT5XXL) [33] and BLIP-2 (FlanT5XXL)[16].\nImplementation details. We process the image by directly resizing to 224 \u00d7 224 without any augmentation. The AdamW [34] optimization algorithm is employed to"}, {"title": "4.2 Results and Analysis", "content": "Our 7B-lite-diht-BA demonstrates competitive or superior performance, especially in the SOC and NO context categories. It signifies the model's balanced capabil-ity across different subjects and contexts, affirming the effectiveness of our proposed adaptations and optimizations. The human benchmark is 88. 40% average accuracy, highlighting the challenging nature of the ScienceQA dataset and the impressive capa-bility of AI models such as LaVIN-13B, MM-CoTLarge, and our LaVIN-7B-lite-diht in approaching human-level performance on complex and multimodal scientific question answering.\nDespite the higher performance of the MM-CoT-Large model, it is important to consider the advantages of our model's lighter and smaller architecture. The MM-COT-Large model outperformed the 7B-lite-dith-BA (ours) model, likely due to a larger parameter space, sophisticated chain-of-thought reasoning capabilities, and potentially more effective multimodal integration. In social sciences, our method achieves 92.46% accuracy, surpassing LaVIN-7B (94.34%) but falling short of LaVIN-13B (94.38%) and LLAVa (95.95%). This indicates a strong performance, albeit with room for improvement in comparison to the top-performing methods. Our method's accuracy in language-related questions stands at 87.91%, which is competitive against LaVIN-7B (85.24%) but not as high as LaVIN-13B (87.73%) or LLAVa (88%). Our method gave an accuracy of 89.25% in text modality, which is comparably better than LaVIN-7B (88.51%) and closely aligned with the human benchmark (89.60%). With 87.01% accuracy in image-based questions, our method outshines LaVIN-7B (87.46%) but is marginally outperformed by human accuracy (87.50%). The 7B-lite-diht-BA(ours) achieves an average accuracy of 90.12% across all categories. This is a remarkable result that closely mirrors human performance (88.40%) and surpasses LaVIN-7B (89.41%) by a narrow margin.\n From table 3, if we look at the impact of the change in the backbone (different version of LLAMA), it can be seen that LLAMA-2 gives a slight edge in overall accu-racy. With LLAMA [2] and Rep-Adapter [24, 35] we obtained an accuracy of 88.35%, while simply changing the backbone to LLAMA-2[25] we got an accuracy of 88.78 %. The vision backbone used here was CLIP [36]. With this result in mind, consecutive experiments were done using LLAMA-2 as a LLM backbone.\nMotivated by CSPNet [37], we did concatenation of features in adapter. We look at the experiments 5 and 6 in which concatenation in adapter was used, it can be found that concatenation didn't increase the accuracy. For concatenation equation 1 was modified to concatenate the features as: [\\(\\psi_1(Z)\\), \\(\\psi_2(Z\\)]. The idea was to preserve the features with each blocks.\nExperiment 7,8 and 9 were done using LoHa[38] as adapter. In the first one, we didn't use any down scaling convolution. We just used two LoHa conv_ld. While adding these two convolutions we used dynamic weight factors same as in LAVIN[24]. This is formulated as follows:\n\\(adapter(Z) = \\hat w_1 \\cdot f(Z, W_1) + \\hat w_2 \\cdot f(Z, W_2)\\)  (6)\nwhere, w\u2081 and w\u2082 are the routing weights and W\u2081 and W2 are the Hadamard product of two low-rank inner matrices, W := \\((X_1Y^T) \\odot (X_2Y^T)\\). In the second experiment, we simply replaced conv_ld in LAVIN with LoHa conv_1d. In the third expeitment, we added silu activation function without any weight factors which can be formulated as follows:\n\\(g(Z) = \\phi_L1(Silu(\\phi_L2(Z))) + Z\\) (7)\nwhere, L1 and L2 are the LoHa convolution 1D operators.\nLower accuracy with LoHa can be linked with higher number of trainable parameter added in the adapter. Since, other adapters are using very less number of trainable parameters, they achieve higher accuracy with less data.\nIn the next experiment (11), we tried an hybrid of LORA and Rep-Adapter. In this LORA was used in LLM backbone (LLAMA-2) and Rep-Adapter was used in the vision backbone (CLIP). The reason of this experiment was to see the relation between dynamic Rep-Adapter with the vision. But the accuracy was not able to surpass the baseline.\nIn the final experiment we tried our mainline adapter with dynamic weight factors, but the results were not promising. Which allowed us to come to the conclusion that dynamic weight factors are not helpful for multi-modal fine tuning.\nWhile selecting vision backbone we did experiment with CLIP [36], ALIP [39] and DIHT[26]. With CLIP we got an accuracy of 88.78% and with ALIP we got an accuracy of 87.98% and with DIHT we got an accuracy of 88.68%. While CLIP and ALIP used an image size of 224x224, DIHT uses an image of 336x336. With this in mind we used DIHT as our vision backbone even though CLIP had a slight edge over DIHT."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel approach to enhance large language models (LLMs) with multimodal capabilities, addressing the critical challenge of computa-tional and storage demands in integrating vision-language tasks. Our method leverages lightweight adapters and quantized encoding layers to significantly reduce the mem-ory footprint, facilitating swift training on a single GPU without compromising the model's natural language processing capabilities. By eschewing conventional modu-lar training strategies for an end-to-end optimization regime, we achieved notable efficiency and performance improvements over existing solutions."}, {"title": "6 Declarations", "content": ""}, {"title": "6.1 Conflict of interest", "content": "The authors declare that they have no conflict of interests."}, {"title": "6.2 Data availability", "content": "The study relies on openly accessible datasets, which can be found at https://scienceqa.github.io/. These datasets are freely available to researchers, pro-moting transparency, reproducibility, and facilitating further examination of the research outcomes."}]}