{"title": "Enhancing Model Performance: Another Approach to Vision-Language Instruction Tuning", "authors": ["Vedanshu", "MM Tripathi", "Bhavnesh Jaint"], "abstract": "The integration of large language models (LLMs) with vision-language (VL) tasks has been a transformative development in the realm of artificial intelligence, highlighting the potential of LLMs as a versatile general-purpose chatbot. However, the current trend in this evolution focuses on the integration of vision and language to create models that can operate in more diverse and real-world contexts. We present a novel approach, termed Bottleneck Adapter, specifically crafted for enhancing the multimodal functionalities of these complex models, enabling joint optimization of the entire multimodal LLM framework through a process known as Multimodal Model Tuning (MMT). Our approach utilizes lightweight adapters to connect the image encoder and LLM without the need for large, complex neural networks. Unlike the conventional modular training schemes, our approach adopts an end-to-end optimization regime, which, when combined with the adapters, facilitates the joint optimization using a significantly smaller parameter set. Our method exhibits robust performance with 90.12% accuracy, outperforming both human-level performance (88.4%) and LaVIN-7B (89.41%).", "sections": [{"title": "1 Introduction", "content": "In the landscape of modern computational linguistics, large language models (LLMs) [1-4] have been at the forefront, consistently expanding the boundaries of what machines understand about human language. This expansion has been largely driven by increases in the models' complexity, measured by the number of parameters, and\nthe breadth of pre-training data. One notable advancement in this domain is the con-\ncept of instruction tuning [1, 5-7], which has significantly enhanced the ability of LLMs\nto mimic human conversational patterns and effectively perform a variety of natural\nlanguage processing (NLP) tasks, inching closer to the elusive goal of artificial gen-\neral intelligence, as exemplified by models like GPT-3.5 [1]. The current trend in this\nevolution aims to imbue LLMs with multimodal understanding, particularly the inte-\ngration of vision and language, to create models that can operate in more diverse and\nreal-world contexts. This ambition has been partly realized in recent advancements\nlike GPT-4 [8], which represents a leap forward by incorporating a substantial vision-\nlanguage dataset directly into the training of a multimodal generative pre-trained\ntransformer.\nIn the expansive domain of artificial intelligence, the integration of large lan-\nguage models (LLMs) with vision-language (VL) tasks has been a transformative\ndevelopment. However, this integration often comes with considerable computational\nand storage demands. Traditional parameter-efficient transfer learning (PETL) [9-\n13] methods, designed to reduce these demands by fine-tuning a minimal number of\nparameters, have not fully met the efficiency and performance needs, particularly for\nmultimodal LLMs. Existing approaches also show a gap in performance compared\nto full model fine-tuning and exhibit significant redundancy when combining LLMs\nwith vision models for tasks like image captioning[14] or text-to-image generation[15].\nMoreover, modular training strategies, although beneficial, involve expensive VL pre-\ntraining and often necessitate extensive updating of LLM parameters, hindering quick\nand efficient model adaptation.\nIn light of these challenges, our model introduces an innovative approach that\nstands out from the current landscape. By not freezing the LLM and vision encoder\nand implementing these layers in a quantized format, our model significantly slashes\nthe memory footprint. It's designed to support not just image-text but also text-\nonly instructions, addressing a major limitation in models like BLIP2[16] and\nminiGPT4[17].\nOur approach utilizes lightweight adapters to connect the image encoder and LLM,\nenabling joint optimization of image and language models without the need for large,\ncomplex neural networks. This framework allows for seamless shifting between single\nand multi-modal instructions without compromising natural language understanding\ncapabilities. Unlike the conventional modular training schemes, our model adopts an\nend-to-end optimization regime, which, when combined with the lightweight adapters,\nfacilitates the joint optimization of the entire multimodal LLM using a significantly\nsmaller parameter set. This leads to an impressive reduction in storage overhead,\noutclassing existing solutions by a considerable margin.\nOur method's ability to process both image-text and text-only instructions, along\nwith its quantized form, ensures that it can be swiftly trained on a single GPU,\nmarking a substantial leap in training efficiency. In summary, our key contributions\nare as follows:\n\u2022 We present a novel method that bypasses the need for costly training while pre-\nserving the LLM's NLP capabilities, addressing a critical gap in current multimodal\nLLMs."}, {"title": "2 Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT) techniques, in contrast to complete fine-\ntuning strategies, keep most of the parameters of pretrained models fixed while\nachieving similar performance on subsequent tasks. In the realm of PEFT, several\nmethods have been investigated. These include prompt tuning [18-21], Low-Rank\nAdaptation (LoRA)[12], and adapters [9, 22, 23]. Prompt tuning entails adding train-\nable prompt tokens to pre-trained large models, either in input embeddings exclusively\nor throughout all intermediate layers. LoRA integrates trainable rank decomposi-\ntion matrices into network weights, demonstrating promising fine-tuning capabilities,\nparticularly in large generative models. Adapters incorporate lightweight adapta-\ntion modules into each layer of pre-trained transformers, providing flexibility across\nvarious domains. These PEFT approaches aim to find a middle ground between effi-\nciency and effectiveness in fine-tuning pre-trained models for a variety of downstream\ntasks. LAVIN [24] examines the incorporation of lightweight adapters into transformer\nblocks. They propose the idea of Mixture-of-Modality Adaptation (MMA) to connect\nLanguage and Vision tasks (LLMs) with Vision-Language (VL) tasks, allowing for the\nconcurrent improvement of image and language models. MMA utilizes a routing algo-\nrithm to facilitate the smooth transition of LLMs between single- and multi-modal\ninstructions while maintaining their expertise in natural language understanding. How-\never, this study argues that dynamic adapters such as MMA may not be essential\nfor achieving similar levels of accuracy, which could streamline both time and space\nrequirements."}, {"title": "3 Method", "content": "Our research introduces a distinctive approach, termed Bottleneck Adapter (BA),\nspecifically crafted for enhancing large language models (LLMs) with vision-language\ncapabilities. The Bottleneck Adapter seamlessly integrates into LLMs, bestowing them\nwith multimodal functionalities. It adeptly navigates between single- and multi-modal\ninstructions while allowing for a unified optimization of the entire multimodal LLM\nframework through a process known as Multi Model Tuning (MMT). This method-\nology is particularly noteworthy for its efficiency, significantly reducing both the\ncomputational resources and storage needed during training."}, {"title": "3.1 Bottleneck Adapter Structure", "content": "At the heart of our methodology lies the Bottleneck Adapter(BA). Traditional visual\nadapters typically include a non-linear function to bolster adaptation for NLP tasks.\nOur research, however, unveils that omitting this non-linearity does not detract from\nthe Adapter's performance in visual tasks. As a result, we redefine the adapter function"}, {"title": "3.2 Bottleneck Adapter Placement", "content": "Recognizing the profound disparities between visual and linguistic models, we metic-\nulously consider the placement of adapters within the model architecture. Inspired by\nthe LAVIN adapter placement strategy, we select to situate each adapter prior to the\nTransformer block. This strategic positioning is not limited to Multi-Head Attention\n(MHA), but is also extended to the Feed-Forward Network (FFN) within the Vision\nTransformer (ViT). Such a deliberate placement strategy ensures that our adapters\nare optimally positioned to promote effective and efficient integration and learning\nwithin the multimodal LLMs.\nIn essence, our Adapter-based methodology marks a significant step forward in\nthe realm of vision language adaptation for LLMs, presenting a lightweight, resource-\nefficient, yet highly effective solution to augmenting the multimodal functionalities of\nthese complex models."}, {"title": "3.3 Model", "content": "In our model architecture, we integrate the Bottleneck Adapter into a large language\nmodel (LLM), specifically LLaMA-2[25], and pair it with the diht-ViT[26] as the image\nencoder. For processing an input image $I \\in R^{h \\times w \\times 3}$, we extract the visual features\nfrom the [cls] tokens at every fourth layer of the ViT, represented as $V\\in R^{n \\times d}$.\nIn the image encoder architecture, adapters are strategically positioned before each\nmulti-head attention module.\nFor text instructions, we utilize word embeddings, denoted as $T\\in R^{l \\times c}$. To align\nthe visual features with the LLM's dimensionality, we employ a visual adapter, which\ntransforms V as follows:\n$V' = \\rho(VW_v + b_v)W_t + b_t$\nHere, $W_v \\in R^{d \\times d_v}$ and $W_t \\in R^{d_v \\times c}$ are the transformation weight matrices, while\n$b_v \\in R^{d_v}$ and $b_t \\in R^c$ serve as bias terms. The activation function $\\rho$ is chosen as the\nSwiGLU[27] function for its efficiency. Importantly, the dimension $d_v$ is selected to be\nconsiderably smaller than both d and c, optimizing resource usage."}, {"title": "4 Experiments", "content": "In Section 4.1, we detail our approach's multimodal performance on the ScienceQA\nbenchmark. An ablation study conducted on the ScienceQA validation set is presented\nin Section 4.2."}, {"title": "4.1 Experiment Setup", "content": "Models. In our approach, we utilize the Vision Transformer Large/14 (ViT-L/14-\n336PX) [29], pre-trained using the DIHT framework[26], to serve as the image encoding\nmechanism. This process involves the extraction of visual features through six [cls]\ntokens, each derived from every fourth layer within the ViT-L/14-336PX structure. For\nthe language model component, we incorporate the LLaMA-2-7B variant. The visual\nprocessing section, referred to as the visual neck, is configured with a dimension of 128.\nMeanwhile, the Adapter is specified to have a dimension of 16, with the temperature\nsettings adjusted to 10 for 7B.\nBaselines. We compare our method with the recent parameter-efficient fine-\ntuning methods and adapters where a new adapter structure with down-projection,\nup-projection and non-linear function are inserted into the transformer (both vision\nand NLP) and only the parameters of this new module are updated. Our base-\nline mmodels for comparison include the LaVIN-7B lite-clip[24], the LaVIN-13B\nclip [24], the LLaMA adapter[30], LLaVA[28], Chameleon[31], the MM-CoTLarge[32],\nInstructBLIP (FlanT5XXL) [33] and BLIP-2 (FlanT5XXL)[16].\nImplementation details. We process the image by directly resizing to 224 \u00d7 224\nwithout any augmentation. The AdamW [34] optimization algorithm is employed to\nfine-tune our model, which undergoes a training regimen spanning 20 epochs, guided\nby an absolute learning rate. The training process is characterized by a batch size of\n1, a learning rate of 0.009, and a weight decay factor of 0.02.\nDuring the text generation phase, a top-p sampling strategy is adopted, utilizing\na temperature setting of 0.1 and a top-p threshold of 0.75."}, {"title": "4.2 Results and Analysis", "content": "Our 7B-lite-diht-BA demonstrates competitive or superior performance, especially\nin the SOC and NO context categories. It signifies the model's balanced capabil-\nity across different subjects and contexts, affirming the effectiveness of our proposed\nadaptations and optimizations. The human benchmark is 88. 40% average accuracy,\nhighlighting the challenging nature of the ScienceQA dataset and the impressive capa-\nbility of AI models such as LaVIN-13B, MM-CoTLarge, and our LaVIN-7B-lite-diht in\napproaching human-level performance on complex and multimodal scientific question\nanswering.\nDespite the higher performance of the MM-CoT-Large model, it is important to\nconsider the advantages of our model's lighter and smaller architecture. The MM-COT-\nLarge model outperformed the 7B-lite-dith-BA (ours) model, likely due to a larger\nparameter space, sophisticated chain-of-thought reasoning capabilities, and potentially\nmore effective multimodal integration. In social sciences, our method achieves 92.46%\naccuracy, surpassing LaVIN-7B (94.34%) but falling short of LaVIN-13B (94.38%)\nand LLAVa (95.95%). This indicates a strong performance, albeit with room for\nimprovement in comparison to the top-performing methods. Our method's accuracy\nin language-related questions stands at 87.91%, which is competitive against LaVIN-\n7B (85.24%) but not as high as LaVIN-13B (87.73%) or LLAVa (88%). Our method\ngave an accuracy of 89.25% in text modality, which is comparably better than LaVIN-\n7B (88.51%) and closely aligned with the human benchmark (89.60%). With 87.01%\naccuracy in image-based questions, our method outshines LaVIN-7B (87.46%) but\nis marginally outperformed by human accuracy (87.50%). The 7B-lite-diht-BA(ours)\nachieves an average accuracy of 90.12% across all categories. This is a remarkable result\nthat closely mirrors human performance (88.40%) and surpasses LaVIN-7B (89.41%)\nby a narrow margin."}, {"title": "5 Conclusion", "content": "In this paper, we introduced a novel approach to enhance large language models\n(LLMs) with multimodal capabilities, addressing the critical challenge of computa-\ntional and storage demands in integrating vision-language tasks. Our method leverages\nlightweight adapters and quantized encoding layers to significantly reduce the mem-\nory footprint, facilitating swift training on a single GPU without compromising the\nmodel's natural language processing capabilities. By eschewing conventional modu-\nlar training strategies for an end-to-end optimization regime, we achieved notable\nefficiency and performance improvements over existing solutions."}, {"title": "6 Declarations", "content": ""}, {"title": "6.1 Conflict of interest", "content": "The authors declare that they have no conflict of interests."}, {"title": "6.2 Data availability", "content": "The study relies on openly accessible datasets, which can be found at\nhttps://scienceqa.github.io/. These datasets are freely available to researchers, pro-\nmoting transparency, reproducibility, and facilitating further examination of the\nresearch outcomes."}]}