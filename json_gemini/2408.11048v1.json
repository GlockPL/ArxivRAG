{"title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands", "authors": ["Yi Zhao", "Le Chen", "Jan Schneider", "Quankai Gao", "Juho Kannala", "Bernhard Sch\u00f6lkopf", "Joni Pajarinen", "Dieter B\u00fcchler"], "abstract": "It has been a long-standing research goal to endow robot hands with human-level dexterity. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.", "sections": [{"title": "1 Introduction", "content": "Empowering robots with human-level dexterity is notoriously challenging. Current robotics research on hand and arm motions focuses on manipulation and dynamic athletic tasks. Manipulation, such as grasping or reorienting [1], requires continuous application of acceptable forces at moderate speeds to various objects with distinct shapes and weight distributions. Environmental changes, like humidity or temperature, alter the already complex contact dynamics, which adds to the complexity of manipulation tasks. Dynamic tasks, like juggling [2] and table tennis [3] involve making and breaking contact, demanding high precision and tolerating less inaccuracy due to rarer contacts. High speeds in these tasks necessitate greater accelerations and introduce a precision-speed tradeoff.\nRobot piano playing combines various aspects of dynamic and manipulation tasks: the agent is required to coordinate multiple fingers to precisely press keys for arbitrary songs, which is a high-dimensional and rich control task. At the same time, the finger motions have to be highly dynamic, especially for songs with fast rhythms. Well-practiced pianists can play arbitrary songs, but this level of generalization is extremely challenging for robots. In this work, we build the foundation to develop methods capable of achieving human-level bi-manual dexterity at the intersection of manipulation and dynamic tasks, while reaching such generalization capabilities in multi-task environments.\nWhile reinforcement learning (RL) is a promising direction, traditional RL approaches often struggle to achieve excellent performance in multi-task settings [4]. The advent of scalable imitation learning techniques [5] enables representing complex and multi-modal distributions. Such large models are most effective when trained on massive datasets that combine the state evolution with the"}, {"title": "2 Related Work", "content": "Dexterous Robot Hands The research of dexterous robot hands aims to replicate the dexterity of human hands with robots. Many previous works [10, 11, 12, 13, 14, 15, 16, 17] use planning to compute a trajectory followed by a controller, thus require an accurate model of the robot hand. Closed-loop approaches have been developed by incorporating sensor feedback [18]. These methods also require an accurate model of the robot hand, which can be difficult to obtain in practice, especially considering the large number of active contacts between the hand and objects.\nDue to the difficulty of actually modeling the dynamics of the dexterous robot hand, recent methods resort to learning-based approaches, especially RL, which has achieved huge success in both"}, {"title": "Piano Playing with Robots", "content": "Piano playing with robot hands has been investigated for decades. It is a challenging task since bi-manual robot hands should precisely press the right keys at the right time, especially considering its high-dimensional action space. Previous methods require specific robot designs [32, 33, 34, 35, 36, 37] or trajectory pre-programming [38, 39]. Recent methods enable piano playing with dexterous hands through planning [40] or RL [41] but are limited to simple music pieces. RoboPianist [4] introduces a benchmark for robot piano playing and demonstrates strong RL performance, but requires human fingering labels and performs worse in multi-task learning.\nHuman fingering informs the agent of the correspondence between fingers and pressed keys at each time step. These labels require expert annotators and are, therefore, expensive to acquire in practice. Several approaches learn fingering from human-annotated data with different machine learning methods [6, 42, 43]. Moryossef et al. [44] extract fingering from videos to acquire fingering labels cheaply. Ramoneda et al. [45] propose to treat piano fingering as a sequential decision-making problem and use RL to calculate fingering but without considering the model of robot hands. Shi et al. [46] automatically acquires fingering via dynamic programming, but the solution is limited to simple tasks. In our paper, we do not introduce a separate fingering model, instead, similar to human pianists, fingering is discovered automatically while playing the piano."}, {"title": "Datasets for Dexterous Robot Hands", "content": "Most large-scale datasets of dexterous robot hands focus on grasping various objects. To get suitable grasp positions, some methods utilize planners [47, 48, 49], while others use learned grasping policies [29], or track grasping motions of humans and imitate these motions on a robot hand [50]. Compared to the abundance of datasets for grasping, there exist relatively few datasets for object manipulation with dexterous robot hands. The D4RL benchmark [51] provides small sets of expert trajectories for four such tasks, consisting of human demonstrations and rollouts of trained policies. Zhao et al. [52] provide a small object manipulation dataset that utilizes a low-cost bi-manual platform with simple parallel grippers. Chen et al. [53] collect offline datasets for two simulated bi-manual manipulation tasks with dexterous hands. To the best of our knowledge, our RP1M dataset is the first large-scale dataset of dynamic, bi-manual manipulation with dexterous robot hands."}, {"title": "3 Background", "content": "Task Setup The simulated piano-playing environment is built upon RoboPianist [4]. It includes a robot piano-playing setup, an RL-based agent for playing piano with simulated robot hands, and a multi-task learner. To avoid confusion, we refer to these components as RoboPianist, RoboPianist-RL,"}, {"title": "4 Large-Scale Motion Dataset Collection", "content": "In this section, we describe our RP1M dataset in detail. We first introduce how to train a specialist piano-playing agent without human fingering labels. Removing the requirement of human fingering labels allows the agent to play any sheet music available on the Internet (under copyright license). We then analyze the performance of our specialist RL agent as well as the learned fingering. Lastly, we introduce our collected large-scale motion dataset, RP1M, which includes ~1M expert trajectories for robot piano playing, covering ~2k pieces of music."}, {"title": "4.1 Piano Playing without Human Fingering Labels", "content": "To mitigate the hard exploration problem posed by the sparse rewards, RoboPianist-RL adds dense reward signals by using human fingering labels. Fingering informs the agent of the \"ground-truth\" fingertip positions, and the agent minimizes the Euclidean distance between the current fingertip positions and the \"ground-truth\u201d positions. We now discuss our OT-based method to lift the requirement of human fingering.\nAlthough fingering is highly personalized, generally speaking, it helps pianists to press keys timely and efficiently. Motivated by this, apart from maximizing the key pressing rewards, we also aim to minimize the moving distances of fingers. Specifically, at time step t, for the i-th key $k^i$ to press, we use the j-th finger $f^j$ to press this key such that the overall moving cost is minimized. We define the minimized cumulative moving distance between fingers and target keys as $d_{t}^{\\text{OT}} \\in \\mathbb{R}^{+}$, given by\n$d_{t}^{\\text{OT}} = \\min_{\\substack{w_t(k^{i},f^{j}) \\ (i,j) \\in K_t \\times F}} \\sum_{(i,j) \\in K_t \\times F} w_t(k^{i}, f^{j}) c_t(k^{i}, f^{j}), \\text{ s.t., } \\\\ i) \\sum_{j \\in F} w_t(k^{i}, f^{j}) = 1, \\text{ for } i \\in K_t, \\\\ ii) \\sum_{i \\in K_t} w_t(k^{i}, f^{j}) \\leq 1, \\text{ for } j \\in F, \\\\ iii) w_t(k^{i}, f^{j}) \\in \\{0, 1\\}, \\text{ for } (i, j) \\in K_t \\times F.$\nKt represents the set of keys to press at time step t and F represents the fingers of the robot hands. $c_t(k^{i}, f^{j})$ represents the cost of moving finger $f^{j}$ to piano key $k^{i}$ at time step t calculated by their Euclidean distance. $w_t(k^{i}, f^{j})$ is a boolean weight. In our case, it enforces that each key in Kt will be pressed by only one finger in F, and each finger presses at most one key. The constrained optimization problem in Eq. (1) is an optimal transport problem. Intuitively, it tries to find the best \"transport\" strategy such that the overall cost of moving (a subset of) fingers F to keys Kt is minimized. We solve this optimization problem with a modified Jonker-Volgenant algorithm [55] from SciPy [56] and use the optimal combinations (i*, j*) as the fingering for the agent.\nWe define a reward $r_t^{\\text{OT}}$ based on $d_t^{\\text{OT}}$ to encourage the agent to move the fingers close to the keys $K_t$, which is defined as:\n$r_t^{\\text{OT}} = \\begin{cases}\n \\exp(c \\cdot (d_t^{\\text{OT}} - 0.01)^2) & \\text{if } d_t^{\\text{OT}} \\geq 0.01, \\\\\n 1.0 & \\text{if } d_t^{\\text{OT}} < 0.01.\n\\end{cases}$\nc is a constant scale value and we use the same value as Tassa et al. [57]. The overall reward function is defined as:\n$r_t = r_t^{\\text{OT}} + r_t^{\\text{Press}} + r_t^{\\text{Sustain}} + \\alpha_1 \\cdot r_t^{\\text{Collision}} + \\alpha_2 \\cdot r_t^{\\text{Energy}}$\n$r_t^{\\text{Press}}$ and $r_t^{\\text{Sustain}}$ represent the reward for correctly pressing the target keys and the sustain pedal. $r_t^{\\text{Collision}}$ encourages the agent to avoid collision between forearms and $r_t^{\\text{Energy}}$ prefers energy-saving behaviors. $\\alpha_1$ and $\\alpha_2$ are coefficient terms, and $\\alpha_1 = 0.5$ and $\\alpha_2 = 5 \\cdot 10^{-3}$ are adopted. Our method is compatible with any RL methods, and we use DroQ [58] in our paper. During RL training, the generated fingering will keep updating according to the state of hands. After giving an initial guess at the beginning, the OT-based fingering will be revised if the agent discovers a better fingering solution."}, {"title": "4.2 Analysis of Specialist RL Agents", "content": "The performance of the specialist RL agents decides the quality of our dataset. In this section, we investigate the performance of our specialist RL agents. We are interested in i) how the proposed OT-based finger placement helps learning, ii) how the fingering discovered by the agent itself compares to human fingering labels, and iii) how our method transfers to other embodiments.\nResults In Fig. 2, we compare our method with RoboPianist-RL both with and without human fingering. We use the same DroQ algorithm with the same hyperparameters for all experiments. RoboPianist-RL includes human fingering in its inputs, and the fingering information is also used in the reward function to force the agent to follow this fingering. Our method, marked as OT, removes the fingering from the observation space and uses OT-based finger placement to guide the agent"}, {"title": "Analysis of the Learned Fingering", "content": "We now compare the fingering discovered by the agent itself and the human annotations. In Fig. 3, we visualize the sample trajectory of playing French Suite No.5 Sarabande and the corresponding fingering. We found that although the agent achieves strong performance for this song (the second plot in Fig. 2), our agent discovers different fingering compared to humans. For example, for the right hand, humans mainly use the middle and ring fingers, while our agent uses the thumb and first finger. Furthermore, in some cases, human annotations are not suitable for the robot hand due to different morphologies. For example, in the second time step of Fig. 3, the human uses the first finger and ring finger. However, due to the mechanical limitation of the robot hand, it can not press keys that far apart with these two fingers, thus mimicking human fingering will miss one key. Instead, our agent discovered to use the thumb and little finger, which satisfies the hardware limitation and accurately presses the target keys."}, {"title": "Cross Emboidments", "content": "Labs usually have different robot platforms, thus having a method that works for different embodiments is highly desirable. We test our method on a different embodiment. To simplify the experiment, we disable the little finger of the Shadow robot hand and obtain a four-finger robot hand, which has a similar morphology to Allegro [59] and LEAP Hand [60]. We evaluate the modified robot hand on the song French Suite No.5 Sarabande (first 550 time steps), where our method achieves a 0.95 F1 score, similar to the 0.96 achieved with the original robot hands. In the bottom row of Fig. 3, we visualize the learned fingering with four-finger hands. The agent discovers different fingering compared to humans and the original hands but still accurately presses active keys, meaning our method is compatible with different embodiments."}, {"title": "4.3 RP1M Dataset", "content": "To facilitate the research on dexterous robot hands, we collect and release a large-scale motion dataset for piano playing. Our dataset includes ~1M expert trajectories covering ~2k musical pieces. For each musical piece, we train an individual DroQ agent with the method introduced in Section 4.1 for 8 million environment steps and collect 500 expert trajectories with the trained agent. We chunk each sheet music every 550 time steps, corresponding to 27.5 seconds, so that each run has the same"}, {"title": "5 Benchmarking Results", "content": "The analysis in the previous section highlighted the diversity of highly dynamic piano-playing motions in the RP1M dataset. In this section, we assess the multi-task imitation learning performance of several widely used methods on our benchmark. To be specific, the objective is to train a single multi-task policy capable of playing various music pieces on the piano. We train the policy on a portion of the RP1M dataset and evaluate its in-distribution performance (F1 scores on songs included in the training data) and its generalization ability (F1 scores on songs not present in the training data).\nBaselines We evaluated Behavior Cloning (BC) [62], Implicit Behavioral Cloning (IBC) [63], BC with a Recurrent Neural Network policy (BC-RNN) [64], and Diffusion Policy [5]. BC directly learns a policy by using supervised learning on observation-action pairs from expert demonstrations. IBC learns an implicit policy as an energy-based model conditioned on observation and action. BC-RNN uses an RNN as the policy network to encode a history of observations. Diffusion Policy learns to model the action distribution by inverting a process that gradually adds noise to a sampled action sequence. We used a CNN-based Diffusion Policy with DDIM [65] as the sampler. We use the same code and hyperparameters as Chi et al. [5].\nExperiment Setup We first train the policies with 3 different sizes of expert data: 50, 150, and 300 songs. We then evaluate the trained policies on 3 different groups of music pieces. (1) In-distribution songs: music pieces that overlap with the training sets. It shows the multitasking performance of the trained policies. (2) Easy out-of-distribution (OOD) songs: simple music pieces that do not overlap with the training songs. Those pieces are easy to play, with only slow motions and short horizons."}, {"title": "6 Limitations & Conclusion", "content": "Limitations Our paper has limitations in several aspects. Firstly, although our method lifts the requirement of human-annotated fingering, enabling RL training on diverse songs, our method still fails to achieve strong performance on challenging songs due to fast rhythms and mechanical limi- tations of the robot hands. This could be solved by proposing a better RL method and improving the hardware design of the robot hands. Secondly, our dataset only includes proprioceptive obser- vations. However, humans play piano with multi-modal inputs, including vision, tactile sensing, and auditory information. Enabling the agent to play the piano from such rich input sources is an intriguing direction. Lastly, although we demonstrate better zero-shot generalization performance than RoboPianist-MT [4], there is still a gap between our best multi-task agent and RL specialists, which requires future investigation.\nConclusion In this paper, we propose a large-scale motion dataset named RP1M for piano playing with bi-manual dexterous robot hands. RP1M includes 1 million expert trajectories for playing 2k musical pieces. To collect such a diverse dataset for piano playing, we lift the need for human- annotated fingering in the previous method by introducing a novel automatic fingering annotation approach based on optimal transport. On single songs, our method matches the baselines with human-annotated fingering and can be adopted across different embodiments. Furthermore, we benchmark various imitation learning approaches for multi-song playing. We report new state-of-the- art results in motion synthesis for novel music pieces and identify the gap to achieving human-level piano-playing ability. We believe the RP1M dataset, with its scale and quality, forms a solid step towards empowering robots with human-level dexterity."}, {"title": "A RP1M Dataset Collection Details", "content": "A.1 Reward formulation\nIn Eq. (3), we give the overall reward function used in our paper. We now give details of each term. $r^{\\text{Press}}$ indicates whether the active keys are correctly pressed and inactive keys are not pressed. We use the same implementation as [4], given as: $r^{\\text{Press}} = 0.5 \\cdot (\\frac{1}{K} \\sum_i g(||k_i - 1||_2)) + 0.5 \\cdot (1 - \\frac{1}{K} 1_{fp})$. K is the number of active keys, ki is the normalized key states with range [0, 1], where 0 means the i-th key is not pressed and 1 means the key is pressed. g is tolerance from Tassa et al. [57], which is similar to the one used in Equation (2). $\\frac{1}{K} 1_{fp}$ indicates whether the inactive keys are pressed, which encourages the agent to avoid pressing keys that should not be pressed. $r^{\\text{Sustain}}$ encourages the agent to press the pseudo sustain pedal at the right time, given as $r^{\\text{Sustain}} = g(s_t - s_{\\text{target}})$. St and $s_{\\text{target}}$ are the state of current and target sustain pedal respectively. $r^{\\text{Collision}}$ penalizes the agent from collision, defined as $r^{\\text{Collision}} = 1 - 1_{\\text{collision}}$, where $1_{\\text{collision}}$ is 1 if collision happens and 0 otherwise. $r^{\\text{Energy}}$ prioritizes energy-saving behavior. It is defined as $r^{\\text{Energy}} = \\frac{|\\tau_{\\text{joints}}|}{\\tau} \\cdot \\frac{|\\upsilon_{\\text{joints}}|}{\\upsilon}$. $\\tau_{\\text{joints}}$ and $\\upsilon_{\\text{joints}}$ are joint torques and joint velocities respectively.\nA.2 Training details\nObservation Space Our 1144-dimensional observation space includes the proprioceptive state of dexterous robot hands and the piano as well as L-step goal states obtained from the MIDI file. In our case, we include the current goal and 10-step future goals in the observation space (L=11). At each time step, an 89-dimensional binary vector is used to represent the goal, where 88 dimensions are for key states and the last dimension is for the sustain pedal. The dimension of each component in the observation space is given in Table 3."}, {"title": "A.3 Computational resources", "content": "We train our RL agents on the cluster equipped with AMD MI250X GPUs, 64 cores AMD EPYC \"Trento\" CPUs, and 64 GBs DDR4 memory. Each agent takes 21 hours to train. The overall data collection cost is roughly 21 hours * 2089 agents = 43,869 GPU hours.\nA.4 MuJoCo XLA Implementation\nTo speed up training, we re-implement the RoboPianist environment with MuJoCo XLA (MJX), which supports simulation in parallel with GPUs. MJX has a slow performance with complex scenes with many contacts. To improve the simulation performance, we made the following modifications:"}, {"title": "B Multitask Benchmarking Details", "content": "A single multi-task policy capable of playing various songs is highly desirable. However, playing different music pieces on the piano results in diverse behaviors, creating a complex action distribution, particularly for dexterous robot hands with a large number of degrees of freedom (DoFs). This section introduces the baseline methods we have compared and the hyperparameters we have used. We also talk about the details of our multitask training and evaluation.\nB.1 Baselines and hyperparameters\nB.1.1 BC\nBehavior Cloning (BC) [62] directly learns a policy by using supervised learning on observation- action pairs from expert demonstrations, which is one of the simplest methods to acquire robotic skills. Due to its straightforward approach and proven efficacy, BC is popular across multiple fields. The method employs a Multi-Layer Perceptron (MLP) as the policy network. Given expert trajectories, the policy network learns to replicate expert behavior by minimizing the Mean Squared Error (MSE) between predicted and actual expert actions. Despite its advantages, BC tends to perform poorly in generalizing to unseen states from the expert demonstrations. In our study, we evaluated three MLP models with varying hidden dimensions\u2014256, 1024, and 4096. The first two models feature three layers, while the model with 4096 hidden dimensions is designed with six layers."}, {"title": "B.1.2 IBC", "content": "Implicit Behavioral Cloning (IBC) [63] adopts a novel angle on behavior cloning by reformulating supervised imitation learning as a conditional energy-based modeling problem. It trains an implicit policy represented by an energy function that is conditioned on both the action and observation, utilizing the InfoNCE loss [66]. This method demonstrates improved generalization over traditional BC. However, it encounters typical difficulties associated with training energy-based models, and the need for intensive action sampling and optimization at inference time, which may not scale well to high-dimensional action spaces."}, {"title": "B.1.3 BC-RNN", "content": "BC-RNN [64] is a variant of BC that incorporates a Recurrent Neural Network as the policy network to capture a sequence of past observations. It is the best-performing baseline in the Robomimic paper [64]."}, {"title": "B.1.4 Diffusion Policy", "content": "Diffusion models have achieved many state-of-the-art results across image, video, and 3D content generation [67, 68, 69, 70, 71]. In the context of robotics, diffusion models have been used as policy networks for imitation learning in both manipulation [5, 72, 73, 74] and locomotion tasks [75], showing remarkable performance across various robotic tasks. Diffusion Policy [5] proposed to learn an imitation learning policy with a conditional diffusion model. It models the action distribution by inverting a process that gradually adds noise to a sampled action sequence, conditioning on a state and a sampled noise vector. We used a CNN-based Diffusion Policy with DDIM [65] as the sampler to"}, {"title": "B.2 Training and evaluation", "content": "We train the policies with 3 different sizes of expert data: 50, 150, and 300 songs, respectively. Subsequently, we assess the trained policies using three distinct categories of musical pieces. The first category, in-distribution songs, includes pieces that are part of the training datasets. Evaluating with in-distribution songs tests the multitasking abilities of the policies and checks if a policy can accurately recall the songs on which it was trained. The second group of songs for evaluation are easy out-of-distribution (OOD) songs: those music pieces do not overlap with the training songs but they are easy to play. They only contain slow motions and short horizons. The third group of evaluation songs are hard out-of-distribution songs: those are difficult music pieces that do not overlap with the training songs. They contain more diverse motions and longer horizons. This out-of-distribution evaluation measures the zero-shot generalization capabilities of the policies. Analogous to an experienced human pianist who can play new pieces at first sight, we aim to determine if it is feasible to develop a generalist agent capable of playing the piano under various conditions.\nAdditionally, our framework is designed with flexibility in mind, allowing users to select songs not included in our dataset for either training data collection or evaluation. Furthermore, users have the option to assess their policies on specific segments of a song rather than the entire piece."}]}