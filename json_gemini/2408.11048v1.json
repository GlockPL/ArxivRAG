{"title": "RP1M: A Large-Scale Motion Dataset for Piano Playing with Bi-Manual Dexterous Robot Hands", "authors": ["Yi Zhao", "Le Chen", "Jan Schneider", "Quankai Gao", "Juho Kannala", "Bernhard Sch\u00f6lkopf", "Joni Pajarinen", "Dieter B\u00fcchler"], "abstract": "It has been a long-standing research goal to endow robot hands with human-level dexterity. Bi-manual robot piano playing constitutes a task that combines challenges from dynamic tasks, such as generating fast while precise motions, with slower but contact-rich manipulation problems. Although reinforcement learning based approaches have shown promising results in single-task performance, these methods struggle in a multi-song setting. Our work aims to close this gap and, thereby, enable imitation learning approaches for robot piano playing at scale. To this end, we introduce the Robot Piano 1 Million (RP1M) dataset, containing bi-manual robot piano playing motion data of more than one million trajectories. We formulate finger placements as an optimal transport problem, thus, enabling automatic annotation of vast amounts of unlabeled songs. Benchmarking existing imitation learning approaches shows that such approaches reach state-of-the-art robot piano playing performance by leveraging RP1M.", "sections": [{"title": "Introduction", "content": "Empowering robots with human-level dexterity is notoriously challenging. Current robotics research on hand and arm motions focuses on manipulation and dynamic athletic tasks. Manipulation, such as grasping or reorienting [1], requires continuous application of acceptable forces at moderate speeds to various objects with distinct shapes and weight distributions. Environmental changes, like humidity or temperature, alter the already complex contact dynamics, which adds to the complexity of manipulation tasks. Dynamic tasks, like juggling [2] and table tennis [3] involve making and breaking contact, demanding high precision and tolerating less inaccuracy due to rarer contacts. High speeds in these tasks necessitate greater accelerations and introduce a precision-speed tradeoff.\nRobot piano playing combines various aspects of dynamic and manipulation tasks: the agent is required to coordinate multiple fingers to precisely press keys for arbitrary songs, which is a high-dimensional and rich control task. At the same time, the finger motions have to be highly dynamic, especially for songs with fast rhythms. Well-practiced pianists can play arbitrary songs, but this level of generalization is extremely challenging for robots. In this work, we build the foundation to develop methods capable of achieving human-level bi-manual dexterity at the intersection of manipulation and dynamic tasks, while reaching such generalization capabilities in multi-task environments.\nWhile reinforcement learning (RL) is a promising direction, traditional RL approaches often struggle to achieve excellent performance in multi-task settings [4]. The advent of scalable imitation learning techniques [5] enables representing complex and multi-modal distributions. Such large models are most effective when trained on massive datasets that combine the state evolution with the"}, {"title": "Related Work", "content": "Dexterous Robot Hands The research of dexterous robot hands aims to replicate the dexterity of human hands with robots. Many previous works [10, 11, 12, 13, 14, 15, 16, 17] use planning to compute a trajectory followed by a controller, thus require an accurate model of the robot hand. Closed-loop approaches have been developed by incorporating sensor feedback [18]. These methods also require an accurate model of the robot hand, which can be difficult to obtain in practice, especially considering the large number of active contacts between the hand and objects.\nDue to the difficulty of actually modeling the dynamics of the dexterous robot hand, recent methods resort to learning-based approaches, especially RL, which has achieved huge success in both"}, {"title": "Background", "content": "Task Setup The simulated piano-playing environment is built upon RoboPianist [4]. It includes a robot piano-playing setup, an RL-based agent for playing piano with simulated robot hands, and a multi-task learner. To avoid confusion, we refer to these components as RoboPianist, RoboPianist-RL,"}, {"title": "Large-Scale Motion Dataset Collection", "content": "In this section, we describe our RP1M dataset in detail. We first introduce how to train a specialist piano-playing agent without human fingering labels. Removing the requirement of human fingering labels allows the agent to play any sheet music available on the Internet (under copyright license). We then analyze the performance of our specialist RL agent as well as the learned fingering. Lastly, we introduce our collected large-scale motion dataset, RP1M, which includes ~1M expert trajectories for robot piano playing, covering ~2k pieces of music."}, {"title": "Piano Playing without Human Fingering Labels", "content": "To mitigate the hard exploration problem posed by the sparse rewards, RoboPianist-RL adds dense reward signals by using human fingering labels. Fingering informs the agent of the \"ground-truth\" fingertip positions, and the agent minimizes the Euclidean distance between the current fingertip positions and the \"ground-truth\u201d positions. We now discuss our OT-based method to lift the requirement of human fingering.\nAlthough fingering is highly personalized, generally speaking, it helps pianists to press keys timely and efficiently. Motivated by this, apart from maximizing the key pressing rewards, we also aim to minimize the moving distances of fingers. Specifically, at time step $t$, for the $i$-th key $k^i$ to press, we use the $j$-th finger $f^j$ to press this key such that the overall moving cost is minimized. We define the"}, {"title": "Analysis of Specialist RL Agents", "content": "The performance of the specialist RL agents decides the quality of our dataset. In this section, we investigate the performance of our specialist RL agents. We are interested in i) how the proposed OT-based finger placement helps learning, ii) how the fingering discovered by the agent itself compares to human fingering labels, and iii) how our method transfers to other embodiments.\nResults In Fig. 2, we compare our method with RoboPianist-RL both with and without human fingering. We use the same DroQ algorithm with the same hyperparameters for all experiments. RoboPianist-RL includes human fingering in its inputs, and the fingering information is also used in the reward function to force the agent to follow this fingering. Our method, marked as OT, removes the fingering from the observation space and uses OT-based finger placement to guide the agent"}, {"title": "RP1M Dataset", "content": "To facilitate the research on dexterous robot hands, we collect and release a large-scale motion dataset for piano playing. Our dataset includes ~1M expert trajectories covering ~2k musical pieces. For each musical piece, we train an individual DroQ agent with the method introduced in Section 4.1 for 8 million environment steps and collect 500 expert trajectories with the trained agent. We chunk each sheet music every 550 time steps, corresponding to 27.5 seconds, so that each run has the same"}, {"title": "Benchmarking Results", "content": "The analysis in the previous section highlighted the diversity of highly dynamic piano-playing motions in the RP1M dataset. In this section, we assess the multi-task imitation learning performance of several widely used methods on our benchmark. To be specific, the objective is to train a single multi-task policy capable of playing various music pieces on the piano. We train the policy on a portion of the RP1M dataset and evaluate its in-distribution performance (F1 scores on songs included in the training data) and its generalization ability (F1 scores on songs not present in the training data).\nBaselines We evaluated Behavior Cloning (BC) [62], Implicit Behavioral Cloning (IBC) [63], BC with a Recurrent Neural Network policy (BC-RNN) [64], and Diffusion Policy [5]. BC directly learns a policy by using supervised learning on observation-action pairs from expert demonstrations. IBC learns an implicit policy as an energy-based model conditioned on observation and action. BC-RNN uses an RNN as the policy network to encode a history of observations. Diffusion Policy learns to model the action distribution by inverting a process that gradually adds noise to a sampled action sequence. We used a CNN-based Diffusion Policy with DDIM [65] as the sampler. We use the same code and hyperparameters as Chi et al. [5].\nExperiment Setup We first train the policies with 3 different sizes of expert data: 50, 150, and 300 songs. We then evaluate the trained policies on 3 different groups of music pieces. (1) In-distribution songs: music pieces that overlap with the training sets. It shows the multitasking performance of the trained policies. (2) Easy out-of-distribution (OOD) songs: simple music pieces that do not overlap with the training songs. Those pieces are easy to play, with only slow motions and short horizons."}, {"title": "Limitations & Conclusion", "content": "Limitations Our paper has limitations in several aspects. Firstly, although our method lifts the requirement of human-annotated fingering, enabling RL training on diverse songs, our method still fails to achieve strong performance on challenging songs due to fast rhythms and mechanical limitations of the robot hands. This could be solved by proposing a better RL method and improving the hardware design of the robot hands. Secondly, our dataset only includes proprioceptive observations. However, humans play piano with multi-modal inputs, including vision, tactile sensing, and auditory information. Enabling the agent to play the piano from such rich input sources is an intriguing direction. Lastly, although we demonstrate better zero-shot generalization performance than RoboPianist-MT [4], there is still a gap between our best multi-task agent and RL specialists, which requires future investigation.\nConclusion In this paper, we propose a large-scale motion dataset named RP1M for piano playing with bi-manual dexterous robot hands. RP1M includes 1 million expert trajectories for playing 2k musical pieces. To collect such a diverse dataset for piano playing, we lift the need for human-annotated fingering in the previous method by introducing a novel automatic fingering annotation approach based on optimal transport. On single songs, our method matches the baselines with human-annotated fingering and can be adopted across different embodiments. Furthermore, we benchmark various imitation learning approaches for multi-song playing. We report new state-of-the-art results in motion synthesis for novel music pieces and identify the gap to achieving human-level piano-playing ability. We believe the RP1M dataset, with its scale and quality, forms a solid step towards empowering robots with human-level dexterity."}, {"title": "Appendix", "content": "A RP1M Dataset Collection Details"}, {"title": "Reward formulation", "content": "In Eq. (3), we give the overall reward function used in our paper. We now give details of each term. rPress indicates whether the active keys are correctly pressed and inactive keys are not pressed. We use the same implementation as [4], given as: $r_{Press} = 0.5 \\cdot (g(||k - 1||_2)) + 0.5 \\cdot (1 - \\frac{1}{K} \\sum_{i=1}^{K}1_{fp})$. K is the number of active keys, ki is the normalized key states with range [0, 1], where 0 means the i-th key is not pressed and 1 means the key is pressed. g is tolerance from Tassa et al. [57], which is similar to the one used in Equation (2). $1_{fp}$ indicates whether the inactive keys are pressed, which encourages the agent to avoid pressing keys that should not be pressed. $r_{\u015fustain}$ encourages the agent to press the pseudo sustain pedal at the right time, given as $r_{sustain} = g(s_t - s_{target})$. $s_t$ and $s_{target}$ are the state of current and target sustain pedal respectively. $rt$ penalizes the agent from collision, defined as $r_{Collision} = 1 - 1_{collision}$, where $1_{collision}$ is 1 if collision happens and 0 otherwise. prioritizes energy-saving behavior. It is defined as $r_{Energy} = \\frac{|\\tau_{joints}|}{|\\tau_{joints}| + |v_{joints}|}$. $\\tau_{joints}$ and $v_{joints}$ are joint torques and joint velocities respectively."}, {"title": "Training details", "content": "Observation Space Our 1144-dimensional observation space includes the proprioceptive state of dexterous robot hands and the piano as well as L-step goal states obtained from the MIDI file. In our case, we include the current goal and 10-step future goals in the observation space (L=11). At each time step, an 89-dimensional binary vector is used to represent the goal, where 88 dimensions are for key states and the last dimension is for the sustain pedal. The dimension of each component in the observation space is given in Table 3."}, {"title": "Computational resources", "content": "We train our RL agents on the cluster equipped with AMD MI250X GPUs, 64 cores AMD EPYC \"Trento\" CPUs, and 64 GBs DDR4 memory. Each agent takes 21 hours to train. The overall data collection cost is roughly 21 hours * 2089 agents = 43,869 GPU hours."}, {"title": "MuJoCo XLA Implementation", "content": "To speed up training, we re-implement the RoboPianist environment with MuJoCo XLA (MJX), which supports simulation in parallel with GPUs. MJX has a slow performance with complex scenes with many contacts. To improve the simulation performance, we made the following modifications:"}]}