{"title": "Gradient-free Post-hoc Explainability Using Distillation Aided Learnable Approach", "authors": ["Debarpan Bhattacharya", "Amir H. Poorjam", "Deepak Mittal", "Sriram Ganapathy"], "abstract": "The recent advancements in artificial intelligence (AI), with the release of several large models having only query access, make a strong case for explainability of deep models in a post-hoc gradient free manner. In this paper, we propose a framework, named distillation aided explainability (DAX), that attempts to generate a saliency-based explanation in a model agnostic gradient free application. The DAX approach poses the problem of explanation in a learnable setting with a mask generation network and a distillation network. The mask generation network learns to generate the multiplier mask that finds the salient regions of the input, while the student distillation network aims to approximate the local behavior of the black-box model. We propose a joint optimization of the two networks in the DAX framework using the locally perturbed input samples, with the targets derived from input-output access to the black-box model. We extensively evaluate DAX across different modalities (image and audio), in a classification setting, using a diverse set of evaluations (intersection over union with ground truth, deletion based and subjective human evaluation based measures) and benchmark it with respect to 9 different methods. In these evaluations, the DAX significantly outperforms the existing approaches on all modalities and evaluation metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's machine learning, deep neural models are the mostly widely used manifestation of mapping functions from the feature space to the target labels. Current deep models with state-of-the-art capabilities consist of specialized units for pro- cessing different kinds of modalities, for example, fully con- nected neural networks [1], long short-term memory units [2], attention units [3], transformers [4], auto encoders [5] etc. Diverse combinations of such architectures at massive scale have manifested in the development of foundation models, with astounding performance on several tasks, for example, Vision Transformer (ViT) [6] in computer vision, Whisper [7] in speech processing and GPT-4 [8] in natural language pro- cessing. In several domains, the sequence-to-sequence models based on transformer architecture [4] have achieved state-of-art results and in some cases elicited super-human performances.\nWhile the models may have illustrated impressive perfor- mance, the deep complex architectures are inherently non- explainable. While knowledge distillation can provide simpler modeling solutions [9], [10], the questions about data contam- ination and leakage [11], as well as the over-estimation of the model performance [12], continue to be pertinent in this age of large language models (LLMs) [13]. The research direction of explainable artificial intelligence (XAI) attempts to provide solutions for this scenario.\nExplaianability of deep models is a crucial requirement if they are deployed in safety-critical scenarios [14]\u2013[17]. For example, in domains like autonomous driving, finance, and healthcare, lack of explainability in the complex neural models may turn the practitioners away from exploiting the impeccable results that these models may offer. Further, \u03a7\u0391\u0399 approaches can also help in identifying spurious correla- tions [18], [19] and biases [20], [21] present in the datasets and models."}, {"title": "A. Types of XAI methods", "content": "The explainability problem first assumes a framework where the model (referred to as the black-box in our discussion) configuration, architecture information, and downstream tasks, are given. The setting explored in this paper is one in which an explanation is sought for an already trained black-box model with no further provision to modify/retrain it, called the post- hoc explainability setting. Based on different criteria, post-hoc methods can be further categorised based on,\nModel architecture: The XAI approaches can be, (a) Model- specific: when the XAI method works only for a specific black-box architecture (for example, XAI for CNN-based models [22], [23]); or (b) Model-agnostic: when the X\u0391\u0399 method is agnostic to the architecture of the black-box [24]\u2013 [26].\nAccess to black-box: Depending on the access requirements of the XAI methods, they can be classified as, (a) gradient- based approaches: XAI methods using gradient access to black-box like [23], [25], [27]; or (b) gradient-free ap- proaches: \u03a7\u0391\u0399 methods that only need input-output access without gradient access like, perturbation-based methods [26], [28], [29].\nLocality of explanations: As the black-box is highly nonlin- ear, the explanation can be, (a) local - the methods that attempt to generate explanations for each test example like [29], [30],"}, {"title": "B. Contributions from the proposed framework", "content": "In computer vision and audio based models, the input dimensionality is high, O(10\u2074) or more. (for example, high- resolution images of size 224 X 224 result in input dimension of 4e4). To avoid dealing with such high dimensional locality, the prior works compute image segments and consider each of them as a dimension rather than the raw pixels. With this approach, the input dimensionality is reduced to O(10\u00b9).\nHowever, explanations generated this way can be unreliable, erroneous and imprecise as discussed in detail in Section II-A. We propose a novel learnable, distillation-based approach to post-hoc gradient-free explainability that uses two non-linear networks, a) mask-generation network and, b) student network to compute explanations. The mask-generation network selects salient parts of the local input. With the mask-multiplied input, the student network attempts to locally approximate the black-box predictions. Using a joint training framework with perturbation samples, the mask-generation network finds the salient region of the input image as the explanation.\nThe proposed framework, termed as distillation aided ex- planations (DAX), is shown to compute explanations that are better than existing approaches in various aspects, although it operates directly on the O(10\u2074) dimensional input space of the black-box, while adding a minimal computational overhead.\nThis paper extends our prior work using student-teacher distillation [34]. The following are the major contributions in this paper,"}, {"title": "II. RELATED PRIOR WORK", "content": "An illustration of the explainability provided by various methods for an image example is shown in Figure 1. There are several prior works using gradient access based explainability, namely,"}, {"title": "III. METHODOLOGY", "content": "We formalize the problem in a supervised classification setting. Let, \\(f_{BB}(\\cdot; \\theta_{BB})\\) denote a trained deep neural network based classifier (black-box), which is to be explained. The training dataset is denoted as \\({x_i \\in X \\sim D_x, z_i \\in Z \\sim D_z\\}_{i=1}^N\\), with \\(x_i \\in \\mathbb{R}^D\\) as inputs, and \\(z_i \\in Z\\) denoting the one-hot labels. The pre-trained model parameters are denoted as \\(\\theta_{BB}\\), with C classes. Thus, the model \\(f_{BB}(\\cdot,\\theta_{BB})\\) has C output nodes with softmax non-linearity,\n\\(|y|=|f_{BB}(x;\\theta_{BB})|= C; \\sum_{i=1}^{C}[f_{BB}(x;\\theta_{BB})]_i = 1, \\forall x \\) (2)\nThe problem of explaining the black-box is defined for a given input \\(x_p\\). For the target-class T, the goal is to explain the prediction \\([y_p]_T\\), given \\(x_p\\), where \\(y_p = f_{BB}(x_p;\\theta_{BB})\\)."}, {"title": "B. Functional view of \u201cexplanation\u201d", "content": "We pose explanation for the black-box as a function,\n\\(E[x_p,T] = E(x_p, y_p|f_{BB}(\\cdot;\\theta_{BB}),T) \\in \\mathbb{R}^D \\) (3)\nwhere \\(E[x_p,T]\\) denotes an explanation computed on \\(x_p\\), in the form of weights assigned to each dimension of \\(x_p\\) for the prediction of the target-class T, denoted as \\([y_p]_T\\). In particular, \\(E[x_p,T] \\in \\mathbb{R}^D\\) denotes a saliency map (also referred to as explanation mask), where \\([E[x_p,T]]_k\\), denotes the importance of k-th dimension of \\(x_p\\) in generating the prediction, \\([y_p]_T\\).\n1) Neighbourhood sampling: To generate the explanation for a local input, \\(x_p\\), our approach relies on neighbourhood samples at close proximity of \\(x_p\\), obtained by perturbing \\(x_p\\).\nAs pointed out by Fong et. al. [42], the perturbations must be \"meaningful\" (having contextual meaning). For images, segmentation [43] is an algorithm that captures contextual meaning. Hence, we perturb image segments to obtain neigh- bourhood samples - similar to the perturbation strategy used by LIME [26]. Let, the local input \\(x_p\\) contain S segments \\({U_1, U_2, ..., U_S}\\) obtained using an image segmentation algo- rithm. The segments are non-intersecting and exhaustive.\n\\(\\bigcup_{i=1}^{S} U_i = x_p \\forall i; U_i \\cap U_j = \\emptyset, i \\neq j; \\bigcup_{i=1} U_i = x_p \\) (4)\nFor generating Q neighbourhood samples, \\(x_p\\) is randomly perturbed Q times by randomly masking off a subset of the segments \\({u_i\\}_{i=1}^S\\). Let the neighbourhood samples be denoted as \\({x^i\\}_{i=1}^Q\\), with the i-th sample containing only a subset of the segments. Let the mask of indices be denoted by \\(I_i \\in \\mathbb{Z}^{S X 1}\\), where \\([I_i]_j = 1\\{\\text{j-th segment is masked-off}\\}\\). Thus, the i-th neighbourhood sample of \\(x_p\\) is given by,\n\\(x_p^i = x_p - \\bigcup_{\\{j|[I_i]_j=0\\}}^{S} U_j \\) (5)"}, {"title": "C. DAX framework", "content": "1) \"Explanation\" as the optimal multiplier: We look at the notion of explanation as an optimal multiplier of the input that causes minimum drop in the black-box prediction of the target- class. The true explanation, \\(E^*[x_p, T]\\), assigns higher weights to the dimensions of \\(x_p\\) that are most important for the model, \\(f_{BB}\\), to make the target-class prediction \\([y_p]_T\\). If \\(M_T \\in \\mathbb{R}^D\\) is any input multiplier and \\(0 \\leq [M_T]_k \\leq 1 \\forall k\\), then we obtain the optimal multiplier, \\(M_T^*\\), with the optimization,\n\\(E^* [x_p,T] = M_T^* = \\arg \\min_{M} ([y_p]_T - f_{BB}(M \\odot x_p;\\theta_{BB}))^2\\) (6)\nEquation (6) poses the problem of finding the explanation as a learnable approach, where explanations can be found by minimizing the mean squared error (MSE) between black-box output for the original input and the black-box response for the \u201cperfectly\u201d masked input.\n2) Distillation to avoid gradient flow restriction: Using the perturbation samples \\({x^i\\}_{i=1}^Q\\) and corresponding black- box responses \\({\\[y^i]_T\\}_{i=1}^Q = {\\[f_{BB}(x^i;\\theta_{BB})]_T\\}_{i=1}^Q\\), one can optimize the explanation motivated by Equation (6).\nHowever, in a gradient-free setting, gradient flow through black-box model is not available. In order to perform the opti-mization described above without gradient-access, we propose a student distillation model, which locally approximates the black-box model. In this way, the distilled network can be used for learning the explanation, as the gradients can be computed.\nThus, we have two optimization problems at hand - i) Find- ing a local approximation of the black-box using distillation and, ii) Finding a multiplier based explanation (Equation 6). We perform a joint optimization of these two sub-problems, as described in the following sub-sections.\n3) Local distillation of black-box: The black-box is locally distilled by approximating the black-box behavior at the vicin- ity of local input, \\(x_p\\). The neighbourhood samples \\({x^i\\}_{i=1}^Q\\) and black-box responses \\({\\[y^i]_T\\}_{i=1}^Q\\) are used to train a small learnable network, \\(f_S(\\cdot;\\theta_S)\\), where \\(\\theta_S\\) denotes the model parameters of the student network. We minimize the MSE loss to learn the parameters of the student network, \\(f_S(x^i; \\theta_S)\\), i.e.,\n\\(\\theta_S = \\arg \\min_{\\theta_S} \\sum_{i=1}^{Q} ([y^i]_T - [f_S(x^i;\\theta_S)]_T)^2\\) (7)"}, {"title": "4) Learnable explanation:", "content": "Interestingly, as seen in Equa- tions (6) and (7), finding the explanation and local distillation are posed as the minimization of objective functions based on the MSE loss. We propose to combine the two optimization problems with two learnable networks, a mask generation network, \\(f_M(\\cdot; \\theta_M)\\) and a student network, \\(f_S(\\cdot;\\theta_S)\\). Now, minimization of the following loss\n\\(\\mathcal{L}_{MSE}(x_p^i, T; \\theta_M,\\theta_S) = [[y^i]_T-f_S(x_p^i \\odot f_M(x_p^i;\\theta_M)) ; \\theta_S)]^2\\) (8)\nallows the estimation of the parameters \\({\\theta_M,\\theta_S}\\). Here, \\(f_M (x_p; \\theta_M)\\) is the multiplier mask generated by the mask gen- eration network, while the product, \\(x_p \\odot f_M(x_p; \\theta_M)\\), is the salient explanation output of the proposed DAX framework. Let \\(\\tilde{y_p}(x^i) = f_S(x^i \\odot f_M(x^i;\\theta_M);\\theta_S)\\) denote the output of the DAX model for the perturbed input \\(x^i\\).\nNote that, optimizing based on Equation (8) can be ill-posed, as it can lead to the trivial identity solution \\(f_M(x_p;\\theta_M) = I\\). It can be avoided by using a regularized loss. We propose an L\u2081 loss, as discussed in Equation (10) given below.\n\\({\\theta_M,\\theta_S}\\) = \\arg \\min_{\\theta_M,\\theta_S} \\mathcal{L}_{TOTAL} (x_p,T;\\theta_M,\\theta_S)\\) (9)\n\\(\\mathcal{L}_{TOTAL}(x_p, T;\\theta_M, \\theta_S) = \\sum_{i=1}^{Q} \\gamma_i \\mathcal{L}_{MSE}(x_p^i, T; \\theta_M,\\theta_S)\\) + \\lambda_1 \\mathcal{L}_{SP}(x^i;\\theta_M)+\\lambda_2 \\mathcal{L}_{KL}(x^i;\\theta_M,\\theta_S) \\) (10)\nwhere,\n\\(\\mathcal{L}_{SP}(x^i;\\theta_M) = \\sum_{i=1}^{Q}||f_M(x^i;\\theta_M)||_{L_1}\\) (11)\n\\(\\mathcal{L}_{KL}(x^i;\\theta_M, \\theta_S) = KL (\\{\\[y^i]_T\\}_{i=1}^Q || {\\tilde{y_p}(x^i)}\\}_1^Q )\\)\nThe first term of the total loss is the MSE loss (defined in Equation (8)), the second term is the \\(L_1\\) penalty loss on the mask explanations, and the third term is the KL- divergence between distribution (q) of black-box model scores (\\({\\[y^i]_T\\}_{i=1}^Q\\)) and the distillation output scores (\\({\\tilde{y_p}(x^i)}\\}_{i=1}^Q\\)).\nThe KL-divergence loss helps in bringing the distribu- tions of DAX scores closer to the black-box scores, which regularizes the MSE loss. The \\(\\gamma_i\\) weighs the \u201ccloseness\u201d of the neighbourhood samples to the actual input \\(x_p\\), i.e., . Further,  is defined in the descrip- tion below Equation (8), and \\(\\lambda_1, \\lambda_2\\) are hyper-parameters that are set based on a held-out validation set."}, {"title": "IV. RESULTS", "content": "We present a diverse set of input images and the compute saliency explanations to enable exten- sive qualitative analysis. We compare the DAX explanations with those generated by 11 other popular XAI approaches, be- longing to 3 different categories, to allow a strong benchmark comparison. For this analysis, the inputs that were correctly predicted by the black-box were considered.\nTask: The object detection task is considered for this analysis. Given an input image, the task is to predict the class of the object present in the image. Images are randomly selected from the evaluation set of ImageNet dataset [44].\nBlack-box: To analyze the explainability of the state-of-the-art models, a vision transformer [45] of patch size 16 (ViT-b16) is used as the black-box model, which was pre-trained on the ImageNet dataset. Figure 6 shows the explanations generated by our approach and 11 other popular XAI approaches. As seen here, the DAX model is able to generate consistent explanations for most of the classes shown here.\nMost of the prior explainability works only report the qualitative analysis on a selected subset of images. While this may provide visual insight, the statistical justification of the explanation quality is lacking. In this paper, we also present statistical performance of our approach on a large dataset. For extensive benchmark comparisons, we also evaluate 11 other existing methods in the same setting.\nTask: We use the Pascal visual object classification (VOC) dataset [46], comprising of images with 20 object classes. It has two sub-parts classification dataset (larger set having 5717 images) and segmentation dataset (smaller set having 1464 images). The images in the segmentation dataset also have the human annotated ground truth segmentation masks corresponding to the objects. We use the classification dataset for fine-tuning the models, while the segmentation dataset is used in our evaluations.\nBlack-box: We perform the statistical analysis separately on two different black-box models to analyze the performance consistency, a) The ResNet-101 [47] and, b) the ViT-b16 [45]."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "All the analysis reported thus far only considered sam- ples that were correctly predicted (positive) by the black- box model. In other words, the explanation was sought for positively predicted samples. For several applications, the explanation for the samples which were inaccurately predicted (negative) by the black-box model are also equally important. For the Pascal VoC task, we apply the XAI methods on the 141 samples which are incorrectly predicted by the ViT model. In the IoU evaluation, we compare the XAI output with the ground truth segmentation of the target class. Thus, a lower value of IoU indicates that the black-box model prediction was inaccurate as the model was focussing on the less salient regions of the image. The mean IoU results for explaining these negative samples is reported in Table IX. As seen here, all the models see a degradation in the mean IoU values compared to the ones seen for positive samples (Table II).\nWe expect the XAI methods to show a large deviation in IoU values for these negative samples from those which"}]}