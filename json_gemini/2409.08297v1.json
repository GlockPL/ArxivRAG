{"title": "Comparative Study of Long Short-Term Memory (LSTM) and Quantum Long Short-Term Memory (QLSTM): Prediction of Stock Market Movement", "authors": ["Tariq Mahmood", "Ibtasam Ahmad", "Malik Muhammad Zeeshan Ansar", "Jumanah Ahmed Darwish", "Rehan Ahmad Khan Sherwani"], "abstract": "In recent years, financial analysts have been trying to develop models to predict the movement of a stock price index. The task becomes challenging in vague economic, social, and political situations like in Pakistan. In this study, we employed efficient models of machine learning such as long short-term memory (LSTM) and quantum long short-term memory (QLSTM) to predict the Karachi Stock Exchange (KSE) 100 index by taking monthly data of twenty-six economic, social, political, and administrative indicators from February 2004 to December 2020. The comparative results of LSTM and QLSTM predicted values of the KSE 100 index with the actual values suggested QLSTM a potential technique to predict stock market trends.", "sections": [{"title": "1. INTRODUCTION", "content": "For developing nations, sustaining macroeconomic stability is a primary challenge [1]. The stock market's pivotal role in redistributing financial resources among diverse economic entities is widely recognized. Consequently, progress within the stock market echoes advancements in a country's economic growth trajectory [2, 3]. This interrelation is evident as the stock market's movements reflect a nation's economic health\u2500positive stock market performance signifies growth, while negative trends signal otherwise. Hence, it is important to identify the economic factors that affect stock market variations since they impact the country's stock market movement. Previous studies indicate that the stock market capitalization rate, influenced by currency rates, gross domestic product, current account, interest rates, and money supply, has a major impact [4, 5].\nHashmi and Chang examined E7 stock indices to show the effect of macroeconomic variables across different states of stock markets\u2014bullish, bearish, and normal [6]. The study's outcomes revealed noteworthy trends. In the long term, trade balance, foreign direct investment, and industrial production index emerged as significant influencers of emerging stock indices. Moreover, employing the QARDL model, the research demonstrated that the short-term effects of factors such as consumer price index, foreign direct investment, interest rate, and exchange rate exhibit variations in different market states. Notably, the long-term effect shows variability for all macroeconomic variables except for the industrial production index [6].\nMoreover, stock prices interlinked to the equilibrium position in the long run by 18.6% adjustment speed through the channel of FDI, GDP, and provision of domestic loans to the private sector. The study's results also showed that while financial development has a negatively influenced stock prices in the short term, FDI has a long-term favorable impact [7]. Another noteworthy economic indicator is the exchange rate. Consequently, Chang et al. revisited the complex associations among oil prices, real exchange rates, and stock market prices within China [6, 8]. Their findings illuminate varying connections between oil and stock prices and between stock prices and exchange rates, dependent on diverse quantile combinations [6]. However, exchange rate fluctuations in China, India, and the USA illustrate the minimal influence on the daily closing price of stock indices such as SSE, Nifty50, and DJI, respectively. These variations considerably impact the number of shares traded on each of the three stock exchanges, as revealed by Krishnan and [9].\nAsymmetry exists in changes to the money supply, industrial production, and real exchange rate (RER) on stock returns, and the asymmetries are more pronounced after the 2002 subsample than for the entire sample period. The empirical findings imply that easy monetary policy enhances stock returns more than restrictive monetary policy [10]. Similarly, understanding how foreign reserves relate to the stock market is crucial because, recently, building up international reserves has been the preferred strategy taken by emerging nations to ensure financial stability [11]. According to [12], the stock market is positively impacted by economic sentiment indices and market capitalization. In comparison, exports and industrial production have encouraging relationships with stock prices. Inflation has demonstrated a negative link with stock prices [13].\nIn addition to macroeconomic indicators, the administrative quality of a nation plays a pivotal part in shaping its performance in the stock market [14, 15]. Aligned with regulatory and legal institutions and Finance-Law theories, they substantially influence the stock market by shaping financial and economic activities [16]. In their attempt to comprehend how corruption control and government integrity shake the long- term growth of Pakistan's stock market, Islam et al. discovered that these factors favorably influence the pace of the country's equity market [17]. Their investigation indicates that government integrity and effective corruption control contribute positively to the progression of Pakistan's equity market. Notably, aspects such as instances of bankruptcy, contractual misconduct, inflexible or stringent securities laws, inadequacies of the legal system, levels of expropriation, uncertainty in property safety laws, and the inadequacy of prosecution agencies all manifest as methods of exploitation, collectively impeding the seamless operation of the equity market [18].\nThe pragmatic findings provide the theoretical claim that indeterminate socio-political circumstances harm economic evolution in the Greek paradigm and show a robust damaging relationship between indeterminate socio- political states and the overall index of the Athens Stock Exchange (ASE). Since political unpredictability has a detrimental impact on market values, an unstable political system will eventually cause stock prices to fall [13].\nIn the work conducted by Asongu, a robust and noteworthy positive connection is defined between measures of performance of the stock market and the eminence of government institutions [19]. To consider the dynamics of capitalization, value traded, turnover, and the number of listed companies on the stock market, researchers looked at the dynamics of government effectiveness in terms of corruption control, government efficiency, political stability or lack of violence, voice and accountability, regulation quality, and rule of law. These dynamics were instrumented using data on income levels, levels of press freedom, religious dominance, and legislative precedents. The research revealed that countries with more advanced governmental institutions would favor stock markets with bigger market capitalization, higher share prices, better turnover ratios, and more listed enterprises [19].\nA larger market capitalization is used to calculate the value of stock sets. A few technical elements may be employed to obtain statistical data from the value of stock prices [20]. Stock indexes frequently evaluate each country's economic status based on the prices of stocks with significant market investment [21]. Because of the uncertain nature of stock price fluctuation, the movement of stock values is ambiguous. Additionally, governments typically have trouble determining the status of the market. Since stock values are typically nonlinear, non-parametric, and dynamic, they frequently result in poor statistical model performance and make it difficult to predict precise values and movements [22].\nThe projection of stock group prices has long been interesting and challenging for investors due to its non-linearity, inherent dynamism, and complexity. Both economists and computer scientists are interested in stock market forecasting since it is a traditional but challenging subject. To develop an effective prediction model, linear and machine-learning approaches have been studied over the past 20 years. Deep learning techniques have recently been suggested as new directions for this subject and issue. Forecasting stock market trends is a significant work that needs close attention since, with the appropriate judgments, a good prediction of stock prices may lead to the possibility of acquiring attractive rewards. Because of the difficult problem of stock market forecasting and the data's chaotic, noisy, and non- stationary nature, the forecast causes investors to pause and consider investing in future benefits [23]. By using monthly data from 26 economic, social, political, and administrative indicators from February 2004 to December 2020, we were able to predict the Karachi Stock Exchange (KSE) 100 index using effective machine learning models like long short-term memory (LSTM) and quantum long short-term memory (QLSTM). The KSE 100 index's anticipated values from LSTM and QLSTM and their actual values imply that QLSTM may be able to forecast stock market developments.\nFurthermore, various empirical studies have scrutinized the influence of macroeconomic variables on stock prices [24, 25]. However, the existing body of literature presents inconsistent findings, primarily focusing on developed countries while lending less attention to their developing counterparts. Hence, a directed spotlight on Pakistan becomes imperative, particularly employing contemporary techniques like QLSTM to prognosticate the KSE100 index. This approach is warranted due to the divergent risk and return dynamics characterizing developing economies in contrast to their developed counterparts [26, 27]."}, {"title": "1.1. Long Short-Term Memory (LSTM)", "content": "Sepp Hochreiter and J\u00fcrgen Schmidhuber introduced Long Short-Term Memory (LSTM) in 1997 [28, 48]. It is a form of recurrent neural network (RNN) and can model complex sequential data by preserving long-term memory and selectively apprising information. It is intended to overcome the vanishing gradients that arise in conventional RNNs [29-33]. Deep learning has become a fundamental building block, particularly in tasks involving a sequence of data, such as natural language processing, time series analysis, and speech recognition [33-39]. In sequential data, LSTMs can capture and remember long-range dependencies to effectively model sequences with complex dependencies and patterns. For the storage and retrieval of information, LSTM networks consist of different memory cells [28]. Three gates-the input, forget, and output gates-control the information flow across the LSTM network. LSTMs can effectively handle both short-term and long-term dependencies in the data [49]. These gates selectively forget and update the information from the past [28]. It may involve the following steps:\n1. Data reparation:- splitting data into training, validation, and testing sets.\n2. Data encoding:- conversion of input data into a suitable format for the LSTM.\n3. Model architecture (specification of the number of layers, neurons in each layer, activation function, etc.).\n4. Model compilation (selection of loss function (e.g., mean squared error for regression or categorical cross-entropy for classification), selection of an optimizer (e.g., Adam, RMSprop) to minimize the loss function, monitoring of additional metrics like accuracy or mean absolute error)\n5. Training (feed training dataset to LSTM network, compute loss function and gradients, update model weights with the help of optimizer and backpropagation, repeat the process until the model converges)\n6. Validation (monitor its generalization ability on the validation dataset)\n7. Testing (asses its real-world performance by feeding unseen data)\n8. Post-processing (conversion of predicted probabilities into class labels for classification tasks)\n9. Deployment (make real-time predictions by integrating the trained LSTM model into the application/system)\n10. Fine-Tuning and Maintenance (fine-tune your LSTM model with new data to ensure it remains accurate and up to date)."}, {"title": "1.2. Quantum Long Short-Term Memory (QLSTM)", "content": "Quantum Long Short-Term Memory (Q-LSTM) builds on LSTM's legacy. In modern deep learning, LSTM is a cornerstone for natural language processing and time series analysis [29-39][47]. Q-LSTM, a quantum-inspired variant, leverages quantum bits (qubits) for data storage and processing. It harnesses quantum principles, like superposition and entanglement, for enhanced efficiency. Q-LSTM models long- range dependencies, surpassing classical LSTMs [40-45]. Quantum gates control information flow. In summary, Q-LSTM offers quantum- level precision, ideal for complex sequential data tasks, marking a significant advancement in deep learning. QLSTMs proficiently manage both short-term and long-term data dependencies [44]. Quantum gates within QLSTM selectively adapt information from the past, akin to traditional LSTMs [40].\nThe QLSTM workflow entails:\n1. Data Preparation: Divide data into training, validation, and test sets to facilitate model training and evaluation.\n2. Data Encoding: Transform input data into a suitable format for QLSTM, considering quantum encoding methods.\n3. Model Architecture: Give a description of the design, including the number of layers, the number of neurons in each layer, the activation mechanisms, and any quantum- inspired improvements.\n4. Model Compilation: Specify the loss function (e.g., mean squared error or quantum-specific variants), choose an optimizer (e.g., quantum-inspired optimizers), and monitor additional metrics such as quantum fidelity.\n5. Training: Feed the training dataset to the QLSTM, compute loss, update weights through quantum backpropagation, and iterate until convergence, harnessing quantum parallelism.\n6. Validation: Consider quantum validation techniques to assess the model's generalization ability on the validation dataset.\n7. Testing: Evaluate QLSTM's real-world performance using unseen data, employing quantum testing strategies.\n8. Post-processing: For classification tasks, convert quantum-aided predicted probabilities into class labels.\n9. Deployment: Seamlessly integrate the trained QLSTM model into applications/systems for real-time quantum- enhanced predictions.\n10. Fine-Tuning and Maintenance: Continuously fine-tune the QLSTM model with new quantum data to ensure its accuracy and relevance in evolving quantum computing landscapes."}, {"title": "2. Material and Methods", "content": "This study has employed monthly data spanning from February 2004 to December 2020. The analysis involved twenty-six distinct, independent variables to predict Pakistan's stock market dynamics. These variables encompassed a wide spectrum, including balance of trade, consumer financing for house building, consumer price index (representing inflation), control of corruption, crude oil prices, domestic savings, exchange rate, external debt stocks, foreign direct investment, foreign exchange reserves, GDP growth rate, gold price, government effectiveness, households final consumption expenditure, industrial production index, industry value added, labor force participation rate, money supply, personal remittances growth, political stability and absence of violence/terrorism, portfolio investment, growth rate, regulatory quality, the rule of law, three-month treasury bill rates, and wholesale price index. The dependent variable, in this case, was the closing price of the KSE 100 index, serving as a measure of the stock market's performance. The data for these variables were drawn from reputable sources, such as the World Development Indicator, the State Bank of Pakistan, and the Pakistan Bureau of Statistics. An artificial neural network was employed to interpolate data, converting monthly data into daily observations.\nWe have utilized LSTM and QLSTM to predict the KSE 100 index based on the above indicators and compared their results."}, {"title": "2.1. Long Short-Term Memory (LSTM) Algorithm", "content": "The Long Short-Term Memory (LSTM) algorithm involves matrix operations and activations. The block diagram of LSTM is shown in the following Figure 1.\nA high-level description of the LSTM algorithm in terms of mathematical operation and data flow is as:\n1. Initialize the LSTM cell state (C) and hidden state (h) with zeros or small random values.\n2. Calculate the input gate activation ($i_t$) by applying a sigmoid function to a weighted sum of the current input ($x_t$) and the previous hidden state ($h_{t\u22121}$). This gate determines which information from the current input and previous hidden state should be stored in the cell state.\n$i_t = \\sigma([h_{t\u22121}, X_t] * W_i + b_i)$\n3. Apply a sigmoid function to the weighted sum of the current input ($x_t$) and the prior hidden state ($h_{t\u22121}$) to determine the forget gate activation ($f_t$). The information from the cell state ($C_{t-1}$) that should be forgotten or maintained is decided by this gate.\n$f_t = \\sigma([h_{t\u22121},x_t] * W_f + b_f)$\n4. Apply the hyperbolic tangent (tanh) activation function to a weighted sum of the current input ($x_t$) and the prior hidden state ($h_{t-1}$) to get the candidate's cell state ($Ctilde$). New potential values for the cell state are computed in this stage.\n$Ctilde = tanh([h_{t\u22121}, X_t - X_{t\u22121}] * W_c + b_c)$\n5. The candidate cell state ($Ctilde$), the prior cell state ($C_{t-1})$), and the forget gate ($f_t$) are combined to update the cell state ($C_t)$). In this stage, the data that should remain in the cell state is decided.\n$C_t = i_t * Ctilde + f_t * C_{t-1}$\n6. Apply a sigmoid function to the weighted sum of the current input ($x_t$) and the prior hidden state ($h_{t\u22121}$) to determine the output gate activation ($o_t$). The information from the cell state that should be produced as the hidden state ($h_t$) is decided by this gate.\n$O_t = \\sigma([h_{t\u22121}, X_t] + w_o + b_o)$\n7. Apply the hyperbolic tangent activation function (tanh) to the updated cell state ($C_t$) multiplied by the output gate ($0_t$) to get the new hidden state ($h_t$). Information that will be sent to the following time step and may be used as the final output is included in this concealed state.\n$h_t = 0_t * tanh (C_t)$\n8. The hidden state ($h_t$) can be used as the output of the LSTM cell for the current time step, or it can be passed to subsequent layers in a deep LSTM network.\n9. Repeat the above steps for each time step in the sequence.\nWhere:\n*   $i_t$ is the input gate activation.\n*   $\\sigma$ is the sigmoid activation function.\n*   $w_i$ is the weight matrix for the input gate.\n*   $x_t$ is the current input.\n*   $b_i$ is the bias of the input gate.\n*   $h_{t-1}$ is the previous hidden state.\n*   $f_t$ is the forget gate activation.\n*   $w_f$ is the weight matrix for the forget gate.\n*   $b_f$ is the bias for the forget gate.\n*   $Ctilde$ is the candidate cell state.\n*   $b_c$ is the bias for the candidate cell state.\n*   $tanh$ is the hyperbolic tangent activation function.\n*   $w_c$ is the weight matrix for the candidate cell state.\n*   $C_t$ is the updated cell stat.\n*   $C_{t-1}$ is the previous cell state.\n*   $O_t$ is the output gate activation.\n*   $w_o$ is the weight matrix for the output gate.\n*   $b_o$ is the bias of the output gate.\n*   $h_t$ is the new hidden state.\nThe LSTM algorithm captures and propagates information over long sequences by allowing the network to update and forget information selectively."}, {"title": "2.2. Quantum Long Short-Term Memory (QLSTM) Algorithm", "content": "Certainly, Quantum Long Short-Term Memory (Q-LSTM) builds upon LSTM's legacy by infusing quantum computing principles into its architecture. While LSTM remains a stalwart in deep learning for sequential data tasks, Q-LSTM represents a quantum leap forward, offering enhanced computational efficiency and the capacity to masterfully capture both short-term and long-term dependencies within data, all within the realm of quantum-inspired computing.\nHere's a brief overview of each line in the simplified Quantum Long Short-Term Memory (Q-LSTM) algorithm code:\n1. Import necessary libraries:\nImports essential libraries, such as NumPy for numerical operations and Qiskit for quantum programming.\n2. Define quantum circuit parameters:\nSets the number of qubits in the quantum circuit (n_qubits') and the number of classical bits used for measurement (`n_bits').\n3. Initialize the quantum circuit:\nCreates a quantum circuit using Qiskit, specifying the number of qubits and classical bits.\n4. Define Q-LSTM quantum gates:\nDefining the quantum bit and classical bit in the co-relation for the maximum probability 1. This streamlined approach helps pave the way for the subsequent layers of our process.\nDefines quantum gates for the input gate, forget gate, candidate cell state, and output gate, each with specific quantum operations.\n$i_t = \\sigma(x_tW_{xi} + h_{t\u22121}W_{hi} + b_i)$\n$f_t = \\sigma(x_tW_{xf} + h_{t-1}W_{hf} + b_f)$\n$C_t = tanh(x_tW_{xc} + h_{t\u22121}W_{hc} + b_c)$\n$O_t = \\sigma(x_tW_{xo} + h_{t-1}W_{ho} + b_o)$\nWhere:\n*   $i_t$ is the input gate at time step t\n*   $\\sigma$ is the sigmoid activation function\n*   $W_{xi}$ and $W_{hi}$ are weight matrices\n*   $x_t$ is the input vector at time step t\n*   $h_{t-1}$ is the hidden state at time step t\u20131\n*   $b_i$ is a bias vector\n*   $f_t$ is the forget gate at time step t\n*   $W_{xf}$ and $W_{hf}$ are weight matrices\n*   $b_f$ is a bias vector\n*   $C_t$ is the candidate cell state at time step t\n*   $tanh$ is the hyperbolic tangent activation function\n*   $W_{xc}$ and $W_{hc}$ are weight matrices\n*   $b_c$ is a bias vector\n*   $O_t$ is the output gate at time step t\n*   $W_{xo}$ and $W_{ho}$ are weight matrices\n*   $b_o$ is a bias vector\n5. Quantum LSTM operation for one time step:\nCalls the functions defined earlier for the input gate, forget gate, candidate cell state, and output gate, combining these gates to represent one Q-LSTM time step (VQC).\n$i_t = \\sigma (VQC_2(V_t))$\n$f_t = \\sigma(VQC_1(V_t))$\n$C_t = tanh (VQC_3(v_t))$\n$O_t = \\sigma (VQC_4(v_t))$\n$C_t = f_t * C_{t-1} + i_t * C_t$\n$y_t = VQC_6(tanh tanh (C_t) * 0_t)$\nWhere:\n*   $v_t$ is the N-dimensional input vector $v_t$ ($x_1, x_2, \u00b7 \u00b7 \u00b7, x_N$)\n*   $VQC1$ and $VQC2$ control the flow of information between the input and output gates.\n*   $VQC3$ and $VQC4$ control the flow of information between the input and forget gates.\n*   $VQC5$ and $VQC6$ control the flow of information between the input and update gates.\nVariational quantum circuits (VQCs) are a type of hybrid quantum-classical algorithm. VQCs work by taking advantage of quantum mechanics' superposition and entanglement properties. Superposition allows VQCs to represent multiple possible solutions to a problem simultaneously. Entanglement allows VQCs to share information between different parts of the circuit. Here, VQCs are checking the probability of occurring. VQC circuits vary from problem to problem; therefore, one of the circuits is shown in the following figure.\n6. Example usage of quantum LSTM:\nGenerates random quantum input ($x_t$) and random previous quantum hidden state ($h_{prev}$) for a single time step. In practice, these values would be real data.\n$X_t = \\sum_{i=1}^{n} \\alpha_i |x_i\u27e9$\n$h_{prev} = \\sum_{i=1}^{n} \\beta_i |h_i\u27e9$\nWhere:\n*   $X_t$ represents a state or vector at time $t$.\n*   $n$ is the number of terms in the summation.\n*   $\\alpha_i$ represents the coefficients or weights associated with each $|x_i\u27e9$ term.\n*   $|x_i\u27e9$ represents a quantum state or vector associated with the i-th term.\n*   $h_{prev}$ represents a state or vector called the \"previous hidden state.\"\n*   $\\beta_i$ represents the coefficients or weights associated with each $|h_i\u27e9$ term.\n*   $h_i\u27e9$ represents a quantum state or vector associated with the i-th term.\n7. Define weight matrices and biases for gates:\nDefines quantum circuits for weight matrices ($W_i, W_f, W_c, W_o$) and biases ($b_i, b_f, b_c, b_o$) for the input, forget, candidate cell state, and output gates.\n8. Perform a quantum LSTM step:\nCalls the 'quantum_lstm_step` function to apply one Q-LSTM time step to the quantum circuit, incorporating the quantum gates, input, and biases.\n9. Measure quantum circuit to obtain output:\nAdds measurement operations to the quantum circuit to obtain classical measurement results from the qubits.\n10. Simulate the quantum circuit:\nSelect a quantum simulator (Qiskit's Aer simulator) to execute the quantum circuit and simulate quantum measurements.\n11. Execute the quantum circuit and obtain results:\nExecutes the quantum circuit and obtains measurement results (`counts') from the simulator.\n12. Display measurement results:\nPrints the measurement results obtained from the quantum circuit, representing the outcome of the Q-LSTM step.\nThis code provides a simplified illustration of a Q-LSTM step using a quantum framework. Developing a complete Q-LSTM model would require more complex implementations, optimized quantum gates, and integration with quantum hardware or advanced simulators for practical applications."}, {"title": "3. Results and Discussion", "content": "One of the most lucrative issues in modern finance is the accurate forecast of future stock values, which may result in significant profit and reduced risk. Recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) may be applied to sequential data processing and classification problems. Because of this, many individuals have had great success using LSTM to predict future stock values using sequences of previous data.\nOn the other hand, recent research has demonstrated that the LSTM may be made more effective and trainable by swapping out part of its layers for variational quantum layers. Therefore, we obtain a hybrid quantum-classical LSTM model, abbreviated QLSTM for quantum LSTM. A study [44, 50] shows that QLSTM is more trainable than its classical counterpart because it learns local features more efficiently and significantly more data after the first training epoch while utilizing similar parameters. Considering these most recent findings, we continue to test our variational quantum-classical hybrid neural network approach on stock price projections.\nIn the following notebook, we provide a proof of concept for applying QLSTM in stock price prediction, demonstrating that it can produce results that are on par with or even superior to those of its traditional counterpart. To do this, we use the same number of features in both LSTM and QLSTM to predict the stock prices of the KSE 100 index.\nWe have gathered historical information on the KSE stock prices, emphasizing the closing price. Using LSTM (or QLSTM), we aim to predict KSE's closing stock prices.\nWe have gathered the following relevant data and information to accomplish this goal. We have chosen not to go into detail about the data collected in this study, even though it is fascinating and significant in and of itself. We divided the data into 80% for training purposes and 20% for training to validate the accuracy. Both models undergo a training phase, during which they learn the underlying patterns by adjusting their internal parameters. This involves feeding the historical data into the models, comparing their predictions against the actual output, and optimizing their parameters to minimize the prediction error. Figure 4 and Figure 5 show the mean squared error loss of both LSTM and QLSTM during training and testing, respectively.\nOnce trained, both models are ready for predictions. Given a set of new input features, they can project how these features relate to the output column based on the patterns they've learned during training. Figure 6 shows the LSTM's prediction of the close value of the KSE 100 index, and Figure 7 shows the QLSTM's prediction of the close value of the KSE 100 index."}, {"title": "4. Conclusion", "content": "The present study identifies twenty-six economic, social, political, and administrative variables as predictors to predict the value of the KSE 100 index. Monthly data from February 2004 to December 2020 was taken for analysis. The mean absolute error loss of the LSTM and QLSTM during data training showed that the difference between actual and predicted values is minimal. Moreover, QLSTM has predicted values more accurately as compared to the LSTM. These results validate that the LSTM and QLSTM used in this study predict the stock market performance of a country based on not only macroeconomic or political indicators but also the inclusion of social and administrative indicators."}]}