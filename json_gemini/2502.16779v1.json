{"title": "UNPOSED SPARSE VIEWS ROOM LAYOUT RECONSTRUCTION IN THE AGE OF PRETRAIN MODEL", "authors": ["Yaxuan Huang", "Xili Dai", "Jianan Wang", "Xianbiao Qi", "Yixing Yuan", "Xiangyu Yue"], "abstract": "Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.", "sections": [{"title": "INTRODUCTION", "content": "3D room layout estimation aims to predict the overall spatial structure of indoor scenes, playing a crucial role in understanding 3D indoor scenes and supporting a wide range of applications. For example, room layouts could serve as a reference for aligning and connecting other objects in indoor environment reconstruction. Accurate layout estimation also aids robotic path planning and navigation by identifying passable areas. Additionally, room layouts are essential in tasks such as augmented reality (AR) where spatial understanding is critical."}, {"title": "RELATED WORK", "content": "Layout estimation. Most room layout estimation research focuses on single-perspective image inputs. Stekovic et al. (2020) formulates layout estimation as a constrained discrete optimization problem to identify 3D polygons. Yang et al. (2022) introduces line-plane constraints and connectivity relations between planes for layout estimation, while Sun et al. (2019) formulates the task as predicting 1D layouts. Other studies, such as Zou et al. (2018), propose to utilize monocular 360-degree panoramic images for more information. Several works extend the input setting from single panoramic to multi-view panoramic images, e.g. Wang et al. (2022) and Hu et al. (2022). However, there is limited research addressing layout estimation from multi-view RGB perspective images. Howard-Jenkins et al. (2019) detects and regresses 3D piece-wise planar surfaces from a series of images and clusters them to obtain the final layout, but this method requires posed images. The most related work is Jin et al. (2021), which focuses on a different task: reconstructing indoor scenes with planar surfaces from wide-baseline, unposed images. It is limited to two views and requires an incremental stitching process to incorporate additional views.\nHolistic scene understanding. Traditional 3D indoor reconstruction methods are widely appli- cable but often lack explicit semantic information. To address this limitation, recent research has increasingly focused on incorporating holistic scene structure information, enhancing scene under- standing by improving reasoning about physical properties, mostly centered on single-perspective images. Several studies have explored the detection of 2D line segments using learning-based de- tectors. However, these approaches often struggle to differentiate between texture-based lines and structural lines formed by intersecting planes. Some research has focused on planar reconstruction to capture higher-level information. Certain studies have tackled multiple tasks alongside layout reconstruction, such as depth estima- tion, object detection, and semantic segmentation. Other works operate on constructed point maps; for instance, Yue et al. (2023) reconstructs floor plans from density maps by predicting sequences of room corners to form polygons. SceneScript employs large language models to represent indoor scenes as structured language commands.\nMulti-view pose estimation and reconstruction. The most widely applied pipeline for pose es- timation and reconstruction on a series of images involves SfM and MVS, which typically includes steps such as feature mapping, finding cor- respondences, solving triangulations and optimizing camera parameters. Most mainstream methods build upon this paradigm with improvements on various aspects of the pipeline. However, recent works such as DUSt3R and MASt3R propose a reconstruc- tion pipeline capable of producing globally-aligned pointmaps from unconstrained images. This is achieved by casting the reconstruction problem as a regression of pointmaps, significantly relaxing input requirements and establishing a simpler end-to-end paradigm for 3D reconstruction."}, {"title": "METHOD", "content": "In this section, we formulate the layout estimation task, transitioning from a single-view to a multi- view scenario. We then derive our multi-view layout estimation pipeline as shown in Figure 2 (Sec- tion 3.1). Our pipeline consists of three parts: a 2D plane detector \\(f_1\\), a 3D information prediction and correspondence establishment method Plane-DUSt3R \\(f_2\\) (Section 3.2), and a post-processing algorithm \\(f_3\\) (Section 3.3)."}, {"title": "FORMULATION OF THE MULTI-VIEW LAYOUT ESTIMATION TASK", "content": "We begin by revisiting the single-view layout estimation task and unifying the formulation of exist- ing methods. Next, we extend the formulation from single-view to multiple-view setting, providing a detailed analysis and discussion focusing on the choice of solutions. Before formulating the lay- out estimation task, we adopt the \"geometric primitives + relationships\" representation from Zheng et al. (2020) to model the room layout.\nGeometric Primitives.\nPlanes: The scene layout could be represented as a set of planes \\(\\{P_1, P_2 . . .\\}\\) in 3D space and their corresponding 2D projections \\(\\{p_1, p_2, . . .\\}\\) in images. Each plane is parameterized by its normal \\(n \\in S^2\\) and offset \\(d\\). For a 3D point \\(x \\in \\mathbb{R}^3\\) lying on the plane, we have \\(n^\\mathsf{T} x + d = 0\\).\nLines & Junction Points: In 3D space, two planes intersect at a 3D line, three planes inter- sect at a 3D junction point. We denote the set of all 3D lines/junction points in the scene as \\(\\{L_1, L_2 ...\\}/\\{J_1, J_2 . . . \\}\\) and their corresponding 2D projections as \\(\\{l_1, l_2, . . .\\}/\\{j_1, j_2, . . .\\}\\) in images.\nRelationships.\nPlane/Line relationships: An adjacent matrix \\(W_p/W_l \\in \\{0,1\\}\\) is used to model the relation- ship between planes/lines. Specifically, \\(W_p(i, j) = 1\\) if and only if \\(P_i\\) and \\(P_j\\) intersect along a line; otherwise, \\(W_p(i, j) = 0\\). Similarly to plane relationship, \\(W_l(i, j) = 1\\) if and only if \\(L_i\\) and \\(L_j\\) intersect at a certain junction, otherwise, \\(W_l(i, j) = 0\\).\nThe pipeline of single-view layout estimation methods can be formulated as:\n\\(I \\xrightarrow{f_1} \\{2\\text{D}, 3\\text{D}\\} \\xrightarrow{f_3} \\{P, L, J, W\\},\\)\nwhere \\(f_1\\) is a function that predicts 2D and 3D information from the input single view. Generally speaking, the final layout result \\(\\{P, L, J, W\\}\\) can be directly inferred from the outputs of \\(f_1\\). However, errors arising from \\(f_1\\) usually adversely affect the results. Hence, a refinement step that utilizes prior information about room layout is employed to further improve the performance. Therefore, \\(f_3\\) typically encompasses post-processing and refinement steps where the post-processing step generates an initial layout estimation, and the refinement step improves the final results.\nFor instance, Yang et al. (2022) chooses the HRnet network (Wang et al., 2020) as \\(f_1\\) backbone to extract 2D plane \\(p\\), line \\(l\\), and predict 3D plane normal \\(n\\) and offset \\(d\\) from the input single"}, {"title": "", "content": "view. After obtaining the initial 3D layout from the outputs of \\(f_1\\), the method reprojects the 3D line to a 2D line \\(\\hat{l}\\) on the image and compares it with the detected line \\(l\\) from \\(f_1\\). \\(f_3\\) minimizes the error \\(\\|\\(\\hat{l} - l)\\|_3\\) to optimize the 3D plane normal. In other words, it uses the better-detected 2D line to improve the estimated 3D plane normal. In contrast, Stekovic et al. (2020) uses a different approach: its \\(f_1\\) predicts a 2.5D depth map instead of a 2D line \\(l\\) and uses the more accurate depth results to refine the estimated 3D plane normal. Among the works that follow the general framework of 1 , Yang et al. (2022) stands out as the best single-view perspective image layout estimation method without relying on the Manhattan assumption. Therefore, we present its formulation in equation (2) and extend it to multi-view scenarios.\n\\xrightarrow{f_1} \\{p,l,n,d\\} \\xrightarrow{f_3} \\{P,L, J, W\\},\nIn room layout estimation from unposed multi-view images, two primary challenges aris: 1) camera pose estimation, and 2) 3D information estimation from multi-view inputs. Camera pose estimation is particularly problematic given the scarcity of annotated multi-view layout dataset. Thanks to the recent advancements in 3D vision with pretrain model, this challenge could be effectively bypassed: DUSt3R has demonstrated the ability to reconstruct scenes from unposed images without requiring camera intrinsic or extrinsic, and even without overlap between views. Moreover, the 3D pointmap generated from DUSt3R can provide significantly improved 3D information, such as plane normal and offset, compared to single-view methods (Yang et al., 2022). Therefore, DUSt3R represents a critical advancement in extending single- view layout estimation to multi-view scenarios. Before formulating the multi-view solution, we first present the key 3D representation of DUSt3R: the pointmap X and the camera pose T. The camera pose T is obtained through global alignment, as described in the DUSt3R (Wang et al., 2024)).\nPointmap X: Given a set of RGB images \\(\\{I_1,...,I_n\\} \\in \\mathbb{R}^{H \\times W \\times 3}\\), captured from distinct viewpoints of the same indoor scene, we associate each image \\(I_i\\) with a canonical pointmap \\(X_i \\in \\mathbb{R}^{H \\times W \\times 3}\\). The pointmap represents a one-to-one mapping from each pixel \\((u, v)\\) in the image to a corresponding 3D point in the world coordinate frame: \\((u, v) \\in \\mathbb{R}^2 \\leftrightarrow X_i(u, v) \\in \\mathbb{R}^3\\).\nCamera Pose T: Each image \\(I_i\\) is associated with a camera-to-world pose \\(T_i \\in SE(3)\\).\nNow, the sparse-view layout estimation problem can be formulated as shown in equation (3)\n\\(\\{I_1, I_2,...\\} \\xrightarrow{f_1, f_2} \\{p, l, X,T\\} \\xrightarrow{f_3} \\{P, L, J, W\\}.\\)\nIn this work, we adopt the HRnet backbone from Yang et al. (2022) as \\(f_1\\). In the original DUSt3R formulation, the ground truth pointmap \\(X_{obt}\\) represents the 3D coordi- nates of the entire indoor scene. In contrast, we are interested in plane pointmap \\(X_p\\) that represents the 3D coordinates of structural plane surfaces, including walls, floors, and ceilings. This formula- tion intentionally disregards occlusions caused by non-structural elements, such as furniture within the room. Our objective is to predict the scene layout pointmap without occlusions from objects, even when the input images contain occluding elements. For simplicity, any subsequent reference to X in this paper refers to the newly defined plane pointmap \\(X_p\\). We introduce Plane-DUSt3R as \\(f_2\\) and directly infer the final layout via \\(f_3\\) without the need for any refinement."}, {"title": "\\(f_2\\): PLANE-BASED DUST3R", "content": "The original DUSt3R outputs pointmaps that capture all 3D information in a scene, including furni- ture, wall decorations, and other objects. However, such excessive information introduces interfer- ence when extracting geometric primitives for layout prediction, such as planes and lines. To obtain a structural plane pointmap X, we modify the data labels from the original depth map to the structural plane depth map , and then retrain the DUSt3R model. This updated objective guides DUSt3R to predict the pointmap of the planes while ignoring other objects. The original DUSt3R does not guarantee output at a metric scale, so we also trained a modified version of Plane-DUSt3R that produces metric-scale results.\nGiven a set of image pairs \\(P = \\{(I_i,I_j) | i \\neq j,1 \\leq i,j \\leq n,I \\in \\mathbb{R}^{H \\times W \\times 3}\\}\\), for each image pair, the model comprises two parallel branches. As shown in Figure 3, the detail"}, {"title": "\\(f_3\\): POST-PROCESSING", "content": "In this section, we introduce how to combine the multi-view plane pointmaps X and 2D detec- tion results p,l to derive the final layout \\(\\{P,L,J,W\\}\\). For each single view \\(I_i\\), we can in- fer a partial layout result \\(\\{P_i, L_i, J_i,W_i\\} = g_1(X_i, p^i, l^i)\\) from the single view pointmaps \\(X_i\\) and 2D detection results \\(p^i, l^i\\) through a post-process algorithm \\(g_1\\) in camera coordinate. Then, a correspondence-establish and merging algorithm \\(g_2\\) combines all partial results to get the final layout \\(\\{P, L, J, W\\} = g_2(\\{P_1, L_1, J_1, W_1\\}, . . .).\\)\nSingle-view room layout estimation \\(g_1\\). For an image \\(I_i\\), \\(g_1\\) mainly addresses two tasks: 1) lifting 2D planes to 3D camera coordinate space with 3D normal from pointmap \\(X_i\\), and 2) inferring the wall adjacency relationship. We follow the post-processing procedure in Yang et al. (2022) but with two improvements. First, the plane normal \\(n\\) and offset \\(d\\) are inferred from \\(X_i\\) instead of directly"}, {"title": "EXPERIMENTS", "content": "SETTINGS.\nDataset. Structured3D is a synthetic dataset that provides a large collection of photo-realistic images with detailed 3D structural annotations. Similar to Yang et al. (2022), the dataset is divided into training, validation, and test sets at the scene level, comprising 3000, 250, and 250 scenes, respectively. Each scene consists of multiple rooms, with each room containing 1 to 5 images captured from different viewpoints. To construct image pairs that share similar visual content, we retain only rooms with at least two images. Within each room, images are paired to form image sets. Ultimately, we obtained 115,836 image pairs for the training set and 11,030 image pairs for the test set. For validation, we assess all rooms from the validation set. For rooms that only have one image, we duplicate that image to form image pairs for pointmap retrieval. In the subsequent inference process, we retain only one pointmap per room."}, {"title": "MULTI-VIEW ROOM LAYOUT ESTIMATION RESULTS", "content": "In this section, we compare our multi-view layout estimation pipeline with two baseline methods, both qualitatively and quantitatively. Additionally, we conduct experiments to verify the effective- ness of our pipeline components \\(f_1\\) 2D detector and \\(f_2\\) Plane-DUSt3R.\nLayout results comparison. Table 1 and Figure 6 present quantitative and qualitative comparisons of our pipeline with two baseline methods. Ours (metric) and Ours (aligned) in Table 1 refer to the methods from our pipeline using Plane-DUSt3R (metric) and Plane-DUSt3R, respectively. The first 4 metrics (re-IoU, re-PE, re-EE, and re-RMSE) are calculated similarly to their 2D counterparts (IoU, PE, EE, and RMSE), except that the predicted 2D results are reprojected from the estimated multi-view 3D layout. Compared with the baseline methods, Plane-DUSt3R achieves superior 3D plane normal estimations compared to Noncuboid's single-view plane normal estimation, even when using ground truth camera pose (Noncuboid + GT pose). Figure 7 further demonstrates that Plane- DUSt3R could predict accurate and robust 3D information with sparse-view input."}, {"title": "CONCLUTION", "content": "This paper introduces the first pipeline for multi-view layout estimation, even in sparse-view set- tings. The proposed pipeline encompasses three components: a 2D plane detector, a 3D information prediction and correspondence establishment method, and a post-processing algorithm. As the first comprehensive approach to the multi-view layout estimation task, this paper provides a detailed analysis and formulates the problem under both single-view and multi-view settings. Additionally, we design several baseline methods for comparison to validate the effectiveness of our pipeline. Our approach consistently outperforms the baselines on both 2D projection and 3D metrics. Furthermore, our pipeline not only performs well on the synthetic Structure3D dataset, but generalizes effectively to in-the-wild datasets and scenarios with different image styles such as the cartoon style."}, {"title": "DUST3R DETAILS", "content": "Given a set of RGB images \\(\\{I_1, I_2, . . ., I_n\\} \\in \\mathbb{R}^{H \\times W \\times 3}\\), we first pair them to create a set of image pairs \\(P = \\{(I_i, I_j) | i \\neq j, 1 \\leq i, j < n\\}\\). For each image pair \\((I_i, I_j) \\in P\\), the model estimates two point maps \\(X_{i,i}, X_{j,i}\\), along with their corresponding confidence maps \\(C_{i,i}, C_{j,i}\\). Specifically, both pointmaps are expressed in the camera coordinate system of \\(I_i\\), which implicitly accomplishes dense 3D reconstruction.\nThe model consists of two parallel branches, as shown in Fig 3, each branch responsible for pro- cessing one image. The two images are first encoded in a Siamese manner with weight-sharing ViT encoder(Dosovitskiy et al., 2020) to produce two latent features \\(F_1, F_2\\): \\(F_i = \\text{Encoder}(I_i)\\). Next, \\(F_1, F_2\\) are fed into two identical decoders that continuously share information through cross- attention mechanisms. By leveraging cross-attention mechanisms, the model is able to learn the relative geometric relationships between the two images. Specifically, for each encoder block:\n\\begin{aligned}\nG_{1,i} &= \\text{DecoderBlock}_{1,i}(G_{1,i-1}, G_{2,i-1}), \\\\\nG_{2,i} &= \\text{DecoderBlock}_{2,i}(G_{1,i-1}, G_{2,i-1})\n\\end{aligned}\nwhere \\(G_{1,0} := F_1\\), \\(G_{2,0} := F_2\\). Finally, the DPT head regresses the pointmap and confidence map from the concatenated features of different layers of the decoder tokens:\n\\begin{aligned}\nX_{1,i}, C_{1,i} &= \\text{Head}_1(G_{1,0}, G_{1,1},..., G_{1,B}) \\\\\nX_{2,i}, C_{2,i} &= \\text{Head}_2(G_{2,0}, G_{2,1},..., G_{2,B}) \n\\end{aligned}\nwhere B is the number of decoder blocks. The regression loss function is defined as the scale- invariant Euclidean distance between the normalized predicted and ground-truth pointmaps:\n\\begin{aligned}\nI_{regr(v, i)} = \\frac{1}{2} \\frac{|X_{v,i} - \\hat{X}_{v,i}|^2}{\\hat{z}_{v} \\ z_{v}}, \\\\\n\\end{aligned}\nwhere \\(v \\in \\{1,2\\}\\) and i is the pixel index. The scaling factors \\(\\hat{z}_{v}\\) and \\(z_{v}\\) represent the average distance of all corresponding valid points to the origin. The original DUSt3R couldn't guarantee output at a metric scale, so we also trained a modified version of Plane-DUSt3R that produces metric- scale results. The key change we made was setting \\(\\hat{z}_{v} := z_{v}\\). By introducing the regression loss in confidence loss, the model could implicitly learn how to identify regions that are more challenging to predict compared to others. Same as in DUSt3R (Wang et al., 2024):\nL_{conf} = \\sum\\limits_{v \\in \\{1,2\\}} \\sum\\limits_{i \\in D_U} C_v \\mid I_{regr}(v, i) - \\alpha \\log{C_v} \\mid\nTo obtain the ground-truth pointmaps \\(\\hat{X}_{v,l}\\), we first transform the ground truth depthmap \\(D \\in \\mathbb{R}^{H \\times W}\\) into a pointmap \\(X^v\\) express in the camera coordinate of v by \\(X^v_{i,j} \\thickapprox K^{-1}[i D_{i,j}, j D_{i,j}, D_{i,j}]\\) with camera intrinsic matrix K \\(\\in \\mathbb{R}^{3 \\times 3}\\). Then we obtain \\(\\hat{X}_{v,l}\\) by \\(\\hat{X}_{v,l} = T_v^{-1}T_h(X^v)\\) with \\(T_v, T_h \\in \\mathbb{R}^{3 \\times 4}\\) the camera-to-world poses and h being the homogeneous transformation.\nGlobal Alignment For global alignment, we aim to assign a global pointmap and camera pose for each image. First, the average confidence scores of each pair of images are utilized as the similarity scores. A higher value of confidence implies a stronger visual similarity between the two images. These scores are employed to construct a Minimum Spanning Tree, denoted as G(V,E), where each vertex V corresponding to an image in the input set and each edge e = (n,m) \\(\\in\\) E indicates that images In and Im share significant visual content. We aim to find globally aligned point maps\\(\\{\\hat{X} \\in \\mathbb{R}^{H \\times W \\times 3}\\}\\) and a transformation \\(T_i \\in \\mathbb{R}^{3 \\times 4}\\) than transform the prediction into the world coordinate frame. To do this, for each image pair e = (n,m) \\(\\in\\) E we have two point maps \\(X^{n,n}, X^{m,n}\\) and their confidence maps \\(C^{n,n}, C^{m,n}\\). For simplicity, we use the annotation \\(X^{n,e} := X^{n,n}, X^{m,e} := X^{m,n}\\). Since \\(X^{n,e}\\) and \\(X^{m,e}\\) are in the same coordinate frame, Te := T\u03b7 should align both point maps with the world coordinate. We then solve the following optimization problem:\n\\begin{aligned}\n\\hat{X}^*, T^* = \\text{arg min} \\sum\\limits_{e \\in E} \\sum\\limits_{v \\in e} \\sum\\limits_{i=1}^{H W} |C_{e,i} \\mid \\hat{X}_i^{v} - \\sigma_e T_e X_i^{v,e} |^2 \n\\end{aligned}\nwhere v \\(\\in\\) e means v can be either n or m for the pair e and \u03c3\u03b5 is a positive scaling factor. To avoid the trivial solution where \u03c3\u03b5 = 0, we ensure that \\(\\prod_{e} \\sigma_{e} = 1\\)"}, {"title": "\\(f_3\\) ALGORITHM", "content": "The goal of multi-view layout estimation is similar to that of single-view: we need to estimate 3D parameters for each plane and determine the relationships between adjacent planes. However, in a multi-view setting, we must ensure that each plane represents a unique physical plane in 3D space. The main challenge in multi-view reconstruction is that the same physical plane may appear in multiple images, causing duplication. Our task is to identify which planes correspond to the same physical plane across different images and merge them, keeping only one representation for each unique plane.\nSince we allow at most one floor and one ceiling detection per image, we simply average the param- eters from all images to obtain the final floor and ceiling parameters. As for walls, we assume all walls are perpendicular to both the floor and ceiling. To simplify the merging process, we project all walls onto the x-z plane defined by the floor and ceiling. This projection reduces the problem to a 2D space, making it easier to identify and merge corresponding walls. Figure 5 illustrates the entire process of merging walls. Each wall in an image is denoted as one line segment, as shown in Figure 5a. We then rotate the scene so that all line segments are approximately horizontal or vertical, as depicted in 5b. In Figure 5c, each line segment is classified and further rotated to be either horizontal or vertical, based on the assumption that all adjacent walls are perpendicular to each other."}]}