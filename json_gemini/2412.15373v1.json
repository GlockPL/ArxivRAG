{"title": "Granger Causality Detection with Kolmogorov-Arnold Networks", "authors": ["Hongyu Lin", "Mohan Ren", "Paolo Barucca", "Tomaso Aste"], "abstract": "Discovering causal relationships in time series data is central in many scientific areas, ranging from economics to climate science. Granger causality is a powerful tool for causality detection. However, its original formulation is limited by its linear form and only recently nonlinear machine-learning generalizations have been introduced. This study contributes to the definition of neural Granger causality models by investigating the application of Kolmogorov-Arnold networks (KANs) in Granger causality detection and comparing their capabilities against multilayer perceptrons (MLP). In this work, we develop a framework called Granger Causality KAN (GC-KAN) along with a tailored training approach designed specifically for Granger causality detection. We test this framework on both Vector Autoregressive (VAR) models and chaotic Lorenz-96 systems, analysing the ability of KANs to sparsify input features by identifying Granger causal relationships, providing a concise yet accurate model for Granger causality detection. Our findings show the potential of KANs to outperform MLPs in discerning interpretable Granger causal relationships, particularly for the ability of identifying sparse Granger causality patterns in high-dimensional settings, and more generally, the potential of AI in causality discovery for the dynamical laws in physical systems.", "sections": [{"title": "I. INTRODUCTION", "content": "In time series analysis, Granger causality, first introduced by [8], has long been a fundamental tool in statistical method for exploring causal relationships between time series. Unlike static correlation analysis, Granger causality examines temporal relationships, assessing whether the historical information of one time series can improve the prediction of another, while accounting for the predictive information contained in the history of the latter. Despite ongoing debates about the general applicability of this framework, it serves as an operational framework that allows causality to be inferred in a data-driven manner from time series.\nGranger causality originally focuses on bivariate analysis under the assumptions of linear dependency and stationary time series. Specifically, if the past values of one time series xi can help predict another series xj beyond what is achieved by the past of xj alone, then xi is said to Granger-cause xj. In the general multivariate setting, Vector Autoregressive (VAR) models have been used, where multiple time series and their lags are jointly modeled to infer Granger-causal relationships [2, 3, 6]. Moreover, extensions to nonlinear dynamics, such as the nonlinear Autoregressive (NAR) model [4], overcome the linearity assumption by incorporating a nonlinear function which takes the past values of all the time series as arguments.\nTo address the complexity of nonlinear and high-dimensional time series, several researchers have utilized neural networks to model Granger causality. Tank et al. [15] introduced the component-wise Multilayer Perceptron (cMLP) and the component-wise Long Short-Term Memory network (cLSTM) for multivariate nonlinear Granger causality. In these approaches, each target time series is independently modeled using a dedicated MLP or LSTM, with the lagged values of all time series serving as inputs. Granger causality is then inferred by analyzing the weights connecting the input layer to the first hidden layer, where significant weights indicate Granger causal relationships. Nauta et al. [12] proposed the Temporal Causal detection Framework (TCDF), which employs convolutional neural networks enhanced with an attention mechanism. TCDF identifies causal relationships by learning which time series and their corresponding lags contribute most to predicting a target series, effectively capturing complex temporal patterns while providing interpretable lag-specific causal inference.\nRecently, Kolmogorov-Arnold Networks (KANs) were introduced by Liu et al. [11], inspired by the Kolmogorov-Arnold representation theorem [5, 10]. KANs have gained significant attention for their accuracy and interpretability. Unlike traditional MLPs, where connections between neurons are linear weights, KANS replace these connections with learnable univariate spline functions, allowing for greater flexibility in activation functions. This design enables KANs to model complex, nonlinear patterns effectively. One of the key features of KANs is their ability to achieve sparsification. The authors introduced a regularisation combining L1 norm and entropy norm on all the splines. As a result, connections with weak contribution can effectively be pruned, i.e. removed, during training.\nSeveral studies have highlighted the potential of KANs in time series forecasting. Xu et al. [18] developed T-KAN and MT-KAN for time series prediction. They demonstrated that with dynamic activation functions, KANs are particularly effective at capturing complex temporal patterns in time series. Aca-Rubio et al. [16] showed that KAN not only has better forecasting performance on satellite traffic data than MLP, it also shows higher parameter efficiency.\nIn this work, we introduce Granger Causality KAN (GC-KAN), a framework for multivariate Granger causality detection that is both accurate and interpretable. Our method builds upon the concept of the CMLP, however, instead of using MLPs for each target time series, we replace them with KANs. To achieve automatic identification of Granger non-causal inputs, we implement an additional proximal operator during training, applied only to the edges connecting the inputs and the first hidden layer. This ensures that irrelevant input features are assigned exact zero weights, allowing Granger causality to be inferred directly from inputs with non-zero weights. The flexibility of KANs' activation functions, combined with the sparsity-inducing regularisation, enhances the model's interpretability.\nIn Section II we present the mathematical foundation of multivariate Granger causality and detail its implementation in both cMLP and GC-KAN. This is followed by a comprehensive description of the GC-KAN framework and its training tailored for Granger causality detection. In Section III we showcase the experimental results, where we compare the model performance of GC-KAN against CMLP with a specific structured penalty on linear VAR data and nonlinear Lorenz-96 datasets."}, {"title": "II. METHODOLOGY", "content": "Multivariate Granger causality determines if one or more time series can predict another within a multivariate framework. In its linear form, it typically uses the VAR model, assuming linear interactions and stationarity. For an n dimensional multivariate time series xt \u2208 Rn with T time steps, t = 1. . . ., T, each component in the VAR model of order p is formulated as:\nXt,i=\\sum_{k=1}^{p}A_{k}x_{t-k} + e_{t},\nwhere Ak) are (1 x n) vectors representing linear coefficients for each lag k and et is the vector of noise with zero mean and constant variance. Granger causality from xt,j to xt,i is inferred by testing the null hypothesis\nHo: (A_{j}^{(k)}) = 0  for j\u2260i, and all k = 1,...,p.\nNonlinear Granger causality relaxes the linearity assumption, allowing for more complex dependencies. The VAR model in Eq. 1 is generalised by replacing the summation with a nonlinear function fi(\u00b7), which reads\nX_{t,i} = f_{i}(A_{1}x_{t-1}, A_{2}x_{t-2},..., A_{p}x_{t-p}) + e_{t}.\nwhere, in this case, Ak) are n\u00d7n diagonal matrices. In practice, such function is usually estimated via neural networks.\nIn the nonlinear case, Granger causality is identified if including past values of a particular series in f improves predictive accuracy for the target series. Mathematically, similarly with the previous case, the null hypothesis for testing Granger causality from Xt,j to xt,i can be expressed as:\nHo: (A_{j}^{(k)}) = 0  for j \u2260i, and all k = 1,..., p.\nUnder this null hypothesis Ho, the past values of Xt,j do not contribute to the prediction of Xt,i, implying no Granger causality from xt,j to xt,i.\nIn practice, linear VAR and nonlinear VAR models are intuitive approaches to identify Granger causality between variables within a system. However, several assumptions are required for these models to be an appropriate framework [1]. Firstly, all the observed time series are assumed to be stationary and form a complete system. Stationarity means the statistical properties of all series do not change over time. The assumption of a complete system implies that there are no unobserved confounders [17]. These conditions ensure that the detected relationships are consistent and without bias.\nMoreover, all time series are assumed to have identical, discrete frequencies that align with true causal lags so that the modelled relationships can accurately reflect the true dependencies [14]. The underlying lagged dependencies are assumed to have a finite order, such lag order p must be appropriately chosen to capture the dynamics of the system. Selecting an insufficient lag order may lead to omitted causal links, while an overly large lag order increases model complexity and the risk of overfitting.\nBoth the linear and nonlinear frameworks offer powerful tools to analyze Granger causality, with linear models providing simplicity and interpretability, while nonlinear models enable the detection of complex functional dependencies."}, {"title": "B. CMLP for Granger Causality", "content": "The cMLP framework introduced in [15] builds on the nonlinear Granger causality concept by using neural networks to estimate the function fi(\u00b7) in Eq. 3. It addresses the challenge of detecting nonlinear causal relationships in high-dimensional time series by combining predictive modeling with structured regularization, enabling the identification of sparse and interpretable causal pattern.\nFor a system described by Eq. 3, the cMLP model trains one feedforward neural network for each target time series in xt. The inputs of each MLP are the lagged observations of all variables up to a max lag p, forming a vector of size nxp. Each MLP is tasked with predicting the value of a specific target series xt,i based on these inputs. Since the lagged input vector requires p previous time steps, the loss function starts at t = p to ensure that all lagged inputs are available.\nA critical innovation in the cMLP framework lies in how Granger causality is extracted from the trained net- works. The first-layer weights of each MLP, which connect the input variables to the hidden layer, are subjected to sparsity-inducing regularization during training. This regularization ensures that only the most relevant input variables retain nonzero weights, with irrelevant variables being effectively pruned. The presence of non-zero weights after training indicates that the corresponding lagged inputs contribute to the prediction of the target series, hence revealing Granger causal relationships.\nMathematically, the cMLP loss function consists of two main components: the mean squared error (MSE) for prediction accuracy and a sparsity-inducing penalty applied to the first-layer weights to shrink the entire set of first layer weights for input series j to zero. The loss function is given by:\nL=\\sum_{t=p}^{T} (x_{t,i} - f_{i}(x_{(t-1):(1-p)}))^{2} + \\lambda C_{H}(W),\nwhere W denotes the weights associated with the j-th input variable across all lags.\nThe three penalties that the cMLP framework incorporates are Group Lasso (GL) [19], Hierarchical Group Lasso (H) [13] and the Group Sparse Group Lasso (GSGL). It is demonstrated in [15] that the hierarchical penalty outperformed the other two across different max lag values, showing robustness to input lag. In this study, we focus on the H penalty, and the results of GC-KAN are compared against CMLP with H in Section. III.\nThe Hierarchical Group Lasso penalty imposes a structured sparsity constraint across time lags. It performs simultaneous selection of Granger causal variables and their maximum time lag order. Specifically, it ensures that higher lags of a variable can only contribute to the prediction if all lower lags are also included. The penalty is defined as:\nC_{H} ((1)) = \\sum_{j=1}^{n} \\sum_{k=1}^{p} |W_{j}^{kp}|_{F}\nwhere Wp includes all weights connecting lags k through p of variable j to the hidden layer, || ||F is the Frobenius norm which penalises the magnitude of the grouped weights.\nThe CMLP model is trained using proximal gradient descent, which alternates between a gradient descent step to minimize the smooth part of the loss function (MSE) and a proximal update to enforce sparsity in the first-layer weights. By combining these techniques, the CMLP framework produces sparse and interpretable models that identify the nonlinear causal relationships driving the target time series. This makes it particularly effective for high-dimensional systems where traditional Granger causality methods struggle to scale."}, {"title": "C. GC-KAN for Granger Causality", "content": "Building upon the structure of the cMLP, our GC-KAN framework models each time series individually using a KAN, enabling flexible and interpretable causality detection.\nInstead of linear weights in MLPs, KANs incorporate learnable, spline-based activation functions on the edges connecting neurons. Specifically, each connection between neurons is parameterized by a combination of base weight Wbase and spline weight Wspline constructed using B-splines [7]. Given an input vector z, the output of the first hidden layer is defined as:\nh\u00b9 = \u03a6\u03bf\u03b6,\n\u03a6\u03bf = W_{base}(z) + W_{spline}spline(z).\nwhere \u03c3(\u00b7) is the basis function (e.g., SiLU). The spline is computed using B-spline basis functions, which are learnable during training. The output of deeper layers follows a similar formulation, hence the network architecture with I layers can be described as:\nKAN(z) = f(z)\n= (\u03a6\u03b9\u22121 \u25cb \u03a6\u03b9\u22122 \u25cb \uff65\uff65\uff65 \u25cb \u03a6\u03bf)(z).\nThese components allow KANs to adaptively capture localized nonlinear interactions while favoring interpretability. Such flexible architecture enables KANS to model both linear and nonlinear dependencies in the data, making them particularly well-suited for Granger causality analysis."}, {"title": "1. Mathematical Formulation for GC-KAN", "content": "The GC-KAN model consists of one KAN for each time series in the multivariate system xt. Each KAN is tasked with predicting a single target series Xti based on the lagged observations of all series. The input vector for each GC-KAN is defined as:\nZ = [xt-1, Xt-2,..., Xt\u2212p] \u2208 Rnp\u00d71.\nThe output of each KAN is the predicted value of the corresponding target series Xt,i:\nX_{t,i} = f_{i}(z),\nwhere fi is the function learned by the i-th KAN.\nThe KANs employ two regularization techniques to promote sparsity and improve interpretability: an L1 norm penalty and an entropy-based penalty. Formally, the L1 norm for activation function is defined as the average magnitude over its N inputs:\n$\\phi_{1} = \\frac{1}{N} \\sum_{s=1}^{N} \\phi(x^{(s)})$,"}, {"title": "2. GC-KAN Optimization for Granger Causality", "content": "The first layer of GC-KAN maps time-lagged inputs to hidden representations. Granger causality is extracted from the functional mappings in the first layer, which encode the contribution of each lagged input series to the prediction of the target series. By analyzing these mappings, significant contributions can be identified, representing causal relationships between input features and the target variable.\nWhile KANs trained solely with gradient descent can identify contributions, noise or irrelevant inputs may still influence the results because KAN aims to express the target series in terms of all available inputs. To address this, we propose adding a proximal update step after the gradient descent update.\nTo achieve this, the GC-KAN training process involves two key steps for optimizing the loss function LGC-KAN.\n\u03b1. Gradient Descent on the Total Loss The smooth prediction loss (mean squared error) and regularization terms are optimized jointly using gradient descent across all layers:\n\u03a6 - \u03a6 \u2013 \u03b7\u2207LGC-KAN,\nwhere n is the learning rate.\nThis step ensures that the network learns both predictive accuracy and sparsity-driven representations simultaneously."}, {"title": "b. Proximal Operator for the First Layer", "content": "To enforce sparsity in the functional mappings of the first layer, a proximal operator is applied to the weights wbase and Wspline after gradient descent:\nW_{base} \u2190 Proxx_{pron}(W_{base}), W_{spline} \u2190 Proxx_{pron}(W_{spline}),\nwhere the proximal operator is defined as:\nProx_{aprox}(w) = sign(w).max(|w| \u2013 nprox, 0).\nThis soft-thresholding operation reduces the magnitude of both weights by \u03b7\u03bbprox, setting values below the threshold to zero and preserving only significant contributions. Importantly, this step is applied exclusively to the first layer, as it directly encodes the Granger causal relationships. Penalizing both Wbase and Wspline ensures that the total contribution of each lagged input is explicitly regularized, leading to a more interpretable representation of causal structure. After training, the total contribution of each lag k for variable j is computed by summing the corresponding weights:\nC_{j,k} = \\sum_{i=1}^{N_{hidden}} \\omega_{j,k,i}\nwhere Oj,k,i represents the mapping from lag k of input series j to the i-th neuron in the hidden layer. Fig. A.1, in the Appendix, illustrates the difference between the standard training and training with proximal operator.\nThe intuition behind this proximal step is to address a limitation of the original training and pruning approach. While the standard method can help identify Granger causal signals in the inputs, the weights associated with irrelevant inputs are not set exactly to zero, hence a threshold is required manually to prune out unwanted features. Such training does provide insights into contributions of features at different lags, but it does not automatically reveal Granger causality. By combining gradient descent for the entire network with a proximal operator for the first layer, GC-KAN balances predictive performance and causal interpretability, allowing for clear identification of Granger causal relationships."}, {"title": "III. RESULTS", "content": "We compare the performance between cMLP and GC-KAN with synthetic dataset generated with VAR models and Lorenz-96 with known causal relations. The comparison is performed in two aspects, causal detection accuracy using area under ROC (AUROC) and qualitative analysis of lag selection. For both experiments, the cMLP for comparison has 1 hidden layer with 100 hidden neurons, and the penalty is Hierarchical Group Lasso. It is showed in [15] that such model setup produces the best AUROC score for CMLP in both VAR and Lorenz-96 cases."}, {"title": "A. VAR model", "content": "For linear Granger causality where the underlying dynamics can be represented using a VAR model, we simulate data from n = 20 VAR(1) and VAR(2) models. Each time series generated has self dependencies and three randomly selected parents among the other n-1 series. The influence of a parent series on a target series is represented by uniform coefficients at lag 1 for VAR(1) and at lag 1 and 2 for VAR(2) models, while the coefficients of all other lagged relationships are set to zero.\nIn experiments with VAR models, the following GC-KAN structure was implemented: nxp input features representing lagged time series, 1 hidden layer with 1 hidden neuron, 1 output neuron representing the target series. The choice of 1 hidden neuron was made for simplicity and interpretability. Our experiments showed that using more hidden neurons did not significantly improve performance, as the relationships in the VAR data are linear and sparse. For VAR data, the target series is a linear combination of a few lagged inputs, which, in theory, can be represented using sum of linear weights. Thus, additional hidden neurons may not be necessary, as they introduce redundancy for such linear relations. Besides observations in our experiments, a few examples in [11] also demonstrate the sufficiency of a single hidden layer with one hidden neuron for similar tasks. Both CMLP and GC-KAN are initialized with max lag p = 5.\nWe compare the results of GC-KAN and CMLP across three model setups with varying time series lengths. Model evaluation is conducted using the average AUROC over 5 runs. To evaluate Granger causality, models are trained with a range of proximal strengths, and the resulting binary Granger causality matrix for each model is compared against the ground truth matrix to compute the true positive rate (TPR) and false positive rate (FPR). The list of sensitivities is then plotted on an ROC curve. The results are shown in Table I for three different time series length T\u2208 (250, 500, 1000). The performance of the GC-KAN and CMLP models is quite similar when T > 500. For both VAR(1) and VAR(2) with 1000 sample size, GC-KAN outperformed cMLP by a slight margin. However, it is observed that GC-KAN struggles more than cMLP with lower sample size. Both show better detection accuracy with increase in sample size."}, {"title": "2. Lag selection comparison", "content": "For CMLP with the Hierarchical Group Lasso penalty, as expressed in Eq.6, the regularization simultaneously selects Granger causal variables and determines the maximum lag order of the interaction. This penalty ensures that for each variable, there exists a maximum lag k such that all weights associated with lags greater than k are zero, while weights for all lags less than or equal to k are non-zero if the variable is relevant. This structure enforces a hierarchy among lags, allowing higher lags to contribute only if all lower lags are already active. During the optimization process, lower lags are prioritized to minimize the penalty. Fig. 1b illustrates an example of the contribution detected by the Hierarchical Group Lasso for each lag. Clearly, k = 1 has much higher contribution than k = 3 for both parents, it shows a smooth decrease in contribution for higher lags.\nOn the other hand, with GC-KAN model, the contributions across different lags are more even thanks to the combination of L1 norm and entropy norm in the loss function. The former encourages sparsity at the element level unlike group shrinkage, the entropy regularisation spreads contributions across the spline weights. We observe a more evenly distributed contribution."}, {"title": "B. Lorenz-96", "content": "For nonlinear Granger causality detection, we apply our model to the simulated Lorenz-96 data [9], whose relationships are defined by:\n\\frac{dx_{t,i}}{dt} = (x_{t,i+1} - X_{t,i\u22122}) X_{t,i\u22121} - X_{t,i} + F,\nwhere F is a forcing constant that determines the level of nonlinearity. The rate of change of xt,i, depends non-linearly on the difference between its next and second previous neighbors, scaled by its previous neighbor, with an additional linear term and a forcing constant. This equation explicitly models the temporal dependencies of a variable xt,i on its neighboring variables over time. In this case, xt,i+1, Xt,i-2 and Xt,i-1 Granger-cause Xti as their values influence its future dynamic.\nFollowing the experimental settings in [15], we simulate Lorenz-96 model with n = 20 and a sampling rate of 0.05. We evaluate GC-KAN performance based on the AUROC score across three time series lengths T\u2208 (250, 500, 1000) and two forcing constants F\u2208 (10,40). The GC-KAN model contains 20 identical KANs, each receiving 100 input neurons corresponding the lagged values of all variables (20 \u00d7 max lag(p = 5)). Each network contains 1 hidden layer with 10 add hidden neurons and a single output neuron. As demonstrated in [11], KANs effectivily approximate multiplication interactions using the formula 2xy = (x + y)\u00b2 \u2013 (x\u00b2 + y\u00b2). This motivated our choice of 10 add hidden neuron to capture nonlinear dependencies.\nThe results in Table II show a very similar performance between CMLP and GC-KAN across different time series length. Both models show an increasing score as the sample size increases. Surprisingly, in this case, GC-KAN outperformed cMLP in low sample size situation, suggesting spline functions adapt more effectively to nonlinear dependencies with limited data. As an additional evaluation, although the GC-KAN and CMLP leverage two different classes of neural networks, we compare their performance when the same number of hidden neurons are used. According to [15], with 10 hidden neurons, the CMLP with hierarchical Group Lasso achieves an AUROC score of 94% for a sample size of 1000 and F = 40, which is approximately 1% - 2% lower than GC-KAN. This difference highlights a key advantage of KANs, while both models with 10 hidden neurons have very limited trainable parameters, KANs offer greater flexibility due to their learnable spline functions, which allows KANS to capture complex nonlinear dynamic more accurately, particularly when the network architecture is small."}, {"title": "IV. CONCLUSIONS", "content": "This paper, introduces GC-KAN, a novel framework for Granger causality detection that combines the flexibility and interpretability of Kolmogorov-Arnold Networks with a sparsity-inducing proximal gradient approach. Unlike traditional methods, GC-KAN uses learnable spline-based activation functions to model both linear and nonlinear temporal dependencies, enabling an accurate identification of Granger causal relationships. The proximal operator, applied to the first layer, ensures sparsity, assigning exact zero weights to irrelevant inputs, and facilitating direct detection of Granger causality. We have shown that, by using compact architectures, even down to one hidden neurons only, GC-KAN achieves strong performance, demonstrating that simple network structures are sufficient for uncovering complex Granger causal patterns.\nWe demonstrated the effectiveness of GC-KAN through extensive experiments on simulated data, including linear VAR models and nonlinear Lorenz-96 systems. For VAR models, GC-KAN and cMLP achieved comparable AUROC scores, with GC-KAN slightly outperforming CMLP in high sample size scenarios but they are slightly less performing with low sample size. In contrast, for the Lorenz-96 system, GC-KAN showed stronger performances in low-sample-size settings, highlighting its adaptability to nonlinear dependencies even with limited data.\nIn this paper, we discussed a minimal application of KAN to Granger causality detection, demonstrating its potential to outperform existing deep learning methodologies. The GC-KAN framework proposed here opens up several exciting avenues for future research. First, incorporating structured penalties, such as hierarchical group lasso, could enhance GC-KAN's performance. Additionally, the pruning capabilities of L1 and entropy regularizations could be explored to optimize the KAN architecture for specific datasets, improving both interpretability and training efficiency. Finally, by exploiting KANs' ability to fit symbolic functions, GC-KAN could be used to derive explicit functional expressions for Granger causal relationships. These symbolic representations could offer deeper insights into the dynamics of complex systems, making GC-KAN a powerful tool for analyzing real-world physical systems."}, {"title": "V. APPENDIX", "content": null}]}