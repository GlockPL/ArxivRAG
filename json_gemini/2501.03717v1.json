{"title": "Materialist: Physically Based Editing Using Single-Image Inverse Rendering", "authors": ["Lezhong Wang", "Duc Minh Tran", "Ruiqi Cui", "Thomson TG", "Manmohan Chandraker", "Jeppe Revall Frisvad"], "abstract": "To perform image editing based on single-view, inverse physically based rendering, we present a method combining a learning-based approach with progressive differentiable rendering. Given an image, our method leverages neural networks to predict initial material properties. Progressive differentiable rendering is then used to optimize the environment map and refine the material properties with the goal of closely matching the rendered result to the input image. We require only a single image while other inverse rendering methods based on the rendering equation require multiple views. In comparison to single-view methods that rely on neural renderers, our approach achieves more realistic light material interactions, accurate shadows, and global illumination. Furthermore, with optimized material properties and illumination, our method enables a variety of tasks, including physically based material editing, object insertion, and relighting. We also propose a method for material transparency editing that operates effectively without requiring full scene geometry. Compared with methods based on Stable Diffusion, our approach offers stronger interpretability and more realistic light refraction based on empirical results. Code is avaiable at github.com/lez-s/Materialist", "sections": [{"title": "1. Introduction", "content": "High-quality image editing often requires professional skills. Reducing the complexity and increasing the accuracy of image editing has long been a focus in computer vision and computer graphics [3, 36, 40, 45]. With the success of Stable Diffusion (SD) [48] in image generation, researchers have explored using SD for image editing [7, 19, 38, 77]. However, SD often struggles with precise material property editing. Alchemist [53] attempts to address this by training SD on a large synthetic dataset, but their method lacks interpretability, a physical foundation, and struggles with accurate light refraction for transparent objects.\nInverse rendering offers a promising solution to these challenges. Single-view inverse rendering [28, 29, 52, 79] decomposes images into albedo, roughness, and metallic properties via neural networks (MaterialNet), enhancing interpretability. However, relying on a neural renderer as a substitute for a physically based renderer still shares limitations with SD, such as difficulties in accurately handling of shadows and refraction. Multi-view inverse rendering [1, 2, 30, 67, 69, 71] leverages multi-view constraints and the rendering equation to adhere better to physical principles and achieve superior results. The multi-view input requirement, however, restricts its applicability. Applying differentiable rendering to a single image leads to an ill-posed problem with infinitely many possible solutions.\nTo address these issues, we adopt a more interpretable inverse rendering approach that combines the strengths of single- and multi-view methods (MaterialNet and the rendering equation). Additionally, we introduce material transparency editing, a capability not previously explored in single-image inverse rendering."}, {"title": "2. Related work", "content": "Multi-view inverse rendering. Early work like IPT [1] was limited to simple scenes. They used multi-view constraints and assumed known scene geometry when using the rendering equation for inverse rendering. With NeRF [37], physically based inverse rendering with multi-view images became more feasible [2, 30, 56, 69, 71]. In addition, use of differentiable Monte Carlo rendering led to better shape and material estimation [18, 33, 64, 67, 73, 76], but these techniques still rely on multi-view inputs. More recent methods employ neural networks as renderers for multi-view inputs [12], bypassing computational optimization but currently lacking the performance of equation-based methods in material editing and object insertion. Like most multi-view techniques, our approach uses the rendering equation but requires only a single-view image as input.\nSingle-view inverse rendering. Unlike multi-view inverse rendering, which relies on the rendering equation, single-view inverse rendering often uses neural networks trained on large datasets to replace physically based rendering [26, 27, 29, 34, 49, 52, 79]. A neural network usually called MaterialNet, predicts material properties (albedo, roughness, metallic) and per-pixel lighting from a single image, which are then used for image synthesis. Although neural renderers perform well in specific tasks like human face relighting [41, 70], they struggle with generalist tasks where physical simulation is more accurate such as material editing and transparent object insertion. Unless trained on a well represented dataset, the neural renderers fall apart. In differentiable rendering, early studies addressed rasterization non-differentiability [10, 11], while recent work has explored inverse Monte Carlo ray tracing [14, 21, 75], similar to our physics-based optimization. However, these methods require significant priors and are limited to single objects, making them less suitable for complex scenes. Our approach also uses MatNet for single-view material property inference but incorporates a physics-based renderer.\nMaterial Editing. Material editing generally follows two categories. The first involves inferring material properties through inverse rendering, then modifying them to achieve edits [1, 25, 78]. These methods provide strong interpretability, and using a physics-based renderer makes results highly reliable, though realism may sometimes be limited. The second set is neural network-based. Following the success of Stable Diffusion [48], many image editing methods have emerged [7, 38, 55, 63]. Alchemist [53] is a representative example, trained on synthetic data to allow editing of albedo, roughness, metallic properties, and transparency. These methods benefit from the strengths of Stable Diffusion and achieve highly realistic results but have limited interpretability due to their neural network-based nature. Our approach falls into the first category with a solid interpretability, meanwhile, it enables realistic results.\nLight Estimation. Light estimation is a distinct research field [13, 16, 42, 57, 58, 60, 62, 72]. Some methods represent lighting implicitly [24, 69, 78], limiting generalizability, while others require multi-view inputs and scene mesh data [42, 60, 72]. DPI [35] is related to our approach. This method combines differential rendering with Stable Diffusion for high-quality envmap generation but relies on multi-view NeRF methods for mesh reconstruction and lacks material BRDF data, reducing accuracy. In contrast, our approach uses MatNet's output for more accurate envmap optimization.\nShadows and Ray Tracing. Shadows are crucial for photorealistic rendering. Previous work by Li et al. [29] emphasized shadow accuracy, using an OptiX-based ray tracer [43] to compute shadows for neural network input. However, shadows are often neglected in recent research [30, 69, 78]; methods based on screen-space or image-based ray tracing lack geometric occlusion and thus fail to generate accurate shadows [24, 69, 78]. Many neural renderers struggle with shadow generation [12, 79], as accurate shadows require physically based light transport, it is worth noting that this limitation has been utilized to identify AI-generated"}, {"title": "3. Method", "content": "Our material estimation and editing framework consists of four main steps. For relighting, only the geometry reconstruction and the material prediction network (Sec. 3.1) are needed. Object insertion and material editing on the other hand require all four steps, including mesh reconstruction.\nWe assume a pinhole camera model with a 35-degree field of view, $w \\times h = 512 \\times 512$ image resolution, and $c = 3$ color channels.\n3.1. Material Prediction Network\nDue to the significant success of Stable Diffusion [48] in image generation, recent work used it for the task of material property estimation, achieving impressive results [24]. However, this approach comes with high training costs and requires computationally intensive multi-step sampling during prediction. In comparison, we found that the model architecture for depth estimation [68] aligns more closely with the requirements of material prediction. We therefore adopt the DPT architecture [46], which has been highly successful in depth estimation, for our material prediction task. Different from [79], we utilize a pretrained DINOv2 [39] encoder for feature extraction. Like previous research in depth estimation [6, 68], we use two DPT decoders [46] for the regression of depth and material properties, respectively. Given the advancements in depth estimation, we initialize our training with weights from the pretrained depth model [68] to expedite the training process. For the DPT depth decoder, we limit training to the last four layers of the refinenet, while the weights of the DINOv2 encoder remain frozen throughout the training. The following loss function for $\\mathcal{L}_{MatNet}$ is used to optimize the model,\n$\\mathcal{L}_{MatNet} = \\sum_{i \\in \\{A,R,M,N,D\\}} L_i,$\\\n where the albedo loss $L_A$ combines the LPIPS perceptual loss and $L_1$ loss, while both the roughness loss $L_R$ and metallic loss $L_M$ are measured using the $L_1$ loss between the predictions and ground truth values. Finally, the $L_D$ is calculated using SiLogLoss [5, 15] to introduce logarithmic error and keep its scale constant to improve the robustness of depth prediction. This loss is defined by\n$L_D = \\mathbb{E}_{i \\in mask} ((\\log(t_i) - \\log(p_i))^2) - \\lambda (\\mathbb{E}_{i \\in mask} (\\log(t_i) - \\log(p_i)))^2,$\\\nwhere $t_i$ is the ground truth and $p_i$ is the predicted depth value for the valid pixel $i$, and we use $\\lambda = 0.5$ as the hyperparameter that balances the variance and mean terms. To prevent outliers in the depth map from causing the model to collapse during training, we set the mask to a depth value in $[0, 20]$. Finally, $L_N$ is calculated using cosine similarity.\nThe prediction process of the neural network can be expressed as\n$A_p, R_p, M_p, N_p, D_p = MatNet(I),$"}, {"title": "3.2. Progressive Differentiable Rendering", "content": "Our method assumes ambient lighting, represented by an environment map, and does not explicitly model light sources within the scene. Ignoring emitters in the scene, the rendering equation becomes the reflected radiance equation:\n$L_o(\\mathbf{x}, \\boldsymbol{\\omega}_o) = \\int_{\\Omega} f_r(\\mathbf{x}, \\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o) L_i(\\mathbf{x}, \\boldsymbol{\\omega}_i)(\\boldsymbol{\\omega}_i \\cdot \\mathbf{n}) d\\boldsymbol{\\omega}_i,$\nwhere $L_o(\\mathbf{x}, \\boldsymbol{\\omega}_o)$ is the outgoing radiance at the world space position $\\mathbf{x}$ in the direction $\\boldsymbol{\\omega}_o$, $f_r(\\mathbf{x}, \\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o)$ is the bidirectional reflectance distribution function (BRDF) at $\\mathbf{x}$, representing the ratio of reflected radiance in the direction $\\boldsymbol{\\omega}_o$ to irradiance from the direction $\\boldsymbol{\\omega}_i$, while $L_i(\\mathbf{x}, \\boldsymbol{\\omega}_i)$ is the incident radiance at $\\mathbf{x}$ from the direction $\\boldsymbol{\\omega}_i$, $\\mathbf{n}$ is the surface normal at $\\mathbf{x}$, and $\\Omega$ is the hemisphere around $\\mathbf{n}$ covering all directions of incidence. We let $S(\\mathbf{x}) = \\mathbf{x}_s$, denote the transformation from world space to screen space coordinates. Detailed steps are in the supplementary material.\nSimplified DisneyBRDF. To model surface scattering we use the DisneyBRDF [8, 9] for our implementation. During the optimization process, we do not model sheen, clearcoat, and glass, so the BRDF in Eq. 4 can be written\n$f_r(\\mathbf{x}, \\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o) = f_s + (1 - M(\\mathbf{x}_s))f_d$\nwith the specular ($f_s$) and diffuse ($f_d$) terms defined by\n$f_s = \\frac{F_s (\\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o; \\eta)D_s(\\mathbf{h}; R(\\mathbf{x}_s))G_s(\\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o; R(\\mathbf{x}))}{4 |\\mathbf{n} \\cdot \\boldsymbol{\\omega}_i| |\\mathbf{n} \\cdot \\boldsymbol{\\omega}_o|},$\n$f_d = \\frac{A(\\mathbf{x}_s)}{\\pi} F_d(\\mathbf{z}) F_d(\\boldsymbol{\\omega}_o),$\nwhere $F$ is the Fresnel reflectance, which here uses the half vector $\\mathbf{h} = (\\boldsymbol{\\omega}_i + \\boldsymbol{\\omega}_o)/||\\boldsymbol{\\omega}_i + \\boldsymbol{\\omega}_o||$ and models the fraction of light reflected from the surface as a function of material properties, including the relative index of refraction $\\eta$. We use the modified Schlick approximation [8]:\n$F_s(\\boldsymbol{\\omega}_i, \\boldsymbol{\\omega}_o; \\eta) = C_0(\\eta) + (1 - C_0(\\eta))(1 - \\mathbf{h} \\cdot \\boldsymbol{\\omega})^5$\n$C_0(\\eta) = R_0(\\eta)(1 - M(\\mathbf{x}_s)) + M(\\mathbf{x}_s)A(\\mathbf{x}_s),$\nwhere $R_0(\\eta)$ is the Fresnel reflectance of a dielectric at normal incidence. Similarly, for the diffuse term,\n$F_d(\\boldsymbol{\\omega}) = 1 + (F_{D90} - 1)(1 - \\mathbf{n} \\cdot \\boldsymbol{\\omega})^5$\n$F_{D90} = 0.5 + 2R(\\mathbf{x}_s) (\\mathbf{h} \\cdot \\boldsymbol{\\omega})^2.$\nRegarding the other terms, $D_s$ is the microfacet normal distribution function, while $G_s$ is the microfacet shadowing-masking function [51]. We use the GGX distribution [61] for these terms due to its simplicity, with just one roughness parameter $R(\\mathbf{x}_s)$, while being able to fit empirical data well.\nEnvironment Map Optimization. We optimize the environment map $E$ (envmap) using a position-embedded multilayer perceptron (MLP), similar to the approach used in NeRF [37]. Although optimizing $E$ directly without using a neural network is theoretically possible, this often yields suboptimal results. Standard MLPs are generally insensitive to high-frequency information, making pixel-level optimization of the image difficult; CNNs encounter similar limitations. The envmap is obtained using\n$E = MLP_{env} (\\gamma(\\mathbf{e}); \\theta_{env}),$\nwhere $\\gamma(\\mathbf{e})$ is a positional encoding function and $\\theta_{env}$ is the set of parameters for the MLP. We use Softplus after the final layer in $MLP_{env}$ to ensure that the output has only positive values. The envmap $E$ and the input image $e$ are both in $\\mathbb{R}^{(w \\times h)\\times c}$ with the input image being initially noise. In a differentiable rendering of the geometry $D_p$, $N_p$ based on Monte Carlo integration of Eq. 4, we evaluate the BRDF using $A_p$, $R_p$, $M_p$ (Eq. 5) and the incident radiance using\n$L_i(\\mathbf{x}, \\boldsymbol{\\omega}_i) = V(\\mathbf{x}, \\boldsymbol{\\omega}_i)L_E(\\boldsymbol{\\omega}_i),$"}, {"title": "5. Ablations", "content": "We tested the results of envmap optimization under two conditions: providing MatNet predictions versus setting $A = R = 0.5, M = 0.1$. As shown in Table 3, providing MatNet predictions significantly improves the accuracy of envmap optimization.\nWe also conducted an ablation study on the value of $\\delta$ in Eq. 19. Using 20 randomly selected images from the IIW dataset [4], we tested different $\\delta$ values and compared the rerendered images with the input images. We evaluated two strategies: simultaneous optimization of ARM and sequential optimization (RM&A), where RM is optimized first, followed by A. To save time, we applied early stopping, halting optimization if $\\mathcal{L}_{re}$ reduced by less than 5% over 20 consecutive steps. Results are shown in Table 4. For additional studies, see the supplementary material."}, {"title": "6. Limitations and Discussion", "content": "Since our images are rendered and mesh reconstruction limitations can cause artifacts along object edges, using super-resolution (SR) [48, 59, 65, 66] may help reduce these artifacts. However, SR models often overly modify the image, as shown in Fig. 12. Improving SR models by giving additional input could potentially mitigate this issue.\nFinally, physically based in this paper refers to physically based rendering (PBR), a computer graphics approach that may not strictly follow physical laws. For more general limitations, please refer to the supplementary material."}, {"title": "7. Conclusion", "content": "In this work, we present a novel pipeline for physically-based inverse rendering that combines neural network predictions with differentiable rendering to optimize material properties and lighting conditions from a single image. Our approach enables accurate relighting, transparent object insertion, and material editing tasks, including transparency editing without requiring complete geometry."}, {"title": "8.1. Optimize SH coefficients instead of envmap", "content": "We tested using SH coefficients instead of envmaps and found that optimizing SH coefficients took longer to reach the same loss. We use 3rd-order SH coefficients to represent the light source. Given the ground truth material properties, we perform light source optimization under different lighting conditions. The time taken by each method to achieve the same  $\\mathcal{L}_{re}$  is compared. The results is shown if Table 5."}, {"title": "8.2. Optimization Steps and Rerendering error", "content": "We analyzed the relationship between optimization steps and rerender error. We randomly selected 10 images from the InteriorVerse dataset [78] to test rerendering error at different optimization steps. Optimization on an RTX 3090 runs at 2 steps per second. Based on MatNet's accuracy, satisfactory results are typically achieved after 10 minutes of optimization. Results are shown in Table. 6."}, {"title": "8.3. Optimization Network", "content": "We experimented with using a CNN-based UNet for Material Properties optimization. The UNet architecture includes 2 downsampling blocks and 2 upsampling blocks, with each block containing two 2D convolution layers (kernel size = 3, padding = 1). Additionally, we compared direct optimization of material properties without a neural network. As shown in Fig. 13, direct optimization was the slowest, followed by the CNN-based UNet, while the position-embedded MLP achieved the fastest optimization speed.\nAdditionally, due to the characteristics of convolutional neural networks, noticeable artifacts may appear during optimization. As shown in Fig. 14, the optimized images show a regular pattern of black dots.\nIt is worth noting that when MatNet predictions are unreliable (e.g., for roughness and metallic), directly optimizing material properties without relying on the neural network can achieve lower  $\\mathcal{L}_{re}$  given sufficient optimization time (such as more than 3000 optimization steps). Table 7 shows the minimum MSE render loss achievable by both methods in the absence of roughness and metallic predictions."}, {"title": "8.4. Optimizing Normal", "content": "As shown in Fig. 15, optimizing material properties while simultaneously optimizing normals often leads to convergence difficulties. To address this, directly optimizing normals without relying on a neural network can help mitigate these issues."}, {"title": "9. Limitations", "content": "In this section, we discuss potential limitations of our proposed method from a broader, user-oriented perspective. It is important to note that some points, such as the need for per-image optimization, are not typically viewed as limitations within the differentiable rendering field. However, from a user's viewpoint, this may make our approach less convenient than stable diffusion-based methods [7, 53].\n1.  As with all differentiable rendering methods, our approach requires optimization for each image to obtain the lighting and optimized material properties, which might be quite time-consuming. Depending on MatNet's prediction accuracy, optimization takes between 5 to 30 minutes. For out-of-domain images (i.e., non-indoor scenes) or higher-quality requirements, optimization time may increase. This limits our method's ability to achieve the rapid, batch image editing enabled by stable diffusion-based techniques [7]. Training MatNet on a broader dataset for more accurate predictions could help mitigate this limitation.\n2. Our method uses an envmap to represent lighting, complicating accurate modeling of physical light sources in the scene. If a light source is present in the original image, optimization may fix it within the albedo, leading to inaccuracies during relighting.\n3. The introduction of differentiable rendering allows our method to perform well even on out-of-domain images. However, if MatNet predictions are poor, differential rendering optimization requires careful tuning for each image, such as adding a mask to the object to be edited or experimenting with different optimization strategies.\n4. For Material Transparency Editing, complex object geometry can lead to inaccurate refraction distortions, as our method assumes only two refractions. Single-view mesh reconstruction [31, 32, 54] may yield more accurate refractions for complex shapes. Besides, since we simulate refraction without the rays truly passing through the object, shadows cast by transparent objects may lack accuracy, and does not have caustic effect. (Note that this problem does not exist for object insertion tasks with complete geometry.)\n5. Due to mesh reconstruction limitations, artifacts may appear along object edges during strong relighting, as the mesh is discontinuous at these edges. This can be mitigated using super resolution.\n6. For images with transparent objects, super-resolution often yields unreliable results and may require multiple samples to reach an acceptable outcome, as shown in Fig. 16. (This is not a limitation of our method, but is noted here due to the potential need for SR techniques.)"}, {"title": "10. Refraction Length Prediction", "content": "Refraction plays a critical role in visual accuracy in physically based rendering. Our pipeline is fine tuned for single view material and geometric estimation. This however means that information behind a reflective/refractive interface is intractable from a single view without any additional geometric information. Furthermore, this also means that objects cannot be edited accurately when secondary interfaces are crucial to the visual fidelity. To overcome this issue we train another neural network essential to predict secondary refractive interfaces if a user wants to change any material to transparent object such as glass, water etc. We describe this pipeline in this section."}, {"title": "10.1. Dataset Preparation", "content": "To create a dataset to predict the secondary interface distance, we collected a set of 200 environment maps, 50 triangle meshes of objects commonly found in the indoors environments like cups, fruits etc. Next we predefined a set of 200 viewpoints that generally covers the entire exterior surface of an object.\nThe data collection was done in 2 parts, first the set of 200 environment maps were rendered without any object in the scene which illustrates the background of the scene. This dataset was 200 (Environment maps) \u00d7 200 (Viewpoints) images in quantity. The second part of the data collection was done in scenes without an environment map, where the objects were placed in the center of the scene and the 200 viewpoints were used to capture the distance to the secondary interface if the object were to be transparent. The distances can be generally defined as,\n$d =\\begin{cases}\n||P_{n+1} - P_{n} ||, & \\text{if } \\boldsymbol{\\omega} \\cdot \\mathbf{n} > 0 \\& \\text{hollow object} \\\\\n||P_{n+1} - P_{n} ||, & \\text{if } \\boldsymbol{\\omega} \\cdot \\mathbf{n} < 0 \\& \\text{non-hollow object} \\\\\n0, & \\text{Otherwise}\n\\end{cases}$,\nwhere,\n*   $P_n$: is the n-th intersection point in 3D world coordinates,\n*   $\\boldsymbol{\\omega} \\cdot \\mathbf{n}$ : is the cosine term of the ray direction  $\\boldsymbol{\\omega}$  and the normal  $\\mathbf{n}$.\nFurthermore, \"hollow\" objects represent things like water jugs with air in between the surfaces and \"non-hollow\" objects are things like lemon with no air inside the object.\nExamples are shown in Fig. 17."}, {"title": "11. Material Editing Settings", "content": "The selected images are out-of-domain for the indoor dataset, resulting in MatNet predictions where only albedo is relatively accurate, while roughness and metallic predictions are poor and unsuitable as initial conditions for optimization.\nTherefore, specific optimization conditions must be set for each image to achieve satisfactory results.\nCherry. The predicted roughness and metallic are relatively inaccurate, leading to excessive red light in the envmap if directly used for optimization. To address this, we initialize the envmap as pure white and start with material property optimization. The default initial values for roughness and metallic are set to 0.5. After optimizing roughness and metallic, we proceed to envmap optimization, followed by albedo optimization, as albedo predictions are more reliable.\nBottle. MatNet predictions are relatively accurate, and optimization follows the method described in the main paper.\nCharizard and Vase. Similar to Cherry, we initialize the envmap as pure white and start with material property optimization. Roughness and metallic are initialized to 0.5, while albedo uses MatNet predictions. Optimization begins with roughness and metallic, followed by envmap optimization, and finally albedo optimization. During editing, setting all mask values uniformly for metallic without adjusting roughness can introduce artifacts. To improve material editing results, we use a mask during optimization. Specifically, SAM2 is used to segment Charizard's and vase's mask, roughness and metallic values are unified within the mask during optimization. This ensures better editing results."}, {"title": "12. Material Editing User Study", "content": "We conducted a user study on material editing results, asking participants to rank the editing results from Alchemist [53], ours (direct rendering), and ours SR (rendered result refined with Super Resolution). The highest rank received 3 points, and the lowest 1 point. Since these are real-world photos with no ground truth, participants were provided with synthetic scene renderings as reference and asked to imagine the expected edit on real photos as closely as possible. It should be noted that participants generally found it challenging to envision the correct editing result, making it difficult to judge which method was better. When uncertain, participants often zoomed in to examine details for ranking, which may have led to lower scores for our direct rendering. However, our scores improved significantly after applying super resolution."}, {"title": "13. Coordinate Transformation", "content": "Since our input into the renderer are based in screen space, we briefly describe how to transform a world-position x into screen-space. Extending x into homogeneous coordinates W = [x, y, z, 1]T, we first transform into camera space coordinates by\n$Q = E^{-1} \\cdot W$\nwhere E is the extrinsic camera matrix. For a perspective projection matrix P we can obtain the Normalized Device Coordinates (NDC) by:\n$N = P \\cdot Q$\nConverting N into inhomogeneous coordinates we can obtain the screen-space coordinates:\n$X_{screen} = (\\frac{X_{ndc} + 1}{2}) \\cdot W$\n$Y_{screen} = (\\frac{Y_{ndc} + 1}{2}) \\cdot H$\nThis process maps the world position to a specific location on the screen which we will denotes as xs = S(x) = [Xscreen, Yscreen]. In the following A, R and M represents the image texture values of albedo, roughness and metallic. Thus for a shading point x, we can obtain the material properties with screen coordinate indexing i.e. A(xs), R(xs) and M(xs)."}]}