{"title": "R2-GUARD: ROBUST REASONING ENABLED LLM GUARDRAIL VIA KNOWLEDGE-ENHANCED LOGICAL REASONING", "authors": ["Mintong Kang", "Bo Li"], "abstract": "As large language models (LLMs) become increasingly prevalent across various applications, it is\ncritical to establish safety guardrails to moderate input/output of LLMs and ensure compliance with\nsafety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety\ncategories (e.g., \u201cself-harm\u201d, \u201cself-harm/instructions\") independently and fail to explicitly capture\nthe intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate\ntraining on long-tail data from correlated safety categories, susceptibility to jailbreak attacks, and\ninflexibility regarding new safety categories. To address these limitations, we propose R2-Guard, a\nrobust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically,\nR2-Guard comprises two parts: the data-driven category-specific learning and reasoning components.\nThe learning component provides unsafety probabilities of input on different safety categories. We\nthen encode safety knowledge among different categories as first-order logical rules and embed them\ninto a probabilistic graphic model (PGM) as the reasoning component. The unsafety probabilities of\ndifferent categories from data-driven models are sent to the reasoning component for final inference.\nWe employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs),\nand optimize PCs to achieve precision-efficiency balance via improved graph structure. We also\npropose different methods to optimize the weights of knowledge. To further perform stress tests, we\nemploy a pairwise construction method to construct a new safety benchmark TwinSafety, which\nfeatures principled categories and presents new challenges for guardrail models. We show that\nR2-Guard is effective even given unrepresentative categories or challenging jailbreak prompts. We\ncompare R2-Guard with eight strong guardrail models on six safety benchmarks, and demonstrate\nthe robustness of R2-Guard against four SOTA jailbreak attacks. R2-Guard significantly surpasses\nLlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreak attacks. We further reveal that\nR2-Guard can effectively adapt to unseen safety categories by simply editing the reasoning graph.", "sections": [{"title": "1 Introduction", "content": "LLMs have recently been deployed in diverse applications, such as chatbots [64, 6], virtual agents [10, 62], and code\nassistants [43, 27]. Given the widespread deployment and extensive interaction with human users, it is imperative\nto ensure that both the input and output of these LLM systems adhere to safety regulations. The regulations include\ngovernment policies like the EU AI Act [13], White House AI Executive Order [49], and industry policies like OpenAI's\nusage policy [36] and Meta's service terms [34]. The safety policies address a wide spectrum of risks, ranging from\npersonal dangers like self-harm and sexual content to societal threats like privacy breaches and group hatred.\nConsiderable efforts are undertaken during different LLM stages to ensure compliance with safety regulations. During\nthe training phase, reinforcement learning from human feedback (RLHF)[37, 38] fine-tunes LLMs to align with\nhuman preferences and conform to regulatory standards. However, RLHF requires substantial computational and\nhuman resources [18] and only functions in the LLM output space. During the inference phase, guardrail models"}, {"title": "2 Related work", "content": "Guardrail models moderate both the input and output content of LLMs to assess the likelihood that the content is\nunsafe. If this likelihood surpasses a predetermined threshold, a corrective action is automatically triggered. Existing\nguardrail models can be classified into several categories: (1) industry APIs from Detoxify [2], Perspective [24],\nAzure [1], and OpenAI [31], (2) fine-tuned guardrail models LlamaGuard [17], ToxicChat-T5 [26], ToxDectRoberta\n65], sentence transformer guardrail [4], and GPT-based guardrail [29], (3) LLM-based guardrail models via prompt\nengineering [23, 54] or constrained dialogue path (Nemo Guardrail) [39], and (4) statistical model fitting such as KNN\nguardrail [59] and Beta regression guardrail [48]. These guardrail models learn the safety knowledge from human\nannotations in a purely data-driven manner, leading to oversights in capturing the internal correlations among various\nsafety categories and vulnerability to jailbreaks. In contrast, R2-Guard explicitly encodes the safety knowledge into\nPGMs and performs logical inference via PGMs to create an effective, robust, and flexible guardrail model."}, {"title": "3 R2-Guard: Robust reasoning enabled LLM guardrail", "content": "R2-Guard enhances the safety of LLMs by providing an effective, robust, and flexible guardrail model. In Section 3.1,\nwe introduce the setup of guardrail models and present an overview of R2-Guard as an effective guardrail framework\nthrough logical inference using probabilistic graphical models (PGMs). In Section 3.2, we employ Markov logical\nnetworks (MLNs), a type of PGM, to encode safety knowledge rules and demonstrate how R2-Guard flags unsafe\ncontents via probabilistic inference on MLNs. In Section 3.3, we explore another type of PGM, probabilistic circuits\n(PCs), and optimize the reasoning graph structure to balance reasoning accuracy and computational efficiency. In\nSection 3.4, we propose two methods for optimizing knowledge weights in R2-Guard, pseudo learning on simulation\ndata and real learning on realistic data samples."}, {"title": "3.1 Overview of R2-Guard", "content": "Guardrail models take any input or output prompt of LLMs as input and compute the probability that the prompt is\nunsafe. If the probability of unsafety exceeds a predetermined level, a corrective action can be triggered to safeguard the"}, {"title": "4 Challenging safety benchmark TwinSafety", "content": "Standard safety benchmarks [31, 26, 42, 46, 19] typically focus on various aspects of unsafety (e.g., \"self-harm\",\n\"sexual\", \"violence\"). However, they often overlook broader moderation challenges posed by different hierarchy levels\nof unsafe text data: (1) paragraph-level: variations in paragraph patterns that can obscure malicious intent within long\nor mismatched contexts, (2) phrase-level: proverbs or double entendres that convey unsafe intentions using benign\nlanguage, and (3) word-level: special nouns such as the names of malicious media.\nTo provide a more comprehensive and challenging stress test for existing guardrail models and R2-Guard, we propose a\nchallenging safety benchmark, TwinSafety. This benchmark includes all hierarchy levels of maliciousness\u2014paragraph-level, phrase-level, and word-level\u2014and features a pairwise construction method with novel categories. Specifically,\nwe construct comprehensive categories including intent-hiding, mismatched-context, proverb-usage, double-entendre,\nmedia-usage, science-purpose, and virtual-context. We construct pairs of safe and unsafe prompts with minimal token\ndifferences but significant semantic gaps in terms of unsafety. The categories and examples are provided in Table 1.\nThe evaluation results in Table 2 demonstrate that TwinSafety introduces new challenges for different guardrail models\ncompared to other standard safety benchmarks."}, {"title": "5 Evaluation", "content": "In this section, we present the evaluation results of R2-Guard. Concretely, we evaluate R2-Guard on six safety datasets,\nincluding (1) five standard safety datasets (OpenAI Mod [31],ToxicChat [26], XSTest [42], Overkill [46], Beaver Tails\n[19]) and (2) our novel safety dataset TwinSafety. We consider the SOTA guardrail models, including (1) industry\nmoderation APIs from Detoxify [2], Perspective [24], Azure [1], and OpenAI [31], (2) fine-tuned guardrail model\nLlamaGuard [17] and ToxicChat-T5 [26], (3) LLM-based guardrail via chain-of-thought prompting (CoT) [54], and\n(4) ensemble-learning based guardrail models [14, 60]. We also evaluate the robustness of R2-Guard against SOTA\njailbreak methods including GCG [66], PAIR [5], TAP [33], and AutoDAN [28].\nAs a summary, we find that (1) R2-Guard consistently outperforms SOTA guardrail models on various safety datasets,\n(2) R2-Guard demonstrates much higher resilience against SOTA jailbreak algorithms compared to other guardrail\nmodels, (3) the pseudo-learning algorithm with only simulation data performs on par with the real learning algorithm,\nenabling efficient training of R2-Guard, and (4) R2-Guard can be adaptable to new safety categories by simply editing\nthe reasoning graph.\nCodes and data are provided at https://github.com/kangmintong/R-2-Guard."}, {"title": "5.1 R2-Guard outperforms SOTA guardrail models", "content": "Experiment setup. We evaluate the guardrail models on six datasets including five standard safety datasets OpenAI\nMod [31],ToxicChat [26], XSTest [42], Overkill [46], BeaverTails [19] and our new safety dataset TwinSafety,\nintroduced in Section 4. We consider four types of strong guardrail models as baselines: (1) industry guardrail APIs\nfrom Detoxify [2], Perspective [24], Azure [1], and OpenAI Mod [31], (2) fine-tuned guardrail model LlamaGuard\n[17] and ToxicChat-T5 [26], (3) LLM-based guardrail model via chain-of-thought prompting (CoT) [54], and (4)\nensemble-learning based guardrail models [14, 60]. We directly evaluate the likelihood of unsafety by different APIs.\nWe keep the default prompt template and parameters in Llamaguard and ToxicChat-T5. We use Llama2-7b-chat as the\ninference model for CoT and carefully select 3 representative examples from corresponding datasets and manually write\nthe reasoning process as demonstrations. Ensemble learning takes the maximal unsafety scores of category-specific\nlearning models for different categories as the prediction. We use the category-specific learning models from OpenAI\nMod, LlamaGuard, and ToxicChat-T5 since they demonstrate high guardrail performance empirically. R2-Guard\nleverages the same category-specific learning models as the ensemble learning to construct the category-specific learning\ncomponent and performs logical inference on PGMs, as illustrated in Section 3. We consider both the MLN inference\nin Section 3.2 and PC inference in Section 3.3 and refer to them as R2-Guard (MLN) and R2-Guard (PC). Following\nliterature[17, 31, 26], We leverage AUPRC as the metric to evaluate the ability of guardrail models to discriminate\nbetween safe and unsafe prompts."}, {"title": "5.2 R2-Guard is robust against SOTA jailbreaks", "content": "Experiment Setup. Jailbreak attacks aim to bypass the detection mechanisms of guardrail models by injecting\noptimized strings. Therefore, it is crucial to evaluate the robustness of guardrail models against these attacks to ensure\nthe security of LLM systems. We consider three types of SOTA jailbreak attack algorithms: (1) the white-box adaptive\nattack GCG [66], which optimizes an adversarial suffix via token gradients; (2) the black-box attack AutoDAN [28],\nwhich leverages genetic algorithms to optimize jailbreak prompts from a pool of seed prompts; and (3) the black-box\nLLM-based jailbreak algorithms PAIR [5] and TAP [33], which prompt LLMs to generate and refine jailbreak prompts\nthrough feedback from target models. Since GCG is a white-box attack, we cannot access the model weights for API-based guardrail models such as OpenAI Mod. Therefore, we consider three types of strong GCG-optimized adversarial\nsuffixes on surrogate models: (1) universal strings optimized to jailbreak multiple LLMs (GCG-U1, GCG-U2); (2)\njailbreak strings against the safety-aligned LLM Vicuna-7B (GCG-V) and the SOTA guardrail model LlamaGuard\n(GCG-L); and (3) jailbreak strings optimized against the distilled Gemma-2B model of R2-Guard (GCG-R). Following\nthe literature [28, 5, 33], we evaluate the robustness of the guardrail models using AdvBench [66], which consists\nsolely of unsafe prompts, and measure the unsafety detection rate (UDR), the portion of flagged unsafe prompts with\nthreshold 0.5 (i.e., the prompt is recognized as unsafe if the unsafety probability exceeds 0.5). Additional details are\nprovided in Appendix A.1."}, {"title": "5.3 Ablation studies", "content": ""}, {"title": "5.3.1 Pseudo learning and real learning", "content": "In Section 3.4, we introduce two methods for optimizing knowledge weights: pseudo\nlearning on simulation data and real learning on realistic data samples. We empirically\ncompare these weight learning methods using the ToxicChat and BeaverTails datasets,\nwhich include training sets for real learning. The results, presented in Figure 2,\nreveal that pseudo learning performs on par with real learning. This demonstrates\nthat in-distribution training samples are not essential for the R2-Guard framework,\nhighlighting its generality."}, {"title": "5.3.2 Effectiveness on unseen safety categories", "content": "R2-Guard can be adaptable to new categories by adding the corresponding category-\nspecific learning models and modifying the reasoning component to include safety\nknowledge related to the new categories. In the evaluation, we consider four sequentially\nadded safety categories: hate (H), sexual (S), harassment (HR), and violence (V).\nCorrespondingly, we have four types of category-specific learning models, which are\nalso added sequentially. We evaluate the performance of R2-Guard with data samples\nrelated to the four safety categories with sequentially added learning models. The results\nin Figure 4 show that R2-Guard can flexibly adapt to new safety categories effectively\n(i.e., high AUPRC in the lower triangle of Figure 4)."}, {"title": "6 Discussion", "content": "One limitation of R2 \u2013Guard is its requirement for explicit specification of safety knowledge rules in PGMs, necessitating\nhuman effort to annotate detailed safety categories and their interconnections. However, this explicit knowledge also\nenhances R2-Guard's effectiveness and robustness compared to purely data-driven guardrail models. R2-Guard\nhas a broader impact in three key areas: 1) motivating the guardrail community to transition from purely data-driven approaches to those enabled by logical reasoning, 2) providing the symbolic reasoning community with a robust framework for encoding knowledge, performing logical inference, and knowledge weight learning with weak supervision, and 3) safeguarding widespread LLM deployments in various systems, We do not see any negative impact of our guardrail model."}, {"title": "A Evaluation", "content": ""}, {"title": "A.1 Implementation details", "content": "GCG-U1 and GCG-U2. These are two universal jailbreaks optimized with GCGC on multiple models and show\nsuperior transferability to GPT-4. Concretely, GCG-U1 is optimized on Vicuna-7B, Vicuna-13B, Guanaco-7B, and\nGuanaco-13B. GCG-U2 is optimized on Vicuna-7B, Vicuna-13B, Guanaco-7B, and Guanaco-13B.\nGCG-R. The jailbreak is optimized with GCG on a distilled Gemma-2b model from our R2-Guard. We perform the\ndistillation on six standard safety datasets in Section 5.1. We apply the prompt template same as LlamaGuard and use\nthe token probability of \"safe\" and \"unsafe\" as the prediction.\nWe will check the identity of users after we release our dataset TwinSafety (e.g., through application forms on\nHuggingFace).\nAll the results are averaged across 3 runs with different randomness seeds. We use one RTX A6000 to run all the\nexperiments.\nWe provide the codes to reproduce all the results in the supplementary material."}, {"title": "A.2 R2-Guard under SOTA jailbreaks", "content": "We evaluate UDRs against PAIR and TAP in Table 4, which shows that the UDR of R2-Guard is decreased but remains\nmuch higher than UDRs of other models. This reduction is because PAIR and TAP may reformulate the original prompt\nso that the modified prompt is semantically less harmful (e.g., reformulating \"grab the gun\" to \"grab the water gun\"),\nwhich highlights the need for future work to develop a fairer benchmark in this scenario."}, {"title": "A.3 MLN reasoning vs. PC reasoning", "content": "We compare the effectiveness and efficiency of logical reasoning with MLNs and that with PCs. The results in Table 5\nshow that PC reasoning achieves comparable performance in content moderation while requiring only 6% of the\ninference time needed for MLN reasoning."}]}