{"title": "R2-GUARD: ROBUST REASONING ENABLED LLM GUARDRAIL VIA KNOWLEDGE-ENHANCED LOGICAL REASONING", "authors": ["Mintong Kang", "Bo Li"], "abstract": "As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., \u201cself-harm\u201d, \u201cself-harm/instructions\") independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreak attacks, and inflexibility regarding new safety categories. To address these limitations, we propose R2-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, R2-Guard comprises two parts: the data-driven category-specific learning and reasoning components. The learning component provides unsafety probabilities of input on different safety categories. We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) as the reasoning component. The unsafety probabilities of different categories from data-driven models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for guardrail models. We show that R2-Guard is effective even given unrepresentative categories or challenging jailbreak prompts. We compare R2-Guard with eight strong guardrail models on six safety benchmarks, and demonstrate the robustness of R2-Guard against four SOTA jailbreak attacks. R2-Guard significantly surpasses LlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreak attacks. We further reveal that R2-Guard can effectively adapt to unseen safety categories by simply editing the reasoning graph.", "sections": [{"title": "1 Introduction", "content": "LLMs have recently been deployed in diverse applications, such as chatbots [64, 6], virtual agents [10, 62], and code\nassistants [43, 27]. Given the widespread deployment and extensive interaction with human users, it is imperative\nto ensure that both the input and output of these LLM systems adhere to safety regulations. The regulations include\ngovernment policies like the EU AI Act [13], White House AI Executive Order [49], and industry policies like OpenAI's\nusage policy [36] and Meta's service terms [34]. The safety policies address a wide spectrum of risks, ranging from\npersonal dangers like self-harm and sexual content to societal threats like privacy breaches and group hatred.\nConsiderable efforts are undertaken during different LLM stages to ensure compliance with safety regulations. During\nthe training phase, reinforcement learning from human feedback (RLHF)[37, 38] fine-tunes LLMs to align with\nhuman preferences and conform to regulatory standards. However, RLHF requires substantial computational and\nhuman resources [18] and only functions in the LLM output space. During the inference phase, guardrail models"}, {"title": "2 Related work", "content": "Guardrail models moderate both the input and output content of LLMs to assess the likelihood that the content is\nunsafe. If this likelihood surpasses a predetermined threshold, a corrective action is automatically triggered. Existing\nguardrail models can be classified into several categories: (1) industry APIs from Detoxify [2], Perspective [24],\nAzure [1], and OpenAI [31], (2) fine-tuned guardrail models LlamaGuard [17], ToxicChat-T5 [26], ToxDectRoberta\n[65], sentence transformer guardrail [4], and GPT-based guardrail [29], (3) LLM-based guardrail models via prompt\nengineering [23, 54] or constrained dialogue path (Nemo Guardrail) [39], and (4) statistical model fitting such as KNN\nguardrail [59] and Beta regression guardrail [48]. These guardrail models learn the safety knowledge from human\nannotations in a purely data-driven manner, leading to oversights in capturing the internal correlations among various\nsafety categories and vulnerability to jailbreaks. In contrast, R2-Guard explicitly encodes the safety knowledge into\nPGMs and performs logical inference via PGMs to create an effective, robust, and flexible guardrail model."}, {"title": "3 R2-Guard: Robust reasoning enabled LLM guardrail", "content": "R2-Guard enhances the safety of LLMs by providing an effective, robust, and flexible guardrail model. In Section 3.1,\nwe introduce the setup of guardrail models and present an overview of R2-Guard as an effective guardrail framework\nthrough logical inference using probabilistic graphical models (PGMs). In Section 3.2, we employ Markov logical\nnetworks (MLNs), a type of PGM, to encode safety knowledge rules and demonstrate how R2-Guard flags unsafe\ncontents via probabilistic inference on MLNs. In Section 3.3, we explore another type of PGM, probabilistic circuits\n(PCs), and optimize the reasoning graph structure to balance reasoning accuracy and computational efficiency. In\nSection 3.4, we propose two methods for optimizing knowledge weights in R2-Guard, pseudo learning on simulation\ndata and real learning on realistic data samples."}, {"title": "3.1 Overview of R2-Guard", "content": "Guardrail models take any input or output prompt of LLMs as input and compute the probability that the prompt is\nunsafe. If the probability of unsafety exceeds a predetermined level, a corrective action can be triggered to safeguard the"}, {"title": "Limitations of existing guardrail models.", "content": "SOTA guardrail models [17, 31, 26] are trained on a base language\nmodel with data samples and safety annotations. These guardrail models learn the safety knowledge from annotated\ntraining instances in a data-driven manner and implicitly encode the safety knowledge in model parameters. The\nparadigm potentially overlooks complex interrelationships among different safety categories, such as \u201cself-harm,\"\n\u201cself-harm/instructions,\" and \u201cself-harm/intents.\" This oversight can lead to ineffectiveness, as the models may not be\nadequately trained on long-tail data from correlated categories, and increase susceptibility to jailbreaks as there is no\nexplicit safety knowledge integrated. Furthermore, existing guardrail models demand retraining to incorporate updated\nsafety categories, demonstrating a lack of flexibility."}, {"title": "Our robust reasoning enabled guardrail model R2-Guard.", "content": "To address these limitations, we propose R2-Guard,\na robust reasoning enabled LLM guardrail via knowledge-enhanced logical inference. R2-Guard takes any LLM\ninput/output prompts as input, computes unsafety probabilities for different categories with category-specific learning\nmodels, performs explicit logical reasoning according to predefined safety knowledge, and finally calculates the\nprobability of the input being unsafe (i.e., $P[``unsafe'' = 1]$). Concretely, in the reasoning step, we first represent the\nsafety knowledge with first-order logical rules, which builds upon the target logical variable (i.e., \u201cunsafe\") and\ncategory logical variables (e.g., \"self-harm\" and \"sexual\"). The logical rules comprise both direct rules that directly\nrelate to the target logical variable (e.g., \u201cself-harm\u201d\u2192\u201cunsafe\u201d) and indirect rules that govern the relationships\namong category logical variables (e.g., \u201cself-harm/intent\u201d \u21d2 \u201cself-harm\u201d). We then compile the logical rules and\nthe associated rule weights into probabilistic graphical models (PGMs), which define a joint distribution over both\nthe target and category logical variables. This design allows us to compute the probability of unsafety by performing\nprobabilistic inference via PGMs. Notably, we consider two types of PGMs: Markov logic networks (MLNs) [40] and\nprobabilistic circuits (PCs) [8, 22, 16]. In addition, we optimize the PC graph structure to achieve an optimized balance\nof knowledge compilation precision and inference efficiency. We also offer two approaches to learning the knowledge\nweights in PGMs: pseudo-learning, which optimizes weights with only simulated scores for different category\nvariables in a self-consistent way, and real-learning, which optimizes weights with realistic data samples. R2-Guard,\nwith explicit safety knowledge rule compilation and logical reasoning, can capture complex intercorrelations among\nvarious safety categories and systematically leverage them to make the final prediction. The grounding knowledge and\nprincipled reasoning procedure enable R2-Guard to be effective, robust against jailbreak algorithms, and flexible given\nnew safety categories."}, {"title": "Challenging safety benchmark TwinSafety.", "content": "Current safety benchmarks [31, 26, 19, 42] have vague category labels\nand noisy data. We propose TwinSafety, created through a novel pairwise construction method with unique categories,\nto stress test existing guardrail models. TwinSafety evaluates unsafety semantics at various hierarchical levels, such\nas paragraph-level unsafe intention hiding, phrase-level unsafe double-entendre, and word-level unsafe media misuse."}, {"title": "Empirical evaluations.", "content": "In addition to five established standard safety benchmarks [31, 26, 42, 46, 19], we also\ncompare different guardrail models on our proposed challenging data TwinSafety. Our evaluations across six safety\nbenchmarks and comparisons with eight advanced guardrail models reveal that (1) R2-Guard consistently outperforms\nSOTA guardrail models across various datasets, (2) R2-Guard empirically demonstrates remarkable resilience against\nfour SOTA jailbreak algorithms compared to other guardrail models, (3) the pseudo-learning algorithm of R2-Guard,\nrelying solely on simulated data, performs on par with the real learning algorithm, which means R2-Guard does not\nneed a large amount of annotated training data, and (4) R2-Guard demonstrates effective adaptability to new safety\ncategories by simply modifying the PGM reasoning graph."}, {"title": "3.1 Overview of $R^2$-Guard", "content": "Guardrail models take any input or output prompt of LLMs as input and compute the probability that the prompt is\nunsafe. If the probability of unsafety exceeds a predetermined level, a corrective action can be triggered to safeguard the"}, {"title": "High-level structure of $R^2$-Guard.", "content": "To address these limitations, we propose $R^2$-Guard, a robust and reasoning\nenabled LLM guardrail. $R^2$-Guard consists of two main components: (1) a data-driven category-specific learning\ncomponent, and (2) a knowledge-enhanced reasoning component. The pipeline of $R^2$-Guard is illustrated in Figure 1.\nThe category-specific learning component takes the LLM prompt as input and computes the probability that the prompt\nfalls into different unsafe categories (e.g., the self-harm predictor assesses the likelihood that the prompt contains\nself-harm-related content). These unsafety probabilities are then forwarded to the reasoning component, which makes\nthe final prediction of the overall probability that the prompt is unsafe based on logical inference. We employ PGMs\nto implement the reasoning component. By incorporating safety knowledge into the PGMs, we perform probabilistic\ninference for the final prediction reasoning."}, {"title": "Knowledge-enhanced logical inference for guardrail in reasoning component of $R^2$-Guard.", "content": "We map the safety\nknowledge rules such as the relationships among safety categories as first-order logical rules, which are built upon\ntwo types of logical variables, the target logical variable which presents the final prediction (i.e., \u201cunsafe\u201d) and\nthe category logical variable which is realted to different safety categories (e.g., \u201cself-harm\", \u201csexual\"). $R^2$-Guard\nencodes two types of safety knowledge: (1) direct rules with the form that category logical variables implicate\nthe target logical variable (e.g., \"self-harm\" \u2192 \"unsafe\"), and (2) indirect rules that build implication logics\namong different category logical variables (e.g., \u201cself-harm/instructions\u201d \u21d2 \u201cself-harm\u201d). Each logical rule is\nassociated with a knowledge rule weight to specify the importance of the knowledge rule to the moderation task.\nThese rules are integrated into probabilistic graphical models (PGMs), employing either Markov logic networks with\ncomplete knowledge compilation (Section 3.2) or probabilistic circuits with our improved graph structure for a better\nprecision-efficiency balance (Section 3.3). Through probabilistic inference on these PGMs, the system mimics human\nlogical deduction, initially understanding the semantics and relationships among safety categories (via indirect rules)\nand subsequently deducing prompt unsafety based on all considered categories (via direct rules). $R^2$-Guard facilitates\neffective and robust detection of unsafe content through explicit logical inference based on given safety knowledge\nwhile allowing for easy adaptation to new safety categories by merely editing the PGM reasoning component."}, {"title": "3.2 $R^2$-Guard via Markov logic networks (MLNS)", "content": "MLNs [40] are a family of statistical models that define a joint distribution over a set of logical variables. This\njoint distribution is determined by predefined logical rules applied to the logical variables, each associated with a\ncorresponding weight. MLNs can compute the probability distribution over possible worlds (i.e., possible assignments\nto logical variables). When considering the probability distribution of a specific logical variable, we typically compute\nthe marginal probability by marginalizing over all other logical variables."}, {"title": "Formulations of safety knowledge rules.", "content": "In $R^2$-Guard, we consider $n$ logical variables taking binary values (i.e.,\n0 or 1), including $n \u2013 1$ category logical variables $\\{v_i\\}_{i=1}^{n-1}$ (e.g., \u201cself-harm\u201d, \u201csexual\u201d) and 1 target logical\nvariable $v_t$ (i.e., \u201cunsafe\u201d). Given any input or output LLM prompt $x$, we denote $p(x) = [p_1(x), ..., p_{n}(x)]$ as a\nconditional unsafety likelihood vector for $n$ logical variables such that $p_i(x) = P[v_i^{(i)} = 1|x]$ for $i \u2208 \\{1, ..., n \u2212 1\\}$ and\n$p_n(x) = P[v_t = 1|x]$. The unsafety likelihood vector $p$ can be computed by the data-driven category-specific learning\ncomponent and serves as the input to the reasoning component, as shown in Figure 1. Suppose that we consider $L$ direct\nand indirect logical rules $\\{R_i\\}_{i=1}^L$, each associated with a knowledge weight $w_i \u2208 R$ ($i \u2208 \\{1, 2, ..., L\\}$)."}, {"title": "Factor function of a possible world.", "content": "We define a possible world $\u03bc \u2208 M = \\{0,1\\}^n$ as a possible assignment to $n$\nlogical variables such that $\u03bc_i = v_i^{(i)}$ for $i \u2208 \\{1, .., n \u2212 1\\}$ and $\u03bc_n = v_t$. Based on it, we define the factor function of a\npossible world $F : \\{0,1\\}^n \u2192 R^+$ which takes as input a possible world $\u03bc$ and outputs the factor value of the world as"}, {"title": "Probability of unsafe content via MLN reasoning.", "content": "$R^2$-Guard eventually outputs the probability that the given\nprompt $x$ is unsafe (i.e., $P[$``unsafe'' = 1$|x]$ or $[P[\u03bc_n = 1|x]$). This requires a marginal probability computation which\nmarginalizes over all other logical variables as the following:\n$P[$``unsafe'' = 1$|x]$ = P[\u03bc_n = 1|x] =$\\frac{\\sum_{\u03bc\u2208M,\u03bc_n=1} F(\u03bc|x)}{\\sum_{\u03bc\u2208M} F(\u03bc|x)}$"}, {"title": "3.3 $R^2$-Guard via probabilistic circuits (PCs)", "content": "Although MLNs facilitate effective logical inference through marginal probability computation with factor functions,\ntheir computational complexity is $O(2^n)$. This complexity becomes impractical when dealing with a large number of\nsafety logical variables $n$. Therefore, we attempt to improve the structure of PGMs to encode safety knowledge for\nmore efficient logical inference."}, {"title": "$R^2$-Guard reasoning via PCs.", "content": "Probabilistic circuits (PCs) [8, 9, 22, 16, 7, 41] are a more expressive type of PGM\ncompared to MLNs. PCs can represent a wide range of probabilistic distributions over a set of random variables through\nsummation and multiplication operations. Structurally, PCs are organized as tree graphs, where leaf nodes represent\nindividual probabilistic distributions of variables and multi-layered internal nodes capture their interconnections. In\n$R^2$-Guard, we exploit the observation that certain safety categories exhibit low logical correlation to each other (e.g.,\n\"self-harm\u201d and \u201csexual\u201d related categories). Therefore, we apply clustering algorithms to partition category logical\nvariables and position different clusters of safety types in different layers of the PC graph, as illustrated in Figure 1. Each\nPC layer is thus able to concentrate on a specific type of safety knowledge (e.g., \u201cself-harm\u201d or \u201csexual\u201d) and perform\nlogical inference within that layer, emulating MLN inference locally as shown Equation (2). This layered design\nfacilitates a sequential reasoning process that conducts logical inference across different types of safety knowledge\nstep by step, ultimately generating a final prediction. By segregating logically less correlated categories into separate\nlayers, we reduce low-yield interactions among these logical variables, thereby enhancing inference efficiency while\nmaintaining high reasoning precision."}, {"title": "Computational complexity of PC reasoning.", "content": "Given the layerwise reasoning pattern on tree graphs, the computational\ncomplexity of PC reasoning is $O(\\sum_{i=1}^{N_k} 2^{|C_i|})$, where $| C_i |$ is the size of the i-th cluster $C_i$. Given that $\\sum_{i=1}^{N_k} |C_i| =\\newline n-1$, the complexity of PC reasoning improves from the exponential-sum order $O(2^{\\sum_{i=1}^{N_k} ||C_i||})$ (MLN reasoning\ncomplexity) to a sum-exponential order $O(\\sum_{i=1}^{N_k} 2^{|C_i|})$. In practice, the safety categories in regulations are well-defined,\nleading to generally uniform partitions across different clusters [31, 36, 17, 34]. Consequently, PC inference empirically\nintroduces significant efficiency improvements, as shown in Appendix A.3. The results in Table 5 in Appendix A.3 show\nthat PC reasoning achieves comparable performance in content moderation while requiring only 6% of the inference\ntime needed for MLN reasoning."}, {"title": "3.4 Knowledge weights learning in $R^2$-Guard", "content": "We propose two methods for learning the weights of knowledge rules (i.e., $\\{w_i\\}_{i=1}^L$) within the $R^2$-Guard framework,\ntailored to different scenarios: (1) pseudo learning, which optimizes the weights using simulated scores in the absence\nof real training samples, and (2) real learning, which optimizes the weights using realistic unsafety scores derived from\nrealistic training samples.\nFor pseudo learning, we first simulate the training data by uniformly sampling the unsafety scores for different unsafety\ncategories. If two unsafety categories have internal implications (e.g., \u201cself-harm/instructions\u201d \u2192 \u201cself-harm\u201d),\nwe reject samples that violate the implication with a threshold of 0.5. For instance, we reject a sample if\n$P[$``self-harm/instructions'' = 1$] > 0.5$ and $P[$``self-harm'' = 1$] < 0.5$. We assign an unsafety label of 1 to an\ninstance if the maximum category unsafety score exceeds 0.5 (i.e., if the sampled unsafety score for any category\nexceeds 0.5, the unsafety label is 1); otherwise, we assign a label of 0. We then optimize the knowledge weights\nby minimizing the binary cross-entropy (BCE) loss between the predictions made by $R^2$-Guard and the simulated\nunsafety labels. In the real learning scenario, we use actual training samples to compute unsafety scores with data-driven\ncategory-specific learning models. We then train the knowledge weights using these unsafety scores and the ground\ntruth labels, again minimizing the BCE loss.\nPseudo-learning does not require real training data samples, offering an annotation-free training paradigm and allowing\nthe learned weights to generalize effectively across different domains. In contrast, real learning can capture intercorrela-\ntions among different unsafety categories within the realistic distribution, resulting in performance improvement on\nin-distribution data samples."}, {"title": "4 Challenging safety benchmark TwinSafety", "content": "Standard safety benchmarks [31, 26, 42, 46, 19] typically focus on various aspects of unsafety (e.g., \u201cself-harm\u201d, \u201csexual\u201d, \u201cviolence\u201d). However, they often overlook broader moderation challenges posed by different hierarchy levels of unsafe text data: (1) paragraph-level: variations in paragraph patterns that can obscure malicious intent within long or mismatched contexts, (2) phrase-level: proverbs or double entendres that convey unsafe intentions using benign language, and (3) word-level: special nouns such as the names of malicious media.\nTo provide a more comprehensive and challenging stress test for existing guardrail models and $R^2$-Guard, we propose a challenging safety benchmark, TwinSafety. This benchmark includes all hierarchy levels of maliciousness\u2014paragraph-level, phrase-level, and word-level\u2014and features a pairwise construction method with novel categories. Specifically, we construct comprehensive categories including intent-hiding, mismatched-context, proverb-usage, double-entendre, media-usage, science-purpose, and virtual-context. We construct pairs of safe and unsafe prompts with minimal token differences but significant semantic gaps in terms of unsafety. The categories and examples are provided in Table 1. The evaluation results in Table 2 demonstrate that TwinSafety introduces new challenges for different guardrail models compared to other standard safety benchmarks."}, {"title": "5 Evaluation", "content": "In this section, we present the evaluation results of $R^2$-Guard. Concretely, we evaluate $R^2$-Guard on six safety datasets, including (1) five standard safety datasets (OpenAI Mod [31],ToxicChat [26], XSTest [42], Overkill [46], Beaver Tails [19]) and (2) our novel safety dataset TwinSafety. We consider the SOTA guardrail models, including (1) industry moderation APIs from Detoxify [2], Perspective [24], Azure [1], and OpenAI [31], (2) fine-tuned guardrail model LlamaGuard [17] and ToxicChat-T5 [26], (3) LLM-based guardrail via chain-of-thought prompting (CoT) [54], and (4) ensemble-learning based guardrail models [14, 60]. We also evaluate the robustness of $R^2$-Guard against SOTA jailbreak methods including GCG [66], PAIR [5], TAP [33], and AutoDAN [28].\nAs a summary, we find that (1) $R^2$-Guard consistently outperforms SOTA guardrail models on various safety datasets, (2) $R^2$-Guard demonstrates much higher resilience against SOTA jailbreak algorithms compared to other guardrail models, (3) the pseudo-learning algorithm with only simulation data performs on par with the real learning algorithm, enabling efficient training of $R^2$-Guard, and (4) $R^2$-Guard can be adaptable to new safety categories by simply editing the reasoning graph."}, {"title": "5.1 $R^2$-Guard outperforms SOTA guardrail models", "content": "Experiment setup. We evaluate the guardrail models on six datasets including five standard safety datasets OpenAI Mod [31],ToxicChat [26], XSTest [42], Overkill [46], BeaverTails [19] and our new safety dataset TwinSafety, introduced in Section 4. We consider four types of strong guardrail models as baselines: (1) industry guardrail APIs from Detoxify [2], Perspective [24], Azure [1], and OpenAI Mod [31], (2) fine-tuned guardrail model LlamaGuard [17] and ToxicChat-T5 [26], (3) LLM-based guardrail model via chain-of-thought prompting (CoT) [54], and (4) ensemble-learning based guardrail models [14, 60]. We directly evaluate the likelihood of unsafety by different APIs. We keep the default prompt template and parameters in Llamaguard and ToxicChat-T5. We use Llama2-7b-chat as the inference model for CoT and carefully select 3 representative examples from corresponding datasets and manually write the reasoning process as demonstrations. Ensemble learning takes the maximal unsafety scores of category-specific learning models for different categories as the prediction. We use the category-specific learning models from OpenAI Mod, LlamaGuard, and ToxicChat-T5 since they demonstrate high guardrail performance empirically. $R^2$-Guard leverages the same category-specific learning models as the ensemble learning to construct the category-specific learning component and performs logical inference on PGMs, as illustrated in Section 3. We consider both the MLN inference in Section 3.2 and PC inference in Section 3.3 and refer to them as $R^2$-Guard (MLN) and $R^2$-Guard (PC). Following literature[17, 31, 26], We leverage AUPRC as the metric to evaluate the ability of guardrail models to discriminate between safe and unsafe prompts.\nResults. The results in Table 2 demonstrate that $R^2$-Guard outperforms various strong baselines while maintaining comparable inference time to local guardrail models such as LlamaGuard. The effectiveness of $R^2$-Guard surpasses CoT reasoning, which implicitly facilitates reasoning through in-context learning. This highlights the power of explicit reasoning by encoding safety knowledge and performing probabilistic inference on MLN and PC graphs. Compared to ensemble learning, the effectiveness of $R^2$-Guard underscores the importance of modeling interactions among unsafety"}, {"title": "5.2 $R^2$-Guard is robust against SOTA jailbreaks", "content": "Experiment Setup. Jailbreak attacks aim to bypass the detection mechanisms of guardrail models by injecting optimized strings. Therefore, it is crucial to evaluate the robustness of guardrail models against these attacks to ensure the security of LLM systems. We consider three types of SOTA jailbreak attack algorithms: (1) the white-box adaptive attack GCG [66], which optimizes an adversarial suffix via token gradients; (2) the black-box attack AutoDAN [28], which leverages genetic algorithms to optimize jailbreak prompts from a pool of seed prompts; and (3) the black-box LLM-based jailbreak algorithms PAIR [5] and TAP [33], which prompt LLMs to generate and refine jailbreak prompts through feedback from target models. Since GCG is a white-box attack, we cannot access the model weights for API- based guardrail models such as OpenAI Mod. Therefore, we consider three types of strong GCG-optimized adversarial suffixes on surrogate models: (1) universal strings optimized to jailbreak multiple LLMs (GCG-U1, GCG-U2); (2) jailbreak strings against the safety-aligned LLM Vicuna-7B (GCG-V) and the SOTA guardrail model LlamaGuard (GCG-L); and (3) jailbreak strings optimized against the distilled Gemma-2B model of $R^2$-Guard (GCG-R). Following the literature [28, 5, 33], we evaluate the robustness of the guardrail models using AdvBench [66], which consists solely of unsafe prompts, and measure the unsafety detection rate (UDR), the portion of flagged unsafe prompts with threshold 0.5 (i.e., the prompt is recognized as unsafe if the unsafety probability exceeds 0.5). Additional details are provided in Appendix A.1.\nResults. The results in Table 3 demonstrate that $R^2$-Guard is more robust against multiple SOTA jailbreaks compared to other guardrail models. Both universal jailbreak strings (GCG-U1, GCG-U2) and optimized jailbreak strings using safety-aligned LLMs (GCG-V) and the guardrail model LlamaGuard (GCG-L) do not perturb the UDR of $R^2$-Guard. Even more adaptive GCG attacks against the distilled model of $R^2$-Guard (GCG-R) and SOTA black-box attacks (AutoDAN) only slightly decrease the UDR of $R^2$-Guard, and $R^2$-Guard still outperforms other guardrail models by a significant margin. We evaluate UDRs against PAIR and TAP in Table 4 in Appendix A.2, which shows that the UDR of $R^2$-Guard is decreased but remains much higher than UDRs of other models. This reduction is because PAIR and TAP may reformulate the original prompt so that the modified prompt is semantically less harmful (e.g., reformulating"}, {"title": "5.3 Ablation studies", "content": "A PREPRINT JULY 9, 2024\n5.3.1 Pseudo learning and real learning\nIn Section 3.4, we introduce two methods for optimizing knowledge weights: pseudo\nlearning on simulation data and real learning on realistic data samples. We empirically\ncompare these weight learning methods using the ToxicChat and BeaverTails datasets,\nwhich include training sets for real learning. The results, presented in Figure 2,\nreveal that pseudo learning performs on par with real learning. This demonstrates\nthat in-distribution training samples are not essential for the $R^2$-Guard framework,\nhighlighting its generality."}, {"title": "5.3.2 Effectiveness on unseen safety categories", "content": "$R^2$-Guard can be adaptable to new categories by adding the corresponding category-\nspecific learning models and modifying the reasoning component to include safety\nknowledge related to the new categories. In the evaluation, we consider four sequentially\nadded safety categories: hate (H), sexual (S), harassment (HR), and violence (V).\nCorrespondingly, we have four types of category-specific learning models, which are\nalso added sequentially. We evaluate the performance of $R^2$-Guard with data samples\nrelated to the four safety categories with sequentially added learning models. The results\nin Figure 4 show that $R^2$-Guard can flexibly adapt to new safety categories effectively\n(i.e., high AUPRC in the lower triangle of Figure 4)."}, {"title": "6 Discussion", "content": "One limitation of $R^2$ \u2013Guard is its requirement for explicit specification of safety knowledge rules in PGMs, necessitating\nhuman effort to annotate detailed safety categories and their interconnections. However, this explicit knowledge also\nenhances $R^2$-Guard's effectiveness and robustness compared to purely data-driven guardrail models. $R^2$-Guard\nhas a broader impact in three key areas: 1) motivating the guardrail community to transition from purely data-\ndriven approaches to those enabled by logical reasoning, 2) providing the symbolic reasoning community with a\nrobust framework for encoding knowledge, performing logical inference, and knowledge weight learning with weak\nsupervision, and 3) safeguarding widespread LLM deployments in various systems, We do not see any negative impact\nof our guardrail model."}, {"title": "Acknolwdgement", "content": "This work is partially supported by the National Science Foundation under grant No. 1910100, No. 2046726,\nNo. 2229876, DARPA GARD, the National Aeronautics and Space Administration (NASA) under grant no.\n80NSSC20M0229, the Alfred P. Sloan Fellowship, the Amazon research award, the eBay research award, and CAIS."}, {"title": "A Evaluation", "content": "A.1 Implementation details\nGCG-U1 and GCG-U2. These are two universal jailbreaks optimized with GCGC on multiple models and show\nsuperior transferability to GPT-4. Concretely, GCG-U1 is optimized on Vicuna-7B, Vicuna-13B, Guanaco-7B, and\nGuanaco-13B. GCG-U2 is optimized on Vicuna-7B, Vicuna-13B, Guanaco-7B, and Guanaco-13B.\nGCG-R. The jailbreak is optimized with GCG on a distilled Gemma-2b model from our $R^2$-Guard. We perform the\ndistillation on six standard safety datasets in Section 5.1. We apply the prompt template same as LlamaGuard and use\nthe token probability of \"safe\" and \"unsafe\" as the prediction.\nWe will check the identity of users after we release our dataset TwinSafety (e.g., through application forms on\nHuggingFace).\nAll the results are averaged across 3 runs with different randomness seeds. We use one RTX A6000 to run all the\nexperiments.\nWe provide the codes to reproduce all the results in the supplementary material."}, {"title": "A.2 $R^2$-Guard under SOTA jailbreaks", "content": "We evaluate UDRs against PAIR and TAP in Table 4, which shows that the UDR of $R^2$-Guard is decreased but remains\nmuch higher than UDRs of other models. This reduction is because PAIR and TAP may reformulate the original prompt\nso that the modified prompt is semantically less harmful (e.g., reformulating \"grab the gun\" to \"grab the water gun\"),\nwhich highlights the need for future work to develop a fairer benchmark in this scenario."}, {"title": "A.3 MLN reasoning vs. PC reasoning", "content": "We compare the effectiveness and efficiency of logical reasoning with MLNs and that with PCs. The results in Table 5\nshow that PC reasoning achieves comparable performance in content moderation while requiring only 6% of the\ninference time needed for MLN reasoning."}]}