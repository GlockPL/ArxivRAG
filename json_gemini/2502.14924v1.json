{"title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?", "authors": ["Ibrahim Alabdulmohsin", "Andreas Steiner"], "abstract": "Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMS) can replicate such fractal characteristics and identify conditions\u2014such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.", "sections": [{"title": "1. Introduction", "content": "The information-theoretic complexity of language (i.e., its bits or \"surprise\") has been shown to exhibit both self-similarity and long-range dependence (LRD) (Alabdulmohsin et al., 2024). In simple terms, a stochastic process is considered self-similar if its statistical properties remain consistent across different scales, regardless of the level of magnification applied. A well-known example of such behavior is found in Ethernet traffic (Crovella & Bestavros, 1995; Leland et al., 1994; Paxson & Floyd, 1995; Willinger et al., 1997), where self-similarity manifests as burstiness across all time scales, thereby impacting the design of network device buffers (Leland & Wilson, 1991). In language, such self-similarity is attributed to its recursive structure (Altmann et al., 2012; Willinger et al., 1995). On the other hand, a stochastic process is called long-range dependent (LRD) if its future is influenced by the distant past, with no particular characteristic context length.\nSelf-similarity and long-range dependence (LRD) can be quantified using the H\u00f6lder and Hurst exponents, respectively. The H\u00f6lder exponent, which we denote by S for self-similarity, characterizes the rate of decay in the autocorrelation function, with smaller values of S indicating a more significant self-similar structure (heavier tail) (Watkins, 2019). By contrast, larger values of the Hurst exponent H > 0.5 indicate more dependence across time (Hurst, 1951). We refer the reader to (Alabdulmohsin et al., 2024) for the exact definitions of these quantities. A natural question that arises, next, is: How different are S and H in LLM-generated texts from natural language? In this work, we investigate this question in depth, aiming to identify conditions such as temperature settings, instruction-tuning, prompting and model size that influence an LLM's ability to replicate such fractal characteristics.\nBefore doing that, however, let us consider some arguments for why LLMs may or may not be capable of replicating the fractal structure of language. One argument for why they should be capable of doing so lies in the chain rule of probability. LLMs are reasonably calibrated at the token level (Kadavath et al., 2022), implying that auto-regressive decoding should theoretically capture the structure of natural language as long as token-level probability scores remain well-calibrated. By the chain rule, each subsequent token's probability is conditioned on the prior tokens, and this process should ideally reflect the self-similarity and long-range dependence inherent in language."}, {"title": "2. Related Works", "content": "Several studies have looked into the statistical properties of LLM-generated texts. For instance, (Guo et al., 2023)"}, {"title": "3. Experimental Setup and Dataset", "content": "Our goal is to investigate when and how LLM-generated texts can vary substantially from natural language in their fractal structure. For that we need to generate synthetic texts.\nIn order to correctly identify the impact of each factor we consider in our study (e.g. temperature setting, prompting, model size), we generate the data ourselves from scratch. We follow a similar setup to the one used in (Verma et al., 2024), in which we restrict analysis to long documents or paragraphs, as opposed to short answers to questions. Similar to (Verma et al., 2024), we query a capable LLM via its API, which is always Gemini 1.0 Pro (Anil et al., 2024) in our experiments, to generate some contextual information about an article (such as keywords or a summary) before asking another model to write an article based on those contextual information. Since each article is matched with a corresponding human-generated text (ground truth), we name this dataset \u201cGenerated And Grounded Language Examples\" (GAGLE). It contains over 240,000 articles\u00b9.\nThe contextual information we use were chosen such that they can be ordered from the least informative to the most, as shown in Table 1. The datasets we use are from five domains: (1) WIKIPEDIA (Wikimedia, 2024), (2) BIGPATENT, consisting of over one million records of U.S. patents (Sharma et al., 2019), (3) NEWSROOM, containing over one million news articles (Grusky et al., 2018), (4) SCIENTIFIC, a collection of research papers obtained from ArXiv and PubMed repositories (Cohan et al., 2018), and (5) BILLSUM, containing US Congressional and California state bills (Kornilova & Eidelman, 2019). We only use, at most, 1,000 articles from each domain.\nIn our experiments, we use pretrained models (with simple continuation only) and instruction-tuned models with various prompting strategies as discussed earlier. During text generation, we experiment with three decoding temperatures: \u03b2 = 0 (greedy decoding), \u03b2 = 0.5, and \u03b2 = 1 (pretraining temperature). The three models we use are Gemini 1.0 Pro (Anil et al., 2024), Mistral-7B (Jiang et al., 2023), and Gemma-2B (Mesnard et al., 2024).\nOnce the texts are generated, we score them using pretrained models. One goal is to identify if our findings remain robust across different scoring models. As mentioned earlier, some previous works suggest that smaller models might be better for scoring and detecting LLM-generated texts (Mireshghallah et al., 2024) while other works suggest that using the same model for both generation and scoring yields better results (Fagni et al., 2021; Mitchell et al., 2023).\nFinally, once all the log-perplexity scores are calculated, we compute fractal parameters. Because S and H are exponents of power laws, sufficiently long documents are required. Hence, we encourage the model to generate long documents in the prompt (see prompting templates in Appendix A). We drop the first 64 tokens to remove any warm-up effects, and ignore documents that are less than 400 tokens in length. When a document is ignored, we also ignore the corresponding ground-truth document, to remove this confounding effect. Then, we clip all documents (both human- and LLM-generated) to 400 tokens to have equal lengths. We estimate fractal parameters using the scales (time gaps) \u03c4\u2208 {8,16,32, 48, 64, 96, 128, 160, 192, 256, 320} with \u03f5 = 10\u207b\u00b2; see (Alabdulmohsin et al., 2024) for details on how to calculate them. We also use bootstrapping (Efron & Tibshirani, 1994) to estimate confidence intervals by sub-sampling with replacement 10 independent samples.\nDisclaimer. Our estimates of S and H differ from those in (Alabdulmohsin et al., 2024) for two reasons. Because LLM-generated texts are short (typically of about 500 tokens), we restrict the range of the scale term to at most 320 tokens. Also, we use a slightly larger value of \u03f5 because we found that smaller values in short documents lead to high variance. Both imply that our estimates are less accurate. Nonetheless, our goal is to use S and H as statistical probes to compare natural texts from LLM-generated, so we use the same hyperparameters for both types of documents."}, {"title": "4. Detailed Analysis", "content": "Q1. How do log-perplexity scores in LLM-generated documents differ from those in language? To answer this question, results are shown in Figure 4. In agreement with prior works, we observe that LLM-generated texts have a lower log-perplexity scores (negative values in the y axis) but not for large pretrained models when prompted using their pretraining temperature \u03b2 = 1. In the latter setting, LLM-generated texts have a similar average log-perplexity score to natural language. We illustrate this using the GLTR tool (Gehrmann et al., 2019) in Figure 5. Gemma-2B is an exception, probably because it is much smaller than the rest of the models. Instruction-tuning, by contrast, lowers the log-perplexity of generated texts compared to natural language even at temperature \u03b2 = 1, although no prompting instructions are used in Figure 4. Overall, this suggests that detection methods relying solely on log-perplexity scores, such as GLTR, may not be adequate for identifying contents generated by pretrained models as they become increasingly more capable in the future.\nQ2. If large pretrained LLMs at their pretraining temperature \u03b2 = 1 can replicate the 1st-order statistics of log-perplexity scores in language, do they also replicate its fractal parameters? Figure 3 summarizes the results. We observe that larger pretrained models at temperature \u03b2 = 1 replicate the fractal properties of natural language better, regardless of which model is used for scoring. In addition, LLM-generated texts are systematically biased towards higher values of both S (less self-similarity) and H (more dependence) than in natural language. As we will discuss later in Q6, this means they are biased towards generating texts with lower quality than in natural language.\nQ3. What about fractal parameters in instruction-tuned models? We examine the impact of instruction tuning on fractal parameters when all texts are generated in both pretrained and instruction-tuned models using simple continuation (no prompting). We focus on simple continuation here to isolate the impact of instruction-tuning alone. To recall, Figure 4 shows that instruction tuning generates texts with lower log-perplexity scores than natural language. Figure 6 shows that texts generated by instruction-tuned models have higher values of the Hurst exponent at low temperatures \u03b2 < 1, indicating more dependence over time. Self-similarity is not impacted, however.\nQ4. What if contextual information is provided in the prompt to instruction-tuned models? As mentioned in Section 1, we also consider the causal graph shown in Figure 1, and examine the impact of adding various contextual cues in the prompt (see Table 1). Figure 7 summarizes the results across all combinations of generating models, scoring models, and datasets. For self-similarity, we observe a double descent. While the second descent is expected, given that LLMs should be eventually capable of replicating the original article if its entire content is provided in the prompt, the fact that providing a sample of unordered keywords is better than a summary is surprising! We also observe that asking the model to generate an outline first, similar to chain-of-thought (CoT) prompting (Wei et al., 2023), yields fractal parameters that are closer to those of natural language than simple continuation. In addition, as shown in Appendix D, generating patent and science articles seem to be more sensitive to prompting than in other domains.\nQ5. How sensitive are fractal parameters of LLM-generated articles to the contextual information provided in the prompt? To answer this question, we first calculate for each of the three architectures Gemini 1.0 Pro, Mistral-7B, and Gemma-2B the average H\u00f6lder and Hurst exponents disaggregated by prompting method, where averages are calculated over all remaining variables (e.g. decoding temperature, scoring algorithm, and dataset). Then, we plot the standard deviation calculated across the prompting methods. The results are displayed in Figure 8. As expected, larger models are less sensitive to the choice of the prompting method than smaller models.\nQ6. How do fractal parameters relate to the quality of output? To answer this question, we first recall that the H\u00f6lder exponent S quantifies the level of self-similarity in a stochastic process, with smaller values indicating a more self-similar structure (heavier tail); i.e. with complex, rich details at all levels of granularity. Hence, lower values of S are desirable. By contrast, the Hurst exponent H quantifies dependence over time. Values close to H \u2248 0.5 indicate no dependence (i.e. the process is random) while values close to H \u2248 1.0 indicate strong predictability (e.g. when the same text is repeated over and over again). Natural language has values close to H \u2248 0.65. In practice, LLMs do not generate words entirely at random, so the correlation between H and model quality is negative. For this reason, H is also strongly and positively correlated with S, with a Pearson coefficient of 0.68 and a p-value < 10\u207b\u00b9\u2075.\nQ7. How well does the range of fractal parameters overlap between natural language and LLM-generated texts? Figure 10 shows that the range of fractal parameters in LLM-generated texts is mostly a narrow subset of those observed in natural language. Clearly, while there is an overlap, natural language maintains S and H in a narrow range whereas they vary widely in LLM-generated texts. This is consistent with the earlier observation about the relation between fractal parameters and quality of texts. Among the different factors considered, we find that prompting has the biggest impact on S and H as shown in Table 2, which calculates the Shannon mutual information between fractal parameters and other variables.\nQ8. Are there notable differences when LLMs are used to score their own outputs? Inspired by prior observations, which suggest that using similar architectures for both scoring and generation might work better for detecting LLM-generated texts (Mitchell et al., 2023; Fagni et al., 2021), we explore if there are differences in fractal parameters when LLMs score their own outputs (i.e. using Mistral-7B to score its output, as opposed to the output of other models)."}, {"title": "5. Discussion and Limitations", "content": "In this work, we investigate whether LLMs are capable of replicating the fractal structure of language and examine the impact of various parameters such as the model size, decoding temperature, and prompting method. Our findings reveal that for pretrained models, larger architectures are more effective at capturing such fractal properties. In addition, with instruction-tuned models, the similarity to human language does not improve monotonically as the amount of contextual information in the prompt increases. Notably, the Hurst parameter emerged as a strong predictor of quality in generated texts, among other significant findings. To facilitate further research in this area, we release our GAGLE dataset, which comprises over 240,000 LLM-generated articles.\nIn terms of limitations, estimating fractal parameters requires analyzing large corpora of lengthy documents because these parameters describe properties of the underlying stochastic processes. Therefore, they may not be reliable at making conclusions about individual documents or short texts. This limitation prevents us from making claims about the ability to detect AI-generated content using these metrics alone. However, we note that perplexity-based detection methods might be enhanced by incorporating second-order statistics such as the H\u00f6lder and Hurst exponents, since the range of those parameters in LLM-generated articles varies widely compared to human-generated texts. We leave the exploration of detection strategies that leverage these fractal characteristics for future work."}, {"title": "A. Prompting Templates", "content": null}]}