{"title": "Semi-IIN: Semi-supervised Intra-inter modal Interaction Learning Network for Multimodal Sentiment Analysis", "authors": ["Jinhao Lin", "Yifei Wang", "Yanwu Xu", "Qi Liu"], "abstract": "Despite multimodal sentiment analysis being a fertile research ground that merits further investigation, current approaches take up high annotation cost and suffer from label ambiguity, non-amicable to high-quality labeled data acquisition. Furthermore, choosing the right interactions is essential because the significance of intra- or inter-modal interactions can differ among various samples. To this end, we propose Semi-IIN, a Semi-supervised Intra-inter modal Interaction learning Network for multimodal sentiment analysis. Semi-IIN integrates masked attention and gating mechanisms, enabling effective dynamic selection after independently capturing intra- and inter-modal interactive information. Combined with the self-training approach, Semi-IIN fully utilizes the knowledge learned from unlabeled data. Experimental results on two public datasets, MOSI and MOSEI, demonstrate the effectiveness of Semi-IIN, establishing a new state-of-the-art on several metrics. Code is available at https://github.com/flow-ljh/Semi-IIN.", "sections": [{"title": "Introduction", "content": "Multimodal sentiment analysis (MSA) has attracted increasing attention in recent years due to the rapid development of online social media platforms (Poria et al. 2020). Multimodal data offers more emotional cues than unimodal sentiment analysis, allowing machines to interpret human behaviors better and make more precise sentiment predictions (Zhang, Xu, and Lin 2021; Hu, Lu, and Zhao 2021). However, utilizing various modalities for analyzing human emotions continues to be a significant obstacle, particularly in the context of multimodal interactions with unlabeled data. Existing methods can be categorized into supervised learning and semi-supervised learning approaches. The former focuses on multimodal fusion and alignment, where the goal is to extract complementary information from different modalities and better understand human emotions. For multimodal fusion, current approaches acquire joint representations by imposing constraints (Hazarika, Zimmermann, and Poria 2020) or employing interactive operations (Zadeh et al. 2017; Liu et al. 2018) on the representations of individual modalities within the feature space. For multimodal alignment, researchers are committed to designing a cross-modal attention mechanism (Tsai et al. 2019) or an inter-modal temporal position prediction task (Yu et al. 2023) to capture cross-modal alignment information. The latter addresses data annotation's time-consuming and labor-intensive nature through semi-supervised methods. Liang (Liang, Li, and Jin 2020) designed a cross-modality distribution matching task to enhance the consistency of emotional representation, while Lian (Lian, Liu, and Tao 2022) proposed a Semi-supervised Multi-modal Interaction Network (SMIN) to learn multimodal interactive and contextual information. Current supervised and semi-supervised learning methods have made significant advancements, but they struggle to incorporate sentiment-related features from unlabeled data or properly separate irrelevant knowledge specific to different modalities. Figure 1 illustrates how crucial it is to maintain a balanced proportion of information passing between intra- and inter-modal interactions. On one side, the speaker's emotion can be inferred from key sentiment words such as \"kind\". Nevertheless, the transfer of information between visual and textual sequences in an inter-modal interactive manner can lead to confusion, as conventional attention may not distinguish between different words (due to the absence of clear positive emotional cues in image sequences) and overlook important emotional expressions. Conversely, when a speaker's comments, gestures, and intonation all convey a uniform emotional tone, inter-modal interaction becomes crucial. It can be used as additional information for interaction within the same mode. Therefore, we suggest a new framework that adjusts the proportion of intra- and inter-modal interaction based on the assumption that independent learning and dynamic selection of information are essential. This framework also utilizes self-training to gain knowledge from unlabeled data. Our contributions can be summarized as follows:\n\u2022 We present a new network called Semi-IIN that combines two unique masked attention mechanisms to capture meaningful interactions among image sequences, audio frames, and text tokens.\n\u2022 We use a self-training method that creates dependable pseudo-labels by a top-k confidence filtering strategy, allowing for model improvements through retraining and the extraction of emotion-related features from data without labels. Under the semi-supervised learning setting, Semi-IIN achieves improved performance.\n\u2022 Experimental results on two public datasets show that Semi-IIN performs better than other current methods. To better understand how effective our approach is, we carry out thorough ablation experiments."}, {"title": "RELATED WORK", "content": "Semi-supervised sentiment analysis\nSupervised learning methods are commonly used for sentiment analysis, but their effectiveness is limited by the lack of labeled data for training. Researchers are trying to address this challenge by incorporating semi-supervised learning techniques to decrease the need for labeled data and enhance overall performance. To generate reliable pseudo-samples, researchers are committed to incorporating consistency-based pseudo-label strategy to identify misleading instances (Yuan et al. 2024), or establishing a specific threshold for prediction confidence in categories with clear and dependable characteristics (Cheng et al. 2023). Another direction is utilizing autoencoders (Lian, Liu, and Tao 2022; Zhang et al. 2020) to extract emotion-salient representations from additional unlabeled data. To narrow the heterogeneous gap between different modalities, Hu (Hu et al. 2020) designed a Semi-supervised Multimodal Learning Network, which correlates different modalities by capturing the multimodal data's intrinsic structure and discriminative correlation. Liang (Liang, Li, and Jin 2020) proposed a semi-supervised learning method based on cross-modal distribution matching. Parthasarathy (Parthasarathy and Busso 2020) employed semi-supervised ladder networks that incorporated skip connections between the encoder and decoder to extract emotion-relevant features.\nMultimodal interaction learning\nPrevious research has focused on creating fusion strategies to capture interactive connections. Existing methods can be categorized into utterance-level and token-level interaction learning. To facilitate learning at the level of utterance interactions, single-mode representations are initially encoded individually and then combined by applying constraints (Hazarika, Zimmermann, and Poria 2020; Yu et al. 2021), separating (Tsai et al. 2018), analyzing correlations (Sun et al. 2020), or capturing relationships (Zadeh et al. 2017; Liu et al. 2018) to enable single-mode, dual-mode, or triple-mode interactions. Recently, Han (Han, Chen, and Poria 2021) introduced information theory to maximize the mutual information between unimodal and multimodal fusion results, while Yang (Yang et al. 2023) performed contrastive representation learning and contrastive feature decomposition to enhance the representation of multimodal information. Nevertheless, these approaches fail to account for the fact that emotions can vary throughout different points in the video as unimodal representations are averaged along the time axis to capture intricate and evolving emotional signals. Additionally, they face either high computational complexity or the introduction of extra hyperparameters. For token-level interaction learning, MAG-BERT proposed by Rahman (Rahman et al. 2020), incorporates non-verbal token-level information by generating a shift based on visual and acoustic modalities to enhance interaction learning. Nevertheless, this method necessitates coordination among modalities. To address this challenge, Tsai (Tsai et al. 2019) proposed a cross-modal attention mechanism to reinforce the target modality with emotional signals from the source modality. Recently, Chen (Chen et al. 2023a) proposed an inter-intra modal representation augmentation approach to enhance modal-representation learning ability.\nOur goal is to utilize sentiment information from text, audio, and visual cues at the token level to improve the model's generalization ability with the help of semi-supervised learning. In contrast to previous studies, the processing of interactive information across different senses is carried out independently, filtering out distracting stimuli and regulated by a gate mechanism to maintain the consistency of emotional cues. Moreover, we use a self-training approach to enhance model training."}, {"title": "METHOD", "content": "Feature encoding\nThis section provides a detailed description of our proposed framework, Semi-IIN. As shown in Figure 2, we first use the pre-trained 24-layer RoBERTa (Liu et al. 2019) to capture lexical features to obtain a single-modal representation, following the approach of previous studies (Lian, Liu, and Tao 2022; Chen et al. 2023a). To capture visual emotions, we use the pre-trained Fabnet (Wiles, Koepke, and Zisserman 2018; Chen et al. 2023a) to depict fundamental emotional characteristics. HuBERT (Hsu et al. 2021) is utilized for extracting the initial vector representations in the context of acoustic modality. They are formulated as:\n$h_{ti} = RoBERTa(X_{ti};\\theta_{RoBERTa}) \\in \\mathbb{R}^{l_t \\times d_t}$"}, {"title": "Feature fusion", "content": "As shown in Figure 3, two distinct Masked Attention(MA), Intra-modal Masked Attention(IntraMA) and Inter-modal Masked Attention(InterMA) are designed to mask unrelated relations between modalities. The former focuses on exploiting interactive information within each modality, while the latter enables the cross-modal exchange of emotional cues. Specifically, we design two different attention masks: Intra-modal MASK(IntraMASK) and Inter-modal MASK(InterMASK):\n$IntraMASK_{ij} = \\begin{cases}0, & \\text{if } i, j \\in Intra_{pos} \\\\-\\infty, & \\text{if } i, j \\notin Intra_{pos}\\end{cases}$\n$InterMASK_{ij} = \\begin{cases}0, & \\text{if } i, j \\in Inter_{pos} \\\\-\\infty, & \\text{if } i, j \\notin Inter_{pos}\\end{cases}$\nwhere $IntraMASK \\in \\mathbb{R}^{T \\times T}$ and $InterMASK \\in \\mathbb{R}^{T \\times T}$. $Intra_{pos}$ and $Inter_{pos}$ are two pre-defined matrices designed to separate the tokens of the interaction position from the masked ones. After that, IntraMA is achieved by adding the IntraMASK with the conventional global attention (Vaswani et al. 2017), which facilitates the extraction of the key emotional clues in each modality. It is mathematically expressed as:\n$Y_{intra} = IntraMA(X_1)$\n$= softmax(\\frac{QK^T}{\\sqrt{d_k}} + IntraMASK)V$\n$= softmax(\\frac{X_1 W_Q W_K^T X_1}{\\sqrt{d_k}} + IntraMASK)X_1 W_V$\nwhere input $X_1 \\in \\mathbb{R}^{T \\times d_h}$, $W_Q \\in \\mathbb{R}^{d_h \\times d_k}$, $W_K \\in \\mathbb{R}^{d_h \\times d_k}$ and $W_V \\in \\mathbb{R}^{d_h \\times d_v}$. Similarly to the IntraMA, InterMA is achieved as:\n$Y_{inter} = InterMA(X_2)$\n$= softmax(\\frac{QK^T}{\\sqrt{d_k}} + InterMASK)V$\n$= softmax(\\frac{X_2 W_Q W_K^T X_2}{\\sqrt{d_k}} + InterMASK)X_2 W_V$\nSubsequently, as shown in Figure 2, by replacing the global attention with the proposed IntraMA and InterMA, the Intra-modal Masked Attention Unit(IntraMAU) and the Inter-modal Masked Attention Unit(InterMAU) are constructed. Specifically, for the $l$-th layer input $Z_{intra} \\in \\mathbb{R}^{T \\times d_h}$, the output of $l$-th layer of the IntraMAU can be calculated as:\n$\\tilde{Z}_{intra} = IntraMA(LN(Z_{intra})) + LN(Z_{intra})$\n$Z_{intra} = FFN(LN(\\tilde{Z}_{intra})) + LN(\\tilde{Z}_{intra})$\nwhere $l \\in [1, n]$, FFN and LN represent the feed-forward network with ReLU as the activation function and the layer normalization, respectively. The InterMAU consists of the same modules as the IntraMAU except for the IntraMA being replaced with the InterMA."}, {"title": "Gate mechanism", "content": "Two special tokens, val, and cls, serve as fusion features, aggregating information from all tokens except for the special ones. Thus, the first element of $Z_{intra}$ and $Z_{inter}$, along with the second position of $Z_{intra}$ and $Z_{inter}$ are treated as the final fusion feature. They are processed via the following dynamic gate mechanism (Lv et al. 2021):\n$G_v = sigmoid(Z_{intra}[0] \\cdot W_1 + Z_{inter}[0] \\cdot W_2 + b_1)$\n$Z_v = G_v \\odot Z_{inter}[0] + (1 - G_v) \\odot Z_{intra}[0]$\n$G_e = sigmoid(Z_{intra}[1] \\cdot W'_1 + Z_{inter}[1] \\cdot W'_2 + b'_1)$\n$Z_e = G_e \\odot Z_{inter}[1] + (1 - G_e) \\odot Z_{intra}[1]$\nwhere $W_1, W_2, W'_1$ and $W'_2$ all $ \\in \\mathbb{R}^{d_h \\times d_h}$, $b_1$ and $b'_1 \\in \\mathbb{R}^{d_h}$. The passed proportions between modality-specific and modality-complimentary knowledge are further dynamically determined through the gating approach, facilitating meaningful modality interaction learning. The predictions of sentiment intensity $v$ and emotional category $e$ are derived from the filtered feature $Z_v$ and $Z_e$. They are\n$v = FC(MLP(Z_v)) \\in \\mathbb{R}^{d_1}$\n$e = FC(MLP(Z_e)) \\in \\mathbb{R}^{d_2}$"}, {"title": "Self-training", "content": "We design a self-training strategy to distill emotional knowledge from unlabeled data. The process is shown in algorithm 1. Initially, we train the MSA model shown in Figure 2 with labeled data and then employ the trained model, referred to as $\\phi$, to make predictions using unlabeled data. Following recent progress in semi-supervised learning (Chen et al. 2023b), the top-k confidence method is employed to eliminate unreliable samples. Due to the significantly larger amount of data in the MOSEI dataset in comparison to the MOSI dataset, the model trained on the MOSEI dataset generates predictions with greater confidence and accuracy. As a result, we assigned a value of 40 to k for the MOSI dataset, whereas, for the MOSEI dataset, k is set to the total number of unlabeled instances. Finally, we combine the labeled and unlabeled data(the labeled portion) to retrain the model(weight initialization from $\\phi$). Note that only the emotion classification task loss is sent back through the network for pseudo-labeled samples."}, {"title": "Loss Function", "content": "Emotional states can be represented either through discrete categories (such as \"sad\" and \"happy\") or dimensional annotations (points in a continuous space). In the MSA, Lian (Lian et al. 2023) and Wang (Wang et al. 2022) highlighted a high correlation between discrete and dimensional annotations. As a result, we classify the data into seven specific emotional categories by determining how close the dimensional labels are to predefined discrete categories. After obtaining discrete labels, we adopt the Mean Squared Error (MSE) $L_v$ and Cross-entropy Loss $L_e$ as our optimization objectives. We have:\n$L_v = \\frac{1}{N_l} \\sum_{i}^N (v_i - \\hat{v_i})^2$\n$L_e = \\frac{1}{N_l} \\sum_i e_i log(\\hat{e_i})$\n$L_u = \\frac{1}{N_u} \\sum_i e_i log(\\hat{e_i})$\nThe loss function is defined in the supervised learning process as follows:\n$L_{total} = \\lambda_1 L_v + (1 - \\lambda_1) L_e$\nwhile for the semi-supervised process, the loss function is as follows:\n$L'_{total} = \\lambda_1 L_v + (1 - \\lambda_1) L_e + \\lambda_2 L_u$\nwhere $N_l$ and $N_u$ are the number of labeled and unlabeled training samples, respectively. $\\lambda_1$ and $\\lambda_2$ are two weighting factors."}, {"title": "Experiment", "content": "Dataset\nCMU-MOSI (Zadeh et al. 2016), a dataset for human MSA, includes 2,199 video segments taken from 93 videos. Each segment is marked with a sentiment score between -3 and +3 to show the level of sentiment expressed in that portion.\nCMU-MOSEI (Zadeh et al. 2018), an enhanced version of MOSI, consists of 22,856 video clips. Each segment is annotated with sentiment and emotion.\nAMI (Carletta et al. 2005) dataset includes 100 hours of meeting recordings. It provides video recordings of each speaker, voice track, and transcripts of their speeches. We use it as the unlabeled dataset because it does not include any sentiment annotation.\nEvaluation metrics\nWe report Mean Absolute Error (MAE), Pearson correlation (Corr), binary classification accuracy (Acc-2), and F1-score on MOSI and MOSEI datasets. The Acc-2 and F1-score are calculated in negative/non-negative (non-exclude zero) and negative/positive (exclude zero).\nImplementation Details\nSemi-IIN is trained with the Adam optimizer, configured with a learning rate of 1e-4. The batch size is 32. The loss"}, {"title": "Ablation study", "content": "Firstly, to verify the effectiveness of the self-training strategy, we conducted experiments with varying ratios of labeled samples in a semi-supervised training scenario. Figure 4 illustrates that Semi-IIN still shows progress even with a small number of labeled samples. However, performance decreases when the ratio is adjusted to 50% or 75%. The decrease in numbers could be due to the lack of balance in the created fake samples. Additionally, experiments are carried out to confirm that the increase in performance is not a result of improving word embeddings. Since Lian (Lian, Liu, and Tao 2022) only shares findings from fully supervised learning with different embedding setups, we conducted training for Semi-IIN in a fully supervised manner to maintain a fair comparison. As illustrated in Figure 5, Semi-IIN-fully consistently outperforms Semi-IIN-fully (without MA) across different embedding configurations on both the MOSI and MOSEI datasets. This result confirms the efficacy of the masked attention strategy. Furthermore, compared to SMIN-fully, Semi-IIN-fully demonstrates superior performance, validating its suitability and scalability.\nFurthermore, various ablation experiments are carried out under different conditions to showcase the effectiveness of the suggested MA, along with the semi-supervised learning approach. The results are presented in Table 2. Compared with the baseline model, despite introducing a few parameters, Semi-IIN(only MA) achieves nearly 1% accuracy improvement and lower MAE. Semi-IIN(with Semi) also results in a 0.5% increase in accuracy. The best outcome is achieved by combining Semi and MA. The findings above show that separating interactions into two branches from both intra- and inter-modal perspectives helps to better utilize consistent emotional signals across modalities. Additionally, using additional unlabeled data slightly improves the training of the model.\nWe also utilize various fusion methods to confirm the importance of dynamically determining the proportions of intra- and inter-modal information, as shown in Table 3. The findings show that the gating fusion method is more effective than other fusion techniques, highlighting the importance of choosing between modality-specific and modality-common information."}, {"title": "Qualitative Analysis", "content": "Case study\nIn this section, we select two examples to verify the importance of dynamically selecting effective interactions. As shown in Figure 6, in the first case, since visual and acoustic modality both contain irrelevant emotional signals such as calm face and unemotional tone, the inter-modal interactive branch is inclined to perceive the speaker's emotions as closer to neutral. In contrast, the intra-modal interactive branch offers a more precise prediction by disregarding the unuseful cross-modal information flow. In the second case, the intra-modal interactive branch is affected by the predominant lexical mode, leading to an inaccurate sentiment evaluation. Conversely, the inter-modal interactive branch fully utilizes visual modality that consists of abundant sentiment cues to reinforce lexical and acoustic modality, leading to accurate emotional polarity. In these two cases, Semi-IIN remains unaffected by irrelevant interactive noise and achieves accurate results overall by effectively exploring interactions."}, {"title": "Visualization of IntraMA and InterMA", "content": "Figure 7 illustrates the visualization results of InterMA and IntraMA. It is noteworthy that the speaker's emotion in this video is positive. Figure 7(a) and Figure 7(c) show that the InterMA pays more attention to important image frames with a lot of emotional content, instead of focusing on unnecessary frames, e.g., neutral facial expressions, unlike traditional global attention. Moreover, Figure 7(b) and Figure 7(d) illustrate how the IntraMA mechanism mitigates the impact of emotion-unrelated words like \"didn't go\", which may lead to incorrect affective polarity, by assigning less attention compared to conventional global attention. We think that IntraMA and InterMA are effective because they can use specific and complementary knowledge from different modalities to filter out unnecessary information."}, {"title": "Conclusion and Future Work", "content": "This paper introduces a new MSA framework, called Semi-IIN, aimed at reducing both intra- and inter-modal noise at a detailed level. Semi-IIN, along with the IntraMA and InterMA mechanisms, successfully captures important interactive information within and between modalities, making it easier to extract consistent emotional cues from multimodal data. In addition, our model decreases the need for extensive human annotations by including semi-supervised learning. The effectiveness of Semi-IIN is demonstrated through experimental results on two benchmark datasets, CMU-MOSI and CMU-MOSEI. Our proposal outperforms previous approaches, setting the new SOTA result for MSA. Our future directions mainly lie in designing semi-supervised intra-inter modal interaction learning networks for multilingual multimodal sentiment analysis, e.g., Spanish, French, and German, and enhancing interpretability."}]}