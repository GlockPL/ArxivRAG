{"title": "Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction", "authors": ["Peter Yichen Chen", "Chao Liu", "Pingchuan Ma", "John Eastman", "Daniela Rus", "Dylan Randle", "Yuri Ivanov", "Wojciech Matusik"], "abstract": "Differentiable simulation has become a powerful tool for system identification. While prior work has focused on identifying robot properties using robot-specific data or object properties using object-specific data, our approach calibrates object properties by using information from the robot, without relying on data from the object itself. Specifically, we utilize robot joint encoder information, which is commonly available in standard robotic systems. Our key observation is that by analyzing the robot's reactions to manipulated objects, we can infer properties of those objects, such as inertia and softness. Leveraging this insight, we develop differentiable simulations of robot-object interactions to inversely identify the properties of the manipulated objects. Our approach relies solely on proprioception the robot's internal sensing capabilities and does not require external measurement tools or vision-based tracking systems. This general method is applicable to any articulated robot and requires only joint position information. We demonstrate the effectiveness of our method on a low-cost robotic platform, achieving accurate mass and elastic modulus estimations of manipulated objects with just a few seconds of computation on a laptop.", "sections": [{"title": "I. INTRODUCTION", "content": "When humans pick up objects, they can easily distinguish between heavy and light ones. When applying the same amount of torque at the elbow to lift an object, the elbow will rotate significantly more for a lighter object than a heavier one. Similarly, by squeezing an object, humans can assess its softness based on how much their finger joints flex. In both cases, humans rely solely on proprioceptive signals-internal sensing capabilities\u2014to learn about the object. In this work, we ask: can robots learn about objects through interactions in the same way humans do?\nTraditional methods for object parameter calibration often rely on external tools and sensors. To determine an object's softness, for example, robots might use specialized equipment like tensile testing machines [1]. While effective, this approach involves additional steps and is impractical in scenarios where direct access to the object is unavailable, such as when the object is inside a container (see Figure 1). Another common approach involves equipping robots with external sensors [2], such as cameras, to track object movement or force sensors to measure applied forces [3]. Although these sensors provide valuable data, they increase cost and are not universally available on all robots.\nOur key insight is that a robot can infer object parameters by leveraging its own proprioceptive signals. During robot-object interactions, the robot's responses vary depending on the object's properties. By analyzing these responses, we can extract information about the object's characteristics, thus eliminating the need for external tools and simplifying the system identification process. The only signals required are the robot's proprioceptive data from the robot's internal sensors, such as joint encoders [4]. Therefore, this approach applies to any articulated robot without the need for advanced sensors. Similar to how humans can feel the weight of objects without visual cues, our approach also does not require vision-based supervision.\nPrevious approaches to object identification using proprioceptive signals often require multiple sensors, such as joint encoders and haptic sensors, necessitating sensor fusion [5], [6]. In contrast, our method relies solely on joint encoder information. A key aspect of our approach is using differentiable simulations of the robot and object dynamics, enabling us to inversely identify the object's characteristics using only a single motion trajectory.\nIn summary, we propose a novel framework for learning object parameters through robot-object interactions. Our contributions are threefold:\n\u2022 We demonstrate that object parameters can be learned using robot proprioception in a highly data-efficient manner.\n\u2022 We develop predictive simulations of robot-object interactions that are highly efficient and serve as key components in our system identification pipeline.\n\u2022 We validate our approach to diverse robot-object interactions, successfully recovering both mass and elastic moduli from a single interaction trajectory."}, {"title": "II. RELATED WORK", "content": "System identification methods have been extensively used to calibrate the properties of both robots and manipulated objects. Traditional system identification techniques focus on identifying multiple inertial parameters of articulated robots by observing the robot's joint movements and employing linear regression [7]\u2013[9]. More recently, researchers have explored alternatives to traditional differential-equation-based models, particularly data-driven approaches like neural networks, which serve as the underlying dynamics models for learning general nonlinear dynamics [10]-[14]. These learned models are crucial for downstream tasks such as model predictive control in dynamic manipulations [15]\u2013[19], locomotion [20]-[22], and physical scene understanding [23]-[26]. Our work builds upon these robot system identification methods but diverges in its primary focus: rather than calibrating robot parameters, we aim to identify the properties of the object being manipulated.\nDifferentiable simulations have emerged as effective tools for system identification, leveraging easily available gradients to enable efficient gradient-based optimization for parameter identification. These simulations have been used to identify parameters of both rigid and soft robots by tracking the robot's movements [24], [25], [27]-[34]. Additionally, differentiable simulations have been employed to calibrate properties of objects manipulated by robots, involving complex interactions such as friction contacts and large deformations [35]-[38]. However, most of these approaches require direct access to the object's movement over time, often through vision-based tracking systems. Our approach also focuses on calibrating the properties of manipulated objects, but it does so by tracking the robot's movement over time, eliminating the need for direct observation of the object.\nProprioception plays a crucial role in human interactions with the environment, particularly when vision is insufficient [39]. Proprioceptive signals provide real-time feedback that is less susceptible to noise, can operate in various lighting conditions, and are inherently privacy-preserving. This has inspired the development of robotic actuators that perceive their own states-such as position, velocity, and force-without relying on external sensors [40]-[43]. Internal sensing capabilities have been increasingly leveraged for various purposes, including robot calibration [44], manipulated object identification [45], [46], parameter estimation [5], control [47], and manipulation of deformable objects [6]. These approaches often require multi-modal signals and sensor fusion algorithms. By contrast, our approach only utilizes proprioceptive signals from joint encoders in simple trajectories to efficiently estimate the physical properties of manipulated objects, bypassing the need for external sensing methods."}, {"title": "III. METHOD: DYNAMICS MODELS", "content": "This section outlines the dynamic models of both the robot and the manipulated objects, which serve as the \"forward\" simulation models. Section IV will describe how to use the corresponding \u201cbackward\" models for parameter learning. These dynamic models take the joint torque as input and predict the dynamic responses resulting from the robot-object interactions.\nA. Robot\na) Equation of Motion: We model the robot's dynamics using articulated rigid body dynamics [48], [49]. The motion of these kinematic chains of multi-body systems can be described by the following ordinary differential equation (ODE):\n$H(q)\\ddot{q} + C(q, \\dot{q}, f_x) = \\tau,$\nwhere $q(t)$, $\\dot{q}(t)$, and $\\ddot{q}(t)$ represent the joint positions, velocities, and accelerations, respectively. The matrix $H$ is the generalized inertia matrix, and $C$ is the bias force matrix that accounts for external forces $f_x$, such as gravity. The term $\\tau(t)$ represents the generalized forces, such as torques applied at the joints. In the \"forward\" model of our experimental setup, we apply known $\\tau(t)$ to the system and aim to solve for $q(t)$ over time.\nb) Temporal Discretization: To solve Equation (1) over time, we discretize it temporally with a time step size $\\Delta t$ and a total of T time steps $\\{t_n\\}_{n=0}^{T-1}$. Using a semi-implicit Euler integration scheme [50], we arrive at:\n$q_{n+1} = q_n + \\Delta t \\dot{q}_{n+1}, \\dot{q}_{n+1} = \\dot{q}_n + \\Delta t \\ddot{q}_{n+1},$\n$\\forall n = 0, 1, ..., T-1$, where $q_n := q(t_n)$, $\\dot{q}_n := \\dot{q}(t_n)$, $\\ddot{q}_n := \\ddot{q}(t_n)$. Semi-implicit time integration is a type of variational integrator that conserves energy, but it is not unconditionally stable and requires a sufficiently small time step to maintain stability. In our experiments, we simulate at 60 frames per second, using between 4 and 64 substeps to ensure accuracy and stability."}, {"title": "B. Object", "content": "Although we focus on articulated robots, the objects being manipulated can vary broadly and may not be limited to the dynamics described by Equation (1). Below, we outline several types of objects and their dynamic models.\na) Fixed Joint: The simplest case involves a robot holding an object modeled as a rigid body attached to the robot via an additional joint in the kinematic tree. For instance, assuming a tight grip, this additional joint can be modeled as a fixed joint (see Figure 2a). The inertial parameters of the object influence the generalized inertia matrix H in Equation (1), leading to different dynamic responses. This fixed joint model can be extended to other joint types, such as prismatic and universal joints [51]. Notably, contact and collision modeling are unnecessary as we assume the object is securely grasped by the robot.\nb) Contacts and Collisions: The second case involves manipulating an object that cannot be modeled as part of the articulated system's kinematic tree. For example, while the robot grasps and moves a container, a small sphere inside the container is not part of the kinematic tree (see Figure 2b). The small sphere and the robot interact dynamically through contact and collision between the sphere and the container. The small sphere's rigid body dynamics are modeled separately from the robot's using its own rigid body equation of motion [48]:\n$H_o(q_o)\\ddot{q}_o + C_o(q_o, \\dot{q}_o, f_x^*) = \\tau_o,$\nwhere the subscript \"o\" denotes the object's quantities. The contact forces between the sphere and the robot's kinematic tree are modeled using penalty-based methods:\n$f_n = k_e d_n - k_a v_n,$\n$f_t = -min(k_f ||v_t||, \\mu||f_n||)\\frac{v_t}{||v_t||},$\nwhere $f_n$ and $f_t$ represent normal and tangential contact forces, $v_n$ and $v_t$ are relative velocities in the normal and tangential directions, and $d_n$ is the penetration depth. The constants $k_e, k_a, k_f, \\mu$ are the normal contact stiffness, normal damping coefficient, friction stiffness, and friction coefficient, respectively [52]\u2013[54]. These contact forces impact both the robot and the object's motion, contributing to the time-varying $\\tau(t)$ and $\\tau_o(t)$.\nc) Deformables: While the previous cases assume rigid bodies, this scenario considers deformable bodies whose shapes change over time due to interactions and contacts with the robot (see Figure 2c). The dynamics of the deformable object are governed by the elastodynamics equation [55]:\n$\\rho_o \\ddot{\\phi} = \\nabla \\cdot P + \\rho_o b,$\nwhere $\\phi$ is the spatiotemporal deformation map, $\\rho_o$ is the initial density, $P = \\frac{\\partial \\Psi}{\\partial F}$ is the first Piola-Kirchhoff stress, $F := \\nabla \\phi$ is the deformation gradient, and $b$ represents the body force. The elastic energy $\\Psi$ is characterized by the stable Neo-Hookean model [56], [57]:\n$\\Psi = \\frac{k_{\\mu}}{2}(I_c - 3) - k_{\\mu}log J + \\frac{k_{\\lambda}}{2}(log J)^2,$"}, {"title": "IV. METHOD: SYSTEM IDENTIFICATION", "content": "Let $\\theta$ represent the system parameters we aim to recover from the coupled dynamical system of the robot and the object (as defined in Section III). For instance, we may wish to determine the inertial parameters of a small sphere moving inside a box that is manipulated by the robot (see Figure 1 and Figure 2b). Given observations of the robot's motion over time, our goal is to learn the parameters $\\theta$ by solving an inverse problem, formulated as the following optimization problem:\n$min_{\\theta} L({\\{q_{GT}\\}_{t=0}^{T-1}, \\{q_n\\}_{n=0}^{T-1}}),$\nwhere L is the loss function, specifically the mean squared error (MSE). The term $q_{GT}^t$ denotes the ground truth joint positions obtained from the robot's joint encoder readings. By using a semi-implicit integration scheme as described in Equation (2), it is equivalent to supervise on joint position, velocity, or acceleration [30].\nThe loss function is constructed solely from the robot's kinematic quantities. In contrast, the system parameters $\\theta$ that we aim to learn can encompass any aspect of the robot-object system, including the object's inertial properties and material properties such as the first and second Lam\u00e9 parameters (see Section III-B), which quantify the object's softness. Fundamentally, our method relies exclusively on proprioceptive signals to calibrate the environment with which the robot interacts (see Figure 3).\nWe use first-order gradient-based optimization techniques to solve Equation (5). To efficiently compute the gradients with respect to the system parameters $\\theta$, we utilize the differentiable simulation framework Nvidia Warp [59], [60]. For each \"forward\" dynamics model described in Section III, Warp automatically generates a corresponding \u201cbackward\" adjoint model. Warp does it by leveraging the chain rule and Jacobian transpose products through built-in functions with corresponding adjoint methods. As such, Warp enables efficient gradient accumulation across long computational chains, accommodating the extensive number of time steps (Equation (2)) required for accurate simulation of robot-object interactions.\""}, {"title": "V. RESULTS", "content": "Section V-A to Section V-C validate our method across a wide range of manipulated object types, demonstrating that it can efficiently and accurately identify object parameters using the robot's proprioceptive system in real-world scenarios. Section V-D to Section V-F present ablation studies that show our method is as accurate as non-proprioception approaches that require external sensors and explicit tracking. All experiments were conducted using the Robotis OpenManipulator-X robot (see Appendix for details). The robot was operated in the current control mode, and for each experimental setup, we report the current applied to generate the torque $\\tau$. Additional optimization details are also provided in the appendix. The temporal evolutions of the dynamical system are best illustrated in the supplementary video.\nA. Experiment: Fixed Joint\nWe aim to identify $\\theta = m_o$, where $m_o$ is the mass of the object being manipulated. The object is held tightly by the robot's end effector and can be modeled through a fixed joint. We apply a constant current (300 mA) to generate torque $\\tau$ at joint-2, applying the torque for 0.6 seconds while keeping the other joints fixed. Intuitively, under constant torque, heavier objects cause the robot to move more slowly (see Figure 2a1 and Figure 2a2). In this case, the supervision signal $q_{GT}^{t=0}$ is the temporal evolution of joint-2's position.\nUsing only a single trajectory of 0.6 seconds for supervision might seem like very little data. However, due to the large number of time steps involved in constructing the loss (Equation (5)) and solving the dynamic equation (Equation (1)) at each time step, we effectively generate hundreds of unique and valid data points.\nWe tested four different objects: heavy ball, light ball, heavy cube, and light cube. Table I reports the masses identified by our algorithm. Figure 4 shows that with the correctly identified mass, the simulation closely matches the real-world robot's motion. Figure 6 demonstrates that our algorithm converges in just a few iterations, thanks to the accurate gradient computation enabled by differentiable simulation. Comparisons with gradient-free optimization will be discussed in Section V-E.\nInstant calibration: Our algorithm runs on laptops (Mac-"}, {"title": "B. Experiment: Contacts and Collisions", "content": "While the previous experimental setup provides direct access to the object under calibration, we now consider a more challenging scenario where explicit access to the object is not available. Specifically, we aim to calibrate the mass of a sphere inside a container without opening the container (See Figure 1), \u0456.\u0435., $\\theta = m_s$, where $m_s$ is the mass of the sphere.\nHumans often estimate the properties of objects inside a container by shaking it [61]. Following this intuition, we use the robot gripper to shake the container holding the sphere. A constant current (100 mA) is applied to generate torque $\\tau$ at joint-3 for 0.6 seconds, while the other joints remain fixed. The position of joint-3 serves as the supervision signal ${\\{q_{GT}\\}}_{t=0}^{T-1}$.\nUnder the influence of the moving container and gravity, the sphere rotates within the container, affecting the supervision signal. For contact and collision modeling, we scaled the default parameters from the Warp library to ensure stable robot simulations. Our algorithm identifies the sphere's mass as 0.018 kg, compared to a measured mass of 0.012 kg, yielding a similar error as the fixed joint experiment (see Table I)."}, {"title": "C. Experiment: Deformable Bodies", "content": "Our model can also identify elastic moduli, specifically the first and second Lam\u00e9 parameters, $\\theta = k_\\mu, k_\\lambda$, as described in Equation (4). We use the heavy and light cubes whose masses were identified in Section V-A. The prismatic joint at the robot's gripper is activated to compress the specimen. A constant current (300 mA) is applied to generate torque $\\tau$ at joint-4 for 1.0 second, with all other joints held fixed. Intuitively, the softer the cube, the greater the compression it undergoes. The supervision signal in this case is the temporal evolution of the gripper's prismatic joint position. Similar to previous experiments, our method does not require tracking the soft cubes themselves.\nFigure 5 shows the real experiment alongside the corresponding simulations. By solving Equation (5), we inversely identify the first and second Lam\u00e9 parameters of the cubes. The lighter cube has smaller elastic moduli ($k_\\mu = 749.6$ Pa, $k_\\lambda = 264.3$ Pa) and undergoes greater deformation compared to the heavy cube ($k_\\mu = 5097.6$ Pa, $k_\\lambda = 5430.4$ Pa). These learned parameters allow the simulations to closely match observed behaviors (see Figure 5)."}, {"title": "D. Ablation Study: Different Initial Seeds", "content": "To evaluate the robustness of our approach, we initialized the optimization solver (Equation (5)) with initial seeds spanning orders of magnitude differences. The task was to identify the mass of the aforementioned heavy ball. Table II shows that our method converges to the correct solution despite vastly different initial guesses. If the initial guess is extremely unrealistic (e.g., an excessively large mass), the simulation may yield divergent dynamics, causing the robotic arm to move incorrectly and trapping the optimization process in a local minimum. To mitigate this, we initialize our gradient-based algorithm with five different initial guesses that span two orders of magnitude [62]."}, {"title": "E. Ablation Study: Gradient-Free Optimization", "content": "In our work, we leverage differentiable simulations and gradient-based optimizers. As a comparison, gradient-free methods such as the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) [63] can also be used. Figure 6 shows that our gradient-based approach converges faster, whereas gradient-free methods require more iterations and may struggle to find the global minimum."}, {"title": "F. Comparison: Non-Proprioception, Explicit-Tracking-Based Approaches", "content": "A key feature of our approach is that it senses the environment using only the robot's proprioceptive signals, without relying on external sensors. We compare our approach with differentiable simulation techniques that use additional supervision signals, such as from computer vision [51]. Specifically, for the sphere-container experiments (see Figure 1 and Section V-B), both the sphere positions and joint positions are used as supervision signals in ${\\{q\\}}_{n=0}^{T-1}$. Under the same number of optimization iterations, this alternative approach identifies the sphere's mass as 0.016 kg, which is comparable to our method. Thus, compared to vision-based approaches, our method does not require explicit tracking of the object's movement while delivering similar performance."}, {"title": "VI. DISCUSSIONS AND CONCLUSIONS", "content": "We introduce a framework for object calibration using robot's proprioceptive sensing capabilities. Our approach leverages differentiable simulations of robot-object interactions to efficiently quantify object properties that influence the robot's responses over time. Notably, our method remains effective even in challenging scenarios where there is no direct access to the manipulated object (See Figure 1).\nWhile we demonstrated three types of robot-object interaction modes in Section III-B, our approach can readily be generalized to handle more complex objects such as sloshing liquids and granular materials. Future extensions of this work could explore applications in other robotics systems, such as wheeled robots and soft robots [42], and incorporate probabilistic estimates of object properties [51].\nAlthough our approach does not explicitly track object trajectories, it does assume knowledge of the objects' initial positions to set up the simulations. Future work could address this limitation by employing domain randomization techniques [37] to relax the requirement of known initial positions.\nProprioception is a fundamental way for humans to perceive and interact with the world. We envision a future where robots can also learn about their environment using proprioceptive signals. With an enhanced understanding of the world, robots can achieve more effective model predictive control, similar to how humans adjust their actions based on an object's known properties to optimize how they move and manipulate it."}, {"title": "APPENDIX", "content": "A. Hardware Details\nOur hardware features a ROBOTIS OpenManipulator-X robotic arm powered by five high-precision DYNAMIXEL XM430-W350-T motors. These actuators have 12-bit absolute encoders for precise joint position measurements and torque control mode. Motor control is handled via the DYNAMIXEL SDK, allowing for communication at 40Hz to send commands and receive real-time motor status updates.\nB. Optimizer Details\nWe utilize the Adam optimizer [64] combined with a cosine annealing learning rate scheduler [65]. An initial learning rate of 0.01 effectively minimizes the loss across all experiments. As shown in Figure 7, our method converges in just a few iterations."}]}