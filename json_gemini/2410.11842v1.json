{"title": "MOH: MULTI-HEAD ATTENTION AS MIXTURE-OF-HEAD ATTENTION", "authors": ["Peng Jin", "Bo Zhu", "Li Yuan", "Shuicheng Yan"], "abstract": "In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the stan-dard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%~90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.", "sections": [{"title": "INTRODUCTION", "content": "Since attention is introduced and becomes a fundamental component of Transformers (Vaswani et al., 2017), multi-head attention has been the standard architecture for natural language processing (Kenton & Toutanova, 2019) and computer vision tasks (Dosovitskiy et al., 2021). It is well known that using multiple heads can improve model accuracy. However, not all attention heads hold equal significance. Some works have shown that many attention heads can be pruned without affecting accuracy. For example, Voita et al. (2019) introduces a method to quantify the usefulness of each attention head and prune those that are redundant. Similarly, Michel et al. (2019) challenges the necessity of multiple heads by examining the impact of extensive pruning across various settings. These findings demonstrate that vanilla multi-head attention contains redundant attention heads.\nBesides, in multi-head attention, each attention head operates in parallel, and the final output is the sum of all attention heads (please refer to Section 3.1). Given that these attention heads operate independently and some may be redundant, we argue that it is possible to build a dynamic attention-head routing mechanism. Such a mechanism would enable each token to adaptively select the appropriate attention heads, enhancing inference efficiency without compromising accuracy.\nTo this end, we introduce Mixture-of-Head attention (MoH), a new architecture that integrates multi-head attention with the Mixture-of-Experts (MoE) mechanism (Jacobs et al., 1991; Jin et al., 2024b)."}, {"title": "RELATED WORK", "content": "Transformers (Vaswani et al., 2017) have garnered significant interest and\nsuccess in both natural language processing and computer vision. The success of transformers has\nbeen long attributed to the multi-head attention mechanism (Cordonnier et al., 2020). Multi-head\nattention mechanism is proposed by Vaswani et al. (2017) to enhance the representation power\nof an attention layer by allowing multiple attention heads to operate on different low-dimensional\nprojections of the input tokens. The outputs from these heads are then concatenated to form the final\nresult. Alternatively, by decomposing the output projection matrix by rows, multi-head attention\ncan be expressed in a summation form. In summation form, each head operates in parallel, and\nthe final output is the sum of all heads. Inspired by this observation, we propose MoH, a dynamic\nattention-head routing mechanism that allows each token to adaptively select the appropriate heads.\n\nThe Mixture-of-Experts (MoE) method (Du et al., 2022; Lewis et al.,\n2021; Rajbhandari et al., 2022; Roller et al., 2021; Zhou et al., 2022; Jin et al., 2024b) is introduced to\nexpand the capacity of deep neural networks without increasing computational costs. In this approach,\nonly a subset of parameters, known as experts, is activated for each input. Shazeer et al. (2017) first\nintroduces an MoE layer between LSTM layers. Switch Transformer (Fedus et al., 2022) further\nsimplifies the gating mechanism by selecting only the Top-1 expert per token. Gshard (Lepikhin et al.,\n2021) improves the Top-2 expert routing strategy. In contrast to MoE, which emphasizes efficient\nparameter scaling while maintaining manageable computational costs, the proposed MoH focuses on\nreducing the activation of redundant attention heads without increasing the number of parameters."}, {"title": "METHODOLOGY", "content": "In this work, we aim to reduce the activation of redundant attention heads without increasing the\nnumber of parameters. A high-level comparison between the vanilla multi-head attention and our\nproposed Mixture-of-Head attention (MoH) is presented in Fig. 1."}, {"title": "MULTI-HEAD ATTENTION", "content": "We begin by reviewing the standard multi-head attention mechanism introduced by Vaswani et al.\n(2017). The multi-head attention mechanism is based on scaled dot-product attention. Specifically,\nfor T tokens $X \\in \\mathbb{R}^{T\\times d_{in}}$ of $d_{in}$ dimensions each and T' tokens $X' \\in \\mathbb{R}^{T'\\times d_{in}}$ of $d_{in}$ dimensions\neach, the scaled dot-product attention is computed as follows:\nAttention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V,\nQ = XW_Q, K = X'W_K, V = X'W_V, (1)\nwhere $W_Q \\in \\mathbb{R}^{d_{in}\\times d_k}$, $W_K \\in \\mathbb{R}^{d_{in}\\times d_k}$, and $W_V \\in \\mathbb{R}^{d_{in}\\times d_v}$ represent the projection matrices for\nthe query, key, and value, respectively. In self-attention, the input tokens are the same, i.e., X' = X,\nand it is common for the key and value dimensions to be equal, i.e., $d_v = d_k$.\nTo enhance the representation power of the attention layer, Vaswani\net al. (2017) proposes to allow multiple attention heads to operate on different low-dimensional\nprojections of the input tokens. Specifically, the multi-head attention mechanism computes h different\nlow-dimensional projections of (Q, K, V), performs scaled dot-product attention for each head,\nconcatenates the results, and applies a final projection to the concatenated output. The concatenation\nform of the multi-head attention can be formulated as:\nMultiHead(X, X') = Concat(H^1, H^2, ..., H^h)W_o,\nH^i = Attention(XW_Q^i, X'W_K^i, X'W_V^i),\n(2)\nwhere $W_Q^i \\in \\mathbb{R}^{d_{in}\\times d_k/h}$, $W_K^i \\in \\mathbb{R}^{d_{in}\\times d_k/h}$, and $W_V^i \\in \\mathbb{R}^{d_{in}\\times d_v/h}$ represent the $i_{th}$ projection\nmatrices for the query, key, and value, respectively. $W_o \\in \\mathbb{R}^{d_v \\times d_{out}}$ is the final projection matrix.\nThe multi-head attention mechanism is typically represented in its concatena-\ntion form. However, from another perspective, if we decompose $W_o \\in \\mathbb{R}^{d_v \\times d_{out}}$ by rows, we can\nexpress multi-head attention in a summation form. Specifically, $W_o$ can be divided into h matrices\nby rows, i.e., $[W_o^1, W_o^2, ..., W_o^h] = W_o$, where $W_o^i \\in \\mathbb{R}^{d_v/h \\times d_{out}}$. Finally, the summation form\nof the multi-head attention can then be formulated as:\nMultiHead(X, X') = \\sum_{i=1}^{h}H^iW_o^i\n(3)"}, {"title": "MIXTURE-OF-HEAD ATTENTION", "content": "Recently, the Mixture-of-Experts (MoE) method has emerged as a popular approach for scaling the\nparameters of large language models (Jiang et al., 2024). A typical MoE layer consists of multiple\nexpert networks and a router that activates the Top-K experts. Generally, the number of activated\nexperts K is significantly smaller than the total number of experts to ensure inference efficiency.\nInspired by the great success of MoE, we propose Mixture-of-Head at-\ntention (MoH), which treats attention heads as experts. Specifically, MoH consists of h heads\n$H = {H^1, H^2, ..., H^h }$ and a router that activates the Top-K heads. Formally, given input tokens X\nand X', the output of MoH is the weighted sum of outputs from the K selected heads:\nMOH(X, X') = \\sum_{i=1}^{h} g_iH^iW_o^i,\n(4)\nwhere $g_i$ represents the routing score. $g_i$ is non-zero only when the $i_{th}$ attention head is activated.\nThis design provides two key advantages: On the one hand, MoH enables each token to select the\nmost relevant attention heads, boosting inference efficiency while maintaining accuracy. On the other\nhand, in contrast to the standard summation in multi-head attention, the weighted summation in MoH\nenhances the flexibility of the attention mechanism and unlocks performance potential.\nIn attention mechanism, some attention heads may capture common knowledge\nacross different contexts, such as grammatical rules in language. Inspired by Dai et al. (2024), we\ndesignate a subset of heads as shared heads that remain always activated. By consolidating common\nknowledge within shared heads, we reduce redundancy among the other dynamically routed heads.\nMoreover, to dynamically balance the weights between shared and routed\nheads, we propose a two-stage routing strategy. In this routing strategy, the routing scores are\ndetermined by both the score of each individual head and the score associated with the head type.\nSpecifically, given the $t_{th}$ input token $x_t \\in \\mathbb{R}^{d_{in}}$ in $X \\in \\mathbb{R}^{T\\times d_{in}}$, the routing score $g_i$ is defined as:\ng_i =\\begin{cases}\n\\alpha_1Softmax(W_s x_t)_i, & \\text{if } 1 \\le i \\le h_s, \\\\\n\\alpha_2Softmax(W_r x_t)_i, & \\text{if } (W_r x_t)_i \\in Top-K(\\{(W_r x_t)_i|h_s +1 \\le i \\le h\\}),\\\\\n0, & \\text{otherwise,}\n\\end{cases} (5)\nwhere $h_s$ denotes the number of shared heads. $W_s \\in \\mathbb{R}^{h_s\\times d_{in}}$ and $W_r \\in \\mathbb{R}^{(h-h_s)\\times d_{in}}$ represent the\nprojection matrices for the shared and routed heads, respectively. The coefficients $\\alpha_1$ and $\\alpha_2$ balance\nthe contributions of the shared and routed heads, and are defined as:\n[\\alpha_1, \\alpha_2] = Softmax(W_h x_t), (6)\nwhere $W_h \\in \\mathbb{R}^{2\\times d_{in}}$ is the trainable projection matrix, and $d_{in}$ is the hidden size of $x_t$.\nDirectly training an MoE layer often causes the majority of tokens to be routed\nto a small number of experts, leaving the remaining experts insufficiently trained (Shazeer et al., 2017).\nTo avoid the unbalanced load in the proposed MoH, following previous MoE methods (Lepikhin\net al., 2021; Wei et al., 2024), we apply a load balance loss. Specifically, for the $t_{th}$ input token\n$x_t \\in \\mathbb{R}^{d_{in}}$ in $X \\in \\mathbb{R}^{T\\times d_{in}}$, the load balance loss $L_b$ is formulated as:\nL_b = \\sum_{i=h_s+1}^{h} \\mathbb{I}(Token x_t \\text{ selects Head } i), P_i = \\frac{1}{T} \\sum_{t=1}^{T} Softmax(W_r x_t)_i, (7)\nwhere T denotes the number of tokens. $\\mathbb{I}(\\cdot)$ denotes the indicator function.\nIt is worth noting that the MoH is a general framework. Therefore,\nwe evaluate our proposed MoH across various popular model frameworks, including Vision Trans-\nformers (ViT), Diffusion models with Transformers (DiT), and Large Language Models (LLMs)."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "VIT FOR IMAGE CLASSIFICATION", "content": "For Vision Transformers (ViT) (Dosovitskiy et al., 2021), our MoH-ViT models\nare implemented based on the TransNeXt (Shi, 2024) framework and trained from scratch on the\nImageNet-1K dataset (Deng et al., 2009), which contains over 1.2 million images in 1,000 categories.\nTo ensure a fair comparison, we only replace the standard multi-head attention with the proposed\nMoH, while keeping all other training parameters identical to TransNeXt.\nOur MoH-ViT models are trained for 300 epochs using automatic mixed\nprecision across 8 GPUs. We follow the training strategy of TransNeXt, which includes various data\naugmentation techniques, including Random Augmentation (Cubuk et al., 2020), Mixup (Zhang,\n2017), CutMix (Yun et al., 2019), and Random Erasing (Zhong et al., 2020). We also apply Label\nSmoothing (Szegedy et al., 2016) and DropPath (Huang et al., 2016) to regularize our models. We\noptimize our models using AdamW optimizer (Loshchilov & Hutter, 2017) with a gradient clipping\nnorm of 1.0 and a weight decay of 0.05. The initial learning rate is set to le-3, with a 5-epoch\nwarm-up starting at 1e-6. A cosine learning rate scheduler (Loshchilov & Hutter, 2016) is employed\nto decay the learning rate. During training, images are randomly cropped to a size of 224x224. It is\nworth noting that we do not use Exponential Moving Average (EMA) weights.\nAs shown in Tab. 1, despite activating only a subset of attention heads, MoH-ViT achieves\nhighly competitive performance compared to current state-of-the-art methods. For example, MoH-\nViT-B achieves 84.9% Top-1 accuracy on the ImageNet-1K classification benchmark with just 75%\nof the attention head. In contrast, the well-established ViT baseline, TransNeXt, attains a slightly\nlower accuracy of 84.8% while requiring 100% of the heads to be activated. Tab. 1 demonstrates that\nMoH-ViT outperforms other models with fewer activated attention heads. This suggests that MoH is\na promising alternative to vanilla multi-head attention for vision model design, offering the potential\nfor competitive performance with more efficient attention head usage."}, {"title": "DIT FOR CLASS-CONDITIONAL IMAGE GENERATION", "content": "For Diffusion models with Transformers (DiT) (Peebles & Xie, 2023), we only\nreplace the standard multi-head attention with our MoH in MoH-DiT models, while keeping all\nother training parameters identical to DiT. We use the ImageNet-1K dataset (Deng et al., 2009) for\nclass-conditional image generation at a resolution of 256\u00d7256.\nFollowing DiT, the final linear layer is initialized with zeros, and all other\nlayers follow standard ViT weight initialization. We train all models using the AdamW opti-\nmizer (Loshchilov & Hutter, 2017) with a constant learning rate of 1e-4, no weight decay, and\na batch size of 256, applying horizontal flips for data augmentation. Following DiT, we employ\nthe Exponential Moving Average (EMA) of MoH-DiT weights during training with a decay rate\nof 0.9999, generating all images using the EMA model. We use an off-the-shelf pre-trained varia-\ntional autoencoder (Kingma, 2013) model from Stable Diffusion (Rombach et al., 2022). Following\nTransNeXt, our attention-head activation budget is unevenly distributed across layers, with fewer\nattention heads activated in the shallow layers and more in the deeper layers.\nTo evaluate generation performance, we use Frechet Inception\nDistance (FID) (Heusel et al., 2017) to assess overall sample quality, Precision and Re-\ncall (Kynk\u00e4\u00e4nniemi et al., 2019) to measure fidelity and diversity separately, and sFID (Nash\net al., 2021) as a metric that better captures spatial relationships than FID. Moreover, we use Inception\nScore (IS) (Salimans et al., 2016) as another metric for fidelity."}, {"title": "TRAINING LLMS FROM SCRATCH", "content": "For training LLMs from scratch, we use Megatron (Shoeybi et al., 2019), an\nopen-source training code, as the training framework. Please refer to the Appendix for detailed\nhyper-parameter settings (Tab. A) of various MoH-LLMs. All models are trained with the AdamW\noptimizer (Loshchilov & Hutter, 2017), using a batch size of 4 million tokens with a sequence length\nof 2048. The final learning rate is set to 10% of the maximum. During training, a weight decay of 0.1\nand gradient clipping of 1.0 are applied. For LLM-S and MoH-LLM-S, the maximum learning rate is\nset to 3e-4. For LLM-B and MoH-LLM-B, the maximum learning rate is set to 5e-4.\nWe only use public datasets for training, ensuring accessibility for academic\nresearch. Specifically, we sample from the RedPajama (Computer, 2023), Dolma (Soldaini et al.,\n2024), and Pile (Gao et al., 2020) datasets according to different sampling probabilities. Please\nrefer to the Appendix for detailed sample ratios (Tab. B). Following previous works, we utilize the\ntokenizer from LLaMA2 (Touvron et al., 2023), which contains 65,536 vocabulary tokens.\nThe evaluation is performed on multiple benchmarks using the Eleuther\nAI Language Model Evaluation Harness (Gao et al., 2024), a unified framework for testing generative\nlanguage models. Since the parameters are only about 0.2B for the smallest model, we select 6 simple\nbenchmarks as the metric. Specifically, we report 0-shot accuracy on SciQ (Welbl et al., 2017),\nPIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), OpenbookQA (Mihaylov et al.,\n2018), LogiQA (Liu et al., 2020), and TruthfulQA (Lin et al., 2022).\nAs shown in Tab. 4, despite activating only a subset of attention heads, MoH-LLMs\nachieve highly competitive performance compared to our baseline models. For example, MoH-LLM-\nS achieves an average accuracy of 45.4% with just 50% of the attention heads activated. In contrast,\nthe baseline model reaches a slightly lower accuracy of 43.9% with 100% of the attention heads\nactivated. These results suggest that MoH is a promising alternative to vanilla multi-head attention\nfor training LLMs from scratch. Surprisingly, we find that for MoH-LLM-S, activating only 50%\nof the attention heads outperforms activating 75%. We consider it may be because when both the"}, {"title": "CONTINUE-TUNING LLAMA3-8B", "content": "To significantly enhance the applicability of the proposed MoH method, we\nalso attempt to further continue-tune pre-trained multi-head attention models, such as LLaMA3-8B,\ninto MoH models. However, this presents three challenges. (i) We\nsimply select the first 16 attention heads of each layer as shared heads. (ii) Integrating a randomly initialized router into the pre-trained model without compromising\nits original performance requires careful training techniques. To address this, we propose a parameter-free router that determines routing scores using the $l_2$ norm of the query of each attention head.\n(iii) We observe that weighting the attention head outputs significantly\nalters the distribution of the output of the attention layer, which necessitates a large amount of training\ndata to restore the original performance. To tackle this, we quantize the routing score and use the\nstraight-through estimator (Bengio et al., 2013; Liu et al., 2022) to back-propagate the gradients\nthrough the sparsity function. Specifically, given the input token \u00e6, we employ a quantizer for\nactivation routing scores, with its forward pass formulated as:\ng = \\mathbb{I} (\\text{Token }x \\text{ selects Head }i), (9)\nwhere $\\mathbb{I}(\\cdot)$ denotes the indicator function. g represents the quantized routing score. We then adopt\na straight-through estimator, which assigns the incoming gradients to a threshold operation to be the\noutgoing gradients, which is formulated as:\n\\frac{\\partial g}{\\partial g_i} = \\frac{\\partial \\mathbb{I}}{\\partial g_i}, (10)\nwhere $g_i$ denotes the real-valued routing score. This simple approximation function significantly\nmitigates the issue of gradient vanishing (Wang et al., 2024). Similar to training LLMs from scratch,\nwe also use Megatron (Shoeybi et al., 2019), an open-source training code, as the training framework."}, {"title": "ABLATIVE ANALYSIS", "content": "To explore the impact of each component of\nour MoH method, we provide the ablation results in Tab. 6. \"Shared Heads\" refers to a subset of\nattention heads that are always activated. \"Two-Stage Routing\" represents the dynamic coefficient\nthat balances the weights between shared and routed heads over the routing score, as described in\nEq. 5 and Eq. 6. As shown in Tab. 6, shared heads significantly improve model performance by\neffectively capturing common knowledge, allowing the routed heads to focus more on domain-specific\ninformation. Moreover, two-stage routing further enhances model performance by dynamically"}, {"title": "DISCUSSION", "content": "As shown in Fig. 3, we observe significant variation\nin attention head assignments across different categories and task topics, indicating that the MoH\nmodel adapts to diverse tasks by employing distinct head assignment patterns. This characteristic of\nMoH allows different attention heads to focus on different types of tasks, making parameter utilization\nmore efficient than multi-head attention. For additional visualizations of MoH-LLaMA3-8B and a\ndetailed analysis of the head load distribution, please refer to Appendix D.\nWe clarify the differences between MoH and\nMoA (Zhang et al., 2022) from the following three aspects. First, in terms of motivation, the\ngoal of MoH is to improve the efficiency and performance of the attention mechanism without\nincreasing the number of parameters. In contrast, MoA shares the motivation of MoE, which is to\nexpand model parameters while keeping inference costs low. Therefore, the model settings of MoH\nare more stringent than those of MoA. Second, in terms of methodology, our MoH introduces\nshared heads and two-stage routing to enhance the standard MoE method. More importantly, we show\nthat pre-trained multi-head attention models can be further continue-tuned into our MoH models,\ngreatly improving the applicability of the proposed MoH method. In contrast, MoA directly combines\nmulti-head attention with MoE. Due to the adoption of shared keys and values, MoA must be trained\nfrom scratch, which limits its applicability. Finally, in terms of model frameworks, our MoH is\nvalidated across various popular model frameworks and tasks, including ViT, DiT, and decoder-only\nLLMs, while MoA is only validated on the encoder-decoder architecture for language tasks."}, {"title": "CONCLUSION", "content": "In this paper, we introduce MoH, a promising alternative to multi-head attention. MoH enables\neach token to adaptively select the appropriate attention heads, improving both model performance\nand inference efficiency without increasing the number of parameters. Extensive experiments"}, {"title": "APPENDIX", "content": "This appendix provides additional discussions (Appendix A), implementation details (Ap-pendix B), several additional experiments (Appendix C), more qualitative analysis (Appendix D),and details of quantitative evaluations for LLMs (Appendix E).\nWe have attached the code to the supplementary material. In this code, we also provide theevaluation process of the proposed method. We promise to release a more detailed and clean codeversion upon publication."}, {"title": "ADDITIONAL DISCUSSIONS", "content": ""}, {"title": "LIMITATIONS AND FUTURE WORK", "content": "In this section, we delineate the limitations of our work and outline avenues for future research.\nWe find that different attention heads operate in parallel within\nthe attention mechanism, suggesting that different heads can have varying hidden sizes. Future work\ncould explore the use of heterogeneous attention heads based on our MoH framework.\nCurrently, MoH outperforms multi-head attention by utilizing only\n50%~90% of the attention heads. However, this is still a relatively high proportion. Future work\ncould aim to further optimize MoH, reducing head activation to less than 50%.\nEffectively processing information from multiple modalities in the attention\nmechanism remains an open question. Recent work (Wan et al., 2024) has shown that visual and\ntextual tokens exhibit distinct attention patterns in multi-head attention. Future work could explore\nthe attention patterns of MoH with different modal inputs, for example within multimodal large\nlanguage models (Jin et al., 2024a; Lin et al., 2023; 2024; Liu et al., 2024; Jin et al., 2023b).\nWe evaluate our proposed MoH across various popular model frame-\nworks, including ViT for image classification, DiT for class-conditional image generation, and LLMs\nfor language tasks. Future work can explore the application of MoH in more downstream tasks, such\nas audio tasks and multimodal tasks (Jin et al., 2023a; 2022).\nDue to computational constraints, the maximum number of MoH model\nparameters in our experiments is limited to 8B (MoH-LLaMA3-8B). However, our MoH method is\nhighly generalizable and can be scaled to larger models in future research."}, {"title": "IMPLEMENTATION DETAILS", "content": ""}, {"title": "TRAINING LLMS FROM SCRATCH", "content": "For training LLMs from scratch, we use Megatron (Shoeybi et al., 2019), an\nopen-source training code, as the training framework."}, {"title": "CONTINUE-TUNING LLAMA3-8B", "content": "Tab. D shows the detailed training hyper-parameters of MoH-LLaMA3-8B. We find that if there is a discrepancy between the continue-training data and theoriginal training data distribution of the model, the performance of the model may fluctuate wildlyat the beginning of the training process. Since we do not have access to the raw training data ofLLaMA3, we address these potential performance fluctuations by dividing the training process intotwo stages. In the first stage, we continue-tune the original LLaMA3-8B model using 300B tokens to"}, {"title": "ADDITIONAL EXPERIMENTS", "content": "We divide the trainingprocess into two stages. Tab. E shows the comparison between MoH-LLaMA3-8B and the modelat the end of the first training stage (LLaMA3-8B-stage1). As shown in Tab. E, MoH-LLaMA3-8Bquickly recovers the performance of LLaMA3-8B-stage1 within a training budget of 100B tokens.Notably, in English language tasks, MoH-LLaMA3-8B surpasses LLaMA3-8B-stage1 while usingonly 75% of the attention heads. However, for Chinese language and math tasks, the recoveryperformance of the MoH model is not as strong as for English. For example, MoH-LLaMA3-8Beachieves an accuracy of 64.4 on CMMLU, compared to 66.0 for LLaMA3-8B-stage1. We attributethis to the fact that the model's Chinese and mathematical capabilities are primarily established duringthe first training stage. Since the first training stage uses only 300B tokens, significantly less than the15T tokens in LLaMA3-8B's pre-training, the model's abilities in these areas are not fully stable. Inthe second training stage, after switching to the MoH model, the model experiences more significant"}, {"title": "ADDITIONAL QUALITATIVE ANALYSIS", "content": "We provide additional visualization ofthe head load distribution in Fig. A. As illustrated in both Fig. 3 and Fig. A, there is notable variationin attention head assignments across different categories and task topics. This suggests that the MoHmodel adapts to a wide range of tasks by utilizing distinct head assignment patterns. This abilityenables MoH to allocate attention heads more effectively to specific task types, leading to moreefficient parameter utilization compared to standard multi-head attention.\nWe provideadditional visualization of the head load distribution in Fig. B. As shown in Fig. B, MoH-LLaMA3-8Bexhibits similar characteristics to MoH-LLMs trained from scratch, with significant variation in atten-tion head assignments across different categories and task topics. This indicates that continue-tuningenables the model to adopt different head assignment patterns quickly. These results demonstrate"}, {"title": "DETAILS OF QUANTITATIVE EVALUATIONS FOR LLMS", "content": "We conduct comparative comparisons of MoH-LLM (MoH-LLaMA3-8B) against vanillaLLMS (LLaMA3-8B). The evaluation is performed on multiple key benchmarks using the Eleuther AILanguage Model Evaluation Harness (Gao et al., 2024), a unified framework for testing generativelanguage models across a wide range of tasks. The benchmarks used for evaluation include:\n(Clark et al., 2018) is a multiple-choice question-answering resource featuring questions fromscience exams for grades 3 to 9. It is divided into two partitions: Easy and Challenge, with the lattercontaining more difficult questions that necessitate reasoning. Most questions offer four answerchoices, while less than 1% feature either three or five choices. Additionally, ARC includes asupporting knowledge base with 14.3 million unstructured text passages. We report 0-shot accuracyon ARC Easy and 25-shot accuracy on ARC Challenge."}]}