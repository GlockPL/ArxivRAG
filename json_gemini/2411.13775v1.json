{"title": "Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation Across Languages, Domains, and Expertise Levels", "authors": ["Jianhao Yan", "Pingchuan Yan", "Yulong Chen", "Jing Li", "Xianchao Zhu", "Yue Zhang"], "abstract": "This study presents a comprehensive evaluation of GPT-4's translation capabilities compared to human translators of varying expertise levels. Through systematic human evaluation using the MQM schema, we assess translations across three language pairs (Chinese English, Russian\u2190\u2192English, and Chinese Hindi) and three domains (News, Technology, and Biomedical). Our findings reveal that GPT-4 achieves perfor- mance comparable to junior-level translators in terms of total errors, while still lagging behind senior translators. Unlike traditional Neural Machine Translation systems, which show significant performance degradation in resource-poor language directions, GPT-4 maintains consistent translation quality across all evaluated language pairs. Through qualitative analysis, we identify distinctive patterns in translation approaches: GPT- 4 tends toward overly literal translations and exhibits lexical inconsistency, while human translators sometimes over-interpret context and introduce hallucinations. This study represents the first systematic comparison between LLM and human translators across different proficiency levels, providing valuable insights into the current capabilities and limitations of LLM-based translation systems.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT studies show that LLMs can serve as a strong translation system and a good substitute for NMT mod- els [1, 2, 3, 4, 5, 6, 7]. For example, [1] and [2] find that GPT-4 can outperform commercial machine translation systems using both automatic and human evaluation. Such impressive results have fostered a wide range of applications, such as the use of GPT-4 for literary translation [8], poetry translation [9] and cross-lingual summarization [10].\nWith the advance of LLM translation, traditional evaluation metrics such as BLEU are inadequate for evaluating their quality [11, 12]. While the research community has taken model-based approaches in recent work [13, 14], these metrics can suffer from model biases and distribution shifts. We take a different perspective by benchmarking them against human translators using systematic human evaluation. Our goal is to understand LLM translators by putting them into the translation industry, hiring professionals to evaluate their translation quality against the translation quality of different levels of human translators, and gaining insight into any systematic differences between LLM translations and human translations [1]. Such evaluation can complement existing studies and give a more comprehensive understanding of LLM translation quality.\nTo determine where LLMs fall within the spectrum of human translation proficiency, we take the current representative LLM, i.e., GPT-4, comparing it against human translators with different expertise, ranging from novice translators to seasoned professionals. A preliminary study comparing human transla- tions against GPT-4 translations shows that even experts cannot reach a high consensus on which translation is better, despite that they tend to give higher scores to human translations. The results indicate that LLMs do generate translations that approach the human level, and there can be potential subtle differences between LLMs and human translators, which are worth a more in-depth study.\nGiven these findings, we take a finer-grained evaluation across different languages and domains, so that translation quality can be better calibrated and systematic differences can be measured. Our evaluation covers three language pairs from resource-rich to resource-poor, i.e., Chinese English, Russian English, and Chinese Hindi, and three domains, i.e., News, Technology, and Biomedicine. Given a source sentence, we ask junior, medium, senior human translators and two machine translators, i.e., GPT-4 and Seamless, a traditional NMT translator, to generate the corresponding translation in the target language. Then we hire independent expert annotators to label the errors in the target sentence under the MQM schema [15]. Results show that while traditional NMT systems significantly underperform, GPT-4 reaches a comparable performance to junior/medium-level translators in the perspective of total errors made, yet lags behind senior ones with a considerable gap. GPT-4 represents a significant milestone in neural machine translation, establishing a baseline for legitimate comparative analysis with human translation performance.\nFurther language-specific results demonstrate that GPT-4 mitigates traditional machine translators' drawback of signif- icant performance gap from resource-high to resource-low directions. Our traditional MT translator Seamless can be a reliable translator for English Russian, but shows poor quality"}, {"title": "II. RELATED WORK", "content": "a) Benchmarking LLMs: Previous studies have bench- marked LLMs on various NLP tasks. [16] benchmark several LLMs on Chinese text, evaluating their Chinese ability. [17] assess LLMs through Question Answering (QA), MMLU [18], and other metrics. From these tests, LLMs with larger scales are generally proved to be more accurate except for certain tasks. [19] demonstrates that LLMs perform well in long-context understanding and are more capable with Out-of-Distribution, which means LLMs have a certain degree of generalization ability.\nFurther to the MT field, [20] find that GPT-4 performed competitively with other SotA translation products. [2] fur- ther investigated the capability of GPT-4 in document-level translation, the results show that GPT-4 performs better than commercial translation products and document NMT methods. Compared to them, our work empirically shows that GPT-4 is comparable to junior human translators.\nb) LLMs as Human Experts: Due to the great capacities of GPT-4 over traditional NLP models, researchers have investigated and compared the performance of GPT-4 as human experts in multiple NLP tasks. [21] highlight that GPT-4 and GPT-4-turbo show top performance on a Chinese financial language understanding task. [22] find the LLMs can be beneficial to biomedical NLP tasks. [23] compare GPT models with several summarization models and humans, and find that GPT can generate summaries preferred by humans. In Al for education area, [24] show GPT-4's can provide teaching feedback for students. [25] find that GPT-4 shows close performance compared with human participants in coordination games. [26] show that GPT-4 is comparable to humans on technical translation tasks. [27] find that GPT-4 can outperform human experts on linguistic pragmatic tasks. In clinical diagnostics, [28] find that GPT-4 can give comparable performance to humans, and GPT-4v (vision version) can even outperform human experts.\nc) Human Evaluation for MT: [29] first propose Direct Assessment (DA), which uses a continuous score from 0 to 100 to represent the quality of a hypothesis. DA has been adopted"}, {"title": "III. PRELIMINARY STUDY", "content": "We aim to first compare GPT-4 translations with human translations qualitatively, in a coarse manner. We hire expert annotators from the Lan-Bridge Group\u00b2, who are native Chinese speakers and dedicated to Chinese-English translation for over 5 years. Our comparison is simple and direct. We sample human-translated texts and prompt GPT-4 to translate the same source sentence. Then, we ask expert annotators to determine which translation is better.\nParticularly, to have a quick overview of the qualities of human translations against GPT-4 translations, we first utilize COMET-QE3 to score our in-house Chinese to English human-translated documents, and select two documents with the highest score and the lowest score. Note that our in- house translated documents are all translated by professional translators. In this way, we gather 40 pairs of translations from professional translators and GPT-4, respectively. Recent findings [15] have demonstrated that crowd-sourced human ratings are less reliable for high-quality MT evaluation. Thus, we hire six expert annotators to compare the two translations and select the better translations they find. We randomly shuffle the GPT-4 and human translations to prevent annotators from identifying GPT-4.\nThe average win rate of GPT is 15.5/40 (36.25%), which indicates a clear win for human translators. However, when delving deeper, we find that the expert annotators have a low ratio of agreement with each other. In Table I, the inter- agreement ratio between most annotators is around 60% (the baseline is 50%). A significance test shows that only annotator B finds human translation significantly better than GPT's"}, {"title": "IV. MAIN EXPERIMENTAL SETUP", "content": "We conduct a comprehensive and fine-grained evaluation of GPT-4 against professional human translators. Specifically, we employed the widely recognized Multidimensional Quality Metrics (MQM) framework [33] and compared human transla- tors with varying levels of expertise to GPT-4. Our evaluation spans multiple languages and domains, aiming to furnish broad insights into these comparisons.\nA. Data Collection\nWe strategically collect multilingual and multi-domain source sentences to evaluate translation across diverse resource levels. Based on GPT-3 corpus reports, we select four languages: English (dominant in most models), Russian (0.1884% words, 8th), Chinese (0.099% words, 16th), and Hindi (0.00483% words, 41st). This choice allows us to assess translations from resource-high to resource-low scenarios. English and Russian, both Indo-European, represent resource-high directions, while English and Chinese, from different language families, serve as resource-medium directions. Recognizing the scarcity of expert translators in low-resource languages, we use Chinese \u2192 Hindi as our resource-low directions. In summary, we evaluate six crucial directions: English Chinese, English Russian, and English \u2192 Hindi. This selection enables us to assess bidirectional translation performance across varied resource levels, providing valuable insights into current capability of the GPT4 translator against human translators.\nFor general domain Chinese English and English Russian translation, we sample source sentences from the test sets of WMT2023 and WMT2022, respectively. For Chinese Hindi, we extract source news text from public websites. For multi-domain evaluation data, we choose two domains, i.e., biomedicine and technology, evaluating Chinese to English translation. The source sentences are extracted news texts from public websites. We ensure that all sources are source language origin to avoid the effect of translationese. We manually evaluate all source sentences for these tasks to ensure that the source sentences are not too easy or too short. Finally, each task contains 200 sentences, making our evaluation a total of 1600 sentences.\nB. Human and Machine Translators\nWe ask different human translators from the same company as Section III to translate our source sentences into the target language. Translators are of three different levels of expertise, categorized into junior-level, medium-level, and senior-level translators. The level of expertise is ranked by in-house criteria covering the translators' educational background, translation experience, and practical proficiency. For instance, a junior translator could be a graduate student majoring in target languages and having one or two years of experience in the translation industry. A medium translator could be a translator who has 3-5 years of experience or is a native speaker of the target language. To be classified as a senior-level translator, an individual must possess a minimum of ten years of translation experience, demonstrate exceptional proficiency by achieving a score of 99% on quarterly assessments, and hold the distinguished CATTI++ translation certification. This categorization follows the tradition of the human translation industry.\nFor all directions except Zh-Hi and Hi-Zh, we collect three human translation results from each level of expertise. For Zh-Hi and Hi-Zh, we only have medium-level and senior-level translators due to the scarcity of translators.\nIn practice, we find that current human translators also heavily rely on GPTs or machine translators as assistance. Thus, to enable a fair comparison, we prohibit human translators from using machine translation or GPTs as assistance.\nWe use gpt-4-1106-preview, the current state-of-the- art large language model released by OpenAI and Seamless M4T [44] as the representative of traditional machine trans- lations to complement our experiments. We directly prompt GPT-4 to obtain the translation, as it is the most common practice for normal users, the easiest to reproduce, and to avoid confusion by various techniques.\nFor GPT-4, we use greedy search for decoding, to ensure the reproducibility of the results. For SeamlessM4T, we use the 2.3B version of seamlessM4T_v2_large and adopt beam search with beam size 5.\nC. Prompt Search\nPrevious study [45, 46] shows that different prompts with LLMs can result in distinctive performance. Thus, we collect three candidate prompts used in previous research [1, 47] and use COMET-QE [48] to select the best prompt to make the best use of GPT-4, as shown in Table III. In particular, we use these three prompts to prompt GPT-4 to translate 100 source sentences in our Chinese-to-English test set and adopt COMET-QE to evaluate the quality of translations. We find that the third prompt yields the best performance, and hence we adopt this prompt for all following experiments."}, {"title": "V. MAIN RESULTS", "content": "A. Overall Results\nThe overall results are shown in Table V, where each row represents a translator and each column represents the error score of each type. We compare the traditional machine translator, i.e., Seamless, LLM translator, i.e., GPT-4, and different levels of human translators.\na) Error Severity: The left part of Table V presents the averaged number of errors of different systems and translators. We normalize each error by the ratio of its span length against the length of the candidate translation. Our results demonstrate that GPT-4 represents a significant milestone in neural machine translation, establishing a baseline for legitimate comparative analysis with human translation performance, while previous models demonstrably lag behind human capabilities. Specif- ically, compared to our MT baseline (seamless), GPT-4 has significantly fewer errors, i.e., 29.52 vs 20.43 minor errors and 17.35 vs 3.71 major errors. GPT4 translator is comparable with junior/medium-level human translators, producing 17.35/3.71 minor/major errors compared to 18.19/3.27 for junior-level and 20.19/3.30 for medium-level translators. However, GPT4 lags behind senior translators by a significant margin, i.e., 3.71 vs 1.83 major errors, indicating machine translation is yet a solved problem. To our knowledge, this is the first report on GPT-4 as a translator against various expertise levels of human translators.\nb) Error Categories: The right part of Table V shows our overall results for two categories of MQM errors, namely Accuracy and Fluency [15]. We merge different error severity by assigning them weights, where a minor error weights 1 and a major error weights 3, as done in [15]. GPT-4's translation demonstrates good accuracy but worse language usage when compared with expert translators. In particular, GPT-4 makes comparable errors in the accuracy of translations with junior/medium human translators (11.12 vs 8.55/12.58), while it makes 12.95 fluency errors, weaker than all human translators. We also find the seniority of human translators mainly manifests progressively refined language usage, i.e., fluency errors from 12.74 to 5.93, while accuracy errors show fluctuations across seniority levels.\nWe further investigate errors made by each translator. Figure 1 shows the top 5 categories of errors made by different systems. 'Mistranslation' is the most frequent error made by all systems. Improving much over the seamless baseline, GPT-4 makes comparable numbers of 'Mistranslation' with junior and medium human translators. \u201cUnnatural Flow' is among the most frequent errors for all translators. Seamless, GPT-4, and junior translators have similar levels of \u2018Unnatural Flow', indicating possible issues of literal translation and not following language conventions. In contrast, medium and senior translators are annotated with significantly fewer 'Unnatural Flow' errors. GPT-4 makes much fewer 'Incorrect Named Entity(NE)' errors compared to Seamless, which can be because of the huge knowledge acquired in the pre-training stage. However, it still has a gap compared to human translators. Finally, we notice that GPT-4 does not have Omission or Addition problems in its top- 5 errors, whereas even senior translators have Addition errors. This can be because of the tendency for machine translators to use literal translation. We will discuss this further in the following sections.\nB. Language-Specific Results\nWe delve deeper into the detailed results for different languages, domains, and cases. The language-specific results are shown in Table VI, which provides our language-specific results on Accuracy and Fluency, as in the previous section, average to and from directions of a language pair. We give language-specific discussions below, with more detailed metrics being plotted in Figure 2.\na) English Russian: For English-Russian translation, we find that the MT baseline is comparable with GPT-4, with slightly better accuracy (14.79 vs 15.82) and slightly worse fluency (14.60 vs 14.11). More importantly, when comparing these two machine translators to human translators, we observe comparable performance. Seamless has the lowest level of accuracy errors among all translators, and GPT-4 reaches the same level of fluency as senior translators, indicating both machine translators are reliable substitutions of human translators in resource-high directions like English Russian. In Figure 2(a), we can find that each different human translator has their strengths and weaknesses, performing well or poorly in certain error categories. For instance, Seamless has more errors in Grammar, Punctuation, and Spelling; but fewer errors in other aspects. GPT-4 is more balanced and similar to human translators in terms of errors made.\nb) English Chinese: Here, we observe a distinct pattern shift. Seamless degrades significantly, scoring 129.19 in Ac- curacy and 55.46 in Fluency. This may be due to unbalanced training data for supervision data. In the meanwhile, GPT-4 is still comparable to human translators, surpassing junior/medium translators for Accuracy (29.57 vs 35.99/37.59) for Accuracy, and performing closely for Fluency (34.98 vs 34.19/34.18). Senior translators substantially outperform GPT-4, attaining a score of 12.40/7.60 for Accuracy and Fluency.\nFrom the radar chart (Figure 2(b)), we also notice a similar trend. Different from the English-Russian translator, Seamless has more Mistranslation, Incorrect NE, Punctuation, and Unnatural Flow errors, accounting for its performance drop. On the other hand, GPT-4 performs well overall, except for more errors in Incorrect NE, Grammar, and Register compared with human translators. As in the absence of reference, GPT-4 would translate unfamiliar words directly and literally instead of seeking online materials or other forms of help like human translators. For example, when translating the phrase 'the Safer Transport Command', GPT4 translates the phrases to \u201c\u66f4(more)\u5b89\u5168(safe)\u4ea4\u901a(traffic)\u6307\u6325\u90e8(command center)\u201d in Chinese. It implies \u2018a more safe traffic command center' rather than capturing the intended meaning of a specialized police unit focused on transport safety. For other errors, GPT-4 is among the level of junior/medium translators.\nc) Hindi Chinese: Compared to the directions discussed before, the overall number of errors made by all translators is higher, revealing the difficulty of our resource-low directions. We also notice the same pattern shift as in Chinese-English traditional machine translation is not reliable anymore but GPT-4 maintains its performance. Specifically, a huge gap is observed between Seamless and other translators, with an error score over 200 vs 40-60 for accuracy, indicating an influence of scarcity of resources. In contrast, GPT-4 attains a better Accuracy score (44.36 vs 59.03/45.39) compared with human translators. Its fluency score (22.83) is the same as medium human translators, and slightly weaker than senior translators (15.19).\nFrom Figure 2(c), Seamless stands out with significantly more errors in Mistranslation (175) than all the other translators. Unlike Seamless, GPT-4 excels in technical aspects like Spelling and Punctuation as well as maintaining source contents, demonstrated in Omission and Addition. However, GPT-4 struggles with Incorrect NE and Grammar, as in Chinese- English.\nd) Discussion: Our results here manifest an imbalance of multilinguality [52]. Our results suggest that GPT-4 is a reliable translator for all three six evaluated directions, while traditional machine translators such as Seamless are limited by the resources of the directions and only reliable in resource-high directions. For most cases, GPT-4 is comparable/better than junior/medium-level human translators, especially in Accuracy. One noticeable drawback of GPT4 across resource-medium and resource-low directions is the Incorrect NE. In future"}, {"title": "VI. CONCLUSION", "content": "We comprehensively evaluated the translation quality of GPT- 4 against human translators of varying expertise levels across multiple language pairs and domains, finding that the LLM per- forms significantly better than the Seamless, a traditional NMT system, and comparably to junior/medium translators in terms of total errors made. However, LLMs still lag behind senior translators. We also notice that GPT-4's translation capability weakens from resource-rich to resource-poor language pairs. Qualitative analysis revealed that GPT-4 produces more literal translations and consistent word usage than human translators but suffers less from imagined information. The results above demonstrate that in all directions, LLMs have the potential to replace human translators, especially junior and medium ones feasibly. To our knowledge, we are the first to report evaluation of machine translation systems by ranking them among professional human translators according to industry- level metrics."}, {"title": "APPENDIX", "content": "To categorize translators into junior, medium, or senior levels, we have established a comprehensive set of criteria that take into account various factors indicative of a trans- lator's expertise and experience. These factors include the translator's educational background, particularly the prestige of the institution from which they graduated, as well as their length of service in the translation industry, the duration of their translation career, the number of translations completed, and any professional certifications they have obtained. To ensure the ongoing competence of our translators, we conduct quarterly assessments to evaluate their performance. For instance, to be classified as a senior-level translator, an individual must possess a minimum of ten years of translation experience, demonstrate exceptional proficiency by achieving a score of 99% on our assessments, and hold the distinguished CATTI++ translation certification. By considering these stringent criteria, we aim to maintain a highly qualified and skilled pool of translators across all levels of expertise.\nA. Error Types\nOur annotation system is built upon the open-sourced doccano system 5. In Figure 4, we provide a screenshot of our annotation system. For each source sentence, outputs for different systems are given and the annotators can select spans of the text and annotate the error type and severity.\nOur evaluation protocol largely follows the MQM criteria released by Unbabel. We provide a detailed annotation manual for annotators, including an explanation for each error type as well as illustrative examples for error types. It is included in the following:\nB. Annotation Requirements\nThe minimum unit that can be selected and annotated is a whole word, a whitespace, a punctuation mark, or an isolated character. In the following example, the version in French has"}]}