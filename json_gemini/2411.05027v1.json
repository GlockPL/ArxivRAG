{"title": "Generative Artificial Intelligence Meets Synthetic Aperture Radar: A Survey", "authors": ["Zhongling Huang", "Xidan Zhang", "Zuqian Tang", "Feng Xu", "Mihai Datcu", "Junwei Han"], "abstract": "SAR images possess unique attributes that present challenges for both human observers and vision Al models to interpret, owing to their electromagnetic characteristics. The interpretation of SAR images encounters various hurdles, with one of the primary obstacles being the data itself, which includes issues related to both the quantity and quality of the data. The challenges can be addressed using generative AI technologies. Generative AI, often known as GenAI, is a very advanced and powerful technology in the field of artificial intelligence that has gained significant attention. The advancement has created possibilities for the creation of texts, photorealistic pictures, videos, and material in various modalities. This paper aims to comprehensively investigate the intersection of GenAI and SAR. First, we illustrate the common data generation-based applications in SAR field and compare them with computer vision tasks, analyzing the similarity, difference, and general challenges of them. Then, an overview of the latest GenAI models is systematically reviewed, including various basic models and their variations targeting the general challenges. Additionally, the corresponding applications in SAR domain are also included. Specifically, we propose to summarize the physical model based simulation approaches for SAR, and analyze the hybrid modeling methods that combine the GenAI and interpretable models. The evaluation methods that have been or could be applied to SAR, are also explored. Finally, the potential challenges and future prospects are discussed. To our best knowledge, this survey is the first exhaustive examination of the interdiscipline of SAR and GenAI, encompassing a wide range of topics, including deep neural networks, physical models, computer vision, and SAR images.", "sections": [{"title": "I. MOTIVATION AND SIGNIFICANCE OF THE TOPIC", "content": "Synthetic Aperture Radar (SAR) has garnered significant interest in a wide range of earth observation applications [1]\u2013[5]. SAR images possess unique attributes that present challenges for both human observers and vision AI models to interpret, owing to their electromagnetic characteristics [6]-[9]. The analysis of SAR images has numerous challenges, with one of the main hindrances being the data itself, encompassing concerns regarding both the volume and the quality of the data. The backscattering properties of objects display notable fluctuations depending on elements such as the frequency of operation, polarization, and viewing angles. As a result, this creates significant inconsistencies in the data and restricts the generalization ability of an AI model trained on that data. There is a pressing requirement to generate additional data in different application scenarios in order to meet the criterion for training data. Moreover, it is frequently important to augment the ability to understand SAR images by enhancing the quality of the data. The presence of sensor-specific speckle noise and azimuth ambiguity, for example, significantly impairs the image quality, making it challenging to comprehend [10].\nThe aforementioned challenges can be addressed using Generative AI technologies. Generative AI, often known as GenAI, is a very advanced and powerful technology in the field of artificial intelligence that has gained significant attention. The field has shown significant growth since the start of generative adversarial networks (GANs) in 2014, and particularly accelerated with the emergence of multi-modality foundation models recently. With the emergence of various powerful GenAI models, such as GPT (OpenAI) [11], LaMDA (Google) [12], LLaMA (Meta) [13] for text generation, DALLE 3 [14], Stable Diffusion [15], Imagen [16], and Midjourney [17] for text-to-image generation, Sora [18] for text-to-video generation, the field has entered a new age. The advancement has created possibilities for the creation of texts, photorealistic pictures, videos, and material in various modalities. The created information emulates human language and visual perception, facilitating human comprehension of the world [19]. The fundamental technologies behind them consist of Transformer-based large language models (LLMs), vision foundation models (VFMs), GANs, Diffusion models, etc. For instance, the LLMs are employed by the text-to-image GenAI models to encode the text prompt as semantic embedding. Subsequently, it is employed to condition the generator in order to generate the image. This can be accomplished through the application of diffusion models, as demonstrated by Imagen and DALL-E 3.\nIn the field of computer vision, the problem of image generation and reconstruction via GenAI models has been thoroughly discussed and explored. There are, however, some important differences when GenAI and SAR collide. Most of the existing GenAI models exhibit a deficiency in SAR knowledge and a lack of awareness regarding the intricate physical properties and complex nature of SAR data. In pursuit of this objective, they encounter limitations in their ability to produce a wide range of SAR images that possess appropriate physical characteristics, including polarimetry and interferometry. For SAR data, the vision-language model, often known as the vision GenAI model, would not be effective. In addition, the process of generating images using Synthetic Aperture Radar (SAR) should take into account certain physical properties, such as sub-aperture coherency. Moreover, the assessment of generated synthetic aperture radar (SAR) images may vary from that of optical data due to the inherent differences in their imaging mechanisms, thereby rendering it unreliable. It is of utmost importance to highlight the particulars of problem definition and the potential challenges associated with it in order to effectively identify multiple solutions for SAR data.\nThe extant literature reviews have furnished a clear overview of the applications of auto-encoders and GAN-based models in the realm of SAR/ISAR, specifically in the domain of despeckling [32], [33]. Nevertheless, a number of cutting-edge GenAI models were excluded, and there is a scarcity of comprehensive comparisons and analyses with other pertinent domains that could provide valuable insights for SAR applications. More specifically, the survey is conducted by examining various aspects, including applications, methods, evaluations, and perspectives. We not only provide a summary of the current approaches for generating SAR images using deep learning, but also conduct a comprehensive analysis of the literature in the related domain that addresses the general challenges associated with SAR. The outline of this survey is briefly illustrated in Fig. 1.\nDespite some initial accomplishments, the full potential of GenAI in the field of SAR remains largely unexplored."}, {"title": "II. APPLICATIONS", "content": "The limitations in both the quality and quantity of data are significant obstacles that impede the progress of deep learning based SAR image interpretation, as previously discussed. In order to accomplish this objective, the primary aims of utilising GenAI models in the context of SAR involve the augmentation of images and the enhancement of image quality. Although computer vision applications are prevalent, there are significant differences in the case of SAR. This section will comprehensively include the comparisons. The illustrations are provided in Fig. 2 to demonstrate the typical applications with similarity and difference in SAR and vision domain. \n\n\n\nA. Towards Increasing Data Quantity\nThe limited availability of training data has impeded the advancement of deep learning methods in the domain of SAR image interpretation. The importance lies not in the amount of SAR data, but rather in the abundance of SAR images in specific application scenarios [84]. This section introduces three common applications: multi-view SAR image synthesis, optical-to-SAR (O2S) translation, and SAR image composition.\n1) Multi-View Image Generation: The community is primarily focused on the issue of generating multi-view SAR images, which has significant practical implications for SAR target detection and recognition when there is a scarcity of training data. This task has been highlighted in the literature by Zhang et al. [32]. The objective is to generate new images from alternative perspectives using a collection of images captured from restricted angles. The multi-view image synthesis or 3D reconstruction work shows a substantial difference due to the unique imaging mechanisms of optical and SAR. Optical images are captured by optical sensors, such as cameras, and rely on light within the visible and near-infrared spectral ranges. They capture optical features such as the color, shape, and texture of the target surface. The objective of multi-view image generation for optical images is to recreate the optical characteristics from several perspectives, simulating the effect of being taken by multiple cameras. As a comparison, SAR system emits a pulsed electromagnetic wave, which interacts with the target surface upon reflection. The radar antenna captures the backscattered signal, which contains information about the target's properties. The electromagnetic features such as backscattering intensity, multiple bouncing effects, instead of optical features of color and shape, are required in multi-view SAR image generation. Moreover, the scattering properties of SAR targets exhibit considerable variation depending on imaging factors like aspect angle and sensor resolution. This poses a greater challenge in generating many views with limited observations. Many researches focus on this task including but not limit to [20], [34]\u2013[43] and we will illustrate them elaborately in the later sections.\n2) Optical-to-SAR Translation: The goal of the optical-to-SAR (O2S) translation task is to utilize optical time series data to complete the missing information in the SAR time series data, either to augment SAR data or to improve SAR's spatial resolution. In contrast to the well-researched field of SAR-to-Optical translation, O2S is more difficult to tackle because of the ill-posed nature of the translation. The complexity occurs due to the fact that a same optical data might have various SAR representations, depending on the SAR viewing geometry.\nSeveral studies have concentrated on this particular task, including [21], [46], [47]. Fu et al. [47] introduced a modified Pix2Pix architecture with multiscale cascaded residual connections for SAR-optical reciprocal translation. The pixel-domain adaptation (PDA) proposed by Shi [21] employed CycleGAN to generate a transition domain. The transition domain and the source domain was then combined to enrich the insufficient SAR data for training and improve the detection model's capacity to generalize. The proposed Temporal Shifting GAN (TSGAN) [46] is a dual conditional GAN designed for optical-to-SAR translation. The model was built based on a novel attention-based siamese encoder, and a change weighted loss function was also employed to prevent the TSGAN from overfitting. The TSGAN obtained foundational geometry from the prior timestamp SAR data and then learned to generate a new sample by adjusting the input SAR data to align with the alterations in the optical data.\n3) Image Composition: The task of image composition is primarily necessary for the detection of SAR targets in complex environments. The objective of this study is to generate a composition image that seamlessly blends a variety of SAR target patches, SAR scene images with complex background, and other source images, if available, in a logically consistent manner. The computer vision domain has extensively investigated and applied the concept in diverse applications, including digital photography and graphic design [85]. The main steps include object placement, image blending, image harmonization and shadow generation. During the era of generative AI, the task of generative image composition has evolved into an integrated process that combines the aforementioned sub-tasks into a single unified model. There are a few SAR-related studies working on this direction [49]\u2013[51] which we introduced in the following.\nSun et al. [49] proposed a style embedded ship augmentation network (SEA) to seamlessly blend the ship target slices with the background images. Given the location masks, the generator could output the harmonized images. Kuang et al. [50] suggested a paradigm for collaborative SAR sample augmentation, aiming to achieve flexible and diversified high-quality sample augmentation. Initially, semantic layout was utilized to guide the generation of detection samples. Additionally, a Pix2Pix network was employed to effectively enhance the variety of backgrounds under the guidance of layout maps. Finally, progressive training strategy of the conditional diffusion model, together with a sample cleaning strategy was adopted for sample quality improvement. Luan et al. [51] introduced a SAR target background conversion network (BCNet) that integrated slices of targets of interest (TOI) with slices of large-scene. The TOI slices' background was transformed into large-scene background, while still maintaining the target's scattering characteristics. In this way, the generated data with background diversity and variability can be supplemented to the original dataset."}, {"title": "B. Towards Improving Data Quality", "content": "Generative models are extensively employed in the domain of computer vision for various image reconstruction tasks, including denoising, deblurring, dehazing, and super-resolution, with the objective of enhancing the overall image quality. There are also some similar tasks in SAR community, such as SAR image despeckling, auto-focusing, super-resolution, and interference suppression [86], [87]. However, there are certain peculiarities in the image quality deterioration process when it comes to the particular properties of SAR.\n1) Despeckling: The major difference of denoising application between SAR and optical data lies in the formation of noise, and it differs due to the nature of the sensors and the imaging processes involved. For SAR image, the thermal noise from the electronics, noise from the radar pulses or antenna, and the well-known speckle noise, could contaminate SAR signal and lead to degraded imaging result. Among them, the speckle noise has attracted most attention in recent years. It is a result of the random scattering of radar waves by the terrain or objects in the scene, and is influenced by radar system parameters, scene characteristics, and the imaging conditions. Speckle noise is spatially correlated and signal dependant as a result of SAR's coherent sensing nature; this can be expressed as a multiplicative model. On the other hand, optical image noise is often addictive. Fracastoro et al. have conducted a thorough survey regarding SAR image despeckling based on deep learning methods [33] so that we will not include too much in this field. Some selected representative work after 2022 are reviewed in this section, including diffusion model-based approaches.\nBy utilizing the capabilities of CycleGANs, the proposed CycleSAR [53] regarded the despeckling task as an unpaired image-to-image translation task, enabling training without ground truth images. The integration of a Conditional VAE (CVAE) allowed for generating diverse speckled images, thus resulting in enhanced despeckling performance. Combing the similarity-based block-matching and noise referenced deep learning network, Wang et al. [54] presented a new self-supervised speckling method based on auto-encoder. The proposed method exhibited both excellent despeckling performance and impressive generalization capacity. The proposed SAR-CAM [55] enhanced the performance of an encoder-decoder CNN architecture for SAR image despeckling by incorporating different attention modules. Furthermore, the multi-scale information were obtained through a context block at the minimum scale.\nRecently, the diffusion model-based SAR image despeckling have emerged. In SAR-DDPM [23], the despeckled images were acquired during the reverse process, where the added noise were predicted by a noise predictor conditioned on the speckled image. Furthermore, in order to improve the despeckling results, a novel inference approach utilizing cycle spinning was implemented. Another diffusion-based despeckling technique is R-DDPM [56], which used overlapping region sampling in the reverse process to direct inverse diffusion, enabling excellent despeckling of SAR images without artifacts.\n\n2) Colorization: In computer vision domain, the colorization aims to synthesize the color information for a gray-scale image, for applications such as historical photography recovery, archival restoration, and artistic manipulation. The single polarized SAR image is visually represented in grayscale, which can be less easily interpreted. Nevertheless, the task of colorization in the SAR domain may exhibit fundamental differences due to the unique imaging mechanism employed. The term \"pseudo-color\" SAR images is used to describe polarimetric SAR data with multiple channels, where polarimetric decomposition processing is performed to assign distinct polarimetric features to color channels. As a result, colorization would leads to different task definition for SAR.\nSAR-to-Optical Translation: It is one of the most widely accepted definitions of SAR image colorization which pertains to the modality transfer problem. Compared with O2S task, SAR-to-Optical (S2O) is well-established and widely explored. The input SAR image is transferred to an optical image with true colors that represent the visual characteristics of objects. The contours, shapes, textures in optical features, are depicted in the generated optical image. This task will facilitate applications such as cloud removal, vegetation index reconstruction, and the construction of high temporal and spectral resolution time-series data, etc.\nThe researches focusing on this task include [24], [47], [58]-[70] and we will elaborate them in the following sections.\nSAR-to-Optical-Color Translation: In this paper, we refer to another form of colorization, similar to S2O, as SAR-to-Optical-color (S2Oc) translation. S2O translates the gray-scale SAR images to optical ones completely with optical features of objects including colors. On the other hand, the S2Oc translation process exclusively converts gray-scale pixels into colored ones, while preserving the primary structure of objects. Thus, by altering only the color information, the scattering characteristics of SAR remain unchanged.\nA few representative studies explored this task, such as [25], [70], [72]. Wang et al. [70] introduced a DCGAN-based method designed for despeckling and colorization progressively. In [72], the creation of artificial color SAR images was initially undertaken as a result of the lack of available color SAR images for training purposes. This was achieved by fusing SAR and multi-spectral images. Furthermore, in order to produce multi-modal colorization hypotheses, both a variational autoencoder (VAE) and a mixture density network (MDN) were employed. Shen et al. [25] proposed a benchmarking protocol for SAR colorization. The authors introduced several baselines for SAR colorization and then a Pix2Pix-based network called CGAN4ColSAR was specially designed for the purpose of SAR colorization.\nPolarimetric SAR Image Generation: In contrast to the aforementioned definitions, this term pertains to the process of inferring the polarimetric physical scattering characteristics from a gray-scale single polarimetric or dual-polarization SAR data. The extraction of physical information from polarized SAR images is more informative compared to single and dual polarization SAR data. From a theoretical standpoint, the problem can be characterized as an inverse and ill-posed one, as it is not possible to directly obtain signals in other polarization channels from a single polarization channel. By utilizing an AI model, it becomes feasible to address this issue to a certain degree. Once the objects are interpreted from single polarized SAR image and the characteristics such as geometry and surface roughness are identified, AI model can be employed to transfer the knowledge learned from physical model to generate the polarimetric physical scattering parameters of the objects. Several studies have identified the feasibility primarily.\nSong et al. [26] proposed the concept of \"radar image colorization\" of reconstructing the full-pol image from a non-full-pol SAR image to enable the current PolSAR image analysis methods can be applied to it. The \"color\" refers to polarimetric decomposition features, and the designed deep neural network aims to predict the polarimetric covariance matrix of each pixel. Based on this work, Deng et al. [89], [90] proposed to generate the pseudo-quad-pol SAR image from dual-pol SAR data which reconstruct the fully polarimetric information partially. The generated pseudo-quad-pol SAR images are then used for urban damage-level estimation based on polarimetric coherence pattern interpretation.\nAnother related field is full-pol coherence matrix or covariance matrix reconstruction from compact polarimetric (CP) SAR with deep neural networks [91]. Zhang et al. [92] proposed a complex-valued dual-branch convolutional neural network (CV-DBCNN) to achieve the reconstruction of the pseudo quad-pol data from CP SAR image, and the method was evaluated with polarimetric decomposition to demonstrate its superiority in terms of the pseudo quad-pol data reconstruction and scattering mechanism preservation. Aghababaei et al. [93] proposed a deep learning based framework to reconstruct the full-pol information from typical dual-pol data. Specifically, the designed loss function accounted for different scattering properties of the polarimetric data, such as the covariance matrix, SPAN which refers to the image generated by the sum of the diagonal elements of the covariance matrix, and the statistical distribution of the eigenvalues of covariance matrix."}, {"title": "C. Others", "content": "In addition to the aforementioned applications, there exist several other less common applications related to generative models. We will provide a brief introduction to these applications below.\nInterference Suppression: Radio frequency interference (RFI) is a critical issue in SAR imagery, as it can significantly degrade the quality and usability of the data. RFI refers to unwanted signals that contaminate the SAR signal and introduce noise into the image. Aiming at interference suppression using deep learning, Oyedare et al. [82] proposed to review the literature in depth, in which the auto-encoder based models are considered as one of the popular approaches. In SAR image domain, Wei et al. [81] proposed an auto-encoder based CARNet to address the SAR image interference suppression problem. The encoder-decoder architecture was designed to suppress interference and produce hierarchical-level feature maps for target information exchange.\nSAR Image Super-resolution: It is a not well-defined task instead of that in optical image domain. Several studies adhere to the concept of enhancing spatial details, which aligns with the definition established for optical images [73], [74], [76]. The aim is to improve the visual quality of the low-resolution SAR images. Another definition posits that SAR image super-resolution involves the generation of a SAR image that closely resembles the output of a high-resolution sensor, based on the SAR image acquired from a different sensor with lower resolution [94]. The high-resolution SAR sensor, due to its wide bandwidth for transmitting waves and large synthesized aperture, is capable of producing SAR images with significantly distinct characteristics compared to low-resolution ones. According to this definition, SAR image super-resolution cannot be equated directly with the enhancement of visual quality.\nSome selected work are briefly introduced. A residual convolutional neural network was proposed for polarimetric SAR image super-resolution in [74]. The interpolation was replaced by deconvolution to reduce the loss of precision and computational burden. The addition of PReLU helps to retain the negative information and enhance the accuracy. A complicated structure block was also specifically designed to preserve the complex features of PolSAR data. Some other researches achieved SAR image resolution based on generative adversarial network. For instance, Gu et al. [73] proposed a noise-free GAN to generate high-resolution SAR images from low-resolution images. The despeckling network and the reconstruction network jointly constituted the generator and discriminator was used to distinguish real and fake high-resolution images. DMSC-GAN [76] was another GAN-based SAR image super-resolution technique. In order to extract informative features, convolutional operations were combined with Deformable Multi-Head Self-Attention (DMSA) and a multi-scale feature extraction pyramid layer was also employed. Moreover, the perceptual loss and feature matching loss enabled the generator to obtain more comprehensive feedback from the discriminator.\nLayout-to-Image Generation: It pertains to the process of generating a variety of images based on a given layout, which includes the target information about the class and location. SARGAN [79] was a GAN-based method proposed to generate various images for SAR ship detection task. The latent vector for each class was initially obtained through the target encoder and the generator can successfully produce diverse SAR ship images under the guidance of the constructed scene. Ship-Go [22] is a specially designed instance-to-image multi-condition diffusion model to generate SAR images for object detection. Taking visual instances and environment prompt as conditions, Ship-Go can smoothly place the ship objects into the generated background of various environment types."}, {"title": "D. General Challenges", "content": "We summarize the general challenges in vision and SAR domain, including controllable generation, data constraint generation, and prior guided generation.\n1) Controllable Generation: Controllable generation refers to the ability to generate images with specific attributes or features defined by users. It often requires the model to disentangle the underlying factors of variation in the data. If the model cannot separate these factors in the latent space, controlling individual attributes becomes difficult because changes in one attribute may inadvertently affect others. In SAR image domain, the most widely explored attributes are target category and azimuth angle [20], [36]\u2013[43]. Besides, the background clutter distribution and the complex environment can be also a controlled factor for SAR image generation [22].\n2) Data Constraint Generation: Data constraint is an important topic when data acquisition is challenging. Training a generative AI model inevitably requires sufficient data to achieve good performance, but lack of training samples is a main obstacle in real-world applications. Abdollahzadeh et al. [95] proposed a comprehensive literature review on generative modeling with limited data, few shots, and zero shot. Similarly, Yang et al. [96] summarized image synthesis under limited data. In this paper, we conclude the data constraint challenge in generative modeling as the following three aspects based on representative computer vision tasks and the corresponding crucial SAR applications, as given in Fig. 5.\nLimited Data: It refers to traditional limited training data challenge, where the generative model is trained on insufficient data that would lead to mode collapse, overfitting, and unstable training. In SAR community, it is often occurs with limited training data due to the high cost of data acquirement compared with computer vision field. On the other hand, some specific applications, such as learning to generate SAR targets of different azimuth angles from sparse observations, also belong to this issue, as illustrated in Fig. 5 (a).\nFew-shot Generative Model Adaption: It refers to transferring the knowledge of pre-trained generative AI models from large-scale source domains to target domains with limited data. The adapted generative model should not only inherit the generation ability of source model with invariant attributes, but also be aware of the target distribution to generate novel images. The similar application in SAR domain is presented in Fig. 5 (b). The generative model M\u2081 is trained on SAR images obtained from sensor A, and it is capable to generate multi-view SAR targets controlled by azimuth angle. Given a few SAR images obtained from sensor B with different system parameters, the few-shot generative model adaption aim to adapt M1 to M2. Different sensor parameters would lead to domain drift such as various noise distribution. As a result, M2 should be adapted to generate SAR images cross the sensors.\nFew-shot Image Generation: It aims to generate the images of unseen classes with few-shot samples based on training with sufficient samples of seen classes. The definition follows few-shot learning with N-way-K-shot. We summarize a typical application scenario in SAR domain, as shown in Fig. 5 (c). Assume there are M classes with sufficient SAR images obtained from complete observation angles. In addition, another N classes only contain K samples each with sparse observation. Given these training data, the few-shot image generation task aims to enable the model to generate diverse samples in unseen classes.\n3) Prior Guided Generation: For image reconstruction task, such as inpainting, denoising, and super-resolution, priors indicate some empirical knowledge or objective laws that characterize the image properties and can be helpful to constrain the optimization space. How to extract or represent the prior from data or knowledge, and how to effectively use them to guide the generation, have become a general challenge for both computer vision and SAR applications. The image degradation model, for example, is one of the well-known priors for image restoration. Although different degradation models are employed by SAR and optical images, the solutions can be learned from each other."}, {"title": "III. GENERATIVE AI METHODS", "content": "This section aims to introduce the popular deep generative models in recent years and review the related SAR applications based on them. First", "Auto-Encoders": "Auto-encoders primarily concentrate on transforming input data into a space with less dimensions and subsequently reverting it back to the original space through decoding. The decoder component can be seen as a generator. The popular AE-based generative models contain variational auto-encoder (VAE) and adversarial auto-encoder (AAE). The brief illustration can be found in Fig. 6.\nVAE and AAE are briefly introduced in this section. The illustrations of the AE-based basic models are given in Fig. 6. More architectures employ the combination of VAE and GAN", "139": ".", "118": "The VAE is a probabilistic model that consists of an encoder and a decoder", "140": ".", "119": "AAE is another form of probabilistic autoencoder that incorporates the adversarial training framework from Generative Adversarial Networks (GANs) to conduct variational inference by aligning the aggregated posterior of the hidden code vector of the autoencoder with a specified prior distribution", "SAR": "Some preliminary studies explored the potential of VAE model for SAR image generation. Wang et al. [98", "97": "and CVAE-GAN [40", "42": "which the auto-encoder was combined with StyleGAN model. The AAE architecture is used for random sampling which is then fed into the causal decoder with disentangled factors.\n2) Generative Adversarial Networks: By implementing a competitive framework comprised of two neural networks\u2014the discriminator and the generator\u2014Generative Adversarial Networks (GANs) have significantly altered the domain of image generation. GANs have effectively been utilized in several areas", "98": ".", "141": "GANs are composed of two neural networks-a generator and a discriminator. The generator tries to create data (e.g.", "game": "the generator tries to get better at fooling the discriminator", "120": "Deep Convolutional Generative Adversarial Network (DCGAN) is a specific architecture of GANs that uses convolutional layers in both the generator and discriminator. It introduced a set of guidelines for designing GANs that are more stable and capable of learning realistic image distributions. DCGANs have been particularly successful in generating high-quality images.\nCGAN [121", "123": "Information Maximizing Generative Adversarial Nets (InfoGAN) is a specialized architecture of GANs designed for the purpose of exerting more control over the generation direction of GANs. The latent codes are introduced for further disentangling the input noise. A strong correlation between latent codes and the attributes can be established in an unsupervised manner by maximizing their mutual information during training process.\nACGAN [122", "131": "Progressive Growing of GANs (PGGAN) is an approach to train GANs that starts with generating very small images and gradually increases the resolution. At each resolution", "132": "StyleGAN is a GAN architecture that allows for more control over the generation process by separating the generation of high-level attributes (like pose and identity) from low-level details (like texture). It achieves this by introducing intermediate latent spaces that control different \"styles\" of the generated images. StyleGAN can produce images with incredible detail and realism. StyleGAN2 [133", "142": "and implementing generator regularization to promote optimal conditioning in the mapping from latent codes to images. In order to solve the problem of texture sticking further improve the quality of generated images", "134": "offering a systematic resolution to suppressing aliasing", "135": "Pix2Pix is a conditional GAN used for image-to-image translation tasks. It takes a pair of images as input (e.g.", "136": "CycleGAN is an approach for learning to translate an image from a source domain to a target domain in the absence of paired examples. In order to make the mapping from source domain to target domain becomes highly constrained", "Sensing": "GANs have found extensive application in remote sensing", "143": [144], "145": "were proposed", "146": "further explored this area by using a GAN framework that considers both imaging mechanisms and spectral mixing to translate RGB images into high-resolution hyperspectral images and subpixel ground-truth annotations. Conducted Semantic Embedding GAN (CSEBGAN) [147"}, {"SAR": "According to the basic models studies applied, a majority of work concerning SAR target generation are based on cGAN [121", "100": [107], "37": "applied the CGAN with projection discrimination [148", "60": [64], "35": ".", "38": [39], "113": [115], "122": "where the auxiliary classifier was used to estimate the class label [113", "114": "and angle [38", "39": [115], "116": "proposed a method based on InfoGAN [123", "131": "it was also applied for generating high quality SAR images in literature [20", "41": [124], "132": [134], "36": [42]}]}