{"title": "Beyond Discrete Personas: Personality Modeling Through Journal Intensive Conversations", "authors": ["Sayantan Pal", "Souvik Das", "Rohini K. Srihari"], "abstract": "Large Language Models (LLMs) have significantly improved personalized conversational capabilities. However, existing datasets like Persona Chat, Synthetic Persona Chat, and Blended Skill Talk rely on static, predefined personas. This approach often results in dialogues that fail to capture human personalities' fluid and evolving nature. To overcome these limitations, we introduce a novel dataset with around 400,000 dialogues and a framework for generating personalized conversations using long-form journal entries from Reddit. Our approach clusters journal entries for each author and filters them by selecting the most representative cluster, ensuring that the retained entries best reflect the author's personality. We further refine the data by capturing the Big Five personality traits\u2014openness, conscientiousness, extraversion, agreeableness, and neuroticism-ensuring that dialogues authentically reflect an individual's personality. Using Llama 3 70B, we generate high-quality, personality-rich dialogues grounded in these journal entries. Fine-tuning models on this dataset leads to an 11% improvement in capturing personality traits on average, outperforming existing approaches in generating more coherent and personality-driven dialogues.", "sections": [{"title": "Introduction", "content": "A conversation reflects the unique threads of a person's life experiences, thoughts, and personalities. However, many existing conversational systems struggle to capture the richness of these tales, often reducing complex individuals to static, predefined personas. Existing datasets like Persona Chat (PC), Synthetic Persona Chat (SPC), and Blended Skill Talk (BST) use hardcoded personas. While these datasets have paved the way for more personalized dialogue systems, they often fall short of capturing the dynamic and evolving nature of real human personalities, as shown in Fig. 1. Conversations generated from such static personas can feel repetitive, shallow, and sometimes even contradictory, failing to engage the user truly. Our research seeks to fill this gap and transform this approach by moving beyond the constraints of discrete personas, instead embracing a model that captures the dynamic nature of personal identity. By leveraging long-form journal entries mined from platforms like Reddit-where individuals share their authentic, unfiltered life experiences, we ensured the preservation of personality traits, achieving greater depth and realism than static personas.\nPersonas are widely used to enhance user representation and conversational flow by simulating human-like dialogue. Our analysis shows that existing datasets fail to capture the complexity of the Big Five personality traits: (O.C.E.A.N.)\u2014openness, conscientiousness, extraversion, agreeableness, and neuroticism resulting in less genuine interactions.\nCreating a dataset that captures personality traits is labor-intensive, traditionally relying on significant human input for persona design, conversation generation, and validation. To overcome these challenges, we utilized large language models (LLMs), specifically LLaMa 3 70B, for synthetic data generation with human-in-the-loop assessment. This personalizes AI systems and enhances human-AI interaction for more relatable and engaging conversational agents.\nOur work introduces a novel method for creating a journal-based conversational dataset named Journal Intensive Conversations (JIC). (1) This process begins with data acquisition from Reddit. (2) We apply multi-step filtration strategies, using clustering algorithms to identify and retain the most representative journal entries per author. Additionally, we filter out dialogues that diverge significantly from the author's average Big 5 Personality Traits, ensuring better alignment. (3) We then use instruct-LLMs to generate journal-grounded conversations, ensuring the resulting dialogues remain true to the author's personality. (4) Finally, we demonstrate that fine-tuning state-of-the-art(SOTA) LLMs on our dataset enhances their ability to capture personality traits effectively in dialogue. Our code, data, and best models are publicly available."}, {"title": "Related Work", "content": "Personality Traits in Conversational AI: In recent years, modeling personality traits in conversational systems has been an area of extensive research to make human-AI interaction more personalized and engaging. Early attempts in this field used static, predefined personas to model users and produce goal-directed faithful conversations. While these systems improved personalization, they fell short of capturing the dynamic nature of human behavior, often reducing users to rigid attributes that limit dialogue adaptability. Recent advancements have called for more sophisticated systems that reflect the evolving nature of human personality. Moreover, the emergence of large language models (LLMs) like GPT-3 and LLaMA offers new opportunities for generating more nuanced, personality-driven dialogues. Fine-tuning these models on personalized datasets enables them to exhibit a deeper understanding of individual traits, fostering more consistent and contextually appropriate interactions. Other works have explored integrating psychological models like the Big Five (O.C.E.A.N. Model) into CA. However, significant challenges remain in accurately capturing and representing dynamic personality traits.\nPersonality Datasets and Challenges: Recent advancements in conversational datasets have highlighted the potential and limitations of existing approaches to simulate human behavior. Static personas or scripted inputs, such as those found in existing datasets, limit their ability to capture human traits' evolving nature in dialogue. For instance, synthetic conversations often mimic human interaction but struggle to reflect persistent personality traits over time. More dynamic datasets, like those generated using instruct-LLMs, aim to address this by leveraging tunable instructions to capture authentic conversations. Despite these advancements, creating datasets that genuinely capture the complexity of human personality remains a crucial area for further research and refinement."}, {"title": "Data Acquisition", "content": "We mined data from two relevant subreddits: r/DiaryOfARedditor and r/Journaling. These communities provided a rich source of personal narratives, allowing us to gather approximately 19,000 submissions from 1,372 unique authors."}, {"title": "Journal Data Scraping", "content": "We used the PullPush API to collect data from Reddit by querying relevant subreddits using a pre-"}, {"title": "Synthetic Conversation Generation", "content": "We used the Groq API's LLaMa 3 70B model to generate synthetic conversations from filtered journal entries. Given rate limitations, we selected 906 out of 1,372 unique authors, pairing them in all possible combinations. For authors with multiple journal entries, dialogues were generated for every entry combination. For instance, two dialogues were generated if Author 1 had two entries and Author 2 had one. This approach produced a total of 418,476 dialogues. The final turn often included superficial exchanges like \"Bye\" or \"Have a nice day.\" To retain conversational depth, the last turn was removed, leaving 8-turn dialogues that better reflected meaningful interactions. Full details of the prompting strategy are provided in Appendix A.\nThe synthetic dialogues were evaluated using GPT4-o and human assessments; the results showed strong agreement, particularly with high Intraclass Correlation Coefficient(ICC) scores, indicating good consistency between LLM and human ratings. Detailed agreement scores in Appendix B."}, {"title": "Data Filtration Strategies", "content": ""}, {"title": "Prominent Journal Clustering and Retention", "content": "We employed a clustering strategy to retain the most representative journal entries for authors with multiple submissions. High-dimensional embeddings were generated using the microsoft/deberta-large model to capture semantic content. K-Means clustering, validated with silhouette scores , was applied to identify optimal clusters. Additionally, agglomerative clustering was used to refine grouping, selecting the most prominent cluster. This ensured the dataset reflected each author's dominant themes for generating synthetic conversations. The filtration process is detailed in Algorithm 1."}, {"title": "Personality Trait Convergence Filtering", "content": "We refined the dataset to capture journal entries and authors with the most prominent and consistent personality traits. Using the facebook/bart-large model, we trained a Big 5 Personality classifier with the PANDORA dataset to evaluate and filter journal entries based on their alignment with core personality traits. The filtering process relied on two key parameters: alpha (\u03b1), which controlled the filtration strictness at the journal level, and beta (\u03b2), which managed the convergence of personality traits across authors. The lower the parameter values, the stricter the filtration process. Algorithm 2 outlines this filtration strategy. The critical components of the process are as follows:\n\u2022 Personality Trait Generation: Each journal entry was processed using a Big 5 Personality classifier, predicting the Big 5 traits: OCEAN. This provided a detailed personality profile for each author across all their entries.\n\u2022 Journal-Level Filtration (\u03b1): We measured each journal entry's deviation from the author's average personality profile, with \u03b1 (can be any value but tested with 0,1) setting a threshold based on the standard deviation of these deviations. Entries with significant deviations were excluded to retain journals that best reflected the author's core traits.\n\u2022 Author-Level Filtration (\u03b2): We assessed personality consistency across authors by comparing their average profiles to the global dataset, with \u03b2 (can be any value but tested with 0,0.5) filtering out authors with excessive divergence to ensure alignment with the overall dataset."}, {"title": "Dataset Statistics", "content": "JIC consists of 418,476 dialogues, 20,000 reserved for the test set H and the rest for training, with 3,347,808 turns and 6,695,616 utterances, averaging 8 turns and 16 utterances per dialogue. Each utterance contains about 15.48 words, resulting in an average conversation length of 247.61 words. The dataset exhibits moderate topic consistency (0.5281) and an average semantic similarity of 0.3611 between consecutive utterances, highlighting its diversity and scale. Table 1 shows detailed comparisons with other datasets."}, {"title": "Experimentation", "content": "Training and Inference were carried out in two settings, as shown in Fig. 3. The training splits for JIC are shown in Table 2."}, {"title": "Training", "content": "To analyze the impact of data scaling, we sampled subsets of dialogues from 398,476 dialogues, consistently holding out 1,000 dialogues for validation across all experiments. Let Dtrain represent the training set and Dval the validation set, where Dtrain \u2286 D, the total dataset. We fine-tuned the LLaMa 3 8B Instruct and Mistral 7B v0.3 models using a parameter-efficient technique, Low-Rank Adaptation (LoRA). Specifically, we adjusted the query, key, value, and output projection layers (i.e., Wq, Wk, Wv, and Wo), updating only these parameters while keeping the rest of the model frozen.\nThe training objective was to minimize the negative log-likelihood (NLL) loss, defined as:\nL(\u03b8) = 1/|Dtrain| \u03a3(x,y)\u2208Dtrain log p(y|x; \u03b8),\nwhere x is the input (dialogue context) and y is the output (next dialogue turn). The model aims to maximize p(y|x), the probability of generating the correct response, conditioned on the context.\nLORA Optimization: We introduced low-rank updates to the projection layers rather than fine-tuning all weights. The update rule for Wq (query projection) can be formulated as:\nW'q = Wq + \u2206Wq, where\n\u2206Wq = AqBq,  Aq \u2208 Rd\u00d7r, Bq \u2208 Rr\u00d7d,\nwith r being the rank of the update. Similar updates apply to Wk, Wu, and W. This approach greatly reduces computational overhead.\nWe extended the training process with Retrieval Augmented Fine-tuning (RAFt.) mechanisms to enhance context relevance. Let xi represent the user's last utterance (the query) and Ci the assistant's journal entry (the context). Using Maximum Marginal Relevance (MMR), the top k most relevant segments, denoted as C(1), ..., C(k), were selected based on their similarity scores to xi, while minimizing redundancy. The enriched input becomes:\nXi = concat(xi, C(1), ...,C(k)),\nwhere i includes both the query and retrieved context. Training arguments were consistent across models and are available in Appendix C, and Training strategies for other datasets are mentioned in Appendix D."}, {"title": "Inference", "content": "Inference was carried out in two specific settings: utterance level and using Retrieval Augmented Generation(RAG). RAG employed a classifier, C, to distinguish between user statements and questions. It was activated for queries classified as questions, C(xi) = 1. The context Ci was retrieved by selecting the top k relevant chunks from the assistant's journal using MMR. The enriched query becomes i = {xi, Ci}, which was passed to the model for response generation. For non-questions, C(xi) = 0, no retrieval was performed. This selective retrieval application improved performance, particularly in handling chit-chat vs. complex queries."}, {"title": "Evaluation Strategy", "content": "We employed both automatic metric-based evaluation and evaluation by the LM Eval Harness framework by EleutherAI. Automatic evaluation used BLEU, METEOR, BERTScore, and ROUGE-1, ROUGE-2, and ROUGE-L. The average of these metrics provided an overall performance score. Whereas, the LM Eval Harness assessed the models on the Big 5 personality traits"}, {"title": "Results and Discussion", "content": "We conducted extensive testing on automated metrics and LM-eval benchmarks to assess model performance across different configurations."}, {"title": "Automatic metric-based evaluation", "content": "achieved when models were trained on the entire dataset, with LLaMA consistently outperforming Mistral. LLaMA achieved a best average score of 0.3105, representing a 35.1% improvement over its zero-shot baseline, while Mistral reached a best score of 0.2646, improving by 23.2%. Table 3 report our findings. Fig. 4 shows comparative results, detailed results in Appendix E. Due to computational constraints, we could not use RAFt on the entire dataset.\nContrary to expectations, where RAFt should theoretically enhance model performance by providing additional context, Mistral did not show any improvements when using RAFt., regardless of the dataset size. This contrasts with LLaMA, which consistently improved with RAFt. across all data splits. One possible explanation for this discrepancy is that Mistral, starting from a weaker baseline, might not have been able to effectively leverage the additional retrieved context provided by the retrieval mechanism. In contrast, LLaMA's stronger baseline performance allowed it to utilize the retrieved information more effectively, leading to consistent gains with RAFt. This suggests that while RAFt is generally beneficial, its effectiveness may depend on the model's inherent capabilities and baseline performance."}, {"title": "LM-Eval Harness Results", "content": "We analyzed the models' ability to capture Big Five personality traits (O, C, E, A, N) across different training configurations. Notably, trait scores plateaued after a certain dataset size, fine-tuned using \u03b1 and \u03b2 parameters. The highest scores were achieved with \u03b1=1 and \u03b2=0 (around one-third of the dataset). LLaMA outperformed Mistral, achieving a top score of 0.8030 with RAFt., while Mistral's strong baseline (0.7654) showed modest improvement (4.9%), whereas Mistral's lower baseline (0.6680) saw a 17% gain, suggesting that Mistral benefits more from data scaling and refinement, despite LLaMA's better overall performance.These results suggest that LLaMA's strong baseline may limit its potential for further improvement, while Mistral benefits more from data scaling and refinement strategies. Table 4 report our findings. Fig. 5 shows performance of LLaMA and Mistral models across various JIC splits. Fig. 6 compares performance of the models across different datasets.\nInterestingly, while Mistral showed no improvement with RAFt in automated metrics, both models displayed significant gains in capturing personality traits with RAFt. This suggests that RAFt enables models to learn and internalize personality traits, even if the generated text doesn't exactly match the golden annotations. The retrieval process helps models better understand and generalize trait-specific behaviors, emphasizing these traits during training, regardless of text alignment."}, {"title": "Ablation Study", "content": "The ablation study focuses solely on personality traits, as the dataset is specifically designed to capture human-like personality dynamics, making automated metric evaluations less relevant in this context. Table 5 shows the impact of various configurations on the overall performance of LLaMA and Mistral. (0): The best configuration, RAFt. (\u03b1 = 1, \u03b2 = 0), yielded the highest average scores for both LLaMA (0.8030) and Mistral (0.7816). (1): No Retriever Augmentation, the performance slightly dropped for both models (LLaMA: 0.7808, Mistral: 0.7396). (2): No filtration (\u03b1, \u03b2 set to None), we observed further performance degradation (LLaMA: 0.7408, Mistral: 0.7204). (3): Random sampling to mimic (1) improved the scores compared to (2) but did not outperform (1), which shows the requirement of filtration (LLaMA: 0.7764, Mistral: 0.7302). (4): No Fine-tuning, resulted in the lowest scores (LLaMA: 0.7654, Mistral: 0.6680)."}, {"title": "Personality Trait Scaling and Dataset Bias", "content": "We found that personality trait accuracy does not scale linearly with dataset size, likely due to dataset bias. The synthetic dialogues, often reflecting negative experiences from Reddit journal entries, led to an over-representation of neuroticism in the JIC dataset, skewing the capture of other traits like extraversion and conscientiousness. LLaMA, when fine-tuned without alpha-beta filtering, showed higher neuroticism, disrupting trait balance, with Mistral exhibiting similar but less pronounced behavior. LLaMA had the highest conscientiousness and extraversion scores in the JIC-medium split, potentially due to its stronger zero-shot baseline, while Mistral required more fine-tuning for similar performance. Furthermore, focusing on personality traits slightly reduced the model's ability in general reasoning tasks, which we believe can be improved through rehearsal learning. Detailed scores are in Appendix J. The relationship between data scaling and trait capture appears model-dependent and influenced by pre-training performance, with alpha-beta hyperparameters (\u03b1=1, \u03b2=0) offering the most balanced results across traits. Due to the computational costs, we experimented with a small range of values, finding the optimal remains an open research challenge."}, {"title": "Qualitative & Toxicity Analysis", "content": "Qualitative analysis is crucial for evaluating LLMs beyond quantitative metrics, offering insights into dialogue subtleties. Table 6 shows how well the models aligned with personality traits from a randomly selected annotated dialogue, ensuring unbiased and representative results. Examples can be found in Appendix G. Additionally, we conducted a toxicity analysis using the Detoxify library to assess harmful content in our dataset, classifying"}, {"title": "Conclusion", "content": "Our research introduces the JIC dataset, which overcomes the limitations of static personas in existing conversational datasets. By grounding dialogues in long-form journal entries and capturing dynamic personality traits through a multi-step filtering process, we enable LMs to generate more authentic, personalized conversations. This approach significantly enhances conversational AI's ability to reflect real human personalities, offering engaging and relatable interactions."}, {"title": "Limitations", "content": "\u2022 A fundamental limitation of our research lies in tuning the optimal \u03b1 and \u03b2 parameters. While the chosen values (\u03b1 = 1, \u03b2 = 0) yielded promising results, refining these parameters remains an open challenge due to the computational demands of extensive experimentation.\n\u2022 Synthetic data generation using LLaMA 70B introduces potential biases and safety concerns inherent in the pre-trained model. These biases could propagate into the dialogues, limiting diversity and authenticity, although the impact may be minimal.\n\u2022 Furthermore, the dataset's inherent bias-stemming from an over-representation of neuroticism in Reddit journals-may have skewed the models' ability to capture traits like extraversion accurately.\n\u2022 Finally, human evaluation remains a significant challenge, as assessing nuanced traits in synthetic dialogues can be tough and labor-intensive, highlighting the difficulty of balancing human insight and scalable evaluation methods."}, {"title": "Ethical Considerations", "content": "In creating the JIC dataset, we ensured that all journal entries were publicly available and anonymized to protect user privacy. We employed the Detoxify library to tag potentially toxic dialogues to mitigate the risk of harmful content. A strict threshold was set, flagging any dialogue where more than 25% of the utterances were classified as toxic. These flagged dialogues were kept separately to prevent their use in downstream tasks. This approach helps ensure the dataset remains safe and responsible for use in developing conversational AI systems."}]}