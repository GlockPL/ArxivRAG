{"title": "Automatic Pruning via Structured Lasso with Class-wise Information", "authors": ["Xiang Liu", "Mingchen Li", "Xia Li", "Leigang Qu", "Zifan Peng", "Yijun Song", "Zemin Liu", "Linshan Jiang", "Jialin Li"], "abstract": "Most pruning methods concentrate on unimportant filters of neural networks. However, they face the loss of statistical information due to a lack of consideration for class-wise data. In this paper, from the perspective of leveraging precise class-wise information for model pruning, we utilize structured lasso with guidance from Information Bottleneck theory. Our approach ensures that statistical information is retained during the pruning process. With these techniques, we introduce two innovative adaptive network pruning schemes: sparse graph-structured lasso pruning with Information Bottleneck (sGLP-IB) and sparse tree-guided lasso pruning with Information Bottleneck (sTLP-IB). The key aspect is pruning model filters using sGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to multiple state-of-the-art methods, our approaches demonstrate superior performance across three datasets and six model architectures in extensive experiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain an accuracy of 94.10% (0.14% higher than the original model); we reduce the parameters by 55% with the accuracy at 76.12% using the ResNet architecture on ImageNet (only drops 0.03%). In summary, we successfully reduce model size and computational resource usage while maintaining accuracy.", "sections": [{"title": "1 Introduction", "content": "Deep Learning [Krizhevsky et al., 2012], especially CNN-based methods [Simonyan and Zisserman, 2014; Szegedy et al., 2015; He et al., 2016], has proven its effectiveness in tasks like object detection and classification. Recently, researchers have been attempting to deploy neural network models on resource-constrained edge devices for real-time analysis [Liu et al., 2024c; Yu et al., 2024]. However, these network parameters are often too large, making deployment difficult. For instance, the VGGNet-16 model has parameters exceeding 500MB [Simonyan and Zisserman, 2014], which introduces a heavy memory burden and computation overhead.\nFortunately, scientists have discovered that the parameters are usually redundant and the key weights of neural networks exhibit sparsity characteristics [He et al., 2017]. As a result, a significant amount of work has focused on pruning unimportant weights to reduce the size of the model, which reduces the model's FLOPs and lowers computational overhead as well. The common methods can be primarily divided into two categories: unstructured pruning and structured pruning [Shao and Shin, 2022]. Unstructured pruning mainly focuses on weight pruning [Ding et al., 2019] and N:M sparsity [Zhang et al., 2022]. However, both methods require hardware support, making them less scalable. Therefore, this paper primarily focuses on structured pruning, which benefits from removing unimportant filters within the CNN.\nTo avoid the limitations of hand-crafted approaches, most structured pruning methods utilize statistical information [Zhang et al., 2021; Barisin and Horenko, 2024] to analyze filters, with the aim of minimizing the loss before and after pruning and achieving better accuracy results. However, these methods all overlook the class-wise information [Hu et al., 2016], resulting in a less precise pruning scheme. Furthermore, an ideal pruning method should also include an adaptive approach [Guo et al., 2023; Xie et al., 2024] to control the level of sparsity based on specific requirements.\nBased on these, we employ the precise Information Bottleneck (IB) [Tishby et al., 2000] theory perspective to model statistical information of feature maps of CNN for pruning, aiming to minimize the loss from pruning. We also utilize structured lasso techniques that account for sparsity, such as graph-structured lasso [Liu et al., 2024b] and tree-guided lasso [Kim and Xing, 2009; Liu et al., 2024a], which can consider the relatedness in feature maps and form corresponding"}, {"title": "2 Related Works", "content": "Structured Pruning\nMPF [He et al., 2019] considers the geometric distance between filters and neighboring filters to guide the pruning process. CPMC [Yan et al., 2021] leverages relationships between filters in the current layer and subsequent layers for pruning decisions. HRank [Lin et al., 2020] compresses models by constructing low-rank matrices using information from the same batch. Scop [Tang et al., 2020] reduces filters based on their deviation from the expected network behavior. GDP [Guo et al., 2021] employs gates with differentiable polarization, learning whether gates are zero during training for pruning purposes. EEMC [Zhang et al., 2021] uses a multivariate Bernoulli distribution along with stochastic gradient Hamiltonian Monte Carlo for pruning. SRR-GR [Wang et al., 2021] identifies redundant structures within CNNs. FTWT [Elkerdawy et al., 2022] predicts pruning strategies using a self-supervised mask. DECORE [Alwani et al., 2022] assigns an agent to each filter and uses lightweight learning to decide whether to keep or discard each filter. EPruner [Lin et al., 2021] employs Affinity Propagation for efficient pruning. DPFPS [Ruan et al., 2021] directly learns the network with structural sparsity for pruning. CC [Li et al., 2021] combines tensor decomposition for filter pruning. NPPM [Gao et al., 2021] trains the network to predict the performance of the pruned model, guiding the pruning process. LRF-60 [Joo et al., 2021] uses Linearly Replaceable Filters to aid pruning decisions. AutoBot [Castells and Yeom, 2021] tests each filter one-by-one to ensure pruning precision. PGMPF [Cai"}, {"title": "3 Our Method", "content": "3.1 Design Overview\nThe overview of our methods is shown in Figure 1. To avoid the imprecision that end-to-end methods might introduce, we perform layer-wise pruning independently [Sakamoto and Sato, 2024]. We reshape each layer's input and output feature map tensors into two-dimensional matrices, enabling us to apply the information bottleneck theory to determine which filters can be pruned. To further reduce computational complexity, capture internal geometric structures, and utilize more flexible kernel functions, we employ Gram matrix [Lanckriet et al., 2002]. Since lasso aligns well with the information bottleneck theory [Tishby and Zaslavsky, 2015; Dai et al., 2018], we use structured lasso for solving this problem. This approach also takes into account class-wise information in the feature maps (as well as filters), resulting in a sparse linkage map between layers' feature maps for pruning.\nMath Notations: In this paper, given a CNN model M with N layers, we let Ci denote the i-th layer of M. The input feature map of Ci is Xi with the shape (bs, hi, wi, ini); the output feature map of Ci is Xi+1 with the shape (bs, hi+1, wi+1, outi), where i \u2208 {1, 2, ..., N}, X\u2081 is the input sample batch from the datasets, XN+1 is the final output results, Xi+1 is the input of Ci+1; for layer i, bs is the batch size, hi and wi are the height and width of each feature map, ini is the input channel, outi is the output channel (filter number of Ci). We convert X\u00bf from the original shape to X\u2081, (bs \u00d7 hi \u00d7 wi, ini), i.e., (di, ini). Thus, the shape of Xi+1 is (bs \u00d7 hi+1 \u00d7 wi+1, outi), i.e., (di+1, outi). Therefore, we use \u03b2 to denote the linkage matrix of Ci among Xi and Xi+1, Bj denotes the j-th column B."}, {"title": "3.2 Structured Lasso", "content": "Information Bottleneck Function and Gram matrix\nFor each layer Ci, we optimize on the Information Bottleneck (IB) formular [Tishby et al., 2000] as:\n$\\min_{X_{i+1}} -I(X_{i+1}; X_i) + \\gamma I(X_{i+1}; X_{i+1})$\nwhere $X_{i+1}$ denotes the pruned ouput features and \u03b3 is a positive Largrange multiplier. Output feature maps are derived from input feature maps through filters and corresponding convolution operations. As such, important filters result in output feature maps that contain more information from the corresponding input feature maps. To better utilize the IB theory, we first reshaped X\u1d62 and X\u1d62\u208a\u2081 into two-dimensional matrices, denoted as $X_i$ and $X_{i+1}$. Therefore, the problem of pruning filters is transformed into the task of exploring to preserve output feature channels that contain the most statistical information with respect to the input feature maps using the Information Bottleneck approach.\n$\\min_{X_{i+1}} -I(\\tilde{X}_{i+1}; \\tilde{X}_i) + \\gamma I(\\tilde{X}_{i+1}; \\tilde{X}_{i+1})$\nGram matrix allows for the use of more flexible kernel functions, enabling linear operations in high-dimensional spaces, thereby reducing computational complexity and better considering the geometric structure between feature maps. To account for class-wise information expansion between feature maps, we employ Gram matrix. We transform X\u1d62 and X\u1d62\u208a\u2081 into centered Gram matrices $\\tilde{X}_i$ and $\\tilde{X}_{i+1}$. Therefore, we have the optimization formula as:\n$\\min_{X_{i+1}} -I(\\tilde{X}_{i+1}; \\tilde{X}) + \\gamma I(\\tilde{X}_{i+1}; \\tilde{X}_{i+1})$\nwhere $\\tilde{X} = \\Psi X \\Psi$, $\\Psi = I_{bs} - \\frac{1}{lbs} 1_{bs}1_{bs}^T$, $I_{bs}$ is a bs-dimensional identity matrix, $1_{bs}$ is a bs-dimensional vector consisting entirely of ones. The element in row a and column b of $\\tilde{X}$ is equal to the inner product of a-th feature map and b-th feature map of X\u1d62. Therefore, we have $\\tilde{X} \\in R^{bs \\times bs,ini}$, $\\tilde{X}_{i+1} \\in R^{bs \\times bs,outi}$, $B \\in R^{ini,outi}$.\nThe first step of reshaping is intended to capture the relationship between input features and output features from the channel perspective. The second step is to utilize Gram matrix. In practice, we can directly transform X\u1d62 to $\\tilde{X}$."}, {"title": "Graph-structured Lasso and sGLP-IB", "content": "The computation of the IB method is often considered computationally infeasible due to the significant overhead introduced by density estimation in high-dimensional spaces [Ma et al., 2020]. We are able to use the Lasso-based methods to get the nonlinear dependencies between the input and output of each layer [Li et al., 2016; Guo et al., 2023].\n$B = \\min_\\beta \\frac{1}{2} ||\\tilde{X}_{i+1} - \\tilde{X}\\beta||_F^2 + \\Phi(\\beta)$\nwhere $|| ||_F$ denotes the Frobenius norm, and \u03a6 is determined by structured lasso. Due to the fact that filters generally contain class-wise information [Hu et al., 2016]. Filters are tailored to different classes, meaning they also possess distinct statistical properties. As a result, it is possible to form corresponding subgroups based on class information. Thus, the output feature maps also conceal class-wise information due to the filters. This results in a complex relationship between output feature maps, especially in terms of statistical subgroups. Therefore, we introduce graph-structured lasso from structured lasso to better model this relationship.\nGiven \u03b2 for Ci, we have the node sets V, {$B_1, B_2, ... B_{outi}$}\nthat forms the graph, where $B_i$ denotes element in j-th column and r-th row of B. We have weight edge set E of the graph by computing pairwise Pearson correlations and linking pair of nodes when their correlation exceeds a threshold th, where the weight of each edge indicates correlation degree. Let $f_{lm}$ represent the weight of edge e = (l,m) \u2208 E, which quantifies the correlation between element l and m. With these, we define graph-structured lasso as:\n$\\Phi(\\beta) = \\lambda ||\\beta||_1 + \\mu \\sum_{e=(l,m)\\in E} f_{lm} \\sum_{j=1}^{ini} |B_{lj} - sign(f_{lm}) B_{mj}|$\nwhere X is the hyperparameter for the sparsity and \u03bc regulates inner class-wise dependency. Substituting (5) into (4) yields our sparse graph-structured lasso pruning with Information Bottleneck (sGLP-IB).\nAs shown in the \u03b2 matrix in Figure 1, each column of B represents the relatedness between all feature maps of this batch for a specific output channel and the current layer's input feature maps. Our method can consider output feature maps and B, forming subgroups according to the classes. Within each subgroup, we identify and select the filters that contribute the most. The larger the value of elements in \u03b2, the stronger the correlation. Therefore, we can determine whether to retain a filter based on the number of non-zero values in its corresponding column."}, {"title": "Tree-Guided Lasso and sTLP-IB", "content": "We also apply the state-of-the-art tree-guided lasso penalty to model the more closely related class-wise information and then form the tree structures. The tree numbers and the overlap among groups are determined by the hierarchical clustering tree due to class information. We have:\n$\\Phi(\\beta) = \\sum_j \\Omega(B_j) = \\lambda\\sum_j \\sum_{v \\in V} w_v(U_{root}) = \\lambda \\sum_j \\sum_{v \\in V} \\sum_{G \\in G_v} w_{ve}||B_G||_2$\nWhere each group of j-th subtree regression factors $B_{G}$ is weighted by $w_{ve}$, as defined in (7), where A is the ancestor set. $B_G$ is a vector and represents regression factors {$B_k^j$| k \u2208 Gv}. Each node v \u2208 V of one tree is associated with a group Gv, whose members are the columns of \u03b2 within the nodes of the same subtree.\n$w_{ve} = {\\begin{cases} (1-h_v) \\prod_{q\\in A(v)} h_q & v \\in internal node\\\\  \\prod_{q\\in A(v)} h_q & v \\in leaf node \\end{cases}}$\nGenerally, $h_v$ represents the weight for selecting covariates relevant specifically to the column of \u03b2 linked to each child of node v, while 1 - $h_v$ is the weight for jointly selecting covariates relevant to all children of node v, $h_v$ \u2208 (0, 1). Assuming |V| represents the number of nodes in a tree, since a tree associated with $out_i$ columns of \u03b2 for Ci has 2$out_i$ \u2212 1 nodes, |V| in the tree-lasso penalty is upper-bounded by 2$out_i$.\nSubstituting (6) into (4) yields our sparse tree-guided lasso pruning with Information Bottleneck (sTLP-IB). Similarly, within each subtree formed by classes, we select the filters that could convey more information."}, {"title": "3.3 Optimization with Proximal Gradient Descent", "content": "With smoothing proximal gradient descent method [Chen et al., 2012; Yuan, 2024] we implemented, we could easily apply into (4) and yield the \u03b2 results for each layer, which also guarantee the convergence rate of O(1/\u221ae) with polynomial time complexity, where e is the differences between the yielded \u03b2 and the optimal \u03b2. We then prune the model M based on the \u03b2 obtained from sGLP-IB and sTLP-IB."}, {"title": "3.4 Adaptive Algorithm", "content": "We illustrate how our approach utilizes sGLP-IB and sTLP-IB to prune networks. We observe that they include a hyperparameter \u03bb, which greatly influences the sparsity of \u03b2. If \u03bb is too small, each element of \u03b2 is a non-zero value, making the pruning ineffective; if \u03bb is too large, \u03b2 may contain too few non-zero values, making the pruning impossible as they will prune all the filters in one layer. Given that models have many layers, setting \u03bb for each layer can be very time-consuming. Therefore, we propose an adaptive method to selectively prune based on memory size and FLOPs requirements as shown in Algorithm 1.\nIn the adaptive method, we use a binary search approach. We set an upper and lower limit for the value of \u03bb and use exponential mapping to quickly find an appropriate \u03bb that fits the sparsity level of the structured Lasso. Given a range of the proportion of parameters to retain or a range of FLOPs to retain, our algorithm can adaptively find suitable \u03bb values for each layer to ensure that only a specific number of columns has non-zero values in \u03b2 and meet the requirements. This method ensures a relatively balanced number of parameters across layers, preventing any layer from having too many or too few parameters. It also allows for the setting of different parameter numbers and FLOP retention ratios for each layer."}, {"title": "4 Experiments", "content": "To demonstrate the effectiveness of our sGLP-IB and sTLP-IB, we conduct extensive experiments based on many representative CNNs, including various variants of VGGNet, ResNet, and GoogleNet, using three widely used datasets: CIFAR-10, CIFAR-100, and ImageNet. We also perform numerous ablation studies to analyze the robustness and characteristics of our methods.\nImplementation Details: All codes are implemented using Pytorch. We use the SGD optimizer with a decaying learning rate initialized to 1e-4, and we set the batch size to 128. After pruning, we finetune VGGNet, ResNet and GoogleNet for 400 to 500 epochs. On ImageNet datasets, we train VGGNet and ResNet for 150 to 200 epochs. We choose the Laplacian kernel as our base kernel function for Gram matrix. We prune without the first four layers.\nEvaluation Metric: We evaluate the pruned model based on its Top-1 test accuracy, the pruned ratio of parameters, the reduced FLOPs ratio with the original pretrained model.\nBaselines: Our extensive baselines are primarily based on the aforementioned structured pruning methods in Section 2. Besides, we include NORTON [Pham et al., 2024a; Pham et al., 2024b], which represents state-of-the-art unstructured pruning methods through the use of tensor decomposition."}, {"title": "4.1 Experiments on CIFAR-10", "content": "VGG-16. Table 1 presents our experiments on CIFAR-10 using VGGNet-16. Our methods, sGLP-IB and sTLP-IB, demonstrate strong performance, achieving Top-1 accuracy rates of 94.05% and 94.10%, and pruning ratios of 61% for FLOPs and 85% for parameters. sTLP-IB achieves the highest Top-1 accuracy among all previous baselines but with lower computational overhead and memory requirements, which is 0.14% higher than the pretrained model and demonstrates the effectiveness of utilizing class-wise information. Although sGLP-IB's Top-1 accuracy is slightly lower than AutoBot by 0.05%, it achieves greater reductions in both FLOPs and parameters. We also present additional results based on varying pruning ratios, showing that our methods remain superior to most baselines even at higher pruning levels. This proves the effectiveness of using graph-structured lasso and tree-guided lasso, which utilize class-wise information, in pruning. Notably, sTLP-IB performs better than sGLP-IB, as the tree-structured approach, which constructs subtrees, captures class information more precisely than graphs.\nResNet-56. Table 1 also shows that our methods, sGLP-IB and sTLP-IB, outperform most baselines using ResNet-56 with Top-1 accuracy rates of 93.93% and 93.96%, respectively, while maintaining the same pruning ratios of 53% for FLOPs and 52% for parameters. sTLP-IB outperforms sGLP-IB, consistent with the results observed with VGGNet-16. Although NORTON slightly surpasses our methods by 0.04%,"}, {"title": "4.2 Experiments on ImageNet", "content": "Compared to the CIFAR dataset, as shown in Table 2, we conduct experiments on a larger dataset, ImageNet, with numerous baselines using ResNet-50. The results indicate that our STLP-IB achieves state-of-the-art performance, outperforming all baselines with a Top-1 accuracy of 76.12%, a 57% reduction in FLOPs, and a 55% reduction in parameters, with a mere 0.03% drop compared to the pretrained model. Our SGLP-IB also outperforms the majority of baselines and was comparable to CSD. Although sGLP-IB achieves a higher pruning ratio than CSD, its accuracy is only 0.01% lower, which may be attributed to CSD benefiting from its Discrete Wavelet Transform on large datasets, which shows the effectiveness of sGLP-IB. Combined with the results on CIFAR-10 from Section 4.1, our methods are applicable not only to simple datasets but also to complex, real-world datasets, showcasing their significant potential. We can leverage class-wise information and structured lasso for pruning across datasets."}, {"title": "4.3 Ablation Studies", "content": "Influence of batch sizes. As we keep the same pruning ratio as Table 1, the accuracy of sGLP-IB and sTLP-IB increases with the batch size but essentially converges once the batch size reaches 128 in Table 3. This observation forms the basis for setting our experimental batch size to 128. With a batch size of 64, the randomness due to smaller data volume results in sGLP-IB slightly outperforming sTLP-IB by 0.01% in Top-1 accuracy. As mentioned in work [Lin et al., 2020], the average rank of feature maps generated by a single filter remains consistently the same and the pruned model's accuracy is not significantly affected by the batch size during the pruning process. Nonetheless, sTLP-IB consistently outperforms SGLP-IB across varying batch sizes, aligning with our previous findings that tree-guided lasso is more effective than graph-structured lasso when the batch size is larger. The experimental results demonstrate that our methods are robust to batch size variations, provided that the random selection of samples within batches is maintained.\nInfluence of Gram matrix. Following previous settings, we conduct experiments on the influence of Gram matrix. With the same pruning ratio for VGGNet-16 on CIFAR-10 as Table 1, sGLP-IB and STLP-IB achieve Top-1 accuracies of 94.06% and 94.10%, respectively. This demonstrates that our methods can be effectively combined with Gram matrix. The integration of Gram matrix with structured lasso accelerates computation without sacrificing class-wise statistical information and accuracy, as evidenced by the comparable results of 94.05% and 94.10% obtained with Gram matrix. Notably, the time required for pruning the pretrained model without Gram matrix is approximately 25.7 times greater than when it is used, indicating that the introduction of Gram matrix significantly enhances pruning efficiency.\nInfluence of kernel selections. With the same pruning ratio, we conduct experiments by selecting different kernel functions, as shown in Table 4. Our methods yield similar results across Linear, Gaussian, and Laplacian kernels, maintaining high Top-1 accuracy shown in Table 5. By utilizing Gram matrix, our methods achieve faster computation and obtain"}, {"title": "5 Conclusion", "content": "We propose two novel automatic pruning methods that both achieve state-of-the-art, sGLP-IB and sTLP-IB, which leverage information bottleneck theory and structured lasso to perform pruning using penalties that construct subgroup and subtree structures based on class-wise statistical information. Additionally, we present an adaptive approach that utilizes Gram matrix to accelerate the process and employs proximal gradient descent to further ensure convergence and polynomial time complexity. Through extensive experiments and ablation studies, we demonstrate the potential of our methods, both theoretically and experimentally, to effectively utilize class-wise information to aid pruning tasks, compared with numerous baselines. Exploring the better method to capture the class-wise information is our future direction."}, {"title": "A Experiments", "content": "A.1 Dataset Information\nWe used three widely adopted datasets to evaluate our proposed methods against baselines: CIFAR10 [Krizhevsky, 2009], CIFAR100 [Krizhevsky, 2009] and Tiny ImageNet [Deng et al., 2009].\n\u2022 CIFAR-10 consists of 60,000 color images with 32 \u00d7 32 pixels, evenly distributed across 10 classes.\n\u2022 CIFAR-100 dataset consists of 60,000 32 \u00d7 32 color images, evenly distributed across 100 classes.\n\u2022 ImageNet dataset contains 1.2 million 224 x 224 color images across 1,000 classes, with each class having around 1,000 images.\nA.2 Time Complexity Discussion\nFor sGLP-IB: the time complexity of graph-structured lasso could be found in the paper [Chen et al., 2010].\nFor STLP-IB: since a tree associated with outi responses can have at most 2outi - 1 nodes, which constrains the Gv. It is computationally efficient and spatially economical to run sTLP-IB. We employ a smoothing proximal gradient method that is originally developed for structured-sparsity-inducing penalties. By using the efficient method, the convergence rate of the algorithm is O(1/\u221ae), given the desired accuracy e and the time complexity per iteration of the smoothing proximal gradient is O(inzouti + ini \u2211 v \u2208 V|Gv|). Thus the overall complexity for our method is O(1/\u221ae\u00d7 (inzouti + ini \u03a3\u03c5\u2208 V|Gv|)).\nA.3 Details of Pruning on ResNet and GoogleNet\nFor ResNet: due to ResNet's architecture featuring shortcuts, when pruning ResNet variants, we initially ignore the shortcuts. We perform pruning layer by layer, and then prune this shortcut layer corresponding to the pruned channels of its related layers inside the residual block when we approach the end of this residual block. This pruning approach ensures the integrity and functionality of the shortcut connections while effectively reducing the network's complexity.\nFor GoogleNet: we perform pruning on each branch of Inception individually using sGLP-IB and sTLP-IB."}, {"title": "A.4 Experiments with CIFAR-100 Dataset", "content": "We compare sGLP-IB and sTLP-IB with CPMC [Yan et al., 2021], PGMPF [Cai et al., 2022], CPGMI [Lee et al., 2020], PGMPF-SFP [Cai et al., 2022], and APIB [Guo et al., 2023] on the CIFAR-100 dataset using VGGNet-16. Our methods, SGLP-IB and sTLP-IB, achieve the highest accuracy with the largest pruning ratio (55% for FLOPs and 71% for parameters), reaching 73.96% and 73.99%, respectively, which are 0.16% and 0.19% higher than the base model. This comparison with these state-of-the-art methods highlights the effectiveness of our approach, considering Top-1 accuracy, FLOPs pruned ratio, and parameter reduction ratio metrics. Combined with results from experiments on CIFAR-10 and ImageNet, it demonstrates that our methods can be extended to various real-world datasets."}, {"title": "A.5 Experiments with Pruning Ratios for FLOPs", "content": "Before the pruning ratio exceeds 50%, we observe that the parameter pruned ratio is also below 60%, and the performance of sGLP-IB and sTLP-IB is nearly identical. It is noteworthy, however, that sTLP-IB prunes more parameters, indicating that it retains fewer parameters while maintaining the same FLOPs. This suggests that sTLP-IB is more effective at focusing on important filters due to its superior ability to model class-wise information. When the pruning ratio exceeds 70%, the accuracy of both methods begins to drop. At a ratio of 80%, the accuracy of our methods drops by 2-3% due to excessive parameter reduction (94%), though sTLP-IB still remains more robust than sGLP-IB."}]}