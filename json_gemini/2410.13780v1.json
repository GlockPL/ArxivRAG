{"title": "Optimal Quantization for Matrix Multiplication", "authors": ["Or Ordentlich", "Yury Polyanskiy"], "abstract": "Recent work in machine learning community proposed multiple methods for performing lossy compression\n(quantization) of large matrices. This quantization is important for accelerating matrix multiplication (main component\nof large language models), which is often bottlenecked by the speed of loading these matrices from memory. Unlike\nclassical vector quantization and rate-distortion theory, the goal of these new compression algorithms is to be able to\napproximate not the matrices themselves, but their matrix product. Specifically, given a pair of real matrices A, B an\nencoder (compressor) is applied to each of them independently producing descriptions with R bits per entry. These\nrepresentations subsequently are used by the decoder to estimate matrix product $A^T B$. In this work, we provide\na non-asymptotic lower bound on the mean squared error of this approximation (as a function of rate R) for the\ncase of matrices A, B with iid Gaussian entries. Algorithmically, we construct a universal quantizer based on nested\nlattices with an explicit guarantee of approximation error for any (non-random) pair of matrices A, B in terms of\nonly Frobenius norms $||A||_F$, $||B||_F$ and $||A^T B||_F$. For iid Gaussian matrices our quantizer achieves the lower bound\nand is, thus, asymptotically optimal. A practical low-complexity version of our quantizer achieves performance quite\nclose to optimal. In information-theoretic terms we derive rate-distortion function for matrix multiplication of iid\nGaussian matrices.", "sections": [{"title": "Introduction and main results", "content": "Matrix multiplication is a key component of many numerical algorithms, and is often the dominant factor in the\nruntime of a program. With the surge of deep neural nets (DNNs) and large language models (LLMs), finding more\nefficient ways to perform matrix multiplication have become one of the most pressing challenges. Classical work\nin this field focused on minimizing the number of required operations [1], [2], [3], [4]. Specifics of contemporary\nproblems, however, require rethinking this classical approach to matrix multiplication. First, in machine learning\napplications requirements for precision of computing matrix products are quite lax. Second, modern computational\nhardware is often bottlenecked by the memory bandwidth. A natural solution explored by many researchers is to\napply lossy compression to matrices leading to deterioration in precision but improvement in the amount of data\ntransferred between memory and computation cores.\nWe formalize this problem as follows. Consider a pair of matrices $A \\in \\mathbb{R}^{n \\times a}$ and $B \\in \\mathbb{R}^{n \\times b}$ which need to be\ndescribed using R bits per entry (using separate compressors), such that a decoder that obtains bit descriptions of both\nmatrices can estimate $A^TB$. The metric for gauging quality of approximation that we will use is the squared error\nbetween ab entries of $A^TB$ and $A^T B$. Note that unlike classical vector quantization, we are requiring compression\nalgorithms to be tailored to the special task of matrix multiplication. As a practical motivation, in Section I-A\nbelow we argue that reducing R down to a few bits/entry is necessary for LLMs to fully leverage modern matrix\nmultiplication hardware.\nOur main result shows existence of universal quantizers (based on lattices) which compress A and B to R\nbits/entry and come with explicit precision guarantees. Furthermore, we also show that these guarantees cannot be\ngenerally improved by proving a matching lower bound for the case of matrices A and B with iid Gaussian entries.\nWe emphasize, though, that quantizers are universal and do not require Gaussian matrices.\nTo introduce our main results, let us define the function\n$\\Gamma(R) =\\begin{cases}1-\\sqrt{1-(2^{-2R}-2^{-4R^*})}& R R^*\\\\2\\cdot 2^{-2R} - 2^{-4R}& R > R^*\\end{cases}$"}, {"content": "where $R^* \\approx 0.906$ is the solution to the fixed-point equation\n$R = \\frac{1}{2} \\log_2(1 + 4R\\ln 2)$"}, {"content": "It will turn out that \u0393(R) is distortion-rate function for the matrix multiplication of iid Gaussian matrices.\nWe say that a matrix $A \\in \\mathbb{R}^{n\\times m}$ has \u201cbounded entries\u201d if $|a_{i,j}| \\in \\{0\\} \\cup [2^{-2000}, 2^{2000}]$ for all $i \\in [n], j \\in [m]$.\nThis extremely mild condition guarantees that we can describe the $l_2$ norm of each column of A with small\nmultiplicative error using o(n) bits (see Section V). Our first result is the following.\nTheorem 1: For any $\u03b5 > 0$ and sufficiently large n\n1) There exist randomized encoders $f_1 : \\mathbb{R}^{n \\times a} \\to [2^{naR}], f_2 : \\mathbb{R}^{n \\times b} \\to [2^{nbR}]$, and decoder $g : [2^{naR}] \\times [2^{nbR}] \\to\n\\mathbb{R}^{a \\times b}$, such that for any $A \\in \\mathbb{R}^{n \\times a}$ and $B\\in \\mathbb{R}^{n \\times b}$ with bounded entries we have\n$\\mathbb{E}||A^T B - g(f_1(A), f_2(B))||_F^2 < \\frac{||A||_F^2||B||_F^2}{n} \\cdot (\\Gamma^2(R) + \u03b5) + \\frac{||A^T B||_F^2}{n} \\cdot (\\Gamma(R) - \\Gamma^2(R) + \u03b5)).$"}, {"content": "Furthermore, denoting $A = [a_1|\\cdots|a_a]$ and $B = [b_1|\\cdots|b_b]$, where $a_i, b_j \\in \\mathbb{R}^n$ are the columns of A and B,\n$C = A^T B$ and $\\hat C = g(f_1(A), f_2(B))$, the have that\n$\\mathbb{E}(C_{i,j} - \\hat C_{i,j})^2 < C_{i,j}^2 \\cdot (\\Gamma^2(R) + \u03b5) + \\frac{||a_i||_2^2||b_j||_2^2}{n} \\cdot (\\Gamma(R) - \\Gamma^2(R) + \u03b5)), \\forall i \\in [a], j\\in [b].$"}, {"title": "Compression for Inner-Product Computation: General Problem Setup and Simple Bounds", "content": "Let P and Q be distributions on $\\mathbb{R}$ with unit variance, and let $U \\sim P^{\\otimes n}$ and $V \\sim Q^{\\otimes n}$ be statistically independent.\nWe consider the problem of quantizing U and V in order to compute their inner product $U^TV$. In particular, an\n(n, R1, R2, D) code consists of mappings\n$f_1: \\mathbb{R}^n \\to [2^{nR_1}]$\n$f_2: \\mathbb{R}^n \\to [2^{nR_2}]$\n$g: [2^{nR_1}] \\times [2^{nR_2}] \\to \\mathbb{R}$,"}, {"content": "with\n$D = \\frac{1}{n}\\mathbb{E} (U^TV \u2013 g(f_1(U), f_2(V)))^2.$"}, {"content": "We define\n$D^{DIP,*}(R_1, R_2) = D^{DIP,*}(R_1, R_2, P, Q) = \\inf \\{D : \\exists (n, R_1, R_2, D) \u2013 code\\}.$"}, {"content": "We further define the asymptotic function\n$D^{DIP} (R_1, R_2) = D^{DIP} (R_1, R_2, P, Q) = \\limsup_{n\\to\\infty} D^{DIP,*} (R_1, R_2).$"}, {"title": "Compression for Inner-Product Computation: The Symmetric Case", "content": "In this section we assume P = Q, R\u2081 = R2 = R, and define $D^{DIP,*}(R, P) = D^{DIP,*}(R, R, P, P)$, and $D^{DIP}(R, P) =$\n$D^{DIP}(R, R, P, P)$. We first develop a simple upper bound based on using the same encoder for both vectors (that\nis f = f1 = f2), that time-shares between a \u201cgood\u201d encoder for P under quadratic distortion, and a zero-rate\nencoder. We then develop a lower bound on the distortion of inner-product compression, which shows that for the\nsymmetric case, using the same encoder f = f1 = f2 for both U and V is optimal, and depends on the spectrum\nof the covariance matrix of $e_U = U \u2013 \\mathbb{E}[U|f(U)]$. We then give some constraints on the error spectrum that can be\nattained by a rate R encoder. Using this characterization we obtain a general lower on $D^{DIP,*}(R, P)$ which meets\nthe upper bound when P is the Gaussian distribution."}, {"title": "Upper Bound", "content": "Define the function\n$\\phi(x) = 2x \u2013 x^2.$"}, {"content": "and note that $x \\to \\phi(x)$ is increasing and concave on [0,1]. We give a time-sharing upper bound on $D^{DIP,*}(R, P)$\nin terms of $\\phi(D_P(R))$.\nTheorem 5:\n$D^{DIP,*}(R, P) \\leq \\min_{0 \\leq \\alpha \\leq 1} (1 \u2013 \\alpha) + \\alpha\\cdot \\phi \\Big(D_P \\big(\\frac{R}{\\alpha}\\big)\\Big)$"}, {"content": "Proof. We will prove that\n$D^{DIP,*}(R, P) \\leq \\min_{\\alpha \\in \\{0,1,...,n\\}} (1 \u2013 \\alpha) + \\alpha\\cdot \\phi \\Big(D_P \\big(\\frac{R}{\\alpha}\\big)\\Big)$"}, {"content": "from which the statement immediately follows. Let \u03b1 \u2208 {0, 1, . . ., n}, and consider a compressor for $P^{\\otimes \\alpha n}$ under\nquadratic distortion: $f : \\mathbb{R}^{\\alpha n} \\to [2^{nR} = 2^{n \\alpha}]$ and $g : [2^{nR} = 2^{n \\alpha}] \\to \\mathbb{R}^{\\alpha n}$, that attains\n$\\frac{1}{\\alpha n}D = \\frac{1}{\\alpha n} \\mathbb{E}||U^{\\alpha n} \u2013 \\hat U^{\\alpha n}||^2 = \\frac{1}{\\alpha n}trace(\\Sigma_{e_{U^{\\alpha n}}}).$"}, {"content": "We encode U by applying $f$ on $U^{\\alpha n}$ and do not describe the other coordinates. The resulting covariance error\nmatrix is therefore block diagonal of the form\n$\\Sigma_{e_U} = \\begin{bmatrix} \\Sigma_{e_{U^{\\alpha n}}} & 0 \\\\0 & I^{(1-\\alpha)n}\\end{bmatrix}.$"}, {"content": "Consequently,\ntrace($\\Sigma_{e_U}$) = trace($\\Sigma_{e_{U^{\\alpha n}}}$) + trace(I(1-\u03b1)n) = n\u03b1D + n(1 \u2212 \u03b1)\ntrace($\\Sigma_{e_U} \\Sigma_{e_U}$) = trace($\\Sigma_{e_{U^{\\alpha n}}} \\Sigma_{e_{U^{\\alpha n}}}$) + trace(I(1-\u03b1)n) = ||$\\Sigma_{e_{U^{\\alpha n}}}$||+ n(1 \u2212 \u03b1)."}, {"title": "Lower Bound", "content": "Lemma 1: For the symmetric case there is no loss of optimality in taking $f_\u2081 = f_2 = f$, and\n$D^{DIP,*}(R, P) = \\frac{1}{n} \\inf_f [2||\\Lambda(f) || - ||\\Lambda(f) ||^2] = \\frac{1}{n} \\inf_f \\sum_i \\phi(\\lambda_i(f)),$"}, {"content": "where the infimum runs over all encoders $f : \\mathbb{R}^n \\to [2^{nR}]$, and\n$\\Lambda(f) = eig(\\Sigma_{e_U}),$"}, {"content": "where $e_f = U \u2013 \\mathbb{E}[U|f(U)], \\Sigma_{e_f} = \\mathbb{E}[e_fe_f^T].$\nProof. By Proposition 2, we have that for any two encoders $f_1 : \\mathbb{R}^n \\to [2^{nR}]$ and $f_2 : \\mathbb{R}^n \\to [2^{nR}]$, when the\noptimal decoder is used, it holds that\n$D^{DIP} = \\frac{1}{n} [trace(\\Sigma_{e_{f_1}}) + trace (\\Sigma_{e_{f_2}}) \u2013 trace(\\Sigma_{e_{f_1}} \\Sigma_{e_{f_2}})]$\n$\\frac{1}{n} [trace(\\Sigma_{e_{f_1}}) + trace(\\Sigma_{e_{f_2}}) \u2013 trace(\\Sigma_{e_{f_1}} \\Sigma_{e_{f_2}})]$,"}, {"content": "where the last equality follows since P = Q, and therefore U and V have the same distribution. By the Cauchy-\nSchwartz inequality,\ntrace ($\\Sigma_{e_{f_1}} \\Sigma_{e_{f_2}}$) = $\\sum_{i=1}^{n}\\sum_{j=1}^{n} [(\\Sigma_{e_{f_1}})_{i,j} (\\Sigma_{e_{f_2}})_{j,i}]$\n\u2264$\\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} ((\\Sigma_{e_{f_1}})_{i,j})^2 \\sum_{i=1}^{n}\\sum_{j=1}^{n} ((\\Sigma_{e_{f_2}})_{j,i})^2 }$"}, {"title": "Compression for Matrix Multiplication", "content": "Let $A \u2208 Rn\u00d7a$ be a matrix whose entries are drawn iid from the distribution P and B\u2208 Rn\u00d7b be a matrix,\nstatistically independent of A, whose entries are drawn iid from the distribution Q. We assume both P and Q are\ndistributions with unit-variance. We consider the problem of quantizing A and B in order to compute their matrix\nmultiplication AT B. In particular, an (n, a, b, R1, R2, D) code consists of mappings\n$f_1: Rn\u00d7a \u2192 [2naR1]$\n$f_2: Rn\u00d7b \u2192 [2nbR2]$\n$g: [2naR1] \u00d7 [2nbR2] \u2192 [Raxb,$"}, {"content": "with\n$D = DMM =\\frac{1}{nab}-E||AT B \u2013 g(f1(A), f2(B))||."}, {"content": "We define\n$D^{MM,*} (R_1, R_2) = D^{MM,*}(R_1, R_2, P, Q) = \\inf \\{D : \\exists (n, a, b, R1, R2, D) \u2013 code\\}.$"}, {"content": "We further define the asymptotic function\n$D^{MM} (R_1, R_2) = D^{MM} (R_1, R_2, P, Q) = \\limsup_{n\u2192\u221e} D,a,b(R1, R2),$"}, {"title": "Basic Properties and Bounds", "content": "Denote WA = f1(A) and WB = f2(B) and further denote A = E[A|WA] and B = E[B|WB]. Define \u03a3A =\nE[(A \u2013 A)(A \u2013 A)T] \u2208 Rn\u00d7n and MA = E[AA] \u2208 Rn\u00d7n. Similarly, \u03a3B = E[(B \u2013 B) (B \u2013 B)] \u2208 Rn\u00d7n and\nMB = E[BB] \u2208 R\u00f1\u00d7n. As in the scalar case, we still have the identities:\n\u03a3A + MA = aIn\n\u03a3B + MB = bIn."}, {"content": "The next theorem generalizes the basic bounds we derived above for the inner product case, to the matrix multi-\nplication case. The proofs are similar to the statements above, and are therefore omitted.\nTheorem 8: The following hold:"}, {"title": "Lattice Quantization Scheme for Matrix Multiplication of Arbitrary Matrices", "content": "Our theoretical analysis in Sections II -IV assumed the entries in the vectors/matrices to be multiplied are drawn\niid from some known distribution. In this section, we drop this assumption, and, building on the observations from\nthe analysis above, develop a robust scheme for compression for matrix multiplication. Our scheme is designed\nto attain the optimal distortion in the case where A and B have iid Gaussian entries, but the error it attains for\narbitrary matrices can also be upper bounded.\nWe first develop encoders f1, f2 : Rn \u2192 [2nR] and a decoder g : [2nR] \u00d7 [2nR] \u2192 R for estimating the inner\nproduct of U, V \u2208 \u221anSn\u22121 where Sn\u22121 = {x \u2208 Rn : ||x|| = 1} is the unit sphere. We then show how these\nencoders and decoder can be leveraged for compression for matrix multiplication. Let On(R) be the orthogonal\ngroup, consisting of all orthonormal matrices in Rnxn. It will be useful to analyze the performance of f1, f2,9\nwith respect to the following distribution on U, V.\nDefinition 1 (p-correlated spherically uniform random vectors): Let S = [S1|S2|\u00b7\u00b7\u00b7|Sn] ~ Uniform(On(R)) be\na random matrix uniformly distributed over the group of orthogonal matrices in Rnxn (that is, S is drawn from\nthe Haar measure on On(R)). We say that the random vectors U \u2208 R and V\u2208 R are p-correlated spherically\nuniform random vectors if U = \u221anS1, Z = \u221anS2 and\n$V = \\rho U + \\sqrt{1 \u2013 \\rho^2} Z.$"}, {"content": "Theorem 11: For any $\u025b > 0$ and sufficiently large n\n1) There exist encoders f1, f2 : Rn \u2192 [2nR] and a decoder g : [2nR] \u00d7 [2nR] \u2192 R, such that if U, V are\n\u03c1-correlated spherically uniform\n$\\mathbb{E}(U^TV - g(f_1(U), f_2(V))^2 < n(\\Gamma(R) - \\Gamma^2(R) + \u03b5) + \\rho^2n^2(\\Gamma^2 (R) + \u03b5),$"}, {"content": "for every 0 < \u03c1 < 1, where \u0393(R) is defined in (1).\n2) There exists an encoder f : R\" \u2192 [2nR] and a decoder g : [2nR] \u00d7 R\" \u2192 R, such that if U, V are \u03c1-correlated\nspherically uniform\n$\\mathbb{E}(U^TV - g(f(U), V))^2 < n(2^{-2R} - 2^{-4R} + \u03b5) + \\rho^2n^2 (2^{-4R} + \u03b5),$"}, {"content": "for every 0 < \u03c1 \u2264 1.\nThe proof of Theorem 11 is based on the nested lattice coding scheme described in Subsection V-A1, and its\nperformance analysis in Subsection V-A2 and Subsection V-B. A simple variation of the scheme described in"}, {"title": "Proof of Theorem 11, Part 1", "content": "1) Dithered Nested Lattice Quantization for Inner Product: It will be convenient to denote the dimension by d\nrather than n. Thus, we assume in this and in the next subsection that that U and V are \u03c1-correlated spherically\nuniform random vectors in Rd rather than in Rn. The reason is that we will later use time-sharing, and describe\nonly the first d \u2264 n coordinates of U and V using the scheme below.\nLet Ac C Af be a pair of nested lattices in Rd. Assume that |Af/Ac| = 2dR. See [37] for basic lattice definitions.\nDenote by Ve the Voronoi region of Ac and by Vf the Voronoi region of Af. Let Z1, Z2 ~ Uniform(Vf) be\nstatistically independent dither vectors. Let 0 < D < 1, and assume that\n$\\sigma^2(\\Lambda_f) = \\mathbb{E}||Z_1||^2 = D.$"}, {"content": "This is without loss of generality, as we can always scale both lattices Ac C Af by the same factor \u03b2 > 0, so that\nthe lattice A will satisfy this constraint. We denote the nearest neighbor quantizer with respect to the lattice Af,\napplied on x \u2208 Rd, as\n$Q_{\\Lambda_f}(x) = \\underset{\\lambda \\in \\Lambda}{\\operatorname{argmin}} ||x \u2013 \u03bb||,$"}, {"content": "where ties are broken arbitrarily, but in systematic manner. The modulo operation with respect to the lattice Af, is\ndefined in this paper as\n$[x] \\mod \\Lambda_f = x - Q_{\\Lambda_f}(x).$"}, {"content": "Note that $[x] \\mod \\Lambda_f \u2208 V_f$. The operations $Q_{\\Lambda_c}(x)$ and $[x] \\mod \\Lambda_c$ are defined similarly.\nLet\n$a = \\sqrt{1 - D}.$"}, {"content": "Our encoders f1, f2 : Rd \u2192 [2dR] compute\n$\\hat U = [Q_{\\Lambda_f}(aU + Z_1)] \\mod \\Lambda_c$\n$\\hat V = [Q_{\\Lambda_f}(aV + Z_2)] \\mod \\Lambda_c, $"}, {"content": "and each of them maps the result to dR bits (which is possible since |Af/Ac| = 2dR).\nThe decoder g(f1(U), f2(V)) computes\n$\\hat{\\hat U} = a ( [\\hat U \u2013 Z_1] \\mod \\Lambda_c)$\n$\\hat{\\hat V} = a ( [\\hat V \u2013 Z_2] \\mod \\Lambda_c),$"}, {"title": "Practical Implementation of Nested Lattice Quantizers", "content": "In the proof of Theorems 11-12 we used a pair of nested lattices Ac C Af C Rd, with |Af/Ac| = 2dR. Given\nsuch a pair of lattices in Rd, in order to implement the coding scheme described above, we need to implement the\nfollowing procedures:\n1) $Q_{\\Lambda_f}(x) = \\underset{\\lambda \\in \\Lambda_f}{\\operatorname{argmin}} ||x \u2013 \u03bb||$\n2) $Q_{\\Lambda_c}(x) = \\underset{\\lambda \\in \\Lambda_c}{\\operatorname{argmin}} ||x \u2013 \u03bb||$\n3) Mapping from Af/Ac to dR bits\n4) Mapping from dR bits to the coset representatives Afn Ve of Af/Ac\n5) Generating a random dither Z ~ Uniform (VA), where VAF is the Voronoi cell of Af\nSelf-similar nested lattice codebooks: Let A \u2282 Rd be a lattice with generating matrix G \u2208 Rd\u00d7d, such that\nA = GZd. Assume that we have access to a procedure that implements the lattice quantizer Q\u2227(x) efficiently, and\nthat there is some \u03ba > 0 such that kZd C A. The assumption that Zd is nested in A (up to scaling) is not very\nimportant, but also not restrictive, since the majority of lattices for which efficient lattice quantizers are known do\nsatisfy it.\nUsing the lattice A, we can construct a pair of nested lattices Ac C Af C Rd, with |Af/Ac| = 2dR, that induce\nan efficiently implementable coding scheme. In particular, let \u03b2 > 0 and set Af = \u03b2A, Ac = qAf = \u03b2\u00b7 qA, where\nq = 2R is an integer. Algorithm 1 below provides the pseudo code for implementing f1, f2 from Subsection V-A1\nwith a = 1 for such a nested lattice codebook. Note that the output OverloadError of Algorithm 1 specifies\nwhether or not Assumption 1 holds, that is, whether or not a modulo error occurred. In order to implement the\ndecoder g from Subsection V-A1 (again with a = 1), one implements (133) by applying Algorithm 2 on the output\nof f1, implements (134) by applying Algorithm 2 on the output of f2, and computes the inner product of the two\nvectors. In order to generate the random dithers Z1, Z2, one applies Algorithm 3.\nChoice of the parameter \u03b2: Using this scheme, we have that\n$D = \u03c3^2(\u039bf) = \u03b2^2\u03c3^2(\u039b).$"}, {"title": "Open problems", "content": "One can interpret our Lemma 2 as follows: Let P = N(0,1) and Un ~ P\u00aen. Then for any random variable Y\nwe have that\n$\\sum_{i=1}^{n}R_P(\\lambda_i) \\leq I(U^n; Y),$"}, {"content": "where RP(D) is the quadratic rate-distortion function for a source with distribution P and ($\u03bb_1,..., \u03bb_n$) are the\neigenvalues of Cov(Un|Y). While Lemma 2 establishes (186) for the Gaussian distribution, we were not able to\nprove (186) for a general distribution, and we could neither find a counterexample. If (186) turns out to hold for\nany P, the proof of Theorem 6 could be easily extended to show that\n$D^{DIP,*}(R, P) = convex envelope of (\\phi(D_P(R)),$"}, {"content": "where DP(R) is the quadratic distortion-rate function for a source with distribution P. Thus, proving or disproving\nthat (186) holds for all P is an interesting problem for future research.\nIn Theorem 1 we have shown the existence of encoders and decoder for quantization for matrix multiplication\nwhose expected approximation error depends only on ||A||\u00b7||B|| and ||AB||, and is optimal for A and B\nwhose entries are iid Gaussian. For iid Gaussian matrices we have that == 0(1) so that the two error\nterms in (3) are well-balanced. However, when the entries of AT B are large, that is when the error term in (3)\nthat involves ||AB|| is the dominant one. On the other hand, in this case our bound from\nTheorem 3 gives much smaller error, despite the fact that it is not optimal for Gaussian matrices. In particular, the\nmultiplicative error of the bound from Theorem 3 vanishes when = 0(1). It is an interesting question\nfor future research to understand whether for any R > 0 there exist schemes that attain the optimal rate-distortion\ntradeoff for Gaussian iid matrices and at the same time attain a multiplicative error E that"}]}