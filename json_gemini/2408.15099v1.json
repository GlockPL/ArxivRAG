{"title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery", "authors": ["Alex Rutherford*", "Michael Beukman*", "Timon Willi", "Bruno Lacerda", "Nick Hawes", "Jakob Foerster"], "abstract": "What data or environments to use for training to improve downstream performance is a longstanding and very topical question in reinforcement learning. In particular, Unsupervised Environment Design (UED) methods have gained recent attention as their adaptive curricula enable agents to be robust to in- and out-of-distribution tasks. We ask to what extent these methods are themselves robust when applied to a novel setting, closely inspired by a real-world robotics problem. Surprisingly, we find that the state-of-the-art UED methods either do not improve upon the na\u00efve baseline of Domain Randomisation (DR), or require substantial hyperparameter tuning to do so. Our analysis shows that this is due to their underlying scoring functions failing to predict intuitive measures of \u201clearnability\", i.e., in finding the settings that the agent sometimes solves, but not always. Based on this, we instead directly train on levels with high learnability and find that this simple and intuitive approach outperforms UED methods and DR in several binary-outcome environments, including on our domain and the standard UED domain of Minigrid. We further introduce a new adversarial evaluation procedure for directly measuring robustness, closely mirroring the conditional value at risk (CVaR). We open-source all our code and present visualisations of final policies here: https://github.com/amacrutherford/sampling-for-learnability.", "sections": [{"title": "1 Introduction", "content": "Curriculum discovery-automatically generating environments for reinforcement learning (RL) agents to train on remains a longstanding and active area of research [1, 2]. Automated curriculum learning (ACL) methods offer the potential to generate diverse environments, leading to the development of more general and robust agents. Recently, a class of methods under the umbrella of Unsupervised Environment Design (UED) has gained popularity, owing to their theoretical guarantees of robustness and empirical improvements in out-of-distribution generalisation [3-6]. However, many of these methods are evaluated using a narrow set of environments, with a strong focus on gridworld mazes [7, 8]. This issue is compounded by the advent of GPU-accelerated environments and algorithm implementations [9-12], leading to a lack of support for a wide range of challenging benchmark environments in current open-source UED codebases [7, 8]. In this paper, we explore how well these UED methods generalise to a different task, specifically JaxNav, a continuous single- and multi-robot navigation task in a partially observable setting modelled via range-based observations. JaxNav modifies the gridworld setting, making it closer to a real-world robotics application.\nPerhaps surprisingly, we find that current state-of-the-art (SoTA) UED approaches either fail to outperform the na\u00efve baseline of Domain Randomisation (DR) or require significant hyperparameter tuning to do so. Our analysis reveals that the scoring functions used by these methods to prioritise"}, {"title": "2 Background", "content": "We model the reinforcement learning problem as an underspecified partially observable Markov decision process (UPOMDP) [3], denoted by M = \u3008A, O, O, S, T,I,R, \u03b3). Here, A, S, and O represent the action, state, and observation spaces, respectively. The agent receives an observation o (without directly knowing the true state s) and selects an action, which results in a transition to a new state, a new observation, and an associated reward.\nrepresents the set of possible parameters, where each \u03b8\u2208 defines a specific level. Each @ corresponds to a particular instantiation of the POMDP, with an associated transition function P: S\u00d7A \u2192 A(S) and an observation function Io : S \u2192 0.\nIn a multi-agent setting, n agents make decisions simultaneously. At each step, agent i chooses an action ar, forming a joint action a = {a1, ..., an} that transitions the environment according to Pe. Each agent then receives a reward based on the reward function R : S \u2192 R, which may be shared among all agents or be agent-specific."}, {"title": "2.2 Unsupervised Environment Design (UED)", "content": "UED is an autocurricula paradigm that frames curriculum design as a two-player zero-sum game between a level-generating adversary and an agent. The agent seeks to maximise its expected return in the standard RL manner, while the adversary can pursue various objectives. Domain Randomisation (DR) fits within this framework by assigning a constant utility to each level, reducing level generation to mere random sampling [13]. Worst-case methods, on the other hand, incentivise the adversary to minimise the agent's reward, aiming to enhance performance on the most challenging levels [14]. However, this approach often results in the generation of unsolvable levels [3].\nRegret-based UED methods offer an alternative by generating levels that maximise the agent's regret. Here, the regret of a policy \u03c0on a level \u03b8 is defined as the difference between the policy's discounted"}, {"title": "3 JaxNav", "content": "In this section, we first touch on hardware-accelerated environments, and robotic navigation. We then go on to introduce JaxNav, a hardware-accelerated, single and multi-agent robotic navigation"}, {"title": "3.1 Hardware Accelerated Environments", "content": "Recently, Bradbury et al. [18] released JAX, a Python numpy-like library that allows computations to run natively on accelerators (such as GPUs and TPUs). This has led to an explosion in reinforcement-learning environments being implemented in JAX, leading to the time it takes to train an RL agent being reduced by hundreds or thousands of times [10, 12, 19, 20]. This has enabled researchers to run experiments that used to take weeks in a few hours [7, 8]. One side effect of this, however, is that current UED libraries are written in JAX, meaning they are primarily compatible with the (relatively small) set of JAX environments."}, {"title": "3.2 Robot Navigation", "content": "Motion planning is a fundamental problem for mobile robotics. The general aim is to find a collision-free path from a starting location to a goal region in two or three dimensions. We focus specifically on the popular setting of 2D navigation problems for differential drive robots using 2D LiDAR readings as the sensory input for their navigation policies. Given range readings, the robot's current velocity and the direction of its goal, the navigation policy must produce velocity commands to move the robot to its goal location while avoiding static and dynamic obstacles."}, {"title": "3.3 Environment Description", "content": "The observation space of JaxNav is highly partially observable and is based on LiDAR readings, depicted by the blue dots in Figure 1b. This is in contrast to Minigrid, which provides a top-down, ego-centric, image-like observation, as shown by the highlighted region in Figure 1a. Additionally, while Minigrid features discrete forward and turn actions, the robots in JaxNav operate in continuous space using differential drive dynamics. Similar to Minigrid, agents in JaxNav must navigate from a starting location to a goal region, with the goal centre represented by the green cross in Figure 1b.\nThese design choices make JaxNav a close approximation of many real-world robotic navigation tasks [21, 22], including the ICRA BARN Challenge [23], which is depicted in Figure 1c. This challenge, which has run annually since 2022, aims to benchmark single-robot navigation policies in constrained environments for differential drive robots using 2D LiDAR as sensory input. Even with a cell size of 1.0 m, JaxNav offers a similar clearance between robots and obstacles as the test maps used in the BARN Challenge, underscoring its relevance not only for evaluating UED methods but also for advancing robotics research. Our environment's full design is outlined in Appendix A."}, {"title": "4 Understanding and Improving Level Selection in Goal Directed Domains", "content": "In this section, we examine current UED methods, and investigate how they select levels to train on. In particular, we investigate how well currently-used score functions correlate with (a) success rate (i.e., the fraction of times the agent solves the level); and (b) learnability (defined below). We then develop a method which directly samples levels according to their learnability potential, with the following sections detailing our experimental setup and results."}, {"title": "4.1 Defining Learnability", "content": "Similarly to the \u201cGoals of Intermediate Difficulty\" objective proposed by Florensa et al. [1], we desire agents to learn on levels that they can solve sometimes but have not yet mastered. Such levels hold the greatest source of possible improvement for an agent's policy and so a successful autocurricula method must be able to find these. Indeed, given a success rate (i.e., the fraction of times the agent solves the level) of p on a given level, we define learnability to be \u0440. (1 \u2013 \u0440).\nIntuitively, we justify our definition as follows (in a goal-based setting where there is only a nonzero reward for reaching the goal):\n1. p represents how likely the agent is to obtain positive learning experiences from a level."}, {"title": "4.2 Analysing Regret Approximations used by UED Methods", "content": "Having defined learnability, we now turn our attention to the current UED score functions. As demonstrated in Section 7, the latest state-of-the-art UED methods fail to outperform Domain Randomisation (DR) in the multi-agent JaxNav environment. To highlight the limitations of these approaches, we examine whether their score functions can reliably identify the frontier of learning, i.e., levels that agents can only sometimes solve.\nWe focus on the single-agent version of JaxNav and conduct rollouts using the top-performing seed for PLR-MaxMC on randomly sampled levels over 5000 timesteps. From these rollouts, we compile a set of 2500 levels, evenly distributed into 10 bins based on mean success rate values ranging from 0 to 1. We then perform additional rollouts on this collected set, running for 512 environment timesteps (the same number as used during training) across 10 parallel workers, and average the results. In Figure 2, we plot the mean MaxMC, PVL and Learnability scores against the mean success rate for each level. We additionally report the Pearson correlation coefficient, r, and p-value for the linear relationship between the success rate and the regret score.\nOur analysis reveals no correlation between MaxMC and learnability, and MaxMC instead shows a slight correlation with success rate. While PVL has a weak correlation with learnability, the high vari-ance causes already-solved maps to be prioritised alongside those with high learnability. These plots contrast heavily with that of our learnability metric, which directly prioritises levels with the greatest expected improvement. We hypothesise that the root cause of this issue is the agent's poor value estimation. In a highly partially observable environment, the agent struggles to accurately estimate the value of a state, leading to noisy MaxMC and PVL scores, which in turn hinder UED methods from effectively identifying the learning frontier. Given that reward is strongly correlated with success rate, these findings also apply when comparing scores against reward, as detailed in Appendix F."}, {"title": "4.3 Sampling For Learnability: Our Simple and Intuitive Fix", "content": "Following on from our analysis, we now present Sampling For Learnability (SFL), a simple approach that directly chooses levels that optimise learnability. Our approach maintains a buffer of levels with high learnability and trains on a set of levels drawn from this buffer alongside randomly generated levels. Algorithm 1 outlines the overall approach for SFL and illustrates the relative simplicity of our method compared to SoTA UED approaches. The policy's weights & are updated using any RL algorithm; we use PPO [16] for all of our experiments. Meanwhile, our method for collecting learnable levels is detailed in Algorithm 2. We find that the default values of T = 50, \u03c1 = 0.5, NL = 256, L = 2000, N = 5000 and K = 1000 work well across domains. However, the"}, {"title": "5 Experimental Setup", "content": "Here, we outline the domains and methods used for evaluation along with our adversarial evaluation protocol. Rather than taking the common approach of reporting average performance on a set of hand designed levels (which by their very nature are arbitrary), we sample a large set of levels and examine each method's performance on their worst-case levels. This directly targets the tails of the level distribution and as such is a superior measure of robustness. We additionally report the comparative performance of methods on the sampled set to determine the degree to which one method dominates another in terms of solvability.\nWe use four domains for our experiments, JaxNav in single-agent mode, JaxNav in multi-agent mode, the common UED domain Minigrid [24] and XLand-Minigrid [10]. See Appendix B for more details about the environments. We use 10 seeds for Minigrid and single-agent JaxNav, and 5 seeds for multi-agent JaxNav and XLand-Minigrid. In all of our plots, we report mean and standard error.\nSince SFL performs more environment rollouts, we perform fewer PPO updates in single-agent JaxNav and XLand-Minigrid to ensure that SFL uses as much compute time as ACCEL. In Minigrid, the additional environment interactions take a negligible amount of time, so we run the same number of PPO updates for all methods. For multi-agent JaxNav, we compare each method using the same number of PPO updates. See Appendix H for more information on the relative speed of each method, and how many updates we run for each method. Generally, the additional SFL rollouts take much less time than the updates themselves, due to the massive parallelisation afforded by hardware-accelerated environments. Additionally, recent work suggests that world-models could also allow more samples to be taken than the base environment allows [25-28], highlighting the future potential of this approach.\nWe compare against several state-of-the-art UED methods as baselines, implemented with JaxUED [8]. We use ACCEL, with the MaxMC score function, where the agent trains on randomly generated, mutated and curated levels. We also include a \"robust\" version [5], where no gradient updates are performed on the former two sets of levels. This uses three times as many environment interactions and is roughly twice as slow as SFL for single-agent JaxNav. We use PLR with both the PVL and MaxMC score functions. We also include a robust version of PLR which only performs gradient updates on the curated levels; this uses twice as many environment interactions and is 80% slower than SFL on single-agent JaxNav. We also use Domain Randomisation (DR), which trains only on randomly generated levels, with no curation or prioritisation."}, {"title": "6 A Risk-Based Evaluation Protocol", "content": "The standard approach to evaluating UED agents is to test them on a set of hand-designed holdout levels [3, 5, 6]. Whilst this evaluation approach illustrates the performance of agents on human-relevant tasks, we believe it has several limitations. First, the hand-designed levels are arbitrary, so performance on them is not representative of general performance; second, it does not test a central"}, {"title": "7 Results", "content": "Figures 5 and 6 illustrate the performance of all methods on multi-agent JaxNav throughout training. We train with 4 agents and report performance over both a hand designed test set and a randomly sampled set of 100 maps. The levels used in the hand designed set are given in Appendix D and feature cases with 1, 2, 4 and 10 agents. The levels in the sampled set all feature 4 agents and solvability is checked for each agent's individual path. As we train with IPPO, regret scores are calculated on a per-agent basis and the score of a level is computed as the mean across individual agent scores. For n agents, learnability is computed as \u2211i=1(Pi\u00b7 (1 \u2212 pi)), where p\u2081 is the success rate for agent i on on a given level. We find that JaxNav significantly outperforms all UED methods."}, {"title": "7.3 Minigrid", "content": "We next move on to the standard domain of Minigrid (see Figure 7). Here we find that most methods perform similarly on the hand-designed test set; however, SFL significantly outperforms all other methods on the adversarial evaluation, indicating it results in more robust policies."}, {"title": "7.4 XLand-Minigrid", "content": "Our final evaluation domain is XLand-Minigrid's [10] meta-RL task using their high-3m benchmark. We report performance using our CVaR evaluation procedure and, in line with [10], as the mean return on an evaluation set during training. Our results are presented in Figure 8, with SFL outperforming both PLR and DR. During evaluation each ruleset was rolled out for 10 episodes. Due to the large number of levels being rolled out to fill SFL's buffer, SFL was slower than DR and PLR. As such, we report results for SFL compute-time matched to PLR."}, {"title": "8 Related Work", "content": "Unsupervised Environment Design (UED) has emerged as a prominent method in curriculum learning for reinforcement learning (RL), promising robust agent training through adaptive curricula. Early works focused on learning potential, where the improvement in an agent's performance determined the choice of training levels [1, 2, 29, 30]. However, robustness-oriented methods such as adversarial minimax introduced the notion of training on levels that minimise agent performance, though these often resulted in infeasible scenarios offering no learning benefits [3, 14, 31]. Minimax regret, a more refined robustness approach, alleviates some issues of adversarial sampling by ensuring the chosen levels are learnable [3, 5, 6]. However, recent work [32] demonstrated that even when regret can be exactly computed, it does not always correspond to learnability, and this mismatch can lead to stagnation during training. Our work extends this line of research by introducing a new scoring mechanism that estimates expected improvement, targeting environments with a positive but not perfect solve rate. Unlike existing minimax regret methods, our approach directly optimises for learnability, instead of using an imperfect proxy for regret, leading to more effective training on our domains. Robust RL methods have the goal of improving an agent's robustness to environmental disturbances, and worst-case environment dynamics [33-45]. However, these methods generally"}, {"title": "9 Discussion and Limitations", "content": "In this work we only consider deterministic, binary outcome domains and due to the nature of the learnability score, SFL is only applicable to such settings. In a non-binary-outcome domain, we could potentially reuse the intuition that p(1 \u2013 p) is the variance of a Bernoulli distribution; in a continuous domain, an analogous metric would be the variance of rewards obtained by playing the same level multiple times. Furthermore, our implementation of SFL is in JAX but the method is general. However, one must take the cost of SFL's additional environment rollouts into account when considering implementing our algorithm; we chose JAX because its speed and parallelisation significantly alleviates this constraint. Next, while most current SoTA UED methods, including SFL, randomly generate and curate levels, this approach may become infeasible when the environment space is vast, as random generation may have a very low likelihood of generating valid levels. Finally, while JaxNav does have deterministic dynamics, Fan et al. [46] had success transferring an RL-based"}, {"title": "10 Conclusion", "content": "In this paper we investigate to what extent the currently popular branch of curriculum discovery methods based on scoring functions that approximate minimax regret can be applied to multi-robot navigation. We find that the poor quality of the scoring functions used results in poor performance of UED methods, and propose a different metric based on an intuitive notion of learnability, which improves the robustness of the final policies. We also introduce a new evaluation protocol, reporting a risk measure on performance over the a% worst-case (but in principle solvable) levels for each method. We hope that our findings provide inspiration for future work on more general and domain agnostic scoring functions and we open-source all of our code to facilitate this process. Ultimately, we believe that this work is a stepping stone towards bridging the gap between popular testbeds for UED and real-world applications, such as multi-robot navigation, and serves an important reality check for this budding subfield of machine learning."}, {"title": "Appendix", "content": "We structure the appendix as follows. Appendix A includes more details about JaxNav, and Appendix B describes the other environments we use. The hyperparameters we use and the hand-designed test sets can be seen in Appendices C and D, respectively. Appendix E discusses multi-robot navigation and automated curriculum learning in more detail.\nWe next provide more results, including more analysis on the UED score functions in Appendix F, additional general results in Appendix G and compute-time analysis in Appendix H. Finally, we thoroughly ablate SFL in Appendix I."}, {"title": "A JaxNav Specification", "content": "The environment is designed as follows, with full parameters listed in Appendix C.\nObservations The robot's observation at a given timestep t is ot = [lt, dt, vt] containing the current LiDAR range readings (1t), the direction to the robot's goal (dt), and the robot's current linear and angular velocities (vt). The LiDAR range readings It is a vector containing the 100 most recent LiDAR range readings from a 360\u00b0 arc centred on the robot's forward axis, the LiDAR's max range Dlidar is set to 6 m. Given the robot's current position p\u2081 and the goal position g, the robot's goal direction dt is defined as:\ndt ={\\begin{array}{ll}polar(g-p_t) & \\text { if }||g-p_t||& otherwise\\end{array}\nwhere polar converts a Cartesian vector to its polar representation. All observation entries are normalised using their maximum possible values.\nActions The policy selects a two-dimensional continuous action at = [v\u0142, w\u0129] representing a target linear (v) and angular velocity (w\u0129). The possible linear and angular velocities are limited to a set range, with actions outside these ranges clipped to be within.\nDynamics The action at is translated into movement in the x-y plane using a differential drive kinematics model [47] which includes limits on linear and angular acceleration.\nRewards Our reward function is inspired by Long et al. [48] and aims to avoid collisions while minimising the expected arrival time. Due to the difficulty of the task we include shaping terms which give a small dense reward at each timestep. The reward r received at timestep t is defined as the sum: rt = r1 + r + Rtime, where ri rewards the robot for reaching the goal, re penalises collisions, and Rtime is a small penalty at each timestep equal to \u22120.01. The goal reaching term ri is defined as:\nif ||pg|| Dgoal otherwise, \\tag{2}\nwhere Rgoal = 4.0, Dgoal = 0.3 and wg = 0.25. This term rewards the agent for reaching the goal, and provides a small dense reward if the agent moves closer to the goal. Meanwhile, the collision penalty term rt is defined:\nr^c_t =\\begin{cases}R_{collision} &\\text{if collision} \\\\ R_{close} &\\text{if min}(l)\\end{cases}\\tag{3}\nwhere I are the un-normalised LiDAR readings at timestep t, Rcollision = -4, Rclose = -0.1, and Dclose = 0.4m. This term avoids collisions and provides a small dense penalty when the agent is close to obstacles, this encourages safe behaviour.\nMulti-Agent Reward In the multi-agent version of JaxNav, the reward for each agent i is defined as dri + (1 \u2212 \u03bb\u03af) \u2211 rj, i.e., it shares its own reward, as well as the team reward. We use X = 0.5."}, {"title": "B Environment Description", "content": "Here we describe the other environments we use, with environment parameters listed in Tables 1 to 3.\nMinigrid is a goal-oriented grid world where a triangle-like agent must navigate a 2D maze. As illustrated in Figure 1a, the agent only observes a small region in front of where it is facing and must explore the world to move to a goal location."}, {"title": "B.2 XLand-Minigrid", "content": "This domain combines an XLand-inspired system of extensible rules and goals with a Minigrid-inspired goal-oriented grid world to create a domain with a diverse distribution of tasks. Each task is specified by a ruleset, which combines rules for environment interactions with a goal, and [10] provide a database of presampled rulesets for use during training. Following [10], we use a 13x13 grid with 4 rooms and sample rulesets from their high diversity benchmark with 3 million unique tasks. As training involves sampling from a database of precomputed rulesets, ACCEL is not applicable. PLR and SFL select rulesets for each meta-RL step to maximise return on a held-out set of evaluation rulesets."}, {"title": "C Hyperparameters", "content": "Table 4 contains the hyperparameters we use, with their selection process for each domain outlined below. We tuned PPO for DR for each domain and then used these same PPO parameters for all methods, tuning only UED-specific parameters."}, {"title": "C.1 JaxNav", "content": "For PPO, we conducted an extensive sweep on the JaxNav environment ensuring robust DR perfor-mance. We only tuned hyperparameters for single-agent JaxNav, and used the best hyperparameters for multi-agent JaxNav. JaxNav hyperparameter searches were done over 3 seeds.\nFor PLR, we performed a grid search, over replay probabilities {0.5,0.8}, buffer capacity {1000, 4000, 8000}, prioritisation {rank, topk}, temperature {0.3,1.0}, and k {1,32,128}. For ACCEL, we searched over the same set and additionally included the number of edits, where we considered values of {5, 20, 50}."}, {"title": "C.2 Minigrid", "content": "For Minigrid, our JaxNav PPO parameters performed similarly to those given in the JaxUED implementation but allowed us to use 256 environment rollouts in parallel during training compared to JaxUED's 32. For the UED-specific parameters we used the same sweep settings as for JaxNav, again conducting the search over 3 seeds."}, {"title": "C.3 XLand-Minigrid", "content": "For PPO, we used the default parameters provided by [10] and the search for UED parameters was conducted over 1 seed. For PLR, we conducted a grid search over replay probabilities {0.5, 0.95}, buffer capacity {20000, 40000}, prioritisation {rank, topk}, temperature {0.3, 1.0}, score function {MaxMC, PVL}. For SFL, initial experiments illustrated that, due to the number of environment rollouts used to fill the buffer, it was slower than PLR and DR. To use a similar compute budget as PLR, we conducted the sweep over only 70B timesteps. For SFL, we performed a grid search over N of {40000, 30000}, L {5070, 7650}, K of {8192}, T of {1, 2, 3, 4} and p of {0.75, 1.0}."}, {"title": "D Hand Designed Test Sets", "content": "The hand-designed levels used for evaluating policy performance throughout training are illustrated in Figures 9 and 10. The set used for multi-agent policies also includes the first 3 maps in Figure 9. Minigrid's levels are shown in Figure 11 and are the same as those used by JaxUED."}, {"title": "E Additional Background Related Work", "content": "Multi-robot path planning presents unique challenges due to the need for coordination among robots to avoid deadlocks in dynamic environments. Traditional methods often discretise the environment, turning the problem into a multi-agent pathfinding task managed by a central planner [49, 50]. While effective at scale, these approaches rely heavily on communication infrastructure, making them impractical in scenarios with unreliable connectivity or third-party obstacles. In contrast, our method leverages decentralised learning approaches without relying on centralised communication, making it more robust in partially observable and communication-limited settings.\nDecentralised approaches like velocity obstacles [51] and ORCA [52] offer a solution by mapping environmental constraints into the robot's velocity space. However, these methods are susceptible to measurement errors and often exhibit short-sighted behavior, limiting their real-world applicabil-ity [17]. Our method, by comparison, incorporates a broader evaluation framework that assesses performance in adversarial and challenging environments, ensuring robustness beyond simple dy-namic obstacle avoidance.\nRecent advancements leverage machine learning to overcome these limitations. CADRL [53] uses RL to address short-sightedness, but its state-based representation limits its adaptability. Our approach, instead, employs lidar-based observations to model a more realistic and complex navigation task, which allows for better generalisation to real-world scenarios.\nMore sophisticated RL-based approaches, such as those by Long et al. [48] and Tan et al. [54], demonstrate improved performance in open spaces but struggle in constrained settings. Our method specifically addresses this by focusing on environments that are solved intermittently, thereby enhanc-ing the agent's ability to learn in varied and complex settings.\nHybrid methods, combining RL with conventional planning [46], show promise but do not fully address these challenges. In contrast, our method integrates an adaptive curriculum that dynamically adjusts based on the agent's performance, leading to sustained learning improvements even in diverse and adversarial environments.\nThe design of reward functions is critical in RL-based navigation. Enhancements using velocity obsta-cles [17, 55] improve performance but still face challenges in real-world transferability. Techniques like perceptual hallucination [56] further enhance robustness by reducing multi-robot planning to static obstacle avoidance, though they typically consider simple scenarios and do not account for dynamic third-party obstacles. Our method, however, introduces a novel evaluation protocol that rigorously tests the robustness of learned policies in a variety of adversarially generated environments, ensuring better real-world applicability."}, {"title": "E.2 Background on Automated Curriculum Learning", "content": "Automated Curriculum Learning (ACL) is a subfield of RL where agents are presented with increas-ingly challenging tasks that are adapted to the agent's current progress [57, 58]. One common idea idea is to train the agent on tasks that are neither too easy nor too hard, such that it achieves maximum learning potential [1, 59]. Autocurricula methods have various aims, such as improving learning speed on a set of target environments [60] or increasing robustness to unknown environment config-urations [3, 5]. Unsupervised environment design (UED) focuses on the latter. One commonality between autocurricula methods is that the environment generator controls aspects of the environment, such as the transition dynamics, state and observation spaces, goals, and so on [1, 3]. Each of these environment configurations is commonly referred to as a level [3].\nMethods also differ in how they generate these environment configurations. One class of methods uses generative models, such as Gaussian Mixture Models (GMMs) [2]. While this approach generally makes the problem theoretically tractable, GMMs are limited to continuous-valued parameter settings. More recently, other generative models, such as Variational Autoencoders, have been used [60]. However, these models often require data to train, which may be unavailable or bias the learning process. Other methods use an RL-based level generator, where the generator's objective is based on how the agent performs on the generated level [3]. This approach has been surpassed by the more recent technique of randomly generating and curating levels [4-6]."}, {"title": "F Extended analysis of UED Score Functions", "content": "An extension of Section 4.2, Figure 12 illustrates the correlation between mean reward and the two most popular regret scores, MaxMC and PVL. These graphs illustrate that the trends seen for success rate also hold for episodic reward, this is expected for our environment as the two are strongly correlated. In Figure 13 we conduct the same analysis for the L1 Loss Score."}, {"title": "G Additional Results", "content": "Figure 14 shows each method's overall solve rate on the set of 10000 sampled solvable levels on a log scale. We find that while all methods solve the vast majority of levels, SFL slightly outperforms all the baseline methods. Figure 15 reports the pairwise comparisons of each base against SFL for Minigrid. While, again, most methods solve most levels, SFL has a slight advantage."}, {"title": "G.1 Episodic Return Plots", "content": "Figure 16 shows episodic return and success rate plots for single-agent JaxNav and Minigrid. We find that the episodic return is very strongly correlated with the success rate, which is why we primarily show the latter in the main text."}, {"title": "G.2 Easy Level Analysis", "content": "To assess performance on easy levels we have run our evaluation procedure over 10,000 uniformly sampled levels with fewer obstacles than usual. For JaxNav, we used a maximum fill % of < 30%, half of the standard 60%. Meanwhile, for Minigrid, we use a maximum number of 30 walls instead of 60. These levels, therefore, are generally easier than the levels we evaluated on in the main paper. Results are reported in Figure 17\nOn JaxNav, SFL still demonstrates a significant performance increase while on Minigrid all methods are very similar (with the robust methods performing slightly better for low values of a). Due to the challenging dynamics of JaxNav, even levels with a small number of obstacles can present difficult control and navigation problems meaning ACL methods (such as SFL) still lead to a performance differential over DR. Meanwhile, in Minigrid, due to its deterministic dynamics, difficulty is heavily linked to the obstacle count as this allows for more complex mazes. As such, DR is competitive to ACL methods in settings with fewer obstacles."}, {"title": "G.3 Analysing the Learnability of Levels", "content": "Table 5 shows the mean and median of learnability and success rate for a variety of methods. We find that the average learnability of levels in the PLR/ACCEL buffers is very low. While not shown"}, {"title": "H Timing Results and Speed Analysis", "content": "Tables 6 and 7 report compute time for all methods on single-agent JaxNav and Minigrid, respectively. Each individual seed was each run on 1 Nvidia L40s using a server which has 8 NVIDIA L40s', two AMD EPYC 9554 processors (128 cores in total) and 768GB of RAM. These times are without logging, and we find that with logging, SFL is around 6% slower than ACCEL on single-agent JaxNav."}]}