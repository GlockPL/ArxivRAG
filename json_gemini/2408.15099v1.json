{"title": "No Regrets: Investigating and Improving Regret Approximations for Curriculum Discovery", "authors": ["Alex Rutherford", "Michael Beukman", "Timon Willi", "Bruno Lacerda", "Nick Hawes", "Jakob Foerster"], "abstract": "What data or environments to use for training to improve downstream performance\nis a longstanding and very topical question in reinforcement learning. In particular,\nUnsupervised Environment Design (UED) methods have gained recent attention\nas their adaptive curricula enable agents to be robust to in- and out-of-distribution\ntasks. We ask to what extent these methods are themselves robust when applied to\na novel setting, closely inspired by a real-world robotics problem. Surprisingly, we\nfind that the state-of-the-art UED methods either do not improve upon the na\u00efve\nbaseline of Domain Randomisation (DR), or require substantial hyperparameter\ntuning to do so. Our analysis shows that this is due to their underlying scoring\nfunctions failing to predict intuitive measures of \u201clearnability\", i.e., in finding\nthe settings that the agent sometimes solves, but not always. Based on this, we\ninstead directly train on levels with high learnability and find that this simple and\nintuitive approach outperforms UED methods and DR in several binary-outcome\nenvironments, including on our domain and the standard UED domain of Minigrid.\nWe further introduce a new adversarial evaluation procedure for directly measuring\nrobustness, closely mirroring the conditional value at risk (CVaR). We open-source\nall our code and present visualisations of final policies here: https://github.\ncom/amacrutherford/sampling-for-learnability.", "sections": [{"title": "1 Introduction", "content": "Curriculum discovery-automatically generating environments for reinforcement learning (RL)\nagents to train on remains a longstanding and active area of research [1, 2]. Automated curricu-\nlum learning (ACL) methods offer the potential to generate diverse environments, leading to the\ndevelopment of more general and robust agents. Recently, a class of methods under the umbrella of\nUnsupervised Environment Design (UED) has gained popularity, owing to their theoretical guaran-\ntees of robustness and empirical improvements in out-of-distribution generalisation [3-6]. However,\nmany of these methods are evaluated using a narrow set of environments, with a strong focus on\ngridworld mazes [7, 8]. This issue is compounded by the advent of GPU-accelerated environments\nand algorithm implementations [9-12], leading to a lack of support for a wide range of challenging\nbenchmark environments in current open-source UED codebases [7, 8]. In this paper, we explore how\nwell these UED methods generalise to a different task, specifically JaxNav, a continuous single- and\nmulti-robot navigation task in a partially observable setting modelled via range-based observations.\nJaxNav modifies the gridworld setting, making it closer to a real-world robotics application.\nPerhaps surprisingly, we find that current state-of-the-art (SoTA) UED approaches either fail to\noutperform the na\u00efve baseline of Domain Randomisation (DR) or require significant hyperparameter\ntuning to do so. Our analysis reveals that the scoring functions used by these methods to prioritise"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Reinforcement Learning & UPOMDPS", "content": "We model the reinforcement learning problem as an underspecified partially observable Markov\ndecision process (UPOMDP) [3], denoted by $\\mathcal{M} = \\langle \\mathcal{A}, \\mathcal{O}, \\Theta, \\mathcal{S}, T, I, R, \\gamma\\rangle$. Here, $\\mathcal{A}, \\mathcal{S}$, and $\\mathcal{O}$\nrepresent the action, state, and observation spaces, respectively. The agent receives an observation $o$\n(without directly knowing the true state $s$) and selects an action, which results in a transition to a new\nstate, a new observation, and an associated reward.\n$\\Theta$ represents the set of possible parameters, where each $\\theta \\in \\Theta$ defines a specific level. Each\n$\\Theta_{\\theta}$ corresponds to a particular instantiation of the POMDP, with an associated transition function\n$P_{\\theta}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta(\\mathcal{S})$ and an observation function $I_{\\theta}: \\mathcal{S} \\rightarrow \\Delta(\\mathcal{O})$.\nIn a multi-agent setting, $n$ agents make decisions simultaneously. At each step, agent $i$ chooses an\naction $a_i$, forming a joint action $a = \\{a_1, ..., a_n\\}$ that transitions the environment according to $P_{\\theta}$.\nEach agent then receives a reward based on the reward function $R: \\mathcal{S} \\rightarrow \\mathbb{R}$, which may be shared\namong all agents or be agent-specific."}, {"title": "2.2 Unsupervised Environment Design (UED)", "content": "UED is an autocurricula paradigm that frames curriculum design as a two-player zero-sum game\nbetween a level-generating adversary and an agent. The agent seeks to maximise its expected return in\nthe standard RL manner, while the adversary can pursue various objectives. Domain Randomisation\n(DR) fits within this framework by assigning a constant utility to each level, reducing level generation\nto mere random sampling [13]. Worst-case methods, on the other hand, incentivise the adversary to\nminimise the agent's reward, aiming to enhance performance on the most challenging levels [14].\nHowever, this approach often results in the generation of unsolvable levels [3].\nRegret-based UED methods offer an alternative by generating levels that maximise the agent's regret.\nHere, the regret of a policy $\\pi$ on a level $\\theta$ is defined as the difference between the policy's discounted"}, {"title": "3 JaxNav", "content": "In this section, we first touch on hardware-accelerated environments, and robotic navigation. We\nthen go on to introduce JaxNav, a hardware-accelerated, single and multi-agent robotic navigation"}, {"title": "3.1 Hardware Accelerated Environments", "content": "Recently, Bradbury et al. [18] released JAX, a Python numpy-like library that allows computations to\nrun natively on accelerators (such as GPUs and TPUs). This has led to an explosion in reinforcement-\nlearning environments being implemented in JAX, leading to the time it takes to train an RL agent\nbeing reduced by hundreds or thousands of times [10, 12, 19, 20]. This has enabled researchers to\nrun experiments that used to take weeks in a few hours [7, 8]. One side effect of this, however, is that\ncurrent UED libraries are written in JAX, meaning they are primarily compatible with the (relatively\nsmall) set of JAX environments."}, {"title": "3.2 Robot Navigation", "content": "Motion planning is a fundamental problem for mobile robotics. The general aim is to find a collision-\nfree path from a starting location to a goal region in two or three dimensions. We focus specifically on\nthe popular setting of 2D navigation problems for differential drive robots using 2D LiDAR readings\nas the sensory input for their navigation policies. Given range readings, the robot's current velocity\nand the direction of its goal, the navigation policy must produce velocity commands to move the\nrobot to its goal location while avoiding static and dynamic obstacles."}, {"title": "3.3 Environment Description", "content": "The observation space of JaxNav is highly partially observable and is based on LiDAR readings,\ndepicted by the blue dots in Figure 1b. This is in contrast to Minigrid, which provides a top-down,\nego-centric, image-like observation, as shown by the highlighted region in Figure 1a. Additionally,\nwhile Minigrid features discrete forward and turn actions, the robots in JaxNav operate in continuous\nspace using differential drive dynamics. Similar to Minigrid, agents in JaxNav must navigate from a\nstarting location to a goal region, with the goal centre represented by the green cross in Figure 1b.\nThese design choices make JaxNav a close approximation of many real-world robotic navigation\ntasks [21, 22], including the ICRA BARN Challenge [23], which is depicted in Figure 1c. This\nchallenge, which has run annually since 2022, aims to benchmark single-robot navigation policies in\nconstrained environments for differential drive robots using 2D LiDAR as sensory input. Even with a\ncell size of 1.0 m, JaxNav offers a similar clearance between robots and obstacles as the test maps\nused in the BARN Challenge, underscoring its relevance not only for evaluating UED methods but\nalso for advancing robotics research. Our environment's full design is outlined in Appendix A."}, {"title": "4 Understanding and Improving Level Selection in Goal Directed Domains", "content": "In this section, we examine current UED methods, and investigate how they select levels to train on.\nIn particular, we investigate how well currently-used score functions correlate with (a) success rate\n(i.e., the fraction of times the agent solves the level); and (b) learnability (defined below). We then\ndevelop a method which directly samples levels according to their learnability potential, with the\nfollowing sections detailing our experimental setup and results."}, {"title": "4.1 Defining Learnability", "content": "Similarly to the \u201cGoals of Intermediate Difficulty\" objective proposed by Florensa et al. [1], we desire\nagents to learn on levels that they can solve sometimes but have not yet mastered. Such levels hold\nthe greatest source of possible improvement for an agent's policy and so a successful autocurricula\nmethod must be able to find these. Indeed, given a success rate (i.e., the fraction of times the agent\nsolves the level) of $p$ on a given level, we define learnability to be $p \\cdot (1 - p)$.\nIntuitively, we justify our definition as follows (in a goal-based setting where there is only a nonzero\nreward for reaching the goal):\n1. $p$ represents how likely the agent is to obtain positive learning experiences from a level."}, {"title": "4.2 Analysing Regret Approximations used by UED Methods", "content": "Having defined learnability, we now turn our attention to the current UED score functions. As\ndemonstrated in Section 7, the latest state-of-the-art UED methods fail to outperform Domain\nRandomisation (DR) in the multi-agent JaxNav environment. To highlight the limitations of these\napproaches, we examine whether their score functions can reliably identify the frontier of learning,\ni.e., levels that agents can only sometimes solve.\nWe focus on the single-agent version of JaxNav and conduct rollouts using the top-performing seed\nfor PLR-MaxMC on randomly sampled levels over 5000 timesteps. From these rollouts, we compile\na set of 2500 levels, evenly distributed into 10 bins based on mean success rate values ranging from 0\nto 1. We then perform additional rollouts on this collected set, running for 512 environment timesteps\n(the same number as used during training) across 10 parallel workers, and average the results. In\nFigure 2, we plot the mean MaxMC, PVL and Learnability scores against the mean success rate for\neach level. We additionally report the Pearson correlation coefficient, $r$, and p-value for the linear\nrelationship between the success rate and the regret score.\nOur analysis reveals no correlation between MaxMC and learnability, and MaxMC instead shows a\nslight correlation with success rate. While PVL has a weak correlation with learnability, the high vari-\nance causes already-solved maps to be prioritised alongside those with high learnability. These plots\ncontrast heavily with that of our learnability metric, which directly prioritises levels with the greatest\nexpected improvement. We hypothesise that the root cause of this issue is the agent's poor value\nestimation. In a highly partially observable environment, the agent struggles to accurately estimate\nthe value of a state, leading to noisy MaxMC and PVL scores, which in turn hinder UED methods\nfrom effectively identifying the learning frontier. Given that reward is strongly correlated with success\nrate, these findings also apply when comparing scores against reward, as detailed in Appendix F."}, {"title": "4.3 Sampling For Learnability: Our Simple and Intuitive Fix", "content": "Following on from our analysis, we now present Sampling For Learnability (SFL), a simple approach\nthat directly chooses levels that optimise learnability. Our approach maintains a buffer of levels with\nhigh learnability and trains on a set of levels drawn from this buffer alongside randomly generated\nlevels. Algorithm 1 outlines the overall approach for SFL and illustrates the relative simplicity of\nour method compared to SoTA UED approaches. The policy's weights $\\varphi$ are updated using any\nRL algorithm; we use PPO [16] for all of our experiments. Meanwhile, our method for collecting\nlearnable levels is detailed in Algorithm 2. We find that the default values of $T = 50, \\rho = 0.5$,\n$N_L = 256, L = 2000, N = 5000$ and $K = 1000$ work well across domains. However, the"}, {"title": "5 Experimental Setup", "content": "Here, we outline the domains and methods used for evaluation along with our adversarial evaluation\nprotocol. Rather than taking the common approach of reporting average performance on a set of hand\ndesigned levels (which by their very nature are arbitrary), we sample a large set of levels and examine\neach method's performance on their worst-case levels. This directly targets the tails of the level\ndistribution and as such is a superior measure of robustness. We additionally report the comparative\nperformance of methods on the sampled set to determine the degree to which one method dominates\nanother in terms of solvability.\nWe use four domains for our experiments, JaxNav in single-agent mode, JaxNav in multi-agent\nmode, the common UED domain Minigrid [24] and XLand-Minigrid [10]. See Appendix B for more\ndetails about the environments. We use 10 seeds for Minigrid and single-agent JaxNav, and 5 seeds\nfor multi-agent JaxNav and XLand-Minigrid. In all of our plots, we report mean and standard error.\nSince SFL performs more environment rollouts, we perform fewer PPO updates in single-agent\nJaxNav and XLand-Minigrid to ensure that SFL uses as much compute time as ACCEL. In Minigrid,\nthe additional environment interactions take a negligible amount of time, so we run the same number\nof PPO updates for all methods. For multi-agent JaxNav, we compare each method using the same\nnumber of PPO updates. See Appendix H for more information on the relative speed of each method,\nand how many updates we run for each method. Generally, the additional SFL rollouts take much less\ntime than the updates themselves, due to the massive parallelisation afforded by hardware-accelerated\nenvironments. Additionally, recent work suggests that world-models could also allow more samples to\nbe taken than the base environment allows [25-28], highlighting the future potential of this approach.\nWe compare against several state-of-the-art UED methods as baselines, implemented with JaxUED [8].\nWe use ACCEL, with the MaxMC score function, where the agent trains on randomly generated,\nmutated and curated levels. We also include a \"robust\" version [5], where no gradient updates are\nperformed on the former two sets of levels. This uses three times as many environment interactions\nand is roughly twice as slow as SFL for single-agent JaxNav. We use PLR with both the PVL and\nMaxMC score functions. We also include a robust version of PLR which only performs gradient\nupdates on the curated levels; this uses twice as many environment interactions and is 80% slower\nthan SFL on single-agent JaxNav. We also use Domain Randomisation (DR), which trains only on\nrandomly generated levels, with no curation or prioritisation."}, {"title": "6 A Risk-Based Evaluation Protocol", "content": "The standard approach to evaluating UED agents is to test them on a set of hand-designed holdout\nlevels [3, 5, 6]. Whilst this evaluation approach illustrates the performance of agents on human-\nrelevant tasks, we believe it has several limitations. First, the hand-designed levels are arbitrary, so\nperformance on them is not representative of general performance; second, it does not test a central"}, {"title": "7 Results", "content": ""}, {"title": "7.1 Single-Agent JaxNav", "content": "Figure 3a shows the CVaR results on single-agent JaxNav. We find that optimising for learnability-\nas our method does-results in superior robustness over a wide range of $\\alpha$ values, despite all methods\nperforming similarly with $\\alpha = 100\\%$ (which amounts to expected success rate over the entire\ndistribution). In this plot, we also plot the results of an oracle method named Perfect Regret. This uses\nthe same procedure as SFL but with the score function: $1 - p(success)$. Importantly (and different\nto all other methods), this method only samples solvable levels, so this metric corresponds closely\nto regret. While not shown here, using the same metric with unrestricted level sampling\u2014which is\na more realistic setting\u2014performs poorly due to it prioritising unsolvable levels. In Figure 4, we\nperform pairwise comparisons of each baseline against our approach. We find that there are a large\nnumber of environments that all methods solve (as illustrated bright top-right corner). However, the\nbottom-right of the heatmaps are generally brighter than the top-left, indicating that SFL performs\nbetter in general. Overall, SFL's superiority, and Perfect Regret's strong performance, indicates that\nthe flawed approximations of regret are responsible for UED's lack of performance. We provide\nfurther evidence for this claim by running ablations in Appendix I.2, where we use learnability as a\nscore function within PLR and ACCEL."}, {"title": "7.2 Multi-Agent JaxNav", "content": "Figures 5 and 6 illustrate the performance of all methods on multi-agent JaxNav throughout training.\nWe train with 4 agents and report performance over both a hand designed test set and a randomly\nsampled set of 100 maps. The levels used in the hand designed set are given in Appendix D and\nfeature cases with 1, 2, 4 and 10 agents. The levels in the sampled set all feature 4 agents and\nsolvability is checked for each agent's individual path. As we train with IPPO, regret scores are\ncalculated on a per-agent basis and the score of a level is computed as the mean across individual\nagent scores. For $n$ agents, learnability is computed as $\\sum_{i=1}^n (P_i\\cdot (1 - p_i))$, where $p_i$ is the success\nrate for agent $i$ on on a given level. We find that JaxNav significantly outperforms all UED methods."}, {"title": "7.3 Minigrid", "content": "We next move on to the standard domain of Minigrid (see Figure 7). Here we find that most methods\nperform similarly on the hand-designed test set; however, SFL significantly outperforms all other\nmethods on the adversarial evaluation, indicating it results in more robust policies."}, {"title": "7.4 XLand-Minigrid", "content": "Our final evaluation domain is XLand-Minigrid's [10] meta-RL task using their high-3m benchmark.\nWe report performance using our CVaR evaluation procedure and, in line with [10], as the mean return\non an evaluation set during training. Our results are presented in Figure 8, with SFL outperforming\nboth PLR and DR. During evaluation each ruleset was rolled out for 10 episodes. Due to the large\nnumber of levels being rolled out to fill SFL's buffer, SFL was slower than DR and PLR. As such, we\nreport results for SFL compute-time matched to PLR."}, {"title": "8 Related Work", "content": "Unsupervised Environment Design (UED) has emerged as a prominent method in curriculum learning\nfor reinforcement learning (RL), promising robust agent training through adaptive curricula. Early\nworks focused on learning potential, where the improvement in an agent's performance determined\nthe choice of training levels [1, 2, 29, 30]. However, robustness-oriented methods such as adversarial\nminimax introduced the notion of training on levels that minimise agent performance, though these\noften resulted in infeasible scenarios offering no learning benefits [3, 14, 31]. Minimax regret, a\nmore refined robustness approach, alleviates some issues of adversarial sampling by ensuring the\nchosen levels are learnable [3, 5, 6]. However, recent work [32] demonstrated that even when regret\ncan be exactly computed, it does not always correspond to learnability, and this mismatch can lead\nto stagnation during training. Our work extends this line of research by introducing a new scoring\nmechanism that estimates expected improvement, targeting environments with a positive but not\nperfect solve rate. Unlike existing minimax regret methods, our approach directly optimises for\nlearnability, instead of using an imperfect proxy for regret, leading to more effective training on our\ndomains. Robust RL methods have the goal of improving an agent's robustness to environmental\ndisturbances, and worst-case environment dynamics [33-45]. However, these methods generally"}, {"title": "9 Discussion and Limitations", "content": "In this work we only consider deterministic, binary outcome domains and due to the nature of the\nlearnability score, SFL is only applicable to such settings. In a non-binary-outcome domain, we\ncould potentially reuse the intuition that $p(1 - p)$ is the variance of a Bernoulli distribution; in a\ncontinuous domain, an analogous metric would be the variance of rewards obtained by playing the\nsame level multiple times. Furthermore, our implementation of SFL is in JAX but the method is\ngeneral. However, one must take the cost of SFL's additional environment rollouts into account\nwhen considering implementing our algorithm; we chose JAX because its speed and parallelisation\nsignificantly alleviates this constraint. Next, while most current SoTA UED methods, including SFL,\nrandomly generate and curate levels, this approach may become infeasible when the environment\nspace is vast, as random generation may have a very low likelihood of generating valid levels. Finally,\nwhile JaxNav does have deterministic dynamics, Fan et al. [46] had success transferring an RL-based"}, {"title": "10 Conclusion", "content": "In this paper we investigate to what extent the currently popular branch of curriculum discovery\nmethods based on scoring functions that approximate minimax regret can be applied to multi-robot\nnavigation. We find that the poor quality of the scoring functions used results in poor performance\nof UED methods, and propose a different metric based on an intuitive notion of learnability, which\nimproves the robustness of the final policies. We also introduce a new evaluation protocol, reporting\na risk measure on performance over the $\\alpha\\%$ worst-case (but in principle solvable) levels for each\nmethod. We hope that our findings provide inspiration for future work on more general and domain\nagnostic scoring functions and we open-source all of our code to facilitate this process. Ultimately,\nwe believe that this work is a stepping stone towards bridging the gap between popular testbeds for\nUED and real-world applications, such as multi-robot navigation, and serves an important reality\ncheck for this budding subfield of machine learning."}, {"title": "Appendix", "content": "We structure the appendix as follows. Appendix A includes more details about JaxNav, and Ap-\npendix B describes the other environments we use. The hyperparameters we use and the hand-\ndesigned test sets can be seen in Appendices C and D, respectively. Appendix E discusses multi-robot\nnavigation and automated curriculum learning in more detail.\nWe next provide more results, including more analysis on the UED score functions in Appendix F,\nadditional general results in Appendix G and compute-time analysis in Appendix H. Finally, we\nthoroughly ablate SFL in Appendix I."}, {"title": "A JaxNav Specification", "content": "The environment is designed as follows, with full parameters listed in Appendix C.\nObservations The robot's observation at a given timestep t is $o_t = [l_t, d_t, v_t]$ containing the\ncurrent LiDAR range readings ($l_t$), the direction to the robot's goal ($d_t$), and the robot's current linear\nand angular velocities ($v_t$). The LiDAR range readings $l_t$ is a vector containing the 100 most recent\nLiDAR range readings from a 360\u00b0 arc centred on the robot's forward axis, the LiDAR's max range\n$D_{lidar}$ is set to 6 m. Given the robot's current position $p_t$ and the goal position $g$, the robot's goal\ndirection $d_t$ is defined as:\n$d_t = \\begin{cases}\n  polar(g - p_t) & \\text{if } ||g - p_t|| \\leq D_{lidar}\\\\\n  polar(\\frac{g - p_t}{||g - p_t||}D_{lidar}) & \\text{otherwise}\n\\end{cases}$\nwhere $polar$ converts a Cartesian vector to its polar representation. All observation entries are\nnormalised using their maximum possible values.\nActions The policy selects a two-dimensional continuous action $a_t = [v_t, w_t]$ representing a target\nlinear ($v_t$) and angular velocity ($w_t$). The possible linear and angular velocities are limited to a set\nrange, with actions outside these ranges clipped to be within.\nDynamics The action $a_t$ is translated into movement in the x-y plane using a differential drive\nkinematics model [47] which includes limits on linear and angular acceleration.\nRewards Our reward function is inspired by Long et al. [48] and aims to avoid collisions while\nminimising the expected arrival time. Due to the difficulty of the task we include shaping terms which\ngive a small dense reward at each timestep. The reward $r_t$ received at timestep $t$ is defined as the sum:\n$r_t = r_t^1 + r_t^c + R_{time}$, where $r_t^1$ rewards the robot for reaching the goal, $r_t^c$ penalises collisions, and\n$R_{time}$ is a small penalty at each timestep equal to $-0.01$. The goal reaching term $r_t^1$ is defined as:\n$r_t^1 = \\begin{cases}\n  R_{goal} & \\text{if } ||p_t - g|| < D_{goal}\\\\\n  w_g(||p_t - g|| - ||p_{t-1} - g||) & \\text{otherwise}\n\\end{cases}$\nwhere $R_{goal} = 4.0, D_{goal} = 0.3$ and $w_g = 0.25$. This term rewards the agent for reaching the goal,\nand provides a small dense reward if the agent moves closer to the goal. Meanwhile, the collision\npenalty term $r_t^c$ is defined:\n$r_t^c = \\begin{cases}\n  R_{collision} & \\text{if collision}\\\\\n  R_{close} & \\text{if } \\min(l_t) < D_{close}\\\\\n  0 & \\text{otherwise}\n\\end{cases}$\nwhere $l_t$ are the un-normalised LiDAR readings at timestep t, $R_{collision} = -4, R_{close} = -0.1$, and\n$D_{close} = 0.4$m. This term avoids collisions and provides a small dense penalty when the agent is\nclose to obstacles, this encourages safe behaviour.\nMulti-Agent Reward In the multi-agent version of JaxNav, the reward for each agent i is defined\nas $\\lambda r_i + (1 - \\lambda_i) \\sum_j r_j$, i.e., it shares its own reward, as well as the team reward. We use $\\lambda = 0.7$."}, {"title": "B Environment Description", "content": "Here we describe the other environments we use, with environment parameters listed in Tables 1 to 3.\nB.1 Minigrid\nMinigrid is a goal-oriented grid world where a triangle-like agent must navigate a 2D maze. As\nillustrated in Figure 1a, the agent only observes a small region in front of where it is facing and must\nexplore the world to move to a goal location.\nB.2 XLand-Minigrid\nThis domain combines an XLand-inspired system of extensible rules and goals with a Minigrid-\ninspired goal-oriented grid world to create a domain with a diverse distribution of tasks. Each task\nis specified by a ruleset, which combines rules for environment interactions with a goal, and [10]\nprovide a database of presampled rulesets for use during training. Following [10], we use a 13x13 grid\nwith 4 rooms and sample rulesets from their high diversity benchmark with 3 million unique tasks.\nAs training involves sampling from a database of precomputed rulesets, ACCEL is not applicable.\nPLR and SFL select rulesets for each meta-RL step to maximise return on a held-out set of evaluation\nrulesets."}, {"title": "C Hyperparameters", "content": "Table 4 contains the hyperparameters we use, with their selection process for each domain outlined\nbelow. We tuned PPO for DR for each domain and then used these same PPO parameters for all\nmethods, tuning only UED-specific parameters."}, {"title": "C.1 JaxNav", "content": "For PPO, we conducted an extensive sweep on the JaxNav environment ensuring robust DR perfor-\nmance. We only tuned hyperparameters for single-agent JaxNav, and used the best hyperparameters\nfor multi-agent JaxNav. JaxNav hyperparameter searches were done over 3 seeds.\nFor PLR, we performed a grid search, over replay probabilities {0.5,0.8}, buffer capacity\n{1000, 4000, 8000}, prioritisation {rank, topk}, temperature {0.3,1.0}, and k {1,32,128}. For\nACCEL, we searched over the same set and additionally included the number of edits, where we\nconsidered values of {5, 20, 50}."}, {"title": "C.2 Minigrid", "content": "For Minigrid, our JaxNav PPO parameters performed similarly to those given in the JaxUED\nimplementation but allowed us to use 256 environment rollouts in parallel during training compared\nto JaxUED's 32. For the UED-specific parameters we used the same sweep settings as for JaxNav,\nagain conducting the search over 3 seeds."}, {"title": "C.3 XLand-Minigrid", "content": "For PPO, we used the default parameters provided by [10] and the search for UED parameters was\nconducted over 1 seed. For PLR, we conducted a grid search over replay probabilities {0.5, 0.95},\nbuffer capacity {20000, 40000}, prioritisation {rank, topk}, temperature {0.3, 1.0}, score function\n{MaxMC, PVL}. For SFL, initial experiments illustrated that, due to the number of environment\nrollouts used to fill the buffer, it was slower than PLR and DR. To use a similar compute budget as\nPLR, we conducted the sweep over only 70B timesteps. For SFL, we performed a grid search over N\nof {40000, 30000}, L {5070, 7650}, K of {8192}, T of {1, 2, 3, 4} and $\\rho$ of {0.75, 1.0}."}, {"title": "D Hand Designed Test Sets", "content": "The hand-designed levels used for evaluating policy performance throughout training are illustrated\nin Figures 9 and 10. The set used for multi-agent policies also includes the first 3 maps in Figure 9.\nMinigrid's levels are shown in Figure 11 and are the same as those used by JaxUED."}, {"title": "E Additional Background Related Work", "content": ""}, {"title": "E.1 Literature Review of Multi-Robot Navigation", "content": "Multi-robot path planning presents unique challenges due to the need for coordination among robots\nto avoid deadlocks in dynamic environments. Traditional methods often discretise the environment,\nturning the problem into a multi-agent pathfinding task managed by a central planner [49, 50", "51": "and ORCA [52", "17": ".", "53": "uses RL\nto address short-sightedness, but its state-based representation limits its adaptability. Our approach,\ninstead, employs lidar-based observations to model a more realistic and complex navigation task,\nwhich allows for better generalisation to real-world scenarios.\nMore sophisticated RL-based approaches, such as those by Long et al. [48", "54": "ndemonstrate improved performance in open spaces but struggle in constrained settings. Our method\nspecifically addresses this by focusing on environments that are solved intermittently, thereby enhanc-\ning the"}]}