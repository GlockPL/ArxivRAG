{"title": "PORCA: Root Cause Analysis with Partially Observed Data", "authors": ["Chang Gong", "Di Yao", "Jin Wang", "Wenbin Li", "Lanting Fang", "Yongtao Xie", "Kaiyu Feng", "Peng Han", "Jingping Bi"], "abstract": "Root Cause Analysis (RCA) aims at identifying the underlying causes of system faults by uncovering and analyzing the causal structure from complex systems. It has been widely used in many application domains. Reliable diagnostic conclusions are of great importance in mitigating system failures and financial losses. However, previous studies implicitly assume a full observation of the system, which neglect the effect of partial observation (i.e., missing nodes and latent malfunction). As a result, they fail in deriving reliable RCA results.\nIn this paper, we unveil the issues of unobserved confounders and heterogeneity in partial observation and come up with a new problem of root cause analysis with partially observed data. To achieve this, we propose PORCA, a novel RCA framework which can explore reliable root causes under both unobserved confounders and unobserved heterogeneity. PORCA leverages magnified score-based causal discovery to efficiently optimize acyclic directed mixed graph under unobserved confounders. In addition, we also develop a heterogeneity-aware scheduling strategy to provide adaptive sample weights. Extensive experimental results on one synthetic and two real-world datasets demonstrate the effectiveness and superiority of the proposed framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Root Cause Analysis (RCA) aims to identify the underlining system failure so as to ensure the availability and reliability. It has been widely used in many fields such as telecommu-nication [1], IT operations [2], [3], and manufacturing [4], [5]. There are complex dependencies and interactions between components in systems of these fields. Due to the large scale and complexity of such systems, they are vulnerable to failures, which could potentially lead to huge economic loss and degradation of user experiences. Many companies (e.g., Google, Amazon, Alibaba) have made huge efforts to apply RCA in system monitoring to address such issues.\nThere is a long stream of research in both data mining [3], [6], [7], [8] and software engineering [9] communities to ensure reliable RCA. The objective is to provide reliable and robust localization for engineers to help address failures in a timely manner and avoid meaningless labours due to noisy false alarms. With the development of causal inference and trustworthy machine learning [10], [11], [12], [13] techniques, recent efforts build a causal dependency graph to represent system architecture and utilize causal discovery methods to boost the RCA pipeline [7], [9], [14], [15], [16]. In such a graph, the nodes represent the performance metrics or services while edges represent the casual effects between them. However, most of existing RCA approaches impose stringent assumption on causal sufficiency and neglect the existence and potential hazards of missing data. The example in Figure 1 shows a simplified illustration of a manufacturing testbed, the actuator can manipulate both two downstream pumps. In many cases, small entities related human activities like this are often neglected or not monitored in system analysis [17], [18]. Besides, the attack or latent malfunction on the pump node in Figure 1 would change the data distribution and cause spurious edges in causal analysis. Similar issues also exist in a wide range of AIOps. For example, in microservice systems, biased models and misleading causal conclusion will result in false alarms and missing calls in root cause analysis [19], [20]. Some previous studies emphasize on the concept of \"observability\" and leverage data engineering techniques, e.g., increasing the number of monitoring metrics or dive into metric, trace, and log data iteratively, to enhance perception domain [2]. However, it is difficult to avoid such issues solely on the data level in real world application.\nWitnessing the limitation of existing solutions, we argue that RCA should be able to automatically recognize and mitigate issues caused by partial observation on the model level. To reach this goal, we need to address technical challenges from two aspects:\nUnobserved Confounders. In the partially observed data, the neglected entities in the system may serve as unobserved confounders and lead to spurious correlation in root cause analysis. Without awareness of unobserved confounders, false edges and misleading causal conclusions may be derived, which will lead to false alarms in root cause analysis. But the practical constraints and statistical limitations make it difficult to mitigate their effects. On the one hand, vanilla structural causal models are misspecified in practice and might fail to account for latent nodes [21]. On the other hand, although some studies explore the additional statistical rules [22] for uncovering relations with confounders, the heavy computa-tional cost restricts their utilization in RCA.\nUnobserved Heterogeneity. Previous studies [23], [24] have proved that the heterogeneity of exogenous factors or noise distributions will lead to spurious edges and causal conclu-sions. Nevertheless, it's nontrivial to account for heterogeneity in RCA, as they are latent to analyzers. Although some previous studies try to alleviate this via manual labeling or rule-based change point detection [7], [9], they still suffer from resource limitation or inflexiblility in dealing with different kinds of malfunctions and sophisticated attacks. What's more, it is further challenging to discover the true causal structures under the circumstance of both unobserved confounders and unobserved heterogeneity [25].\nTo address these challenges, we come up with a new problem of Partially Observed Root Cause Analysis (PORCA) that aims at tackling both issues and propose a comprehensive solution to it. To deal with unobserved confounders, we first replace existing causal models that have strong assumptions with magnified structural causal models to avoid potential model misspecification. We propose a magnified score func-tion which allows for efficient causal discovery in a gradient-descent way. In the aspect of unobserved heterogeneity, we distinguish normal and abnormal observations in a flexible manner via the distortion of causal mechanisms. To be spe-cific, we propose a heterogeneity-aware scheduling process to boost causal discovery, which distinguishes different observa-tions and regulates the optimization procedures with adaptive weights. Finally, we locate potential root causes by accounting for both node-level anomaly and anomaly propagation in deconfounded causal structures. In a nutshell, the contributions of this paper are summarized as follows:\n\u2022 We decompose the partial observation issues into un-observed confounders and unobserved heterogeneity, and define the new problem, i.e., Root Cause Analysis with Partially Observed data (PORCA).\n\u2022 We propose a novel framework to systematically support the new task 1, with the technical contributions of magnified score-based causal discovery, heterogeneity-aware schedul-ing, and deconfounded root cause localization. And we also provide a theoretical guarantee of it.\n\u2022 We conduct extensive experiments on one synthetic dataset and two datasets from real-world testbeds. Experimental re-sults demonstrate the supiriority of the proposed framework."}, {"title": "II. RELATED WORK", "content": "A. Root Cause Analysis\nRoot cause analysis aims to identify the underlying causes of system problem. It shows great practical values and has been a popular research topic in a wide range of domains [1], [2], [4]. Many causal approaches are introduced in root cause analysis to derive more reliable diagnostic conclusions [3], [9]. Most of them follow a two stage framework, which first conduct causal discovery reflecting the complex depen-dencies within the system and then leverage downstream analysis to find out potential root causes [9], [14], [26]. And some recent researches explore advanced topics, such as learning from human feedback [6], and propagation on network of networks [16]. However, few works noticed the issues originated from partial observations, i.e., unobserved confounders and unobserved heterogeneity. As for the first issue, several endeavors [9], [27] have mentioned the existence of unobserved confounders in RCA, but not accounted for it technically. Recently, Xu et al. [19] emphasize the effect of unobserved confounders. However, they focus on the design of experimental trail and debiasing, which is orthogonal to our work. As for the second issue, several studies [7], [9] try to identify data heterogeneity via rule-based change point detections while neglect the latent malfunctions. Chakraborty et al. [20] alleviate heterogeneity by introducing external nodes, which relies on domain knowledge and cannot extend to general cases. Our study proposes a framework which systematically account for both unobserved confounders and unobserved heterogeneity in root cause analysis.\nB. Causal Discovery\nCausal discovery aims at uncovering causal structures from observational data. By providing insights towards complex"}, {"title": "III. PRELIMINARY", "content": "A. Causality Basics\nFor the sake of clarity, we follow Pearl's framework [35] and first briefly describe basic concepts. Then, the idea of differentiable score-based causal discovery will be reviewed.\nA causal graph represents the causal relations between variables as the directed acyclic graph (DAG) $G=(V,E)$. V denotes the set of nodes, where each node corresponds to a variable, for example, a system metric. And $E = \\{(v_i, v_j) | v_i, v_j \\in V\\}$ denotes the set of edges, where each edge $(v_i,v_j)$ represents a directional relationship from the causal variable $v_i$ to the outcome variable $v_j$. On this ba-sis, Structural Causal Model (SCM) captures the asymmetry between causal direction on the data generation process. To be specific, SCM with additive noise can be formalized as $x = f(x)+u$, where independent noise term $u$ reflects effect of the exogenous factor. Let metric $x_i$ correspond to node $v_i$, the mapping function $f(\u00b7)$ could be deduced via the DAG $G$. As a scalable framework with promising statistical properties [35], SCM and its variants have been a mainstream methodology in causal machine learning [36], [10].\nCausal discovery aims at uncovering the graph structure corresponding to SCM, which can provide valuable insights for further analysis. Compared with previous studies, differ-entiable score-based causal discovery [21] can be seamlessly integrated with machine learning techniques and offers addi-tional advantages, such as flexibility, scalability, and the ability to model nonlinear relations. The basic idea is (i) to define a score function $S$ that evaluates how well a given causal structure fits the observed data; (ii) to specify the constraints $h(\u00b7)$ such as acyclicity; (iii) to derive causal conclusions by optimizing the score function, which can be conducted in a gradient-descent way.\nB. Problem Definition\nTo involve partially observed data into RCA, we need to explicitly account for unobserved confounders and heterogene-ity. Previous studies use acyclic directed mixed graphs (AD-MGs) [28], [37] with both directed and bidirected edges, where the latter indicates the existence of unobserved confounders. Let $d$ be the number of observed metrics, $D \\in \\mathbb{R}^{d\\times d}$ and $B \\in \\mathbb{R}^{d\\times d}$ represent directed and bidirected adjacency matrix, respectively. In this paper, we turn the mixed graphs $(D, B)$ to magnified adjacency matrix form $M \\in \\mathbb{R}^{(d+r)\\times (d+r)}$, where $r$ is the number of latent nodes $C$. To be specific, the $x_i \\rightarrow x_j$ in $B$ can be represented as $C_k \\rightarrow x_i$ and $C_k \\rightarrow x_j$ in $M$. In addition, the noise term $u$ has heterogeneous distribution with latent malfunction. To formally include above items, we leverage magnified SCM as Definition 1.\nDEFINITION 1 (Magnified Structural Causal Model): The partially observed data can be modulated as:\n$[x, c] = f_M(x, c) + u,$\nwhere $f_M(\u00b7)$ describes how observed and latent nodes being affected and can be transformed as magnified adjacency matrix $M$. And the noise term $u$ follows heterogeneous distri-butions across different observations.\nBased on the above definition, in this paper, the problem of PORCA can be formulated as following: Given observed metrics $X = \\{x_1^{1:T}, ..., x_i^{1:T}\\}$, for alarm of front-end metric $y^t$ ($y \\in X$), we aim to build a model that (i) uncovers causal structures of the system by leveraging magnified SCM to simultaneously account for unobsreved confounders and heterogeneity; (ii) identifies the top $K$ metrics in $X$ as the potential root causes of $y$.\nC. Method Overview\nAs shown in Figure 2, the proposed PORCA framework contains three concise components: (i) magnified score-based causal discovery; (ii) heterogeneity-aware scheduling; and (iii) deconfounded root cause localization. Assume $\\Theta_{MSB}$ and $\\Theta_{HAS}$ denote sets of learned parameters for magnified score-based causal discovery and heterogeneity-aware scheduling, respectively. In the causal discovery component, we optimize $\\Theta_{MSB}$ for magnified SCM. In the scheduling component, given $\\Theta_{MSB}$, we derive adaptive weights to schedule causal discovery procedures by optimizing $\\Theta_{HAS}$. In the localization component,"}, {"title": "IV. METHODOLOGY", "content": "A. Magnified Score-Based Causal Discovery\nGiven partially observed data $X = \\{x_1^{1:T}, ..., x_i^{1:T}\\}$, magnified score-based causal discovery aims to continuously op-timize parameters in maginified SCMs, which allows for inferring causal structures at the presence of unobserved con-founders in a Bayesian perspective. To this end, it first derives latent representations as posterior distribution for unobserved confounders $C$, and the magnified graph $M$. And it takes these representations and historical information as input to learn the system dynamics via neural networks. Then the algebraic constraints [21], [30] on acyclic and ancestral properties is utilized for continuously optimization. This could be realized in three steps, i.e., (i) ADMG representation learning, (ii) structural causal networks, and (iii) magnified score function.\n1) ADMG Representation Learning: The magnified score-based causal discovery process is built on the basis of the ADMG structure $M$ and the unobserved confounders $C=\\{c_1^{1:T}, ..., c_i^{1:T}\\}$. To parameterize them, we decompose the likelihood of an edge $p(M_{ij})$ via ENCO representation [34], i.e., $\\gamma_{ij}$ and $b_{ij}$ denotes the existence and direction of edges, respectively. Thus the posterior structural distribution to be approximated could be denoted as Equation (1):\n$q_{\\gamma,\\theta}(M_{ij}) \\sim Bern(sigmoid(\\gamma_{ij}).sigmoid(b_{ij})).$ (1)\nAs for the representation of unobserved confounders $C$, we parameterize it as the mixture of Gaussian distributions in Equation (2):\n$q(c) \\sim \\{\\mathcal{N}(\\mu_c, \\sigma_c)\\}_{l=1}^L$ (2)\nwhere the parameters $\\mu_c$ and $\\sigma_c^2$ are determined by MLP $f_{gauss}(x)$. The learned representation can be reckoned as C's posterior distribution given partial observations X.\n2) Structural Causal Networks: We utilize structural causal networks to model the complex nonlinear mappings entailed in magnified SCM, i.e., $x_j = f_{M,j}(x, c)$. According to the decoupling assumption [30], the formulation of structural causal network $f_{M,j}(\u00b7)$ is detailed as Equation (3):\n$x_j^t = f_{obs,j}(\\sum_{i=1}^dM_{iji} g_i(x_i^{<t}))+f_{conf,j}(\\sum_{i=d+1}^{d+r}M_{iji} g_i(C_d^{<t}))$ (3)\nWe follow [38] and let $x_i^{<t}=(x_i^{t-2},x_i^{t-1})$ denote the temporal information of variable $i$. And $f_{obs,j}(\u00b7)$ aggregates the observed variables' impact on target variable $j$, $f_{conf,j}(\u00b7)$ approximates the influence of unobserved confounders, and $g_i(\u00b7)$ introduces nonlinearity for the effect of each component $i$. They can be implemented based on MLPs.\nLet $\\Omega$ denote parameters to be optimized in structural causal networks. To reduce the search space of $\\Omega$, we introduce shared weights for similar neural networks and trainable hidden representations indicating different variables. To be specific, suppose $z$ denotes a trainbable hidden representation reflecting the target or input nodes, we have $f_{obs,j}(\u00b7) = f_{obs}(z_{j,\u00b7})$, $f_{conf,j}(\u00b7) = f_{conf}(z_{j,\u00b7})$, $g_i(\u00b7) = g(z_{i,\u00b7})$. Thus, the number of neural networks to be estimated in structural causal networks are reduced from $3d + r$ to 3, where $d$ and $r$ are the numbers of observed and latent nodes, respectively.\n3) Magnified Score Function: The score function has three terms, i.e., the data-fitting term, the structural restriction, and the restriction for unobserved confounders. Thus, we have $S = \\mathcal{L}+R_G + R_C$. The data-fitting term is given as Equation (4):\n$\\mathcal{L}(x, c, M) = \\mathbb{E}_{q_{\\gamma,\\Theta}(M)} [log p_{\\Omega} (x^t|x^{<t}, c^{<t}, M)]$ (4)\nThe structural restriction term of $S$ is implemented via KL divergence between the posterior distribution $p(M)$ and prior distribution $q_{\\gamma,\\theta}(M)$ over uncertain graph $M$ as Equation (5):\n$R_G(M) = -KL [q_{\\gamma,\\theta}(M)||p(M)],$ (5)\nThrough prior distribution, sparsity penalty, acyclic con-straints, and ancestral constraints can be added as Equation (6):\n$p(M) \\propto exp (-\\lambda||M|| - \\rho h(M)^2 - \\alpha h(M))$, (6)\nwhere $\\alpha$ and $\\rho$ are increased while optimizing score functions in the augmented Lagrangian framework [39], $\\lambda$ controls the sparse penalty. And $h(M)$ is an extension of NOTEARS constraint [21]. We first decompose $M$ into $(D, B)$. Following the theoretical results in the previous study [30], we set the algebraic constraint $h(M)$ as Equation (7):\n$h(M) = trace (e^{D}) - d + sum (e^{D \\circ B}).$ (7)\nRegarding the term of restriction for unobserved confounders $S$, the KL divergence can be calculated as Equation (8):\n$R_C (x, c) = -\\sum_{t=1}^T KL [q(c^t|x^t)\\big||p(c^t)],$ (8)\nwhere the posterior distribution of C is derived based on ADMG representation learning.\nB. Heterogeneity-Aware Scheduling\nNext we introduce how to handle unobserved heterogeneity (e.g., latent attack, device fault), which could hurt the perfor-mance of causal discovery and the following process of root cause analysis. The core idea is to develop a heterogeneity-aware scheduling approach. It is inspired by curriculum learn-ing which mocks the recognition of human-being to learn from easy tasks to hard ones [40]. It has been well explored in enhancing causal learning with heterogeneous data [23], [41]. To be more specific, we reshape the optimization procedure of score-based causal discovery with adaptive sample weights. Given learned parameters $\\Theta_{MSB}$ for magnified SCM of an iteration, we first distinguish the heterogeneity of different samples via the goodness of data reconstruction. Then we concurrently derive optimal sample weights mitigating the influences of unobserved heterogeneity."}, {"title": "V. EXPERIMENT", "content": "In this section, we evaluate the performance of PORCA and answer the following questions:\n\u2022 Q1: What is the performance of PORCA in RCA compared with previous works without considering partially observed data?\n\u2022 Q2: How could PORCA recover causal structures as inter-mediate results of RCA?\n\u2022 Q3: What are the capabilities of the deconfounding module and heterogeneity-aware scheduling module?\n\u2022 Q4: Is PORCA sensitive to parameter changes?\nMoreover, we provide a case study to illustrate the causal patterns entailed in PORCA can be beneficial to RCA in practice.\nA. Experimental Settings\n1) Datasets: We evaluate the performance of PORCA on both synthetic and real-world datasets.\nTo remedy the problem that real-world datasets have no ground-truth of causal structure or explicitly labeled fault injection, we construct a synthetic dataset named Simulation that consists of 20 nodes with 1000 timesteps. The causal structure is predefined, and the faults are injected by changing the edge weights and noise distributions. The details of the data generation procedure, including graph sampling, metrics generation, and fault injection are available in our code link. With the propagation of causal networks, we can obtain the synthetic dataset. It allows us to explicitly and quantitatively account for both unobserved confounders and unobserved heterogeneity in the experimental evaluation.\nFor the real-world scenarios, we choose two public datasets: CRACs is the monitoring data of a cooling system in a data center [43]. It consists of 38 variables from January 1st, 2023 to May 1st, 2023. The occurrences and roots of abnormal tem-perature need to be identified for stable maintenance. SWaT is monitoring data collected from a real-world water treatment testbed [44]. It consists of 51 metrics from December 22nd, 2015 to January 2nd, 2016. Physical and cyber attacks took place in the last four days.\n2) Experimental Protocol: We evaluate the performance of PORCA on two tasks i.e., root cause analysis and causal discovery. In RCA experiment, the main results are the ranking based results on three datasets. For each dataset, we randomly mask 4 nodes as the unobserved confounders. Moreover, we change the number of masked nodes to test the robustness of PORCA. In the causal discovery experiment, we report the performance of PORCA and other baselines on the synthetic and CRACs datasets whose causal structures are available.\n3) Evaluation Metrics: We apply a variety of metrics for evaluating PORCA's performance. For evaluating the perfor-mance of root cause analysis, we use Precision@K (PR@K), Precision@Average (PR@Avg), and RankScore. PR@K in-dicates the number of correct root causes among top-K predic-tions, which is defined as: PR@K. For K = 1,2,.., 5, we derive PR@Avg by averaging the values of PR@K. RankScore, ranged from 0 to 1, is leveraged to evaluate the ranking ability of RCA methods. To be specific, RankScore, where . As for evaluation on causal discovery results, two evaluation metrics are presented: Area Under the Curve (AUC) and Structural Hamming Distance (SHD). For aligning partial ancestral graphs and temporal causal graphs in the evaluation of causal discovery, we follow the similar setting as [45], (i) for usual directed edges $\\rightarrow$, we treat them as regular edges; (ii) for bidirected edges $\\leftrightarrow$, we drop them before evaluation; (iii) for other types of edges with ambiguous, only check whether skeleton is correct.\n4) Baselines: We compare PORCA with seven repre-sentative baselines which can be categorized into two groups. The first group comprises existing RCA methods, such as CloudRanger[14], MicroCause [9], AutoMAP [26], CORAL [7] and RCD [15]. These methods are specifi-cally designed for RCA without considering the influence of unobserved confounders and partially observed data. The second group consists of causal methods with unobserved confounders, such as RCD* and FCI*. To the best of our knowledge, existing solutions for RCA are incapable of model-ing the unobserved confounders. Thus, we extend RCD to the confounding settings, denoted by RCD*, for a fair comparison. As for FCI*, we select the causal discovery method FCI [28] to build the causal graph and conduct random walk to obtain the RCA results.\nB. Performance Comparison of RCA\nTo answer Q1, we compare PORCA with baseline methods in RCA. From the results shown in Table I, we have the following observations:\nFirstly, PORCA outperforms all the compared baselines by a large margin. For example, the improvements of PR@5 on the synthetic dataset are over 6% while the absolute performance is over 90%. It proves that the score-based causal discovery and heterogeneity-aware scheduling techniques are effective in modeling the unobserved confounders. Secondly, the methods accounting for the effect of unobserved nodes (e.g., RCD* and FCI*) outperform other approaches assuming causal sufficiency (e.g., CloudRanger, MicroCause, AutoMAP, CORAL, and RCD). We can see that RCD* is the ablation of RCD which only replaces the causal discovery method from $-PC with $-FCI. The PR@5 improvement on CRACS dataset is about 6% compared with its original version. This is"}, {"title": "VI. CONCLUSION", "content": "In this paper, we define the problem of Root Cause Analysis with Partially Observed data (PORCA), which is essential in ensuring availability and reliability. To tackle the issues of unobserved confounders and unobserved heterogeneity, we propose a brand new framework as the solution. We prove that PORCA is capable of identifying the true causal structures with unobserved confounders, and the learned weights can correspond to the heterogeneity. We conduct an extensive set of experiments on the synthetic dataset and real-world data from two testbeds. The results show that PORCA outperform all the compared baselines with partially observed data and works well in real-world applications."}]}